{"created":"2024-05-15 17:57:56","title":"BEHAVIOR Vision Suite: Customizable Dataset Generation via Simulation","abstract":"The systematic evaluation and understanding of computer vision models under varying conditions require large amounts of data with comprehensive and customized labels, which real-world vision datasets rarely satisfy. While current synthetic data generators offer a promising alternative, particularly for embodied AI tasks, they often fall short for computer vision tasks due to low asset and rendering quality, limited diversity, and unrealistic physical properties. We introduce the BEHAVIOR Vision Suite (BVS), a set of tools and assets to generate fully customized synthetic data for systematic evaluation of computer vision models, based on the newly developed embodied AI benchmark, BEHAVIOR-1K. BVS supports a large number of adjustable parameters at the scene level (e.g., lighting, object placement), the object level (e.g., joint configuration, attributes such as \"filled\" and \"folded\"), and the camera level (e.g., field of view, focal length). Researchers can arbitrarily vary these parameters during data generation to perform controlled experiments. We showcase three example application scenarios: systematically evaluating the robustness of models across different continuous axes of domain shift, evaluating scene understanding models on the same set of images, and training and evaluating simulation-to-real transfer for a novel vision task: unary and binary state prediction. Project website: https://behavior-vision-suite.github.io/","sentences":["The systematic evaluation and understanding of computer vision models under varying conditions require large amounts of data with comprehensive and customized labels, which real-world vision datasets rarely satisfy.","While current synthetic data generators offer a promising alternative, particularly for embodied AI tasks, they often fall short for computer vision tasks due to low asset and rendering quality, limited diversity, and unrealistic physical properties.","We introduce the BEHAVIOR Vision Suite (BVS), a set of tools and assets to generate fully customized synthetic data for systematic evaluation of computer vision models, based on the newly developed embodied AI benchmark, BEHAVIOR-1K. BVS supports a large number of adjustable parameters at the scene level (e.g., lighting, object placement), the object level (e.g., joint configuration, attributes such as \"filled\" and \"folded\"), and the camera level (e.g., field of view, focal length).","Researchers can arbitrarily vary these parameters during data generation to perform controlled experiments.","We showcase three example application scenarios: systematically evaluating the robustness of models across different continuous axes of domain shift, evaluating scene understanding models on the same set of images, and training and evaluating simulation-to-real transfer for a novel vision task: unary and binary state prediction.","Project website: https://behavior-vision-suite.github.io/"],"url":"http://arxiv.org/abs/2405.09546v1","category":"cs.CV"}
{"created":"2024-05-15 17:56:49","title":"Classifying geospatial objects from multiview aerial imagery using semantic meshes","abstract":"Aerial imagery is increasingly used in Earth science and natural resource management as a complement to labor-intensive ground-based surveys. Aerial systems can collect overlapping images that provide multiple views of each location from different perspectives. However, most prediction approaches (e.g. for tree species classification) use a single, synthesized top-down \"orthomosaic\" image as input that contains little to no information about the vertical aspects of objects and may include processing artifacts. We propose an alternate approach that generates predictions directly on the raw images and accurately maps these predictions into geospatial coordinates using semantic meshes. This method$\\unicode{x2013}$released as a user-friendly open-source toolkit$\\unicode{x2013}$enables analysts to use the highest quality data for predictions, capture information about the sides of objects, and leverage multiple viewpoints of each location for added robustness. We demonstrate the value of this approach on a new benchmark dataset of four forest sites in the western U.S. that consists of drone images, photogrammetry results, predicted tree locations, and species classification data derived from manual surveys. We show that our proposed multiview method improves classification accuracy from 53% to 75% relative to an orthomosaic baseline on a challenging cross-site tree species classification task.","sentences":["Aerial imagery is increasingly used in Earth science and natural resource management as a complement to labor-intensive ground-based surveys.","Aerial systems can collect overlapping images that provide multiple views of each location from different perspectives.","However, most prediction approaches (e.g. for tree species classification) use a single, synthesized top-down \"orthomosaic\" image as input that contains little to no information about the vertical aspects of objects and may include processing artifacts.","We propose an alternate approach that generates predictions directly on the raw images and accurately maps these predictions into geospatial coordinates using semantic meshes.","This method$\\unicode{x2013}$released as a user-friendly open-source toolkit$\\unicode{x2013}$enables analysts to use the highest quality data for predictions, capture information about the sides of objects, and leverage multiple viewpoints of each location for added robustness.","We demonstrate the value of this approach on a new benchmark dataset of four forest sites in the western U.S. that consists of drone images, photogrammetry results, predicted tree locations, and species classification data derived from manual surveys.","We show that our proposed multiview method improves classification accuracy from 53% to 75% relative to an orthomosaic baseline on a challenging cross-site tree species classification task."],"url":"http://arxiv.org/abs/2405.09544v1","category":"cs.CV"}
{"created":"2024-05-15 17:53:32","title":"Singular parabolic operators in the half-space with boundary degeneracy: Dirichlet and oblique derivative boundary conditions","abstract":"We study elliptic and parabolic problems governed by the singular elliptic operators $$ \\mathcal L=y^{\\alpha_1}\\mbox{Tr }\\left(QD^2_x\\right)+2y^{\\frac{\\alpha_1+\\alpha_2}{2}}q\\cdot \\nabla_xD_y+\\gamma y^{\\alpha_2} D_{yy}+y^{\\frac{\\alpha_1+\\alpha_2}{2}-1}\\left(d,\\nabla_x\\right)+cy^{\\alpha_2-1}D_y-by^{\\alpha_2-2}$$ in the half-space $\\mathcal{R}^{N+1}_+=\\{(x,y): x \\in \\mathcal{R}^N, y>0\\}$, under Dirichlet or oblique derivative boundary conditions. In the special case $\\alpha_1=\\alpha_2=\\alpha$ the operator $\\mathcal L$ takes the form $$ \\mathcal L=y^{\\alpha}\\mbox{Tr }\\left(AD^2\\right)+y^{\\alpha-1}\\left(v,\\nabla\\right)-by^{\\alpha-2},$$ where $v=(d,c)\\in\\mathcal{R}^{N+1}$, $b\\in\\mathcal{R}$ and $ A=\\left( \\begin{array}{c|c}   Q & { q}^t \\\\[1ex] \\hline   q& \\gamma \\end{array}\\right)$ is an elliptic matrix. We prove elliptic and parabolic $L^p$-estimates and solvability for the associated problems. In the language of semigroup theory, we prove that $\\mathcal L$ generates an analytic semigroup, characterize its domain as a weighted Sobolev space and show that it has maximal regularity.","sentences":["We study elliptic and parabolic problems governed by the singular elliptic operators $$ \\mathcal L=y^{\\alpha_1}\\mbox{Tr }\\left(QD^2_x\\right)+2y^{\\frac{\\alpha_1+\\alpha_2}{2}}q\\cdot \\nabla_xD_y+\\gamma y^{\\alpha_2} D_{yy}+y^{\\frac{\\alpha_1+\\alpha_2}{2}-1}\\left(d,\\nabla_x\\right)+cy^{\\alpha_2-1}D_y-by^{\\alpha_2-2}$$ in the half-space $\\mathcal{R}^{N+1}_+=\\{(x,y): x \\in \\mathcal{R}^N, y>0\\}$, under Dirichlet or oblique derivative boundary conditions.","In the special case $\\alpha_1=\\alpha_2=\\alpha$ the operator $\\mathcal L$ takes the form $$ \\mathcal L=y^{\\alpha}\\mbox{Tr }\\left(AD^2\\right)+y^{\\alpha-1}\\left(v,\\nabla\\right)-by^{\\alpha-2},$$ where $v=(d,c)\\in\\mathcal{R}^{N+1}$, $b\\in\\mathcal{R}$ and $ A=\\left( \\begin{array}{c|c}   Q & { q}^t \\\\[1ex] \\hline   q& \\gamma \\end{array}\\right)$ is an elliptic matrix.","We prove elliptic and parabolic $L^p$-estimates and solvability for the associated problems.","In the language of semigroup theory, we prove that $\\mathcal L$ generates an analytic semigroup, characterize its domain as a weighted Sobolev space and show that it has maximal regularity."],"url":"http://arxiv.org/abs/2405.09540v1","category":"math.AP"}
{"created":"2024-05-15 17:50:57","title":"Phonon Inverse Faraday effect from electron-phonon coupling","abstract":"The phonon inverse Faraday effect describes the emergence of a DC magnetization due to circularly polarized phonons. In this work we present a microscopic formalism for the phonon inverse Faraday effect. The formalism is based on time-dependent second order perturbation theory and electron phonon coupling. While our final equation is general and material independent, we provide estimates for the effective magnetic field expected for the ferroelectric soft mode in the oxide perovskite SrTiO$_3$. Our estimates are consistent with recent experiments showing a huge magnetization after a coherent excitation of circularly polarized phonons with THz laser light. Hence, the theoretical approach presented here is promising for shedding light into the microscopic mechanism of angular momentum transfer between ionic and electronic angular momentum, which is expected to play a central role in the phononic manipulation of magnetism.","sentences":["The phonon inverse Faraday effect describes the emergence of a DC magnetization due to circularly polarized phonons.","In this work we present a microscopic formalism for the phonon inverse Faraday effect.","The formalism is based on time-dependent second order perturbation theory and electron phonon coupling.","While our final equation is general and material independent, we provide estimates for the effective magnetic field expected for the ferroelectric soft mode in the oxide perovskite SrTiO$_3$. Our estimates are consistent with recent experiments showing a huge magnetization after a coherent excitation of circularly polarized phonons with THz laser light.","Hence, the theoretical approach presented here is promising for shedding light into the microscopic mechanism of angular momentum transfer between ionic and electronic angular momentum, which is expected to play a central role in the phononic manipulation of magnetism."],"url":"http://arxiv.org/abs/2405.09538v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-15 17:45:59","title":"Wasserstein Gradient Boosting: A General Framework with Applications to Posterior Regression","abstract":"Gradient boosting is a sequential ensemble method that fits a new base learner to the gradient of the remaining loss at each step. We propose a novel family of gradient boosting, Wasserstein gradient boosting, which fits a new base learner to an exactly or approximately available Wasserstein gradient of a loss functional on the space of probability distributions. Wasserstein gradient boosting returns a set of particles that approximates a target probability distribution assigned at each input. In probabilistic prediction, a parametric probability distribution is often specified on the space of output variables, and a point estimate of the output-distribution parameter is produced for each input by a model. Our main application of Wasserstein gradient boosting is a novel distributional estimate of the output-distribution parameter, which approximates the posterior distribution over the output-distribution parameter determined pointwise at each data point. We empirically demonstrate the superior performance of the probabilistic prediction by Wasserstein gradient boosting in comparison with various existing methods.","sentences":["Gradient boosting is a sequential ensemble method that fits a new base learner to the gradient of the remaining loss at each step.","We propose a novel family of gradient boosting, Wasserstein gradient boosting, which fits a new base learner to an exactly or approximately available Wasserstein gradient of a loss functional on the space of probability distributions.","Wasserstein gradient boosting returns a set of particles that approximates a target probability distribution assigned at each input.","In probabilistic prediction, a parametric probability distribution is often specified on the space of output variables, and a point estimate of the output-distribution parameter is produced for each input by a model.","Our main application of Wasserstein gradient boosting is a novel distributional estimate of the output-distribution parameter, which approximates the posterior distribution over the output-distribution parameter determined pointwise at each data point.","We empirically demonstrate the superior performance of the probabilistic prediction by Wasserstein gradient boosting in comparison with various existing methods."],"url":"http://arxiv.org/abs/2405.09536v1","category":"stat.ME"}
{"created":"2024-05-15 17:45:34","title":"Restoring balance: principled under/oversampling of data for optimal classification","abstract":"Class imbalance in real-world data poses a common bottleneck for machine learning tasks, since achieving good generalization on under-represented examples is often challenging. Mitigation strategies, such as under or oversampling the data depending on their abundances, are routinely proposed and tested empirically, but how they should adapt to the data statistics remains poorly understood. In this work, we determine exact analytical expressions of the generalization curves in the high-dimensional regime for linear classifiers (Support Vector Machines). We also provide a sharp prediction of the effects of under/oversampling strategies depending on class imbalance, first and second moments of the data, and the metrics of performance considered. We show that mixed strategies involving under and oversampling of data lead to performance improvement. Through numerical experiments, we show the relevance of our theoretical predictions on real datasets, on deeper architectures and with sampling strategies based on unsupervised probabilistic models.","sentences":["Class imbalance in real-world data poses a common bottleneck for machine learning tasks, since achieving good generalization on under-represented examples is often challenging.","Mitigation strategies, such as under or oversampling the data depending on their abundances, are routinely proposed and tested empirically, but how they should adapt to the data statistics remains poorly understood.","In this work, we determine exact analytical expressions of the generalization curves in the high-dimensional regime for linear classifiers (Support Vector Machines).","We also provide a sharp prediction of the effects of under/oversampling strategies depending on class imbalance, first and second moments of the data, and the metrics of performance considered.","We show that mixed strategies involving under and oversampling of data lead to performance improvement.","Through numerical experiments, we show the relevance of our theoretical predictions on real datasets, on deeper architectures and with sampling strategies based on unsupervised probabilistic models."],"url":"http://arxiv.org/abs/2405.09535v1","category":"cond-mat.dis-nn"}
{"created":"2024-05-15 17:44:42","title":"Learning-Based Compress-and-Forward Schemes for the Relay Channel","abstract":"The relay channel, consisting of a source-destination pair along with a relay, is a fundamental component of cooperative communications. While the capacity of a general relay channel remains unknown, various relaying strategies, including compress-and-forward (CF), have been proposed. In CF, the relay forwards a quantized version of its received signal to the destination. Given the correlated signals at the relay and destination, distributed compression techniques, such as Wyner--Ziv coding, can be harnessed to utilize the relay-to-destination link more efficiently. Leveraging recent advances in neural network-based distributed compression, we revisit the relay channel problem and integrate a learned task-aware Wyner--Ziv compressor into a primitive relay channel with a finite-capacity out-of-band relay-to-destination link. The resulting neural CF scheme demonstrates that our compressor recovers binning of the quantized indices at the relay, mimicking the optimal asymptotic CF strategy, although no structure exploiting the knowledge of source statistics was imposed into the design. The proposed neural CF, employing finite order modulation, operates closely to the rate achievable in a primitive relay channel with a Gaussian codebook. We showcase the advantages of exploiting the correlated destination signal for relay compression through various neural CF architectures that involve end-to-end training of the compressor and the demodulator components. Our learned task-oriented compressors provide the first proof-of-concept work toward interpretable and practical neural CF relaying schemes.","sentences":["The relay channel, consisting of a source-destination pair along with a relay, is a fundamental component of cooperative communications.","While the capacity of a general relay channel remains unknown, various relaying strategies, including compress-and-forward (CF), have been proposed.","In CF, the relay forwards a quantized version of its received signal to the destination.","Given the correlated signals at the relay and destination, distributed compression techniques, such as Wyner--Ziv coding, can be harnessed to utilize the relay-to-destination link more efficiently.","Leveraging recent advances in neural network-based distributed compression, we revisit the relay channel problem and integrate a learned task-aware Wyner--Ziv compressor into a primitive relay channel with a finite-capacity out-of-band relay-to-destination link.","The resulting neural CF scheme demonstrates that our compressor recovers binning of the quantized indices at the relay, mimicking the optimal asymptotic CF strategy, although no structure exploiting the knowledge of source statistics was imposed into the design.","The proposed neural CF, employing finite order modulation, operates closely to the rate achievable in a primitive relay channel with a Gaussian codebook.","We showcase the advantages of exploiting the correlated destination signal for relay compression through various neural CF architectures that involve end-to-end training of the compressor and the demodulator components.","Our learned task-oriented compressors provide the first proof-of-concept work toward interpretable and practical neural CF relaying schemes."],"url":"http://arxiv.org/abs/2405.09534v1","category":"cs.IT"}
{"created":"2024-05-15 17:37:28","title":"Energy-Efficient Sleep Mode Optimization of 5G mmWave Networks Using Deep Contextual MAB","abstract":"Millimeter-wave (mmWave) networks, integral to 5G communication, offer a vast spectrum that addresses the issue of spectrum scarcity and enhances peak rate and capacity. However, their dense deployment, necessary to counteract propagation losses, leads to high power consumption. An effective strategy to reduce this energy consumption in mobile networks is the sleep mode optimization (SMO) of base stations (BSs). In this paper, we propose a novel SMO approach for mmWave BSs in a 3D urban environment. This approach, which incorporates a neural network (NN) based contextual multi-armed bandit (C-MAB) with an epsilon decay algorithm, accommodates the dynamic and diverse traffic of user equipment (UE) by clustering the UEs in their respective tracking areas (TAs). Our strategy includes beamforming, which helps reduce energy consumption from the UE side, while SMO minimizes energy use from the BS perspective. We extended our investigation to include Random, Epsilon Greedy, Upper Confidence Bound (UCB), and Load Based sleep mode (SM) strategies. We compared the performance of our proposed C-MAB based SM algorithm with those of All On and other alternative approaches. Simulation results show that our proposed method outperforms all other SM strategies in terms of the $10^{th}$ percentile of user rate and average throughput while demonstrating comparable average throughput to the All On approach. Importantly, it outperforms all approaches in terms of energy efficiency (EE).","sentences":["Millimeter-wave (mmWave) networks, integral to 5G communication, offer a vast spectrum that addresses the issue of spectrum scarcity and enhances peak rate and capacity.","However, their dense deployment, necessary to counteract propagation losses, leads to high power consumption.","An effective strategy to reduce this energy consumption in mobile networks is the sleep mode optimization (SMO) of base stations (BSs).","In this paper, we propose a novel SMO approach for mmWave BSs in a 3D urban environment.","This approach, which incorporates a neural network (NN) based contextual multi-armed bandit (C-MAB) with an epsilon decay algorithm, accommodates the dynamic and diverse traffic of user equipment (UE) by clustering the UEs in their respective tracking areas (TAs).","Our strategy includes beamforming, which helps reduce energy consumption from the UE side, while SMO minimizes energy use from the BS perspective.","We extended our investigation to include Random, Epsilon Greedy, Upper Confidence Bound (UCB), and Load Based sleep mode (SM) strategies.","We compared the performance of our proposed C-MAB based SM algorithm with those of All On and other alternative approaches.","Simulation results show that our proposed method outperforms all other SM strategies in terms of the $10^{th}$ percentile of user rate and average throughput while demonstrating comparable average throughput to the All On approach.","Importantly, it outperforms all approaches in terms of energy efficiency (EE)."],"url":"http://arxiv.org/abs/2405.09528v1","category":"eess.SP"}
{"created":"2024-05-15 17:36:12","title":"Background-induced complex mass states of graviton: quantization and tensor power spectrum","abstract":"We start from the assumption that the theory of gravity can be formulated in terms of 4-dimensional action, and there are only 2 graviton polarization states, as in general relativity. It can be a non-perturbative effective action discussed in the asymptotic safety program or the result of some other UV modification of the general relativity making it a complete theory. From these general grounds, we study the properties of the graviton two-point function on top of cosmological de-Sitter space. We find that the no-ghost requirement formulated for the flat background does not necessarily hold for the de-Sitter space where the graviton two-point function can have an infinite number of complex-conjugate poles. We show that under certain stability conditions, their appearance doesn't contradict any fundamental principle. We discuss their observational consequences for the tensor power spectrum from inflation and for the stochastic gravitational wave background.","sentences":["We start from the assumption that the theory of gravity can be formulated in terms of 4-dimensional action, and there are only 2 graviton polarization states, as in general relativity.","It can be a non-perturbative effective action discussed in the asymptotic safety program or the result of some other UV modification of the general relativity making it a complete theory.","From these general grounds, we study the properties of the graviton two-point function on top of cosmological de-Sitter space.","We find that the no-ghost requirement formulated for the flat background does not necessarily hold for the de-Sitter space where the graviton two-point function can have an infinite number of complex-conjugate poles.","We show that under certain stability conditions, their appearance doesn't contradict any fundamental principle.","We discuss their observational consequences for the tensor power spectrum from inflation and for the stochastic gravitational wave background."],"url":"http://arxiv.org/abs/2405.09527v1","category":"gr-qc"}
{"created":"2024-05-15 17:33:10","title":"Improved classical shadows from local symmetries in the Schur basis","abstract":"We study the sample complexity of the classical shadows task: what is the fewest number of copies of an unknown state you need to measure to predict expected values with respect to some class of observables? Large joint measurements are likely required in order to minimize sample complexity, but previous joint measurement protocols only work when the unknown state is pure. We present the first joint measurement protocol for classical shadows whose sample complexity scales with the rank of the unknown state. In particular we prove $\\mathcal O(\\sqrt{rB}/\\epsilon^2)$ samples suffice, where $r$ is the rank of the state, $B$ is a bound on the squared Frobenius norm of the observables, and $\\epsilon$ is the target accuracy. In the low-rank regime, this is a nearly quadratic advantage over traditional approaches that use single-copy measurements.   We present several intermediate results that may be of independent interest: a solution to a new formulation of classical shadows that captures functions of non-identical input states; a generalization of a ``nice'' Schur basis used for optimal qubit purification and quantum majority vote; and a measurement strategy that allows us to use local symmetries in the Schur basis to avoid intractable Weingarten calculations in the analysis.","sentences":["We study the sample complexity of the classical shadows task: what is the fewest number of copies of an unknown state you need to measure to predict expected values with respect to some class of observables?","Large joint measurements are likely required in order to minimize sample complexity, but previous joint measurement protocols only work when the unknown state is pure.","We present the first joint measurement protocol for classical shadows whose sample complexity scales with the rank of the unknown state.","In particular we prove $\\mathcal O(\\sqrt{rB}/\\epsilon^2)$ samples suffice, where $r$ is the rank of the state, $B$ is a bound on the squared Frobenius norm of the observables, and $\\epsilon$ is the target accuracy.","In the low-rank regime, this is a nearly quadratic advantage over traditional approaches that use single-copy measurements.   ","We present several intermediate results that may be of independent interest: a solution to a new formulation of classical shadows that captures functions of non-identical input states; a generalization of a ``nice'' Schur basis used for optimal qubit purification and quantum majority vote; and a measurement strategy that allows us to use local symmetries in the Schur basis to avoid intractable Weingarten calculations in the analysis."],"url":"http://arxiv.org/abs/2405.09525v1","category":"quant-ph"}
{"created":"2024-05-15 17:24:34","title":"Towards a fully declarative neuro-symbolic language","abstract":"Neuro-symbolic systems (NeSy), which claim to combine the best of both learning and reasoning capabilities of artificial intelligence, are missing a core property of reasoning systems: Declarativeness. The lack of declarativeness is caused by the functional nature of neural predicates inherited from neural networks. We propose and implement a general framework for fully declarative neural predicates, which hence extends to fully declarative NeSy frameworks. We first show that the declarative extension preserves the learning and reasoning capabilities while being able to answer arbitrary queries while only being trained on a single query type.","sentences":["Neuro-symbolic systems (NeSy), which claim to combine the best of both learning and reasoning capabilities of artificial intelligence, are missing a core property of reasoning systems: Declarativeness.","The lack of declarativeness is caused by the functional nature of neural predicates inherited from neural networks.","We propose and implement a general framework for fully declarative neural predicates, which hence extends to fully declarative NeSy frameworks.","We first show that the declarative extension preserves the learning and reasoning capabilities while being able to answer arbitrary queries while only being trained on a single query type."],"url":"http://arxiv.org/abs/2405.09521v1","category":"cs.AI"}
{"created":"2024-05-15 17:19:28","title":"Note on the Taub-NUT Instanton Metric","abstract":"It is shown that a complex coordinate transformation maps the Taub-NUT instanton metric to a Kerr-Schild metric. This metric involves a semi-infinite line defect as a gravitational analog of the Dirac string, much like the original metric. Also, it corresponds to self-dual dyon in electromagnetism through three versions of classical double copy, one of them being a novel nonlocal operator form. The relevance to Newman-Janis algorithm is briefly noted.","sentences":["It is shown that a complex coordinate transformation maps the Taub-NUT instanton metric to a Kerr-Schild metric.","This metric involves a semi-infinite line defect as a gravitational analog of the Dirac string, much like the original metric.","Also, it corresponds to self-dual dyon in electromagnetism through three versions of classical double copy, one of them being a novel nonlocal operator form.","The relevance to Newman-Janis algorithm is briefly noted."],"url":"http://arxiv.org/abs/2405.09518v1","category":"hep-th"}
{"created":"2024-05-15 17:17:27","title":"Generalization Bounds for Causal Regression: Insights, Guarantees and Sensitivity Analysis","abstract":"Many algorithms have been recently proposed for causal machine learning. Yet, there is little to no theory on their quality, especially considering finite samples. In this work, we propose a theory based on generalization bounds that provides such guarantees. By introducing a novel change-of-measure inequality, we are able to tightly bound the model loss in terms of the deviation of the treatment propensities over the population, which we show can be empirically limited. Our theory is fully rigorous and holds even in the face of hidden confounding and violations of positivity. We demonstrate our bounds on semi-synthetic and real data, showcasing their remarkable tightness and practical utility.","sentences":["Many algorithms have been recently proposed for causal machine learning.","Yet, there is little to no theory on their quality, especially considering finite samples.","In this work, we propose a theory based on generalization bounds that provides such guarantees.","By introducing a novel change-of-measure inequality, we are able to tightly bound the model loss in terms of the deviation of the treatment propensities over the population, which we show can be empirically limited.","Our theory is fully rigorous and holds even in the face of hidden confounding and violations of positivity.","We demonstrate our bounds on semi-synthetic and real data, showcasing their remarkable tightness and practical utility."],"url":"http://arxiv.org/abs/2405.09516v1","category":"stat.ML"}
{"created":"2024-05-15 17:07:55","title":"Tackling Distribution Shifts in Task-Oriented Communication with Information Bottleneck","abstract":"Task-oriented communication aims to extract and transmit task-relevant information to significantly reduce the communication overhead and transmission latency. However, the unpredictable distribution shifts between training and test data, including domain shift and semantic shift, can dramatically undermine the system performance. In order to tackle these challenges, it is crucial to ensure that the encoded features can generalize to domain-shifted data and detect semanticshifted data, while remaining compact for transmission. In this paper, we propose a novel approach based on the information bottleneck (IB) principle and invariant risk minimization (IRM) framework. The proposed method aims to extract compact and informative features that possess high capability for effective domain-shift generalization and accurate semantic-shift detection without any knowledge of the test data during training. Specifically, we propose an invariant feature encoding approach based on the IB principle and IRM framework for domainshift generalization, which aims to find the causal relationship between the input data and task result by minimizing the complexity and domain dependence of the encoded feature. Furthermore, we enhance the task-oriented communication with the label-dependent feature encoding approach for semanticshift detection which achieves joint gains in IB optimization and detection performance. To avoid the intractable computation of the IB-based objective, we leverage variational approximation to derive a tractable upper bound for optimization. Extensive simulation results on image classification tasks demonstrate that the proposed scheme outperforms state-of-the-art approaches and achieves a better rate-distortion tradeoff.","sentences":["Task-oriented communication aims to extract and transmit task-relevant information to significantly reduce the communication overhead and transmission latency.","However, the unpredictable distribution shifts between training and test data, including domain shift and semantic shift, can dramatically undermine the system performance.","In order to tackle these challenges, it is crucial to ensure that the encoded features can generalize to domain-shifted data and detect semanticshifted data, while remaining compact for transmission.","In this paper, we propose a novel approach based on the information bottleneck (IB) principle and invariant risk minimization (IRM) framework.","The proposed method aims to extract compact and informative features that possess high capability for effective domain-shift generalization and accurate semantic-shift detection without any knowledge of the test data during training.","Specifically, we propose an invariant feature encoding approach based on the IB principle and IRM framework for domainshift generalization, which aims to find the causal relationship between the input data and task result by minimizing the complexity and domain dependence of the encoded feature.","Furthermore, we enhance the task-oriented communication with the label-dependent feature encoding approach for semanticshift detection which achieves joint gains in IB optimization and detection performance.","To avoid the intractable computation of the IB-based objective, we leverage variational approximation to derive a tractable upper bound for optimization.","Extensive simulation results on image classification tasks demonstrate that the proposed scheme outperforms state-of-the-art approaches and achieves a better rate-distortion tradeoff."],"url":"http://arxiv.org/abs/2405.09514v1","category":"eess.SP"}
{"created":"2024-05-15 17:06:57","title":"Gravitational-wave model for neutron star merger remnants with supervised learning","abstract":"We present a time-domain model for the gravitational waves emitted by equal-mass binary neutron star merger remnants for a fixed equation of state. We construct a large set of numerical relativity simulations for a single equation of state consistent with current constraints, totaling 157 equal-mass binary neutron star merger configurations. The gravitational-wave model is constructed using the supervised learning method of K-nearest neighbor regression. As a first step toward developing a general model with supervised learning methods that accounts for the dependencies on equation of state and the binary masses of the system, we explore the impact of the size of the dataset on the model. We assess the accuracy of the model for a varied dataset size and number density in total binary mass. Specifically, we consider five training sets of $\\{ 20,40, 60, 80, 100\\}$ simulations uniformly distributed in total binary mass. We evaluate the resulting models in terms of faithfulness using a test set of 30 additional simulations that are not used during training and which are equidistantly spaced in total binary mass. The models achieve faithfulness with maximum values in the range of $0.980$ to $0.995$. We assess our models simulating signals observed by the three-detector network of Advanced LIGO-Virgo. We find that all models with training sets of size equal to or larger than $40$ achieve an unbiased measurement of the main gravitational-wave frequency. We confirm that our results do not depend qualitatively on the choice of the (fixed) equation of state. We conclude that training sets, with a minimum size of $40$ simulations, or a number density of approximately $11$ simulations per $0.1\\,M_\\odot$ of total binary mass, suffice for the construction of faithful templates for the post-merger signal for a single equation of state and equal-mass binaries (abbreviated).","sentences":["We present a time-domain model for the gravitational waves emitted by equal-mass binary neutron star merger remnants for a fixed equation of state.","We construct a large set of numerical relativity simulations for a single equation of state consistent with current constraints, totaling 157 equal-mass binary neutron star merger configurations.","The gravitational-wave model is constructed using the supervised learning method of K-nearest neighbor regression.","As a first step toward developing a general model with supervised learning methods that accounts for the dependencies on equation of state and the binary masses of the system, we explore the impact of the size of the dataset on the model.","We assess the accuracy of the model for a varied dataset size and number density in total binary mass.","Specifically, we consider five training sets of $\\{ 20,40, 60, 80, 100\\}$ simulations uniformly distributed in total binary mass.","We evaluate the resulting models in terms of faithfulness using a test set of 30 additional simulations that are not used during training and which are equidistantly spaced in total binary mass.","The models achieve faithfulness with maximum values in the range of $0.980$ to $0.995$. We assess our models simulating signals observed by the three-detector network of Advanced LIGO-Virgo.","We find that all models with training sets of size equal to or larger than $40$ achieve an unbiased measurement of the main gravitational-wave frequency.","We confirm that our results do not depend qualitatively on the choice of the (fixed) equation of state.","We conclude that training sets, with a minimum size of $40$ simulations, or a number density of approximately $11$ simulations per $0.1\\,M_\\odot$ of total binary mass, suffice for the construction of faithful templates for the post-merger signal for a single equation of state and equal-mass binaries (abbreviated)."],"url":"http://arxiv.org/abs/2405.09513v1","category":"astro-ph.HE"}
{"created":"2024-05-15 17:04:44","title":"Stability via resampling: statistical problems beyond the real line","abstract":"Model averaging techniques based on resampling methods (such as bootstrapping or subsampling) have been utilized across many areas of statistics, often with the explicit goal of promoting stability in the resulting output. We provide a general, finite-sample theoretical result guaranteeing the stability of bagging when applied to algorithms that return outputs in a general space, so that the output is not necessarily a real-valued -- for example, an algorithm that estimates a vector of weights or a density function. We empirically assess the stability of bagging on synthetic and real-world data for a range of problem settings, including causal inference, nonparametric regression, and Bayesian model selection.","sentences":["Model averaging techniques based on resampling methods (such as bootstrapping or subsampling) have been utilized across many areas of statistics, often with the explicit goal of promoting stability in the resulting output.","We provide a general, finite-sample theoretical result guaranteeing the stability of bagging when applied to algorithms that return outputs in a general space, so that the output is not necessarily a real-valued -- for example, an algorithm that estimates a vector of weights or a density function.","We empirically assess the stability of bagging on synthetic and real-world data for a range of problem settings, including causal inference, nonparametric regression, and Bayesian model selection."],"url":"http://arxiv.org/abs/2405.09511v1","category":"math.ST"}
{"created":"2024-05-15 17:01:02","title":"Modeling Bilingual Sentence Processing: Evaluating RNN and Transformer Architectures for Cross-Language Structural Priming","abstract":"This study evaluates the performance of Recurrent Neural Network (RNN) and Transformer in replicating cross-language structural priming: a key indicator of abstract grammatical representations in human language processing. Focusing on Chinese-English priming, which involves two typologically distinct languages, we examine how these models handle the robust phenomenon of structural priming, where exposure to a particular sentence structure increases the likelihood of selecting a similar structure subsequently. Additionally, we utilize large language models (LLM) to measure the cross-lingual structural priming effect. Our findings indicate that Transformer outperform RNN in generating primed sentence structures, challenging the conventional belief that human sentence processing primarily involves recurrent and immediate processing and suggesting a role for cue-based retrieval mechanisms. Overall, this work contributes to our understanding of how computational models may reflect human cognitive processes in multilingual contexts.","sentences":["This study evaluates the performance of Recurrent Neural Network (RNN) and Transformer in replicating cross-language structural priming: a key indicator of abstract grammatical representations in human language processing.","Focusing on Chinese-English priming, which involves two typologically distinct languages, we examine how these models handle the robust phenomenon of structural priming, where exposure to a particular sentence structure increases the likelihood of selecting a similar structure subsequently.","Additionally, we utilize large language models (LLM) to measure the cross-lingual structural priming effect.","Our findings indicate that Transformer outperform RNN in generating primed sentence structures, challenging the conventional belief that human sentence processing primarily involves recurrent and immediate processing and suggesting a role for cue-based retrieval mechanisms.","Overall, this work contributes to our understanding of how computational models may reflect human cognitive processes in multilingual contexts."],"url":"http://arxiv.org/abs/2405.09508v1","category":"cs.CL"}
{"created":"2024-05-15 16:58:35","title":"QueryNER: Segmentation of E-commerce Queries","abstract":"We present QueryNER, a manually-annotated dataset and accompanying model for e-commerce query segmentation. Prior work in sequence labeling for e-commerce has largely addressed aspect-value extraction which focuses on extracting portions of a product title or query for narrowly defined aspects. Our work instead focuses on the goal of dividing a query into meaningful chunks with broadly applicable types. We report baseline tagging results and conduct experiments comparing token and entity dropping for null and low recall query recovery. Challenging test sets are created using automatic transformations and show how simple data augmentation techniques can make the models more robust to noise. We make the QueryNER dataset publicly available.","sentences":["We present QueryNER, a manually-annotated dataset and accompanying model for e-commerce query segmentation.","Prior work in sequence labeling for e-commerce has largely addressed aspect-value extraction which focuses on extracting portions of a product title or query for narrowly defined aspects.","Our work instead focuses on the goal of dividing a query into meaningful chunks with broadly applicable types.","We report baseline tagging results and conduct experiments comparing token and entity dropping for null and low recall query recovery.","Challenging test sets are created using automatic transformations and show how simple data augmentation techniques can make the models more robust to noise.","We make the QueryNER dataset publicly available."],"url":"http://arxiv.org/abs/2405.09507v1","category":"cs.CL"}
{"created":"2024-05-15 16:49:19","title":"Identifying Heterogeneous Decision Rules From Choices When Menus Are Unobserved","abstract":"Given only aggregate choice data and limited information about how menus are distributed across the population, we describe what can be inferred robustly about the distribution of preferences (or more general decision rules). We strengthen and generalize existing results on such identification and provide an alternative analytical approach to study the problem. We show further that our model and results are applicable, after suitable reinterpretation, to other contexts. One application is to the robust identification of the distribution of updating rules given only the population distribution of beliefs and limited information about heterogeneous information sources.","sentences":["Given only aggregate choice data and limited information about how menus are distributed across the population, we describe what can be inferred robustly about the distribution of preferences (or more general decision rules).","We strengthen and generalize existing results on such identification and provide an alternative analytical approach to study the problem.","We show further that our model and results are applicable, after suitable reinterpretation, to other contexts.","One application is to the robust identification of the distribution of updating rules given only the population distribution of beliefs and limited information about heterogeneous information sources."],"url":"http://arxiv.org/abs/2405.09500v1","category":"econ.TH"}
{"created":"2024-05-15 16:46:48","title":"A Comprehensive Survey on SmartNICs: Architectures, Development Models, Applications, and Research Directions","abstract":"The end of Moore's Law and Dennard Scaling has slowed processor improvements in the past decade. While multi-core processors have improved performance, they are limited by the application's level of parallelism, as prescribed by Amdahl's Law. This has led to the emergence of domain-specific processors that specialize in a narrow range of functions. Smart Network Interface Cards (SmartNICs) can be seen as an evolutionary technology that combines heterogeneous domain-specific processors and general-purpose cores to offload infrastructure tasks. Despite the impressive advantages of SmartNICs and their importance in modern networks, the literature has been missing a comprehensive survey. To this end, this paper provides a background encompassing an overview of the evolution of NICs from basic to SmartNICs, describing their architectures, development environments, and advantages over legacy NICs. The paper then presents a comprehensive taxonomy of applications offloaded to SmartNICs, covering network, security, storage, and machine learning functions. Challenges associated with SmartNIC development and deployment are discussed, along with current initiatives and open research issues.","sentences":["The end of Moore's Law and Dennard Scaling has slowed processor improvements in the past decade.","While multi-core processors have improved performance, they are limited by the application's level of parallelism, as prescribed by Amdahl's Law.","This has led to the emergence of domain-specific processors that specialize in a narrow range of functions.","Smart Network Interface Cards (SmartNICs) can be seen as an evolutionary technology that combines heterogeneous domain-specific processors and general-purpose cores to offload infrastructure tasks.","Despite the impressive advantages of SmartNICs and their importance in modern networks, the literature has been missing a comprehensive survey.","To this end, this paper provides a background encompassing an overview of the evolution of NICs from basic to SmartNICs, describing their architectures, development environments, and advantages over legacy NICs.","The paper then presents a comprehensive taxonomy of applications offloaded to SmartNICs, covering network, security, storage, and machine learning functions.","Challenges associated with SmartNIC development and deployment are discussed, along with current initiatives and open research issues."],"url":"http://arxiv.org/abs/2405.09499v1","category":"cs.NI"}
{"created":"2024-05-15 16:45:30","title":"Towards the limits: Sensing Capability Measurement for ISAC Through Channel Encoder","abstract":"Integrated Sensing and Communication (ISAC) is gradually becoming a reality due to the significant increase in frequency and bandwidth of next-generation wireless communication technologies. Therefore it becomes crucial to evaluate the communication and sensing performance using appropriate channel models to address resource competition from each other. Existing work only models the sensing capability based on the mutual information between the channel response and the received signal, and its theoretical resolution is difficult to support the high-precision requirements of ISAC for sensing tasks, and may even affect its communication optimal.   In this paper, we propose a sensing channel encoder model to measure the sensing capacity with higher resolution by discrete task mutual information. For the first time, derive upper and lower bounds on the sensing accuracy for a given channel. This model not only provides the possibility of optimizing the ISAC systems at a finer granularity and balancing communication and sensing resources, but also provides theoretical explanations for classical intuitive feelings (like more modalities more accuracy) in wireless sensing. Furthermore, we validate the effectiveness of the proposed channel model through real-case studies, including person identification, displacement detection, direction estimation, and device recognition. The evaluation results indicate a Pearson correlation coefficient exceeding 0.9 between our task mutual information and conventional experimental metrics (e.g., accuracy).","sentences":["Integrated Sensing and Communication (ISAC) is gradually becoming a reality due to the significant increase in frequency and bandwidth of next-generation wireless communication technologies.","Therefore it becomes crucial to evaluate the communication and sensing performance using appropriate channel models to address resource competition from each other.","Existing work only models the sensing capability based on the mutual information between the channel response and the received signal, and its theoretical resolution is difficult to support the high-precision requirements of ISAC for sensing tasks, and may even affect its communication optimal.   ","In this paper, we propose a sensing channel encoder model to measure the sensing capacity with higher resolution by discrete task mutual information.","For the first time, derive upper and lower bounds on the sensing accuracy for a given channel.","This model not only provides the possibility of optimizing the ISAC systems at a finer granularity and balancing communication and sensing resources, but also provides theoretical explanations for classical intuitive feelings (like more modalities more accuracy) in wireless sensing.","Furthermore, we validate the effectiveness of the proposed channel model through real-case studies, including person identification, displacement detection, direction estimation, and device recognition.","The evaluation results indicate a Pearson correlation coefficient exceeding 0.9 between our task mutual information and conventional experimental metrics (e.g., accuracy)."],"url":"http://arxiv.org/abs/2405.09497v1","category":"cs.IT"}
{"created":"2024-05-15 16:44:54","title":"ParaNames 1.0: Creating an Entity Name Corpus for 400+ Languages using Wikidata","abstract":"We introduce ParaNames, a massively multilingual parallel name resource consisting of 140 million names spanning over 400 languages. Names are provided for 16.8 million entities, and each entity is mapped from a complex type hierarchy to a standard type (PER/LOC/ORG). Using Wikidata as a source, we create the largest resource of this type to date. We describe our approach to filtering and standardizing the data to provide the best quality possible. ParaNames is useful for multilingual language processing, both in defining tasks for name translation/transliteration and as supplementary data for tasks such as named entity recognition and linking. We demonstrate the usefulness of ParaNames on two tasks. First, we perform canonical name translation between English and 17 other languages. Second, we use it as a gazetteer for multilingual named entity recognition, obtaining performance improvements on all 10 languages evaluated.","sentences":["We introduce ParaNames, a massively multilingual parallel name resource consisting of 140 million names spanning over 400 languages.","Names are provided for 16.8 million entities, and each entity is mapped from a complex type hierarchy to a standard type (PER/LOC/ORG).","Using Wikidata as a source, we create the largest resource of this type to date.","We describe our approach to filtering and standardizing the data to provide the best quality possible.","ParaNames is useful for multilingual language processing, both in defining tasks for name translation/transliteration and as supplementary data for tasks such as named entity recognition and linking.","We demonstrate the usefulness of ParaNames on two tasks.","First, we perform canonical name translation between English and 17 other languages.","Second, we use it as a gazetteer for multilingual named entity recognition, obtaining performance improvements on all 10 languages evaluated."],"url":"http://arxiv.org/abs/2405.09496v1","category":"cs.CL"}
{"created":"2024-05-15 16:41:07","title":"A dissipative extension to ideal hydrodynamics","abstract":"We present a formulation of special relativistic, dissipative hydrodynamics (SRDHD) derived from the well-established M\\\"uller- Israel-Stewart (MIS) formalism using an expansion in deviations from ideal behaviour. By re-summing the non-ideal terms, our approach extends the Euler equations of motion for an ideal fluid through a series of additional source terms that capture the effects of bulk viscosity, shear viscosity and heat flux. For efficiency these additional terms are built from purely spatial derivatives of the primitive fluid variables. The series expansion is parametrized by the dissipation strength and timescale coefficients, and is therefore rapidly convergent near the ideal limit. We show, using numerical simulations, that our model reproduces the dissipative fluid behaviour of other formulations. As our formulation is designed to avoid the numerical stiffness issues that arise in the traditional MIS formalism for fast relaxation timescales, it is roughly an order of magnitude faster than standard methods near the ideal limit.","sentences":["We present a formulation of special relativistic, dissipative hydrodynamics (SRDHD) derived from the well-established M\\\"uller- Israel-Stewart (MIS) formalism using an expansion in deviations from ideal behaviour.","By re-summing the non-ideal terms, our approach extends the Euler equations of motion for an ideal fluid through a series of additional source terms that capture the effects of bulk viscosity, shear viscosity and heat flux.","For efficiency these additional terms are built from purely spatial derivatives of the primitive fluid variables.","The series expansion is parametrized by the dissipation strength and timescale coefficients, and is therefore rapidly convergent near the ideal limit.","We show, using numerical simulations, that our model reproduces the dissipative fluid behaviour of other formulations.","As our formulation is designed to avoid the numerical stiffness issues that arise in the traditional MIS formalism for fast relaxation timescales, it is roughly an order of magnitude faster than standard methods near the ideal limit."],"url":"http://arxiv.org/abs/2405.09495v1","category":"gr-qc"}
{"created":"2024-05-15 16:37:09","title":"MGSER-SAM: Memory-Guided Soft Experience Replay with Sharpness-Aware Optimization for Enhanced Continual Learning","abstract":"Deep neural networks suffer from the catastrophic forgetting problem in the field of continual learning (CL). To address this challenge, we propose MGSER-SAM, a novel memory replay-based algorithm specifically engineered to enhance the generalization capabilities of CL models. We first intergrate the SAM optimizer, a component designed for optimizing flatness, which seamlessly fits into well-known Experience Replay frameworks such as ER and DER++. Then, MGSER-SAM distinctively addresses the complex challenge of reconciling conflicts in weight perturbation directions between ongoing tasks and previously stored memories, which is underexplored in the SAM optimizer. This is effectively accomplished by the strategic integration of soft logits and the alignment of memory gradient directions, where the regularization terms facilitate the concurrent minimization of various training loss terms integral to the CL process. Through rigorous experimental analysis conducted across multiple benchmarks, MGSER-SAM has demonstrated a consistent ability to outperform existing baselines in all three CL scenarios. Comparing to the representative memory replay-based baselines ER and DER++, MGSER-SAM not only improves the testing accuracy by $24.4\\%$ and $17.6\\%$ respectively, but also achieves the lowest forgetting on each benchmark.","sentences":["Deep neural networks suffer from the catastrophic forgetting problem in the field of continual learning (CL).","To address this challenge, we propose MGSER-SAM, a novel memory replay-based algorithm specifically engineered to enhance the generalization capabilities of CL models.","We first intergrate the SAM optimizer, a component designed for optimizing flatness, which seamlessly fits into well-known Experience Replay frameworks such as ER and DER++.","Then, MGSER-SAM distinctively addresses the complex challenge of reconciling conflicts in weight perturbation directions between ongoing tasks and previously stored memories, which is underexplored in the SAM optimizer.","This is effectively accomplished by the strategic integration of soft logits and the alignment of memory gradient directions, where the regularization terms facilitate the concurrent minimization of various training loss terms integral to the CL process.","Through rigorous experimental analysis conducted across multiple benchmarks, MGSER-SAM has demonstrated a consistent ability to outperform existing baselines in all three CL scenarios.","Comparing to the representative memory replay-based baselines ER and DER++, MGSER-SAM not only improves the testing accuracy by $24.4\\%$ and $17.6\\%$ respectively, but also achieves the lowest forgetting on each benchmark."],"url":"http://arxiv.org/abs/2405.09492v1","category":"cs.LG"}
{"created":"2024-05-15 16:36:54","title":"The McKay Correspondence for Dihedral Groups: The Moduli Space and the Tautological Bundles","abstract":"A conjecture in [Ish20] states that for a finite subgroup $G$ of $GL(2; \\mathbb{C})$, a resolution $Y$ of $\\mathbb{C}^2/G$ is isomorphic to a moduli space $\\mathcal{M}_{\\theta}$ of $G$-constellations for some generic stability parameter $\\theta$ if and only if $Y$ is dominated by the maximal resolution. This paper affirms the conjecture in the case of dihedral groups as a class of complex reflection groups, and offers an extension of McKay correspondence (via [IN1], [IN2], and [Ish02]).","sentences":["A conjecture in [Ish20] states that for a finite subgroup $G$ of $GL(2; \\mathbb{C})$, a resolution $Y$ of $\\mathbb{C}^2/G$ is isomorphic to a moduli space $\\mathcal{M}_{\\theta}$ of $G$-constellations for some generic stability parameter $\\theta$ if and only if $Y$ is dominated by the maximal resolution.","This paper affirms the conjecture in the case of dihedral groups as a class of complex reflection groups, and offers an extension of McKay correspondence (via [IN1], [IN2], and","[Ish02])."],"url":"http://arxiv.org/abs/2405.09491v1","category":"math.AG"}
{"created":"2024-05-15 16:24:18","title":"On the absence of the Chiral Magnetic Effect in equilibrium QCD","abstract":"In this paper we investigate the chiral magnetic effect (CME): the generation of an electric current due to a homogeneous background magnetic field and a homogeneous chiral imbalance in QCD. We demonstrate that the leading coefficient describing the CME vanishes in equilibrium, both for free fermions as well as in full QCD. Our full QCD results are based on continuum extrapolated lattice simulations using dynamical staggered quarks with physical masses as well as quenched Wilson quarks. We show that it is crucial that a gauge invariant ultraviolet regularization is used to compute the CME and elaborate on why some of the existing in-equilibrium calculations of this effect gave a nonzero result. We stress that our findings imply the absence of a time-independent CME current flowing in equilibrium QCD, but do not concern the CME as an out-of-equilibrium, time-dependent effect.","sentences":["In this paper we investigate the chiral magnetic effect (CME): the generation of an electric current due to a homogeneous background magnetic field and a homogeneous chiral imbalance in QCD.","We demonstrate that the leading coefficient describing the CME vanishes in equilibrium, both for free fermions as well as in full QCD.","Our full QCD results are based on continuum extrapolated lattice simulations using dynamical staggered quarks with physical masses as well as quenched Wilson quarks.","We show that it is crucial that a gauge invariant ultraviolet regularization is used to compute the CME and elaborate on why some of the existing in-equilibrium calculations of this effect gave a nonzero result.","We stress that our findings imply the absence of a time-independent CME current flowing in equilibrium QCD, but do not concern the CME as an out-of-equilibrium, time-dependent effect."],"url":"http://arxiv.org/abs/2405.09484v1","category":"hep-lat"}
{"created":"2024-05-15 16:22:16","title":"Beyond Flesch-Kincaid: Prompt-based Metrics Improve Difficulty Classification of Educational Texts","abstract":"Using large language models (LLMs) for educational applications like dialogue-based teaching is a hot topic. Effective teaching, however, requires teachers to adapt the difficulty of content and explanations to the education level of their students. Even the best LLMs today struggle to do this well. If we want to improve LLMs on this adaptation task, we need to be able to measure adaptation success reliably. However, current Static metrics for text difficulty, like the Flesch-Kincaid Reading Ease score, are known to be crude and brittle. We, therefore, introduce and evaluate a new set of Prompt-based metrics for text difficulty. Based on a user study, we create Prompt-based metrics as inputs for LLMs. They leverage LLM's general language understanding capabilities to capture more abstract and complex features than Static metrics. Regression experiments show that adding our Prompt-based metrics significantly improves text difficulty classification over Static metrics alone. Our results demonstrate the promise of using LLMs to evaluate text adaptation to different education levels.","sentences":["Using large language models (LLMs) for educational applications like dialogue-based teaching is a hot topic.","Effective teaching, however, requires teachers to adapt the difficulty of content and explanations to the education level of their students.","Even the best LLMs today struggle to do this well.","If we want to improve LLMs on this adaptation task, we need to be able to measure adaptation success reliably.","However, current Static metrics for text difficulty, like the Flesch-Kincaid Reading Ease score, are known to be crude and brittle.","We, therefore, introduce and evaluate a new set of Prompt-based metrics for text difficulty.","Based on a user study, we create Prompt-based metrics as inputs for LLMs.","They leverage LLM's general language understanding capabilities to capture more abstract and complex features than Static metrics.","Regression experiments show that adding our Prompt-based metrics significantly improves text difficulty classification over Static metrics alone.","Our results demonstrate the promise of using LLMs to evaluate text adaptation to different education levels."],"url":"http://arxiv.org/abs/2405.09482v1","category":"cs.CL"}
{"created":"2024-05-15 16:20:54","title":"Ghost stars in general relativity","abstract":"We explore an idea put forward many years ago by Zeldovich and Novikov concerning the existence of compact objects endowed with arbitrarily small mass. The energy-density of such objects, which we call ``Ghost stars'', is negative in some regions of the fluid distribution, producing a vanishing total mass. Thus, the interior is matched on the boundary surface to Minkowski space-time. Some exact analytical solutions are exhibited and their properties are analyzed. Observational data that could confirm or dismiss the existence of this kind of stellar object is commented.","sentences":["We explore an idea put forward many years ago by Zeldovich and Novikov concerning the existence of compact objects endowed with arbitrarily small mass.","The energy-density of such objects, which we call ``Ghost stars'', is negative in some regions of the fluid distribution, producing a vanishing total mass.","Thus, the interior is matched on the boundary surface to Minkowski space-time.","Some exact analytical solutions are exhibited and their properties are analyzed.","Observational data that could confirm or dismiss the existence of this kind of stellar object is commented."],"url":"http://arxiv.org/abs/2405.09480v1","category":"gr-qc"}
{"created":"2024-05-15 16:16:37","title":"Harmonizing Human Insights and AI Precision: Hand in Hand for Advancing Knowledge Graph Task","abstract":"Knowledge graph embedding (KGE) has caught significant interest for its effectiveness in knowledge graph completion (KGC), specifically link prediction (LP), with recent KGE models cracking the LP benchmarks. Despite the rapidly growing literature, insufficient attention has been paid to the cooperation between humans and AI on KG. However, humans' capability to analyze graphs conceptually may further improve the efficacy of KGE models with semantic information. To this effect, we carefully designed a human-AI team (HAIT) system dubbed KG-HAIT, which harnesses the human insights on KG by leveraging fully human-designed ad-hoc dynamic programming (DP) on KG to produce human insightful feature (HIF) vectors that capture the subgraph structural feature and semantic similarities. By integrating HIF vectors into the training of KGE models, notable improvements are observed across various benchmarks and metrics, accompanied by accelerated model convergence. Our results underscore the effectiveness of human-designed DP in the task of LP, emphasizing the pivotal role of collaboration between humans and AI on KG. We open avenues for further exploration and innovation through KG-HAIT, paving the way towards more effective and insightful KG analysis techniques.","sentences":["Knowledge graph embedding (KGE) has caught significant interest for its effectiveness in knowledge graph completion (KGC), specifically link prediction (LP), with recent KGE models cracking the LP benchmarks.","Despite the rapidly growing literature, insufficient attention has been paid to the cooperation between humans and AI on KG.","However, humans' capability to analyze graphs conceptually may further improve the efficacy of KGE models with semantic information.","To this effect, we carefully designed a human-AI team (HAIT) system dubbed KG-HAIT, which harnesses the human insights on KG by leveraging fully human-designed ad-hoc dynamic programming (DP) on KG to produce human insightful feature (HIF) vectors that capture the subgraph structural feature and semantic similarities.","By integrating HIF vectors into the training of KGE models, notable improvements are observed across various benchmarks and metrics, accompanied by accelerated model convergence.","Our results underscore the effectiveness of human-designed DP in the task of LP, emphasizing the pivotal role of collaboration between humans and AI on KG.","We open avenues for further exploration and innovation through KG-HAIT, paving the way towards more effective and insightful KG analysis techniques."],"url":"http://arxiv.org/abs/2405.09477v1","category":"cs.LG"}
{"created":"2024-05-15 16:11:49","title":"Robust inference of gravitational wave source parameters in the presence of noise transients using normalizing flows","abstract":"Gravitational wave (GW) detection is of paramount importance in fundamental physics and GW astronomy, yet it presents formidable challenges. One significant challenge is the removal of noise transient artifacts known as ``glitches,\" which greatly impact the search and identification of GWs. Recent research has achieved remarkable results in data denoising, often using effective modeling methods to remove glitches. However, for glitches from uncertain or unknown sources, current methods cannot completely eliminate them from the GW signal. In this work, we leverage the inherent robustness of machine learning to obtain reliable posterior parameter distributions directly from GW data contaminated by glitches. Our network model provides reasonable and rapid parameter inference even in the presence of glitches, without needing to remove them. We also investigate various factors affecting the rationality of parameter inference in our normalizing flow network, including glitch and GW parameters. The results demonstrate that the normalizing flow can reasonably infer the source parameters of GWs even with unknown contamination. We find that the nature of the glitch itself is the only factor that can affect the rationality of the inferred results. With improvements to our model, we anticipate accelerating the localization of electromagnetic counterparts and providing priors for more accurate deglitching, thereby speeding up subsequent data processing procedures.","sentences":["Gravitational wave (GW) detection is of paramount importance in fundamental physics and GW astronomy, yet it presents formidable challenges.","One significant challenge is the removal of noise transient artifacts known as ``glitches,\" which greatly impact the search and identification of GWs.","Recent research has achieved remarkable results in data denoising, often using effective modeling methods to remove glitches.","However, for glitches from uncertain or unknown sources, current methods cannot completely eliminate them from the GW signal.","In this work, we leverage the inherent robustness of machine learning to obtain reliable posterior parameter distributions directly from GW data contaminated by glitches.","Our network model provides reasonable and rapid parameter inference even in the presence of glitches, without needing to remove them.","We also investigate various factors affecting the rationality of parameter inference in our normalizing flow network, including glitch and GW parameters.","The results demonstrate that the normalizing flow can reasonably infer the source parameters of GWs even with unknown contamination.","We find that the nature of the glitch itself is the only factor that can affect the rationality of the inferred results.","With improvements to our model, we anticipate accelerating the localization of electromagnetic counterparts and providing priors for more accurate deglitching, thereby speeding up subsequent data processing procedures."],"url":"http://arxiv.org/abs/2405.09475v1","category":"gr-qc"}
{"created":"2024-05-15 16:09:42","title":"Some 1d (Supersymmetric) Quantum Field Theories Reduced from Chern-Simons Gauge Theories","abstract":"We study dimensional reduction by spherical symmetry in the context of Euclidean Chern-Simons gauge theories with gauge groups $SU(2)$ and $SL(2,\\mathbb{C})$ (de Sitter gravity). We start with pure Chern-Simons theories and show that the reduced theory obtained by employing a spherically symmetric ansatz on the gauge field is similar to a fermionic theory in 1d coupled to a $U(1)$ gauge field with a 1-dimensional Chern-Simons kinetic term. Furthermore, we carry out a semiclassical computation that demonstrates that in the large $\\kappa$ limit, the path integral of the 3d pure Chern-Simons theory is equivalent to the path integral of the dimensionally reduced theory, which is a 1d QFT. Moreover, we argue that the Wilson loop operators in the large $\\kappa$ limit should reduce to some observables of the 1-dimensional theory. This agreement hints at the existence of a duality at the quantum level between the 3d Chern-Simons theory and the 1d reduced theory. We then study Chern-Simons Higgs theories in the Euclidean formalism. We again employ the spherically symmetric ansatz\\\"e on the fields, and show that at the clasical level, the action of the reduced theory is a theory with some scalar fields $\\mathsf{X}_i$ coupled to fermionic fields $\\Psi_i$. To get an appropriate reduced action, we define the original theory on a manifold $\\mathcal{M}$ with the geometry of $\\mathbb{R} \\times S^2$. With a suitable choice of the potential of the Higgs field, we show that the reduced theory, which is the action that governs the spherically symmetric monopoles in the Chern-Simons Higgs theory, is equivalent to a supersymmetric quantum mechanical model. The flux of the Chern-Simons Higgs monopole is shown to be related to the fermionic number operator $F = \\overline{\\Psi} \\Psi$, which in turn is related to the Witten index.","sentences":["We study dimensional reduction by spherical symmetry in the context of Euclidean Chern-Simons gauge theories with gauge groups $SU(2)$ and $SL(2,\\mathbb{C})$ (de Sitter gravity).","We start with pure Chern-Simons theories and show that the reduced theory obtained by employing a spherically symmetric ansatz on the gauge field is similar to a fermionic theory in 1d coupled to a $U(1)$ gauge field with a 1-dimensional Chern-Simons kinetic term.","Furthermore, we carry out a semiclassical computation that demonstrates that in the large $\\kappa$ limit, the path integral of the 3d pure Chern-Simons theory is equivalent to the path integral of the dimensionally reduced theory, which is a 1d QFT.","Moreover, we argue that the Wilson loop operators in the large $\\kappa$ limit should reduce to some observables of the 1-dimensional theory.","This agreement hints at the existence of a duality at the quantum level between the 3d Chern-Simons theory and the 1d reduced theory.","We then study Chern-Simons Higgs theories in the Euclidean formalism.","We again employ the spherically symmetric ansatz\\\"e on the fields, and show that at the clasical level, the action of the reduced theory is a theory with some scalar fields $\\mathsf{X}_i$ coupled to fermionic fields $\\Psi_i$. To get an appropriate reduced action, we define the original theory on a manifold $\\mathcal{M}$ with the geometry of $\\mathbb{R} \\times S^2$.","With a suitable choice of the potential of the Higgs field, we show that the reduced theory, which is the action that governs the spherically symmetric monopoles in the Chern-Simons Higgs theory, is equivalent to a supersymmetric quantum mechanical model.","The flux of the Chern-Simons Higgs monopole is shown to be related to the fermionic number operator $F = \\overline{\\Psi} \\Psi$, which in turn is related to the Witten index."],"url":"http://arxiv.org/abs/2405.09473v1","category":"hep-th"}
{"created":"2024-05-15 16:09:22","title":"Perception- and Fidelity-aware Reduced-Reference Super-Resolution Image Quality Assessment","abstract":"With the advent of image super-resolution (SR) algorithms, how to evaluate the quality of generated SR images has become an urgent task. Although full-reference methods perform well in SR image quality assessment (SR-IQA), their reliance on high-resolution (HR) images limits their practical applicability. Leveraging available reconstruction information as much as possible for SR-IQA, such as low-resolution (LR) images and the scale factors, is a promising way to enhance assessment performance for SR-IQA without HR for reference. In this letter, we attempt to evaluate the perceptual quality and reconstruction fidelity of SR images considering LR images and scale factors. Specifically, we propose a novel dual-branch reduced-reference SR-IQA network, \\ie, Perception- and Fidelity-aware SR-IQA (PFIQA). The perception-aware branch evaluates the perceptual quality of SR images by leveraging the merits of global modeling of Vision Transformer (ViT) and local relation of ResNet, and incorporating the scale factor to enable comprehensive visual perception. Meanwhile, the fidelity-aware branch assesses the reconstruction fidelity between LR and SR images through their visual perception. The combination of the two branches substantially aligns with the human visual system, enabling a comprehensive SR image evaluation. Experimental results indicate that our PFIQA outperforms current state-of-the-art models across three widely-used SR-IQA benchmarks. Notably, PFIQA excels in assessing the quality of real-world SR images.","sentences":["With the advent of image super-resolution (SR) algorithms, how to evaluate the quality of generated SR images has become an urgent task.","Although full-reference methods perform well in SR image quality assessment (SR-IQA), their reliance on high-resolution (HR) images limits their practical applicability.","Leveraging available reconstruction information as much as possible for SR-IQA, such as low-resolution (LR) images and the scale factors, is a promising way to enhance assessment performance for SR-IQA without HR for reference.","In this letter, we attempt to evaluate the perceptual quality and reconstruction fidelity of SR images considering LR images and scale factors.","Specifically, we propose a novel dual-branch reduced-reference SR-IQA network, \\ie, Perception- and Fidelity-aware SR-IQA (PFIQA).","The perception-aware branch evaluates the perceptual quality of SR images by leveraging the merits of global modeling of Vision Transformer (ViT) and local relation of ResNet, and incorporating the scale factor to enable comprehensive visual perception.","Meanwhile, the fidelity-aware branch assesses the reconstruction fidelity between LR and SR images through their visual perception.","The combination of the two branches substantially aligns with the human visual system, enabling a comprehensive SR image evaluation.","Experimental results indicate that our PFIQA outperforms current state-of-the-art models across three widely-used SR-IQA benchmarks.","Notably, PFIQA excels in assessing the quality of real-world SR images."],"url":"http://arxiv.org/abs/2405.09472v1","category":"eess.IV"}
{"created":"2024-05-15 16:09:02","title":"Charged scalar bosons in a Bonnor-Melvin-$\u039b$ universe at conical approximation","abstract":"The quantum dynamics of charged scalar bosons in a Bonnor-Melvin-$\\Lambda$ universe is considered. In this study, the behavior of charged scalar bosons is explored within the framework of the Duffin-Kemmer-Petiau (DKP) formalism. Adopting a conical approximation ($\\Lambda\\ll 1$), we are considered two scenarios for the vector potential: a linear and quadratic vector potentials. In particular, the effects of this background in the equation of motion, phase shift, $S$-matrix, energy spectrum and DKP spinor are analyzed and discussed.","sentences":["The quantum dynamics of charged scalar bosons in a Bonnor-Melvin-$\\Lambda$ universe is considered.","In this study, the behavior of charged scalar bosons is explored within the framework of the Duffin-Kemmer-Petiau (DKP) formalism.","Adopting a conical approximation ($\\Lambda\\ll 1$), we are considered two scenarios for the vector potential: a linear and quadratic vector potentials.","In particular, the effects of this background in the equation of motion, phase shift, $S$-matrix, energy spectrum and DKP spinor are analyzed and discussed."],"url":"http://arxiv.org/abs/2405.09471v1","category":"gr-qc"}
{"created":"2024-05-15 16:03:51","title":"Predicting Binary Neutron Star Postmerger Spectra Using Artificial Neural Networks","abstract":"Gravitational waves in the postmerger phase of binary neutron star mergers may become detectable with planned upgrades of existing gravitational-wave detectors or with more sensitive next-generation detectors. The construction of template banks for the postmerger phase can facilitate signal detection and parameter estimation. Here, we investigate the performance of an artificial neural network in predicting simulation-based waveforms in the frequency domain (restricted to the magnitude of the frequency spectrum and to equal-mass models) that depend on three parameters that can be inferred through observations, neutron star mass, tidal deformability, and the gradient of radius versus mass. Compared to a baseline study using multiple linear regression, we find that the artificial neural network can predict waveforms with higher accuracy and more consistent performance in a cross-validation study. We also demonstrate, through a recalibration procedure, that future reduction of uncertainties in empirical relations that are used in our hierarchical scheme will result in more accurate predicted postmerger spectra.","sentences":["Gravitational waves in the postmerger phase of binary neutron star mergers may become detectable with planned upgrades of existing gravitational-wave detectors or with more sensitive next-generation detectors.","The construction of template banks for the postmerger phase can facilitate signal detection and parameter estimation.","Here, we investigate the performance of an artificial neural network in predicting simulation-based waveforms in the frequency domain (restricted to the magnitude of the frequency spectrum and to equal-mass models) that depend on three parameters that can be inferred through observations, neutron star mass, tidal deformability, and the gradient of radius versus mass.","Compared to a baseline study using multiple linear regression, we find that the artificial neural network can predict waveforms with higher accuracy and more consistent performance in a cross-validation study.","We also demonstrate, through a recalibration procedure, that future reduction of uncertainties in empirical relations that are used in our hierarchical scheme will result in more accurate predicted postmerger spectra."],"url":"http://arxiv.org/abs/2405.09468v1","category":"gr-qc"}
{"created":"2024-05-15 15:58:12","title":"Scalable Scheduling Policies for Quantum Satellite Networks","abstract":"As Low Earth Orbit (LEO) satellite mega constellations continue to be deployed for satellite internet and recent successful experiments in satellite-based quantum entanglement distribution emerge, a natural question arises: How should we coordinate transmissions and design scalable scheduling policies for a quantum satellite internet? In this work, we consider the problem of transmission scheduling in quantum satellite networks subject to resource constraints at the satellites and ground stations. We show that the most general problem of assigning satellites to ground station pairs for entanglement distribution is NP-hard. We then propose four heuristic algorithms and evaluate their performance for Starlink mega constellation under various amount of resources and placements of the ground stations. We find that the maximum number of receivers necessary per ground station grows very slowly with the total number of deployed ground stations. Our proposed algorithms, leveraging optimal weighted b-matching and the global greedy heuristic, outperform others in entanglement distribution rate, entanglement fidelity, and handover cost metrics. While we develop these scheduling algorithms, we have also designed a software system to simulate, visualize, and evaluate satellite mega-constellations for entanglement distribution.","sentences":["As Low Earth Orbit (LEO) satellite mega constellations continue to be deployed for satellite internet and recent successful experiments in satellite-based quantum entanglement distribution emerge, a natural question arises: How should we coordinate transmissions and design scalable scheduling policies for a quantum satellite internet?","In this work, we consider the problem of transmission scheduling in quantum satellite networks subject to resource constraints at the satellites and ground stations.","We show that the most general problem of assigning satellites to ground station pairs for entanglement distribution is NP-hard.","We then propose four heuristic algorithms and evaluate their performance for Starlink mega constellation under various amount of resources and placements of the ground stations.","We find that the maximum number of receivers necessary per ground station grows very slowly with the total number of deployed ground stations.","Our proposed algorithms, leveraging optimal weighted b-matching and the global greedy heuristic, outperform others in entanglement distribution rate, entanglement fidelity, and handover cost metrics.","While we develop these scheduling algorithms, we have also designed a software system to simulate, visualize, and evaluate satellite mega-constellations for entanglement distribution."],"url":"http://arxiv.org/abs/2405.09464v1","category":"quant-ph"}
{"created":"2024-05-15 15:54:25","title":"The OU$^2$ process: Characterising dissipative confinement in noisy traps","abstract":"The Ornstein-Uhlenbeck (OU) process describes the dynamics of Brownian particles in a confining harmonic potential, thereby constituting the paradigmatic model of overdamped, mean-reverting Langevin dynamics. Despite its widespread applicability, this model falls short when describing physical systems where the confining potential is itself subjected to stochastic fluctuations. However, such stochastic fluctuations generically emerge in numerous situations, including in the context of colloidal manipulation by optical tweezers, leading to inherently out-of-equilibrium trapped dynamics. To explore the consequences of stochasticity at this level, we introduce a natural extension of the OU process, in which the stiffness of the harmonic potential is itself subjected to OU-like fluctuations. We call this model the OU$^2$ process. We examine its statistical, dynamic, and thermodynamic properties through a combination of analytical and numerical methods. Importantly, we show that the probability density for the particle position presents power-law tails, in contrast to the Gaussian decay of the standard OU process. In turn, this causes the trapping behavior, extreme value statistics, first passage statistics, and entropy production of the OU$^2$ process to differ qualitatively from their standard OU counterpart. Due to the wide applicability of the standard OU process and of the proposed OU$^2$ generalisation, our study sheds light on the peculiar properties of stochastic dynamics in random potentials and lays the foundation for the refined analysis of the dynamics and thermodynamics of numerous experimental systems.","sentences":["The Ornstein-Uhlenbeck (OU) process describes the dynamics of Brownian particles in a confining harmonic potential, thereby constituting the paradigmatic model of overdamped, mean-reverting Langevin dynamics.","Despite its widespread applicability, this model falls short when describing physical systems where the confining potential is itself subjected to stochastic fluctuations.","However, such stochastic fluctuations generically emerge in numerous situations, including in the context of colloidal manipulation by optical tweezers, leading to inherently out-of-equilibrium trapped dynamics.","To explore the consequences of stochasticity at this level, we introduce a natural extension of the OU process, in which the stiffness of the harmonic potential is itself subjected to OU-like fluctuations.","We call this model the OU$^2$ process.","We examine its statistical, dynamic, and thermodynamic properties through a combination of analytical and numerical methods.","Importantly, we show that the probability density for the particle position presents power-law tails, in contrast to the Gaussian decay of the standard OU process.","In turn, this causes the trapping behavior, extreme value statistics, first passage statistics, and entropy production of the OU$^2$ process to differ qualitatively from their standard OU counterpart.","Due to the wide applicability of the standard OU process and of the proposed OU$^2$ generalisation, our study sheds light on the peculiar properties of stochastic dynamics in random potentials and lays the foundation for the refined analysis of the dynamics and thermodynamics of numerous experimental systems."],"url":"http://arxiv.org/abs/2405.09460v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-15 15:52:27","title":"Fourier Boundary Features Network with Wider Catchers for Glass Segmentation","abstract":"Glass largely blurs the boundary between the real world and the reflection. The special transmittance and reflectance quality have confused the semantic tasks related to machine vision. Therefore, how to clear the boundary built by glass, and avoid over-capturing features as false positive information in deep structure, matters for constraining the segmentation of reflection surface and penetrating glass. We proposed the Fourier Boundary Features Network with Wider Catchers (FBWC), which might be the first attempt to utilize sufficiently wide horizontal shallow branches without vertical deepening for guiding the fine granularity segmentation boundary through primary glass semantic information. Specifically, we designed the Wider Coarse-Catchers (WCC) for anchoring large area segmentation and reducing excessive extraction from a structural perspective. We embed fine-grained features by Cross Transpose Attention (CTA), which is introduced to avoid the incomplete area within the boundary caused by reflection noise. For excavating glass features and balancing high-low layers context, a learnable Fourier Convolution Controller (FCC) is proposed to regulate information integration robustly. The proposed method has been validated on three different public glass segmentation datasets. Experimental results reveal that the proposed method yields better segmentation performance compared with the state-of-the-art (SOTA) methods in glass image segmentation.","sentences":["Glass largely blurs the boundary between the real world and the reflection.","The special transmittance and reflectance quality have confused the semantic tasks related to machine vision.","Therefore, how to clear the boundary built by glass, and avoid over-capturing features as false positive information in deep structure, matters for constraining the segmentation of reflection surface and penetrating glass.","We proposed the Fourier Boundary Features Network with Wider Catchers (FBWC), which might be the first attempt to utilize sufficiently wide horizontal shallow branches without vertical deepening for guiding the fine granularity segmentation boundary through primary glass semantic information.","Specifically, we designed the Wider Coarse-Catchers (WCC) for anchoring large area segmentation and reducing excessive extraction from a structural perspective.","We embed fine-grained features by Cross Transpose Attention (CTA), which is introduced to avoid the incomplete area within the boundary caused by reflection noise.","For excavating glass features and balancing high-low layers context, a learnable Fourier Convolution Controller (FCC) is proposed to regulate information integration robustly.","The proposed method has been validated on three different public glass segmentation datasets.","Experimental results reveal that the proposed method yields better segmentation performance compared with the state-of-the-art (SOTA) methods in glass image segmentation."],"url":"http://arxiv.org/abs/2405.09459v1","category":"cs.CV"}
{"created":"2024-05-15 15:51:35","title":"Recurrence solution of monomer-polymer models on two-dimensional rectangular lattices","abstract":"The problem of counting polymer coverings on the rectangular lattices is investigated. In this model, a linear rigid polymer covers $k$ adjacent lattice sites such that no two polymers occupy a common site. Those unoccupied lattice sites are considered as monomers. We prove that for a given number of polymers ($k$-mers), the number of arrangements for the polymers on two-dimensional rectangular lattices satisfies simple recurrence relations. These recurrence relations are quite general and apply for arbitrary polymer length ($k$) and the width of the lattices ($n$). The well-studied monomer-dimer problem is a special case of the monomer-polymer model when $k=2$. It is known the enumeration of monomer-dimer configurations in planar lattices is #P-complete. The recurrence relations shown here have the potential for hints for the solution of long-standing problems in this class of computational complexity.","sentences":["The problem of counting polymer coverings on the rectangular lattices is investigated.","In this model, a linear rigid polymer covers $k$ adjacent lattice sites such that no two polymers occupy a common site.","Those unoccupied lattice sites are considered as monomers.","We prove that for a given number of polymers ($k$-mers), the number of arrangements for the polymers on two-dimensional rectangular lattices satisfies simple recurrence relations.","These recurrence relations are quite general and apply for arbitrary polymer length ($k$) and the width of the lattices ($n$).","The well-studied monomer-dimer problem is a special case of the monomer-polymer model when $k=2$. It is known the enumeration of monomer-dimer configurations in planar lattices is #P-complete.","The recurrence relations shown here have the potential for hints for the solution of long-standing problems in this class of computational complexity."],"url":"http://arxiv.org/abs/2405.09457v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-15 15:50:51","title":"Photonic Landau levels in a high-dimensional frequency-degenerate cavity","abstract":"Topological orders emerge in both microscopic quantum dynamics and macroscopic materials as a fundamental principle to characterize intricate properties in nature with vital significance, for instance, the Landau levels of electron systems in magnetic field. Whilst, recent advances of synthetic photonic systems enable generalized concepts of Landau levels across fermionic and bosonic systems, extending the modern physical frontier. However, the controls of Landau levels of photons were only confined in complex artificial metamaterials or multifolded cavities. Here, we exploit advanced structured light laser technology and propose the theory of high-dimensional frequency-degeneracy, which enables photonic Landau level control in a linear open laser cavity with simple displacement tuning of intracavity elements. This work not only create novel structured light with new topological effects but also provides broad prospects for Bose-analogue quantum Hall effects and topological physics.","sentences":["Topological orders emerge in both microscopic quantum dynamics and macroscopic materials as a fundamental principle to characterize intricate properties in nature with vital significance, for instance, the Landau levels of electron systems in magnetic field.","Whilst, recent advances of synthetic photonic systems enable generalized concepts of Landau levels across fermionic and bosonic systems, extending the modern physical frontier.","However, the controls of Landau levels of photons were only confined in complex artificial metamaterials or multifolded cavities.","Here, we exploit advanced structured light laser technology and propose the theory of high-dimensional frequency-degeneracy, which enables photonic Landau level control in a linear open laser cavity with simple displacement tuning of intracavity elements.","This work not only create novel structured light with new topological effects but also provides broad prospects for Bose-analogue quantum Hall effects and topological physics."],"url":"http://arxiv.org/abs/2405.09456v1","category":"physics.optics"}
{"created":"2024-05-15 15:49:06","title":"Tell Me Why: Explainable Public Health Fact-Checking with Large Language Models","abstract":"This paper presents a comprehensive analysis of explainable fact-checking through a series of experiments, focusing on the ability of large language models to verify public health claims and provide explanations or justifications for their veracity assessments. We examine the effectiveness of zero/few-shot prompting and parameter-efficient fine-tuning across various open and closed-source models, examining their performance in both isolated and joint tasks of veracity prediction and explanation generation. Importantly, we employ a dual evaluation approach comprising previously established automatic metrics and a novel set of criteria through human evaluation. Our automatic evaluation indicates that, within the zero-shot scenario, GPT-4 emerges as the standout performer, but in few-shot and parameter-efficient fine-tuning contexts, open-source models demonstrate their capacity to not only bridge the performance gap but, in some instances, surpass GPT-4. Human evaluation reveals yet more nuance as well as indicating potential problems with the gold explanations.","sentences":["This paper presents a comprehensive analysis of explainable fact-checking through a series of experiments, focusing on the ability of large language models to verify public health claims and provide explanations or justifications for their veracity assessments.","We examine the effectiveness of zero/few-shot prompting and parameter-efficient fine-tuning across various open and closed-source models, examining their performance in both isolated and joint tasks of veracity prediction and explanation generation.","Importantly, we employ a dual evaluation approach comprising previously established automatic metrics and a novel set of criteria through human evaluation.","Our automatic evaluation indicates that, within the zero-shot scenario, GPT-4 emerges as the standout performer, but in few-shot and parameter-efficient fine-tuning contexts, open-source models demonstrate their capacity to not only bridge the performance gap but, in some instances, surpass GPT-4.","Human evaluation reveals yet more nuance as well as indicating potential problems with the gold explanations."],"url":"http://arxiv.org/abs/2405.09454v1","category":"cs.CL"}
{"created":"2024-05-15 15:48:11","title":"Kuramoto Oscillators and Swarms on Manifolds for Geometry Informed Machine Learning","abstract":"We propose the idea of using Kuramoto models (including their higher-dimensional generalizations) for machine learning over non-Euclidean data sets. These models are systems of matrix ODE's describing collective motions (swarming dynamics) of abstract particles (generalized oscillators) on spheres, homogeneous spaces and Lie groups. Such models have been extensively studied from the beginning of XXI century both in statistical physics and control theory. They provide a suitable framework for encoding maps between various manifolds and are capable of learning over spherical and hyperbolic geometries. In addition, they can learn coupled actions of transformation groups (such as special orthogonal, unitary and Lorentz groups). Furthermore, we overview families of probability distributions that provide appropriate statistical models for probabilistic modeling and inference in Geometric Deep Learning. We argue in favor of using statistical models which arise in different Kuramoto models in the continuum limit of particles. The most convenient families of probability distributions are those which are invariant with respect to actions of certain symmetry groups.","sentences":["We propose the idea of using Kuramoto models (including their higher-dimensional generalizations) for machine learning over non-Euclidean data sets.","These models are systems of matrix ODE's describing collective motions (swarming dynamics) of abstract particles (generalized oscillators) on spheres, homogeneous spaces and Lie groups.","Such models have been extensively studied from the beginning of XXI century both in statistical physics and control theory.","They provide a suitable framework for encoding maps between various manifolds and are capable of learning over spherical and hyperbolic geometries.","In addition, they can learn coupled actions of transformation groups (such as special orthogonal, unitary and Lorentz groups).","Furthermore, we overview families of probability distributions that provide appropriate statistical models for probabilistic modeling and inference in Geometric Deep Learning.","We argue in favor of using statistical models which arise in different Kuramoto models in the continuum limit of particles.","The most convenient families of probability distributions are those which are invariant with respect to actions of certain symmetry groups."],"url":"http://arxiv.org/abs/2405.09453v1","category":"cs.LG"}
{"created":"2024-05-15 15:46:07","title":"Constraining UV freeze-in of light relics with current and next-generation CMB observations","abstract":"Cosmological observations allow to measure the abundance of light relics produced in the early Universe. Most studies focus on the thermal freeze-out scenario, yet light relics produced by freeze-in are generic for models in which new light degrees of freedom do not couple strongly enough to the Standard Model (SM) plasma to allow for full thermalization in the early Universe. In ultraviolet (UV) freeze-in scenarios, rates for light relic production associated with non-renormalizable interactions typical of beyond the SM (BSM) models grow with temperature more quickly than the Hubble rate. Thus, relatively small couplings to the SM can be probed by current and next-generation cosmic microwave background (CMB) experiments. We investigate several representative benchmark BSM models, such as axion-like particles from Primakoff production, massless dark photons and light right-handed neutrinos. We calculate contributions to the effective number of neutrino species, $\\Delta N_{\\rm eff}$, in corners of parameter space not previously considered and discuss the sensitivity of CMB experiments compared to other probes. In contrast to freeze-out scenarios, $\\Delta N_{\\rm eff}$ from UV freeze-in is more dependent on both the specific BSM physics model and the reheating temperature. Depending on the details of the BSM scenario, we find that the sensitivity of next-generation CMB experiments can complement or surpass the current astrophysical, laboratory or collider constraints on the couplings of the SM to the light relic.","sentences":["Cosmological observations allow to measure the abundance of light relics produced in the early Universe.","Most studies focus on the thermal freeze-out scenario, yet light relics produced by freeze-in are generic for models in which new light degrees of freedom do not couple strongly enough to the Standard Model (SM) plasma to allow for full thermalization in the early Universe.","In ultraviolet (UV) freeze-in scenarios, rates for light relic production associated with non-renormalizable interactions typical of beyond the SM (BSM) models grow with temperature more quickly than the Hubble rate.","Thus, relatively small couplings to the SM can be probed by current and next-generation cosmic microwave background (CMB) experiments.","We investigate several representative benchmark BSM models, such as axion-like particles from Primakoff production, massless dark photons and light right-handed neutrinos.","We calculate contributions to the effective number of neutrino species, $\\Delta N_{\\rm eff}$, in corners of parameter space not previously considered and discuss the sensitivity of CMB experiments compared to other probes.","In contrast to freeze-out scenarios, $\\Delta N_{\\rm eff}$ from UV freeze-in is more dependent on both the specific BSM physics model and the reheating temperature.","Depending on the details of the BSM scenario, we find that the sensitivity of next-generation CMB experiments can complement or surpass the current astrophysical, laboratory or collider constraints on the couplings of the SM to the light relic."],"url":"http://arxiv.org/abs/2405.09449v1","category":"astro-ph.CO"}
{"created":"2024-05-15 15:42:41","title":"M$^4$oE: A Foundation Model for Medical Multimodal Image Segmentation with Mixture of Experts","abstract":"Medical imaging data is inherently heterogeneous across different modalities and clinical centers, posing unique challenges for developing generalizable foundation models. Conventional entails training distinct models per dataset or using a shared encoder with modality-specific decoders. However, these approaches incur heavy computational overheads and suffer from poor scalability. To address these limitations, we propose the Medical Multimodal Mixture of Experts (M$^4$oE) framework, leveraging the SwinUNet architecture. Specifically, M$^4$oE comprises modality-specific experts; each separately initialized to learn features encoding domain knowledge. Subsequently, a gating network is integrated during fine-tuning to modulate each expert's contribution to the collective predictions dynamically. This enhances model interpretability and generalization ability while retaining expertise specialization. Simultaneously, the M$^4$oE architecture amplifies the model's parallel processing capabilities, and it also ensures the model's adaptation to new modalities with ease. Experiments across three modalities reveal that M$^4$oE can achieve 3.45% over STU-Net-L, 5.11% over MED3D, and 11.93% over SAM-Med2D across the MICCAI FLARE22, AMOS2022, and ATLAS2023 datasets. Moreover, M$^4$oE showcases a significant reduction in training duration with 7 hours less while maintaining a parameter count that is only 30% of its compared methods. The code is available at https://github.com/JefferyJiang-YF/M4oE.","sentences":["Medical imaging data is inherently heterogeneous across different modalities and clinical centers, posing unique challenges for developing generalizable foundation models.","Conventional entails training distinct models per dataset or using a shared encoder with modality-specific decoders.","However, these approaches incur heavy computational overheads and suffer from poor scalability.","To address these limitations, we propose the Medical Multimodal Mixture of Experts (M$^4$oE) framework, leveraging the SwinUNet architecture.","Specifically, M$^4$oE comprises modality-specific experts; each separately initialized to learn features encoding domain knowledge.","Subsequently, a gating network is integrated during fine-tuning to modulate each expert's contribution to the collective predictions dynamically.","This enhances model interpretability and generalization ability while retaining expertise specialization.","Simultaneously, the M$^4$oE architecture amplifies the model's parallel processing capabilities, and it also ensures the model's adaptation to new modalities with ease.","Experiments across three modalities reveal that M$^4$oE can achieve 3.45% over STU-Net-L, 5.11% over MED3D, and 11.93% over SAM-Med2D across the MICCAI FLARE22, AMOS2022, and ATLAS2023 datasets.","Moreover, M$^4$oE showcases a significant reduction in training duration with 7 hours less while maintaining a parameter count that is only 30% of its compared methods.","The code is available at https://github.com/JefferyJiang-YF/M4oE."],"url":"http://arxiv.org/abs/2405.09446v1","category":"eess.IV"}
{"created":"2024-05-15 15:39:35","title":"Desk-AId: Humanitarian Aid Desk Assessment with Geospatial AI for Predicting Landmine Areas","abstract":"The process of clearing areas, namely demining, starts by assessing and prioritizing potential hazardous areas (i.e., desk assessment) to go under thorough investigation of experts, who confirm the risk and proceed with the mines clearance operations. This paper presents Desk-AId that supports the desk assessment phase by estimating landmine risks using geospatial data and socioeconomic information. Desk-AId uses a Geospatial AI approach specialized to landmines. The approach includes mixed data sampling strategies and context-enrichment by historical conflicts and key multi-domain facilities (e.g., buildings, roads, health sites). The proposed system addresses the issue of having only ground-truth for confirmed hazardous areas by implementing a new hard-negative data sampling strategy, where negative points are sampled in the vicinity of hazardous areas. Experiments validate Desk-Aid in two domains for landmine risk assessment: 1) country-wide, and 2) uncharted study areas). The proposed approach increases the estimation accuracies up to 92%, for different classification models such as RandomForest (RF), Feedforward Neural Networks (FNN), and Graph Neural Networks (GNN).","sentences":["The process of clearing areas, namely demining, starts by assessing and prioritizing potential hazardous areas (i.e., desk assessment) to go under thorough investigation of experts, who confirm the risk and proceed with the mines clearance operations.","This paper presents Desk-AId that supports the desk assessment phase by estimating landmine risks using geospatial data and socioeconomic information.","Desk-AId uses a Geospatial AI approach specialized to landmines.","The approach includes mixed data sampling strategies and context-enrichment by historical conflicts and key multi-domain facilities (e.g., buildings, roads, health sites).","The proposed system addresses the issue of having only ground-truth for confirmed hazardous areas by implementing a new hard-negative data sampling strategy, where negative points are sampled in the vicinity of hazardous areas.","Experiments validate Desk-Aid in two domains for landmine risk assessment: 1) country-wide, and 2) uncharted study areas).","The proposed approach increases the estimation accuracies up to 92%, for different classification models such as RandomForest (RF), Feedforward Neural Networks (FNN), and Graph Neural Networks (GNN)."],"url":"http://arxiv.org/abs/2405.09444v1","category":"cs.CY"}
{"created":"2024-05-15 15:38:55","title":"Low-Complexity Joint Azimuth-Range-Velocity Estimation for Integrated Sensing and Communication with OFDM Waveform","abstract":"Integrated sensing and communication (ISAC) is a main application scenario of the sixth-generation mobile communication systems. Due to the fast-growing number of antennas and subcarriers in cellular systems, the computational complexity of joint azimuth-range-velocity estimation (JARVE) in ISAC systems is extremely high. This paper studies the JARVE problem for a monostatic ISAC system with orthogonal frequency division multiplexing (OFDM) waveform, in which a base station receives the echos of its transmitted cellular OFDM signals to sense multiple targets. The Cramer-Rao bounds are first derived for JARVE. A low-complexity algorithm is further designed for super-resolution JARVE, which utilizes the proposed iterative subspace update scheme and Levenberg-Marquardt optimization method to replace the exhaustive search of spatial spectrum in multiple-signal-classification (MUSIC) algorithm. Finally, with the practical parameters of 5G New Radio, simulation results verify that the proposed algorithm can reduce the computational complexity by three orders of magnitude and two orders of magnitude compared to the existing three-dimensional MUSIC algorithm and estimation-of-signal-parameters-using-rotational-invariance-techniques (ESPRIT) algorithm, respectively, and also improve the estimation performance.","sentences":["Integrated sensing and communication (ISAC) is a main application scenario of the sixth-generation mobile communication systems.","Due to the fast-growing number of antennas and subcarriers in cellular systems, the computational complexity of joint azimuth-range-velocity estimation (JARVE) in ISAC systems is extremely high.","This paper studies the JARVE problem for a monostatic ISAC system with orthogonal frequency division multiplexing (OFDM) waveform, in which a base station receives the echos of its transmitted cellular OFDM signals to sense multiple targets.","The Cramer-Rao bounds are first derived for JARVE.","A low-complexity algorithm is further designed for super-resolution JARVE, which utilizes the proposed iterative subspace update scheme and Levenberg-Marquardt optimization method to replace the exhaustive search of spatial spectrum in multiple-signal-classification (MUSIC) algorithm.","Finally, with the practical parameters of 5G New Radio, simulation results verify that the proposed algorithm can reduce the computational complexity by three orders of magnitude and two orders of magnitude compared to the existing three-dimensional MUSIC algorithm and estimation-of-signal-parameters-using-rotational-invariance-techniques (ESPRIT) algorithm, respectively, and also improve the estimation performance."],"url":"http://arxiv.org/abs/2405.09443v1","category":"cs.IT"}
{"created":"2024-05-15 15:30:17","title":"Facilitating Opinion Diversity through Hybrid NLP Approaches","abstract":"Modern democracies face a critical issue of declining citizen participation in decision-making. Online discussion forums are an important avenue for enhancing citizen participation. This thesis proposal 1) identifies the challenges involved in facilitating large-scale online discussions with Natural Language Processing (NLP), 2) suggests solutions to these challenges by incorporating hybrid human-AI technologies, and 3) investigates what these technologies can reveal about individual perspectives in online discussions. We propose a three-layered hierarchy for representing perspectives that can be obtained by a mixture of human intelligence and large language models. We illustrate how these representations can draw insights into the diversity of perspectives and allow us to investigate interactions in online discussions.","sentences":["Modern democracies face a critical issue of declining citizen participation in decision-making.","Online discussion forums are an important avenue for enhancing citizen participation.","This thesis proposal 1) identifies the challenges involved in facilitating large-scale online discussions with Natural Language Processing (NLP), 2) suggests solutions to these challenges by incorporating hybrid human-AI technologies, and 3) investigates what these technologies can reveal about individual perspectives in online discussions.","We propose a three-layered hierarchy for representing perspectives that can be obtained by a mixture of human intelligence and large language models.","We illustrate how these representations can draw insights into the diversity of perspectives and allow us to investigate interactions in online discussions."],"url":"http://arxiv.org/abs/2405.09439v1","category":"cs.CL"}
{"created":"2024-05-15 15:28:18","title":"$\u0393(X)$ as Polish Space","abstract":"We will see how to define the metric $\\beta$, which turns the topological space of continuous functions whose domains are open subsets of a locally compact and second countable space $X$ to values in a polish space $Y$, called $(C_{od}(X,Y),\\tau_{\\iota,D})$ into a polish space.","sentences":["We will see how to define the metric $\\beta$, which turns the topological space of continuous functions whose domains are open subsets of a locally compact and second countable space $X$ to values in a polish space $Y$, called $(C_{od}(X,Y),\\tau_{\\iota,D})$ into a polish space."],"url":"http://arxiv.org/abs/2405.09437v1","category":"math.GN"}
{"created":"2024-05-15 15:26:48","title":"Outlier-resilient model fitting via percentile losses: Methods for general and convex residuals","abstract":"We consider the problem of robustly fitting a model to data that includes outliers by formulating a percentile optimization problem. This problem is non-smooth and non-convex, hence hard to solve. We derive properties that the minimizers of such problems must satisfy. These properties lead to methods that solve the percentile formulation both for general residuals and for convex residuals. The methods fit the model to subsets of the data, and then extract the solution of the percentile formulation from these partial fits. As illustrative simulations show, such methods endure higher outlier percentages, when compared with standard robust estimates. Additionally, the derived properties provide a broader and alternative theoretical validation for existing robust methods, whose validity was previously limited to specific forms of the residuals.","sentences":["We consider the problem of robustly fitting a model to data that includes outliers by formulating a percentile optimization problem.","This problem is non-smooth and non-convex, hence hard to solve.","We derive properties that the minimizers of such problems must satisfy.","These properties lead to methods that solve the percentile formulation both for general residuals and for convex residuals.","The methods fit the model to subsets of the data, and then extract the solution of the percentile formulation from these partial fits.","As illustrative simulations show, such methods endure higher outlier percentages, when compared with standard robust estimates.","Additionally, the derived properties provide a broader and alternative theoretical validation for existing robust methods, whose validity was previously limited to specific forms of the residuals."],"url":"http://arxiv.org/abs/2405.09436v1","category":"eess.SP"}
{"created":"2024-05-15 15:26:47","title":"On the radiation field of a linearly accelerated charged particle in Born-Infeld theory","abstract":"The electric potential and the electromagnetic field for a linearly accelerated Born-Infeld charged particle are obtained in an inertial frame by a method that can, in principle, be applied to any electromagnetic theory. The method is based on (i) the fact that the metric near the horizon of a Schwarzschild black hole is equivalent to that of Rindler spacetime, and (ii) a theorem that guarantees that the electrostatic potential for a given nonlinear theory in a static, spherically symmetric spacetime is entirely specified by the Maxwellian electrostatic potential in the same background. Using analytical and numerical methods, the features of the radiation field and the radiation-reaction for such an accelerated particle are discussed in detail.","sentences":["The electric potential and the electromagnetic field for a linearly accelerated Born-Infeld charged particle are obtained in an inertial frame by a method that can, in principle, be applied to any electromagnetic theory.","The method is based on (i) the fact that the metric near the horizon of a Schwarzschild black hole is equivalent to that of Rindler spacetime, and (ii) a theorem that guarantees that the electrostatic potential for a given nonlinear theory in a static, spherically symmetric spacetime is entirely specified by the Maxwellian electrostatic potential in the same background.","Using analytical and numerical methods, the features of the radiation field and the radiation-reaction for such an accelerated particle are discussed in detail."],"url":"http://arxiv.org/abs/2405.09435v1","category":"gr-qc"}
{"created":"2024-05-15 15:25:37","title":"Infinitary primitive positive definability over the real numbers with convex relations","abstract":"On a finite structure, the polymorphism invariant relations are exactly the primitively positively definable relations. On infinite structures, these two sets of relations are different in general. Infinitary primitively positively definable relations are a natural intermediate concept which extends primitive positive definability by infinite conjunctions.   We consider for every convex set $S\\subset \\mathbb{R}^n$ the structure of the real numbers $\\mathbb{R}$ with addition, scalar multiplication, constants, and additionally the relation $S$. We prove that depending on $S$, the set of all relations with an infinitary primitive positive definition in this structure equals one out of six possible sets. This dependency gives a natural partition of the convex sets into six nonempty classes. We also give an elementary geometric description of the classes and a description in terms of linear maps.   The classification also implies that there is no locally closed clone between the clone of affine combinations and the clone of convex combinations.","sentences":["On a finite structure, the polymorphism invariant relations are exactly the primitively positively definable relations.","On infinite structures, these two sets of relations are different in general.","Infinitary primitively positively definable relations are a natural intermediate concept which extends primitive positive definability by infinite conjunctions.   ","We consider for every convex set $S\\subset \\mathbb{R}^n$ the structure of the real numbers $\\mathbb{R}$ with addition, scalar multiplication, constants, and additionally the relation $S$. We prove that depending on $S$, the set of all relations with an infinitary primitive positive definition in this structure equals one out of six possible sets.","This dependency gives a natural partition of the convex sets into six nonempty classes.","We also give an elementary geometric description of the classes and a description in terms of linear maps.   ","The classification also implies that there is no locally closed clone between the clone of affine combinations and the clone of convex combinations."],"url":"http://arxiv.org/abs/2405.09433v1","category":"math.RA"}
{"created":"2024-05-15 15:23:22","title":"A Survey On Text-to-3D Contents Generation In The Wild","abstract":"3D content creation plays a vital role in various applications, such as gaming, robotics simulation, and virtual reality. However, the process is labor-intensive and time-consuming, requiring skilled designers to invest considerable effort in creating a single 3D asset. To address this challenge, text-to-3D generation technologies have emerged as a promising solution for automating 3D creation. Leveraging the success of large vision language models, these techniques aim to generate 3D content based on textual descriptions. Despite recent advancements in this area, existing solutions still face significant limitations in terms of generation quality and efficiency. In this survey, we conduct an in-depth investigation of the latest text-to-3D creation methods. We provide a comprehensive background on text-to-3D creation, including discussions on datasets employed in training and evaluation metrics used to assess the quality of generated 3D models. Then, we delve into the various 3D representations that serve as the foundation for the 3D generation process. Furthermore, we present a thorough comparison of the rapidly growing literature on generative pipelines, categorizing them into feedforward generators, optimization-based generation, and view reconstruction approaches. By examining the strengths and weaknesses of these methods, we aim to shed light on their respective capabilities and limitations. Lastly, we point out several promising avenues for future research. With this survey, we hope to inspire researchers further to explore the potential of open-vocabulary text-conditioned 3D content creation.","sentences":["3D content creation plays a vital role in various applications, such as gaming, robotics simulation, and virtual reality.","However, the process is labor-intensive and time-consuming, requiring skilled designers to invest considerable effort in creating a single 3D asset.","To address this challenge, text-to-3D generation technologies have emerged as a promising solution for automating 3D creation.","Leveraging the success of large vision language models, these techniques aim to generate 3D content based on textual descriptions.","Despite recent advancements in this area, existing solutions still face significant limitations in terms of generation quality and efficiency.","In this survey, we conduct an in-depth investigation of the latest text-to-3D creation methods.","We provide a comprehensive background on text-to-3D creation, including discussions on datasets employed in training and evaluation metrics used to assess the quality of generated 3D models.","Then, we delve into the various 3D representations that serve as the foundation for the 3D generation process.","Furthermore, we present a thorough comparison of the rapidly growing literature on generative pipelines, categorizing them into feedforward generators, optimization-based generation, and view reconstruction approaches.","By examining the strengths and weaknesses of these methods, we aim to shed light on their respective capabilities and limitations.","Lastly, we point out several promising avenues for future research.","With this survey, we hope to inspire researchers further to explore the potential of open-vocabulary text-conditioned 3D content creation."],"url":"http://arxiv.org/abs/2405.09431v1","category":"cs.CV"}
{"created":"2024-05-15 15:22:32","title":"Generalizations of cyclic polytopes","abstract":"A generalization is a construction of a class of polytopes such that the polytopes have some of their properties.The best known example is the class of neighbourly polytopes. Cyclic polytopes have explicit facet structures, important properties and applications in different branches of mathematics. In the past few decades, generalizations of their combinatorial properties have yielded new classes of polytopes that also have explicit facet structures and useful applications. We present an overview of these generalizations along with some applications of the resultant polytopes, and some possible approaches to other generalizations.","sentences":["A generalization is a construction of a class of polytopes such that the polytopes have some of their properties.","The best known example is the class of neighbourly polytopes.","Cyclic polytopes have explicit facet structures, important properties and applications in different branches of mathematics.","In the past few decades, generalizations of their combinatorial properties have yielded new classes of polytopes that also have explicit facet structures and useful applications.","We present an overview of these generalizations along with some applications of the resultant polytopes, and some possible approaches to other generalizations."],"url":"http://arxiv.org/abs/2405.09429v1","category":"math.CO"}
{"created":"2024-05-15 15:19:23","title":"Global-Local Image Perceptual Score (GLIPS): Evaluating Photorealistic Quality of AI-Generated Images","abstract":"This paper introduces the Global-Local Image Perceptual Score (GLIPS), an image metric designed to assess the photorealistic image quality of AI-generated images with a high degree of alignment to human visual perception. Traditional metrics such as FID and KID scores do not align closely with human evaluations. The proposed metric incorporates advanced transformer-based attention mechanisms to assess local similarity and Maximum Mean Discrepancy (MMD) to evaluate global distributional similarity. To evaluate the performance of GLIPS, we conducted a human study on photorealistic image quality. Comprehensive tests across various generative models demonstrate that GLIPS consistently outperforms existing metrics like FID, SSIM, and MS-SSIM in terms of correlation with human scores. Additionally, we introduce the Interpolative Binning Scale (IBS), a refined scaling method that enhances the interpretability of metric scores by aligning them more closely with human evaluative standards. The proposed metric and scaling approach not only provides more reliable assessments of AI-generated images but also suggest pathways for future enhancements in image generation technologies.","sentences":["This paper introduces the Global-Local Image Perceptual Score (GLIPS), an image metric designed to assess the photorealistic image quality of AI-generated images with a high degree of alignment to human visual perception.","Traditional metrics such as FID and KID scores do not align closely with human evaluations.","The proposed metric incorporates advanced transformer-based attention mechanisms to assess local similarity and Maximum Mean Discrepancy (MMD) to evaluate global distributional similarity.","To evaluate the performance of GLIPS, we conducted a human study on photorealistic image quality.","Comprehensive tests across various generative models demonstrate that GLIPS consistently outperforms existing metrics like FID, SSIM, and MS-SSIM in terms of correlation with human scores.","Additionally, we introduce the Interpolative Binning Scale (IBS), a refined scaling method that enhances the interpretability of metric scores by aligning them more closely with human evaluative standards.","The proposed metric and scaling approach not only provides more reliable assessments of AI-generated images but also suggest pathways for future enhancements in image generation technologies."],"url":"http://arxiv.org/abs/2405.09426v1","category":"cs.CV"}
{"created":"2024-05-15 15:18:25","title":"On backward problem for a time-fractional fourth order parabolic equation","abstract":"This paper is concerned with the inverse problem of retrieving the initial value of a time-fractional fourth order parabolic equation from source and final time observation. The considered problem is an {\\it ill-posed problem.} We obtain regularized approximations for the sought initial value by employing the quasi-boundary value method, its modified version and by Fourier truncation method(FTM). We provide both the apriori and aposteriori parameter choice strategies and derive the error estimates for all these methods under some {\\it source conditions} involving some Sobolev smoothness. As an important implication of the obtained rates, we observe that for both the apriori and aposteriori cases, the rates obtained by all these three methods are same for some source sets. Moreover, we observe that in both the apriori and aposteriori cases, the FTM is free from the so-called {\\it saturation effect}, whereas both the quasi-boundary value method and its generalizations possesses the saturation effect for both the cases. Further, we observe that the rates obtained by the FTM is always order optimal for all the considered source sets.","sentences":["This paper is concerned with the inverse problem of retrieving the initial value of a time-fractional fourth order parabolic equation from source and final time observation.","The considered problem is an {\\it ill-posed problem.}","We obtain regularized approximations for the sought initial value by employing the quasi-boundary value method, its modified version and by Fourier truncation method(FTM).","We provide both the apriori and aposteriori parameter choice strategies and derive the error estimates for all these methods under some {\\it source conditions} involving some Sobolev smoothness.","As an important implication of the obtained rates, we observe that for both the apriori and aposteriori cases, the rates obtained by all these three methods are same for some source sets.","Moreover, we observe that in both the apriori and aposteriori cases, the FTM is free from the so-called {\\it saturation effect}, whereas both the quasi-boundary value method and its generalizations possesses the saturation effect for both the cases.","Further, we observe that the rates obtained by the FTM is always order optimal for all the considered source sets."],"url":"http://arxiv.org/abs/2405.09424v1","category":"math.NA"}
{"created":"2024-05-15 15:17:04","title":"MicroPython Testbed for Federated Learning Algorithms","abstract":"Recently, Python Testbed for Federated Learning Algorithms emerged as a low code and generative large language models amenable framework for developing decentralized and distributed applications, primarily targeting edge systems, by nonprofessional programmers with the help of emerging artificial intelligence tools. This light framework is written in pure Python to be easy to install and to fit into a small IoT memory. It supports formally verified generic centralized and decentralized federated learning algorithms, as well as the peer-to-peer data exchange used in time division multiplexing communication, and its current main limitation is that all the application instances can run only on a single PC. This paper presents the MicroPyton Testbed for Federated Learning Algorithms, the new framework that overcomes its predecessor's limitation such that individual application instances may run on different network nodes like PCs and IoTs, primarily in edge systems. The new framework carries on the pure Python ideal, is based on asynchronous I/O abstractions, and runs on MicroPython, and therefore is a great match for IoTs and devices in edge systems. The new framework was experimentally validated on a wireless network comprising PCs and Raspberry Pi Pico W boards, by using application examples originally developed for the predecessor framework.","sentences":["Recently, Python Testbed for Federated Learning Algorithms emerged as a low code and generative large language models amenable framework for developing decentralized and distributed applications, primarily targeting edge systems, by nonprofessional programmers with the help of emerging artificial intelligence tools.","This light framework is written in pure Python to be easy to install and to fit into a small IoT memory.","It supports formally verified generic centralized and decentralized federated learning algorithms, as well as the peer-to-peer data exchange used in time division multiplexing communication, and its current main limitation is that all the application instances can run only on a single PC.","This paper presents the MicroPyton Testbed for Federated Learning Algorithms, the new framework that overcomes its predecessor's limitation such that individual application instances may run on different network nodes like PCs and IoTs, primarily in edge systems.","The new framework carries on the pure Python ideal, is based on asynchronous I/O abstractions, and runs on MicroPython, and therefore is a great match for IoTs and devices in edge systems.","The new framework was experimentally validated on a wireless network comprising PCs and Raspberry Pi Pico W boards, by using application examples originally developed for the predecessor framework."],"url":"http://arxiv.org/abs/2405.09423v1","category":"cs.DC"}
{"created":"2024-05-15 15:16:52","title":"Was there a Big Bang?","abstract":"New one parameter family of exact solutions in General Relativity with a scalar field is found. The metric is of Liouville type which admits complete separation of variables in the geodesic Hamilton--Jacobi equation. This solution exists for the exponential potential for a scalar field and is invariant with respect to global Lorentz transformations. It describes, in particular, a black hole formation as well as a naked singularity. Solutions corresponding to the naked singularity describe accelerating expansion of the homogeneous and isotropic Universe, and can be smoothly continued along geodesics to infinite past without Big Bang.","sentences":["New one parameter family of exact solutions in General Relativity with a scalar field is found.","The metric is of Liouville type which admits complete separation of variables in the geodesic Hamilton--Jacobi equation.","This solution exists for the exponential potential for a scalar field and is invariant with respect to global Lorentz transformations.","It describes, in particular, a black hole formation as well as a naked singularity.","Solutions corresponding to the naked singularity describe accelerating expansion of the homogeneous and isotropic Universe, and can be smoothly continued along geodesics to infinite past without Big Bang."],"url":"http://arxiv.org/abs/2405.09422v1","category":"gr-qc"}
{"created":"2024-05-15 15:15:01","title":"Exploiting Sign Symmetries in Minimizing Sums of Rational Functions","abstract":"This paper is devoted to the problem of minimizing a sum of rational functions over a basic semialgebraic set. We provide a hierarchy of sum of squares (SOS) relaxations that is dual to the generalized moment problem approach due to Bugarin, Henrion, and Lasserre. The investigation of the dual SOS aspect offers two benefits: 1) it allows us to conduct a convergence rate analysis for the hierarchy; 2) it leads to a sign symmetry adapted hierarchy consisting of block-diagonal semidefinite relaxations. When the problem possesses correlative sparsity as well as sign symmetries, we propose sparse semidefinite relaxations by exploiting both structures. Various numerical experiments are performed to demonstrate the efficiency of our approach. Finally, an application to maximizing sums of generalized Rayleigh quotients is presented.","sentences":["This paper is devoted to the problem of minimizing a sum of rational functions over a basic semialgebraic set.","We provide a hierarchy of sum of squares (SOS) relaxations that is dual to the generalized moment problem approach due to Bugarin, Henrion, and Lasserre.","The investigation of the dual SOS aspect offers two benefits: 1) it allows us to conduct a convergence rate analysis for the hierarchy; 2) it leads to a sign symmetry adapted hierarchy consisting of block-diagonal semidefinite relaxations.","When the problem possesses correlative sparsity as well as sign symmetries, we propose sparse semidefinite relaxations by exploiting both structures.","Various numerical experiments are performed to demonstrate the efficiency of our approach.","Finally, an application to maximizing sums of generalized Rayleigh quotients is presented."],"url":"http://arxiv.org/abs/2405.09419v1","category":"math.OC"}
{"created":"2024-05-15 15:10:03","title":"On the Correspondence of Non-flat Assumption-based Argumentation and Logic Programming with Negation as Failure in the Head","abstract":"The relation between (a fragment of) assumption-based argumentation (ABA) and logic programs (LPs) under stable model semantics is well-studied. However, for obtaining this relation, the ABA framework needs to be restricted to being flat, i.e., a fragment where the (defeasible) assumptions can never be entailed, only assumed to be true or false. Here, we remove this restriction and show a correspondence between non-flat ABA and LPs with negation as failure in their head. We then extend this result to so-called set-stable ABA semantics, originally defined for the fragment of non-flat ABA called bipolar ABA. We showcase how to define set-stable semantics for LPs with negation as failure in their head and show the correspondence to set-stable ABA semantics.","sentences":["The relation between (a fragment of) assumption-based argumentation (ABA) and logic programs (LPs) under stable model semantics is well-studied.","However, for obtaining this relation, the ABA framework needs to be restricted to being flat, i.e., a fragment where the (defeasible) assumptions can never be entailed, only assumed to be true or false.","Here, we remove this restriction and show a correspondence between non-flat ABA and LPs with negation as failure in their head.","We then extend this result to so-called set-stable ABA semantics, originally defined for the fragment of non-flat ABA called bipolar ABA.","We showcase how to define set-stable semantics for LPs with negation as failure in their head and show the correspondence to set-stable ABA semantics."],"url":"http://arxiv.org/abs/2405.09415v1","category":"cs.AI"}
{"created":"2024-05-15 15:09:36","title":"Improving the convergence analysis of linear subdivision schemes","abstract":"This work presents several new results concerning the analysis of the convergence of binary, univariate, and linear subdivision schemes, all related to the contractivity factor of a convergent scheme. First, we prove that a convergent scheme cannot have a contractivity factor lower than half. Since the lower this factor is, the faster the scheme's convergence, and schemes with contractivity factor $\\frac{1}{2}$, such as those generating spline functions, have optimal convergence rates.   Additionally, we provide further insights and conditions for the convergence of linear schemes and demonstrate their applicability in an improved algorithm for determining the convergence of such subdivision schemes.","sentences":["This work presents several new results concerning the analysis of the convergence of binary, univariate, and linear subdivision schemes, all related to the contractivity factor of a convergent scheme.","First, we prove that a convergent scheme cannot have a contractivity factor lower than half.","Since the lower this factor is, the faster the scheme's convergence, and schemes with contractivity factor $\\frac{1}{2}$, such as those generating spline functions, have optimal convergence rates.   ","Additionally, we provide further insights and conditions for the convergence of linear schemes and demonstrate their applicability in an improved algorithm for determining the convergence of such subdivision schemes."],"url":"http://arxiv.org/abs/2405.09414v1","category":"math.NA"}
{"created":"2024-05-15 15:07:30","title":"Spin-spin Interactions in General Relativity versus Linearized Massive Gravity: $N$-body Simulations","abstract":"We simulated spin-spin interactions of $N$-bodies in linearized General Relativity (GR) and linearized Massive Gravity of the Fierz-Pauli type (mGR). It was noted earlier that there is a discrete difference between the spin-spin interaction potential in GR and mGR for a $2$-body system, akin to the van Dam-Veltman-Zakharov discontinuity in the static Newton's potential. Specifically, at large distances, GR favors anti-parallel spin orientation with total spin pointing along the interaction axis, while mGR favors parallel spin orientation with total spin perpendicular to the axis between the sources. For an $N$-body system, a simulation in mGR hitherto has not been done and one would like to know the total spin of the system in both theories. Here we remedy this. In the simulations of GR, we observed that the total spin tends to decrease from a random initial configuration, while for mGR with a large distance, the total spin increases.","sentences":["We simulated spin-spin interactions of $N$-bodies in linearized General Relativity (GR) and linearized Massive Gravity of the Fierz-Pauli type (mGR).","It was noted earlier that there is a discrete difference between the spin-spin interaction potential in GR and mGR for a $2$-body system, akin to the van Dam-Veltman-Zakharov discontinuity in the static Newton's potential.","Specifically, at large distances, GR favors anti-parallel spin orientation with total spin pointing along the interaction axis, while mGR favors parallel spin orientation with total spin perpendicular to the axis between the sources.","For an $N$-body system, a simulation in mGR hitherto has not been done and one would like to know the total spin of the system in both theories.","Here we remedy this.","In the simulations of GR, we observed that the total spin tends to decrease from a random initial configuration, while for mGR with a large distance, the total spin increases."],"url":"http://arxiv.org/abs/2405.09411v1","category":"gr-qc"}
{"created":"2024-05-15 15:02:06","title":"Bounded-Memory Strategies in Partial-Information Games","abstract":"We study the computational complexity of solving stochastic games with mean-payoff objectives. Instead of identifying special classes in which simple strategies are sufficient to play $\\epsilon$-optimally, or form $\\epsilon$-Nash equilibria, we consider general partial-information multiplayer games and ask what can be achieved with (and against) finite-memory strategies up to a {given} bound on the memory. We show $NP$-hardness for approximating zero-sum values, already with respect to memoryless strategies and for 1-player reachability games. On the other hand, we provide upper bounds for solving games of any fixed number of players $k$. We show that one can decide in polynomial space if, for a given $k$-player game, $\\epsilon\\ge 0$ and bound $b$, there exists an $\\epsilon$-Nash equilibrium in which all strategies use at most $b$ memory modes. For given $\\epsilon>0$, finding an $\\epsilon$-Nash equilibrium with respect to $b$-bounded strategies can be done in $FN[NP]$. Similarly for 2-player zero-sum games, finding a $b$-bounded strategy that, against all $b$-bounded opponent strategies, guarantees an outcome within $\\epsilon$ of a given value, can be done in $FNP[NP]$. Our constructions apply to parity objectives with minimal simplifications. Our results improve the status quo in several well-known special cases of games. In particular, for $2$-player zero-sum concurrent mean-payoff games, one can approximate ordinary zero-sum values (without restricting admissible strategies) in $FNP[NP]$.","sentences":["We study the computational complexity of solving stochastic games with mean-payoff objectives.","Instead of identifying special classes in which simple strategies are sufficient to play $\\epsilon$-optimally, or form $\\epsilon$-Nash equilibria, we consider general partial-information multiplayer games and ask what can be achieved with (and against) finite-memory strategies up to a {given} bound on the memory.","We show $NP$-hardness for approximating zero-sum values, already with respect to memoryless strategies and for 1-player reachability games.","On the other hand, we provide upper bounds for solving games of any fixed number of players $k$.","We show that one can decide in polynomial space if, for a given $k$-player game, $\\epsilon\\ge 0$ and bound $b$, there exists an $\\epsilon$-Nash equilibrium in which all strategies use at most $b$ memory modes.","For given $\\epsilon>0$, finding an $\\epsilon$-Nash equilibrium with respect to $b$-bounded strategies can be done in $FN[NP]$. Similarly for 2-player zero-sum games, finding a $b$-bounded strategy that, against all $b$-bounded opponent strategies, guarantees an outcome within $\\epsilon$ of a given value, can be done in $FNP[NP]$. Our constructions apply to parity objectives with minimal simplifications.","Our results improve the status quo in several well-known special cases of games.","In particular, for $2$-player zero-sum concurrent mean-payoff games, one can approximate ordinary zero-sum values (without restricting admissible strategies) in $FNP[NP]$."],"url":"http://arxiv.org/abs/2405.09406v1","category":"cs.GT"}
{"created":"2024-05-15 14:57:46","title":"A Generalization of Varnavides's Theorem","abstract":"A linear equation $E$ is said to be sparse if there is $c>0$ so that every subset of $[n]$ of size $n^{1-c}$ contains a solution of $E$ in distinct integers. The problem of characterizing the sparse equations, first raised by Ruzsa in the 90's, is one of the most important open problems in additive combinatorics. We say that $E$ in $k$ variables is abundant if every subset of $[n]$ of size $\\varepsilon n$ contains at least poly$(\\varepsilon)\\cdot n^{k-1}$ solutions of $E$. It is clear that every abundant $E$ is sparse, and Gir\\~{a}o, Hurley, Illingworth and Michel asked if the converse implication also holds. In this note we show that this is the case for every $E$ in $4$ variables. We further discuss a generalization of this problem which applies to all linear equations.","sentences":["A linear equation $E$ is said to be sparse if there is $c>0$ so that every subset of $[n]$ of size $n^{1-c}$ contains a solution of $E$ in distinct integers.","The problem of characterizing the sparse equations, first raised by Ruzsa in the 90's, is one of the most important open problems in additive combinatorics.","We say that $E$ in $k$ variables is abundant if every subset of $[n]$ of size $\\varepsilon n$ contains at least poly$(\\varepsilon)\\cdot n^{k-1}$ solutions of $E$. It is clear that every abundant $E$ is sparse, and Gir\\~{a}o, Hurley, Illingworth and Michel asked if the converse implication also holds.","In this note we show that this is the case for every $E$ in $4$ variables.","We further discuss a generalization of this problem which applies to all linear equations."],"url":"http://arxiv.org/abs/2405.09402v1","category":"math.CO"}
{"created":"2024-05-15 14:53:57","title":"Flow updates for domain decomposition of entropic optimal transport","abstract":"Domain decomposition has been shown to be a computationally efficient distributed method for solving large scale entropic optimal transport problems. However, a naive implementation of the algorithm can freeze in the limit of very fine partition cells (i.e.~it asymptotically becomes stationary and does not find the global minimizer), since information can only travel slowly between cells. In practice this can be avoided by a coarse-to-fine multiscale scheme. In this article we introduce flow updates as an alternative approach. Flow updates can be interpreted as a variant of the celebrated algorithm by Angenent, Haker, and Tannenbaum, and can be combined canonically with domain decomposition. We prove convergence to the global minimizer and provide a formal discussion of its continuity limit. We give a numerical comparison with naive and multiscale domain decomposition, and show that the hybrid method does not suffer from freezing in the regime of very many cells. While the multiscale scheme is observed to be faster than the hybrid approach in general, the latter could be a viable alternative in cases where a good initial coupling is available. Our numerical experiments are based on a novel GPU implementation of domain decomposition that we describe in the appendix.","sentences":["Domain decomposition has been shown to be a computationally efficient distributed method for solving large scale entropic optimal transport problems.","However, a naive implementation of the algorithm can freeze in the limit of very fine partition cells (i.e.~it asymptotically becomes stationary and does not find the global minimizer), since information can only travel slowly between cells.","In practice this can be avoided by a coarse-to-fine multiscale scheme.","In this article we introduce flow updates as an alternative approach.","Flow updates can be interpreted as a variant of the celebrated algorithm by Angenent, Haker, and Tannenbaum, and can be combined canonically with domain decomposition.","We prove convergence to the global minimizer and provide a formal discussion of its continuity limit.","We give a numerical comparison with naive and multiscale domain decomposition, and show that the hybrid method does not suffer from freezing in the regime of very many cells.","While the multiscale scheme is observed to be faster than the hybrid approach in general, the latter could be a viable alternative in cases where a good initial coupling is available.","Our numerical experiments are based on a novel GPU implementation of domain decomposition that we describe in the appendix."],"url":"http://arxiv.org/abs/2405.09400v1","category":"math.NA"}
{"created":"2024-05-15 14:51:22","title":"Sharp PDE estimates for random two-dimensional bipartite matching with power cost function","abstract":"We investigate the random bipartite optimal matching problem on a flat torus in two-dimensions, considering general strictly convex power costs of the distance. We extend the successful ansatz first introduced by Caracciolo et al. for the quadratic case, involving a linear Poisson equation, to a non-linear equation of $q$-Poisson type, allowing for a more comprehensive analysis of the optimal transport cost. Our results establish new asymptotic connections between the energy of the solution to the PDE and the optimal transport cost, providing insights on their asymptotic behavior.","sentences":["We investigate the random bipartite optimal matching problem on a flat torus in two-dimensions, considering general strictly convex power costs of the distance.","We extend the successful ansatz first introduced by Caracciolo et al.","for the quadratic case, involving a linear Poisson equation, to a non-linear equation of $q$-Poisson type, allowing for a more comprehensive analysis of the optimal transport cost.","Our results establish new asymptotic connections between the energy of the solution to the PDE and the optimal transport cost, providing insights on their asymptotic behavior."],"url":"http://arxiv.org/abs/2405.09397v1","category":"math.AP"}
{"created":"2024-05-15 14:51:11","title":"$O_2$ is a multiple context-free grammar: an implementation-, formalisation-friendly proof","abstract":"Classifying formal languages according to the expressiveness of grammars able to generate them is a fundamental problem in computational linguistics and, therefore, in the theory of computation. Furthermore, such kind of analysis can give insight into the classification of abstract algebraic structure such as groups, for example through the correspondence given by the word problem. While many such classification problems remain open, others have been settled. Recently, it was proved that $n$-balanced languages (i.e., whose strings contain the same occurrences of letters $a_i$ and $A_i$ with $1\\leq i \\leq n$) can be generated by multiple context-free grammars (MCFGs), which are one of the several slight extensions of context free grammars added to the classical Chomsky hierarchy to make the mentioned classification more precise. This paper analyses the existing proofs from the computational and the proof-theoretical point of views, systematically studying whether each proof can lead to a verified (i.e., checked by a proof assistant) algorithm parsing balanced languages via MCFGs. We conclude that none of the existing proofs is realistically suitable against this practical goal, and proceed to provide a radically new, elementary, extremely short proof for the crucial case $n \\leq 2$. A comparative analysis with respect to the existing proofs is finally performed to justify why the proposed proof is a substantial step towards concretely obtaining a verified parsing algorithm for $O_2$.","sentences":["Classifying formal languages according to the expressiveness of grammars able to generate them is a fundamental problem in computational linguistics and, therefore, in the theory of computation.","Furthermore, such kind of analysis can give insight into the classification of abstract algebraic structure such as groups, for example through the correspondence given by the word problem.","While many such classification problems remain open, others have been settled.","Recently, it was proved that $n$-balanced languages (i.e., whose strings contain the same occurrences of letters $a_i$ and $A_i$ with $1\\leq i \\leq n$) can be generated by multiple context-free grammars (MCFGs), which are one of the several slight extensions of context free grammars added to the classical Chomsky hierarchy to make the mentioned classification more precise.","This paper analyses the existing proofs from the computational and the proof-theoretical point of views, systematically studying whether each proof can lead to a verified (i.e., checked by a proof assistant) algorithm parsing balanced languages via MCFGs.","We conclude that none of the existing proofs is realistically suitable against this practical goal, and proceed to provide a radically new, elementary, extremely short proof for the crucial case $n \\leq 2$.","A comparative analysis with respect to the existing proofs is finally performed to justify why the proposed proof is a substantial step towards concretely obtaining a verified parsing algorithm for $O_2$."],"url":"http://arxiv.org/abs/2405.09396v1","category":"cs.FL"}
{"created":"2024-05-15 14:50:51","title":"Matching domain experts by training from scratch on domain knowledge","abstract":"Recently, large language models (LLMs) have outperformed human experts in predicting the results of neuroscience experiments (Luo et al., 2024). What is the basis for this performance? One possibility is that statistical patterns in that specific scientific literature, as opposed to emergent reasoning abilities arising from broader training, underlie LLMs' performance. To evaluate this possibility, we trained (next word prediction) a relatively small 124M-parameter GPT-2 model on 1.3 billion tokens of domain-specific knowledge. Despite being orders of magnitude smaller than larger LLMs trained on trillions of tokens, small models achieved expert-level performance in predicting neuroscience results. Small models trained on the neuroscience literature succeeded when they were trained from scratch using a tokenizer specifically trained on neuroscience text or when the neuroscience literature was used to finetune a pretrained GPT-2. Our results indicate that expert-level performance may be attained by even small LLMs through domain-specific, auto-regressive training approaches.","sentences":["Recently, large language models (LLMs) have outperformed human experts in predicting the results of neuroscience experiments (Luo et al., 2024).","What is the basis for this performance?","One possibility is that statistical patterns in that specific scientific literature, as opposed to emergent reasoning abilities arising from broader training, underlie LLMs' performance.","To evaluate this possibility, we trained (next word prediction) a relatively small 124M-parameter GPT-2 model on 1.3 billion tokens of domain-specific knowledge.","Despite being orders of magnitude smaller than larger LLMs trained on trillions of tokens, small models achieved expert-level performance in predicting neuroscience results.","Small models trained on the neuroscience literature succeeded when they were trained from scratch using a tokenizer specifically trained on neuroscience text or when the neuroscience literature was used to finetune a pretrained GPT-2.","Our results indicate that expert-level performance may be attained by even small LLMs through domain-specific, auto-regressive training approaches."],"url":"http://arxiv.org/abs/2405.09395v1","category":"q-bio.NC"}
{"created":"2024-05-15 14:47:43","title":"Phylotrack: C++ and Python libraries for in silico phylogenetic tracking","abstract":"In silico evolution instantiates the processes of heredity, variation, and differential reproductive success (the three \"ingredients\" for evolution by natural selection) within digital populations of computational agents. Consequently, these populations undergo evolution, and can be used as virtual model systems for studying evolutionary dynamics. This experimental paradigm -- used across biological modeling, artificial life, and evolutionary computation -- complements research done using in vitro and in vivo systems by enabling experiments that would be impossible in the lab or field. One key benefit is complete, exact observability. For example, it is possible to perfectly record all parent-child relationships across simulation history, yielding complete phylogenies (ancestry trees). This information reveals when traits were gained or lost, and also facilitates inference of underlying evolutionary dynamics.   The Phylotrack project provides libraries for tracking and analyzing phylogenies in in silico evolution. The project is composed of 1) Phylotracklib: a header-only C++ library, developed under the umbrella of the Empirical project, and 2) Phylotrackpy: a Python wrapper around Phylotracklib, created with Pybind11. Both components supply a public-facing API to attach phylogenetic tracking to digital evolution systems, as well as a stand-alone interface for measuring a variety of popular phylogenetic topology metrics. Underlying design and C++ implementation prioritizes efficiency, allowing for fast generational turnover for agent populations numbering in the tens of thousands. Several explicit features (e.g., phylogeny pruning and abstraction, etc.) are provided for reducing the memory footprint of phylogenetic information.","sentences":["In silico evolution instantiates the processes of heredity, variation, and differential reproductive success (the three \"ingredients\" for evolution by natural selection) within digital populations of computational agents.","Consequently, these populations undergo evolution, and can be used as virtual model systems for studying evolutionary dynamics.","This experimental paradigm -- used across biological modeling, artificial life, and evolutionary computation -- complements research done using in vitro and in vivo systems by enabling experiments that would be impossible in the lab or field.","One key benefit is complete, exact observability.","For example, it is possible to perfectly record all parent-child relationships across simulation history, yielding complete phylogenies (ancestry trees).","This information reveals when traits were gained or lost, and also facilitates inference of underlying evolutionary dynamics.   ","The Phylotrack project provides libraries for tracking and analyzing phylogenies in in silico evolution.","The project is composed of 1) Phylotracklib: a header-only C++ library, developed under the umbrella of the Empirical project, and 2) Phylotrackpy: a Python wrapper around Phylotracklib, created with Pybind11.","Both components supply a public-facing API to attach phylogenetic tracking to digital evolution systems, as well as a stand-alone interface for measuring a variety of popular phylogenetic topology metrics.","Underlying design and C++ implementation prioritizes efficiency, allowing for fast generational turnover for agent populations numbering in the tens of thousands.","Several explicit features (e.g., phylogeny pruning and abstraction, etc.) are provided for reducing the memory footprint of phylogenetic information."],"url":"http://arxiv.org/abs/2405.09389v1","category":"q-bio.PE"}
{"created":"2024-05-15 14:40:27","title":"On representations and topological aspects of positive maps on non-unital quasi *-algebras","abstract":"In this paper, we provide a representation of a certain class of C*-valued positive sesquilinear and linear maps on non-unital quasi *-algebras. Also, we illustrate our results on the concrete examples of non-unital Banach quasi *-algebras, such as the standard Hilbert module over a commutative C*-algebra, Schatten p-ideals, and noncommutative L2 spaces induced by a semifinite, nonfinite trace. As a consequence of our results, we obtain a representation of all bounded positive linear C*-valued maps on non-unital C*-algebras. We also deduce some norm inequalities for these maps. Finally, we consider a noncommutative L2 space equipped with the topology generated by a positive sesquilinear form and we construct a topologically transitive operator on this space.","sentences":["In this paper, we provide a representation of a certain class of C*-valued positive sesquilinear and linear maps on non-unital quasi *-algebras.","Also, we illustrate our results on the concrete examples of non-unital Banach quasi *-algebras, such as the standard Hilbert module over a commutative C*-algebra, Schatten p-ideals, and noncommutative L2 spaces induced by a semifinite, nonfinite trace.","As a consequence of our results, we obtain a representation of all bounded positive linear C*-valued maps on non-unital C*-algebras.","We also deduce some norm inequalities for these maps.","Finally, we consider a noncommutative L2 space equipped with the topology generated by a positive sesquilinear form and we construct a topologically transitive operator on this space."],"url":"http://arxiv.org/abs/2405.09387v1","category":"math.OA"}
{"created":"2024-05-15 14:25:03","title":"VascularPilot3D: Toward a 3D fully autonomous navigation for endovascular robotics","abstract":"This research reports VascularPilot3D, the first 3D fully autonomous endovascular robot navigation system. As an exploration toward autonomous guidewire navigation, VascularPilot3D is developed as a complete navigation system based on intra-operative imaging systems (fluoroscopic X-ray in this study) and typical endovascular robots. VascularPilot3D adopts previously researched fast 3D-2D vessel registration algorithms and guidewire segmentation methods as its perception modules. We additionally propose three modules: a topology-constrained 2D-3D instrument end-point lifting method, a tree-based fast path planning algorithm, and a prior-free endovascular navigation strategy. VascularPilot3D is compatible with most mainstream endovascular robots. Ex-vivo experiments validate that VascularPilot3D achieves 100% success rate among 25 trials. It reduces the human surgeon's overall control loops by 18.38%. VascularPilot3D is promising for general clinical autonomous endovascular navigations.","sentences":["This research reports VascularPilot3D, the first 3D fully autonomous endovascular robot navigation system.","As an exploration toward autonomous guidewire navigation, VascularPilot3D is developed as a complete navigation system based on intra-operative imaging systems (fluoroscopic X-ray in this study) and typical endovascular robots.","VascularPilot3D adopts previously researched fast 3D-2D vessel registration algorithms and guidewire segmentation methods as its perception modules.","We additionally propose three modules: a topology-constrained 2D-3D instrument end-point lifting method, a tree-based fast path planning algorithm, and a prior-free endovascular navigation strategy.","VascularPilot3D is compatible with most mainstream endovascular robots.","Ex-vivo experiments validate that VascularPilot3D achieves 100% success rate among 25 trials.","It reduces the human surgeon's overall control loops by 18.38%.","VascularPilot3D is promising for general clinical autonomous endovascular navigations."],"url":"http://arxiv.org/abs/2405.09375v1","category":"cs.RO"}
{"created":"2024-05-15 14:23:04","title":"A note on some moduli spaces of Ulrich Bundles","abstract":"We prove that the modular component $\\mathcal M(r)$, constructed in the Main Theorem of a former paper of us (published in Adv. Math on 2024), paramatrizing (isomorphism classes of) Ulrich vector bundles of rank $r$ and given Chern classes, on suitable $3$-fold scrolls $X_e$ over Hirzebruch surfaces $\\mathbb{F}_{e\\geq 0}$, which arise as tautological embeddings of projectivization of very-ample vector bundles on $\\mathbb{F}_e$, is generically smooth and unirational. A stronger result holds for the suitable associated moduli space $\\mathcal M_{\\mathbb F_e}(r)$ of vector bundles of rank $r$ and given Chern classes on $\\mathbb{F}_e$, Ulrich w.r.t. the very ample polarization $c_1({\\mathcal E}_e) = \\mathcal O_{\\mathbb F_e}(3, b_e),$ which turns out to be generically smooth, irreducible and unirational.","sentences":["We prove that the modular component $\\mathcal M(r)$, constructed in the Main Theorem of a former paper of us (published in Adv.","Math on 2024), paramatrizing (isomorphism classes of) Ulrich vector bundles of rank $r$ and given Chern classes, on suitable $3$-fold scrolls $X_e$ over Hirzebruch surfaces $\\mathbb{F}_{e\\geq 0}$, which arise as tautological embeddings of projectivization of very-ample vector bundles on $\\mathbb{F}_e$, is generically smooth and unirational.","A stronger result holds for the suitable associated moduli space $\\mathcal M_{\\mathbb F_e}(r)$ of vector bundles of rank $r$ and given Chern classes on $\\mathbb{F}_e$, Ulrich w.r.t.","the very ample polarization $c_1({\\mathcal E}_e)","= \\mathcal O_{\\mathbb F_e}(3, b_e),$ which turns out to be generically smooth, irreducible and unirational."],"url":"http://arxiv.org/abs/2405.09374v1","category":"math.AG"}
{"created":"2024-05-15 14:22:05","title":"Formation of Beta-Indium Selenide Layers Grown via Selenium Passivation of InP(111)B Substrate","abstract":"Indium selenide, In2Se3, has recently attracted growing interest due to its novel properties, including room temperature ferroelectricity, outstanding photoresponsivity, and exotic in-plane ferroelectricity, which open up new regimes for next generation electronics. In2Se3 also provides the important advantage of tuning the electrical properties of ultra-thin layers with an external electrical and magnetic field, making it a potential platform to study novel two-dimensional physics. Yet, In2Se3 has many different polymorphs, and it has been challenging to synthesize single-phase material, especially using scalable growth methods, as needed for technological applications. In this paper, we use aberration-corrected scanning transmission electron microscopy to characterize the microstructure of twin-free single-phase ultra-thin layers of beta-In2Se3, prepared by a unique molecular beam epitaxy approach. We emphasize features of the In2Se3 layer and In2Se3/InP interface which provide evidence for understanding the growth mechanism of the single-phase In2Se3. This novel approach for forming high-quality twin-free single phase two-dimensional crystals on InP substrates is likely to be applicable to other technologically important substrates.","sentences":["Indium selenide, In2Se3, has recently attracted growing interest due to its novel properties, including room temperature ferroelectricity, outstanding photoresponsivity, and exotic in-plane ferroelectricity, which open up new regimes for next generation electronics.","In2Se3 also provides the important advantage of tuning the electrical properties of ultra-thin layers with an external electrical and magnetic field, making it a potential platform to study novel two-dimensional physics.","Yet, In2Se3 has many different polymorphs, and it has been challenging to synthesize single-phase material, especially using scalable growth methods, as needed for technological applications.","In this paper, we use aberration-corrected scanning transmission electron microscopy to characterize the microstructure of twin-free single-phase ultra-thin layers of beta-In2Se3, prepared by a unique molecular beam epitaxy approach.","We emphasize features of the In2Se3 layer and In2Se3/InP interface which provide evidence for understanding the growth mechanism of the single-phase In2Se3.","This novel approach for forming high-quality twin-free single phase two-dimensional crystals on InP substrates is likely to be applicable to other technologically important substrates."],"url":"http://arxiv.org/abs/2405.09371v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-15 14:20:37","title":"Diffusion-based Contrastive Learning for Sequential Recommendation","abstract":"Contrastive learning has been effectively applied to alleviate the data sparsity issue and enhance recommendation performance.The majority of existing methods employ random augmentation to generate augmented views of original sequences. The learning objective then aims to minimize the distance between representations of different views for the same user. However, these random augmentation strategies (e.g., mask or substitution) neglect the semantic consistency of different augmented views for the same user, leading to semantically inconsistent sequences with similar representations. Furthermore, most augmentation methods fail to utilize context information, which is critical for understanding sequence semantics. To address these limitations, we introduce a diffusion-based contrastive learning approach for sequential recommendation. Specifically, given a user sequence, we first select some positions and then leverage context information to guide the generation of alternative items via a guided diffusion model. By repeating this approach, we can get semantically consistent augmented views for the same user, which are used to improve the effectiveness of contrastive learning. To maintain cohesion between the representation spaces of both the diffusion model and the recommendation model, we train the entire framework in an end-to-end fashion with shared item embeddings. Extensive experiments on five benchmark datasets demonstrate the superiority of our proposed method.","sentences":["Contrastive learning has been effectively applied to alleviate the data sparsity issue and enhance recommendation performance.","The majority of existing methods employ random augmentation to generate augmented views of original sequences.","The learning objective then aims to minimize the distance between representations of different views for the same user.","However, these random augmentation strategies (e.g., mask or substitution) neglect the semantic consistency of different augmented views for the same user, leading to semantically inconsistent sequences with similar representations.","Furthermore, most augmentation methods fail to utilize context information, which is critical for understanding sequence semantics.","To address these limitations, we introduce a diffusion-based contrastive learning approach for sequential recommendation.","Specifically, given a user sequence, we first select some positions and then leverage context information to guide the generation of alternative items via a guided diffusion model.","By repeating this approach, we can get semantically consistent augmented views for the same user, which are used to improve the effectiveness of contrastive learning.","To maintain cohesion between the representation spaces of both the diffusion model and the recommendation model, we train the entire framework in an end-to-end fashion with shared item embeddings.","Extensive experiments on five benchmark datasets demonstrate the superiority of our proposed method."],"url":"http://arxiv.org/abs/2405.09369v1","category":"cs.IR"}
{"created":"2024-05-15 14:18:48","title":"Efficient WENO schemes for nonuniform grids","abstract":"A set of arbitrarily high-order WENO schemes for reconstructions on nonuniform grids is presented. These non-linear interpolation methods use simple smoothness indicators with a linear cost with respect to the order, making them easy to implement and computationally efficient. The theoretical analysis to verify the accuracy and the essentially non-oscillatory properties are presented together with some numerical experiments involving algebraic problems in order to validate them. Also, these general schemes are applied for the solution of conservation laws and hyperbolic systems in the context of finite volume methods.","sentences":["A set of arbitrarily high-order WENO schemes for reconstructions on nonuniform grids is presented.","These non-linear interpolation methods use simple smoothness indicators with a linear cost with respect to the order, making them easy to implement and computationally efficient.","The theoretical analysis to verify the accuracy and the essentially non-oscillatory properties are presented together with some numerical experiments involving algebraic problems in order to validate them.","Also, these general schemes are applied for the solution of conservation laws and hyperbolic systems in the context of finite volume methods."],"url":"http://arxiv.org/abs/2405.09367v1","category":"math.NA"}
{"created":"2024-05-15 14:17:44","title":"SARATR-X: A Foundation Model for Synthetic Aperture Radar Images Target Recognition","abstract":"Synthetic aperture radar (SAR) is essential in actively acquiring information for Earth observation. SAR Automatic Target Recognition (ATR) focuses on detecting and classifying various target categories under different image conditions. The current deep learning-based SAR ATR methods are typically designed for specific datasets and applications. Various target characteristics, scene background information, and sensor parameters across ATR datasets challenge the generalization of those methods. This paper aims to achieve general SAR ATR based on a foundation model with Self-Supervised Learning (SSL). Our motivation is to break through the specific dataset and condition limitations and obtain universal perceptual capabilities across the target, scene, and sensor. A foundation model named SARATR-X is proposed with the following four aspects: pre-training dataset, model backbone, SSL, and evaluation task. First, we integrated 14 datasets with various target categories and imaging conditions as a pre-training dataset. Second, different model backbones were discussed to find the most suitable approaches for remote-sensing images. Third, we applied two-stage training and SAR gradient features to ensure the diversity and scalability of SARATR-X. Finally, SARATR-X has achieved competitive and superior performance on 5 datasets with 8 task settings, which shows that the foundation model can achieve universal SAR ATR. We believe it is time to embrace fundamental models for SAR image interpretation in the era of increasing big data.","sentences":["Synthetic aperture radar (SAR) is essential in actively acquiring information for Earth observation.","SAR Automatic Target Recognition (ATR) focuses on detecting and classifying various target categories under different image conditions.","The current deep learning-based SAR ATR methods are typically designed for specific datasets and applications.","Various target characteristics, scene background information, and sensor parameters across ATR datasets challenge the generalization of those methods.","This paper aims to achieve general SAR ATR based on a foundation model with Self-Supervised Learning (SSL).","Our motivation is to break through the specific dataset and condition limitations and obtain universal perceptual capabilities across the target, scene, and sensor.","A foundation model named SARATR-X is proposed with the following four aspects: pre-training dataset, model backbone, SSL, and evaluation task.","First, we integrated 14 datasets with various target categories and imaging conditions as a pre-training dataset.","Second, different model backbones were discussed to find the most suitable approaches for remote-sensing images.","Third, we applied two-stage training and SAR gradient features to ensure the diversity and scalability of SARATR-X. Finally, SARATR-X has achieved competitive and superior performance on 5 datasets with 8 task settings, which shows that the foundation model can achieve universal SAR ATR.","We believe it is time to embrace fundamental models for SAR image interpretation in the era of increasing big data."],"url":"http://arxiv.org/abs/2405.09365v1","category":"cs.CV"}
{"created":"2024-05-15 14:16:52","title":"Orbital Stability Study of the Taiji Space Gravitational Wave Detector","abstract":"Space-based gravitational wave detection is extremely sensitive to disturbances. The Keplerian configuration cannot accurately reflect the variations in spacecraft configuration. Planetary gravitational disturbances are one of the main sources. Numerical simulation is an effective method to investigate the impact of perturbation on spacecraft orbits. This study shows that, in the context of the Taiji project, Earth's gravity is an essential factor in the change in heliocentric formation configuration, contributing to the relative acceleration between spacecrafts in the order of $\\mathcal O(10^{-6})\\,{\\rm m\\cdot s^{-2}}$. Considering 00:00:00 on 27 October 2032 as the initial orbiting moment, under the influence of Earth's gravitational perturbation, the maximum relative change in armlengths and variation rates of armlengths for Taiji is $1.6\\times 10^{5}\\,{\\rm km}$, $32\\,{\\rm m\\cdot s^{-1}}$, respectively, compared with the unperturbed Keplerian orbit. Additionally, by considering the gravitational perturbations of Venus and Jupiter, the armlength and relative velocity for Taiji are reduced by $16.01\\%$ and $17.45\\%$, respectively, compared with when only considering that of Earth. The maximum amplitude of the formation motion indicator changes with the orbit entry time. Results show that the relative velocity increase between the spacecrafts is minimal when the initial orbital moment occurs in July. Moreover, the numerical simulation results are inconsistent when using different ephemerides. The differences between ephemerides DE440 and DE430 are smaller than those between DE440 and DE421.","sentences":["Space-based gravitational wave detection is extremely sensitive to disturbances.","The Keplerian configuration cannot accurately reflect the variations in spacecraft configuration.","Planetary gravitational disturbances are one of the main sources.","Numerical simulation is an effective method to investigate the impact of perturbation on spacecraft orbits.","This study shows that, in the context of the Taiji project, Earth's gravity is an essential factor in the change in heliocentric formation configuration, contributing to the relative acceleration between spacecrafts in the order of $\\mathcal O(10^{-6})\\,{\\rm m\\cdot s^{-2}}$. Considering 00:00:00 on 27 October 2032 as the initial orbiting moment, under the influence of Earth's gravitational perturbation, the maximum relative change in armlengths and variation rates of armlengths for Taiji is $1.6\\times 10^{5}\\,{\\rm km}$, $32\\,{\\rm m\\cdot s^{-1}}$, respectively, compared with the unperturbed Keplerian orbit.","Additionally, by considering the gravitational perturbations of Venus and Jupiter, the armlength and relative velocity for Taiji are reduced by $16.01\\%$ and $17.45\\%$, respectively, compared with when only considering that of Earth.","The maximum amplitude of the formation motion indicator changes with the orbit entry time.","Results show that the relative velocity increase between the spacecrafts is minimal when the initial orbital moment occurs in July.","Moreover, the numerical simulation results are inconsistent when using different ephemerides.","The differences between ephemerides DE440 and DE430 are smaller than those between DE440 and DE421."],"url":"http://arxiv.org/abs/2405.09364v1","category":"gr-qc"}
{"created":"2024-05-15 14:16:48","title":"Tri-resonant leptogenesis from modular symmetry neutrino models","abstract":"In this paper, we have studied the consequences of some representative modular symmetry neutrino models for tri-resonant leptogenesis (which is realized by having three nearly degenerate right-handed neutrinos). To be specific, we have considered modular ${\\rm A}^{}_4$, ${\\rm S}^{}_4$ and ${\\rm A}^{}_5$ symmetry models that have a right-handed neutrino mass matrix $M^{}_{\\rm R}$ as shown in Eq.~(\\ref{5}) which gives three degenerate right-handed neutrino masses and consequently prohibits the leptogenesis mechanism to work. For these models, we have considered two minimal ways to generate the desired small mass splittings among the three right-handed neutrinos: one scenario is to modify $M^{}_{\\rm R}$ to a form as shown in Eq.~(\\ref{7}), while another scenario is to consider the renormalization-group corrections for the right-handed neutrino masses. In the former scenario, for the considered models, the observed value of $\\eta^{}_{\\rm B}$ can always be successfully reproduced for appropriate values of $\\mu$. But in the latter scenario not all the considered models can successfully reproduce the observed value of $\\eta^{}_{\\rm B}$.","sentences":["In this paper, we have studied the consequences of some representative modular symmetry neutrino models for tri-resonant leptogenesis (which is realized by having three nearly degenerate right-handed neutrinos).","To be specific, we have considered modular ${\\rm A}^{}_4$, ${\\rm S}^{}_4$ and ${\\rm A}^{}_5$ symmetry models that have a right-handed neutrino mass matrix $M^{}_{\\rm R}$ as shown in Eq.~(\\ref{5}) which gives three degenerate right-handed neutrino masses and consequently prohibits the leptogenesis mechanism to work.","For these models, we have considered two minimal ways to generate the desired small mass splittings among the three right-handed neutrinos: one scenario is to modify $M^{}_{\\rm R}$ to a form as shown in Eq.~(\\ref{7}), while another scenario is to consider the renormalization-group corrections for the right-handed neutrino masses.","In the former scenario, for the considered models, the observed value of $\\eta^{}_{\\rm B}$ can always be successfully reproduced for appropriate values of $\\mu$. But in the latter scenario not all the considered models can successfully reproduce the observed value of $\\eta^{}_{\\rm B}$."],"url":"http://arxiv.org/abs/2405.09363v1","category":"hep-ph"}
{"created":"2024-05-15 14:09:11","title":"Vision-Based Neurosurgical Guidance: Unsupervised Localization and Camera-Pose Prediction","abstract":"Localizing oneself during endoscopic procedures can be problematic due to the lack of distinguishable textures and landmarks, as well as difficulties due to the endoscopic device such as a limited field of view and challenging lighting conditions. Expert knowledge shaped by years of experience is required for localization within the human body during endoscopic procedures. In this work, we present a deep learning method based on anatomy recognition, that constructs a surgical path in an unsupervised manner from surgical videos, modelling relative location and variations due to different viewing angles. At inference time, the model can map an unseen video's frames on the path and estimate the viewing angle, aiming to provide guidance, for instance, to reach a particular destination. We test the method on a dataset consisting of surgical videos of transsphenoidal adenomectomies, as well as on a synthetic dataset. An online tool that lets researchers upload their surgical videos to obtain anatomy detections and the weights of the trained YOLOv7 model are available at: https://surgicalvision.bmic.ethz.ch.","sentences":["Localizing oneself during endoscopic procedures can be problematic due to the lack of distinguishable textures and landmarks, as well as difficulties due to the endoscopic device such as a limited field of view and challenging lighting conditions.","Expert knowledge shaped by years of experience is required for localization within the human body during endoscopic procedures.","In this work, we present a deep learning method based on anatomy recognition, that constructs a surgical path in an unsupervised manner from surgical videos, modelling relative location and variations due to different viewing angles.","At inference time, the model can map an unseen video's frames on the path and estimate the viewing angle, aiming to provide guidance, for instance, to reach a particular destination.","We test the method on a dataset consisting of surgical videos of transsphenoidal adenomectomies, as well as on a synthetic dataset.","An online tool that lets researchers upload their surgical videos to obtain anatomy detections and the weights of the trained YOLOv7 model are available at: https://surgicalvision.bmic.ethz.ch."],"url":"http://arxiv.org/abs/2405.09355v1","category":"cs.CV"}
{"created":"2024-05-15 14:04:04","title":"3D-DASH: The Evolution of Size, Shape, and Intrinsic Scatter in Populations of Young and Old Quiescent Galaxies at 0.5 < z < 3","abstract":"We present a study of the growth of the quiescent galaxy population between 0.5 < z < 3 by tracing the number density and structural evolution of a sample of 4518 old and 583 young quiescent galaxies with log($M_*$/$M_{\\odot}$)>10.4, selected from the COSMOS2020 catalog with complementary HST/F160W imaging from the 3D-DASH survey. Among the quiescent population at z$\\sim$2, roughly 50% are recently quenched galaxies; these young quiescent galaxies become increasingly rare towards lower redshift, supporting the idea that the peak epoch of massive galaxy quenching occurred at z>2. Our data show that while the effective half-light radii of quiescent galaxies generally increases with time, young quiescent galaxies are significantly smaller than their older counterparts at the same redshift. In this work we investigate the connection between this size difference and other structural properties, including axis ratios, color gradients, stellar mass, and the intrinsic scatter in effective radii. We demonstrate that the size difference is driven by the most massive sub-population (log($M_*$/$M_{\\odot}$)>11) and does not persist when restricting the sample to intermediate mass galaxies (10.4<log($M_*$/$M_{\\odot}$)<11). Interestingly, the intrinsic scatter in physical size shows a strong co-evolution over the investigated time period and peaks around z$\\sim$2 for both populations, only diverging at z < 1. Taken together, and assuming we are not missing a significant population of lower surface brightness galaxies, while the formation and quenching mechanisms that dominate at higher redshifts yield compact remnants, multiple evolutionary pathways may explain the diverse morphologies of galaxies that quench at z<1.","sentences":["We present a study of the growth of the quiescent galaxy population between 0.5 < z < 3 by tracing the number density and structural evolution of a sample of 4518 old and 583 young quiescent galaxies with log($M_*$/$M_{\\odot}$)>10.4, selected from the COSMOS2020 catalog with complementary HST/F160W imaging from the 3D-DASH survey.","Among the quiescent population at z$\\sim$2, roughly 50% are recently quenched galaxies; these young quiescent galaxies become increasingly rare towards lower redshift, supporting the idea that the peak epoch of massive galaxy quenching occurred at z>2.","Our data show that while the effective half-light radii of quiescent galaxies generally increases with time, young quiescent galaxies are significantly smaller than their older counterparts at the same redshift.","In this work we investigate the connection between this size difference and other structural properties, including axis ratios, color gradients, stellar mass, and the intrinsic scatter in effective radii.","We demonstrate that the size difference is driven by the most massive sub-population (log($M_*$/$M_{\\odot}$)>11) and does not persist when restricting the sample to intermediate mass galaxies (10.4<log($M_*$/$M_{\\odot}$)<11).","Interestingly, the intrinsic scatter in physical size shows a strong co-evolution over the investigated time period and peaks around z$\\sim$2 for both populations, only diverging at z < 1.","Taken together, and assuming we are not missing a significant population of lower surface brightness galaxies, while the formation and quenching mechanisms that dominate at higher redshifts yield compact remnants, multiple evolutionary pathways may explain the diverse morphologies of galaxies that quench at z<1."],"url":"http://arxiv.org/abs/2405.09354v1","category":"astro-ph.GA"}
{"created":"2024-05-15 13:55:05","title":"Tip-leakage-flow excited unsteadiness and associated control","abstract":"Tip leakage flow in turbomachinery inherently generates intense unsteady features, named self-excited unsteadiness, which has not been well understood. A Zonalised LES (ZLES) is employed around a linear cascade, with wall-modelled Large Eddy Simulation (LES) forced to be active in the tip region. The simulation is well validated and demonstrated the advantages of effectively reducing the computational effort while maintaining an equivalent prediction accuracy in the region of interest. The time-averaged and spatial-spectral characteristics of tip leakage vortex (TLV) structures are systematically discussed. The self-excited unsteady processes of TLV include unsteady vortex separation inside the tip gap, the tip leakage jet-mainstream interaction, the primary tip leakage vortex (PTLV) wandering motion and the induced separation near endwall. The Spectral Proper Orthogonal Decomposition (SPOD) is used to examine the dominant frequencies and their coherent structures. It is found that these unsteady features change from a single high-frequency mode to a multiple lower-frequencies mode due to the PTLV breakdown. The SPOD and correlation analyses reveal that the self-excited unsteadiness is mainly induced by the interactions between unsteady vortex separation, tip leakage jet, and mainstream. The associated unsteady fluctuations are convected along the tip leakage jet trajectory, causing the wandering motion of PTLV core. Based on the cause of the investigated unsteadiness, a micro-offset tip design is proposed and validated for effectively suppressing this unsteadiness, and associated turbulence generation and hence pressure fluctuations. This work improves the understanding of tip-leakage-flow dynamics and informs the control of the associated unsteady fluid oscillation and noise.","sentences":["Tip leakage flow in turbomachinery inherently generates intense unsteady features, named self-excited unsteadiness, which has not been well understood.","A Zonalised LES (ZLES) is employed around a linear cascade, with wall-modelled Large Eddy Simulation (LES) forced to be active in the tip region.","The simulation is well validated and demonstrated the advantages of effectively reducing the computational effort while maintaining an equivalent prediction accuracy in the region of interest.","The time-averaged and spatial-spectral characteristics of tip leakage vortex (TLV) structures are systematically discussed.","The self-excited unsteady processes of TLV include unsteady vortex separation inside the tip gap, the tip leakage jet-mainstream interaction, the primary tip leakage vortex (PTLV) wandering motion and the induced separation near endwall.","The Spectral Proper Orthogonal Decomposition (SPOD) is used to examine the dominant frequencies and their coherent structures.","It is found that these unsteady features change from a single high-frequency mode to a multiple lower-frequencies mode due to the PTLV breakdown.","The SPOD and correlation analyses reveal that the self-excited unsteadiness is mainly induced by the interactions between unsteady vortex separation, tip leakage jet, and mainstream.","The associated unsteady fluctuations are convected along the tip leakage jet trajectory, causing the wandering motion of PTLV core.","Based on the cause of the investigated unsteadiness, a micro-offset tip design is proposed and validated for effectively suppressing this unsteadiness, and associated turbulence generation and hence pressure fluctuations.","This work improves the understanding of tip-leakage-flow dynamics and informs the control of the associated unsteady fluid oscillation and noise."],"url":"http://arxiv.org/abs/2405.09347v1","category":"physics.flu-dyn"}
{"created":"2024-05-15 13:45:33","title":"Progressive Depth Decoupling and Modulating for Flexible Depth Completion","abstract":"Image-guided depth completion aims at generating a dense depth map from sparse LiDAR data and RGB image. Recent methods have shown promising performance by reformulating it as a classification problem with two sub-tasks: depth discretization and probability prediction. They divide the depth range into several discrete depth values as depth categories, serving as priors for scene depth distributions. However, previous depth discretization methods are easy to be impacted by depth distribution variations across different scenes, resulting in suboptimal scene depth distribution priors. To address the above problem, we propose a progressive depth decoupling and modulating network, which incrementally decouples the depth range into bins and adaptively generates multi-scale dense depth maps in multiple stages. Specifically, we first design a Bins Initializing Module (BIM) to construct the seed bins by exploring the depth distribution information within a sparse depth map, adapting variations of depth distribution. Then, we devise an incremental depth decoupling branch to progressively refine the depth distribution information from global to local. Meanwhile, an adaptive depth modulating branch is developed to progressively improve the probability representation from coarse-grained to fine-grained. And the bi-directional information interactions are proposed to strengthen the information interaction between those two branches (sub-tasks) for promoting information complementation in each branch. Further, we introduce a multi-scale supervision mechanism to learn the depth distribution information in latent features and enhance the adaptation capability across different scenes. Experimental results on public datasets demonstrate that our method outperforms the state-of-the-art methods. The code will be open-sourced at [this https URL](https://github.com/Cisse-away/PDDM).","sentences":["Image-guided depth completion aims at generating a dense depth map from sparse LiDAR data and RGB image.","Recent methods have shown promising performance by reformulating it as a classification problem with two sub-tasks: depth discretization and probability prediction.","They divide the depth range into several discrete depth values as depth categories, serving as priors for scene depth distributions.","However, previous depth discretization methods are easy to be impacted by depth distribution variations across different scenes, resulting in suboptimal scene depth distribution priors.","To address the above problem, we propose a progressive depth decoupling and modulating network, which incrementally decouples the depth range into bins and adaptively generates multi-scale dense depth maps in multiple stages.","Specifically, we first design a Bins Initializing Module (BIM) to construct the seed bins by exploring the depth distribution information within a sparse depth map, adapting variations of depth distribution.","Then, we devise an incremental depth decoupling branch to progressively refine the depth distribution information from global to local.","Meanwhile, an adaptive depth modulating branch is developed to progressively improve the probability representation from coarse-grained to fine-grained.","And the bi-directional information interactions are proposed to strengthen the information interaction between those two branches (sub-tasks) for promoting information complementation in each branch.","Further, we introduce a multi-scale supervision mechanism to learn the depth distribution information in latent features and enhance the adaptation capability across different scenes.","Experimental results on public datasets demonstrate that our method outperforms the state-of-the-art methods.","The code will be open-sourced at [this https URL](https://github.com/Cisse-away/PDDM)."],"url":"http://arxiv.org/abs/2405.09342v1","category":"cs.CV"}
{"created":"2024-05-15 13:44:13","title":"Large Language Model Bias Mitigation from the Perspective of Knowledge Editing","abstract":"Existing debiasing methods inevitably make unreasonable or undesired predictions as they are designated and evaluated to achieve parity across different social groups but leave aside individual facts, resulting in modified existing knowledge. In this paper, we first establish a new bias mitigation benchmark BiasKE leveraging existing and additional constructed datasets, which systematically assesses debiasing performance by complementary metrics on fairness, specificity, and generalization. Meanwhile, we propose a novel debiasing method, Fairness Stamp (FAST), which enables editable fairness through fine-grained calibration on individual biased knowledge. Comprehensive experiments demonstrate that FAST surpasses state-of-the-art baselines with remarkable debiasing performance while not hampering overall model capability for knowledge preservation, highlighting the prospect of fine-grained debiasing strategies for editable fairness in LLMs.","sentences":["Existing debiasing methods inevitably make unreasonable or undesired predictions as they are designated and evaluated to achieve parity across different social groups but leave aside individual facts, resulting in modified existing knowledge.","In this paper, we first establish a new bias mitigation benchmark BiasKE leveraging existing and additional constructed datasets, which systematically assesses debiasing performance by complementary metrics on fairness, specificity, and generalization.","Meanwhile, we propose a novel debiasing method, Fairness Stamp (FAST), which enables editable fairness through fine-grained calibration on individual biased knowledge.","Comprehensive experiments demonstrate that FAST surpasses state-of-the-art baselines with remarkable debiasing performance while not hampering overall model capability for knowledge preservation, highlighting the prospect of fine-grained debiasing strategies for editable fairness in LLMs."],"url":"http://arxiv.org/abs/2405.09341v1","category":"cs.CL"}
{"created":"2024-05-15 13:43:57","title":"Laser Printing of Silver and Silver Oxide","abstract":"We show that direct laser writing (DLW) in aqueous silver nitrate with a 1030 nm femtosecond (fs) laser results in deposition of a mixture of silver oxide and silver, in contrast to the pure silver deposition previously reported with 780 nm fs DLW. However, adding photoinitiator prevents silver oxide formation in a concentration-dependent manner. As a result, the resistivity of the material can also be controlled by photoinitiator concentration with resistivity being reduced from approximately 9e-3 $\\Omega m$ to 3e-7 $\\Omega m$. Silver oxide peaks dominate the X-ray diffraction spectra when no photoinitiator is present, while the peaks disappear with photoinitiator concentrations above 0.05wt%. While femtosecond pulses are needed to initiate deposition, a continues-wave laser when well overlapped with the previously written material and supplying enough average power can lead to further printing, suggesting thermal deposition can also occur where the photoinitiator molecule also acts as a general reducing agent that prevents oxide formation. We also compare the surface quality of printed lines for different photoinitiator concentrations and laser printing conditions. A THz polarizer and metamaterial are printed as a demonstration of silver oxide printing.","sentences":["We show that direct laser writing (DLW) in aqueous silver nitrate with a 1030 nm femtosecond (fs) laser results in deposition of a mixture of silver oxide and silver, in contrast to the pure silver deposition previously reported with 780 nm fs DLW.","However, adding photoinitiator prevents silver oxide formation in a concentration-dependent manner.","As a result, the resistivity of the material can also be controlled by photoinitiator concentration with resistivity being reduced from approximately 9e-3 $\\Omega m$ to 3e-7 $\\Omega m$.","Silver oxide peaks dominate the X-ray diffraction spectra when no photoinitiator is present, while the peaks disappear with photoinitiator concentrations above 0.05wt%.","While femtosecond pulses are needed to initiate deposition, a continues-wave laser when well overlapped with the previously written material and supplying enough average power can lead to further printing, suggesting thermal deposition can also occur where the photoinitiator molecule also acts as a general reducing agent that prevents oxide formation.","We also compare the surface quality of printed lines for different photoinitiator concentrations and laser printing conditions.","A THz polarizer and metamaterial are printed as a demonstration of silver oxide printing."],"url":"http://arxiv.org/abs/2405.09340v1","category":"physics.optics"}
{"created":"2024-05-15 13:41:00","title":"Imaginary quadratic fields $F$ with $X_0(15)(F)$ finite","abstract":"Caraiani and Newton have proven that if $F$ is an imaginary quadratic number field such that $X_0(15)$ has rank $0$ over $F$, then every elliptic curve over $F$ is modular. This paper is concerned with the quadratic fields $F=\\mathbb{Q}(\\sqrt{-p})$ for a prime number $p$. We give explicit conditions on $p$ under which the rank is $0$, and prove that these conditions are satisfied for $87,5\\%$ of the primes for which the rank is expected to be even based on the parity conjecture. We also show these conditions are satisfied if and only if rank $0$ follows from a $4$-descent over $\\mathbb{Q}$ on the quadratic twist $X_0(15)_{-p}$. To prove this, we perform two consecutive $2$-descents and prove this gives rank bounds equivalent to those obtained from a $4$-descent using visualisation techniques for $\\mathrm{Sha}[2]$. In fact we prove a more general connection between higher descents for elliptic curves which seems interesting in its own right.","sentences":["Caraiani and Newton have proven that if $F$ is an imaginary quadratic number field such that $X_0(15)$ has rank $0$ over $F$, then every elliptic curve over $F$ is modular.","This paper is concerned with the quadratic fields $F=\\mathbb{Q}(\\sqrt{-p})$ for a prime number $p$. We give explicit conditions on $p$ under which the rank is $0$, and prove that these conditions are satisfied for $87,5\\%$ of the primes for which the rank is expected to be even based on the parity conjecture.","We also show these conditions are satisfied if and only if rank $0$ follows from a $4$-descent over $\\mathbb{Q}$ on the quadratic twist $X_0(15)_{-p}$. To prove this, we perform two consecutive $2$-descents and prove this gives rank bounds equivalent to those obtained from a $4$-descent using visualisation techniques for $\\mathrm{Sha}[2]$. In fact we prove a more general connection between higher descents for elliptic curves which seems interesting in its own right."],"url":"http://arxiv.org/abs/2405.09337v1","category":"math.NT"}
{"created":"2024-05-15 13:36:43","title":"Prompting-based Synthetic Data Generation for Few-Shot Question Answering","abstract":"Although language models (LMs) have boosted the performance of Question Answering, they still need plenty of data. Data annotation, in contrast, is a time-consuming process. This especially applies to Question Answering, where possibly large documents have to be parsed and annotated with questions and their corresponding answers. Furthermore, Question Answering models often only work well for the domain they were trained on. Since annotation is costly, we argue that domain-agnostic knowledge from LMs, such as linguistic understanding, is sufficient to create a well-curated dataset. With this motivation, we show that using large language models can improve Question Answering performance on various datasets in the few-shot setting compared to state-of-the-art approaches. For this, we perform data generation leveraging the Prompting framework, suggesting that language models contain valuable task-agnostic knowledge that can be used beyond the common pre-training/fine-tuning scheme. As a result, we consistently outperform previous approaches on few-shot Question Answering.","sentences":["Although language models (LMs) have boosted the performance of Question Answering, they still need plenty of data.","Data annotation, in contrast, is a time-consuming process.","This especially applies to Question Answering, where possibly large documents have to be parsed and annotated with questions and their corresponding answers.","Furthermore, Question Answering models often only work well for the domain they were trained on.","Since annotation is costly, we argue that domain-agnostic knowledge from LMs, such as linguistic understanding, is sufficient to create a well-curated dataset.","With this motivation, we show that using large language models can improve Question Answering performance on various datasets in the few-shot setting compared to state-of-the-art approaches.","For this, we perform data generation leveraging the Prompting framework, suggesting that language models contain valuable task-agnostic knowledge that can be used beyond the common pre-training/fine-tuning scheme.","As a result, we consistently outperform previous approaches on few-shot Question Answering."],"url":"http://arxiv.org/abs/2405.09335v1","category":"cs.CL"}
{"created":"2024-05-15 13:34:07","title":"Content-Based Image Retrieval for Multi-Class Volumetric Radiology Images: A Benchmark Study","abstract":"While content-based image retrieval (CBIR) has been extensively studied in natural image retrieval, its application to medical images presents ongoing challenges, primarily due to the 3D nature of medical images. Recent studies have shown the potential use of pre-trained vision embeddings for CBIR in the context of radiology image retrieval. However, a benchmark for the retrieval of 3D volumetric medical images is still lacking, hindering the ability to objectively evaluate and compare the efficiency of proposed CBIR approaches in medical imaging. In this study, we extend previous work and establish a benchmark for region-based and multi-organ retrieval using the TotalSegmentator dataset (TS) with detailed multi-organ annotations. We benchmark embeddings derived from pre-trained supervised models on medical images against embeddings derived from pre-trained unsupervised models on non-medical images for 29 coarse and 104 detailed anatomical structures in volume and region levels. We adopt a late interaction re-ranking method inspired by text matching for image retrieval, comparing it against the original method proposed for volume and region retrieval achieving retrieval recall of 1.0 for diverse anatomical regions with a wide size range. The findings and methodologies presented in this paper provide essential insights and benchmarks for the development and evaluation of CBIR approaches in the context of medical imaging.","sentences":["While content-based image retrieval (CBIR) has been extensively studied in natural image retrieval, its application to medical images presents ongoing challenges, primarily due to the 3D nature of medical images.","Recent studies have shown the potential use of pre-trained vision embeddings for CBIR in the context of radiology image retrieval.","However, a benchmark for the retrieval of 3D volumetric medical images is still lacking, hindering the ability to objectively evaluate and compare the efficiency of proposed CBIR approaches in medical imaging.","In this study, we extend previous work and establish a benchmark for region-based and multi-organ retrieval using the TotalSegmentator dataset (TS) with detailed multi-organ annotations.","We benchmark embeddings derived from pre-trained supervised models on medical images against embeddings derived from pre-trained unsupervised models on non-medical images for 29 coarse and 104 detailed anatomical structures in volume and region levels.","We adopt a late interaction re-ranking method inspired by text matching for image retrieval, comparing it against the original method proposed for volume and region retrieval achieving retrieval recall of 1.0 for diverse anatomical regions with a wide size range.","The findings and methodologies presented in this paper provide essential insights and benchmarks for the development and evaluation of CBIR approaches in the context of medical imaging."],"url":"http://arxiv.org/abs/2405.09334v1","category":"cs.CV"}
{"created":"2024-05-15 13:33:12","title":"Revealing the Production Mechanism of High-Energy Neutrinos from NGC 1068","abstract":"The detection of high-energy neutrino signals from the nearby Seyfert galaxy NGC 1068 provides us with an opportunity to study nonthermal processes near the center of supermassive black holes. Using the IceCube and latest Fermi-LAT data, we present general multimessenger constraints on the energetics of cosmic rays and the size of neutrino emission regions. In the photohadronic scenario, the required cosmic-ray luminosity should be larger than about 1-10% of the Eddington luminosity and the emission radius should be 15 Schwarzschild radii in low-beta plasma and 3 Schwarzschild radii in high-beta plasma. The leptonic scenario overshoots the NuSTAR or Fermi-LAT data for any emission radii we consider, and the required gamma-ray luminosity is much larger than the Eddington luminosity. The beta decay scenario also violates not only the energetics requirement but also gamma-ray constraints especially when the Bethe-Heitler and photomeson production processes are consistently considered. Our results rule out the leptonic and beta decay scenarios in a nearly model-independent manner, and support hadronic mechanisms in magnetically-powered coronae if NGC 1068 is a source of high-energy neutrinos.","sentences":["The detection of high-energy neutrino signals from the nearby Seyfert galaxy NGC 1068 provides us with an opportunity to study nonthermal processes near the center of supermassive black holes.","Using the IceCube and latest Fermi-LAT data, we present general multimessenger constraints on the energetics of cosmic rays and the size of neutrino emission regions.","In the photohadronic scenario, the required cosmic-ray luminosity should be larger than about 1-10% of the Eddington luminosity and the emission radius should be 15 Schwarzschild radii in low-beta plasma and 3 Schwarzschild radii in high-beta plasma.","The leptonic scenario overshoots the NuSTAR or Fermi-LAT data for any emission radii we consider, and the required gamma-ray luminosity is much larger than the Eddington luminosity.","The beta decay scenario also violates not only the energetics requirement but also gamma-ray constraints especially when the Bethe-Heitler and photomeson production processes are consistently considered.","Our results rule out the leptonic and beta decay scenarios in a nearly model-independent manner, and support hadronic mechanisms in magnetically-powered coronae if NGC 1068 is a source of high-energy neutrinos."],"url":"http://arxiv.org/abs/2405.09332v1","category":"astro-ph.HE"}
{"created":"2024-05-15 13:33:09","title":"Multi-Source Conformal Inference Under Distribution Shift","abstract":"Recent years have experienced increasing utilization of complex machine learning models across multiple sources of data to inform more generalizable decision-making. However, distribution shifts across data sources and privacy concerns related to sharing individual-level data, coupled with a lack of uncertainty quantification from machine learning predictions, make it challenging to achieve valid inferences in multi-source environments. In this paper, we consider the problem of obtaining distribution-free prediction intervals for a target population, leveraging multiple potentially biased data sources. We derive the efficient influence functions for the quantiles of unobserved outcomes in the target and source populations, and show that one can incorporate machine learning prediction algorithms in the estimation of nuisance functions while still achieving parametric rates of convergence to nominal coverage probabilities. Moreover, when conditional outcome invariance is violated, we propose a data-adaptive strategy to upweight informative data sources for efficiency gain and downweight non-informative data sources for bias reduction. We highlight the robustness and efficiency of our proposals for a variety of conformal scores and data-generating mechanisms via extensive synthetic experiments. Hospital length of stay prediction intervals for pediatric patients undergoing a high-risk cardiac surgical procedure between 2016-2022 in the U.S. illustrate the utility of our methodology.","sentences":["Recent years have experienced increasing utilization of complex machine learning models across multiple sources of data to inform more generalizable decision-making.","However, distribution shifts across data sources and privacy concerns related to sharing individual-level data, coupled with a lack of uncertainty quantification from machine learning predictions, make it challenging to achieve valid inferences in multi-source environments.","In this paper, we consider the problem of obtaining distribution-free prediction intervals for a target population, leveraging multiple potentially biased data sources.","We derive the efficient influence functions for the quantiles of unobserved outcomes in the target and source populations, and show that one can incorporate machine learning prediction algorithms in the estimation of nuisance functions while still achieving parametric rates of convergence to nominal coverage probabilities.","Moreover, when conditional outcome invariance is violated, we propose a data-adaptive strategy to upweight informative data sources for efficiency gain and downweight non-informative data sources for bias reduction.","We highlight the robustness and efficiency of our proposals for a variety of conformal scores and data-generating mechanisms via extensive synthetic experiments.","Hospital length of stay prediction intervals for pediatric patients undergoing a high-risk cardiac surgical procedure between 2016-2022 in the U.S. illustrate the utility of our methodology."],"url":"http://arxiv.org/abs/2405.09331v1","category":"stat.ME"}
{"created":"2024-05-15 13:30:40","title":"WENO scheme on characteristics for the equilibrium dispersive model of chromatography with generalized Langmuir isotherms","abstract":"Column chromatography is a laboratory and industrial technique used to separate different substances mixed in a solution. Mathematically, it can be modelled using non-linear partial differential equations whose main ingredients are the adsorption isotherms, which are non-linear functions modelling the affinity between the different substances in the solution and the solid stationary phase filling the column. The goal of this work is twofold. Firstly, we aim to extend the techniques of Donat, Guerrero and Mulet (Appl. Numer. Math. 123 (2018) 22-42) to other adsorption isotherms. In particular, we propose a family of generalized Langmuir-type isotherms and prove that the correspondence between the concentrations of solutes in the liquid phase (the primitive variables) and the conserved variables is well defined and admits a global smooth inverse that can be computed numerically. Secondly, to establish the well-posedness of the mathematical model, we study the eigenstructure of the Jacobian of the mentioned correspondence and use this characteristic information to get oscillation-free sharp interfaces on the numerical approximate solutions. To do so, we determine the structure of the Jacobian matrix of the system and use it to deduce its eigenstructure. We combine the use of characteristic-based numerical fluxes with a second-order implicit-explicit scheme proposed in the cited reference and perform some numerical experiments with T\\'oth's adsorption isotherms to demonstrate that the characteristic-based schemes produce accurate numerical solutions with no oscillations, even when steep gradients appear in the solutions.","sentences":["Column chromatography is a laboratory and industrial technique used to separate different substances mixed in a solution.","Mathematically, it can be modelled using non-linear partial differential equations whose main ingredients are the adsorption isotherms, which are non-linear functions modelling the affinity between the different substances in the solution and the solid stationary phase filling the column.","The goal of this work is twofold.","Firstly, we aim to extend the techniques of Donat, Guerrero and Mulet (Appl.","Numer.","Math. 123 (2018) 22-42) to other adsorption isotherms.","In particular, we propose a family of generalized Langmuir-type isotherms and prove that the correspondence between the concentrations of solutes in the liquid phase (the primitive variables) and the conserved variables is well defined and admits a global smooth inverse that can be computed numerically.","Secondly, to establish the well-posedness of the mathematical model, we study the eigenstructure of the Jacobian of the mentioned correspondence and use this characteristic information to get oscillation-free sharp interfaces on the numerical approximate solutions.","To do so, we determine the structure of the Jacobian matrix of the system and use it to deduce its eigenstructure.","We combine the use of characteristic-based numerical fluxes with a second-order implicit-explicit scheme proposed in the cited reference and perform some numerical experiments with T\\'oth's adsorption isotherms to demonstrate that the characteristic-based schemes produce accurate numerical solutions with no oscillations, even when steep gradients appear in the solutions."],"url":"http://arxiv.org/abs/2405.09328v1","category":"math.NA"}
{"created":"2024-05-15 13:27:03","title":"Leveraging graphical model techniques to study evolution on phylogenetic networks","abstract":"The evolution of molecular and phenotypic traits is commonly modelled using Markov processes along a rooted phylogeny. This phylogeny can be a tree, or a network if it includes reticulations, representing events such as hybridization or admixture. Computing the likelihood of data observed at the leaves is costly as the size and complexity of the phylogeny grows. Efficient algorithms exist for trees, but cannot be applied to networks. We show that a vast array of models for trait evolution along phylogenetic networks can be reformulated as graphical models, for which efficient belief propagation algorithms exist. We provide a brief review of belief propagation on general graphical models, then focus on linear Gaussian models for continuous traits. We show how belief propagation techniques can be applied for exact or approximate (but more scalable) likelihood and gradient calculations, and prove novel results for efficient parameter inference of some models. We highlight the possible fruitful interactions between graphical models and phylogenetic methods. For example, approximate likelihood approaches have the potential to greatly reduce computational costs for phylogenies with reticulations.","sentences":["The evolution of molecular and phenotypic traits is commonly modelled using Markov processes along a rooted phylogeny.","This phylogeny can be a tree, or a network if it includes reticulations, representing events such as hybridization or admixture.","Computing the likelihood of data observed at the leaves is costly as the size and complexity of the phylogeny grows.","Efficient algorithms exist for trees, but cannot be applied to networks.","We show that a vast array of models for trait evolution along phylogenetic networks can be reformulated as graphical models, for which efficient belief propagation algorithms exist.","We provide a brief review of belief propagation on general graphical models, then focus on linear Gaussian models for continuous traits.","We show how belief propagation techniques can be applied for exact or approximate (but more scalable) likelihood and gradient calculations, and prove novel results for efficient parameter inference of some models.","We highlight the possible fruitful interactions between graphical models and phylogenetic methods.","For example, approximate likelihood approaches have the potential to greatly reduce computational costs for phylogenies with reticulations."],"url":"http://arxiv.org/abs/2405.09327v1","category":"q-bio.PE"}
{"created":"2024-05-15 13:24:39","title":"Exponential Quintessence: curved, steep and stringy?","abstract":"We explore the possibility that our universe's current accelerated expansion is explained by a quintessence model with an exponential scalar potential, $V =V_0\\, e^{-\\lambda\\, \\phi}$, keeping an eye towards $\\lambda \\geq \\sqrt{2}$ and an open universe, favorable to a string theory realisation and with no cosmological horizon. We work out the full cosmology of the model, including matter, radiation, and optionally negative spatial curvature, for all $\\lambda>0$, performing an extensive analysis of the dynamical system and its phase space. The minimal physical requirements of a past epoch of radiation domination and an accelerated expansion today lead to an upper bound $\\lambda \\lesssim \\sqrt{3}$, which is driven slightly up in the presence of observationally allowed spatial curvature. Cosmological solutions start universally in a kination epoch, go through radiation and matter dominated phases and enter an epoch of acceleration, which is only transient for $\\lambda>\\sqrt{2}$. Field distances traversed between BBN and today are sub-Planckian. We discuss possible string theory origins and phenomenological challenges, such as time variation of fundamental constants. We provide theoretical predictions for the model parameters to be fitted to data, most notably the varying dark energy equation of state parameter, in light of recent results from DES-Y5 and DESI.","sentences":["We explore the possibility that our universe's current accelerated expansion is explained by a quintessence model with an exponential scalar potential, $V =V_0\\, e^{-\\lambda\\, \\phi}$, keeping an eye towards $\\lambda \\geq \\sqrt{2}$ and an open universe, favorable to a string theory realisation and with no cosmological horizon.","We work out the full cosmology of the model, including matter, radiation, and optionally negative spatial curvature, for all $\\lambda>0$, performing an extensive analysis of the dynamical system and its phase space.","The minimal physical requirements of a past epoch of radiation domination and an accelerated expansion today lead to an upper bound $\\lambda \\lesssim \\sqrt{3}$, which is driven slightly up in the presence of observationally allowed spatial curvature.","Cosmological solutions start universally in a kination epoch, go through radiation and matter dominated phases and enter an epoch of acceleration, which is only transient for $\\lambda>\\sqrt{2}$. Field distances traversed between BBN and today are sub-Planckian.","We discuss possible string theory origins and phenomenological challenges, such as time variation of fundamental constants.","We provide theoretical predictions for the model parameters to be fitted to data, most notably the varying dark energy equation of state parameter, in light of recent results from DES-Y5 and DESI."],"url":"http://arxiv.org/abs/2405.09323v1","category":"hep-th"}
{"created":"2024-05-15 13:22:39","title":"ReconBoost: Boosting Can Achieve Modality Reconcilement","abstract":"This paper explores a novel multi-modal alternating learning paradigm pursuing a reconciliation between the exploitation of uni-modal features and the exploration of cross-modal interactions. This is motivated by the fact that current paradigms of multi-modal learning tend to explore multi-modal features simultaneously. The resulting gradient prohibits further exploitation of the features in the weak modality, leading to modality competition, where the dominant modality overpowers the learning process. To address this issue, we study the modality-alternating learning paradigm to achieve reconcilement. Specifically, we propose a new method called ReconBoost to update a fixed modality each time. Herein, the learning objective is dynamically adjusted with a reconcilement regularization against competition with the historical models. By choosing a KL-based reconcilement, we show that the proposed method resembles Friedman's Gradient-Boosting (GB) algorithm, where the updated learner can correct errors made by others and help enhance the overall performance. The major difference with the classic GB is that we only preserve the newest model for each modality to avoid overfitting caused by ensembling strong learners. Furthermore, we propose a memory consolidation scheme and a global rectification scheme to make this strategy more effective. Experiments over six multi-modal benchmarks speak to the efficacy of the method. We release the code at https://github.com/huacong/ReconBoost.","sentences":["This paper explores a novel multi-modal alternating learning paradigm pursuing a reconciliation between the exploitation of uni-modal features and the exploration of cross-modal interactions.","This is motivated by the fact that current paradigms of multi-modal learning tend to explore multi-modal features simultaneously.","The resulting gradient prohibits further exploitation of the features in the weak modality, leading to modality competition, where the dominant modality overpowers the learning process.","To address this issue, we study the modality-alternating learning paradigm to achieve reconcilement.","Specifically, we propose a new method called ReconBoost to update a fixed modality each time.","Herein, the learning objective is dynamically adjusted with a reconcilement regularization against competition with the historical models.","By choosing a KL-based reconcilement, we show that the proposed method resembles Friedman's Gradient-Boosting (GB) algorithm, where the updated learner can correct errors made by others and help enhance the overall performance.","The major difference with the classic GB is that we only preserve the newest model for each modality to avoid overfitting caused by ensembling strong learners.","Furthermore, we propose a memory consolidation scheme and a global rectification scheme to make this strategy more effective.","Experiments over six multi-modal benchmarks speak to the efficacy of the method.","We release the code at https://github.com/huacong/ReconBoost."],"url":"http://arxiv.org/abs/2405.09321v1","category":"cs.CV"}
{"created":"2024-05-15 13:20:26","title":"Paley-like quasi-random graphs arising from polynomials","abstract":"Paley graphs and Paley sum graphs are classical examples of quasi-random graphs. In this paper, we provide new constructions of families of quasi-random graphs that behave like Paley graphs but are neither Cayley graphs nor Cayley sum graphs. These graphs give a unified perspective of studying various graphs arising from polynomials over finite fields such as Paley graphs, Paley sum graphs, and graphs coming from Diophantine tuples and their generalizations. We also provide new lower bounds on the clique number and independence number of general quasi-random graphs. In particular, we give a sufficient condition for the clique number of quasi-random graphs of $n$ vertices to be at least $(1-o(1)\\log_{3.008}n$. Such a condition applies to many classical quasi-random graphs, including Paley graphs and Paley sum graphs, as well as some new Paley-like graphs we construct.","sentences":["Paley graphs and Paley sum graphs are classical examples of quasi-random graphs.","In this paper, we provide new constructions of families of quasi-random graphs that behave like Paley graphs but are neither Cayley graphs nor Cayley sum graphs.","These graphs give a unified perspective of studying various graphs arising from polynomials over finite fields such as Paley graphs, Paley sum graphs, and graphs coming from Diophantine tuples and their generalizations.","We also provide new lower bounds on the clique number and independence number of general quasi-random graphs.","In particular, we give a sufficient condition for the clique number of quasi-random graphs of $n$ vertices to be at least $(1-o(1)\\log_{3.008}n$. Such a condition applies to many classical quasi-random graphs, including Paley graphs and Paley sum graphs, as well as some new Paley-like graphs we construct."],"url":"http://arxiv.org/abs/2405.09319v1","category":"math.CO"}
{"created":"2024-05-15 13:18:08","title":"Controllability Test for Nonlinear Datatic Systems","abstract":"Controllability is a fundamental property of control systems, serving as the prerequisite for controller design. While controllability test is well established in modelic (i.e., model-driven) control systems, extending it to datatic (i.e., data-driven) control systems is still a challenging task due to the absence of system models. In this study, we propose a general controllability test method for nonlinear systems with datatic description, where the system behaviors are merely described by data. In this situation, the state transition information of a dynamic system is available only at a limited number of data points, leaving the behaviors beyond these points unknown. Different from traditional exact controllability, we introduce a new concept called $\\epsilon$-controllability, which extends the definition from point-to-point form to point-to-region form. Accordingly, our focus shifts to checking whether the system state can be steered to a closed state ball centered on the target state, rather than exactly at that target state. On its basis, we propose a tree search algorithm called maximum expansion of controllable subset (MECS) to identify controllable states in the dataset. Starting with a specific target state, our algorithm can iteratively propagate controllability from a known state ball to a new one. This iterative process gradually enlarges the $\\epsilon$-controllable subset by incorporating new controllable balls until all $\\epsilon$-controllable states are searched. Besides, a simplified version of MECS is proposed by solving a special shortest path problem, called Floyd expansion with radius fixed (FERF). FERF maintains a fixed radius of all controllable balls based on a mutual controllability assumption of neighboring states. The effectiveness of our method is validated in three datatic control systems whose dynamic behaviors are described by sampled data.","sentences":["Controllability is a fundamental property of control systems, serving as the prerequisite for controller design.","While controllability test is well established in modelic (i.e., model-driven) control systems, extending it to datatic (i.e., data-driven) control systems is still a challenging task due to the absence of system models.","In this study, we propose a general controllability test method for nonlinear systems with datatic description, where the system behaviors are merely described by data.","In this situation, the state transition information of a dynamic system is available only at a limited number of data points, leaving the behaviors beyond these points unknown.","Different from traditional exact controllability, we introduce a new concept called $\\epsilon$-controllability, which extends the definition from point-to-point form to point-to-region form.","Accordingly, our focus shifts to checking whether the system state can be steered to a closed state ball centered on the target state, rather than exactly at that target state.","On its basis, we propose a tree search algorithm called maximum expansion of controllable subset (MECS) to identify controllable states in the dataset.","Starting with a specific target state, our algorithm can iteratively propagate controllability from a known state ball to a new one.","This iterative process gradually enlarges the $\\epsilon$-controllable subset by incorporating new controllable balls until all $\\epsilon$-controllable states are searched.","Besides, a simplified version of MECS is proposed by solving a special shortest path problem, called Floyd expansion with radius fixed (FERF).","FERF maintains a fixed radius of all controllable balls based on a mutual controllability assumption of neighboring states.","The effectiveness of our method is validated in three datatic control systems whose dynamic behaviors are described by sampled data."],"url":"http://arxiv.org/abs/2405.09317v1","category":"eess.SY"}
{"created":"2024-05-15 13:16:11","title":"Positive operator-valued kernels and non-commutative probability","abstract":"We prove new factorization and dilation results for general positive operator-valued kernels, and we present their implications for associated Hilbert space-valued Gaussian processes, and their covariance structure. Further applications are to non-commutative probability theory, including a non-commutative Radon--Nikodym theorem for systems of completely positive maps.","sentences":["We prove new factorization and dilation results for general positive operator-valued kernels, and we present their implications for associated Hilbert space-valued Gaussian processes, and their covariance structure.","Further applications are to non-commutative probability theory, including a non-commutative Radon--Nikodym theorem for systems of completely positive maps."],"url":"http://arxiv.org/abs/2405.09315v1","category":"math.OA"}
{"created":"2024-05-15 13:14:05","title":"Themis: Automatic and Efficient Deep Learning System Testing with Strong Fault Detection Capability","abstract":"Deep Learning Systems (DLSs) have been widely applied in safety-critical tasks such as autopilot. However, when a perturbed input is fed into a DLS for inference, the DLS often has incorrect outputs (i.e., faults). DLS testing techniques (e.g., DeepXplore) detect such faults by generating perturbed inputs to explore data flows that induce faults. Since a DLS often has infinitely many data flows, existing techniques require developers to manually specify a set of activation values in a DLS's neurons for exploring fault-inducing data flows. Unfortunately, recent studies show that such manual effort is tedious and can detect only a tiny proportion of fault-inducing data flows.   In this paper, we present Themis, the first automatic DLS testing system, which attains strong fault detection capability by ensuring a full coverage of fault-inducing data flows at a high probability. Themis carries a new workflow for automatically and systematically revealing data flows whose internal neurons' outputs vary substantially when the inputs are slightly perturbed, as these data flows are likely fault-inducing. We evaluated Themis on ten different DLSs and found that on average the number of faults detected by Themis was 3.78X more than four notable DLS testing techniques. By retraining all evaluated DLSs with the detected faults, Themis also increased (regained) these DLSs' accuracies on average 14.7X higher than all baselines.","sentences":["Deep Learning Systems (DLSs) have been widely applied in safety-critical tasks such as autopilot.","However, when a perturbed input is fed into a DLS for inference, the DLS often has incorrect outputs (i.e., faults).","DLS testing techniques (e.g., DeepXplore) detect such faults by generating perturbed inputs to explore data flows that induce faults.","Since a DLS often has infinitely many data flows, existing techniques require developers to manually specify a set of activation values in a DLS's neurons for exploring fault-inducing data flows.","Unfortunately, recent studies show that such manual effort is tedious and can detect only a tiny proportion of fault-inducing data flows.   ","In this paper, we present Themis, the first automatic DLS testing system, which attains strong fault detection capability by ensuring a full coverage of fault-inducing data flows at a high probability.","Themis carries a new workflow for automatically and systematically revealing data flows whose internal neurons' outputs vary substantially when the inputs are slightly perturbed, as these data flows are likely fault-inducing.","We evaluated Themis on ten different DLSs and found that on average the number of faults detected by Themis was 3.78X more than four notable DLS testing techniques.","By retraining all evaluated DLSs with the detected faults, Themis also increased (regained) these DLSs' accuracies on average 14.7X higher than all baselines."],"url":"http://arxiv.org/abs/2405.09314v1","category":"cs.SE"}
{"created":"2024-05-15 13:11:29","title":"Vortex-comb spectroscopy","abstract":"We propose a new Fourier-transform spectroscopy technique based on the rotational Doppler effect. The technique offers an application for optical vortex frequency combs, where each frequency component carries a unique amount of orbital angular momentum (OAM). Here, we emulate a vortex comb using a tunable single frequency laser and a collection of spiral phase plates, generating up to eleven distinct OAM modes. Unlike in traditional Fourier-transform spectroscopy based on the Michelson interferometer (linear Doppler effect), the spectral resolution of vortex-comb spectroscopy is not limited by the mechanical scan distance of the instrument but only by the measurement time. Although the spectrometer requires just one free-running frequency comb, the down-conversion scheme resembles dual-comb spectroscopy, leading to fast mode-resolved measurements.","sentences":["We propose a new Fourier-transform spectroscopy technique based on the rotational Doppler effect.","The technique offers an application for optical vortex frequency combs, where each frequency component carries a unique amount of orbital angular momentum (OAM).","Here, we emulate a vortex comb using a tunable single frequency laser and a collection of spiral phase plates, generating up to eleven distinct OAM modes.","Unlike in traditional Fourier-transform spectroscopy based on the Michelson interferometer (linear Doppler effect), the spectral resolution of vortex-comb spectroscopy is not limited by the mechanical scan distance of the instrument but only by the measurement time.","Although the spectrometer requires just one free-running frequency comb, the down-conversion scheme resembles dual-comb spectroscopy, leading to fast mode-resolved measurements."],"url":"http://arxiv.org/abs/2405.09313v1","category":"physics.optics"}
{"created":"2024-05-15 13:11:13","title":"Sums of rational cubes and the $3$-Selmer group","abstract":"Recently, Alp\\\"oge-Bhargava-Shnidman determined the average size of the $2$-Selmer group in the cubic twist family of any elliptic curve over $\\mathbb{Q}$ with $j$-invariant $0$. We obtain the distribution of the $3$-Selmer groups in the same family. As a consequence, we improve their upper bound on the density of integers expressible as a sum of two rational cubes. Assuming a $3$-converse theorem, we also improve their lower bound on this density.   The $\\sqrt{-3}$-Selmer group in this cubic twist family is well-known to be large, which poses significant challenges to the methods previously developed by the second author. We overcome this problem by strengthening the analytic core of these methods. Specifically, we prove a \"trilinear large sieve\" for an appropriate generalization of the classical R\\'edei symbol, then use this to control the restriction of the Cassels-Tate pairing to the $\\sqrt{-3}$-Selmer groups in these twist families.","sentences":["Recently, Alp\\\"oge-Bhargava-Shnidman determined the average size of the $2$-Selmer group in the cubic twist family of any elliptic curve over $\\mathbb{Q}$ with $j$-invariant $0$. We obtain the distribution of the $3$-Selmer groups in the same family.","As a consequence, we improve their upper bound on the density of integers expressible as a sum of two rational cubes.","Assuming a $3$-converse theorem, we also improve their lower bound on this density.   ","The $\\sqrt{-3}$-Selmer group in this cubic twist family is well-known to be large, which poses significant challenges to the methods previously developed by the second author.","We overcome this problem by strengthening the analytic core of these methods.","Specifically, we prove a \"trilinear large sieve\" for an appropriate generalization of the classical R\\'edei symbol, then use this to control the restriction of the Cassels-Tate pairing to the $\\sqrt{-3}$-Selmer groups in these twist families."],"url":"http://arxiv.org/abs/2405.09311v1","category":"math.NT"}
{"created":"2024-05-15 13:08:27","title":"GrainGrasp: Dexterous Grasp Generation with Fine-grained Contact Guidance","abstract":"One goal of dexterous robotic grasping is to allow robots to handle objects with the same level of flexibility and adaptability as humans. However, it remains a challenging task to generate an optimal grasping strategy for dexterous hands, especially when it comes to delicate manipulation and accurate adjustment the desired grasping poses for objects of varying shapes and sizes. In this paper, we propose a novel dexterous grasp generation scheme called \\textbf{\\textit{GrainGrasp}} that provides fine-grained contact guidance for each fingertip. In particular, we employ a generative model to predict separate contact maps for each fingertip on the object point cloud, effectively capturing the specifics of finger-object interactions. In addition, we develop a new dexterous grasping optimization algorithm that solely relies on the point cloud as input, eliminating the necessity for complete mesh information of the object. By leveraging the contact maps of different fingertips, the proposed optimization algorithm can generate precise and determinable strategies for human-like object grasping. Experimental results confirm the efficiency of the proposed scheme. Our code is available at https://github.com/wmtlab/GrainGrasp","sentences":["One goal of dexterous robotic grasping is to allow robots to handle objects with the same level of flexibility and adaptability as humans.","However, it remains a challenging task to generate an optimal grasping strategy for dexterous hands, especially when it comes to delicate manipulation and accurate adjustment the desired grasping poses for objects of varying shapes and sizes.","In this paper, we propose a novel dexterous grasp generation scheme called \\textbf{\\textit{GrainGrasp}} that provides fine-grained contact guidance for each fingertip.","In particular, we employ a generative model to predict separate contact maps for each fingertip on the object point cloud, effectively capturing the specifics of finger-object interactions.","In addition, we develop a new dexterous grasping optimization algorithm that solely relies on the point cloud as input, eliminating the necessity for complete mesh information of the object.","By leveraging the contact maps of different fingertips, the proposed optimization algorithm can generate precise and determinable strategies for human-like object grasping.","Experimental results confirm the efficiency of the proposed scheme.","Our code is available at https://github.com/wmtlab/GrainGrasp"],"url":"http://arxiv.org/abs/2405.09310v1","category":"cs.RO"}
{"created":"2024-05-15 13:07:35","title":"Identification via Binary Uniform Permutation Channel","abstract":"We study message identification over the binary uniform permutation channels. For DMCs, the number of identifiable messages grows doubly exponentially. Identification capacity, the maximum second-order exponent, is known to be the same as the Shannon capacity of a DMC. We consider a binary uniform permutation channel where the transmitted vector is permuted by a permutation chosen uniformly at random. Permutation channels support reliable communication of only polynomially many messages. While this implies a zero second-order identification rate, we prove a soft converse result showing that even non-zero first-order identification rates are not achievable with a power-law decay of error probability for identification over binary uniform permutation channels. To prove the converse, we use a sequence of steps to construct a new identification code with a simpler structure and then use a lower bound on the normalized maximum pairwise intersection of a set system on {0, . . . , n}. We provide generalizations for arbitrary alphabet size.","sentences":["We study message identification over the binary uniform permutation channels.","For DMCs, the number of identifiable messages grows doubly exponentially.","Identification capacity, the maximum second-order exponent, is known to be the same as the Shannon capacity of a DMC.","We consider a binary uniform permutation channel where the transmitted vector is permuted by a permutation chosen uniformly at random.","Permutation channels support reliable communication of only polynomially many messages.","While this implies a zero second-order identification rate, we prove a soft converse result showing that even non-zero first-order identification rates are not achievable with a power-law decay of error probability for identification over binary uniform permutation channels.","To prove the converse, we use a sequence of steps to construct a new identification code with a simpler structure and then use a lower bound on the normalized maximum pairwise intersection of a set system on {0, . .",". , n}.","We provide generalizations for arbitrary alphabet size."],"url":"http://arxiv.org/abs/2405.09309v1","category":"cs.IT"}
{"created":"2024-05-15 13:03:41","title":"TimeX++: Learning Time-Series Explanations with Information Bottleneck","abstract":"Explaining deep learning models operating on time series data is crucial in various applications of interest which require interpretable and transparent insights from time series signals. In this work, we investigate this problem from an information theoretic perspective and show that most existing measures of explainability may suffer from trivial solutions and distributional shift issues. To address these issues, we introduce a simple yet practical objective function for time series explainable learning. The design of the objective function builds upon the principle of information bottleneck (IB), and modifies the IB objective function to avoid trivial solutions and distributional shift issues. We further present TimeX++, a novel explanation framework that leverages a parametric network to produce explanation-embedded instances that are both in-distributed and label-preserving. We evaluate TimeX++ on both synthetic and real-world datasets comparing its performance against leading baselines, and validate its practical efficacy through case studies in a real-world environmental application. Quantitative and qualitative evaluations show that TimeX++ outperforms baselines across all datasets, demonstrating a substantial improvement in explanation quality for time series data. The source code is available at \\url{https://github.com/zichuan-liu/TimeXplusplus}.","sentences":["Explaining deep learning models operating on time series data is crucial in various applications of interest which require interpretable and transparent insights from time series signals.","In this work, we investigate this problem from an information theoretic perspective and show that most existing measures of explainability may suffer from trivial solutions and distributional shift issues.","To address these issues, we introduce a simple yet practical objective function for time series explainable learning.","The design of the objective function builds upon the principle of information bottleneck (IB), and modifies the IB objective function to avoid trivial solutions and distributional shift issues.","We further present TimeX++, a novel explanation framework that leverages a parametric network to produce explanation-embedded instances that are both in-distributed and label-preserving.","We evaluate TimeX++ on both synthetic and real-world datasets comparing its performance against leading baselines, and validate its practical efficacy through case studies in a real-world environmental application.","Quantitative and qualitative evaluations show that TimeX++ outperforms baselines across all datasets, demonstrating a substantial improvement in explanation quality for time series data.","The source code is available at \\url{https://github.com/zichuan-liu/TimeXplusplus}."],"url":"http://arxiv.org/abs/2405.09308v1","category":"cs.LG"}
{"created":"2024-05-15 13:02:35","title":"Quantum entanglement and non-Gaussianity in the primordial Universe","abstract":"We propose a new method to investigate signatures of a quantum gravity phase in the primordial state of cosmological perturbations. We formulate and study a quantum model of a perturbed Friedmann-Lemaitre-Robertson-Walker universe beyond the conventional Born-Oppenheimer factorization, that is, without restricting the wave function of the universe to the product of the background and perturbation wave functions. We show that the quantum dynamics generically does not preserve the product form of the universe's wave function, that spontaneously evolves into a more general entangled state. Upon expanding this state in a suitable basis of background wave functions and setting Gaussian initial conditions for the perturbations, we numerically find that each of these wave functions becomes associated to a non-Gaussian state of an inhomogeneous perturbation.","sentences":["We propose a new method to investigate signatures of a quantum gravity phase in the primordial state of cosmological perturbations.","We formulate and study a quantum model of a perturbed Friedmann-Lemaitre-Robertson-Walker universe beyond the conventional Born-Oppenheimer factorization, that is, without restricting the wave function of the universe to the product of the background and perturbation wave functions.","We show that the quantum dynamics generically does not preserve the product form of the universe's wave function, that spontaneously evolves into a more general entangled state.","Upon expanding this state in a suitable basis of background wave functions and setting Gaussian initial conditions for the perturbations, we numerically find that each of these wave functions becomes associated to a non-Gaussian state of an inhomogeneous perturbation."],"url":"http://arxiv.org/abs/2405.09307v1","category":"gr-qc"}
{"created":"2024-05-15 12:47:06","title":"Vacuum energy in the effective field theory of general relativity with a scalar field","abstract":"A consistency condition of general relativity as an effective field theory in Minkowskian background uniquely fixes the value of the cosmological constant. In two-loop calculations, including the interaction of gravitons with matter fields, it has been shown that this value of the cosmological constant leads to vanishing vacuum energy, under the assumption that the energy-momentum tensor of the gravitational field is given by the pseudotensor of Landau-Lifshitz's classic textbook. Here, we demonstrate that this result also holds when the self-interaction of a scalar field is taken into account. That is, our two-loop-order calculation suggests that in an effective field theory of metric and scalar fields, one arrives at a consistent theory with massless gravitons if the cosmological constant is fixed from the condition of vanishing vacuum energy. Vice versa, imposing the consistency condition in Minkowskian background leads to a vanishing vacuum energy.","sentences":["A consistency condition of general relativity as an effective field theory in Minkowskian background uniquely fixes the value of the cosmological constant.","In two-loop calculations, including the interaction of gravitons with matter fields, it has been shown that this value of the cosmological constant leads to vanishing vacuum energy, under the assumption that the energy-momentum tensor of the gravitational field is given by the pseudotensor of Landau-Lifshitz's classic textbook.","Here, we demonstrate that this result also holds when the self-interaction of a scalar field is taken into account.","That is, our two-loop-order calculation suggests that in an effective field theory of metric and scalar fields, one arrives at a consistent theory with massless gravitons if the cosmological constant is fixed from the condition of vanishing vacuum energy.","Vice versa, imposing the consistency condition in Minkowskian background leads to a vanishing vacuum energy."],"url":"http://arxiv.org/abs/2405.09301v1","category":"hep-th"}
{"created":"2024-05-15 12:44:54","title":"Comparing the Efficacy of GPT-4 and Chat-GPT in Mental Health Care: A Blind Assessment of Large Language Models for Psychological Support","abstract":"Background: Rapid advancements in natural language processing have led to the development of large language models with the potential to revolutionize mental health care. These models have shown promise in assisting clinicians and providing support to individuals experiencing various psychological challenges.   Objective: This study aims to compare the performance of two large language models, GPT-4 and Chat-GPT, in responding to a set of 18 psychological prompts, to assess their potential applicability in mental health care settings.   Methods: A blind methodology was employed, with a clinical psychologist evaluating the models' responses without knowledge of their origins. The prompts encompassed a diverse range of mental health topics, including depression, anxiety, and trauma, to ensure a comprehensive assessment.   Results: The results demonstrated a significant difference in performance between the two models (p > 0.05). GPT-4 achieved an average rating of 8.29 out of 10, while Chat-GPT received an average rating of 6.52. The clinical psychologist's evaluation suggested that GPT-4 was more effective at generating clinically relevant and empathetic responses, thereby providing better support and guidance to potential users.   Conclusions: This study contributes to the growing body of literature on the applicability of large language models in mental health care settings. The findings underscore the importance of continued research and development in the field to optimize these models for clinical use. Further investigation is necessary to understand the specific factors underlying the performance differences between the two models and to explore their generalizability across various populations and mental health conditions.","sentences":["Background: Rapid advancements in natural language processing have led to the development of large language models with the potential to revolutionize mental health care.","These models have shown promise in assisting clinicians and providing support to individuals experiencing various psychological challenges.   ","Objective: This study aims to compare the performance of two large language models, GPT-4 and Chat-GPT, in responding to a set of 18 psychological prompts, to assess their potential applicability in mental health care settings.   ","Methods: A blind methodology was employed, with a clinical psychologist evaluating the models' responses without knowledge of their origins.","The prompts encompassed a diverse range of mental health topics, including depression, anxiety, and trauma, to ensure a comprehensive assessment.   ","Results:","The results demonstrated a significant difference in performance between the two models (p > 0.05).","GPT-4 achieved an average rating of 8.29 out of 10, while Chat-GPT received an average rating of 6.52.","The clinical psychologist's evaluation suggested that GPT-4 was more effective at generating clinically relevant and empathetic responses, thereby providing better support and guidance to potential users.   ","Conclusions: This study contributes to the growing body of literature on the applicability of large language models in mental health care settings.","The findings underscore the importance of continued research and development in the field to optimize these models for clinical use.","Further investigation is necessary to understand the specific factors underlying the performance differences between the two models and to explore their generalizability across various populations and mental health conditions."],"url":"http://arxiv.org/abs/2405.09300v1","category":"cs.CL"}
{"created":"2024-05-15 12:37:03","title":"Tight Bounds for Online Convex Optimization with Adversarial Constraints","abstract":"A well-studied generalization of the standard online convex optimization (OCO) is constrained online convex optimization (COCO). In COCO, on every round, a convex cost function and a convex constraint function are revealed to the learner after the action for that round is chosen. The objective is to design an online policy that simultaneously achieves a small regret while ensuring small cumulative constraint violation (CCV) against an adaptive adversary. A long-standing open question in COCO is whether an online policy can simultaneously achieve $O(\\sqrt{T})$ regret and $O(\\sqrt{T})$ CCV without any restrictive assumptions. For the first time, we answer this in the affirmative and show that an online policy can simultaneously achieve $O(\\sqrt{T})$ regret and $\\tilde{O}(\\sqrt{T})$ CCV. We establish this result by effectively combining the adaptive regret bound of the AdaGrad algorithm with Lyapunov optimization - a classic tool from control theory. Surprisingly, the analysis is short and elegant.","sentences":["A well-studied generalization of the standard online convex optimization (OCO) is constrained online convex optimization (COCO).","In COCO, on every round, a convex cost function and a convex constraint function are revealed to the learner after the action for that round is chosen.","The objective is to design an online policy that simultaneously achieves a small regret while ensuring small cumulative constraint violation (CCV) against an adaptive adversary.","A long-standing open question in COCO is whether an online policy can simultaneously achieve $O(\\sqrt{T})$ regret and $O(\\sqrt{T})$ CCV without any restrictive assumptions.","For the first time, we answer this in the affirmative and show that an online policy can simultaneously achieve $O(\\sqrt{T})$ regret and $\\tilde{O}(\\sqrt{T})$ CCV.","We establish this result by effectively combining the adaptive regret bound of the AdaGrad algorithm with Lyapunov optimization - a classic tool from control theory.","Surprisingly, the analysis is short and elegant."],"url":"http://arxiv.org/abs/2405.09296v1","category":"cs.LG"}
{"created":"2024-05-15 12:36:10","title":"Strong Forms of Weakly e-continuous Functions","abstract":"The main purpose of this study is to introduce and study two new classes of continuity called eR-continuous functions and weakly eR-continuous functions via e-regular sets. Both of the forms of continuous functions we have described are stronger than the weakly e-continuity. Furthermore, we obtain various characterizations of weakly eR-continuous functions. In addition, we examine not only the relations of these functions with some other forms of existing continuous functions, but also some of their fundamental properties.","sentences":["The main purpose of this study is to introduce and study two new classes of continuity called eR-continuous functions and weakly eR-continuous functions via e-regular sets.","Both of the forms of continuous functions we have described are stronger than the weakly e-continuity.","Furthermore, we obtain various characterizations of weakly eR-continuous functions.","In addition, we examine not only the relations of these functions with some other forms of existing continuous functions, but also some of their fundamental properties."],"url":"http://arxiv.org/abs/2405.09294v1","category":"math.GN"}
{"created":"2024-05-15 12:34:40","title":"Do language models capture implied discourse meanings? An investigation with exhaustivity implicatures of Korean morphology","abstract":"Markedness in natural language is often associated with non-literal meanings in discourse. Differential Object Marking (DOM) in Korean is one instance of this phenomenon, where post-positional markers are selected based on both the semantic features of the noun phrases and the discourse features that are orthogonal to the semantic features. Previous work has shown that distributional models of language recover certain semantic features of words -- do these models capture implied discourse-level meanings as well? We evaluate whether a set of large language models are capable of associating discourse meanings with different object markings in Korean. Results suggest that discourse meanings of a grammatical marker can be more challenging to encode than that of a discourse marker.","sentences":["Markedness in natural language is often associated with non-literal meanings in discourse.","Differential Object Marking (DOM) in Korean is one instance of this phenomenon, where post-positional markers are selected based on both the semantic features of the noun phrases and the discourse features that are orthogonal to the semantic features.","Previous work has shown that distributional models of language recover certain semantic features of words -- do these models capture implied discourse-level meanings as well?","We evaluate whether a set of large language models are capable of associating discourse meanings with different object markings in Korean.","Results suggest that discourse meanings of a grammatical marker can be more challenging to encode than that of a discourse marker."],"url":"http://arxiv.org/abs/2405.09293v1","category":"cs.CL"}
{"created":"2024-05-15 12:30:19","title":"Attribute reduction algorithm of rough sets based on spatial optimization","abstract":"Rough set is one of the important methods for rule acquisition and attribute reduction. The current goal of rough set attribute reduction focuses more on minimizing the number of reduced attributes, but ignores the spatial similarity between reduced and decision attributes, which may lead to problems such as increased number of rules and limited generality. In this paper, a rough set attribute reduction algorithm based on spatial optimization is proposed. By introducing the concept of spatial similarity, to find the reduction with the highest spatial similarity, so that the spatial similarity between reduction and decision attributes is higher, and more concise and widespread rules are obtained. In addition, a comparative experiment with the traditional rough set attribute reduction algorithms is designed to prove the effectiveness of the rough set attribute reduction algorithm based on spatial optimization, which has made significant improvements on many datasets.","sentences":["Rough set is one of the important methods for rule acquisition and attribute reduction.","The current goal of rough set attribute reduction focuses more on minimizing the number of reduced attributes, but ignores the spatial similarity between reduced and decision attributes, which may lead to problems such as increased number of rules and limited generality.","In this paper, a rough set attribute reduction algorithm based on spatial optimization is proposed.","By introducing the concept of spatial similarity, to find the reduction with the highest spatial similarity, so that the spatial similarity between reduction and decision attributes is higher, and more concise and widespread rules are obtained.","In addition, a comparative experiment with the traditional rough set attribute reduction algorithms is designed to prove the effectiveness of the rough set attribute reduction algorithm based on spatial optimization, which has made significant improvements on many datasets."],"url":"http://arxiv.org/abs/2405.09292v1","category":"cs.AI"}
{"created":"2024-05-15 12:29:35","title":"Sensitivity Decouple Learning for Image Compression Artifacts Reduction","abstract":"With the benefit of deep learning techniques, recent researches have made significant progress in image compression artifacts reduction. Despite their improved performances, prevailing methods only focus on learning a mapping from the compressed image to the original one but ignore the intrinsic attributes of the given compressed images, which greatly harms the performance of downstream parsing tasks. Different from these methods, we propose to decouple the intrinsic attributes into two complementary features for artifacts reduction,ie, the compression-insensitive features to regularize the high-level semantic representations during training and the compression-sensitive features to be aware of the compression degree. To achieve this, we first employ adversarial training to regularize the compressed and original encoded features for retaining high-level semantics, and we then develop the compression quality-aware feature encoder for compression-sensitive features. Based on these dual complementary features, we propose a Dual Awareness Guidance Network (DAGN) to utilize these awareness features as transformation guidance during the decoding phase. In our proposed DAGN, we develop a cross-feature fusion module to maintain the consistency of compression-insensitive features by fusing compression-insensitive features into the artifacts reduction baseline. Our method achieves an average 2.06 dB PSNR gains on BSD500, outperforming state-of-the-art methods, and only requires 29.7 ms to process one image on BSD500. Besides, the experimental results on LIVE1 and LIU4K also demonstrate the efficiency, effectiveness, and superiority of the proposed method in terms of quantitative metrics, visual quality, and downstream machine vision tasks.","sentences":["With the benefit of deep learning techniques, recent researches have made significant progress in image compression artifacts reduction.","Despite their improved performances, prevailing methods only focus on learning a mapping from the compressed image to the original one but ignore the intrinsic attributes of the given compressed images, which greatly harms the performance of downstream parsing tasks.","Different from these methods, we propose to decouple the intrinsic attributes into two complementary features for artifacts reduction,ie, the compression-insensitive features to regularize the high-level semantic representations during training and the compression-sensitive features to be aware of the compression degree.","To achieve this, we first employ adversarial training to regularize the compressed and original encoded features for retaining high-level semantics, and we then develop the compression quality-aware feature encoder for compression-sensitive features.","Based on these dual complementary features, we propose a Dual Awareness Guidance Network (DAGN) to utilize these awareness features as transformation guidance during the decoding phase.","In our proposed DAGN, we develop a cross-feature fusion module to maintain the consistency of compression-insensitive features by fusing compression-insensitive features into the artifacts reduction baseline.","Our method achieves an average 2.06 dB PSNR gains on BSD500, outperforming state-of-the-art methods, and only requires 29.7 ms to process one image on BSD500.","Besides, the experimental results on LIVE1 and LIU4K also demonstrate the efficiency, effectiveness, and superiority of the proposed method in terms of quantitative metrics, visual quality, and downstream machine vision tasks."],"url":"http://arxiv.org/abs/2405.09291v1","category":"cs.CV"}
{"created":"2024-05-15 12:22:21","title":"Gravitational wavefunctions in JT supergravity","abstract":"We determine explicit expressions for the continuous two-sided gravitational wavefunctions in supersymmetric versions of JT gravity, focusing mainly on $\\mathcal{N}=2$ JT supergravity. Our approach is based on representation theory of the associated supergroup, for which we determine the relevant mixed parabolic matrix elements that implement asymptotic AdS boundary conditions at the quantum level. We match our expressions with those found by solving the energy-eigenvalue equation of arXiv:2207.00408. We discuss gravitational applications by computing several amplitudes of interest, and address how our framework can be generalized further.","sentences":["We determine explicit expressions for the continuous two-sided gravitational wavefunctions in supersymmetric versions of JT gravity, focusing mainly on $\\mathcal{N}=2$ JT supergravity.","Our approach is based on representation theory of the associated supergroup, for which we determine the relevant mixed parabolic matrix elements that implement asymptotic AdS boundary conditions at the quantum level.","We match our expressions with those found by solving the energy-eigenvalue equation of arXiv:2207.00408.","We discuss gravitational applications by computing several amplitudes of interest, and address how our framework can be generalized further."],"url":"http://arxiv.org/abs/2405.09289v1","category":"hep-th"}
{"created":"2024-05-15 12:17:35","title":"DeCoDEx: Confounder Detector Guidance for Improved Diffusion-based Counterfactual Explanations","abstract":"Deep learning classifiers are prone to latching onto dominant confounders present in a dataset rather than on the causal markers associated with the target class, leading to poor generalization and biased predictions. Although explainability via counterfactual image generation has been successful at exposing the problem, bias mitigation strategies that permit accurate explainability in the presence of dominant and diverse artifacts remain unsolved. In this work, we propose the DeCoDEx framework and show how an external, pre-trained binary artifact detector can be leveraged during inference to guide a diffusion-based counterfactual image generator towards accurate explainability. Experiments on the CheXpert dataset, using both synthetic artifacts and real visual artifacts (support devices), show that the proposed method successfully synthesizes the counterfactual images that change the causal pathology markers associated with Pleural Effusion while preserving or ignoring the visual artifacts. Augmentation of ERM and Group-DRO classifiers with the DeCoDEx generated images substantially improves the results across underrepresented groups that are out of distribution for each class. The code is made publicly available at https://github.com/NimaFathi/DeCoDEx.","sentences":["Deep learning classifiers are prone to latching onto dominant confounders present in a dataset rather than on the causal markers associated with the target class, leading to poor generalization and biased predictions.","Although explainability via counterfactual image generation has been successful at exposing the problem, bias mitigation strategies that permit accurate explainability in the presence of dominant and diverse artifacts remain unsolved.","In this work, we propose the DeCoDEx framework and show how an external, pre-trained binary artifact detector can be leveraged during inference to guide a diffusion-based counterfactual image generator towards accurate explainability.","Experiments on the CheXpert dataset, using both synthetic artifacts and real visual artifacts (support devices), show that the proposed method successfully synthesizes the counterfactual images that change the causal pathology markers associated with Pleural Effusion while preserving or ignoring the visual artifacts.","Augmentation of ERM and Group-DRO classifiers with the DeCoDEx generated images substantially improves the results across underrepresented groups that are out of distribution for each class.","The code is made publicly available at https://github.com/NimaFathi/DeCoDEx."],"url":"http://arxiv.org/abs/2405.09288v1","category":"cs.CV"}
{"created":"2024-05-15 12:11:28","title":"MVBIND: Self-Supervised Music Recommendation For Videos Via Embedding Space Binding","abstract":"Recent years have witnessed the rapid development of short videos, which usually contain both visual and audio modalities. Background music is important to the short videos, which can significantly influence the emotions of the viewers. However, at present, the background music of short videos is generally chosen by the video producer, and there is a lack of automatic music recommendation methods for short videos. This paper introduces MVBind, an innovative Music-Video embedding space Binding model for cross-modal retrieval. MVBind operates as a self-supervised approach, acquiring inherent knowledge of intermodal relationships directly from data, without the need of manual annotations. Additionally, to compensate the lack of a corresponding musical-visual pair dataset for short videos, we construct a dataset, SVM-10K(Short Video with Music-10K), which mainly consists of meticulously selected short videos. On this dataset, MVBind manifests significantly improved performance compared to other baseline methods. The constructed dataset and code will be released to facilitate future research.","sentences":["Recent years have witnessed the rapid development of short videos, which usually contain both visual and audio modalities.","Background music is important to the short videos, which can significantly influence the emotions of the viewers.","However, at present, the background music of short videos is generally chosen by the video producer, and there is a lack of automatic music recommendation methods for short videos.","This paper introduces MVBind, an innovative Music-Video embedding space Binding model for cross-modal retrieval.","MVBind operates as a self-supervised approach, acquiring inherent knowledge of intermodal relationships directly from data, without the need of manual annotations.","Additionally, to compensate the lack of a corresponding musical-visual pair dataset for short videos, we construct a dataset, SVM-10K(Short Video with Music-10K), which mainly consists of meticulously selected short videos.","On this dataset, MVBind manifests significantly improved performance compared to other baseline methods.","The constructed dataset and code will be released to facilitate future research."],"url":"http://arxiv.org/abs/2405.09286v1","category":"cs.MM"}
{"created":"2024-05-15 12:08:02","title":"Volatile atmospheres of lava worlds","abstract":"A magma ocean (MO) is thought to be a ubiquitous stage in the early evolution of rocky planets and exoplanets. During the lifetime of the MO, exchanges between the interior and exterior envelopes of the planet are very efficient. In particular, volatile elements that initially are contained in the solid part of the planet can be released and form a secondary outgassed atmosphere. We determine trends in the H-C-N-O-S composition and thickness of these secondary atmospheres for varying planetary sizes and MO extents, and the oxygen fugacity of MOs, which provides the main control for the atmospheric chemistry. We used a model with coupled chemical gas-gas and silicate melt-gas equilibria and mass conservation to predict the composition of an atmosphere at equilibrium with the MO depending on the planet size and the extent and redox state of the MO. We used a self-consistent mass-radius model for the rocky core to inform the structure of the planet, which we combined with an atmosphere model to predict the transit radius of lava worlds. We find that MOs (especially the shallow ones) on small planets are generally more reduced, and are thus dominated by H2-rich atmospheres (whose outgassing is strengthened at low planetary mass), while larger planets and deeper MOs vary from CO to CO2-N2-SO2 atmospheres, with increasing fO2 . In the former case, the low molecular mass of the atmosphere combined with the low gravity of the planets yields a large vertical extension of the atmosphere, while in the latter cases, secondary outgassed atmospheres on super-Earths are likely significantly shrunk. Both N and C are largely outgassed regardless of the conditions, while the S and H outgassing is strongly dependent on the fO2 , as well as on the planetary mass and MO extent for the latter.","sentences":["A magma ocean (MO) is thought to be a ubiquitous stage in the early evolution of rocky planets and exoplanets.","During the lifetime of the MO, exchanges between the interior and exterior envelopes of the planet are very efficient.","In particular, volatile elements that initially are contained in the solid part of the planet can be released and form a secondary outgassed atmosphere.","We determine trends in the H-C-N-O-S composition and thickness of these secondary atmospheres for varying planetary sizes and MO extents, and the oxygen fugacity of MOs, which provides the main control for the atmospheric chemistry.","We used a model with coupled chemical gas-gas and silicate melt-gas equilibria and mass conservation to predict the composition of an atmosphere at equilibrium with the MO depending on the planet size and the extent and redox state of the MO.","We used a self-consistent mass-radius model for the rocky core to inform the structure of the planet, which we combined with an atmosphere model to predict the transit radius of lava worlds.","We find that MOs (especially the shallow ones) on small planets are generally more reduced, and are thus dominated by H2-rich atmospheres (whose outgassing is strengthened at low planetary mass), while larger planets and deeper MOs vary from CO to CO2-N2-SO2 atmospheres, with increasing fO2 .","In the former case, the low molecular mass of the atmosphere combined with the low gravity of the planets yields a large vertical extension of the atmosphere, while in the latter cases, secondary outgassed atmospheres on super-Earths are likely significantly shrunk.","Both N and C are largely outgassed regardless of the conditions, while the S and H outgassing is strongly dependent on the fO2 , as well as on the planetary mass and MO extent for the latter."],"url":"http://arxiv.org/abs/2405.09284v1","category":"astro-ph.EP"}
{"created":"2024-05-15 12:05:30","title":"Three-Dimensional Path Planning: Navigating through Rough Mereology","abstract":"In this paper, we present an innovative technique for the path planning of flying robots in a 3D environment in Rough Mereology terms. The main goal was to construct the algorithm that would generate the mereological potential fields in 3-dimensional space. To avoid falling into the local minimum, we assist with a weighted Euclidean distance. Moreover, a searching path from the start point to the target, with respect to avoiding the obstacles was applied. The environment was created by connecting two cameras working in real-time. To determine the gate and elements of the world inside the map was responsible the Python Library OpenCV [1] which recognized shapes and colors. The main purpose of this paper is to apply the given results to drones.","sentences":["In this paper, we present an innovative technique for the path planning of flying robots in a 3D environment in Rough Mereology terms.","The main goal was to construct the algorithm that would generate the mereological potential fields in 3-dimensional space.","To avoid falling into the local minimum, we assist with a weighted Euclidean distance.","Moreover, a searching path from the start point to the target, with respect to avoiding the obstacles was applied.","The environment was created by connecting two cameras working in real-time.","To determine the gate and elements of the world inside the map was responsible the Python Library OpenCV [1] which recognized shapes and colors.","The main purpose of this paper is to apply the given results to drones."],"url":"http://arxiv.org/abs/2405.09282v1","category":"cs.RO"}
{"created":"2024-05-15 11:55:14","title":"Sign of the Times: Evaluating the use of Large Language Models for Idiomaticity Detection","abstract":"Despite the recent ubiquity of large language models and their high zero-shot prompted performance across a wide range of tasks, it is still not known how well they perform on tasks which require processing of potentially idiomatic language. In particular, how well do such models perform in comparison to encoder-only models fine-tuned specifically for idiomaticity tasks? In this work, we attempt to answer this question by looking at the performance of a range of LLMs (both local and software-as-a-service models) on three idiomaticity datasets: SemEval 2022 Task 2a, FLUTE, and MAGPIE. Overall, we find that whilst these models do give competitive performance, they do not match the results of fine-tuned task-specific models, even at the largest scales (e.g. for GPT-4). Nevertheless, we do see consistent performance improvements across model scale. Additionally, we investigate prompting approaches to improve performance, and discuss the practicalities of using LLMs for these tasks.","sentences":["Despite the recent ubiquity of large language models and their high zero-shot prompted performance across a wide range of tasks, it is still not known how well they perform on tasks which require processing of potentially idiomatic language.","In particular, how well do such models perform in comparison to encoder-only models fine-tuned specifically for idiomaticity tasks?","In this work, we attempt to answer this question by looking at the performance of a range of LLMs (both local and software-as-a-service models) on three idiomaticity datasets:","SemEval 2022 Task 2a, FLUTE, and MAGPIE.","Overall, we find that whilst these models do give competitive performance, they do not match the results of fine-tuned task-specific models, even at the largest scales (e.g. for GPT-4).","Nevertheless, we do see consistent performance improvements across model scale.","Additionally, we investigate prompting approaches to improve performance, and discuss the practicalities of using LLMs for these tasks."],"url":"http://arxiv.org/abs/2405.09279v1","category":"cs.CL"}
{"created":"2024-05-15 11:46:52","title":"Generalized cluster states from Hopf algebras: non-invertible symmetry and Hopf tensor network representation","abstract":"Cluster states are crucial resources for measurement-based quantum computation (MBQC). It exhibits symmetry-protected topological (SPT) order, thus also playing a crucial role in studying topological phases. We present the construction of cluster states based on Hopf algebras. By generalizing the finite group valued qudit to a Hopf algebra valued qudit and introducing the generalized Pauli-X operator based on the regular action of the Hopf algebra, as well as the generalized Pauli-Z operator based on the irreducible representation action on the Hopf algebra, we develop a comprehensive theory of Hopf qudits. We demonstrate that non-invertible symmetry naturally emerges for Hopf qudits. Subsequently, for a bipartite graph termed the cluster graph, we assign the identity state and trivial representation state to even and odd vertices, respectively. Introducing the edge entangler as controlled regular action, we provide a general construction of Hopf cluster states. To ensure the commutativity of the edge entangler, we propose a method to construct a cluster lattice for any triangulable manifold. We use the 1d cluster state as an example to illustrate our construction. As this serves as a promising candidate for SPT phases, we construct the gapped Hamiltonian for this scenario and delve into a detailed discussion of its non-invertible symmetries. We also show that the 1d cluster state model is equivalent to the quasi-1d Hopf quantum double model. We also introduce the Hopf tensor network representation of Hopf cluster states by integrating the tensor representation of structure constants with the string diagrams of the Hopf algebra.","sentences":["Cluster states are crucial resources for measurement-based quantum computation (MBQC).","It exhibits symmetry-protected topological (SPT) order, thus also playing a crucial role in studying topological phases.","We present the construction of cluster states based on Hopf algebras.","By generalizing the finite group valued qudit to a Hopf algebra valued qudit and introducing the generalized Pauli-X operator based on the regular action of the Hopf algebra, as well as the generalized Pauli-Z operator based on the irreducible representation action on the Hopf algebra, we develop a comprehensive theory of Hopf qudits.","We demonstrate that non-invertible symmetry naturally emerges for Hopf qudits.","Subsequently, for a bipartite graph termed the cluster graph, we assign the identity state and trivial representation state to even and odd vertices, respectively.","Introducing the edge entangler as controlled regular action, we provide a general construction of Hopf cluster states.","To ensure the commutativity of the edge entangler, we propose a method to construct a cluster lattice for any triangulable manifold.","We use the 1d cluster state as an example to illustrate our construction.","As this serves as a promising candidate for SPT phases, we construct the gapped Hamiltonian for this scenario and delve into a detailed discussion of its non-invertible symmetries.","We also show that the 1d cluster state model is equivalent to the quasi-1d","Hopf quantum double model.","We also introduce the Hopf tensor network representation of Hopf cluster states by integrating the tensor representation of structure constants with the string diagrams of the Hopf algebra."],"url":"http://arxiv.org/abs/2405.09277v1","category":"quant-ph"}
{"created":"2024-05-15 11:46:47","title":"Dual-Segment Clustering Strategy for Federated Learning in Heterogeneous Environments","abstract":"Federated learning (FL) is a distributed machine learning paradigm with high efficiency and low communication load, only transmitting parameters or gradients of network. However, the non-independent and identically distributed (Non-IID) data characteristic has a negative impact on this paradigm. Furthermore, the heterogeneity of communication quality will significantly affect the accuracy of parameter transmission, causing a degradation in the performance of the FL system or even preventing its convergence. This letter proposes a dual-segment clustering (DSC) strategy, which first clusters the clients according to the heterogeneous communication conditions and then performs a second clustering by the sample size and label distribution, so as to solve the problem of data and communication heterogeneity. Experimental results show that the DSC strategy proposed in this letter can improve the convergence rate of FL, and has superiority on accuracy in a heterogeneous environment compared with the classical algorithm of cluster.","sentences":["Federated learning (FL) is a distributed machine learning paradigm with high efficiency and low communication load, only transmitting parameters or gradients of network.","However, the non-independent and identically distributed (Non-IID) data characteristic has a negative impact on this paradigm.","Furthermore, the heterogeneity of communication quality will significantly affect the accuracy of parameter transmission, causing a degradation in the performance of the FL system or even preventing its convergence.","This letter proposes a dual-segment clustering (DSC) strategy, which first clusters the clients according to the heterogeneous communication conditions and then performs a second clustering by the sample size and label distribution, so as to solve the problem of data and communication heterogeneity.","Experimental results show that the DSC strategy proposed in this letter can improve the convergence rate of FL, and has superiority on accuracy in a heterogeneous environment compared with the classical algorithm of cluster."],"url":"http://arxiv.org/abs/2405.09276v1","category":"cs.LG"}
{"created":"2024-05-15 11:42:41","title":"Fair Generalized Linear Mixed Models","abstract":"When using machine learning for automated prediction, it is important to account for fairness in the prediction. Fairness in machine learning aims to ensure that biases in the data and model inaccuracies do not lead to discriminatory decisions. E.g., predictions from fair machine learning models should not discriminate against sensitive variables such as sexual orientation and ethnicity. The training data often in obtained from social surveys. In social surveys, oftentimes the data collection process is a strata sampling, e.g. due to cost restrictions. In strata samples, the assumption of independence between the observation is not fulfilled. Hence, if the machine learning models do not account for the strata correlations, the results may be biased. Especially high is the bias in cases where the strata assignment is correlated to the variable of interest. We present in this paper an algorithm that can handle both problems simultaneously, and we demonstrate the impact of stratified sampling on the quality of fair machine learning predictions in a reproducible simulation study.","sentences":["When using machine learning for automated prediction, it is important to account for fairness in the prediction.","Fairness in machine learning aims to ensure that biases in the data and model inaccuracies do not lead to discriminatory decisions.","E.g., predictions from fair machine learning models should not discriminate against sensitive variables such as sexual orientation and ethnicity.","The training data often in obtained from social surveys.","In social surveys, oftentimes the data collection process is a strata sampling, e.g. due to cost restrictions.","In strata samples, the assumption of independence between the observation is not fulfilled.","Hence, if the machine learning models do not account for the strata correlations, the results may be biased.","Especially high is the bias in cases where the strata assignment is correlated to the variable of interest.","We present in this paper an algorithm that can handle both problems simultaneously, and we demonstrate the impact of stratified sampling on the quality of fair machine learning predictions in a reproducible simulation study."],"url":"http://arxiv.org/abs/2405.09273v1","category":"cs.LG"}
{"created":"2024-05-15 11:40:02","title":"Gravitational Collapse in Higher-Dimensional Rastall Gravity with and without Cosmological Constant","abstract":"We consider a spherically symmetric homogeneous perfect fluid undergoing a gravitational collapse to singularity in the framework of higher-dimensional Rastall gravity in the cases of vanishing and nonvanishing cosmological constants. The possible final states of the collapse in any finite dimension are black hole and naked singularity, hence violating the cosmic censorship conjecture, but the naked singularity formation becomes less favored when the dimension is increased, such that the conjecture is fully restored in the limit of very high dimensions. We find that there are two physically distinct solutions for the collapse evolution in the case of nonzero cosmological constant: trigonometric and exponential solutions. The effective energy density of the fluid is decreasing (increasing) in the former (latter) when the magnitude of the cosmological constant is increased, which implies that the former undergoes a slower collapse than the latter. Furthermore, we find that a temporary trapped surface is possible to emerge in the case of trigonometric solution in the naked singularity region only. Therefore, faraway observers with observational time shorter than the collapse duration may conclude that a black hole is formed, although the collapse will eventually lead to a naked singularity formation.","sentences":["We consider a spherically symmetric homogeneous perfect fluid undergoing a gravitational collapse to singularity in the framework of higher-dimensional Rastall gravity in the cases of vanishing and nonvanishing cosmological constants.","The possible final states of the collapse in any finite dimension are black hole and naked singularity, hence violating the cosmic censorship conjecture, but the naked singularity formation becomes less favored when the dimension is increased, such that the conjecture is fully restored in the limit of very high dimensions.","We find that there are two physically distinct solutions for the collapse evolution in the case of nonzero cosmological constant: trigonometric and exponential solutions.","The effective energy density of the fluid is decreasing (increasing) in the former (latter) when the magnitude of the cosmological constant is increased, which implies that the former undergoes a slower collapse than the latter.","Furthermore, we find that a temporary trapped surface is possible to emerge in the case of trigonometric solution in the naked singularity region only.","Therefore, faraway observers with observational time shorter than the collapse duration may conclude that a black hole is formed, although the collapse will eventually lead to a naked singularity formation."],"url":"http://arxiv.org/abs/2405.09271v1","category":"gr-qc"}
{"created":"2024-05-15 11:38:10","title":"Preconceptual Modeling in Software Engineering: Metaphysics of Diagrammatic Representations","abstract":"According to many researchers, conceptual model (CM) development is a hard task, and system requirements are difficult to collect, causing many miscommunication problems. CMs require more than modeling ability alone - they first require an understanding of the targeted domain that the model attempts to represent. Accordingly, a preconceptual modeling (pre-CM) stage is intended to address ontological issues before typical CM development is initiated. It involves defining a portion of reality when entities and processes are differentiated and integrated as unified wholes. This pre-CM phase forms the focus of research in this paper. The purpose is not show how to model; rather, it is to demonstrate how to establish a metaphysical basis of the involved portion of reality. To demonstrate such a venture, we employ the so-called thinging machine (TM) modeling that has been proposed as a high-level CM. A TM model integrates staticity and dynamism grounded in a fundamental construct called a thimac (things/machine). It involves two modes of reality, existence (events) and subsistence (regions - roughly, specifications of things and processes). Currently, the dominant approach in CM has evolved to limit its scope of application to develop ontological categorization (types of things). In the TM approach, pre-CM metaphysics is viewed as a part and parcel of CM itself. The general research problem is how to map TM constructs to what is out there in the targeted domain. Discussions involve the nature of thimacs (things and processes) and subsistence and existence as they are superimposed over each other in reality. Specifically, we make two claims, (a) the perceptibility of regions as a phenomenon and (b) the distinctiveness of existence as a construct for events. The results contribute to further the understanding of TM modeling in addition to introducing some metaphysical insights.","sentences":["According to many researchers, conceptual model (CM) development is a hard task, and system requirements are difficult to collect, causing many miscommunication problems.","CMs require more than modeling ability alone - they first require an understanding of the targeted domain that the model attempts to represent.","Accordingly, a preconceptual modeling (pre-CM) stage is intended to address ontological issues before typical CM development is initiated.","It involves defining a portion of reality when entities and processes are differentiated and integrated as unified wholes.","This pre-CM phase forms the focus of research in this paper.","The purpose is not show how to model; rather, it is to demonstrate how to establish a metaphysical basis of the involved portion of reality.","To demonstrate such a venture, we employ the so-called thinging machine (TM) modeling that has been proposed as a high-level CM.","A TM model integrates staticity and dynamism grounded in a fundamental construct called a thimac (things/machine).","It involves two modes of reality, existence (events) and subsistence (regions - roughly, specifications of things and processes).","Currently, the dominant approach in CM has evolved to limit its scope of application to develop ontological categorization (types of things).","In the TM approach, pre-CM metaphysics is viewed as a part and parcel of CM itself.","The general research problem is how to map TM constructs to what is out there in the targeted domain.","Discussions involve the nature of thimacs (things and processes) and subsistence and existence as they are superimposed over each other in reality.","Specifically, we make two claims, (a) the perceptibility of regions as a phenomenon and (b) the distinctiveness of existence as a construct for events.","The results contribute to further the understanding of TM modeling in addition to introducing some metaphysical insights."],"url":"http://arxiv.org/abs/2405.09269v1","category":"cs.SE"}
{"created":"2024-05-15 11:35:40","title":"On the orbital stability of solitary waves for the fourth order nonlinear Schr\u00f6dinger equation","abstract":"In this paper, we present new results regarding the orbital stability of solitary standing waves for the general fourth-order Schr\\\"odinger equation with mixed dispersion. The existence of solitary waves can be determined both as minimizers of a constrained complex functional and by using a numerical approach. In addition, for specific values of the frequency associated with the standing wave, one obtains explicit solutions with a hyperbolic secant profile. Despite these explicit solutions being minimizers of the constrained functional, they cannot be seen as a smooth curve of solitary waves, and this fact prevents their determination of stability using classical approaches in the current literature. To overcome this difficulty, we employ a numerical approach to construct a smooth curve of solitary waves. The existence of a smooth curve is useful for showing the existence of a threshold power $\\alpha_0\\approx 4.8$ of the nonlinear term such that if $\\alpha\\in (0,\\alpha_0),$ the explicit solitary wave is stable, and if $\\alpha>\\alpha_0$, the wave is unstable. An important feature of our work, caused by the presence of the mixed dispersion term, concerns the fact that the threshold value $\\alpha_0 \\approx 4.8$ is not the same as that established for proving the existence of global solutions in the energy space, as is well known for the classical nonlinear Schr\\\"odinger equation.","sentences":["In this paper, we present new results regarding the orbital stability of solitary standing waves for the general fourth-order Schr\\\"odinger equation with mixed dispersion.","The existence of solitary waves can be determined both as minimizers of a constrained complex functional and by using a numerical approach.","In addition, for specific values of the frequency associated with the standing wave, one obtains explicit solutions with a hyperbolic secant profile.","Despite these explicit solutions being minimizers of the constrained functional, they cannot be seen as a smooth curve of solitary waves, and this fact prevents their determination of stability using classical approaches in the current literature.","To overcome this difficulty, we employ a numerical approach to construct a smooth curve of solitary waves.","The existence of a smooth curve is useful for showing the existence of a threshold power $\\alpha_0\\approx 4.8$ of the nonlinear term such that if $\\alpha\\in (0,\\alpha_0),$ the explicit solitary wave is stable, and if $\\alpha>\\alpha_0$, the wave is unstable.","An important feature of our work, caused by the presence of the mixed dispersion term, concerns the fact that the threshold value $\\alpha_0 \\approx 4.8$ is not the same as that established for proving the existence of global solutions in the energy space, as is well known for the classical nonlinear Schr\\\"odinger equation."],"url":"http://arxiv.org/abs/2405.09268v1","category":"math.AP"}
{"created":"2024-05-15 11:33:07","title":"Dance Any Beat: Blending Beats with Visuals in Dance Video Generation","abstract":"The task of generating dance from music is crucial, yet current methods, which mainly produce joint sequences, lead to outputs that lack intuitiveness and complicate data collection due to the necessity for precise joint annotations. We introduce a Dance Any Beat Diffusion model, namely DabFusion, that employs music as a conditional input to directly create dance videos from still images, utilizing conditional image-to-video generation principles. This approach pioneers the use of music as a conditioning factor in image-to-video synthesis. Our method unfolds in two stages: training an auto-encoder to predict latent optical flow between reference and driving frames, eliminating the need for joint annotation, and training a U-Net-based diffusion model to produce these latent optical flows guided by music rhythm encoded by CLAP. Although capable of producing high-quality dance videos, the baseline model struggles with rhythm alignment. We enhance the model by adding beat information, improving synchronization. We introduce a 2D motion-music alignment score (2D-MM Align) for quantitative assessment. Evaluated on the AIST++ dataset, our enhanced model shows marked improvements in 2D-MM Align score and established metrics. Video results can be found on our project page: https://DabFusion.github.io.","sentences":["The task of generating dance from music is crucial, yet current methods, which mainly produce joint sequences, lead to outputs that lack intuitiveness and complicate data collection due to the necessity for precise joint annotations.","We introduce a Dance Any Beat Diffusion model, namely DabFusion, that employs music as a conditional input to directly create dance videos from still images, utilizing conditional image-to-video generation principles.","This approach pioneers the use of music as a conditioning factor in image-to-video synthesis.","Our method unfolds in two stages: training an auto-encoder to predict latent optical flow between reference and driving frames, eliminating the need for joint annotation, and training a U-Net-based diffusion model to produce these latent optical flows guided by music rhythm encoded by CLAP.","Although capable of producing high-quality dance videos, the baseline model struggles with rhythm alignment.","We enhance the model by adding beat information, improving synchronization.","We introduce a 2D motion-music alignment score (2D-MM Align) for quantitative assessment.","Evaluated on the AIST++ dataset, our enhanced model shows marked improvements in 2D-MM Align score and established metrics.","Video results can be found on our project page: https://DabFusion.github.io."],"url":"http://arxiv.org/abs/2405.09266v1","category":"cs.CV"}
{"created":"2024-05-15 11:25:26","title":"Large solitons flattened by small quantum corrections","abstract":"We propose a general form of the UV-completed Friedberg-Lee-Sirlin (FLS) model. As can be seen from the mechanical interpretation, UV-completion allows thin-wall approximation for non-topological solitons. The 1-loop renormalized effective potential for the UV-completed FLS model is constructed under the assumption of mass hierarchy. By special choice of parameters, we studied how the Coleman-Weinberg mechanism induces a new mass scale $\\omega_{-}$ in the classical FLS model. It can be expounded in terms of a stable condensate. As a consequence, the asymptotic of a non-topological soliton's energy at large charge is changed from $\\sim Q^{3/4}$ to the linear law.","sentences":["We propose a general form of the UV-completed Friedberg-Lee-Sirlin (FLS) model.","As can be seen from the mechanical interpretation, UV-completion allows thin-wall approximation for non-topological solitons.","The 1-loop renormalized effective potential for the UV-completed FLS model is constructed under the assumption of mass hierarchy.","By special choice of parameters, we studied how the Coleman-Weinberg mechanism induces a new mass scale $\\omega_{-}$ in the classical FLS model.","It can be expounded in terms of a stable condensate.","As a consequence, the asymptotic of a non-topological soliton's energy at large charge is changed from $\\sim Q^{3/4}$ to the linear law."],"url":"http://arxiv.org/abs/2405.09262v1","category":"hep-ph"}
{"created":"2024-05-15 11:17:47","title":"Static spherically symmetric perfect fluid solutions in teleparallel F(T) gravity","abstract":"In this paper, we investigate static spherically symmetric teleparallel F(T) gravity containing a perfect isotropic fluid. We first write the field equations and proceed to find new teleparallel F(T) solutions for perfect isotropic and linear fluids. By using a power-law ansatz for the coframe components, we find several classes of new non-trivial teleparallel F(T) solutions. We also find a new class of teleparallel F(T) solutions for a matter dust fluid. After we solve the field equations for a non-linear perfect fluid. Once again, there are several new exact teleparallel F(T) solutions and also some approximated teleparallel F(T) solutions. All these classes of new solutions may be relevant for future cosmological and astrophysical applications.","sentences":["In this paper, we investigate static spherically symmetric teleparallel F(T) gravity containing a perfect isotropic fluid.","We first write the field equations and proceed to find new teleparallel F(T) solutions for perfect isotropic and linear fluids.","By using a power-law ansatz for the coframe components, we find several classes of new non-trivial teleparallel F(T) solutions.","We also find a new class of teleparallel F(T) solutions for a matter dust fluid.","After we solve the field equations for a non-linear perfect fluid.","Once again, there are several new exact teleparallel F(T) solutions and also some approximated teleparallel F(T) solutions.","All these classes of new solutions may be relevant for future cosmological and astrophysical applications."],"url":"http://arxiv.org/abs/2405.09257v1","category":"gr-qc"}
{"created":"2024-05-15 11:14:33","title":"Reinforcement Learning-Based Framework for the Intelligent Adaptation of User Interfaces","abstract":"Adapting the user interface (UI) of software systems to meet the needs and preferences of users is a complex task. The main challenge is to provide the appropriate adaptations at the appropriate time to offer value to end-users. Recent advances in Machine Learning (ML) techniques may provide effective means to support the adaptation process. In this paper, we instantiate a reference framework for Intelligent User Interface Adaptation by using Reinforcement Learning (RL) as the ML component to adapt user interfaces and ultimately improving the overall User Experience (UX). By using RL, the system is able to learn from past adaptations to improve the decision-making capabilities. Moreover, assessing the success of such adaptations remains a challenge. To overcome this issue, we propose to use predictive Human-Computer Interaction (HCI) models to evaluate the outcome of each action (ie adaptations) performed by the RL agent. In addition, we present an implementation of the instantiated framework, which is an extension of OpenAI Gym, that serves as a toolkit for developing and comparing RL algorithms. This Gym environment is highly configurable and extensible to other UI adaptation contexts. The evaluation results show that our RL-based framework can successfully train RL agents able to learn how to adapt UIs in a specific context to maximize the user engagement by using an HCI model as rewards predictor.","sentences":["Adapting the user interface (UI) of software systems to meet the needs and preferences of users is a complex task.","The main challenge is to provide the appropriate adaptations at the appropriate time to offer value to end-users.","Recent advances in Machine Learning (ML) techniques may provide effective means to support the adaptation process.","In this paper, we instantiate a reference framework for Intelligent User Interface Adaptation by using Reinforcement Learning (RL) as the ML component to adapt user interfaces and ultimately improving the overall User Experience (UX).","By using RL, the system is able to learn from past adaptations to improve the decision-making capabilities.","Moreover, assessing the success of such adaptations remains a challenge.","To overcome this issue, we propose to use predictive Human-Computer Interaction (HCI) models to evaluate the outcome of each action (ie adaptations) performed by the RL agent.","In addition, we present an implementation of the instantiated framework, which is an extension of OpenAI Gym, that serves as a toolkit for developing and comparing RL algorithms.","This Gym environment is highly configurable and extensible to other UI adaptation contexts.","The evaluation results show that our RL-based framework can successfully train RL agents able to learn how to adapt UIs in a specific context to maximize the user engagement by using an HCI model as rewards predictor."],"url":"http://arxiv.org/abs/2405.09255v1","category":"cs.HC"}
{"created":"2024-05-15 11:10:17","title":"Efficient motion of 90^{\\circ} domain walls in Mn_{2}Au via pure optical torques","abstract":"Discovering alternative ways to drive domain wall (DW) dynamics is crucial for advancing spintronic applications. Here we demonstrate via atomistic spin dynamics simulations that optical torques can efficiently drive 90^{\\circ} DWs in the Mn2Au antiferromagnet but their spatial symmetry forbids the motion of 180^{\\circ} walls. In the steady-state regime, the kinematics display special relativity signatures accessed for low laser intensities. At velocities higher than the magnonic limit, the DW enters a proliferation regime in which part of its relativistic energy is invested into the nucleation of novel magnetic textures. Our investigation contributes towards the fundamental understanding of opto-magnetic effects, supporting the development of next generation, all-optically controlled antiferromagnetic spintronics.","sentences":["Discovering alternative ways to drive domain wall (DW) dynamics is crucial for advancing spintronic applications.","Here we demonstrate via atomistic spin dynamics simulations that optical torques can efficiently drive 90^{\\circ} DWs in the Mn2Au antiferromagnet but their spatial symmetry forbids the motion of 180^{\\circ} walls.","In the steady-state regime, the kinematics display special relativity signatures accessed for low laser intensities.","At velocities higher than the magnonic limit, the DW enters a proliferation regime in which part of its relativistic energy is invested into the nucleation of novel magnetic textures.","Our investigation contributes towards the fundamental understanding of opto-magnetic effects, supporting the development of next generation, all-optically controlled antiferromagnetic spintronics."],"url":"http://arxiv.org/abs/2405.09253v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-15 10:55:16","title":"NeuralCMS: A deep learning approach to study Jupiter's interior","abstract":"NASA's Juno mission provided exquisite measurements of Jupiter's gravity field that together with the Galileo entry probe atmospheric measurements constrains the interior structure of the giant planet. Inferring its interior structure range remains a challenging inverse problem requiring a computationally intensive search of combinations of various planetary properties, such as the cloud-level temperature, composition, and core features, requiring the computation of ~10^9 interior models. We propose an efficient deep neural network (DNN) model to generate high-precision wide-ranged interior models based on the very accurate but computationally demanding concentric MacLaurin spheroid (CMS) method. We trained a sharing-based DNN with a large set of CMS results for a four-layer interior model of Jupiter, including a dilute core, to accurately predict the gravity moments and mass, given a combination of interior features. We evaluated the performance of the trained DNN (NeuralCMS) to inspect its predictive limitations. NeuralCMS shows very good performance in predicting the gravity moments, with errors comparable with the uncertainty due to differential rotation, and a very accurate mass prediction. This allowed us to perform a broad parameter space search by computing only ~10^4 actual CMS interior models, resulting in a large sample of plausible interior structures, and reducing the computation time by a factor of 10^5. Moreover, we used a DNN explainability algorithm to analyze the impact of the parameters setting the interior model on the predicted observables, providing information on their nonlinear relation.","sentences":["NASA's Juno mission provided exquisite measurements of Jupiter's gravity field that together with the Galileo entry probe atmospheric measurements constrains the interior structure of the giant planet.","Inferring its interior structure range remains a challenging inverse problem requiring a computationally intensive search of combinations of various planetary properties, such as the cloud-level temperature, composition, and core features, requiring the computation of ~10^9 interior models.","We propose an efficient deep neural network (DNN) model to generate high-precision wide-ranged interior models based on the very accurate but computationally demanding concentric MacLaurin spheroid (CMS) method.","We trained a sharing-based DNN with a large set of CMS results for a four-layer interior model of Jupiter, including a dilute core, to accurately predict the gravity moments and mass, given a combination of interior features.","We evaluated the performance of the trained DNN (NeuralCMS) to inspect its predictive limitations.","NeuralCMS shows very good performance in predicting the gravity moments, with errors comparable with the uncertainty due to differential rotation, and a very accurate mass prediction.","This allowed us to perform a broad parameter space search by computing only ~10^4 actual CMS interior models, resulting in a large sample of plausible interior structures, and reducing the computation time by a factor of 10^5.","Moreover, we used a DNN explainability algorithm to analyze the impact of the parameters setting the interior model on the predicted observables, providing information on their nonlinear relation."],"url":"http://arxiv.org/abs/2405.09244v1","category":"astro-ph.EP"}
{"created":"2024-05-15 10:47:05","title":"Noether's theorem and Ward-Takahashi identities from homotopy algebras","abstract":"We derive the new identity in homotopy algebras which directly corresponds to the Schwinger-Dyson equations in quantum field theory. As an application, we derive the Ward-Takahashi identities. We demonstrate that the Ward-Takahashi identities are reproduced in several examples. In general, our formula contains divergence. We mediate this problem by introducing stubs known in the context of string field theory. With the regularization, we can calculate the anomaly such as axial U(1) anomaly in vector-like U(1) gauge theory.","sentences":["We derive the new identity in homotopy algebras which directly corresponds to the Schwinger-Dyson equations in quantum field theory.","As an application, we derive the Ward-Takahashi identities.","We demonstrate that the Ward-Takahashi identities are reproduced in several examples.","In general, our formula contains divergence.","We mediate this problem by introducing stubs known in the context of string field theory.","With the regularization, we can calculate the anomaly such as axial U(1) anomaly in vector-like U(1) gauge theory."],"url":"http://arxiv.org/abs/2405.09243v1","category":"hep-th"}
{"created":"2024-05-15 10:43:02","title":"The representation of $\\mathfrak{S}_n$ on the cohomology of the permutohedral variety and gamma vectors of partitioned permutohedra","abstract":"Foata and Sch\\\"{u}tzenberger gave an expansion for the Eulerian polynomial $A_n(t)$ in terms of the basis $\\{t^j(1+t)^{n-1-2j}\\}$ for the space of polynomials $f(t)$ satisfying $f(t)=t^{n-1}f(1/t)$. We generalize this result in two ways. First, we provide an analogue for the graded representation of the symmetric group $\\mathfrak{S}_n$ on the cohomology of the permutohedral variety. Then we give expansions $h$-polynomials of polytopes obtained by cutting permutohedra with hyperplanes orthogonal to simple roots in terms of the same basis.","sentences":["Foata and Sch\\\"{u}tzenberger gave an expansion for the Eulerian polynomial $A_n(t)$ in terms of the basis $\\{t^j(1+t)^{n-1-2j}\\}$ for the space of polynomials $f(t)$ satisfying $f(t)=t^{n-1}f(1/t)$. We generalize this result in two ways.","First, we provide an analogue for the graded representation of the symmetric group $\\mathfrak{S}_n$ on the cohomology of the permutohedral variety.","Then we give expansions $h$-polynomials of polytopes obtained by cutting permutohedra with hyperplanes orthogonal to simple roots in terms of the same basis."],"url":"http://arxiv.org/abs/2405.09242v1","category":"math.CO"}
{"created":"2024-05-15 10:41:47","title":"SMUG-Explain: A Framework for Symbolic Music Graph Explanations","abstract":"In this work, we present Score MUsic Graph (SMUG)-Explain, a framework for generating and visualizing explanations of graph neural networks applied to arbitrary prediction tasks on musical scores. Our system allows the user to visualize the contribution of input notes (and note features) to the network output, directly in the context of the musical score. We provide an interactive interface based on the music notation engraving library Verovio. We showcase the usage of SMUG-Explain on the task of cadence detection in classical music. All code is available on https://github.com/manoskary/SMUG-Explain.","sentences":["In this work, we present Score MUsic Graph (SMUG)-Explain, a framework for generating and visualizing explanations of graph neural networks applied to arbitrary prediction tasks on musical scores.","Our system allows the user to visualize the contribution of input notes (and note features) to the network output, directly in the context of the musical score.","We provide an interactive interface based on the music notation engraving library Verovio.","We showcase the usage of SMUG-Explain on the task of cadence detection in classical music.","All code is available on https://github.com/manoskary/SMUG-Explain."],"url":"http://arxiv.org/abs/2405.09241v1","category":"cs.SD"}
{"created":"2024-05-15 10:31:42","title":"Roots in the semiring of finite deterministic dynamical systems","abstract":"Finite discrete-time dynamical systems (FDDS) model phenomena that evolve deterministically in discrete time. It is possible to define sum and product operations on these systems (disjoint union and direct product, respectively) giving a commutative semiring. This algebraic structure led to several works employing polynomial equations to model hypotheses on phenomena modelled using FDDS. To solve these equations, algorithms for performing the division and computing $k$-th roots are needed. In this paper, we propose two polynomial algorithms for these tasks, under the condition that the result is a connected FDDS. This ultimately leads to an efficient solution to equations of the type $AX^k=B$ for connected $X$. These results are some of the important final steps for solving more general polynomial equations on FDDS.","sentences":["Finite discrete-time dynamical systems (FDDS) model phenomena that evolve deterministically in discrete time.","It is possible to define sum and product operations on these systems (disjoint union and direct product, respectively) giving a commutative semiring.","This algebraic structure led to several works employing polynomial equations to model hypotheses on phenomena modelled using FDDS.","To solve these equations, algorithms for performing the division and computing $k$-th roots are needed.","In this paper, we propose two polynomial algorithms for these tasks, under the condition that the result is a connected FDDS.","This ultimately leads to an efficient solution to equations of the type $AX^k=B$ for connected $X$. These results are some of the important final steps for solving more general polynomial equations on FDDS."],"url":"http://arxiv.org/abs/2405.09236v1","category":"cs.DM"}
{"created":"2024-05-15 10:26:24","title":"Algebraic Tools for Computing Polynomial Loop Invariants","abstract":"Loop invariants are properties of a program loop that hold before and after each iteration of the loop. They are often employed to verify programs and ensure that algorithms consistently produce correct results during execution. Consequently, the generation of invariants becomes a crucial task for loops. We specifically focus on polynomial loops, where both the loop conditions and assignments within the loop are expressed as polynomials. Although computing polynomial invariants for general loops is undecidable, efficient algorithms have been developed for certain classes of loops. For instance, when all assignments within a while loop involve linear polynomials, the loop becomes solvable. In this work, we study the more general case where the polynomials exhibit arbitrary degrees.   Applying tools from algebraic geometry, we present two algorithms designed to generate all polynomial invariants for a while loop, up to a specified degree. These algorithms differ based on whether the initial values of the loop variables are given or treated as parameters. Furthermore, we introduce various methods to address cases where the algebraic problem exceeds the computational capabilities of our methods. In such instances, we identify alternative approaches to generate specific polynomial invariants.","sentences":["Loop invariants are properties of a program loop that hold before and after each iteration of the loop.","They are often employed to verify programs and ensure that algorithms consistently produce correct results during execution.","Consequently, the generation of invariants becomes a crucial task for loops.","We specifically focus on polynomial loops, where both the loop conditions and assignments within the loop are expressed as polynomials.","Although computing polynomial invariants for general loops is undecidable, efficient algorithms have been developed for certain classes of loops.","For instance, when all assignments within a while loop involve linear polynomials, the loop becomes solvable.","In this work, we study the more general case where the polynomials exhibit arbitrary degrees.   ","Applying tools from algebraic geometry, we present two algorithms designed to generate all polynomial invariants for a while loop, up to a specified degree.","These algorithms differ based on whether the initial values of the loop variables are given or treated as parameters.","Furthermore, we introduce various methods to address cases where the algebraic problem exceeds the computational capabilities of our methods.","In such instances, we identify alternative approaches to generate specific polynomial invariants."],"url":"http://arxiv.org/abs/2405.09232v1","category":"cs.SC"}
{"created":"2024-05-15 10:18:30","title":"Reduce to the MACs -- Privacy Friendly Generic Probe Requests","abstract":"Abstract. Since the introduction of active discovery in Wi-Fi networks, users can be tracked via their probe requests. Although manufacturers typically try to conceal Media Access Control (MAC) addresses using MAC address randomisation, probe requests still contain Information Elements (IEs) that facilitate device identification. This paper introduces generic probe requests: By removing all unnecessary information from IEs, the requests become indistinguishable from one another, letting single devices disappear in the largest possible anonymity set. Conducting a comprehensive evaluation, we demonstrate that a large IE set contained within undirected probe requests does not necessarily imply fast connection establishment. Furthermore, we show that minimising IEs to nothing but Supported Rates would enable 82.55% of the devices to share the same anonymity set. Our contributions provide a significant advancement in the pursuit of robust privacy solutions for wireless networks, paving the way for more user anonymity and less surveillance in wireless communication ecosystems.","sentences":["Abstract.","Since the introduction of active discovery in Wi-Fi networks, users can be tracked via their probe requests.","Although manufacturers typically try to conceal Media Access Control (MAC) addresses using MAC address randomisation, probe requests still contain Information Elements (IEs) that facilitate device identification.","This paper introduces generic probe requests: By removing all unnecessary information from IEs, the requests become indistinguishable from one another, letting single devices disappear in the largest possible anonymity set.","Conducting a comprehensive evaluation, we demonstrate that a large IE set contained within undirected probe requests does not necessarily imply fast connection establishment.","Furthermore, we show that minimising IEs to nothing but Supported Rates would enable 82.55% of the devices to share the same anonymity set.","Our contributions provide a significant advancement in the pursuit of robust privacy solutions for wireless networks, paving the way for more user anonymity and less surveillance in wireless communication ecosystems."],"url":"http://arxiv.org/abs/2405.09230v1","category":"cs.CR"}
{"created":"2024-05-15 10:17:03","title":"Theoretical analysis of the reactions induced by interaction of $^{6}$Li with nuclei $^{3}$H and $^{3}$He","abstract":"We determine cross sections and astrophysical S-factors of the reactions generated in collisions between $^{6}$Li and $^{3}$H, and $^{6}$Li and $^{3}$He. A microscopic three-cluster model is employed to study the dynamics of reactions occurring in the mirror nuclei $^{9}$Be and $^{9}$B. In a previous study [Phys. Rev. C {\\bf 109}, 045803 (2024)], this model was successfully applied to investigate the resonance structure of $^{9}$Be and $^{9}$B, as well as reactions induced by the interaction of deuterons with $^7$Li and $^7$Be. A fairly good agreement between theoretical results and available experimental data was achieved. To the best of our knowledge, appropriate experimental data for the astrophysical S-factors of the reactions generated by the interaction of $^6$Li with $^3$H and with $^3$He are currently unavailable. Thus, our results can serve as a guideline for future experimental efforts.","sentences":["We determine cross sections and astrophysical S-factors of the reactions generated in collisions between $^{6}$Li and $^{3}$H, and $^{6}$Li and $^{3}$He.","A microscopic three-cluster model is employed to study the dynamics of reactions occurring in the mirror nuclei $^{9}$Be and $^{9}$B. In a previous study [Phys. Rev. C {\\bf 109}, 045803 (2024)], this model was successfully applied to investigate the resonance structure of $^{9}$Be and $^{9}$B, as well as reactions induced by the interaction of deuterons with $^7$Li and $^7$Be.","A fairly good agreement between theoretical results and available experimental data was achieved.","To the best of our knowledge, appropriate experimental data for the astrophysical S-factors of the reactions generated by the interaction of $^6$Li with $^3$H and with $^3$He are currently unavailable.","Thus, our results can serve as a guideline for future experimental efforts."],"url":"http://arxiv.org/abs/2405.09229v1","category":"nucl-th"}
{"created":"2024-05-15 10:04:44","title":"Perception-Inspired Graph Convolution for Music Understanding Tasks","abstract":"We propose a new graph convolutional block, called MusGConv, specifically designed for the efficient processing of musical score data and motivated by general perceptual principles. It focuses on two fundamental dimensions of music, pitch and rhythm, and considers both relative and absolute representations of these components. We evaluate our approach on four different musical understanding problems: monophonic voice separation, harmonic analysis, cadence detection, and composer identification which, in abstract terms, translate to different graph learning problems, namely, node classification, link prediction, and graph classification. Our experiments demonstrate that MusGConv improves the performance on three of the aforementioned tasks while being conceptually very simple and efficient. We interpret this as evidence that it is beneficial to include perception-informed processing of fundamental musical concepts when developing graph network applications on musical score data.","sentences":["We propose a new graph convolutional block, called MusGConv, specifically designed for the efficient processing of musical score data and motivated by general perceptual principles.","It focuses on two fundamental dimensions of music, pitch and rhythm, and considers both relative and absolute representations of these components.","We evaluate our approach on four different musical understanding problems: monophonic voice separation, harmonic analysis, cadence detection, and composer identification which, in abstract terms, translate to different graph learning problems, namely, node classification, link prediction, and graph classification.","Our experiments demonstrate that MusGConv improves the performance on three of the aforementioned tasks while being conceptually very simple and efficient.","We interpret this as evidence that it is beneficial to include perception-informed processing of fundamental musical concepts when developing graph network applications on musical score data."],"url":"http://arxiv.org/abs/2405.09224v1","category":"cs.SD"}
{"created":"2024-05-15 10:04:19","title":"Word Alignment as Preference for Machine Translation","abstract":"The problem of hallucination and omission, a long-standing problem in machine translation (MT), is more pronounced when a large language model (LLM) is used in MT because an LLM itself is susceptible to these phenomena. In this work, we mitigate the problem in an LLM-based MT model by guiding it to better word alignment. We first study the correlation between word alignment and the phenomena of hallucination and omission in MT. Then we propose to utilize word alignment as preference to optimize the LLM-based MT model. The preference data are constructed by selecting chosen and rejected translations from multiple MT tools. Subsequently, direct preference optimization is used to optimize the LLM-based model towards the preference signal. Given the absence of evaluators specifically designed for hallucination and omission in MT, we further propose selecting hard instances and utilizing GPT-4 to directly evaluate the performance of the models in mitigating these issues. We verify the rationality of these designed evaluation methods by experiments, followed by extensive results demonstrating the effectiveness of word alignment-based preference optimization to mitigate hallucination and omission.","sentences":["The problem of hallucination and omission, a long-standing problem in machine translation (MT), is more pronounced when a large language model (LLM) is used in MT because an LLM itself is susceptible to these phenomena.","In this work, we mitigate the problem in an LLM-based MT model by guiding it to better word alignment.","We first study the correlation between word alignment and the phenomena of hallucination and omission in MT.","Then we propose to utilize word alignment as preference to optimize the LLM-based MT model.","The preference data are constructed by selecting chosen and rejected translations from multiple MT tools.","Subsequently, direct preference optimization is used to optimize the LLM-based model towards the preference signal.","Given the absence of evaluators specifically designed for hallucination and omission in MT, we further propose selecting hard instances and utilizing GPT-4 to directly evaluate the performance of the models in mitigating these issues.","We verify the rationality of these designed evaluation methods by experiments, followed by extensive results demonstrating the effectiveness of word alignment-based preference optimization to mitigate hallucination and omission."],"url":"http://arxiv.org/abs/2405.09223v1","category":"cs.CL"}
{"created":"2024-05-15 10:03:22","title":"Anchor Layout Optimization for Ultrasonic Indoor Positioning Using Swarm Intelligence","abstract":"Indoor positioning applications are craving for ever higher precision and accuracy across the entire coverage zone. Optimal anchor placement and the deployment of multiple distributed anchor nodes could have a major impact in this regard. This paper examines the influences of these two difficult to approach hypotheses by means of a straightforward ultrasonic 3D indoor positioning system deployed in a real-life scenario via a geometric based simulation framework. To obtain an optimal anchor placement, a particle swarm optimization (PSO) algorithm is introduced and consequently performed for setups ranging from 4 to 10 anchors. In this way, besides the optimal anchor placement layout, the influence of deploying several distributed anchor nodes is investigated. In order to theoretically compare the optimization progress, a system model and Cram\\'er-Rao lower bound (CRLB) are established and the results are quantified based on the simulation data. With limited anchors, the placement is crucial to obtain a high precision high reliability (HPHR) indoor positioning system (IPS), while the addition of anchors, to a lesser extent, gives a supplementary improvement.","sentences":["Indoor positioning applications are craving for ever higher precision and accuracy across the entire coverage zone.","Optimal anchor placement and the deployment of multiple distributed anchor nodes could have a major impact in this regard.","This paper examines the influences of these two difficult to approach hypotheses by means of a straightforward ultrasonic 3D indoor positioning system deployed in a real-life scenario via a geometric based simulation framework.","To obtain an optimal anchor placement, a particle swarm optimization (PSO) algorithm is introduced and consequently performed for setups ranging from 4 to 10 anchors.","In this way, besides the optimal anchor placement layout, the influence of deploying several distributed anchor nodes is investigated.","In order to theoretically compare the optimization progress, a system model and Cram\\'er-Rao lower bound (CRLB) are established and the results are quantified based on the simulation data.","With limited anchors, the placement is crucial to obtain a high precision high reliability (HPHR) indoor positioning system (IPS), while the addition of anchors, to a lesser extent, gives a supplementary improvement."],"url":"http://arxiv.org/abs/2405.09222v1","category":"eess.SP"}
{"created":"2024-05-15 10:02:47","title":"Bridging the gap in online hate speech detection: a comparative analysis of BERT and traditional models for homophobic content identification on X/Twitter","abstract":"Our study addresses a significant gap in online hate speech detection research by focusing on homophobia, an area often neglected in sentiment analysis research. Utilising advanced sentiment analysis models, particularly BERT, and traditional machine learning methods, we developed a nuanced approach to identify homophobic content on X/Twitter. This research is pivotal due to the persistent underrepresentation of homophobia in detection models. Our findings reveal that while BERT outperforms traditional methods, the choice of validation technique can impact model performance. This underscores the importance of contextual understanding in detecting nuanced hate speech. By releasing the largest open-source labelled English dataset for homophobia detection known to us, an analysis of various models' performance and our strongest BERT-based model, we aim to enhance online safety and inclusivity. Future work will extend to broader LGBTQIA+ hate speech detection, addressing the challenges of sourcing diverse datasets. Through this endeavour, we contribute to the larger effort against online hate, advocating for a more inclusive digital landscape. Our study not only offers insights into the effective detection of homophobic content by improving on previous research results, but it also lays groundwork for future advancements in hate speech analysis.","sentences":["Our study addresses a significant gap in online hate speech detection research by focusing on homophobia, an area often neglected in sentiment analysis research.","Utilising advanced sentiment analysis models, particularly BERT, and traditional machine learning methods, we developed a nuanced approach to identify homophobic content on X/Twitter.","This research is pivotal due to the persistent underrepresentation of homophobia in detection models.","Our findings reveal that while BERT outperforms traditional methods, the choice of validation technique can impact model performance.","This underscores the importance of contextual understanding in detecting nuanced hate speech.","By releasing the largest open-source labelled English dataset for homophobia detection known to us, an analysis of various models' performance and our strongest BERT-based model, we aim to enhance online safety and inclusivity.","Future work will extend to broader LGBTQIA+ hate speech detection, addressing the challenges of sourcing diverse datasets.","Through this endeavour, we contribute to the larger effort against online hate, advocating for a more inclusive digital landscape.","Our study not only offers insights into the effective detection of homophobic content by improving on previous research results, but it also lays groundwork for future advancements in hate speech analysis."],"url":"http://arxiv.org/abs/2405.09221v1","category":"cs.CL"}
{"created":"2024-05-15 09:59:37","title":"ALPINE: Unveiling the Planning Capability of Autoregressive Learning in Language Models","abstract":"In this paper, we present the findings of our Project ALPINE which stands for ``Autoregressive Learning for Planning In NEtworks.\" Project ALPINE initiates a theoretical investigation into the development of planning capabilities in Transformer-based language models through their autoregressive learning mechanisms, aiming to identify any potential limitations in their planning abilities. We abstract planning as a network path-finding task where the objective is to generate a valid path from a specified source node to a designated target node. In terms of expressiveness, we show that the Transformer is capable of executing path-finding by embedding the adjacency and reachability matrices within its weights. Our theoretical analysis of the gradient-based learning dynamic of the Transformer reveals that the Transformer is capable of learning both the adjacency matrix and a limited form of the reachability matrix. These theoretical insights are then validated through experiments, which demonstrate that the Transformer indeed learns the adjacency matrix and an incomplete reachability matrix, which aligns with the predictions made in our theoretical analysis. Additionally, when applying our methodology to a real-world planning benchmark, called Blocksworld, our observations remain consistent. Our theoretical and empirical analyses further unveil a potential limitation of Transformer in path-finding: it cannot identify reachability relationships through transitivity, and thus would fail when path concatenation is needed to generate a path. In summary, our findings shed new light on how the internal mechanisms of autoregressive learning enable planning in networks. This study may contribute to our understanding of the general planning capabilities in other related domains.","sentences":["In this paper, we present the findings of our Project ALPINE which stands for ``Autoregressive Learning for Planning In NEtworks.\"","Project ALPINE initiates a theoretical investigation into the development of planning capabilities in Transformer-based language models through their autoregressive learning mechanisms, aiming to identify any potential limitations in their planning abilities.","We abstract planning as a network path-finding task where the objective is to generate a valid path from a specified source node to a designated target node.","In terms of expressiveness, we show that the Transformer is capable of executing path-finding by embedding the adjacency and reachability matrices within its weights.","Our theoretical analysis of the gradient-based learning dynamic of the Transformer reveals that the Transformer is capable of learning both the adjacency matrix and a limited form of the reachability matrix.","These theoretical insights are then validated through experiments, which demonstrate that the Transformer indeed learns the adjacency matrix and an incomplete reachability matrix, which aligns with the predictions made in our theoretical analysis.","Additionally, when applying our methodology to a real-world planning benchmark, called Blocksworld, our observations remain consistent.","Our theoretical and empirical analyses further unveil a potential limitation of Transformer in path-finding: it cannot identify reachability relationships through transitivity, and thus would fail when path concatenation is needed to generate a path.","In summary, our findings shed new light on how the internal mechanisms of autoregressive learning enable planning in networks.","This study may contribute to our understanding of the general planning capabilities in other related domains."],"url":"http://arxiv.org/abs/2405.09220v1","category":"cs.LG"}
{"created":"2024-05-15 09:50:43","title":"The Genomic Landscape of Oceania","abstract":"Encompassing regions that were amongst the first inhabited by humans following the out-of-Africa expansion, hosting populations with the highest levels of archaic hominid introgression, and including Pacific islands that are the most isolated inhabited locations on the planet, Oceania has a rich, but understudied, human genomic landscape. Here we describe the first region-wide analysis of genome-wide data from population groups spanning Oceania and its surroundings, from island and peninsular southeast Asia to Papua New Guinea, east across the Pacific through Melanesia, Micronesia, and Polynesia, and west across the Indian Ocean to related island populations in the Andamans and Madagascar. In total we generate and analyze genome-wide data from 981 individuals from 92 different populations, 58 separate islands, and 30 countries, representing the most expansive study of Pacific genetics to date. In each sample we disentangle the Papuan and more recent Austronesian ancestries, which have admixed in various proportions across this region, using ancestry-specific analyses, and characterize the distinct patterns of settlement, migration, and archaic introgression separately in these two ancestries. We also focus on the patterns of clinically relevant genetic variation across Oceania--a landscape rippled with strong founder effects and island-specific genetic drift in allele frequencies--providing an atlas for the development of precision genetic health strategies in this understudied region of the world.","sentences":["Encompassing regions that were amongst the first inhabited by humans following the out-of-Africa expansion, hosting populations with the highest levels of archaic hominid introgression, and including Pacific islands that are the most isolated inhabited locations on the planet, Oceania has a rich, but understudied, human genomic landscape.","Here we describe the first region-wide analysis of genome-wide data from population groups spanning Oceania and its surroundings, from island and peninsular southeast Asia to Papua New Guinea, east across the Pacific through Melanesia, Micronesia, and Polynesia, and west across the Indian Ocean to related island populations in the Andamans and Madagascar.","In total we generate and analyze genome-wide data from 981 individuals from 92 different populations, 58 separate islands, and 30 countries, representing the most expansive study of Pacific genetics to date.","In each sample we disentangle the Papuan and more recent Austronesian ancestries, which have admixed in various proportions across this region, using ancestry-specific analyses, and characterize the distinct patterns of settlement, migration, and archaic introgression separately in these two ancestries.","We also focus on the patterns of clinically relevant genetic variation across Oceania--a landscape rippled with strong founder effects and island-specific genetic drift in allele frequencies--providing an atlas for the development of precision genetic health strategies in this understudied region of the world."],"url":"http://arxiv.org/abs/2405.09216v1","category":"q-bio.PE"}
{"created":"2024-05-15 09:47:59","title":"Xmodel-VLM: A Simple Baseline for Multimodal Vision Language Model","abstract":"We introduce Xmodel-VLM, a cutting-edge multimodal vision language model. It is designed for efficient deployment on consumer GPU servers. Our work directly confronts a pivotal industry issue by grappling with the prohibitive service costs that hinder the broad adoption of large-scale multimodal systems. Through rigorous training, we have developed a 1B-scale language model from the ground up, employing the LLaVA paradigm for modal alignment. The result, which we call Xmodel-VLM, is a lightweight yet powerful multimodal vision language model. Extensive testing across numerous classic multimodal benchmarks has revealed that despite its smaller size and faster execution, Xmodel-VLM delivers performance comparable to that of larger models. Our model checkpoints and code are publicly available on GitHub at https://github.com/XiaoduoAILab/XmodelVLM.","sentences":["We introduce Xmodel-VLM, a cutting-edge multimodal vision language model.","It is designed for efficient deployment on consumer GPU servers.","Our work directly confronts a pivotal industry issue by grappling with the prohibitive service costs that hinder the broad adoption of large-scale multimodal systems.","Through rigorous training, we have developed a 1B-scale language model from the ground up, employing the LLaVA paradigm for modal alignment.","The result, which we call Xmodel-VLM, is a lightweight yet powerful multimodal vision language model.","Extensive testing across numerous classic multimodal benchmarks has revealed that despite its smaller size and faster execution, Xmodel-VLM delivers performance comparable to that of larger models.","Our model checkpoints and code are publicly available on GitHub at https://github.com/XiaoduoAILab/XmodelVLM."],"url":"http://arxiv.org/abs/2405.09215v1","category":"cs.CV"}
{"created":"2024-05-15 09:47:15","title":"Hypergraph C*-algebras","abstract":"We give a definition of hypergraph C*-algebras. These generalize the well-known graph C*-algebras as well as ultragraph C*-algebras. In contrast to those objects, hypergraph C*-algebras are not always nuclear. We provide a number of non-nuclear examples, we prove a Gauge-Invariant Uniqueness Theorem for a subclass of hypergraph C*-algebras and we study moves on hypergraphs which generalize the moves in the theory of graph C*-algebras.","sentences":["We give a definition of hypergraph C*-algebras.","These generalize the well-known graph C*-algebras as well as ultragraph C*-algebras.","In contrast to those objects, hypergraph C*-algebras are not always nuclear.","We provide a number of non-nuclear examples, we prove a Gauge-Invariant Uniqueness Theorem for a subclass of hypergraph C*-algebras and we study moves on hypergraphs which generalize the moves in the theory of graph C*-algebras."],"url":"http://arxiv.org/abs/2405.09214v1","category":"math.OA"}
{"created":"2024-05-15 09:32:41","title":"A refined Weyl character formula for comodules on $\\operatorname{GL}(2,A)$","abstract":"Let $A$ be any commutative unital ring and let $\\operatorname{GL}(2,A)$ be the general linear group scheme on $A$ of rank $2$. We study the representation theory of $\\operatorname{GL}(2,A)$ and the symmetric powers $\\operatorname{Sym}^d(V)$, where $(V, \\Delta)$ is the standard right comodule on $\\operatorname{GL}(2,A)$. We prove a refined Weyl character formula for $\\operatorname{Sym}^d(V)$. There is for any integer $d \\geq 1$ a (canonical) refined weight space decomposition $\\operatorname{Sym}^d(V) \\cong \\oplus_i \\operatorname{Sym}^d(V)^i$ where each direct summand $\\operatorname{Sym}^d(V)^i$ is a comodule on $N \\subseteq \\operatorname{GL}(2,A)$. Here $N$ is the schematic normalizer of the diagonal torus $T \\subseteq \\operatorname{GL}(2,A)$. We prove a character formula for the direct summands of $\\operatorname{Sym}^d(V)$ for any integer $d \\geq 1$. This refined Weyl character formula implies the classical Weyl character formula. As a Corollary we get a refined Weyl character formula for the pull back $\\operatorname{Sym}^d(V \\otimes K)$ as a comodule on $\\operatorname{GL}(2,K)$ where $K$ is any field. We also calculate explicit examples involving the symmetric powers, symmetric tensors and their duals. The refined weight space decomposition exists in general for group schemes such as $\\operatorname{GL}(n,A)$ and $\\operatorname{SL}(n,A)$. The methods introduced in the paper may have applications to the study of finite rank torsion free comodules on $\\operatorname{SL}(n,Z)$ and $\\operatorname{GL}(n,Z)$. There is no \"highest weight theory\" or \"complete reducibility property\" such comodules, and we want to give a definition of the notion \"good filtration\" for such modules. Such a study may have applications to the study of groups $G$ such as $\\operatorname{SL}(n,k)$ and $\\operatorname{GL}(n,k)$ and quotients $G/H$ where $k$ is an arbitrary field and $H \\subseteq G$ is a closed subgroup.","sentences":["Let $A$ be any commutative unital ring and let $\\operatorname{GL}(2,A)$ be the general linear group scheme on $A$ of rank $2$. We study the representation theory of $\\operatorname{GL}(2,A)$ and the symmetric powers $\\operatorname{Sym}^d(V)$, where $(V, \\Delta)$ is the standard right comodule on $\\operatorname{GL}(2,A)$. We prove a refined Weyl character formula for $\\operatorname{Sym}^d(V)$. There is for any integer $d \\geq 1$ a (canonical) refined weight space decomposition $\\operatorname{Sym}^d(V)","\\cong \\oplus_i \\operatorname{Sym}^d(V)^i$ where each direct summand $\\operatorname{Sym}^d(V)^i$ is a comodule on $N \\subseteq","\\operatorname{GL}(2,A)$. Here $N$ is the schematic normalizer of the diagonal torus $T \\subseteq","\\operatorname{GL}(2,A)$. We prove a character formula for the direct summands of $\\operatorname{Sym}^d(V)$ for any integer $d \\geq 1$.","This refined Weyl character formula implies the classical Weyl character formula.","As a Corollary we get a refined Weyl character formula for the pull back $\\operatorname{Sym}^d(V \\otimes K)$ as a comodule on $\\operatorname{GL}(2,K)$ where $K$ is any field.","We also calculate explicit examples involving the symmetric powers, symmetric tensors and their duals.","The refined weight space decomposition exists in general for group schemes such as $\\operatorname{GL}(n,A)$ and $\\operatorname{SL}(n,A)$.","The methods introduced in the paper may have applications to the study of finite rank torsion free comodules on $\\operatorname{SL}(n,Z)$ and $\\operatorname{GL}(n,Z)$. There is no \"highest weight theory\" or \"complete reducibility property\" such comodules, and we want to give a definition of the notion \"good filtration\" for such modules.","Such a study may have applications to the study of groups $G$ such as $\\operatorname{SL}(n,k)$ and $\\operatorname{GL}(n,k)$ and quotients $G/H$ where $k$ is an arbitrary field and $H \\subseteq G$ is a closed subgroup."],"url":"http://arxiv.org/abs/2405.09210v1","category":"math.AG"}
{"created":"2024-05-15 09:25:06","title":"An Exact Theory of Causal Emergence for Linear Stochastic Iteration Systems","abstract":"After coarse-graining a complex system, the dynamics of its macro-state may exhibit more pronounced causal effects than those of its micro-state. This phenomenon, known as causal emergence, is quantified by the indicator of effective information. However, two challenges confront this theory: the absence of well-developed frameworks in continuous stochastic dynamical systems and the reliance on coarse-graining methodologies. In this study, we introduce an exact theoretic framework for causal emergence within linear stochastic iteration systems featuring continuous state spaces and Gaussian noise. Building upon this foundation, we derive an analytical expression for effective information across general dynamics and identify optimal linear coarse-graining strategies that maximize the degree of causal emergence when the dimension averaged uncertainty eliminated by coarse-graining has an upper bound. Our investigation reveals that the maximal causal emergence and the optimal coarse-graining methods are primarily determined by the principal eigenvalues and eigenvectors of the dynamic system's parameter matrix, with the latter not being unique. To validate our propositions, we apply our analytical models to three simplified physical systems, comparing the outcomes with numerical simulations, and consistently achieve congruent results.","sentences":["After coarse-graining a complex system, the dynamics of its macro-state may exhibit more pronounced causal effects than those of its micro-state.","This phenomenon, known as causal emergence, is quantified by the indicator of effective information.","However, two challenges confront this theory: the absence of well-developed frameworks in continuous stochastic dynamical systems and the reliance on coarse-graining methodologies.","In this study, we introduce an exact theoretic framework for causal emergence within linear stochastic iteration systems featuring continuous state spaces and Gaussian noise.","Building upon this foundation, we derive an analytical expression for effective information across general dynamics and identify optimal linear coarse-graining strategies that maximize the degree of causal emergence when the dimension averaged uncertainty eliminated by coarse-graining has an upper bound.","Our investigation reveals that the maximal causal emergence and the optimal coarse-graining methods are primarily determined by the principal eigenvalues and eigenvectors of the dynamic system's parameter matrix, with the latter not being unique.","To validate our propositions, we apply our analytical models to three simplified physical systems, comparing the outcomes with numerical simulations, and consistently achieve congruent results."],"url":"http://arxiv.org/abs/2405.09207v1","category":"cs.IT"}
{"created":"2024-05-15 09:23:59","title":"A first look into Utiq: Next-generation cookies at the ISP level","abstract":"Online privacy has become increasingly important in recent years. While third-party cookies have been widely used for years, they have also been criticized for their potential impact on user privacy. They can be used by advertisers to track users across multiple sites, allowing them to build detailed profiles of their behavior and interests. However, nowadays, many browsers allow users to block third-party cookies, which limits their usefulness for advertisers. In this paper, we take a first look at Utiq, a new way of user tracking performed directly by the ISP, to substitute the third-party cookies used until now. We study the main properties of this new identification methodology and their adoption on the 10K most popular websites. Our results show that, although still marginal due to the restrictions imposed by the system, between 0.7% and 1.2% of websites already include Utiq as one of their user identification methods.","sentences":["Online privacy has become increasingly important in recent years.","While third-party cookies have been widely used for years, they have also been criticized for their potential impact on user privacy.","They can be used by advertisers to track users across multiple sites, allowing them to build detailed profiles of their behavior and interests.","However, nowadays, many browsers allow users to block third-party cookies, which limits their usefulness for advertisers.","In this paper, we take a first look at Utiq, a new way of user tracking performed directly by the ISP, to substitute the third-party cookies used until now.","We study the main properties of this new identification methodology and their adoption on the 10K most popular websites.","Our results show that, although still marginal due to the restrictions imposed by the system, between 0.7% and 1.2% of websites already include Utiq as one of their user identification methods."],"url":"http://arxiv.org/abs/2405.09205v1","category":"cs.CR"}
{"created":"2024-05-15 09:21:41","title":"Strain-Induced Intrinsic Antiferromagnetic Skyrmions in Two-Dimensional Janus Magnets","abstract":"Antiferromagnetic (AFM) skyrmions, which are resistant to both the skyrmion Hall effect and external magnetic perturbations, are expected to be promising candidates for next-generation spintronics devices. Despite being observed in bulk materials and synthetic AFM layered systems, the existence of intrinsic AFM skyrmions within single magnetic layers, which offer potential advantages for spintronic device fabrication, has remained elusive. In this work, taking monolayer CrSi(Te,Se)$_{3}$ as a representative system, we demonstrate the emergence of intrinsic AFM skyrmions in two-dimensional Janus magnets. It is found that under moderate compressive strain, the interplay between considerable Dyzaloshinskii-Moriya interaction and the strain-induced AFM Heisenberg exchange interaction in monolayer CrSi(Te,Se)$_{3}$ would give rise to the emergence of intrinsic AFM skyrmions assembled from AFM spin spirals. Moreover, the application of an external magnetic field could trigger the emergence of AFM merons as well as a canted AFM state. Our findings propose a feasible approach for achieving intrinsic AFM skyrmions in realistic systems, which paves the way for developments in AFM topological spintronics devices.","sentences":["Antiferromagnetic (AFM) skyrmions, which are resistant to both the skyrmion Hall effect and external magnetic perturbations, are expected to be promising candidates for next-generation spintronics devices.","Despite being observed in bulk materials and synthetic AFM layered systems, the existence of intrinsic AFM skyrmions within single magnetic layers, which offer potential advantages for spintronic device fabrication, has remained elusive.","In this work, taking monolayer CrSi(Te,Se)$_{3}$ as a representative system, we demonstrate the emergence of intrinsic AFM skyrmions in two-dimensional Janus magnets.","It is found that under moderate compressive strain, the interplay between considerable Dyzaloshinskii-Moriya interaction and the strain-induced AFM Heisenberg exchange interaction in monolayer CrSi(Te,Se)$_{3}$ would give rise to the emergence of intrinsic AFM skyrmions assembled from AFM spin spirals.","Moreover, the application of an external magnetic field could trigger the emergence of AFM merons as well as a canted AFM state.","Our findings propose a feasible approach for achieving intrinsic AFM skyrmions in realistic systems, which paves the way for developments in AFM topological spintronics devices."],"url":"http://arxiv.org/abs/2405.09202v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-15 09:15:48","title":"Performance Analysis of RIS-aided MISO Systems with EMI and Channel Aging","abstract":"In this paper, we investigate a reconfigurable intelligent surface (RIS)-aided multiple-input single-output (MISO) system in the presence of electromagnetic interference (EMI) and channel aging with a Rician fading channel model between the base station (BS) and user equipment (UE). Specifically, we derive the closed-form expression for downlink spectral efficiency (SE) with maximum ratio transmission (MRT) precoding. The Monte-Carlo simulation supports the theoretical results, demonstrating that amplifying the weight of the line-of-sight (LoS) component in Rician fading channels can boost SE, while EMI has a detrimental impact. Furthermore, continuously increasing the number of RIS elements is not an optimal choice when EMI exists. Nonetheless, RIS can be deployed to compensate for SE degradation caused by channel aging effects. Finally, enlarging the RIS elements size can significantly improve system performance.","sentences":["In this paper, we investigate a reconfigurable intelligent surface (RIS)-aided multiple-input single-output (MISO) system in the presence of electromagnetic interference (EMI) and channel aging with a Rician fading channel model between the base station (BS) and user equipment (UE).","Specifically, we derive the closed-form expression for downlink spectral efficiency (SE) with maximum ratio transmission (MRT) precoding.","The Monte-Carlo simulation supports the theoretical results, demonstrating that amplifying the weight of the line-of-sight (LoS) component in Rician fading channels can boost SE, while EMI has a detrimental impact.","Furthermore, continuously increasing the number of RIS elements is not an optimal choice when EMI exists.","Nonetheless, RIS can be deployed to compensate for SE degradation caused by channel aging effects.","Finally, enlarging the RIS elements size can significantly improve system performance."],"url":"http://arxiv.org/abs/2405.09200v1","category":"cs.IT"}
{"created":"2024-05-15 09:13:14","title":"Particle transport based study of nucleation in a ferromagnetic three-state spin system with conservative dynamics","abstract":"We pose the problem of metastability for a three--state spin system with conservative dynamics. We consider the Blume--Capel model with the Kawasaki dynamics, we prove that, in a particular region of the parameter plane, the metastable state is the unique homogeneous minus state, and we estimate the exit time. To achieve our goal we have to solve several variational problems in the configuration space which result to be particularly involved, due to complicated structure of the trajectories. They key ingredient is the control of the energy differences between the configurations crossed when a spin is transported from the boundary to an internal site of the lattice through a completely arbitrary mixture of the three--state spin species. To master these mechanisms we have introduced a new approach based on the transport of spins along nearest neighbor connected regions of the lattice with constant spin configuration. This novel approach goes beyond the Blume--Capel model and can be used for the study of more general multi--state spin models.","sentences":["We pose the problem of metastability for a three--state spin system with conservative dynamics.","We consider the Blume--Capel model with the Kawasaki dynamics, we prove that, in a particular region of the parameter plane, the metastable state is the unique homogeneous minus state, and we estimate the exit time.","To achieve our goal we have to solve several variational problems in the configuration space which result to be particularly involved, due to complicated structure of the trajectories.","They key ingredient is the control of the energy differences between the configurations crossed when a spin is transported from the boundary to an internal site of the lattice through a completely arbitrary mixture of the three--state spin species.","To master these mechanisms we have introduced a new approach based on the transport of spins along nearest neighbor connected regions of the lattice with constant spin configuration.","This novel approach goes beyond the Blume--Capel model and can be used for the study of more general multi--state spin models."],"url":"http://arxiv.org/abs/2405.09198v1","category":"math.PR"}
{"created":"2024-05-15 09:09:35","title":"Parallel and Proximal Linear-Quadratic Methods for Real-Time Constrained Model-Predictive Control","abstract":"-Recent strides in model predictive control (MPC)underscore a dependence on numerical advancements to efficientlyand accurately solve large-scale problems. Given the substantialnumber of variables characterizing typical whole-body optimalcontrol (OC) problems -often numbering in the thousands-exploiting the sparse structure of the numerical problem becomescrucial to meet computational demands, typically in the range ofa few milliseconds. A fundamental building block for computingNewton or Sequential Quadratic Programming (SQP) steps indirect optimal control methods involves addressing the linearquadratic regulator (LQR) problem. This paper concentrateson equality-constrained problems featuring implicit systemdynamics and dual regularization, a characteristic found inadvanced interior-point or augmented Lagrangian solvers. Here,we introduce a parallel algorithm designed for solving an LQRproblem with dual regularization. Leveraging a rewriting of theLQR recursion through block elimination, we first enhanced theefficiency of the serial algorithm, then subsequently generalized itto handle parametric problems. This extension enables us to splitdecision variables and solve multiple subproblems concurrently.Our algorithm is implemented in our nonlinear numerical optimalcontrol library ALIGATOR. It showcases improved performanceover previous serial formulations and we validate its efficacy bydeploying it in the model predictive control of a real quadrupedrobot. This paper follows up from our prior work on augmentedLagrangian methods for numerical optimal control with implicitdynamics and constraints.","sentences":["-Recent strides in model predictive control (MPC)underscore a dependence on numerical advancements to efficientlyand accurately solve large-scale problems.","Given the substantialnumber of variables characterizing typical whole-body optimalcontrol (OC) problems -often numbering in the thousands-exploiting the sparse structure of the numerical problem becomescrucial to meet computational demands, typically in the range ofa few milliseconds.","A fundamental building block for computingNewton or Sequential Quadratic Programming (SQP) steps indirect optimal control methods involves addressing the linearquadratic regulator (LQR) problem.","This paper concentrateson equality-constrained problems featuring implicit systemdynamics and dual regularization, a characteristic found inadvanced interior-point or augmented Lagrangian solvers.","Here,we introduce a parallel algorithm designed for solving an LQRproblem with dual regularization.","Leveraging a rewriting of theLQR recursion through block elimination, we first enhanced theefficiency of the serial algorithm, then subsequently generalized itto handle parametric problems.","This extension enables us to splitdecision variables and solve multiple subproblems concurrently.","Our algorithm is implemented in our nonlinear numerical optimalcontrol library ALIGATOR.","It showcases improved performanceover previous serial formulations and we validate its efficacy bydeploying it in the model predictive control of a real quadrupedrobot.","This paper follows up from our prior work on augmentedLagrangian methods for numerical optimal control with implicitdynamics and constraints."],"url":"http://arxiv.org/abs/2405.09197v1","category":"cs.RO"}
{"created":"2024-05-15 09:02:17","title":"Flexible image analysis for law enforcement agencies with deep neural networks to determine: where, who and what","abstract":"Due to the increasing need for effective security measures and the integration of cameras in commercial products, a hugeamount of visual data is created today. Law enforcement agencies (LEAs) are inspecting images and videos to findradicalization, propaganda for terrorist organizations and illegal products on darknet markets. This is time consuming.Instead of an undirected search, LEAs would like to adapt to new crimes and threats, and focus only on data from specificlocations, persons or objects, which requires flexible interpretation of image content. Visual concept detection with deepconvolutional neural networks (CNNs) is a crucial component to understand the image content. This paper has fivecontributions. The first contribution allows image-based geo-localization to estimate the origin of an image. CNNs andgeotagged images are used to create a model that determines the location of an image by its pixel values. The secondcontribution enables analysis of fine-grained concepts to distinguish sub-categories in a generic concept. The proposedmethod encompasses data acquisition and cleaning and concept hierarchies. The third contribution is the recognition ofperson attributes (e.g., glasses or moustache) to enable query by textual description for a person. The person-attributeproblem is treated as a specific sub-task of concept classification. The fourth contribution is an intuitive image annotationtool based on active learning. Active learning allows users to define novel concepts flexibly and train CNNs with minimalannotation effort. The fifth contribution increases the flexibility for LEAs in the query definition by using query expansion.Query expansion maps user queries to known and detectable concepts. Therefore, no prior knowledge of the detectableconcepts is required for the users. The methods are validated on data with varying locations (popular and non-touristiclocations), varying person attributes (CelebA dataset), and varying number of annotations.","sentences":["Due to the increasing need for effective security measures and the integration of cameras in commercial products, a hugeamount of visual data is created today.","Law enforcement agencies (LEAs) are inspecting images and videos to findradicalization, propaganda for terrorist organizations and illegal products on darknet markets.","This is time consuming.","Instead of an undirected search, LEAs would like to adapt to new crimes and threats, and focus only on data from specificlocations, persons or objects, which requires flexible interpretation of image content.","Visual concept detection with deepconvolutional neural networks (CNNs) is a crucial component to understand the image content.","This paper has fivecontributions.","The first contribution allows image-based geo-localization to estimate the origin of an image.","CNNs andgeotagged images are used to create a model that determines the location of an image by its pixel values.","The secondcontribution enables analysis of fine-grained concepts to distinguish sub-categories in a generic concept.","The proposedmethod encompasses data acquisition and cleaning and concept hierarchies.","The third contribution is the recognition ofperson attributes (e.g., glasses or moustache) to enable query by textual description for a person.","The person-attributeproblem is treated as a specific sub-task of concept classification.","The fourth contribution is an intuitive image annotationtool based on active learning.","Active learning allows users to define novel concepts flexibly and train CNNs with minimalannotation effort.","The fifth contribution increases the flexibility for LEAs in the query definition by using query expansion.","Query expansion maps user queries to known and detectable concepts.","Therefore, no prior knowledge of the detectableconcepts is required for the users.","The methods are validated on data with varying locations (popular and non-touristiclocations), varying person attributes (CelebA dataset), and varying number of annotations."],"url":"http://arxiv.org/abs/2405.09194v1","category":"cs.CV"}
{"created":"2024-05-15 09:01:54","title":"Autonomous Cooperative Levels of Multiple-Heterogeneous Unmanned Vehicle Systems","abstract":"As multiple and heterogenous unmanned vehicle systems continue to play an increasingly important role in addressing complex missions in the real world, the need for effective cooperation among unmanned vehicles becomes paramount. The concept of autonomous cooperation, wherein unmanned vehicles cooperate without human intervention or human control, offers promising avenues for enhancing the efficiency and adaptability of intelligence of multiple-heterogeneous unmanned vehicle systems. Despite the growing interests in this domain, as far as the authors are concerned, there exists a notable lack of comprehensive literature on defining explicit concept and classifying levels of autonomous cooperation of multiple-heterogeneous unmanned vehicle systems. In this aspect, this article aims to define the explicit concept of autonomous cooperation of multiple-heterogeneous unmanned vehicle systems. Furthermore, we provide a novel criterion to assess the technical maturity of the developed unmanned vehicle systems by classifying the autonomous cooperative levels of multiple-heterogeneous unmanned vehicle systems.","sentences":["As multiple and heterogenous unmanned vehicle systems continue to play an increasingly important role in addressing complex missions in the real world, the need for effective cooperation among unmanned vehicles becomes paramount.","The concept of autonomous cooperation, wherein unmanned vehicles cooperate without human intervention or human control, offers promising avenues for enhancing the efficiency and adaptability of intelligence of multiple-heterogeneous unmanned vehicle systems.","Despite the growing interests in this domain, as far as the authors are concerned, there exists a notable lack of comprehensive literature on defining explicit concept and classifying levels of autonomous cooperation of multiple-heterogeneous unmanned vehicle systems.","In this aspect, this article aims to define the explicit concept of autonomous cooperation of multiple-heterogeneous unmanned vehicle systems.","Furthermore, we provide a novel criterion to assess the technical maturity of the developed unmanned vehicle systems by classifying the autonomous cooperative levels of multiple-heterogeneous unmanned vehicle systems."],"url":"http://arxiv.org/abs/2405.09193v1","category":"eess.SY"}
{"created":"2024-05-15 08:53:47","title":"Advancing Explainable AI with Causal Analysis in Large-Scale Fuzzy Cognitive Maps","abstract":"In the quest for accurate and interpretable AI models, eXplainable AI (XAI) has become crucial. Fuzzy Cognitive Maps (FCMs) stand out as an advanced XAI method because of their ability to synergistically combine and exploit both expert knowledge and data-driven insights, providing transparency and intrinsic interpretability. This letter introduces and investigates the \"Total Causal Effect Calculation for FCMs\" (TCEC-FCM) algorithm, an innovative approach that, for the first time, enables the efficient calculation of total causal effects among concepts in large-scale FCMs by leveraging binary search and graph traversal techniques, thereby overcoming the challenge of exhaustive causal path exploration that hinder existing methods. We evaluate the proposed method across various synthetic FCMs that demonstrate TCEC-FCM's superior performance over exhaustive methods, marking a significant advancement in causal effect analysis within FCMs, thus broadening their usability for modern complex XAI applications.","sentences":["In the quest for accurate and interpretable AI models, eXplainable AI (XAI) has become crucial.","Fuzzy Cognitive Maps (FCMs) stand out as an advanced XAI method because of their ability to synergistically combine and exploit both expert knowledge and data-driven insights, providing transparency and intrinsic interpretability.","This letter introduces and investigates the \"Total Causal Effect Calculation for FCMs\" (TCEC-FCM) algorithm, an innovative approach that, for the first time, enables the efficient calculation of total causal effects among concepts in large-scale FCMs by leveraging binary search and graph traversal techniques, thereby overcoming the challenge of exhaustive causal path exploration that hinder existing methods.","We evaluate the proposed method across various synthetic FCMs that demonstrate TCEC-FCM's superior performance over exhaustive methods, marking a significant advancement in causal effect analysis within FCMs, thus broadening their usability for modern complex XAI applications."],"url":"http://arxiv.org/abs/2405.09190v1","category":"cs.AI"}
{"created":"2024-05-15 08:53:20","title":"Two-sided tilting complexes for generalized Brauer tree algebras","abstract":"We explicitly construct two-sided tilting complexes corresponding to Membrillo-Hern\\'{a}ndez's tree-to-star tilting complexes for generalized Brauer tree algebras.","sentences":["We explicitly construct two-sided tilting complexes corresponding to Membrillo-Hern\\'{a}ndez's tree-to-star tilting complexes for generalized Brauer tree algebras."],"url":"http://arxiv.org/abs/2405.09188v1","category":"math.RA"}
{"created":"2024-05-15 08:47:26","title":"HumanRankEval: Automatic Evaluation of LMs as Conversational Assistants","abstract":"Language models (LMs) as conversational assistants recently became popular tools that help people accomplish a variety of tasks. These typically result from adapting LMs pretrained on general domain text sequences through further instruction-tuning and possibly preference optimisation methods. The evaluation of such LMs would ideally be performed using human judgement, however, this is not scalable. On the other hand, automatic evaluation featuring auxiliary LMs as judges and/or knowledge-based tasks is scalable but struggles with assessing conversational ability and adherence to instructions. To help accelerate the development of LMs as conversational assistants, we propose a novel automatic evaluation task: HumanRankEval (HRE). It consists of a large-scale, diverse and high-quality set of questions, each with several answers authored and scored by humans. To perform evaluation, HRE ranks these answers based on their log-likelihood under the LM's distribution, and subsequently calculates their correlation with the corresponding human rankings. We support HRE's efficacy by investigating how efficiently it separates pretrained and instruction-tuned LMs of various sizes. We show that HRE correlates well with human judgements and is particularly responsive to model changes following instruction-tuning.","sentences":["Language models (LMs) as conversational assistants recently became popular tools that help people accomplish a variety of tasks.","These typically result from adapting LMs pretrained on general domain text sequences through further instruction-tuning and possibly preference optimisation methods.","The evaluation of such LMs would ideally be performed using human judgement, however, this is not scalable.","On the other hand, automatic evaluation featuring auxiliary LMs as judges and/or knowledge-based tasks is scalable but struggles with assessing conversational ability and adherence to instructions.","To help accelerate the development of LMs as conversational assistants, we propose a novel automatic evaluation task: HumanRankEval (HRE).","It consists of a large-scale, diverse and high-quality set of questions, each with several answers authored and scored by humans.","To perform evaluation, HRE ranks these answers based on their log-likelihood under the LM's distribution, and subsequently calculates their correlation with the corresponding human rankings.","We support HRE's efficacy by investigating how efficiently it separates pretrained and instruction-tuned LMs of various sizes.","We show that HRE correlates well with human judgements and is particularly responsive to model changes following instruction-tuning."],"url":"http://arxiv.org/abs/2405.09186v1","category":"cs.CL"}
{"created":"2024-05-15 08:40:29","title":"StateGuard: Detecting State Derailment Defects in Decentralized Exchange Smart Contract","abstract":"Decentralized Exchanges (DEXs), leveraging blockchain technology and smart contracts, have emerged in decentralized finance. However, the DEX project with multi-contract interaction is accompanied by complex state logic, which makes it challenging to solve state defects. In this paper, we conduct the first systematic study on state derailment defects of DEXs. These defects could lead to incorrect, incomplete, or unauthorized changes to the system state during contract execution, potentially causing security threats. We propose StateGuard, a deep learning-based framework to detect state derailment defects in DEX smart contracts. StateGuard constructs an Abstract Syntax Tree (AST) of the smart contract, extracting key features to generate a graph representation. Then, it leverages a Graph Convolutional Network (GCN) to discover defects. Evaluating StateGuard on 46 DEX projects with 5,671 smart contracts reveals its effectiveness, with a precision of 92.24%. To further verify its practicality, we used StateGuard to audit real-world smart contracts and successfully authenticated multiple novel CVEs.","sentences":["Decentralized Exchanges (DEXs), leveraging blockchain technology and smart contracts, have emerged in decentralized finance.","However, the DEX project with multi-contract interaction is accompanied by complex state logic, which makes it challenging to solve state defects.","In this paper, we conduct the first systematic study on state derailment defects of DEXs.","These defects could lead to incorrect, incomplete, or unauthorized changes to the system state during contract execution, potentially causing security threats.","We propose StateGuard, a deep learning-based framework to detect state derailment defects in DEX smart contracts.","StateGuard constructs an Abstract Syntax Tree (AST) of the smart contract, extracting key features to generate a graph representation.","Then, it leverages a Graph Convolutional Network (GCN) to discover defects.","Evaluating StateGuard on 46 DEX projects with 5,671 smart contracts reveals its effectiveness, with a precision of 92.24%.","To further verify its practicality, we used StateGuard to audit real-world smart contracts and successfully authenticated multiple novel CVEs."],"url":"http://arxiv.org/abs/2405.09181v1","category":"cs.SE"}
{"created":"2024-05-15 08:38:37","title":"Integrated and DC-powered superconducting microcomb","abstract":"Frequency combs, specialized laser sources emitting multiple equidistant frequency lines, have revolutionized science and technology with unprecedented precision and versatility. Recently, integrated frequency combs are emerging as scalable solutions for on-chip photonics. Here, we demonstrate a fully integrated superconducting microcomb that is easy to manufacture, simple to operate, and consumes ultra-low power. Our turnkey apparatus comprises a basic nonlinear superconducting device, a Josephson junction, directly coupled to a superconducting microstrip resonator. We showcase coherent comb generation through self-started mode-locking. Therefore, comb emission is initiated solely by activating a DC bias source, with power consumption as low as tens of picowatts. The resulting comb spectrum resides in the microwave domain and spans multiple octaves. The linewidths of all comb lines can be narrowed down to 1 Hz through a unique coherent injection-locking technique. Our work represents a critical step towards fully integrated microwave photonics and offers the potential for integrated quantum processors.","sentences":["Frequency combs, specialized laser sources emitting multiple equidistant frequency lines, have revolutionized science and technology with unprecedented precision and versatility.","Recently, integrated frequency combs are emerging as scalable solutions for on-chip photonics.","Here, we demonstrate a fully integrated superconducting microcomb that is easy to manufacture, simple to operate, and consumes ultra-low power.","Our turnkey apparatus comprises a basic nonlinear superconducting device, a Josephson junction, directly coupled to a superconducting microstrip resonator.","We showcase coherent comb generation through self-started mode-locking.","Therefore, comb emission is initiated solely by activating a DC bias source, with power consumption as low as tens of picowatts.","The resulting comb spectrum resides in the microwave domain and spans multiple octaves.","The linewidths of all comb lines can be narrowed down to 1 Hz through a unique coherent injection-locking technique.","Our work represents a critical step towards fully integrated microwave photonics and offers the potential for integrated quantum processors."],"url":"http://arxiv.org/abs/2405.09180v1","category":"cond-mat.supr-con"}
{"created":"2024-05-15 08:37:41","title":"Integrated Sensing and Communication Enabled Cooperative Passive Sensing Using Mobile Communication System","abstract":"Integrated sensing and communication (ISAC) is a potential technology of the sixth-generation (6G) mobile communication system, which enables communication base station (BS) with sensing capability. However, the performance of single-BS sensing is limited, which can be overcome by multi-BS cooperative sensing. There are three types of multi-BS cooperative sensing, including cooperative active sensing, cooperative passive sensing, and cooperative active and passive sensing, where the multi-BS cooperative passive sensing has the advantages of low hardware modification cost and large sensing coverage. However, multi-BS cooperative passive sensing faces the challenges of synchronization offsets mitigation and sensing information fusion. To address these challenges, a non-line of sight (NLoS) and line of sight (LoS) signal cross-correlation (NLCC) method is proposed to mitigate carrier frequency offset (CFO) and time offset (TO). Besides, a symbol-level multi-BS sensing information fusion method is proposed. The discrete samplings of echo signals from multiple BSs are matched independently and coherent accumulated to improve sensing accuracy. Moreover, a lowcomplexity joint angle-of-arrival (AoA) and angle-of-departure (AoD) estimation method is proposed to reduce the computational complexity. Simulation results show that symbol-level multi-BS cooperative passive sensing scheme has an order of magnitude higher sensing accuracy than single-BS passive sensing. This work provides a reference for the research on multi-BS cooperative passive sensing.","sentences":["Integrated sensing and communication (ISAC) is a potential technology of the sixth-generation (6G) mobile communication system, which enables communication base station (BS) with sensing capability.","However, the performance of single-BS sensing is limited, which can be overcome by multi-BS cooperative sensing.","There are three types of multi-BS cooperative sensing, including cooperative active sensing, cooperative passive sensing, and cooperative active and passive sensing, where the multi-BS cooperative passive sensing has the advantages of low hardware modification cost and large sensing coverage.","However, multi-BS cooperative passive sensing faces the challenges of synchronization offsets mitigation and sensing information fusion.","To address these challenges, a non-line of sight (NLoS) and line of sight (LoS) signal cross-correlation (NLCC) method is proposed to mitigate carrier frequency offset (CFO) and time offset (TO).","Besides, a symbol-level multi-BS sensing information fusion method is proposed.","The discrete samplings of echo signals from multiple BSs are matched independently and coherent accumulated to improve sensing accuracy.","Moreover, a lowcomplexity joint angle-of-arrival (AoA) and angle-of-departure (AoD) estimation method is proposed to reduce the computational complexity.","Simulation results show that symbol-level multi-BS cooperative passive sensing scheme has an order of magnitude higher sensing accuracy than single-BS passive sensing.","This work provides a reference for the research on multi-BS cooperative passive sensing."],"url":"http://arxiv.org/abs/2405.09179v1","category":"eess.SP"}
{"created":"2024-05-15 08:33:27","title":"Continuum emission from within the plunging region of black hole discs","abstract":"The thermal continuum emission observed from accreting black holes across X-ray bands has the potential to be leveraged as a powerful probe of the mass and spin of the central black hole. The vast majority of existing ``continuum fitting'' models neglect emission sourced at and within the innermost stable circular orbit (ISCO) of the black hole. Numerical simulations, however, find non-zero emission sourced from these regions. In this work we extend existing techniques by including the emission sourced from within the plunging region, utilising new analytical models which reproduce the properties of numerical accretion simulations. We show that in general the neglected intra-ISCO emission produces a hot-and-small quasi-blackbody component, but can also produce a weak power-law tail for more extreme parameter regions. A similar hot-and-small blackbody component has been added in by hand in an ad-hoc manner to previous analyses of X-ray binary spectra. We show that the X-ray spectrum of MAXI J1820+070 in a soft-state outburst is extremely well described by a full Kerr black hole disc, while conventional models which neglect intra-ISCO emission are unable to reproduce the data. We believe this represents the first robust detection of intra-ISCO emission in the literature, and allows additional constraints to be placed on the MAXI J1820+070 black hole spin which must be low $a_\\bullet < 0.5$ to allow a detectable intra-ISCO region. Emission from within the ISCO is the dominant emission component in the MAXI J1820+070 spectrum between $6$ and $10$ keV, highlighting the necessity of including this region. Our continuum fitting model is made publicly available.","sentences":["The thermal continuum emission observed from accreting black holes across X-ray bands has the potential to be leveraged as a powerful probe of the mass and spin of the central black hole.","The vast majority of existing ``continuum fitting'' models neglect emission sourced at and within the innermost stable circular orbit (ISCO) of the black hole.","Numerical simulations, however, find non-zero emission sourced from these regions.","In this work we extend existing techniques by including the emission sourced from within the plunging region, utilising new analytical models which reproduce the properties of numerical accretion simulations.","We show that in general the neglected intra-ISCO emission produces a hot-and-small quasi-blackbody component, but can also produce a weak power-law tail for more extreme parameter regions.","A similar hot-and-small blackbody component has been added in by hand in an ad-hoc manner to previous analyses of X-ray binary spectra.","We show that the X-ray spectrum of MAXI J1820+070 in a soft-state outburst is extremely well described by a full Kerr black hole disc, while conventional models which neglect intra-ISCO emission are unable to reproduce the data.","We believe this represents the first robust detection of intra-ISCO emission in the literature, and allows additional constraints to be placed on the MAXI J1820+070 black hole spin which must be low $a_\\bullet < 0.5$ to allow a detectable intra-ISCO region.","Emission from within the ISCO is the dominant emission component in the MAXI J1820+070 spectrum between $6$ and $10$ keV, highlighting the necessity of including this region.","Our continuum fitting model is made publicly available."],"url":"http://arxiv.org/abs/2405.09175v1","category":"astro-ph.HE"}
{"created":"2024-05-15 08:27:22","title":"Revealing Nonclassicality of Multiphoton Optical Beams via Artificial Neural Networks","abstract":"The identification of nonclassical features of multiphoton quantum states represents a task of the utmost importance in the development of many quantum photonic technologies. Under realistic experimental conditions, a photonic quantum state gets affected by its interaction with several nonideal opto-electronic devices, including those used to guide, detect or characterize it. The result of such noisy interaction is that the nonclassical features of the original quantum state get considerably reduced or are completely absent in the detected, final state. In this work, the self-learning features of artificial neural networks are exploited to experimentally show that the nonclassicality of multiphoton quantum states can be assessed and fully characterized, even in the cases in which the nonclassical features are concealed by the measuring devices. Our work paves the way toward artificial-intelligence-assisted experimental-setup characterization, as well as smart quantum-state nonclassicality identification.","sentences":["The identification of nonclassical features of multiphoton quantum states represents a task of the utmost importance in the development of many quantum photonic technologies.","Under realistic experimental conditions, a photonic quantum state gets affected by its interaction with several nonideal opto-electronic devices, including those used to guide, detect or characterize it.","The result of such noisy interaction is that the nonclassical features of the original quantum state get considerably reduced or are completely absent in the detected, final state.","In this work, the self-learning features of artificial neural networks are exploited to experimentally show that the nonclassicality of multiphoton quantum states can be assessed and fully characterized, even in the cases in which the nonclassical features are concealed by the measuring devices.","Our work paves the way toward artificial-intelligence-assisted experimental-setup characterization, as well as smart quantum-state nonclassicality identification."],"url":"http://arxiv.org/abs/2405.09174v1","category":"quant-ph"}
{"created":"2024-05-15 08:21:56","title":"Hierarchical Emotion Prediction and Control in Text-to-Speech Synthesis","abstract":"It remains a challenge to effectively control the emotion rendering in text-to-speech (TTS) synthesis. Prior studies have primarily focused on learning a global prosodic representation at the utterance level, which strongly correlates with linguistic prosody. Our goal is to construct a hierarchical emotion distribution (ED) that effectively encapsulates intensity variations of emotions at various levels of granularity, encompassing phonemes, words, and utterances. During TTS training, the hierarchical ED is extracted from the ground-truth audio and guides the predictor to establish a connection between emotional and linguistic prosody. At run-time inference, the TTS model generates emotional speech and, at the same time, provides quantitative control of emotion over the speech constituents. Both objective and subjective evaluations validate the effectiveness of the proposed framework in terms of emotion prediction and control.","sentences":["It remains a challenge to effectively control the emotion rendering in text-to-speech (TTS) synthesis.","Prior studies have primarily focused on learning a global prosodic representation at the utterance level, which strongly correlates with linguistic prosody.","Our goal is to construct a hierarchical emotion distribution (ED) that effectively encapsulates intensity variations of emotions at various levels of granularity, encompassing phonemes, words, and utterances.","During TTS training, the hierarchical ED is extracted from the ground-truth audio and guides the predictor to establish a connection between emotional and linguistic prosody.","At run-time inference, the TTS model generates emotional speech and, at the same time, provides quantitative control of emotion over the speech constituents.","Both objective and subjective evaluations validate the effectiveness of the proposed framework in terms of emotion prediction and control."],"url":"http://arxiv.org/abs/2405.09171v1","category":"cs.SD"}
{"created":"2024-05-15 08:18:31","title":"Tunable superconducting resonators via on-chip control of local magnetic field","abstract":"Superconducting microwave resonators play a pivotal role in superconducting quantum circuits. The ability to fine-tune their resonant frequencies provides enhanced control and flexibility. Here, we introduce a frequency-tunable superconducting coplanar waveguide resonator. By applying electrical currents through specifically designed ground wires, we achieve the generation and control of a localized magnetic field on the central line of the resonator, enabling continuous tuning of its resonant frequency. We demonstrate a frequency tuning range of 54.85 MHz in a 6.21 GHz resonator. This integrated and tunable resonator holds great potential as a dynamically tunable filter and as a key component of communication buses and memory elements in superconducting quantum computing.","sentences":["Superconducting microwave resonators play a pivotal role in superconducting quantum circuits.","The ability to fine-tune their resonant frequencies provides enhanced control and flexibility.","Here, we introduce a frequency-tunable superconducting coplanar waveguide resonator.","By applying electrical currents through specifically designed ground wires, we achieve the generation and control of a localized magnetic field on the central line of the resonator, enabling continuous tuning of its resonant frequency.","We demonstrate a frequency tuning range of 54.85 MHz in a 6.21 GHz resonator.","This integrated and tunable resonator holds great potential as a dynamically tunable filter and as a key component of communication buses and memory elements in superconducting quantum computing."],"url":"http://arxiv.org/abs/2405.09170v1","category":"cond-mat.supr-con"}
{"created":"2024-05-15 08:03:33","title":"On the effect of derivative interactions in quantum field theory","abstract":"There exist several good reasons why one may wish to add a total derivative to an interaction in quantum field theory, e.g., in order to improve the perturbative construction. Unlike in classical field theory, adding derivatives in general changes the theory. The analysis whether and how this can be prevented, is presently limited to perturbative orders $g^n$ , $n \\leq 3$. We drastically simplify it by an all-orders formula, which also allows to answer some salient structural questions. The method is part of a larger program to (re)derive interactions of particles by quantum consistency conditions, rather than a classical principle of gauge invariance.","sentences":["There exist several good reasons why one may wish to add a total derivative to an interaction in quantum field theory, e.g., in order to improve the perturbative construction.","Unlike in classical field theory, adding derivatives in general changes the theory.","The analysis whether and how this can be prevented, is presently limited to perturbative orders $g^n$ , $n \\leq 3$.","We drastically simplify it by an all-orders formula, which also allows to answer some salient structural questions.","The method is part of a larger program to (re)derive interactions of particles by quantum consistency conditions, rather than a classical principle of gauge invariance."],"url":"http://arxiv.org/abs/2405.09168v1","category":"hep-th"}
{"created":"2024-05-15 08:02:55","title":"Emperical Study on the Effect of Multi-Sampling in the Prediction Step of the Particle Filter","abstract":"Particle filters are applicable to a wide range of nonlinear, non-Gaussian state-space models and have already been applied to a variety of problems. However, there is a problem in the calculation of smoothed distributions, where particles gradually degenerate and accuracy is reduced. The purpose of this paper is to consider the possibility of generating multiple particles in the prediction step of the particle filter and to empirically verify the effect using real data.","sentences":["Particle filters are applicable to a wide range of nonlinear, non-Gaussian state-space models and have already been applied to a variety of problems.","However, there is a problem in the calculation of smoothed distributions, where particles gradually degenerate and accuracy is reduced.","The purpose of this paper is to consider the possibility of generating multiple particles in the prediction step of the particle filter and to empirically verify the effect using real data."],"url":"http://arxiv.org/abs/2405.09167v1","category":"stat.CO"}
{"created":"2024-05-15 07:50:57","title":"Rapidly Achieving Chemical Accuracy with Quantum Computing Enforced Language Model","abstract":"Finding accurate ground state energy of a many-body system has been a major challenge in quantum chemistry. The integration of classic and quantum computers has shed new light on resolving this outstanding problem. Here we propose QiankunNet-VQE, a transformer based language models enforced with quantum computing to learn and generate quantum states. It has been implemented using up to 12 qubits and attaining an accuracy level competitive with state-of-the-art classical methods. By leveraging both quantum and classical resources, this scheme overcomes the limitations of variational quantum eigensolver(VQE) without the need for cumbersome error mitigation. Moreover, QiankunNet-VQE provides a different route to achieve a practical quantum advantage for solving many-electron Schr\\\"odinger equation without requiring extremely precise preparation and measurement of the ground-state wavefunction on quantum computer.","sentences":["Finding accurate ground state energy of a many-body system has been a major challenge in quantum chemistry.","The integration of classic and quantum computers has shed new light on resolving this outstanding problem.","Here we propose QiankunNet-VQE, a transformer based language models enforced with quantum computing to learn and generate quantum states.","It has been implemented using up to 12 qubits and attaining an accuracy level competitive with state-of-the-art classical methods.","By leveraging both quantum and classical resources, this scheme overcomes the limitations of variational quantum eigensolver(VQE) without the need for cumbersome error mitigation.","Moreover, QiankunNet-VQE provides a different route to achieve a practical quantum advantage for solving many-electron Schr\\\"odinger equation without requiring extremely precise preparation and measurement of the ground-state wavefunction on quantum computer."],"url":"http://arxiv.org/abs/2405.09164v1","category":"quant-ph"}
{"created":"2024-05-15 07:48:10","title":"Exploring the Potential of Large Language Models for Automation in Technical Customer Service","abstract":"Purpose: The purpose of this study is to investigate the potential of Large Language Models (LLMs) in transforming technical customer service (TCS) through the automation of cognitive tasks. Design/Methodology/Approach: Using a prototyping approach, the research assesses the feasibility of automating cognitive tasks in TCS with LLMs, employing real-world technical incident data from a Swiss telecommunications operator. Findings: Lower-level cognitive tasks such as translation, summarization, and content generation can be effectively automated with LLMs like GPT-4, while higher-level tasks such as reasoning require more advanced technological approaches such as Retrieval-Augmented Generation (RAG) or finetuning ; furthermore, the study underscores the significance of data ecosystems in enabling more complex cognitive tasks by fostering data sharing among various actors involved. Originality/Value: This study contributes to the emerging theory on LLM potential and technical feasibility in service management, providing concrete insights for operators of TCS units and highlighting the need for further research to address limitations and validate the applicability of LLMs across different domains.","sentences":["Purpose: The purpose of this study is to investigate the potential of Large Language Models (LLMs) in transforming technical customer service (TCS) through the automation of cognitive tasks.","Design/Methodology/Approach: Using a prototyping approach, the research assesses the feasibility of automating cognitive tasks in TCS with LLMs, employing real-world technical incident data from a Swiss telecommunications operator.","Findings: Lower-level cognitive tasks such as translation, summarization, and content generation can be effectively automated with LLMs like GPT-4, while higher-level tasks such as reasoning require more advanced technological approaches such as Retrieval-Augmented Generation (RAG) or finetuning ; furthermore, the study underscores the significance of data ecosystems in enabling more complex cognitive tasks by fostering data sharing among various actors involved.","Originality/Value: This study contributes to the emerging theory on LLM potential and technical feasibility in service management, providing concrete insights for operators of TCS units and highlighting the need for further research to address limitations and validate the applicability of LLMs across different domains."],"url":"http://arxiv.org/abs/2405.09161v1","category":"econ.GN"}
{"created":"2024-05-15 07:43:36","title":"A Primal-Dual Framework for Symmetric Cone Programming","abstract":"In this paper, we introduce a primal-dual algorithmic framework for solving Symmetric Cone Programs (SCPs), a versatile optimization model that unifies and extends Linear, Second-Order Cone (SOCP), and Semidefinite Programming (SDP). Our work generalizes the primal-dual framework for SDPs introduced by Arora and Kale, leveraging a recent extension of the Multiplicative Weights Update method (MWU) to symmetric cones. Going beyond existing works, our framework can handle SOCPs and mixed SCPs, exhibits nearly linear time complexity, and can be effectively parallelized. To illustrate the efficacy of our framework, we employ it to develop approximation algorithms for two geometric optimization problems: the Smallest Enclosing Sphere problem and the Support Vector Machine problem. Our theoretical analyses demonstrate that the two algorithms compute approximate solutions in nearly linear running time and with parallel depth scaling polylogarithmically with the input size. We compare our algorithms against CGAL as well as interior point solvers applied to these problems. Experiments show that our algorithms are highly efficient when implemented on a CPU and achieve substantial speedups when parallelized on a GPU, allowing us to solve large-scale instances of these problems.","sentences":["In this paper, we introduce a primal-dual algorithmic framework for solving Symmetric Cone Programs (SCPs), a versatile optimization model that unifies and extends Linear, Second-Order Cone (SOCP), and Semidefinite Programming (SDP).","Our work generalizes the primal-dual framework for SDPs introduced by Arora and Kale, leveraging a recent extension of the Multiplicative Weights Update method (MWU) to symmetric cones.","Going beyond existing works, our framework can handle SOCPs and mixed SCPs, exhibits nearly linear time complexity, and can be effectively parallelized.","To illustrate the efficacy of our framework, we employ it to develop approximation algorithms for two geometric optimization problems: the Smallest Enclosing Sphere problem and the Support Vector Machine problem.","Our theoretical analyses demonstrate that the two algorithms compute approximate solutions in nearly linear running time and with parallel depth scaling polylogarithmically with the input size.","We compare our algorithms against CGAL as well as interior point solvers applied to these problems.","Experiments show that our algorithms are highly efficient when implemented on a CPU and achieve substantial speedups when parallelized on a GPU, allowing us to solve large-scale instances of these problems."],"url":"http://arxiv.org/abs/2405.09157v1","category":"math.OC"}
{"created":"2024-05-15 07:39:13","title":"TunnelSense: Low-power, Non-Contact Sensing using Tunnel Diodes","abstract":"Sensing the motion of physical objects in an environment enables numerous applications, from tracking occupancy in buildings and monitoring vital signs to diagnosing faults in machines. Typically, these application scenarios involve attaching a sensor, such as an accelerometer, to the object of interest, like a wearable device that tracks our steps. However, many of these scenarios require tracking motion in a noncontact manner where the sensor is not in touch with the object. A sensor in such a scenario observes variations in radio, light, acoustic, and infrared fields disturbed by the object's motion. Current noncontact sensing mechanisms often require substantial energy and involve complex processing on sophisticated hardware. We present TunnelSense, a novel mechanism that rethinks noncontact sensing using tunnel diode oscillators. They are highly sensitive to changes in their electromagnetic environments. The motion of an object near a tunnel diode oscillator induces corresponding changes in its resonant frequency and thus in the generated radio waves. Additionally, the low-power characteristics of the tunnel diode allow tags designed using them to operate on less than 100microwatt of power consumption and with a biasing voltage starting at 70 millivolt. This enables prolonged tag operation on a small battery or energy harvested from the environment. Among numerous applications enabled by the TunnelSense system, this work demonstrates its ability to detect breathing at distances up to 30 centimeter between the subject and the TunnelSense tag.","sentences":["Sensing the motion of physical objects in an environment enables numerous applications, from tracking occupancy in buildings and monitoring vital signs to diagnosing faults in machines.","Typically, these application scenarios involve attaching a sensor, such as an accelerometer, to the object of interest, like a wearable device that tracks our steps.","However, many of these scenarios require tracking motion in a noncontact manner where the sensor is not in touch with the object.","A sensor in such a scenario observes variations in radio, light, acoustic, and infrared fields disturbed by the object's motion.","Current noncontact sensing mechanisms often require substantial energy and involve complex processing on sophisticated hardware.","We present TunnelSense, a novel mechanism that rethinks noncontact sensing using tunnel diode oscillators.","They are highly sensitive to changes in their electromagnetic environments.","The motion of an object near a tunnel diode oscillator induces corresponding changes in its resonant frequency and thus in the generated radio waves.","Additionally, the low-power characteristics of the tunnel diode allow tags designed using them to operate on less than 100microwatt of power consumption and with a biasing voltage starting at 70 millivolt.","This enables prolonged tag operation on a small battery or energy harvested from the environment.","Among numerous applications enabled by the TunnelSense system, this work demonstrates its ability to detect breathing at distances up to 30 centimeter between the subject and the TunnelSense tag."],"url":"http://arxiv.org/abs/2405.09155v1","category":"cs.ET"}
{"created":"2024-05-15 07:31:15","title":"Thermodynamics and kinetics of state switching for the asymptotically flat black hole in a cavity","abstract":"We propose that the thermodynamics and the kinetics of state switching for the asymptotically flat black hole enclosed by a cavity can be studied in terms of the free energy landscape formalism. The generalized free energy for the black hole enclosed by a cavity in the canonical ensemble is derived by using the York's approach, where the temperature on the cavity and the charges inside the cavity are kept as the fixed parameters. By quantifying the corresponding free energy landscape, we obtain the phase diagrams for the black hole in cavity, which reveal a Hawking-Page type transition for the uncharged black hole and a Van der Waals type transition for the charged black hole. We further assume that the dynamics of black hole state switching is mutually determined by the gradient force and the stochastic force arising from the free energy landscape and the thermal noises respectively. We then derive a recurrence relation for the $n$-momentum of the first passage time distribution function, which enables the calculation of the kinetic times characterized by the mean first passage time and its relative fluctuation. Our analysis illustrates that the kinetics of black hole state switching is determined by the ensemble temperature and the barrier height on the free energy landscape.","sentences":["We propose that the thermodynamics and the kinetics of state switching for the asymptotically flat black hole enclosed by a cavity can be studied in terms of the free energy landscape formalism.","The generalized free energy for the black hole enclosed by a cavity in the canonical ensemble is derived by using the York's approach, where the temperature on the cavity and the charges inside the cavity are kept as the fixed parameters.","By quantifying the corresponding free energy landscape, we obtain the phase diagrams for the black hole in cavity, which reveal a Hawking-Page type transition for the uncharged black hole and a Van der Waals type transition for the charged black hole.","We further assume that the dynamics of black hole state switching is mutually determined by the gradient force and the stochastic force arising from the free energy landscape and the thermal noises respectively.","We then derive a recurrence relation for the $n$-momentum of the first passage time distribution function, which enables the calculation of the kinetic times characterized by the mean first passage time and its relative fluctuation.","Our analysis illustrates that the kinetics of black hole state switching is determined by the ensemble temperature and the barrier height on the free energy landscape."],"url":"http://arxiv.org/abs/2405.09151v1","category":"gr-qc"}
{"created":"2024-05-15 07:27:14","title":"Curriculum Dataset Distillation","abstract":"Most dataset distillation methods struggle to accommodate large-scale datasets due to their substantial computational and memory requirements. In this paper, we present a curriculum-based dataset distillation framework designed to harmonize scalability with efficiency. This framework strategically distills synthetic images, adhering to a curriculum that transitions from simple to complex. By incorporating curriculum evaluation, we address the issue of previous methods generating images that tend to be homogeneous and simplistic, doing so at a manageable computational cost. Furthermore, we introduce adversarial optimization towards synthetic images to further improve their representativeness and safeguard against their overfitting to the neural network involved in distilling. This enhances the generalization capability of the distilled images across various neural network architectures and also increases their robustness to noise. Extensive experiments demonstrate that our framework sets new benchmarks in large-scale dataset distillation, achieving substantial improvements of 11.1\\% on Tiny-ImageNet, 9.0\\% on ImageNet-1K, and 7.3\\% on ImageNet-21K. The source code will be released to the community.","sentences":["Most dataset distillation methods struggle to accommodate large-scale datasets due to their substantial computational and memory requirements.","In this paper, we present a curriculum-based dataset distillation framework designed to harmonize scalability with efficiency.","This framework strategically distills synthetic images, adhering to a curriculum that transitions from simple to complex.","By incorporating curriculum evaluation, we address the issue of previous methods generating images that tend to be homogeneous and simplistic, doing so at a manageable computational cost.","Furthermore, we introduce adversarial optimization towards synthetic images to further improve their representativeness and safeguard against their overfitting to the neural network involved in distilling.","This enhances the generalization capability of the distilled images across various neural network architectures and also increases their robustness to noise.","Extensive experiments demonstrate that our framework sets new benchmarks in large-scale dataset distillation, achieving substantial improvements of 11.1\\% on Tiny-ImageNet, 9.0\\% on ImageNet-1K, and 7.3\\% on ImageNet-21K.","The source code will be released to the community."],"url":"http://arxiv.org/abs/2405.09150v1","category":"cs.CV"}
{"created":"2024-05-15 07:23:01","title":"Exploring uniformity and maximum entropy distribution on torus through intrinsic geometry: Application to protein-chemistry","abstract":"A generic family of distributions, defined on the surface of a curved torus is introduced using the area element of it. The area uniformity and the maximum entropy distribution are identified using the trigonometric moments of the proposed family. A marginal distribution is obtained as a three-parameter modification of the von Mises distribution that encompasses the von Mises, Cardioid, and Uniform distributions as special cases. The proposed family of the marginal distribution exhibits both symmetric and asymmetric, unimodal or bimodal shapes, contingent upon parameters. Furthermore, we scrutinize a two-parameter symmetric submodel, examining its moments, measure of variation, Kullback-Leibler divergence, and maximum likelihood estimation, among other properties. In addition, we introduce a modified acceptance-rejection sampling with a thin envelope obtained from the upper-Riemann-sum of a circular density, achieving a high rate of acceptance. This proposed sampling scheme will accelerate the empirical studies for a large-scale simulation reducing the processing time. Furthermore, we extend the Uniform, Wrapped Cauchy, and Kato-Jones distributions to the surface of the curved torus and implemented the proposed bivariate toroidal distribution for different groups of protein data, namely, $\\alpha$-helix, $\\beta$-sheet, and their mixture. A marginal of this proposed distribution is fitted to the wind direction data.","sentences":["A generic family of distributions, defined on the surface of a curved torus is introduced using the area element of it.","The area uniformity and the maximum entropy distribution are identified using the trigonometric moments of the proposed family.","A marginal distribution is obtained as a three-parameter modification of the von Mises distribution that encompasses the von Mises, Cardioid, and Uniform distributions as special cases.","The proposed family of the marginal distribution exhibits both symmetric and asymmetric, unimodal or bimodal shapes, contingent upon parameters.","Furthermore, we scrutinize a two-parameter symmetric submodel, examining its moments, measure of variation, Kullback-Leibler divergence, and maximum likelihood estimation, among other properties.","In addition, we introduce a modified acceptance-rejection sampling with a thin envelope obtained from the upper-Riemann-sum of a circular density, achieving a high rate of acceptance.","This proposed sampling scheme will accelerate the empirical studies for a large-scale simulation reducing the processing time.","Furthermore, we extend the Uniform, Wrapped Cauchy, and Kato-Jones distributions to the surface of the curved torus and implemented the proposed bivariate toroidal distribution for different groups of protein data, namely, $\\alpha$-helix, $\\beta$-sheet, and their mixture.","A marginal of this proposed distribution is fitted to the wind direction data."],"url":"http://arxiv.org/abs/2405.09149v1","category":"stat.ME"}
{"created":"2024-05-15 07:20:18","title":"Synchronization of E. coli bacteria moving in coupled wells","abstract":"Synchronization plays a crucial role in the dynamics of living organisms, from fireflies flashing in unison to pacemaker cells that jointly generate heartbeats. Uncovering the mechanism behind these phenomena requires an understanding of individual biological oscillators and the coupling forces between them. Here, we develop a single-cell assay that studies rhythmic behavior in the motility of individual E.coli cells that can be mutually synchronized. Circular microcavities are used to isolate E.coli cells that swim along the cavity wall, resulting in self-sustained oscillations. Upon connecting these cavities by microchannels the bacterial motions can be coupled, yielding nonlinear dynamic synchronization patterns with phase slips. We demonstrate that the coordinated movement observed in coupled E. coli oscillators follows mathematical rules of synchronization which we use to quantify the coupling strength. These findings advance our understanding of motility in confinement, and lay the foundation for engineering desired dynamics in microbial active matter.","sentences":["Synchronization plays a crucial role in the dynamics of living organisms, from fireflies flashing in unison to pacemaker cells that jointly generate heartbeats.","Uncovering the mechanism behind these phenomena requires an understanding of individual biological oscillators and the coupling forces between them.","Here, we develop a single-cell assay that studies rhythmic behavior in the motility of individual E.coli cells that can be mutually synchronized.","Circular microcavities are used to isolate E.coli cells that swim along the cavity wall, resulting in self-sustained oscillations.","Upon connecting these cavities by microchannels the bacterial motions can be coupled, yielding nonlinear dynamic synchronization patterns with phase slips.","We demonstrate that the coordinated movement observed in coupled E. coli oscillators follows mathematical rules of synchronization which we use to quantify the coupling strength.","These findings advance our understanding of motility in confinement, and lay the foundation for engineering desired dynamics in microbial active matter."],"url":"http://arxiv.org/abs/2405.09147v1","category":"nlin.AO"}
{"created":"2024-05-15 07:19:36","title":"First order distinguishability of sparse random graphs","abstract":"We study the problem of distinguishing between two independent samples $\\mathbf{G}_n^1,\\mathbf{G}_n^2$ of a binomial random graph $G(n,p)$ by first order (FO) sentences. Shelah and Spencer proved that, for a constant $\\alpha\\in(0,1)$, $G(n,n^{-\\alpha})$ obeys FO zero-one law if and only if $\\alpha$ is irrational. Therefore, for irrational $\\alpha\\in(0,1)$, any fixed FO sentence does not distinguish between $\\mathbf{G}_n^1,\\mathbf{G}_n^2$ with asymptotical probability 1 (w.h.p.) as $n\\to\\infty$. We show that the minimum quantifier depth $\\mathbf{k}_{\\alpha}$ of a FO sentence $\\varphi=\\varphi(\\mathbf{G}_n^1,\\mathbf{G}_n^2)$ distinguishing between $\\mathbf{G}_n^1,\\mathbf{G}_n^2$ depends on how closely $\\alpha$ can be approximated by rationals: (1) for all non-Liouville $\\alpha\\in(0,1)$, $\\mathbf{k}_{\\alpha}=\\Omega(\\ln\\ln\\ln n)$ w.h.p.; (2) there are irrational $\\alpha\\in(0,1)$ with $\\mathbf{k}_{\\alpha}$ that grow arbitrarily slowly w.h.p.; (3) $\\mathbf{k}_{\\alpha}=O_p(\\frac{\\ln n}{\\ln\\ln n})$ for all $\\alpha\\in(0,1)$. The main ingredients in our proofs are a novel randomized algorithm that generates asymmetric strictly balanced graphs as well as a new method to study symmetry groups of randomly perturbed graphs.","sentences":["We study the problem of distinguishing between two independent samples $\\mathbf{G}_n^1,\\mathbf{G}_n^2$ of a binomial random graph $G(n,p)$ by first order (FO) sentences.","Shelah and Spencer proved that, for a constant $\\alpha\\in(0,1)$, $G(n,n^{-\\alpha})$ obeys FO zero-one law if and only if $\\alpha$ is irrational.","Therefore, for irrational $\\alpha\\in(0,1)$, any fixed FO sentence does not distinguish between $\\mathbf{G}_n^1,\\mathbf{G}_n^2$ with asymptotical probability 1 (w.h.p.)","as $n\\to\\infty$. We show that the minimum quantifier depth $\\mathbf{k}_{\\alpha}$ of a FO sentence $\\varphi=\\varphi(\\mathbf{G}_n^1,\\mathbf{G}_n^2)$ distinguishing between $\\mathbf{G}_n^1,\\mathbf{G}_n^2$ depends on how closely $\\alpha$ can be approximated by rationals: (1) for all non-Liouville $\\alpha\\in(0,1)$, $\\mathbf{k}_{\\alpha}=\\Omega(\\ln\\ln\\ln n)$ w.h.p.; (2) there are irrational $\\alpha\\in(0,1)$ with $\\mathbf{k}_{\\alpha}$ that grow arbitrarily slowly w.h.p.; (3) $\\mathbf{k}_{\\alpha}=O_p(\\frac{\\ln n}{\\ln\\ln n})$ for all $\\alpha\\in(0,1)$. The main ingredients in our proofs are a novel randomized algorithm that generates asymmetric strictly balanced graphs as well as a new method to study symmetry groups of randomly perturbed graphs."],"url":"http://arxiv.org/abs/2405.09146v1","category":"math.CO"}
{"created":"2024-05-15 07:13:08","title":"Tree-Packing Revisited: Faster Fully Dynamic Min-Cut and Arboricity","abstract":"A tree-packing is a collection of spanning trees of a graph. It has been a useful tool for computing the minimum cut in static, dynamic, and distributed settings. In particular, [Thorup, Comb. 2007] used them to obtain his dynamic min-cut algorithm with $\\tilde O(\\lambda^{14.5}\\sqrt{n})$ worst-case update time. We reexamine this relationship, showing that we need to maintain fewer spanning trees for such a result; we show that we only need to pack $\\Theta(\\lambda^3 \\log m)$ greedy trees to guarantee a 1-respecting cut or a trivial cut in some contracted graph.   Based on this structural result, we then provide a deterministic algorithm for fully dynamic exact min-cut, that has $\\tilde O(\\lambda^{5.5}\\sqrt{n})$ worst-case update time, for min-cut value bounded by $\\lambda$. In particular, this also leads to an algorithm for general fully dynamic exact min-cut with $\\tilde O(m^{1-1/12})$ amortized update time, improving upon $\\tilde O(m^{1-1/31})$ [Goranci et al., SODA 2023].   We also give the first fully dynamic algorithm that maintains a $(1+\\varepsilon)$-approximation of the fractional arboricity -- which is strictly harder than the integral arboricity. Our algorithm is deterministic and has $O(\\alpha \\log^6m/\\varepsilon^4)$ amortized update time, for arboricity at most $\\alpha$. We extend these results to a Monte Carlo algorithm with $O(\\text{poly}(\\log m,\\varepsilon^{-1}))$ amortized update time against an adaptive adversary. Our algorithms work on multi-graphs as well.   Both result are obtained by exploring the connection between the min-cut/arboricity and (greedy) tree-packing. We investigate tree-packing in a broader sense; including a lower bound for greedy tree-packing, which - to the best of our knowledge - is the first progress on this topic since [Thorup, Comb. 2007].","sentences":["A tree-packing is a collection of spanning trees of a graph.","It has been a useful tool for computing the minimum cut in static, dynamic, and distributed settings.","In particular, [Thorup, Comb. 2007] used them to obtain his dynamic min-cut algorithm with $\\tilde O(\\lambda^{14.5}\\sqrt{n})$ worst-case update time.","We reexamine this relationship, showing that we need to maintain fewer spanning trees for such a result; we show that we only need to pack $\\Theta(\\lambda^3 \\log m)$ greedy trees to guarantee a 1-respecting cut or a trivial cut in some contracted graph.   ","Based on this structural result, we then provide a deterministic algorithm for fully dynamic exact min-cut, that has $\\tilde O(\\lambda^{5.5}\\sqrt{n})$ worst-case update time, for min-cut value bounded by $\\lambda$. In particular, this also leads to an algorithm for general fully dynamic exact min-cut with $\\tilde O(m^{1-1/12})$ amortized update time, improving upon $\\tilde O(m^{1","-1/31})$","[Goranci et al., SODA 2023].   ","We also give the first fully dynamic algorithm that maintains a $(1+\\varepsilon)$-approximation of the fractional arboricity -- which is strictly harder than the integral arboricity.","Our algorithm is deterministic and has $O(\\alpha \\log^6m/\\varepsilon^4)$ amortized update time, for arboricity at most $\\alpha$. We extend these results to a Monte Carlo algorithm with $O(\\text{poly}(\\log m,\\varepsilon^{-1}))$ amortized update time against an adaptive adversary.","Our algorithms work on multi-graphs as well.   ","Both result are obtained by exploring the connection between the min-cut/arboricity and (greedy) tree-packing.","We investigate tree-packing in a broader sense; including a lower bound for greedy tree-packing, which - to the best of our knowledge - is the first progress on this topic since [Thorup, Comb. 2007]."],"url":"http://arxiv.org/abs/2405.09141v1","category":"cs.DS"}
{"created":"2024-05-15 07:12:43","title":"An ultra wide-band, high-sensitivity Q-band receiver for single-dish telescopes, eQ: rest frequency determination of CCS ($J_N$ = $4_3$-$3_2$) and SO ($J_N$ = $1_0$-$0_1$), and high-redshift CO ($J$ = 1-0) detection","abstract":"We report on the development and commissioning of a new Q-band receiver for the Nobeyama 45-m telescope, covering 30--50 GHz with a receiver noise temperature of about 15 K. We name it eQ (extended Q-band) receiver. The system noise temperatures for observations are measured to be $\\sim$ 30 K at 33 GHz and $\\sim$ 75 K at 45 GHz. The Half-Power-Beam-Width (HPBW) is around 38\\arcsec at 43 GHz. To enhance the observation capability, we tested the smoothed bandpass calibration technique and demonstrated the observation time can be significantly reduced compared to the standard position switch technique. The wide-bandwidth capability of this receiver provides precise determination of rest frequencies for molecular transitions with an accuracy of a few kHz through simultaneous observations of multiple transitions. Particularly, we determined the rest frequency of SO ($J_N$ = $1_0$--$0_1$) to be 30.001542 GHz, along with the rest frequency of CCS ($J_N$ = $4_3$--$3_2$) being 45.379033 GHz, adopting CCS ($J_N$ = $3_2$--$2_1$) at 33.751370 GHz as a reference line. The SO profile shows a double peak shape at the Cyanopolyyne Peak (CP) position of the Taurus Molecular Cloud-1 (TMC-1). The SO peaks coincide well with the CCS sub-components located near the outer parts of the TMC-1 filament. We interpret that the gravitational infall of TMC-1 generates shocks which enhance the SO abundance. The TMC-1 map shows that carbon-chain molecules are more abundant in the southern part of the filament, whereas SO is more abundant in the northern part. The eQ's excellent sensitivity allowed us to detect faint CO ($J$ = 1--0) spectra from the high-redshift object at a redshift of 2.442. Our receiver is expected to open new avenues for high-sensitivity molecular line observations in the Q-band.","sentences":["We report on the development and commissioning of a new Q-band receiver for the Nobeyama 45-m telescope, covering 30--50 GHz with a receiver noise temperature of about 15 K. We name it eQ (extended Q-band) receiver.","The system noise temperatures for observations are measured to be $\\sim$ 30 K at 33 GHz and $\\sim$ 75 K at 45 GHz.","The Half-Power-Beam-Width (HPBW) is around 38\\arcsec at 43 GHz.","To enhance the observation capability, we tested the smoothed bandpass calibration technique and demonstrated the observation time can be significantly reduced compared to the standard position switch technique.","The wide-bandwidth capability of this receiver provides precise determination of rest frequencies for molecular transitions with an accuracy of a few kHz through simultaneous observations of multiple transitions.","Particularly, we determined the rest frequency of SO ($J_N$ = $1_0$--$0_1$) to be 30.001542 GHz, along with the rest frequency of CCS ($J_N$ = $4_3$--$3_2$) being 45.379033 GHz, adopting CCS ($J_N$ = $3_2$--$2_1$) at 33.751370 GHz as a reference line.","The SO profile shows a double peak shape at the Cyanopolyyne Peak (CP) position of the Taurus Molecular Cloud-1 (TMC-1).","The SO peaks coincide well with the CCS sub-components located near the outer parts of the TMC-1 filament.","We interpret that the gravitational infall of TMC-1 generates shocks which enhance the SO abundance.","The TMC-1 map shows that carbon-chain molecules are more abundant in the southern part of the filament, whereas SO is more abundant in the northern part.","The eQ's excellent sensitivity allowed us to detect faint CO ($J$ = 1--0) spectra from the high-redshift object at a redshift of 2.442.","Our receiver is expected to open new avenues for high-sensitivity molecular line observations in the Q-band."],"url":"http://arxiv.org/abs/2405.09140v1","category":"astro-ph.GA"}
{"created":"2024-05-15 07:11:24","title":"Tunable magnetic anisotropy, Curie temperature and band alignment of two-dimensional ferromagnet VSiSnN4 via non-volatile ferroelectrical control","abstract":"The emergence of multiferroic materials, which possess both ferromagnetic (FM) and ferroelectric (FE) properties, drive advancements in magnetoelectric applications and the next generation of spintronics. Based on first-principles calculations, we investigate an engineered two-dimensional multiferroic van der Waals heterostructures consisting of FM VSiSnN4 monolayer (ML) and fully hydrogenated FE AlN bilayer. We find that the magnetic anisotropy of VSiSnN4 ML is tunable between out-of-plane and in-plane and a phase transition between semiconductor and metal is induced in VSiSnN4/AlN bilayer when the FE polarization direction of AlN bilayer is reversed. Surprisingly, when the FE polarization of AlN bilayer is upward, the Curie temperature of VSiSnN4/AlN bilayer can be significantly increased from 204K to 284K. Such non-volatile and tunable magnetic anisotropy, Curie temperature and band alignment in VSiSnN4/AlN multiferroic heterostructure are highly promising for future low-current operation of data storage and logic devices.","sentences":["The emergence of multiferroic materials, which possess both ferromagnetic (FM) and ferroelectric (FE) properties, drive advancements in magnetoelectric applications and the next generation of spintronics.","Based on first-principles calculations, we investigate an engineered two-dimensional multiferroic van der Waals heterostructures consisting of FM VSiSnN4 monolayer (ML) and fully hydrogenated FE AlN bilayer.","We find that the magnetic anisotropy of VSiSnN4 ML is tunable between out-of-plane and in-plane and a phase transition between semiconductor and metal is induced in VSiSnN4/AlN bilayer when the FE polarization direction of AlN bilayer is reversed.","Surprisingly, when the FE polarization of AlN bilayer is upward, the Curie temperature of VSiSnN4/AlN bilayer can be significantly increased from 204K to 284K. Such non-volatile and tunable magnetic anisotropy, Curie temperature and band alignment in VSiSnN4/AlN multiferroic heterostructure are highly promising for future low-current operation of data storage and logic devices."],"url":"http://arxiv.org/abs/2405.09139v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-15 07:11:12","title":"OpenGait: A Comprehensive Benchmark Study for Gait Recognition towards Better Practicality","abstract":"Gait recognition, a rapidly advancing vision technology for person identification from a distance, has made significant strides in indoor settings. However, evidence suggests that existing methods often yield unsatisfactory results when applied to newly released real-world gait datasets. Furthermore, conclusions drawn from indoor gait datasets may not easily generalize to outdoor ones. Therefore, the primary goal of this work is to present a comprehensive benchmark study aimed at improving practicality rather than solely focusing on enhancing performance. To this end, we first develop OpenGait, a flexible and efficient gait recognition platform. Using OpenGait as a foundation, we conduct in-depth ablation experiments to revisit recent developments in gait recognition. Surprisingly, we detect some imperfect parts of certain prior methods thereby resulting in several critical yet undiscovered insights. Inspired by these findings, we develop three structurally simple yet empirically powerful and practically robust baseline models, i.e., DeepGaitV2, SkeletonGait, and SkeletonGait++, respectively representing the appearance-based, model-based, and multi-modal methodology for gait pattern description. Beyond achieving SoTA performances, more importantly, our careful exploration sheds new light on the modeling experience of deep gait models, the representational capacity of typical gait modalities, and so on. We hope this work can inspire further research and application of gait recognition towards better practicality. The code is available at https://github.com/ShiqiYu/OpenGait.","sentences":["Gait recognition, a rapidly advancing vision technology for person identification from a distance, has made significant strides in indoor settings.","However, evidence suggests that existing methods often yield unsatisfactory results when applied to newly released real-world gait datasets.","Furthermore, conclusions drawn from indoor gait datasets may not easily generalize to outdoor ones.","Therefore, the primary goal of this work is to present a comprehensive benchmark study aimed at improving practicality rather than solely focusing on enhancing performance.","To this end, we first develop OpenGait, a flexible and efficient gait recognition platform.","Using OpenGait as a foundation, we conduct in-depth ablation experiments to revisit recent developments in gait recognition.","Surprisingly, we detect some imperfect parts of certain prior methods thereby resulting in several critical yet undiscovered insights.","Inspired by these findings, we develop three structurally simple yet empirically powerful and practically robust baseline models, i.e., DeepGaitV2, SkeletonGait, and SkeletonGait++, respectively representing the appearance-based, model-based, and multi-modal methodology for gait pattern description.","Beyond achieving SoTA performances, more importantly, our careful exploration sheds new light on the modeling experience of deep gait models, the representational capacity of typical gait modalities, and so on.","We hope this work can inspire further research and application of gait recognition towards better practicality.","The code is available at https://github.com/ShiqiYu/OpenGait."],"url":"http://arxiv.org/abs/2405.09138v1","category":"cs.CV"}
{"created":"2024-05-15 07:02:41","title":"On the Role of Controllability in Pulse-based Quantum Machine Learning Models","abstract":"Pulse-based quantum machine learning (QML) models possess full expressivity when they are ensemble controllable. However, it has also been shown that barren plateaus emerge in such models, rendering training intractable for systems with large dimension. In this paper, we show that the trade-off is closely related to the controllability of the underlying pulse-based models. We first apply the Fliess-series expansion to pulse-based QML models to investigate the effect of control system structure on model expressivity, which leads to a universal criterion for assessing the expressivity of generic QML models. Guided by this criterion, we then demonstrate how designing pulse-based models on low-dimensional manifolds can balance expressivity and trainability. Finally, numerical experiments are carried out to verify the proposed criterion and our analysis, which futher demonstrate that increasing dimensionality enhances expressivity but avoids barren plateaus if the model is designed with limited controllability on a submanifold. Our approach provides a promising path for designing pulse-based QML models that are both highly expressive and trainable.","sentences":["Pulse-based quantum machine learning (QML) models possess full expressivity when they are ensemble controllable.","However, it has also been shown that barren plateaus emerge in such models, rendering training intractable for systems with large dimension.","In this paper, we show that the trade-off is closely related to the controllability of the underlying pulse-based models.","We first apply the Fliess-series expansion to pulse-based QML models to investigate the effect of control system structure on model expressivity, which leads to a universal criterion for assessing the expressivity of generic QML models.","Guided by this criterion, we then demonstrate how designing pulse-based models on low-dimensional manifolds can balance expressivity and trainability.","Finally, numerical experiments are carried out to verify the proposed criterion and our analysis, which futher demonstrate that increasing dimensionality enhances expressivity but avoids barren plateaus if the model is designed with limited controllability on a submanifold.","Our approach provides a promising path for designing pulse-based QML models that are both highly expressive and trainable."],"url":"http://arxiv.org/abs/2405.09135v1","category":"quant-ph"}
{"created":"2024-05-15 06:56:05","title":"RobustMVS: Single Domain Generalized Deep Multi-view Stereo","abstract":"Despite the impressive performance of Multi-view Stereo (MVS) approaches given plenty of training samples, the performance degradation when generalizing to unseen domains has not been clearly explored yet. In this work, we focus on the domain generalization problem in MVS. To evaluate the generalization results, we build a novel MVS domain generalization benchmark including synthetic and real-world datasets. In contrast to conventional domain generalization benchmarks, we consider a more realistic but challenging scenario, where only one source domain is available for training. The MVS problem can be analogized back to the feature matching task, and maintaining robust feature consistency among views is an important factor for improving generalization performance. To address the domain generalization problem in MVS, we propose a novel MVS framework, namely RobustMVS. A DepthClustering-guided Whitening (DCW) loss is further introduced to preserve the feature consistency among different views, which decorrelates multi-view features from viewpoint-specific style information based on geometric priors from depth maps. The experimental results further show that our method achieves superior performance on the domain generalization benchmark.","sentences":["Despite the impressive performance of Multi-view Stereo (MVS) approaches given plenty of training samples, the performance degradation when generalizing to unseen domains has not been clearly explored yet.","In this work, we focus on the domain generalization problem in MVS.","To evaluate the generalization results, we build a novel MVS domain generalization benchmark including synthetic and real-world datasets.","In contrast to conventional domain generalization benchmarks, we consider a more realistic but challenging scenario, where only one source domain is available for training.","The MVS problem can be analogized back to the feature matching task, and maintaining robust feature consistency among views is an important factor for improving generalization performance.","To address the domain generalization problem in MVS, we propose a novel MVS framework, namely RobustMVS.","A DepthClustering-guided Whitening (DCW) loss is further introduced to preserve the feature consistency among different views, which decorrelates multi-view features from viewpoint-specific style information based on geometric priors from depth maps.","The experimental results further show that our method achieves superior performance on the domain generalization benchmark."],"url":"http://arxiv.org/abs/2405.09131v1","category":"cs.CV"}
{"created":"2024-05-15 06:44:18","title":"Capacity of entanglement for scalar fields in squeezed states","abstract":"We study various aspects of capacity of entanglement in the squeezed states of a scalar field theory. This quantity is a quantum informational counterpart of heat capacity and characterizes the width of the eigenvalue spectrum of the reduced density matrix. In particular, we carefully examine the dependence of capacity of entanglement and its universal terms on the squeezing parameter in the specific regimes of the parameter space. Remarkably, we find that the capacity of entanglement obeys a volume law in the large squeezing limit. We discuss how these results are consistent with the behavior of other entanglement measures including entanglement and Renyi entropies. We also comment on the existence of consistent holographic duals for a family of Gaussian states with generic squeezing parameter based on the ratio of entanglement entropy and the capacity of entanglement.","sentences":["We study various aspects of capacity of entanglement in the squeezed states of a scalar field theory.","This quantity is a quantum informational counterpart of heat capacity and characterizes the width of the eigenvalue spectrum of the reduced density matrix.","In particular, we carefully examine the dependence of capacity of entanglement and its universal terms on the squeezing parameter in the specific regimes of the parameter space.","Remarkably, we find that the capacity of entanglement obeys a volume law in the large squeezing limit.","We discuss how these results are consistent with the behavior of other entanglement measures including entanglement and Renyi entropies.","We also comment on the existence of consistent holographic duals for a family of Gaussian states with generic squeezing parameter based on the ratio of entanglement entropy and the capacity of entanglement."],"url":"http://arxiv.org/abs/2405.09128v1","category":"hep-th"}
{"created":"2024-05-15 06:41:43","title":"HAAP: Vision-context Hierarchical Attention Autoregressive with Adaptive Permutation for Scene Text Recognition","abstract":"Internal Language Model (LM)-based methods use permutation language modeling (PLM) to solve the error correction caused by conditional independence in external LM-based methods. However, random permutations of human interference cause fit oscillations in the model training, and Iterative Refinement (IR) operation to improve multimodal information decoupling also introduces additional overhead. To address these issues, this paper proposes the Hierarchical Attention autoregressive Model with Adaptive Permutation (HAAP) to enhance the location-context-image interaction capability, improving autoregressive generalization with internal LM. First, we propose Implicit Permutation Neurons (IPN) to generate adaptive attention masks to dynamically exploit token dependencies. The adaptive masks increase the diversity of training data and prevent model dependency on a specific order. It reduces the training overhead of PLM while avoiding training fit oscillations. Second, we develop Cross-modal Hierarchical Attention mechanism (CHA) to couple context and image features. This processing establishes rich positional semantic dependencies between context and image while avoiding IR. Extensive experimental results show the proposed HAAP achieves state-of-the-art (SOTA) performance in terms of accuracy, complexity, and latency on several datasets.","sentences":["Internal Language Model (LM)-based methods use permutation language modeling (PLM) to solve the error correction caused by conditional independence in external LM-based methods.","However, random permutations of human interference cause fit oscillations in the model training, and Iterative Refinement (IR) operation to improve multimodal information decoupling also introduces additional overhead.","To address these issues, this paper proposes the Hierarchical Attention autoregressive Model with Adaptive Permutation (HAAP) to enhance the location-context-image interaction capability, improving autoregressive generalization with internal LM.","First, we propose Implicit Permutation Neurons (IPN) to generate adaptive attention masks to dynamically exploit token dependencies.","The adaptive masks increase the diversity of training data and prevent model dependency on a specific order.","It reduces the training overhead of PLM while avoiding training fit oscillations.","Second, we develop Cross-modal Hierarchical Attention mechanism (CHA) to couple context and image features.","This processing establishes rich positional semantic dependencies between context and image while avoiding IR.","Extensive experimental results show the proposed HAAP achieves state-of-the-art (SOTA) performance in terms of accuracy, complexity, and latency on several datasets."],"url":"http://arxiv.org/abs/2405.09125v1","category":"cs.CV"}
{"created":"2024-05-15 06:40:32","title":"A regular center instead of a black bounce","abstract":"The widely discussed ``black-bounce'' mechanism of removing a singularity at $r=0$ in a spherically symmetric space-time, proposed by Simpson and Visser, consists in removing the point $r=0$ and its close neighborhood, resulting in emergence of a regular minimum of the spherical radius that can be a wormhole throat or a regular bounce. Instead, it has been recently proposed to make $r=0$ a regular center by properly modifying the metric, still preserving its form in regions far from $r=0$. Different algorithms of such modifications have been formulated for a few classes of singularities. The previous paper considered space-times whose Ricci tensor satisfies the condition $R^t_t =R^r_r$, and regular modifications were obtained for the Schwarzschild, Reissner-Nordstr\\\"om metrics, and two examples of solutions with magnetic fields obeying nonlinear electrodynamics (NED). The present paper considers regular modifications of more general space-times, and as examples, modifications with a regular center have been obtained for the Fisher (also known as JNW) solution with a naked singularity and a family of dilatonic black holes. Possible field sources of the new regular metrics are considered in the framework of general relativity (GR), using the fact that any static, spherically symmetric metric with a combined source involving NED and a scalar field with some self-interaction potential. This scalar field is, in general, not required to be of phantom nature (unlike the sources for black bounces), but in the examples discussed here, the possible scalar sources are phantom in a close neighborhood of $r=0$ and are canonical outside it.","sentences":["The widely discussed ``black-bounce'' mechanism of removing a singularity at $r=0$ in a spherically symmetric space-time, proposed by Simpson and Visser, consists in removing the point $r=0$ and its close neighborhood, resulting in emergence of a regular minimum of the spherical radius that can be a wormhole throat or a regular bounce.","Instead, it has been recently proposed to make $r=0$ a regular center by properly modifying the metric, still preserving its form in regions far from $r=0$. Different algorithms of such modifications have been formulated for a few classes of singularities.","The previous paper considered space-times whose Ricci tensor satisfies the condition $R^t_t =R^r_r$, and regular modifications were obtained for the Schwarzschild, Reissner-Nordstr\\\"om metrics, and two examples of solutions with magnetic fields obeying nonlinear electrodynamics (NED).","The present paper considers regular modifications of more general space-times, and as examples, modifications with a regular center have been obtained for the Fisher (also known as JNW) solution with a naked singularity and a family of dilatonic black holes.","Possible field sources of the new regular metrics are considered in the framework of general relativity (GR), using the fact that any static, spherically symmetric metric with a combined source involving NED and a scalar field with some self-interaction potential.","This scalar field is, in general, not required to be of phantom nature (unlike the sources for black bounces), but in the examples discussed here, the possible scalar sources are phantom in a close neighborhood of $r=0$ and are canonical outside it."],"url":"http://arxiv.org/abs/2405.09124v1","category":"gr-qc"}
{"created":"2024-05-15 06:38:01","title":"A new infinite family of maximum $h$-scattered $\\mathbb{F}_q$-subspaces of $V(m(h+1),q^n)$ and associated MRD codes","abstract":"The exploration of linear subspaces, particularly scattered subspaces, has garnered considerable attention across diverse mathematical disciplines in recent years, notably within finite geometries and coding theory. Scattered subspaces play a pivotal role in analyzing various geometric structures such as blocking sets, two-intersection sets, complete arcs, caps in affine and projective spaces over finite fields and rank metric codes. This paper introduces a new infinite family of $h$-subspaces, along with their associated MRD codes. Additionally, it addresses the task of determining the generalized weights of these codes. Notably, we demonstrate that these MRD codes exhibit some larger generalized weights compared to those previously identified.","sentences":["The exploration of linear subspaces, particularly scattered subspaces, has garnered considerable attention across diverse mathematical disciplines in recent years, notably within finite geometries and coding theory.","Scattered subspaces play a pivotal role in analyzing various geometric structures such as blocking sets, two-intersection sets, complete arcs, caps in affine and projective spaces over finite fields and rank metric codes.","This paper introduces a new infinite family of $h$-subspaces, along with their associated MRD codes.","Additionally, it addresses the task of determining the generalized weights of these codes.","Notably, we demonstrate that these MRD codes exhibit some larger generalized weights compared to those previously identified."],"url":"http://arxiv.org/abs/2405.09123v1","category":"math.CO"}
{"created":"2024-05-15 06:35:39","title":"Full Band Structure Calculation of Semiconducting Materials on a Noisy Quantum Processor","abstract":"Quantum chemistry is a promising application in the era of quantum computing since the unique effects of quantum mechanics that take exponential growing resources to simulate classically are controllable on quantum computers. Fermionic degrees of freedom can be encoded efficiently onto qubits and allow for algorithms such as the Quantum Equation-of-Motion method to find the entire energy spectrum of a quantum system. In this paper, we propose the Reduced Quantum Equation-of-Motion method by reducing the dimensionality of its generalized eigenvalue equation, which results in half the measurements required compared to the Quantum Equation-of-Motion method, leading to speed up the algorithm and less noise accumulation on real devices. In particular, we analyse the performance of our method on two noise models and calculate the excitation energies of a bulk Silicon and Gallium Arsenide using our method on an IBM quantum processor. Our method is fully robust to the uniform depolarizing error and we demonstrate that the selection of suitable atomic orbital complexity could increase the robustness of our algorithm under real noise. We also find that taking the average of multiple experiments tends towards the correct energies due to the fluctuations around the exact values. Such noise resilience of our approach could be used on current quantum devices to solve quantum chemistry problems.","sentences":["Quantum chemistry is a promising application in the era of quantum computing since the unique effects of quantum mechanics that take exponential growing resources to simulate classically are controllable on quantum computers.","Fermionic degrees of freedom can be encoded efficiently onto qubits and allow for algorithms such as the Quantum Equation-of-Motion method to find the entire energy spectrum of a quantum system.","In this paper, we propose the Reduced Quantum Equation-of-Motion method by reducing the dimensionality of its generalized eigenvalue equation, which results in half the measurements required compared to the Quantum Equation-of-Motion method, leading to speed up the algorithm and less noise accumulation on real devices.","In particular, we analyse the performance of our method on two noise models and calculate the excitation energies of a bulk Silicon and Gallium Arsenide using our method on an IBM quantum processor.","Our method is fully robust to the uniform depolarizing error and we demonstrate that the selection of suitable atomic orbital complexity could increase the robustness of our algorithm under real noise.","We also find that taking the average of multiple experiments tends towards the correct energies due to the fluctuations around the exact values.","Such noise resilience of our approach could be used on current quantum devices to solve quantum chemistry problems."],"url":"http://arxiv.org/abs/2405.09122v1","category":"quant-ph"}
{"created":"2024-05-15 06:23:59","title":"BonnBot-I Plus: A Bio-diversity Aware Precise Weed Management Robotic Platform","abstract":"In this article, we focus on the critical tasks of plant protection in arable farms, addressing a modern challenge in agriculture: integrating ecological considerations into the operational strategy of precision weeding robots like \\bbot. This article presents the recent advancements in weed management algorithms and the real-world performance of \\bbot\\ at the University of Bonn's Klein-Altendorf campus. We present a novel Rolling-view observation model for the BonnBot-Is weed monitoring section which leads to an average absolute weeding performance enhancement of $3.4\\%$. Furthermore, for the first time, we show how precision weeding robots could consider bio-diversity-aware concerns in challenging weeding scenarios. We carried out comprehensive weeding experiments in sugar-beet fields, covering both weed-only and mixed crop-weed situations, and introduced a new dataset compatible with precision weeding. Our real-field experiments revealed that our weeding approach is capable of handling diverse weed distributions, with a minimal loss of only $11.66\\%$ attributable to intervention planning and $14.7\\%$ to vision system limitations highlighting required improvements of the vision system.","sentences":["In this article, we focus on the critical tasks of plant protection in arable farms, addressing a modern challenge in agriculture: integrating ecological considerations into the operational strategy of precision weeding robots like \\bbot.","This article presents the recent advancements in weed management algorithms and the real-world performance of \\bbot\\ at the University of Bonn's Klein-Altendorf campus.","We present a novel Rolling-view observation model for the BonnBot-Is weed monitoring section which leads to an average absolute weeding performance enhancement of $3.4\\%$. Furthermore, for the first time, we show how precision weeding robots could consider bio-diversity-aware concerns in challenging weeding scenarios.","We carried out comprehensive weeding experiments in sugar-beet fields, covering both weed-only and mixed crop-weed situations, and introduced a new dataset compatible with precision weeding.","Our real-field experiments revealed that our weeding approach is capable of handling diverse weed distributions, with a minimal loss of only $11.66\\%$ attributable to intervention planning and $14.7\\%$ to vision system limitations highlighting required improvements of the vision system."],"url":"http://arxiv.org/abs/2405.09118v1","category":"cs.RO"}
{"created":"2024-05-15 06:14:31","title":"SOEDiff: Efficient Distillation for Small Object Editing","abstract":"In this paper, we delve into a new task known as small object editing (SOE), which focuses on text-based image inpainting within a constrained, small-sized area. Despite the remarkable success have been achieved by current image inpainting approaches, their application to the SOE task generally results in failure cases such as Object Missing, Text-Image Mismatch, and Distortion. These failures stem from the limited use of small-sized objects in training datasets and the downsampling operations employed by U-Net models, which hinders accurate generation. To overcome these challenges, we introduce a novel training-based approach, SOEDiff, aimed at enhancing the capability of baseline models like StableDiffusion in editing small-sized objects while minimizing training costs. Specifically, our method involves two key components: SO-LoRA, which efficiently fine-tunes low-rank matrices, and Cross-Scale Score Distillation loss, which leverages high-resolution predictions from the pre-trained teacher diffusion model. Our method presents significant improvements on the test dataset collected from MSCOCO and OpenImage, validating the effectiveness of our proposed method in small object editing. In particular, when comparing SOEDiff with SD-I model on the OpenImage-f dataset, we observe a 0.99 improvement in CLIP-Score and a reduction of 2.87 in FID. Our project page can be found in https://soediff.github.io/.","sentences":["In this paper, we delve into a new task known as small object editing (SOE), which focuses on text-based image inpainting within a constrained, small-sized area.","Despite the remarkable success have been achieved by current image inpainting approaches, their application to the SOE task generally results in failure cases such as Object Missing, Text-Image Mismatch, and Distortion.","These failures stem from the limited use of small-sized objects in training datasets and the downsampling operations employed by U-Net models, which hinders accurate generation.","To overcome these challenges, we introduce a novel training-based approach, SOEDiff, aimed at enhancing the capability of baseline models like StableDiffusion in editing small-sized objects while minimizing training costs.","Specifically, our method involves two key components: SO-LoRA, which efficiently fine-tunes low-rank matrices, and Cross-Scale Score Distillation loss, which leverages high-resolution predictions from the pre-trained teacher diffusion model.","Our method presents significant improvements on the test dataset collected from MSCOCO and OpenImage, validating the effectiveness of our proposed method in small object editing.","In particular, when comparing SOEDiff with SD-I model on the OpenImage-f dataset, we observe a 0.99 improvement in CLIP-Score and a reduction of 2.87 in FID.","Our project page can be found in https://soediff.github.io/."],"url":"http://arxiv.org/abs/2405.09114v1","category":"cs.CV"}
{"created":"2024-05-15 06:11:24","title":"Efficient LLM Jailbreak via Adaptive Dense-to-sparse Constrained Optimization","abstract":"Recent research indicates that large language models (LLMs) are susceptible to jailbreaking attacks that can generate harmful content. This paper introduces a novel token-level attack method, Adaptive Dense-to-Sparse Constrained Optimization (ADC), which effectively jailbreaks several open-source LLMs. Our approach relaxes the discrete jailbreak optimization into a continuous optimization and progressively increases the sparsity of the optimizing vectors. Consequently, our method effectively bridges the gap between discrete and continuous space optimization. Experimental results demonstrate that our method is more effective and efficient than existing token-level methods. On Harmbench, our method achieves state of the art attack success rate on seven out of eight LLMs. Code will be made available. Trigger Warning: This paper contains model behavior that can be offensive in nature.","sentences":["Recent research indicates that large language models (LLMs) are susceptible to jailbreaking attacks that can generate harmful content.","This paper introduces a novel token-level attack method, Adaptive Dense-to-Sparse Constrained Optimization (ADC), which effectively jailbreaks several open-source LLMs.","Our approach relaxes the discrete jailbreak optimization into a continuous optimization and progressively increases the sparsity of the optimizing vectors.","Consequently, our method effectively bridges the gap between discrete and continuous space optimization.","Experimental results demonstrate that our method is more effective and efficient than existing token-level methods.","On Harmbench, our method achieves state of the art attack success rate on seven out of eight LLMs.","Code will be made available.","Trigger Warning:","This paper contains model behavior that can be offensive in nature."],"url":"http://arxiv.org/abs/2405.09113v1","category":"cs.LG"}
{"created":"2024-05-15 05:57:20","title":"CarDreamer: Open-Source Learning Platform for World Model based Autonomous Driving","abstract":"To safely navigate intricate real-world scenarios, autonomous vehicles must be able to adapt to diverse road conditions and anticipate future events. World model (WM) based reinforcement learning (RL) has emerged as a promising approach by learning and predicting the complex dynamics of various environments. Nevertheless, to the best of our knowledge, there does not exist an accessible platform for training and testing such algorithms in sophisticated driving environments. To fill this void, we introduce CarDreamer, the first open-source learning platform designed specifically for developing WM based autonomous driving algorithms. It comprises three key components: 1) World model backbone: CarDreamer has integrated some state-of-the-art WMs, which simplifies the reproduction of RL algorithms. The backbone is decoupled from the rest and communicates using the standard Gym interface, so that users can easily integrate and test their own algorithms. 2) Built-in tasks: CarDreamer offers a comprehensive set of highly configurable driving tasks which are compatible with Gym interfaces and are equipped with empirically optimized reward functions. 3) Task development suite: This suite streamlines the creation of driving tasks, enabling easy definition of traffic flows and vehicle routes, along with automatic collection of multi-modal observation data. A visualization server allows users to trace real-time agent driving videos and performance metrics through a browser. Furthermore, we conduct extensive experiments using built-in tasks to evaluate the performance and potential of WMs in autonomous driving. Thanks to the richness and flexibility of CarDreamer, we also systematically study the impact of observation modality, observability, and sharing of vehicle intentions on AV safety and efficiency. All code and documents are accessible on https://github.com/ucd-dare/CarDreamer.","sentences":["To safely navigate intricate real-world scenarios, autonomous vehicles must be able to adapt to diverse road conditions and anticipate future events.","World model (WM) based reinforcement learning (RL) has emerged as a promising approach by learning and predicting the complex dynamics of various environments.","Nevertheless, to the best of our knowledge, there does not exist an accessible platform for training and testing such algorithms in sophisticated driving environments.","To fill this void, we introduce CarDreamer, the first open-source learning platform designed specifically for developing WM based autonomous driving algorithms.","It comprises three key components: 1) World model backbone: CarDreamer has integrated some state-of-the-art WMs, which simplifies the reproduction of RL algorithms.","The backbone is decoupled from the rest and communicates using the standard Gym interface, so that users can easily integrate and test their own algorithms.","2) Built-in tasks: CarDreamer offers a comprehensive set of highly configurable driving tasks which are compatible with Gym interfaces and are equipped with empirically optimized reward functions.","3) Task development suite: This suite streamlines the creation of driving tasks, enabling easy definition of traffic flows and vehicle routes, along with automatic collection of multi-modal observation data.","A visualization server allows users to trace real-time agent driving videos and performance metrics through a browser.","Furthermore, we conduct extensive experiments using built-in tasks to evaluate the performance and potential of WMs in autonomous driving.","Thanks to the richness and flexibility of CarDreamer, we also systematically study the impact of observation modality, observability, and sharing of vehicle intentions on AV safety and efficiency.","All code and documents are accessible on https://github.com/ucd-dare/CarDreamer."],"url":"http://arxiv.org/abs/2405.09111v1","category":"cs.RO"}
{"created":"2024-05-15 05:51:41","title":"Motion Prediction with Gaussian Processes for Safe Human-Robot Interaction in Virtual Environments","abstract":"Humans use collaborative robots as tools for accomplishing various tasks. The interaction between humans and robots happens in tight shared workspaces. However, these machines must be safe to operate alongside humans to minimize the risk of accidental collisions. Ensuring safety imposes many constraints, such as reduced torque and velocity limits during operation, thus increasing the time to accomplish many tasks. However, for applications such as using collaborative robots as haptic interfaces with intermittent contacts for virtual reality applications, speed limitations result in poor user experiences. This research aims to improve the efficiency of a collaborative robot while improving the safety of the human user. We used Gaussian process models to predict human hand motion and developed strategies for human intention detection based on hand motion and gaze to improve the time for the robot and human security in a virtual environment. We then studied the effect of prediction. Results from comparisons show that the prediction models improved the robot time by 3\\% and safety by 17\\%. When used alongside gaze, prediction with Gaussian process models resulted in an improvement of the robot time by 2\\% and the safety by 13\\%.","sentences":["Humans use collaborative robots as tools for accomplishing various tasks.","The interaction between humans and robots happens in tight shared workspaces.","However, these machines must be safe to operate alongside humans to minimize the risk of accidental collisions.","Ensuring safety imposes many constraints, such as reduced torque and velocity limits during operation, thus increasing the time to accomplish many tasks.","However, for applications such as using collaborative robots as haptic interfaces with intermittent contacts for virtual reality applications, speed limitations result in poor user experiences.","This research aims to improve the efficiency of a collaborative robot while improving the safety of the human user.","We used Gaussian process models to predict human hand motion and developed strategies for human intention detection based on hand motion and gaze to improve the time for the robot and human security in a virtual environment.","We then studied the effect of prediction.","Results from comparisons show that the prediction models improved the robot time by 3\\% and safety by 17\\%.","When used alongside gaze, prediction with Gaussian process models resulted in an improvement of the robot time by 2\\% and the safety by 13\\%."],"url":"http://arxiv.org/abs/2405.09109v1","category":"cs.RO"}
{"created":"2024-05-15 05:48:31","title":"A Linear Test for Global Nonlinear Controllability","abstract":"It is known that if a nonlinear control affine system without drift is bracket generating, then its associated sub-Laplacian is invertible under some conditions on the domain. In this note, we investigate the converse. We show how invertibility of the sub-Laplacian operator implies a weaker form of controllability, where the reachable sets of a neighborhood of a point have full measure. From a computational point of view, one can then use the spectral gap of the (infinite-dimensional) self-adjoint operator to define a notion of degree of controllability.   An essential tool to establish the converse result is to use the relation between invertibility of the sub-Laplacian to the the controllability of the corresponding continuity equation using possibly non-smooth controls. Then using Ambrosio-Gigli-Savare's superposition principle from optimal transport theory we relate it to controllability properties of the control system. While the proof can be considered of the Perron-Frobenius type, we also provide a second dual Koopman point of view.","sentences":["It is known that if a nonlinear control affine system without drift is bracket generating, then its associated sub-Laplacian is invertible under some conditions on the domain.","In this note, we investigate the converse.","We show how invertibility of the sub-Laplacian operator implies a weaker form of controllability, where the reachable sets of a neighborhood of a point have full measure.","From a computational point of view, one can then use the spectral gap of the (infinite-dimensional) self-adjoint operator to define a notion of degree of controllability.   ","An essential tool to establish the converse result is to use the relation between invertibility of the sub-Laplacian to the the controllability of the corresponding continuity equation using possibly non-smooth controls.","Then using Ambrosio-Gigli-Savare's superposition principle from optimal transport theory we relate it to controllability properties of the control system.","While the proof can be considered of the Perron-Frobenius type, we also provide a second dual Koopman point of view."],"url":"http://arxiv.org/abs/2405.09108v1","category":"math.OC"}
{"created":"2024-05-15 05:20:16","title":"Adaptive Koopman Embedding for Robust Control of Complex Dynamical Systems","abstract":"The discovery of linear embedding is the key to the synthesis of linear control techniques for nonlinear systems. In recent years, while Koopman operator theory has become a prominent approach for learning these linear embeddings through data-driven methods, these algorithms often exhibit limitations in generalizability beyond the distribution captured by training data and are not robust to changes in the nominal system dynamics induced by intrinsic or environmental factors. To overcome these limitations, this study presents an adaptive Koopman architecture capable of responding to the changes in system dynamics online. The proposed framework initially employs an autoencoder-based neural network that utilizes input-output information from the nominal system to learn the corresponding Koopman embedding offline. Subsequently, we augment this nominal Koopman architecture with a feed-forward neural network that learns to modify the nominal dynamics in response to any deviation between the predicted and observed lifted states, leading to improved generalization and robustness to a wide range of uncertainties and disturbances compared to contemporary methods. Extensive tracking control simulations, which are undertaken by integrating the proposed scheme within a Model Predictive Control framework, are used to highlight its robustness against measurement noise, disturbances, and parametric variations in system dynamics.","sentences":["The discovery of linear embedding is the key to the synthesis of linear control techniques for nonlinear systems.","In recent years, while Koopman operator theory has become a prominent approach for learning these linear embeddings through data-driven methods, these algorithms often exhibit limitations in generalizability beyond the distribution captured by training data and are not robust to changes in the nominal system dynamics induced by intrinsic or environmental factors.","To overcome these limitations, this study presents an adaptive Koopman architecture capable of responding to the changes in system dynamics online.","The proposed framework initially employs an autoencoder-based neural network that utilizes input-output information from the nominal system to learn the corresponding Koopman embedding offline.","Subsequently, we augment this nominal Koopman architecture with a feed-forward neural network that learns to modify the nominal dynamics in response to any deviation between the predicted and observed lifted states, leading to improved generalization and robustness to a wide range of uncertainties and disturbances compared to contemporary methods.","Extensive tracking control simulations, which are undertaken by integrating the proposed scheme within a Model Predictive Control framework, are used to highlight its robustness against measurement noise, disturbances, and parametric variations in system dynamics."],"url":"http://arxiv.org/abs/2405.09101v1","category":"cs.RO"}
{"created":"2024-05-15 05:13:20","title":"Optimizing Sensor Network Design for Multiple Coverage","abstract":"Sensor placement optimization methods have been studied extensively. They can be applied to a wide range of applications, including surveillance of known environments, optimal locations for 5G towers, and placement of missile defense systems. However, few works explore the robustness and efficiency of the resulting sensor network concerning sensor failure or adversarial attacks. This paper addresses this issue by optimizing for the least number of sensors to achieve multiple coverage of non-simply connected domains by a prescribed number of sensors. We introduce a new objective function for the greedy (next-best-view) algorithm to design efficient and robust sensor networks and derive theoretical bounds on the network's optimality. We further introduce a Deep Learning model to accelerate the algorithm for near real-time computations. The Deep Learning model requires the generation of training examples. Correspondingly, we show that understanding the geometric properties of the training data set provides important insights into the performance and training process of deep learning techniques. Finally, we demonstrate that a simple parallel version of the greedy approach using a simpler objective can be highly competitive.","sentences":["Sensor placement optimization methods have been studied extensively.","They can be applied to a wide range of applications, including surveillance of known environments, optimal locations for 5G towers, and placement of missile defense systems.","However, few works explore the robustness and efficiency of the resulting sensor network concerning sensor failure or adversarial attacks.","This paper addresses this issue by optimizing for the least number of sensors to achieve multiple coverage of non-simply connected domains by a prescribed number of sensors.","We introduce a new objective function for the greedy (next-best-view) algorithm to design efficient and robust sensor networks and derive theoretical bounds on the network's optimality.","We further introduce a Deep Learning model to accelerate the algorithm for near real-time computations.","The Deep Learning model requires the generation of training examples.","Correspondingly, we show that understanding the geometric properties of the training data set provides important insights into the performance and training process of deep learning techniques.","Finally, we demonstrate that a simple parallel version of the greedy approach using a simpler objective can be highly competitive."],"url":"http://arxiv.org/abs/2405.09096v1","category":"cs.LG"}
{"created":"2024-05-15 05:13:08","title":"Past instability of FLRW solutions of the Einstein-Euler-scalar field equations for linear equations of state $p=K\u03c1$ with $0 \\leq K<1/3$","abstract":"Using numerical methods, we examine, under a Gowdy symmetry assumption, the dynamics of nonlinearly perturbed FLRW fluid solutions of the Einstein-Euler-scalar field equations in the contracting direction for linear equations of state $p = K\\rho$ and sound speeds $0\\leq K<1/3$. This article builds upon the numerical work from \\cite{BMO:2023} in which perturbations of FLRW solutions to the Einstein-Euler equations with positive cosmological constant in the expanding time direction were studied. The numerical results presented here confirm that the instabilities observed in \\cite{BMO:2023,MarshallOliynyk:2022} for $1/3<K<1$, first conjectured to occur in the expanding direction by Rendall in \\cite{Rendall:2004}, are also present in the contracting direction over the complementary parameter range $0\\leq K<1/3$. Our numerical solutions show that the fractional density gradient of the nonlinear perturbations develop steep gradients near a finite number of spatial points and become unbounded towards the big bang. This behaviour, and in particular the characteristic profile of the fractional density gradient near the big bang, is strikingly similar to what was observed in the expanding direction near timelike infinity in the article \\cite{BMO:2023}.","sentences":["Using numerical methods, we examine, under a Gowdy symmetry assumption, the dynamics of nonlinearly perturbed FLRW fluid solutions of the Einstein-Euler-scalar field equations in the contracting direction for linear equations of state $p = K\\rho$ and sound speeds $0\\leq K<1/3$.","This article builds upon the numerical work from \\cite{BMO:2023} in which perturbations of FLRW solutions to the Einstein-Euler equations with positive cosmological constant in the expanding time direction were studied.","The numerical results presented here confirm that the instabilities observed in \\cite{BMO:2023,MarshallOliynyk:2022} for $1/3<K<1$, first conjectured to occur in the expanding direction by Rendall in \\cite{Rendall:2004}, are also present in the contracting direction over the complementary parameter range $0\\leq K<1/3$.","Our numerical solutions show that the fractional density gradient of the nonlinear perturbations develop steep gradients near a finite number of spatial points and become unbounded towards the big bang.","This behaviour, and in particular the characteristic profile of the fractional density gradient near the big bang, is strikingly similar to what was observed in the expanding direction near timelike infinity in the article \\cite{BMO:2023}."],"url":"http://arxiv.org/abs/2405.09095v1","category":"gr-qc"}
{"created":"2024-05-15 04:52:09","title":"Towards Next-Generation Steganalysis: LLMs Unleash the Power of Detecting Steganography","abstract":"Linguistic steganography provides convenient implementation to hide messages, particularly with the emergence of AI generation technology. The potential abuse of this technology raises security concerns within societies, calling for powerful linguistic steganalysis to detect carrier containing steganographic messages. Existing methods are limited to finding distribution differences between steganographic texts and normal texts from the aspect of symbolic statistics. However, the distribution differences of both kinds of texts are hard to build precisely, which heavily hurts the detection ability of the existing methods in realistic scenarios. To seek a feasible way to construct practical steganalysis in real world, this paper propose to employ human-like text processing abilities of large language models (LLMs) to realize the difference from the aspect of human perception, addition to traditional statistic aspect. Specifically, we systematically investigate the performance of LLMs in this task by modeling it as a generative paradigm, instead of traditional classification paradigm. Extensive experiment results reveal that generative LLMs exhibit significant advantages in linguistic steganalysis and demonstrate performance trends distinct from traditional approaches. Results also reveal that LLMs outperform existing baselines by a wide margin, and the domain-agnostic ability of LLMs makes it possible to train a generic steganalysis model (Both codes and trained models are openly available in https://github.com/ba0z1/Linguistic-Steganalysis-with-LLMs).","sentences":["Linguistic steganography provides convenient implementation to hide messages, particularly with the emergence of AI generation technology.","The potential abuse of this technology raises security concerns within societies, calling for powerful linguistic steganalysis to detect carrier containing steganographic messages.","Existing methods are limited to finding distribution differences between steganographic texts and normal texts from the aspect of symbolic statistics.","However, the distribution differences of both kinds of texts are hard to build precisely, which heavily hurts the detection ability of the existing methods in realistic scenarios.","To seek a feasible way to construct practical steganalysis in real world, this paper propose to employ human-like text processing abilities of large language models (LLMs) to realize the difference from the aspect of human perception, addition to traditional statistic aspect.","Specifically, we systematically investigate the performance of LLMs in this task by modeling it as a generative paradigm, instead of traditional classification paradigm.","Extensive experiment results reveal that generative LLMs exhibit significant advantages in linguistic steganalysis and demonstrate performance trends distinct from traditional approaches.","Results also reveal that LLMs outperform existing baselines by a wide margin, and the domain-agnostic ability of LLMs makes it possible to train a generic steganalysis model (Both codes and trained models are openly available in https://github.com/ba0z1/Linguistic-Steganalysis-with-LLMs)."],"url":"http://arxiv.org/abs/2405.09090v1","category":"cs.CR"}
{"created":"2024-05-15 04:47:31","title":"Chaos-based reinforcement learning with TD3","abstract":"Chaos-based reinforcement learning (CBRL) is a method in which the agent's internal chaotic dynamics drives exploration. This approach offers a model for considering how the biological brain can create variability in its behavior and learn in an exploratory manner. At the same time, it is a learning model that has the ability to automatically switch between exploration and exploitation modes and the potential to realize higher explorations that reflect what it has learned so far. However, the learning algorithms in CBRL have not been well-established in previous studies and have yet to incorporate recent advances in reinforcement learning. This study introduced Twin Delayed Deep Deterministic Policy Gradients (TD3), which is one of the state-of-the-art deep reinforcement learning algorithms that can treat deterministic and continuous action spaces, to CBRL. The validation results provide several insights. First, TD3 works as a learning algorithm for CBRL in a simple goal-reaching task. Second, CBRL agents with TD3 can autonomously suppress their exploratory behavior as learning progresses and resume exploration when the environment changes. Finally, examining the effect of the agent's chaoticity on learning shows that extremely strong chaos negatively impacts the flexible switching between exploration and exploitation.","sentences":["Chaos-based reinforcement learning (CBRL) is a method in which the agent's internal chaotic dynamics drives exploration.","This approach offers a model for considering how the biological brain can create variability in its behavior and learn in an exploratory manner.","At the same time, it is a learning model that has the ability to automatically switch between exploration and exploitation modes and the potential to realize higher explorations that reflect what it has learned so far.","However, the learning algorithms in CBRL have not been well-established in previous studies and have yet to incorporate recent advances in reinforcement learning.","This study introduced Twin Delayed Deep Deterministic Policy Gradients (TD3), which is one of the state-of-the-art deep reinforcement learning algorithms that can treat deterministic and continuous action spaces, to CBRL.","The validation results provide several insights.","First, TD3 works as a learning algorithm for CBRL in a simple goal-reaching task.","Second, CBRL agents with TD3 can autonomously suppress their exploratory behavior as learning progresses and resume exploration when the environment changes.","Finally, examining the effect of the agent's chaoticity on learning shows that extremely strong chaos negatively impacts the flexible switching between exploration and exploitation."],"url":"http://arxiv.org/abs/2405.09086v1","category":"cs.LG"}
{"created":"2024-05-15 04:22:27","title":"RSHazeDiff: A Unified Fourier-aware Diffusion Model for Remote Sensing Image Dehazing","abstract":"Haze severely degrades the visual quality of remote sensing images and hampers the performance of automotive navigation, intelligent monitoring, and urban management. The emerging denoising diffusion probabilistic model (DDPM) exhibits the significant potential for dense haze removal with its strong generation ability. Since remote sensing images contain extensive small-scale texture structures, it is important to effectively restore image details from hazy images. However, current wisdom of DDPM fails to preserve image details and color fidelity well, limiting its dehazing capacity for remote sensing images. In this paper, we propose a novel unified Fourier-aware diffusion model for remote sensing image dehazing, termed RSHazeDiff. From a new perspective, RSHazeDiff explores the conditional DDPM to improve image quality in dense hazy scenarios, and it makes three key contributions. First, RSHazeDiff refines the training phase of diffusion process by performing noise estimation and reconstruction constraints in a coarse-to-fine fashion. Thus, it remedies the unpleasing results caused by the simple noise estimation constraint in DDPM. Second, by taking the frequency information as important prior knowledge during iterative sampling steps, RSHazeDiff can preserve more texture details and color fidelity in dehazed images. Third, we design a global compensated learning module to utilize the Fourier transform to capture the global dependency features of input images, which can effectively mitigate the effects of boundary artifacts when processing fixed-size patches. Experiments on both synthetic and real-world benchmarks validate the favorable performance of RSHazeDiff over multiple state-of-the-art methods. Source code will be released at https://github.com/jm-xiong/RSHazeDiff.","sentences":["Haze severely degrades the visual quality of remote sensing images and hampers the performance of automotive navigation, intelligent monitoring, and urban management.","The emerging denoising diffusion probabilistic model (DDPM) exhibits the significant potential for dense haze removal with its strong generation ability.","Since remote sensing images contain extensive small-scale texture structures, it is important to effectively restore image details from hazy images.","However, current wisdom of DDPM fails to preserve image details and color fidelity well, limiting its dehazing capacity for remote sensing images.","In this paper, we propose a novel unified Fourier-aware diffusion model for remote sensing image dehazing, termed RSHazeDiff.","From a new perspective, RSHazeDiff explores the conditional DDPM to improve image quality in dense hazy scenarios, and it makes three key contributions.","First, RSHazeDiff refines the training phase of diffusion process by performing noise estimation and reconstruction constraints in a coarse-to-fine fashion.","Thus, it remedies the unpleasing results caused by the simple noise estimation constraint in DDPM.","Second, by taking the frequency information as important prior knowledge during iterative sampling steps, RSHazeDiff can preserve more texture details and color fidelity in dehazed images.","Third, we design a global compensated learning module to utilize the Fourier transform to capture the global dependency features of input images, which can effectively mitigate the effects of boundary artifacts when processing fixed-size patches.","Experiments on both synthetic and real-world benchmarks validate the favorable performance of RSHazeDiff over multiple state-of-the-art methods.","Source code will be released at https://github.com/jm-xiong/RSHazeDiff."],"url":"http://arxiv.org/abs/2405.09083v1","category":"cs.CV"}
{"created":"2024-05-15 04:09:46","title":"Explainable AI for Ship Collision Avoidance: Decoding Decision-Making Processes and Behavioral Intentions","abstract":"This study developed an explainable AI for ship collision avoidance. Initially, a critic network composed of sub-task critic networks was proposed to individually evaluate each sub-task in collision avoidance to clarify the AI decision-making processes involved. Additionally, an attempt was made to discern behavioral intentions through a Q-value analysis and an Attention mechanism. The former focused on interpreting intentions by examining the increment of the Q-value resulting from AI actions, while the latter incorporated the significance of other ships in the decision-making process for collision avoidance into the learning objective. AI's behavioral intentions in collision avoidance were visualized by combining the perceived collision danger with the degree of attention to other ships. The proposed method was evaluated through a numerical experiment. The developed AI was confirmed to be able to safely avoid collisions under various congestion levels, and AI's decision-making process was rendered comprehensible to humans. The proposed method not only facilitates the understanding of DRL-based controllers/systems in the ship collision avoidance task but also extends to any task comprising sub-tasks.","sentences":["This study developed an explainable AI for ship collision avoidance.","Initially, a critic network composed of sub-task critic networks was proposed to individually evaluate each sub-task in collision avoidance to clarify the AI decision-making processes involved.","Additionally, an attempt was made to discern behavioral intentions through a Q-value analysis and an Attention mechanism.","The former focused on interpreting intentions by examining the increment of the Q-value resulting from AI actions, while the latter incorporated the significance of other ships in the decision-making process for collision avoidance into the learning objective.","AI's behavioral intentions in collision avoidance were visualized by combining the perceived collision danger with the degree of attention to other ships.","The proposed method was evaluated through a numerical experiment.","The developed AI was confirmed to be able to safely avoid collisions under various congestion levels, and AI's decision-making process was rendered comprehensible to humans.","The proposed method not only facilitates the understanding of DRL-based controllers/systems in the ship collision avoidance task but also extends to any task comprising sub-tasks."],"url":"http://arxiv.org/abs/2405.09081v1","category":"cs.RO"}
{"created":"2024-05-15 04:09:30","title":"Causal Inference for a Hidden Treatment","abstract":"In many empirical settings, directly observing a treatment variable may be infeasible although an error-prone surrogate measurement of the latter will often be available. Causal inference based solely on the observed surrogate measurement of the hidden treatment may be particularly challenging without an additional assumption or auxiliary data. To address this issue, we propose a method that carefully incorporates the surrogate measurement together with a proxy of the hidden treatment to identify its causal effect on any scale for which identification would in principle be feasible had contrary to fact the treatment been observed error-free. Beyond identification, we provide general semiparametric theory for causal effects identified using our approach, and we derive a large class of semiparametric estimators with an appealing multiple robustness property. A significant obstacle to our approach is the estimation of nuisance functions involving the hidden treatment, which prevents the direct application of standard machine learning algorithms. To resolve this, we introduce a novel semiparametric EM algorithm, thus adding a practical dimension to our theoretical contributions. This methodology can be adapted to analyze a large class of causal parameters in the proposed hidden treatment model, including the population average treatment effect, the effect of treatment on the treated, quantile treatment effects, and causal effects under marginal structural models. We examine the finite-sample performance of our method using simulations and an application which aims to estimate the causal effect of Alzheimer's disease on hippocampal volume using data from the Alzheimer's Disease Neuroimaging Initiative.","sentences":["In many empirical settings, directly observing a treatment variable may be infeasible although an error-prone surrogate measurement of the latter will often be available.","Causal inference based solely on the observed surrogate measurement of the hidden treatment may be particularly challenging without an additional assumption or auxiliary data.","To address this issue, we propose a method that carefully incorporates the surrogate measurement together with a proxy of the hidden treatment to identify its causal effect on any scale for which identification would in principle be feasible had contrary to fact the treatment been observed error-free.","Beyond identification, we provide general semiparametric theory for causal effects identified using our approach, and we derive a large class of semiparametric estimators with an appealing multiple robustness property.","A significant obstacle to our approach is the estimation of nuisance functions involving the hidden treatment, which prevents the direct application of standard machine learning algorithms.","To resolve this, we introduce a novel semiparametric EM algorithm, thus adding a practical dimension to our theoretical contributions.","This methodology can be adapted to analyze a large class of causal parameters in the proposed hidden treatment model, including the population average treatment effect, the effect of treatment on the treated, quantile treatment effects, and causal effects under marginal structural models.","We examine the finite-sample performance of our method using simulations and an application which aims to estimate the causal effect of Alzheimer's disease on hippocampal volume using data from the Alzheimer's Disease Neuroimaging Initiative."],"url":"http://arxiv.org/abs/2405.09080v1","category":"stat.ME"}
{"created":"2024-05-15 03:51:18","title":"Effect of a fixed downstream cylinder on the flow-induced vibration of an elastically-supported primary cylinder","abstract":"This paper numerically investigates the influence of a fixed downstream control cylinder on the flow-induced vibration of an elastically-supported primary cylinder. These two cylinders are situated in a tandem arrangement with small dimensionless centre-to-centre spacing ($L/D$, $L$ is the intermediate spacing, and $D$ is the cylinder diameter). The present two-dimensional (2D) simulations are carried out in the low Reynolds number ($Re$) regime. The primary focus of this study is to reveal the underlying flow physics behind the transition from vortex-induced vibration to galloping in the response of the primary cylinder due to the presence of another fixed downstream cylinder. Two distinct flow field regimes, namely steady flow and alternate attachment regimes, are observed for different $L/D$ and Re values. Depending on the evolution of the near-field flow structures, four different wake patterns - `2S', `2P', `2C', and `aperiodic' - are observed. The corresponding vibration response of the upstream cylinder is characterized as interference galloping and extended vortex-induced vibration. As the $L/D$ ratio increases, the lift enhancement due to flow-induced vibration is seen to be weakened. The detailed correlation between the force generation and the near-wake interactions is investigated. The present findings will augment the understanding of vibration reduction or flow-induced energy harvesting of tandem cylindrical structures.","sentences":["This paper numerically investigates the influence of a fixed downstream control cylinder on the flow-induced vibration of an elastically-supported primary cylinder.","These two cylinders are situated in a tandem arrangement with small dimensionless centre-to-centre spacing ($L/D$, $L$ is the intermediate spacing, and $D$ is the cylinder diameter).","The present two-dimensional (2D) simulations are carried out in the low Reynolds number ($Re$) regime.","The primary focus of this study is to reveal the underlying flow physics behind the transition from vortex-induced vibration to galloping in the response of the primary cylinder due to the presence of another fixed downstream cylinder.","Two distinct flow field regimes, namely steady flow and alternate attachment regimes, are observed for different $L/D$ and Re values.","Depending on the evolution of the near-field flow structures, four different wake patterns - `2S', `2P', `2C', and `aperiodic' - are observed.","The corresponding vibration response of the upstream cylinder is characterized as interference galloping and extended vortex-induced vibration.","As the $L/D$ ratio increases, the lift enhancement due to flow-induced vibration is seen to be weakened.","The detailed correlation between the force generation and the near-wake interactions is investigated.","The present findings will augment the understanding of vibration reduction or flow-induced energy harvesting of tandem cylindrical structures."],"url":"http://arxiv.org/abs/2405.09072v1","category":"physics.flu-dyn"}
{"created":"2024-05-15 03:38:09","title":"A unifying construction of semifields of order $p^{2m}$","abstract":"In this article, we present two new constructions for semifields of order $p^{2m}$. Together, the constructions unify and generalize around a dozen distinct semifield constructions, including both the oldest known construction by Dickson and the largest known construction in odd characteristic by Taniguchi. The constructions also provably yield many new semifields. We give precise conditions when the new semifields we find are equivalent and count precisely how many new inequivalent semifields we construct.","sentences":["In this article, we present two new constructions for semifields of order $p^{2m}$. Together, the constructions unify and generalize around a dozen distinct semifield constructions, including both the oldest known construction by Dickson and the largest known construction in odd characteristic by Taniguchi.","The constructions also provably yield many new semifields.","We give precise conditions when the new semifields we find are equivalent and count precisely how many new inequivalent semifields we construct."],"url":"http://arxiv.org/abs/2405.09068v1","category":"math.CO"}
{"created":"2024-05-15 03:28:47","title":"Magnetic Properties of NH$_4$H$_2$PO$_4$ and KH$_2$PO$_4$: Emergence of Multiferroic Salts","abstract":"We observe sharp step-down discontinuities in the magnetic susceptibility of NH$_4$H$_2$PO$_4$ and NH$_4$H$_2$PO$_4$-$d$$_{60}$ (60% deuterated) along the $a$ and $c$-axes occurring exactly at their antiferroelectric transition temperatures. For the case of KH$_2$PO$_4$, less pronounced discontinuities occur at the ferroelectric transition temperature. To explain this, we treat the acid protons as individual oscillators that generate current elements which translate to magnetic forces in near resonance with each other. With decreasing temperature, the resonant forces become more commensurate which amplifies a disproportionate drop off of two types of magnetic forces to eventually trigger the structural phase transitions. For the case of NH$_4$H$_2$PO$_4$, the associated internal magnetic field appears to aid the NH$_4$$^+$ to order at higher temperature. At 49 K, a shoulder-like anomaly in both NH$_4$H$_2$PO$_4$ and KH$_2$PO$_4$ is attributed to a possible onset of macroscopic quantum tunneling of protons. Our findings bring forth a new category of intrinsic multiferroic systems.","sentences":["We observe sharp step-down discontinuities in the magnetic susceptibility of NH$_4$H$_2$PO$_4$ and NH$_4$H$_2$PO$_4$-$d$$_{60}$ (60% deuterated) along the $a$ and $c$-axes occurring exactly at their antiferroelectric transition temperatures.","For the case of KH$_2$PO$_4$, less pronounced discontinuities occur at the ferroelectric transition temperature.","To explain this, we treat the acid protons as individual oscillators that generate current elements which translate to magnetic forces in near resonance with each other.","With decreasing temperature, the resonant forces become more commensurate which amplifies a disproportionate drop off of two types of magnetic forces to eventually trigger the structural phase transitions.","For the case of NH$_4$H$_2$PO$_4$, the associated internal magnetic field appears to aid the NH$_4$$^+$ to order at higher temperature.","At 49 K, a shoulder-like anomaly in both NH$_4$H$_2$PO$_4$ and KH$_2$PO$_4$ is attributed to a possible onset of macroscopic quantum tunneling of protons.","Our findings bring forth a new category of intrinsic multiferroic systems."],"url":"http://arxiv.org/abs/2405.09064v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-15 03:26:01","title":"Naturalistic Music Decoding from EEG Data via Latent Diffusion Models","abstract":"In this article, we explore the potential of using latent diffusion models, a family of powerful generative models, for the task of reconstructing naturalistic music from electroencephalogram (EEG) recordings. Unlike simpler music with limited timbres, such as MIDI-generated tunes or monophonic pieces, the focus here is on intricate music featuring a diverse array of instruments, voices, and effects, rich in harmonics and timbre. This study represents an initial foray into achieving general music reconstruction of high-quality using non-invasive EEG data, employing an end-to-end training approach directly on raw data without the need for manual pre-processing and channel selection. We train our models on the public NMED-T dataset and perform quantitative evaluation proposing neural embedding-based metrics. We additionally perform song classification based on the generated tracks. Our work contributes to the ongoing research in neural decoding and brain-computer interfaces, offering insights into the feasibility of using EEG data for complex auditory information reconstruction.","sentences":["In this article, we explore the potential of using latent diffusion models, a family of powerful generative models, for the task of reconstructing naturalistic music from electroencephalogram (EEG) recordings.","Unlike simpler music with limited timbres, such as MIDI-generated tunes or monophonic pieces, the focus here is on intricate music featuring a diverse array of instruments, voices, and effects, rich in harmonics and timbre.","This study represents an initial foray into achieving general music reconstruction of high-quality using non-invasive EEG data, employing an end-to-end training approach directly on raw data without the need for manual pre-processing and channel selection.","We train our models on the public NMED-T dataset and perform quantitative evaluation proposing neural embedding-based metrics.","We additionally perform song classification based on the generated tracks.","Our work contributes to the ongoing research in neural decoding and brain-computer interfaces, offering insights into the feasibility of using EEG data for complex auditory information reconstruction."],"url":"http://arxiv.org/abs/2405.09062v1","category":"cs.SD"}
{"created":"2024-05-15 16:03:56","title":"Dominating Lengthscales of Zebrafish Collective Behaviour","abstract":"Collective behaviour in living systems is observed across many scales, from bacteria to insects, to fish shoals. Zebrafish have emerged as a model system amenable to laboratory study. Here we report a three-dimensional study of the collective dynamics of fifty zebrafish. We observed the emergence of collective behaviour changing between \\yy{ordered} to randomised, upon \\yy{adaptation} to new environmental conditions. We quantify the spatial and temporal correlation functions of the fish and identify two length scales, the persistence length and the nearest neighbour distance, that capture the essence of the behavioural changes. The ratio of the two length scales correlates robustly with the polarisation of collective motion that we explain with a reductionist model of self--propelled particles with alignment interactions.","sentences":["Collective behaviour in living systems is observed across many scales, from bacteria to insects, to fish shoals.","Zebrafish have emerged as a model system amenable to laboratory study.","Here we report a three-dimensional study of the collective dynamics of fifty zebrafish.","We observed the emergence of collective behaviour changing between \\yy{ordered} to randomised, upon \\yy{adaptation} to new environmental conditions.","We quantify the spatial and temporal correlation functions of the fish and identify two length scales, the persistence length and the nearest neighbour distance, that capture the essence of the behavioural changes.","The ratio of the two length scales correlates robustly with the polarisation of collective motion that we explain with a reductionist model of self--propelled particles with alignment interactions."],"url":"http://arxiv.org/abs/2405.09469v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-15 15:50:43","title":"Efficient pooling designs and screening performance in group testing for two type defectives","abstract":"Group testing is utilized in the case when we want to find a few defectives among large amount of items. Testing n items one by one requires n tests, but if the ratio of defectives is small, group testing is an efficient way to reduce the number of tests. Many research have been developed for group testing for a single type of defectives. In this paper, we consider the case where two types of defective A and B exist. For two types of defectives, we develop a belief propagation algorithm to compute marginal posterior probability of defectives. Furthermore, we construct several kinds of collections of pools in order to test for A and B. And by utilizing our belief propagation algorithm, we evaluate the performance of group testing by conducting simulations.","sentences":["Group testing is utilized in the case when we want to find a few defectives among large amount of items.","Testing n items one by one requires n tests, but if the ratio of defectives is small, group testing is an efficient way to reduce the number of tests.","Many research have been developed for group testing for a single type of defectives.","In this paper, we consider the case where two types of defective A and B exist.","For two types of defectives, we develop a belief propagation algorithm to compute marginal posterior probability of defectives.","Furthermore, we construct several kinds of collections of pools in order to test for A and B. And by utilizing our belief propagation algorithm, we evaluate the performance of group testing by conducting simulations."],"url":"http://arxiv.org/abs/2405.09455v1","category":"stat.CO"}
{"created":"2024-05-15 13:53:29","title":"Full-wave EM simulation analysis of human body blockage by dense 2D antenna arrays","abstract":"Recently, proposals of human-sensing-based services for cellular and local area networks have brought indoor localization to the attention of several research groups. In response to these stimuli, various Device-Free Localization (DFL) techniques, also known as Passive Localization methods, have emerged by exploiting ambient signals to locate and track individuals that do not carry any electronic device. This study delves into human passive indoor localization through full-wave electromagnetic simulations. For the scope, we exploit simulations from the commercial tool FEKO software that employs the Method of Moments (MoM). In particular, we collect and analyze the electric field values in a scenario constituted by a dense 2D/3D deployment of receivers in the presence of an anthropomorphic mobile target. The paper describes in detail the collected dataset and provides a first analysis based on a statistical approach. Possible use cases are also investigated through examples in the context of passive localization, sensing, and imaging.","sentences":["Recently, proposals of human-sensing-based services for cellular and local area networks have brought indoor localization to the attention of several research groups.","In response to these stimuli, various Device-Free Localization (DFL) techniques, also known as Passive Localization methods, have emerged by exploiting ambient signals to locate and track individuals that do not carry any electronic device.","This study delves into human passive indoor localization through full-wave electromagnetic simulations.","For the scope, we exploit simulations from the commercial tool FEKO software that employs the Method of Moments (MoM).","In particular, we collect and analyze the electric field values in a scenario constituted by a dense 2D/3D deployment of receivers in the presence of an anthropomorphic mobile target.","The paper describes in detail the collected dataset and provides a first analysis based on a statistical approach.","Possible use cases are also investigated through examples in the context of passive localization, sensing, and imaging."],"url":"http://arxiv.org/abs/2405.09346v1","category":"eess.SP"}
{"created":"2024-05-15 13:21:30","title":"Search for new physics in high-mass diphoton events from proton-proton collisions at $\\sqrt{s}$ = 13 TeV","abstract":"Results are presented from a search for new physics in high-mass diphoton events from proton-proton collisions at $\\sqrt{s}$ = 13 TeV. The data set was collected in 2016-2018 with the CMS detector at the LHC and corresponds to an integrated luminosity of 138 fb$^{-1}$. Events with a diphoton invariant mass greater than 500\\GeV are considered. Two different techniques are used to predict the standard model backgrounds: parametric fits to the smoothly-falling background and a first-principles calculation of the standard model diphoton spectrum at next-to-next-to-leading order in perturbative quantum chromodynamics calculations. The first technique is sensitive to resonant excesses while the second technique can identify broad differences in the invariant mass shape. The data are used to constrain the production of heavy Higgs bosons, Randall-Sundrum gravitons, the large extra dimensions model of Arkani-Hamed, Dimopoulos, and Dvali (ADD), and the continuum clockwork mechanism. No statistically significant excess is observed. The present results are the strongest limits to date on ADD extra dimensions and RS gravitons with a coupling parameter greater than 0.1.","sentences":["Results are presented from a search for new physics in high-mass diphoton events from proton-proton collisions at $\\sqrt{s}$ = 13 TeV.","The data set was collected in 2016-2018 with the CMS detector at the LHC and corresponds to an integrated luminosity of 138 fb$^{-1}$. Events with a diphoton invariant mass greater than 500\\GeV are considered.","Two different techniques are used to predict the standard model backgrounds: parametric fits to the smoothly-falling background and a first-principles calculation of the standard model diphoton spectrum at next-to-next-to-leading order in perturbative quantum chromodynamics calculations.","The first technique is sensitive to resonant excesses while the second technique can identify broad differences in the invariant mass shape.","The data are used to constrain the production of heavy Higgs bosons, Randall-Sundrum gravitons, the large extra dimensions model of Arkani-Hamed, Dimopoulos, and Dvali (ADD), and the continuum clockwork mechanism.","No statistically significant excess is observed.","The present results are the strongest limits to date on ADD extra dimensions and RS gravitons with a coupling parameter greater than 0.1."],"url":"http://arxiv.org/abs/2405.09320v1","category":"hep-ex"}
{"created":"2024-05-15 13:11:28","title":"Agnostic Active Learning of Single Index Models with Linear Sample Complexity","abstract":"We study active learning methods for single index models of the form $F({\\mathbf x}) = f(\\langle {\\mathbf w}, {\\mathbf x}\\rangle)$, where $f:\\mathbb{R} \\to \\mathbb{R}$ and ${\\mathbf x,\\mathbf w} \\in \\mathbb{R}^d$. In addition to their theoretical interest as simple examples of non-linear neural networks, single index models have received significant recent attention due to applications in scientific machine learning like surrogate modeling for partial differential equations (PDEs). Such applications require sample-efficient active learning methods that are robust to adversarial noise. I.e., that work even in the challenging agnostic learning setting.   We provide two main results on agnostic active learning of single index models. First, when $f$ is known and Lipschitz, we show that $\\tilde{O}(d)$ samples collected via {statistical leverage score sampling} are sufficient to learn a near-optimal single index model. Leverage score sampling is simple to implement, efficient, and already widely used for actively learning linear models. Our result requires no assumptions on the data distribution, is optimal up to log factors, and improves quadratically on a recent ${O}(d^{2})$ bound of \\cite{gajjar2023active}. Second, we show that $\\tilde{O}(d)$ samples suffice even in the more difficult setting when $f$ is \\emph{unknown}. Our results leverage tools from high dimensional probability, including Dudley's inequality and dual Sudakov minoration, as well as a novel, distribution-aware discretization of the class of Lipschitz functions.","sentences":["We study active learning methods for single index models of the form $F({\\mathbf x})","= f(\\langle {\\mathbf w}, {\\mathbf x}\\rangle)$, where $f:\\mathbb{R} \\to \\mathbb{R}$ and ${\\mathbf x,\\mathbf w} \\in \\mathbb{R}^d$.","In addition to their theoretical interest as simple examples of non-linear neural networks, single index models have received significant recent attention due to applications in scientific machine learning like surrogate modeling for partial differential equations (PDEs).","Such applications require sample-efficient active learning methods that are robust to adversarial noise.","I.e., that work even in the challenging agnostic learning setting.   ","We provide two main results on agnostic active learning of single index models.","First, when $f$ is known and Lipschitz, we show that $\\tilde{O}(d)$ samples collected via {statistical leverage score sampling} are sufficient to learn a near-optimal single index model.","Leverage score sampling is simple to implement, efficient, and already widely used for actively learning linear models.","Our result requires no assumptions on the data distribution, is optimal up to log factors, and improves quadratically on a recent ${O}(d^{2})$ bound of \\cite{gajjar2023active}.","Second, we show that $\\tilde{O}(d)$ samples suffice even in the more difficult setting when $f$ is \\emph{unknown}.","Our results leverage tools from high dimensional probability, including Dudley's inequality and dual Sudakov minoration, as well as a novel, distribution-aware discretization of the class of Lipschitz functions."],"url":"http://arxiv.org/abs/2405.09312v1","category":"cs.LG"}
{"created":"2024-05-15 10:30:53","title":"Design and commissioning of a high-level control system for a medical isochronous cyclotron","abstract":"MEDICYC (MEDical CYClotron) is an isochronous cyclotron dedicated to radiotherapy which was built and commissioned in Nice, France, in 1990 by a local team aided by experts from CERN. The cyclotron accelerates negative H to a maximum energy of 65 MeV and uses stripping to extract a proton beam. Its primary purpose is treating ocular melanoma by protontherapy but a significant research activity is also present on beam-lines dedicated for this purpose. An extensive refurbishment program of the cyclotron has been started to cope with the end-of-life and/or the obsolescence of several sub-systems. In this context, a new high-level cyclotron control system has been developed and commissioned in 2021-2024. The primary responsibility of the system is the high-level coordination of the source, the RF system, the beam-line and cyclotron magnets, to produce and deliver a beam with a given set of characteristics. A secondary responsibility is the collection, visualization and analysis of sub-system and beam data for monitoring and pre-emptive fault detection. In this contribution, the control system software architecture is presented and the infrastructure on which the systems are deployed is laid out.","sentences":["MEDICYC (MEDical CYClotron) is an isochronous cyclotron dedicated to radiotherapy which was built and commissioned in Nice, France, in 1990 by a local team aided by experts from CERN.","The cyclotron accelerates negative H to a maximum energy of 65 MeV and uses stripping to extract a proton beam.","Its primary purpose is treating ocular melanoma by protontherapy but a significant research activity is also present on beam-lines dedicated for this purpose.","An extensive refurbishment program of the cyclotron has been started to cope with the end-of-life and/or the obsolescence of several sub-systems.","In this context, a new high-level cyclotron control system has been developed and commissioned in 2021-2024.","The primary responsibility of the system is the high-level coordination of the source, the RF system, the beam-line and cyclotron magnets, to produce and deliver a beam with a given set of characteristics.","A secondary responsibility is the collection, visualization and analysis of sub-system and beam data for monitoring and pre-emptive fault detection.","In this contribution, the control system software architecture is presented and the infrastructure on which the systems are deployed is laid out."],"url":"http://arxiv.org/abs/2405.09235v1","category":"physics.acc-ph"}
{"created":"2024-05-15 08:46:33","title":"Influence Maximization in Hypergraphs Using A Genetic Algorithm with New Initialization and Evaluation Methods","abstract":"Influence maximization (IM) is a crucial optimization task related to analyzing complex networks in the real world, such as social networks, disease propagation networks, and marketing networks. Publications to date about the IM problem focus mainly on graphs, which fail to capture high-order interaction relationships from the real world. Therefore, the use of hypergraphs for addressing the IM problem has been receiving increasing attention. However, identifying the most influential nodes in hypergraphs remains challenging, mainly because nodes and hyperedges are often strongly coupled and correlated. In this paper, to effectively identify the most influential nodes, we first propose a novel hypergraph-independent cascade model that integrates the influences of both node and hyperedge failures. Afterward, we introduce genetic algorithms (GA) to identify the most influential nodes that leverage hypergraph collective influences. In the GA-based method, the hypergraph collective influence is effectively used to initialize the population, thereby enhancing the quality of initial candidate solutions. The designed fitness function considers the joint influences of both nodes and hyperedges. This ensures the optimal set of nodes with the best influence on both nodes and hyperedges to be evaluated accurately. Moreover, a new mutation operator is designed by introducing factors, i.e., the collective influence and overlapping effects of nodes in hypergraphs, to breed high-quality offspring. In the experiments, several simulations on both synthetic and real hypergraphs have been conducted, and the results demonstrate that the proposed method outperforms the compared methods.","sentences":["Influence maximization (IM) is a crucial optimization task related to analyzing complex networks in the real world, such as social networks, disease propagation networks, and marketing networks.","Publications to date about the IM problem focus mainly on graphs, which fail to capture high-order interaction relationships from the real world.","Therefore, the use of hypergraphs for addressing the IM problem has been receiving increasing attention.","However, identifying the most influential nodes in hypergraphs remains challenging, mainly because nodes and hyperedges are often strongly coupled and correlated.","In this paper, to effectively identify the most influential nodes, we first propose a novel hypergraph-independent cascade model that integrates the influences of both node and hyperedge failures.","Afterward, we introduce genetic algorithms (GA) to identify the most influential nodes that leverage hypergraph collective influences.","In the GA-based method, the hypergraph collective influence is effectively used to initialize the population, thereby enhancing the quality of initial candidate solutions.","The designed fitness function considers the joint influences of both nodes and hyperedges.","This ensures the optimal set of nodes with the best influence on both nodes and hyperedges to be evaluated accurately.","Moreover, a new mutation operator is designed by introducing factors, i.e., the collective influence and overlapping effects of nodes in hypergraphs, to breed high-quality offspring.","In the experiments, several simulations on both synthetic and real hypergraphs have been conducted, and the results demonstrate that the proposed method outperforms the compared methods."],"url":"http://arxiv.org/abs/2405.09185v1","category":"cs.SI"}
{"created":"2024-05-15 07:17:06","title":"Evaluation scheme for children-centered language interaction competence of AI-driven robots","abstract":"This article explores the evaluation method for the language communication proficiency of AI-driven robots engaging in interactive communication with children. The utilization of AI-driven robots in children's everyday communication is swiftly advancing, underscoring the importance of evaluating these robots'language communication skills. Based on 11 Chinese families' interviews and thematic analysis of the comment text from shopping websites, a framework is introduced in the article to assess five key dimensions of child-robot language communication: interactivity, specificity, development, sociality, and safety. We draw on the concept of \"children's agency\", viewing children as active participants in shaping society and cultural life alongside adults. Therefore, this article places particular emphasis on collecting data related to children. Whether through survey interviews or direct interactive experiments, we treat children as an independent object for data collection. The study involved empirical research following the mentioned framework, which involved capturing interaction videos in natural conversation settings among children from 6 families. Analysis was performed on quantitative data obtained from video recordings, alongside questionnaires and interviews carried out by parents acting as participants or observers. We found that the presence or absence of parents during children's interactions with robots can impact the evaluation of robots'language communication abilities. Ultimately, this article proposes an enhanced comprehensive evaluation framework incorporating insights from parents and children, supported by empirical evidence and inter-rater consistency assessments, showcasing the scheme's efficacy.","sentences":["This article explores the evaluation method for the language communication proficiency of AI-driven robots engaging in interactive communication with children.","The utilization of AI-driven robots in children's everyday communication is swiftly advancing, underscoring the importance of evaluating these robots'language communication skills.","Based on 11 Chinese families' interviews and thematic analysis of the comment text from shopping websites, a framework is introduced in the article to assess five key dimensions of child-robot language communication: interactivity, specificity, development, sociality, and safety.","We draw on the concept of \"children's agency\", viewing children as active participants in shaping society and cultural life alongside adults.","Therefore, this article places particular emphasis on collecting data related to children.","Whether through survey interviews or direct interactive experiments, we treat children as an independent object for data collection.","The study involved empirical research following the mentioned framework, which involved capturing interaction videos in natural conversation settings among children from 6 families.","Analysis was performed on quantitative data obtained from video recordings, alongside questionnaires and interviews carried out by parents acting as participants or observers.","We found that the presence or absence of parents during children's interactions with robots can impact the evaluation of robots'language communication abilities.","Ultimately, this article proposes an enhanced comprehensive evaluation framework incorporating insights from parents and children, supported by empirical evidence and inter-rater consistency assessments, showcasing the scheme's efficacy."],"url":"http://arxiv.org/abs/2405.09144v1","category":"cs.HC"}
{"created":"2024-05-15 03:07:42","title":"CTS: A Consistency-Based Medical Image Segmentation Model","abstract":"In medical image segmentation tasks, diffusion models have shown significant potential. However, mainstream diffusion models suffer from drawbacks such as multiple sampling times and slow prediction results. Recently, consistency models, as a standalone generative network, have resolved this issue. Compared to diffusion models, consistency models can reduce the sampling times to once, not only achieving similar generative effects but also significantly speeding up training and prediction. However, they are not suitable for image segmentation tasks, and their application in the medical imaging field has not yet been explored. Therefore, this paper applies the consistency model to medical image segmentation tasks, designing multi-scale feature signal supervision modes and loss function guidance to achieve model convergence. Experiments have verified that the CTS model can obtain better medical image segmentation results with a single sampling during the test phase.","sentences":["In medical image segmentation tasks, diffusion models have shown significant potential.","However, mainstream diffusion models suffer from drawbacks such as multiple sampling times and slow prediction results.","Recently, consistency models, as a standalone generative network, have resolved this issue.","Compared to diffusion models, consistency models can reduce the sampling times to once, not only achieving similar generative effects but also significantly speeding up training and prediction.","However, they are not suitable for image segmentation tasks, and their application in the medical imaging field has not yet been explored.","Therefore, this paper applies the consistency model to medical image segmentation tasks, designing multi-scale feature signal supervision modes and loss function guidance to achieve model convergence.","Experiments have verified that the CTS model can obtain better medical image segmentation results with a single sampling during the test phase."],"url":"http://arxiv.org/abs/2405.09056v1","category":"cs.CV"}
{"created":"2024-05-15 02:54:11","title":"Perception Without Vision for Trajectory Prediction: Ego Vehicle Dynamics as Scene Representation for Efficient Active Learning in Autonomous Driving","abstract":"This study investigates the use of trajectory and dynamic state information for efficient data curation in autonomous driving machine learning tasks. We propose methods for clustering trajectory-states and sampling strategies in an active learning framework, aiming to reduce annotation and data costs while maintaining model performance. Our approach leverages trajectory information to guide data selection, promoting diversity in the training data. We demonstrate the effectiveness of our methods on the trajectory prediction task using the nuScenes dataset, showing consistent performance gains over random sampling across different data pool sizes, and even reaching sub-baseline displacement errors at just 50% of the data cost. Our results suggest that sampling typical data initially helps overcome the ''cold start problem,'' while introducing novelty becomes more beneficial as the training pool size increases. By integrating trajectory-state-informed active learning, we demonstrate that more efficient and robust autonomous driving systems are possible and practical using low-cost data curation strategies.","sentences":["This study investigates the use of trajectory and dynamic state information for efficient data curation in autonomous driving machine learning tasks.","We propose methods for clustering trajectory-states and sampling strategies in an active learning framework, aiming to reduce annotation and data costs while maintaining model performance.","Our approach leverages trajectory information to guide data selection, promoting diversity in the training data.","We demonstrate the effectiveness of our methods on the trajectory prediction task using the nuScenes dataset, showing consistent performance gains over random sampling across different data pool sizes, and even reaching sub-baseline displacement errors at just 50% of the data cost.","Our results suggest that sampling typical data initially helps overcome the ''cold start problem,'' while introducing novelty becomes more beneficial as the training pool size increases.","By integrating trajectory-state-informed active learning, we demonstrate that more efficient and robust autonomous driving systems are possible and practical using low-cost data curation strategies."],"url":"http://arxiv.org/abs/2405.09049v1","category":"cs.LG"}
{"created":"2024-05-15 02:31:26","title":"Exploring the Individuality and Collectivity of Intents behind Interactions for Graph Collaborative Filtering","abstract":"Intent modeling has attracted widespread attention in recommender systems. As the core motivation behind user selection of items, intent is crucial for elucidating recommendation results. The current mainstream modeling method is to abstract the intent into unknowable but learnable shared or non-shared parameters. Despite considerable progress, we argue that it still confronts the following challenges: firstly, these methods only capture the coarse-grained aspects of intent, ignoring the fact that user-item interactions will be affected by collective and individual factors (e.g., a user may choose a movie because of its high box office or because of his own unique preferences); secondly, modeling believable intent is severely hampered by implicit feedback, which is incredibly sparse and devoid of true semantics. To address these challenges, we propose a novel recommendation framework designated as Bilateral Intent-guided Graph Collaborative Filtering (BIGCF). Specifically, we take a closer look at user-item interactions from a causal perspective and put forth the concepts of individual intent-which signifies private preferences-and collective intent-which denotes overall awareness. To counter the sparsity of implicit feedback, the feature distributions of users and items are encoded via a Gaussian-based graph generation strategy, and we implement the recommendation process through bilateral intent-guided graph reconstruction re-sampling. Finally, we propose graph contrastive regularization for both interaction and intent spaces to uniformize users, items, intents, and interactions in a self-supervised and non-augmented paradigm. Experimental results on three real-world datasets demonstrate the effectiveness of BIGCF compared with existing solutions.","sentences":["Intent modeling has attracted widespread attention in recommender systems.","As the core motivation behind user selection of items, intent is crucial for elucidating recommendation results.","The current mainstream modeling method is to abstract the intent into unknowable but learnable shared or non-shared parameters.","Despite considerable progress, we argue that it still confronts the following challenges: firstly, these methods only capture the coarse-grained aspects of intent, ignoring the fact that user-item interactions will be affected by collective and individual factors (e.g., a user may choose a movie because of its high box office or because of his own unique preferences); secondly, modeling believable intent is severely hampered by implicit feedback, which is incredibly sparse and devoid of true semantics.","To address these challenges, we propose a novel recommendation framework designated as Bilateral Intent-guided Graph Collaborative Filtering (BIGCF).","Specifically, we take a closer look at user-item interactions from a causal perspective and put forth the concepts of individual intent-which signifies private preferences-and collective intent-which denotes overall awareness.","To counter the sparsity of implicit feedback, the feature distributions of users and items are encoded via a Gaussian-based graph generation strategy, and we implement the recommendation process through bilateral intent-guided graph reconstruction re-sampling.","Finally, we propose graph contrastive regularization for both interaction and intent spaces to uniformize users, items, intents, and interactions in a self-supervised and non-augmented paradigm.","Experimental results on three real-world datasets demonstrate the effectiveness of BIGCF compared with existing solutions."],"url":"http://arxiv.org/abs/2405.09042v1","category":"cs.IR"}
{"created":"2024-05-15 02:13:51","title":"Unmasking Efficiency: Learning Salient Sparse Models in Non-IID Federated Learning","abstract":"In this work, we propose Salient Sparse Federated Learning (SSFL), a streamlined approach for sparse federated learning with efficient communication. SSFL identifies a sparse subnetwork prior to training, leveraging parameter saliency scores computed separately on local client data in non-IID scenarios, and then aggregated, to determine a global mask. Only the sparse model weights are communicated each round between the clients and the server. We validate SSFL's effectiveness using standard non-IID benchmarks, noting marked improvements in the sparsity--accuracy trade-offs. Finally, we deploy our method in a real-world federated learning framework and report improvement in communication time.","sentences":["In this work, we propose Salient Sparse Federated Learning (SSFL), a streamlined approach for sparse federated learning with efficient communication.","SSFL identifies a sparse subnetwork prior to training, leveraging parameter saliency scores computed separately on local client data in non-IID scenarios, and then aggregated, to determine a global mask.","Only the sparse model weights are communicated each round between the clients and the server.","We validate SSFL's effectiveness using standard non-IID benchmarks, noting marked improvements in the sparsity--accuracy trade-offs.","Finally, we deploy our method in a real-world federated learning framework and report improvement in communication time."],"url":"http://arxiv.org/abs/2405.09037v1","category":"cs.LG"}
{"created":"2024-05-15 01:29:28","title":"Dynamic Loss Decay based Robust Oriented Object Detection on Remote Sensing Images with Noisy Labels","abstract":"The ambiguous appearance, tiny scale, and fine-grained classes of objects in remote sensing imagery inevitably lead to the noisy annotations in category labels of detection dataset. However, the effects and treatments of the label noises are underexplored in modern oriented remote sensing object detectors. To address this issue, we propose a robust oriented remote sensing object detection method through dynamic loss decay (DLD) mechanism, inspired by the two phase ``early-learning'' and ``memorization'' learning dynamics of deep neural networks on clean and noisy samples. To be specific, we first observe the end point of early learning phase termed as EL, after which the models begin to memorize the false labels that significantly degrade the detection accuracy. Secondly, under the guidance of the training indicator, the losses of each sample are ranked in descending order, and we adaptively decay the losses of the top K largest ones (bad samples) in the following epochs. Because these large losses are of high confidence to be calculated with wrong labels. Experimental results show that the method achieves excellent noise resistance performance tested on multiple public datasets such as HRSC2016 and DOTA-v1.0/v2.0 with synthetic category label noise. Our solution also has won the 2st place in the \"fine-grained object detection based on sub-meter remote sensing imagery\" track with noisy labels of 2023 National Big Data and Computing Intelligence Challenge.","sentences":["The ambiguous appearance, tiny scale, and fine-grained classes of objects in remote sensing imagery inevitably lead to the noisy annotations in category labels of detection dataset.","However, the effects and treatments of the label noises are underexplored in modern oriented remote sensing object detectors.","To address this issue, we propose a robust oriented remote sensing object detection method through dynamic loss decay (DLD) mechanism, inspired by the two phase ``early-learning'' and ``memorization'' learning dynamics of deep neural networks on clean and noisy samples.","To be specific, we first observe the end point of early learning phase termed as EL, after which the models begin to memorize the false labels that significantly degrade the detection accuracy.","Secondly, under the guidance of the training indicator, the losses of each sample are ranked in descending order, and we adaptively decay the losses of the top K largest ones (bad samples) in the following epochs.","Because these large losses are of high confidence to be calculated with wrong labels.","Experimental results show that the method achieves excellent noise resistance performance tested on multiple public datasets such as HRSC2016 and DOTA-v1.0/v2.0 with synthetic category label noise.","Our solution also has won the 2st place in the \"fine-grained object detection based on sub-meter remote sensing imagery\" track with noisy labels of 2023 National Big Data and Computing Intelligence Challenge."],"url":"http://arxiv.org/abs/2405.09024v1","category":"cs.CV"}
{"created":"2024-05-15 00:54:40","title":"A Japanese-Chinese Parallel Corpus Using Crowdsourcing for Web Mining","abstract":"Using crowdsourcing, we collected more than 10,000 URL pairs (parallel top page pairs) of bilingual websites that contain parallel documents and created a Japanese-Chinese parallel corpus of 4.6M sentence pairs from these websites. We used a Japanese-Chinese bilingual dictionary of 160K word pairs for document and sentence alignment. We then used high-quality 1.2M Japanese-Chinese sentence pairs to train a parallel corpus filter based on statistical language models and word translation probabilities. We compared the translation accuracy of the model trained on these 4.6M sentence pairs with that of the model trained on Japanese-Chinese sentence pairs from CCMatrix (12.4M), a parallel corpus from global web mining. Although our corpus is only one-third the size of CCMatrix, we found that the accuracy of the two models was comparable and confirmed that it is feasible to use crowdsourcing for web mining of parallel data.","sentences":["Using crowdsourcing, we collected more than 10,000 URL pairs (parallel top page pairs) of bilingual websites that contain parallel documents and created a Japanese-Chinese parallel corpus of 4.6M sentence pairs from these websites.","We used a Japanese-Chinese bilingual dictionary of 160K word pairs for document and sentence alignment.","We then used high-quality 1.2M Japanese-Chinese sentence pairs to train a parallel corpus filter based on statistical language models and word translation probabilities.","We compared the translation accuracy of the model trained on these 4.6M sentence pairs with that of the model trained on Japanese-Chinese sentence pairs from CCMatrix (12.4M), a parallel corpus from global web mining.","Although our corpus is only one-third the size of CCMatrix, we found that the accuracy of the two models was comparable and confirmed that it is feasible to use crowdsourcing for web mining of parallel data."],"url":"http://arxiv.org/abs/2405.09017v1","category":"cs.CL"}
{"created":"2024-05-14 23:47:02","title":"Robust Approximate Sampling via Stochastic Gradient Barker Dynamics","abstract":"Stochastic Gradient (SG) Markov Chain Monte Carlo algorithms (MCMC) are popular algorithms for Bayesian sampling in the presence of large datasets. However, they come with little theoretical guarantees and assessing their empirical performances is non-trivial. In such context, it is crucial to develop algorithms that are robust to the choice of hyperparameters and to gradients heterogeneity since, in practice, both the choice of step-size and behaviour of target gradients induce hard-to-control biases in the invariant distribution. In this work we introduce the stochastic gradient Barker dynamics (SGBD) algorithm, extending the recently developed Barker MCMC scheme, a robust alternative to Langevin-based sampling algorithms, to the stochastic gradient framework. We characterize the impact of stochastic gradients on the Barker transition mechanism and develop a bias-corrected version that, under suitable assumptions, eliminates the error due to the gradient noise in the proposal. We illustrate the performance on a number of high-dimensional examples, showing that SGBD is more robust to hyperparameter tuning and to irregular behavior of the target gradients compared to the popular stochastic gradient Langevin dynamics algorithm.","sentences":["Stochastic Gradient (SG) Markov Chain Monte Carlo algorithms (MCMC) are popular algorithms for Bayesian sampling in the presence of large datasets.","However, they come with little theoretical guarantees and assessing their empirical performances is non-trivial.","In such context, it is crucial to develop algorithms that are robust to the choice of hyperparameters and to gradients heterogeneity since, in practice, both the choice of step-size and behaviour of target gradients induce hard-to-control biases in the invariant distribution.","In this work we introduce the stochastic gradient Barker dynamics (SGBD) algorithm, extending the recently developed Barker MCMC scheme, a robust alternative to Langevin-based sampling algorithms, to the stochastic gradient framework.","We characterize the impact of stochastic gradients on the Barker transition mechanism and develop a bias-corrected version that, under suitable assumptions, eliminates the error due to the gradient noise in the proposal.","We illustrate the performance on a number of high-dimensional examples, showing that SGBD is more robust to hyperparameter tuning and to irregular behavior of the target gradients compared to the popular stochastic gradient Langevin dynamics algorithm."],"url":"http://arxiv.org/abs/2405.08999v1","category":"stat.ML"}
{"created":"2024-05-14 23:03:52","title":"What is it for a Machine Learning Model to Have a Capability?","abstract":"What can contemporary machine learning (ML) models do? Given the proliferation of ML models in society, answering this question matters to a variety of stakeholders, both public and private. The evaluation of models' capabilities is rapidly emerging as a key subfield of modern ML, buoyed by regulatory attention and government grants. Despite this, the notion of an ML model possessing a capability has not been interrogated: what are we saying when we say that a model is able to do something? And what sorts of evidence bear upon this question? In this paper, we aim to answer these questions, using the capabilities of large language models (LLMs) as a running example. Drawing on the large philosophical literature on abilities, we develop an account of ML models' capabilities which can be usefully applied to the nascent science of model evaluation. Our core proposal is a conditional analysis of model abilities (CAMA): crudely, a machine learning model has a capability to X just when it would reliably succeed at doing X if it 'tried'. The main contribution of the paper is making this proposal precise in the context of ML, resulting in an operationalisation of CAMA applicable to LLMs. We then put CAMA to work, showing that it can help make sense of various features of ML model evaluation practice, as well as suggest procedures for performing fair inter-model comparisons.","sentences":["What can contemporary machine learning (ML) models do?","Given the proliferation of ML models in society, answering this question matters to a variety of stakeholders, both public and private.","The evaluation of models' capabilities is rapidly emerging as a key subfield of modern ML, buoyed by regulatory attention and government grants.","Despite this, the notion of an ML model possessing a capability has not been interrogated: what are we saying when we say that a model is able to do something?","And what sorts of evidence bear upon this question?","In this paper, we aim to answer these questions, using the capabilities of large language models (LLMs) as a running example.","Drawing on the large philosophical literature on abilities, we develop an account of ML models' capabilities which can be usefully applied to the nascent science of model evaluation.","Our core proposal is a conditional analysis of model abilities (CAMA): crudely, a machine learning model has a capability to X just when it would reliably succeed at doing X if it 'tried'.","The main contribution of the paper is making this proposal precise in the context of ML, resulting in an operationalisation of CAMA applicable to LLMs.","We then put CAMA to work, showing that it can help make sense of various features of ML model evaluation practice, as well as suggest procedures for performing fair inter-model comparisons."],"url":"http://arxiv.org/abs/2405.08989v1","category":"cs.AI"}
{"created":"2024-05-14 22:01:04","title":"A distribution-free valid p-value for finite samples of bounded random variables","abstract":"We build a valid p-value based on a concentration inequality for bounded random variables introduced by Pelekis, Ramon and Wang. The motivation behind this work is the calibration of predictive algorithms in a distribution-free setting. The super-uniform p-value is tighter than Hoeffding and Bentkus alternatives in certain regions. Even though we are motivated by a calibration setting in a machine learning context, the ideas presented in this work are also relevant in classical statistical inference. Furthermore, we compare the power of a collection of valid p- values for bounded losses, which are presented in previous literature.","sentences":["We build a valid p-value based on a concentration inequality for bounded random variables introduced by Pelekis, Ramon and Wang.","The motivation behind this work is the calibration of predictive algorithms in a distribution-free setting.","The super-uniform p-value is tighter than Hoeffding and Bentkus alternatives in certain regions.","Even though we are motivated by a calibration setting in a machine learning context, the ideas presented in this work are also relevant in classical statistical inference.","Furthermore, we compare the power of a collection of valid p- values for bounded losses, which are presented in previous literature."],"url":"http://arxiv.org/abs/2405.08975v1","category":"stat.ML"}
{"created":"2024-05-14 21:20:27","title":"Wearable Sensor-Based Few-Shot Continual Learning on Hand Gestures for Motor-Impaired Individuals via Latent Embedding Exploitation","abstract":"Hand gestures can provide a natural means of human-computer interaction and enable people who cannot speak to communicate efficiently. Existing hand gesture recognition methods heavily depend on pre-defined gestures, however, motor-impaired individuals require new gestures tailored to each individual's gesture motion and style. Gesture samples collected from different persons have distribution shifts due to their health conditions, the severity of the disability, motion patterns of the arms, etc. In this paper, we introduce the Latent Embedding Exploitation (LEE) mechanism in our replay-based Few-Shot Continual Learning (FSCL) framework that significantly improves the performance of fine-tuning a model for out-of-distribution data. Our method produces a diversified latent feature space by leveraging a preserved latent embedding known as \\textit{gesture prior knowledge}, along with \\textit{intra-gesture divergence} derived from two additional embeddings. Thus, the model can capture latent statistical structure in highly variable gestures with limited samples. We conduct an experimental evaluation using the SmartWatch Gesture and the Motion Gesture datasets. The proposed method results in an average test accuracy of 57.0\\%, 64.6\\%, and 69.3\\% by using one, three, and five samples for six different gestures. Our method helps motor-impaired persons leverage wearable devices, and their unique styles of movement can be learned and applied in human-computer interaction and social communication.","sentences":["Hand gestures can provide a natural means of human-computer interaction and enable people who cannot speak to communicate efficiently.","Existing hand gesture recognition methods heavily depend on pre-defined gestures, however, motor-impaired individuals require new gestures tailored to each individual's gesture motion and style.","Gesture samples collected from different persons have distribution shifts due to their health conditions, the severity of the disability, motion patterns of the arms, etc.","In this paper, we introduce the Latent Embedding Exploitation (LEE) mechanism in our replay-based Few-Shot Continual Learning (FSCL) framework that significantly improves the performance of fine-tuning a model for out-of-distribution data.","Our method produces a diversified latent feature space by leveraging a preserved latent embedding known as \\textit{gesture prior knowledge}, along with \\textit{intra-gesture divergence} derived from two additional embeddings.","Thus, the model can capture latent statistical structure in highly variable gestures with limited samples.","We conduct an experimental evaluation using the SmartWatch Gesture and the Motion Gesture datasets.","The proposed method results in an average test accuracy of 57.0\\%, 64.6\\%, and 69.3\\% by using one, three, and five samples for six different gestures.","Our method helps motor-impaired persons leverage wearable devices, and their unique styles of movement can be learned and applied in human-computer interaction and social communication."],"url":"http://arxiv.org/abs/2405.08969v1","category":"cs.LG"}
{"created":"2024-05-14 21:12:01","title":"LLMs are Meaning-Typed Code Constructs","abstract":"Programming with Generative AI (GenAI) models is a type of Neurosymbolic programming and has seen tremendous adoption across many domains. However, leveraging GenAI models in code today can be complex, counter-intuitive and often require specialized frameworks, leading to increased complexity. This is because it is currently unclear as to the right abstractions through which we should marry GenAI models with the nature of traditional programming code constructs. In this paper, we introduce a set of novel abstractions to help bridge the gap between Neuro- and symbolic programming. We introduce Meaning, a new specialized type that represents the underlying semantic value of traditional types (e.g., string). We make the case that GenAI models, LLMs in particular, should be reasoned as a meaning-type wrapped code construct at the language level. We formulate the problem of translation between meaning and traditional types and propose Automatic Meaning-Type Transformation (A-MTT), a runtime feature that abstracts this translation away from the developers by automatically converting between M eaning and types at the interface of LLM invocation. Leveraging this new set of code constructs and OTT, we demonstrate example implementation of neurosymbolic programs that seamlessly utilizes LLMs to solve problems in place of potentially complex traditional programming logic.","sentences":["Programming with Generative AI (GenAI) models is a type of Neurosymbolic programming and has seen tremendous adoption across many domains.","However, leveraging GenAI models in code today can be complex, counter-intuitive and often require specialized frameworks, leading to increased complexity.","This is because it is currently unclear as to the right abstractions through which we should marry GenAI models with the nature of traditional programming code constructs.","In this paper, we introduce a set of novel abstractions to help bridge the gap between Neuro- and symbolic programming.","We introduce Meaning, a new specialized type that represents the underlying semantic value of traditional types (e.g., string).","We make the case that GenAI models, LLMs in particular, should be reasoned as a meaning-type wrapped code construct at the language level.","We formulate the problem of translation between meaning and traditional types and propose Automatic Meaning-Type Transformation (A-MTT), a runtime feature that abstracts this translation away from the developers by automatically converting between M eaning and types at the interface of LLM invocation.","Leveraging this new set of code constructs and OTT, we demonstrate example implementation of neurosymbolic programs that seamlessly utilizes LLMs to solve problems in place of potentially complex traditional programming logic."],"url":"http://arxiv.org/abs/2405.08965v1","category":"cs.PL"}
{"created":"2024-05-14 21:01:12","title":"Bird's-Eye View to Street-View: A Survey","abstract":"In recent years, street view imagery has grown to become one of the most important sources of geospatial data collection and urban analytics, which facilitates generating meaningful insights and assisting in decision-making. Synthesizing a street-view image from its corresponding satellite image is a challenging task due to the significant differences in appearance and viewpoint between the two domains. In this study, we screened 20 recent research papers to provide a thorough review of the state-of-the-art of how street-view images are synthesized from their corresponding satellite counterparts. The main findings are: (i) novel deep learning techniques are required for synthesizing more realistic and accurate street-view images; (ii) more datasets need to be collected for public usage; and (iii) more specific evaluation metrics need to be investigated for evaluating the generated images appropriately. We conclude that, due to applying outdated deep learning techniques, the recent literature failed to generate detailed and diverse street-view images.","sentences":["In recent years, street view imagery has grown to become one of the most important sources of geospatial data collection and urban analytics, which facilitates generating meaningful insights and assisting in decision-making.","Synthesizing a street-view image from its corresponding satellite image is a challenging task due to the significant differences in appearance and viewpoint between the two domains.","In this study, we screened 20 recent research papers to provide a thorough review of the state-of-the-art of how street-view images are synthesized from their corresponding satellite counterparts.","The main findings are: (i) novel deep learning techniques are required for synthesizing more realistic and accurate street-view images; (ii) more datasets need to be collected for public usage; and (iii) more specific evaluation metrics need to be investigated for evaluating the generated images appropriately.","We conclude that, due to applying outdated deep learning techniques, the recent literature failed to generate detailed and diverse street-view images."],"url":"http://arxiv.org/abs/2405.08961v1","category":"cs.CV"}
{"created":"2024-05-14 20:33:19","title":"Task-Oriented Mulsemedia Communication using Unified Perceiver and Conformal Prediction in 6G Metaverse Systems","abstract":"The growing prominence of extended reality (XR), holographic-type communications, and metaverse demands truly immersive user experiences by using many sensory modalities, including sight, hearing, touch, smell, taste, etc. Additionally, the widespread deployment of sensors in areas such as agriculture, manufacturing, and smart homes is generating a diverse array of sensory data. A new media format known as multisensory media (mulsemedia) has emerged, which incorporates a wide range of sensory modalities beyond the traditional visual and auditory media. 6G wireless systems are envisioned to support the internet of senses, making it crucial to explore effective data fusion and communication strategies for mulsemedia. In this paper, we introduce a task-oriented multi-task mulsemedia communication system named MuSeCo, which is developed using unified Perceiver models and Conformal Prediction. This unified model can accept any sensory input and efficiently extract latent semantic features, making it adaptable for deployment across various Artificial Intelligence of Things (AIoT) devices. Conformal Prediction is employed for modality selection and combination, enhancing task accuracy while minimizing data communication overhead. The model has been trained using six sensory modalities across four classification tasks. Simulations and experiments demonstrate that MuSeCo can effectively select and combine sensory modalities, significantly reduce end-to-end communication latency and energy consumption, and maintain high accuracy in communication-constrained systems.","sentences":["The growing prominence of extended reality (XR), holographic-type communications, and metaverse demands truly immersive user experiences by using many sensory modalities, including sight, hearing, touch, smell, taste, etc.","Additionally, the widespread deployment of sensors in areas such as agriculture, manufacturing, and smart homes is generating a diverse array of sensory data.","A new media format known as multisensory media (mulsemedia) has emerged, which incorporates a wide range of sensory modalities beyond the traditional visual and auditory media.","6G wireless systems are envisioned to support the internet of senses, making it crucial to explore effective data fusion and communication strategies for mulsemedia.","In this paper, we introduce a task-oriented multi-task mulsemedia communication system named MuSeCo, which is developed using unified Perceiver models and Conformal Prediction.","This unified model can accept any sensory input and efficiently extract latent semantic features, making it adaptable for deployment across various Artificial Intelligence of Things (AIoT) devices.","Conformal Prediction is employed for modality selection and combination, enhancing task accuracy while minimizing data communication overhead.","The model has been trained using six sensory modalities across four classification tasks.","Simulations and experiments demonstrate that MuSeCo can effectively select and combine sensory modalities, significantly reduce end-to-end communication latency and energy consumption, and maintain high accuracy in communication-constrained systems."],"url":"http://arxiv.org/abs/2405.08949v1","category":"eess.SP"}
{"created":"2024-05-14 20:17:22","title":"Challenges in Deploying Long-Context Transformers: A Theoretical Peak Performance Analysis","abstract":"Transformer-based long context generative models power emerging AI applications like hour-long video understanding and project-level coding agent. Deploying long context transformers (e.g., 100K to 10M tokens) is prohibitively expensive compared to short context (e.g., 4K tokens) model variants. Reducing the cost of long-context transformers is becoming a pressing research and engineering challenge starting from the year of 2024. This work describes a concurrent programming framework for quantitatively analyzing the efficiency challenges in serving multiple long-context requests under limited size of GPU high-bandwidth memory (HBM) regime. We give a detailed analysis of how all additional computational costs, compared to 4K context, trace back to \\textit{one single source: the large size of the KV cache}. We use a 34B GPT-3.5 level model of 50K context on A100 NVLink as a running example, and describe how its large KV cache causes four types of deployment challenges: (1) prefilling long inputs takes much longer compute time and GPU memory than short inputs; (2) after prefilling, the large KV cache residing on the GPU HBM substantially restricts the number of concurrent users being served; (3) during decoding, repeatedly reading the KV cache from HBM to SM largely increases latency; (4) when KV cache memory overflows, swapping it from HBM to DDR causes significant context switching latency. We use this framework to analyze existing works and identify possibilities of combining them to build end-to-end systems. Overall, this work offers a foundational framework for analyzing long context transformer deployment and identifies directions towards reducing the inference cost of 1M context to be as cheap as 4K.","sentences":["Transformer-based long context generative models power emerging AI applications like hour-long video understanding and project-level coding agent.","Deploying long context transformers (e.g., 100K to 10M tokens) is prohibitively expensive compared to short context (e.g., 4K tokens) model variants.","Reducing the cost of long-context transformers is becoming a pressing research and engineering challenge starting from the year of 2024.","This work describes a concurrent programming framework for quantitatively analyzing the efficiency challenges in serving multiple long-context requests under limited size of GPU high-bandwidth memory (HBM) regime.","We give a detailed analysis of how all additional computational costs, compared to 4K context, trace back to \\textit{one single source: the large size of the KV cache}.","We use a 34B GPT-3.5 level model of 50K context on A100 NVLink as a running example, and describe how its large KV cache causes four types of deployment challenges: (1) prefilling long inputs takes much longer compute time and GPU memory than short inputs; (2) after prefilling, the large KV cache residing on the GPU HBM substantially restricts the number of concurrent users being served; (3) during decoding, repeatedly reading the KV cache from HBM to SM largely increases latency; (4) when KV cache memory overflows, swapping it from HBM to DDR causes significant context switching latency.","We use this framework to analyze existing works and identify possibilities of combining them to build end-to-end systems.","Overall, this work offers a foundational framework for analyzing long context transformer deployment and identifies directions towards reducing the inference cost of 1M context to be as cheap as 4K."],"url":"http://arxiv.org/abs/2405.08944v1","category":"cs.LG"}
{"created":"2024-05-14 19:53:20","title":"Self-supervised vision-langage alignment of deep learning representations for bone X-rays analysis","abstract":"This paper proposes leveraging vision-language pretraining on bone X-rays paired with French reports to address downstream tasks of interest on bone radiography. A practical processing pipeline is introduced to anonymize and process French medical reports. Pretraining then consists in the self-supervised alignment of visual and textual embedding spaces derived from deep model encoders. The resulting image encoder is then used to handle various downstream tasks, including quantification of osteoarthritis, estimation of bone age on pediatric wrists, bone fracture and anomaly detection. Our approach demonstrates competitive performance on downstream tasks, compared to alternatives requiring a significantly larger amount of human expert annotations. Our work stands as the first study to integrate French reports to shape the embedding space devoted to bone X-Rays representations, capitalizing on the large quantity of paired images and reports data available in an hospital. By relying on generic vision-laguage deep models in a language-specific scenario, it contributes to the deployement of vision models for wider healthcare applications.","sentences":["This paper proposes leveraging vision-language pretraining on bone X-rays paired with French reports to address downstream tasks of interest on bone radiography.","A practical processing pipeline is introduced to anonymize and process French medical reports.","Pretraining then consists in the self-supervised alignment of visual and textual embedding spaces derived from deep model encoders.","The resulting image encoder is then used to handle various downstream tasks, including quantification of osteoarthritis, estimation of bone age on pediatric wrists, bone fracture and anomaly detection.","Our approach demonstrates competitive performance on downstream tasks, compared to alternatives requiring a significantly larger amount of human expert annotations.","Our work stands as the first study to integrate French reports to shape the embedding space devoted to bone X-Rays representations, capitalizing on the large quantity of paired images and reports data available in an hospital.","By relying on generic vision-laguage deep models in a language-specific scenario, it contributes to the deployement of vision models for wider healthcare applications."],"url":"http://arxiv.org/abs/2405.08932v1","category":"cs.CV"}
{"created":"2024-05-14 19:45:00","title":"Directional cues affect the collective behaviour of Self propelled particles in one dimension","abstract":"This study explores the effect of quenched disorder on the characteristic of self-propelled particles in one-dimension. Here,particles interact with disorder which serve as directional cues. The study investigates how the density of the disorder influence the emergence of ordering and clustering in the collection of the self propelled particles. We introduce the microscopic model as well as corresponding coarse-grained equations of motion for the local density and the orientation of particle. Disorder affects the macroscopic ordering in the system, the size of the ordered clusters decays algebraically with disorder. Further, the disorder also affects the clustering of particles; in the presence of disorder, a big macroscopic cluster breaks into small clusters, leads to the localization of particles around it and results in high density around the disorder.","sentences":["This study explores the effect of quenched disorder on the characteristic of self-propelled particles in one-dimension.","Here,particles interact with disorder which serve as directional cues.","The study investigates how the density of the disorder influence the emergence of ordering and clustering in the collection of the self propelled particles.","We introduce the microscopic model as well as corresponding coarse-grained equations of motion for the local density and the orientation of particle.","Disorder affects the macroscopic ordering in the system, the size of the ordered clusters decays algebraically with disorder.","Further, the disorder also affects the clustering of particles; in the presence of disorder, a big macroscopic cluster breaks into small clusters, leads to the localization of particles around it and results in high density around the disorder."],"url":"http://arxiv.org/abs/2405.08925v1","category":"cond-mat.soft"}
{"created":"2024-05-14 19:07:08","title":"High dimensional test for functional covariates","abstract":"As medical devices become more complex, they routinely collect extensive and complicated data. While classical regressions typically examine the relationship between an outcome and a vector of predictors, it becomes imperative to identify the relationship with predictors possessing functional structures. In this article, we introduce a novel inference procedure for examining the relationship between outcomes and large-scale functional predictors. We target testing the linear hypothesis on the functional parameters under the generalized functional linear regression framework, where the number of the functional parameters grows with the sample size. We develop the estimation procedure for the high dimensional generalized functional linear model incorporating B-spline functional approximation and amenable regularization. Furthermore, we construct a procedure that is able to test the local alternative hypothesis on the linear combinations of the functional parameters. We establish the statistical guarantees in terms of non-asymptotic convergence of the parameter estimation and the oracle property and asymptotic normality of the estimators. Moreover, we derive the asymptotic distribution of the test statistic. We carry out intensive simulations and illustrate with a new dataset from an Alzheimer's disease magnetoencephalography study.","sentences":["As medical devices become more complex, they routinely collect extensive and complicated data.","While classical regressions typically examine the relationship between an outcome and a vector of predictors, it becomes imperative to identify the relationship with predictors possessing functional structures.","In this article, we introduce a novel inference procedure for examining the relationship between outcomes and large-scale functional predictors.","We target testing the linear hypothesis on the functional parameters under the generalized functional linear regression framework, where the number of the functional parameters grows with the sample size.","We develop the estimation procedure for the high dimensional generalized functional linear model incorporating B-spline functional approximation and amenable regularization.","Furthermore, we construct a procedure that is able to test the local alternative hypothesis on the linear combinations of the functional parameters.","We establish the statistical guarantees in terms of non-asymptotic convergence of the parameter estimation and the oracle property and asymptotic normality of the estimators.","Moreover, we derive the asymptotic distribution of the test statistic.","We carry out intensive simulations and illustrate with a new dataset from an Alzheimer's disease magnetoencephalography study."],"url":"http://arxiv.org/abs/2405.08912v1","category":"stat.ME"}
{"created":"2024-05-14 18:47:04","title":"fNIRS Analysis of Interaction Techniques in Touchscreen-Based Educational Gaming","abstract":"Touchscreens are becoming increasingly widespread in educational games, enhancing the quality of learner experience. Traditional metrics are often used to evaluate various input modalities, including hand and stylus. However, there exists a gap in understanding the cognitive impacts of these modalities during educational gameplay, which can be addressed through brain signal analysis to gain deeper insights into the underlying cognitive function and necessary brain resources for each condition. This facilitates a more precise comparison between conditions. In this study, we compared the brain signal and user experience of using hands and stylus on touchscreens while playing an educational game by analyzing hemodynamic response and self-reported measures. Participants engaged in a Unity-based educational quiz game using both hand and stylus on a touchscreen in a counterbalanced within-subject design. Oxygenated and deoxygenated hemoglobin data were collected using fNIRS, alongside quiz performance scores and standardized and customized user experience questionnaire ratings. Our findings show almost the same performance level with both input modalities, however, the hand requires less oxygen flow which suggests a lower cognitive effort than using a stylus while playing the educational game. Although the result shows that the stylus condition required more neural involvement than the hand condition, there is no significant difference between the use of both input modalities. However, there is a statistically significant difference in self-reported measures that support the findings mentioned above, favoring the hand that enhances understanding of modality effects in interactive educational environments.","sentences":["Touchscreens are becoming increasingly widespread in educational games, enhancing the quality of learner experience.","Traditional metrics are often used to evaluate various input modalities, including hand and stylus.","However, there exists a gap in understanding the cognitive impacts of these modalities during educational gameplay, which can be addressed through brain signal analysis to gain deeper insights into the underlying cognitive function and necessary brain resources for each condition.","This facilitates a more precise comparison between conditions.","In this study, we compared the brain signal and user experience of using hands and stylus on touchscreens while playing an educational game by analyzing hemodynamic response and self-reported measures.","Participants engaged in a Unity-based educational quiz game using both hand and stylus on a touchscreen in a counterbalanced within-subject design.","Oxygenated and deoxygenated hemoglobin data were collected using fNIRS, alongside quiz performance scores and standardized and customized user experience questionnaire ratings.","Our findings show almost the same performance level with both input modalities, however, the hand requires less oxygen flow which suggests a lower cognitive effort than using a stylus while playing the educational game.","Although the result shows that the stylus condition required more neural involvement than the hand condition, there is no significant difference between the use of both input modalities.","However, there is a statistically significant difference in self-reported measures that support the findings mentioned above, favoring the hand that enhances understanding of modality effects in interactive educational environments."],"url":"http://arxiv.org/abs/2405.08906v1","category":"cs.HC"}
{"created":"2024-05-14 18:05:44","title":"Large Language Models for Human-Machine Collaborative Particle Accelerator Tuning through Natural Language","abstract":"Autonomous tuning of particle accelerators is an active and challenging field of research with the goal of enabling novel accelerator technologies cutting-edge high-impact applications, such as physics discovery, cancer research and material sciences. A key challenge with autonomous accelerator tuning remains that the most capable algorithms require an expert in optimisation, machine learning or a similar field to implement the algorithm for every new tuning task. In this work, we propose the use of large language models (LLMs) to tune particle accelerators. We demonstrate on a proof-of-principle example the ability of LLMs to successfully and autonomously tune a particle accelerator subsystem based on nothing more than a natural language prompt from the operator, and compare the performance of our LLM-based solution to state-of-the-art optimisation algorithms, such as Bayesian optimisation (BO) and reinforcement learning-trained optimisation (RLO). In doing so, we also show how LLMs can perform numerical optimisation of a highly non-linear real-world objective function. Ultimately, this work represents yet another complex task that LLMs are capable of solving and promises to help accelerate the deployment of autonomous tuning algorithms to the day-to-day operations of particle accelerators.","sentences":["Autonomous tuning of particle accelerators is an active and challenging field of research with the goal of enabling novel accelerator technologies cutting-edge high-impact applications, such as physics discovery, cancer research and material sciences.","A key challenge with autonomous accelerator tuning remains that the most capable algorithms require an expert in optimisation, machine learning or a similar field to implement the algorithm for every new tuning task.","In this work, we propose the use of large language models (LLMs) to tune particle accelerators.","We demonstrate on a proof-of-principle example the ability of LLMs to successfully and autonomously tune a particle accelerator subsystem based on nothing more than a natural language prompt from the operator, and compare the performance of our LLM-based solution to state-of-the-art optimisation algorithms, such as Bayesian optimisation (BO) and reinforcement learning-trained optimisation (RLO).","In doing so, we also show how LLMs can perform numerical optimisation of a highly non-linear real-world objective function.","Ultimately, this work represents yet another complex task that LLMs are capable of solving and promises to help accelerate the deployment of autonomous tuning algorithms to the day-to-day operations of particle accelerators."],"url":"http://arxiv.org/abs/2405.08888v1","category":"cs.CL"}
{"created":"2024-05-14 18:04:25","title":"Finite-element assembly approach of optical quantum walk networks","abstract":"We present a finite-element approach for computing the aggregate scattering matrix of a network of linear coherent scatterers. These might be optical scatterers or more general scattering coins studied in quantum walk theory. While techniques exist for two-dimensional lattices of feed-forward scatterers, the present approach is applicable to any network configuration of any collection of scatterers. Unlike traditional finite-element methods in optics, this method does not directly solve Maxwell's equations; instead it is used to assemble and solve a linear, coupled scattering problem that emerges after Maxwell's equations are abstracted within the scattering matrix method. With this approach, a global unitary is assembled corresponding to one time step of the quantum walk on the network. After applying the relevant boundary conditions to this global matrix, the problem becomes non-unitary, and possesses a steady-state solution which is the output scattering state. We provide an algorithm to obtain this steady-state solution exactly using a matrix inversion, yielding the scattering state without requiring a direct calculation of the eigenspectrum. The approach is then numerically validated on a coupled-cavity interferometer example that possesses a known, closed-form solution. Finally, the method is shown to be a generalization of the Redheffer star product, which describes scatterers on one-dimensional lattices (2-regular graphs) and is often applied to the design of thin-film optics, making the current approach an invaluable tool for the design and validation of high-dimensional phase-reprogrammable optical devices and study of quantum walks on arbitrary graphs.","sentences":["We present a finite-element approach for computing the aggregate scattering matrix of a network of linear coherent scatterers.","These might be optical scatterers or more general scattering coins studied in quantum walk theory.","While techniques exist for two-dimensional lattices of feed-forward scatterers, the present approach is applicable to any network configuration of any collection of scatterers.","Unlike traditional finite-element methods in optics, this method does not directly solve Maxwell's equations; instead it is used to assemble and solve a linear, coupled scattering problem that emerges after Maxwell's equations are abstracted within the scattering matrix method.","With this approach, a global unitary is assembled corresponding to one time step of the quantum walk on the network.","After applying the relevant boundary conditions to this global matrix, the problem becomes non-unitary, and possesses a steady-state solution which is the output scattering state.","We provide an algorithm to obtain this steady-state solution exactly using a matrix inversion, yielding the scattering state without requiring a direct calculation of the eigenspectrum.","The approach is then numerically validated on a coupled-cavity interferometer example that possesses a known, closed-form solution.","Finally, the method is shown to be a generalization of the Redheffer star product, which describes scatterers on one-dimensional lattices (2-regular graphs) and is often applied to the design of thin-film optics, making the current approach an invaluable tool for the design and validation of high-dimensional phase-reprogrammable optical devices and study of quantum walks on arbitrary graphs."],"url":"http://arxiv.org/abs/2405.08884v1","category":"quant-ph"}
{"created":"2024-05-14 18:00:01","title":"HepLean: Digitalising high energy physics","abstract":"We introduce HepLean, an open-source project to digitalise definitions, theorems, proofs, and calculations in high energy physics using the interactive theorem prover Lean 4. HepLean has the potential to benefit the high energy physics community in four ways: making it easier to find existing results, allowing the creation of new results using artificial intelligence and automated methods, allowing easy review of papers for mathematical correctness, and providing new ways to teach high energy physics. We will discuss these in detail. We will also demonstrate the digitalisation of three areas of high energy physics in HepLean: Cabibbo-Kobayashi-Maskawa matrices in flavour physics, local anomaly cancellation, and Higgs physics.","sentences":["We introduce HepLean, an open-source project to digitalise definitions, theorems, proofs, and calculations in high energy physics using the interactive theorem prover Lean 4.","HepLean has the potential to benefit the high energy physics community in four ways: making it easier to find existing results, allowing the creation of new results using artificial intelligence and automated methods, allowing easy review of papers for mathematical correctness, and providing new ways to teach high energy physics.","We will discuss these in detail.","We will also demonstrate the digitalisation of three areas of high energy physics in HepLean: Cabibbo-Kobayashi-Maskawa matrices in flavour physics, local anomaly cancellation, and Higgs physics."],"url":"http://arxiv.org/abs/2405.08863v1","category":"hep-ph"}
{"created":"2024-05-14 16:05:57","title":"A Click-Through Rate Prediction Method Based on Cross-Importance of Multi-Order Features","abstract":"Most current click-through rate prediction(CTR)models create explicit or implicit high-order feature crosses through Hadamard product or inner product, with little attention to the importance of feature crossing; only few models are either limited to the second-order explicit feature crossing, implicitly to high-order feature crossing, or can learn the importance of high-order explicit feature crossing but fail to provide good interpretability for the model. This paper proposes a new model, FiiNet (Multiple Order Feature Interaction Importance Neural Networks). The model first uses the selective kernel network (SKNet) to explicitly construct multi-order feature crosses. It dynamically learns the importance of feature interaction combinations in a fine grained manner, increasing the attention weight of important feature cross combinations and reducing the weight of featureless crosses. To verify that the FiiNet model can dynamically learn the importance of feature interaction combinations in a fine-grained manner and improve the model's recommendation performance and interpretability, this paper compares it with many click-through rate prediction models on two real datasets, proving that the FiiNet model incorporating the selective kernel network can effectively improve the recommendation effect and provide better interpretability. FiiNet model implementations are available in PyTorch.","sentences":["Most current click-through rate prediction(CTR)models create explicit or implicit high-order feature crosses through Hadamard product or inner product, with little attention to the importance of feature crossing; only few models are either limited to the second-order explicit feature crossing, implicitly to high-order feature crossing, or can learn the importance of high-order explicit feature crossing but fail to provide good interpretability for the model.","This paper proposes a new model, FiiNet (Multiple Order Feature Interaction Importance Neural Networks).","The model first uses the selective kernel network (SKNet) to explicitly construct multi-order feature crosses.","It dynamically learns the importance of feature interaction combinations in a fine grained manner, increasing the attention weight of important feature cross combinations and reducing the weight of featureless crosses.","To verify that the FiiNet model can dynamically learn the importance of feature interaction combinations in a fine-grained manner and improve the model's recommendation performance and interpretability, this paper compares it with many click-through rate prediction models on two real datasets, proving that the FiiNet model incorporating the selective kernel network can effectively improve the recommendation effect and provide better interpretability.","FiiNet model implementations are available in PyTorch."],"url":"http://arxiv.org/abs/2405.08852v1","category":"cs.LG"}
{"created":"2024-05-14 12:48:03","title":"Rooting of thornless blackberry cuttings as induced by the extract of white willow (Salix alba L.) shoots collected in different times","abstract":"The aqueous extract of Salix spp contains many compounds which may act as root-promoting agents in cuttings. S. alba is a deciduous tree containing variable phytochemicals which are variable throughout the year. So, in this study, one- and two-year-old shoots of S. alba were collected on the 15th of each month in the year 2022, extracted in 2% ethanol at 9 g.L-1, and placed in a water bath at 35 {\\deg}C, then they applied to thornless blackberry cuttings for 1.5 hr. The results explained that the highest rooting percentage (66.67%) was obtained in the cuttings soaked in the extract of willow shoots collected on 15th of January. They were not significantly different from control cuttings, but they were different from the cuttings soaked in the extract of willow shoots collected on 15th of August and October (33.33%). The majority of other shoot and root traits were high in the cuttings soaked in the extract of willow shoots collected on 15th of December. The willow shoots collected on 15th of January contained the lowest total phenols (51.4 {\\mu}g.mL-1) and total flavonoids (29.07 {\\mu}g.mL-1). Moreover, the highest total phenols (57 {\\mu}g.mL-1) and IAA (365.17 {\\mu}g.mL-1) were recorded in the willow shoots collected on 15th of March, however each total flavonoids (44.96 {\\mu}g.mL-1) and salicylic acid (492.61 {\\mu}g.mL-1) were the highest in the willow shoots collected on 15th of April. Generally, based on rooting percentage, it is advisable to collect willow shoots on 15th of January and February for extraction and application to the thornless blackberry cuttings.","sentences":["The aqueous extract of Salix spp contains many compounds which may act as root-promoting agents in cuttings.","S. alba is a deciduous tree containing variable phytochemicals which are variable throughout the year.","So, in this study, one- and two-year-old shoots of S. alba were collected on the 15th of each month in the year 2022, extracted in 2% ethanol at 9 g.L-1, and placed in a water bath at 35 {\\deg}C, then they applied to thornless blackberry cuttings for 1.5 hr.","The results explained that the highest rooting percentage (66.67%) was obtained in the cuttings soaked in the extract of willow shoots collected on 15th of January.","They were not significantly different from control cuttings, but they were different from the cuttings soaked in the extract of willow shoots collected on 15th of August and October (33.33%).","The majority of other shoot and root traits were high in the cuttings soaked in the extract of willow shoots collected on 15th of December.","The willow shoots collected on 15th of January contained the lowest total phenols (51.4 {\\mu}g.mL-1) and total flavonoids (29.07 {\\mu}g.mL-1).","Moreover, the highest total phenols (57 {\\mu}g.mL-1) and IAA (365.17 {\\mu}g.mL-1) were recorded in the willow shoots collected on 15th of March, however each total flavonoids (44.96 {\\mu}g.mL-1) and salicylic acid (492.61 {\\mu}g.mL-1) were the highest in the willow shoots collected on 15th of April.","Generally, based on rooting percentage, it is advisable to collect willow shoots on 15th of January and February for extraction and application to the thornless blackberry cuttings."],"url":"http://arxiv.org/abs/2405.08849v1","category":"q-bio.OT"}
{"created":"2024-05-14 11:52:56","title":"Automated Repair of AI Code with Large Language Models and Formal Verification","abstract":"The next generation of AI systems requires strong safety guarantees. This report looks at the software implementation of neural networks and related memory safety properties, including NULL pointer deference, out-of-bound access, double-free, and memory leaks. Our goal is to detect these vulnerabilities, and automatically repair them with the help of large language models. To this end, we first expand the size of NeuroCodeBench, an existing dataset of neural network code, to about 81k programs via an automated process of program mutation. Then, we verify the memory safety of the mutated neural network implementations with ESBMC, a state-of-the-art software verifier. Whenever ESBMC spots a vulnerability, we invoke a large language model to repair the source code. For the latest task, we compare the performance of various state-of-the-art prompt engineering techniques, and an iterative approach that repeatedly calls the large language model.","sentences":["The next generation of AI systems requires strong safety guarantees.","This report looks at the software implementation of neural networks and related memory safety properties, including NULL pointer deference, out-of-bound access, double-free, and memory leaks.","Our goal is to detect these vulnerabilities, and automatically repair them with the help of large language models.","To this end, we first expand the size of NeuroCodeBench, an existing dataset of neural network code, to about 81k programs via an automated process of program mutation.","Then, we verify the memory safety of the mutated neural network implementations with ESBMC, a state-of-the-art software verifier.","Whenever ESBMC spots a vulnerability, we invoke a large language model to repair the source code.","For the latest task, we compare the performance of various state-of-the-art prompt engineering techniques, and an iterative approach that repeatedly calls the large language model."],"url":"http://arxiv.org/abs/2405.08848v1","category":"cs.SE"}
{"created":"2024-05-14 08:00:26","title":"Scalable synchronization cluster in networked chaotic oscillators","abstract":"Cluster synchronization in synthetic networks of coupled chaotic oscillators is investigated. It is found that despite the asymmetric nature of the network structure, a subset of the oscillators can be synchronized as a cluster while the other oscillators remain desynchronized. Interestingly, with the increase of the coupling strength, the cluster is expanding gradually by recruiting the desynchronized oscillators one by one. This new synchronization phenomenon, which is named ``scalable synchronization cluster\", is explored theoretically by the method of eigenvector-based analysis, and it is revealed that the scalability of the cluster is attributed to the unique feature of the eigenvectors of the network coupling matrix. The transient dynamics of the cluster in response to random perturbations are also studied, and it is shown that in restoring to the synchronization state, oscillators inside the cluster are stabilized in sequence, illustrating again the hierarchy of the oscillators. The findings shed new light on the collective behaviors of networked chaotic oscillators, and are helpful for the design of real-world networks where scalable synchronization clusters are concerned.","sentences":["Cluster synchronization in synthetic networks of coupled chaotic oscillators is investigated.","It is found that despite the asymmetric nature of the network structure, a subset of the oscillators can be synchronized as a cluster while the other oscillators remain desynchronized.","Interestingly, with the increase of the coupling strength, the cluster is expanding gradually by recruiting the desynchronized oscillators one by one.","This new synchronization phenomenon, which is named ``scalable synchronization cluster\", is explored theoretically by the method of eigenvector-based analysis, and it is revealed that the scalability of the cluster is attributed to the unique feature of the eigenvectors of the network coupling matrix.","The transient dynamics of the cluster in response to random perturbations are also studied, and it is shown that in restoring to the synchronization state, oscillators inside the cluster are stabilized in sequence, illustrating again the hierarchy of the oscillators.","The findings shed new light on the collective behaviors of networked chaotic oscillators, and are helpful for the design of real-world networks where scalable synchronization clusters are concerned."],"url":"http://arxiv.org/abs/2405.08844v1","category":"nlin.AO"}
{"created":"2024-05-14 07:53:23","title":"FLEXIBLE: Forecasting Cellular Traffic by Leveraging Explicit Inductive Graph-Based Learning","abstract":"From a telecommunication standpoint, the surge in users and services challenges next-generation networks with escalating traffic demands and limited resources. Accurate traffic prediction can offer network operators valuable insights into network conditions and suggest optimal allocation policies. Recently, spatio-temporal forecasting, employing Graph Neural Networks (GNNs), has emerged as a promising method for cellular traffic prediction. However, existing studies, inspired by road traffic forecasting formulations, overlook the dynamic deployment and removal of base stations, requiring the GNN-based forecaster to handle an evolving graph. This work introduces a novel inductive learning scheme and a generalizable GNN-based forecasting model that can process diverse graphs of cellular traffic with one-time training. We also demonstrate that this model can be easily leveraged by transfer learning with minimal effort, making it applicable to different areas. Experimental results show up to 9.8% performance improvement compared to the state-of-the-art, especially in rare-data settings with training data reduced to below 20%.","sentences":["From a telecommunication standpoint, the surge in users and services challenges next-generation networks with escalating traffic demands and limited resources.","Accurate traffic prediction can offer network operators valuable insights into network conditions and suggest optimal allocation policies.","Recently, spatio-temporal forecasting, employing Graph Neural Networks (GNNs), has emerged as a promising method for cellular traffic prediction.","However, existing studies, inspired by road traffic forecasting formulations, overlook the dynamic deployment and removal of base stations, requiring the GNN-based forecaster to handle an evolving graph.","This work introduces a novel inductive learning scheme and a generalizable GNN-based forecasting model that can process diverse graphs of cellular traffic with one-time training.","We also demonstrate that this model can be easily leveraged by transfer learning with minimal effort, making it applicable to different areas.","Experimental results show up to 9.8% performance improvement compared to the state-of-the-art, especially in rare-data settings with training data reduced to below 20%."],"url":"http://arxiv.org/abs/2405.08843v1","category":"cs.LG"}
{"created":"2024-05-14 07:51:55","title":"Automated Deep Learning for Load Forecasting","abstract":"Accurate forecasting of electricity consumption is essential to ensure the performance and stability of the grid, especially as the use of renewable energy increases. Forecasting electricity is challenging because it depends on many external factors, such as weather and calendar variables. While regression-based models are currently effective, the emergence of new explanatory variables and the need to refine the temporality of the signals to be forecasted is encouraging the exploration of novel methodologies, in particular deep learning models. However, Deep Neural Networks (DNNs) struggle with this task due to the lack of data points and the different types of explanatory variables (e.g. integer, float, or categorical). In this paper, we explain why and how we used Automated Deep Learning (AutoDL) to find performing DNNs for load forecasting. We ended up creating an AutoDL framework called EnergyDragon by extending the DRAGON package and applying it to load forecasting. EnergyDragon automatically selects the features embedded in the DNN training in an innovative way and optimizes the architecture and the hyperparameters of the networks. We demonstrate on the French load signal that EnergyDragon can find original DNNs that outperform state-of-the-art load forecasting methods as well as other AutoDL approaches.","sentences":["Accurate forecasting of electricity consumption is essential to ensure the performance and stability of the grid, especially as the use of renewable energy increases.","Forecasting electricity is challenging because it depends on many external factors, such as weather and calendar variables.","While regression-based models are currently effective, the emergence of new explanatory variables and the need to refine the temporality of the signals to be forecasted is encouraging the exploration of novel methodologies, in particular deep learning models.","However, Deep Neural Networks (DNNs) struggle with this task due to the lack of data points and the different types of explanatory variables (e.g. integer, float, or categorical).","In this paper, we explain why and how we used Automated Deep Learning (AutoDL) to find performing DNNs for load forecasting.","We ended up creating an AutoDL framework called EnergyDragon by extending the DRAGON package and applying it to load forecasting.","EnergyDragon automatically selects the features embedded in the DNN training in an innovative way and optimizes the architecture and the hyperparameters of the networks.","We demonstrate on the French load signal that EnergyDragon can find original DNNs that outperform state-of-the-art load forecasting methods as well as other AutoDL approaches."],"url":"http://arxiv.org/abs/2405.08842v1","category":"cs.LG"}
{"created":"2024-05-14 07:16:56","title":"PromptMind Team at EHRSQL-2024: Improving Reliability of SQL Generation using Ensemble LLMs","abstract":"This paper presents our approach to the EHRSQL-2024 shared task, which aims to develop a reliable Text-to-SQL system for electronic health records. We propose two approaches that leverage large language models (LLMs) for prompting and fine-tuning to generate EHRSQL queries. In both techniques, we concentrate on bridging the gap between the real-world knowledge on which LLMs are trained and the domain specific knowledge required for the task. The paper provides the results of each approach individually, demonstrating that they achieve high execution accuracy. Additionally, we show that an ensemble approach further enhances generation reliability by reducing errors. This approach secured us 2nd place in the shared task competition. The methodologies outlined in this paper are designed to be transferable to domain-specific Text-to-SQL problems that emphasize both accuracy and reliability.","sentences":["This paper presents our approach to the EHRSQL-2024 shared task, which aims to develop a reliable Text-to-SQL system for electronic health records.","We propose two approaches that leverage large language models (LLMs) for prompting and fine-tuning to generate EHRSQL queries.","In both techniques, we concentrate on bridging the gap between the real-world knowledge on which LLMs are trained and the domain specific knowledge required for the task.","The paper provides the results of each approach individually, demonstrating that they achieve high execution accuracy.","Additionally, we show that an ensemble approach further enhances generation reliability by reducing errors.","This approach secured us 2nd place in the shared task competition.","The methodologies outlined in this paper are designed to be transferable to domain-specific Text-to-SQL problems that emphasize both accuracy and reliability."],"url":"http://arxiv.org/abs/2405.08839v1","category":"cs.DB"}
{"created":"2024-05-14 06:40:05","title":"PolyGlotFake: A Novel Multilingual and Multimodal DeepFake Dataset","abstract":"With the rapid advancement of generative AI, multimodal deepfakes, which manipulate both audio and visual modalities, have drawn increasing public concern. Currently, deepfake detection has emerged as a crucial strategy in countering these growing threats. However, as a key factor in training and validating deepfake detectors, most existing deepfake datasets primarily focus on the visual modal, and the few that are multimodal employ outdated techniques, and their audio content is limited to a single language, thereby failing to represent the cutting-edge advancements and globalization trends in current deepfake technologies. To address this gap, we propose a novel, multilingual, and multimodal deepfake dataset: PolyGlotFake. It includes content in seven languages, created using a variety of cutting-edge and popular Text-to-Speech, voice cloning, and lip-sync technologies. We conduct comprehensive experiments using state-of-the-art detection methods on PolyGlotFake dataset. These experiments demonstrate the dataset's significant challenges and its practical value in advancing research into multimodal deepfake detection.","sentences":["With the rapid advancement of generative AI, multimodal deepfakes, which manipulate both audio and visual modalities, have drawn increasing public concern.","Currently, deepfake detection has emerged as a crucial strategy in countering these growing threats.","However, as a key factor in training and validating deepfake detectors, most existing deepfake datasets primarily focus on the visual modal, and the few that are multimodal employ outdated techniques, and their audio content is limited to a single language, thereby failing to represent the cutting-edge advancements and globalization trends in current deepfake technologies.","To address this gap, we propose a novel, multilingual, and multimodal deepfake dataset: PolyGlotFake.","It includes content in seven languages, created using a variety of cutting-edge and popular Text-to-Speech, voice cloning, and lip-sync technologies.","We conduct comprehensive experiments using state-of-the-art detection methods on PolyGlotFake dataset.","These experiments demonstrate the dataset's significant challenges and its practical value in advancing research into multimodal deepfake detection."],"url":"http://arxiv.org/abs/2405.08838v1","category":"cs.SD"}
{"created":"2024-05-14 02:42:40","title":"Adversarial Machine Learning Threats to Spacecraft","abstract":"Spacecraft are among the earliest autonomous systems. Their ability to function without a human in the loop have afforded some of humanity's grandest achievements. As reliance on autonomy grows, space vehicles will become increasingly vulnerable to attacks designed to disrupt autonomous processes-especially probabilistic ones based on machine learning. This paper aims to elucidate and demonstrate the threats that adversarial machine learning (AML) capabilities pose to spacecraft. First, an AML threat taxonomy for spacecraft is introduced. Next, we demonstrate the execution of AML attacks against spacecraft through experimental simulations using NASA's Core Flight System (cFS) and NASA's On-board Artificial Intelligence Research (OnAIR) Platform. Our findings highlight the imperative for incorporating AML-focused security measures in spacecraft that engage autonomy.","sentences":["Spacecraft are among the earliest autonomous systems.","Their ability to function without a human in the loop have afforded some of humanity's grandest achievements.","As reliance on autonomy grows, space vehicles will become increasingly vulnerable to attacks designed to disrupt autonomous processes-especially probabilistic ones based on machine learning.","This paper aims to elucidate and demonstrate the threats that adversarial machine learning (AML) capabilities pose to spacecraft.","First, an AML threat taxonomy for spacecraft is introduced.","Next, we demonstrate the execution of AML attacks against spacecraft through experimental simulations using NASA's Core Flight System (cFS) and NASA's On-board Artificial Intelligence Research (OnAIR) Platform.","Our findings highlight the imperative for incorporating AML-focused security measures in spacecraft that engage autonomy."],"url":"http://arxiv.org/abs/2405.08834v1","category":"cs.LG"}
{"created":"2024-05-13 17:59:56","title":"MambaOut: Do We Really Need Mamba for Vision?","abstract":"Mamba, an architecture with RNN-like token mixer of state space model (SSM), was recently introduced to address the quadratic complexity of the attention mechanism and subsequently applied to vision tasks. Nevertheless, the performance of Mamba for vision is often underwhelming when compared with convolutional and attention-based models. In this paper, we delve into the essence of Mamba, and conceptually conclude that Mamba is ideally suited for tasks with long-sequence and autoregressive characteristics. For vision tasks, as image classification does not align with either characteristic, we hypothesize that Mamba is not necessary for this task; Detection and segmentation tasks are also not autoregressive, yet they adhere to the long-sequence characteristic, so we believe it is still worthwhile to explore Mamba's potential for these tasks. To empirically verify our hypotheses, we construct a series of models named MambaOut through stacking Mamba blocks while removing their core token mixer, SSM. Experimental results strongly support our hypotheses. Specifically, our MambaOut model surpasses all visual Mamba models on ImageNet image classification, indicating that Mamba is indeed unnecessary for this task. As for detection and segmentation, MambaOut cannot match the performance of state-of-the-art visual Mamba models, demonstrating the potential of Mamba for long-sequence visual tasks. The code is available at https://github.com/yuweihao/MambaOut","sentences":["Mamba, an architecture with RNN-like token mixer of state space model (SSM), was recently introduced to address the quadratic complexity of the attention mechanism and subsequently applied to vision tasks.","Nevertheless, the performance of Mamba for vision is often underwhelming when compared with convolutional and attention-based models.","In this paper, we delve into the essence of Mamba, and conceptually conclude that Mamba is ideally suited for tasks with long-sequence and autoregressive characteristics.","For vision tasks, as image classification does not align with either characteristic, we hypothesize that Mamba is not necessary for this task; Detection and segmentation tasks are also not autoregressive, yet they adhere to the long-sequence characteristic, so we believe it is still worthwhile to explore Mamba's potential for these tasks.","To empirically verify our hypotheses, we construct a series of models named MambaOut through stacking Mamba blocks while removing their core token mixer, SSM.","Experimental results strongly support our hypotheses.","Specifically, our MambaOut model surpasses all visual Mamba models on ImageNet image classification, indicating that Mamba is indeed unnecessary for this task.","As for detection and segmentation, MambaOut cannot match the performance of state-of-the-art visual Mamba models, demonstrating the potential of Mamba for long-sequence visual tasks.","The code is available at https://github.com/yuweihao/MambaOut"],"url":"http://arxiv.org/abs/2405.07992v2","category":"cs.CV"}
{"created":"2024-05-13 17:59:36","title":"SPIN: Simultaneous Perception, Interaction and Navigation","abstract":"While there has been remarkable progress recently in the fields of manipulation and locomotion, mobile manipulation remains a long-standing challenge. Compared to locomotion or static manipulation, a mobile system must make a diverse range of long-horizon tasks feasible in unstructured and dynamic environments. While the applications are broad and interesting, there are a plethora of challenges in developing these systems such as coordination between the base and arm, reliance on onboard perception for perceiving and interacting with the environment, and most importantly, simultaneously integrating all these parts together. Prior works approach the problem using disentangled modular skills for mobility and manipulation that are trivially tied together. This causes several limitations such as compounding errors, delays in decision-making, and no whole-body coordination. In this work, we present a reactive mobile manipulation framework that uses an active visual system to consciously perceive and react to its environment. Similar to how humans leverage whole-body and hand-eye coordination, we develop a mobile manipulator that exploits its ability to move and see, more specifically -- to move in order to see and to see in order to move. This allows it to not only move around and interact with its environment but also, choose \"when\" to perceive \"what\" using an active visual system. We observe that such an agent learns to navigate around complex cluttered scenarios while displaying agile whole-body coordination using only ego-vision without needing to create environment maps. Results visualizations and videos at https://spin-robot.github.io/","sentences":["While there has been remarkable progress recently in the fields of manipulation and locomotion, mobile manipulation remains a long-standing challenge.","Compared to locomotion or static manipulation, a mobile system must make a diverse range of long-horizon tasks feasible in unstructured and dynamic environments.","While the applications are broad and interesting, there are a plethora of challenges in developing these systems such as coordination between the base and arm, reliance on onboard perception for perceiving and interacting with the environment, and most importantly, simultaneously integrating all these parts together.","Prior works approach the problem using disentangled modular skills for mobility and manipulation that are trivially tied together.","This causes several limitations such as compounding errors, delays in decision-making, and no whole-body coordination.","In this work, we present a reactive mobile manipulation framework that uses an active visual system to consciously perceive and react to its environment.","Similar to how humans leverage whole-body and hand-eye coordination, we develop a mobile manipulator that exploits its ability to move and see, more specifically -- to move in order to see and to see in order to move.","This allows it to not only move around and interact with its environment but also, choose \"when\" to perceive \"what\" using an active visual system.","We observe that such an agent learns to navigate around complex cluttered scenarios while displaying agile whole-body coordination using only ego-vision without needing to create environment maps.","Results visualizations and videos at https://spin-robot.github.io/"],"url":"http://arxiv.org/abs/2405.07991v1","category":"cs.RO"}
{"created":"2024-05-13 17:59:22","title":"Plot2Code: A Comprehensive Benchmark for Evaluating Multi-modal Large Language Models in Code Generation from Scientific Plots","abstract":"The remarkable progress of Multi-modal Large Language Models (MLLMs) has attracted significant attention due to their superior performance in visual contexts. However, their capabilities in turning visual figure to executable code, have not been evaluated thoroughly. To address this, we introduce Plot2Code, a comprehensive visual coding benchmark designed for a fair and in-depth assessment of MLLMs. We carefully collect 132 manually selected high-quality matplotlib plots across six plot types from publicly available matplotlib galleries. For each plot, we carefully offer its source code, and an descriptive instruction summarized by GPT-4. This approach enables Plot2Code to extensively evaluate MLLMs' code capabilities across various input modalities. Furthermore, we propose three automatic evaluation metrics, including code pass rate, text-match ratio, and GPT-4V overall rating, for a fine-grained assessment of the output code and rendered images. Instead of simply judging pass or fail, we employ GPT-4V to make an overall judgement between the generated and reference images, which has been shown to be consistent with human evaluation. The evaluation results, which include analyses of 14 MLLMs such as the proprietary GPT-4V, Gemini-Pro, and the open-sourced Mini-Gemini, highlight the substantial challenges presented by Plot2Code. With Plot2Code, we reveal that most existing MLLMs struggle with visual coding for text-dense plots, heavily relying on textual instruction. We hope that the evaluation results from Plot2Code on visual coding will guide the future development of MLLMs. All data involved with Plot2Code are available at https://huggingface.co/datasets/TencentARC/Plot2Code.","sentences":["The remarkable progress of Multi-modal Large Language Models (MLLMs) has attracted significant attention due to their superior performance in visual contexts.","However, their capabilities in turning visual figure to executable code, have not been evaluated thoroughly.","To address this, we introduce Plot2Code, a comprehensive visual coding benchmark designed for a fair and in-depth assessment of MLLMs.","We carefully collect 132 manually selected high-quality matplotlib plots across six plot types from publicly available matplotlib galleries.","For each plot, we carefully offer its source code, and an descriptive instruction summarized by GPT-4.","This approach enables Plot2Code to extensively evaluate MLLMs' code capabilities across various input modalities.","Furthermore, we propose three automatic evaluation metrics, including code pass rate, text-match ratio, and GPT-4V overall rating, for a fine-grained assessment of the output code and rendered images.","Instead of simply judging pass or fail, we employ GPT-4V to make an overall judgement between the generated and reference images, which has been shown to be consistent with human evaluation.","The evaluation results, which include analyses of 14 MLLMs such as the proprietary GPT-4V, Gemini-Pro, and the open-sourced Mini-Gemini, highlight the substantial challenges presented by Plot2Code.","With Plot2Code, we reveal that most existing MLLMs struggle with visual coding for text-dense plots, heavily relying on textual instruction.","We hope that the evaluation results from Plot2Code on visual coding will guide the future development of MLLMs.","All data involved with Plot2Code are available at https://huggingface.co/datasets/TencentARC/Plot2Code."],"url":"http://arxiv.org/abs/2405.07990v1","category":"cs.CL"}
{"created":"2024-05-13 17:58:51","title":"A Generalist Learner for Multifaceted Medical Image Interpretation","abstract":"Current medical artificial intelligence systems are often limited to narrow applications, hindering their widespread adoption in clinical practice. To address this limitation, we propose MedVersa, a generalist learner that enables flexible learning and tasking for medical image interpretation. By leveraging a large language model as a learnable orchestrator, MedVersa can learn from both visual and linguistic supervision, support multimodal inputs, and perform real-time task specification. This versatility allows MedVersa to adapt to various clinical scenarios and perform multifaceted medical image analysis. We introduce MedInterp, the largest multimodal dataset to date for medical image interpretation, consisting of over 13 million annotated instances spanning 11 tasks across 3 modalities, to support the development of MedVersa. Our experiments demonstrate that MedVersa achieves state-of-the-art performance in 9 tasks, sometimes outperforming specialist counterparts by over 10%. MedVersa is the first to showcase the viability of multimodal generative medical AI in implementing multimodal outputs, inputs, and dynamic task specification, highlighting its potential as a multifunctional system for comprehensive medical image analysis. This generalist approach to medical image interpretation paves the way for more adaptable and efficient AI-assisted clinical decision-making.","sentences":["Current medical artificial intelligence systems are often limited to narrow applications, hindering their widespread adoption in clinical practice.","To address this limitation, we propose MedVersa, a generalist learner that enables flexible learning and tasking for medical image interpretation.","By leveraging a large language model as a learnable orchestrator, MedVersa can learn from both visual and linguistic supervision, support multimodal inputs, and perform real-time task specification.","This versatility allows MedVersa to adapt to various clinical scenarios and perform multifaceted medical image analysis.","We introduce MedInterp, the largest multimodal dataset to date for medical image interpretation, consisting of over 13 million annotated instances spanning 11 tasks across 3 modalities, to support the development of MedVersa.","Our experiments demonstrate that MedVersa achieves state-of-the-art performance in 9 tasks, sometimes outperforming specialist counterparts by over 10%.","MedVersa is the first to showcase the viability of multimodal generative medical AI in implementing multimodal outputs, inputs, and dynamic task specification, highlighting its potential as a multifunctional system for comprehensive medical image analysis.","This generalist approach to medical image interpretation paves the way for more adaptable and efficient AI-assisted clinical decision-making."],"url":"http://arxiv.org/abs/2405.07988v1","category":"cs.CV"}
{"created":"2024-05-13 17:58:30","title":"The Platonic Representation Hypothesis","abstract":"We argue that representations in AI models, particularly deep networks, are converging. First, we survey many examples of convergence in the literature: over time and across multiple domains, the ways by which different neural networks represent data are becoming more aligned. Next, we demonstrate convergence across data modalities: as vision models and language models get larger, they measure distance between datapoints in a more and more alike way. We hypothesize that this convergence is driving toward a shared statistical model of reality, akin to Plato's concept of an ideal reality. We term such a representation the platonic representation and discuss several possible selective pressures toward it. Finally, we discuss the implications of these trends, their limitations, and counterexamples to our analysis.","sentences":["We argue that representations in AI models, particularly deep networks, are converging.","First, we survey many examples of convergence in the literature: over time and across multiple domains, the ways by which different neural networks represent data are becoming more aligned.","Next, we demonstrate convergence across data modalities: as vision models and language models get larger, they measure distance between datapoints in a more and more alike way.","We hypothesize that this convergence is driving toward a shared statistical model of reality, akin to Plato's concept of an ideal reality.","We term such a representation the platonic representation and discuss several possible selective pressures toward it.","Finally, we discuss the implications of these trends, their limitations, and counterexamples to our analysis."],"url":"http://arxiv.org/abs/2405.07987v1","category":"cs.LG"}
{"created":"2024-05-13 17:52:25","title":"Low-order outcomes and clustered designs: combining design and analysis for causal inference under network interference","abstract":"Variance reduction for causal inference in the presence of network interference is often achieved through either outcome modeling, which is typically analyzed under unit-randomized Bernoulli designs, or clustered experimental designs, which are typically analyzed without strong parametric assumptions. In this work, we study the intersection of these two approaches and consider the problem of estimation in low-order outcome models using data from a general experimental design. Our contributions are threefold. First, we present an estimator of the total treatment effect (also called the global average treatment effect) in a low-degree outcome model when the data are collected under general experimental designs, generalizing previous results for Bernoulli designs. We refer to this estimator as the pseudoinverse estimator and give bounds on its bias and variance in terms of properties of the experimental design. Second, we evaluate these bounds for the case of cluster randomized designs with both Bernoulli and complete randomization. For clustered Bernoulli randomization, we find that our estimator is always unbiased and that its variance scales like the smaller of the variance obtained from a low-order assumption and the variance obtained from cluster randomization, showing that combining these variance reduction strategies is preferable to using either individually. For clustered complete randomization, we find a notable bias-variance trade-off mediated by specific features of the clustering. Third, when choosing a clustered experimental design, our bounds can be used to select a clustering from a set of candidate clusterings. Across a range of graphs and clustering algorithms, we show that our method consistently selects clusterings that perform well on a range of response models, suggesting that our bounds are useful to practitioners.","sentences":["Variance reduction for causal inference in the presence of network interference is often achieved through either outcome modeling, which is typically analyzed under unit-randomized Bernoulli designs, or clustered experimental designs, which are typically analyzed without strong parametric assumptions.","In this work, we study the intersection of these two approaches and consider the problem of estimation in low-order outcome models using data from a general experimental design.","Our contributions are threefold.","First, we present an estimator of the total treatment effect (also called the global average treatment effect) in a low-degree outcome model when the data are collected under general experimental designs, generalizing previous results for Bernoulli designs.","We refer to this estimator as the pseudoinverse estimator and give bounds on its bias and variance in terms of properties of the experimental design.","Second, we evaluate these bounds for the case of cluster randomized designs with both Bernoulli and complete randomization.","For clustered Bernoulli randomization, we find that our estimator is always unbiased and that its variance scales like the smaller of the variance obtained from a low-order assumption and the variance obtained from cluster randomization, showing that combining these variance reduction strategies is preferable to using either individually.","For clustered complete randomization, we find a notable bias-variance trade-off mediated by specific features of the clustering.","Third, when choosing a clustered experimental design, our bounds can be used to select a clustering from a set of candidate clusterings.","Across a range of graphs and clustering algorithms, we show that our method consistently selects clusterings that perform well on a range of response models, suggesting that our bounds are useful to practitioners."],"url":"http://arxiv.org/abs/2405.07979v2","category":"stat.ME"}
{"created":"2024-05-13 17:49:20","title":"A Demographic-Conditioned Variational Autoencoder for fMRI Distribution Sampling and Removal of Confounds","abstract":"Objective: fMRI and derived measures such as functional connectivity (FC) have been used to predict brain age, general fluid intelligence, psychiatric disease status, and preclinical neurodegenerative disease. However, it is not always clear that all demographic confounds, such as age, sex, and race, have been removed from fMRI data. Additionally, many fMRI datasets are restricted to authorized researchers, making dissemination of these valuable data sources challenging. Methods: We create a variational autoencoder (VAE)-based model, DemoVAE, to decorrelate fMRI features from demographics and generate high-quality synthetic fMRI data based on user-supplied demographics. We train and validate our model using two large, widely used datasets, the Philadelphia Neurodevelopmental Cohort (PNC) and Bipolar and Schizophrenia Network for Intermediate Phenotypes (BSNIP). Results: We find that DemoVAE recapitulates group differences in fMRI data while capturing the full breadth of individual variations. Significantly, we also find that most clinical and computerized battery fields that are correlated with fMRI data are not correlated with DemoVAE latents. An exception are several fields related to schizophrenia medication and symptom severity. Conclusion: Our model generates fMRI data that captures the full distribution of FC better than traditional VAE or GAN models. We also find that most prediction using fMRI data is dependent on correlation with, and prediction of, demographics. Significance: Our DemoVAE model allows for generation of high quality synthetic data conditioned on subject demographics as well as the removal of the confounding effects of demographics. We identify that FC-based prediction tasks are highly influenced by demographic confounds.","sentences":["Objective: fMRI and derived measures such as functional connectivity (FC) have been used to predict brain age, general fluid intelligence, psychiatric disease status, and preclinical neurodegenerative disease.","However, it is not always clear that all demographic confounds, such as age, sex, and race, have been removed from fMRI data.","Additionally, many fMRI datasets are restricted to authorized researchers, making dissemination of these valuable data sources challenging.","Methods: We create a variational autoencoder (VAE)-based model, DemoVAE, to decorrelate fMRI features from demographics and generate high-quality synthetic fMRI data based on user-supplied demographics.","We train and validate our model using two large, widely used datasets, the Philadelphia Neurodevelopmental Cohort (PNC) and Bipolar and Schizophrenia Network for Intermediate Phenotypes (BSNIP).","Results:","We find that DemoVAE recapitulates group differences in fMRI data while capturing the full breadth of individual variations.","Significantly, we also find that most clinical and computerized battery fields that are correlated with fMRI data are not correlated with DemoVAE latents.","An exception are several fields related to schizophrenia medication and symptom severity.","Conclusion: Our model generates fMRI data that captures the full distribution of FC better than traditional VAE or GAN models.","We also find that most prediction using fMRI data is dependent on correlation with, and prediction of, demographics.","Significance: Our DemoVAE model allows for generation of high quality synthetic data conditioned on subject demographics as well as the removal of the confounding effects of demographics.","We identify that FC-based prediction tasks are highly influenced by demographic confounds."],"url":"http://arxiv.org/abs/2405.07977v1","category":"q-bio.QM"}
{"created":"2024-05-13 17:48:45","title":"Localized Adaptive Risk Control","abstract":"Adaptive Risk Control (ARC) is an online calibration strategy based on set prediction that offers worst-case deterministic long-term risk control, as well as statistical marginal coverage guarantees. ARC adjusts the size of the prediction set by varying a single scalar threshold based on feedback from past decisions. In this work, we introduce Localized Adaptive Risk Control (L-ARC), an online calibration scheme that targets statistical localized risk guarantees ranging from conditional risk to marginal risk, while preserving the worst-case performance of ARC. L-ARC updates a threshold function within a reproducing kernel Hilbert space (RKHS), with the kernel determining the level of localization of the statistical risk guarantee. The theoretical results highlight a trade-off between localization of the statistical risk and convergence speed to the long-term risk target. Thanks to localization, L-ARC is demonstrated via experiments to produce prediction sets with risk guarantees across different data subpopulations, significantly improving the fairness of the calibrated model for tasks such as image segmentation and beam selection in wireless networks.","sentences":["Adaptive Risk Control (ARC) is an online calibration strategy based on set prediction that offers worst-case deterministic long-term risk control, as well as statistical marginal coverage guarantees.","ARC adjusts the size of the prediction set by varying a single scalar threshold based on feedback from past decisions.","In this work, we introduce Localized Adaptive Risk Control (L-ARC), an online calibration scheme that targets statistical localized risk guarantees ranging from conditional risk to marginal risk, while preserving the worst-case performance of ARC.","L-ARC updates a threshold function within a reproducing kernel Hilbert space (RKHS), with the kernel determining the level of localization of the statistical risk guarantee.","The theoretical results highlight a trade-off between localization of the statistical risk and convergence speed to the long-term risk target.","Thanks to localization, L-ARC is demonstrated via experiments to produce prediction sets with risk guarantees across different data subpopulations, significantly improving the fairness of the calibrated model for tasks such as image segmentation and beam selection in wireless networks."],"url":"http://arxiv.org/abs/2405.07976v1","category":"stat.ML"}
{"created":"2024-05-13 17:48:20","title":"A Natural Formalized Proof Language","abstract":"Artificial intelligence assisted mathematical proof has become a highly focused area nowadays. One key problem in this field is to generate formal mathematical proofs from natural language proofs. Due to historical reasons, the formal proof languages adopted by traditional theorem provers were not intended to represent natural language proofs. Therefore, they are not well-suited for the aforementioned tasks and proof-checking work for educational purposes. In this paper, we design a proof language and its corresponding abstract syntax tree and implement a proof checking tool for it. This language can be easily converted from natural language, thus providing a rich corpus of formal proof. Additionally, it supports the handling of issues in informal proofs through static analysis, and enhances the expressive power of the language by introducing the structure of partial proofs. This design combines the expressiveness of natural language and the accuracy of formal language, resulting in an improved mathematical proof language.","sentences":["Artificial intelligence assisted mathematical proof has become a highly focused area nowadays.","One key problem in this field is to generate formal mathematical proofs from natural language proofs.","Due to historical reasons, the formal proof languages adopted by traditional theorem provers were not intended to represent natural language proofs.","Therefore, they are not well-suited for the aforementioned tasks and proof-checking work for educational purposes.","In this paper, we design a proof language and its corresponding abstract syntax tree and implement a proof checking tool for it.","This language can be easily converted from natural language, thus providing a rich corpus of formal proof.","Additionally, it supports the handling of issues in informal proofs through static analysis, and enhances the expressive power of the language by introducing the structure of partial proofs.","This design combines the expressiveness of natural language and the accuracy of formal language, resulting in an improved mathematical proof language."],"url":"http://arxiv.org/abs/2405.07973v1","category":"cs.PL"}
{"created":"2024-05-13 17:47:08","title":"Investigating the Semantic Robustness of CLIP-based Zero-Shot Anomaly Segmentation","abstract":"Zero-shot anomaly segmentation using pre-trained foundation models is a promising approach that enables effective algorithms without expensive, domain-specific training or fine-tuning. Ensuring that these methods work across various environmental conditions and are robust to distribution shifts is an open problem. We investigate the performance of WinCLIP [14] zero-shot anomaly segmentation algorithm by perturbing test data using three semantic transformations: bounded angular rotations, bounded saturation shifts, and hue shifts. We empirically measure a lower performance bound by aggregating across per-sample worst-case perturbations and find that average performance drops by up to 20% in area under the ROC curve and 40% in area under the per-region overlap curve. We find that performance is consistently lowered on three CLIP backbones, regardless of model architecture or learning objective, demonstrating a need for careful performance evaluation.","sentences":["Zero-shot anomaly segmentation using pre-trained foundation models is a promising approach that enables effective algorithms without expensive, domain-specific training or fine-tuning.","Ensuring that these methods work across various environmental conditions and are robust to distribution shifts is an open problem.","We investigate the performance of WinCLIP [14] zero-shot anomaly segmentation algorithm by perturbing test data using three semantic transformations: bounded angular rotations, bounded saturation shifts, and hue shifts.","We empirically measure a lower performance bound by aggregating across per-sample worst-case perturbations and find that average performance drops by up to 20% in area under the ROC curve and 40% in area under the per-region overlap curve.","We find that performance is consistently lowered on three CLIP backbones, regardless of model architecture or learning objective, demonstrating a need for careful performance evaluation."],"url":"http://arxiv.org/abs/2405.07969v1","category":"cs.CV"}
{"created":"2024-05-13 17:46:35","title":"OverlapMamba: Novel Shift State Space Model for LiDAR-based Place Recognition","abstract":"Place recognition is the foundation for enabling autonomous systems to achieve independent decision-making and safe operations. It is also crucial in tasks such as loop closure detection and global localization within SLAM. Previous methods utilize mundane point cloud representations as input and deep learning-based LiDAR-based Place Recognition (LPR) approaches employing different point cloud image inputs with convolutional neural networks (CNNs) or transformer architectures. However, the recently proposed Mamba deep learning model, combined with state space models (SSMs), holds great potential for long sequence modeling. Therefore, we developed OverlapMamba, a novel network for place recognition, which represents input range views (RVs) as sequences. In a novel way, we employ a stochastic reconstruction approach to build shift state space models, compressing the visual representation. Evaluated on three different public datasets, our method effectively detects loop closures, showing robustness even when traversing previously visited locations from different directions. Relying on raw range view inputs, it outperforms typical LiDAR and multi-view combination methods in time complexity and speed, indicating strong place recognition capabilities and real-time efficiency.","sentences":["Place recognition is the foundation for enabling autonomous systems to achieve independent decision-making and safe operations.","It is also crucial in tasks such as loop closure detection and global localization within SLAM.","Previous methods utilize mundane point cloud representations as input and deep learning-based LiDAR-based Place Recognition (LPR) approaches employing different point cloud image inputs with convolutional neural networks (CNNs) or transformer architectures.","However, the recently proposed Mamba deep learning model, combined with state space models (SSMs), holds great potential for long sequence modeling.","Therefore, we developed OverlapMamba, a novel network for place recognition, which represents input range views (RVs) as sequences.","In a novel way, we employ a stochastic reconstruction approach to build shift state space models, compressing the visual representation.","Evaluated on three different public datasets, our method effectively detects loop closures, showing robustness even when traversing previously visited locations from different directions.","Relying on raw range view inputs, it outperforms typical LiDAR and multi-view combination methods in time complexity and speed, indicating strong place recognition capabilities and real-time efficiency."],"url":"http://arxiv.org/abs/2405.07966v1","category":"cs.CV"}
{"created":"2024-05-13 17:38:53","title":"AgentClinic: a multimodal agent benchmark to evaluate AI in simulated clinical environments","abstract":"Diagnosing and managing a patient is a complex, sequential decision making process that requires physicians to obtain information -- such as which tests to perform -- and to act upon it. Recent advances in artificial intelligence (AI) and large language models (LLMs) promise to profoundly impact clinical care. However, current evaluation schemes overrely on static medical question-answering benchmarks, falling short on interactive decision-making that is required in real-life clinical work. Here, we present AgentClinic: a multimodal benchmark to evaluate LLMs in their ability to operate as agents in simulated clinical environments. In our benchmark, the doctor agent must uncover the patient's diagnosis through dialogue and active data collection. We present two open benchmarks: a multimodal image and dialogue environment, AgentClinic-NEJM, and a dialogue-only environment, AgentClinic-MedQA. We embed cognitive and implicit biases both in patient and doctor agents to emulate realistic interactions between biased agents. We find that introducing bias leads to large reductions in diagnostic accuracy of the doctor agents, as well as reduced compliance, confidence, and follow-up consultation willingness in patient agents. Evaluating a suite of state-of-the-art LLMs, we find that several models that excel in benchmarks like MedQA are performing poorly in AgentClinic-MedQA. We find that the LLM used in the patient agent is an important factor for performance in the AgentClinic benchmark. We show that both having limited interactions as well as too many interaction reduces diagnostic accuracy in doctor agents. The code and data for this work is publicly available at https://AgentClinic.github.io.","sentences":["Diagnosing and managing a patient is a complex, sequential decision making process that requires physicians to obtain information -- such as which tests to perform -- and to act upon it.","Recent advances in artificial intelligence (AI) and large language models (LLMs) promise to profoundly impact clinical care.","However, current evaluation schemes overrely on static medical question-answering benchmarks, falling short on interactive decision-making that is required in real-life clinical work.","Here, we present AgentClinic: a multimodal benchmark to evaluate LLMs in their ability to operate as agents in simulated clinical environments.","In our benchmark, the doctor agent must uncover the patient's diagnosis through dialogue and active data collection.","We present two open benchmarks: a multimodal image and dialogue environment, AgentClinic-NEJM, and a dialogue-only environment, AgentClinic-MedQA.","We embed cognitive and implicit biases both in patient and doctor agents to emulate realistic interactions between biased agents.","We find that introducing bias leads to large reductions in diagnostic accuracy of the doctor agents, as well as reduced compliance, confidence, and follow-up consultation willingness in patient agents.","Evaluating a suite of state-of-the-art LLMs, we find that several models that excel in benchmarks like MedQA are performing poorly in AgentClinic-MedQA.","We find that the LLM used in the patient agent is an important factor for performance in the AgentClinic benchmark.","We show that both having limited interactions as well as too many interaction reduces diagnostic accuracy in doctor agents.","The code and data for this work is publicly available at https://AgentClinic.github.io."],"url":"http://arxiv.org/abs/2405.07960v1","category":"cs.HC"}
{"created":"2024-05-13 17:25:40","title":"Online Load and Graph Balancing for Random Order Inputs","abstract":"Online load balancing for heterogeneous machines aims to minimize the makespan (maximum machine workload) by scheduling arriving jobs with varying sizes on different machines. In the adversarial setting, where an adversary chooses not only the collection of job sizes but also their arrival order, the problem is well-understood and the optimal competitive ratio is known to be $\\Theta(\\log m)$ where $m$ is the number of machines. In the more realistic random arrival order model, the understanding is limited. Previously, the best lower bound on the competitive ratio was only $\\Omega(\\log \\log m)$.   We significantly improve this bound by showing an $\\Omega( \\sqrt {\\log m})$ lower bound, even for the restricted case where each job has a unit size on two machines and infinite size on the others. On the positive side, we propose an $O(\\log m/\\log \\log m)$-competitive algorithm, demonstrating that better performance is possible in the random arrival model.","sentences":["Online load balancing for heterogeneous machines aims to minimize the makespan (maximum machine workload) by scheduling arriving jobs with varying sizes on different machines.","In the adversarial setting, where an adversary chooses not only the collection of job sizes but also their arrival order, the problem is well-understood and the optimal competitive ratio is known to be $\\Theta(\\log m)$ where $m$ is the number of machines.","In the more realistic random arrival order model, the understanding is limited.","Previously, the best lower bound on the competitive ratio was only $\\Omega(\\log \\log m)$.   ","We significantly improve this bound by showing an $\\Omega( \\sqrt {\\log m})$ lower bound, even for the restricted case where each job has a unit size on two machines and infinite size on the others.","On the positive side, we propose an $O(\\log m/\\log \\log m)$-competitive algorithm, demonstrating that better performance is possible in the random arrival model."],"url":"http://arxiv.org/abs/2405.07949v1","category":"cs.DS"}
{"created":"2024-05-13 17:18:08","title":"Hierarchical Decision Mamba","abstract":"Recent advancements in imitation learning have been largely fueled by the integration of sequence models, which provide a structured flow of information to effectively mimic task behaviours. Currently, Decision Transformer (DT) and subsequently, the Hierarchical Decision Transformer (HDT), presented Transformer-based approaches to learn task policies. Recently, the Mamba architecture has shown to outperform Transformers across various task domains. In this work, we introduce two novel methods, Decision Mamba (DM) and Hierarchical Decision Mamba (HDM), aimed at enhancing the performance of the Transformer models. Through extensive experimentation across diverse environments such as OpenAI Gym and D4RL, leveraging varying demonstration data sets, we demonstrate the superiority of Mamba models over their Transformer counterparts in a majority of tasks. Results show that HDM outperforms other methods in most settings. The code can be found at https://github.com/meowatthemoon/HierarchicalDecisionMamba.","sentences":["Recent advancements in imitation learning have been largely fueled by the integration of sequence models, which provide a structured flow of information to effectively mimic task behaviours.","Currently, Decision Transformer (DT) and subsequently, the Hierarchical Decision Transformer (HDT), presented Transformer-based approaches to learn task policies.","Recently, the Mamba architecture has shown to outperform Transformers across various task domains.","In this work, we introduce two novel methods, Decision Mamba (DM) and Hierarchical Decision Mamba (HDM), aimed at enhancing the performance of the Transformer models.","Through extensive experimentation across diverse environments such as OpenAI Gym and D4RL, leveraging varying demonstration data sets, we demonstrate the superiority of Mamba models over their Transformer counterparts in a majority of tasks.","Results show that HDM outperforms other methods in most settings.","The code can be found at https://github.com/meowatthemoon/HierarchicalDecisionMamba."],"url":"http://arxiv.org/abs/2405.07943v1","category":"cs.LG"}
{"created":"2024-05-15 17:55:05","title":"Spectral complexity of deep neural networks","abstract":"It is well-known that randomly initialized, push-forward, fully-connected neural networks weakly converge to isotropic Gaussian processes, in the limit where the width of all layers goes to infinity. In this paper, we propose to use the angular power spectrum of the limiting field to characterize the complexity of the network architecture. In particular, we define sequences of random variables associated with the angular power spectrum, and provide a full characterization of the network complexity in terms of the asymptotic distribution of these sequences as the depth diverges. On this basis, we classify neural networks as low-disorder, sparse, or high-disorder; we show how this classification highlights a number of distinct features for standard activation functions, and in particular, sparsity properties of ReLU networks. Our theoretical results are also validated by numerical simulations.","sentences":["It is well-known that randomly initialized, push-forward, fully-connected neural networks weakly converge to isotropic Gaussian processes, in the limit where the width of all layers goes to infinity.","In this paper, we propose to use the angular power spectrum of the limiting field to characterize the complexity of the network architecture.","In particular, we define sequences of random variables associated with the angular power spectrum, and provide a full characterization of the network complexity in terms of the asymptotic distribution of these sequences as the depth diverges.","On this basis, we classify neural networks as low-disorder, sparse, or high-disorder; we show how this classification highlights a number of distinct features for standard activation functions, and in particular, sparsity properties of ReLU networks.","Our theoretical results are also validated by numerical simulations."],"url":"http://arxiv.org/abs/2405.09541v1","category":"stat.ML"}
{"created":"2024-05-15 17:40:49","title":"Ticket-based multi-strand method for increased efficiency in proof-of-work based blockchains","abstract":"This paper outlines a method aiming to increase the efficiency of proof-of-work based blockchains using a ticket-based approach. To avoid the limitation of serially adding one block at a time to a blockchain, multiple semi-independent chains are used such that several valid blocks can be added in parallel, when they are added to separate chains. Blocks are added to different chains, the chain index being determined by a ``ticket'' that the miner must produce before mining a new block. This allows increasing the transaction rate by several orders of magnitude while the system is still fully decentralized and permissionless, and maintaining security in the sense that a successful attack would require the attacker to control a significant portion of the whole network.","sentences":["This paper outlines a method aiming to increase the efficiency of proof-of-work based blockchains using a ticket-based approach.","To avoid the limitation of serially adding one block at a time to a blockchain, multiple semi-independent chains are used such that several valid blocks can be added in parallel, when they are added to separate chains.","Blocks are added to different chains, the chain index being determined by a ``ticket'' that the miner must produce before mining a new block.","This allows increasing the transaction rate by several orders of magnitude while the system is still fully decentralized and permissionless, and maintaining security in the sense that a successful attack would require the attacker to control a significant portion of the whole network."],"url":"http://arxiv.org/abs/2405.09531v1","category":"cs.DC"}
{"created":"2024-05-15 17:32:23","title":"Water Purification Via Plasma","abstract":"Water purification via plasma is considered a healthy, effective alternative to traditional water purification systems due to the lack of harmful chemical additives that traditional purification systems use. A reactor capable of eliminating bacteria colonies from contaminated water using plasma discharge was designed and tested. Materials and reactor parameters are discussed in this paper. It was confirmed that glow discharge plasma in water creates oxidants, eliminating bacteria colonies.","sentences":["Water purification via plasma is considered a healthy, effective alternative to traditional water purification systems due to the lack of harmful chemical additives that traditional purification systems use.","A reactor capable of eliminating bacteria colonies from contaminated water using plasma discharge was designed and tested.","Materials and reactor parameters are discussed in this paper.","It was confirmed that glow discharge plasma in water creates oxidants, eliminating bacteria colonies."],"url":"http://arxiv.org/abs/2405.09524v1","category":"physics.plasm-ph"}
{"created":"2024-05-15 17:21:57","title":"Cost-Benefit Analysis using Modular Dynamic Fault Tree Analysis and Monte Carlo Simulations for Condition-based Maintenance of Unmanned Systems","abstract":"Recent developments in condition-based maintenance (CBM) have helped make it a promising approach to maintenance cost avoidance in engineering systems. By performing maintenance based on conditions of the component with regards to failure or time, there is potential to avoid the large costs of system shutdown and maintenance delays. However, CBM requires a large investment cost compared to other available maintenance strategies. The investment cost is required for research, development, and implementation. Despite the potential to avoid significant maintenance costs, the large investment cost of CBM makes decision makers hesitant to implement. This study is the first in the literature that attempts to address the problem of conducting a cost-benefit analysis (CBA) for implementing CBM concepts for unmanned systems. This paper proposes a method for conducting a CBA to determine the return on investment (ROI) of potential CBM strategies. The CBA seeks to compare different CBM strategies based on the differences in the various maintenance requirements associated with maintaining a multi-component, unmanned system. The proposed method uses modular dynamic fault tree analysis (MDFTA) with Monte Carlo simulations (MCS) to assess the various maintenance requirements. The proposed method is demonstrated on an unmanned surface vessel (USV) example taken from the literature that consists of 5 subsystems and 71 components. Following this USV example, it is found that selecting different combinations of components for a CBM strategy can have a significant impact on maintenance requirements and ROI by impacting cost avoidances and investment costs.","sentences":["Recent developments in condition-based maintenance (CBM) have helped make it a promising approach to maintenance cost avoidance in engineering systems.","By performing maintenance based on conditions of the component with regards to failure or time, there is potential to avoid the large costs of system shutdown and maintenance delays.","However, CBM requires a large investment cost compared to other available maintenance strategies.","The investment cost is required for research, development, and implementation.","Despite the potential to avoid significant maintenance costs, the large investment cost of CBM makes decision makers hesitant to implement.","This study is the first in the literature that attempts to address the problem of conducting a cost-benefit analysis (CBA) for implementing CBM concepts for unmanned systems.","This paper proposes a method for conducting a CBA to determine the return on investment (ROI) of potential CBM strategies.","The CBA seeks to compare different CBM strategies based on the differences in the various maintenance requirements associated with maintaining a multi-component, unmanned system.","The proposed method uses modular dynamic fault tree analysis (MDFTA) with Monte Carlo simulations (MCS) to assess the various maintenance requirements.","The proposed method is demonstrated on an unmanned surface vessel (USV) example taken from the literature that consists of 5 subsystems and 71 components.","Following this USV example, it is found that selecting different combinations of components for a CBM strategy can have a significant impact on maintenance requirements and ROI by impacting cost avoidances and investment costs."],"url":"http://arxiv.org/abs/2405.09519v1","category":"q-fin.CP"}
{"created":"2024-05-15 17:08:46","title":"Interplay between the charge density wave phase and a pseudogap under antiferromagnetic correlations","abstract":"In this study, we explore the impact of short-range antiferromagnetic correlations on the charge density wave (CDW) phase in strongly correlated electron systems exhibiting the pseudogap phenomenon. Our investigation employs a n-pole approximation to consider the repulsive Coulomb interaction $(U)$ and antiferromagnetic correlations. Utilizing a two-dimensional Hubbard model for the Coulomb interaction and a BCS-like model for the CDW order parameter, we observe that an increase in $U$ enhances antiferromagnetic fluctuations, resulting in a flattened re-normalized band around the anti-nodal point $(\\pi,0)$. The pseudogap manifests in the band structure and density of states, prompting an exploration across various $U$ and occupation number values. Our findings indicate that antiferromagnetic correlations significantly influence the CDW state, as the Fermi surface is reconstructed within the ordered phase. Furthermore, we find a Lifhsitz transition inside both the CDW phase and the normal state, with the latter preceding the onset of the pseudogap.","sentences":["In this study, we explore the impact of short-range antiferromagnetic correlations on the charge density wave (CDW) phase in strongly correlated electron systems exhibiting the pseudogap phenomenon.","Our investigation employs a n-pole approximation to consider the repulsive Coulomb interaction $(U)$ and antiferromagnetic correlations.","Utilizing a two-dimensional Hubbard model for the Coulomb interaction and a BCS-like model for the CDW order parameter, we observe that an increase in $U$ enhances antiferromagnetic fluctuations, resulting in a flattened re-normalized band around the anti-nodal point $(\\pi,0)$. The pseudogap manifests in the band structure and density of states, prompting an exploration across various $U$ and occupation number values.","Our findings indicate that antiferromagnetic correlations significantly influence the CDW state, as the Fermi surface is reconstructed within the ordered phase.","Furthermore, we find a Lifhsitz transition inside both the CDW phase and the normal state, with the latter preceding the onset of the pseudogap."],"url":"http://arxiv.org/abs/2405.09515v1","category":"cond-mat.str-el"}
{"created":"2024-05-15 17:05:08","title":"Voltage-Driven Breakdown of Electronic Order","abstract":"The non-thermal breakdown of a Mott insulator has been a topic of great theoretical and experimental interest with technological relevance. Recent experiments have found a sharp non-equilibrium insulator-to-metal transition that is accompanied by hysteresis, a negative differential conductance and lattice deformations. However, a thorough understanding of the underlying breakdown mechanism is still lacking. Here, we examine a scenario in which the breakdown is induced by chemical pressure in a paradigmatic model of interacting spinless fermions on a chain coupled to metallic reservoirs (leads). For the Markovian regime, at infinite bias, we qualitatively reproduce several established results. Beyond infinite bias, we find a rich phase diagram where the nature of the breakdown depends on the coupling strength as the bias voltage is tuned up, yielding different current-carrying non-equilibrium phases. For weak to intermediate coupling, we find a conducting CDW phase with a bias-dependent ordering wave vector. At large interaction strength, the breakdown connects the system to a charge-separated insulating phase. We find instances of hysteretic behavior, sharp current onset and negative differential conductance. Our results can help to shed light on recent experimental findings that address current-induced Mott breakdown.","sentences":["The non-thermal breakdown of a Mott insulator has been a topic of great theoretical and experimental interest with technological relevance.","Recent experiments have found a sharp non-equilibrium insulator-to-metal transition that is accompanied by hysteresis, a negative differential conductance and lattice deformations.","However, a thorough understanding of the underlying breakdown mechanism is still lacking.","Here, we examine a scenario in which the breakdown is induced by chemical pressure in a paradigmatic model of interacting spinless fermions on a chain coupled to metallic reservoirs (leads).","For the Markovian regime, at infinite bias, we qualitatively reproduce several established results.","Beyond infinite bias, we find a rich phase diagram where the nature of the breakdown depends on the coupling strength as the bias voltage is tuned up, yielding different current-carrying non-equilibrium phases.","For weak to intermediate coupling, we find a conducting CDW phase with a bias-dependent ordering wave vector.","At large interaction strength, the breakdown connects the system to a charge-separated insulating phase.","We find instances of hysteretic behavior, sharp current onset and negative differential conductance.","Our results can help to shed light on recent experimental findings that address current-induced Mott breakdown."],"url":"http://arxiv.org/abs/2405.09512v1","category":"cond-mat.str-el"}
{"created":"2024-05-15 16:55:33","title":"On automorphism groups of smooth hypersurfaces","abstract":"We show that smooth hypersurfaces in complex projective spaces with automorphism groups of maximum size are isomorphic to Fermat hypersurfaces, with a few exceptions. For the exceptions, we give explicitly the defining equations and automorphism groups.","sentences":["We show that smooth hypersurfaces in complex projective spaces with automorphism groups of maximum size are isomorphic to Fermat hypersurfaces, with a few exceptions.","For the exceptions, we give explicitly the defining equations and automorphism groups."],"url":"http://arxiv.org/abs/2405.09505v1","category":"math.AG"}
{"created":"2024-05-15 16:53:22","title":"Self-consistent modelling of the Milky Way structure using live potentials","abstract":"To advance our understanding of the evolution of the interstellar medium (ISM) of our Galaxy, numerical models of Milky Way (MW) type galaxies are widely used. However, most models only vaguely resemble the MW (e.g. in total mass), and often use imposed analytic potentials (which cannot evolve dynamically). This poses a problem in asserting their applicability for the interpretation of observations of our own Galaxy. The goal of this work is to identify a numerical model that is not only a MW-type galaxy, but one that can mimic some of the main observed structures of our Galaxy, using dynamically evolving potentials, so that it can be used as a base model to study the ISM cycle in a galaxy like our own. This paper introduces a suite of 15 MW-type galaxy models developed using the {\\sc arepo} numerical code, that are compared to Galactic observations of $^{12}$CO and \\ion{H}{I} emission via longitude-velocity plots, from where we extract and compare the skeletons of major galactic features and the terminal gas velocities. We found that our best-fitting model to the overall structure, also reproduces some of the more specific observed features of the MW, including a bar with a pattern speed of $30.0 \\pm 0.2$ km\\,s$^{-1}$\\,kpc$^{-1}$, a bar half-length of $3.2 \\pm 0.8$\\,kpc. Our model shows large streaming motions around spiral arms, and strong radial motions well beyond the inner bar. This model highlights the complex motions of a dynamic MW-type galaxy and has the potential to offer valuable insight into how our Galaxy regulates the ISM and star formation.","sentences":["To advance our understanding of the evolution of the interstellar medium (ISM) of our Galaxy, numerical models of Milky Way (MW) type galaxies are widely used.","However, most models only vaguely resemble the MW (e.g. in total mass), and often use imposed analytic potentials (which cannot evolve dynamically).","This poses a problem in asserting their applicability for the interpretation of observations of our own Galaxy.","The goal of this work is to identify a numerical model that is not only a MW-type galaxy, but one that can mimic some of the main observed structures of our Galaxy, using dynamically evolving potentials, so that it can be used as a base model to study the ISM cycle in a galaxy like our own.","This paper introduces a suite of 15 MW-type galaxy models developed using the {\\sc arepo} numerical code, that are compared to Galactic observations of $^{12}$CO and \\ion{H}{I} emission via longitude-velocity plots, from where we extract and compare the skeletons of major galactic features and the terminal gas velocities.","We found that our best-fitting model to the overall structure, also reproduces some of the more specific observed features of the MW, including a bar with a pattern speed of $30.0 \\pm 0.2$ km\\,s$^{-1}$\\,kpc$^{-1}$, a bar half-length of $3.2 \\pm 0.8$\\,kpc.","Our model shows large streaming motions around spiral arms, and strong radial motions well beyond the inner bar.","This model highlights the complex motions of a dynamic MW-type galaxy and has the potential to offer valuable insight into how our Galaxy regulates the ISM and star formation."],"url":"http://arxiv.org/abs/2405.09503v1","category":"astro-ph.GA"}
{"created":"2024-05-15 16:38:28","title":"Constrained Learning for Causal Inference and Semiparametric Statistics","abstract":"Causal estimation (e.g. of the average treatment effect) requires estimating complex nuisance parameters (e.g. outcome models). To adjust for errors in nuisance parameter estimation, we present a novel correction method that solves for the best plug-in estimator under the constraint that the first-order error of the estimator with respect to the nuisance parameter estimate is zero. Our constrained learning framework provides a unifying perspective to prominent first-order correction approaches including debiasing (a.k.a. augmented inverse probability weighting) and targeting (a.k.a. targeted maximum likelihood estimation). Our semiparametric inference approach, which we call the \"C-Learner\", can be implemented with modern machine learning methods such as neural networks and tree ensembles, and enjoys standard guarantees like semiparametric efficiency and double robustness. Empirically, we demonstrate our approach on several datasets, including those with text features that require fine-tuning language models. We observe the C-Learner matches or outperforms other asymptotically optimal estimators, with better performance in settings with less estimated overlap.","sentences":["Causal estimation (e.g. of the average treatment effect) requires estimating complex nuisance parameters (e.g. outcome models).","To adjust for errors in nuisance parameter estimation, we present a novel correction method that solves for the best plug-in estimator under the constraint that the first-order error of the estimator with respect to the nuisance parameter estimate is zero.","Our constrained learning framework provides a unifying perspective to prominent first-order correction approaches including debiasing (a.k.a. augmented inverse probability weighting) and targeting (a.k.a. targeted maximum likelihood estimation).","Our semiparametric inference approach, which we call the \"C-Learner\", can be implemented with modern machine learning methods such as neural networks and tree ensembles, and enjoys standard guarantees like semiparametric efficiency and double robustness.","Empirically, we demonstrate our approach on several datasets, including those with text features that require fine-tuning language models.","We observe the C-Learner matches or outperforms other asymptotically optimal estimators, with better performance in settings with less estimated overlap."],"url":"http://arxiv.org/abs/2405.09493v1","category":"stat.ML"}
{"created":"2024-05-15 16:21:22","title":"Thermodynamic properties of a relativistic Bose gas under rigid rotation","abstract":"We study the thermodynamic properties of a rigidly rotating relativistic Bose gas. First, we derive the solution of the equation of motion corresponding to a free rotating complex Klein-Gordon field and determine the free propagator of this model utilizing the Fock-Schwinger proper-time method. Using this propagator, we then obtain the thermodynamic potential of this model in the zeroth and first perturbative level. In addition, we compute the nonperturbative ring contribution to this potential. Our focus is on the dependence of these expressions on the angular velocity, which effectively acts as a chemical potential. Using this thermodynamic potential, we calculate several quantities, including the pressure, angular momentum and entropy densities, heat capacity, speed of sound, and moment of inertia of this rigidly rotating Bose gas as functions of temperature ($T$), angular velocity ($\\Omega$), and the coupling constant ($\\alpha$). We show that certain thermodynamic instabilities appear at high temperatures and large couplings. They are manifested as zero and negative values of the above quantities, particularly the moment of inertia and heat capacity. Zero moment of inertia leads to the phenomenon of supervorticity at certain $T$ or $\\alpha$. Supervortical temperatures (couplings) decrease with increasing coupling (temperature). We also observe superluminal sound velocities at high $T$ and for large $\\alpha$.","sentences":["We study the thermodynamic properties of a rigidly rotating relativistic Bose gas.","First, we derive the solution of the equation of motion corresponding to a free rotating complex Klein-Gordon field and determine the free propagator of this model utilizing the Fock-Schwinger proper-time method.","Using this propagator, we then obtain the thermodynamic potential of this model in the zeroth and first perturbative level.","In addition, we compute the nonperturbative ring contribution to this potential.","Our focus is on the dependence of these expressions on the angular velocity, which effectively acts as a chemical potential.","Using this thermodynamic potential, we calculate several quantities, including the pressure, angular momentum and entropy densities, heat capacity, speed of sound, and moment of inertia of this rigidly rotating Bose gas as functions of temperature ($T$), angular velocity ($\\Omega$), and the coupling constant ($\\alpha$).","We show that certain thermodynamic instabilities appear at high temperatures and large couplings.","They are manifested as zero and negative values of the above quantities, particularly the moment of inertia and heat capacity.","Zero moment of inertia leads to the phenomenon of supervorticity at certain $T$ or $\\alpha$. Supervortical temperatures (couplings) decrease with increasing coupling (temperature).","We also observe superluminal sound velocities at high $T$ and for large $\\alpha$."],"url":"http://arxiv.org/abs/2405.09481v1","category":"hep-ph"}
{"created":"2024-05-15 16:20:20","title":"Scenarios for the appearance of strange attractors in a model of three interacting microbubble contrast agents","abstract":"We study nonlinear dynamics in a model of three interacting encapsulated gas bubbles in a liquid. The model is a system of three coupled nonlinear oscillators with an external periodic force. Such bubbles have numerous applications, for instance, they are used as contrast agents in ultrasound visualization. Certain types of bubbles oscillations may be beneficial or undesirable depending on a given application and, hence, the dependence of the regimes of bubbles oscillations on the control parameters is worth studying. We demonstrate that there is a wide variety of types of dynamics in the model by constructing a chart of dynamical regimes in the control parameters space. Here we focus on hyperchaotic attractors characterized by three positive Lyapunov exponents and strange attractors with one or two positive Lyapunov exponents possessing an additional zero Lyapunov exponent, which have not been observed previously in the context of bubbles oscillations. We also believe that we provide a first example of a hyperchaotic attractor with additional zero Lyaponov exponent. Furthermore, the mechanisms of the onset of these types of attractors are still not well studied. We identify two-parametric regions in the control parameter space where these hyperchaotic and chaotic attractors appear and study one-parametric routes leading to them. We associate the appearance of hyperchaotic attractors with three positive Lyapunov exponents with the inclusion of a periodic orbit with a three-dimensional unstable manifold, while the onset of chaotic oscillations with an additional zero Lyapunov exponent is connected to the partial synchronization of bubbles oscillations. We propose several underlying bifurcation mechanisms that explain the emergence of these regimes. We believe that these bifurcation scenarios are universal and can be observed in other systems of coupled oscillators.","sentences":["We study nonlinear dynamics in a model of three interacting encapsulated gas bubbles in a liquid.","The model is a system of three coupled nonlinear oscillators with an external periodic force.","Such bubbles have numerous applications, for instance, they are used as contrast agents in ultrasound visualization.","Certain types of bubbles oscillations may be beneficial or undesirable depending on a given application and, hence, the dependence of the regimes of bubbles oscillations on the control parameters is worth studying.","We demonstrate that there is a wide variety of types of dynamics in the model by constructing a chart of dynamical regimes in the control parameters space.","Here we focus on hyperchaotic attractors characterized by three positive Lyapunov exponents and strange attractors with one or two positive Lyapunov exponents possessing an additional zero Lyapunov exponent, which have not been observed previously in the context of bubbles oscillations.","We also believe that we provide a first example of a hyperchaotic attractor with additional zero Lyaponov exponent.","Furthermore, the mechanisms of the onset of these types of attractors are still not well studied.","We identify two-parametric regions in the control parameter space where these hyperchaotic and chaotic attractors appear and study one-parametric routes leading to them.","We associate the appearance of hyperchaotic attractors with three positive Lyapunov exponents with the inclusion of a periodic orbit with a three-dimensional unstable manifold, while the onset of chaotic oscillations with an additional zero Lyapunov exponent is connected to the partial synchronization of bubbles oscillations.","We propose several underlying bifurcation mechanisms that explain the emergence of these regimes.","We believe that these bifurcation scenarios are universal and can be observed in other systems of coupled oscillators."],"url":"http://arxiv.org/abs/2405.09479v1","category":"math.DS"}
{"created":"2024-05-15 16:12:39","title":"Size and kinematics of the CIV broad emission line region from microlensing-induced line profile distortions in two gravitationally lensed quasars","abstract":"We analyzed the CIV line profile distortions due to microlensing in two quasars, J1339 and J1138. J1339 shows a strong, asymmetric line profile deformation, while J1138 shows a more modest, symmetric deformation. To probe the CIV broad line region (BLR), we compared the observed line profile deformations to simulated ones. The simulations are based on three simple BLR models, a Keplerian disk (KD), an equatorial wind (EW), and a polar wind (PW), of various sizes, inclinations, and emissivities. We find that the line profile deformations can be reproduced with the simple BLR models under consideration, with no need for more complex geometries or kinematics. The models with disk geometries (KD and EW) are preferred, while the PW model is definitely less likely. For J1339, we find the CIV BLR half-light radii to be $r_{1/2} =$ 5.1 $^{+4.6}_{-2.9}$ light-days and $r_{1/2} =$ 6.7 $^{+6.0}_{-3.8}$ light-days from spectra obtained in 2014 and 2017, respectively. They do agree within uncertainties. For J1138, the amplitude of microlensing is smaller and more dependent on the macro-magnification factor. From spectra obtained in 2005 (single epoch), we find $r_{1/2} =$ 4.9 $^{+4.9}_{-2.7}$ light-days and $r_{1/2}= $ 12 $^{+13}_{-8}$ light-days for two extreme values of the macro-magnification factor. Combining these new measurements with those previously obtained for the quasars Q2237$+$0305 and J1004$+$4112, we show that the BLR radii estimated from microlensing do follow the CIV radius--luminosity relation obtained from reverberation mapping, although the microlensing radii seem to be systematically smaller, which could indicate either a selection bias or a real offset (abridged).","sentences":["We analyzed the CIV line profile distortions due to microlensing in two quasars, J1339 and J1138.","J1339 shows a strong, asymmetric line profile deformation, while J1138 shows a more modest, symmetric deformation.","To probe the CIV broad line region (BLR), we compared the observed line profile deformations to simulated ones.","The simulations are based on three simple BLR models, a Keplerian disk (KD), an equatorial wind (EW), and a polar wind (PW), of various sizes, inclinations, and emissivities.","We find that the line profile deformations can be reproduced with the simple BLR models under consideration, with no need for more complex geometries or kinematics.","The models with disk geometries (KD and EW) are preferred, while the PW model is definitely less likely.","For J1339, we find the CIV BLR half-light radii to be $r_{1/2} =$ 5.1 $^{+4.6}_{-2.9}$ light-days and $r_{1/2} =$ 6.7 $^{+6.0}_{-3.8}$ light-days from spectra obtained in 2014 and 2017, respectively.","They do agree within uncertainties.","For J1138, the amplitude of microlensing is smaller and more dependent on the macro-magnification factor.","From spectra obtained in 2005 (single epoch), we find $r_{1/2} =$ 4.9 $^{+4.9}_{-2.7}$ light-days and $r_{1/2}= $ 12 $^{+13}_{-8}$ light-days for two extreme values of the macro-magnification factor.","Combining these new measurements with those previously obtained for the quasars Q2237$+$0305 and J1004$+$4112, we show that the BLR radii estimated from microlensing do follow the CIV radius--luminosity relation obtained from reverberation mapping, although the microlensing radii seem to be systematically smaller, which could indicate either a selection bias or a real offset (abridged)."],"url":"http://arxiv.org/abs/2405.09476v1","category":"astro-ph.GA"}
{"created":"2024-05-15 16:05:24","title":"Towards Evaluating the Robustness of Automatic Speech Recognition Systems via Audio Style Transfer","abstract":"In light of the widespread application of Automatic Speech Recognition (ASR) systems, their security concerns have received much more attention than ever before, primarily due to the susceptibility of Deep Neural Networks. Previous studies have illustrated that surreptitiously crafting adversarial perturbations enables the manipulation of speech recognition systems, resulting in the production of malicious commands. These attack methods mostly require adding noise perturbations under $\\ell_p$ norm constraints, inevitably leaving behind artifacts of manual modifications. Recent research has alleviated this limitation by manipulating style vectors to synthesize adversarial examples based on Text-to-Speech (TTS) synthesis audio. However, style modifications based on optimization objectives significantly reduce the controllability and editability of audio styles. In this paper, we propose an attack on ASR systems based on user-customized style transfer. We first test the effect of Style Transfer Attack (STA) which combines style transfer and adversarial attack in sequential order. And then, as an improvement, we propose an iterative Style Code Attack (SCA) to maintain audio quality. Experimental results show that our method can meet the need for user-customized styles and achieve a success rate of 82% in attacks, while keeping sound naturalness due to our user study.","sentences":["In light of the widespread application of Automatic Speech Recognition (ASR) systems, their security concerns have received much more attention than ever before, primarily due to the susceptibility of Deep Neural Networks.","Previous studies have illustrated that surreptitiously crafting adversarial perturbations enables the manipulation of speech recognition systems, resulting in the production of malicious commands.","These attack methods mostly require adding noise perturbations under $\\ell_p$ norm constraints, inevitably leaving behind artifacts of manual modifications.","Recent research has alleviated this limitation by manipulating style vectors to synthesize adversarial examples based on Text-to-Speech (TTS) synthesis audio.","However, style modifications based on optimization objectives significantly reduce the controllability and editability of audio styles.","In this paper, we propose an attack on ASR systems based on user-customized style transfer.","We first test the effect of Style Transfer Attack (STA) which combines style transfer and adversarial attack in sequential order.","And then, as an improvement, we propose an iterative Style Code Attack (SCA) to maintain audio quality.","Experimental results show that our method can meet the need for user-customized styles and achieve a success rate of 82% in attacks, while keeping sound naturalness due to our user study."],"url":"http://arxiv.org/abs/2405.09470v1","category":"cs.SD"}
{"created":"2024-05-15 16:01:24","title":"Study of possible $DND^*$ bound states","abstract":"We start from a recently favored picture in which the $\\Lambda_c(2940)$ and $\\Lambda_c(2910)$ correspond mostly to $ND^*$ bound states with $J^P = 1/2^-,\\, 3/2^-$ and then add a $D$ as a third particle, looking for the possible binding of the $DND^*$ three body system within the framework of the Fixed Center Approximation. We find that the system is bound with respect to the corresponding $\\Lambda_c^* D$ threshold with a binding of about $60$ MeV and a width of about $90$ MeV. Alternatively we assume a cluster of $ND$ and a $D^*$ meson interacting with the cluster and we find similar results. The observation of these states of $J^P = 1/2^+,\\, 3/2^+$ would provide new and valuable information concerning the $DN$ and $D^* N$ interaction, an issue of current debate.","sentences":["We start from a recently favored picture in which the $\\Lambda_c(2940)$ and $\\Lambda_c(2910)$ correspond mostly to $ND^*$ bound states with $J^P = 1/2^-,\\, 3/2^-$ and then add a $D$ as a third particle, looking for the possible binding of the $DND^*$ three body system within the framework of the Fixed Center Approximation.","We find that the system is bound with respect to the corresponding $\\Lambda_c^* D$ threshold with a binding of about $60$ MeV and a width of about $90$ MeV.","Alternatively we assume a cluster of $ND$ and a $D^*$ meson interacting with the cluster and we find similar results.","The observation of these states of $J^P = 1/2^+,\\, 3/2^+$ would provide new and valuable information concerning the $DN$ and $D^* N$ interaction, an issue of current debate."],"url":"http://arxiv.org/abs/2405.09467v1","category":"hep-ph"}
{"created":"2024-05-15 16:01:04","title":"A geometric formulation to measure global and genuine entanglement in three-qubit systems","abstract":"We introduce a purely geometric formulation for two different measures addressed to quantify the entanglement between different parts of a tripartite qubit system. Our approach considers the entanglement-polytope defined by the smallest eigenvalues of the reduced density matrices of the qubit-components. The measures identify global and genuine entanglement, and are respectively associated with the projection and rejection of a given point of the polytope on the corresponding biseparable segments. Solving the so called `inverse problem', we also discuss a way to force the system to behave in a particular form, which opens the possibility of controlling and manipulating entanglement for practical purposes.","sentences":["We introduce a purely geometric formulation for two different measures addressed to quantify the entanglement between different parts of a tripartite qubit system.","Our approach considers the entanglement-polytope defined by the smallest eigenvalues of the reduced density matrices of the qubit-components.","The measures identify global and genuine entanglement, and are respectively associated with the projection and rejection of a given point of the polytope on the corresponding biseparable segments.","Solving the so called `inverse problem', we also discuss a way to force the system to behave in a particular form, which opens the possibility of controlling and manipulating entanglement for practical purposes."],"url":"http://arxiv.org/abs/2405.09466v1","category":"quant-ph"}
{"created":"2024-05-15 15:55:33","title":"Zeno Effect Suppression of Gauge Drift in Quantum Simulations","abstract":"Quantum simulation of lattice gauge theories is a promising tool for the study of many complicated problems including ones with real-time dynamics. For gauge theories, however, there is a major challenge in maintaining gauge invariance during time evolution. Such theories have a full Hilbert space that is larger than the physical space -- the set of states which are gauge invariant or equivalently respect the Gauss law. While an exact implementation of Hamiltonian dynamics starting in the physical Hilbert space will keep the system in the physical space, various types of errors will inevitably produce components outside of it. This work proposes a method of suppressing this gauge drift via the Zeno effect. As in the standard picture of the Zeno effect, our method relies on frequent projection onto the physical subspace. Additionally, a technique is discussed to reduce the speed of the gauge drift, which helps to reduce the required frequency of projections. We demonstrate our method on a $\\mathbb{Z}_2$ gauge theory toy model.","sentences":["Quantum simulation of lattice gauge theories is a promising tool for the study of many complicated problems including ones with real-time dynamics.","For gauge theories, however, there is a major challenge in maintaining gauge invariance during time evolution.","Such theories have a full Hilbert space that is larger than the physical space -- the set of states which are gauge invariant or equivalently respect the Gauss law.","While an exact implementation of Hamiltonian dynamics starting in the physical Hilbert space will keep the system in the physical space, various types of errors will inevitably produce components outside of it.","This work proposes a method of suppressing this gauge drift via the Zeno effect.","As in the standard picture of the Zeno effect, our method relies on frequent projection onto the physical subspace.","Additionally, a technique is discussed to reduce the speed of the gauge drift, which helps to reduce the required frequency of projections.","We demonstrate our method on a $\\mathbb{Z}_2$ gauge theory toy model."],"url":"http://arxiv.org/abs/2405.09462v1","category":"hep-lat"}
{"created":"2024-05-15 15:51:38","title":"Non-contact Lung Disease Classification via OFDM-based Passive 6G ISAC Sensing","abstract":"This paper is the first to present a novel, non-contact method that utilizes orthogonal frequency division multiplexing (OFDM) signals (of frequency 5.23 GHz, emitted by a software defined radio) to radio-expose the pulmonary patients in order to differentiate between five prevalent respiratory diseases, i.e., Asthma, Chronic obstructive pulmonary disease (COPD), Interstitial lung disease (ILD), Pneumonia (PN), and Tuberculosis (TB). The fact that each pulmonary disease leads to a distinct breathing pattern, and thus modulates the OFDM signal in a different way, motivates us to acquire OFDM-Breathe dataset, first of its kind. It consists of 13,920 seconds of raw RF data (at 64 distinct OFDM frequencies) that we have acquired from a total of 116 subjects in a hospital setting (25 healthy control subjects, and 91 pulmonary patients). Among the 91 patients, 25 have Asthma, 25 have COPD, 25 have TB, 5 have ILD, and 11 have PN. We implement a number of machine and deep learning models in order to do lung disease classification using OFDM-Breathe dataset. The vanilla convolutional neural network outperforms all the models with an accuracy of 97%, and stands out in terms of precision, recall, and F1-score. The ablation study reveals that it is sufficient to radio-observe the human chest on seven different microwave frequencies only, in order to make a reliable diagnosis (with 96% accuracy) of the underlying lung disease. This corresponds to a sensing overhead that is merely 10.93% of the allocated bandwidth. This points to the feasibility of 6G integrated sensing and communication (ISAC) systems of future where 89.07% of bandwidth still remains available for information exchange amidst on-demand health sensing. Through 6G ISAC, this work provides a tool for mass screening for respiratory diseases (e.g., COVID-19) at public places.","sentences":["This paper is the first to present a novel, non-contact method that utilizes orthogonal frequency division multiplexing (OFDM) signals (of frequency 5.23 GHz, emitted by a software defined radio) to radio-expose the pulmonary patients in order to differentiate between five prevalent respiratory diseases, i.e., Asthma, Chronic obstructive pulmonary disease (COPD), Interstitial lung disease (ILD), Pneumonia (PN), and Tuberculosis (TB).","The fact that each pulmonary disease leads to a distinct breathing pattern, and thus modulates the OFDM signal in a different way, motivates us to acquire OFDM-Breathe dataset, first of its kind.","It consists of 13,920 seconds of raw RF data (at 64 distinct OFDM frequencies) that we have acquired from a total of 116 subjects in a hospital setting (25 healthy control subjects, and 91 pulmonary patients).","Among the 91 patients, 25 have Asthma, 25 have COPD, 25 have TB, 5 have ILD, and 11 have PN.","We implement a number of machine and deep learning models in order to do lung disease classification using OFDM-Breathe dataset.","The vanilla convolutional neural network outperforms all the models with an accuracy of 97%, and stands out in terms of precision, recall, and F1-score.","The ablation study reveals that it is sufficient to radio-observe the human chest on seven different microwave frequencies only, in order to make a reliable diagnosis (with 96% accuracy) of the underlying lung disease.","This corresponds to a sensing overhead that is merely 10.93% of the allocated bandwidth.","This points to the feasibility of 6G integrated sensing and communication (ISAC) systems of future where 89.07% of bandwidth still remains available for information exchange amidst on-demand health sensing.","Through 6G ISAC, this work provides a tool for mass screening for respiratory diseases (e.g., COVID-19) at public places."],"url":"http://arxiv.org/abs/2405.09458v1","category":"eess.SP"}
{"created":"2024-05-15 15:29:35","title":"Perturbed Integrators Chain Control via Barrier Function Adaptation and Lyapunov Redesign","abstract":"Lyapunov redesign is a classical technique that uses a nominal control and its corresponding nominal Lyapunov function to design a discontinuous control, such that it compensates the uncertainties and disturbances. In this paper, the idea of Lyapunov redesign is used to propose an adaptive time-varying gain controller to stabilize a class of perturbed chain of integrators with an unknown control coefficient. It is assumed that the upper bound of the perturbation exists but is unknown. A proportional navigation feedback type gain is used to drive the system's trajectories into a prescribed vicinity of the origin in a predefined time, measured using a quadratic Lyapunov function. Once this neighborhood is reached, a barrier function-based gain is used, ensuring that the system's trajectories never leave this neighborhood despite uncertainties and perturbations. Experimental validation of the proposed controller in Furuta's pendulum is presented.","sentences":["Lyapunov redesign is a classical technique that uses a nominal control and its corresponding nominal Lyapunov function to design a discontinuous control, such that it compensates the uncertainties and disturbances.","In this paper, the idea of Lyapunov redesign is used to propose an adaptive time-varying gain controller to stabilize a class of perturbed chain of integrators with an unknown control coefficient.","It is assumed that the upper bound of the perturbation exists but is unknown.","A proportional navigation feedback type gain is used to drive the system's trajectories into a prescribed vicinity of the origin in a predefined time, measured using a quadratic Lyapunov function.","Once this neighborhood is reached, a barrier function-based gain is used, ensuring that the system's trajectories never leave this neighborhood despite uncertainties and perturbations.","Experimental validation of the proposed controller in Furuta's pendulum is presented."],"url":"http://arxiv.org/abs/2405.09438v1","category":"eess.SY"}
{"created":"2024-05-15 15:23:18","title":"Analyzing and Enhancing Queue Sampling for Energy-Efficient Remote Control of Bandits","abstract":"In recent years, the integration of communication and control systems has gained significant traction in various domains, ranging from autonomous vehicles to industrial automation and beyond. Multi-armed bandit (MAB) algorithms have proven their effectiveness as a robust framework for solving control problems. In this work, we investigate the use of MAB algorithms to control remote devices, which faces considerable challenges primarily represented by latency and reliability. We analyze the effectiveness of MABs operating in environments where the action feedback from controlled devices is transmitted over an unreliable communication channel and stored in a Geo/Geo/1 queue. We investigate the impact of queue sampling strategies on the MAB performance, and introduce a new stochastic approach. Its performance in terms of regret is evaluated against established algorithms in the literature for both upper confidence bound (UCB) and Thompson Sampling (TS) algorithms. Additionally, we study the trade-off between maximizing rewards and minimizing energy consumption.","sentences":["In recent years, the integration of communication and control systems has gained significant traction in various domains, ranging from autonomous vehicles to industrial automation and beyond.","Multi-armed bandit (MAB) algorithms have proven their effectiveness as a robust framework for solving control problems.","In this work, we investigate the use of MAB algorithms to control remote devices, which faces considerable challenges primarily represented by latency and reliability.","We analyze the effectiveness of MABs operating in environments where the action feedback from controlled devices is transmitted over an unreliable communication channel and stored in a Geo/Geo/1 queue.","We investigate the impact of queue sampling strategies on the MAB performance, and introduce a new stochastic approach.","Its performance in terms of regret is evaluated against established algorithms in the literature for both upper confidence bound (UCB) and Thompson Sampling (TS) algorithms.","Additionally, we study the trade-off between maximizing rewards and minimizing energy consumption."],"url":"http://arxiv.org/abs/2405.09430v1","category":"eess.SY"}
{"created":"2024-05-15 15:20:18","title":"Physics-Informed Neural Network for Multirotor Slung Load Systems Modeling","abstract":"Recent advances in aerial robotics have enabled the use of multirotor vehicles for autonomous payload transportation. Resorting only to classical methods to reliably model a quadrotor carrying a cable-slung load poses significant challenges. On the other hand, purely data-driven learning methods do not comply by design with the problem's physical constraints, especially in states that are not densely represented in training data. In this work, we explore the use of physics informed neural networks to learn an end-to-end model of the multirotor-slung-load system and, at a given time, estimate a sequence of the future system states. An LSTM encoder decoder with an attention mechanism is used to capture the dynamics of the system. To guarantee the cohesiveness between the multiple predicted states of the system, we propose the use of a physics-based term in the loss function, which includes a discretized physical model derived from first principles together with slack variables that allow for a small mismatch between expected and predicted values. To train the model, a dataset using a real-world quadrotor carrying a slung load was curated and is made available. Prediction results are presented and corroborate the feasibility of the approach. The proposed method outperforms both the first principles physical model and a comparable neural network model trained without the physics regularization proposed.","sentences":["Recent advances in aerial robotics have enabled the use of multirotor vehicles for autonomous payload transportation.","Resorting only to classical methods to reliably model a quadrotor carrying a cable-slung load poses significant challenges.","On the other hand, purely data-driven learning methods do not comply by design with the problem's physical constraints, especially in states that are not densely represented in training data.","In this work, we explore the use of physics informed neural networks to learn an end-to-end model of the multirotor-slung-load system and, at a given time, estimate a sequence of the future system states.","An LSTM encoder decoder with an attention mechanism is used to capture the dynamics of the system.","To guarantee the cohesiveness between the multiple predicted states of the system, we propose the use of a physics-based term in the loss function, which includes a discretized physical model derived from first principles together with slack variables that allow for a small mismatch between expected and predicted values.","To train the model, a dataset using a real-world quadrotor carrying a slung load was curated and is made available.","Prediction results are presented and corroborate the feasibility of the approach.","The proposed method outperforms both the first principles physical model and a comparable neural network model trained without the physics regularization proposed."],"url":"http://arxiv.org/abs/2405.09428v1","category":"cs.RO"}
{"created":"2024-05-15 15:20:05","title":"Laser cooling of barium monofluoride molecules using synthesized optical spectra","abstract":"We demonstrate laser cooling of barium monofluoride (138BaF) molecules. We use serrodynes to synthesize time-sequenced optical spectra that can be precisely tailored to the hyperfine structure of this heaviest non-radioactive alkaline earth monofluoride. By optimizing these optical spectra, we realize strong Sisyphus cooling forces that efficiently collimate a molecular beam. Our technique is an important step towards using intense beams of barium monofluoride for precision measurement applications, and will be useful for cooling other molecular species with complex level structure.","sentences":["We demonstrate laser cooling of barium monofluoride (138BaF) molecules.","We use serrodynes to synthesize time-sequenced optical spectra that can be precisely tailored to the hyperfine structure of this heaviest non-radioactive alkaline earth monofluoride.","By optimizing these optical spectra, we realize strong Sisyphus cooling forces that efficiently collimate a molecular beam.","Our technique is an important step towards using intense beams of barium monofluoride for precision measurement applications, and will be useful for cooling other molecular species with complex level structure."],"url":"http://arxiv.org/abs/2405.09427v1","category":"physics.atom-ph"}
{"created":"2024-05-15 15:13:01","title":"Highly Tunable Ru-dimer Molecular Orbital State in 6H-perovskite Ba$_3$MRu$_2$O$_9$","abstract":"Molecular orbital (MO) systems with clusters of heavy transition metal (TM) ions are one of the most important classes of model materials for studying the interplay between local physics and effects of itinerancy. Despite a large number of candidates identified in the family of 4d TM materials, an understanding of their physics from competing \\textit{microscopic} energy scales is still missing. We bridge this gap by reporting the first resonant inelastic X-ray scattering (RIXS) measurement on a well-known series of Ru dimer systems with a 6H-perovskite structure, Ba$_3$MRu$_2$O$_9$ (M$^{3+}$=In$^{3+}$, Y$^{3+}$, La$^{3+}$). Our RIXS measurements reveal an extremely fragile MO state in these Ru dimer compounds, evidenced by an abrupt change in the RIXS spectrum accompanying a tiny change in the local structure tuned by the M-site ion. By modelling the RIXS spectra, we attribute the enhanced electronic instability in Ba$_3$MRu$_2$O$_9$ to the combined effect of a large hopping and a small spin-orbit coupling in the Ru dimers. The unique combination of energy scales uncovered in the present study make Ru MO systems ideal model systems for studying quantum phase transitions with molecular orbitals.","sentences":["Molecular orbital (MO) systems with clusters of heavy transition metal (TM) ions are one of the most important classes of model materials for studying the interplay between local physics and effects of itinerancy.","Despite a large number of candidates identified in the family of 4d TM materials, an understanding of their physics from competing \\textit{microscopic} energy scales is still missing.","We bridge this gap by reporting the first resonant inelastic X-ray scattering (RIXS) measurement on a well-known series of Ru dimer systems with a 6H-perovskite structure, Ba$_3$MRu$_2$O$_9$ (M$^{3+}$=In$^{3+}$, Y$^{3+}$, La$^{3+}$).","Our RIXS measurements reveal an extremely fragile MO state in these Ru dimer compounds, evidenced by an abrupt change in the RIXS spectrum accompanying a tiny change in the local structure tuned by the M-site ion.","By modelling the RIXS spectra, we attribute the enhanced electronic instability in Ba$_3$MRu$_2$O$_9$ to the combined effect of a large hopping and a small spin-orbit coupling in the Ru dimers.","The unique combination of energy scales uncovered in the present study make Ru MO systems ideal model systems for studying quantum phase transitions with molecular orbitals."],"url":"http://arxiv.org/abs/2405.09418v1","category":"cond-mat.str-el"}
{"created":"2024-05-15 15:04:27","title":"Real-World Federated Learning in Radiology: Hurdles to overcome and Benefits to gain","abstract":"Objective: Federated Learning (FL) enables collaborative model training while keeping data locally. Currently, most FL studies in radiology are conducted in simulated environments due to numerous hurdles impeding its translation into practice. The few existing real-world FL initiatives rarely communicate specific measures taken to overcome these hurdles, leaving behind a significant knowledge gap. Minding efforts to implement real-world FL, there is a notable lack of comprehensive assessment comparing FL to less complex alternatives. Materials & Methods: We extensively reviewed FL literature, categorizing insights along with our findings according to their nature and phase while establishing a FL initiative, summarized to a comprehensive guide. We developed our own FL infrastructure within the German Radiological Cooperative Network (RACOON) and demonstrated its functionality by training FL models on lung pathology segmentation tasks across six university hospitals. We extensively evaluated FL against less complex alternatives in three distinct evaluation scenarios. Results: The proposed guide outlines essential steps, identified hurdles, and proposed solutions for establishing successful FL initiatives conducting real-world experiments. Our experimental results show that FL outperforms less complex alternatives in all evaluation scenarios, justifying the effort required to translate FL into real-world applications. Discussion & Conclusion: Our proposed guide aims to aid future FL researchers in circumventing pitfalls and accelerating translation of FL into radiological applications. Our results underscore the value of efforts needed to translate FL into real-world applications by demonstrating advantageous performance over alternatives, and emphasize the importance of strategic organization, robust management of distributed data and infrastructure in real-world settings.","sentences":["Objective: Federated Learning (FL) enables collaborative model training while keeping data locally.","Currently, most FL studies in radiology are conducted in simulated environments due to numerous hurdles impeding its translation into practice.","The few existing real-world FL initiatives rarely communicate specific measures taken to overcome these hurdles, leaving behind a significant knowledge gap.","Minding efforts to implement real-world FL, there is a notable lack of comprehensive assessment comparing FL to less complex alternatives.","Materials & Methods: We extensively reviewed FL literature, categorizing insights along with our findings according to their nature and phase while establishing a FL initiative, summarized to a comprehensive guide.","We developed our own FL infrastructure within the German Radiological Cooperative Network (RACOON) and demonstrated its functionality by training FL models on lung pathology segmentation tasks across six university hospitals.","We extensively evaluated FL against less complex alternatives in three distinct evaluation scenarios.","Results:","The proposed guide outlines essential steps, identified hurdles, and proposed solutions for establishing successful FL initiatives conducting real-world experiments.","Our experimental results show that FL outperforms less complex alternatives in all evaluation scenarios, justifying the effort required to translate FL into real-world applications.","Discussion & Conclusion: Our proposed guide aims to aid future FL researchers in circumventing pitfalls and accelerating translation of FL into radiological applications.","Our results underscore the value of efforts needed to translate FL into real-world applications by demonstrating advantageous performance over alternatives, and emphasize the importance of strategic organization, robust management of distributed data and infrastructure in real-world settings."],"url":"http://arxiv.org/abs/2405.09409v1","category":"cs.CV"}
{"created":"2024-05-15 15:01:51","title":"On identifying the non-linear dynamics of a hovercraft using an end-to-end deep learning approach","abstract":"We present the identification of the non-linear dynamics of a novel hovercraft design, employing end-to-end deep learning techniques. Our experimental setup consists of a hovercraft propelled by racing drone propellers mounted on a lightweight foam base, allowing it to float and be controlled freely on an air hockey table. We learn parametrized physics-inspired non-linear models directly from data trajectories, leveraging gradient-based optimization techniques prevalent in machine learning research. The chosen model structure allows us to control the position of the hovercraft precisely on the air hockey table. We then analyze the prediction performance and demonstrate the closed-loop control performance on the real system.","sentences":["We present the identification of the non-linear dynamics of a novel hovercraft design, employing end-to-end deep learning techniques.","Our experimental setup consists of a hovercraft propelled by racing drone propellers mounted on a lightweight foam base, allowing it to float and be controlled freely on an air hockey table.","We learn parametrized physics-inspired non-linear models directly from data trajectories, leveraging gradient-based optimization techniques prevalent in machine learning research.","The chosen model structure allows us to control the position of the hovercraft precisely on the air hockey table.","We then analyze the prediction performance and demonstrate the closed-loop control performance on the real system."],"url":"http://arxiv.org/abs/2405.09405v1","category":"eess.SY"}
{"created":"2024-05-15 14:53:55","title":"Giant branch planetary systems: Dynamical and radiative evolution","abstract":"In seven billion years, the Sun will be dead. As stars like the Sun pass from their present state to that of a dead white dwarf star, they undergo two phases of extremely high luminosity and radius -- the red giant branch and the asymptotic giant branch -- during which they will lose half or more of their mass. These changes to the star have a significant impact on orbiting planets, asteroids and comets. The large stellar radius (beyond the current orbit of the Earth) leads to the engulfment of bodies entering the stellar envelope, a process enhanced by strong tidal interactions. The high luminosity affects bodies' orbits and physical properties, while mass loss can later trigger the destabilisation of bodies around white dwarfs. It is necessary to understand these processes to understand both the future of our Solar System, and to interpret growing observations of planetary systems around evolved stars.","sentences":["In seven billion years, the Sun will be dead.","As stars like the Sun pass from their present state to that of a dead white dwarf star, they undergo two phases of extremely high luminosity and radius -- the red giant branch and the asymptotic giant branch -- during which they will lose half or more of their mass.","These changes to the star have a significant impact on orbiting planets, asteroids and comets.","The large stellar radius (beyond the current orbit of the Earth) leads to the engulfment of bodies entering the stellar envelope, a process enhanced by strong tidal interactions.","The high luminosity affects bodies' orbits and physical properties, while mass loss can later trigger the destabilisation of bodies around white dwarfs.","It is necessary to understand these processes to understand both the future of our Solar System, and to interpret growing observations of planetary systems around evolved stars."],"url":"http://arxiv.org/abs/2405.09399v1","category":"astro-ph.EP"}
{"created":"2024-05-15 14:43:28","title":"Clustering of higher order connected correlations in C$^*$ dynamical systems","abstract":"In the context of $C^*$ dynamical systems, we consider a locally compact group $G$ acting by $^*$-automorphisms on a C$^*$ algebra $\\mathfrak{U}$ of observables, and assume a state of $\\mathfrak{U}$ that satisfies the clustering property with respect to a net of group elements of $G$. That is, the two-point connected correlation function vanishes in the limit on the net, when one observable is translated under the group action. Then we show that all higher order connected correlation functions (Ursell functions, or classical cumulants) and all free correlation functions (free cumulants, from free probability) vanish at the same rate in that limit. Additionally, we show that mean clustering, also called ergodicity, extends to higher order correlations. We then apply those results to equilibrium states of quantum spin lattice models. Under certain assumptions on the range of the interaction, high temperature Gibbs states are known to be exponentially clustering w.r.t. space translations. Combined with the Lieb-Robinson bound, one obtains exponential clustering for space-time translations outside the Lieb-Robinson light-cone. Therefore, by our present results, all the higher order connected and free correlation functions will vanish exponentially under space-time translations outside the Lieb-Robinson light cone, in high temperature Gibbs states. Another consequence is that their long-time averaging over a space-time ray vanishes for almost every ray velocity.","sentences":["In the context of $C^*$ dynamical systems, we consider a locally compact group $G$ acting by $^*$-automorphisms on a C$^*$ algebra $\\mathfrak{U}$ of observables, and assume a state of $\\mathfrak{U}$ that satisfies the clustering property with respect to a net of group elements of $G$. That is, the two-point connected correlation function vanishes in the limit on the net, when one observable is translated under the group action.","Then we show that all higher order connected correlation functions (Ursell functions, or classical cumulants) and all free correlation functions (free cumulants, from free probability) vanish at the same rate in that limit.","Additionally, we show that mean clustering, also called ergodicity, extends to higher order correlations.","We then apply those results to equilibrium states of quantum spin lattice models.","Under certain assumptions on the range of the interaction, high temperature Gibbs states are known to be exponentially clustering w.r.t. space translations.","Combined with the Lieb-Robinson bound, one obtains exponential clustering for space-time translations outside the Lieb-Robinson light-cone.","Therefore, by our present results, all the higher order connected and free correlation functions will vanish exponentially under space-time translations outside the Lieb-Robinson light cone, in high temperature Gibbs states.","Another consequence is that their long-time averaging over a space-time ray vanishes for almost every ray velocity."],"url":"http://arxiv.org/abs/2405.09388v1","category":"math-ph"}
{"created":"2024-05-15 14:29:03","title":"Boundedness of metaplectic operators within $L^p$ spaces, applications to pseudodifferential calculus, and time-frequency representations","abstract":"Housdorff-Young's inequality establishes the boundedness of the Fourier transform from $L^p$ to $L^q$ spaces for $1\\leq p\\leq2$ and $q=p'$, where $p'$ denotes the Lebesgue-conjugate exponent of $p$. This paper extends this classical result by characterizing the $L^p-L^q$ boundedness of all metaplectic operators, which play a significant role in harmonic analysis. We demonstrate that metaplectic operators are bounded on Lebesgue spaces if and only if their symplectic projection is either free or lower block triangular. As a byproduct, we identify metaplectic operators that serve as homeomorphisms of $L^p$ spaces. To achieve this, we leverage a parametrization of the symplectic group by F. M. Dopico and C. R. Johnson involving products of complex exponentials with quadratic phase, Fourier multipliers, linear changes of variables, and partial Fourier transforms. Then, we use our findings to provide boundedness results within $L^p$ spaces for pseudodifferential operators with symbols in Lebesgue spaces, and quantized by means of metaplectic operators. These quantizations consists of shift-invertible metaplectic Wigner distributions, which play a fundamental role in measuring local phase-space concentration of signals. Using the Dopico-Johnson factorization, we infer a decomposition law for metaplectic operators on $L^2(\\mathbb{R}^{2d})$ in terms of shift-invertible metaplectic operators, establish the density of shift-invertible symplectic matrices in $Sp(2d,\\mathbb{R})$, and prove that the lack of shift-invertibility prevents metaplectic Wigner distributions to define the so-called modulation spaces $M^p(\\mathbb{R}^d)$.","sentences":["Housdorff-Young's inequality establishes the boundedness of the Fourier transform from $L^p$ to $L^q$ spaces for $1\\leq p\\leq2$ and $q=p'$, where $p'$ denotes the Lebesgue-conjugate exponent of $p$. This paper extends this classical result by characterizing the $L^p-L^q$ boundedness of all metaplectic operators, which play a significant role in harmonic analysis.","We demonstrate that metaplectic operators are bounded on Lebesgue spaces if and only if their symplectic projection is either free or lower block triangular.","As a byproduct, we identify metaplectic operators that serve as homeomorphisms of $L^p$ spaces.","To achieve this, we leverage a parametrization of the symplectic group by F. M. Dopico and C. R. Johnson involving products of complex exponentials with quadratic phase, Fourier multipliers, linear changes of variables, and partial Fourier transforms.","Then, we use our findings to provide boundedness results within $L^p$ spaces for pseudodifferential operators with symbols in Lebesgue spaces, and quantized by means of metaplectic operators.","These quantizations consists of shift-invertible metaplectic Wigner distributions, which play a fundamental role in measuring local phase-space concentration of signals.","Using the Dopico-Johnson factorization, we infer a decomposition law for metaplectic operators on $L^2(\\mathbb{R}^{2d})$ in terms of shift-invertible metaplectic operators, establish the density of shift-invertible symplectic matrices in $Sp(2d,\\mathbb{R})$, and prove that the lack of shift-invertibility prevents metaplectic Wigner distributions to define the so-called modulation spaces $M^p(\\mathbb{R}^d)$."],"url":"http://arxiv.org/abs/2405.09378v1","category":"math.FA"}
{"created":"2024-05-15 14:26:57","title":"Maxwell's demon across the quantum-to-classical transition","abstract":"In scenarios coined Maxwell's demon, information on microscopic degrees of freedom is used to seemingly violate the second law of thermodynamics. This has been studied in the classical as well as the quantum domain. In this paper, we study an implementation of Maxwell's demon that can operate in both domains. In particular, we investigate information-to-work conversion over the quantum-to-classical transition. The demon continuously measures the charge state of a double quantum dot, and uses this information to guide electrons against a voltage bias by tuning the on-site energies of the dots. Coherent tunneling between the dots allows for the buildup of quantum coherence in the system. Under strong measurements, the coherence is suppressed, and the system is well-described by a classical model. As the measurement strength is further increased, the Zeno effect prohibits interdot tunneling. A Zeno-like effect is also observed for weak measurements, where measurement errors lead to fluctuations in the on-site energies, dephasing the system. We anticipate similar behaviors in other quantum systems under continuous measurement and feedback control, making our results relevant for implementations in quantum technology and quantum control.","sentences":["In scenarios coined Maxwell's demon, information on microscopic degrees of freedom is used to seemingly violate the second law of thermodynamics.","This has been studied in the classical as well as the quantum domain.","In this paper, we study an implementation of Maxwell's demon that can operate in both domains.","In particular, we investigate information-to-work conversion over the quantum-to-classical transition.","The demon continuously measures the charge state of a double quantum dot, and uses this information to guide electrons against a voltage bias by tuning the on-site energies of the dots.","Coherent tunneling between the dots allows for the buildup of quantum coherence in the system.","Under strong measurements, the coherence is suppressed, and the system is well-described by a classical model.","As the measurement strength is further increased, the Zeno effect prohibits interdot tunneling.","A Zeno-like effect is also observed for weak measurements, where measurement errors lead to fluctuations in the on-site energies, dephasing the system.","We anticipate similar behaviors in other quantum systems under continuous measurement and feedback control, making our results relevant for implementations in quantum technology and quantum control."],"url":"http://arxiv.org/abs/2405.09376v1","category":"quant-ph"}
{"created":"2024-05-15 14:22:25","title":"Investigating the Effect of Operation Mode and Manifestation on Physicalizations of Dynamic Processes","abstract":"We conducted a study to systematically investigate the communication of complex dynamic processes along a two-dimensional design space, where the axes represent a representation's manifestation (physical or virtual) and operation (manual or automatic). We exemplify the design space on a model embodying cardiovascular pathologies, represented by a mechanism where a liquid is pumped into a draining vessel, with complications illustrated through modifications to the model. The results of a mixed-methods lab study with 28 participants show that both physical manifestation and manual operation have a strong positive impact on the audience's engagement. The study does not show a measurable knowledge increase with respect to cardiovascular pathologies using manually operated physical representations. However, subjectively, participants report a better understanding of the process-mainly through non-visual cues like haptics, but also auditory cues. The study also indicates an increased task load when interacting with the process, which, however, seems to play a minor role for the participants. Overall, the study shows a clear potential of physicalization for the communication of complex dynamic processes, which only fully unfold if observers have to chance to interact with the process.","sentences":["We conducted a study to systematically investigate the communication of complex dynamic processes along a two-dimensional design space, where the axes represent a representation's manifestation (physical or virtual) and operation (manual or automatic).","We exemplify the design space on a model embodying cardiovascular pathologies, represented by a mechanism where a liquid is pumped into a draining vessel, with complications illustrated through modifications to the model.","The results of a mixed-methods lab study with 28 participants show that both physical manifestation and manual operation have a strong positive impact on the audience's engagement.","The study does not show a measurable knowledge increase with respect to cardiovascular pathologies using manually operated physical representations.","However, subjectively, participants report a better understanding of the process-mainly through non-visual cues like haptics, but also auditory cues.","The study also indicates an increased task load when interacting with the process, which, however, seems to play a minor role for the participants.","Overall, the study shows a clear potential of physicalization for the communication of complex dynamic processes, which only fully unfold if observers have to chance to interact with the process."],"url":"http://arxiv.org/abs/2405.09372v1","category":"cs.HC"}
{"created":"2024-05-15 14:12:38","title":"Visual Attention Based Cognitive Human-Robot Collaboration for Pedicle Screw Placement in Robot-Assisted Orthopedic Surgery","abstract":"Current orthopedic robotic systems largely focus on navigation, aiding surgeons in positioning a guiding tube but still requiring manual drilling and screw placement. The automation of this task not only demands high precision and safety due to the intricate physical interactions between the surgical tool and bone but also poses significant risks when executed without adequate human oversight. As it involves continuous physical interaction, the robot should collaborate with the surgeon, understand the human intent, and always include the surgeon in the loop. To achieve this, this paper proposes a new cognitive human-robot collaboration framework, including the intuitive AR-haptic human-robot interface, the visual-attention-based surgeon model, and the shared interaction control scheme for the robot. User studies on a robotic platform for orthopedic surgery are presented to illustrate the performance of the proposed method. The results demonstrate that the proposed human-robot collaboration framework outperforms full robot and full human control in terms of safety and ergonomics.","sentences":["Current orthopedic robotic systems largely focus on navigation, aiding surgeons in positioning a guiding tube but still requiring manual drilling and screw placement.","The automation of this task not only demands high precision and safety due to the intricate physical interactions between the surgical tool and bone but also poses significant risks when executed without adequate human oversight.","As it involves continuous physical interaction, the robot should collaborate with the surgeon, understand the human intent, and always include the surgeon in the loop.","To achieve this, this paper proposes a new cognitive human-robot collaboration framework, including the intuitive AR-haptic human-robot interface, the visual-attention-based surgeon model, and the shared interaction control scheme for the robot.","User studies on a robotic platform for orthopedic surgery are presented to illustrate the performance of the proposed method.","The results demonstrate that the proposed human-robot collaboration framework outperforms full robot and full human control in terms of safety and ergonomics."],"url":"http://arxiv.org/abs/2405.09359v1","category":"cs.RO"}
{"created":"2024-05-15 14:11:59","title":"A universal optimization framework based on cycle ranking for influence maximization in complex networks","abstract":"Influence maximization aims to identify a set of influential individuals, referred to as influencers, as information sources to maximize the spread of information within networks, constituting a vital combinatorial optimization problem with extensive practical applications and sustained interdisciplinary interest. Diverse approaches have been devised to efficiently address this issue, one of which involves selecting the influencers from a given centrality ranking. In this paper, we propose a novel optimization framework based on ranking basic cycles in networks, capable of selecting the influencers from diverse centrality measures. The experimental results demonstrate that, compared to directly selecting the top-k nodes from centrality sequences and other state-of-the-art optimization approaches, the new framework can expand the dissemination range by 1.5 to 3 times. Counterintuitively, it exhibits minimal hub property, with the average distance between influencers being only one-third of alternative approaches, regardless of the centrality metrics or network types. Our study not only paves the way for novel strategies in influence maximization but also underscores the unique potential of underappreciated cycle structures.","sentences":["Influence maximization aims to identify a set of influential individuals, referred to as influencers, as information sources to maximize the spread of information within networks, constituting a vital combinatorial optimization problem with extensive practical applications and sustained interdisciplinary interest.","Diverse approaches have been devised to efficiently address this issue, one of which involves selecting the influencers from a given centrality ranking.","In this paper, we propose a novel optimization framework based on ranking basic cycles in networks, capable of selecting the influencers from diverse centrality measures.","The experimental results demonstrate that, compared to directly selecting the top-k nodes from centrality sequences and other state-of-the-art optimization approaches, the new framework can expand the dissemination range by 1.5 to 3 times.","Counterintuitively, it exhibits minimal hub property, with the average distance between influencers being only one-third of alternative approaches, regardless of the centrality metrics or network types.","Our study not only paves the way for novel strategies in influence maximization but also underscores the unique potential of underappreciated cycle structures."],"url":"http://arxiv.org/abs/2405.09357v1","category":"cs.SI"}
{"created":"2024-05-15 14:03:38","title":"Large coordinate kernel attention network for lightweight image super-resolution","abstract":"The multi-scale receptive field and large kernel attention (LKA) module have been shown to significantly improve performance in the lightweight image super-resolution task. However, existing lightweight super-resolution (SR) methods seldom pay attention to designing efficient building block with multi-scale receptive field for local modeling, and their LKA modules face a quadratic increase in computational and memory footprints as the convolutional kernel size increases. To address the first issue, we propose the multi-scale blueprint separable convolutions (MBSConv) as highly efficient building block with multi-scale receptive field, it can focus on the learning for the multi-scale information which is a vital component of discriminative representation. As for the second issue, we revisit the key properties of LKA in which we find that the adjacent direct interaction of local information and long-distance dependencies is crucial to provide remarkable performance. Thus, taking this into account and in order to mitigate the complexity of LKA, we propose a large coordinate kernel attention (LCKA) module which decomposes the 2D convolutional kernels of the depth-wise convolutional layers in LKA into horizontal and vertical 1-D kernels. LCKA enables the adjacent direct interaction of local information and long-distance dependencies not only in the horizontal direction but also in the vertical. Besides, LCKA allows for the direct use of extremely large kernels in the depth-wise convolutional layers to capture more contextual information, which helps to significantly improve the reconstruction performance, and it incurs lower computational complexity and memory footprints. Integrating MBSConv and LCKA, we propose a large coordinate kernel attention network (LCAN).","sentences":["The multi-scale receptive field and large kernel attention (LKA) module have been shown to significantly improve performance in the lightweight image super-resolution task.","However, existing lightweight super-resolution (SR) methods seldom pay attention to designing efficient building block with multi-scale receptive field for local modeling, and their LKA modules face a quadratic increase in computational and memory footprints as the convolutional kernel size increases.","To address the first issue, we propose the multi-scale blueprint separable convolutions (MBSConv) as highly efficient building block with multi-scale receptive field, it can focus on the learning for the multi-scale information which is a vital component of discriminative representation.","As for the second issue, we revisit the key properties of LKA in which we find that the adjacent direct interaction of local information and long-distance dependencies is crucial to provide remarkable performance.","Thus, taking this into account and in order to mitigate the complexity of LKA, we propose a large coordinate kernel attention (LCKA) module which decomposes the 2D convolutional kernels of the depth-wise convolutional layers in LKA into horizontal and vertical 1-D kernels.","LCKA enables the adjacent direct interaction of local information and long-distance dependencies not only in the horizontal direction but also in the vertical.","Besides, LCKA allows for the direct use of extremely large kernels in the depth-wise convolutional layers to capture more contextual information, which helps to significantly improve the reconstruction performance, and it incurs lower computational complexity and memory footprints.","Integrating MBSConv and LCKA, we propose a large coordinate kernel attention network (LCAN)."],"url":"http://arxiv.org/abs/2405.09353v1","category":"eess.IV"}
{"created":"2024-05-15 14:00:58","title":"On the impact of the antenna radiation patterns in passive radio sensing","abstract":"Electromagnetic (EM) body models based on the scalar diffraction theory allow to predict the impact of subject motions on the radio propagation channel without requiring a time-consuming full-wave approach. On the other hand, they are less effective in complex environments characterized by significant multipath effects. Recently, emerging radio sensing applications have proposed the adoption of smart antennas with non-isotropic radiation characteristics to improve coverage.This letter investigates the impact of antenna radiation patterns in passive radio sensing applications. Adaptations of diffraction-based EM models are proposed to account for antenna non-uniform angular filtering. Next, we quantify experimentally the impact of diffraction and multipath disturbance components on radio sensing accuracy in environments with smart antennas.","sentences":["Electromagnetic (EM) body models based on the scalar diffraction theory allow to predict the impact of subject motions on the radio propagation channel without requiring a time-consuming full-wave approach.","On the other hand, they are less effective in complex environments characterized by significant multipath effects.","Recently, emerging radio sensing applications have proposed the adoption of smart antennas with non-isotropic radiation characteristics to improve coverage.","This letter investigates the impact of antenna radiation patterns in passive radio sensing applications.","Adaptations of diffraction-based EM models are proposed to account for antenna non-uniform angular filtering.","Next, we quantify experimentally the impact of diffraction and multipath disturbance components on radio sensing accuracy in environments with smart antennas."],"url":"http://arxiv.org/abs/2405.09352v1","category":"eess.SP"}
{"created":"2024-05-15 14:00:48","title":"Analysis of the Geometric Structure of Neural Networks and Neural ODEs via Morse Functions","abstract":"Besides classical feed-forward neural networks, also neural ordinary differential equations (neural ODEs) gained particular interest in recent years. Neural ODEs can be interpreted as an infinite depth limit of feed-forward or residual neural networks. We study the input-output dynamics of finite and infinite depth neural networks with scalar output. In the finite depth case, the input is a state associated to a finite number of nodes, which maps under multiple non-linear transformations to the state of one output node. In analogy, a neural ODE maps a linear transformation of the input to a linear transformation of its time-$T$ map. We show that depending on the specific structure of the network, the input-output map has different properties regarding the existence and regularity of critical points. These properties can be characterized via Morse functions, which are scalar functions, where every critical point is non-degenerate. We prove that critical points cannot exist, if the dimension of the hidden layer is monotonically decreasing or the dimension of the phase space is smaller or equal to the input dimension. In the case that critical points exist, we classify their regularity depending on the specific architecture of the network. We show that each critical point is non-degenerate, if for finite depth neural networks the underlying graph has no bottleneck, and if for neural ODEs, the linear transformations used have full rank. For each type of architecture, the proven properties are comparable in the finite and in the infinite depth case. The established theorems allow us to formulate results on universal embedding, i.e.\\ on the exact representation of maps by neural networks and neural ODEs. Our dynamical systems viewpoint on the geometric structure of the input-output map provides a fundamental understanding, why certain architectures perform better than others.","sentences":["Besides classical feed-forward neural networks, also neural ordinary differential equations (neural ODEs) gained particular interest in recent years.","Neural ODEs can be interpreted as an infinite depth limit of feed-forward or residual neural networks.","We study the input-output dynamics of finite and infinite depth neural networks with scalar output.","In the finite depth case, the input is a state associated to a finite number of nodes, which maps under multiple non-linear transformations to the state of one output node.","In analogy, a neural ODE maps a linear transformation of the input to a linear transformation of its time-$T$ map.","We show that depending on the specific structure of the network, the input-output map has different properties regarding the existence and regularity of critical points.","These properties can be characterized via Morse functions, which are scalar functions, where every critical point is non-degenerate.","We prove that critical points cannot exist, if the dimension of the hidden layer is monotonically decreasing or the dimension of the phase space is smaller or equal to the input dimension.","In the case that critical points exist, we classify their regularity depending on the specific architecture of the network.","We show that each critical point is non-degenerate, if for finite depth neural networks the underlying graph has no bottleneck, and if for neural ODEs, the linear transformations used have full rank.","For each type of architecture, the proven properties are comparable in the finite and in the infinite depth case.","The established theorems allow us to formulate results on universal embedding, i.e.\\ on the exact representation of maps by neural networks and neural ODEs.","Our dynamical systems viewpoint on the geometric structure of the input-output map provides a fundamental understanding, why certain architectures perform better than others."],"url":"http://arxiv.org/abs/2405.09351v1","category":"math.DS"}
{"created":"2024-05-15 13:33:23","title":"Application of Gated Recurrent Units for CT Trajectory Optimization","abstract":"Recent advances in computed tomography (CT) imaging, especially with dual-robot systems, have introduced new challenges for scan trajectory optimization. This paper presents a novel approach using Gated Recurrent Units (GRUs) to optimize CT scan trajectories. Our approach exploits the flexibility of robotic CT systems to select projections that enhance image quality by improving resolution and contrast while reducing scan time. We focus on cone-beam CT and employ several projection-based metrics, including absorption, pixel intensities, contrast-to-noise ratio, and data completeness. The GRU network aims to minimize data redundancy and maximize completeness with a limited number of projections. We validate our method using simulated data of a test specimen, focusing on a specific voxel of interest. The results show that the GRU-optimized scan trajectories can outperform traditional circular CT trajectories in terms of image quality metrics. For the used specimen, SSIM improves from 0.38 to 0.49 and CNR increases from 6.97 to 9.08. This finding suggests that the application of GRU in CT scan trajectory optimization can lead to more efficient, cost-effective, and high-quality imaging solutions.","sentences":["Recent advances in computed tomography (CT) imaging, especially with dual-robot systems, have introduced new challenges for scan trajectory optimization.","This paper presents a novel approach using Gated Recurrent Units (GRUs) to optimize CT scan trajectories.","Our approach exploits the flexibility of robotic CT systems to select projections that enhance image quality by improving resolution and contrast while reducing scan time.","We focus on cone-beam CT and employ several projection-based metrics, including absorption, pixel intensities, contrast-to-noise ratio, and data completeness.","The GRU network aims to minimize data redundancy and maximize completeness with a limited number of projections.","We validate our method using simulated data of a test specimen, focusing on a specific voxel of interest.","The results show that the GRU-optimized scan trajectories can outperform traditional circular CT trajectories in terms of image quality metrics.","For the used specimen, SSIM improves from 0.38 to 0.49 and CNR increases from 6.97 to 9.08.","This finding suggests that the application of GRU in CT scan trajectory optimization can lead to more efficient, cost-effective, and high-quality imaging solutions."],"url":"http://arxiv.org/abs/2405.09333v1","category":"cs.CV"}
{"created":"2024-05-15 13:32:59","title":"BARO: Robust Root Cause Analysis for Microservices via Multivariate Bayesian Online Change Point Detection","abstract":"Detecting failures and identifying their root causes promptly and accurately is crucial for ensuring the availability of microservice systems. A typical failure troubleshooting pipeline for microservices consists of two phases: anomaly detection and root cause analysis. While various existing works on root cause analysis require accurate anomaly detection, there is no guarantee of accurate estimation with anomaly detection techniques. Inaccurate anomaly detection results can significantly affect the root cause localization results. To address this challenge, we propose BARO, an end-to-end approach that integrates anomaly detection and root cause analysis for effectively troubleshooting failures in microservice systems. BARO leverages the Multivariate Bayesian Online Change Point Detection technique to model the dependency within multivariate time-series metrics data, enabling it to detect anomalies more accurately. BARO also incorporates a novel nonparametric statistical hypothesis testing technique for robustly identifying root causes, which is less sensitive to the accuracy of anomaly detection compared to existing works. Our comprehensive experiments conducted on three popular benchmark microservice systems demonstrate that BARO consistently outperforms state-of-the-art approaches in both anomaly detection and root cause analysis.","sentences":["Detecting failures and identifying their root causes promptly and accurately is crucial for ensuring the availability of microservice systems.","A typical failure troubleshooting pipeline for microservices consists of two phases: anomaly detection and root cause analysis.","While various existing works on root cause analysis require accurate anomaly detection, there is no guarantee of accurate estimation with anomaly detection techniques.","Inaccurate anomaly detection results can significantly affect the root cause localization results.","To address this challenge, we propose BARO, an end-to-end approach that integrates anomaly detection and root cause analysis for effectively troubleshooting failures in microservice systems.","BARO leverages the Multivariate Bayesian Online Change Point Detection technique to model the dependency within multivariate time-series metrics data, enabling it to detect anomalies more accurately.","BARO also incorporates a novel nonparametric statistical hypothesis testing technique for robustly identifying root causes, which is less sensitive to the accuracy of anomaly detection compared to existing works.","Our comprehensive experiments conducted on three popular benchmark microservice systems demonstrate that BARO consistently outperforms state-of-the-art approaches in both anomaly detection and root cause analysis."],"url":"http://arxiv.org/abs/2405.09330v1","category":"cs.SE"}
{"created":"2024-05-15 13:25:34","title":"Learning Coarse-Grained Dynamics on Graph","abstract":"We consider a Graph Neural Network (GNN) non-Markovian modeling framework to identify coarse-grained dynamical systems on graphs. Our main idea is to systematically determine the GNN architecture by inspecting how the leading term of the Mori-Zwanzig memory term depends on the coarse-grained interaction coefficients that encode the graph topology. Based on this analysis, we found that the appropriate GNN architecture that will account for $K$-hop dynamical interactions has to employ a Message Passing (MP) mechanism with at least $2K$ steps. We also deduce that the memory length required for an accurate closure model decreases as a function of the interaction strength under the assumption that the interaction strength exhibits a power law that decays as a function of the hop distance. Supporting numerical demonstrations on two examples, a heterogeneous Kuramoto oscillator model and a power system, suggest that the proposed GNN architecture can predict the coarse-grained dynamics under fixed and time-varying graph topologies.","sentences":["We consider a Graph Neural Network (GNN) non-Markovian modeling framework to identify coarse-grained dynamical systems on graphs.","Our main idea is to systematically determine the GNN architecture by inspecting how the leading term of the Mori-Zwanzig memory term depends on the coarse-grained interaction coefficients that encode the graph topology.","Based on this analysis, we found that the appropriate GNN architecture that will account for $K$-hop dynamical interactions has to employ a Message Passing (MP) mechanism with at least $2K$ steps.","We also deduce that the memory length required for an accurate closure model decreases as a function of the interaction strength under the assumption that the interaction strength exhibits a power law that decays as a function of the hop distance.","Supporting numerical demonstrations on two examples, a heterogeneous Kuramoto oscillator model and a power system, suggest that the proposed GNN architecture can predict the coarse-grained dynamics under fixed and time-varying graph topologies."],"url":"http://arxiv.org/abs/2405.09324v1","category":"math.NA"}
{"created":"2024-05-15 13:19:43","title":"Transfer Learning in Pre-Trained Large Language Models for Malware Detection Based on System Calls","abstract":"In the current cybersecurity landscape, protecting military devices such as communication and battlefield management systems against sophisticated cyber attacks is crucial. Malware exploits vulnerabilities through stealth methods, often evading traditional detection mechanisms such as software signatures. The application of ML/DL in vulnerability detection has been extensively explored in the literature. However, current ML/DL vulnerability detection methods struggle with understanding the context and intent behind complex attacks. Integrating large language models (LLMs) with system call analysis offers a promising approach to enhance malware detection. This work presents a novel framework leveraging LLMs to classify malware based on system call data. The framework uses transfer learning to adapt pre-trained LLMs for malware detection. By retraining LLMs on a dataset of benign and malicious system calls, the models are refined to detect signs of malware activity. Experiments with a dataset of over 1TB of system calls demonstrate that models with larger context sizes, such as BigBird and Longformer, achieve superior accuracy and F1-Score of approximately 0.86. The results highlight the importance of context size in improving detection rates and underscore the trade-offs between computational complexity and performance. This approach shows significant potential for real-time detection in high-stakes environments, offering a robust solution to evolving cyber threats.","sentences":["In the current cybersecurity landscape, protecting military devices such as communication and battlefield management systems against sophisticated cyber attacks is crucial.","Malware exploits vulnerabilities through stealth methods, often evading traditional detection mechanisms such as software signatures.","The application of ML/DL in vulnerability detection has been extensively explored in the literature.","However, current ML/DL vulnerability detection methods struggle with understanding the context and intent behind complex attacks.","Integrating large language models (LLMs) with system call analysis offers a promising approach to enhance malware detection.","This work presents a novel framework leveraging LLMs to classify malware based on system call data.","The framework uses transfer learning to adapt pre-trained LLMs for malware detection.","By retraining LLMs on a dataset of benign and malicious system calls, the models are refined to detect signs of malware activity.","Experiments with a dataset of over 1TB of system calls demonstrate that models with larger context sizes, such as BigBird and Longformer, achieve superior accuracy and F1-Score of approximately 0.86.","The results highlight the importance of context size in improving detection rates and underscore the trade-offs between computational complexity and performance.","This approach shows significant potential for real-time detection in high-stakes environments, offering a robust solution to evolving cyber threats."],"url":"http://arxiv.org/abs/2405.09318v1","category":"cs.CR"}
{"created":"2024-05-15 12:51:36","title":"Words Blending Boxes. Obfuscating Queries in Information Retrieval using Differential Privacy","abstract":"Ensuring the effectiveness of search queries while protecting user privacy remains an open issue. When an Information Retrieval System (IRS) does not protect the privacy of its users, sensitive information may be disclosed through the queries sent to the system. Recent improvements, especially in NLP, have shown the potential of using Differential Privacy to obfuscate texts while maintaining satisfactory effectiveness. However, such approaches may protect the user's privacy only from a theoretical perspective while, in practice, the real user's information need can still be inferred if perturbed terms are too semantically similar to the original ones. We overcome such limitations by proposing Word Blending Boxes, a novel differentially private mechanism for query obfuscation, which protects the words in the user queries by employing safe boxes. To measure the overall effectiveness of the proposed WBB mechanism, we measure the privacy obtained by the obfuscation process, i.e., the lexical and semantic similarity between original and obfuscated queries. Moreover, we assess the effectiveness of the privatized queries in retrieving relevant documents from the IRS. Our findings indicate that WBB can be integrated effectively into existing IRSs, offering a key to the challenge of protecting user privacy from both a theoretical and a practical point of view.","sentences":["Ensuring the effectiveness of search queries while protecting user privacy remains an open issue.","When an Information Retrieval System (IRS) does not protect the privacy of its users, sensitive information may be disclosed through the queries sent to the system.","Recent improvements, especially in NLP, have shown the potential of using Differential Privacy to obfuscate texts while maintaining satisfactory effectiveness.","However, such approaches may protect the user's privacy only from a theoretical perspective while, in practice, the real user's information need can still be inferred if perturbed terms are too semantically similar to the original ones.","We overcome such limitations by proposing Word Blending Boxes, a novel differentially private mechanism for query obfuscation, which protects the words in the user queries by employing safe boxes.","To measure the overall effectiveness of the proposed WBB mechanism, we measure the privacy obtained by the obfuscation process, i.e., the lexical and semantic similarity between original and obfuscated queries.","Moreover, we assess the effectiveness of the privatized queries in retrieving relevant documents from the IRS.","Our findings indicate that WBB can be integrated effectively into existing IRSs, offering a key to the challenge of protecting user privacy from both a theoretical and a practical point of view."],"url":"http://arxiv.org/abs/2405.09306v1","category":"cs.IR"}
{"created":"2024-05-15 12:49:57","title":"Gradient Boosted Filters For Signal Processing","abstract":"Gradient boosted decision trees have achieved remarkable success in several domains, particularly those that work with static tabular data. However, the application of gradient boosted models to signal processing is underexplored. In this work, we introduce gradient boosted filters for dynamic data, by employing Hammerstein systems in place of decision trees. We discuss the relationship of our approach to the Volterra series, providing the theoretical underpinning for its application. We demonstrate the effective generalizability of our approach with examples.","sentences":["Gradient boosted decision trees have achieved remarkable success in several domains, particularly those that work with static tabular data.","However, the application of gradient boosted models to signal processing is underexplored.","In this work, we introduce gradient boosted filters for dynamic data, by employing Hammerstein systems in place of decision trees.","We discuss the relationship of our approach to the Volterra series, providing the theoretical underpinning for its application.","We demonstrate the effective generalizability of our approach with examples."],"url":"http://arxiv.org/abs/2405.09305v1","category":"cs.LG"}
{"created":"2024-05-15 12:49:36","title":"Kolmogorov complexity as a combinatorial tool","abstract":"Kolmogorov complexity is often used as a convenient language for counting and/or probabilistic existence proofs. However, there are some applications where Kolmogorov complexity is used in a more subtle way. We provide one (somehow) surprising example where an existence of a winning strategy in a natural combinatorial game is proven (and no direct proof is known).","sentences":["Kolmogorov complexity is often used as a convenient language for counting and/or probabilistic existence proofs.","However, there are some applications where Kolmogorov complexity is used in a more subtle way.","We provide one (somehow) surprising example where an existence of a winning strategy in a natural combinatorial game is proven (and no direct proof is known)."],"url":"http://arxiv.org/abs/2405.09304v1","category":"cs.DM"}
{"created":"2024-05-15 12:48:21","title":"Probing the broad line region geometry and size of the gravitationally lensed quasar Q2237+0305 with microlensing time series","abstract":"Lensed quasars are powerful cosmic laboratories; they are used to simultaneously probe various astrophysical phenomena. Microlensing by stars within distant galaxies acts as strong gravitational lenses of multiply imaged quasars, and provides a unique and direct measurement of the lensed quasar internal structure. Microlensing of the continuum emitting region as well as the broad-line region (BLR) is well characterized by four observable indices, $\\mu^{cont}$, $\\mu^{BLR}$, $WCI$ (wing-core), and $RBI$ (red-blue), measured directly from the spectra. During the 2004-2007 monitoring period, image A of the quadruply lensed system Q2237+0305 underwent a strong microlensing amplification, while image D remained unaffected. We used 35 epochs of archival spectrophotometric data of Q2237+0305 obtained with the Very Large Telescope of the European Southern Observatory to develop an independent microlensing method for estimating the geometry and size of the BLR. We measured the index time series for the CIV line and the continuum emission at $1450\\,\\unicode{x212B}$. We built a library of the simulated microlensing index time series that reproduce the observed times series based on three representative BLR models: Keplerian disk (KD), polar wind (PW), and equatorial wind (EW). After sampling the model parameter space, we find that KD is the predominant model, while PW and EW are less likely. We infer that the system is viewed at an intermediate viewing angle $i\\sim 35^\\circ$, and we estimate the most likely CIV BLR half-light radius $r_\\mathrm{1/2}=51\\pm 23$ light days. Our results are in good agreement with previous findings in the literature and extend the validity of the index-based approach to a temporal domain.","sentences":["Lensed quasars are powerful cosmic laboratories; they are used to simultaneously probe various astrophysical phenomena.","Microlensing by stars within distant galaxies acts as strong gravitational lenses of multiply imaged quasars, and provides a unique and direct measurement of the lensed quasar internal structure.","Microlensing of the continuum emitting region as well as the broad-line region (BLR) is well characterized by four observable indices, $\\mu^{cont}$, $\\mu^{BLR}$, $WCI$ (wing-core), and $RBI$ (red-blue), measured directly from the spectra.","During the 2004-2007 monitoring period, image A of the quadruply lensed system Q2237+0305 underwent a strong microlensing amplification, while image D remained unaffected.","We used 35 epochs of archival spectrophotometric data of Q2237+0305 obtained with the Very Large Telescope of the European Southern Observatory to develop an independent microlensing method for estimating the geometry and size of the BLR.","We measured the index time series for the CIV line and the continuum emission at $1450\\,\\unicode{x212B}$. We built a library of the simulated microlensing index time series that reproduce the observed times series based on three representative BLR models: Keplerian disk (KD), polar wind (PW), and equatorial wind (EW).","After sampling the model parameter space, we find that KD is the predominant model, while PW and EW are less likely.","We infer that the system is viewed at an intermediate viewing angle $i\\sim 35^\\circ$, and we estimate the most likely CIV BLR half-light radius $r_\\mathrm{1/2}=51\\pm 23$ light days.","Our results are in good agreement with previous findings in the literature and extend the validity of the index-based approach to a temporal domain."],"url":"http://arxiv.org/abs/2405.09303v1","category":"astro-ph.GA"}
{"created":"2024-05-15 12:27:07","title":"Emergence of high-mass stars in complex fiber networks (EMERGE) II. The need for data combination in ALMA observations","abstract":"ALMA's high-resolution images allow to resolve the filamentary structure of the ISM down to few thousand au at kpc distances. We aim to systematically quantify the impact of the interferometric response and the effects of the short-spacing information during the characterization of the ISM structure using ALMA observations. We create a series of continuum ALMA synthetic observations to test the recovery of the observational properties of dense cores and filaments (i.e. intensity peak, radial profile, and width) at different scales. We compare the results obtained with and without different data combination techniques using different ALMA arrays and SD telescopes in simulated data and real observations. Our analysis illustrates the severity of interferometric filtering effects. ALMA-12m alone observations show significant scale-dependent flux losses systematically corrupting (>30%error) all the physical properties inferred in cores and filaments (i.e. column density, mass, and size) before the maximum recoverable scale of the interferometer. These effects are only partially mitigated by the addition of the ALMA ACA-7m array although degrading the telescope PSF. Our results demonstrate only the addition of the ALMA Total Power information allows to recover the true sky emission down to few times the ALMA beamsize with satisfactory accuracy (<10% error). Additional tests demonstrate the emission recovery at all scales is further improved if the 7mTP data are replaced by maps obtained by a larger SD telescope (e.g., IRAM-30m), even if the latter are noisier than expected. These observational biases particularly affect partially resolved targets, becoming critical especially for studies in nearby regions such as Taurus or Orion. Our results demonstrate the need for the use of data combination techniques to accurately characterize the complex physical structure of the ISM in the ALMA era.","sentences":["ALMA's high-resolution images allow to resolve the filamentary structure of the ISM down to few thousand au at kpc distances.","We aim to systematically quantify the impact of the interferometric response and the effects of the short-spacing information during the characterization of the ISM structure using ALMA observations.","We create a series of continuum ALMA synthetic observations to test the recovery of the observational properties of dense cores and filaments (i.e. intensity peak, radial profile, and width) at different scales.","We compare the results obtained with and without different data combination techniques using different ALMA arrays and SD telescopes in simulated data and real observations.","Our analysis illustrates the severity of interferometric filtering effects.","ALMA-12m alone observations show significant scale-dependent flux losses systematically corrupting (>30%error) all the physical properties inferred in cores and filaments (i.e. column density, mass, and size) before the maximum recoverable scale of the interferometer.","These effects are only partially mitigated by the addition of the ALMA ACA-7m array although degrading the telescope PSF.","Our results demonstrate only the addition of the ALMA Total Power information allows to recover the true sky emission down to few times the ALMA beamsize with satisfactory accuracy (<10% error).","Additional tests demonstrate the emission recovery at all scales is further improved if the 7mTP data are replaced by maps obtained by a larger SD telescope (e.g., IRAM-30m), even if the latter are noisier than expected.","These observational biases particularly affect partially resolved targets, becoming critical especially for studies in nearby regions such as Taurus or Orion.","Our results demonstrate the need for the use of data combination techniques to accurately characterize the complex physical structure of the ISM in the ALMA era."],"url":"http://arxiv.org/abs/2405.09290v1","category":"astro-ph.GA"}
{"created":"2024-05-15 12:09:24","title":"Positional Knowledge is All You Need: Position-induced Transformer (PiT) for Operator Learning","abstract":"Operator learning for Partial Differential Equations (PDEs) is rapidly emerging as a promising approach for surrogate modeling of intricate systems. Transformers with the self-attention mechanism$\\unicode{x2013}$a powerful tool originally designed for natural language processing$\\unicode{x2013}$have recently been adapted for operator learning. However, they confront challenges, including high computational demands and limited interpretability. This raises a critical question: Is there a more efficient attention mechanism for Transformer-based operator learning? This paper proposes the Position-induced Transformer (PiT), built on an innovative position-attention mechanism, which demonstrates significant advantages over the classical self-attention in operator learning. Position-attention draws inspiration from numerical methods for PDEs. Different from self-attention, position-attention is induced by only the spatial interrelations of sampling positions for input functions of the operators, and does not rely on the input function values themselves, thereby greatly boosting efficiency. PiT exhibits superior performance over current state-of-the-art neural operators in a variety of complex operator learning tasks across diverse PDE benchmarks. Additionally, PiT possesses an enhanced discretization convergence feature, compared to the widely-used Fourier neural operator.","sentences":["Operator learning for Partial Differential Equations (PDEs) is rapidly emerging as a promising approach for surrogate modeling of intricate systems.","Transformers with the self-attention mechanism$\\unicode{x2013}$a powerful tool originally designed for natural language processing$\\unicode{x2013}$have recently been adapted for operator learning.","However, they confront challenges, including high computational demands and limited interpretability.","This raises a critical question: Is there a more efficient attention mechanism for Transformer-based operator learning?","This paper proposes the Position-induced Transformer (PiT), built on an innovative position-attention mechanism, which demonstrates significant advantages over the classical self-attention in operator learning.","Position-attention draws inspiration from numerical methods for PDEs.","Different from self-attention, position-attention is induced by only the spatial interrelations of sampling positions for input functions of the operators, and does not rely on the input function values themselves, thereby greatly boosting efficiency.","PiT exhibits superior performance over current state-of-the-art neural operators in a variety of complex operator learning tasks across diverse PDE benchmarks.","Additionally, PiT possesses an enhanced discretization convergence feature, compared to the widely-used Fourier neural operator."],"url":"http://arxiv.org/abs/2405.09285v1","category":"cs.LG"}
{"created":"2024-05-15 12:03:28","title":"Localized Attractor Computations for Infinite-State Games (Full Version)","abstract":"Infinite-state games are a commonly used model for the synthesis of reactive systems with unbounded data domains. Symbolic methods for solving such games need to be able to construct intricate arguments to establish the existence of winning strategies. Often, large problem instances require prohibitively complex arguments. Therefore, techniques that identify smaller and simpler sub-problems and exploit the respective results for the given game-solving task are highly desirable. In this paper, we propose the first such technique for infinite-state games. The main idea is to enhance symbolic game-solving with the results of localized attractor computations performed in sub-games. The crux of our approach lies in identifying useful sub-games by computing permissive winning strategy templates in finite abstractions of the infinite-state game. The experimental evaluation of our method demonstrates that it outperforms existing techniques and is applicable to infinite-state games beyond the state of the art.","sentences":["Infinite-state games are a commonly used model for the synthesis of reactive systems with unbounded data domains.","Symbolic methods for solving such games need to be able to construct intricate arguments to establish the existence of winning strategies.","Often, large problem instances require prohibitively complex arguments.","Therefore, techniques that identify smaller and simpler sub-problems and exploit the respective results for the given game-solving task are highly desirable.","In this paper, we propose the first such technique for infinite-state games.","The main idea is to enhance symbolic game-solving with the results of localized attractor computations performed in sub-games.","The crux of our approach lies in identifying useful sub-games by computing permissive winning strategy templates in finite abstractions of the infinite-state game.","The experimental evaluation of our method demonstrates that it outperforms existing techniques and is applicable to infinite-state games beyond the state of the art."],"url":"http://arxiv.org/abs/2405.09281v1","category":"cs.LO"}
{"created":"2024-05-15 11:58:14","title":"1/3 and other magnetization plateaus in a quasi-one-dimensional Ising magnet $\\mathbf{TbTi_3Bi_4}$ with zigzag spin chain","abstract":"We report the magnetic properties of newly synthesized, single crystals of $\\mathrm{TbTi_3Bi_4}$ whose crystal structure is highlighted by the stacking of terbium-based zigzag chains and titanium-based kagome lattices. This compound demonstrates extreme easy-axis magnetic anisotropy due to the crystalline-electric-field effect which aligns the $\\mathrm{Tb^{3+}}$ moments along the zigzag chain direction. As the result of the strong single-ion anisotropy and multiple magnetic interactions, $\\mathrm{TbTi_3Bi_4}$ behaves as a quasi-one-dimensional Ising magnet with a remarkable antiferromagnetic ordering at $T_\\mathrm{N}$ = 20.4 K. When a magnetic field is applied along the direction of the zigzag chain, multiple meta-magnetic transitions occur between 1/3 and other magnetization plateaus. We have created a field-temperature phase diagram and mapped out the complex magnetic structures resulting from frustration.","sentences":["We report the magnetic properties of newly synthesized, single crystals of $\\mathrm{TbTi_3Bi_4}$ whose crystal structure is highlighted by the stacking of terbium-based zigzag chains and titanium-based kagome lattices.","This compound demonstrates extreme easy-axis magnetic anisotropy due to the crystalline-electric-field effect which aligns the $\\mathrm{Tb^{3+}}$ moments along the zigzag chain direction.","As the result of the strong single-ion anisotropy and multiple magnetic interactions, $\\mathrm{TbTi_3Bi_4}$ behaves as a quasi-one-dimensional Ising magnet with a remarkable antiferromagnetic ordering at $T_\\mathrm{N}$ = 20.4 K. When a magnetic field is applied along the direction of the zigzag chain, multiple meta-magnetic transitions occur between 1/3 and other magnetization plateaus.","We have created a field-temperature phase diagram and mapped out the complex magnetic structures resulting from frustration."],"url":"http://arxiv.org/abs/2405.09280v1","category":"cond-mat.str-el"}
{"created":"2024-05-15 11:42:42","title":"Dynamic Activation Pitfalls in LLaMA Models: An Empirical Study","abstract":"In this work, we systematically investigate the efficacy of dynamic activation mechanisms within the LLaMA family of language models. Despite the potential of dynamic activation methods to reduce computation and increase speed in models using the ReLU activation function, our empirical findings have uncovered several inherent pitfalls in the current dynamic activation schemes. Through extensive experiments across various dynamic activation strategies, we demonstrate that LLaMA models usually underperform when compared to their ReLU counterparts, particularly in scenarios demanding high sparsity ratio. We attribute these deficiencies to a combination of factors: 1) the inherent complexity of dynamically predicting activation heads and neurons; 2) the inadequate sparsity resulting from activation functions; 3) the insufficient preservation of information resulting from KV cache skipping. Our analysis not only sheds light on the limitations of dynamic activation in the context of large-scale LLaMA models but also proposes roadmaps for enhancing the design of future sparsity schemes.","sentences":["In this work, we systematically investigate the efficacy of dynamic activation mechanisms within the LLaMA family of language models.","Despite the potential of dynamic activation methods to reduce computation and increase speed in models using the ReLU activation function, our empirical findings have uncovered several inherent pitfalls in the current dynamic activation schemes.","Through extensive experiments across various dynamic activation strategies, we demonstrate that LLaMA models usually underperform when compared to their ReLU counterparts, particularly in scenarios demanding high sparsity ratio.","We attribute these deficiencies to a combination of factors: 1) the inherent complexity of dynamically predicting activation heads and neurons; 2) the inadequate sparsity resulting from activation functions; 3) the insufficient preservation of information resulting from KV cache skipping.","Our analysis not only sheds light on the limitations of dynamic activation in the context of large-scale LLaMA models but also proposes roadmaps for enhancing the design of future sparsity schemes."],"url":"http://arxiv.org/abs/2405.09274v1","category":"cs.LG"}
{"created":"2024-05-15 11:27:28","title":"A Quantum of QUIC: Dissecting Cryptography with Post-Quantum Insights","abstract":"QUIC is a new network protocol standardized in 2021. It was designed to replace the TCP/TLS stack and is based on UDP. The most current web standard HTTP/3 is specifically designed to use QUIC as transport protocol. QUIC claims to provide secure and fast transport with low-latency connection establishment, flow and congestion control, reliable delivery, and stream multiplexing. To achieve the security goals, QUIC enforces the usage of TLS 1.3. It uses authenticated encryption with additional data (AEAD) algorithms to not only protect the payload but also parts of the header. The handshake relies on asymmetric cryptography, which will be broken with the introduction of powerful quantum computers, making the use of post-quantum cryptography inevitable. This paper presents a detailed evaluation of the impact of cryptography on QUIC performance. The high-performance QUIC implementations LSQUIC, quiche, and MsQuic are evaluated under different aspects. We break symmetric cryptography down to the different security features. To be able to isolate the impact of cryptography, we implemented a NOOP AEAD algorithm which leaves plaintext unaltered. We show that QUIC performance increases by 10 to 20% when removing packet protection. The header protection has negligible impact on performance, especially for AES ciphers. We integrate post-quantum cryptographic algorithms into QUIC, demonstrating its feasibility without major changes to the QUIC libraries by using a TLS library that implements post-quantum algorithms. Kyber, Dilithium, and FALCON are promising candidates for post-quantum secure QUIC, as they have a low impact on the handshake duration. Algorithms like SPHINCS+ with larger key sizes or more complex calculations significantly impact the handshake duration and cause additional issues in our measurements.","sentences":["QUIC is a new network protocol standardized in 2021.","It was designed to replace the TCP/TLS stack and is based on UDP.","The most current web standard HTTP/3 is specifically designed to use QUIC as transport protocol.","QUIC claims to provide secure and fast transport with low-latency connection establishment, flow and congestion control, reliable delivery, and stream multiplexing.","To achieve the security goals, QUIC enforces the usage of TLS 1.3.","It uses authenticated encryption with additional data (AEAD) algorithms to not only protect the payload but also parts of the header.","The handshake relies on asymmetric cryptography, which will be broken with the introduction of powerful quantum computers, making the use of post-quantum cryptography inevitable.","This paper presents a detailed evaluation of the impact of cryptography on QUIC performance.","The high-performance QUIC implementations LSQUIC, quiche, and MsQuic are evaluated under different aspects.","We break symmetric cryptography down to the different security features.","To be able to isolate the impact of cryptography, we implemented a NOOP AEAD algorithm which leaves plaintext unaltered.","We show that QUIC performance increases by 10 to 20% when removing packet protection.","The header protection has negligible impact on performance, especially for AES ciphers.","We integrate post-quantum cryptographic algorithms into QUIC, demonstrating its feasibility without major changes to the QUIC libraries by using a TLS library that implements post-quantum algorithms.","Kyber, Dilithium, and FALCON are promising candidates for post-quantum secure QUIC, as they have a low impact on the handshake duration.","Algorithms like SPHINCS+ with larger key sizes or more complex calculations significantly impact the handshake duration and cause additional issues in our measurements."],"url":"http://arxiv.org/abs/2405.09264v1","category":"cs.NI"}
{"created":"2024-05-15 11:24:07","title":"Exact analysis of the two-dimensional asymmetric simple exclusion process with attachment and detachment of particles","abstract":"The asymmetric simple exclusion process (ASEP) is a paradigmatic driven-diffusive system that describes the asymmetric diffusion of particles with hardcore interactions in a lattice. Although the ASEP is known as an exactly solvable model, most exact results are limited to one-dimensional systems. Recently, the exact steady state in the multi-dimensional ASEP has been proposed [1]. The research focused on the situation where the number of particles is conserved. In this paper, we consider the two-dimensional ASEP with the attachment and detachment of particles (ASEP-LK), where particle number conservation is violated. By employing the result in Ref. [1], we construct the exact steady state of the ASEP-LK and reveal its properties through the exact computation of physical quantities.","sentences":["The asymmetric simple exclusion process (ASEP) is a paradigmatic driven-diffusive system that describes the asymmetric diffusion of particles with hardcore interactions in a lattice.","Although the ASEP is known as an exactly solvable model, most exact results are limited to one-dimensional systems.","Recently, the exact steady state in the multi-dimensional ASEP has been proposed [1].","The research focused on the situation where the number of particles is conserved.","In this paper, we consider the two-dimensional ASEP with the attachment and detachment of particles (ASEP-LK), where particle number conservation is violated.","By employing the result in Ref.","[1], we construct the exact steady state of the ASEP-LK and reveal its properties through the exact computation of physical quantities."],"url":"http://arxiv.org/abs/2405.09261v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-15 11:00:51","title":"Regularity of geodesics in the spaces of convex and plurisubharmonic functions II","abstract":"In this note we continue our investigation of geodesics in the space of convex and plurisubharmonic functions. We show optimal regularity for geodesics joining two smooth strictly convex functions. We also investigate the regularity theory in the $C^{1,\\alpha}$ realm. Finally we discuss the regularity of geodesics joining two toric strictly plurisubharmonic functions.","sentences":["In this note we continue our investigation of geodesics in the space of convex and plurisubharmonic functions.","We show optimal regularity for geodesics joining two smooth strictly convex functions.","We also investigate the regularity theory in the $C^{1,\\alpha}$ realm.","Finally we discuss the regularity of geodesics joining two toric strictly plurisubharmonic functions."],"url":"http://arxiv.org/abs/2405.09248v1","category":"math.CV"}
{"created":"2024-05-15 10:56:06","title":"A Robust UAV-Based Approach for Power-Modulated Jammer Localization Using DoA","abstract":"Unmanned aerial vehicles (UAVs) are well-suited to localize jammers, particularly when jammers are at non-terrestrial locations, where conventional detection methods face challenges. In this work we propose a novel localization method, sample pruning gradient descend (SPGD), which offers robust performance against multiple power-modulated jammers with low computational complexity.","sentences":["Unmanned aerial vehicles (UAVs) are well-suited to localize jammers, particularly when jammers are at non-terrestrial locations, where conventional detection methods face challenges.","In this work we propose a novel localization method, sample pruning gradient descend (SPGD), which offers robust performance against multiple power-modulated jammers with low computational complexity."],"url":"http://arxiv.org/abs/2405.09245v1","category":"eess.SP"}
{"created":"2024-05-15 10:38:04","title":"Electromagnetic response of dense quark matter around color-superconducting phase transition and QCD critical point","abstract":"We explore how the electric conductivity and associated relaxation time are modified near the QCD critical point and the phase transition to a color superconducting phase using the two-flavor Nambu-Jona-Lasinio model with finite current quark masses. We give a comprehensive account of the nature of the soft modes associated with these phase transitions and how they affect the photon self-energy when the system approaches these phase transitions in a combined way with an emphasis on the common and different aspects in the two transitions. The formalism developed for describing the paraconductivity in metallic superconductors is used for the analysis of the photon self-energy. We show that the transport coefficients calculated from the self-energy show anomalous enhancements in both cases with different critical exponents for the individual transitions. We briefly discuss the possibility of detecting the enhancements in the relativistic heavy-ion collisions   in the present and future facilities.","sentences":["We explore how the electric conductivity and associated relaxation time are modified near the QCD critical point and the phase transition to a color superconducting phase using the two-flavor Nambu-Jona-Lasinio model with finite current quark masses.","We give a comprehensive account of the nature of the soft modes associated with these phase transitions and how they affect the photon self-energy when the system approaches these phase transitions in a combined way with an emphasis on the common and different aspects in the two transitions.","The formalism developed for describing the paraconductivity in metallic superconductors is used for the analysis of the photon self-energy.","We show that the transport coefficients calculated from the self-energy show anomalous enhancements in both cases with different critical exponents for the individual transitions.","We briefly discuss the possibility of detecting the enhancements in the relativistic heavy-ion collisions   in the present and future facilities."],"url":"http://arxiv.org/abs/2405.09240v1","category":"hep-ph"}
{"created":"2024-05-15 10:33:54","title":"Fast formation of large ice pebbles after FU Orionis outbursts","abstract":"During their formation, nascent planetary systems are subject to FU Orionis outbursts that heat a substantial part of the disc. This causes water ice in the affected part of the disc to sublimate as the ice line moves outwards to several to tens of astronomical units. In this paper, we investigate how the subsequent cooling of the disc impacts the particle sizes. We calculate the resulting particle sizes in a disc model with cooling times between 100 and 1,000 years, corresponding to typical FU Orionis outbursts. As the disc cools and the ice line retreats inwards, water vapour forms icy mantles on existing silicate particles. This process is called heterogeneous nucleation. The nucleation rate per surface area of silicate substrate strongly depends on the degree of super-saturation of the water vapour in the gas. Fast cooling results in high super-saturation levels, high nucleation rates, and limited condensation growth because the main ice budget is spent in the nucleation. Slow cooling, on the other hand, leads to rare ice nucleation and efficient growth of ice-nucleated particles by subsequent condensation. We demonstrate that close to the quiescent ice line, pebbles with a size of about centimetres to decimetres form by this process. The largest of these are expected to undergo cracking collisions. However, their Stokes numbers still reach values that are high enough to potentially trigger planetesimal formation by the streaming instability if the background turbulence is weak. Stellar outbursts may thus promote planetesimal formation around the water ice line in protoplanetary discs.","sentences":["During their formation, nascent planetary systems are subject to FU Orionis outbursts that heat a substantial part of the disc.","This causes water ice in the affected part of the disc to sublimate as the ice line moves outwards to several to tens of astronomical units.","In this paper, we investigate how the subsequent cooling of the disc impacts the particle sizes.","We calculate the resulting particle sizes in a disc model with cooling times between 100 and 1,000 years, corresponding to typical FU Orionis outbursts.","As the disc cools and the ice line retreats inwards, water vapour forms icy mantles on existing silicate particles.","This process is called heterogeneous nucleation.","The nucleation rate per surface area of silicate substrate strongly depends on the degree of super-saturation of the water vapour in the gas.","Fast cooling results in high super-saturation levels, high nucleation rates, and limited condensation growth because the main ice budget is spent in the nucleation.","Slow cooling, on the other hand, leads to rare ice nucleation and efficient growth of ice-nucleated particles by subsequent condensation.","We demonstrate that close to the quiescent ice line, pebbles with a size of about centimetres to decimetres form by this process.","The largest of these are expected to undergo cracking collisions.","However, their Stokes numbers still reach values that are high enough to potentially trigger planetesimal formation by the streaming instability if the background turbulence is weak.","Stellar outbursts may thus promote planetesimal formation around the water ice line in protoplanetary discs."],"url":"http://arxiv.org/abs/2405.09237v1","category":"astro-ph.EP"}
{"created":"2024-05-15 10:05:01","title":"Exploring Ground States of Fermi-Hubbard Model on Honeycomb Lattices with Counterdiabaticity","abstract":"Exploring the ground state properties of many-body quantum systems conventionally involves adiabatic processes, alongside exact diagonalization, in the context of quantum annealing or adiabatic quantum computation. Shortcuts to adiabaticity by counter-diabatic driving serve to accelerate these processes by suppressing energy excitations. Motivated by this, we develop variational quantum algorithms incorporating the auxiliary counterdiabatic interactions, comparing them with digitized adiabatic algorithms. These algorithms are then implemented on gate-based quantum circuits to explore the ground states of the Fermi-Hubbard model on honeycomb lattices, utilizing systems with up to 26 qubits. The comparison reveals that the counter-diabatic inspired ansatz is superior to traditional Hamiltonian variational ansatz. Furthermore, the number and duration of Trotter steps are analyzed to understand and mitigate errors. Given the model's relevance to materials in condensed matter, our study paves the way for using variational quantum algorithms with counterdiabaticity to explore quantum materials in the noisy intermediate-scale quantum era.","sentences":["Exploring the ground state properties of many-body quantum systems conventionally involves adiabatic processes, alongside exact diagonalization, in the context of quantum annealing or adiabatic quantum computation.","Shortcuts to adiabaticity by counter-diabatic driving serve to accelerate these processes by suppressing energy excitations.","Motivated by this, we develop variational quantum algorithms incorporating the auxiliary counterdiabatic interactions, comparing them with digitized adiabatic algorithms.","These algorithms are then implemented on gate-based quantum circuits to explore the ground states of the Fermi-Hubbard model on honeycomb lattices, utilizing systems with up to 26 qubits.","The comparison reveals that the counter-diabatic inspired ansatz is superior to traditional Hamiltonian variational ansatz.","Furthermore, the number and duration of Trotter steps are analyzed to understand and mitigate errors.","Given the model's relevance to materials in condensed matter, our study paves the way for using variational quantum algorithms with counterdiabaticity to explore quantum materials in the noisy intermediate-scale quantum era."],"url":"http://arxiv.org/abs/2405.09225v1","category":"quant-ph"}
{"created":"2024-05-15 09:53:08","title":"Augmenting Density Matrix Renormalization Group with Clifford Circuits","abstract":"Density Matrix Renormalization Group (DMRG) or Matrix Product States (MPS) are widely acknowledged as highly effective and accurate methods for solving one-dimensional quantum many-body systems. However, the direct application of DMRG to the study two-dimensional systems encounters challenges due to the limited entanglement encoded in the wave-function ansatz. Conversely, Clifford circuits offer a promising avenue for simulating states with substantial entanglement, albeit confined to stabilizer states. In this work, we present the seamless integration of Clifford circuits within the DMRG algorithm, leveraging the advantages of both Clifford circuits and DMRG. This integration leads to a significant enhancement in simulation accuracy with small additional computational cost. Moreover, this framework is useful not only for its current application but also for its potential to be easily adapted to various other numerical approaches","sentences":["Density Matrix Renormalization Group (DMRG) or Matrix Product States (MPS) are widely acknowledged as highly effective and accurate methods for solving one-dimensional quantum many-body systems.","However, the direct application of DMRG to the study two-dimensional systems encounters challenges due to the limited entanglement encoded in the wave-function ansatz.","Conversely, Clifford circuits offer a promising avenue for simulating states with substantial entanglement, albeit confined to stabilizer states.","In this work, we present the seamless integration of Clifford circuits within the DMRG algorithm, leveraging the advantages of both Clifford circuits and DMRG.","This integration leads to a significant enhancement in simulation accuracy with small additional computational cost.","Moreover, this framework is useful not only for its current application but also for its potential to be easily adapted to various other numerical approaches"],"url":"http://arxiv.org/abs/2405.09217v1","category":"cond-mat.str-el"}
{"created":"2024-05-15 09:43:47","title":"Potential of WebAssembly for Embedded Systems","abstract":"Application virtual machines provide strong isolation properties and are established in the context of software portability. Those opportunities make them interesting for scalable and secure IoT deployments. WebAssembly is an application virtual machine with origins in web browsers, that is getting rapidly adopted in other domains. The strong and steadily growing ecosystem makes WebAssembly an interesting candidate for Embedded Systems. This position paper discusses the usage of WebAssembly in Embedded Systems. After introducing the basic concepts of WebAssembly and existing runtime environments, we give an overview of the challenges for the efficient usage of WebAssembly in Embedded Systems. The paper concludes with a real world case study that demonstrates the viability, before giving an outlook on open issues and upcoming work.","sentences":["Application virtual machines provide strong isolation properties and are established in the context of software portability.","Those opportunities make them interesting for scalable and secure IoT deployments.","WebAssembly is an application virtual machine with origins in web browsers, that is getting rapidly adopted in other domains.","The strong and steadily growing ecosystem makes WebAssembly an interesting candidate for Embedded Systems.","This position paper discusses the usage of WebAssembly in Embedded Systems.","After introducing the basic concepts of WebAssembly and existing runtime environments, we give an overview of the challenges for the efficient usage of WebAssembly in Embedded Systems.","The paper concludes with a real world case study that demonstrates the viability, before giving an outlook on open issues and upcoming work."],"url":"http://arxiv.org/abs/2405.09213v1","category":"cs.OS"}
{"created":"2024-05-15 09:33:24","title":"Equivalence of flexible stripline and coaxial cables for superconducting qubit control and readout pulses","abstract":"We report a comparative study on microwave control lines for a transmon qubit using: (i) flexible stripline transmission lines, and (ii) semi-rigid coaxial cables. During each experiment we performed repeated measurements of the energy relaxation and coherence times of a transmon qubit using one of the wiring configurations. Each measurement run spanned 70 h to 250 h of measurement time, and four separate cooldowns were performed so that each configuration could be tested twice. From these datasets we observe that changing the microwave control lines from coaxial cables to flexible stripline transmission lines does not have a measurable effect on coherence compared to thermal cycling the system, or random coherence fluctuations. Our results open up the possibility of large scale integration of qubit control lines with integrated component with planar layouts on flexible substrate.","sentences":["We report a comparative study on microwave control lines for a transmon qubit using: (i) flexible stripline transmission lines, and (ii) semi-rigid coaxial cables.","During each experiment we performed repeated measurements of the energy relaxation and coherence times of a transmon qubit using one of the wiring configurations.","Each measurement run spanned 70 h to 250 h of measurement time, and four separate cooldowns were performed so that each configuration could be tested twice.","From these datasets we observe that changing the microwave control lines from coaxial cables to flexible stripline transmission lines does not have a measurable effect on coherence compared to thermal cycling the system, or random coherence fluctuations.","Our results open up the possibility of large scale integration of qubit control lines with integrated component with planar layouts on flexible substrate."],"url":"http://arxiv.org/abs/2405.09211v1","category":"quant-ph"}
{"created":"2024-05-15 09:29:07","title":"Stochastic Error Bounds in Nonlinear Model Predictive Control with Gaussian Processes via Parameter-Varying Embeddings","abstract":"This study utilized the Gaussian Processes (GPs) regression framework to establish stochastic error bounds between the actual and predicted state evolution of nonlinear systems. These systems are embedded in the linear parameter-varying (LPV) formulation and controlled using model predictive control (MPC). Our main focus is quantifying the uncertainty of the LPVMPC framework's forward error resulting from scheduling signal estimation mismatch. We compared our stochastic approach with a recent deterministic approach and observed improvements in conservatism and robustness. To validate our analysis and method, we solved the regulator problem of an unbalanced disk.","sentences":["This study utilized the Gaussian Processes (GPs) regression framework to establish stochastic error bounds between the actual and predicted state evolution of nonlinear systems.","These systems are embedded in the linear parameter-varying (LPV) formulation and controlled using model predictive control (MPC).","Our main focus is quantifying the uncertainty of the LPVMPC framework's forward error resulting from scheduling signal estimation mismatch.","We compared our stochastic approach with a recent deterministic approach and observed improvements in conservatism and robustness.","To validate our analysis and method, we solved the regulator problem of an unbalanced disk."],"url":"http://arxiv.org/abs/2405.09209v1","category":"math.OC"}
{"created":"2024-05-15 09:25:24","title":"Extended time Petri nets","abstract":"In many complex systems that can be modeled using Petri nets time can be a very important factor which should be taken into account during creation and analysis of the model. Time data can describe starting moments of some actions or their duration before their immediate effects start to influence some other areas of the modeled system. Places in a Petri net often describe static components of the system, but they can also describe states. Such a state can have time restrictions, for example, telling how long it can influence other elements in the model. Time values describing some system may be inconsistent or incomplete, which can cause problems during the creation of the model. In this paper, a new extension of time Petri nets is proposed, which allows the creation of models with different types of time data, which previously were possible to be properly used in separate types of well-known time Petri nets. The proposed new time Petri net solves this problem by integrating different aspects of already existing time Petri nets into one unified net.","sentences":["In many complex systems that can be modeled using Petri nets time can be a very important factor which should be taken into account during creation and analysis of the model.","Time data can describe starting moments of some actions or their duration before their immediate effects start to influence some other areas of the modeled system.","Places in a Petri net often describe static components of the system, but they can also describe states.","Such a state can have time restrictions, for example, telling how long it can influence other elements in the model.","Time values describing some system may be inconsistent or incomplete, which can cause problems during the creation of the model.","In this paper, a new extension of time Petri nets is proposed, which allows the creation of models with different types of time data, which previously were possible to be properly used in separate types of well-known time Petri nets.","The proposed new time Petri net solves this problem by integrating different aspects of already existing time Petri nets into one unified net."],"url":"http://arxiv.org/abs/2405.09208v1","category":"cs.FL"}
{"created":"2024-05-15 09:22:29","title":"Monte Carlo methods on compact complex manifolds using Bergman kernels","abstract":"In this paper, we propose a new randomized method for numerical integration on a compact complex manifold with respect to a continuous volume form. Taking for quadrature nodes a suitable determinantal point process, we build an unbiased Monte Carlo estimator of the integral of any Lipschitz function, and show that the estimator satisfies a central limit theorem, with a faster rate than under independent sampling. In particular, seeing a complex manifold of dimension $d$ as a real manifold of dimension $d_{\\mathbb{R}}=2d$, the mean squared error for $N$ quadrature nodes decays as $N^{-1-2/d_{\\mathbb{R}}}$; this is faster than previous DPP-based quadratures and reaches the optimal worst-case rate investigated by [Bakhvalov 1965] in Euclidean spaces. The determinantal point process we use is characterized by its kernel, which is the Bergman kernel of a holomorphic Hermitian line bundle, and we strongly build upon the work of Berman that led to the central limit theorem in [Berman, 2018].We provide numerical illustrations for the Riemann sphere.","sentences":["In this paper, we propose a new randomized method for numerical integration on a compact complex manifold with respect to a continuous volume form.","Taking for quadrature nodes a suitable determinantal point process, we build an unbiased Monte Carlo estimator of the integral of any Lipschitz function, and show that the estimator satisfies a central limit theorem, with a faster rate than under independent sampling.","In particular, seeing a complex manifold of dimension $d$ as a real manifold of dimension $d_{\\mathbb{R}}=2d$, the mean squared error for $N$ quadrature nodes decays as $N^{-1-2/d_{\\mathbb{R}}}$; this is faster than previous DPP-based quadratures and reaches the optimal worst-case rate investigated by [Bakhvalov 1965] in Euclidean spaces.","The determinantal point process we use is characterized by its kernel, which is the Bergman kernel of a holomorphic Hermitian line bundle, and we strongly build upon the work of Berman that led to the central limit theorem in [Berman, 2018].We provide numerical illustrations for the Riemann sphere."],"url":"http://arxiv.org/abs/2405.09203v1","category":"math.CV"}
{"created":"2024-05-15 09:08:22","title":"Harnessing pattern-by-pattern linear classifiers for prediction with missing data","abstract":"Missing values have been thoroughly analyzed in the context of linear models, where the final aim is to build coefficient estimates. However, estimating coefficients does not directly solve the problem of prediction with missing entries: a manner to address empty components must be designed. Major approaches to deal with prediction with missing values are empirically driven and can be decomposed into two families: imputation (filling in empty fields) and pattern-by-pattern prediction, where a predictor is built on each missing pattern. Unfortunately, most simple imputation techniques used in practice (as constant imputation) are not consistent when combined with linear models. In this paper, we focus on the more flexible pattern-by-pattern approaches and study their predictive performances on Missing Completely At Random (MCAR) data. We first show that a pattern-by-pattern logistic regression model is intrinsically ill-defined, implying that even classical logistic regression is impossible to apply to missing data. We then analyze the perceptron model and show how the linear separability property extends to partially-observed inputs. Finally, we use the Linear Discriminant Analysis to prove that pattern-by-pattern LDA is consistent in a high-dimensional regime. We refine our analysis to more complex MNAR data.","sentences":["Missing values have been thoroughly analyzed in the context of linear models, where the final aim is to build coefficient estimates.","However, estimating coefficients does not directly solve the problem of prediction with missing entries: a manner to address empty components must be designed.","Major approaches to deal with prediction with missing values are empirically driven and can be decomposed into two families: imputation (filling in empty fields) and pattern-by-pattern prediction, where a predictor is built on each missing pattern.","Unfortunately, most simple imputation techniques used in practice (as constant imputation) are not consistent when combined with linear models.","In this paper, we focus on the more flexible pattern-by-pattern approaches and study their predictive performances on Missing Completely At Random (MCAR) data.","We first show that a pattern-by-pattern logistic regression model is intrinsically ill-defined, implying that even classical logistic regression is impossible to apply to missing data.","We then analyze the perceptron model and show how the linear separability property extends to partially-observed inputs.","Finally, we use the Linear Discriminant Analysis to prove that pattern-by-pattern LDA is consistent in a high-dimensional regime.","We refine our analysis to more complex MNAR data."],"url":"http://arxiv.org/abs/2405.09196v1","category":"math.ST"}
{"created":"2024-05-15 09:04:18","title":"Propagation of chaos for moderately interacting particle systems related to singular kinetic Mckean-Vlasov SDEs","abstract":"We study the propagation of chaos in a class of moderately interacting particle systems for the approximation of singular kinetic McKean-Vlasov SDEs driven by alpha-stable processes. Diffusion parts include Brownian (alpha=2) and pure-jump (1<\\alpha<2) perturbations and interaction kernels are considered in a non-smooth anisotropic Besov space. Using Duhamel formula, sharp density estimates (recently issued in Hao, Rockner and Zhang 2023), and suitable martingale functional inequalities, we obtain direct estimates on the convergence rate between the empirical measure of the particle systems toward the McKean-Vlasov distribution. These estimates further lead to quantitative propagation of chaos results in the weak and strong sense.","sentences":["We study the propagation of chaos in a class of moderately interacting particle systems for the approximation of singular kinetic McKean-Vlasov SDEs driven by alpha-stable processes.","Diffusion parts include Brownian (alpha=2) and pure-jump (1<\\alpha<2) perturbations and interaction kernels are considered in a non-smooth anisotropic Besov space.","Using Duhamel formula, sharp density estimates (recently issued in Hao, Rockner and Zhang 2023), and suitable martingale functional inequalities, we obtain direct estimates on the convergence rate between the empirical measure of the particle systems toward the McKean-Vlasov distribution.","These estimates further lead to quantitative propagation of chaos results in the weak and strong sense."],"url":"http://arxiv.org/abs/2405.09195v1","category":"math.AP"}
{"created":"2024-05-15 08:53:33","title":"Evolution of Measures in Nonsmooth Dynamical Systems: Formalisms and Computation","abstract":"This article develops mathematical formalisms and provides numerical methods for studying the evolution of measures in nonsmooth dynamical systems using the continuity equation. The nonsmooth dynamical system is described by an evolution variational inequality and we derive the continuity equation associated with this system class using three different formalisms. The first formalism consists of using the {superposition principle} to describe the continuity equation for a measure that disintegrates into a probability measure supported on the set of vector fields and another measure representing the distribution of system trajectories at each time instant. The second formalism is based on the regularization of the nonsmooth vector field and describing the measure as the limit of a sequence of measures associated with the regularization parameter. In doing so, we obtain quantitative bounds on the Wasserstein metric between measure solutions of the regularized vector field and the limiting measure associated with the nonsmooth vector field. The third formalism uses a time-stepping algorithm to model a time-discretized evolution of the measures and show that the absolutely continuous trajectories associated with the continuity equation are recovered in the limit as the sampling time goes to zero. We also validate each formalism with numerical examples. For the first formalism, we use polynomial optimization techniques and the moment-SOS hierarchy to obtain approximate moments of the measures. For the second formalism, we illustrate the bounds on the Wasserstein metric for an academic example for which the closed-form expression of the Wasserstein metric can be calculated. For the third formalism, we illustrate the time-stepping based algorithm for measure evolution on an example that shows the effect of the concentration of measures.","sentences":["This article develops mathematical formalisms and provides numerical methods for studying the evolution of measures in nonsmooth dynamical systems using the continuity equation.","The nonsmooth dynamical system is described by an evolution variational inequality and we derive the continuity equation associated with this system class using three different formalisms.","The first formalism consists of using the {superposition principle} to describe the continuity equation for a measure that disintegrates into a probability measure supported on the set of vector fields and another measure representing the distribution of system trajectories at each time instant.","The second formalism is based on the regularization of the nonsmooth vector field and describing the measure as the limit of a sequence of measures associated with the regularization parameter.","In doing so, we obtain quantitative bounds on the Wasserstein metric between measure solutions of the regularized vector field and the limiting measure associated with the nonsmooth vector field.","The third formalism uses a time-stepping algorithm to model a time-discretized evolution of the measures and show that the absolutely continuous trajectories associated with the continuity equation are recovered in the limit as the sampling time goes to zero.","We also validate each formalism with numerical examples.","For the first formalism, we use polynomial optimization techniques and the moment-SOS hierarchy to obtain approximate moments of the measures.","For the second formalism, we illustrate the bounds on the Wasserstein metric for an academic example for which the closed-form expression of the Wasserstein metric can be calculated.","For the third formalism, we illustrate the time-stepping based algorithm for measure evolution on an example that shows the effect of the concentration of measures."],"url":"http://arxiv.org/abs/2405.09189v1","category":"math.OC"}
{"created":"2024-05-15 08:48:24","title":"Spin Symmetry in Thermally-Assisted-Occupation Density Functional Theory","abstract":"For electronic systems with multi-reference (MR) character, Kohn-Sham density functional theory (KS-DFT) with the conventional exchange-correlation (xc) energy functionals can lead to incorrect spin densities and related properties. For example, for H2 dissociation, the spin-restricted and spin-unrestricted solutions obtained with the same xc energy functional in KS-DFT can be distinctly different, yielding the unphysical spin-symmetry breaking effects in the spin-unrestricted solutions. Recently, thermally-assisted-occupation density functional theory (TAO-DFT) has been shown to resolve the aforementioned spin-symmetry breaking, when the fictitious temperature is properly chosen. In this work, a response theory based on TAO-DFT is developed to demonstrate that TAO-DFT with a sufficiently large fictitious temperature can always resolve the unphysical spin-symmetry breaking in MR systems. To further support this, TAO-DFT calculations with various fictitious temperatures are performed for the dissociation of H2, N2, He2, and Ne2 as well as the twisted ethylene.","sentences":["For electronic systems with multi-reference (MR) character, Kohn-Sham density functional theory (KS-DFT) with the conventional exchange-correlation (xc) energy functionals can lead to incorrect spin densities and related properties.","For example, for H2 dissociation, the spin-restricted and spin-unrestricted solutions obtained with the same xc energy functional in KS-DFT can be distinctly different, yielding the unphysical spin-symmetry breaking effects in the spin-unrestricted solutions.","Recently, thermally-assisted-occupation density functional theory (TAO-DFT) has been shown to resolve the aforementioned spin-symmetry breaking, when the fictitious temperature is properly chosen.","In this work, a response theory based on TAO-DFT is developed to demonstrate that TAO-DFT with a sufficiently large fictitious temperature can always resolve the unphysical spin-symmetry breaking in MR systems.","To further support this, TAO-DFT calculations with various fictitious temperatures are performed for the dissociation of H2, N2, He2, and Ne2 as well as the twisted ethylene."],"url":"http://arxiv.org/abs/2405.09187v1","category":"physics.chem-ph"}
{"created":"2024-05-15 08:40:44","title":"A Formal Approach for Tuning Stochastic Oscillators","abstract":"Periodic recurrence is a prominent behavioural of many biological phenomena, including cell cycle and circadian rhythms. Although deterministic models are commonly used to represent the dynamics of periodic phenomena, it is known that they are little appropriate in the case of systems in which stochastic noise induced by small population numbers is actually responsible for periodicity. Within the stochastic modelling settings automata-based model checking approaches have proven an effective means for the analysis of oscillatory dynamics, the main idea being that of coupling a period detector automaton with a continuous-time Markov chain model of an alleged oscillator. In this paper we address a complementary aspect, i.e. that of assessing the dependency of oscillation related measure (period and amplitude) against the parameters of a stochastic oscillator. To this aim we introduce a framework which, by combining an Approximate Bayesian Computation scheme with a hybrid automata capable of quantifying how distant an instance of a stochastic oscillator is from matching a desired (average) period, leads us to identify regions of the parameter space in which oscillation with given period are highly likely. The method is demonstrated through a couple of case studies, including a model of the popular Repressilator circuit.","sentences":["Periodic recurrence is a prominent behavioural of many biological phenomena, including cell cycle and circadian rhythms.","Although deterministic models are commonly used to represent the dynamics of periodic phenomena, it is known that they are little appropriate in the case of systems in which stochastic noise induced by small population numbers is actually responsible for periodicity.","Within the stochastic modelling settings automata-based model checking approaches have proven an effective means for the analysis of oscillatory dynamics, the main idea being that of coupling a period detector automaton with a continuous-time Markov chain model of an alleged oscillator.","In this paper we address a complementary aspect, i.e. that of assessing the dependency of oscillation related measure (period and amplitude) against the parameters of a stochastic oscillator.","To this aim we introduce a framework which, by combining an Approximate Bayesian Computation scheme with a hybrid automata capable of quantifying how distant an instance of a stochastic oscillator is from matching a desired (average) period, leads us to identify regions of the parameter space in which oscillation with given period are highly likely.","The method is demonstrated through a couple of case studies, including a model of the popular Repressilator circuit."],"url":"http://arxiv.org/abs/2405.09183v1","category":"cs.FL"}
{"created":"2024-05-15 07:50:51","title":"DVS-RG: Differential Variable Speed Limits Control using Deep Reinforcement Learning with Graph State Representation","abstract":"Variable speed limit (VSL) control is an established yet challenging problem to improve freeway traffic mobility and alleviate bottlenecks by customizing speed limits at proper locations based on traffic conditions. Recent advances in deep reinforcement learning (DRL) have shown promising results in solving VSL control problems by interacting with sophisticated environments. However, the modeling of these methods ignores the inherent graph structure of the traffic state which can be a key factor for more efficient VSL control. Graph structure can not only capture the static spatial feature but also the dynamic temporal features of traffic. Therefore, we propose the DVS-RG: DRL-based differential variable speed limit controller with graph state representation. DVS-RG provides distinct speed limits per lane in different locations dynamically. The road network topology and traffic information(e.g., occupancy, speed) are integrated as the state space of DVS-RG so that the spatial features can be learned. The normalization reward which combines efficiency and safety is used to train the VSL controller to avoid excessive inefficiencies or low safety. The results obtained from the simulation study on SUMO show that DRL-RG achieves higher traffic efficiency (the average waiting time reduced to 68.44\\%) and improves the safety measures (the number of potential collision reduced by 15.93\\% ) compared to state-of-the-art DRL methods.","sentences":["Variable speed limit (VSL) control is an established yet challenging problem to improve freeway traffic mobility and alleviate bottlenecks by customizing speed limits at proper locations based on traffic conditions.","Recent advances in deep reinforcement learning (DRL) have shown promising results in solving VSL control problems by interacting with sophisticated environments.","However, the modeling of these methods ignores the inherent graph structure of the traffic state which can be a key factor for more efficient VSL control.","Graph structure can not only capture the static spatial feature but also the dynamic temporal features of traffic.","Therefore, we propose the DVS-RG: DRL-based differential variable speed limit controller with graph state representation.","DVS-RG provides distinct speed limits per lane in different locations dynamically.","The road network topology and traffic information(e.g., occupancy, speed) are integrated as the state space of DVS-RG so that the spatial features can be learned.","The normalization reward which combines efficiency and safety is used to train the VSL controller to avoid excessive inefficiencies or low safety.","The results obtained from the simulation study on SUMO show that DRL-RG achieves higher traffic efficiency (the average waiting time reduced to 68.44\\%) and improves the safety measures (the number of potential collision reduced by 15.93\\% ) compared to state-of-the-art DRL methods."],"url":"http://arxiv.org/abs/2405.09163v1","category":"eess.SY"}
{"created":"2024-05-15 07:45:07","title":"An indirect geometry crystal time-of-flight spectrometer for FRM II","abstract":"We present a concept for an indirect geometry crystal time-of-flight spectrometer, which we propose for the FRM-II reactor in Garching. Recently, crystal analyzer spectrometers at modern spallation sources have been proposed and are under construction. The secondary spectrometers of these instruments are evolutions of the flat cone multi-analyzer for three-axis spectrometers (TAS). The instruments will provide exceptional reciprocal space coverage and intensity to map out the excitation landscape in novel materials. We will discuss the benefits of such a time-of-flight primary spectrometer with a large crystal analyzer spectrometer at a continuous neutron source. The dynamical range can be very flexibly matched to the requirements of the experiment without sacrificing the neutron intensity. At the same time, the chopper system allows a quasi-continuous variation of the initial energy resolution. The neutron optics of the proposed instrument employs the novel nested mirror optics, which images neutrons from a bright virtual source onto the sample. The spot size of less than 1 cm x 1 cm at the virtual source allows the realization of very short neutron pulses by the choppers, while the small and well-defined spot size at the sample position provides an excellent energy resolution of the secondary spectrometer thanks to the prismatic focusing of the analyzer.","sentences":["We present a concept for an indirect geometry crystal time-of-flight spectrometer, which we propose for the FRM-II reactor in Garching.","Recently, crystal analyzer spectrometers at modern spallation sources have been proposed and are under construction.","The secondary spectrometers of these instruments are evolutions of the flat cone multi-analyzer for three-axis spectrometers (TAS).","The instruments will provide exceptional reciprocal space coverage and intensity to map out the excitation landscape in novel materials.","We will discuss the benefits of such a time-of-flight primary spectrometer with a large crystal analyzer spectrometer at a continuous neutron source.","The dynamical range can be very flexibly matched to the requirements of the experiment without sacrificing the neutron intensity.","At the same time, the chopper system allows a quasi-continuous variation of the initial energy resolution.","The neutron optics of the proposed instrument employs the novel nested mirror optics, which images neutrons from a bright virtual source onto the sample.","The spot size of less than 1 cm x 1 cm at the virtual source allows the realization of very short neutron pulses by the choppers, while the small and well-defined spot size at the sample position provides an excellent energy resolution of the secondary spectrometer thanks to the prismatic focusing of the analyzer."],"url":"http://arxiv.org/abs/2405.09159v1","category":"physics.ins-det"}
{"created":"2024-05-15 07:37:38","title":"Two models for the orbital modulation of $\u03b3$-rays in Cyg X-3","abstract":"We model the currently available $\\gamma$-ray data from the Fermi Large Area Telescope on Cyg X-3. Thanks to its very strong $\\gamma$-ray activity during 2018--2021, the data quality has significantly improved. We study the strong orbital modulation of the $\\gamma$-rays observed during at high $\\gamma$-ray fluxes. The modulation, as found earlier, is well modeled by anisotropic Compton scattering of the donor blackbody emission by relativistic electrons in a jet strongly misaligned with respect to the orbital axis. We confirm that this model fits well both the average $\\gamma$-ray modulation light curve and the spectrum. However, we find that if the jet is aligned with the spin axis of a rotating black hole, it would undergo geodetic precession with the period of $\\sim$50 years. However, its presence is ruled out by both the $\\gamma$-ray and radio data. Therefore, we consider an alternative model in which the average jet direction jet is aligned, but it is bent to outside the orbit owing to the thrust of the donor stellar wind, and thus precesses at the orbital period. The $\\gamma$-ray modulation appears then owing to the variable Doppler boosting of synchrotron self-Compton jet emission. The model also fits well the data. However, the fitted bending angle is much larger than the theoretical one based on the binary and wind parameters as currently known. Thus, both models disagree with important aspects of our current theoretical understanding of the system. We discuss possible ways to find the correct model.","sentences":["We model the currently available $\\gamma$-ray data from the Fermi Large Area Telescope on Cyg X-3.","Thanks to its very strong $\\gamma$-ray activity during 2018--2021, the data quality has significantly improved.","We study the strong orbital modulation of the $\\gamma$-rays observed during at high $\\gamma$-ray fluxes.","The modulation, as found earlier, is well modeled by anisotropic Compton scattering of the donor blackbody emission by relativistic electrons in a jet strongly misaligned with respect to the orbital axis.","We confirm that this model fits well both the average $\\gamma$-ray modulation light curve and the spectrum.","However, we find that if the jet is aligned with the spin axis of a rotating black hole, it would undergo geodetic precession with the period of $\\sim$50 years.","However, its presence is ruled out by both the $\\gamma$-ray and radio data.","Therefore, we consider an alternative model in which the average jet direction jet is aligned, but it is bent to outside the orbit owing to the thrust of the donor stellar wind, and thus precesses at the orbital period.","The $\\gamma$-ray modulation appears then owing to the variable Doppler boosting of synchrotron self-Compton jet emission.","The model also fits well the data.","However, the fitted bending angle is much larger than the theoretical one based on the binary and wind parameters as currently known.","Thus, both models disagree with important aspects of our current theoretical understanding of the system.","We discuss possible ways to find the correct model."],"url":"http://arxiv.org/abs/2405.09154v1","category":"astro-ph.HE"}
{"created":"2024-05-15 07:20:27","title":"A Hierarchically Feature Reconstructed Autoencoder for Unsupervised Anomaly Detection","abstract":"Anomaly detection and localization without any manual annotations and prior knowledge is a challenging task under the setting of unsupervised learning. The existing works achieve excellent performance in the anomaly detection, but with complex networks or cumbersome pipelines. To address this issue, this paper explores a simple but effective architecture in the anomaly detection. It consists of a well pre-trained encoder to extract hierarchical feature representations and a decoder to reconstruct these intermediate features from the encoder. In particular, it does not require any data augmentations and anomalous images for training. The anomalies can be detected when the decoder fails to reconstruct features well, and then errors of hierarchical feature reconstruction are aggregated into an anomaly map to achieve anomaly localization. The difference comparison between those features of encoder and decode lead to more accurate and robust localization results than the comparison in single feature or pixel-by-pixel comparison in the conventional works. Experiment results show that the proposed method outperforms the state-of-the-art methods on MNIST, Fashion-MNIST, CIFAR-10, and MVTec Anomaly Detection datasets on both anomaly detection and localization.","sentences":["Anomaly detection and localization without any manual annotations and prior knowledge is a challenging task under the setting of unsupervised learning.","The existing works achieve excellent performance in the anomaly detection, but with complex networks or cumbersome pipelines.","To address this issue, this paper explores a simple but effective architecture in the anomaly detection.","It consists of a well pre-trained encoder to extract hierarchical feature representations and a decoder to reconstruct these intermediate features from the encoder.","In particular, it does not require any data augmentations and anomalous images for training.","The anomalies can be detected when the decoder fails to reconstruct features well, and then errors of hierarchical feature reconstruction are aggregated into an anomaly map to achieve anomaly localization.","The difference comparison between those features of encoder and decode lead to more accurate and robust localization results than the comparison in single feature or pixel-by-pixel comparison in the conventional works.","Experiment results show that the proposed method outperforms the state-of-the-art methods on MNIST, Fashion-MNIST, CIFAR-10, and MVTec Anomaly Detection datasets on both anomaly detection and localization."],"url":"http://arxiv.org/abs/2405.09148v1","category":"cs.CV"}
{"created":"2024-05-15 07:13:24","title":"Speaker Embeddings With Weakly Supervised Voice Activity Detection For Efficient Speaker Diarization","abstract":"Current speaker diarization systems rely on an external voice activity detection model prior to speaker embedding extraction on the detected speech segments. In this paper, we establish that the attention system of a speaker embedding extractor acts as a weakly supervised internal VAD model and performs equally or better than comparable supervised VAD systems. Subsequently, speaker diarization can be performed efficiently by extracting the VAD logits and corresponding speaker embedding simultaneously, alleviating the need and computational overhead of an external VAD model. We provide an extensive analysis of the behavior of the frame-level attention system in current speaker verification models and propose a novel speaker diarization pipeline using ECAPA2 speaker embeddings for both VAD and embedding extraction. The proposed strategy gains state-of-the-art performance on the AMI, VoxConverse and DIHARD III diarization benchmarks.","sentences":["Current speaker diarization systems rely on an external voice activity detection model prior to speaker embedding extraction on the detected speech segments.","In this paper, we establish that the attention system of a speaker embedding extractor acts as a weakly supervised internal VAD model and performs equally or better than comparable supervised VAD systems.","Subsequently, speaker diarization can be performed efficiently by extracting the VAD logits and corresponding speaker embedding simultaneously, alleviating the need and computational overhead of an external VAD model.","We provide an extensive analysis of the behavior of the frame-level attention system in current speaker verification models and propose a novel speaker diarization pipeline using ECAPA2 speaker embeddings for both VAD and embedding extraction.","The proposed strategy gains state-of-the-art performance on the AMI, VoxConverse and DIHARD III diarization benchmarks."],"url":"http://arxiv.org/abs/2405.09142v1","category":"eess.AS"}
{"created":"2024-05-15 07:10:35","title":"On Convergence of the Iteratively Preconditioned Gradient-Descent (IPG) Observer","abstract":"This paper considers the observer design problem for discrete-time nonlinear dynamical systems with sampled measurement data. Earlier, the recently proposed Iteratively Preconditioned Gradient-Descent (IPG) observer, a Newton-type observer, has been empirically shown to have improved robustness against measurement noise than the prominent nonlinear observers, a property that other Newton-type observers lack. However, no theoretical guarantees on the convergence of the IPG observer were provided. This paper presents a rigorous convergence analysis of the IPG observer for a class of nonlinear systems in deterministic settings, proving its local linear convergence to the actual trajectory. Our assumptions are standard in the existing literature of Newton-type observers, and the analysis further confirms the relation of the IPG observer with the Newton observer, which was only hypothesized earlier.","sentences":["This paper considers the observer design problem for discrete-time nonlinear dynamical systems with sampled measurement data.","Earlier, the recently proposed Iteratively Preconditioned Gradient-Descent (IPG) observer, a Newton-type observer, has been empirically shown to have improved robustness against measurement noise than the prominent nonlinear observers, a property that other Newton-type observers lack.","However, no theoretical guarantees on the convergence of the IPG observer were provided.","This paper presents a rigorous convergence analysis of the IPG observer for a class of nonlinear systems in deterministic settings, proving its local linear convergence to the actual trajectory.","Our assumptions are standard in the existing literature of Newton-type observers, and the analysis further confirms the relation of the IPG observer with the Newton observer, which was only hypothesized earlier."],"url":"http://arxiv.org/abs/2405.09137v1","category":"math.OC"}
{"created":"2024-05-15 06:57:42","title":"Contractibility of the Rips complexes of Integer lattices via local domination","abstract":"We prove that for each positive integer $n$, the Rips complexes of the $n$-dimensional integer lattice in the $d_1$ metric (i.e., the Manhattan metric, also called the natural word metric in the Cayley graph) are contractible at scales above $n^2(2n-1)^2$, with the bounds arising from the Jung's constants. We introduce a new concept of locally dominated vertices in a simplicial complex, upon which our proof strategy is based. This allows us to deduce the contractibility of the Rips complexes from a local geometric condition called local crushing. In the case of the integer lattices in dimension $n$ and a fixed scale $r$, this condition entails the comparison of finitely many distances to conclude that the corresponding Rips complex is contractible. In particular, we are able to verify that for $n=1,2,3$, the Rips complex of the $n$-dimensional integer lattice at scale greater or equal to $n$ is contractible. We conjecture that the same proof strategy can be used to extend this result to all dimensions $n$","sentences":["We prove that for each positive integer $n$, the Rips complexes of the $n$-dimensional integer lattice in the $d_1$ metric (i.e., the Manhattan metric, also called the natural word metric in the Cayley graph) are contractible at scales above $n^2(2n-1)^2$, with the bounds arising from the Jung's constants.","We introduce a new concept of locally dominated vertices in a simplicial complex, upon which our proof strategy is based.","This allows us to deduce the contractibility of the Rips complexes from a local geometric condition called local crushing.","In the case of the integer lattices in dimension $n$ and a fixed scale $r$, this condition entails the comparison of finitely many distances to conclude that the corresponding Rips complex is contractible.","In particular, we are able to verify that for $n=1,2,3$, the Rips complex of the $n$-dimensional integer lattice at scale greater or equal to $n$ is contractible.","We conjecture that the same proof strategy can be used to extend this result to all dimensions $n$"],"url":"http://arxiv.org/abs/2405.09134v1","category":"math.MG"}
{"created":"2024-05-15 06:44:01","title":"Quantum-Amplified Simultaneous Quantum-Classical Communications","abstract":"Classical free-space optical (FSO) communication promises massive data throughput rates relative to traditional wireless technologies - an attractive outcome now being pursued in the context of satellite-ground, inter-satellite and deep-space communications. The question we investigate here is: how can we minimally alter classical FSO systems, both in infrastructure and in energy input, to provide some element of quantum communication coexisting with classical communications? To address this question, we explore additional Gaussian displacements to classical FSO encoding on the satellite, determining the minimum signal requirements that will meet given specifications on the combined classical and quantum communications throughput. We then investigate whether enhanced quantum-based amplifiers embedded in receivers, which have proven advantageous in standalone quantum communication, can enhance our combined classical-quantum communication throughput. We show how this is indeed the case, but only at the cost of some additional receiver complexity, relative to standalone quantum communications. This additional complexity takes the form of an additional beamsplitter and two heterodyne detectors at the receiver. Our results illustrate a viable pathway to realising quantum communication from classical FSO systems with minimal design changes.","sentences":["Classical free-space optical (FSO) communication promises massive data throughput rates relative to traditional wireless technologies - an attractive outcome now being pursued in the context of satellite-ground, inter-satellite and deep-space communications.","The question we investigate here is: how can we minimally alter classical FSO systems, both in infrastructure and in energy input, to provide some element of quantum communication coexisting with classical communications?","To address this question, we explore additional Gaussian displacements to classical FSO encoding on the satellite, determining the minimum signal requirements that will meet given specifications on the combined classical and quantum communications throughput.","We then investigate whether enhanced quantum-based amplifiers embedded in receivers, which have proven advantageous in standalone quantum communication, can enhance our combined classical-quantum communication throughput.","We show how this is indeed the case, but only at the cost of some additional receiver complexity, relative to standalone quantum communications.","This additional complexity takes the form of an additional beamsplitter and two heterodyne detectors at the receiver.","Our results illustrate a viable pathway to realising quantum communication from classical FSO systems with minimal design changes."],"url":"http://arxiv.org/abs/2405.09127v1","category":"quant-ph"}
{"created":"2024-05-15 06:42:11","title":"Floquet engineering of quantum thermal machines: A gradient-based procedure to optimize their performance","abstract":"A procedure to find optimal regimes for quantum thermal engines (QTMs) is described and demonstrated. The QTMs are modelled as the periodically-driven non-equilibrium steady states of open quantum systems, whose dynamics is approximated in this work with Markovian master equations. The action of the external agent, and the couplings to the heat reservoirs can be modulated with control functions, and it is the time-dependent shape of these control functions the object of optimisation. Those functions can be freely parameterised, which permits to constrain the solutions according to experimental or physical requirements.","sentences":["A procedure to find optimal regimes for quantum thermal engines (QTMs) is described and demonstrated.","The QTMs are modelled as the periodically-driven non-equilibrium steady states of open quantum systems, whose dynamics is approximated in this work with Markovian master equations.","The action of the external agent, and the couplings to the heat reservoirs can be modulated with control functions, and it is the time-dependent shape of these control functions the object of optimisation.","Those functions can be freely parameterised, which permits to constrain the solutions according to experimental or physical requirements."],"url":"http://arxiv.org/abs/2405.09126v1","category":"quant-ph"}
{"created":"2024-05-15 06:22:21","title":"Complex-valued 3D atomic spectroscopy with Gaussian-assisted inline holography","abstract":"When a laser-cooled atomic sample is optically excited, the envelope of coherent forward scattering can often be decomposed into a few complex Gaussian profiles. The convenience of Gaussian propagation helps addressing key challenges in digital holography. In this work, we theoretically develop and experimentally demonstrate a Gaussian-decomposition-assisted approach to inline holography, for single-shot, simultaneous measurements of absorption and phase shift of small atomic samples sparsely distributed in 3D. Experimentally, we image a sparse lattice of $^{87}$Rb samples on the D2 line, to resolve their axial positions with micrometer precision, and to retrieve their complex-valued spectroscopic images. With the phase-angle readouts that are highly insensitive to atom-number and interaction-strength uncertainties, we achieve hundred-kHz-level single-shot-resolution to the transition frequency with merely hundreds of atoms. We further demonstrate 3D sensing of local light shift with micrometer spatial resolution.","sentences":["When a laser-cooled atomic sample is optically excited, the envelope of coherent forward scattering can often be decomposed into a few complex Gaussian profiles.","The convenience of Gaussian propagation helps addressing key challenges in digital holography.","In this work, we theoretically develop and experimentally demonstrate a Gaussian-decomposition-assisted approach to inline holography, for single-shot, simultaneous measurements of absorption and phase shift of small atomic samples sparsely distributed in 3D. Experimentally, we image a sparse lattice of $^{87}$Rb samples on the D2 line, to resolve their axial positions with micrometer precision, and to retrieve their complex-valued spectroscopic images.","With the phase-angle readouts that are highly insensitive to atom-number and interaction-strength uncertainties, we achieve hundred-kHz-level single-shot-resolution to the transition frequency with merely hundreds of atoms.","We further demonstrate 3D sensing of local light shift with micrometer spatial resolution."],"url":"http://arxiv.org/abs/2405.09117v1","category":"physics.atom-ph"}
{"created":"2024-05-15 05:51:47","title":"Bismut torsion parallel metrics with constant holomorphic sectional curvature","abstract":"An old conjecture in non-K\\\"ahler geometry states that, if a compact Hermitian manifold has constant holomorphic sectional curvature, then the metric must be K\\\"ahler (when the constant is non-zero) or Chern flat (when the constant is zero). It is known to be true in complex dimension $2$ by the work of Balas and Gauduchon in 1985 (when the constant is negative or zero) and Apostolov, Davidov and Muskarov in 1996 (when the constant is positive). In dimension $3$ or higher, the conjecture is only known in some special cases, such as the locally conformally K\\\"ahler case (when the constant is negative or zero) by the work of Chen, Chen and Nie, or for complex nilmanifolds with nilpotent $J$ by the work of Li and the second named author.   In this note, we confirm the above conjecture for all non-balanced Bismut torsion parallel (BTP) manifolds. Here the BTP condition means that the Bismut connection has parallel torsion. In particular, the conjecture is valid for all Vaisman manifolds.","sentences":["An old conjecture in non-K\\\"ahler geometry states that, if a compact Hermitian manifold has constant holomorphic sectional curvature, then the metric must be K\\\"ahler (when the constant is non-zero) or Chern flat (when the constant is zero).","It is known to be true in complex dimension $2$ by the work of Balas and Gauduchon in 1985 (when the constant is negative or zero) and Apostolov, Davidov and Muskarov in 1996 (when the constant is positive).","In dimension $3$ or higher, the conjecture is only known in some special cases, such as the locally conformally K\\\"ahler case (when the constant is negative or zero) by the work of Chen, Chen and Nie, or for complex nilmanifolds with nilpotent $J$ by the work of Li and the second named author.   ","In this note, we confirm the above conjecture for all non-balanced Bismut torsion parallel (BTP) manifolds.","Here the BTP condition means that the Bismut connection has parallel torsion.","In particular, the conjecture is valid for all Vaisman manifolds."],"url":"http://arxiv.org/abs/2405.09110v1","category":"math.DG"}
{"created":"2024-05-15 05:43:16","title":"Minimisation of Polyak-\u0141ojasewicz Functions Using Random Zeroth-Order Oracles","abstract":"The application of a zeroth-order scheme for minimising Polyak-\\L{}ojasewicz (PL) functions is considered. The framework is based on exploiting a random oracle to estimate the function gradient. The convergence of the algorithm to a global minimum in the unconstrained case and to a neighbourhood of the global minimum in the constrained case along with their corresponding complexity bounds are presented. The theoretical results are demonstrated via numerical examples.","sentences":["The application of a zeroth-order scheme for minimising Polyak-\\L{}ojasewicz (PL) functions is considered.","The framework is based on exploiting a random oracle to estimate the function gradient.","The convergence of the algorithm to a global minimum in the unconstrained case and to a neighbourhood of the global minimum in the constrained case along with their corresponding complexity bounds are presented.","The theoretical results are demonstrated via numerical examples."],"url":"http://arxiv.org/abs/2405.09106v1","category":"math.OC"}
{"created":"2024-05-15 05:39:34","title":"Virtual melting and cyclic transformations between amorphous Si, Si I, and Si IV in a shear band","abstract":"Virtual melting (VM) as alternative deformation and stress relaxation mechanisms under extreme load is directly validated by molecular dynamics (MD) simulations of the simple shear of single crystal Si I at a temperature 1,383 K below the melting temperature. The shear band consisting of liquid Si is formed immediately after the shear instability while stress drops to zero. A thermodynamic criterion for VM, which depends on the ratio of the sample to shear band widths, is derived analytically and confirmed by MD simulations. With further shear, the VM immediately transforms to a mixture of low-density amorphous a-Si, Si I, and IV, which undergo cyclic transformations a-Si to and from Si I, a-Si to Si IV, and Si I to and from Si IV with volume fraction of phases mostly between 0.2 and 0.4 and non-repeatable nanostructure evolution. Such cyclic transformations produce additional important carriers for plastic deformation through transformation strain and transformation-induced plasticity due to volume change, which may occur in shear bands in various material systems but missed in experiments and simulations.","sentences":["Virtual melting (VM) as alternative deformation and stress relaxation mechanisms under extreme load is directly validated by molecular dynamics (MD) simulations of the simple shear of single crystal","Si I at a temperature 1,383 K below the melting temperature.","The shear band consisting of liquid Si is formed immediately after the shear instability while stress drops to zero.","A thermodynamic criterion for VM, which depends on the ratio of the sample to shear band widths, is derived analytically and confirmed by MD simulations.","With further shear, the VM immediately transforms to a mixture of low-density amorphous a-Si, Si I, and IV, which undergo cyclic transformations a-Si to and from Si I, a-Si to Si IV, and Si I to and from Si IV with volume fraction of phases mostly between 0.2 and 0.4 and non-repeatable nanostructure evolution.","Such cyclic transformations produce additional important carriers for plastic deformation through transformation strain and transformation-induced plasticity due to volume change, which may occur in shear bands in various material systems but missed in experiments and simulations."],"url":"http://arxiv.org/abs/2405.09105v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-15 05:28:46","title":"A Catalogue and Analysis of Ultra-Diffuse Galaxy Spectroscopic Properties","abstract":"In order to help facilitate the future study of ultra-diffuse galaxies (UDGs) we compile a catalogue of their spectroscopic properties. Using it, we investigate some of the biases inherent in the current UDG sample that have been targeted for spectroscopy. In comparison to a larger sample of UDGs studied via their spectral energy distributions (SED), current spectroscopic targets are intrinsically brighter, have higher stellar mass, are larger, more globular cluster-rich, older, and have a wider spread in their metallicities. In particular, many spectroscopically studied UDGs have a significant fraction of their stellar mass contained within their globular cluster (GC) system. We also search for correlations between parameters in the catalogue. Of note is a correlation between alpha element abundance and metallicity as may be expected for a `failed galaxy' scenario. However, the expected correlations of metallicity with age are not found and it is unclear if this is evidence against a `failed galaxy' scenario or simply due to the low number statistics and the presence of outliers. Finally, we attempt to segment our catalogue into different classes using a machine learning K-means method. We find that the clustering is very weak and that it is currently not warranted to split the catalogue into multiple, distinct sub-populations. Our catalogue is available online and we aim to maintain it beyond the publication of this work.","sentences":["In order to help facilitate the future study of ultra-diffuse galaxies (UDGs) we compile a catalogue of their spectroscopic properties.","Using it, we investigate some of the biases inherent in the current UDG sample that have been targeted for spectroscopy.","In comparison to a larger sample of UDGs studied via their spectral energy distributions (SED), current spectroscopic targets are intrinsically brighter, have higher stellar mass, are larger, more globular cluster-rich, older, and have a wider spread in their metallicities.","In particular, many spectroscopically studied UDGs have a significant fraction of their stellar mass contained within their globular cluster (GC) system.","We also search for correlations between parameters in the catalogue.","Of note is a correlation between alpha element abundance and metallicity as may be expected for a `failed galaxy' scenario.","However, the expected correlations of metallicity with age are not found and it is unclear if this is evidence against a `failed galaxy' scenario or simply due to the low number statistics and the presence of outliers.","Finally, we attempt to segment our catalogue into different classes using a machine learning K-means method.","We find that the clustering is very weak and that it is currently not warranted to split the catalogue into multiple, distinct sub-populations.","Our catalogue is available online","and we aim to maintain it beyond the publication of this work."],"url":"http://arxiv.org/abs/2405.09104v1","category":"astro-ph.GA"}
{"created":"2024-05-15 05:19:21","title":"Bistellar Cluster Algebras and Piecewise Linear Invariants","abstract":"Inspired by the ideas and techniques used in the study of cluster algebras we construct a new class of algebras, called bistellar cluster algebras, from closed oriented triangulated even-dimensional manifolds by performing middle-dimensional bistellar moves. This class of algebras exhibit the algebraic behaviour of middle-dimensional bistellar moves but do not satisfy the classical cluster algebra axiom: \"every cluster variable in every cluster is exchangeable\". Thus the construction of bistellar cluster algebras is quite different from that of a classical cluster algebra. Secondly, using bistellar cluster algebras and the techniques of combinatorial topology, we construct a direct system associated with a set of PL homeomorphic PL manifolds of dimension 2 or 4, and show that the limit of this direct system is a PL invariant.","sentences":["Inspired by the ideas and techniques used in the study of cluster algebras we construct a new class of algebras, called bistellar cluster algebras, from closed oriented triangulated even-dimensional manifolds by performing middle-dimensional bistellar moves.","This class of algebras exhibit the algebraic behaviour of middle-dimensional bistellar moves but do not satisfy the classical cluster algebra axiom: \"every cluster variable in every cluster is exchangeable\".","Thus the construction of bistellar cluster algebras is quite different from that of a classical cluster algebra.","Secondly, using bistellar cluster algebras and the techniques of combinatorial topology, we construct a direct system associated with a set of PL homeomorphic PL manifolds of dimension 2 or 4, and show that the limit of this direct system is a PL invariant."],"url":"http://arxiv.org/abs/2405.09100v1","category":"math.AT"}
{"created":"2024-05-15 05:18:10","title":"Analysis of Galaxies at the Extremes: A Kinematic Analysis of the Virgo Cluster Dwarfs VCC 9 and VCC 1448 using the Keck Cosmic Web Imager","abstract":"We present spatially resolved Keck Cosmic Web Imager stellar spectroscopy of the Virgo cluster dwarf galaxies VCC 9 and VCC 1448. These galaxies have similar stellar masses and large half-light radii but very different globular cluster (GC) system richness ($\\sim$25 vs. $\\sim$99 GCs). Using the KCWI data, we spectroscopically confirm 10 GCs associated with VCC 1448 and one GC associated with VCC 9. We make two measurements of dynamical mass for VCC 1448 based on the stellar and GC velocities respectively. VCC 1448's mass measurements suggest that it resides in a halo in better agreement with the expectation of the stellar mass -- halo mass relationship than the expectation from its large GC counts. For VCC 9, the dynamical mass we measure agrees with the expected halo mass from both relationships. We compare VCC 1448 and VCC 9 to the GC-rich galaxy Dragonfly 44 ($\\sim74$ GCs), which is similar in size but has $\\sim 1$ dex less stellar mass than either Virgo galaxy. In dynamical mass -- GC number space, Dragonfly 44 and VCC 1448 exhibit richer GC systems given their dynamical mass than that of VCC 9 and other `normal' galaxies. We also place the galaxies in kinematics -- ellipticity space finding evidence of an anticorrelation between rotational support and the fraction of a galaxy's stellar mass in its GC system. i.e., VCC 9 is more rotationally supported than VCC 1448, which is more rotationally supported than Dragonfly 44. This trend may be expected if a galaxy's GC content depends on its natal gas properties at formation.","sentences":["We present spatially resolved Keck Cosmic Web Imager stellar spectroscopy of the Virgo cluster dwarf galaxies VCC 9 and VCC 1448.","These galaxies have similar stellar masses and large half-light radii but very different globular cluster (GC) system richness ($\\sim$25 vs. $\\sim$99 GCs).","Using the KCWI data, we spectroscopically confirm 10 GCs associated with VCC 1448 and one GC associated with VCC 9.","We make two measurements of dynamical mass for VCC 1448 based on the stellar and GC velocities respectively.","VCC 1448's mass measurements suggest that it resides in a halo in better agreement with the expectation of the stellar mass -- halo mass relationship than the expectation from its large GC counts.","For VCC 9, the dynamical mass we measure agrees with the expected halo mass from both relationships.","We compare VCC 1448 and VCC 9 to the GC-rich galaxy Dragonfly 44 ($\\sim74$ GCs), which is similar in size but has $\\sim 1$ dex less stellar mass than either Virgo galaxy.","In dynamical mass -- GC number space, Dragonfly 44 and VCC 1448 exhibit richer GC systems given their dynamical mass than that of VCC 9 and other `normal' galaxies.","We also place the galaxies in kinematics -- ellipticity space finding evidence of an anticorrelation between rotational support and the fraction of a galaxy's stellar mass in its GC system.","i.e., VCC 9 is more rotationally supported than VCC 1448, which is more rotationally supported than Dragonfly 44.","This trend may be expected if a galaxy's GC content depends on its natal gas properties at formation."],"url":"http://arxiv.org/abs/2405.09098v1","category":"astro-ph.GA"}
{"created":"2024-05-15 05:16:59","title":"Temperature Dependence of Electron Viscosity in Superballistic GaAs Point Contacts","abstract":"Electron transport in suspended and non-suspended GaAs point contacts (PCs) of different widths is experimentally studied. The superballistic contribution to the conductance, that demonstrates a distinctive quadratic dependence on the PC width and temperature growth, is extracted from the experiment. The studied PCs are shown to be described in the framework of hydrodynamic electron flow a in wide temperature range. At low temperatures, $T$, the viscosity is found out to obey the law $1/T^2$ expected for 2D systems, while at higher temperatures it has the dependence $1/T$. Similar measurements performed after the suspension of PCs, i.e. their separation from substrate, show that the electron viscosity reduces in the whole temperature range, that indicates an enhanced electron-electron interaction in suspended structures.","sentences":["Electron transport in suspended and non-suspended GaAs point contacts (PCs) of different widths is experimentally studied.","The superballistic contribution to the conductance, that demonstrates a distinctive quadratic dependence on the PC width and temperature growth, is extracted from the experiment.","The studied PCs are shown to be described in the framework of hydrodynamic electron flow a in wide temperature range.","At low temperatures, $T$, the viscosity is found out to obey the law $1/T^2$ expected for 2D systems, while at higher temperatures it has the dependence $1/T$. Similar measurements performed after the suspension of PCs, i.e. their separation from substrate, show that the electron viscosity reduces in the whole temperature range, that indicates an enhanced electron-electron interaction in suspended structures."],"url":"http://arxiv.org/abs/2405.09097v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-15 04:54:09","title":"On the analyticity of the lightest particle mass of Ising field theory in a magnetic field","abstract":"We study the scaling functions associated with the lightest particle mass $M_1$ in 2d Ising field theory in external magnetic field. The scaling functions depend on the scaling parameter $\\xi = h/|m|^{\\frac{15}{8}}$, or related parameter $\\eta = m / h^{\\frac{8}{15}}$. Analytic properties of $M_1$ in the high-T domain were discussed in arXiv:2203.11262. In this work, we study analyticity of $M_1$ in the low-T domain. Important feature of this analytic structure is represented by the Fisher-Langer's branch cut. The discontinuity across this branch cut determines the behavior of $M_1$ at all complex $\\xi$ via associated low-T dispersion relation. Also, we put forward the \"extended analyticity\" conjecture for $M_1$ in the complex $\\eta$-plane, similar to the analyticity of the free energy density previously proposed in arXiv:hep-th/0112167. The extended analyticity implies the \"extended dispersion relation\", which we verify against the numerics from the Truncated Free Fermion Approach (TFFSA), giving strong support to the conjecture.","sentences":["We study the scaling functions associated with the lightest particle mass $M_1$ in 2d Ising field theory in external magnetic field.","The scaling functions depend on the scaling parameter $\\xi = h/|m|^{\\frac{15}{8}}$, or related parameter $\\eta = m / h^{\\frac{8}{15}}$. Analytic properties of $M_1$ in the high-T domain were discussed in arXiv:2203.11262.","In this work, we study analyticity of $M_1$ in the low-T domain.","Important feature of this analytic structure is represented by the Fisher-Langer's branch cut.","The discontinuity across this branch cut determines the behavior of $M_1$ at all complex $\\xi$ via associated low-T dispersion relation.","Also, we put forward the \"extended analyticity\" conjecture for $M_1$ in the complex $\\eta$-plane, similar to the analyticity of the free energy density previously proposed in arXiv:hep-th/0112167.","The extended analyticity implies the \"extended dispersion relation\", which we verify against the numerics from the Truncated Free Fermion Approach (TFFSA), giving strong support to the conjecture."],"url":"http://arxiv.org/abs/2405.09091v1","category":"hep-th"}
{"created":"2024-05-15 04:49:06","title":"Contracting a Single Element in a Transversal Matroid","abstract":"It is well known that the class of transversal matroids is not closed under contraction or duality. The complexity of deciding whether a minor or dual of a transversal matroid remains transversal is in $\\Sigma_2$ and thus far there has been no improvement on this bound. We explore this issue, providing a polynomial time algorithm for determining whether a single element contraction of a transversal matroid remains transversal. If so, our algorithm also provides a transversal representation. We then develop the techniques used in search of a polynomial time algorithm for determining whether the dual of a transversal matroid remains transversal.","sentences":["It is well known that the class of transversal matroids is not closed under contraction or duality.","The complexity of deciding whether a minor or dual of a transversal matroid remains transversal is in $\\Sigma_2$ and thus far there has been no improvement on this bound.","We explore this issue, providing a polynomial time algorithm for determining whether a single element contraction of a transversal matroid remains transversal.","If so, our algorithm also provides a transversal representation.","We then develop the techniques used in search of a polynomial time algorithm for determining whether the dual of a transversal matroid remains transversal."],"url":"http://arxiv.org/abs/2405.09088v1","category":"math.CO"}
{"created":"2024-05-15 04:48:25","title":"The Nature of Transaction Cost: Eliminating Misunderstandings and Reconstructing Cognition","abstract":"The connotation of transaction costs has never been definitively determined, and the independence of the concept has never been rigorously demonstrated. This paper delves into the thought systems of several prominent economists in the development of transaction cost economics, starting from first-hand materials. By combining multiple works of the authors, it reconstructs the true meanings and identifies endogeneity issues and logical inconsistencies. The conclusion of this paper is bold. Previous research has been largely filled with misinterpretations and misunderstandings, as people have focused solely on the wording of transaction cost definitions, neglecting the nature of transaction costs. The intention of transaction cost theory has been unwittingly assimilated into the objects it intends to criticize. After delineating the framework of \"transaction costs-property rights-competition\", this paper reconstructs the concept of transaction costs and the history of transaction cost concepts, providing a direct response to this theoretical puzzle that has plagued the academic community for nearly a century.","sentences":["The connotation of transaction costs has never been definitively determined, and the independence of the concept has never been rigorously demonstrated.","This paper delves into the thought systems of several prominent economists in the development of transaction cost economics, starting from first-hand materials.","By combining multiple works of the authors, it reconstructs the true meanings and identifies endogeneity issues and logical inconsistencies.","The conclusion of this paper is bold.","Previous research has been largely filled with misinterpretations and misunderstandings, as people have focused solely on the wording of transaction cost definitions, neglecting the nature of transaction costs.","The intention of transaction cost theory has been unwittingly assimilated into the objects it intends to criticize.","After delineating the framework of \"transaction costs-property rights-competition\", this paper reconstructs the concept of transaction costs and the history of transaction cost concepts, providing a direct response to this theoretical puzzle that has plagued the academic community for nearly a century."],"url":"http://arxiv.org/abs/2405.09087v1","category":"econ.TH"}
{"created":"2024-05-15 04:14:45","title":"A closer look at the host-galaxy environment of high-velocity Type Ia supernovae","abstract":"Recent studies suggested that the ejecta velocity of Type Ia supernova (SN Ia) is a promising indicator in distinguishing the progenitor systems and explosion mechanisms. By classifying the SNe Ia based on their ejecta velocities, studies found SNe Ia with high Si II $\\lambda$6355 velocities (HV SNe Ia; v>12000 km/s) tend to be physically different from their normal-velocity counterparts (NV SNe Ia). In this work, we revisit the low-$z$ sample studied in previous work and closely look into the spatially resolved environment local to the site of SN explosion. Our results reveal a possible trend (at $2.4\\sigma$ significance) that HV SNe Ia are likely associated with older stellar populations than NV SNe Ia. While the trend is inconclusive, the local host-galaxy sample studied in this work is likely skewed toward massive galaxies, limiting the parameter space that we would like to investigate from the original parent sample. Nevertheless, our results do not rule out the possibility that parameters other than the host-galaxy age (such as metallicity) could be the underlying factors driving the differences between HV and NV SNe Ia due to the limitation of our dataset.","sentences":["Recent studies suggested that the ejecta velocity of Type Ia supernova (SN Ia) is a promising indicator in distinguishing the progenitor systems and explosion mechanisms.","By classifying the SNe Ia based on their ejecta velocities, studies found SNe Ia with high Si II $\\lambda$6355 velocities (HV SNe Ia; v>12000 km/s) tend to be physically different from their normal-velocity counterparts (NV SNe Ia).","In this work, we revisit the low-$z$ sample studied in previous work and closely look into the spatially resolved environment local to the site of SN explosion.","Our results reveal a possible trend (at $2.4\\sigma$ significance) that HV SNe Ia are likely associated with older stellar populations than NV SNe Ia.","While the trend is inconclusive, the local host-galaxy sample studied in this work is likely skewed toward massive galaxies, limiting the parameter space that we would like to investigate from the original parent sample.","Nevertheless, our results do not rule out the possibility that parameters other than the host-galaxy age (such as metallicity) could be the underlying factors driving the differences between HV and NV SNe Ia due to the limitation of our dataset."],"url":"http://arxiv.org/abs/2405.09082v1","category":"astro-ph.HE"}
{"created":"2024-05-15 04:05:51","title":"Integrated Monostatic Sensing and Full-Duplex Multiuser Communication for mmWave Systems","abstract":"In this paper, we propose a hybrid precoding/combining framework for communication-centric integrated sensing and full-duplex (FD) communication operating at mmWave bands. The designed precoders and combiners enable multiuser (MU) FD communication while simultaneously supporting monostatic sensing in a frequency-selective setting. The joint design of precoders and combiners involves the mitigation of self-interference (SI) caused by simultaneous transmission and reception at the FD base station (BS). Additionally, MU interference needs to be handled by the precoder/combiner design. The resulting optimization problem involves non-convex constraints since hybrid analog/digital architectures utilize networks of phase shifters. To solve the proposed problem, we separate the optimization of each precoder/combiner, and design each one of them while fixing the others. The precoders at the FD BS are designed by reformulating the communication and sensing constraints as signal-to-leakage-plus-noise ratio (SLNR) maximization problems that consider SI and MU interference as leakage. Furthermore, we design the frequency-flat analog combiner such that the residual SI at the FD BS is minimized under communication and sensing gain constraints. Finally, we design an interference-aware digital combining stage that separates MU signals and target reflections. The communication performance and sensing results show that the proposed framework efficiently supports both functionalities simultaneously.","sentences":["In this paper, we propose a hybrid precoding/combining framework for communication-centric integrated sensing and full-duplex (FD) communication operating at mmWave bands.","The designed precoders and combiners enable multiuser (MU) FD communication while simultaneously supporting monostatic sensing in a frequency-selective setting.","The joint design of precoders and combiners involves the mitigation of self-interference (SI) caused by simultaneous transmission and reception at the FD base station (BS).","Additionally, MU interference needs to be handled by the precoder/combiner design.","The resulting optimization problem involves non-convex constraints since hybrid analog/digital architectures utilize networks of phase shifters.","To solve the proposed problem, we separate the optimization of each precoder/combiner, and design each one of them while fixing the others.","The precoders at the FD BS are designed by reformulating the communication and sensing constraints as signal-to-leakage-plus-noise ratio (SLNR) maximization problems that consider SI and MU interference as leakage.","Furthermore, we design the frequency-flat analog combiner such that the residual SI at the FD BS is minimized under communication and sensing gain constraints.","Finally, we design an interference-aware digital combining stage that separates MU signals and target reflections.","The communication performance and sensing results show that the proposed framework efficiently supports both functionalities simultaneously."],"url":"http://arxiv.org/abs/2405.09079v1","category":"eess.SP"}
{"created":"2024-05-15 04:05:48","title":"Dynamic constraints predict the relaxation of granular materials","abstract":"Granular materials such as sand, powders, and food grains are ubiquitous in civil engineering, geoscience, agriculture, and medicine. While the influence of friction between the grains on the static structure of these systems is well understood, its impact on the dynamics is an open problem. Here we use particle-based simulations of a granular pack under cyclic shear and discover that the relaxation time of the system is a non-monotonic function of friction. By introducing the concept of dynamic constraints, we reveal that this re-entrant dynamics is due to the competition between increasing frictional coupling and a concurrent change in the structure of the granular pack. Our theoretical approach, which unifies the dynamics of friction-less systems with frictional ones, is applicable to other systems that have a complex free energy landscape and a dynamics which involves time-dependent constraints, thus setting the stage for a description of the dynamic behavior of a large class of complex systems.","sentences":["Granular materials such as sand, powders, and food grains are ubiquitous in civil engineering, geoscience, agriculture, and medicine.","While the influence of friction between the grains on the static structure of these systems is well understood, its impact on the dynamics is an open problem.","Here we use particle-based simulations of a granular pack under cyclic shear and discover that the relaxation time of the system is a non-monotonic function of friction.","By introducing the concept of dynamic constraints, we reveal that this re-entrant dynamics is due to the competition between increasing frictional coupling and a concurrent change in the structure of the granular pack.","Our theoretical approach, which unifies the dynamics of friction-less systems with frictional ones, is applicable to other systems that have a complex free energy landscape and a dynamics which involves time-dependent constraints, thus setting the stage for a description of the dynamic behavior of a large class of complex systems."],"url":"http://arxiv.org/abs/2405.09078v1","category":"cond-mat.soft"}
{"created":"2024-05-15 03:57:27","title":"See to Believe: Using Visualization To Motivate Updating Third-party Dependencies","abstract":"Security vulnerabilities introduced by applications using third-party dependencies are on the increase, caused by the emergence of large ecosystems of libraries such as the NPM packages for JavaScript. Nowadays, libraries depend on each other. Relying on these large ecosystems thus means that vulnerable dependencies are not only direct but also indirect (transitive) dependencies. There are automated tool supports to manage these complex dependencies but recent work still shows that developers are wary of library updates, even to fix vulnerabilities, citing that being unaware, or that the migration effort to update outweighs the decision.   In this paper, we hypothesize that the dependency graph visualization (DGV) approach will motivate developers to update, especially when convincing developers. To test this hypothesis, we performed a user study involving 20 participants divided equally into experimental and control groups, comparing the state-of-the-art tools with the tasks of reviewing vulnerabilities with complexities and vulnerabilities with indirect dependencies.   We find that 70% of the participants who saw the visualization did re-prioritize their updates in both tasks. This is higher than the 30% and 60% of the participants who used the npm audit tool in both tasks, respectively.","sentences":["Security vulnerabilities introduced by applications using third-party dependencies are on the increase, caused by the emergence of large ecosystems of libraries such as the NPM packages for JavaScript.","Nowadays, libraries depend on each other.","Relying on these large ecosystems thus means that vulnerable dependencies are not only direct but also indirect (transitive) dependencies.","There are automated tool supports to manage these complex dependencies but recent work still shows that developers are wary of library updates, even to fix vulnerabilities, citing that being unaware, or that the migration effort to update outweighs the decision.   ","In this paper, we hypothesize that the dependency graph visualization (DGV) approach will motivate developers to update, especially when convincing developers.","To test this hypothesis, we performed a user study involving 20 participants divided equally into experimental and control groups, comparing the state-of-the-art tools with the tasks of reviewing vulnerabilities with complexities and vulnerabilities with indirect dependencies.   ","We find that 70% of the participants who saw the visualization did re-prioritize their updates in both tasks.","This is higher than the 30% and 60% of the participants who used the npm audit tool in both tasks, respectively."],"url":"http://arxiv.org/abs/2405.09074v1","category":"cs.SE"}
{"created":"2024-05-15 03:47:14","title":"Confined subgroups in groups with contracting elements","abstract":"In this paper, we study the growth of confined subgroups through boundary actions of groups with contracting elements. We establish that the growth rate of a confined subgroup is strictly greater than half of the ambient growth rate in groups with purely exponential growth. Along the way, several results are obtained on the Hopf decomposition for boundary actions of subgroups with respect to conformal measures. In particular, we prove that confined subgroups are conservative, and examples of subgroups with nontrivial Hopf decomposition are constructed. We show a connection between Hopf decomposition and quotient growth and provide a dichotomy on quotient growth of Schreier graphs for subgroups in hyperbolic groups.","sentences":["In this paper, we study the growth of confined subgroups through boundary actions of groups with contracting elements.","We establish that the growth rate of a confined subgroup is strictly greater than half of the ambient growth rate in groups with purely exponential growth.","Along the way, several results are obtained on the Hopf decomposition for boundary actions of subgroups with respect to conformal measures.","In particular, we prove that confined subgroups are conservative, and examples of subgroups with nontrivial Hopf decomposition are constructed.","We show a connection between Hopf decomposition and quotient growth and provide a dichotomy on quotient growth of Schreier graphs for subgroups in hyperbolic groups."],"url":"http://arxiv.org/abs/2405.09070v1","category":"math.GR"}
{"created":"2024-05-15 03:27:11","title":"Early Planet Formation in Embedded Disks (eDisk) XV: Influence of Magnetic Field Morphology in Dense Cores on Sizes of Protostellar Disks","abstract":"The magnetic field of a molecular cloud core may play a role in the formation of circumstellar disks in the core. We present magnetic field morphologies in protostellar cores of 16 targets in the Atacama Large Millimeter/submillimeter Array large program \"Early Planet Formation in Embedded Disks (eDisk)\", which resolved their disks with 7 au resolutions. The 0.1-pc scale magnetic field morphologies were inferred from the James Clerk Maxwell Telescope (JCMT) POL-2 observations. The mean orientations and angular dispersions of the magnetic fields in the dense cores are measured and compared with the radii of the 1.3 mm continuum disks and the dynamically determined protostellar masses from the eDisk program. We observe a significant correlation between the disk radii and the stellar masses. We do not find any statistically significant dependence of the disk radii on the projected misalignment angles between the rotational axes of the disks and the magnetic fields in the dense cores, nor on the angular dispersions of the magnetic fields within these cores. However, when considering the projection effect, we cannot rule out a positive correlation between disk radii and misalignment angles in three-dimensional space. Our results suggest that the morphologies of magnetic fields in dense cores do not play a dominant role in the disk formation process. Instead, the sizes of protostellar disks may be more strongly affected by the amount of mass that has been accreted onto star+disk systems, and possibly other parameters, for example, magnetic field strength, core rotation, and magnetic diffusivity.","sentences":["The magnetic field of a molecular cloud core may play a role in the formation of circumstellar disks in the core.","We present magnetic field morphologies in protostellar cores of 16 targets in the Atacama Large Millimeter/submillimeter Array large program \"Early Planet Formation in Embedded Disks (eDisk)\", which resolved their disks with 7 au resolutions.","The 0.1-pc scale magnetic field morphologies were inferred from the James Clerk Maxwell Telescope (JCMT) POL-2 observations.","The mean orientations and angular dispersions of the magnetic fields in the dense cores are measured and compared with the radii of the 1.3 mm continuum disks and the dynamically determined protostellar masses from the eDisk program.","We observe a significant correlation between the disk radii and the stellar masses.","We do not find any statistically significant dependence of the disk radii on the projected misalignment angles between the rotational axes of the disks and the magnetic fields in the dense cores, nor on the angular dispersions of the magnetic fields within these cores.","However, when considering the projection effect, we cannot rule out a positive correlation between disk radii and misalignment angles in three-dimensional space.","Our results suggest that the morphologies of magnetic fields in dense cores do not play a dominant role in the disk formation process.","Instead, the sizes of protostellar disks may be more strongly affected by the amount of mass that has been accreted onto star+disk systems, and possibly other parameters, for example, magnetic field strength, core rotation, and magnetic diffusivity."],"url":"http://arxiv.org/abs/2405.09063v1","category":"astro-ph.SR"}
{"created":"2024-05-15 03:08:21","title":"Response Matching for generating materials and molecules","abstract":"Machine learning has recently emerged as a powerful tool for generating new molecular and material structures. The success of state-of-the-art models stems from their ability to incorporate physical symmetries, such as translation, rotation, and periodicity. Here, we present a novel generative method called Response Matching (RM), which leverages the fact that each stable material or molecule exists at the minimum of its potential energy surface. Consequently, any perturbation induces a response in energy and stress, driving the structure back to equilibrium. Matching to such response is closely related to score matching in diffusion models. By employing the combination of a machine learning interatomic potential and random structure search as the denoising model, RM exploits the locality of atomic interactions, and inherently respects permutation, translation, rotation, and periodic invariances. RM is the first model to handle both molecules and bulk materials under the same framework. We demonstrate the efficiency and generalization of RM across three systems: a small organic molecular dataset, stable crystals from the Materials Project, and one-shot learning on a single diamond configuration.","sentences":["Machine learning has recently emerged as a powerful tool for generating new molecular and material structures.","The success of state-of-the-art models stems from their ability to incorporate physical symmetries, such as translation, rotation, and periodicity.","Here, we present a novel generative method called Response Matching (RM), which leverages the fact that each stable material or molecule exists at the minimum of its potential energy surface.","Consequently, any perturbation induces a response in energy and stress, driving the structure back to equilibrium.","Matching to such response is closely related to score matching in diffusion models.","By employing the combination of a machine learning interatomic potential and random structure search as the denoising model, RM exploits the locality of atomic interactions, and inherently respects permutation, translation, rotation, and periodic invariances.","RM is the first model to handle both molecules and bulk materials under the same framework.","We demonstrate the efficiency and generalization of RM across three systems: a small organic molecular dataset, stable crystals from the Materials Project, and one-shot learning on a single diamond configuration."],"url":"http://arxiv.org/abs/2405.09057v1","category":"cs.LG"}
{"created":"2024-05-15 03:01:18","title":"Deep Learning-Based CSI Feedback for XL-MIMO Systems in the Near-Field Domain","abstract":"In this paper, we consider an extremely large-scale massive multiple-input-multiple-output (XL-MIMO) system. As the scale of antenna arrays increases, the range of near-field communications also expands. In this case, the signals no longer exhibit planar wave characteristics but spherical wave characteristics in the near-field channel, which makes the channel state information (CSI) highly complex. Additionally, the increase of the antenna arrays scale also makes the size of the CSI matrix significantly increase. Therefore, CSI feedback in the near-field channel becomes highly challenging. To solve this issue, we propose a deep-learning (DL)-based ExtendNLNet that can compress the CSI, and further reduce the overhead of CSI feedback. In addition, we have introduced the Non-Local block to obtain a larger area of CSI features. Simulation results show that the proposed ExtendNLNet can significantly improve the CSI recovery quality compared to other DL-based methods.","sentences":["In this paper, we consider an extremely large-scale massive multiple-input-multiple-output (XL-MIMO) system.","As the scale of antenna arrays increases, the range of near-field communications also expands.","In this case, the signals no longer exhibit planar wave characteristics but spherical wave characteristics in the near-field channel, which makes the channel state information (CSI) highly complex.","Additionally, the increase of the antenna arrays scale also makes the size of the CSI matrix significantly increase.","Therefore, CSI feedback in the near-field channel becomes highly challenging.","To solve this issue, we propose a deep-learning (DL)-based ExtendNLNet that can compress the CSI, and further reduce the overhead of CSI feedback.","In addition, we have introduced the Non-Local block to obtain a larger area of CSI features.","Simulation results show that the proposed ExtendNLNet can significantly improve the CSI recovery quality compared to other DL-based methods."],"url":"http://arxiv.org/abs/2405.09053v1","category":"eess.SP"}
{"created":"2024-05-15 02:51:35","title":"Topological Laser in Anomalous Quadrupole Topological Phases","abstract":"Topological photonics shows considerable promise in revolutionizing photonic devices through the use of topological phases, leading to innovations like topological lasers that enhance light control. One of recent breakthroughs is reducing the size of these systems by utilizing lower-dimensional boundary states, notably via higher-order topological phases. This paper presents the first experimental demonstration of topological laser in anomalous quadrupole topological phase, an instance of higher-order phases. To facilitate this, a topological nanocavity with quality factor near 6,000 is engineered through a twisting operation. The topological nature of our system is validated by calculation of nested Wannier center and the emergency condition of corner states. Our experimental observations reveal the manifestation of corner states and the achievement of single-mode pulsed laser, driven by optical gain from multiple quantum wells at telecommunication wavelengths and at a temperature of 4 K. A lasing threshold of 23 uW and a cold quality factor of 1,500 are deduce through rate equation. Our work gives a new potential in the application of topological principles to advance nanophotonic technologies.","sentences":["Topological photonics shows considerable promise in revolutionizing photonic devices through the use of topological phases, leading to innovations like topological lasers that enhance light control.","One of recent breakthroughs is reducing the size of these systems by utilizing lower-dimensional boundary states, notably via higher-order topological phases.","This paper presents the first experimental demonstration of topological laser in anomalous quadrupole topological phase, an instance of higher-order phases.","To facilitate this, a topological nanocavity with quality factor near 6,000 is engineered through a twisting operation.","The topological nature of our system is validated by calculation of nested Wannier center and the emergency condition of corner states.","Our experimental observations reveal the manifestation of corner states and the achievement of single-mode pulsed laser, driven by optical gain from multiple quantum wells at telecommunication wavelengths and at a temperature of 4 K. A lasing threshold of 23 uW and a cold quality factor of 1,500 are deduce through rate equation.","Our work gives a new potential in the application of topological principles to advance nanophotonic technologies."],"url":"http://arxiv.org/abs/2405.09047v1","category":"physics.optics"}
{"created":"2024-05-15 02:50:26","title":"Entanglement parity effects in the Kane-Fisher problem","abstract":"We study the entanglement of a segment of length $\\ell$ in an XXZ chain with one free extremity and the other connected to the rest of the system with a weak bond. We find that the von-Neumann entropy exhibits terms of order $O(1)$ with strong parity effects, that probe the physics associated with the weakened bond and its behavior under the RG (Kane Fisher problem). In contrast with the XX case studied previously the entropy difference $\\delta S\\equiv S^e-S^o$ gives rise now to a \"resonance\" curve which depends on the product $\\ell T_B$, with $1/T_B$ a characteristic length scale akin to the Kondo length in Kondo problems. The problem is studied both numerically using DMRG and analytically near the healed and split fixed points. Interestingly - and in contrast with what happens in other impurity problems- $\\delta S$ can, at least at lowest order, be tackled by conformal perturbation theory.","sentences":["We study the entanglement of a segment of length $\\ell$ in an XXZ chain with one free extremity and the other connected to the rest of the system with a weak bond.","We find that the von-Neumann entropy exhibits terms of order $O(1)$ with strong parity effects, that probe the physics associated with the weakened bond and its behavior under the RG (Kane Fisher problem).","In contrast with the XX case studied previously the entropy difference $\\delta S\\equiv S^e-S^o$ gives rise now to a \"resonance\" curve which depends on the product $\\ell T_B$, with $1/T_B$ a characteristic length scale akin to the Kondo length in Kondo problems.","The problem is studied both numerically using DMRG and analytically near the healed and split fixed points.","Interestingly - and in contrast with what happens in other impurity problems- $\\delta S$ can, at least at lowest order, be tackled by conformal perturbation theory."],"url":"http://arxiv.org/abs/2405.09046v1","category":"cond-mat.str-el"}
{"created":"2024-05-15 02:42:10","title":"Modeling and Design Optimization of Looped Water Distribution Networks using MS Excel: Developing the Open-Source X-WHAT Model","abstract":"Cost-effective water distribution network (WDN) design with acceptable pressure performance is crucial for the management of drinking water in cities. This paper presents a Microsoft Excel tool to model, simulate, and optimize WDNs with looped pipelines under steady-state incompressible flow simulations. Typically, the hardy-cross method is applied using spreadsheet calculations to estimate discharges. This method requires mass-conservative initial estimates and requires successive iterations to converge. In this paper, however, we develop an alternative method that uses the built-in solver capabilities of Excel, does not require initial mass-conservative estimation, and is free of flow corrections. The main objective of this paper is to develop an open-source accessible tool for simulating hydraulic networks also adapted for teaching and learning purposes. The governing equations and the mathematical basis for the hydraulic modeling of the system are mathematically described, considering the topology of the network, mass and energy conservation, cost of tank material, foundation, and cost of pumping energy to fill the tank. The use of this tool is encouraged at the undergraduate and graduate engineering levels, as it offers the opportunity to address complex concepts in a comprehensive way using a spreadsheet that does not require coding expertise. Hence, users can debug all cells and understand all equations used in the hydraulic model, as well as modify them. To demonstrate the model capabilities, three practical examples are presented, with the first one solved step by step, and the results are compared with the EPANET and with the results reported in the literature. Using the optimization method presented in this paper, it was possible to achieve a cost reduction of 151,790 USD (9.8% of the total cost) in a network that supplies a 44,416 population.","sentences":["Cost-effective water distribution network (WDN) design with acceptable pressure performance is crucial for the management of drinking water in cities.","This paper presents a Microsoft Excel tool to model, simulate, and optimize WDNs with looped pipelines under steady-state incompressible flow simulations.","Typically, the hardy-cross method is applied using spreadsheet calculations to estimate discharges.","This method requires mass-conservative initial estimates and requires successive iterations to converge.","In this paper, however, we develop an alternative method that uses the built-in solver capabilities of Excel, does not require initial mass-conservative estimation, and is free of flow corrections.","The main objective of this paper is to develop an open-source accessible tool for simulating hydraulic networks also adapted for teaching and learning purposes.","The governing equations and the mathematical basis for the hydraulic modeling of the system are mathematically described, considering the topology of the network, mass and energy conservation, cost of tank material, foundation, and cost of pumping energy to fill the tank.","The use of this tool is encouraged at the undergraduate and graduate engineering levels, as it offers the opportunity to address complex concepts in a comprehensive way using a spreadsheet that does not require coding expertise.","Hence, users can debug all cells and understand all equations used in the hydraulic model, as well as modify them.","To demonstrate the model capabilities, three practical examples are presented, with the first one solved step by step, and the results are compared with the EPANET and with the results reported in the literature.","Using the optimization method presented in this paper, it was possible to achieve a cost reduction of 151,790 USD (9.8% of the total cost) in a network that supplies a 44,416 population."],"url":"http://arxiv.org/abs/2405.09044v1","category":"eess.SY"}
{"created":"2024-05-15 02:04:34","title":"Demonstrating a universal logical gate set on a superconducting quantum processor","abstract":"Fault-tolerant quantum computing (FTQC) is essential for achieving large-scale practical quantum computation. Implementing arbitrary FTQC requires the execution of a universal gate set on logical qubits, which is highly challenging. Particularly, in the superconducting system, two-qubit gates on surface code logical qubits have not been realized. Here, we experimentally implement logical CNOT gate as well as arbitrary single-qubit rotation gates on distance-2 surface codes using the superconducting quantum processor \\textit{Wukong}, thereby demonstrating a universal logical gate set. In the experiment, we design encoding circuits to prepare the required logical states, where the fidelities of the fault-tolerantly prepared logical states surpass those of the physical states. Furthermore, we demonstrate the transversal CNOT gate between two logical qubits and fault-tolerantly prepare four logical Bell states, all with fidelities exceeding those of the Bell states on the physical qubits. Using the logical CNOT gate and an ancilla logical state, arbitrary single-qubit rotation gate is implemented through gate teleportation. All logical gates are characterized on a complete state set and their fidelities are evaluated by logical Pauli transfer matrices. Implementation of the universal logical gate set and entangled logical states beyond physical fidelity marks a significant step towards FTQC on superconducting quantum processors.","sentences":["Fault-tolerant quantum computing (FTQC) is essential for achieving large-scale practical quantum computation.","Implementing arbitrary FTQC requires the execution of a universal gate set on logical qubits, which is highly challenging.","Particularly, in the superconducting system, two-qubit gates on surface code logical qubits have not been realized.","Here, we experimentally implement logical CNOT gate as well as arbitrary single-qubit rotation gates on distance-2 surface codes using the superconducting quantum processor \\textit{Wukong}, thereby demonstrating a universal logical gate set.","In the experiment, we design encoding circuits to prepare the required logical states, where the fidelities of the fault-tolerantly prepared logical states surpass those of the physical states.","Furthermore, we demonstrate the transversal CNOT gate between two logical qubits and fault-tolerantly prepare four logical Bell states, all with fidelities exceeding those of the Bell states on the physical qubits.","Using the logical CNOT gate and an ancilla logical state, arbitrary single-qubit rotation gate is implemented through gate teleportation.","All logical gates are characterized on a complete state set and their fidelities are evaluated by logical Pauli transfer matrices.","Implementation of the universal logical gate set and entangled logical states beyond physical fidelity marks a significant step towards FTQC on superconducting quantum processors."],"url":"http://arxiv.org/abs/2405.09035v1","category":"quant-ph"}
{"created":"2024-05-15 01:57:48","title":"Principal eigenvalue for some elliptic operators with large drift: Neumann boundary conditions","abstract":"The paper is concerned with the principal eigenvalue of some linear elliptic operators with drift in two dimensional space. We provide a refined description of the asymptotic behavior for the principal eigenvalue as the drift rate approaches infinity. Under some non-degeneracy assumptions, our results illustrate that these asymptotic behaviors are completely determined by some connected components in the omega-limit set of the system of ordinary differential equations associated with the drift term, which includes stable fixed points, stable limit cycles, hyperbolic saddles connecting homoclinic orbits, and families of closed orbits. Some discussions on degenerate cases are also included.","sentences":["The paper is concerned with the principal eigenvalue of some linear elliptic operators with drift in two dimensional space.","We provide a refined description of the asymptotic behavior for the principal eigenvalue as the drift rate approaches infinity.","Under some non-degeneracy assumptions, our results illustrate that these asymptotic behaviors are completely determined by some connected components in the omega-limit set of the system of ordinary differential equations associated with the drift term, which includes stable fixed points, stable limit cycles, hyperbolic saddles connecting homoclinic orbits, and families of closed orbits.","Some discussions on degenerate cases are also included."],"url":"http://arxiv.org/abs/2405.09031v1","category":"math.AP"}
{"created":"2024-05-15 01:49:31","title":"Blow-up criterion for a three-dimensional compressible non-Newtonian fluid with vacuum","abstract":"This work is devoted to establish an improved blow-up criterion for strong solutions to a three-dimensional compressible non-Newtonian fluid with vacuum. The considered system is the Power Law model in a bounded periodic domain in R^3.We establish a blow-up criterion for the local strong solutions in terms of the L^4(0,T;L^{\\infty}({\\Omega}))norm of the gradient of the velocity for any power-law index q is greater than 1.","sentences":["This work is devoted to establish an improved blow-up criterion for strong solutions to a three-dimensional compressible non-Newtonian fluid with vacuum.","The considered system is the Power Law model in a bounded periodic domain in R^3.We establish a blow-up criterion for the local strong solutions in terms of the L^4(0,T;L^{\\infty}({\\Omega}))norm of the gradient of the velocity for any power-law index q is greater than 1."],"url":"http://arxiv.org/abs/2405.09028v1","category":"math.AP"}
{"created":"2024-05-15 01:37:44","title":"Thermodynamic Signatures and Phase Transitions in High-Energy Au-Au Collisions","abstract":"In this study, we systematically investigate the dynamics of various hadrons namely \\( \\pi^+ \\), \\( \\pi^- \\), \\( K^+ \\), \\( K^- \\), \\( p \\), \\( \\bar{p} \\), \\( \\Lambda \\), \\( \\bar{\\Lambda} \\), \\( \\Xi^- \\) and \\( \\bar{\\Xi}^+ \\) produced in central Au-Au collisions. We analyze data of AGS and RHIC, which span a broad range of collision energies, ranging from \\( \\sqrt{s_{NN}}\\) = 1.9 to 200 GeV. To analyze the transverse momentum (\\( p_T \\)) and transverse mass (\\( m_T \\)) distributions, we employ a two-component standard distribution function, achieving a very good representation of the experimental data across these energy regimes. We extract key thermodynamic parameters, including the effective temperature \\( T \\), the mean transverse momentum \\( \\langle p_T \\rangle \\), and the initial temperature \\( T_i \\), and analyze their dependence on the values of collision energy and particle mass. Our findings reveal a distinct transition behaviour around \\( \\sqrt{s_{NN}} = 19.6 \\) GeV. Below \\( \\sqrt{s_{NN}} = 19.6 \\) GeV, the values of \\( T \\), \\( \\langle p_T \\rangle \\), and \\( T_i \\) increase monotonically for all hadrons due to higher energy transfer into the system. Above this energy threshold, these extracted parameters plateau, suggesting that the additional energy is utilized as latent heat for phase transition rather than increasing the system's temperature. These observations delineate two distinct regions: a hadron-dominated region at lower energies and a parton-dominated region at higher energies, each potentially indicative of different phases of matter, with the latter possibly signalling the onset of a Quark-Gluon Plasma (QGP). The study thus provides critical insights into the complex interplay of thermodynamics, phase transitions, and particle interactions in high-energy Au-Au collisions.","sentences":["In this study, we systematically investigate the dynamics of various hadrons namely \\( \\pi^+ \\), \\( \\pi^- \\), \\( K^+ \\), \\( K^- \\), \\( p \\), \\( \\bar{p} \\), \\( \\Lambda \\), \\( \\bar{\\Lambda} \\), \\( \\Xi^- \\) and \\( \\bar{\\Xi}^+ \\) produced in central Au-Au collisions.","We analyze data of AGS and RHIC, which span a broad range of collision energies, ranging from \\( \\sqrt{s_{NN}}\\) = 1.9 to 200 GeV.","To analyze the transverse momentum (\\( p_T \\)) and transverse mass (\\( m_T \\))","distributions, we employ a two-component standard distribution function, achieving a very good representation of the experimental data across these energy regimes.","We extract key thermodynamic parameters, including the effective temperature \\( T \\), the mean transverse momentum \\( \\langle p_T \\rangle \\), and the initial temperature \\( T_i \\), and analyze their dependence on the values of collision energy and particle mass.","Our findings reveal a distinct transition behaviour around \\( \\sqrt{s_{NN}} = 19.6 \\)","GeV.","Below \\( \\sqrt{s_{NN}} = 19.6 \\) GeV, the values of \\( T \\), \\( \\langle p_T \\rangle \\), and \\( T_i \\) increase monotonically for all hadrons due to higher energy transfer into the system.","Above this energy threshold, these extracted parameters plateau, suggesting that the additional energy is utilized as latent heat for phase transition rather than increasing the system's temperature.","These observations delineate two distinct regions: a hadron-dominated region at lower energies and a parton-dominated region at higher energies, each potentially indicative of different phases of matter, with the latter possibly signalling the onset of a Quark-Gluon Plasma (QGP).","The study thus provides critical insights into the complex interplay of thermodynamics, phase transitions, and particle interactions in high-energy Au-Au collisions."],"url":"http://arxiv.org/abs/2405.09026v1","category":"hep-ph"}
{"created":"2024-05-15 01:25:06","title":"Multi-Objective Optimization-based Transmit Beamforming for Multi-Target and Multi-User MIMO-ISAC Systems","abstract":"Integrated sensing and communication (ISAC) is an enabling technology for the sixth-generation mobile communications, which equips the wireless communication networks with sensing capabilities. In this paper, we investigate transmit beamforming design for multiple-input and multiple-output (MIMO)-ISAC systems in scenarios with multiple radar targets and communication users. A general form of multi-target sensing mutual information (MI) is derived, along with its upper bound, which can be interpreted as the sum of individual single-target sensing MI. Additionally, this upper bound can be achieved by suppressing the cross-correlation among reflected signals from different targets, which aligns with the principles of adaptive MIMO radar. Then, we propose a multi-objective optimization framework based on the signal-to-interference-plus-noise ratio of each user and the tight upper bound of sensing MI, introducing the Pareto boundary to characterize the achievable communication-sensing performance boundary of the proposed ISAC system. To achieve the Pareto boundary, the max-min system utility function method is employed, while considering the fairness between communication users and radar targets. Subsequently, the bisection search method is employed to find a specific Pareto optimal solution by solving a series of convex feasible problems. Finally, simulation results validate that the proposed method achieves a better tradeoff between multi-user communication and multi-target sensing performance. Additionally, utilizing the tight upper bound of sensing MI as a performance metric can enhance the multi-target resolution capability and angle estimation accuracy.","sentences":["Integrated sensing and communication (ISAC) is an enabling technology for the sixth-generation mobile communications, which equips the wireless communication networks with sensing capabilities.","In this paper, we investigate transmit beamforming design for multiple-input and multiple-output (MIMO)-ISAC systems in scenarios with multiple radar targets and communication users.","A general form of multi-target sensing mutual information (MI) is derived, along with its upper bound, which can be interpreted as the sum of individual single-target sensing MI.","Additionally, this upper bound can be achieved by suppressing the cross-correlation among reflected signals from different targets, which aligns with the principles of adaptive MIMO radar.","Then, we propose a multi-objective optimization framework based on the signal-to-interference-plus-noise ratio of each user and the tight upper bound of sensing MI, introducing the Pareto boundary to characterize the achievable communication-sensing performance boundary of the proposed ISAC system.","To achieve the Pareto boundary, the max-min system utility function method is employed, while considering the fairness between communication users and radar targets.","Subsequently, the bisection search method is employed to find a specific Pareto optimal solution by solving a series of convex feasible problems.","Finally, simulation results validate that the proposed method achieves a better tradeoff between multi-user communication and multi-target sensing performance.","Additionally, utilizing the tight upper bound of sensing MI as a performance metric can enhance the multi-target resolution capability and angle estimation accuracy."],"url":"http://arxiv.org/abs/2405.09022v1","category":"eess.SP"}
{"created":"2024-05-15 01:22:30","title":"Deep Learning in Earthquake Engineering: A Comprehensive Review","abstract":"This article surveys the growing interest in utilizing Deep Learning (DL) as a powerful tool to address challenging problems in earthquake engineering. Despite decades of advancement in domain knowledge, issues such as uncertainty in earthquake occurrence, unpredictable seismic loads, nonlinear structural responses, and community engagement remain difficult to tackle using domain-specific methods. DL offers promising solutions by leveraging its data-driven capacity for nonlinear mapping, sequential data modeling, automatic feature extraction, dimensionality reduction, optimal decision-making, etc. However, the literature lacks a comprehensive review that systematically covers a consistent scope intersecting DL and earthquake engineering. To bridge the gap, the article first discusses methodological advances to elucidate various applicable DL techniques, such as multi-layer perceptron (MLP), convolutional neural network (CNN), recurrent neural network (RNN), generative adversarial network (GAN), autoencoder (AE), transfer learning (TL), reinforcement learning (RL), and graph neural network (GNN). A thorough research landscape is then disclosed by exploring various DL applications across different research topics, including vision-based seismic damage assessment and structural characterization, seismic demand and damage state prediction, seismic response history prediction, regional seismic risk assessment and community resilience, ground motion (GM) for engineering use, seismic response control, and the inverse problem of system/damage identification. Suitable DL techniques for each research topic are identified, emphasizing the preeminence of CNN for vision-based tasks, RNN for sequential data, RL for community resilience, and unsupervised learning for GM analysis. The article also discusses opportunities and challenges for leveraging DL in earthquake engineering research and practice.","sentences":["This article surveys the growing interest in utilizing Deep Learning (DL) as a powerful tool to address challenging problems in earthquake engineering.","Despite decades of advancement in domain knowledge, issues such as uncertainty in earthquake occurrence, unpredictable seismic loads, nonlinear structural responses, and community engagement remain difficult to tackle using domain-specific methods.","DL offers promising solutions by leveraging its data-driven capacity for nonlinear mapping, sequential data modeling, automatic feature extraction, dimensionality reduction, optimal decision-making, etc.","However, the literature lacks a comprehensive review that systematically covers a consistent scope intersecting DL and earthquake engineering.","To bridge the gap, the article first discusses methodological advances to elucidate various applicable DL techniques, such as multi-layer perceptron (MLP), convolutional neural network (CNN), recurrent neural network (RNN), generative adversarial network (GAN), autoencoder (AE), transfer learning (TL), reinforcement learning (RL), and graph neural network (GNN).","A thorough research landscape is then disclosed by exploring various DL applications across different research topics, including vision-based seismic damage assessment and structural characterization, seismic demand and damage state prediction, seismic response history prediction, regional seismic risk assessment and community resilience, ground motion (GM) for engineering use, seismic response control, and the inverse problem of system/damage identification.","Suitable DL techniques for each research topic are identified, emphasizing the preeminence of CNN for vision-based tasks, RNN for sequential data, RL for community resilience, and unsupervised learning for GM analysis.","The article also discusses opportunities and challenges for leveraging DL in earthquake engineering research and practice."],"url":"http://arxiv.org/abs/2405.09021v1","category":"cs.LG"}
{"created":"2024-05-15 01:15:10","title":"Scalable single-microring hybrid III-V/Si lasers for emerging narrow-linewidth applications","abstract":"Silicon photonics, compatible with large-scale silicon manufacturing, is a disruptive photonic platform that has indicated significant implications in industry and research areas (e.g., quantum, neuromorphic computing, LiDAR). Cutting-edge applications such as high-capacity coherent optical communication and heterodyne LiDAR have escalated the demand for integrated narrow-linewidth laser sources. To that effect, this work seeks to address this requirement through the development of a high-performance hybrid III-V/silicon laser. The developed integrated laser, utilizes a single microring resonator (MRR), demonstrating single-mode operation with a side mode suppression ratio (SMSR) exceeding 40 dB, with laser output power as high as 16.4 mW. Moving away from current hybrid/heterogeneous laser architectures that necessitate multiple complex control, the developed laser architecture requires only two control parameters. Importantly, this serves to streamline industrial adoption by reducing the complexity involved in characterizing these lasers, at-scale. Through the succinct structure and control framework, a narrow laser linewidth of 2.79 kHz and low relative intensity noise (RIN) of -135 dB/Hz are achieved. Furthermore, optical data transmission at 12.5 Gb/s is demonstrated where a signal-to-noise ratio (SNR) of 10 dB is measured.","sentences":["Silicon photonics, compatible with large-scale silicon manufacturing, is a disruptive photonic platform that has indicated significant implications in industry and research areas (e.g., quantum, neuromorphic computing, LiDAR).","Cutting-edge applications such as high-capacity coherent optical communication and heterodyne LiDAR have escalated the demand for integrated narrow-linewidth laser sources.","To that effect, this work seeks to address this requirement through the development of a high-performance hybrid III-V/silicon laser.","The developed integrated laser, utilizes a single microring resonator (MRR), demonstrating single-mode operation with a side mode suppression ratio (SMSR) exceeding 40 dB, with laser output power as high as 16.4 mW. Moving away from current hybrid/heterogeneous laser architectures that necessitate multiple complex control, the developed laser architecture requires only two control parameters.","Importantly, this serves to streamline industrial adoption by reducing the complexity involved in characterizing these lasers, at-scale.","Through the succinct structure and control framework, a narrow laser linewidth of 2.79 kHz and low relative intensity noise (RIN) of -135 dB/Hz are achieved.","Furthermore, optical data transmission at 12.5 Gb/s is demonstrated where a signal-to-noise ratio (SNR) of 10 dB is measured."],"url":"http://arxiv.org/abs/2405.09020v1","category":"physics.optics"}
{"created":"2024-05-15 00:51:52","title":"IoT-enabled Stability Chamber for the Pharmaceutical Industry","abstract":"A stability chamber is a critical piece of equipment for any pharmaceutical facility to retain the manufactured product for testing the stability and quality of the products over a certain period of time by keeping the products in different sets of environmental conditions. In this paper, we proposed an IoT-enabled stability chamber for the pharmaceutical industry. We developed four stability chambers by using the existing utilities of a manufacturing facility. The state-of-the-art automatic PID controlling system of Siemens S7-1200 PLC was used to control each chamber. Seven precise temperature and humidity sensors were used to monitor the environment of each chamber. PC-based Siemens WinCC Runtime Advanced visualization platform was used to visualize the data of the chamber which is FDA 21 CFR Part 11 Compliant. Sensor data of the chamber are stored in the database in a periodic manner and also have report generation features. This chamber also has an alarm management system. The critical alarms are automatically emailed to the user to take action. Additionally, an Internet of Things-based (IoT-based) application was also developed to monitor the sensor's data remotely using any client application.","sentences":["A stability chamber is a critical piece of equipment for any pharmaceutical facility to retain the manufactured product for testing the stability and quality of the products over a certain period of time by keeping the products in different sets of environmental conditions.","In this paper, we proposed an IoT-enabled stability chamber for the pharmaceutical industry.","We developed four stability chambers by using the existing utilities of a manufacturing facility.","The state-of-the-art automatic PID controlling system of Siemens S7-1200 PLC was used to control each chamber.","Seven precise temperature and humidity sensors were used to monitor the environment of each chamber.","PC-based Siemens WinCC Runtime Advanced visualization platform was used to visualize the data of the chamber which is FDA 21 CFR Part 11 Compliant.","Sensor data of the chamber are stored in the database in a periodic manner and also have report generation features.","This chamber also has an alarm management system.","The critical alarms are automatically emailed to the user to take action.","Additionally, an Internet of Things-based (IoT-based) application was also developed to monitor the sensor's data remotely using any client application."],"url":"http://arxiv.org/abs/2405.09016v1","category":"eess.SY"}
{"created":"2024-05-15 00:43:42","title":"Phase diagram of strongly-coupled Rashba systems","abstract":"Motivated by the recent discovery of a possible field-mediated parity switch within the superconducting state of CeRh2As2 [Khim et al., Science 373, 1012 (2021)], we thoroughly investigate the dependence of the superconducting state of a strongly-coupled Rashba mono- and bilayer on internal parameters and an applied magnetic field. The role of interlayer pairing, spin orbit coupling, doping rate and applied magnetic field and their interplay was examined numerically at low temperature within a t-J-like model, uncovering complex phase diagrams and transitions between superconducting states with different symmetry.","sentences":["Motivated by the recent discovery of a possible field-mediated parity switch within the superconducting state of CeRh2As2 [Khim et al., Science 373, 1012 (2021)], we thoroughly investigate the dependence of the superconducting state of a strongly-coupled Rashba mono- and bilayer on internal parameters and an applied magnetic field.","The role of interlayer pairing, spin orbit coupling, doping rate and applied magnetic field and their interplay was examined numerically at low temperature within a t-J-like model, uncovering complex phase diagrams and transitions between superconducting states with different symmetry."],"url":"http://arxiv.org/abs/2405.09015v1","category":"cond-mat.supr-con"}
{"created":"2024-05-15 00:43:19","title":"Feature-based Federated Transfer Learning: Communication Efficiency, Robustness and Privacy","abstract":"In this paper, we propose feature-based federated transfer learning as a novel approach to improve communication efficiency by reducing the uplink payload by multiple orders of magnitude compared to that of existing approaches in federated learning and federated transfer learning. Specifically, in the proposed feature-based federated learning, we design the extracted features and outputs to be uploaded instead of parameter updates. For this distributed learning model, we determine the required payload and provide comparisons with the existing schemes. Subsequently, we analyze the robustness of feature-based federated transfer learning against packet loss, data insufficiency, and quantization. Finally, we address privacy considerations by defining and analyzing label privacy leakage and feature privacy leakage, and investigating mitigating approaches. For all aforementioned analyses, we evaluate the performance of the proposed learning scheme via experiments on an image classification task and a natural language processing task to demonstrate its effectiveness.","sentences":["In this paper, we propose feature-based federated transfer learning as a novel approach to improve communication efficiency by reducing the uplink payload by multiple orders of magnitude compared to that of existing approaches in federated learning and federated transfer learning.","Specifically, in the proposed feature-based federated learning, we design the extracted features and outputs to be uploaded instead of parameter updates.","For this distributed learning model, we determine the required payload and provide comparisons with the existing schemes.","Subsequently, we analyze the robustness of feature-based federated transfer learning against packet loss, data insufficiency, and quantization.","Finally, we address privacy considerations by defining and analyzing label privacy leakage and feature privacy leakage, and investigating mitigating approaches.","For all aforementioned analyses, we evaluate the performance of the proposed learning scheme via experiments on an image classification task and a natural language processing task to demonstrate its effectiveness."],"url":"http://arxiv.org/abs/2405.09014v1","category":"cs.LG"}
{"created":"2024-05-15 00:33:13","title":"An analogue of Koebe's theorem and the openness of a limit map in one class","abstract":"We study mappings that satisfy the inverse modulus inequality of Poletsky type in a fixed domain. It is shown that, under some additional restrictions, the image of a ball under such mappings contains a fixed ball uniformly over the class. This statement can be interpreted as the well-known analogue of Koebe's theorem for analytic functions. As an application of the obtained result, we show that, if a sequence of mappings belonging to the specified class converges locally uniformly, then the limit mapping is open.","sentences":["We study mappings that satisfy the inverse modulus inequality of Poletsky type in a fixed domain.","It is shown that, under some additional restrictions, the image of a ball under such mappings contains a fixed ball uniformly over the class.","This statement can be interpreted as the well-known analogue of Koebe's theorem for analytic functions.","As an application of the obtained result, we show that, if a sequence of mappings belonging to the specified class converges locally uniformly, then the limit mapping is open."],"url":"http://arxiv.org/abs/2405.09012v1","category":"math.CV"}
{"created":"2024-05-15 00:31:01","title":"Symmetric-Difference (Degeneracy) and Signed Tree Models","abstract":"We introduce a dense counterpart of graph degeneracy, which extends the recently-proposed invariant symmetric difference. We say that a graph has sd-degeneracy (for symmetric-difference degeneracy) at most $d$ if it admits an elimination order of its vertices where a vertex $u$ can be removed whenever it has a $d$-twin, i.e., another vertex $v$ such that at most $d$ vertices outside $\\{u,v\\}$ are neighbors of exactly one of $u, v$. The family of graph classes of bounded sd-degeneracy is a superset of that of graph classes of bounded degeneracy or of bounded flip-width, and more generally, of bounded symmetric difference. Unlike most graph parameters, sd-degeneracy is not hereditary: it may be strictly smaller on a graph than on some of its induced subgraphs. In particular, every $n$-vertex graph is an induced subgraph of some $O(n^2)$-vertex graph of sd-degeneracy 1. In spite of this and the breadth of classes of bounded sd-degeneracy, we devise $\\tilde{O}(\\sqrt n)$-bit adjacency labeling schemes for them, which are optimal up to the hidden polylogarithmic factor. This is attained on some even more general classes, consisting of graphs $G$ whose vertices bijectively map to the leaves of a tree $T$, where transversal edges and anti-edges added to $T$ define the edge set of $G$. We call such graph representations signed tree models as they extend the so-called tree models (or twin-decompositions) developed in the context of twin-width, by adding transversal anti-edges. While computing the degeneracy of an input graph can be done in linear time, we show that deciding whether its symmetric difference is at most 8 is co-NP-complete, and whether its sd-degeneracy is at most 1 is NP-complete.","sentences":["We introduce a dense counterpart of graph degeneracy, which extends the recently-proposed invariant symmetric difference.","We say that a graph has sd-degeneracy (for symmetric-difference degeneracy) at most $d$ if it admits an elimination order of its vertices where a vertex $u$ can be removed whenever it has a $d$-twin, i.e., another vertex $v$ such that at most $d$ vertices outside $\\{u,v\\}$ are neighbors of exactly one of $u, v$.","The family of graph classes of bounded sd-degeneracy is a superset of that of graph classes of bounded degeneracy or of bounded flip-width, and more generally, of bounded symmetric difference.","Unlike most graph parameters, sd-degeneracy is not hereditary: it may be strictly smaller on a graph than on some of its induced subgraphs.","In particular, every $n$-vertex graph is an induced subgraph of some $O(n^2)$-vertex graph of sd-degeneracy 1.","In spite of this and the breadth of classes of bounded sd-degeneracy, we devise $\\tilde{O}(\\sqrt n)$-bit adjacency labeling schemes for them, which are optimal up to the hidden polylogarithmic factor.","This is attained on some even more general classes, consisting of graphs $G$ whose vertices bijectively map to the leaves of a tree $T$, where transversal edges and anti-edges added to $T$ define the edge set of $G$. We call such graph representations signed tree models as they extend the so-called tree models (or twin-decompositions) developed in the context of twin-width, by adding transversal anti-edges.","While computing the degeneracy of an input graph can be done in linear time, we show that deciding whether its symmetric difference is at most 8 is co-NP-complete, and whether its sd-degeneracy is at most 1 is NP-complete."],"url":"http://arxiv.org/abs/2405.09011v1","category":"cs.DS"}
{"created":"2024-05-15 00:30:10","title":"On Low Field Size Constructions of Access-Optimal Convertible Codes","abstract":"Most large-scale storage systems employ erasure coding to provide resilience against disk failures. Recent work has shown that tuning this redundancy to changes in disk failure rates leads to substantial storage savings. This process requires code conversion, wherein data encoded using an $[n^{I\\mskip-2mu},k^{I\\mskip-2mu}]$ initial code has to be transformed into data encoded using an $[n^{F\\mskip-2mu},k^{F\\mskip-2mu}]$ final code, a resource-intensive operation. Convertible codes are a class of codes that enable efficient code conversion while maintaining other desirable properties. In this paper, we focus on the access cost of conversion (total number of code symbols accessed in the conversion process) and on an important subclass of conversions known as the merge regime (combining multiple initial codewords into a single final codeword).   In this setting, explicit constructions are known for systematic access-optimal Maximum Distance Separable (MDS) convertible codes for all parameters in the merge regime. However, the existing construction for a key subset of these parameters, which makes use of Vandermonde parity matrices, requires a large field size making it unsuitable for practical applications. In this paper, we provide (1) sharper bounds on the minimum field size requirement for such codes, and (2) explicit constructions for low field sizes for several parameter ranges. In doing so, we provide a proof of super-regularity of specially designed classes of Vandermonde matrices that could be of independent interest.","sentences":["Most large-scale storage systems employ erasure coding to provide resilience against disk failures.","Recent work has shown that tuning this redundancy to changes in disk failure rates leads to substantial storage savings.","This process requires code conversion, wherein data encoded using an $[n^{I\\mskip-2mu},k^{I\\mskip-2mu}]$ initial code has to be transformed into data encoded using an $[n^{F\\mskip-2mu},k^{F\\mskip-2mu}]$ final code, a resource-intensive operation.","Convertible codes are a class of codes that enable efficient code conversion while maintaining other desirable properties.","In this paper, we focus on the access cost of conversion (total number of code symbols accessed in the conversion process) and on an important subclass of conversions known as the merge regime (combining multiple initial codewords into a single final codeword).   ","In this setting, explicit constructions are known for systematic access-optimal Maximum Distance Separable (MDS) convertible codes for all parameters in the merge regime.","However, the existing construction for a key subset of these parameters, which makes use of Vandermonde parity matrices, requires a large field size making it unsuitable for practical applications.","In this paper, we provide (1) sharper bounds on the minimum field size requirement for such codes, and (2) explicit constructions for low field sizes for several parameter ranges.","In doing so, we provide a proof of super-regularity of specially designed classes of Vandermonde matrices that could be of independent interest."],"url":"http://arxiv.org/abs/2405.09010v1","category":"cs.IT"}
{"created":"2024-05-15 00:19:57","title":"Hyperbolicity of renormalization of critical quasicircle maps","abstract":"There is a well developed renormalization theory of real analytic critical circle maps by de Faria, de Melo, and Yampolsky. In this paper, we extend Yampolsky's result on hyperbolicity of renormalization periodic points to a larger class of dynamical objects, namely critical quasicircle maps, i.e. analytic self homeomorphisms of a quasicircle with a single critical point. Unlike critical circle maps, the inner and outer criticalities of critical quasicircle maps can be distinct. We develop a compact analytic renormalization operator called Corona Renormalization with a hyperbolic fixed point whose stable manifold has codimension one and consists of critical quasicircle maps of the same criticality and periodic type rotation number. Our proof is an adaptation of Pacman Renormalization Theory for Siegel disks as well as rigidity results on the escaping dynamics of transcendental entire functions.","sentences":["There is a well developed renormalization theory of real analytic critical circle maps by de Faria, de Melo, and Yampolsky.","In this paper, we extend Yampolsky's result on hyperbolicity of renormalization periodic points to a larger class of dynamical objects, namely critical quasicircle maps, i.e. analytic self homeomorphisms of a quasicircle with a single critical point.","Unlike critical circle maps, the inner and outer criticalities of critical quasicircle maps can be distinct.","We develop a compact analytic renormalization operator called Corona Renormalization with a hyperbolic fixed point whose stable manifold has codimension one and consists of critical quasicircle maps of the same criticality and periodic type rotation number.","Our proof is an adaptation of Pacman Renormalization Theory for Siegel disks as well as rigidity results on the escaping dynamics of transcendental entire functions."],"url":"http://arxiv.org/abs/2405.09008v1","category":"math.DS"}
{"created":"2024-05-15 00:13:18","title":"Cons-training tensor networks","abstract":"In this study, we introduce a novel family of tensor networks, termed constrained matrix product states (MPS), designed to incorporate exactly arbitrary linear constraints into sparse block structures. These tensor networks effectively bridge the gap between U(1) symmetric MPS and traditional, unconstrained MPS. Central to our approach is the concept of a quantum region, an extension of quantum numbers traditionally used in symmetric tensor networks, adapted to capture any linear constraint, including the unconstrained scenario. We further develop canonical forms for these new MPS, which allow for the merging and factorization of tensor blocks according to quantum region fusion rules. Utilizing this canonical form, we apply an unsupervised training strategy to optimize arbitrary cost functions subject to linear constraints. We use this to solve the quadratic knapsack problem and show a superior performance against a leading nonlinear integer programming solver, highlighting the potential of our method in tackling complex constrained combinatorial optimization problems","sentences":["In this study, we introduce a novel family of tensor networks, termed constrained matrix product states (MPS), designed to incorporate exactly arbitrary linear constraints into sparse block structures.","These tensor networks effectively bridge the gap between U(1) symmetric MPS and traditional, unconstrained MPS.","Central to our approach is the concept of a quantum region, an extension of quantum numbers traditionally used in symmetric tensor networks, adapted to capture any linear constraint, including the unconstrained scenario.","We further develop canonical forms for these new MPS, which allow for the merging and factorization of tensor blocks according to quantum region fusion rules.","Utilizing this canonical form, we apply an unsupervised training strategy to optimize arbitrary cost functions subject to linear constraints.","We use this to solve the quadratic knapsack problem and show a superior performance against a leading nonlinear integer programming solver, highlighting the potential of our method in tackling complex constrained combinatorial optimization problems"],"url":"http://arxiv.org/abs/2405.09005v1","category":"math.NA"}
{"created":"2024-05-15 00:04:08","title":"Improving Sequential Market Clearing via Value-oriented Renewable Energy Forecasting","abstract":"Large penetration of renewable energy sources (RESs) brings huge uncertainty into the electricity markets. While existing deterministic market clearing fails to accommodate the uncertainty, the recently proposed stochastic market clearing struggles to achieve desirable market properties. In this work, we propose a value-oriented forecasting approach, which tactically determines the RESs generation that enters the day-ahead market. With such a forecast, the existing deterministic market clearing framework can be maintained, and the day-ahead and real-time overall operation cost is reduced. At the training phase, the forecast model parameters are estimated to minimize expected day-ahead and real-time overall operation costs, instead of minimizing forecast errors in a statistical sense. Theoretically, we derive the exact form of the loss function for training the forecast model that aligns with such a goal. For market clearing modeled by linear programs, this loss function is a piecewise linear function. Additionally, we derive the analytical gradient of the loss function with respect to the forecast, which inspires an efficient training strategy. A numerical study shows our forecasts can bring significant benefits of the overall cost reduction to deterministic market clearing, compared to quality-oriented forecasting approach.","sentences":["Large penetration of renewable energy sources (RESs) brings huge uncertainty into the electricity markets.","While existing deterministic market clearing fails to accommodate the uncertainty, the recently proposed stochastic market clearing struggles to achieve desirable market properties.","In this work, we propose a value-oriented forecasting approach, which tactically determines the RESs generation that enters the day-ahead market.","With such a forecast, the existing deterministic market clearing framework can be maintained, and the day-ahead and real-time overall operation cost is reduced.","At the training phase, the forecast model parameters are estimated to minimize expected day-ahead and real-time overall operation costs, instead of minimizing forecast errors in a statistical sense.","Theoretically, we derive the exact form of the loss function for training the forecast model that aligns with such a goal.","For market clearing modeled by linear programs, this loss function is a piecewise linear function.","Additionally, we derive the analytical gradient of the loss function with respect to the forecast, which inspires an efficient training strategy.","A numerical study shows our forecasts can bring significant benefits of the overall cost reduction to deterministic market clearing, compared to quality-oriented forecasting approach."],"url":"http://arxiv.org/abs/2405.09004v1","category":"eess.SY"}
{"created":"2024-05-14 23:58:01","title":"BEVRender: Vision-based Cross-view Vehicle Registration in Off-road GNSS-denied Environment","abstract":"We introduce BEVRender, a novel learning-based approach for the localization of ground vehicles in Global Navigation Satellite System (GNSS)-denied off-road scenarios. These environments are typically challenging for conventional vision-based state estimation due to the lack of distinct visual landmarks and the instability of vehicle poses. To address this, BEVRender generates high-quality local bird's eye view (BEV) images of the local terrain. Subsequently, these images are aligned with a geo-referenced aerial map via template-matching to achieve accurate cross-view registration. Our approach overcomes the inherent limitations of visual inertial odometry systems and the substantial storage requirements of image-retrieval localization strategies, which are susceptible to drift and scalability issues, respectively. Extensive experimentation validates BEVRender's advancement over existing GNSS-denied visual localization methods, demonstrating notable enhancements in both localization accuracy and update frequency. The code for BEVRender will be made available soon.","sentences":["We introduce BEVRender, a novel learning-based approach for the localization of ground vehicles in Global Navigation Satellite System (GNSS)-denied off-road scenarios.","These environments are typically challenging for conventional vision-based state estimation due to the lack of distinct visual landmarks and the instability of vehicle poses.","To address this, BEVRender generates high-quality local bird's eye view (BEV) images of the local terrain.","Subsequently, these images are aligned with a geo-referenced aerial map via template-matching to achieve accurate cross-view registration.","Our approach overcomes the inherent limitations of visual inertial odometry systems and the substantial storage requirements of image-retrieval localization strategies, which are susceptible to drift and scalability issues, respectively.","Extensive experimentation validates BEVRender's advancement over existing GNSS-denied visual localization methods, demonstrating notable enhancements in both localization accuracy and update frequency.","The code for BEVRender will be made available soon."],"url":"http://arxiv.org/abs/2405.09001v1","category":"cs.RO"}
{"created":"2024-05-14 23:41:08","title":"Motility Modulates the Partitioning of Bacteria in Aqueous Two-Phase Systems","abstract":"We study the partitioning of motile bacteria into different thermodynamic states of a phase-separated dextran (DEX)/polyethylene glycol (PEG) aqueous mixture. While non-motile bacteria partition exclusively into the DEX-rich phase in all conditions tested, we observed that motile bacteria transverse the soft DEX/PEG interface and partition variably among the two phases. For our model organism \\textit{Bacillus subtilis}, the fraction of motile bacteria in the DEX-rich phase increased from 0.58 to 1 as we increased DEX composition within the two-phase region. We hypothesized that the chemical affinity between dextran and the bacteria cell wall acts to weakly confine the bacteria within the DEX-rich phase; however, motility can generate sufficient mechanical forces to overcome the soft confinement and propel the bacteria into the PEG-rich phase. Using optical tweezers to drag a bacterium across the DEX/PEG interface, we developed a model to predict the overall phase partitioning based on a competition between the interfacial forces and bacterial propulsive forces. Our measurements are supported by a theoretical model of dilute active rods embedded within a periodic soft confinement potential.","sentences":["We study the partitioning of motile bacteria into different thermodynamic states of a phase-separated dextran (DEX)/polyethylene glycol (PEG) aqueous mixture.","While non-motile bacteria partition exclusively into the DEX-rich phase in all conditions tested, we observed that motile bacteria transverse the soft DEX/PEG interface and partition variably among the two phases.","For our model organism \\textit{Bacillus subtilis}, the fraction of motile bacteria in the DEX-rich phase increased from 0.58 to 1 as we increased DEX composition within the two-phase region.","We hypothesized that the chemical affinity between dextran and the bacteria cell wall acts to weakly confine the bacteria within the DEX-rich phase; however, motility can generate sufficient mechanical forces to overcome the soft confinement and propel the bacteria into the PEG-rich phase.","Using optical tweezers to drag a bacterium across the DEX/PEG interface, we developed a model to predict the overall phase partitioning based on a competition between the interfacial forces and bacterial propulsive forces.","Our measurements are supported by a theoretical model of dilute active rods embedded within a periodic soft confinement potential."],"url":"http://arxiv.org/abs/2405.08995v1","category":"cond-mat.soft"}
{"created":"2024-05-14 23:26:49","title":"Knots of low knot Floer width","abstract":"This paper classifies the chain homotopy equivalence types of knot Floer complexes $CFK_{\\mathbb{F}[U,V]}(K)$ of knot Floer width 2. They have no nontrivial local systems. As an application, this shows that all Montesinos knots admit a basis that can be simultaneously horizontally and vertically simplified.","sentences":["This paper classifies the chain homotopy equivalence types of knot Floer complexes $CFK_{\\mathbb{F}[U,V]}(K)$ of knot Floer width 2.","They have no nontrivial local systems.","As an application, this shows that all Montesinos knots admit a basis that can be simultaneously horizontally and vertically simplified."],"url":"http://arxiv.org/abs/2405.08993v1","category":"math.GT"}
{"created":"2024-05-14 23:03:03","title":"Containment Problem for Deterministic Multicounter Machine Models","abstract":"There are many types of automata and grammar models that have been studied in the literature, and for these models, it is common to determine whether certain problems are decidable. One problem that has been difficult to answer throughout the history of automata and formal language theory is to decide whether a given system $M$ accepts a bounded language (whether there exist words $w_1, \\ldots,w_k$ such that $L(M) \\subseteq w_1^* \\cdots w_k^*$?). Boundedness was only known to be decidable for regular and context-free languages until recently when it was shown to also be decidable for finite automata and pushdown automata augmented with reversal-bounded counters, and for vector addition systems with states. However, decidability of this problem has still gone unanswered for the majority of automata/grammar models with a decidable emptiness problem that have been studied in the literature.   In this paper, we develop new techniques to show that the boundedness problem is decidable for larger classes of one-way nondeterministic automata and grammar models by reducing the problem to the decidability of boundedness for simpler classes of automata. One technique involves characterizing the models in terms of multi-tape automata. We give new characterizations of finite-turn Turing machines, finite-turn Turing machines augmented with various storage structures (like a pushdown, multiple reversal-bounded counters, partially-blind counters, etc.), and simple matrix grammars. The characterizations are then used to show that the boundedness problem for these models is decidable. Another technique uses the concept of the store language of an automaton. This is used to show that the boundedness problem is decidable for pushdown automata that can \"flip\" their pushdown a bounded number of times. Boundedness remains decidable even if we augment this device with additional stores.","sentences":["There are many types of automata and grammar models that have been studied in the literature, and for these models, it is common to determine whether certain problems are decidable.","One problem that has been difficult to answer throughout the history of automata and formal language theory is to decide whether a given system $M$ accepts a bounded language (whether there exist words $w_1, \\ldots,w_k$ such that $L(M)","\\subseteq w_1^* \\cdots w_k^*$?).","Boundedness was only known to be decidable for regular and context-free languages until recently when it was shown to also be decidable for finite automata and pushdown automata augmented with reversal-bounded counters, and for vector addition systems with states.","However, decidability of this problem has still gone unanswered for the majority of automata/grammar models with a decidable emptiness problem that have been studied in the literature.   ","In this paper, we develop new techniques to show that the boundedness problem is decidable for larger classes of one-way nondeterministic automata and grammar models by reducing the problem to the decidability of boundedness for simpler classes of automata.","One technique involves characterizing the models in terms of multi-tape automata.","We give new characterizations of finite-turn Turing machines, finite-turn Turing machines augmented with various storage structures (like a pushdown, multiple reversal-bounded counters, partially-blind counters, etc.), and simple matrix grammars.","The characterizations are then used to show that the boundedness problem for these models is decidable.","Another technique uses the concept of the store language of an automaton.","This is used to show that the boundedness problem is decidable for pushdown automata that can \"flip\" their pushdown a bounded number of times.","Boundedness remains decidable even if we augment this device with additional stores."],"url":"http://arxiv.org/abs/2405.08988v1","category":"cs.FL"}
{"created":"2024-05-14 22:59:22","title":"A practical guide to light-sheet microscopy for nanoscale imaging: Looking beyond the cell","abstract":"We present a comprehensive guide to light-sheet microscopy (LSM) to assist scientists in navigating the practical implementation of this microscopy technique. Emphasizing the applicability of LSM to image both static microscale and nanoscale features, as well as diffusion dynamics, we present the fundamental concepts of microscopy, progressing through beam profile considerations, to image reconstruction. We outline key practical decisions in constructing a home-built system and provide insight into the alignment and calibration processes. We briefly discuss the conditions necessary for constructing a continuous 3D image and introduce our home-built code for data analysis. By providing this guide, we aim to alleviate the challenges associated with designing and constructing LSM systems and offer scientists new to LSM a valuable resource in navigating this complex field.","sentences":["We present a comprehensive guide to light-sheet microscopy (LSM) to assist scientists in navigating the practical implementation of this microscopy technique.","Emphasizing the applicability of LSM to image both static microscale and nanoscale features, as well as diffusion dynamics, we present the fundamental concepts of microscopy, progressing through beam profile considerations, to image reconstruction.","We outline key practical decisions in constructing a home-built system and provide insight into the alignment and calibration processes.","We briefly discuss the conditions necessary for constructing a continuous 3D image and introduce our home-built code for data analysis.","By providing this guide, we aim to alleviate the challenges associated with designing and constructing LSM systems and offer scientists new to LSM a valuable resource in navigating this complex field."],"url":"http://arxiv.org/abs/2405.08987v1","category":"physics.optics"}
{"created":"2024-05-14 22:55:42","title":"Optimal control of perturbed sweeping processes with applications to general robotics models","abstract":"This paper primarily focuses on the practical applications of optimal control theory for perturbed sweeping processes within the realm of robotics dynamics. By describing these models as controlled sweeping processes with pointwise control and state constraints and by employing necessary optimality conditions for such systems, we formulate optimal control problems suitable to these models and develop numerical algorithms for their solving. Subsequently, we use the Python Dynamic Optimization library GEKKO to simulate solutions to the posed robotics problems in the case of any fixed number of robots under different initial conditions.","sentences":["This paper primarily focuses on the practical applications of optimal control theory for perturbed sweeping processes within the realm of robotics dynamics.","By describing these models as controlled sweeping processes with pointwise control and state constraints and by employing necessary optimality conditions for such systems, we formulate optimal control problems suitable to these models and develop numerical algorithms for their solving.","Subsequently, we use the Python Dynamic Optimization library GEKKO to simulate solutions to the posed robotics problems in the case of any fixed number of robots under different initial conditions."],"url":"http://arxiv.org/abs/2405.08986v1","category":"math.OC"}
{"created":"2024-05-14 22:38:10","title":"Charge-Transfer Hyperbolic Polaritons in $\u03b1$-MoO$_3$/graphene heterostructures","abstract":"Charge transfer is a fundamental interface process that can be harnessed for light detection, photovoltaics, and photosynthesis. Recently, charge transfer was exploited in nanophotonics to alter plasmon polaritons by involving additional non-polaritonic materials to activate the charge transfer. Yet, direct charge transfer between polaritonic materials hasn't been demonstrated. We report the direct charge transfer in pure polaritonic van der Waals (vdW) heterostructures of $\\alpha$-MoO$_3$/graphene. We extracted the Fermi energy of 0.6 eV for graphene by infrared nano-imaging of charge transfer hyperbolic polaritons in the vdW heterostructure. This unusually high Fermi energy is attributed to the charge transfer between graphene and $\\alpha$-MoO$_3$. Moreover, we have observed charge transfer hyperbolic polaritons in multiple energy-momentum dispersion branches with a wavelength elongation of up to 150%. With support from the DFT calculation, we find that the charge transfer between graphene and $\\alpha$-MoO$_3$, absent in mechanically assembled vdW heterostructures, is attributed to the relatively pristine heterointerface preserved in the epitaxially grown vdW heterostructure. The direct charge transfer and charge transfer hyperbolic polaritons demonstrated in our work hold great promise for developing nano-optical circuits, computational devices, communication systems, and light and energy manipulation devices.","sentences":["Charge transfer is a fundamental interface process that can be harnessed for light detection, photovoltaics, and photosynthesis.","Recently, charge transfer was exploited in nanophotonics to alter plasmon polaritons by involving additional non-polaritonic materials to activate the charge transfer.","Yet, direct charge transfer between polaritonic materials hasn't been demonstrated.","We report the direct charge transfer in pure polaritonic van der Waals (vdW) heterostructures of $\\alpha$-MoO$_3$/graphene.","We extracted the Fermi energy of 0.6 eV for graphene by infrared nano-imaging of charge transfer hyperbolic polaritons in the vdW heterostructure.","This unusually high Fermi energy is attributed to the charge transfer between graphene and $\\alpha$-MoO$_3$. Moreover, we have observed charge transfer hyperbolic polaritons in multiple energy-momentum dispersion branches with a wavelength elongation of up to 150%.","With support from the DFT calculation, we find that the charge transfer between graphene and $\\alpha$-MoO$_3$, absent in mechanically assembled vdW heterostructures, is attributed to the relatively pristine heterointerface preserved in the epitaxially grown vdW heterostructure.","The direct charge transfer and charge transfer hyperbolic polaritons demonstrated in our work hold great promise for developing nano-optical circuits, computational devices, communication systems, and light and energy manipulation devices."],"url":"http://arxiv.org/abs/2405.08984v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-15 17:31:15","title":"On Semi-supervised Estimation of Discrete Distributions under f-divergences","abstract":"We study the problem of estimating the joint probability mass function (pmf) over two random variables. In particular, the estimation is based on the observation of $m$ samples containing both variables and $n$ samples missing one fixed variable. We adopt the minimax framework with $l^p_p$ loss functions. Recent work established that univariate minimax estimator combinations achieve minimax risk with the optimal first-order constant for $p \\ge 2$ in the regime $m = o(n)$, questions remained for $p \\le 2$ and various $f$-divergences. In our study, we affirm that these composite estimators are indeed minimax optimal for $l^p_p$ loss functions, specifically for the range $1 \\le p \\le 2$, including the critical $l_1$ loss. Additionally, we ascertain their optimality for a suite of $f$-divergences, such as KL, $\\chi^2$, Squared Hellinger, and Le Cam divergences.","sentences":["We study the problem of estimating the joint probability mass function (pmf) over two random variables.","In particular, the estimation is based on the observation of $m$ samples containing both variables and $n$ samples missing one fixed variable.","We adopt the minimax framework with $l^p_p$ loss functions.","Recent work established that univariate minimax estimator combinations achieve minimax risk with the optimal first-order constant for $p \\ge 2$ in the regime $m","=","o(n)$, questions remained for $p \\le 2$ and various $f$-divergences.","In our study, we affirm that these composite estimators are indeed minimax optimal for $l^p_p$ loss functions, specifically for the range $1 \\le p \\le 2$, including the critical $l_1$ loss.","Additionally, we ascertain their optimality for a suite of $f$-divergences, such as KL, $\\chi^2$, Squared Hellinger, and Le Cam divergences."],"url":"http://arxiv.org/abs/2405.09523v1","category":"math.ST"}
{"created":"2024-05-15 17:25:59","title":"ContourCraft: Learning to Resolve Intersections in Neural Multi-Garment Simulations","abstract":"Learning-based approaches to cloth simulation have started to show their potential in recent years. However, handling collisions and intersections in neural simulations remains a largely unsolved problem. In this work, we present \\moniker{}, a learning-based solution for handling intersections in neural cloth simulations. Unlike conventional approaches that critically rely on intersection-free inputs, \\moniker{} robustly recovers from intersections introduced through missed collisions, self-penetrating bodies, or errors in manually designed multi-layer outfits. The technical core of \\moniker{} is a novel intersection contour loss that penalizes interpenetrations and encourages rapid resolution thereof. We integrate our intersection loss with a collision-avoiding repulsion objective into a neural cloth simulation method based on graph neural networks (GNNs). We demonstrate our method's ability across a challenging set of diverse multi-layer outfits under dynamic human motions. Our extensive analysis indicates that \\moniker{} significantly improves collision handling for learned simulation and produces visually compelling results.","sentences":["Learning-based approaches to cloth simulation have started to show their potential in recent years.","However, handling collisions and intersections in neural simulations remains a largely unsolved problem.","In this work, we present \\moniker{}, a learning-based solution for handling intersections in neural cloth simulations.","Unlike conventional approaches that critically rely on intersection-free inputs, \\moniker{} robustly recovers from intersections introduced through missed collisions, self-penetrating bodies, or errors in manually designed multi-layer outfits.","The technical core of \\moniker{} is a novel intersection contour loss that penalizes interpenetrations and encourages rapid resolution thereof.","We integrate our intersection loss with a collision-avoiding repulsion objective into a neural cloth simulation method based on graph neural networks (GNNs).","We demonstrate our method's ability across a challenging set of diverse multi-layer outfits under dynamic human motions.","Our extensive analysis indicates that \\moniker{} significantly improves collision handling for learned simulation and produces visually compelling results."],"url":"http://arxiv.org/abs/2405.09522v1","category":"cs.GR"}
{"created":"2024-05-15 17:23:28","title":"Edwards-Wilkinson fluctuations in subcritical 2D stochastic heat equations","abstract":"We study 2D nonlinear stochastic heat equations under a logarithmically attenuated white-noise limit with subcritical coupling. We show that solutions asymptotically exhibit Edwards-Wilkinson fluctuations. This extends work of Ran Tao, which required a stricter condition on the coupling. Part of the limiting fluctuation is measurable with respect to the original noise and the remainder is independent.","sentences":["We study 2D nonlinear stochastic heat equations under a logarithmically attenuated white-noise limit with subcritical coupling.","We show that solutions asymptotically exhibit Edwards-Wilkinson fluctuations.","This extends work of Ran Tao, which required a stricter condition on the coupling.","Part of the limiting fluctuation is measurable with respect to the original noise and the remainder is independent."],"url":"http://arxiv.org/abs/2405.09520v1","category":"math.PR"}
{"created":"2024-05-15 16:22:46","title":"DemOpts: Fairness corrections in COVID-19 case prediction models","abstract":"COVID-19 forecasting models have been used to inform decision making around resource allocation and intervention decisions e.g., hospital beds or stay-at-home orders. State of the art deep learning models often use multimodal data such as mobility or socio-demographic data to enhance COVID-19 case prediction models. Nevertheless, related work has revealed under-reporting bias in COVID-19 cases as well as sampling bias in mobility data for certain minority racial and ethnic groups, which could in turn affect the fairness of the COVID-19 predictions along race labels. In this paper, we show that state of the art deep learning models output mean prediction errors that are significantly different across racial and ethnic groups; and which could, in turn, support unfair policy decisions. We also propose a novel de-biasing method, DemOpts, to increase the fairness of deep learning based forecasting models trained on potentially biased datasets. Our results show that DemOpts can achieve better error parity that other state of the art de-biasing approaches, thus effectively reducing the differences in the mean error distributions across more racial and ethnic groups.","sentences":["COVID-19 forecasting models have been used to inform decision making around resource allocation and intervention decisions e.g., hospital beds or stay-at-home orders.","State of the art deep learning models often use multimodal data such as mobility or socio-demographic data to enhance COVID-19 case prediction models.","Nevertheless, related work has revealed under-reporting bias in COVID-19 cases as well as sampling bias in mobility data for certain minority racial and ethnic groups, which could in turn affect the fairness of the COVID-19 predictions along race labels.","In this paper, we show that state of the art deep learning models output mean prediction errors that are significantly different across racial and ethnic groups; and which could, in turn, support unfair policy decisions.","We also propose a novel de-biasing method, DemOpts, to increase the fairness of deep learning based forecasting models trained on potentially biased datasets.","Our results show that DemOpts can achieve better error parity that other state of the art de-biasing approaches, thus effectively reducing the differences in the mean error distributions across more racial and ethnic groups."],"url":"http://arxiv.org/abs/2405.09483v1","category":"cs.LG"}
{"created":"2024-05-15 16:10:27","title":"Advanced surrogate model for electron-scale turbulence in tokamak pedestals","abstract":"We derive an advanced surrogate model for predicting turbulent transport in the edge of tokamaks driven by electron temperature gradient (ETG) modes. Our derivation is based on a recently developed sensitivity-driven sparse grid interpolation approach for uncertainty quantification and sensitivity analysis at scale, which informs the set of parameters that define the surrogate model as a scaling law. Our model reveals that ETG-driven electron heat flux is influenced by the safety factor $q$, electron beta $\\beta_e$, and normalized electron Debye length $\\lambda_D$, in addition to well established parameters such as the electron temperature and density gradients. To assess the trustworthiness of our model's predictions beyond training, we compute prediction intervals using bootstrapping. The surrogate model's predictive power is tested across a wide range of parameter values, including within-distribution testing parameters (to verify our model) as well as out-of-bounds and out-of-distribution testing (to validate the proposed model). Overall, validation efforts show that our model competes well with, or even outperforms, existing scaling laws in predicting ETG-driven transport.","sentences":["We derive an advanced surrogate model for predicting turbulent transport in the edge of tokamaks driven by electron temperature gradient (ETG) modes.","Our derivation is based on a recently developed sensitivity-driven sparse grid interpolation approach for uncertainty quantification and sensitivity analysis at scale, which informs the set of parameters that define the surrogate model as a scaling law.","Our model reveals that ETG-driven electron heat flux is influenced by the safety factor $q$, electron beta $\\beta_e$, and normalized electron Debye length $\\lambda_D$, in addition to well established parameters such as the electron temperature and density gradients.","To assess the trustworthiness of our model's predictions beyond training, we compute prediction intervals using bootstrapping.","The surrogate model's predictive power is tested across a wide range of parameter values, including within-distribution testing parameters (to verify our model) as well as out-of-bounds and out-of-distribution testing (to validate the proposed model).","Overall, validation efforts show that our model competes well with, or even outperforms, existing scaling laws in predicting ETG-driven transport."],"url":"http://arxiv.org/abs/2405.09474v1","category":"physics.plasm-ph"}
{"created":"2024-05-15 15:48:08","title":"Anderson Impurity Mechanism for a Multi-Level Model in $\u03b4$-Pu","abstract":"Electronic correlations and spin-orbit interactions in plutonium create variations in the bonding behavior of each of its allotropes. In $\\delta$-Pu, the 5f electrons lie at the tipping point between itinerant and localized behavior which makes the creation of predictive models very difficult. We perform density functional theory calculations to study the effect of correlated descriptions on the mechanical properties of $\\delta$-Pu. We find that 7.5% $E_{xx}$ in the HSE functional yields the experimental lattice parameters, moreover, this functional recovers the experimental elastic constants while other approximations fail. The electronic structure of the hybrid functional yields several signatures of strong correlations including orbital-selective bonding of a single 5f electron and a pseudogap-like feature which work in tandem to improve the description of mechanical properties. We show how the emergence of orbital-selective bonding in the hybrid functional can be understood through an Anderson impurity picture which predicts the augmentation of $\\pi$-bonding, the decrease of $\\sigma$-bonding, and expands the volume of the cell, enabling the accurate description of the mechanical properties of $\\delta$-Pu.","sentences":["Electronic correlations and spin-orbit interactions in plutonium create variations in the bonding behavior of each of its allotropes.","In $\\delta$-Pu, the 5f electrons lie at the tipping point between itinerant and localized behavior which makes the creation of predictive models very difficult.","We perform density functional theory calculations to study the effect of correlated descriptions on the mechanical properties of $\\delta$-Pu.","We find that 7.5% $E_{xx}$ in the HSE functional yields the experimental lattice parameters, moreover, this functional recovers the experimental elastic constants while other approximations fail.","The electronic structure of the hybrid functional yields several signatures of strong correlations including orbital-selective bonding of a single 5f electron and a pseudogap-like feature which work in tandem to improve the description of mechanical properties.","We show how the emergence of orbital-selective bonding in the hybrid functional can be understood through an Anderson impurity picture which predicts the augmentation of $\\pi$-bonding, the decrease of $\\sigma$-bonding, and expands the volume of the cell, enabling the accurate description of the mechanical properties of $\\delta$-Pu."],"url":"http://arxiv.org/abs/2405.09452v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-15 15:46:17","title":"Space of non-Fermi liquids","abstract":"In metals, low-energy effective theories are characterized by a set of coupling functions. Among them, the angle-dependent Fermi momentum specifies the size and shape of Fermi surface. Since the Fermi momentum grows incessantly under the renormalization group flow, a metallic fixed point is defined only modulo a rescaling of Fermi momentum. In this paper, we discuss the physical consequences of this projective nature of fixed points for non-Fermi liquids with hot Fermi surfaces. The first is the absence of a unique dynamical critical exponent that dictates the relative scaling between energy and momentum. The second is mismatches between the scaling dimensions of couplings and their relevancy. Nonetheless, each projective fixed point is characterized by a few marginal and relevant coupling functions, and the notion of universality survives. We illustrate our findings by charting out the space of projective fixed points and extracting their universal properties for the Ising-nematic quantum critical metal beyond the patch theory. To control the theory, we use the dimensional regularization scheme that tunes the co-dimension of Fermi surface. Near the upper critical dimension, two exactly marginal coupling functions span the space of stable projective fixed points: functions that specify the shape of the Fermi surface and the angle-dependent Fermi velocity. All other coupling functions, including the Landau functions and the universal pairing interaction, are fixed by those two marginal functions. With decreasing dimensions, the forward scattering remains irrelevant while the pairing interaction becomes relevant near two dimensions. In two dimensions, it is expected that the universal superconducting fluctuations lower the symmetry of the non-Fermi liquid realized above the superconducting transition temperatures from the loop U(1) group to a proper subgroup.","sentences":["In metals, low-energy effective theories are characterized by a set of coupling functions.","Among them, the angle-dependent Fermi momentum specifies the size and shape of Fermi surface.","Since the Fermi momentum grows incessantly under the renormalization group flow, a metallic fixed point is defined only modulo a rescaling of Fermi momentum.","In this paper, we discuss the physical consequences of this projective nature of fixed points for non-Fermi liquids with hot Fermi surfaces.","The first is the absence of a unique dynamical critical exponent that dictates the relative scaling between energy and momentum.","The second is mismatches between the scaling dimensions of couplings and their relevancy.","Nonetheless, each projective fixed point is characterized by a few marginal and relevant coupling functions, and the notion of universality survives.","We illustrate our findings by charting out the space of projective fixed points and extracting their universal properties for the Ising-nematic quantum critical metal beyond the patch theory.","To control the theory, we use the dimensional regularization scheme that tunes the co-dimension of Fermi surface.","Near the upper critical dimension, two exactly marginal coupling functions span the space of stable projective fixed points: functions that specify the shape of the Fermi surface and the angle-dependent Fermi velocity.","All other coupling functions, including the Landau functions and the universal pairing interaction, are fixed by those two marginal functions.","With decreasing dimensions, the forward scattering remains irrelevant while the pairing interaction becomes relevant near two dimensions.","In two dimensions, it is expected that the universal superconducting fluctuations lower the symmetry of the non-Fermi liquid realized above the superconducting transition temperatures from the loop U(1) group to a proper subgroup."],"url":"http://arxiv.org/abs/2405.09450v1","category":"cond-mat.str-el"}
{"created":"2024-05-15 15:44:25","title":"Cohomogeneity one RCD-spaces","abstract":"We study $\\mathsf{RCD}$-spaces $(X,d,\\mathfrak{m})$ with group actions by isometries preserving the reference measure $\\mathfrak{m}$ and whose orbit space has dimension one, i.e. cohomogeneity one actions. To this end we prove a Slice Theorem asserting that each slice at a point is homeomorphic to a non-negatively curved $\\mathsf{RCD}$-space. Under the assumption that $X$ is non-collapsed we further show that the slices are homeomorphic to metric cones over homogeneous spaces with $\\mathrm{Ric} \\geq 0$. As a consequence we obtain complete topological structural results and a principal orbit representation theorem. Conversely, we show how to construct new $\\mathsf{RCD}$-spaces from a cohomogeneity one group diagram, giving a complete description of $\\mathsf{RCD}$-spaces of cohomogeneity one. As an application of these results we obtain the classification of cohomogeneity one, non-collapsed $\\mathsf{RCD}$-spaces of essential dimension at most $4$.","sentences":["We study $\\mathsf{RCD}$-spaces $(X,d,\\mathfrak{m})$ with group actions by isometries preserving the reference measure $\\mathfrak{m}$ and whose orbit space has dimension one, i.e. cohomogeneity one actions.","To this end we prove a Slice Theorem asserting that each slice at a point is homeomorphic to a non-negatively curved $\\mathsf{RCD}$-space.","Under the assumption that $X$ is non-collapsed we further show that the slices are homeomorphic to metric cones over homogeneous spaces with $\\mathrm{Ric} \\geq 0$.","As a consequence we obtain complete topological structural results and a principal orbit representation theorem.","Conversely, we show how to construct new $\\mathsf{RCD}$-spaces from a cohomogeneity one group diagram, giving a complete description of $\\mathsf{RCD}$-spaces of cohomogeneity one.","As an application of these results we obtain the classification of cohomogeneity one, non-collapsed $\\mathsf{RCD}$-spaces of essential dimension at most $4$."],"url":"http://arxiv.org/abs/2405.09448v1","category":"math.MG"}
{"created":"2024-05-15 15:39:49","title":"Revisiting first-principles thermodynamics by quasiharmonic approach: Application to study thermal expansion of additively-manufactured Inconel 625","abstract":"An innovative method is developed for accurate determination of thermodynamic properties as a function of temperature by revisiting the density functional theory (DFT) based quasiharmonic approach (QHA). The present methodology individually evaluates the contributions from static total energy, phonon, and thermal electron to free energy for increased efficiency and accuracy. The Akaike information criterion with a correction (AICc) is used to select models and model parameters for fitting each contribution as a function of volume. Using the additively manufactured Inconel alloy 625 (IN625) as an example, predicted temperature-dependent linear coefficient of thermal expansion (CTE) agrees well with dilatometer measurements and values in the literature. Sensitivity and uncertainty are also analyzed for the predicted IN625 CTE due to different structural configurations used by DFT, and hence different equilibrium properties determined.","sentences":["An innovative method is developed for accurate determination of thermodynamic properties as a function of temperature by revisiting the density functional theory (DFT) based quasiharmonic approach (QHA).","The present methodology individually evaluates the contributions from static total energy, phonon, and thermal electron to free energy for increased efficiency and accuracy.","The Akaike information criterion with a correction (AICc) is used to select models and model parameters for fitting each contribution as a function of volume.","Using the additively manufactured Inconel alloy 625 (IN625) as an example, predicted temperature-dependent linear coefficient of thermal expansion (CTE) agrees well with dilatometer measurements and values in the literature.","Sensitivity and uncertainty are also analyzed for the predicted IN625 CTE due to different structural configurations used by DFT, and hence different equilibrium properties determined."],"url":"http://arxiv.org/abs/2405.09445v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-15 15:37:53","title":"Network Function Capacity Reconnaissance by Remote Adversaries","abstract":"There is anecdotal evidence that attackers use reconnaissance to learn the capacity of their victims before DDoS attacks to maximize their impact. The first step to mitigate capacity reconnaissance attacks is to understand their feasibility. However, the feasibility of capacity reconnaissance in network functions (NFs) (e.g., firewalls, NATs) is unknown. To this end, we formulate the problem of network function capacity reconnaissance (NFCR) and explore the feasibility of inferring the processing capacity of an NF while avoiding detection. We identify key factors that make NFCR challenging and analyze how these factors affect accuracy (measured as a divergence from ground truth) and stealthiness (measured in packets sent). We propose a flexible tool, NFTY, that performs NFCR and we evaluate two practical NFTY configurations to showcase the stealthiness vs. accuracy tradeoffs. We evaluate these strategies in controlled, Internet and/or cloud settings with commercial NFs. NFTY can accurately estimate the capacity of different NF deployments within 10% error in the controlled experiments and the Internet, and within 7% error for a commercial NF deployed in the cloud (AWS). Moreover, NFTY outperforms link-bandwidth estimation baselines by up to 30x.","sentences":["There is anecdotal evidence that attackers use reconnaissance to learn the capacity of their victims before DDoS attacks to maximize their impact.","The first step to mitigate capacity reconnaissance attacks is to understand their feasibility.","However, the feasibility of capacity reconnaissance in network functions (NFs) (e.g., firewalls, NATs) is unknown.","To this end, we formulate the problem of network function capacity reconnaissance (NFCR) and explore the feasibility of inferring the processing capacity of an NF while avoiding detection.","We identify key factors that make NFCR challenging and analyze how these factors affect accuracy (measured as a divergence from ground truth) and stealthiness (measured in packets sent).","We propose a flexible tool, NFTY, that performs NFCR","and we evaluate two practical NFTY configurations to showcase the stealthiness vs. accuracy tradeoffs.","We evaluate these strategies in controlled, Internet and/or cloud settings with commercial NFs.","NFTY can accurately estimate the capacity of different NF deployments within 10% error in the controlled experiments and the Internet, and within 7% error for a commercial NF deployed in the cloud (AWS).","Moreover, NFTY outperforms link-bandwidth estimation baselines by up to 30x."],"url":"http://arxiv.org/abs/2405.09442v1","category":"cs.NI"}
{"created":"2024-05-15 15:11:15","title":"Refined determination of the weak mixing angle at low energy","abstract":"The weak mixing angle is a fundamental parameter of the electroweak theory of the standard model whose measurement in the low-energy regime is still not precisely determined. Different probes are sensitive to its value, among which atomic parity violation, coherent elastic neutrino-nucleus scattering and parity-violating electron scattering on different nuclei. In this work, we attempt for the first time to combine all these various determinations by performing a global fit that also keeps into account the unavoidable dependence on the experimentally poorly known neutron distribution radius of the nuclei employed, for which a new measurement using proton-cesium elastic scattering became available. By using all present direct determinations of the neutron distribution radius of cesium we find $\\sin^2\\!\\vartheta_{W} =0.2396^{+0.0020}_{-0.0019}$, which should supersede the previous value determined from atomic parity violation on cesium. When including electroweak only, but also indirect, determinations of the neutron distribution radius of cesium the uncertainty reduces to 0.0017 maintaining the same central value, showing an excellent agreement independently of the method used.","sentences":["The weak mixing angle is a fundamental parameter of the electroweak theory of the standard model whose measurement in the low-energy regime is still not precisely determined.","Different probes are sensitive to its value, among which atomic parity violation, coherent elastic neutrino-nucleus scattering and parity-violating electron scattering on different nuclei.","In this work, we attempt for the first time to combine all these various determinations by performing a global fit that also keeps into account the unavoidable dependence on the experimentally poorly known neutron distribution radius of the nuclei employed, for which a new measurement using proton-cesium elastic scattering became available.","By using all present direct determinations of the neutron distribution radius of cesium we find $\\sin^2\\!\\vartheta_{W} =0.2396^{+0.0020}_{-0.0019}$, which should supersede the previous value determined from atomic parity violation on cesium.","When including electroweak only, but also indirect, determinations of the neutron distribution radius of cesium the uncertainty reduces to 0.0017 maintaining the same central value, showing an excellent agreement independently of the method used."],"url":"http://arxiv.org/abs/2405.09416v1","category":"hep-ph"}
{"created":"2024-05-15 15:05:34","title":"Advection of the image point in probabilistically-reconstructed phase spaces","abstract":"Insufficient reference data is ubiquitous in data-driven computational fluid dynamics, as it is usually too expensive to compute or impossible to observe over long enough times needed for data-driven methods to properly operate out of the sampled range. Ultimately, the lack of data can significantly compromise the fidelity of results computed with data-driven methods or make the data-driven approach inapplicable. In order to challenge this problem, we propose a probabilistic reconstruction method which enriches the hyper-parameterisation approach with ideas underlying the probabilistic-evolutionary approach. We offer to use Advection of the image point (a hyper-parameterisation method) on data sampled from the joint probability distribution of the reference dataset. The idea is to blend together the simplicity of the hyper-parameterisation (HP) method with an extra source of reference data provided through sampling from the joint probability distribution.   The HP method has been tested on the sea surface temperature and surface relative vorticity computed with the global 1/4-degree and 1/12-degree resolution NEMO model. Our results show that the HP solution (the solution computed with the HP method) in the probabilistically-reconstructed and reduced (in terms of dimensionality) phase space at 1/4-degree resolution is more accurate compared to the 1/4-degree solution computed with NEMO and it is also several orders of magnitude faster to compute than the 1/4-degree run. The proposed method shows encouraging results for the NEMO model and the potential for the use in other operational ocean and ocean-atmospheric models for both deterministic and probabilistic predictions.","sentences":["Insufficient reference data is ubiquitous in data-driven computational fluid dynamics, as it is usually too expensive to compute or impossible to observe over long enough times needed for data-driven methods to properly operate out of the sampled range.","Ultimately, the lack of data can significantly compromise the fidelity of results computed with data-driven methods or make the data-driven approach inapplicable.","In order to challenge this problem, we propose a probabilistic reconstruction method which enriches the hyper-parameterisation approach with ideas underlying the probabilistic-evolutionary approach.","We offer to use Advection of the image point (a hyper-parameterisation method) on data sampled from the joint probability distribution of the reference dataset.","The idea is to blend together the simplicity of the hyper-parameterisation (HP) method with an extra source of reference data provided through sampling from the joint probability distribution.   ","The HP method has been tested on the sea surface temperature and surface relative vorticity computed with the global 1/4-degree and 1/12-degree resolution NEMO model.","Our results show that the HP solution (the solution computed with the HP method) in the probabilistically-reconstructed and reduced (in terms of dimensionality) phase space at 1/4-degree resolution is more accurate compared to the 1/4-degree solution computed with NEMO and it is also several orders of magnitude faster to compute than the 1/4-degree run.","The proposed method shows encouraging results for the NEMO model and the potential for the use in other operational ocean and ocean-atmospheric models for both deterministic and probabilistic predictions."],"url":"http://arxiv.org/abs/2405.09410v1","category":"physics.flu-dyn"}
{"created":"2024-05-15 14:49:13","title":"Compositional imprecise probability","abstract":"Imprecise probability is concerned with uncertainty about which probability distributions to use. It has applications in robust statistics and elsewhere. Imprecise probability can be modelled in various ways, including by convex sets of probability distributions.   We look at programming language models for imprecise probability. Our desiderata are that we would like our model to support all kinds of composition, categorical and monoidal, in other words, guided by dataflow diagrams. Another equivalent perspective is that we would like a model of synthetic probability in the sense of Markov categories.   There is already a fairly popular monad-based approach to imprecise probability, but it is not fully compositional because the monad involved is not commutative, which means that we do not have a proper monoidal structure. In this work, we provide a new fully compositional account. The key idea is to name the non-deterministic choices. To manage the renamings and disjointness of names, we use graded monads. We show that the resulting compositional model is maximal. We relate with the earlier monad approach, showing that we obtain tighter bounds on the uncertainty.","sentences":["Imprecise probability is concerned with uncertainty about which probability distributions to use.","It has applications in robust statistics and elsewhere.","Imprecise probability can be modelled in various ways, including by convex sets of probability distributions.   ","We look at programming language models for imprecise probability.","Our desiderata are that we would like our model to support all kinds of composition, categorical and monoidal, in other words, guided by dataflow diagrams.","Another equivalent perspective is that we would like a model of synthetic probability in the sense of Markov categories.   ","There is already a fairly popular monad-based approach to imprecise probability, but it is not fully compositional because the monad involved is not commutative, which means that we do not have a proper monoidal structure.","In this work, we provide a new fully compositional account.","The key idea is to name the non-deterministic choices.","To manage the renamings and disjointness of names, we use graded monads.","We show that the resulting compositional model is maximal.","We relate with the earlier monad approach, showing that we obtain tighter bounds on the uncertainty."],"url":"http://arxiv.org/abs/2405.09391v1","category":"cs.PL"}
{"created":"2024-05-15 14:49:08","title":"Continuous Multi-Link Operation: A Contention-Free Mechanism for the Unlicensed Spectrum","abstract":"This paper proposes a novel mechanism to enforce contention-free channel access in the unlicensed spectrum, as opposed to the traditional contention-based approach. To achieve this objective, we build on the Wi-Fi~7 multi-link operation (MLO) and define the means whereby independent channel access attempts are performed in all the addressable links to ensure one available channel/link is ready for transmission at all times, such that a sequence of continuous acquired channels can be maintained. We call this method continuous multi-link operation (ConMLO). In this work, we aim to verify the applicability of ConMLO, its ability to retain spectrum resources for a given duration of time, and its fairness with respect existing approaches, namely legacy single-link operation (SLO) and MLO. To this end, we use realistic data traffic measurements acquired in a crowded football stadium as an exemplary case of challenging spectrum occupation. Our results show that the proposed ConMLO can effectively guarantee continuous channel acquisition under different occupancy scenarios without compromising fairness of channel access compared to existing legacy modes.","sentences":["This paper proposes a novel mechanism to enforce contention-free channel access in the unlicensed spectrum, as opposed to the traditional contention-based approach.","To achieve this objective, we build on the Wi-Fi~7 multi-link operation (MLO) and define the means whereby independent channel access attempts are performed in all the addressable links to ensure one available channel/link is ready for transmission at all times, such that a sequence of continuous acquired channels can be maintained.","We call this method continuous multi-link operation (ConMLO).","In this work, we aim to verify the applicability of ConMLO, its ability to retain spectrum resources for a given duration of time, and its fairness with respect existing approaches, namely legacy single-link operation (SLO) and MLO.","To this end, we use realistic data traffic measurements acquired in a crowded football stadium as an exemplary case of challenging spectrum occupation.","Our results show that the proposed ConMLO can effectively guarantee continuous channel acquisition under different occupancy scenarios without compromising fairness of channel access compared to existing legacy modes."],"url":"http://arxiv.org/abs/2405.09390v1","category":"eess.SP"}
{"created":"2024-05-15 14:13:35","title":"The Unfairness of $\\varepsilon$-Fairness","abstract":"Fairness in decision-making processes is often quantified using probabilistic metrics. However, these metrics may not fully capture the real-world consequences of unfairness. In this article, we adopt a utility-based approach to more accurately measure the real-world impacts of decision-making process. In particular, we show that if the concept of $\\varepsilon$-fairness is employed, it can possibly lead to outcomes that are maximally unfair in the real-world context. Additionally, we address the common issue of unavailable data on false negatives by proposing a reduced setting that still captures essential fairness considerations. We illustrate our findings with two real-world examples: college admissions and credit risk assessment. Our analysis reveals that while traditional probability-based evaluations might suggest fairness, a utility-based approach uncovers the necessary actions to truly achieve equality. For instance, in the college admission case, we find that enhancing completion rates is crucial for ensuring fairness. Summarizing, this paper highlights the importance of considering the real-world context when evaluating fairness.","sentences":["Fairness in decision-making processes is often quantified using probabilistic metrics.","However, these metrics may not fully capture the real-world consequences of unfairness.","In this article, we adopt a utility-based approach to more accurately measure the real-world impacts of decision-making process.","In particular, we show that if the concept of $\\varepsilon$-fairness is employed, it can possibly lead to outcomes that are maximally unfair in the real-world context.","Additionally, we address the common issue of unavailable data on false negatives by proposing a reduced setting that still captures essential fairness considerations.","We illustrate our findings with two real-world examples: college admissions and credit risk assessment.","Our analysis reveals that while traditional probability-based evaluations might suggest fairness, a utility-based approach uncovers the necessary actions to truly achieve equality.","For instance, in the college admission case, we find that enhancing completion rates is crucial for ensuring fairness.","Summarizing, this paper highlights the importance of considering the real-world context when evaluating fairness."],"url":"http://arxiv.org/abs/2405.09360v1","category":"cs.LG"}
{"created":"2024-05-15 14:12:28","title":"Sobolev estimates for Kolmogorov-Fokker-Planck operators with coefficients measurable in time and $VMO$ in space","abstract":"We consider Kolmogorov-Fokker-Planck operators of the form \\[ \\mathcal{L}u=\\sum_{i,j=1}^{q}a_{ij}(x,t)u_{x_{i}x_{j}}+\\sum_{k,j=1}^{N}b_{jk}x_{k}u_{x_{j}}-\\partial_{t}u, \\] with $\\left( x,t\\right) \\in\\mathbb{R}^{N+1},N\\geq q\\geq1$. We assume that $a_{ij}\\in L^{\\infty}\\left( \\mathbb{R}^{N+1}\\right) $, the matrix $\\left\\{a_{ij}\\right\\} $ is symmetric and uniformly positive on $\\mathbb{R}^{q}$, and the \\emph{drift}% \\[ Y=\\sum_{k,j=1}^{N}b_{jk}x_{k}\\partial_{x_{j}}-\\partial_{t}% \\] has a structure which makes the model operator with constant $a_{ij}$ hypoelliptic, translation invariant w.r.t. a suitable Lie group operation, and $2$-homogeneus w.r.t. a suitable family of dilations. We also assume that the coefficients $a_{ij}$ are $VMO$ w.r.t. the space variable, and only bounded measurable in $t$. We prove, for every $p\\in\\left( 1,\\infty\\right) $, global Sobolev estimates of the kind:% \\begin{align*} & \\sum_{i,j=1}^{q}\\left\\Vert u_{x_{i}x_{j}}\\right\\Vert _{L^{p}\\left( \\mathbb{R}^{N+1}\\right) }+\\left\\Vert Yu\\right\\Vert _{L^{p}\\left( \\mathbb{R}^{N+1}\\right) }+\\sum_{i=1}^{q}\\left\\Vert u_{x_{i}}\\right\\Vert _{L^{p}\\left( \\mathbb{R}^{N+1}\\right) }\\\\ & \\leq c\\left\\{ \\left\\Vert \\mathcal{L}u\\right\\Vert _{L^{p}\\left( \\mathbb{R}^{N+1}\\right) }+\\left\\Vert u\\right\\Vert _{L^{p}\\left( \\mathbb{R}^{N+1}\\right) }\\right\\} . \\end{align*}","sentences":["We consider Kolmogorov-Fokker-Planck operators of the form \\[ \\mathcal{L}u=\\sum_{i,j=1}^{q}a_{ij}(x,t)u_{x_{i}x_{j}}+\\sum_{k,j=1}^{N}b_{jk}x_{k}u_{x_{j}}-\\partial_{t}u, \\] with $\\left( x,t\\right)","\\in\\mathbb{R}^{N+1},N\\geq","q\\geq1$.","We assume that $a_{ij}\\in L^{\\infty}\\left( \\mathbb{R}^{N+1}\\right) $, the matrix $\\left\\{a_{ij}\\right\\} $ is symmetric and uniformly positive on $\\mathbb{R}^{q}$, and the \\emph{drift}% \\[ Y=\\sum_{k,j=1}^{N}b_{jk}x_{k}\\partial_{x_{j}}-\\partial_{t}% \\] has a structure which makes the model operator with constant $a_{ij}$ hypoelliptic, translation invariant w.r.t.","a suitable Lie group operation, and $2$-homogeneus w.r.t.","a suitable family of dilations.","We also assume that the coefficients $a_{ij}$ are $VMO$ w.r.t.","the space variable, and only bounded measurable in $t$. We prove, for every $p\\in\\left( 1,\\infty\\right) $, global Sobolev estimates of the kind:% \\begin{align*} & \\sum_{i,j=1}^{q}\\left\\Vert u_{x_{i}x_{j}}\\right\\Vert _{L^{p}\\left( \\mathbb{R}^{N+1}\\right) }+\\left\\Vert Yu\\right\\Vert _{L^{p}\\left( \\mathbb{R}^{N+1}\\right) }+\\sum_{i=1}^{q}\\left\\Vert u_{x_{i}}\\right\\Vert _{L^{p}\\left( \\mathbb{R}^{N+1}\\right) }\\\\ & \\leq c\\left\\{ \\left\\Vert \\mathcal{L}u\\right\\Vert _{L^{p}\\left( \\mathbb{R}^{N+1}\\right) }+\\left\\Vert u\\right\\Vert _{L^{p}\\left( \\mathbb{R}^{N+1}\\right) }\\right\\} .","\\end{align*}"],"url":"http://arxiv.org/abs/2405.09358v1","category":"math.AP"}
{"created":"2024-05-15 13:32:11","title":"Systematic investigation of the nuclear multiple deformations in U+U collisions with A Multi-Phase Transport model","abstract":"Relativistic heavy ion collisions provide a unique opportunity to study the shape of colliding nuclei, even up to higher-order multiple deformations. In this work, several observables that are sensitive to quadrupole and hexadecapole deformations of Uranium-238 in relativistic U+U collisions have been systematically investigated with A Multi-Phase Transport model. We find that the flow harmonic $v_{2}$, the $v_{2}$ and mean transverse momentum correlation, and the three-particle asymmetry cumulant ${\\rm ac}_{2}\\{3\\}$ are sensitive to nuclear quadrupole deformation, while ${\\rm ac}_{2}\\{3\\}$ and nonlinear response coefficient $\\chi_{4,22}$ are sensitive to nuclear hexadecapole deformation. Our results from transport model studies are in qualitative agreement with previous hydrodynamic studies. The results indicate that the uncertainties of the hexadecapole deformation of Uranium on the quadrupole deformation determination can be reduced by the abundance of correlation observables provided by the relativistic heavy ion collisions.","sentences":["Relativistic heavy ion collisions provide a unique opportunity to study the shape of colliding nuclei, even up to higher-order multiple deformations.","In this work, several observables that are sensitive to quadrupole and hexadecapole deformations of Uranium-238 in relativistic U+U collisions have been systematically investigated with A Multi-Phase Transport model.","We find that the flow harmonic $v_{2}$, the $v_{2}$ and mean transverse momentum correlation, and the three-particle asymmetry cumulant ${\\rm ac}_{2}\\{3\\}$ are sensitive to nuclear quadrupole deformation, while ${\\rm ac}_{2}\\{3\\}$ and nonlinear response coefficient $\\chi_{4,22}$ are sensitive to nuclear hexadecapole deformation.","Our results from transport model studies are in qualitative agreement with previous hydrodynamic studies.","The results indicate that the uncertainties of the hexadecapole deformation of Uranium on the quadrupole deformation determination can be reduced by the abundance of correlation observables provided by the relativistic heavy ion collisions."],"url":"http://arxiv.org/abs/2405.09329v1","category":"nucl-th"}
{"created":"2024-05-15 13:16:26","title":"Energy conservation for 3D Euler and Navier-Stokes equations in a bounded domain. Applications to Beltrami flows","abstract":"In this paper we consider the incompressible 3D Euler and Navier-Stokes equations in a smooth bounded domain. First, we study the 3D Euler equations endowed with slip boundary conditions and we prove the same criteria for energy conservation involving the gradient, already known for the Navier-Stokes equations. Subsequently, we utilise this finding, which is based on a proper approximation of the velocity (and doesn't require estimates or additional assumptions on the pressure), to explore energy conservation for Beltrami flows. Finally, we explore Beltrami solutions to the Navier-Stokes equations and demonstrate that conditions leading to energy conservation are significantly distinct from those implying regularity. This remains true even when making use of the bootstrap regularity improvement, stemming from the solution being a Beltrami vector field.","sentences":["In this paper we consider the incompressible 3D Euler and Navier-Stokes equations in a smooth bounded domain.","First, we study the 3D Euler equations endowed with slip boundary conditions and we prove the same criteria for energy conservation involving the gradient, already known for the Navier-Stokes equations.","Subsequently, we utilise this finding, which is based on a proper approximation of the velocity (and doesn't require estimates or additional assumptions on the pressure), to explore energy conservation for Beltrami flows.","Finally, we explore Beltrami solutions to the Navier-Stokes equations and demonstrate that conditions leading to energy conservation are significantly distinct from those implying regularity.","This remains true even when making use of the bootstrap regularity improvement, stemming from the solution being a Beltrami vector field."],"url":"http://arxiv.org/abs/2405.09316v1","category":"math.AP"}
{"created":"2024-05-15 12:40:41","title":"Deep Blur Multi-Model (DeepBlurMM) -- a strategy to mitigate the impact of image blur on deep learning model performance in histopathology image analysis","abstract":"AI-based analysis of histopathology whole slide images (WSIs) is central in computational pathology. However, image quality can impact model performance. Here, we investigate to what extent unsharp areas of WSIs impact deep convolutional neural network classification performance. We propose a multi-model approach, i.e. DeepBlurMM, to alleviate the impact of unsharp image areas and improve the model performance. DeepBlurMM uses the sigma cut-offs to determine the most suitable model for predicting tiles with various levels of blurring within a single WSI, where sigma is the standard deviation of the Gaussian distribution. Specifically, the cut-offs categorise the tiles into sharp or slight blur, moderate blur, and high blur. Each blur level has a corresponding model to be selected for tile-level predictions. Throughout the simulation study, we demonstrated the application of DeepBlurMM in a binary classification task for breast cancer Nottingham Histological Grade 1 vs 3. Performance, evaluated over 5-fold cross-validation, showed that DeepBlurMM outperformed the base model under moderate blur and mixed blur conditions. Unsharp image tiles (local blurriness) at prediction time reduced model performance. The proposed multi-model approach improved performance under some conditions, with the potential to improve quality in both research and clinical applications.","sentences":["AI-based analysis of histopathology whole slide images (WSIs) is central in computational pathology.","However, image quality can impact model performance.","Here, we investigate to what extent unsharp areas of WSIs impact deep convolutional neural network classification performance.","We propose a multi-model approach, i.e. DeepBlurMM, to alleviate the impact of unsharp image areas and improve the model performance.","DeepBlurMM uses the sigma cut-offs to determine the most suitable model for predicting tiles with various levels of blurring within a single WSI, where sigma is the standard deviation of the Gaussian distribution.","Specifically, the cut-offs categorise the tiles into sharp or slight blur, moderate blur, and high blur.","Each blur level has a corresponding model to be selected for tile-level predictions.","Throughout the simulation study, we demonstrated the application of DeepBlurMM in a binary classification task for breast cancer Nottingham Histological Grade 1 vs 3.","Performance, evaluated over 5-fold cross-validation, showed that DeepBlurMM outperformed the base model under moderate blur and mixed blur conditions.","Unsharp image tiles (local blurriness) at prediction time reduced model performance.","The proposed multi-model approach improved performance under some conditions, with the potential to improve quality in both research and clinical applications."],"url":"http://arxiv.org/abs/2405.09298v1","category":"eess.IV"}
{"created":"2024-05-15 12:15:21","title":"Logical coherence in 2D compass codes","abstract":"2D compass codes are a family of quantum error-correcting codes that contain the Bacon-Shor codes, the X-Shor and Z-Shor codes, and the rotated surface codes. Previous numerical results suggest that the surface code has a constant accuracy and coherence threshold under uniform coherent rotation. However, having analytical proof supporting a constant threshold is still an open problem. It is analytically proven that the toric code can exponentially suppress logical coherence in the code distance $L$. However, the current analytical lower bound on the threshold for the rotation angle $\\theta$ is $|\\sin(\\theta)| < 1/L$, which linearly vanishes in $L$ instead of being constant. We show that this lower bound is achievable by the Z-Shor code which does not have a threshold under stochastic noise. Compass codes provide a promising direction to improve on the previous bounds. We analytically determine thresholds for two new compass code families with thresholds near the rotated surface code's numerically established coherence threshold. Furthermore, using a Majorana mode-based simulator, we use random families of compass codes to smoothly interpolate between the Z-Shor codes and the X-Shor codes.","sentences":["2D compass codes are a family of quantum error-correcting codes that contain the Bacon-Shor codes, the X-Shor and Z-Shor codes, and the rotated surface codes.","Previous numerical results suggest that the surface code has a constant accuracy and coherence threshold under uniform coherent rotation.","However, having analytical proof supporting a constant threshold is still an open problem.","It is analytically proven that the toric code can exponentially suppress logical coherence in the code distance $L$.","However, the current analytical lower bound on the threshold for the rotation angle $\\theta$ is $|\\sin(\\theta)| < 1/L$, which linearly vanishes in $L$ instead of being constant.","We show that this lower bound is achievable by the Z-Shor code which does not have a threshold under stochastic noise.","Compass codes provide a promising direction to improve on the previous bounds.","We analytically determine thresholds for two new compass code families with thresholds near the rotated surface code's numerically established coherence threshold.","Furthermore, using a Majorana mode-based simulator, we use random families of compass codes to smoothly interpolate between the Z-Shor codes and the X-Shor codes."],"url":"http://arxiv.org/abs/2405.09287v1","category":"quant-ph"}
{"created":"2024-05-15 11:15:17","title":"Drag prediction of rough-wall turbulent flow using data-driven regression","abstract":"Efficient tools for predicting the drag of rough walls in turbulent flows would have a tremendous impact. However, methods for drag prediction rely on experiments or numerical simulations which are costly and time-consuming. Data-driven regression methods have the potential to provide a prediction that is accurate and fast. We assess the performance and limitations of linear regression, kernel methods and neural networks for drag prediction using a database of 1000 homogeneous rough surfaces. Model performance is evaluated using the roughness function obtained at friction-scaled Reynolds number 500. With two trainable parameters, the kernel method can fully account for nonlinear relations between $\\Delta U^+$ and surface statistics (roughness height, effective slope, skewness, etc). In contrast, linear regression cannot account for nonlinear correlations and display large errors and high uncertainty. Multilayer perceptron and convolutional neural networks demonstrate performance on par with the kernel method but have orders of magnitude more trainable parameters. For the current database size, the networks' capacity cannot be fully exploited, resulting in reduced generalizability and reliability. Our study provides insight into the appropriateness of different regression models for drag prediction. We also discuss the remaining steps before data-driven methods emerge as useful tools in applications.","sentences":["Efficient tools for predicting the drag of rough walls in turbulent flows would have a tremendous impact.","However, methods for drag prediction rely on experiments or numerical simulations which are costly and time-consuming.","Data-driven regression methods have the potential to provide a prediction that is accurate and fast.","We assess the performance and limitations of linear regression, kernel methods and neural networks for drag prediction using a database of 1000 homogeneous rough surfaces.","Model performance is evaluated using the roughness function obtained at friction-scaled Reynolds number 500.","With two trainable parameters, the kernel method can fully account for nonlinear relations between $\\Delta U^+$","and surface statistics (roughness height, effective slope, skewness, etc).","In contrast, linear regression cannot account for nonlinear correlations and display large errors and high uncertainty.","Multilayer perceptron and convolutional neural networks demonstrate performance on par with the kernel method but have orders of magnitude more trainable parameters.","For the current database size, the networks' capacity cannot be fully exploited, resulting in reduced generalizability and reliability.","Our study provides insight into the appropriateness of different regression models for drag prediction.","We also discuss the remaining steps before data-driven methods emerge as useful tools in applications."],"url":"http://arxiv.org/abs/2405.09256v1","category":"physics.flu-dyn"}
{"created":"2024-05-15 11:07:40","title":"Does Machine Bring in Extra Bias in Learning? Approximating Fairness in Models Promptly","abstract":"Providing various machine learning (ML) applications in the real world, concerns about discrimination hidden in ML models are growing, particularly in high-stakes domains. Existing techniques for assessing the discrimination level of ML models include commonly used group and individual fairness measures. However, these two types of fairness measures are usually hard to be compatible with each other, and even two different group fairness measures might be incompatible as well. To address this issue, we investigate to evaluate the discrimination level of classifiers from a manifold perspective and propose a \"harmonic fairness measure via manifolds (HFM)\" based on distances between sets. Yet the direct calculation of distances might be too expensive to afford, reducing its practical applicability. Therefore, we devise an approximation algorithm named \"Approximation of distance between sets (ApproxDist)\" to facilitate accurate estimation of distances, and we further demonstrate its algorithmic effectiveness under certain reasonable assumptions. Empirical results indicate that the proposed fairness measure HFM is valid and that the proposed ApproxDist is effective and efficient.","sentences":["Providing various machine learning (ML) applications in the real world, concerns about discrimination hidden in ML models are growing, particularly in high-stakes domains.","Existing techniques for assessing the discrimination level of ML models include commonly used group and individual fairness measures.","However, these two types of fairness measures are usually hard to be compatible with each other, and even two different group fairness measures might be incompatible as well.","To address this issue, we investigate to evaluate the discrimination level of classifiers from a manifold perspective and propose a \"harmonic fairness measure via manifolds (HFM)\" based on distances between sets.","Yet the direct calculation of distances might be too expensive to afford, reducing its practical applicability.","Therefore, we devise an approximation algorithm named \"Approximation of distance between sets (ApproxDist)\" to facilitate accurate estimation of distances, and we further demonstrate its algorithmic effectiveness under certain reasonable assumptions.","Empirical results indicate that the proposed fairness measure HFM is valid and that the proposed ApproxDist is effective and efficient."],"url":"http://arxiv.org/abs/2405.09251v1","category":"cs.LG"}
{"created":"2024-05-15 10:58:03","title":"Unconventional magnetism mediated by spin-phonon-photon coupling","abstract":"Magnetic order typically emerges due to the short-range exchange interaction between the constituent electronic spins. Recent discoveries have found a crucial role for spin-phonon coupling in various phenomena from optical ultrafast magnetization switching to dynamical control of the magnetic state. Here, we demonstrate theoretically the emergence of a biquadratic long-range interaction between spins mediated by their coupling to phonons hybridized with vacuum photons into polaritons. The resulting ordered state enabled by the exchange of virtual polaritons between spins is reminiscent of superconductivity mediated by the exchange of virtual phonons. The biquadratic nature of the spin-spin interaction promotes ordering without favoring ferro- or antiferromagnetism. It further makes the phase transition to magnetic order a first-order transition, unlike in conventional magnets. Consequently, a large magnetization develops abruptly on lowering the temperature which \\aknew{could} enable magnetic memories admitting ultralow-power thermally-assisted writing while maintaining a high data stability. The role of photons in the phenomenon further enables an in-situ static control over the magnetism. These unique features make our predicted spin-spin interaction and magnetism highly unconventional paving the way for novel scientific and technological opportunities.","sentences":["Magnetic order typically emerges due to the short-range exchange interaction between the constituent electronic spins.","Recent discoveries have found a crucial role for spin-phonon coupling in various phenomena from optical ultrafast magnetization switching to dynamical control of the magnetic state.","Here, we demonstrate theoretically the emergence of a biquadratic long-range interaction between spins mediated by their coupling to phonons hybridized with vacuum photons into polaritons.","The resulting ordered state enabled by the exchange of virtual polaritons between spins is reminiscent of superconductivity mediated by the exchange of virtual phonons.","The biquadratic nature of the spin-spin interaction promotes ordering without favoring ferro- or antiferromagnetism.","It further makes the phase transition to magnetic order a first-order transition, unlike in conventional magnets.","Consequently, a large magnetization develops abruptly on lowering the temperature which \\aknew{could} enable magnetic memories admitting ultralow-power thermally-assisted writing while maintaining a high data stability.","The role of photons in the phenomenon further enables an in-situ static control over the magnetism.","These unique features make our predicted spin-spin interaction and magnetism highly unconventional paving the way for novel scientific and technological opportunities."],"url":"http://arxiv.org/abs/2405.09246v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-15 09:17:38","title":"Quantum tomography with $\u03c4$ leptons at the FCC-ee","abstract":"The Future Circular Collider (FCC) -- in its first incarnation as a lepton collider -- will produce, according to the proposed design, more than 100 billion pairs of $\\tau$ leptons after working for four years at the energy of the $Z$-boson resonance. The $\\tau$ lepton is special because its relatively long lifetime makes it possible to reconstruct the momenta of neutrinos emitted in the single pion decay mode. The resulting large number of events is an ideal source for a full quantum tomography of the process that will test quantum entanglement and the violation of Bell inequality with unprecedented precision. In addition, the study of polarizations and spin correlations can provide a competitive determination of the Weinberg angle $ \\theta_W$ and constrain possible anomalous couplings in the neutral electroweak current. We utilize analytic results and Monte Carlo simulations to explore to what extent these goals might be accomplished.","sentences":["The Future Circular Collider (FCC) -- in its first incarnation as a lepton collider -- will produce, according to the proposed design, more than 100 billion pairs of $\\tau$ leptons after working for four years at the energy of the $Z$-boson resonance.","The $\\tau$ lepton is special because its relatively long lifetime makes it possible to reconstruct the momenta of neutrinos emitted in the single pion decay mode.","The resulting large number of events is an ideal source for a full quantum tomography of the process that will test quantum entanglement and the violation of Bell inequality with unprecedented precision.","In addition, the study of polarizations and spin correlations can provide a competitive determination of the Weinberg angle $ \\theta_W$ and constrain possible anomalous couplings in the neutral electroweak current.","We utilize analytic results and Monte Carlo simulations to explore to what extent these goals might be accomplished."],"url":"http://arxiv.org/abs/2405.09201v1","category":"hep-ph"}
{"created":"2024-05-15 08:56:16","title":"QMedShield: A Novel Quantum Chaos-based Image Encryption Scheme for Secure Medical Image Storage in the Cloud","abstract":"In the age of digital technology, medical images play a crucial role in the healthcare industry which aids surgeons in making precise decisions and reducing the diagnosis time. However, the storage of large amounts of these images in third-party cloud services raises privacy and security concerns. There are a lot of classical security mechanisms to protect them. Although, the advent of quantum computing entails the development of quantum-based encryption models for healthcare. Hence, we introduce a novel quantum chaos-based encryption scheme for medical images in this article. The model comprises bit-plane scrambling, quantum logistic map, quantum operations in the diffusion phase and hybrid chaotic map, DNA encoding, and computations in the confusion phase to transform the plain medical image into a cipher medical image. The proposed scheme has been evaluated using multiple statistical measures and validated against more attacks such as differential attacks with three different medical datasets. Hence the introduced encryption model has proved to be attack-resistant and robust than other existing image encryption schemes, ensuring the secure storage of medical images in cloud environments.","sentences":["In the age of digital technology, medical images play a crucial role in the healthcare industry which aids surgeons in making precise decisions and reducing the diagnosis time.","However, the storage of large amounts of these images in third-party cloud services raises privacy and security concerns.","There are a lot of classical security mechanisms to protect them.","Although, the advent of quantum computing entails the development of quantum-based encryption models for healthcare.","Hence, we introduce a novel quantum chaos-based encryption scheme for medical images in this article.","The model comprises bit-plane scrambling, quantum logistic map, quantum operations in the diffusion phase and hybrid chaotic map, DNA encoding, and computations in the confusion phase to transform the plain medical image into a cipher medical image.","The proposed scheme has been evaluated using multiple statistical measures and validated against more attacks such as differential attacks with three different medical datasets.","Hence the introduced encryption model has proved to be attack-resistant and robust than other existing image encryption schemes, ensuring the secure storage of medical images in cloud environments."],"url":"http://arxiv.org/abs/2405.09191v1","category":"cs.CR"}
{"created":"2024-05-15 08:24:53","title":"The Economic Limits of Permissionless Consensus","abstract":"The purpose of a consensus protocol is to keep a distributed network of nodes \"in sync,\" even in the presence of an unpredictable communication network and adversarial behavior by some of the participating nodes. In the permissionless setting, these nodes may be operated by unknown players, with each player free to use multiple identifiers and to start or stop running the protocol at any time. Establishing that a permissionless consensus protocol is \"secure\" thus requires both a distributed computing argument (that the protocol guarantees consistency and liveness unless the fraction of adversarial participation is sufficiently large) and an economic argument (that carrying out an attack would be prohibitively expensive for an attacker). There is a mature toolbox for assembling arguments of the former type; the goal of this paper is to lay the foundations for arguments of the latter type.   An ideal permissionless consensus protocol would, in addition to satisfying standard consistency and liveness guarantees, render consistency violations prohibitively expensive for the attacker without collateral damage to honest participants. We make this idea precise with our notion of the EAAC (expensive to attack in the absence of collapse) property, and prove the following results:   1. In the synchronous and dynamically available setting, with an adversary that controls at least one-half of the overall resources, no protocol can be EAAC.   2. In the partially synchronous and quasi-permissionless setting, with an adversary that controls at least one-third of the overall resources, no protocol can be EAAC.   3. In the synchronous and quasi-permissionless setting, there is a proof-of-stake protocol that, provided the adversary controls less than two-thirds of the overall stake, satisfies the EAAC property.   All three results are optimal with respect to the size of the adversary.","sentences":["The purpose of a consensus protocol is to keep a distributed network of nodes \"in sync,\" even in the presence of an unpredictable communication network and adversarial behavior by some of the participating nodes.","In the permissionless setting, these nodes may be operated by unknown players, with each player free to use multiple identifiers and to start or stop running the protocol at any time.","Establishing that a permissionless consensus protocol is \"secure\" thus requires both a distributed computing argument (that the protocol guarantees consistency and liveness unless the fraction of adversarial participation is sufficiently large) and an economic argument (that carrying out an attack would be prohibitively expensive for an attacker).","There is a mature toolbox for assembling arguments of the former type; the goal of this paper is to lay the foundations for arguments of the latter type.   ","An ideal permissionless consensus protocol would, in addition to satisfying standard consistency and liveness guarantees, render consistency violations prohibitively expensive for the attacker without collateral damage to honest participants.","We make this idea precise with our notion of the EAAC (expensive to attack in the absence of collapse) property, and prove the following results:   1.","In the synchronous and dynamically available setting, with an adversary that controls at least one-half of the overall resources, no protocol can be EAAC.   ","2.","In the partially synchronous and quasi-permissionless setting, with an adversary that controls at least one-third of the overall resources, no protocol can be EAAC.   ","3.","In the synchronous and quasi-permissionless setting, there is a proof-of-stake protocol that, provided the adversary controls less than two-thirds of the overall stake, satisfies the EAAC property.   ","All three results are optimal with respect to the size of the adversary."],"url":"http://arxiv.org/abs/2405.09173v1","category":"cs.DC"}
{"created":"2024-05-15 07:41:12","title":"A note on convergence of densities to free extreme value distributions","abstract":"The concept of free extreme value distributions as universal limit laws for the spectral maximum of free noncommutative real random variables was discovered by Ben Arous and Voiculescu in 2006. This paper contributes to study the convergence of densities towards free extreme value distributions under the von Mises condition for sample distributions.","sentences":["The concept of free extreme value distributions as universal limit laws for the spectral maximum of free noncommutative real random variables was discovered by Ben Arous and Voiculescu in 2006.","This paper contributes to study the convergence of densities towards free extreme value distributions under the von Mises condition for sample distributions."],"url":"http://arxiv.org/abs/2405.09156v1","category":"math.PR"}
{"created":"2024-05-15 06:57:18","title":"Overcoming Domain Drift in Online Continual Learning","abstract":"Online Continual Learning (OCL) empowers machine learning models to acquire new knowledge online across a sequence of tasks. However, OCL faces a significant challenge: catastrophic forgetting, wherein the model learned in previous tasks is substantially overwritten upon encountering new tasks, leading to a biased forgetting of prior knowledge. Moreover, the continual doman drift in sequential learning tasks may entail the gradual displacement of the decision boundaries in the learned feature space, rendering the learned knowledge susceptible to forgetting. To address the above problem, in this paper, we propose a novel rehearsal strategy, termed Drift-Reducing Rehearsal (DRR), to anchor the domain of old tasks and reduce the negative transfer effects. First, we propose to select memory for more representative samples guided by constructed centroids in a data stream. Then, to keep the model from domain chaos in drifting, a two-level angular cross-task Contrastive Margin Loss (CML) is proposed, to encourage the intra-class and intra-task compactness, and increase the inter-class and inter-task discrepancy. Finally, to further suppress the continual domain drift, we present an optional Centorid Distillation Loss (CDL) on the rehearsal memory to anchor the knowledge in feature space for each previous old task. Extensive experimental results on four benchmark datasets validate that the proposed DRR can effectively mitigate the continual domain drift and achieve the state-of-the-art (SOTA) performance in OCL.","sentences":["Online Continual Learning (OCL) empowers machine learning models to acquire new knowledge online across a sequence of tasks.","However, OCL faces a significant challenge: catastrophic forgetting, wherein the model learned in previous tasks is substantially overwritten upon encountering new tasks, leading to a biased forgetting of prior knowledge.","Moreover, the continual doman drift in sequential learning tasks may entail the gradual displacement of the decision boundaries in the learned feature space, rendering the learned knowledge susceptible to forgetting.","To address the above problem, in this paper, we propose a novel rehearsal strategy, termed Drift-Reducing Rehearsal (DRR), to anchor the domain of old tasks and reduce the negative transfer effects.","First, we propose to select memory for more representative samples guided by constructed centroids in a data stream.","Then, to keep the model from domain chaos in drifting, a two-level angular cross-task Contrastive Margin Loss (CML) is proposed, to encourage the intra-class and intra-task compactness, and increase the inter-class and inter-task discrepancy.","Finally, to further suppress the continual domain drift, we present an optional Centorid Distillation Loss (CDL) on the rehearsal memory to anchor the knowledge in feature space for each previous old task.","Extensive experimental results on four benchmark datasets validate that the proposed DRR can effectively mitigate the continual domain drift and achieve the state-of-the-art (SOTA) performance in OCL."],"url":"http://arxiv.org/abs/2405.09133v1","category":"cs.LG"}
{"created":"2024-05-15 06:21:04","title":"Atomic transport dynamics in crossed optical dipole trap","abstract":"We study the dynamical evolution of cold atoms in crossed optical dipole trap theoretically and experimentally. The atomic transport process is accompanied by two competitive kinds of physical mechanics, atomic loading and atomic loss. The loading process normally is negligible in the evaporative cooling experiment on the ground, while it is significant in the preparation of ultra-cold atoms in the space station. Normally, the atomic loading process is much weaker than the atomic loss process, and the atomic number in the center region of the trap decreases monotonically, as reported in previous research. However, when the atomic loading process is comparable to the atomic loss process, the atomic number in the center region of the trap will initially increase to a maximum value and then slowly decrease, and we have observed the phenomenon first. The increase of atomic number in the center region of the trap shows the presence of the loading process, and this will be significant especially under microgravity conditions. We build a theoretical model to analyze the competitive relationship, which coincides with the experimental results well. Furthermore, we have also given the predicted evolutionary behaviors under different conditions. This research provides a solid foundation for further understanding of the atomic transport process in traps. The analysis of loading process is of significant importance for the preparation of ultra-cold atoms in a crossed optical dipole trap under microgravity conditions.","sentences":["We study the dynamical evolution of cold atoms in crossed optical dipole trap theoretically and experimentally.","The atomic transport process is accompanied by two competitive kinds of physical mechanics, atomic loading and atomic loss.","The loading process normally is negligible in the evaporative cooling experiment on the ground, while it is significant in the preparation of ultra-cold atoms in the space station.","Normally, the atomic loading process is much weaker than the atomic loss process, and the atomic number in the center region of the trap decreases monotonically, as reported in previous research.","However, when the atomic loading process is comparable to the atomic loss process, the atomic number in the center region of the trap will initially increase to a maximum value and then slowly decrease, and we have observed the phenomenon first.","The increase of atomic number in the center region of the trap shows the presence of the loading process, and this will be significant especially under microgravity conditions.","We build a theoretical model to analyze the competitive relationship, which coincides with the experimental results well.","Furthermore, we have also given the predicted evolutionary behaviors under different conditions.","This research provides a solid foundation for further understanding of the atomic transport process in traps.","The analysis of loading process is of significant importance for the preparation of ultra-cold atoms in a crossed optical dipole trap under microgravity conditions."],"url":"http://arxiv.org/abs/2405.09116v1","category":"quant-ph"}
{"created":"2024-05-15 05:27:49","title":"Mean Reflected Backward Stochastic Differential Equations Driven by G-Brownian Motion with Double Constraints","abstract":"In this paper, we study the backward stochastic differential equations driven by G-Brownian motion with double mean reflections, which means that the constraints are made on the law of the solution. Making full use of the backward Skorokhod problem with two nonlinear reflecting boundaries and the fixed-point theory, the existence and uniqueness of solutions are established. We also consider the case where the coefficients satisfy a non-Lipschitz condition using the Picard iteration argument only for the Y component. Moreover, some basic properties including a new version of comparison theorem and connection with a deterministic optimization problem are also obtained.","sentences":["In this paper, we study the backward stochastic differential equations driven by G-Brownian motion with double mean reflections, which means that the constraints are made on the law of the solution.","Making full use of the backward Skorokhod problem with two nonlinear reflecting boundaries and the fixed-point theory, the existence and uniqueness of solutions are established.","We also consider the case where the coefficients satisfy a non-Lipschitz condition using the Picard iteration argument only for the Y component.","Moreover, some basic properties including a new version of comparison theorem and connection with a deterministic optimization problem are also obtained."],"url":"http://arxiv.org/abs/2405.09103v1","category":"math.PR"}
{"created":"2024-05-15 04:38:50","title":"Temporarily Restricting Solidity Smart Contract Interactions","abstract":"In this work we explore ways to restrict the ability to call Solidity smart contract functions for a specified duration. We describe methods to restrict functions from being called twice in the same transaction, block, or time period. This is related to the notion of non-reentrant functions, which are functions that can be called within a previous execution. These methods can be used to restrict interactions with entire sets of functions of smart contracts. We are motivated to revisit this topic for two reasons. First, we note that sixteen real-world smart contracts exploits in 2023 resulting in over $136M USD lost or stolen that could have been prevented by restricting function calls. As part of this survey, we dissect a new class of exploit that involves so-called read-only reentrancy: exploits that re-enter read-only functions to make smart contract state inconsistent in order to enable their exploitation. Second, while some of these approaches are simple, they may not always behave the same across different blockchains that support Solidity.","sentences":["In this work we explore ways to restrict the ability to call Solidity smart contract functions for a specified duration.","We describe methods to restrict functions from being called twice in the same transaction, block, or time period.","This is related to the notion of non-reentrant functions, which are functions that can be called within a previous execution.","These methods can be used to restrict interactions with entire sets of functions of smart contracts.","We are motivated to revisit this topic for two reasons.","First, we note that sixteen real-world smart contracts exploits in 2023 resulting in over $136M USD lost or stolen that could have been prevented by restricting function calls.","As part of this survey, we dissect a new class of exploit that involves so-called read-only reentrancy: exploits that re-enter read-only functions to make smart contract state inconsistent in order to enable their exploitation.","Second, while some of these approaches are simple, they may not always behave the same across different blockchains that support Solidity."],"url":"http://arxiv.org/abs/2405.09084v1","category":"cs.CR"}
{"created":"2024-05-15 04:01:47","title":"Enhancing Airline Customer Satisfaction: A Machine Learning and Causal Analysis Approach","abstract":"This study explores the enhancement of customer satisfaction in the airline industry, a critical factor for retaining customers and building brand reputation, which are vital for revenue growth. Utilizing a combination of machine learning and causal inference methods, we examine the specific impact of service improvements on customer satisfaction, with a focus on the online boarding pass experience. Through detailed data analysis involving several predictive and causal models, we demonstrate that improvements in the digital aspects of customer service significantly elevate overall customer satisfaction. This paper highlights how airlines can strategically leverage these insights to make data-driven decisions that enhance customer experiences and, consequently, their market competitiveness.","sentences":["This study explores the enhancement of customer satisfaction in the airline industry, a critical factor for retaining customers and building brand reputation, which are vital for revenue growth.","Utilizing a combination of machine learning and causal inference methods, we examine the specific impact of service improvements on customer satisfaction, with a focus on the online boarding pass experience.","Through detailed data analysis involving several predictive and causal models, we demonstrate that improvements in the digital aspects of customer service significantly elevate overall customer satisfaction.","This paper highlights how airlines can strategically leverage these insights to make data-driven decisions that enhance customer experiences and, consequently, their market competitiveness."],"url":"http://arxiv.org/abs/2405.09076v1","category":"cs.LG"}
{"created":"2024-05-15 03:36:54","title":"Mass spectra of strange double charm pentaquarks with strangeness $S=-1$","abstract":"The observation of the $T_{c\\bar{s}}(2900)$ indicates the potential existence of strange double charm pentaquarks based on the heavy antidiquark symmetry. We systematically study the mass spectra of strange double charm pentaquarks with strangeness $S=-1$ in both molecular and compact structures for quantum numbers $J^{P}=1/2^{-}$, $3/2^{-}$, $5/2^{-}$. By constructing the interpolating currents, the mass spectra can be extracted from the two-point correlation functions in the framework of QCD sum rule method. In the molecular picture, we find that the $\\Xi_c^+D^{\\ast +}$, $\\Xi_c^{'+}D^{\\ast +}$, $\\Xi_{c}^{\\ast +}D^{\\ast +}$, $\\Xi_{cc}^{\\ast ++}K^{\\ast 0}$ and $\\Omega_{cc}^{\\ast ++}\\rho^0$ may form molecular strange double charm pentaquarks. In both pictures, the masses of the $J^P=1/2^-, 3/2^-$ pentaquarks locate within the $4.2-4.6~\\mathrm{GeV}$ and $4.2-4.5~\\mathrm{GeV}$ regions, respectively. As all of them are above the thresholds of their strong decay channels, they behave as a broad state, making them challenging to be detected in experiment. On the contrary, the mass of the $J^P=5/2^-$ strange double charm pentaquark is located at $4.3~\\mathrm{GeV}$ and below its strong decay channel. This makes it as a narrow state and easy to be identified in experiment. The best observed channel is its semi-leptonic decay to double charm baryon. As the result, we strongly suggest experiments to search for $J^P=5/2^-$ strange double charm pentaquarks as a first try.","sentences":["The observation of the $T_{c\\bar{s}}(2900)$ indicates the potential existence of strange double charm pentaquarks based on the heavy antidiquark symmetry.","We systematically study the mass spectra of strange double charm pentaquarks with strangeness $S=-1$ in both molecular and compact structures for quantum numbers $J^{P}=1/2^{-}$, $3/2^{-}$, $5/2^{-}$. By constructing the interpolating currents, the mass spectra can be extracted from the two-point correlation functions in the framework of QCD sum rule method.","In the molecular picture, we find that the $\\Xi_c^+D^{\\ast +}$, $\\Xi_c^{'+}D^{\\ast +}$, $\\Xi_{c}^{\\ast +}D^{\\ast +}$, $\\Xi_{cc}^{\\ast ++}K^{\\ast 0}$ and $\\Omega_{cc}^{\\ast ++}\\rho^0$ may form molecular strange double charm pentaquarks.","In both pictures, the masses of the $J^P=1/2^-, 3/2^-$ pentaquarks locate within the $4.2-4.6~\\mathrm{GeV}$ and $4.2-4.5~\\mathrm{GeV}$ regions, respectively.","As all of them are above the thresholds of their strong decay channels, they behave as a broad state, making them challenging to be detected in experiment.","On the contrary, the mass of the $J^P=5/2^-$ strange double charm pentaquark is located at $4.3~\\mathrm{GeV}$ and below its strong decay channel.","This makes it as a narrow state and easy to be identified in experiment.","The best observed channel is its semi-leptonic decay to double charm baryon.","As the result, we strongly suggest experiments to search for $J^P=5/2^-$ strange double charm pentaquarks as a first try."],"url":"http://arxiv.org/abs/2405.09067v1","category":"hep-ph"}
{"created":"2024-05-15 03:16:10","title":"Further study of modulation spaces as Banach algebras","abstract":"This paper discusses spectral synthesis for those modulation spaces $M^{p,q}_s({\\mathbf R}^n)$ which form Banach algebras under pointwise multiplication. An important argument will be the ``ideal theory for Segal algebras'' by H. Reiter [15]. This paper is a continuation of our paper [5] where the case $q=1$ is treated. As a by-product we obtain a variant of Wiener-L\\'evy theorem for $M^{p,q}_s({\\mathbf R}^n)$ and Fourier-Wermer algebras ${\\mathcal F}L^q_s({\\mathbf R}^n)$.","sentences":["This paper discusses spectral synthesis for those modulation spaces $M^{p,q}_s({\\mathbf R}^n)$ which form Banach algebras under pointwise multiplication.","An important argument will be the ``ideal theory for Segal algebras'' by H. Reiter","[15].","This paper is a continuation of our paper [5] where the case $q=1$ is treated.","As a by-product we obtain a variant of Wiener-L\\'evy theorem for $M^{p,q}_s({\\mathbf R}^n)$ and Fourier-Wermer algebras ${\\mathcal F}L^q_s({\\mathbf R}^n)$."],"url":"http://arxiv.org/abs/2405.09060v1","category":"math.FA"}
{"created":"2024-05-15 03:04:05","title":"A safety realignment framework via subspace-oriented model fusion for large language models","abstract":"The current safeguard mechanisms for large language models (LLMs) are indeed susceptible to jailbreak attacks, making them inherently fragile. Even the process of fine-tuning on apparently benign data for downstream tasks can jeopardize safety. One potential solution is to conduct safety fine-tuning subsequent to downstream fine-tuning. However, there's a risk of catastrophic forgetting during safety fine-tuning, where LLMs may regain safety measures but lose the task-specific knowledge acquired during downstream fine-tuning. In this paper, we introduce a safety realignment framework through subspace-oriented model fusion (SOMF), aiming to combine the safeguard capabilities of initially aligned model and the current fine-tuned model into a realigned model. Our approach begins by disentangling all task vectors from the weights of each fine-tuned model. We then identify safety-related regions within these vectors by subspace masking techniques. Finally, we explore the fusion of the initial safely aligned LLM with all task vectors based on the identified safety subspace. We validate that our safety realignment framework satisfies the safety requirements of a single fine-tuned model as well as multiple models during their fusion. Our findings confirm that SOMF preserves safety without notably compromising performance on downstream tasks, including instruction following in Chinese, English, and Hindi, as well as problem-solving capabilities in Code and Math.","sentences":["The current safeguard mechanisms for large language models (LLMs) are indeed susceptible to jailbreak attacks, making them inherently fragile.","Even the process of fine-tuning on apparently benign data for downstream tasks can jeopardize safety.","One potential solution is to conduct safety fine-tuning subsequent to downstream fine-tuning.","However, there's a risk of catastrophic forgetting during safety fine-tuning, where LLMs may regain safety measures but lose the task-specific knowledge acquired during downstream fine-tuning.","In this paper, we introduce a safety realignment framework through subspace-oriented model fusion (SOMF), aiming to combine the safeguard capabilities of initially aligned model and the current fine-tuned model into a realigned model.","Our approach begins by disentangling all task vectors from the weights of each fine-tuned model.","We then identify safety-related regions within these vectors by subspace masking techniques.","Finally, we explore the fusion of the initial safely aligned LLM with all task vectors based on the identified safety subspace.","We validate that our safety realignment framework satisfies the safety requirements of a single fine-tuned model as well as multiple models during their fusion.","Our findings confirm that SOMF preserves safety without notably compromising performance on downstream tasks, including instruction following in Chinese, English, and Hindi, as well as problem-solving capabilities in Code and Math."],"url":"http://arxiv.org/abs/2405.09055v1","category":"cs.CL"}
{"created":"2024-05-15 02:58:19","title":"Dielectric Tensor Prediction for Inorganic Materials Using Latent Information from Preferred Potential","abstract":"Dielectrics are materials with widespread applications in flash memory, central processing units, photovoltaics, capacitors, etc. However, the availability of public dielectric data remains limited, hindering research and development efforts. Previously, machine learning models focused on predicting dielectric constants as scalars, overlooking the importance of dielectric tensors in understanding material properties under directional electric fields for material design and simulation. This study demonstrates the value of common equivariant structural embedding features derived from a universal neural network potential in enhancing the prediction of dielectric properties. To integrate channel information from various-rank latent features while preserving the desired SE(3) equivariance to the second-rank dielectric tensors, we design an equivariant readout decoder to predict the total, electronic, and ionic dielectric tensors individually, and compare our model with the state-of-the-art models. Finally, we evaluate our model by conducting virtual screening on thermodynamical stable structure candidates in Materials Project. The material Ba\\textsubscript{2}SmTaO\\textsubscript{6} with large band gaps ($E_g=3.36 \\mathrm{eV}$) and dielectric constants ($\\epsilon=93.81$) is successfully identified out of the 14k candidate set. The results show that our methods give good accuracy on predicting dielectric tensors of inorganic materials, emphasizing their potential in contributing to the discovery of novel dielectrics.","sentences":["Dielectrics are materials with widespread applications in flash memory, central processing units, photovoltaics, capacitors, etc.","However, the availability of public dielectric data remains limited, hindering research and development efforts.","Previously, machine learning models focused on predicting dielectric constants as scalars, overlooking the importance of dielectric tensors in understanding material properties under directional electric fields for material design and simulation.","This study demonstrates the value of common equivariant structural embedding features derived from a universal neural network potential in enhancing the prediction of dielectric properties.","To integrate channel information from various-rank latent features while preserving the desired SE(3) equivariance to the second-rank dielectric tensors, we design an equivariant readout decoder to predict the total, electronic, and ionic dielectric tensors individually, and compare our model with the state-of-the-art models.","Finally, we evaluate our model by conducting virtual screening on thermodynamical stable structure candidates in Materials Project.","The material Ba\\textsubscript{2}SmTaO\\textsubscript{6} with large band gaps ($E_g=3.36 \\mathrm{eV}$) and dielectric constants ($\\epsilon=93.81$) is successfully identified out of the 14k candidate set.","The results show that our methods give good accuracy on predicting dielectric tensors of inorganic materials, emphasizing their potential in contributing to the discovery of novel dielectrics."],"url":"http://arxiv.org/abs/2405.09052v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-15 02:46:04","title":"AMSNet: Netlist Dataset for AMS Circuits","abstract":"Today's analog/mixed-signal (AMS) integrated circuit (IC) designs demand substantial manual intervention. The advent of multimodal large language models (MLLMs) has unveiled significant potential across various fields, suggesting their applicability in streamlining large-scale AMS IC design as well. A bottleneck in employing MLLMs for automatic AMS circuit generation is the absence of a comprehensive dataset delineating the schematic-netlist relationship. We therefore design an automatic technique for converting schematics into netlists, and create dataset AMSNet, encompassing transistor-level schematics and corresponding SPICE format netlists. With a growing size, AMSNet can significantly facilitate exploration of MLLM applications in AMS circuit design. We have made an initial set of netlists public, and will make both our netlist generation tool and the full dataset available upon publishing of this paper.","sentences":["Today's analog/mixed-signal (AMS) integrated circuit (IC) designs demand substantial manual intervention.","The advent of multimodal large language models (MLLMs) has unveiled significant potential across various fields, suggesting their applicability in streamlining large-scale AMS IC design as well.","A bottleneck in employing MLLMs for automatic AMS circuit generation is the absence of a comprehensive dataset delineating the schematic-netlist relationship.","We therefore design an automatic technique for converting schematics into netlists, and create dataset AMSNet, encompassing transistor-level schematics and corresponding SPICE format netlists.","With a growing size, AMSNet can significantly facilitate exploration of MLLM applications in AMS circuit design.","We have made an initial set of netlists public, and will make both our netlist generation tool and the full dataset available upon publishing of this paper."],"url":"http://arxiv.org/abs/2405.09045v1","category":"cs.CV"}
{"created":"2024-05-15 02:19:34","title":"SMART: Towards Pre-trained Missing-Aware Model for Patient Health Status Prediction","abstract":"Electronic health record (EHR) data has emerged as a valuable resource for analyzing patient health status. However, the prevalence of missing data in EHR poses significant challenges to existing methods, leading to spurious correlations and suboptimal predictions. While various imputation techniques have been developed to address this issue, they often obsess unnecessary details and may introduce additional noise when making clinical predictions. To tackle this problem, we propose SMART, a Self-Supervised Missing-Aware RepresenTation Learning approach for patient health status prediction, which encodes missing information via elaborated attentions and learns to impute missing values through a novel self-supervised pre-training approach that reconstructs missing data representations in the latent space. By adopting missing-aware attentions and focusing on learning higher-order representations, SMART promotes better generalization and robustness to missing data. We validate the effectiveness of SMART through extensive experiments on six EHR tasks, demonstrating its superiority over state-of-the-art methods.","sentences":["Electronic health record (EHR) data has emerged as a valuable resource for analyzing patient health status.","However, the prevalence of missing data in EHR poses significant challenges to existing methods, leading to spurious correlations and suboptimal predictions.","While various imputation techniques have been developed to address this issue, they often obsess unnecessary details and may introduce additional noise when making clinical predictions.","To tackle this problem, we propose SMART, a Self-Supervised Missing-Aware RepresenTation Learning approach for patient health status prediction, which encodes missing information via elaborated attentions and learns to impute missing values through a novel self-supervised pre-training approach that reconstructs missing data representations in the latent space.","By adopting missing-aware attentions and focusing on learning higher-order representations, SMART promotes better generalization and robustness to missing data.","We validate the effectiveness of SMART through extensive experiments on six EHR tasks, demonstrating its superiority over state-of-the-art methods."],"url":"http://arxiv.org/abs/2405.09039v1","category":"cs.LG"}
{"created":"2024-05-15 02:05:07","title":"Construction of special Lagrangian submanifolds of the Taub-NUT manifold and the Atiyah-Hitchin manifold","abstract":"We construct special Lagrangian submanifolds of the Taub-NUT manifold and the Atiyah-Hitchin manifold by combining the generalized Legendre transform approach and the moment map technique. The generalized Legendre transform approach provides a formulation to construct hyperk\\\"ahler manifolds and can make their Calabi-Yau structures manifest. In this approach, the K\\\"ahler $2$-forms and the holomorphic volume forms can be written in terms of holomorphic coordinates, which are convenient to employ the moment map technique. This technique derives the condition that a submanifold in the Calabi-Yau manifold is special Lagrangian. For the Taub-NUT manifold and the Atiyah-Hitchin manifold, by the moment map technique, special Lagrangian submanifolds are obtained as a one-parameter family of the orbits corresponding to Hamiltonian action with respect to their K\\\"ahler 2-forms. The resultant special Lagrangian submanifolds have cohomogeneity-one symmetry. To demonstrate that our method is useful, we recover the conditions for the special Lagrangian submanifold of the Taub-NUT manifold which is invariant under the tri-holomorphic $U(1)$ symmetry. As new applications of our method, we construct special Lagrangian submanifolds of the Taub-NUT manifold and the Atiyah-Hitchin manifold which are invariant under the action of a Lie subgroup of $SO(3)$. In these constructions, our conditions for being special Lagrangian are expressed by ordinary differential equations (ODEs) with respect to the one-parameters. We numerically give solution curves for the ODEs which specify the special Lagrangian submanifolds for the above cases.","sentences":["We construct special Lagrangian submanifolds of the Taub-NUT manifold and the Atiyah-Hitchin manifold by combining the generalized Legendre transform approach and the moment map technique.","The generalized Legendre transform approach provides a formulation to construct hyperk\\\"ahler manifolds and can make their Calabi-Yau structures manifest.","In this approach, the K\\\"ahler $2$-forms and the holomorphic volume forms can be written in terms of holomorphic coordinates, which are convenient to employ the moment map technique.","This technique derives the condition that a submanifold in the Calabi-Yau manifold is special Lagrangian.","For the Taub-NUT manifold and the Atiyah-Hitchin manifold, by the moment map technique, special Lagrangian submanifolds are obtained as a one-parameter family of the orbits corresponding to Hamiltonian action with respect to their K\\\"ahler 2-forms.","The resultant special Lagrangian submanifolds have cohomogeneity-one symmetry.","To demonstrate that our method is useful, we recover the conditions for the special Lagrangian submanifold of the Taub-NUT manifold which is invariant under the tri-holomorphic $U(1)$ symmetry.","As new applications of our method, we construct special Lagrangian submanifolds of the Taub-NUT manifold and the Atiyah-Hitchin manifold which are invariant under the action of a Lie subgroup of $SO(3)$. In these constructions, our conditions for being special Lagrangian are expressed by ordinary differential equations (ODEs) with respect to the one-parameters.","We numerically give solution curves for the ODEs which specify the special Lagrangian submanifolds for the above cases."],"url":"http://arxiv.org/abs/2405.09036v1","category":"math-ph"}
{"created":"2024-05-15 02:03:50","title":"Accelerating Decision Diagram-based Multi-node Quantum Simulation with Ring Communication and Automatic SWAP Insertion","abstract":"An N-bit quantum state requires a vector of length $2^N$, leading to an exponential increase in the required memory with N in conventional statevector-based quantum simulators. A proposed solution to this issue is the decision diagram-based quantum simulator, which can significantly decrease the necessary memory and is expected to operate faster for specific quantum circuits. However, decision diagram-based quantum simulators are not easily parallelizable because data must be manipulated dynamically, and most implementations run on one thread. This paper introduces ring communication-based optimal parallelization and automatic swap insertion techniques for multi-node implementation of decision diagram-based quantum simulators. The ring communication approach is designed so that each node communicates with its neighboring nodes, which can facilitate faster and more parallel communication than broadcasting where one node needs to communicate with all nodes simultaneously. The automatic swap insertion method, an approach to minimize inter-node communication, has been employed in existing multi-node state vector-based simulators, but this paper proposes two methods specifically designed for decision diagram-based quantum simulators. These techniques were implemented and evaluated using the Shor algorithm and random circuits with up to 38 qubits using a maximum of 256 nodes. The experimental results have revealed that multi-node implementation can reduce run-time by up to 26 times. For example, Shor circuits that need 38 qubits can finish simulation in 147 seconds. Additionally, it was shown that ring communication has a higher speed-up effect than broadcast communication, and the importance of selecting the appropriate automatic swap insertion method was revealed.","sentences":["An N-bit quantum state requires a vector of length $2^N$, leading to an exponential increase in the required memory with N in conventional statevector-based quantum simulators.","A proposed solution to this issue is the decision diagram-based quantum simulator, which can significantly decrease the necessary memory and is expected to operate faster for specific quantum circuits.","However, decision diagram-based quantum simulators are not easily parallelizable because data must be manipulated dynamically, and most implementations run on one thread.","This paper introduces ring communication-based optimal parallelization and automatic swap insertion techniques for multi-node implementation of decision diagram-based quantum simulators.","The ring communication approach is designed so that each node communicates with its neighboring nodes, which can facilitate faster and more parallel communication than broadcasting where one node needs to communicate with all nodes simultaneously.","The automatic swap insertion method, an approach to minimize inter-node communication, has been employed in existing multi-node state vector-based simulators, but this paper proposes two methods specifically designed for decision diagram-based quantum simulators.","These techniques were implemented and evaluated using the Shor algorithm and random circuits with up to 38 qubits using a maximum of 256 nodes.","The experimental results have revealed that multi-node implementation can reduce run-time by up to 26 times.","For example, Shor circuits that need 38 qubits can finish simulation in 147 seconds.","Additionally, it was shown that ring communication has a higher speed-up effect than broadcast communication, and the importance of selecting the appropriate automatic swap insertion method was revealed."],"url":"http://arxiv.org/abs/2405.09033v1","category":"quant-ph"}
{"created":"2024-05-15 02:03:44","title":"ICAL: Implicit Character-Aided Learning for Enhanced Handwritten Mathematical Expression Recognition","abstract":"Significant progress has been made in the field of handwritten mathematical expression recognition, while existing encoder-decoder methods are usually difficult to model global information in \\LaTeX. Therefore, this paper introduces a novel approach, Implicit Character-Aided Learning (ICAL), to mine the global expression information and enhance handwritten mathematical expression recognition. Specifically, we propose the Implicit Character Construction Module (ICCM) to predict implicit character sequences and use a Fusion Module to merge the outputs of the ICCM and the decoder, thereby producing corrected predictions. By modeling and utilizing implicit character information, ICAL achieves a more accurate and context-aware interpretation of handwritten mathematical expressions. Experimental results demonstrate that ICAL notably surpasses the state-of-the-art(SOTA) models, improving the expression recognition rate (ExpRate) by 2.21\\%/1.75\\%/1.28\\% on the CROHME 2014/2016/2019 datasets respectively, and achieves a remarkable 69.25\\% on the challenging HME100k test set. We make our code available on the GitHub: https://github.com/qingzhenduyu/ICAL","sentences":["Significant progress has been made in the field of handwritten mathematical expression recognition, while existing encoder-decoder methods are usually difficult to model global information in \\LaTeX. Therefore, this paper introduces a novel approach, Implicit Character-Aided Learning (ICAL), to mine the global expression information and enhance handwritten mathematical expression recognition.","Specifically, we propose the Implicit Character Construction Module (ICCM) to predict implicit character sequences and use a Fusion Module to merge the outputs of the ICCM and the decoder, thereby producing corrected predictions.","By modeling and utilizing implicit character information, ICAL achieves a more accurate and context-aware interpretation of handwritten mathematical expressions.","Experimental results demonstrate that ICAL notably surpasses the state-of-the-art(SOTA) models, improving the expression recognition rate (ExpRate) by 2.21\\%/1.75\\%/1.28\\% on the CROHME 2014/2016/2019 datasets respectively, and achieves a remarkable 69.25\\% on the challenging HME100k test set.","We make our code available on the GitHub:","https://github.com/qingzhenduyu/ICAL"],"url":"http://arxiv.org/abs/2405.09032v1","category":"cs.CV"}
{"created":"2024-05-15 01:33:22","title":"Group-Graded Twisted Calabi-Yau Algebras","abstract":"Historically, the study of graded (twisted or otherwise) Calabi-Yau algebras has meant the study of such algebras under an $\\mathbb{N}$-grading. In this paper, we propose a suitable definition for a twisted $G$-graded Calabi-Yau algebra, for $G$ an arbitrary abelian group. Building on the work of Reyes and Rogalski, we show that a $G$-graded algebra is twisted Calabi-Yau if and only if it is $G$-graded twisted Calabi-Yau. In the second half of the paper, we prove that localizations of twisted Calabi-Yau algebras at elements which form both left and right denominator sets remain twisted Calabi-Yau. As such, we obtain a large class of $\\mathbb{Z}$-graded twisted Calabi-Yau algebras arising as localizations of Artin-Schelter regular algebras. Throughout the paper, we survey a number of concrete examples of $G$-graded twisted Calabi-Yau algebras, including the Weyl algebras, families of generalized Weyl algebras, and universal enveloping algebras of finite dimensional Lie algebras.","sentences":["Historically, the study of graded (twisted or otherwise) Calabi-Yau algebras has meant the study of such algebras under an $\\mathbb{N}$-grading.","In this paper, we propose a suitable definition for a twisted $G$-graded Calabi-Yau algebra, for $G$ an arbitrary abelian group.","Building on the work of Reyes and Rogalski, we show that a $G$-graded algebra is twisted Calabi-Yau if and only if it is $G$-graded twisted Calabi-Yau.","In the second half of the paper, we prove that localizations of twisted Calabi-Yau algebras at elements which form both left and right denominator sets remain twisted Calabi-Yau.","As such, we obtain a large class of $\\mathbb{Z}$-graded twisted Calabi-Yau algebras arising as localizations of Artin-Schelter regular algebras.","Throughout the paper, we survey a number of concrete examples of $G$-graded twisted Calabi-Yau algebras, including the Weyl algebras, families of generalized Weyl algebras, and universal enveloping algebras of finite dimensional Lie algebras."],"url":"http://arxiv.org/abs/2405.09025v1","category":"math.RA"}
{"created":"2024-05-15 01:27:42","title":"The Rise of Recommerce: Ownership and Sustainability with Overlapping Generations","abstract":"The emergence of the branded recommerce channel - digitally enabled and branded marketplaces that facilitate purchasing pre-owned items directly from a manufacturer's e-commerce site - leads to new variants of classic IS and economic questions relating to secondary markets. Such branded recommerce is increasingly platform-enabled, creating opportunities for greater sustainability and stronger brand experience control but posing a greater risk of cannibalization of the sales of new items. We model the effects that the sales of pre-owned items have on market segmentation and product durability choices for a monopolist facing heterogeneous customers, contrasting outcomes when the trade of pre-owned goods takes place through a third-party marketplace with outcomes under branded recommerce. We show that the direct revenue benefits of branded recommerce are not their primary source of value to the monopolist, and rather, there are three indirect effects that alter profits and sustainability. Product durability increases, a seller finds it optimal to forgo marketplace fees altogether, and there are greater seller incentives to lower the quality uncertainty associated with pre-owned items. We establish these results for a simple two-period model as well as developing a new infinite horizon model with overlapping generations. Our paper sheds new insight into this emerging digital channel phenomenon, underscoring the importance of recommerce platforms in aligning seller profits with sustainability goals.","sentences":["The emergence of the branded recommerce channel - digitally enabled and branded marketplaces that facilitate purchasing pre-owned items directly from a manufacturer's e-commerce site - leads to new variants of classic IS and economic questions relating to secondary markets.","Such branded recommerce is increasingly platform-enabled, creating opportunities for greater sustainability and stronger brand experience control but posing a greater risk of cannibalization of the sales of new items.","We model the effects that the sales of pre-owned items have on market segmentation and product durability choices for a monopolist facing heterogeneous customers, contrasting outcomes when the trade of pre-owned goods takes place through a third-party marketplace with outcomes under branded recommerce.","We show that the direct revenue benefits of branded recommerce are not their primary source of value to the monopolist, and rather, there are three indirect effects that alter profits and sustainability.","Product durability increases, a seller finds it optimal to forgo marketplace fees altogether, and there are greater seller incentives to lower the quality uncertainty associated with pre-owned items.","We establish these results for a simple two-period model as well as developing a new infinite horizon model with overlapping generations.","Our paper sheds new insight into this emerging digital channel phenomenon, underscoring the importance of recommerce platforms in aligning seller profits with sustainability goals."],"url":"http://arxiv.org/abs/2405.09023v1","category":"econ.GN"}
{"created":"2024-05-15 01:09:48","title":"Tails of extinction time and maximal displacement for critical branching killed L\u00e9vy process","abstract":"In this paper, we study asymptotic behaviors of the tails of extinction time and maximal displacement of a critical branching killed L\\'{e}vy process $(Z_t^{(0,\\infty)})_{t\\ge 0}$ in $\\mathbb{R}$, in which all particles (and their descendants) are killed upon exiting $(0, \\infty)$. Let $\\zeta^{(0,\\infty)}$ and $M_t^{(0,\\infty)}$ be the extinction time and maximal position of all the particles alive at time $t$ of this branching killed L\\'{e}vy process and define $M^{(0,\\infty)}: = \\sup_{t\\geq 0} M_t^{(0,\\infty)}$. Under the assumption that the offspring distribution belongs to the domain of attraction of an $\\alpha$-stable distribution, $\\alpha\\in (1, 2]$, and some moment conditions on the spatial motion, we give the decay rates of the survival probabilities $$ \\mathbb{P}_{y}(\\zeta^{(0,\\infty)}>t), \\quad \\mathbb{P}_{\\sqrt{t}y}(\\zeta^{(0,\\infty)}>t) $$ and the tail probabilities $$ \\mathbb{P}_{y}(M^{(0,\\infty)}\\geq x), \\quad \\mathbb{P}_{xy}(M^{(0,\\infty)}\\geq x). $$ We also study the scaling limits of $M_t^{(0,\\infty)}$ and the point process $Z_t^{(0,\\infty)}$ under $\\mathbb{P}_{\\sqrt{t}y}(\\cdot |\\zeta^{(0,\\infty)}>t)$ and $\\mathbb{P}_y(\\cdot |\\zeta^{(0,\\infty)}>t)$. The scaling limits under $\\mathbb{P}_{\\sqrt{t}y}(\\cdot |\\zeta^{(0,\\infty)}>t)$ are represented in terms of super killed Brownian motion.","sentences":["In this paper, we study asymptotic behaviors of the tails of extinction time and maximal displacement of a critical branching killed L\\'{e}vy process $(Z_t^{(0,\\infty)})_{t\\ge 0}$ in $\\mathbb{R}$, in which all particles (and their descendants) are killed upon exiting $(0, \\infty)$. Let $\\zeta^{(0,\\infty)}$ and $M_t^{(0,\\infty)}$ be the extinction time and maximal position of all the particles alive at time $t$ of this branching killed L\\'{e}vy process and define $M^{(0,\\infty)}: = \\sup_{t\\geq 0} M_t^{(0,\\infty)}$. Under the assumption that the offspring distribution belongs to the domain of attraction of an $\\alpha$-stable distribution, $\\alpha\\in (1, 2]$, and some moment conditions on the spatial motion, we give the decay rates of the survival probabilities $$ \\mathbb{P}_{y}(\\zeta^{(0,\\infty)}>t), \\quad \\mathbb{P}_{\\sqrt{t}y}(\\zeta^{(0,\\infty)}>t) $$ and the tail probabilities $$ \\mathbb{P}_{y}(M^{(0,\\infty)}\\geq x), \\quad \\mathbb{P}_{xy}(M^{(0,\\infty)}\\geq x).","$$ We also study the scaling limits of $M_t^{(0,\\infty)}$ and the point process $Z_t^{(0,\\infty)}$ under $\\mathbb{P}_{\\sqrt{t}y}(\\cdot |\\zeta^{(0,\\infty)}>t)$ and $\\mathbb{P}_y(\\cdot |\\zeta^{(0,\\infty)}>t)$. The scaling limits under $\\mathbb{P}_{\\sqrt{t}y}(\\cdot |\\zeta^{(0,\\infty)}>t)$ are represented in terms of super killed Brownian motion."],"url":"http://arxiv.org/abs/2405.09019v1","category":"math.PR"}
{"created":"2024-05-15 00:17:48","title":"Spatial Semantic Recurrent Mining for Referring Image Segmentation","abstract":"Referring Image Segmentation (RIS) consistently requires language and appearance semantics to more understand each other. The need becomes acute especially under hard situations. To achieve, existing works tend to resort to various trans-representing mechanisms to directly feed forward language semantic along main RGB branch, which however will result in referent distribution weakly-mined in space and non-referent semantic contaminated along channel. In this paper, we propose Spatial Semantic Recurrent Mining (S\\textsuperscript{2}RM) to achieve high-quality cross-modality fusion. It follows a working strategy of trilogy: distributing language feature, spatial semantic recurrent coparsing, and parsed-semantic balancing. During fusion, S\\textsuperscript{2}RM will first generate a constraint-weak yet distribution-aware language feature, then bundle features of each row and column from rotated features of one modality context to recurrently correlate relevant semantic contained in feature from other modality context, and finally resort to self-distilled weights to weigh on the contributions of different parsed semantics. Via coparsing, S\\textsuperscript{2}RM transports information from the near and remote slice layers of generator context to the current slice layer of parsed context, capable of better modeling global relationship bidirectional and structured. Besides, we also propose a Cross-scale Abstract Semantic Guided Decoder (CASG) to emphasize the foreground of the referent, finally integrating different grained features at a comparatively low cost. Extensive experimental results on four current challenging datasets show that our proposed method performs favorably against other state-of-the-art algorithms.","sentences":["Referring Image Segmentation (RIS) consistently requires language and appearance semantics to more understand each other.","The need becomes acute especially under hard situations.","To achieve, existing works tend to resort to various trans-representing mechanisms to directly feed forward language semantic along main RGB branch, which however will result in referent distribution weakly-mined in space and non-referent semantic contaminated along channel.","In this paper, we propose Spatial Semantic Recurrent Mining (S\\textsuperscript{2}RM) to achieve high-quality cross-modality fusion.","It follows a working strategy of trilogy: distributing language feature, spatial semantic recurrent coparsing, and parsed-semantic balancing.","During fusion, S\\textsuperscript{2}RM will first generate a constraint-weak yet distribution-aware language feature, then bundle features of each row and column from rotated features of one modality context to recurrently correlate relevant semantic contained in feature from other modality context, and finally resort to self-distilled weights to weigh on the contributions of different parsed semantics.","Via coparsing, S\\textsuperscript{2}RM transports information from the near and remote slice layers of generator context to the current slice layer of parsed context, capable of better modeling global relationship bidirectional and structured.","Besides, we also propose a Cross-scale Abstract Semantic Guided Decoder (CASG) to emphasize the foreground of the referent, finally integrating different grained features at a comparatively low cost.","Extensive experimental results on four current challenging datasets show that our proposed method performs favorably against other state-of-the-art algorithms."],"url":"http://arxiv.org/abs/2405.09006v1","category":"cs.CV"}
{"created":"2024-05-14 23:46:44","title":"Puffy Venuses: the Mass-Radius Impact of Carbon-Rich Atmospheres on Lava Worlds","abstract":"The recent advancements in exoplanet observations enable the potential detection of exo-Venuses, rocky planets with carbon-rich atmospheres. How extended these atmospheres can be, given high carbon abundances, has not been studied. To answer this, we present a model for a theoretical class of exoplanets - puffy Venuses - characterized by thick, carbon-dominated atmospheres in equilibrium with global magma oceans. Our model accounts for carbon and hydrogen partition between the atmosphere and the magma ocean, as well as the C-H-O equilibrium chemistry throughout a semi-grey, radiative-convective atmosphere. We find that radius inflation by puffy Venus atmospheres is significant on small and irradiated planets: carbon content of 1200 ppm (or that of ordinary chondrites) can generate an atmosphere of ~0.16 - 0.3 $R_{\\oplus}$ for an Earth-mass planet with equilibrium temperatures of 1500 to 2000 K. We identify TOI-561 b as an especially promising puffy Venus candidate, whose under-density could be attributed to a thick C-rich atmosphere. We also advocate for a puffy Venus interpretation of 55 Cancri e, where recent JWST observation indicates the presence of a CO/CO2 atmosphere. Puffy Venuses may thus constitute a testable alternative interpretation for the interior structure of underdense low-mass exoplanets.","sentences":["The recent advancements in exoplanet observations enable the potential detection of exo-Venuses, rocky planets with carbon-rich atmospheres.","How extended these atmospheres can be, given high carbon abundances, has not been studied.","To answer this, we present a model for a theoretical class of exoplanets - puffy Venuses - characterized by thick, carbon-dominated atmospheres in equilibrium with global magma oceans.","Our model accounts for carbon and hydrogen partition between the atmosphere and the magma ocean, as well as the C-H-O equilibrium chemistry throughout a semi-grey, radiative-convective atmosphere.","We find that radius inflation by puffy Venus atmospheres is significant on small and irradiated planets: carbon content of 1200 ppm (or that of ordinary chondrites) can generate an atmosphere of ~0.16 - 0.3 $R_{\\oplus}$ for an Earth-mass planet with equilibrium temperatures of 1500 to 2000","K.","We identify TOI-561 b as an especially promising puffy Venus candidate, whose under-density could be attributed to a thick C-rich atmosphere.","We also advocate for a puffy Venus interpretation of 55 Cancri e, where recent JWST observation indicates the presence of a CO/CO2 atmosphere.","Puffy Venuses may thus constitute a testable alternative interpretation for the interior structure of underdense low-mass exoplanets."],"url":"http://arxiv.org/abs/2405.08998v1","category":"astro-ph.EP"}
{"created":"2024-05-14 23:41:24","title":"Learning Correspondence for Deformable Objects","abstract":"We investigate the problem of pixelwise correspondence for deformable objects, namely cloth and rope, by comparing both classical and learning-based methods. We choose cloth and rope because they are traditionally some of the most difficult deformable objects to analytically model with their large configuration space, and they are meaningful in the context of robotic tasks like cloth folding, rope knot-tying, T-shirt folding, curtain closing, etc. The correspondence problem is heavily motivated in robotics, with wide-ranging applications including semantic grasping, object tracking, and manipulation policies built on top of correspondences. We present an exhaustive survey of existing classical methods for doing correspondence via feature-matching, including SIFT, SURF, and ORB, and two recently published learning-based methods including TimeCycle and Dense Object Nets. We make three main contributions: (1) a framework for simulating and rendering synthetic images of deformable objects, with qualitative results demonstrating transfer between our simulated and real domains (2) a new learning-based correspondence method extending Dense Object Nets, and (3) a standardized comparison across state-of-the-art correspondence methods. Our proposed method provides a flexible, general formulation for learning temporally and spatially continuous correspondences for nonrigid (and rigid) objects. We report root mean squared error statistics for all methods and find that Dense Object Nets outperforms baseline classical methods for correspondence, and our proposed extension of Dense Object Nets performs similarly.","sentences":["We investigate the problem of pixelwise correspondence for deformable objects, namely cloth and rope, by comparing both classical and learning-based methods.","We choose cloth and rope because they are traditionally some of the most difficult deformable objects to analytically model with their large configuration space, and they are meaningful in the context of robotic tasks like cloth folding, rope knot-tying, T-shirt folding, curtain closing, etc.","The correspondence problem is heavily motivated in robotics, with wide-ranging applications including semantic grasping, object tracking, and manipulation policies built on top of correspondences.","We present an exhaustive survey of existing classical methods for doing correspondence via feature-matching, including SIFT, SURF, and ORB, and two recently published learning-based methods including TimeCycle and Dense Object Nets.","We make three main contributions: (1) a framework for simulating and rendering synthetic images of deformable objects, with qualitative results demonstrating transfer between our simulated and real domains (2) a new learning-based correspondence method extending Dense Object Nets, and (3) a standardized comparison across state-of-the-art correspondence methods.","Our proposed method provides a flexible, general formulation for learning temporally and spatially continuous correspondences for nonrigid (and rigid) objects.","We report root mean squared error statistics for all methods and find that Dense Object Nets outperforms baseline classical methods for correspondence, and our proposed extension of Dense Object Nets performs similarly."],"url":"http://arxiv.org/abs/2405.08996v1","category":"cs.CV"}
{"created":"2024-05-14 23:24:12","title":"Contextual Emotion Recognition using Large Vision Language Models","abstract":"\"How does the person in the bounding box feel?\" Achieving human-level recognition of the apparent emotion of a person in real world situations remains an unsolved task in computer vision. Facial expressions are not enough: body pose, contextual knowledge, and commonsense reasoning all contribute to how humans perform this emotional theory of mind task. In this paper, we examine two major approaches enabled by recent large vision language models: 1) image captioning followed by a language-only LLM, and 2) vision language models, under zero-shot and fine-tuned setups. We evaluate the methods on the Emotions in Context (EMOTIC) dataset and demonstrate that a vision language model, fine-tuned even on a small dataset, can significantly outperform traditional baselines. The results of this work aim to help robots and agents perform emotionally sensitive decision-making and interaction in the future.","sentences":["\"How does the person in the bounding box feel?\"","Achieving human-level recognition of the apparent emotion of a person in real world situations remains an unsolved task in computer vision.","Facial expressions are not enough: body pose, contextual knowledge, and commonsense reasoning all contribute to how humans perform this emotional theory of mind task.","In this paper, we examine two major approaches enabled by recent large vision language models: 1) image captioning followed by a language-only LLM, and 2) vision language models, under zero-shot and fine-tuned setups.","We evaluate the methods on the Emotions in Context (EMOTIC) dataset and demonstrate that a vision language model, fine-tuned even on a small dataset, can significantly outperform traditional baselines.","The results of this work aim to help robots and agents perform emotionally sensitive decision-making and interaction in the future."],"url":"http://arxiv.org/abs/2405.08992v1","category":"cs.CV"}
{"created":"2024-05-14 23:23:07","title":"Theoretical Analysis for Expectation-Maximization-Based Multi-Model 3D Registration","abstract":"We perform detailed theoretical analysis of an expectation-maximization-based algorithm recently proposed in for solving a variation of the 3D registration problem, named multi-model 3D registration. Despite having shown superior empirical results, did not theoretically justify the conditions under which the EM approach converges to the ground truth. In this project, we aim to close this gap by establishing such conditions. In particular, the analysis revolves around the usage of probabilistic tail bounds that are developed and applied in various instances throughout the course. The problem studied in this project stands as another example, different from those seen in the course, in which tail-bounds help advance our algorithmic understanding in a probabilistic way. We provide self-contained background materials on 3D Registration","sentences":["We perform detailed theoretical analysis of an expectation-maximization-based algorithm recently proposed in for solving a variation of the 3D registration problem, named multi-model 3D registration.","Despite having shown superior empirical results, did not theoretically justify the conditions under which the EM approach converges to the ground truth.","In this project, we aim to close this gap by establishing such conditions.","In particular, the analysis revolves around the usage of probabilistic tail bounds that are developed and applied in various instances throughout the course.","The problem studied in this project stands as another example, different from those seen in the course, in which tail-bounds help advance our algorithmic understanding in a probabilistic way.","We provide self-contained background materials on 3D Registration"],"url":"http://arxiv.org/abs/2405.08991v1","category":"cs.CV"}
{"created":"2024-05-14 22:32:51","title":"Enabling Leakage Reduction via Fast and High-Fidelity Qutrit Readout","abstract":"Quantum Error Correction (QEC) is key to operating quantum processors effectively at practical scales. QECs are designed for systems comprising two-level systems, such as qubits, as their fundamental building block. Unfortunately, qubits can leak to third and higher energy levels, making these leaks challenging to detect and mitigate. If not addressed promptly, these leakage errors can proliferate and undermine QEC, leading to significant computational inaccuracies. Here, we present a high-fidelity three-level qubit readout protocol that is simple to implement on dedicated hardware such as FPGAs. Our design enables faster and higher-fidelity leakage detection over approaches using conventional qubit-state discriminators.","sentences":["Quantum Error Correction (QEC) is key to operating quantum processors effectively at practical scales.","QECs are designed for systems comprising two-level systems, such as qubits, as their fundamental building block.","Unfortunately, qubits can leak to third and higher energy levels, making these leaks challenging to detect and mitigate.","If not addressed promptly, these leakage errors can proliferate and undermine QEC, leading to significant computational inaccuracies.","Here, we present a high-fidelity three-level qubit readout protocol that is simple to implement on dedicated hardware such as FPGAs.","Our design enables faster and higher-fidelity leakage detection over approaches using conventional qubit-state discriminators."],"url":"http://arxiv.org/abs/2405.08982v1","category":"quant-ph"}
{"created":"2024-05-14 22:27:12","title":"Impact of Design Decisions in Scanpath Modeling","abstract":"Modeling visual saliency in graphical user interfaces (GUIs) allows to understand how people perceive GUI designs and what elements attract their attention. One aspect that is often overlooked is the fact that computational models depend on a series of design parameters that are not straightforward to decide. We systematically analyze how different design parameters affect scanpath evaluation metrics using a state-of-the-art computational model (DeepGaze++). We particularly focus on three design parameters: input image size, inhibition-of-return decay, and masking radius. We show that even small variations of these design parameters have a noticeable impact on standard evaluation metrics such as DTW or Eyenalysis. These effects also occur in other scanpath models, such as UMSS and ScanGAN, and in other datasets such as MASSVIS. Taken together, our results put forward the impact of design decisions for predicting users' viewing behavior on GUIs.","sentences":["Modeling visual saliency in graphical user interfaces (GUIs) allows to understand how people perceive GUI designs and what elements attract their attention.","One aspect that is often overlooked is the fact that computational models depend on a series of design parameters that are not straightforward to decide.","We systematically analyze how different design parameters affect scanpath evaluation metrics using a state-of-the-art computational model (DeepGaze++).","We particularly focus on three design parameters: input image size, inhibition-of-return decay, and masking radius.","We show that even small variations of these design parameters have a noticeable impact on standard evaluation metrics such as DTW or Eyenalysis.","These effects also occur in other scanpath models, such as UMSS and ScanGAN, and in other datasets such as MASSVIS.","Taken together, our results put forward the impact of design decisions for predicting users' viewing behavior on GUIs."],"url":"http://arxiv.org/abs/2405.08981v1","category":"cs.HC"}
{"created":"2024-05-14 22:16:19","title":"General expressions for Stevens and Racah operator equivalents","abstract":"Several definitions of the crystal field have been used over time and their variety has lead to many misunderstandings, in both theoretical and experimental literature. Two categories of definitions can be mentioned, the first being the operators equivalents introduced by Stevens in 1952 and the second being the crystal-field operators, introduced by different authors from 1962 and expanded on the Racah spherical tensors. This paper aims at providing some clarification in this field. We first make a review of several expressions introduced in various references to compute crystal-field operators and we describe connections between them. Then, we introduce an explicit way to compute crystal-field operators, in terms of angular momentum quantum numbers $j$ and $m$ as well as in terms of $J^2$ and $J_z$ operators. We eventually give some connections between the Stevens operators equivalents and the crystal-field operators, and make usage of the coefficients of fractional parentage for the expression of the crystal-field operators for the many-body states. Various computational codes, using different crystal-field conventions, are also reviewed.","sentences":["Several definitions of the crystal field have been used over time and their variety has lead to many misunderstandings, in both theoretical and experimental literature.","Two categories of definitions can be mentioned, the first being the operators equivalents introduced by Stevens in 1952 and the second being the crystal-field operators, introduced by different authors from 1962 and expanded on the Racah spherical tensors.","This paper aims at providing some clarification in this field.","We first make a review of several expressions introduced in various references to compute crystal-field operators and we describe connections between them.","Then, we introduce an explicit way to compute crystal-field operators, in terms of angular momentum quantum numbers $j$ and","$m$ as well as in terms of $J^2$ and $J_z$ operators.","We eventually give some connections between the Stevens operators equivalents and the crystal-field operators, and make usage of the coefficients of fractional parentage for the expression of the crystal-field operators for the many-body states.","Various computational codes, using different crystal-field conventions, are also reviewed."],"url":"http://arxiv.org/abs/2405.08978v1","category":"physics.chem-ph"}
{"created":"2024-05-14 21:31:11","title":"Computation-Aware Kalman Filtering and Smoothing","abstract":"Kalman filtering and smoothing are the foundational mechanisms for efficient inference in Gauss-Markov models. However, their time and memory complexities scale prohibitively with the size of the state space. This is particularly problematic in spatiotemporal regression problems, where the state dimension scales with the number of spatial observations. Existing approximate frameworks leverage low-rank approximations of the covariance matrix. Since they do not model the error introduced by the computational approximation, their predictive uncertainty estimates can be overly optimistic. In this work, we propose a probabilistic numerical method for inference in high-dimensional Gauss-Markov models which mitigates these scaling issues. Our matrix-free iterative algorithm leverages GPU acceleration and crucially enables a tunable trade-off between computational cost and predictive uncertainty. Finally, we demonstrate the scalability of our method on a large-scale climate dataset.","sentences":["Kalman filtering and smoothing are the foundational mechanisms for efficient inference in Gauss-Markov models.","However, their time and memory complexities scale prohibitively with the size of the state space.","This is particularly problematic in spatiotemporal regression problems, where the state dimension scales with the number of spatial observations.","Existing approximate frameworks leverage low-rank approximations of the covariance matrix.","Since they do not model the error introduced by the computational approximation, their predictive uncertainty estimates can be overly optimistic.","In this work, we propose a probabilistic numerical method for inference in high-dimensional Gauss-Markov models which mitigates these scaling issues.","Our matrix-free iterative algorithm leverages GPU acceleration and crucially enables a tunable trade-off between computational cost and predictive uncertainty.","Finally, we demonstrate the scalability of our method on a large-scale climate dataset."],"url":"http://arxiv.org/abs/2405.08971v1","category":"cs.LG"}
{"created":"2024-05-14 20:56:05","title":"Learned radio interferometric imaging for varying visibility coverage","abstract":"With the next generation of interferometric telescopes, such as the Square Kilometre Array (SKA), the need for highly computationally efficient reconstruction techniques is particularly acute. The challenge in designing learned, data-driven reconstruction techniques for radio interferometry is that they need to be agnostic to the varying visibility coverages of the telescope, since these are different for each observation. Because of this, learned post-processing or learned unrolled iterative reconstruction methods must typically be retrained for each specific observation, amounting to a large computational overhead. In this work we develop learned post-processing and unrolled iterative methods for varying visibility coverages, proposing training strategies to make these methods agnostic to variations in visibility coverage with minimal to no fine-tuning. Learned post-processing techniques are heavily dependent on the prior information encoded in training data and generalise poorly to other visibility coverages. In contrast, unrolled iterative methods, which include the telescope measurement operator inside the network, achieve state-of-the-art reconstruction quality and computation time, generalising well to other coverages and require little to no fine-tuning. Furthermore, they generalise well to realistic radio observations and are able to reconstruct the high dynamic range of these images.","sentences":["With the next generation of interferometric telescopes, such as the Square Kilometre Array (SKA), the need for highly computationally efficient reconstruction techniques is particularly acute.","The challenge in designing learned, data-driven reconstruction techniques for radio interferometry is that they need to be agnostic to the varying visibility coverages of the telescope, since these are different for each observation.","Because of this, learned post-processing or learned unrolled iterative reconstruction methods must typically be retrained for each specific observation, amounting to a large computational overhead.","In this work we develop learned post-processing and unrolled iterative methods for varying visibility coverages, proposing training strategies to make these methods agnostic to variations in visibility coverage with minimal to no fine-tuning.","Learned post-processing techniques are heavily dependent on the prior information encoded in training data and generalise poorly to other visibility coverages.","In contrast, unrolled iterative methods, which include the telescope measurement operator inside the network, achieve state-of-the-art reconstruction quality and computation time, generalising well to other coverages and require little to no fine-tuning.","Furthermore, they generalise well to realistic radio observations and are able to reconstruct the high dynamic range of these images."],"url":"http://arxiv.org/abs/2405.08958v1","category":"astro-ph.IM"}
{"created":"2024-05-14 20:37:50","title":"Applications of Fast Magnetic Reconnection Models to the Atmospheres of the Sun and Protoplanetary Disks","abstract":"Partially-ionized plasmas consist of charged and neutral particles whose mutual collisions modify magnetic reconnection compared with the fully-ionized case. The collisions alter the rate and locations of the magnetic dissipation heating and the distribution of energies among the particles accelerated into the non-thermal tail. We examine the collisional regimes for the onset of fast reconnection in two environments: the partially-ionized layers of the solar atmosphere and the protoplanetary disks that are the birthplaces for planets around young stars. In both these environments, magnetic nulls readily develop into resistive current sheets in the regime where the charged and neutral particles are fully coupled by collisions, but the current sheets quickly break down under the ideal tearing instability. The current sheets collapse repeatedly, forming magnetic islands at successively smaller scales, till they enter a collisionally-decoupled regime where the magnetic energy is rapidly turned into heat and charged-particle kinetic energy. Small-scale, decoupled fast reconnection in the solar atmosphere may lead to preferential heating and energization of ions and electrons that escape into the corona. In protoplanetary disks such reconnection causes localized heating in the atmospheric layers that produce much of the infrared atomic and molecular line emission observed with the Spitzer and James Webb Space Telescopes.","sentences":["Partially-ionized plasmas consist of charged and neutral particles whose mutual collisions modify magnetic reconnection compared with the fully-ionized case.","The collisions alter the rate and locations of the magnetic dissipation heating and the distribution of energies among the particles accelerated into the non-thermal tail.","We examine the collisional regimes for the onset of fast reconnection in two environments: the partially-ionized layers of the solar atmosphere and the protoplanetary disks that are the birthplaces for planets around young stars.","In both these environments, magnetic nulls readily develop into resistive current sheets in the regime where the charged and neutral particles are fully coupled by collisions, but the current sheets quickly break down under the ideal tearing instability.","The current sheets collapse repeatedly, forming magnetic islands at successively smaller scales, till they enter a collisionally-decoupled regime where the magnetic energy is rapidly turned into heat and charged-particle kinetic energy.","Small-scale, decoupled fast reconnection in the solar atmosphere may lead to preferential heating and energization of ions and electrons that escape into the corona.","In protoplanetary disks such reconnection causes localized heating in the atmospheric layers that produce much of the infrared atomic and molecular line emission observed with the Spitzer and James Webb Space Telescopes."],"url":"http://arxiv.org/abs/2405.08951v1","category":"astro-ph.SR"}
{"created":"2024-05-14 20:10:32","title":"Dynamical systems and complex networks: A Koopman operator perspective","abstract":"The Koopman operator has entered and transformed many research areas over the last years. Although the underlying concept$\\unicode{x2013}$representing highly nonlinear dynamical systems by infinite-dimensional linear operators$\\unicode{x2013}$has been known for a long time, the availability of large data sets and efficient machine learning algorithms for estimating the Koopman operator from data make this framework extremely powerful and popular. Koopman operator theory allows us to gain insights into the characteristic global properties of a system without requiring detailed mathematical models. We will show how these methods can also be used to analyze complex networks and highlight relationships between Koopman operators and graph Laplacians.","sentences":["The Koopman operator has entered and transformed many research areas over the last years.","Although the underlying concept$\\unicode{x2013}$representing highly nonlinear dynamical systems by infinite-dimensional linear operators$\\unicode{x2013}$has been known for a long time, the availability of large data sets and efficient machine learning algorithms for estimating the Koopman operator from data make this framework extremely powerful and popular.","Koopman operator theory allows us to gain insights into the characteristic global properties of a system without requiring detailed mathematical models.","We will show how these methods can also be used to analyze complex networks and highlight relationships between Koopman operators and graph Laplacians."],"url":"http://arxiv.org/abs/2405.08940v1","category":"math.DS"}
{"created":"2024-05-14 20:09:25","title":"Pointwise Lipschitz Continuous Graph Algorithms via Proximal Gradient Analysis","abstract":"In many real-world applications, it is prohibitively expensive to drastically change the solution to a problem after a small perturbation in the environment. Therefore, the stability of an algorithm is a very desirable property. In this paper, we study the class of pointwise Lipschitz continuous algorithms as introduced in the recent work of Kumabe and Yoshida [KY23b, FOCS'23]. The Lipschitz constant of an algorithm, intuitively, bounds the ratio of the changes in its output (measured in $\\ell_1$ distance) over the perturbations of its input. Prior to our work, most of the attention was focused on the weighted setting whereas only the maximum bipartite matching and the minimum spanning tree problems were studied in the unweighted which is our focus.   In this paper, we give a general and simple framework for bounding the Lipschitz constant of algorithms measured through the unweighted $\\ell_1$ distance of their outputs. Our approach consists of three main steps. First, we consider a natural continuous relaxation of the underlying graph problem by adding a smooth and strongly convex regularizer to the objective function. Then, we give upper bounds on the $\\ell_1$ distance of the optimal solutions of the convex programs, under small perturbations of the weights, via a stability analysis of the trajectory of the proximal gradient method. Finally, we present new problem-specific rounding techniques to obtain integral solutions to several graph problems that approximately maintain the stability guarantees of the fractional solutions. We apply our framework to a number of problems including minimum $s$-$t$ cut, multiway cut, densest subgraph, maximum ($b$-)matching, and packing integer programs. To complement our algorithms, we show the tightness of our results for certain problems by establishing matching lower bounds.","sentences":["In many real-world applications, it is prohibitively expensive to drastically change the solution to a problem after a small perturbation in the environment.","Therefore, the stability of an algorithm is a very desirable property.","In this paper, we study the class of pointwise Lipschitz continuous algorithms as introduced in the recent work of Kumabe and Yoshida","[KY23b, FOCS'23].","The Lipschitz constant of an algorithm, intuitively, bounds the ratio of the changes in its output (measured in $\\ell_1$ distance) over the perturbations of its input.","Prior to our work, most of the attention was focused on the weighted setting whereas only the maximum bipartite matching and the minimum spanning tree problems were studied in the unweighted which is our focus.   ","In this paper, we give a general and simple framework for bounding the Lipschitz constant of algorithms measured through the unweighted $\\ell_1$ distance of their outputs.","Our approach consists of three main steps.","First, we consider a natural continuous relaxation of the underlying graph problem by adding a smooth and strongly convex regularizer to the objective function.","Then, we give upper bounds on the $\\ell_1$ distance of the optimal solutions of the convex programs, under small perturbations of the weights, via a stability analysis of the trajectory of the proximal gradient method.","Finally, we present new problem-specific rounding techniques to obtain integral solutions to several graph problems that approximately maintain the stability guarantees of the fractional solutions.","We apply our framework to a number of problems including minimum $s$-$t$ cut, multiway cut, densest subgraph, maximum ($b$-)matching, and packing integer programs.","To complement our algorithms, we show the tightness of our results for certain problems by establishing matching lower bounds."],"url":"http://arxiv.org/abs/2405.08938v1","category":"cs.DS"}
{"created":"2024-05-14 20:05:53","title":"Necklaces over a group with identity product","abstract":"We address two variants of the classical necklace counting problem from enumerative combinatorics. In both cases, we fix a finite group $\\mathcal{G}$ and a positive integer $n$. In the first variant, we count the ``identity-product $n$-necklaces'' -- that is, the orbits of $n$-tuples $\\left(a_1, a_2, \\ldots, a_n\\right) \\in \\mathcal{G}^n$ that satisfy $a_1 a_2 \\cdots a_n = 1$ under cyclic rotation. In the second, we count the orbits of all $n$-tuples $\\left(a_1, a_2, \\ldots, a_n\\right) \\in \\mathcal{G}^n$ under cyclic rotation and left multiplication (i.e., the operation of $\\mathcal{G}$ on $\\mathcal{G}^n$ given by $h \\cdot \\left(a_1, a_2, \\ldots, a_n\\right) = \\left(ha_1, ha_2, \\ldots, ha_n\\right)$). We prove bijectively that both answers are the same, and express them as a sum over divisors of $n$.","sentences":["We address two variants of the classical necklace counting problem from enumerative combinatorics.","In both cases, we fix a finite group $\\mathcal{G}$ and a positive integer $n$. In the first variant, we count the ``identity-product $n$-necklaces'' -- that is, the orbits of $n$-tuples $\\left(a_1, a_2, \\ldots, a_n\\right) \\in \\mathcal{G}^n$ that satisfy $a_1 a_2 \\cdots a_n","= 1$ under cyclic rotation.","In the second, we count the orbits of all $n$-tuples $\\left(a_1, a_2, \\ldots, a_n\\right) \\in \\mathcal{G}^n$ under cyclic rotation and left multiplication (i.e., the operation of $\\mathcal{G}$ on $\\mathcal{G}^n$ given by $h \\cdot \\left(a_1, a_2, \\ldots, a_n\\right) = \\left(ha_1, ha_2, \\ldots, ha_n\\right)$).","We prove bijectively that both answers are the same, and express them as a sum over divisors of $n$."],"url":"http://arxiv.org/abs/2405.08937v1","category":"math.CO"}
{"created":"2024-05-14 19:58:10","title":"On the Characteristics of the Conjugate Function Enabling Effective Dual Decomposition Methods","abstract":"We investigate a novel characteristic of the conjugate function associated to a generic convex optimization problem which can subsequently be leveraged for efficient dual decomposition methods. In particular, under mild assumptions, we show a specific region of the domain of the conjugate function where there is always a ray originating from any point of the region such that the gradients of the conjugate remain constant along the ray. We refer to this characteristic as a fixed gradient over rays (FGOR). We further show that the dual function inherits from the conjugate the characteristic of FGOR. Then we provide a thorough exposition of applying the FGOR characteristic to dual subgradient methods. More importantly, we leverage FGOR to devise a simple stepsize rule that can be prepended with state-of-the-art stepsize methods enabling them to be more efficient. Furthermore, we investigate how the characteristic of FGOR is used when solving the global consensus problem. We show that FGOR can be exploited not only to expedite the convergence of the dual decomposition methods but also to reduce the communication overhead when distributed implementations are sought. Numerical experiments using quadratic objectives and a regularized linear regression with a real data set are conducted to compare the practical performance of FGOR with state-of-the-art stepsize methods. Results show that the proposed approach can significantly improve the convergence of existing methods while saving a considerable amount of communication overhead.","sentences":["We investigate a novel characteristic of the conjugate function associated to a generic convex optimization problem which can subsequently be leveraged for efficient dual decomposition methods.","In particular, under mild assumptions, we show a specific region of the domain of the conjugate function where there is always a ray originating from any point of the region such that the gradients of the conjugate remain constant along the ray.","We refer to this characteristic as a fixed gradient over rays (FGOR).","We further show that the dual function inherits from the conjugate the characteristic of FGOR.","Then we provide a thorough exposition of applying the FGOR characteristic to dual subgradient methods.","More importantly, we leverage FGOR to devise a simple stepsize rule that can be prepended with state-of-the-art stepsize methods enabling them to be more efficient.","Furthermore, we investigate how the characteristic of FGOR is used when solving the global consensus problem.","We show that FGOR can be exploited not only to expedite the convergence of the dual decomposition methods but also to reduce the communication overhead when distributed implementations are sought.","Numerical experiments using quadratic objectives and a regularized linear regression with a real data set are conducted to compare the practical performance of FGOR with state-of-the-art stepsize methods.","Results show that the proposed approach can significantly improve the convergence of existing methods while saving a considerable amount of communication overhead."],"url":"http://arxiv.org/abs/2405.08933v1","category":"math.OC"}
{"created":"2024-05-14 19:49:22","title":"Time-adaptive phase estimation","abstract":"Phase estimation is known to be a robust method for single-qubit gate calibration in quantum computers, while Bayesian estimation is widely used in devising optimal methods for learning in quantum systems. We present Bayesian phase estimation methods that adaptively choose a control phase and the time of coherent evolution based on prior phase knowledge. In the presence of noise, we find near-optimal performance with respect to known theoretical bounds, and demonstrate some robustness of the estimates to noise that is not accounted for in the model of the estimator, making the methods suitable for calibrating operations in quantum computers. We determine the utility of control parameter values using functions of the prior probability of the phase that quantify expected knowledge gain either in terms of expected narrowing of the posterior or expected information gain. In particular, we find that by maximising the rate of expected gain we obtain phase estimates having standard deviation a factor of 1.42 above the Heisenberg limit, which is the lowest value we know of for sequential phase estimation. The methods provide optimal solutions accounting for available prior knowledge and experimental imperfections with minimal effort from the user. The effect of many types of noise can be specified in the model of the measurement probabilities, and the rate of knowledge gain can easily be adjusted to account for times included in the measurement sequence other than the coherent evolution leading to the unknown phase, such as times required for state preparation or readout.","sentences":["Phase estimation is known to be a robust method for single-qubit gate calibration in quantum computers, while Bayesian estimation is widely used in devising optimal methods for learning in quantum systems.","We present Bayesian phase estimation methods that adaptively choose a control phase and the time of coherent evolution based on prior phase knowledge.","In the presence of noise, we find near-optimal performance with respect to known theoretical bounds, and demonstrate some robustness of the estimates to noise that is not accounted for in the model of the estimator, making the methods suitable for calibrating operations in quantum computers.","We determine the utility of control parameter values using functions of the prior probability of the phase that quantify expected knowledge gain either in terms of expected narrowing of the posterior or expected information gain.","In particular, we find that by maximising the rate of expected gain we obtain phase estimates having standard deviation a factor of 1.42 above the Heisenberg limit, which is the lowest value we know of for sequential phase estimation.","The methods provide optimal solutions accounting for available prior knowledge and experimental imperfections with minimal effort from the user.","The effect of many types of noise can be specified in the model of the measurement probabilities, and the rate of knowledge gain can easily be adjusted to account for times included in the measurement sequence other than the coherent evolution leading to the unknown phase, such as times required for state preparation or readout."],"url":"http://arxiv.org/abs/2405.08930v1","category":"quant-ph"}
{"created":"2024-05-14 19:25:37","title":"Neural Active Learning Meets the Partial Monitoring Framework","abstract":"We focus on the online-based active learning (OAL) setting where an agent operates over a stream of observations and trades-off between the costly acquisition of information (labelled observations) and the cost of prediction errors. We propose a novel foundation for OAL tasks based on partial monitoring, a theoretical framework specialized in online learning from partially informative actions. We show that previously studied binary and multi-class OAL tasks are instances of partial monitoring. We expand the real-world potential of OAL by introducing a new class of cost-sensitive OAL tasks. We propose NeuralCBP, the first PM strategy that accounts for predictive uncertainty with deep neural networks. Our extensive empirical evaluation on open source datasets shows that NeuralCBP has favorable performance against state-of-the-art baselines on multiple binary, multi-class and cost-sensitive OAL tasks.","sentences":["We focus on the online-based active learning (OAL) setting where an agent operates over a stream of observations and trades-off between the costly acquisition of information (labelled observations) and the cost of prediction errors.","We propose a novel foundation for OAL tasks based on partial monitoring, a theoretical framework specialized in online learning from partially informative actions.","We show that previously studied binary and multi-class OAL tasks are instances of partial monitoring.","We expand the real-world potential of OAL by introducing a new class of cost-sensitive OAL tasks.","We propose NeuralCBP, the first PM strategy that accounts for predictive uncertainty with deep neural networks.","Our extensive empirical evaluation on open source datasets shows that NeuralCBP has favorable performance against state-of-the-art baselines on multiple binary, multi-class and cost-sensitive OAL tasks."],"url":"http://arxiv.org/abs/2405.08921v1","category":"cs.LG"}
{"created":"2024-05-14 19:18:19","title":"Neural Collapse Meets Differential Privacy: Curious Behaviors of NoisyGD with Near-perfect Representation Learning","abstract":"A recent study by De et al. (2022) has reported that large-scale representation learning through pre-training on a public dataset significantly enhances differentially private (DP) learning in downstream tasks, despite the high dimensionality of the feature space. To theoretically explain this phenomenon, we consider the setting of a layer-peeled model in representation learning, which results in interesting phenomena related to learned features in deep learning and transfer learning, known as Neural Collapse (NC).   Within the framework of NC, we establish an error bound indicating that the misclassification error is independent of dimension when the distance between actual features and the ideal ones is smaller than a threshold. Additionally, the quality of the features in the last layer is empirically evaluated under different pre-trained models within the framework of NC, showing that a more powerful transformer leads to a better feature representation. Furthermore, we reveal that DP fine-tuning is less robust compared to fine-tuning without DP, particularly in the presence of perturbations. These observations are supported by both theoretical analyses and experimental evaluation. Moreover, to enhance the robustness of DP fine-tuning, we suggest several strategies, such as feature normalization or employing dimension reduction methods like Principal Component Analysis (PCA). Empirically, we demonstrate a significant improvement in testing accuracy by conducting PCA on the last-layer features.","sentences":["A recent study by De et al. (2022) has reported that large-scale representation learning through pre-training on a public dataset significantly enhances differentially private (DP) learning in downstream tasks, despite the high dimensionality of the feature space.","To theoretically explain this phenomenon, we consider the setting of a layer-peeled model in representation learning, which results in interesting phenomena related to learned features in deep learning and transfer learning, known as Neural Collapse (NC).   ","Within the framework of NC, we establish an error bound indicating that the misclassification error is independent of dimension when the distance between actual features and the ideal ones is smaller than a threshold.","Additionally, the quality of the features in the last layer is empirically evaluated under different pre-trained models within the framework of NC, showing that a more powerful transformer leads to a better feature representation.","Furthermore, we reveal that DP fine-tuning is less robust compared to fine-tuning without DP, particularly in the presence of perturbations.","These observations are supported by both theoretical analyses and experimental evaluation.","Moreover, to enhance the robustness of DP fine-tuning, we suggest several strategies, such as feature normalization or employing dimension reduction methods like Principal Component Analysis (PCA).","Empirically, we demonstrate a significant improvement in testing accuracy by conducting PCA on the last-layer features."],"url":"http://arxiv.org/abs/2405.08920v1","category":"cs.LG"}
{"created":"2024-05-14 19:15:30","title":"Joint Instantaneous Amplitude-Frequency Analysis of Vibration Signals for Vibration-Based Condition Monitoring of Rolling Bearings","abstract":"Vibrations of damaged bearings are manifested as modulations in the amplitude of the generated vibration signal, making envelope analysis an effective approach for discriminating between healthy and abnormal vibration patterns. Motivated by this, we introduce a low-complexity method for vibration-based condition monitoring (VBCM) of rolling bearings based on envelope analysis. In the proposed method, the instantaneous amplitude (envelope) and instantaneous frequency of the vibration signal are jointly utilized to facilitate three novel envelope representations: instantaneous amplitude-frequency mapping (IAFM), instantaneous amplitude-frequency correlation (IAFC), and instantaneous energy-frequency distribution (IEFD). Maintaining temporal information, these representations effectively capture energy-frequency variations that are unique to the condition of the bearing, thereby enabling the extraction of discriminative features with high sensitivity to variations in operational conditions. Accordingly, six new highly discriminative features are engineered from these representations, capturing and characterizing their shapes. The experimental results show outstanding performance in detecting and diagnosing various fault types, demonstrating the effectiveness of the proposed method in capturing unique variations in energy and frequency between healthy and faulty bearings. Moreover, the proposed method has moderate computational complexity, meeting the requirements of real-time applications. Further, the Python code of the proposed method is made public to support collaborative research efforts and ensure the reproducibility of the presented work","sentences":["Vibrations of damaged bearings are manifested as modulations in the amplitude of the generated vibration signal, making envelope analysis an effective approach for discriminating between healthy and abnormal vibration patterns.","Motivated by this, we introduce a low-complexity method for vibration-based condition monitoring (VBCM) of rolling bearings based on envelope analysis.","In the proposed method, the instantaneous amplitude (envelope) and instantaneous frequency of the vibration signal are jointly utilized to facilitate three novel envelope representations: instantaneous amplitude-frequency mapping (IAFM), instantaneous amplitude-frequency correlation (IAFC), and instantaneous energy-frequency distribution (IEFD).","Maintaining temporal information, these representations effectively capture energy-frequency variations that are unique to the condition of the bearing, thereby enabling the extraction of discriminative features with high sensitivity to variations in operational conditions.","Accordingly, six new highly discriminative features are engineered from these representations, capturing and characterizing their shapes.","The experimental results show outstanding performance in detecting and diagnosing various fault types, demonstrating the effectiveness of the proposed method in capturing unique variations in energy and frequency between healthy and faulty bearings.","Moreover, the proposed method has moderate computational complexity, meeting the requirements of real-time applications.","Further, the Python code of the proposed method is made public to support collaborative research efforts and ensure the reproducibility of the presented work"],"url":"http://arxiv.org/abs/2405.08919v1","category":"eess.SP"}
{"created":"2024-05-14 19:15:17","title":"New spectral Bishop--Gromov and Bonnet--Myers theorems and applications to isoperimetry","abstract":"We show a sharp and rigid spectral generalization of the classical Bishop--Gromov volume comparison theorem: if a closed Riemannian manifold $(M,g)$ of dimension $n\\geq3$ satisfies \\[ \\lambda_1(-\\gamma\\Delta+\\mathrm{Ric})\\geqslant n-1 \\] for some $0\\leqslant\\gamma\\leqslant\\frac{n-1}{n-2}$, then $\\mathrm{vol}(M)\\leqslant\\mathrm{vol}(\\mathbb S^{n})$, and $\\pi_1(M)$ is finite. Moreover, the bound on $\\gamma$ is sharp for this result to hold. A generalization of the Bonnet--Myers theorem is also shown under the same spectral condition. The proofs involve the use of a new unequally weighted isoperimetric problem and unequally warped $\\mu$-bubbles. As an application, in dimensions $3\\leqslant n\\leqslant 5$, we infer sharp results on the isoperimetric structure at infinity of complete manifolds with nonnegative Ricci curvature and uniformly positive biRicci curvature.","sentences":["We show a sharp and rigid spectral generalization of the classical Bishop--Gromov volume comparison theorem: if a closed Riemannian manifold $(M,g)$ of dimension $n\\geq3$ satisfies \\[ \\lambda_1(-\\gamma\\Delta+\\mathrm{Ric})\\geqslant n-1 \\] for some $0\\leqslant\\gamma\\leqslant\\frac{n-1}{n-2}$, then $\\mathrm{vol}(M)\\leqslant\\mathrm{vol}(\\mathbb S^{n})$, and $\\pi_1(M)$ is finite.","Moreover, the bound on $\\gamma$ is sharp for this result to hold.","A generalization of the Bonnet--Myers theorem is also shown under the same spectral condition.","The proofs involve the use of a new unequally weighted isoperimetric problem and unequally warped $\\mu$-bubbles.","As an application, in dimensions $3\\leqslant n\\leqslant 5$, we infer sharp results on the isoperimetric structure at infinity of complete manifolds with nonnegative Ricci curvature and uniformly positive biRicci curvature."],"url":"http://arxiv.org/abs/2405.08918v1","category":"math.DG"}
{"created":"2024-05-14 19:11:58","title":"Experimental Demonstration of Turbulence-resistant Lidar via Quantum Entanglement","abstract":"We report a proof-of-principle experimental demonstration of a turbulence-resistant quantum Lidar system. As a key technology for sensing and ranging, Lidar has drawn considerable attention for a study from quantum perspective, in search of proven advantages complementary to the capabilities of conventional Lidar technologies. Environmental factors such as strong atmospheric turbulence can have detrimental effects on the performance of these systems. We demonstrate the possibility of turbulence-resistant operation of a quantum Lidar system via two-photon interference of entangled photon pairs. Additionally, the reported quantum Lidar also demonstrates the expected noise resistance. This study suggests a potential high precision timing-positioning technology operable under turbulence and noise.","sentences":["We report a proof-of-principle experimental demonstration of a turbulence-resistant quantum Lidar system.","As a key technology for sensing and ranging, Lidar has drawn considerable attention for a study from quantum perspective, in search of proven advantages complementary to the capabilities of conventional Lidar technologies.","Environmental factors such as strong atmospheric turbulence can have detrimental effects on the performance of these systems.","We demonstrate the possibility of turbulence-resistant operation of a quantum Lidar system via two-photon interference of entangled photon pairs.","Additionally, the reported quantum Lidar also demonstrates the expected noise resistance.","This study suggests a potential high precision timing-positioning technology operable under turbulence and noise."],"url":"http://arxiv.org/abs/2405.08916v1","category":"quant-ph"}
{"created":"2024-05-14 19:03:21","title":"Investigation of BaTiO$_3$-NiO composite as compact Dielectric Resonator Antenna","abstract":"A compact dielectric resonator antenna has been fabricated on a microstrip transmission line for the purpose of C-band wireless communication using a ceramic material made out of a sintered mixture of BTO and NiO. The antenna parameters are optimized using Ansys HFSS software and verified experimentally. Ni replaces both Ba at A site and Ti at B site. Such a solid solution has a limit depending on the amount of NiO provided during sintering. A complete study of the structural changes and the dielectric constant enables the correlation with the resonating property. All the samples retain the ferroelectric tetragonal P4mm phase with a nominal decrease in the c/a ratio. NiO incorporation in BTO decreases the sintering temperature and shows two types of morphology associated with BTO-like and NiO-like phases. It induces prominent reduction in the permittivity and loss tangent (<0.01) in the range 100Hz to 1MHz. These properties make these samples suitable for DRA application in the C-Band range [4-8 GHz]. Experimental and theoretical assessment using HFSS software yields a C-band signal at ~7.27 GHz.","sentences":["A compact dielectric resonator antenna has been fabricated on a microstrip transmission line for the purpose of C-band wireless communication using a ceramic material made out of a sintered mixture of BTO and NiO.","The antenna parameters are optimized using Ansys HFSS software and verified experimentally.","Ni replaces both Ba at A site and Ti at B site.","Such a solid solution has a limit depending on the amount of NiO provided during sintering.","A complete study of the structural changes and the dielectric constant enables the correlation with the resonating property.","All the samples retain the ferroelectric tetragonal P4mm phase with a nominal decrease in the c/a ratio.","NiO incorporation in BTO decreases the sintering temperature and shows two types of morphology associated with BTO-like and NiO-like phases.","It induces prominent reduction in the permittivity and loss tangent (<0.01) in the range 100Hz to 1MHz.","These properties make these samples suitable for DRA application in the C-Band range [4-8 GHz].","Experimental and theoretical assessment using HFSS software yields a C-band signal at ~7.27 GHz."],"url":"http://arxiv.org/abs/2405.08910v1","category":"physics.app-ph"}
{"created":"2024-05-14 18:12:09","title":"Global weight optimization of frame structures under free-vibration eigenvalue constraints","abstract":"Minimizing the weight in topology optimization of frame structures under free-vibration eigenvalue constraints constitutes a challenging nonconvex polynomial optimization problem with strong singularities in the feasible set. Here, we adopt a nonlinear semidefinite programming formulation, which consists of a minimization of a linear function over a basic semi-algebraic feasible set, and provide its bilevel reformulation. This bilevel program maintains a special structure: The lower level is univariate and quasiconvex, and the upper level is enumerative. After deriving the sufficient and necessary conditions for the solvability of the lower-level problem, we provide a way to construct feasible points to the original semidefinite program, and using such a feasible point, we show that the conditions for convergence of the Lasserre hierarchy are met. Moreover, we show how to construct lower and upper bounds for each level of the Lasserre hierarchy. Using these bounds, we develop a simple sufficient condition of global {\\epsilon}-optimality. Finally, we prove that the optimality gap {\\epsilon} converges to zero in the limit if the set of global minimizers is convex. We demonstrate these results with three representative problems, for which the hierarchy indeed converges in a finite number of steps.","sentences":["Minimizing the weight in topology optimization of frame structures under free-vibration eigenvalue constraints constitutes a challenging nonconvex polynomial optimization problem with strong singularities in the feasible set.","Here, we adopt a nonlinear semidefinite programming formulation, which consists of a minimization of a linear function over a basic semi-algebraic feasible set, and provide its bilevel reformulation.","This bilevel program maintains a special structure: The lower level is univariate and quasiconvex, and the upper level is enumerative.","After deriving the sufficient and necessary conditions for the solvability of the lower-level problem, we provide a way to construct feasible points to the original semidefinite program, and using such a feasible point, we show that the conditions for convergence of the Lasserre hierarchy are met.","Moreover, we show how to construct lower and upper bounds for each level of the Lasserre hierarchy.","Using these bounds, we develop a simple sufficient condition of global {\\epsilon}-optimality.","Finally, we prove that the optimality gap {\\epsilon} converges to zero in the limit if the set of global minimizers is convex.","We demonstrate these results with three representative problems, for which the hierarchy indeed converges in a finite number of steps."],"url":"http://arxiv.org/abs/2405.08894v1","category":"math.OC"}
{"created":"2024-05-14 18:07:04","title":"Language-Guided Self-Supervised Video Summarization Using Text Semantic Matching Considering the Diversity of the Video","abstract":"Current video summarization methods primarily depend on supervised computer vision techniques, which demands time-consuming manual annotations. Further, the annotations are always subjective which make this task more challenging. To address these issues, we analyzed the feasibility in transforming the video summarization into a text summary task and leverage Large Language Models (LLMs) to boost video summarization. This paper proposes a novel self-supervised framework for video summarization guided by LLMs. Our method begins by generating captions for video frames, which are then synthesized into text summaries by LLMs. Subsequently, we measure semantic distance between the frame captions and the text summary. It's worth noting that we propose a novel loss function to optimize our model according to the diversity of the video. Finally, the summarized video can be generated by selecting the frames whose captions are similar with the text summary. Our model achieves competitive results against other state-of-the-art methods and paves a novel pathway in video summarization.","sentences":["Current video summarization methods primarily depend on supervised computer vision techniques, which demands time-consuming manual annotations.","Further, the annotations are always subjective which make this task more challenging.","To address these issues, we analyzed the feasibility in transforming the video summarization into a text summary task and leverage Large Language Models (LLMs) to boost video summarization.","This paper proposes a novel self-supervised framework for video summarization guided by LLMs.","Our method begins by generating captions for video frames, which are then synthesized into text summaries by LLMs.","Subsequently, we measure semantic distance between the frame captions and the text summary.","It's worth noting that we propose a novel loss function to optimize our model according to the diversity of the video.","Finally, the summarized video can be generated by selecting the frames whose captions are similar with the text summary.","Our model achieves competitive results against other state-of-the-art methods and paves a novel pathway in video summarization."],"url":"http://arxiv.org/abs/2405.08890v1","category":"cs.CV"}
{"created":"2024-05-14 18:05:19","title":"The Pitfalls and Promise of Conformal Inference Under Adversarial Attacks","abstract":"In safety-critical applications such as medical imaging and autonomous driving, where decisions have profound implications for patient health and road safety, it is imperative to maintain both high adversarial robustness to protect against potential adversarial attacks and reliable uncertainty quantification in decision-making. With extensive research focused on enhancing adversarial robustness through various forms of adversarial training (AT), a notable knowledge gap remains concerning the uncertainty inherent in adversarially trained models. To address this gap, this study investigates the uncertainty of deep learning models by examining the performance of conformal prediction (CP) in the context of standard adversarial attacks within the adversarial defense community. It is first unveiled that existing CP methods do not produce informative prediction sets under the commonly used $l_{\\infty}$-norm bounded attack if the model is not adversarially trained, which underpins the importance of adversarial training for CP. Our paper next demonstrates that the prediction set size (PSS) of CP using adversarially trained models with AT variants is often worse than using standard AT, inspiring us to research into CP-efficient AT for improved PSS. We propose to optimize a Beta-weighting loss with an entropy minimization regularizer during AT to improve CP-efficiency, where the Beta-weighting loss is shown to be an upper bound of PSS at the population level by our theoretical analysis. Moreover, our empirical study on four image classification datasets across three popular AT baselines validates the effectiveness of the proposed Uncertainty-Reducing AT (AT-UR).","sentences":["In safety-critical applications such as medical imaging and autonomous driving, where decisions have profound implications for patient health and road safety, it is imperative to maintain both high adversarial robustness to protect against potential adversarial attacks and reliable uncertainty quantification in decision-making.","With extensive research focused on enhancing adversarial robustness through various forms of adversarial training (AT), a notable knowledge gap remains concerning the uncertainty inherent in adversarially trained models.","To address this gap, this study investigates the uncertainty of deep learning models by examining the performance of conformal prediction (CP) in the context of standard adversarial attacks within the adversarial defense community.","It is first unveiled that existing CP methods do not produce informative prediction sets under the commonly used $l_{\\infty}$-norm bounded attack if the model is not adversarially trained, which underpins the importance of adversarial training for CP.","Our paper next demonstrates that the prediction set size (PSS) of CP using adversarially trained models with AT variants is often worse than using standard AT, inspiring us to research into CP-efficient AT for improved PSS.","We propose to optimize a Beta-weighting loss with an entropy minimization regularizer during AT to improve CP-efficiency, where the Beta-weighting loss is shown to be an upper bound of PSS at the population level by our theoretical analysis.","Moreover, our empirical study on four image classification datasets across three popular AT baselines validates the effectiveness of the proposed Uncertainty-Reducing AT (AT-UR)."],"url":"http://arxiv.org/abs/2405.08886v1","category":"cs.LG"}
{"created":"2024-05-14 18:01:02","title":"Lollipop: SVM Rollups on Solana","abstract":"We present a formal specification for the implementation of Solana virtual machine (SVM) rollups deployed on top of the Solana Layer 1 (L1) blockchain. We further discuss our motivation, implementation, design decisions, limitations, and preliminary results. Overall, this paper is intended to serve as an initial introduction to building such system(s) on top of the Solana L1 blockchain, but does not represent an absolute. Lastly, we comment discuss on extensions of this specification to support SVM rollups on other well-established L1 blockchains systems such as Ethereum.","sentences":["We present a formal specification for the implementation of Solana virtual machine (SVM) rollups deployed on top of the Solana Layer 1 (L1) blockchain.","We further discuss our motivation, implementation, design decisions, limitations, and preliminary results.","Overall, this paper is intended to serve as an initial introduction to building such system(s) on top of the Solana L1 blockchain, but does not represent an absolute.","Lastly, we comment discuss on extensions of this specification to support SVM rollups on other well-established L1 blockchains systems such as Ethereum."],"url":"http://arxiv.org/abs/2405.08882v1","category":"cs.DC"}
{"created":"2024-05-14 18:00:57","title":"Radiative Corrections to Light Thermal Pseudo-Dirac Dark Matter","abstract":"Light thermal dark matter has emerged as an attractive theoretical possibility and a promising target for discovery at experiments in the near future. Such scenarios generically invoke mediators with very small couplings to the Standard Model, but moderately strong couplings within the dark sector, calling into question theoretical estimates based on the lowest order of perturbation theory. As an example, we focus on a scenario in which (pseudo)-Dirac fermion dark matter is connected to the standard model via a dark photon charged under a new $U(1)^{\\prime}$ extension of the standard model, and we investigate the impact of the next-to-leading order corrections to annihilation and scattering. We find that radiative corrections can significantly impact model predictions for the relic density and scattering cross-section, depending on the strength of the dark sector coupling and ratio of the dark matter to mediator mass. We also show why factorization into the yield parameter $Y$ typically presented in literature leads to imprecision. Our results are necessary to accurately map experimental searches into the model parameter space and assess their ability to reach thermal production targets.","sentences":["Light thermal dark matter has emerged as an attractive theoretical possibility and a promising target for discovery at experiments in the near future.","Such scenarios generically invoke mediators with very small couplings to the Standard Model, but moderately strong couplings within the dark sector, calling into question theoretical estimates based on the lowest order of perturbation theory.","As an example, we focus on a scenario in which (pseudo)-Dirac fermion dark matter is connected to the standard model via a dark photon charged under a new $U(1)^{\\prime}$ extension of the standard model, and we investigate the impact of the next-to-leading order corrections to annihilation and scattering.","We find that radiative corrections can significantly impact model predictions for the relic density and scattering cross-section, depending on the strength of the dark sector coupling and ratio of the dark matter to mediator mass.","We also show why factorization into the yield parameter $Y$ typically presented in literature leads to imprecision.","Our results are necessary to accurately map experimental searches into the model parameter space and assess their ability to reach thermal production targets."],"url":"http://arxiv.org/abs/2405.08881v1","category":"hep-ph"}
{"created":"2024-05-14 18:00:55","title":"Direct CKM determination from W decays at future lepton colliders","abstract":"We project the reach of future lepton colliders for measuring CKM elements from direct observations of $W$ decays. We focus our attention to $|V_{cs}|$ and $|V_{cb}|$ determinations, using FCC-ee as case study. We employ state-of-the-art jet flavor taggers to obtain the projected sensitivity, and scan over tagger performances to show their effect. We conclude that future lepton collider can sizeably improve the sensitivity on $|V_{cs}|$ and $|V_{cb}|$, albeit the achievable reach will strongly depend on the level of systematic uncertainties on tagger parameters.","sentences":["We project the reach of future lepton colliders for measuring CKM elements from direct observations of $W$ decays.","We focus our attention to $|V_{cs}|$ and $|V_{cb}|$ determinations, using FCC-ee as case study.","We employ state-of-the-art jet flavor taggers to obtain the projected sensitivity, and scan over tagger performances to show their effect.","We conclude that future lepton collider can sizeably improve the sensitivity on $|V_{cs}|$ and $|V_{cb}|$, albeit the achievable reach will strongly depend on the level of systematic uncertainties on tagger parameters."],"url":"http://arxiv.org/abs/2405.08880v1","category":"hep-ph"}
{"created":"2024-05-14 18:00:24","title":"A power spectrum approach to search for Axion-like Particles from resolved galaxy clusters using CMB as a backlight","abstract":"Axions or ALPs are hypothetical particles predicted by BSM theories, which make one of the dark matter candidates. These particles can convert into photons and vice-versa in the presence of magnetic field, with a probability decided by its coupling strength $\\mathrm{g_{a\\gamma}}$. One of the ways to detect these particles is using the CMB as a backlight. As the CMB photons pass through a galaxy cluster, they can get converted into ALPs in the mass range $10^{-15}$ eV to $10^{-11}$ eV through resonant conversion in the presence of cluster magnetic fields. This leads to a polarized spectral distortion ($\\alpha$-distortion) in the CMB as the photon polarization parallel to the magnetic field in the galaxy cluster is involved in the conversion. The fluctuations in the magnetic field and electron density in a galaxy cluster lead to spatially varying $\\alpha$-distortion around the cluster, with a power spectrum that is different from the lensed CMB polarization power spectrum for the standard model of cosmology. By measuring the difference in the polarization power spectrum around a galaxy cluster from the all-sky signal, one can find new $\\alpha$-distortion in the sky. For galaxy clusters resolvable in multiple EM bands, one can measure the coupling strength $\\mathrm{g_{a\\gamma}}$ from the ALP power spectrum. Using multi-frequency techniques like ILC to clean the foregrounds, we show that the new power spectrum-based approach of the resolved galaxy clusters from upcoming CMB experiments such as Simons Observatory and CMB-S4 can detect (or put constraints) on the ALP-photon coupling strength of $\\mathrm{g_{a\\gamma} < 5.24 \\times 10^{-12} \\, GeV^{-1}}$ and $\\mathrm{g_{a\\gamma} < 3.61 \\times 10^{-12} \\, GeV^{-1}}$ at 95\\% C.I. respectively for ALPs of masses $10^{-13}$ eV or for smaller $\\mathrm{g_{a\\gamma}}$ for lighter ALP masses (Abridged).","sentences":["Axions or ALPs are hypothetical particles predicted by BSM theories, which make one of the dark matter candidates.","These particles can convert into photons and vice-versa in the presence of magnetic field, with a probability decided by its coupling strength $\\mathrm{g_{a\\gamma}}$. One of the ways to detect these particles is using the CMB as a backlight.","As the CMB photons pass through a galaxy cluster, they can get converted into ALPs in the mass range $10^{-15}$ eV to $10^{-11}$ eV through resonant conversion in the presence of cluster magnetic fields.","This leads to a polarized spectral distortion ($\\alpha$-distortion) in the CMB as the photon polarization parallel to the magnetic field in the galaxy cluster is involved in the conversion.","The fluctuations in the magnetic field and electron density in a galaxy cluster lead to spatially varying $\\alpha$-distortion around the cluster, with a power spectrum that is different from the lensed CMB polarization power spectrum for the standard model of cosmology.","By measuring the difference in the polarization power spectrum around a galaxy cluster from the all-sky signal, one can find new $\\alpha$-distortion in the sky.","For galaxy clusters resolvable in multiple EM bands, one can measure the coupling strength $\\mathrm{g_{a\\gamma}}$ from the ALP power spectrum.","Using multi-frequency techniques like ILC to clean the foregrounds, we show that the new power spectrum-based approach of the resolved galaxy clusters from upcoming CMB experiments such as Simons Observatory and CMB-S4 can detect (or put constraints) on the ALP-photon coupling strength of $\\mathrm{g_{a\\gamma} < 5.24 \\times 10^{-12} \\, GeV^{-1}}$ and $\\mathrm{g_{a\\gamma} < 3.61 \\times 10^{-12} \\, GeV^{-1}}$ at 95\\% C.I. respectively for ALPs of masses $10^{-13}$ eV or for smaller $\\mathrm{g_{a\\gamma}}$ for lighter ALP masses (Abridged)."],"url":"http://arxiv.org/abs/2405.08878v1","category":"astro-ph.CO"}
{"created":"2024-05-14 18:00:03","title":"The Interplay of Finite and Infinite Size Stability in Quadratic Bosonic Lindbladians","abstract":"We provide a framework for understanding dynamical metastability in open many-body systems of free bosons, whereby the dynamical stability properties of the system in the infinite-size (thermodynamic) limit may sharply differ from those of any finite-size truncation, and anomalous transient dynamics may arise. By leveraging pseudospectral techniques, we trace the discrepancy between asymptotic and transient dynamics to the non-normality of the underlying quadratic bosonic Lindbladian (QBL) generator, and show that two distinct flavors of dynamical metastability can arise. QBLs exhibiting type I dynamical metastability, previously discussed in the context of anomalous transient amplification [Phys. Rev. Lett. 127, 245701 (2021)], are dynamically unstable in the infinite-size limit, yet stable once open boundaries are imposed. Type II-dynamically metastable QBLs, which we uncover in this work, are dynamically stable for infinite size, but become unstable under open boundary conditions for arbitrary finite system size. We exhibit representative models for both types of metastability in the dissipative, as well as the limiting closed-system (Hamiltonian) settings, and analyze distinctive physical behavior they can engender. We show that dynamical metastability manifests itself in the generation of entanglement entropy, by way of a transient which reflects the stability phase of the infinite (rather than the actual finite) system and, as a result, is directly tied to the emergence of super-volume scaling in type I systems. Finally, we demonstrate how, even in Hermitian, and especially in highly non-normal regimes, the spectral properties of an infinite-size QBL are reflected in the linear response functions of the corresponding finite QBLs, by way of resonant pseudospectral modes.","sentences":["We provide a framework for understanding dynamical metastability in open many-body systems of free bosons, whereby the dynamical stability properties of the system in the infinite-size (thermodynamic) limit may sharply differ from those of any finite-size truncation, and anomalous transient dynamics may arise.","By leveraging pseudospectral techniques, we trace the discrepancy between asymptotic and transient dynamics to the non-normality of the underlying quadratic bosonic Lindbladian (QBL) generator, and show that two distinct flavors of dynamical metastability can arise.","QBLs exhibiting type I dynamical metastability, previously discussed in the context of anomalous transient amplification [Phys.","Rev. Lett.","127, 245701 (2021)], are dynamically unstable in the infinite-size limit, yet stable once open boundaries are imposed.","Type II-dynamically metastable QBLs, which we uncover in this work, are dynamically stable for infinite size, but become unstable under open boundary conditions for arbitrary finite system size.","We exhibit representative models for both types of metastability in the dissipative, as well as the limiting closed-system (Hamiltonian) settings, and analyze distinctive physical behavior they can engender.","We show that dynamical metastability manifests itself in the generation of entanglement entropy, by way of a transient which reflects the stability phase of the infinite (rather than the actual finite) system and, as a result, is directly tied to the emergence of super-volume scaling in type I systems.","Finally, we demonstrate how, even in Hermitian, and especially in highly non-normal regimes, the spectral properties of an infinite-size QBL are reflected in the linear response functions of the corresponding finite QBLs, by way of resonant pseudospectral modes."],"url":"http://arxiv.org/abs/2405.08873v1","category":"quant-ph"}
{"created":"2024-05-14 18:00:02","title":"A calibrated model for N-body dynamical friction acting on supermassive black holes","abstract":"Black holes are believed to be crucial in regulating star formation in massive galaxies, which makes it essential to faithfully represent the physics of these objects in cosmological hydrodynamics simulations. Limited spatial and mass resolution and the associated discreteness noise make following the dynamics of black holes especially challenging. In particular, dynamical friction, which is responsible for driving massive black holes towards the centres of galaxies, cannot be accurately modelled with softened $N$-body interactions. A number of subgrid models have been proposed to mimic dynamical friction or directly include its full effects in simulations. Each of these methods has its individual benefits and shortcomings, while all suffer from a common issue of being unable to represent black holes with masses below a few times the simulated dark matter particle mass. In this paper, we propose a correction for unresolved dynamical friction, which has been calibrated on simulations run with the code KETJU, in which gravitational interactions of black holes are not softened. We demonstrate that our correction is able to sink black holes with masses greater than the dark matter particle mass at the correct rate. We show that the impact of stochasticity is significant for low-mass black holes ($M_{\\rm BH} \\leq 5 M_{\\rm DM}$) and propose a correction for stochastic heating. Combined, this approach is applicable to next generation cosmological hydrodynamics simulations that jointly track galaxy and black hole growth with realistic black hole orbits.","sentences":["Black holes are believed to be crucial in regulating star formation in massive galaxies, which makes it essential to faithfully represent the physics of these objects in cosmological hydrodynamics simulations.","Limited spatial and mass resolution and the associated discreteness noise make following the dynamics of black holes especially challenging.","In particular, dynamical friction, which is responsible for driving massive black holes towards the centres of galaxies, cannot be accurately modelled with softened $N$-body interactions.","A number of subgrid models have been proposed to mimic dynamical friction or directly include its full effects in simulations.","Each of these methods has its individual benefits and shortcomings, while all suffer from a common issue of being unable to represent black holes with masses below a few times the simulated dark matter particle mass.","In this paper, we propose a correction for unresolved dynamical friction, which has been calibrated on simulations run with the code KETJU, in which gravitational interactions of black holes are not softened.","We demonstrate that our correction is able to sink black holes with masses greater than the dark matter particle mass at the correct rate.","We show that the impact of stochasticity is significant for low-mass black holes ($M_{\\rm BH} \\leq 5 M_{\\rm DM}$) and propose a correction for stochastic heating.","Combined, this approach is applicable to next generation cosmological hydrodynamics simulations that jointly track galaxy and black hole growth with realistic black hole orbits."],"url":"http://arxiv.org/abs/2405.08870v1","category":"astro-ph.GA"}
{"created":"2024-05-14 18:00:00","title":"Chiral properties of the nucleon interpolating current and $\u03b8$-dependent observables","abstract":"We revisit the chiral properties of nucleon interpolating currents, and show that of the two leading order currents $j_1$ and $j_2$, only two linear combinations $j_1\\pm j_2$ transform covariantly under the anomalous $U(1)_A$ symmetry. As a result, calculations of quantities which vanish by symmetry in the chiral limit may produce unphysical results if carried out with different linear combinations of the currents. This includes observables such as electric dipole moments, induced by the QCD parameter $\\theta$, and the $\\theta$-dependence of the nucleon mass. For completeness, we also exhibit the leading order results for nucleon electric dipole moments ($d_{n,p}$) induced by $\\theta$, and the nucleon magnetic moments ($\\mu_{n,p}$), when calculated using QCD sum rules for both the covariant choices of the nucleon interpolating current. The results in each channel, conveniently expressed as the ratios, $d_{n,p}/\\mu_{n,p}$, are numerically consistent, and reflect the required physical dependence on $\\theta$.","sentences":["We revisit the chiral properties of nucleon interpolating currents, and show that of the two leading order currents $j_1$ and $j_2$, only two linear combinations $j_1\\pm j_2$ transform covariantly under the anomalous $U(1)_A$ symmetry.","As a result, calculations of quantities which vanish by symmetry in the chiral limit may produce unphysical results if carried out with different linear combinations of the currents.","This includes observables such as electric dipole moments, induced by the QCD parameter $\\theta$, and the $\\theta$-dependence of the nucleon mass.","For completeness, we also exhibit the leading order results for nucleon electric dipole moments ($d_{n,p}$) induced by $\\theta$, and the nucleon magnetic moments ($\\mu_{n,p}$), when calculated using QCD sum rules for both the covariant choices of the nucleon interpolating current.","The results in each channel, conveniently expressed as the ratios, $d_{n,p}/\\mu_{n,p}$, are numerically consistent, and reflect the required physical dependence on $\\theta$."],"url":"http://arxiv.org/abs/2405.08856v1","category":"hep-ph"}
{"created":"2024-05-14 18:00:00","title":"Rapid parameter estimation for pulsar-timing-array datasets with variational inference and normalizing flows","abstract":"In the gravitational-wave analysis of pulsar-timing-array datasets, parameter estimation is usually performed using Markov Chain Monte Carlo methods to explore posterior probability densities. We introduce an alternative procedure that relies instead on stochastic gradient-descent Bayesian variational inference, whereby we obtain the weights of a neural-network approximation of the posterior by minimizing the Kullback-Leibler divergence of the approximation from the exact posterior. This technique is distinct from simulation-based inference with normalizing flows, since we train the network for a single dataset, rather than the population of all possible datasets, and we require the computation of the data likelihood and its gradient. Unlike Markov Chain methods, our technique can transparently exploit highly parallel computing platforms. This makes it extremely fast on modern graphical processing units, where it can analyze the NANOGrav 15-yr dataset in few tens of minutes, depending on the probabilistic model, as opposed to hours or days with the analysis codes used until now. We expect that this speed will unlock new kinds of astrophysical and cosmological studies of pulsar-timing-array datasets. Furthermore, variational inference would be viable in other contexts of gravitational-wave data analysis as long as differentiable and parallelizable likelihoods are available.","sentences":["In the gravitational-wave analysis of pulsar-timing-array datasets, parameter estimation is usually performed using Markov Chain Monte Carlo methods to explore posterior probability densities.","We introduce an alternative procedure that relies instead on stochastic gradient-descent Bayesian variational inference, whereby we obtain the weights of a neural-network approximation of the posterior by minimizing the Kullback-Leibler divergence of the approximation from the exact posterior.","This technique is distinct from simulation-based inference with normalizing flows, since we train the network for a single dataset, rather than the population of all possible datasets, and we require the computation of the data likelihood and its gradient.","Unlike Markov Chain methods, our technique can transparently exploit highly parallel computing platforms.","This makes it extremely fast on modern graphical processing units, where it can analyze the NANOGrav 15-yr dataset in few tens of minutes, depending on the probabilistic model, as opposed to hours or days with the analysis codes used until now.","We expect that this speed will unlock new kinds of astrophysical and cosmological studies of pulsar-timing-array datasets.","Furthermore, variational inference would be viable in other contexts of gravitational-wave data analysis as long as differentiable and parallelizable likelihoods are available."],"url":"http://arxiv.org/abs/2405.08857v1","category":"gr-qc"}
{"created":"2024-05-14 16:12:27","title":"Evaluating the Uncertainty in Mean Residual Times: Estimators Based on Residence Times from Discrete Time Processes","abstract":"In this work, we propose estimators for the uncertainty in mean residual times that require, for their evaluation, statistically independent individual residence times obtained from a discrete time process. We examine their performance through numerical experiments involving well-known probability distributions, and an application example using molecular dynamics simulation results, from an aqueous NaCl solution, is provided. These computationally inexpensive estimators, capable of achieving very accurate outcomes, serve as useful tools for assessing and reporting uncertainties in mean residual times across a wide range of simulations.","sentences":["In this work, we propose estimators for the uncertainty in mean residual times that require, for their evaluation, statistically independent individual residence times obtained from a discrete time process.","We examine their performance through numerical experiments involving well-known probability distributions, and an application example using molecular dynamics simulation results, from an aqueous NaCl solution, is provided.","These computationally inexpensive estimators, capable of achieving very accurate outcomes, serve as useful tools for assessing and reporting uncertainties in mean residual times across a wide range of simulations."],"url":"http://arxiv.org/abs/2405.08853v1","category":"stat.ME"}
{"created":"2024-05-15 16:39:11","title":"Symmetry adaptation for self-consistent many-body calculations","abstract":"The exploitation of space group symmetries in numerical calculations of periodic crystalline solids accelerates calculations and provides physical insight. We present results for a space-group symmetry adaptation of electronic structure calculations within the finite-temperature self-consistent GW method along with an efficient parallelization scheme on accelerators. Our implementation employs the simultaneous diagonalization of the Dirac characters of the orbital representation. Results show that symmetry adaptation in self-consistent many-body codes results in substantial improvements of the runtime, and that block diagonalization on top of a restriction to the irreducible wedge results in additional speedup.","sentences":["The exploitation of space group symmetries in numerical calculations of periodic crystalline solids accelerates calculations and provides physical insight.","We present results for a space-group symmetry adaptation of electronic structure calculations within the finite-temperature self-consistent GW method along with an efficient parallelization scheme on accelerators.","Our implementation employs the simultaneous diagonalization of the Dirac characters of the orbital representation.","Results show that symmetry adaptation in self-consistent many-body codes results in substantial improvements of the runtime, and that block diagonalization on top of a restriction to the irreducible wedge results in additional speedup."],"url":"http://arxiv.org/abs/2405.09494v1","category":"physics.comp-ph"}
{"created":"2024-05-15 15:02:55","title":"A velocity-based moving mesh Discontinuous Galerkin method for the advection-diffusion equation","abstract":"In convection-dominated flows, robustness of the spatial discretisation is a key property. While Interior Penalty Galerkin (IPG) methods already proved efficient in the situation of large mesh Peclet numbers, Arbitrary Lagrangian-Eulerian (ALE) methods are able to reduce the convection-dominance by moving the mesh. In this paper, we introduce and analyse a velocity-based moving mesh discontinuous Galerkin method for the solution of the linear advection-diffusion equation. By introducing a smooth parameterized velocity $\\tilde{V}$ that separates the flow into a mean flow, also called moving mesh velocity, and a remaining advection field $V-\\tilde{V}$, we made a convergence analysis based on the smoothness of the mesh velocity. Furthermore, the reduction of the advection speed improves the stability of an explicit time-stepping and the use of the nonconservative ALE formulation changes the coercivity condition. Finally, by adapting the existing robust error criteria to this moving mesh situation, we derived robust \\textit{a posteriori} error criteria that describe the potentially small deviation to the mean flow and include the information of a transition towards $V=\\tilde{V}$.","sentences":["In convection-dominated flows, robustness of the spatial discretisation is a key property.","While Interior Penalty Galerkin (IPG) methods already proved efficient in the situation of large mesh Peclet numbers, Arbitrary Lagrangian-Eulerian (ALE) methods are able to reduce the convection-dominance by moving the mesh.","In this paper, we introduce and analyse a velocity-based moving mesh discontinuous Galerkin method for the solution of the linear advection-diffusion equation.","By introducing a smooth parameterized velocity $\\tilde{V}$ that separates the flow into a mean flow, also called moving mesh velocity, and a remaining advection field $V-\\tilde{V}$, we made a convergence analysis based on the smoothness of the mesh velocity.","Furthermore, the reduction of the advection speed improves the stability of an explicit time-stepping and the use of the nonconservative ALE formulation changes the coercivity condition.","Finally, by adapting the existing robust error criteria to this moving mesh situation, we derived robust \\textit{a posteriori} error criteria that describe the potentially small deviation to the mean flow and include the information of a transition towards $V=\\tilde{V}$."],"url":"http://arxiv.org/abs/2405.09408v1","category":"math.NA"}
{"created":"2024-05-15 14:50:46","title":"SA-FedLora: Adaptive Parameter Allocation for Efficient Federated Learning with LoRA Tuning","abstract":"Fine-tuning large-scale pre-trained models via transfer learning is an emerging important paradigm for a wide range of downstream tasks, with performance heavily reliant on extensive data. Federated learning (FL), as a distributed framework, provides a secure solution to train models on local datasets while safeguarding raw sensitive data. However, FL networks encounter high communication costs due to the massive parameters of large-scale pre-trained models, necessitating parameter-efficient methods. Notably, parameter efficient fine tuning, such as Low-Rank Adaptation (LoRA), has shown remarkable success in fine-tuning pre-trained models. However, prior research indicates that the fixed parameter budget may be prone to the overfitting or slower convergence. To address this challenge, we propose a Simulated Annealing-based Federated Learning with LoRA tuning (SA-FedLoRA) approach by reducing trainable parameters. Specifically, SA-FedLoRA comprises two stages: initiating and annealing. (1) In the initiating stage, we implement a parameter regularization approach during the early rounds of aggregation, aiming to mitigate client drift and accelerate the convergence for the subsequent tuning. (2) In the annealing stage, we allocate higher parameter budget during the early 'heating' phase and then gradually shrink the budget until the 'cooling' phase. This strategy not only facilitates convergence to the global optimum but also reduces communication costs. Experimental results demonstrate that SA-FedLoRA is an efficient FL, achieving superior performance to FedAvg and significantly reducing communication parameters by up to 93.62%.","sentences":["Fine-tuning large-scale pre-trained models via transfer learning is an emerging important paradigm for a wide range of downstream tasks, with performance heavily reliant on extensive data.","Federated learning (FL), as a distributed framework, provides a secure solution to train models on local datasets while safeguarding raw sensitive data.","However, FL networks encounter high communication costs due to the massive parameters of large-scale pre-trained models, necessitating parameter-efficient methods.","Notably, parameter efficient fine tuning, such as Low-Rank Adaptation (LoRA), has shown remarkable success in fine-tuning pre-trained models.","However, prior research indicates that the fixed parameter budget may be prone to the overfitting or slower convergence.","To address this challenge, we propose a Simulated Annealing-based Federated Learning with LoRA tuning (SA-FedLoRA) approach by reducing trainable parameters.","Specifically, SA-FedLoRA comprises two stages: initiating and annealing.","(1) In the initiating stage, we implement a parameter regularization approach during the early rounds of aggregation, aiming to mitigate client drift and accelerate the convergence for the subsequent tuning.","(2) In the annealing stage, we allocate higher parameter budget during the early 'heating' phase and then gradually shrink the budget until the 'cooling' phase.","This strategy not only facilitates convergence to the global optimum but also reduces communication costs.","Experimental results demonstrate that SA-FedLoRA is an efficient FL, achieving superior performance to FedAvg and significantly reducing communication parameters by up to 93.62%."],"url":"http://arxiv.org/abs/2405.09394v1","category":"cs.LG"}
{"created":"2024-05-15 09:23:21","title":"Lens functions for exploring UMAP Projections with Domain Knowledge","abstract":"Dimensionality reduction algorithms are often used to visualise high-dimensional data. Previously, studies have used prior information to enhance or suppress expected patterns in projections. In this paper, we adapt such techniques for domain knowledge guided interactive exploration. Inspired by Mapper and STAD, we present three types of lens functions for UMAP, a state-of-the-art dimensionality reduction algorithm. Lens functions enable analysts to adapt projections to their questions, revealing otherwise hidden patterns. They filter the modelled connectivity to explore the interaction between manually selected features and the data's structure, creating configurable perspectives each potentially revealing new insights. The effectiveness of the lens functions is demonstrated in two use cases and their computational cost is analysed in a synthetic benchmark. Our implementation is available in an open-source Python package: https://github.com/vda-lab/lensed_umap.","sentences":["Dimensionality reduction algorithms are often used to visualise high-dimensional data.","Previously, studies have used prior information to enhance or suppress expected patterns in projections.","In this paper, we adapt such techniques for domain knowledge guided interactive exploration.","Inspired by Mapper and STAD, we present three types of lens functions for UMAP, a state-of-the-art dimensionality reduction algorithm.","Lens functions enable analysts to adapt projections to their questions, revealing otherwise hidden patterns.","They filter the modelled connectivity to explore the interaction between manually selected features and the data's structure, creating configurable perspectives each potentially revealing new insights.","The effectiveness of the lens functions is demonstrated in two use cases and their computational cost is analysed in a synthetic benchmark.","Our implementation is available in an open-source Python package: https://github.com/vda-lab/lensed_umap."],"url":"http://arxiv.org/abs/2405.09204v1","category":"cs.LG"}
{"created":"2024-05-15 07:32:43","title":"Adapting Abstract Meaning Representation Parsing to the Clinical Narrative -- the SPRING THYME parser","abstract":"This paper is dedicated to the design and evaluation of the first AMR parser tailored for clinical notes. Our objective was to facilitate the precise transformation of the clinical notes into structured AMR expressions, thereby enhancing the interpretability and usability of clinical text data at scale. Leveraging the colon cancer dataset from the Temporal Histories of Your Medical Events (THYME) corpus, we adapted a state-of-the-art AMR parser utilizing continuous training. Our approach incorporates data augmentation techniques to enhance the accuracy of AMR structure predictions. Notably, through this learning strategy, our parser achieved an impressive F1 score of 88% on the THYME corpus's colon cancer dataset. Moreover, our research delved into the efficacy of data required for domain adaptation within the realm of clinical notes, presenting domain adaptation data requirements for AMR parsing. This exploration not only underscores the parser's robust performance but also highlights its potential in facilitating a deeper understanding of clinical narratives through structured semantic representations.","sentences":["This paper is dedicated to the design and evaluation of the first AMR parser tailored for clinical notes.","Our objective was to facilitate the precise transformation of the clinical notes into structured AMR expressions, thereby enhancing the interpretability and usability of clinical text data at scale.","Leveraging the colon cancer dataset from the Temporal Histories of Your Medical Events (THYME) corpus, we adapted a state-of-the-art AMR parser utilizing continuous training.","Our approach incorporates data augmentation techniques to enhance the accuracy of AMR structure predictions.","Notably, through this learning strategy, our parser achieved an impressive F1 score of 88% on the THYME corpus's colon cancer dataset.","Moreover, our research delved into the efficacy of data required for domain adaptation within the realm of clinical notes, presenting domain adaptation data requirements for AMR parsing.","This exploration not only underscores the parser's robust performance but also highlights its potential in facilitating a deeper understanding of clinical narratives through structured semantic representations."],"url":"http://arxiv.org/abs/2405.09153v1","category":"cs.CL"}
{"created":"2024-05-15 07:16:00","title":"Role of melting and solidification in the spreading of an impacting water drop","abstract":"The present study reports experiments of water droplet impacting on ice or on a cold metallic substrate, with the aim of understanding the effect of phase change on the impingement process. Both liquid and substrate temperatures are varied, as well as the height of fall. The dimensionless maximum spreading diameter, $\\beta_m$, is found to increase with both temperatures as well as with the impact velocity. Furthermore, $\\beta_m$ is reduced when solidification, which enhances dissipation, is present, whereas fusion favours the liquid film spreading. These observations are rationalized by extending an existing model of effective viscosity, in which phase change alters the size and shape of the developing viscous boundary layer, thereby modifying the value of $\\beta_m$. The use of this correction allows to adapt a scaling law existing for isothermal drop impacts to propose a universal law giving the maximum diameter of an impacting water droplet in the presence of melting or solidification.","sentences":["The present study reports experiments of water droplet impacting on ice or on a cold metallic substrate, with the aim of understanding the effect of phase change on the impingement process.","Both liquid and substrate temperatures are varied, as well as the height of fall.","The dimensionless maximum spreading diameter, $\\beta_m$, is found to increase with both temperatures as well as with the impact velocity.","Furthermore, $\\beta_m$ is reduced when solidification, which enhances dissipation, is present, whereas fusion favours the liquid film spreading.","These observations are rationalized by extending an existing model of effective viscosity, in which phase change alters the size and shape of the developing viscous boundary layer, thereby modifying the value of $\\beta_m$. The use of this correction allows to adapt a scaling law existing for isothermal drop impacts to propose a universal law giving the maximum diameter of an impacting water droplet in the presence of melting or solidification."],"url":"http://arxiv.org/abs/2405.09143v1","category":"physics.flu-dyn"}
{"created":"2024-05-15 03:13:11","title":"Task-adaptive Q-Face","abstract":"Although face analysis has achieved remarkable improvements in the past few years, designing a multi-task face analysis model is still challenging. Most face analysis tasks are studied as separate problems and do not benefit from the synergy among related tasks. In this work, we propose a novel task-adaptive multi-task face analysis method named as Q-Face, which simultaneously performs multiple face analysis tasks with a unified model. We fuse the features from multiple layers of a large-scale pre-trained model so that the whole model can use both local and global facial information to support multiple tasks. Furthermore, we design a task-adaptive module that performs cross-attention between a set of query vectors and the fused multi-stage features and finally adaptively extracts desired features for each face analysis task. Extensive experiments show that our method can perform multiple tasks simultaneously and achieves state-of-the-art performance on face expression recognition, action unit detection, face attribute analysis, age estimation, and face pose estimation. Compared to conventional methods, our method opens up new possibilities for multi-task face analysis and shows the potential for both accuracy and efficiency.","sentences":["Although face analysis has achieved remarkable improvements in the past few years, designing a multi-task face analysis model is still challenging.","Most face analysis tasks are studied as separate problems and do not benefit from the synergy among related tasks.","In this work, we propose a novel task-adaptive multi-task face analysis method named as Q-Face, which simultaneously performs multiple face analysis tasks with a unified model.","We fuse the features from multiple layers of a large-scale pre-trained model so that the whole model can use both local and global facial information to support multiple tasks.","Furthermore, we design a task-adaptive module that performs cross-attention between a set of query vectors and the fused multi-stage features and finally adaptively extracts desired features for each face analysis task.","Extensive experiments show that our method can perform multiple tasks simultaneously and achieves state-of-the-art performance on face expression recognition, action unit detection, face attribute analysis, age estimation, and face pose estimation.","Compared to conventional methods, our method opens up new possibilities for multi-task face analysis and shows the potential for both accuracy and efficiency."],"url":"http://arxiv.org/abs/2405.09059v1","category":"cs.CV"}
{"created":"2024-05-14 21:55:02","title":"An adaptive approach to Bayesian Optimization with switching costs","abstract":"We investigate modifications to Bayesian Optimization for a resource-constrained setting of sequential experimental design where changes to certain design variables of the search space incur a switching cost. This models the scenario where there is a trade-off between evaluating more while maintaining the same setup, or switching and restricting the number of possible evaluations due to the incurred cost. We adapt two process-constrained batch algorithms to this sequential problem formulation, and propose two new methods: one cost-aware and one cost-ignorant. We validate and compare the algorithms using a set of 7 scalable test functions in different dimensionalities and switching-cost settings for 30 total configurations. Our proposed cost-aware hyperparameter-free algorithm yields comparable results to tuned process-constrained algorithms in all settings we considered, suggesting some degree of robustness to varying landscape features and cost trade-offs. This method starts to outperform the other algorithms with increasing switching-cost. Our work broadens out from other recent Bayesian Optimization studies in resource-constrained settings that consider a batch setting only. While the contributions of this work are relevant to the general class of resource-constrained problems, they are particularly relevant to problems where adaptability to varying resource availability is of high importance","sentences":["We investigate modifications to Bayesian Optimization for a resource-constrained setting of sequential experimental design where changes to certain design variables of the search space incur a switching cost.","This models the scenario where there is a trade-off between evaluating more while maintaining the same setup, or switching and restricting the number of possible evaluations due to the incurred cost.","We adapt two process-constrained batch algorithms to this sequential problem formulation, and propose two new methods: one cost-aware and one cost-ignorant.","We validate and compare the algorithms using a set of 7 scalable test functions in different dimensionalities and switching-cost settings for 30 total configurations.","Our proposed cost-aware hyperparameter-free algorithm yields comparable results to tuned process-constrained algorithms in all settings we considered, suggesting some degree of robustness to varying landscape features and cost trade-offs.","This method starts to outperform the other algorithms with increasing switching-cost.","Our work broadens out from other recent Bayesian Optimization studies in resource-constrained settings that consider a batch setting only.","While the contributions of this work are relevant to the general class of resource-constrained problems, they are particularly relevant to problems where adaptability to varying resource availability is of high importance"],"url":"http://arxiv.org/abs/2405.08973v1","category":"cs.LG"}
{"created":"2024-05-14 20:46:14","title":"Zero-Shot Transfer of Neural ODEs","abstract":"Autonomous systems often encounter environments and scenarios beyond the scope of their training data, which underscores a critical challenge: the need to generalize and adapt to unseen scenarios in real time. This challenge necessitates new mathematical and algorithmic tools that enable adaptation and zero-shot transfer. To this end, we leverage the theory of function encoders, which enables zero-shot transfer by combining the flexibility of neural networks with the mathematical principles of Hilbert spaces. Using this theory, we first present a method for learning a space of dynamics spanned by a set of neural ODE basis functions. After training, the proposed approach can rapidly identify dynamics in the learned space using an efficient inner product calculation. Critically, this calculation requires no gradient calculations or retraining during the online phase. This method enables zero-shot transfer for autonomous systems at runtime and opens the door for a new class of adaptable control algorithms. We demonstrate state-of-the-art system modeling accuracy for two MuJoCo robot environments and show that the learned models can be used for more efficient MPC control of a quadrotor.","sentences":["Autonomous systems often encounter environments and scenarios beyond the scope of their training data, which underscores a critical challenge: the need to generalize and adapt to unseen scenarios in real time.","This challenge necessitates new mathematical and algorithmic tools that enable adaptation and zero-shot transfer.","To this end, we leverage the theory of function encoders, which enables zero-shot transfer by combining the flexibility of neural networks with the mathematical principles of Hilbert spaces.","Using this theory, we first present a method for learning a space of dynamics spanned by a set of neural ODE basis functions.","After training, the proposed approach can rapidly identify dynamics in the learned space using an efficient inner product calculation.","Critically, this calculation requires no gradient calculations or retraining during the online phase.","This method enables zero-shot transfer for autonomous systems at runtime and opens the door for a new class of adaptable control algorithms.","We demonstrate state-of-the-art system modeling accuracy for two MuJoCo robot environments and show that the learned models can be used for more efficient MPC control of a quadrotor."],"url":"http://arxiv.org/abs/2405.08954v1","category":"cs.RO"}
{"created":"2024-05-14 19:07:25","title":"The quantum Mpemba effect in free-fermionic mixed states","abstract":"Recently, a novel probe to study symmetry breaking, known as entanglement asymmetry, has emerged and has been utilized to explore how symmetry is dynamically restored following quantum quenches. Interestingly, it has been shown that, in certain scenarios, greater initial symmetry breaking leads to faster restoration, akin to a quantum Mpemba effect. This study focuses on investigating the effect of mixed initial states and non-unitary dynamics on symmetry restoration. The mixedness of a state can arise from different sources. We consider dephasing or dissipative processes affecting initial pure states or unitary dynamics of initially thermal states. In the former case, the stationary state after the quench is independent of the initial configuration, resembling the phenomenology of the classical Mpemba effect. Investigating the XY spin chain model, through a combination of analytical calculations and numerical simulations, we identify the conditions for the occurrence of the quantum Mpemba effect. It turns out that this phenomenon still occurs in the presence of dissipation or at finite temperature, even though it will be eventually suppressed as the state becomes more mixed.","sentences":["Recently, a novel probe to study symmetry breaking, known as entanglement asymmetry, has emerged and has been utilized to explore how symmetry is dynamically restored following quantum quenches.","Interestingly, it has been shown that, in certain scenarios, greater initial symmetry breaking leads to faster restoration, akin to a quantum Mpemba effect.","This study focuses on investigating the effect of mixed initial states and non-unitary dynamics on symmetry restoration.","The mixedness of a state can arise from different sources.","We consider dephasing or dissipative processes affecting initial pure states or unitary dynamics of initially thermal states.","In the former case, the stationary state after the quench is independent of the initial configuration, resembling the phenomenology of the classical Mpemba effect.","Investigating the XY spin chain model, through a combination of analytical calculations and numerical simulations, we identify the conditions for the occurrence of the quantum Mpemba effect.","It turns out that this phenomenon still occurs in the presence of dissipation or at finite temperature, even though it will be eventually suppressed as the state becomes more mixed."],"url":"http://arxiv.org/abs/2405.08913v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-14 18:38:18","title":"Multi-resolution Isogeometric Analysis -- Efficient adaptivity utilizing the multi-patch structure","abstract":"Isogeometric Analysis (IgA) is a spline based approach to the numerical solution of partial differential equations. There are two major issues that IgA was designed to address. The first issue is the exact representation of domains stemming from Computer Aided Design (CAD) software. In practice, this can be realized only with multi-patch IgA, often in combination with trimming or similar techniques. The second issue is the realization of high-order discretizations (by increasing the spline degree) with numbers of degrees of freedom comparable to low-order methods. High-order methods can deliver their full potential only if the solution to be approximated is sufficiently smooth; otherwise, adaptive methods are required. In the last decades, a zoo of local refinement strategies for splines has been developed. The authors think that many of these approaches are a burden to implement efficiently and impede the utilization of recent advances that rely on tensor-product splines, e.g., concerning matrix assembly and preconditioning. The implementation seems to be particularly cumbersome in the context of multi-patch IgA. Our approach is to moderately increase the number of patches and to utilize different grid sizes on different patches. This allows reusing the existing code bases, recovers the convergence rates of other adaptive approaches and increases the number of degrees of freedom only marginally.","sentences":["Isogeometric Analysis (IgA) is a spline based approach to the numerical solution of partial differential equations.","There are two major issues that IgA was designed to address.","The first issue is the exact representation of domains stemming from Computer Aided Design (CAD) software.","In practice, this can be realized only with multi-patch IgA, often in combination with trimming or similar techniques.","The second issue is the realization of high-order discretizations (by increasing the spline degree) with numbers of degrees of freedom comparable to low-order methods.","High-order methods can deliver their full potential only if the solution to be approximated is sufficiently smooth; otherwise, adaptive methods are required.","In the last decades, a zoo of local refinement strategies for splines has been developed.","The authors think that many of these approaches are a burden to implement efficiently and impede the utilization of recent advances that rely on tensor-product splines, e.g., concerning matrix assembly and preconditioning.","The implementation seems to be particularly cumbersome in the context of multi-patch IgA. Our approach is to moderately increase the number of patches and to utilize different grid sizes on different patches.","This allows reusing the existing code bases, recovers the convergence rates of other adaptive approaches and increases the number of degrees of freedom only marginally."],"url":"http://arxiv.org/abs/2405.08904v1","category":"math.NA"}
{"created":"2024-05-14 18:15:14","title":"Active flow control over a sphere using a smart morphable skin","abstract":"Dimples on a sphere's surface can lead to significant drag reduction. However, the optimal dimple depth to minimize the drag varies with the Reynolds number ($Re$). In this study, a smart surface-morphing technique is devised that can adjust dimple depth based on the flow conditions to minimize drag across a wide range of $Re$ values. By depressurizing the core of a rigid skeleton enclosed with a thin latex membrane, the dimple depth can be precisely controlled in response to flow velocity changes. A comprehensive series of systematic experiments are performed for Reynolds number range of $6\\times10^4 \\leq Re \\leq 1.3\\times10^5$, and dimple depth ratios of $0 \\leq k/d \\leq 2\\times10^{-2}$ using the morphable sphere. It is observed that the dimple depth ratio $k/d$ significantly affects both the onset of the drag crisis and the minimum achievable drag. As $k/d$ increases, the critical Reynolds number for the drag crisis decreases. However, the minimum achievable drag coefficient decreases as $k/d$ increases. By carefully adjusting the $k/d$ to $Re$ using the morphable approach, our experiments show that $C_D$ reductions up to 50 % can be achieved when compared to a smooth counterpart for all the $Re$ considered. For a constant $Re$, drag reduces as $k/d$ increases. However, there is a critical threshold beyond which drag amplification starts to occur. Particle image velocimetry (PIV) reveals a delay in flow separation on the sphere's surface with increasing $k/d$, causing the separation angle to shift downstream. However, when $k/d$ exceeds the critical threshold, flow separation moves upstream, causing an increase in drag. By using the experimental data, a control model is also developed relating optimum $k/d$ with $Re$ to minimize drag. This model also serves as the basis for adaptive drag control of the sphere for a wide range of Reynolds number.","sentences":["Dimples on a sphere's surface can lead to significant drag reduction.","However, the optimal dimple depth to minimize the drag varies with the Reynolds number ($Re$).","In this study, a smart surface-morphing technique is devised that can adjust dimple depth based on the flow conditions to minimize drag across a wide range of $Re$ values.","By depressurizing the core of a rigid skeleton enclosed with a thin latex membrane, the dimple depth can be precisely controlled in response to flow velocity changes.","A comprehensive series of systematic experiments are performed for Reynolds number range of $6\\times10^4 \\leq Re \\leq 1.3\\times10^5$, and dimple depth ratios of $0 \\leq k/d \\leq 2\\times10^{-2}$ using the morphable sphere.","It is observed that the dimple depth ratio $k/d$ significantly affects both the onset of the drag crisis and the minimum achievable drag.","As $k/d$ increases, the critical Reynolds number for the drag crisis decreases.","However, the minimum achievable drag coefficient decreases as $k/d$ increases.","By carefully adjusting the $k/d$ to $Re$ using the morphable approach, our experiments show that $C_D$ reductions up to 50 % can be achieved when compared to a smooth counterpart for all the $Re$ considered.","For a constant $Re$, drag reduces as $k/d$ increases.","However, there is a critical threshold beyond which drag amplification starts to occur.","Particle image velocimetry (PIV) reveals a delay in flow separation on the sphere's surface with increasing $k/d$, causing the separation angle to shift downstream.","However, when $k/d$ exceeds the critical threshold, flow separation moves upstream, causing an increase in drag.","By using the experimental data, a control model is also developed relating optimum $k/d$ with $Re$ to minimize drag.","This model also serves as the basis for adaptive drag control of the sphere for a wide range of Reynolds number."],"url":"http://arxiv.org/abs/2405.08896v1","category":"physics.flu-dyn"}
{"created":"2024-05-14 18:00:01","title":"On the quantum origin of limit cycles, fixed points, and critical slowing down","abstract":"Among the most iconic features of classical dissipative dynamics are persistent limit-cycle oscillations and critical slowing down at the onset of such oscillations, where the system relaxes purely algebraically in time. On the other hand, quantum systems subject to generic Markovian dissipation decohere exponentially in time, approaching a unique steady state. Here we show how coherent limit-cycle oscillations and algebraic decay can emerge in a quantum system governed by a Markovian master equation as one approaches the classical limit, illustrating general mechanisms using a single-spin model and a two-site lossy Bose-Hubbard model. In particular, we demonstrate that the fingerprint of a limit cycle is a slow-decaying branch with vanishing decoherence rates in the Liouville spectrum, while a power-law decay is realized by a spectral collapse at the bifurcation point. We also show how these are distinct from the case of a classical fixed point, for which the quantum spectrum is gapped and can be generated from the linearized classical dynamics.","sentences":["Among the most iconic features of classical dissipative dynamics are persistent limit-cycle oscillations and critical slowing down at the onset of such oscillations, where the system relaxes purely algebraically in time.","On the other hand, quantum systems subject to generic Markovian dissipation decohere exponentially in time, approaching a unique steady state.","Here we show how coherent limit-cycle oscillations and algebraic decay can emerge in a quantum system governed by a Markovian master equation as one approaches the classical limit, illustrating general mechanisms using a single-spin model and a two-site lossy Bose-Hubbard model.","In particular, we demonstrate that the fingerprint of a limit cycle is a slow-decaying branch with vanishing decoherence rates in the Liouville spectrum, while a power-law decay is realized by a spectral collapse at the bifurcation point.","We also show how these are distinct from the case of a classical fixed point, for which the quantum spectrum is gapped and can be generated from the linearized classical dynamics."],"url":"http://arxiv.org/abs/2405.08866v1","category":"quant-ph"}
{"created":"2024-05-14 18:00:01","title":"Into the red: an M-band study of the chemistry and rotation of $\u03b2$ Pictoris b at high spectral resolution","abstract":"High-resolution cross-correlation spectroscopy (HRCCS) combined with adaptive optics has been enormously successful in advancing our knowledge of exoplanet atmospheres, from chemistry to rotation and atmospheric dynamics. This powerful technique now drives major science cases for ELT instrumentation including METIS/ELT, GMTNIRS/GMT and MICHI/TMT, targeting biosignatures on rocky planets at 3-5 $\\mu$m, but remains untested beyond 3.5 $\\mu$m where the sky thermal background begins to provide the dominant contribution to the noise. We present 3.51-5.21 $\\mu$m M-band CRIRES+/VLT observations of the archetypal young directly imaged gas giant $\\beta$ Pictoris b, detecting CO absorption at S/N = 6.6 at 4.73 $\\mu$m and H$_2$O at S/N = 5.7, and thus extending the use of HRCCS into the thermal background noise dominated infrared. Using this novel spectral range to search for more diverse chemistry we report marginal evidence of SiO at S/N = 4.3, potentially indicative that previously proposed magnesium-silicate clouds in the atmosphere are either patchy, transparent at M-band wavelengths, or possibly absent on the planetary hemisphere observed. The molecular detections are rotationally broadened by the spin of $\\beta$ Pic b, and we infer a planetary rotation velocity of $v$sin(i) = 22$\\pm$2 km s$^{-1}$ from the cross-correlation with the H$_2$O model template, consistent with previous K-band studies. We discuss the observational challenges posed by the thermal background and telluric contamination in the M-band, the custom analysis procedures required to mitigate these issues, and the opportunities to exploit this new infrared window for HRCCS using existing and next-generation instrumentation.","sentences":["High-resolution cross-correlation spectroscopy (HRCCS) combined with adaptive optics has been enormously successful in advancing our knowledge of exoplanet atmospheres, from chemistry to rotation and atmospheric dynamics.","This powerful technique now drives major science cases for ELT instrumentation including METIS/ELT, GMTNIRS/GMT and MICHI/TMT, targeting biosignatures on rocky planets at 3-5 $\\mu$m, but remains untested beyond 3.5 $\\mu$m where the sky thermal background begins to provide the dominant contribution to the noise.","We present 3.51-5.21 $\\mu$m M-band CRIRES+/VLT observations of the archetypal young directly imaged gas giant $\\beta$ Pictoris b, detecting CO absorption at S/N = 6.6 at 4.73 $\\mu$m and H$_2$O at S/N = 5.7, and thus extending the use of HRCCS into the thermal background noise dominated infrared.","Using this novel spectral range to search for more diverse chemistry we report marginal evidence of SiO at S/N = 4.3, potentially indicative that previously proposed magnesium-silicate clouds in the atmosphere are either patchy, transparent at M-band wavelengths, or possibly absent on the planetary","hemisphere observed.","The molecular detections are rotationally broadened by the spin of $\\beta$ Pic b, and we infer a planetary rotation velocity of $v$sin(i)","= 22$\\pm$2 km s$^{-1}$ from the cross-correlation with the H$_2$O model template, consistent with previous K-band studies.","We discuss the observational challenges posed by the thermal background and telluric contamination in the M-band, the custom analysis procedures required to mitigate these issues, and the opportunities to exploit this new infrared window for HRCCS using existing and next-generation instrumentation."],"url":"http://arxiv.org/abs/2405.08867v1","category":"astro-ph.EP"}
{"created":"2024-05-13 17:58:14","title":"Improved LARS algorithm for adaptive LASSO in the linear regression model","abstract":"The adaptive LASSO has been used for consistent variable selection in place of LASSO in the linear regression model. In this article, we propose a modified LARS algorithm to combine adaptive LASSO with some biased estimators, namely the Almost Unbiased Ridge Estimator (AURE), Liu Estimator (LE), Almost Unbiased Liu Estimator (AULE), Principal Component Regression Estimator (PCRE), r-k class estimator, and r-d class estimator. Furthermore, we examine the performance of the proposed algorithm using a Monte Carlo simulation study and real-world examples.","sentences":["The adaptive LASSO has been used for consistent variable selection in place of LASSO in the linear regression model.","In this article, we propose a modified LARS algorithm to combine adaptive LASSO with some biased estimators, namely the Almost Unbiased Ridge Estimator (AURE), Liu Estimator (LE), Almost Unbiased Liu Estimator (AULE), Principal Component Regression Estimator (PCRE), r-k class estimator, and r-d class estimator.","Furthermore, we examine the performance of the proposed algorithm using a Monte Carlo simulation study and real-world examples."],"url":"http://arxiv.org/abs/2405.07985v1","category":"stat.ME"}
{"created":"2024-05-13 17:15:38","title":"Efficient and Universal Merkle Tree Inclusion Proofs via OR Aggregation","abstract":"Zero-knowledge proofs have emerged as a powerful tool for enhancing privacy and security in blockchain applications. However, the efficiency and scalability of proof systems remain a significant challenge, particularly in the context of Merkle tree inclusion proofs. Traditional proof aggregation techniques based on AND logic suffer from high verification complexity and data communication overhead, limiting their practicality for large-scale applications. In this paper, we propose a novel proof aggregation approach based on OR logic, which enables the generation of compact and universally verifiable proofs for Merkle tree inclusion. By aggregating proofs using OR logic, we achieve a proof size that is independent of the number of leaves in the tree, and verification can be performed using any single valid leaf hash. This represents a significant improvement over AND aggregation, which requires the verifier to process all leaf hashes. We formally define the OR aggregation logic, describe the process of generating universal proofs, and provide a comparative analysis demonstrating the advantages of our approach in terms of proof size, verification data, and universality. Furthermore, we discuss the potential of combining OR and AND aggregation logics to create complex acceptance functions, enabling the development of expressive and efficient proof systems for various blockchain applications. The proposed techniques have the potential to significantly enhance the scalability, efficiency, and flexibility of zero-knowledge proof systems, paving the way for more practical and adaptive solutions in the blockchain ecosystem.","sentences":["Zero-knowledge proofs have emerged as a powerful tool for enhancing privacy and security in blockchain applications.","However, the efficiency and scalability of proof systems remain a significant challenge, particularly in the context of Merkle tree inclusion proofs.","Traditional proof aggregation techniques based on AND logic suffer from high verification complexity and data communication overhead, limiting their practicality for large-scale applications.","In this paper, we propose a novel proof aggregation approach based on OR logic, which enables the generation of compact and universally verifiable proofs for Merkle tree inclusion.","By aggregating proofs using OR logic, we achieve a proof size that is independent of the number of leaves in the tree, and verification can be performed using any single valid leaf hash.","This represents a significant improvement over AND aggregation, which requires the verifier to process all leaf hashes.","We formally define the OR aggregation logic, describe the process of generating universal proofs, and provide a comparative analysis demonstrating the advantages of our approach in terms of proof size, verification data, and universality.","Furthermore, we discuss the potential of combining OR and AND aggregation logics to create complex acceptance functions, enabling the development of expressive and efficient proof systems for various blockchain applications.","The proposed techniques have the potential to significantly enhance the scalability, efficiency, and flexibility of zero-knowledge proof systems, paving the way for more practical and adaptive solutions in the blockchain ecosystem."],"url":"http://arxiv.org/abs/2405.07941v1","category":"cs.CR"}
{"created":"2024-05-13 17:09:03","title":"Authentic Hand Avatar from a Phone Scan via Universal Hand Model","abstract":"The authentic 3D hand avatar with every identifiable information, such as hand shapes and textures, is necessary for immersive experiences in AR/VR. In this paper, we present a universal hand model (UHM), which 1) can universally represent high-fidelity 3D hand meshes of arbitrary identities (IDs) and 2) can be adapted to each person with a short phone scan for the authentic hand avatar. For effective universal hand modeling, we perform tracking and modeling at the same time, while previous 3D hand models perform them separately. The conventional separate pipeline suffers from the accumulated errors from the tracking stage, which cannot be recovered in the modeling stage. On the other hand, ours does not suffer from the accumulated errors while having a much more concise overall pipeline. We additionally introduce a novel image matching loss function to address a skin sliding during the tracking and modeling, while existing works have not focused on it much. Finally, using learned priors from our UHM, we effectively adapt our UHM to each person's short phone scan for the authentic hand avatar.","sentences":["The authentic 3D hand avatar with every identifiable information, such as hand shapes and textures, is necessary for immersive experiences in AR/VR.","In this paper, we present a universal hand model (UHM), which 1) can universally represent high-fidelity 3D hand meshes of arbitrary identities (IDs) and 2) can be adapted to each person with a short phone scan for the authentic hand avatar.","For effective universal hand modeling, we perform tracking and modeling at the same time, while previous 3D hand models perform them separately.","The conventional separate pipeline suffers from the accumulated errors from the tracking stage, which cannot be recovered in the modeling stage.","On the other hand, ours does not suffer from the accumulated errors while having a much more concise overall pipeline.","We additionally introduce a novel image matching loss function to address a skin sliding during the tracking and modeling, while existing works have not focused on it much.","Finally, using learned priors from our UHM, we effectively adapt our UHM to each person's short phone scan for the authentic hand avatar."],"url":"http://arxiv.org/abs/2405.07933v1","category":"cs.CV"}
{"created":"2024-05-13 16:57:52","title":"Adaptive first-order methods with enhanced worst-case rates","abstract":"The Optimized Gradient Method (OGM), its strongly convex extension, the Information Theoretical Exact Method (ITEM), as well as the related Triple Momentum Method (TMM) have superior convergence guarantees when compared to the Fast Gradient Method but lack adaptivity and their derivation is incompatible with composite problems. In this work we introduce a slightly modified version of the estimate sequence that can be used to simultaneously derive OGM, ITEM and TMM while adding memory along with the ability to dynamically adjust the convergence guarantees at runtime. Our framework can be extended to the composite setup and we use it to construct an Enhanced Accelerated Composite Gradient Method equipped with fully-adaptive line-search.","sentences":["The Optimized Gradient Method (OGM), its strongly convex extension, the Information Theoretical Exact Method (ITEM), as well as the related Triple Momentum Method (TMM) have superior convergence guarantees when compared to the Fast Gradient Method but lack adaptivity and their derivation is incompatible with composite problems.","In this work we introduce a slightly modified version of the estimate sequence that can be used to simultaneously derive OGM, ITEM and TMM while adding memory along with the ability to dynamically adjust the convergence guarantees at runtime.","Our framework can be extended to the composite setup and we use it to construct an Enhanced Accelerated Composite Gradient Method equipped with fully-adaptive line-search."],"url":"http://arxiv.org/abs/2405.07926v1","category":"math.OC"}
{"created":"2024-05-13 16:52:17","title":"Can Better Text Semantics in Prompt Tuning Improve VLM Generalization?","abstract":"Going beyond mere fine-tuning of vision-language models (VLMs), learnable prompt tuning has emerged as a promising, resource-efficient alternative. Despite their potential, effectively learning prompts faces the following challenges: (i) training in a low-shot scenario results in overfitting, limiting adaptability and yielding weaker performance on newer classes or datasets; (ii) prompt-tuning's efficacy heavily relies on the label space, with decreased performance in large class spaces, signaling potential gaps in bridging image and class concepts. In this work, we ask the question if better text semantics can help address these concerns. In particular, we introduce a prompt-tuning method that leverages class descriptions obtained from large language models (LLMs). Our approach constructs part-level description-guided views of both image and text features, which are subsequently aligned to learn more generalizable prompts. Our comprehensive experiments, conducted across 11 benchmark datasets, outperform established methods, demonstrating substantial improvements.","sentences":["Going beyond mere fine-tuning of vision-language models (VLMs), learnable prompt tuning has emerged as a promising, resource-efficient alternative.","Despite their potential, effectively learning prompts faces the following challenges: (i) training in a low-shot scenario results in overfitting, limiting adaptability and yielding weaker performance on newer classes or datasets; (ii) prompt-tuning's efficacy heavily relies on the label space, with decreased performance in large class spaces, signaling potential gaps in bridging image and class concepts.","In this work, we ask the question if better text semantics can help address these concerns.","In particular, we introduce a prompt-tuning method that leverages class descriptions obtained from large language models (LLMs).","Our approach constructs part-level description-guided views of both image and text features, which are subsequently aligned to learn more generalizable prompts.","Our comprehensive experiments, conducted across 11 benchmark datasets, outperform established methods, demonstrating substantial improvements."],"url":"http://arxiv.org/abs/2405.07921v1","category":"cs.CV"}
{"created":"2024-05-13 16:41:47","title":"Collaborative Planar Pushing of Polytopic Objects with Multiple Robots in Complex Scenes","abstract":"Pushing is a simple yet effective skill for robots to interact with and further change the environment. Related work has been mostly focused on utilizing it as a non-prehensile manipulation primitive for a robotic manipulator. However, it can also be beneficial for low-cost mobile robots that are not equipped with a manipulator. This work tackles the general problem of controlling a team of mobile robots to push collaboratively polytopic objects within complex obstacle-cluttered environments. It incorporates several characteristic challenges for contact-rich tasks such as the hybrid switching among different contact modes and under-actuation due to constrained contact forces. The proposed method is based on hybrid optimization over a sequence of possible modes and the associated pushing forces, where (i) a set of sufficient modes is generated with a multi-directional feasibility estimation, based on quasi-static analyses for general objects and any number of robots; (ii) a hierarchical hybrid search algorithm is designed to iteratively decompose the navigation path via arch segments and select the optimal parameterized mode; and (iii) a nonlinear model predictive controller is proposed to track the desired pushing velocities adaptively online for each robot. The proposed framework is complete under mild assumptions. Its efficiency and effectiveness are validated in high-fidelity simulations and hardware experiments. Robustness to motion and actuation uncertainties is also demonstrated.","sentences":["Pushing is a simple yet effective skill for robots to interact with and further change the environment.","Related work has been mostly focused on utilizing it as a non-prehensile manipulation primitive for a robotic manipulator.","However, it can also be beneficial for low-cost mobile robots that are not equipped with a manipulator.","This work tackles the general problem of controlling a team of mobile robots to push collaboratively polytopic objects within complex obstacle-cluttered environments.","It incorporates several characteristic challenges for contact-rich tasks such as the hybrid switching among different contact modes and under-actuation due to constrained contact forces.","The proposed method is based on hybrid optimization over a sequence of possible modes and the associated pushing forces, where (i) a set of sufficient modes is generated with a multi-directional feasibility estimation, based on quasi-static analyses for general objects and any number of robots; (ii) a hierarchical hybrid search algorithm is designed to iteratively decompose the navigation path via arch segments and select the optimal parameterized mode; and (iii) a nonlinear model predictive controller is proposed to track the desired pushing velocities adaptively online for each robot.","The proposed framework is complete under mild assumptions.","Its efficiency and effectiveness are validated in high-fidelity simulations and hardware experiments.","Robustness to motion and actuation uncertainties is also demonstrated."],"url":"http://arxiv.org/abs/2405.07908v1","category":"cs.RO"}
{"created":"2024-05-13 16:41:32","title":"Robust Quantum Sensing with Multiparameter Decorrelation","abstract":"The performance of a quantum sensor is fundamentally limited by noise. This noise is particularly damaging when it becomes correlated with the readout of a target signal, caused by fluctuations of the sensor's operating parameters. These uncertainties limit sensitivity in a way that can be understood with multiparameter estimation theory. We develop a new approach, adaptable to any quantum platform, for designing robust sensing protocols that leverages multiparameter estimation theory and machine learning to decorrelate a target signal from fluctuating off-target (``nuisance'') parameters. Central to our approach is the identification of information-theoretic goals that guide a machine learning agent through an otherwise intractably large space of potential sensing protocols. As an illustrative example, we apply our approach to a reconfigurable optical lattice to design an accelerometer whose sensitivity is decorrelated from lattice depth noise. We demonstrate the effect of decorrelation on outcomes and Bayesian inferencing through statistical analysis in parameter space, and discuss implications for future applications in quantum metrology and computing.","sentences":["The performance of a quantum sensor is fundamentally limited by noise.","This noise is particularly damaging when it becomes correlated with the readout of a target signal, caused by fluctuations of the sensor's operating parameters.","These uncertainties limit sensitivity in a way that can be understood with multiparameter estimation theory.","We develop a new approach, adaptable to any quantum platform, for designing robust sensing protocols that leverages multiparameter estimation theory and machine learning to decorrelate a target signal from fluctuating off-target (``nuisance'') parameters.","Central to our approach is the identification of information-theoretic goals that guide a machine learning agent through an otherwise intractably large space of potential sensing protocols.","As an illustrative example, we apply our approach to a reconfigurable optical lattice to design an accelerometer whose sensitivity is decorrelated from lattice depth noise.","We demonstrate the effect of decorrelation on outcomes and Bayesian inferencing through statistical analysis in parameter space, and discuss implications for future applications in quantum metrology and computing."],"url":"http://arxiv.org/abs/2405.07907v1","category":"quant-ph"}
{"created":"2024-05-13 16:40:17","title":"PLUTO: Pathology-Universal Transformer","abstract":"Pathology is the study of microscopic inspection of tissue, and a pathology diagnosis is often the medical gold standard to diagnose disease. Pathology images provide a unique challenge for computer-vision-based analysis: a single pathology Whole Slide Image (WSI) is gigapixel-sized and often contains hundreds of thousands to millions of objects of interest across multiple resolutions. In this work, we propose PathoLogy Universal TransfOrmer (PLUTO): a light-weight pathology FM that is pre-trained on a diverse dataset of 195 million image tiles collected from multiple sites and extracts meaningful representations across multiple WSI scales that enable a large variety of downstream pathology tasks. In particular, we design task-specific adaptation heads that utilize PLUTO's output embeddings for tasks which span pathology scales ranging from subcellular to slide-scale, including instance segmentation, tile classification, and slide-level prediction. We compare PLUTO's performance to other state-of-the-art methods on a diverse set of external and internal benchmarks covering multiple biologically relevant tasks, tissue types, resolutions, stains, and scanners. We find that PLUTO matches or outperforms existing task-specific baselines and pathology-specific foundation models, some of which use orders-of-magnitude larger datasets and model sizes when compared to PLUTO. Our findings present a path towards a universal embedding to power pathology image analysis, and motivate further exploration around pathology foundation models in terms of data diversity, architectural improvements, sample efficiency, and practical deployability in real-world applications.","sentences":["Pathology is the study of microscopic inspection of tissue, and a pathology diagnosis is often the medical gold standard to diagnose disease.","Pathology images provide a unique challenge for computer-vision-based analysis: a single pathology Whole Slide Image (WSI) is gigapixel-sized and often contains hundreds of thousands to millions of objects of interest across multiple resolutions.","In this work, we propose PathoLogy Universal TransfOrmer (PLUTO): a light-weight pathology FM that is pre-trained on a diverse dataset of 195 million image tiles collected from multiple sites and extracts meaningful representations across multiple WSI scales that enable a large variety of downstream pathology tasks.","In particular, we design task-specific adaptation heads that utilize PLUTO's output embeddings for tasks which span pathology scales ranging from subcellular to slide-scale, including instance segmentation, tile classification, and slide-level prediction.","We compare PLUTO's performance to other state-of-the-art methods on a diverse set of external and internal benchmarks covering multiple biologically relevant tasks, tissue types, resolutions, stains, and scanners.","We find that PLUTO matches or outperforms existing task-specific baselines and pathology-specific foundation models, some of which use orders-of-magnitude larger datasets and model sizes when compared to PLUTO.","Our findings present a path towards a universal embedding to power pathology image analysis, and motivate further exploration around pathology foundation models in terms of data diversity, architectural improvements, sample efficiency, and practical deployability in real-world applications."],"url":"http://arxiv.org/abs/2405.07905v1","category":"eess.IV"}
{"created":"2024-05-13 16:38:09","title":"Predicting State Transitions in Autonomous Nonlinear Bistable Systems with Hidden Stochasticity","abstract":"Bistable autonomous systems can be found inmany areas of science. When the intrinsic noise intensity is large, these systems exhibits stochastic transitions from onemetastable steady state to another. In electronic bistable memories, these transitions are failures, usually simulated in a Monte-Carlo fashion at a high CPU-time price. Existing closed-form formulas, relying on near-stable-steady-state approximations of the nonlinear system dynamics to estimate the mean transition time, have turned out inaccurate. Our contribution is twofold. From a unidimensional stochastic model of overdamped autonomous systems, we propose an extended Eyring-Kramers analytical formula accounting for both nonlinear drift and state-dependent white noise variance, rigorously derived from It\\^o stochastic calculus. We also adapt it to practical system engineering situations where the intrinsic noise sources are hidden and can only be inferred from the fluctuations of observables measured in steady states. First numerical trials on an industrial electronic case study suggest that our approximate prediction formula achieve remarkable accuracy, outperforming previous non-Monte-Carlo approaches.","sentences":["Bistable autonomous systems can be found inmany areas of science.","When the intrinsic noise intensity is large, these systems exhibits stochastic transitions from onemetastable steady state to another.","In electronic bistable memories, these transitions are failures, usually simulated in a Monte-Carlo fashion at a high CPU-time price.","Existing closed-form formulas, relying on near-stable-steady-state approximations of the nonlinear system dynamics to estimate the mean transition time, have turned out inaccurate.","Our contribution is twofold.","From a unidimensional stochastic model of overdamped autonomous systems, we propose an extended Eyring-Kramers analytical formula accounting for both nonlinear drift and state-dependent white noise variance, rigorously derived from It\\^o stochastic calculus.","We also adapt it to practical system engineering situations where the intrinsic noise sources are hidden and can only be inferred from the fluctuations of observables measured in steady states.","First numerical trials on an industrial electronic case study suggest that our approximate prediction formula achieve remarkable accuracy, outperforming previous non-Monte-Carlo approaches."],"url":"http://arxiv.org/abs/2405.07902v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-13 16:37:23","title":"Physically Consistent Online Inertial Adaptation for Humanoid Loco-manipulation","abstract":"The ability to accomplish manipulation and locomotion tasks in the presence of significant time-varying external loads is a remarkable skill of humans that has yet to be replicated convincingly by humanoid robots. Such an ability will be a key requirement in the environments we envision deploying our robots: dull, dirty, and dangerous. External loads constitute a large model bias, which is typically unaccounted for. In this work, we enable our humanoid robot to engage in loco-manipulation tasks in the presence of significant model bias due to external loads. We propose an online estimation and control framework involving the combination of a physically consistent extended Kalman filter for inertial parameter estimation coupled to a whole-body controller. We showcase our results both in simulation and in hardware, where weights are mounted on Nadia's wrist links as a proxy for engaging in tasks where large external loads are applied to the robot.","sentences":["The ability to accomplish manipulation and locomotion tasks in the presence of significant time-varying external loads is a remarkable skill of humans that has yet to be replicated convincingly by humanoid robots.","Such an ability will be a key requirement in the environments we envision deploying our robots: dull, dirty, and dangerous.","External loads constitute a large model bias, which is typically unaccounted for.","In this work, we enable our humanoid robot to engage in loco-manipulation tasks in the presence of significant model bias due to external loads.","We propose an online estimation and control framework involving the combination of a physically consistent extended Kalman filter for inertial parameter estimation coupled to a whole-body controller.","We showcase our results both in simulation and in hardware, where weights are mounted on Nadia's wrist links as a proxy for engaging in tasks where large external loads are applied to the robot."],"url":"http://arxiv.org/abs/2405.07901v1","category":"cs.RO"}
{"created":"2024-05-13 15:38:33","title":"Knowledge Graph Embedding in Intent-Based Networking","abstract":"This paper presents a novel approach to network management by integrating intent-based networking (IBN) with knowledge graphs (KGs), creating a more intuitive and efficient pipeline for service orchestration. By mapping high-level business intents onto network configurations using KGs, the system dynamically adapts to network changes and service demands, ensuring optimal performance and resource allocation. We utilize knowledge graph embedding (KGE) to acquire context information from the network and service providers. The KGE model is trained using a custom KG and Gaussian embedding model and maps intents to services via service prediction and intent validation processes. The proposed intent lifecycle enables intent translation and assurance by only deploying validated intents according to network and resource availability. We evaluate the trained model for its efficiency in service mapping and intent validation tasks using simulated environments and extensive experiments. The service prediction and intent verification accuracy greater than 80 percent is achieved for the trained KGE model on a custom service orchestration intent knowledge graph (IKG) based on TMForum's intent common model.","sentences":["This paper presents a novel approach to network management by integrating intent-based networking (IBN) with knowledge graphs (KGs), creating a more intuitive and efficient pipeline for service orchestration.","By mapping high-level business intents onto network configurations using KGs, the system dynamically adapts to network changes and service demands, ensuring optimal performance and resource allocation.","We utilize knowledge graph embedding (KGE) to acquire context information from the network and service providers.","The KGE model is trained using a custom KG and Gaussian embedding model and maps intents to services via service prediction and intent validation processes.","The proposed intent lifecycle enables intent translation and assurance by only deploying validated intents according to network and resource availability.","We evaluate the trained model for its efficiency in service mapping and intent validation tasks using simulated environments and extensive experiments.","The service prediction and intent verification accuracy greater than 80 percent is achieved for the trained KGE model on a custom service orchestration intent knowledge graph (IKG) based on TMForum's intent common model."],"url":"http://arxiv.org/abs/2405.07850v1","category":"cs.NI"}
{"created":"2024-05-13 15:24:27","title":"Adaptive Exploration for Data-Efficient General Value Function Evaluations","abstract":"General Value Functions (GVFs) (Sutton et al, 2011) are an established way to represent predictive knowledge in reinforcement learning. Each GVF computes the expected return for a given policy, based on a unique pseudo-reward. Multiple GVFs can be estimated in parallel using off-policy learning from a single stream of data, often sourced from a fixed behavior policy or pre-collected dataset. This leaves an open question: how can behavior policy be chosen for data-efficient GVF learning? To address this gap, we propose GVFExplorer, which aims at learning a behavior policy that efficiently gathers data for evaluating multiple GVFs in parallel. This behavior policy selects actions in proportion to the total variance in the return across all GVFs, reducing the number of environmental interactions. To enable accurate variance estimation, we use a recently proposed temporal-difference-style variance estimator. We prove that each behavior policy update reduces the mean squared error in the summed predictions over all GVFs. We empirically demonstrate our method's performance in both tabular representations and nonlinear function approximation.","sentences":["General Value Functions (GVFs) (Sutton et al, 2011) are an established way to represent predictive knowledge in reinforcement learning.","Each GVF computes the expected return for a given policy, based on a unique pseudo-reward.","Multiple GVFs can be estimated in parallel using off-policy learning from a single stream of data, often sourced from a fixed behavior policy or pre-collected dataset.","This leaves an open question: how can behavior policy be chosen for data-efficient GVF learning?","To address this gap, we propose GVFExplorer, which aims at learning a behavior policy that efficiently gathers data for evaluating multiple GVFs in parallel.","This behavior policy selects actions in proportion to the total variance in the return across all GVFs, reducing the number of environmental interactions.","To enable accurate variance estimation, we use a recently proposed temporal-difference-style variance estimator.","We prove that each behavior policy update reduces the mean squared error in the summed predictions over all GVFs.","We empirically demonstrate our method's performance in both tabular representations and nonlinear function approximation."],"url":"http://arxiv.org/abs/2405.07838v1","category":"cs.LG"}
{"created":"2024-05-13 15:20:31","title":"Adaptive Human-Swarm Interaction based on Workload Measurement using Functional Near-Infrared Spectroscopy","abstract":"One of the challenges of human-swarm interaction (HSI) is how to manage the operator's workload. In order to do this, we propose a novel neurofeedback technique for the real-time measurement of workload using functional near-infrared spectroscopy (fNIRS). The objective is to develop a baseline for workload measurement in human-swarm interaction using fNIRS and to develop an interface that dynamically adapts to the operator's workload. The proposed method consists of using fNIRS device to measure brain activity, process this through a machine learning algorithm, and pass it on to the HSI interface. By dynamically adapting the HSI interface, the swarm operator's workload could be reduced and the performance improved.","sentences":["One of the challenges of human-swarm interaction (HSI) is how to manage the operator's workload.","In order to do this, we propose a novel neurofeedback technique for the real-time measurement of workload using functional near-infrared spectroscopy (fNIRS).","The objective is to develop a baseline for workload measurement in human-swarm interaction using fNIRS and to develop an interface that dynamically adapts to the operator's workload.","The proposed method consists of using fNIRS device to measure brain activity, process this through a machine learning algorithm, and pass it on to the HSI interface.","By dynamically adapting the HSI interface, the swarm operator's workload could be reduced and the performance improved."],"url":"http://arxiv.org/abs/2405.07834v1","category":"cs.RO"}
{"created":"2024-05-13 15:20:21","title":"Subradiance and Superradiant Long Range Excitation Transport among Quantum Emitter Ensembles in a Waveguide","abstract":"In contrast to free space, in waveguides the dispersive and dissipative dipole-dipole interactions among quantum emitters exhibit a periodic behavior over remarkably long distances. We propose a novel setup exploiting this long-range periodicity in order to create highly excited subradiant states and facilitate fast controlled collective energy transport amongst far-apart ensembles coupled to a waveguide. For sufficiently large ensembles collective superradiant emission into the fiber modes dominates over its free space counterpart. We show that for a large number of emitters a fast transverse coherent pulse can create almost perfect subradiant states with up to $50\\%$ excitation. On the other hand, for a coherent excitation of one sub-ensemble above an overall excitation fraction of $50\\%$ we find a nearly lossless and fast energy transfer to the ground state sub-ensemble. This transport can be enhanced or suppressed by controlling the positions of the ensembles relative to each other, while it can also be realized with a random position distribution. In the optimally enhanced case this fast transfer appears as superradiant emission with subsequent superabsorption, yet, without a superradiant decay after the absorption. The highly excited subradiant states as well as the superradiant excitation transfer appear as suitable building blocks in applications like active atomic clocks, quantum batteries, quantum information protocols and quantum metrology procedures such as fiber-based Ramsey schemes.","sentences":["In contrast to free space, in waveguides the dispersive and dissipative dipole-dipole interactions among quantum emitters exhibit a periodic behavior over remarkably long distances.","We propose a novel setup exploiting this long-range periodicity in order to create highly excited subradiant states and facilitate fast controlled collective energy transport amongst far-apart ensembles coupled to a waveguide.","For sufficiently large ensembles collective superradiant emission into the fiber modes dominates over its free space counterpart.","We show that for a large number of emitters a fast transverse coherent pulse can create almost perfect subradiant states with up to $50\\%$ excitation.","On the other hand, for a coherent excitation of one sub-ensemble above an overall excitation fraction of $50\\%$ we find a nearly lossless and fast energy transfer to the ground state sub-ensemble.","This transport can be enhanced or suppressed by controlling the positions of the ensembles relative to each other, while it can also be realized with a random position distribution.","In the optimally enhanced case this fast transfer appears as superradiant emission with subsequent superabsorption, yet, without a superradiant decay after the absorption.","The highly excited subradiant states as well as the superradiant excitation transfer appear as suitable building blocks in applications like active atomic clocks, quantum batteries, quantum information protocols and quantum metrology procedures such as fiber-based Ramsey schemes."],"url":"http://arxiv.org/abs/2405.07833v1","category":"quant-ph"}
{"created":"2024-05-13 14:47:34","title":"A Decentralized and Self-Adaptive Approach for Monitoring Volatile Edge Environments","abstract":"Edge computing provides resources for IoT workloads at the network edge. Monitoring systems are vital for efficiently managing resources and application workloads by collecting, storing, and providing relevant information about the state of the resources. However, traditional monitoring systems have a centralized architecture for both data plane and control plane, which increases latency, creates a failure bottleneck, and faces challenges in providing quick and trustworthy data in volatile edge environments, especially where infrastructures are often built upon failure-prone, unsophisticated computing and network resources. Thus, we propose DEMon, a decentralized, self-adaptive monitoring system for edge. DEMon leverages the stochastic gossip communication protocol at its core. It develops efficient protocols for information dissemination, communication, and retrieval, avoiding a single point of failure and ensuring fast and trustworthy data access. Its decentralized control enables self-adaptive management of monitoring parameters, addressing the trade-offs between the quality of service of monitoring and resource consumption. We implement the proposed system as a lightweight and portable container-based system and evaluate it through experiments. We also present a use case demonstrating its feasibility. The results show that DEMon efficiently disseminates and retrieves the monitoring information, addressing the challenges of edge monitoring.","sentences":["Edge computing provides resources for IoT workloads at the network edge.","Monitoring systems are vital for efficiently managing resources and application workloads by collecting, storing, and providing relevant information about the state of the resources.","However, traditional monitoring systems have a centralized architecture for both data plane and control plane, which increases latency, creates a failure bottleneck, and faces challenges in providing quick and trustworthy data in volatile edge environments, especially where infrastructures are often built upon failure-prone, unsophisticated computing and network resources.","Thus, we propose DEMon, a decentralized, self-adaptive monitoring system for edge.","DEMon leverages the stochastic gossip communication protocol at its core.","It develops efficient protocols for information dissemination, communication, and retrieval, avoiding a single point of failure and ensuring fast and trustworthy data access.","Its decentralized control enables self-adaptive management of monitoring parameters, addressing the trade-offs between the quality of service of monitoring and resource consumption.","We implement the proposed system as a lightweight and portable container-based system and evaluate it through experiments.","We also present a use case demonstrating its feasibility.","The results show that DEMon efficiently disseminates and retrieves the monitoring information, addressing the challenges of edge monitoring."],"url":"http://arxiv.org/abs/2405.07806v1","category":"cs.DC"}
{"created":"2024-05-13 14:46:39","title":"Multiple stochastic resonances and inverse stochastic resonances in asymmetric bistable system under the ultra-high frequency excitation","abstract":"Ultra-high frequency linear frequency modulation (UHF-LFM) signal, as a kind of typical non-stationary signal, has been widely used in microwave radar and other fields, with advantages such as long transmission distance, strong anti-interference ability, and wide bandwidth. Utilizing optimal dynamics response has unique advantages in weak feature identification under strong background noise. We propose a new stochastic resonance method in an asymmetric bistable system with the time-varying parameter to handle this special non-stationary signal. Interestingly, the nonlinear response exhibits multiple stochastic resonances (MSR) and inverse stochastic resonances (ISR) under UHF-LFM signal excitation, and some resonance regions may deviate or collapse due to the influence of system asymmetry. In addition, we analyze the responses of each resonance region and the mechanism and evolution law of each resonance region in detail. Finally, we significantly expand the resonance region within the parameter range by optimizing the time scale, which verifies the effectiveness of the proposed time-varying scale method. The mechanism and evolution law of MSR and ISR will provide references for researchers in related fields.","sentences":["Ultra-high frequency linear frequency modulation (UHF-LFM) signal, as a kind of typical non-stationary signal, has been widely used in microwave radar and other fields, with advantages such as long transmission distance, strong anti-interference ability, and wide bandwidth.","Utilizing optimal dynamics response has unique advantages in weak feature identification under strong background noise.","We propose a new stochastic resonance method in an asymmetric bistable system with the time-varying parameter to handle this special non-stationary signal.","Interestingly, the nonlinear response exhibits multiple stochastic resonances (MSR) and inverse stochastic resonances (ISR) under UHF-LFM signal excitation, and some resonance regions may deviate or collapse due to the influence of system asymmetry.","In addition, we analyze the responses of each resonance region and the mechanism and evolution law of each resonance region in detail.","Finally, we significantly expand the resonance region within the parameter range by optimizing the time scale, which verifies the effectiveness of the proposed time-varying scale method.","The mechanism and evolution law of MSR and ISR will provide references for researchers in related fields."],"url":"http://arxiv.org/abs/2405.07804v1","category":"nlin.AO"}
{"created":"2024-05-13 14:37:03","title":"Decentralized Kernel Ridge Regression Based on Data-dependent Random Feature","abstract":"Random feature (RF) has been widely used for node consistency in decentralized kernel ridge regression (KRR). Currently, the consistency is guaranteed by imposing constraints on coefficients of features, necessitating that the random features on different nodes are identical. However, in many applications, data on different nodes varies significantly on the number or distribution, which calls for adaptive and data-dependent methods that generate different RFs. To tackle the essential difficulty, we propose a new decentralized KRR algorithm that pursues consensus on decision functions, which allows great flexibility and well adapts data on nodes. The convergence is rigorously given and the effectiveness is numerically verified: by capturing the characteristics of the data on each node, while maintaining the same communication costs as other methods, we achieved an average regression accuracy improvement of 25.5\\% across six real-world data sets.","sentences":["Random feature (RF) has been widely used for node consistency in decentralized kernel ridge regression (KRR).","Currently, the consistency is guaranteed by imposing constraints on coefficients of features, necessitating that the random features on different nodes are identical.","However, in many applications, data on different nodes varies significantly on the number or distribution, which calls for adaptive and data-dependent methods that generate different RFs.","To tackle the essential difficulty, we propose a new decentralized KRR algorithm that pursues consensus on decision functions, which allows great flexibility and well adapts data on nodes.","The convergence is rigorously given and the effectiveness is numerically verified: by capturing the characteristics of the data on each node, while maintaining the same communication costs as other methods, we achieved an average regression accuracy improvement of 25.5\\% across six real-world data sets."],"url":"http://arxiv.org/abs/2405.07791v1","category":"cs.LG"}
{"created":"2024-05-13 14:21:18","title":"SAR Image Synthesis with Diffusion Models","abstract":"In recent years, diffusion models (DMs) have become a popular method for generating synthetic data. By achieving samples of higher quality, they quickly became superior to generative adversarial networks (GANs) and the current state-of-the-art method in generative modeling. However, their potential has not yet been exploited in radar, where the lack of available training data is a long-standing problem. In this work, a specific type of DMs, namely denoising diffusion probabilistic model (DDPM) is adapted to the SAR domain. We investigate the network choice and specific diffusion parameters for conditional and unconditional SAR image generation. In our experiments, we show that DDPM qualitatively and quantitatively outperforms state-of-the-art GAN-based methods for SAR image generation. Finally, we show that DDPM profits from pretraining on largescale clutter data, generating SAR images of even higher quality.","sentences":["In recent years, diffusion models (DMs) have become a popular method for generating synthetic data.","By achieving samples of higher quality, they quickly became superior to generative adversarial networks (GANs) and the current state-of-the-art method in generative modeling.","However, their potential has not yet been exploited in radar, where the lack of available training data is a long-standing problem.","In this work, a specific type of DMs, namely denoising diffusion probabilistic model (DDPM) is adapted to the SAR domain.","We investigate the network choice and specific diffusion parameters for conditional and unconditional SAR image generation.","In our experiments, we show that DDPM qualitatively and quantitatively outperforms state-of-the-art GAN-based methods for SAR image generation.","Finally, we show that DDPM profits from pretraining on largescale clutter data, generating SAR images of even higher quality."],"url":"http://arxiv.org/abs/2405.07776v1","category":"cs.CV"}
{"created":"2024-05-13 13:59:59","title":"MADRL-Based Rate Adaptation for 360$\\degree$ Video Streaming with Multi-Viewpoint Prediction","abstract":"Over the last few years, 360$\\degree$ video traffic on the network has grown significantly. A key challenge of 360$\\degree$ video playback is ensuring a high quality of experience (QoE) with limited network bandwidth. Currently, most studies focus on tile-based adaptive bitrate (ABR) streaming based on single viewport prediction to reduce bandwidth consumption. However, the performance of models for single-viewpoint prediction is severely limited by the inherent uncertainty in head movement, which can not cope with the sudden movement of users very well. This paper first presents a multimodal spatial-temporal attention transformer to generate multiple viewpoint trajectories with their probabilities given a historical trajectory. The proposed method models viewpoint prediction as a classification problem and uses attention mechanisms to capture the spatial and temporal characteristics of input video frames and viewpoint trajectories for multi-viewpoint prediction. After that, a multi-agent deep reinforcement learning (MADRL)-based ABR algorithm utilizing multi-viewpoint prediction for 360$\\degree$ video streaming is proposed for maximizing different QoE objectives under various network conditions. We formulate the ABR problem as a decentralized partially observable Markov decision process (Dec-POMDP) problem and present a MAPPO algorithm based on centralized training and decentralized execution (CTDE) framework to solve the problem. The experimental results show that our proposed method improves the defined QoE metric by up to 85.5\\% compared to existing ABR methods.","sentences":["Over the last few years, 360$\\degree$ video traffic on the network has grown significantly.","A key challenge of 360$\\degree$ video playback is ensuring a high quality of experience (QoE) with limited network bandwidth.","Currently, most studies focus on tile-based adaptive bitrate (ABR) streaming based on single viewport prediction to reduce bandwidth consumption.","However, the performance of models for single-viewpoint prediction is severely limited by the inherent uncertainty in head movement, which can not cope with the sudden movement of users very well.","This paper first presents a multimodal spatial-temporal attention transformer to generate multiple viewpoint trajectories with their probabilities given a historical trajectory.","The proposed method models viewpoint prediction as a classification problem and uses attention mechanisms to capture the spatial and temporal characteristics of input video frames and viewpoint trajectories for multi-viewpoint prediction.","After that, a multi-agent deep reinforcement learning (MADRL)-based ABR algorithm utilizing multi-viewpoint prediction for 360$\\degree$ video streaming is proposed for maximizing different QoE objectives under various network conditions.","We formulate the ABR problem as a decentralized partially observable Markov decision process (Dec-POMDP) problem and present a MAPPO algorithm based on centralized training and decentralized execution (CTDE) framework to solve the problem.","The experimental results show that our proposed method improves the defined QoE metric by up to 85.5\\% compared to existing ABR methods."],"url":"http://arxiv.org/abs/2405.07759v1","category":"cs.MM"}
{"created":"2024-05-13 13:56:25","title":"Minimax rates in variance and covariance changepoint testing","abstract":"We study the detection of a change in the spatial covariance matrix of $n$ independent sub-Gaussian random variables of dimension $p$. Our first contribution is to show that $\\log\\log(8n)$ is the exact minimax testing rate for a change in variance when $p=1$, thereby giving a complete characterization of the problem for univariate data. Our second contribution is to derive a lower bound on the minimax testing rate under the operator norm, taking a certain notion of sparsity into account. In the low- to moderate-dimensional region of the parameter space, we are able to match the lower bound from above with an optimal test based on sparse eigenvalues. In the remaining region of the parameter space, where the dimensionality is high, the minimax lower bound implies that changepoint testing is very difficult. As our third contribution, we propose a computationally feasible variant of the optimal multivariate test for a change in covariance, which is also adaptive to the nominal noise level and the sparsity level of the change.","sentences":["We study the detection of a change in the spatial covariance matrix of $n$ independent sub-Gaussian random variables of dimension $p$. Our first contribution is to show that $\\log\\log(8n)$ is the exact minimax testing rate for a change in variance when $p=1$, thereby giving a complete characterization of the problem for univariate data.","Our second contribution is to derive a lower bound on the minimax testing rate under the operator norm, taking a certain notion of sparsity into account.","In the low- to moderate-dimensional region of the parameter space, we are able to match the lower bound from above with an optimal test based on sparse eigenvalues.","In the remaining region of the parameter space, where the dimensionality is high, the minimax lower bound implies that changepoint testing is very difficult.","As our third contribution, we propose a computationally feasible variant of the optimal multivariate test for a change in covariance, which is also adaptive to the nominal noise level and the sparsity level of the change."],"url":"http://arxiv.org/abs/2405.07757v1","category":"math.ST"}
{"created":"2024-05-13 13:41:59","title":"LlamaTurk: Adapting Open-Source Generative Large Language Models for Low-Resource Language","abstract":"Despite advancements in English-dominant generative large language models, further development is needed for low-resource languages to enhance global accessibility. The primary methods for representing these languages are monolingual and multilingual pretraining. Monolingual pretraining is expensive due to hardware requirements, and multilingual models often have uneven performance across languages. This study explores an alternative solution by adapting large language models, primarily trained on English, to low-resource languages. We assess various strategies, including continual training, instruction fine-tuning, task-specific fine-tuning, and vocabulary extension. The results show that continual training improves language comprehension, as reflected in perplexity scores, and task-specific tuning generally enhances performance of downstream tasks. However, extending the vocabulary shows no substantial benefits. Additionally, while larger models improve task performance with few-shot tuning, multilingual models perform worse than their monolingual counterparts when adapted.","sentences":["Despite advancements in English-dominant generative large language models, further development is needed for low-resource languages to enhance global accessibility.","The primary methods for representing these languages are monolingual and multilingual pretraining.","Monolingual pretraining is expensive due to hardware requirements, and multilingual models often have uneven performance across languages.","This study explores an alternative solution by adapting large language models, primarily trained on English, to low-resource languages.","We assess various strategies, including continual training, instruction fine-tuning, task-specific fine-tuning, and vocabulary extension.","The results show that continual training improves language comprehension, as reflected in perplexity scores, and task-specific tuning generally enhances performance of downstream tasks.","However, extending the vocabulary shows no substantial benefits.","Additionally, while larger models improve task performance with few-shot tuning, multilingual models perform worse than their monolingual counterparts when adapted."],"url":"http://arxiv.org/abs/2405.07745v1","category":"cs.CL"}
{"created":"2024-05-13 13:40:55","title":"MoCo: Fuzzing Deep Learning Libraries via Assembling Code","abstract":"The rapidly developing deep learning (DL) techniques have been applied in software systems with various application scenarios. However, they could also pose new safety threats with potentially serious consequences, especially in safety-critical domains. DL libraries serve as the underlying foundation for DL systems, and bugs in them can have unpredictable impacts that directly affect the behaviors of DL systems. Previous research on fuzzing DL libraries still has limitations in the diversity of test inputs, the construction of test oracles, and the precision of detection. In this paper, we propose MoCo, a novel fuzzing testing method for DL libraries via assembling code. MoCo first disassembles the seed code file to obtain the template and code blocks, and then employs code block mutation operators (e.g., API replacement, random generation, and boundary checking) to generate more new code blocks adapted to the template. By inserting context-appropriate code blocks into the template step by step, MoCo can generate a tree of code files with intergenerational relations. According to the derivation relations in this tree and the applied mutation operators, we construct the test oracle based on the execution state consistency. Since the granularity of code assembly and mutation is controlled rather than randomly divergent, we can quickly pinpoint the lines of code where the bugs are located and the corresponding triggering conditions. We conduct a comprehensive experiment to evaluate the efficiency and effectiveness of MoCo using three widely-used DL libraries (i.e., TensorFlow, PyTorch, and Jittor). During the experiment, MoCo detects 64 new bugs of four types in three DL libraries, where 51 bugs have been confirmed, and 13 bugs have been fixed by developers.","sentences":["The rapidly developing deep learning (DL) techniques have been applied in software systems with various application scenarios.","However, they could also pose new safety threats with potentially serious consequences, especially in safety-critical domains.","DL libraries serve as the underlying foundation for DL systems, and bugs in them can have unpredictable impacts that directly affect the behaviors of DL systems.","Previous research on fuzzing DL libraries still has limitations in the diversity of test inputs, the construction of test oracles, and the precision of detection.","In this paper, we propose MoCo, a novel fuzzing testing method for DL libraries via assembling code.","MoCo first disassembles the seed code file to obtain the template and code blocks, and then employs code block mutation operators (e.g., API replacement, random generation, and boundary checking) to generate more new code blocks adapted to the template.","By inserting context-appropriate code blocks into the template step by step, MoCo can generate a tree of code files with intergenerational relations.","According to the derivation relations in this tree and the applied mutation operators, we construct the test oracle based on the execution state consistency.","Since the granularity of code assembly and mutation is controlled rather than randomly divergent, we can quickly pinpoint the lines of code where the bugs are located and the corresponding triggering conditions.","We conduct a comprehensive experiment to evaluate the efficiency and effectiveness of MoCo using three widely-used DL libraries (i.e., TensorFlow, PyTorch, and Jittor).","During the experiment, MoCo detects 64 new bugs of four types in three DL libraries, where 51 bugs have been confirmed, and 13 bugs have been fixed by developers."],"url":"http://arxiv.org/abs/2405.07744v1","category":"cs.SE"}
{"created":"2024-05-13 13:39:59","title":"The interaction of gravitational waves with matter","abstract":"It is well-known that gravitational waves (GWs) undergo no absorption or dissipation when traversing through a perfect fluid. However, in the presence of a viscous fluid, GWs transfer energy to the fluid medium. In this essay, we present a review of our recent series of results regarding the interaction between gravitational waves and surrounding matter. Additionally, we examine the impact of a viscous fluid shell on gravitational wave propagation, focusing particularly on GW damping and GW heating. Furthermore, we explore the significance of these effects in various astrophysical scenarios such as core-collapse Supernovae and primordial gravitational waves.","sentences":["It is well-known that gravitational waves (GWs) undergo no absorption or dissipation when traversing through a perfect fluid.","However, in the presence of a viscous fluid, GWs transfer energy to the fluid medium.","In this essay, we present a review of our recent series of results regarding the interaction between gravitational waves and surrounding matter.","Additionally, we examine the impact of a viscous fluid shell on gravitational wave propagation, focusing particularly on GW damping and GW heating.","Furthermore, we explore the significance of these effects in various astrophysical scenarios such as core-collapse Supernovae and primordial gravitational waves."],"url":"http://arxiv.org/abs/2405.07743v1","category":"gr-qc"}
{"created":"2024-05-13 13:24:17","title":"Does Dependency Locality Predict Non-canonical Word Order in Hindi?","abstract":"Previous work has shown that isolated non-canonical sentences with Object-before-Subject (OSV) order are initially harder to process than their canonical counterparts with Subject-before-Object (SOV) order. Although this difficulty diminishes with appropriate discourse context, the underlying cognitive factors responsible for alleviating processing challenges in OSV sentences remain a question. In this work, we test the hypothesis that dependency length minimization is a significant predictor of non-canonical (OSV) syntactic choices, especially when controlling for information status such as givenness and surprisal measures. We extract sentences from the Hindi-Urdu Treebank corpus (HUTB) that contain clearly-defined subjects and objects, systematically permute the preverbal constituents of those sentences, and deploy a classifier to distinguish between original corpus sentences and artificially generated alternatives. The classifier leverages various discourse-based and cognitive features, including dependency length, surprisal, and information status, to inform its predictions. Our results suggest that, although there exists a preference for minimizing dependency length in non-canonical corpus sentences amidst the generated variants, this factor does not significantly contribute in identifying corpus sentences above and beyond surprisal and givenness measures. Notably, discourse predictability emerges as the primary determinant of constituent-order preferences. These findings are further supported by human evaluations involving 44 native Hindi speakers. Overall, this work sheds light on the role of expectation adaptation in word-ordering decisions. We conclude by situating our results within the theories of discourse production and information locality.","sentences":["Previous work has shown that isolated non-canonical sentences with Object-before-Subject (OSV) order are initially harder to process than their canonical counterparts with Subject-before-Object (SOV) order.","Although this difficulty diminishes with appropriate discourse context, the underlying cognitive factors responsible for alleviating processing challenges in OSV sentences remain a question.","In this work, we test the hypothesis that dependency length minimization is a significant predictor of non-canonical (OSV) syntactic choices, especially when controlling for information status such as givenness and surprisal measures.","We extract sentences from the Hindi-Urdu Treebank corpus (HUTB) that contain clearly-defined subjects and objects, systematically permute the preverbal constituents of those sentences, and deploy a classifier to distinguish between original corpus sentences and artificially generated alternatives.","The classifier leverages various discourse-based and cognitive features, including dependency length, surprisal, and information status, to inform its predictions.","Our results suggest that, although there exists a preference for minimizing dependency length in non-canonical corpus sentences amidst the generated variants, this factor does not significantly contribute in identifying corpus sentences above and beyond surprisal and givenness measures.","Notably, discourse predictability emerges as the primary determinant of constituent-order preferences.","These findings are further supported by human evaluations involving 44 native Hindi speakers.","Overall, this work sheds light on the role of expectation adaptation in word-ordering decisions.","We conclude by situating our results within the theories of discourse production and information locality."],"url":"http://arxiv.org/abs/2405.07730v1","category":"cs.CL"}
{"created":"2024-05-13 12:57:03","title":"Valence Quark PDFs of the Proton from Two-Current Correlations in Lattice QCD","abstract":"Following previous works on that topic, we consider Euclidean hadronic matrix elements in position space of two spatially separated local currents on the lattice, in order to extract the $x$-dependence of parton distribution functions (PDFs). The corresponding approach is often referred to by the term lattice cross section (LCS). In this work we will consider valence quark PDFs of an unpolarized proton. We adapt the previously established formalism to our choice of operators. The calculation of the two-current matrix elements requires the evaluation of four-point functions. The corresponding calculation is carried out on a $n_f = 2+1$ gauge ensemble with lattice spacing $a = 0.0856~\\mathrm{fm}$ and pseudoscalar masses $m_\\pi = 355~\\mathrm{MeV}$, $m_K = 441~\\mathrm{MeV}$. The four-point functions have been evaluated in a previous project. The lattice data is converted to the $\\overline{\\mathrm{MS}}$-scheme at a scale $\\mu=2~\\mathrm{GeV}$ and improved w.r.t. lattice artifacts. We use a common model as fit ansatz for the lattice data in order to extract the PDFs.","sentences":["Following previous works on that topic, we consider Euclidean hadronic matrix elements in position space of two spatially separated local currents on the lattice, in order to extract the $x$-dependence of parton distribution functions (PDFs).","The corresponding approach is often referred to by the term lattice cross section (LCS).","In this work we will consider valence quark PDFs of an unpolarized proton.","We adapt the previously established formalism to our choice of operators.","The calculation of the two-current matrix elements requires the evaluation of four-point functions.","The corresponding calculation is carried out on a $n_f = 2+1$ gauge ensemble with lattice spacing $a = 0.0856~\\mathrm{fm}$ and pseudoscalar masses $m_\\pi = 355~\\mathrm{MeV}$, $m_K = 441~\\mathrm{MeV}$.","The four-point functions have been evaluated in a previous project.","The lattice data is converted to the $\\overline{\\mathrm{MS}}$-scheme at a scale $\\mu=2~\\mathrm{GeV}$ and improved w.r.t. lattice artifacts.","We use a common model as fit ansatz for the lattice data in order to extract the PDFs."],"url":"http://arxiv.org/abs/2405.07712v1","category":"hep-lat"}
{"created":"2024-05-13 12:52:58","title":"Secure Aggregation Meets Sparsification in Decentralized Learning","abstract":"Decentralized learning (DL) faces increased vulnerability to privacy breaches due to sophisticated attacks on machine learning (ML) models. Secure aggregation is a computationally efficient cryptographic technique that enables multiple parties to compute an aggregate of their private data while keeping their individual inputs concealed from each other and from any central aggregator. To enhance communication efficiency in DL, sparsification techniques are used, selectively sharing only the most crucial parameters or gradients in a model, thereby maintaining efficiency without notably compromising accuracy. However, applying secure aggregation to sparsified models in DL is challenging due to the transmission of disjoint parameter sets by distinct nodes, which can prevent masks from canceling out effectively. This paper introduces CESAR, a novel secure aggregation protocol for DL designed to be compatible with existing sparsification mechanisms. CESAR provably defends against honest-but-curious adversaries and can be formally adapted to counteract collusion between them. We provide a foundational understanding of the interaction between the sparsification carried out by the nodes and the proportion of the parameters shared under CESAR in both colluding and non-colluding environments, offering analytical insight into the working and applicability of the protocol. Experiments on a network with 48 nodes in a 3-regular topology show that with random subsampling, CESAR is always within 0.5% accuracy of decentralized parallel stochastic gradient descent (D-PSGD), while adding only 11% of data overhead. Moreover, it surpasses the accuracy on TopK by up to 0.3% on independent and identically distributed (IID) data.","sentences":["Decentralized learning (DL) faces increased vulnerability to privacy breaches due to sophisticated attacks on machine learning (ML) models.","Secure aggregation is a computationally efficient cryptographic technique that enables multiple parties to compute an aggregate of their private data while keeping their individual inputs concealed from each other and from any central aggregator.","To enhance communication efficiency in DL, sparsification techniques are used, selectively sharing only the most crucial parameters or gradients in a model, thereby maintaining efficiency without notably compromising accuracy.","However, applying secure aggregation to sparsified models in DL is challenging due to the transmission of disjoint parameter sets by distinct nodes, which can prevent masks from canceling out effectively.","This paper introduces CESAR, a novel secure aggregation protocol for DL designed to be compatible with existing sparsification mechanisms.","CESAR provably defends against honest-but-curious adversaries and can be formally adapted to counteract collusion between them.","We provide a foundational understanding of the interaction between the sparsification carried out by the nodes and the proportion of the parameters shared under CESAR in both colluding and non-colluding environments, offering analytical insight into the working and applicability of the protocol.","Experiments on a network with 48 nodes in a 3-regular topology show that with random subsampling, CESAR is always within 0.5% accuracy of decentralized parallel stochastic gradient descent (D-PSGD), while adding only 11% of data overhead.","Moreover, it surpasses the accuracy on TopK by up to 0.3% on independent and identically distributed (IID) data."],"url":"http://arxiv.org/abs/2405.07708v2","category":"cs.LG"}
{"created":"2024-05-13 12:32:45","title":"MonoMAE: Enhancing Monocular 3D Detection through Depth-Aware Masked Autoencoders","abstract":"Monocular 3D object detection aims for precise 3D localization and identification of objects from a single-view image. Despite its recent progress, it often struggles while handling pervasive object occlusions that tend to complicate and degrade the prediction of object dimensions, depths, and orientations. We design MonoMAE, a monocular 3D detector inspired by Masked Autoencoders that addresses the object occlusion issue by masking and reconstructing objects in the feature space. MonoMAE consists of two novel designs. The first is depth-aware masking that selectively masks certain parts of non-occluded object queries in the feature space for simulating occluded object queries for network training. It masks non-occluded object queries by balancing the masked and preserved query portions adaptively according to the depth information. The second is lightweight query completion that works with the depth-aware masking to learn to reconstruct and complete the masked object queries. With the proposed object occlusion and completion, MonoMAE learns enriched 3D representations that achieve superior monocular 3D detection performance qualitatively and quantitatively for both occluded and non-occluded objects. Additionally, MonoMAE learns generalizable representations that can work well in new domains.","sentences":["Monocular 3D object detection aims for precise 3D localization and identification of objects from a single-view image.","Despite its recent progress, it often struggles while handling pervasive object occlusions that tend to complicate and degrade the prediction of object dimensions, depths, and orientations.","We design MonoMAE, a monocular 3D detector inspired by Masked Autoencoders that addresses the object occlusion issue by masking and reconstructing objects in the feature space.","MonoMAE consists of two novel designs.","The first is depth-aware masking that selectively masks certain parts of non-occluded object queries in the feature space for simulating occluded object queries for network training.","It masks non-occluded object queries by balancing the masked and preserved query portions adaptively according to the depth information.","The second is lightweight query completion that works with the depth-aware masking to learn to reconstruct and complete the masked object queries.","With the proposed object occlusion and completion, MonoMAE learns enriched 3D representations that achieve superior monocular 3D detection performance qualitatively and quantitatively for both occluded and non-occluded objects.","Additionally, MonoMAE learns generalizable representations that can work well in new domains."],"url":"http://arxiv.org/abs/2405.07696v1","category":"cs.CV"}
{"created":"2024-05-13 12:30:09","title":"High-energy neutrinos from late-time jets of gamma-ray bursts with cocoon photons","abstract":"In gamma-ray bursts (GRBs), $\\sim$ 100 - 1000 s after the prompt emission, afterglow observations have consistently shown X-ray excesses detected in the form of flares (XFs; in long GRBs) or extended emission (EEs; in short GRBs). These observations are interpreted as emissions from jets launched by late central engine activity. However, the characteristics of these late-time jets, particularly the dissipation radius ($r_{\\rm diss}$), Lorentz factor ($\\Gamma$), and cosmic-ray loading factor ($\\xi_p$), remain unknown despite their importance. Here, in order to understand the properties of the late-time jets with future multi-messenger observations, we estimate the detectability of neutrinos associated with late-time emissions for a wide range of $r_{\\rm diss}$ and $\\Gamma$, assuming $\\xi_p=10$. We take into account external seed photons from the cocoon around the jets, which can enhance the neutrino production through photohadronic interaction in the jet dissipation region. Our results are still consistent with the upper limit obtained by IceCube. Our calculations indicate a promising prospect for neutrino detection with IceCube-Gen2 through the stacking of $\\sim 1000-2000$ events, for a wide range of $r_{\\rm diss}$ and $\\Gamma$. We found that setting an optimal energy threshold of 10 TeV can significantly reduce noise without negatively affecting neutrino detection. Furthermore, even in the case of non-detection, we show that meaningful constraints on the characteristics of the late-time jets can be obtained.","sentences":["In gamma-ray bursts (GRBs), $\\sim$ 100 - 1000 s after the prompt emission, afterglow observations have consistently shown X-ray excesses detected in the form of flares (XFs; in long GRBs) or extended emission (EEs; in short GRBs).","These observations are interpreted as emissions from jets launched by late central engine activity.","However, the characteristics of these late-time jets, particularly the dissipation radius ($r_{\\rm diss}$), Lorentz factor ($\\Gamma$), and cosmic-ray loading factor ($\\xi_p$), remain unknown despite their importance.","Here, in order to understand the properties of the late-time jets with future multi-messenger observations, we estimate the detectability of neutrinos associated with late-time emissions for a wide range of $r_{\\rm diss}$ and $\\Gamma$, assuming $\\xi_p=10$. We take into account external seed photons from the cocoon around the jets, which can enhance the neutrino production through photohadronic interaction in the jet dissipation region.","Our results are still consistent with the upper limit obtained by IceCube.","Our calculations indicate a promising prospect for neutrino detection with IceCube-Gen2 through the stacking of $\\sim 1000-2000$ events, for a wide range of $r_{\\rm diss}$ and $\\Gamma$. We found that setting an optimal energy threshold of 10 TeV can significantly reduce noise without negatively affecting neutrino detection.","Furthermore, even in the case of non-detection, we show that meaningful constraints on the characteristics of the late-time jets can be obtained."],"url":"http://arxiv.org/abs/2405.07695v1","category":"astro-ph.HE"}
{"created":"2024-05-13 12:23:13","title":"Quality of Experience Optimization for Real-time XR Video Transmission with Energy Constraints","abstract":"Extended Reality (XR) is an important service in the 5G network and in future 6G networks. In contrast to traditional video on demand services, real-time XR video is transmitted frame-by-frame, requiring low latency and being highly sensitive to network fluctuations. In this paper, we model the quality of experience (QoE) for real-time XR video transmission on a frame-by-frame basis. Based on the proposed QoE model, we formulate an optimization problem that maximizes QoE with constraints on wireless resources and long-term energy consumption. We utilize Lyapunov optimization to transform the original problem into a single-frame optimization problem and then allocate wireless subchannels. We propose an adaptive XR video bitrate algorithm that employs a Long Short Term Memory (LSTM) based Deep Q-Network (DQN) algorithm for video bitrate selection. Through numerical results, we show that our proposed algorithm outperforms the baseline algorithms, with the average QoE improvements of 0.04 to 0.46. Specifically, compared to baseline algorithms, the proposed algorithm reduces average video quality variations by 29% to 50% and improves the frame transmission success rate by 5% to 48%.","sentences":["Extended Reality (XR) is an important service in the 5G network and in future 6G networks.","In contrast to traditional video on demand services, real-time XR video is transmitted frame-by-frame, requiring low latency and being highly sensitive to network fluctuations.","In this paper, we model the quality of experience (QoE) for real-time XR video transmission on a frame-by-frame basis.","Based on the proposed QoE model, we formulate an optimization problem that maximizes QoE with constraints on wireless resources and long-term energy consumption.","We utilize Lyapunov optimization to transform the original problem into a single-frame optimization problem and then allocate wireless subchannels.","We propose an adaptive XR video bitrate algorithm that employs a Long Short Term Memory (LSTM) based Deep Q-Network (DQN) algorithm for video bitrate selection.","Through numerical results, we show that our proposed algorithm outperforms the baseline algorithms, with the average QoE improvements of 0.04 to 0.46.","Specifically, compared to baseline algorithms, the proposed algorithm reduces average video quality variations by 29% to 50% and improves the frame transmission success rate by 5% to 48%."],"url":"http://arxiv.org/abs/2405.07689v1","category":"cs.MM"}
{"created":"2024-05-13 11:13:17","title":"CDFormer:When Degradation Prediction Embraces Diffusion Model for Blind Image Super-Resolution","abstract":"Existing Blind image Super-Resolution (BSR) methods focus on estimating either kernel or degradation information, but have long overlooked the essential content details. In this paper, we propose a novel BSR approach, Content-aware Degradation-driven Transformer (CDFormer), to capture both degradation and content representations. However, low-resolution images cannot provide enough content details, and thus we introduce a diffusion-based module $CDFormer_{diff}$ to first learn Content Degradation Prior (CDP) in both low- and high-resolution images, and then approximate the real distribution given only low-resolution information. Moreover, we apply an adaptive SR network $CDFormer_{SR}$ that effectively utilizes CDP to refine features. Compared to previous diffusion-based SR methods, we treat the diffusion model as an estimator that can overcome the limitations of expensive sampling time and excessive diversity. Experiments show that CDFormer can outperform existing methods, establishing a new state-of-the-art performance on various benchmarks under blind settings. Codes and models will be available at \\href{https://github.com/I2-Multimedia-Lab/CDFormer}{https://github.com/I2-Multimedia-Lab/CDFormer}.","sentences":["Existing Blind image Super-Resolution (BSR) methods focus on estimating either kernel or degradation information, but have long overlooked the essential content details.","In this paper, we propose a novel BSR approach, Content-aware Degradation-driven Transformer (CDFormer), to capture both degradation and content representations.","However, low-resolution images cannot provide enough content details, and thus we introduce a diffusion-based module $CDFormer_{diff}$ to first learn Content Degradation","Prior (CDP) in both low- and high-resolution images, and then approximate the real distribution given only low-resolution information.","Moreover, we apply an adaptive SR network $CDFormer_{SR}$ that effectively utilizes CDP to refine features.","Compared to previous diffusion-based SR methods, we treat the diffusion model as an estimator that can overcome the limitations of expensive sampling time and excessive diversity.","Experiments show that CDFormer can outperform existing methods, establishing a new state-of-the-art performance on various benchmarks under blind settings.","Codes and models will be available at \\href{https://github.com/I2-Multimedia-Lab/CDFormer}{https://github.com/I2-Multimedia-Lab/CDFormer}."],"url":"http://arxiv.org/abs/2405.07648v1","category":"cs.CV"}
{"created":"2024-05-13 10:27:11","title":"Towards Adaptive IMFs -- Generalization of utility functions in Multi-Agent Frameworks","abstract":"Intent Management Function (IMF) is an integral part of future-generation networks. In recent years, there has been some work on AI-based IMFs that can handle conflicting intents and prioritize the global objective based on apriori definition of the utility function and accorded priorities for competing intents. Some of the earlier works use Multi-Agent Reinforcement Learning (MARL) techniques with AdHoc Teaming (AHT) approaches for efficient conflict handling in IMF. However, the success of such frameworks in real-life scenarios requires them to be flexible to business situations. The intent priorities can change and the utility function, which measures the extent of intent fulfilment, may also vary in definition. This paper proposes a novel mechanism whereby the IMF can generalize to different forms of utility functions and change of intent priorities at run-time without additional training. Such generalization ability, without additional training requirements, would help to deploy IMF in live networks where customer intents and priorities change frequently. Results on the network emulator demonstrate the efficacy of the approach, scalability for new intents, outperforming existing techniques that require additional training to achieve the same degree of flexibility thereby saving cost, and increasing efficiency and adaptability.","sentences":["Intent Management Function (IMF) is an integral part of future-generation networks.","In recent years, there has been some work on AI-based IMFs that can handle conflicting intents and prioritize the global objective based on apriori definition of the utility function and accorded priorities for competing intents.","Some of the earlier works use Multi-Agent Reinforcement Learning (MARL) techniques with AdHoc Teaming (AHT) approaches for efficient conflict handling in IMF.","However, the success of such frameworks in real-life scenarios requires them to be flexible to business situations.","The intent priorities can change and the utility function, which measures the extent of intent fulfilment, may also vary in definition.","This paper proposes a novel mechanism whereby the IMF can generalize to different forms of utility functions and change of intent priorities at run-time without additional training.","Such generalization ability, without additional training requirements, would help to deploy IMF in live networks where customer intents and priorities change frequently.","Results on the network emulator demonstrate the efficacy of the approach, scalability for new intents, outperforming existing techniques that require additional training to achieve the same degree of flexibility thereby saving cost, and increasing efficiency and adaptability."],"url":"http://arxiv.org/abs/2405.07621v2","category":"cs.LG"}
{"created":"2024-05-13 10:26:53","title":"New Low-Dissipation Central-Upwind Schemes. Part II","abstract":"The low-dissipation central-upwind (LDCU) schemes have been recently introduced in [A. Kurganov and R. Xin, J. Sci. Comput., 96 (2023), Paper No. 56] as a modification of the central-upwind (CU) schemes from [{\\sc A. Kurganov and C. T. Lin, Commun. Comput. Phys., 2 (2007), pp. 141-163}]. The LDCU schemes achieve much higher resolution of contact waves and many (two-dimensional) structures resulting from complicated wave interaction. However, the LDCU schemes sometimes produce more oscillatory results compared with the CU schemes, especially near the computational domain boundaries.   In this paper, we propose a very simple -- yet systematic -- modification of the LDCU schemes, which completely eliminates the aforementioned oscillations almost without affecting the quality of the computed solution.","sentences":["The low-dissipation central-upwind (LDCU) schemes have been recently introduced in [A. Kurganov and R. Xin, J. Sci.","Comput., 96 (2023), Paper No. 56] as a modification of the central-upwind (CU) schemes from [{\\sc A. Kurganov and C. T. Lin, Commun.","Comput.","Phys., 2 (2007), pp. 141-163}].","The LDCU schemes achieve much higher resolution of contact waves and many (two-dimensional) structures resulting from complicated wave interaction.","However, the LDCU schemes sometimes produce more oscillatory results compared with the CU schemes, especially near the computational domain boundaries.   ","In this paper, we propose a very simple -- yet systematic -- modification of the LDCU schemes, which completely eliminates the aforementioned oscillations almost without affecting the quality of the computed solution."],"url":"http://arxiv.org/abs/2405.07620v1","category":"math.NA"}
{"created":"2024-05-13 10:06:25","title":"Robustness of Interferometric Power to Sudden Death","abstract":"We study the dissipative dynamics of interferometric power as a discordlike measure in Markovian environments, such as dephasing, depolarizing, and generalized amplitude damping. Moreover, we compare the dynamics of interferometric power and entanglement by choosing proper initial conditions. Our study shows that in all cases where the sudden death of entanglement appears, interferometric power decays asymptotically. Therefore, quantum metrology based on interferometric power is more robust than entanglement.","sentences":["We study the dissipative dynamics of interferometric power as a discordlike measure in Markovian environments, such as dephasing, depolarizing, and generalized amplitude damping.","Moreover, we compare the dynamics of interferometric power and entanglement by choosing proper initial conditions.","Our study shows that in all cases where the sudden death of entanglement appears, interferometric power decays asymptotically.","Therefore, quantum metrology based on interferometric power is more robust than entanglement."],"url":"http://arxiv.org/abs/2405.07602v1","category":"quant-ph"}
{"created":"2024-05-13 10:03:34","title":"On-device Online Learning and Semantic Management of TinyML Systems","abstract":"Recent advances in Tiny Machine Learning (TinyML) empower low-footprint embedded devices for real-time on-device Machine Learning. While many acknowledge the potential benefits of TinyML, its practical implementation presents unique challenges. This study aims to bridge the gap between prototyping single TinyML models and developing reliable TinyML systems in production: (1) Embedded devices operate in dynamically changing conditions. Existing TinyML solutions primarily focus on inference, with models trained offline on powerful machines and deployed as static objects. However, static models may underperform in the real world due to evolving input data distributions. We propose online learning to enable training on constrained devices, adapting local models towards the latest field conditions. (2) Nevertheless, current on-device learning methods struggle with heterogeneous deployment conditions and the scarcity of labeled data when applied across numerous devices. We introduce federated meta-learning incorporating online learning to enhance model generalization, facilitating rapid learning. This approach ensures optimal performance among distributed devices by knowledge sharing. (3) Moreover, TinyML's pivotal advantage is widespread adoption. Embedded devices and TinyML models prioritize extreme efficiency, leading to diverse characteristics ranging from memory and sensors to model architectures. Given their diversity and non-standardized representations, managing these resources becomes challenging as TinyML systems scale up. We present semantic management for the joint management of models and devices at scale. We demonstrate our methods through a basic regression example and then assess them in three real-world TinyML applications: handwritten character image classification, keyword audio classification, and smart building presence detection, confirming our approaches' effectiveness.","sentences":["Recent advances in Tiny Machine Learning (TinyML) empower low-footprint embedded devices for real-time on-device Machine Learning.","While many acknowledge the potential benefits of TinyML, its practical implementation presents unique challenges.","This study aims to bridge the gap between prototyping single TinyML models and developing reliable TinyML systems in production: (1) Embedded devices operate in dynamically changing conditions.","Existing TinyML solutions primarily focus on inference, with models trained offline on powerful machines and deployed as static objects.","However, static models may underperform in the real world due to evolving input data distributions.","We propose online learning to enable training on constrained devices, adapting local models towards the latest field conditions.","(2) Nevertheless, current on-device learning methods struggle with heterogeneous deployment conditions and the scarcity of labeled data when applied across numerous devices.","We introduce federated meta-learning incorporating online learning to enhance model generalization, facilitating rapid learning.","This approach ensures optimal performance among distributed devices by knowledge sharing.","(3) Moreover, TinyML's pivotal advantage is widespread adoption.","Embedded devices and TinyML models prioritize extreme efficiency, leading to diverse characteristics ranging from memory and sensors to model architectures.","Given their diversity and non-standardized representations, managing these resources becomes challenging as TinyML systems scale up.","We present semantic management for the joint management of models and devices at scale.","We demonstrate our methods through a basic regression example and then assess them in three real-world TinyML applications: handwritten character image classification, keyword audio classification, and smart building presence detection, confirming our approaches' effectiveness."],"url":"http://arxiv.org/abs/2405.07601v1","category":"cs.LG"}
{"created":"2024-05-13 09:56:28","title":"RGBD-Glue: General Feature Combination for Robust RGB-D Point Cloud Registration","abstract":"Point cloud registration is a fundamental task for estimating rigid transformations between point clouds. Previous studies have used geometric information for extracting features, matching and estimating transformation. Recently, owing to the advancement of RGB-D sensors, researchers have attempted to utilize visual information to improve registration performance. However, these studies focused on extracting distinctive features by deep feature fusion, which cannot effectively solve the negative effects of each feature's weakness, and cannot sufficiently leverage the valid information. In this paper, we propose a new feature combination framework, which applies a looser but more effective fusion and can achieve better performance. An explicit filter based on transformation consistency is designed for the combination framework, which can overcome each feature's weakness. And an adaptive threshold determined by the error distribution is proposed to extract more valid information from the two types of features. Owing to the distinctive design, our proposed framework can estimate more accurate correspondences and is applicable to both hand-crafted and learning-based feature descriptors. Experiments on ScanNet show that our method achieves a state-of-the-art performance and the rotation accuracy of 99.1%.","sentences":["Point cloud registration is a fundamental task for estimating rigid transformations between point clouds.","Previous studies have used geometric information for extracting features, matching and estimating transformation.","Recently, owing to the advancement of RGB-D sensors, researchers have attempted to utilize visual information to improve registration performance.","However, these studies focused on extracting distinctive features by deep feature fusion, which cannot effectively solve the negative effects of each feature's weakness, and cannot sufficiently leverage the valid information.","In this paper, we propose a new feature combination framework, which applies a looser but more effective fusion and can achieve better performance.","An explicit filter based on transformation consistency is designed for the combination framework, which can overcome each feature's weakness.","And an adaptive threshold determined by the error distribution is proposed to extract more valid information from the two types of features.","Owing to the distinctive design, our proposed framework can estimate more accurate correspondences and is applicable to both hand-crafted and learning-based feature descriptors.","Experiments on ScanNet show that our method achieves a state-of-the-art performance and the rotation accuracy of 99.1%."],"url":"http://arxiv.org/abs/2405.07594v1","category":"cs.CV"}
{"created":"2024-05-13 08:36:06","title":"Safety-Aware Human-Lead Vehicle Platooning by Proactively Reacting to Uncertain Human Behaving","abstract":"Human-Lead Cooperative Adaptive Cruise Control (HL-CACC) is regarded as a promising vehicle platooning technology in real-world implementation. By utilizing a Human-driven Vehicle (HV) as the platoon leader, HL-CACC reduces the cost and enhances the reliability of perception and decision-making. However, state-of-the-art HL-CACC technology still has a great limitation on driving safety for the lack of considering the leading human driver's uncertain behaving. In this study, a HL-CACC controller is designed based on Stochastic Model Predictive Control (SMPC). It is enabled to predict the driving intention of the leading Connected Human-Driven Vehicle (CHV). The proposed controller has the following features: i) enhanced perceived safety in oscillating traffic; ii) guaranteed safety against hard brakes; iii) computational efficient for real-time implementation. The proposed controller is evaluated on a PreScan&Simulink simulation platform. Real vehicle trajectory data is collected for the calibration of simulation. Results reveal that the proposed controller: i) improves perceived safety by 19.17% in oscillating traffic; ii) enhances actual safety by 7.76% against hard brake; iii) is confirmed with string stability. The computation time is approximately 3 milliseconds when running on a laptop equipped with an Intel i5-13500H CPU. This indicates the proposed controller is ready for real-time implementation.","sentences":["Human-Lead Cooperative Adaptive Cruise Control (HL-CACC) is regarded as a promising vehicle platooning technology in real-world implementation.","By utilizing a Human-driven Vehicle (HV) as the platoon leader, HL-CACC reduces the cost and enhances the reliability of perception and decision-making.","However, state-of-the-art HL-CACC technology still has a great limitation on driving safety for the lack of considering the leading human driver's uncertain behaving.","In this study, a HL-CACC controller is designed based on Stochastic Model Predictive Control (SMPC).","It is enabled to predict the driving intention of the leading Connected Human-Driven Vehicle (CHV).","The proposed controller has the following features: i) enhanced perceived safety in oscillating traffic; ii) guaranteed safety against hard brakes; iii) computational efficient for real-time implementation.","The proposed controller is evaluated on a PreScan&Simulink simulation platform.","Real vehicle trajectory data is collected for the calibration of simulation.","Results reveal that the proposed controller: i) improves perceived safety by 19.17% in oscillating traffic; ii) enhances actual safety by 7.76% against hard brake; iii) is confirmed with string stability.","The computation time is approximately 3 milliseconds when running on a laptop equipped with an Intel i5-13500H CPU.","This indicates the proposed controller is ready for real-time implementation."],"url":"http://arxiv.org/abs/2405.07556v1","category":"cs.RO"}
{"created":"2024-05-13 08:33:00","title":"Space Domain based Ecological Cooperative and Adaptive Cruise Control on Rolling Terrain","abstract":"Ecological Cooperative and Adaptive Cruise Control (Eco-CACC) is widely focused to enhance sustainability of CACC. However, state-of-the-art Eco-CACC studies are still facing challenges in adopting on rolling terrain. Furthermore, they cannot ensure both ecology optimality and computational efficiency. Hence, this paper proposes a nonlinear optimal control based Eco-CACC controller. It has the following features: i) enhancing performance across rolling terrains by modeling in space domain; ii) enhancing fuel efficiency via globally optimizing all vehicle's fuel consumptions; iii) ensuring computational efficiency by developing a differential dynamic programming-based solving method for the non-linear optimal control problem; iv) ensuring string stability through theoretically proving and experimentally validating. The performance of the proposed Eco-CACC controller was evaluated. Results showed that the proposed Eco-CACC controller can improve average fuel saving by 37.67% at collector road and about 17.30% at major arterial.","sentences":["Ecological Cooperative and Adaptive Cruise Control (Eco-CACC) is widely focused to enhance sustainability of CACC.","However, state-of-the-art Eco-CACC studies are still facing challenges in adopting on rolling terrain.","Furthermore, they cannot ensure both ecology optimality and computational efficiency.","Hence, this paper proposes a nonlinear optimal control based Eco-CACC controller.","It has the following features: i) enhancing performance across rolling terrains by modeling in space domain; ii) enhancing fuel efficiency via globally optimizing all vehicle's fuel consumptions; iii) ensuring computational efficiency by developing a differential dynamic programming-based solving method for the non-linear optimal control problem; iv) ensuring string stability through theoretically proving and experimentally validating.","The performance of the proposed Eco-CACC controller was evaluated.","Results showed that the proposed Eco-CACC controller can improve average fuel saving by 37.67% at collector road and about 17.30% at major arterial."],"url":"http://arxiv.org/abs/2405.07553v1","category":"cs.RO"}
{"created":"2024-05-13 08:19:27","title":"Intrinsic Langevin dynamics of rigid inclusions on curved surfaces","abstract":"The stochastic dynamics of a rigid inclusion constrained to move on a curved surface has many applications in biological and soft matter physics, ranging from the diffusion of passive or active membrane proteins to the motion of phoretic particles on liquid-liquid interfaces. Here we construct intrinsic Langevin equations for an oriented rigid inclusion on a curved surface using Cartan's method of moving frames. We first derive the Hamiltonian equations of motion for the translational and rotational momenta in the body frame. Surprisingly, surface curvature couples the linear and angular momenta of the inclusion. We then add to the Hamiltonian equations linear friction, white noise and arbitrary configuration-dependent forces and torques to obtain intrinsic Langevin equations of motion in phase space. We provide the integrability conditions, made non-trivial by surface curvature, for the forces and torques to admit a potential, thus distinguishing between passive and active stochastic motion. We derive the corresponding Fokker-Planck equation in geometric form and obtain fluctuation-dissipation relations that ensure Gibbsian equilibrium. We extract the overdamped equations of motion by adiabatically eliminating the momenta from the Fokker-Planck equation, showing how a peculiar cancellation leads to the naively expected Smoluchowski limit. The overdamped equations can be used for accurate and efficient intrinsic Brownian dynamics simulations of passive, driven and active diffusion processes on curved surfaces. Our work generalises to the collective dynamics of many inclusions on curved surfaces.","sentences":["The stochastic dynamics of a rigid inclusion constrained to move on a curved surface has many applications in biological and soft matter physics, ranging from the diffusion of passive or active membrane proteins to the motion of phoretic particles on liquid-liquid interfaces.","Here we construct intrinsic Langevin equations for an oriented rigid inclusion on a curved surface using Cartan's method of moving frames.","We first derive the Hamiltonian equations of motion for the translational and rotational momenta in the body frame.","Surprisingly, surface curvature couples the linear and angular momenta of the inclusion.","We then add to the Hamiltonian equations linear friction, white noise and arbitrary configuration-dependent forces and torques to obtain intrinsic Langevin equations of motion in phase space.","We provide the integrability conditions, made non-trivial by surface curvature, for the forces and torques to admit a potential, thus distinguishing between passive and active stochastic motion.","We derive the corresponding Fokker-Planck equation in geometric form and obtain fluctuation-dissipation relations that ensure Gibbsian equilibrium.","We extract the overdamped equations of motion by adiabatically eliminating the momenta from the Fokker-Planck equation, showing how a peculiar cancellation leads to the naively expected Smoluchowski limit.","The overdamped equations can be used for accurate and efficient intrinsic Brownian dynamics simulations of passive, driven and active diffusion processes on curved surfaces.","Our work generalises to the collective dynamics of many inclusions on curved surfaces."],"url":"http://arxiv.org/abs/2405.07539v1","category":"cond-mat.soft"}
{"created":"2024-05-15 17:43:13","title":"Formal self-adjointness of a family of conformally invariant bidifferential operators","abstract":"We prove that the curved Ovsienko--Redou operators and a related family of differential operators are formally self-adjoint. This verifies two conjectures of Case, Lin, and Yuan.","sentences":["We prove that the curved Ovsienko--Redou operators and a related family of differential operators are formally self-adjoint.","This verifies two conjectures of Case, Lin, and Yuan."],"url":"http://arxiv.org/abs/2405.09532v1","category":"math.DG"}
{"created":"2024-05-15 16:46:15","title":"A Comparison of Electronic, Dielectric, and Thermoelectric Properties of Monolayer of HfX2N4(X = Si, Ge) through First-Principles Calculations","abstract":"The newly emerged two-dimensional (2D) materials family of MSi2N4, where M is a transition metal atom (i.e., Mo, W, etc.), has the potential to be named after the conventional and very popular transition metal di-chalcogenides (TMDC), which got their reputation for having bandgap tunability and high mobility. The HfSi2N4 and HfGe2N4 2D materials are members of the MSi2N4 family and possess very good figure of merit (ZT) and have high mobility, proving their suitability for thermoelectric applications. The HfSi2N4 and HfGe2N4 showed considerable ZT of 0.90 and 0.89, respectively, for p-type and 0.83 and 0.79 for n-type, at 900 K along with high mobility according to the solutions obtained after solving the Boltzmann Transport Equation (BTE). The HfGe2N4 also showed a ZT of 0.84 at 600 K and 0.68 at 300 K, which is also excellent for low-temperature operation. The bandgaps (BG) obtained for HfSi2N4 and HfGe2N4 according to the Heyd-Scuseria-Ernzerhof (HSE) approximation were 2.89 eV and 2.75 eV. The first absorption peak showed in the blue region of the visible spectrum; from this, their usefulness in visible range photodetectors can also be inferred.","sentences":["The newly emerged two-dimensional (2D) materials family of MSi2N4, where M is a transition metal atom (i.e., Mo, W, etc.), has the potential to be named after the conventional and very popular transition metal di-chalcogenides (TMDC), which got their reputation for having bandgap tunability and high mobility.","The HfSi2N4 and HfGe2N4 2D materials are members of the MSi2N4 family and possess very good figure of merit (ZT) and have high mobility, proving their suitability for thermoelectric applications.","The HfSi2N4 and HfGe2N4 showed considerable ZT of 0.90 and 0.89, respectively, for p-type and 0.83 and 0.79 for n-type, at 900 K along with high mobility according to the solutions obtained after solving the Boltzmann Transport Equation (BTE).","The HfGe2N4 also showed a ZT of 0.84 at 600 K and 0.68 at 300 K, which is also excellent for low-temperature operation.","The bandgaps (BG) obtained for HfSi2N4 and HfGe2N4 according to the Heyd-Scuseria-Ernzerhof (HSE) approximation were 2.89 eV and 2.75 eV. The first absorption peak showed in the blue region of the visible spectrum; from this, their usefulness in visible range photodetectors can also be inferred."],"url":"http://arxiv.org/abs/2405.09498v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-15 16:35:14","title":"Distributed Nonlinear Conic Optimisation with partially separable Structure","abstract":"In this paper we consider the problem of distributed nonlinear optimisation of a separable convex cost function over a graph subject to cone constraints. We show how to generalise, using convex analysis, monotone operator theory and fixed-point theory, the primal-dual method of multipliers (PDMM), originally designed for equality constraint optimisation and recently extended to include linear inequality constraints, to accommodate for cone constraints. The resulting algorithm can be used to implement a variety of optimisation problems, including the important class of semidefinite programs with partially separable structure, in a fully distributed fashion. We derive update equations by applying the Peaceman-Rachford splitting algorithm to the monotonic inclusion related to the lifted dual problem. The cone constraints are implemented by a reflection method in the lifted dual domain where auxiliary variables are reflected with respect to the intersection of the polar cone and a subspace relating the dual and lifted dual domain. Convergence results for both synchronous and stochastic update schemes are provided and an application of the proposed algorithm is demonstrated to implement an approximate algorithm for maximum cut problems based on semidefinite programming in a fully distributed fashion.","sentences":["In this paper we consider the problem of distributed nonlinear optimisation of a separable convex cost function over a graph subject to cone constraints.","We show how to generalise, using convex analysis, monotone operator theory and fixed-point theory, the primal-dual method of multipliers (PDMM), originally designed for equality constraint optimisation and recently extended to include linear inequality constraints, to accommodate for cone constraints.","The resulting algorithm can be used to implement a variety of optimisation problems, including the important class of semidefinite programs with partially separable structure, in a fully distributed fashion.","We derive update equations by applying the Peaceman-Rachford splitting algorithm to the monotonic inclusion related to the lifted dual problem.","The cone constraints are implemented by a reflection method in the lifted dual domain where auxiliary variables are reflected with respect to the intersection of the polar cone and a subspace relating the dual and lifted dual domain.","Convergence results for both synchronous and stochastic update schemes are provided and an application of the proposed algorithm is demonstrated to implement an approximate algorithm for maximum cut problems based on semidefinite programming in a fully distributed fashion."],"url":"http://arxiv.org/abs/2405.09490v1","category":"cs.DC"}
{"created":"2024-05-15 16:27:00","title":"Nonequilibrium phase transitions and absorbing states in a model for the dynamics of religious affiliation","abstract":"We propose a simple model to describe the dynamics of religious affiliation. For such purpose, we built a compartmental model with three distinct subpopulations, namely religious committed individuals, religious noncommitted individuals and not religious affiliated individuals. The transitions among the compartments are governed by probabilities, modeling social interactions among the groups and also spontaneous transitions among the compartments. First of all, we consider the model on a fully-connected network. Thus, we write a set of ordinary differential equations to study the evolution of the subpopulations. Our analytical and numerical results show that there is an absorbing state in the model where only one of the subpopulations survive in the long-time limit. There are also regions of parameters where some of the subpopulations coexist (two or three). We also verified the occurrence of two distinct critical points. In addition, we also present Monte Carlo simulations of the model on two-dimensional square lattices, in order to analyze the impact of the presence of a lattice structure on the critical behavior of the model. Comparison of the models' results with data for religious affiliation in Northern Ireland shows a good qualitative agreement. Finally, we considered the presence of inflexible individuals in the population, i.e., individuals that never change their states. The impact of such special agents on the critical behavior of the model is also discussed.","sentences":["We propose a simple model to describe the dynamics of religious affiliation.","For such purpose, we built a compartmental model with three distinct subpopulations, namely religious committed individuals, religious noncommitted individuals and not religious affiliated individuals.","The transitions among the compartments are governed by probabilities, modeling social interactions among the groups and also spontaneous transitions among the compartments.","First of all, we consider the model on a fully-connected network.","Thus, we write a set of ordinary differential equations to study the evolution of the subpopulations.","Our analytical and numerical results show that there is an absorbing state in the model where only one of the subpopulations survive in the long-time limit.","There are also regions of parameters where some of the subpopulations coexist (two or three).","We also verified the occurrence of two distinct critical points.","In addition, we also present Monte Carlo simulations of the model on two-dimensional square lattices, in order to analyze the impact of the presence of a lattice structure on the critical behavior of the model.","Comparison of the models' results with data for religious affiliation in Northern Ireland shows a good qualitative agreement.","Finally, we considered the presence of inflexible individuals in the population, i.e., individuals that never change their states.","The impact of such special agents on the critical behavior of the model is also discussed."],"url":"http://arxiv.org/abs/2405.09488v1","category":"physics.soc-ph"}
{"created":"2024-05-15 15:56:18","title":"Gaze-DETR: Using Expert Gaze to Reduce False Positives in Vulvovaginal Candidiasis Screening","abstract":"Accurate detection of vulvovaginal candidiasis is critical for women's health, yet its sparse distribution and visually ambiguous characteristics pose significant challenges for accurate identification by pathologists and neural networks alike. Our eye-tracking data reveals that areas garnering sustained attention - yet not marked by experts after deliberation - are often aligned with false positives of neural networks. Leveraging this finding, we introduce Gaze-DETR, a pioneering method that integrates gaze data to enhance neural network precision by diminishing false positives. Gaze-DETR incorporates a universal gaze-guided warm-up protocol applicable across various detection methods and a gaze-guided rectification strategy specifically designed for DETR-based models. Our comprehensive tests confirm that Gaze-DETR surpasses existing leading methods, showcasing remarkable improvements in detection accuracy and generalizability.","sentences":["Accurate detection of vulvovaginal candidiasis is critical for women's health, yet its sparse distribution and visually ambiguous characteristics pose significant challenges for accurate identification by pathologists and neural networks alike.","Our eye-tracking data reveals that areas garnering sustained attention - yet not marked by experts after deliberation - are often aligned with false positives of neural networks.","Leveraging this finding, we introduce Gaze-DETR, a pioneering method that integrates gaze data to enhance neural network precision by diminishing false positives.","Gaze-DETR incorporates a universal gaze-guided warm-up protocol applicable across various detection methods and a gaze-guided rectification strategy specifically designed for DETR-based models.","Our comprehensive tests confirm that Gaze-DETR surpasses existing leading methods, showcasing remarkable improvements in detection accuracy and generalizability."],"url":"http://arxiv.org/abs/2405.09463v1","category":"cs.CV"}
{"created":"2024-05-15 15:42:47","title":"Rotated reference frames in radiative transport theory","abstract":"Rotated reference frames offer fast algorithms for the radiative transport equation (RTE). We review the singular-eigenfunction approach and related numerical methods for the multi-dimensional RTE with rotated reference frames.","sentences":["Rotated reference frames offer fast algorithms for the radiative transport equation (RTE).","We review the singular-eigenfunction approach and related numerical methods for the multi-dimensional RTE with rotated reference frames."],"url":"http://arxiv.org/abs/2405.09447v1","category":"math.NA"}
{"created":"2024-05-15 15:07:40","title":"Three Dimensional Spatial Cognition: Bees and Bats","abstract":"The paper describes a program which computes the best possible Bayesian model of 3D space from vision (in bees) or echo location (in bats), at Marrs [1982] Level 2. The model exploits the strong Bayesian prior probability that most other things do not move, as the animal moves. 3D locations of things are computed from successive sightings or echoes, computing structure from the animals motion (SFM). The program can be downloaded and run. It also computes a tracking approximate model, which is more tractable for animal brains than the full Bayesian computation. The tracking model is nearly as good as the full Bayesian model, but only if spatial memory storage errors are small. Neural storage of spatial positions gives too high error levels, and is too slow. Alternatively, a 3D model of space could be stored in a wave excitation, as a Fourier transform of real space. This could give high memory capacity and precision, with low spatial distortion, fast response, and simpler computation. Evidence is summarized from related papers that a wave excitation holds spatial memory in the mammalian thalamus, and in the central body of the insect brain.","sentences":["The paper describes a program which computes the best possible Bayesian model of 3D space from vision (in bees) or echo location (in bats), at Marrs [1982] Level 2.","The model exploits the strong Bayesian prior probability that most other things do not move, as the animal moves.","3D locations of things are computed from successive sightings or echoes, computing structure from the animals motion (SFM).","The program can be downloaded and run.","It also computes a tracking approximate model, which is more tractable for animal brains than the full Bayesian computation.","The tracking model is nearly as good as the full Bayesian model, but only if spatial memory storage errors are small.","Neural storage of spatial positions gives too high error levels, and is too slow.","Alternatively, a 3D model of space could be stored in a wave excitation, as a Fourier transform of real space.","This could give high memory capacity and precision, with low spatial distortion, fast response, and simpler computation.","Evidence is summarized from related papers that a wave excitation holds spatial memory in the mammalian thalamus, and in the central body of the insect brain."],"url":"http://arxiv.org/abs/2405.09413v1","category":"q-bio.NC"}
{"created":"2024-05-15 14:50:04","title":"Coherent $\u03c0^0\u03b7d$ photoproduction at forward deuteron angles measured at BGOOD","abstract":"The coherent reaction, $\\gamma d \\rightarrow \\pi^0\\eta d$ was studied with the BGOOD experiment at ELSA from threshold to a centre-of-mass energy of 3200\\,MeV. A full kinematic reconstruction was made, with final state deuterons identified in the forward spectrometer and $\\pi^0$ and $\\eta$ decays in the central BGO Rugby Ball. The strength of the differential cross section exceeds what can be described by models of coherent photoproduction at forward angles by orders of magnitude. The distribution of the differential cross section has an excellent agreement with a model including quasi-free $\\Delta \\pi$ photoproduction, pion re-scattering and $N(1535)$ formation and subsequent nucleon coalescence to the deuteron. This also gives a reasonable description of the two-body invariant mass distributions and naturally explains the similar magnitudes of this channel and $\\pi^0\\pi^0 d$ coherent photoproduction.","sentences":["The coherent reaction, $\\gamma d \\rightarrow \\pi^0\\eta d$ was studied with the BGOOD experiment at ELSA from threshold to a centre-of-mass energy of 3200\\,MeV. A full kinematic reconstruction was made, with final state deuterons identified in the forward spectrometer and $\\pi^0$ and $\\eta$ decays in the central BGO Rugby Ball.","The strength of the differential cross section exceeds what can be described by models of coherent photoproduction at forward angles by orders of magnitude.","The distribution of the differential cross section has an excellent agreement with a model including quasi-free $\\Delta \\pi$ photoproduction, pion re-scattering and $N(1535)$ formation and subsequent nucleon coalescence to the deuteron.","This also gives a reasonable description of the two-body invariant mass distributions and naturally explains the similar magnitudes of this channel and $\\pi^0\\pi^0 d$ coherent photoproduction."],"url":"http://arxiv.org/abs/2405.09392v1","category":"nucl-ex"}
{"created":"2024-05-15 14:30:05","title":"Optimal asymptotic volume ratio for noncompact 3-manifolds with asymptotically nonnegative Ricci curvature and a uniformly positive scalar curvature lower bound","abstract":"In this paper, we study 3-dimensional complete non-compact Riemannian manifolds with asymptotically nonnegative Ricci curvature and a uniformly positive scalar curvature lower bound. Our main result is that, if this manifold has $k$ ends and finite first Betti number, then it has at most linear volume growth, and furthermore, if the negative part of Ricci curvature decays sufficiently fast at infinity, then we have an optimal asymptotic volume ratio $\\limsup_{r\\rightarrow\\infty}\\frac{\\mathrm{Vol}(B(p, r))}{r}\\leq4k\\pi$. In particular, our results apply to 3-dimensional complete non-compact Riemannian manifolds with nonnegative Ricci curvature and a uniformly positive scalar curvature lower bound.","sentences":["In this paper, we study 3-dimensional complete non-compact Riemannian manifolds with asymptotically nonnegative Ricci curvature and a uniformly positive scalar curvature lower bound.","Our main result is that, if this manifold has $k$ ends and finite first Betti number, then it has at most linear volume growth, and furthermore, if the negative part of Ricci curvature decays sufficiently fast at infinity, then we have an optimal asymptotic volume ratio $\\limsup_{r\\rightarrow\\infty}\\frac{\\mathrm{Vol}(B(p, r))}{r}\\leq4k\\pi$.","In particular, our results apply to 3-dimensional complete non-compact Riemannian manifolds with nonnegative Ricci curvature and a uniformly positive scalar curvature lower bound."],"url":"http://arxiv.org/abs/2405.09379v1","category":"math.DG"}
{"created":"2024-05-15 14:22:33","title":"PolygloToxicityPrompts: Multilingual Evaluation of Neural Toxic Degeneration in Large Language Models","abstract":"Recent advances in large language models (LLMs) have led to their extensive global deployment, and ensuring their safety calls for comprehensive and multilingual toxicity evaluations. However, existing toxicity benchmarks are overwhelmingly focused on English, posing serious risks to deploying LLMs in other languages. We address this by introducing PolygloToxicityPrompts (PTP), the first large-scale multilingual toxicity evaluation benchmark of 425K naturally occurring prompts spanning 17 languages. We overcome the scarcity of naturally occurring toxicity in web-text and ensure coverage across languages with varying resources by automatically scraping over 100M web-text documents. Using PTP, we investigate research questions to study the impact of model size, prompt language, and instruction and preference-tuning methods on toxicity by benchmarking over 60 LLMs. Notably, we find that toxicity increases as language resources decrease or model size increases. Although instruction- and preference-tuning reduce toxicity, the choice of preference-tuning method does not have any significant impact. Our findings shed light on crucial shortcomings of LLM safeguarding and highlight areas for future research.","sentences":["Recent advances in large language models (LLMs) have led to their extensive global deployment, and ensuring their safety calls for comprehensive and multilingual toxicity evaluations.","However, existing toxicity benchmarks are overwhelmingly focused on English, posing serious risks to deploying LLMs in other languages.","We address this by introducing PolygloToxicityPrompts (PTP), the first large-scale multilingual toxicity evaluation benchmark of 425K naturally occurring prompts spanning 17 languages.","We overcome the scarcity of naturally occurring toxicity in web-text and ensure coverage across languages with varying resources by automatically scraping over 100M web-text documents.","Using PTP, we investigate research questions to study the impact of model size, prompt language, and instruction and preference-tuning methods on toxicity by benchmarking over 60 LLMs.","Notably, we find that toxicity increases as language resources decrease or model size increases.","Although instruction- and preference-tuning reduce toxicity",", the choice of preference-tuning method does not have any significant impact.","Our findings shed light on crucial shortcomings of LLM safeguarding and highlight areas for future research."],"url":"http://arxiv.org/abs/2405.09373v1","category":"cs.CL"}
{"created":"2024-05-15 14:20:31","title":"Quantum droplets in two-dimensional Bose mixtures at finite temperature","abstract":"We investigate the formation of quantum droplets at finite temperature in attractive Bose mixtures subject to a strong transverse harmonic confinement. By means of exact path-integral Monte Carlo methods we determine the equilibrium density of the gas and the liquid as well as the pressure vs. volume dependence along isothermal curves. Results for the equation of state and for the gas-liquid coexistence region in quasi-2D configurations are compared with calculations in strictly two dimensions, finding excellent agreement. Within the pure 2D model we explore the relevance of the quantum scale anomaly and we determine the critical interaction strength for the occurrence of the first-order gas to liquid transition. Furthermore, we find that the superfluid response develops suddenly, following the density jump from the gas to the liquid state.","sentences":["We investigate the formation of quantum droplets at finite temperature in attractive Bose mixtures subject to a strong transverse harmonic confinement.","By means of exact path-integral Monte Carlo methods we determine the equilibrium density of the gas and the liquid as well as the pressure vs. volume dependence along isothermal curves.","Results for the equation of state and for the gas-liquid coexistence region in quasi-2D configurations are compared with calculations in strictly two dimensions, finding excellent agreement.","Within the pure 2D model we explore the relevance of the quantum scale anomaly and we determine the critical interaction strength for the occurrence of the first-order gas to liquid transition.","Furthermore, we find that the superfluid response develops suddenly, following the density jump from the gas to the liquid state."],"url":"http://arxiv.org/abs/2405.09368v1","category":"cond-mat.quant-gas"}
{"created":"2024-05-15 13:58:12","title":"Optimal constants of smoothing estimates for the 3D Dirac equation","abstract":"Recently, Ikoma (2022) considered optimal constants and extremisers for the $2$-dimensional Dirac equation using the spherical harmonics decomposition. Though its argument is valid in any dimensions $d \\geq 2$, the case $d \\geq 3$ remains open since it leads us to too complicated calculation: determining all eigenvalues and eigenvectors of infinite dimensional matrices. In this paper, we give optimal constants and extremisers of smoothing estimates for the $3$-dimensional Dirac equation. In order to prove this, we construct a certain orthonormal basis of spherical harmonics. With respect to this basis, infinite dimensional matrices actually become block diagonal and so that eigenvalues and eigenvectors can be easily found.","sentences":["Recently, Ikoma (2022) considered optimal constants and extremisers for the $2$-dimensional Dirac equation using the spherical harmonics decomposition.","Though its argument is valid in any dimensions $d \\geq 2$, the case $d \\geq 3$ remains open since it leads us to too complicated calculation: determining all eigenvalues and eigenvectors of infinite dimensional matrices.","In this paper, we give optimal constants and extremisers of smoothing estimates for the $3$-dimensional Dirac equation.","In order to prove this, we construct a certain orthonormal basis of spherical harmonics.","With respect to this basis, infinite dimensional matrices actually become block diagonal and so that eigenvalues and eigenvectors can be easily found."],"url":"http://arxiv.org/abs/2405.09349v1","category":"math.AP"}
{"created":"2024-05-15 11:22:06","title":"Geometric BSDEs","abstract":"We introduce and develop the concepts of Geometric Backward Stochastic Differential Equations (GBSDEs, for short) and two-driver BSDEs. We demonstrate their natural suitability for modeling dynamic return risk measures. We characterize a broad spectrum of associated BSDEs with drivers exhibiting growth rates involving terms of the form $y|\\ln(y)|+|z|^2/y$. We investigate the existence, regularity, uniqueness, and stability of solutions for these BSDEs and related two-driver BSDEs, considering both bounded and unbounded coefficients and terminal conditions. Furthermore, we present a GBSDE framework for representing the dynamics of (robust) $L^{p}$-norms and related risk measures.","sentences":["We introduce and develop the concepts of Geometric Backward Stochastic Differential Equations (GBSDEs, for short) and two-driver BSDEs.","We demonstrate their natural suitability for modeling dynamic return risk measures.","We characterize a broad spectrum of associated BSDEs with drivers exhibiting growth rates involving terms of the form $y|\\ln(y)|+|z|^2/y$. We investigate the existence, regularity, uniqueness, and stability of solutions for these BSDEs and related two-driver BSDEs, considering both bounded and unbounded coefficients and terminal conditions.","Furthermore, we present a GBSDE framework for representing the dynamics of (robust) $L^{p}$-norms and related risk measures."],"url":"http://arxiv.org/abs/2405.09260v1","category":"math.PR"}
{"created":"2024-05-15 11:08:00","title":"An Exponential Diophantine equation $x^2+3^\u03b1 113^\u03b2=y^{\\mathfrak{n}}$","abstract":"The objective of the paper is to determine the complete solutions for the Diophantine equation $x^2 + 3^{\\alpha}113^{\\beta} = y^{\\mathfrak{n}}$ in positive integers $x$ and $y$ (where $x, y \\geq 1$), non-negative exponents $\\alpha$ and $\\beta$, and an integer $\\mathfrak{n}\\geq 3$, subject to the condition $\\text{gcd}(x, y) = 1$.","sentences":["The objective of the paper is to determine the complete solutions for the Diophantine equation $x^2 + 3^{\\alpha}113^{\\beta} = y^{\\mathfrak{n}}$ in positive integers $x$ and $y$ (where $x, y \\geq 1$), non-negative exponents $\\alpha$ and $\\beta$, and an integer $\\mathfrak{n}\\geq 3$, subject to the condition $\\text{gcd}(x, y) = 1$."],"url":"http://arxiv.org/abs/2405.09252v1","category":"math.NT"}
{"created":"2024-05-15 11:00:42","title":"Graph Neural Network based Handwritten Trajectories Recognition","abstract":"The graph neural networks has been proved to be an efficient machine learning technique in real life applications. The handwritten recognition is one of the useful area in real life use where both offline and online handwriting recognition are required. The chain code as feature extraction technique has shown significant results in literature and we have been able to use chain codes with graph neural networks. To the best of our knowledge, this work presents first time a novel combination of handwritten trajectories features as chain codes and graph neural networks together. The handwritten trajectories for offline handwritten text has been evaluated using recovery of drawing order, whereas online handwritten trajectories are directly used with chain codes. Our results prove that present combination surpass previous results and minimize error rate in few epochs only.","sentences":["The graph neural networks has been proved to be an efficient machine learning technique in real life applications.","The handwritten recognition is one of the useful area in real life use where both offline and online handwriting recognition are required.","The chain code as feature extraction technique has shown significant results in literature and we have been able to use chain codes with graph neural networks.","To the best of our knowledge, this work presents first time a novel combination of handwritten trajectories features as chain codes and graph neural networks together.","The handwritten trajectories for offline handwritten text has been evaluated using recovery of drawing order, whereas online handwritten trajectories are directly used with chain codes.","Our results prove that present combination surpass previous results and minimize error rate in few epochs only."],"url":"http://arxiv.org/abs/2405.09247v1","category":"cs.CV"}
{"created":"2024-05-15 10:30:18","title":"Enhancing Image Privacy in Semantic Communication over Wiretap Channels leveraging Differential Privacy","abstract":"Semantic communication (SemCom) enhances transmission efficiency by sending only task-relevant information compared to traditional methods. However, transmitting semantic-rich data over insecure or public channels poses security and privacy risks. This paper addresses the privacy problem of transmitting images over wiretap channels and proposes a novel SemCom approach ensuring privacy through a differential privacy (DP)-based image protection and deprotection mechanism. The method utilizes the GAN inversion technique to extract disentangled semantic features and applies a DP mechanism to protect sensitive features within the extracted semantic information. To address the non-invertibility of DP, we introduce two neural networks to approximate the DP application and removal processes, offering a privacy protection level close to that by the original DP process. Simulation results validate the effectiveness of our method in preventing eavesdroppers from obtaining sensitive information while maintaining high-fidelity image reconstruction at the legitimate receiver.","sentences":["Semantic communication (SemCom) enhances transmission efficiency by sending only task-relevant information compared to traditional methods.","However, transmitting semantic-rich data over insecure or public channels poses security and privacy risks.","This paper addresses the privacy problem of transmitting images over wiretap channels and proposes a novel SemCom approach ensuring privacy through a differential privacy (DP)-based image protection and deprotection mechanism.","The method utilizes the GAN inversion technique to extract disentangled semantic features and applies a DP mechanism to protect sensitive features within the extracted semantic information.","To address the non-invertibility of DP, we introduce two neural networks to approximate the DP application and removal processes, offering a privacy protection level close to that by the original DP process.","Simulation results validate the effectiveness of our method in preventing eavesdroppers from obtaining sensitive information while maintaining high-fidelity image reconstruction at the legitimate receiver."],"url":"http://arxiv.org/abs/2405.09234v1","category":"eess.IV"}
{"created":"2024-05-15 10:28:34","title":"Tensor Krylov subspace methods via the T-product for large Sylvester tensor equations","abstract":"In the present paper, we introduce new tensor krylov subspace methods for solving large Sylvester tensor equations. The proposed method uses the well-known T-product for tensors and tensor subspaces. We introduce some new tensor products and the related algebraic properties. These new products will enable us to develop third-order the tensor FOM (tFOM), GMRES (tGMRES), tubal Block Arnoldi and the tensor tubal Block Arnoldi method to solve large Sylvester tensor equation. We give some properties related to these method and present some numerical experiments.","sentences":["In the present paper, we introduce new tensor krylov subspace methods for solving large Sylvester tensor equations.","The proposed method uses the well-known T-product for tensors and tensor subspaces.","We introduce some new tensor products and the related algebraic properties.","These new products will enable us to develop third-order the tensor FOM (tFOM), GMRES (tGMRES), tubal Block Arnoldi and the tensor tubal Block Arnoldi method to solve large Sylvester tensor equation.","We give some properties related to these method and present some numerical experiments."],"url":"http://arxiv.org/abs/2405.09233v1","category":"math.NA"}
{"created":"2024-05-15 10:06:37","title":"Unraveling impacts of polycrystalline microstructures on ionic conductivity of ceramic electrolytes by computational homogenization and machine learning","abstract":"The ionic conductivity at the grain boundaries (GBs) in oxide ceramics is typically several orders of magnitude lower than that within the grain interior. This detrimental GB effect is the main bottleneck for designing high-performance ceramic electrolytes intended for use in solid-state Lithium-ion batteries, fuel cells, and electrolyzer cells. The macroscopic ionic conductivity in oxide ceramics is essentially governed by the underlying polycrystalline microstructures where GBs and grain morphology go hand in hand. This provides the possibility to enhance the ion conductivity by microstructure engineering. To this end, a thorough understanding of microstructure-property correlation is highly desirable. In this work, we investigate numerous polycrystalline microstructure samples with varying grain and grain boundary features. Their macroscopic ionic conductivities are numerically evaluated by the finite element homogenization method, whereby the GB resistance is explicitly regarded. The influence of different microstructural features on the effective ionic conductivity is systematically studied. The microstructure-property relationships are revealed. Additionally, a graph neural network-based machine learning model is constructed and trained. It can accurately predict the effective ionic conductivity for a given polycrystalline microstructure. This work provides crucial quantitative guidelines for optimizing the ionic conducting performance of oxide ceramics by tailoring microstructures.","sentences":["The ionic conductivity at the grain boundaries (GBs) in oxide ceramics is typically several orders of magnitude lower than that within the grain interior.","This detrimental GB effect is the main bottleneck for designing high-performance ceramic electrolytes intended for use in solid-state Lithium-ion batteries, fuel cells, and electrolyzer cells.","The macroscopic ionic conductivity in oxide ceramics is essentially governed by the underlying polycrystalline microstructures where GBs and grain morphology go hand in hand.","This provides the possibility to enhance the ion conductivity by microstructure engineering.","To this end, a thorough understanding of microstructure-property correlation is highly desirable.","In this work, we investigate numerous polycrystalline microstructure samples with varying grain and grain boundary features.","Their macroscopic ionic conductivities are numerically evaluated by the finite element homogenization method, whereby the GB resistance is explicitly regarded.","The influence of different microstructural features on the effective ionic conductivity is systematically studied.","The microstructure-property relationships are revealed.","Additionally, a graph neural network-based machine learning model is constructed and trained.","It can accurately predict the effective ionic conductivity for a given polycrystalline microstructure.","This work provides crucial quantitative guidelines for optimizing the ionic conducting performance of oxide ceramics by tailoring microstructures."],"url":"http://arxiv.org/abs/2405.09227v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-15 09:38:52","title":"SOMTP: Self-Supervised Learning-Based Optimizer for MPC-Based Safe Trajectory Planning Problems in Robotics","abstract":"Model Predictive Control (MPC)-based trajectory planning has been widely used in robotics, and incorporating Control Barrier Function (CBF) constraints into MPC can greatly improve its obstacle avoidance efficiency. Unfortunately, traditional optimizers are resource-consuming and slow to solve such non-convex constrained optimization problems (COPs) while learning-based methods struggle to satisfy the non-convex constraints. In this paper, we propose SOMTP algorithm, a self-supervised learning-based optimizer for CBF-MPC trajectory planning. Specifically, first, SOMTP employs problem transcription to satisfy most of the constraints. Then the differentiable SLPG correction is proposed to move the solution closer to the safe set and is then converted as the guide policy in the following training process. After that, inspired by the Augmented Lagrangian Method (ALM), our training algorithm integrated with guide policy constraints is proposed to enable the optimizer network to converge to a feasible solution. Finally, experiments show that the proposed algorithm has better feasibility than other learning-based methods and can provide solutions much faster than traditional optimizers with similar optimality.","sentences":["Model Predictive Control (MPC)-based trajectory planning has been widely used in robotics, and incorporating Control Barrier Function (CBF) constraints into MPC can greatly improve its obstacle avoidance efficiency.","Unfortunately, traditional optimizers are resource-consuming and slow to solve such non-convex constrained optimization problems (COPs) while learning-based methods struggle to satisfy the non-convex constraints.","In this paper, we propose SOMTP algorithm, a self-supervised learning-based optimizer for CBF-MPC trajectory planning.","Specifically, first, SOMTP employs problem transcription to satisfy most of the constraints.","Then the differentiable SLPG correction is proposed to move the solution closer to the safe set and is then converted as the guide policy in the following training process.","After that, inspired by the Augmented Lagrangian Method (ALM), our training algorithm integrated with guide policy constraints is proposed to enable the optimizer network to converge to a feasible solution.","Finally, experiments show that the proposed algorithm has better feasibility than other learning-based methods and can provide solutions much faster than traditional optimizers with similar optimality."],"url":"http://arxiv.org/abs/2405.09212v1","category":"cs.RO"}
{"created":"2024-05-15 09:24:18","title":"All convex bodies are in the subdifferential of some everywhere differentiable locally Lipschitz function","abstract":"We construct a differentiable locally Lipschitz function $f$ in $\\mathbb{R}^{N}$ with the property that for every convex body $K\\subset \\mathbb{R}^N$ there exists $\\bar x \\in \\mathbb{R}^N$ such that $K$ coincides with the set $\\partial_L f(\\bar x)$ of limits of derivatives $\\{Df(x_n)\\}_{n\\geq 1}$ of sequences $\\{x_n\\}_{n\\geq 1}$ converging to~$\\bar x$. The technique can be further refined to recover all compact connected subsets with nonempty interior, disclosing an important difference between differentiable and continuously differentiable functions. It stems out from our approach that the class of these pathological functions contains an infinite dimensional vector space and is dense in the space of all locally Lipschitz functions for the uniform convergence.","sentences":["We construct a differentiable locally Lipschitz function $f$ in $\\mathbb{R}^{N}$ with the property that for every convex body $K\\subset \\mathbb{R}^N$ there exists $\\bar x \\in \\mathbb{R}^N$ such that $K$ coincides with the set $\\partial_L f(\\bar x)$ of limits of derivatives $\\{Df(x_n)\\}_{n\\geq 1}$ of sequences $\\{x_n\\}_{n\\geq 1}$ converging to~$\\bar x$.","The technique can be further refined to recover all compact connected subsets with nonempty interior, disclosing an important difference between differentiable and continuously differentiable functions.","It stems out from our approach that the class of these pathological functions contains an infinite dimensional vector space and is dense in the space of all locally Lipschitz functions for the uniform convergence."],"url":"http://arxiv.org/abs/2405.09206v1","category":"math.CA"}
{"created":"2024-05-15 09:13:24","title":"Investigating the whirling heat current density in the Guyer--Krumhansl equation","abstract":"Among the numerous heat conduction models, the Guyer--Krumhansl equation has a special role. Besides its various application possibilities in nanotechnology, cryotechnology, and even in case of modeling heterogeneous materials, it poses additional mathematical challenges compared to the Fourier or Cattaneo {(a.k.a. Maxwell--Cattaneo--Vernotte)} equations. Furthermore, the Guyer--Krumhansl equation is the first heat conduction model, which includes the curl of the heat flux density in the evolution equation. In the present paper, we place our focus on the consequences of the existence of such whirling heat current density by solving the two-dimensional Guyer--Krumhansl equation with a space and time-dependent heat pulse boundary condition. The discretization poses further challenges in regard to the boundary condition for which we propose a particular extrapolation method. Furthermore, with the help of the Helmholtz decomposition, we show the analogy with the linearized acoustics of Newtonian fluids, which reveals how the heat flux density plays the role of the velocity field. Our solutions also reveal an unexpected temperature evolution caused by the whirling heat flux density, namely, the temperature can locally be decreased for a short time in a case when the curl of the heat flux density dominates the heat conduction process.","sentences":["Among the numerous heat conduction models, the Guyer--Krumhansl equation has a special role.","Besides its various application possibilities in nanotechnology, cryotechnology, and even in case of modeling heterogeneous materials, it poses additional mathematical challenges compared to the Fourier or Cattaneo {(a.k.a. Maxwell--Cattaneo--Vernotte)} equations.","Furthermore, the Guyer--Krumhansl equation is the first heat conduction model, which includes the curl of the heat flux density in the evolution equation.","In the present paper, we place our focus on the consequences of the existence of such whirling heat current density by solving the two-dimensional Guyer--Krumhansl equation with a space and time-dependent heat pulse boundary condition.","The discretization poses further challenges in regard to the boundary condition for which we propose a particular extrapolation method.","Furthermore, with the help of the Helmholtz decomposition, we show the analogy with the linearized acoustics of Newtonian fluids, which reveals how the heat flux density plays the role of the velocity field.","Our solutions also reveal an unexpected temperature evolution caused by the whirling heat flux density, namely, the temperature can locally be decreased for a short time in a case when the curl of the heat flux density dominates the heat conduction process."],"url":"http://arxiv.org/abs/2405.09199v1","category":"physics.app-ph"}
{"created":"2024-05-15 06:01:40","title":"Enhancing Function Name Prediction using Votes-Based Name Tokenization and Multi-Task Learning","abstract":"Reverse engineers would acquire valuable insights from descriptive function names, which are absent in publicly released binaries. Recent advances in binary function name prediction using data-driven machine learning show promise. However, existing approaches encounter difficulties in capturing function semantics in diverse optimized binaries and fail to reserve the meaning of labels in function names. We propose Epitome, a framework that enhances function name prediction using votes-based name tokenization and multi-task learning, specifically tailored for different compilation optimization binaries. Epitome learns comprehensive function semantics by pre-trained assembly language model and graph neural network, incorporating function semantics similarity prediction task, to maximize the similarity of function semantics in the context of different compilation optimization levels. In addition, we present two data preprocessing methods to improve the comprehensibility of function names. We evaluate the performance of Epitome using 2,597,346 functions extracted from binaries compiled with 5 optimizations (O0-Os) for 4 architectures (x64, x86, ARM, and MIPS). Epitome outperforms the state-of-the-art function name prediction tool by up to 44.34%, 64.16%, and 54.44% in precision, recall, and F1 score, while also exhibiting superior generalizability.","sentences":["Reverse engineers would acquire valuable insights from descriptive function names, which are absent in publicly released binaries.","Recent advances in binary function name prediction using data-driven machine learning show promise.","However, existing approaches encounter difficulties in capturing function semantics in diverse optimized binaries and fail to reserve the meaning of labels in function names.","We propose Epitome, a framework that enhances function name prediction using votes-based name tokenization and multi-task learning, specifically tailored for different compilation optimization binaries.","Epitome learns comprehensive function semantics by pre-trained assembly language model and graph neural network, incorporating function semantics similarity prediction task, to maximize the similarity of function semantics in the context of different compilation optimization levels.","In addition, we present two data preprocessing methods to improve the comprehensibility of function names.","We evaluate the performance of Epitome using 2,597,346 functions extracted from binaries compiled with 5 optimizations (O0-Os) for 4 architectures (x64, x86, ARM, and MIPS).","Epitome outperforms the state-of-the-art function name prediction tool by up to 44.34%, 64.16%, and 54.44% in precision, recall, and F1 score, while also exhibiting superior generalizability."],"url":"http://arxiv.org/abs/2405.09112v1","category":"cs.SE"}
{"created":"2024-05-15 04:58:47","title":"The equation and solution of 4-point correlation function of galaxies in Gaussian approximation and its parity-odd part","abstract":"Starting with the density field equation of a self-gravity fluid   in a static Universe, using the Schwinger functional differentiation technique, we derive the field equation of the 4-point correlation function (4PCF) of galaxies in the Gaussian approximation, which contains hierarchically 2PCF and 3PCF. By use of the known solutions of 2PCF and 3PCF, the equation of 4PCF becomes an inhomogeneous, Helmholtz equation, and contains only two physical parameters: the mass $m$ of galaxy and the Jeans wavenumber $k_J$, like the equations of the 2PCF and 3PCF. We obtain the analytical solution of 4PCF that consists of four portions, $\\eta= \\eta^0_{odd} + \\eta^0_{even} +\\eta^{FP} +\\eta^I$, and has a very rich structure. $\\eta^0_{odd}$ and $\\eta^0_{even}$ form the homogeneous solution and depend on boundary conditions. The parity-odd $\\eta^0_{odd}$ is more interesting and qualitatively explains the observed parity-odd data of BOSS CMASS, the parity-even $\\eta^0_{even}$ contains the disconnected 4PCF $\\eta^{disc}$ (arising from a Gaussian random process), and both $\\eta^0_{odd}$ and $\\eta^0_{even}$ are prominent at large scales $r\\gtrsim 10$Mpc, and exhibit radial oscillations determined by the Jeans wavenumber. $\\eta^{FP}$ and $ \\eta^I$ are parity-even, and form the inhomogeneous solution. $\\eta^{FP}$ is the same as the Fry-Peebles ansatz for 4PCF, and dominates at small scales $r \\lesssim 10$Mpc. $\\eta^I$ is an integration of the inhomogeneous term, subdominant. We also compare the parity-even 4PCF with the observation data.","sentences":["Starting with the density field equation of a self-gravity fluid   in a static Universe, using the Schwinger functional differentiation technique, we derive the field equation of the 4-point correlation function (4PCF) of galaxies in the Gaussian approximation, which contains hierarchically 2PCF and 3PCF.","By use of the known solutions of 2PCF and 3PCF, the equation of 4PCF becomes an inhomogeneous, Helmholtz equation, and contains only two physical parameters: the mass $m$ of galaxy and the Jeans wavenumber $k_J$, like the equations of the 2PCF and 3PCF.","We obtain the analytical solution of 4PCF that consists of four portions, $\\eta= \\eta^0_{odd} + \\eta^0_{even} +\\eta^{FP} +\\eta^I$, and has a very rich structure.","$\\eta^0_{odd}$ and $\\eta^0_{even}$ form the homogeneous solution and depend on boundary conditions.","The parity-odd $\\eta^0_{odd}$ is more interesting and qualitatively explains the observed parity-odd data of BOSS CMASS, the parity-even $\\eta^0_{even}$ contains the disconnected 4PCF $\\eta^{disc}$ (arising from a Gaussian random process), and both $\\eta^0_{odd}$ and $\\eta^0_{even}$ are prominent at large scales $r\\gtrsim 10$Mpc, and exhibit radial oscillations determined by the Jeans wavenumber.","$\\eta^{FP}$ and $ \\eta^I$ are parity-even, and form the inhomogeneous solution.","$\\eta^{FP}$ is the same as the Fry-Peebles ansatz for 4PCF, and dominates at small scales $r \\lesssim 10$Mpc.","$\\eta^I$ is an integration of the inhomogeneous term, subdominant.","We also compare the parity-even 4PCF with the observation data."],"url":"http://arxiv.org/abs/2405.09092v1","category":"astro-ph.GA"}
{"created":"2024-05-15 03:56:34","title":"Interpretable attributed scattering center extracted via deep unfolding","abstract":"Most existing sparse representation-based approaches for attributed scattering center (ASC) extraction adopt traditional iterative optimization algorithms, which suffer from lengthy computation times and limited precision. This paper presents a solution by introducing an interpretable network that can effectively and rapidly extract ASC via deep unfolding. Initially, we create a dictionary containing reliable prior knowledge and apply it to the iterative shrinkage-thresholding algorithm (ISTA). Then, we unfold ISTA into a neural network, employing it to autonomously and precisely optimize the hyperparameters. The interpretability of physics is retained by applying a dictionary with physical meaning. The experiments are conducted on multiple test sets with diverse data distributions and demonstrate the superior performance and generalizability of our method.","sentences":["Most existing sparse representation-based approaches for attributed scattering center (ASC) extraction adopt traditional iterative optimization algorithms, which suffer from lengthy computation times and limited precision.","This paper presents a solution by introducing an interpretable network that can effectively and rapidly extract ASC via deep unfolding.","Initially, we create a dictionary containing reliable prior knowledge and apply it to the iterative shrinkage-thresholding algorithm (ISTA).","Then, we unfold ISTA into a neural network, employing it to autonomously and precisely optimize the hyperparameters.","The interpretability of physics is retained by applying a dictionary with physical meaning.","The experiments are conducted on multiple test sets with diverse data distributions and demonstrate the superior performance and generalizability of our method."],"url":"http://arxiv.org/abs/2405.09073v1","category":"eess.SP"}
{"created":"2024-05-15 03:49:35","title":"Data-driven discovery of drag-inducing elements on a rough surface through convolutional neural networks","abstract":"Understanding the influence of surface roughness on drag forces remains a significant challenge in fluid dynamics. This paper presents a convolutional neural network (CNN) that predicts drag solely by the topography of rough surfaces and is capable of discovering spatial patterns linked to drag-inducing structures. A CNN model was developed to analyze spatial information from the topography of a rough surface and predict the roughness function, $\\Delta U^+$, obtained from direct numerical simulation. This model enables the prediction of drag from rough surface data alone, which was not possible with previous methods owing to the large number of surface-derived parameters. Additionally, the retention of spatial information by the model enables the creation of a feature map that accentuates critical areas for drag prediction on rough surfaces. By interpreting the feature maps, we show that the developed CNN model is able to discover spatial patterns associated with drag distributions across rough surfaces, even without a direct training on drag distribution data. The analysis of the feature map indicates that, even without flow field information, the CNN model extracts the importance of the flow-directional slope and height of roughness elements as key factors in inducing pressure drag. This study demonstrates that CNN-based drag prediction is grounded in physical principles of fluid dynamics, underscoring the utility of CNNs in both predicting and understanding drag on rough surfaces.","sentences":["Understanding the influence of surface roughness on drag forces remains a significant challenge in fluid dynamics.","This paper presents a convolutional neural network (CNN) that predicts drag solely by the topography of rough surfaces and is capable of discovering spatial patterns linked to drag-inducing structures.","A CNN model was developed to analyze spatial information from the topography of a rough surface and predict the roughness function, $\\Delta U^+$, obtained from direct numerical simulation.","This model enables the prediction of drag from rough surface data alone, which was not possible with previous methods owing to the large number of surface-derived parameters.","Additionally, the retention of spatial information by the model enables the creation of a feature map that accentuates critical areas for drag prediction on rough surfaces.","By interpreting the feature maps, we show that the developed CNN model is able to discover spatial patterns associated with drag distributions across rough surfaces, even without a direct training on drag distribution data.","The analysis of the feature map indicates that, even without flow field information, the CNN model extracts the importance of the flow-directional slope and height of roughness elements as key factors in inducing pressure drag.","This study demonstrates that CNN-based drag prediction is grounded in physical principles of fluid dynamics, underscoring the utility of CNNs in both predicting and understanding drag on rough surfaces."],"url":"http://arxiv.org/abs/2405.09071v1","category":"physics.flu-dyn"}
{"created":"2024-05-15 03:17:30","title":"Improving Transformers using Faithful Positional Encoding","abstract":"We propose a new positional encoding method for a neural network architecture called the Transformer. Unlike the standard sinusoidal positional encoding, our approach is based on solid mathematical grounds and has a guarantee of not losing information about the positional order of the input sequence. We show that the new encoding approach systematically improves the prediction performance in the time-series classification task.","sentences":["We propose a new positional encoding method for a neural network architecture called the Transformer.","Unlike the standard sinusoidal positional encoding, our approach is based on solid mathematical grounds and has a guarantee of not losing information about the positional order of the input sequence.","We show that the new encoding approach systematically improves the prediction performance in the time-series classification task."],"url":"http://arxiv.org/abs/2405.09061v1","category":"cs.LG"}
{"created":"2024-05-15 02:40:19","title":"Differential phase contrast from electrons that cause inner shell ionization","abstract":"Differential Phase Contrast (DPC) imaging, in which deviations in the bright field beam are in proportion to the electric field, has been extensively studied in the context of pure elastic scattering. Here we discuss differential phase contrast formed from core-loss scattered electrons, i.e. those that have caused inner shell ionization of atoms in the specimen, using a transition potential approach for which we study the convergence properties. In the phase object approximation, we show formally that this is mainly a result of preservation of elastic contrast. Through simulation we demonstrate that whether the inelastic DPC images show element selective contrast depends on the spatial range of the ionization interaction, and specifically that when the energy loss is low the delocalisation can lead to contributions to the contrast from atoms other than that ionized. We further show that inelastic DPC images remain robustly interpretable to larger thicknesses than is the case for elastic DPC images, owing to the incoherence of the inelastic wavefields, though subtleties due to channelling remain.","sentences":["Differential Phase Contrast (DPC) imaging, in which deviations in the bright field beam are in proportion to the electric field, has been extensively studied in the context of pure elastic scattering.","Here we discuss differential phase contrast formed from core-loss scattered electrons, i.e. those that have caused inner shell ionization of atoms in the specimen, using a transition potential approach for which we study the convergence properties.","In the phase object approximation, we show formally that this is mainly a result of preservation of elastic contrast.","Through simulation we demonstrate that whether the inelastic DPC images show element selective contrast depends on the spatial range of the ionization interaction, and specifically that when the energy loss is low the delocalisation can lead to contributions to the contrast from atoms other than that ionized.","We further show that inelastic DPC images remain robustly interpretable to larger thicknesses than is the case for elastic DPC images, owing to the incoherence of the inelastic wavefields, though subtleties due to channelling remain."],"url":"http://arxiv.org/abs/2405.09043v1","category":"cond-mat.other"}
{"created":"2024-05-14 22:34:59","title":"Generalized quantum master equations can improve the accuracy of semiclassical predictions of multitime correlation functions","abstract":"Multitime quantum correlation functions are central objects in physical science, offering a direct link between experimental observables and the dynamics of an underlying model. While experiments such as 2D spectroscopy and quantum control can now measure such quantities, the accurate simulation of such responses remains computationally expensive and sometimes impossible, depending on the system's complexity. A natural tool to employ is the generalized quantum master equation (GQME), which can offer computational savings by extending reference dynamics at a comparatively trivial cost. However, dynamical methods that can tackle chemical systems with atomistic resolution, such as those in the semiclassical hierarchy, often suffer from poor accuracy, limiting the credence one might lend to their results. By combining work on the accuracy-boosting formulation of semiclassical memory kernels with recent work on the multitime GQME, here we show for the first time that one can exploit a multitime semiclassical GQME to dramatically improve both the accuracy of coarse mean-field Ehrenfest dynamics and obtain orders of magnitude efficiency gains.","sentences":["Multitime quantum correlation functions are central objects in physical science, offering a direct link between experimental observables and the dynamics of an underlying model.","While experiments such as 2D spectroscopy and quantum control can now measure such quantities, the accurate simulation of such responses remains computationally expensive and sometimes impossible, depending on the system's complexity.","A natural tool to employ is the generalized quantum master equation (GQME), which can offer computational savings by extending reference dynamics at a comparatively trivial cost.","However, dynamical methods that can tackle chemical systems with atomistic resolution, such as those in the semiclassical hierarchy, often suffer from poor accuracy, limiting the credence one might lend to their results.","By combining work on the accuracy-boosting formulation of semiclassical memory kernels with recent work on the multitime GQME, here we show for the first time that one can exploit a multitime semiclassical GQME to dramatically improve both the accuracy of coarse mean-field Ehrenfest dynamics and obtain orders of magnitude efficiency gains."],"url":"http://arxiv.org/abs/2405.08983v1","category":"physics.chem-ph"}
{"created":"2024-05-14 21:15:29","title":"Perturbation-based Learning for Recurrent Neural Networks","abstract":"Recurrent neural networks (RNNs) hold immense potential for computations due to their Turing completeness and sequential processing capabilities, yet existing methods for their training encounter efficiency challenges. Backpropagation through time (BPTT), the prevailing method, extends the backpropagation (BP) algorithm by unrolling the RNN over time. However, this approach suffers from significant drawbacks, including the need to interleave forward and backward phases and store exact gradient information. Furthermore, BPTT has been shown to struggle with propagating gradient information for long sequences, leading to vanishing gradients. An alternative strategy to using gradient-based methods like BPTT involves stochastically approximating gradients through perturbation-based methods. This learning approach is exceptionally simple, necessitating only forward passes in the network and a global reinforcement signal as feedback. Despite its simplicity, the random nature of its updates typically leads to inefficient optimization, limiting its effectiveness in training neural networks. In this study, we present a new approach to perturbation-based learning in RNNs whose performance is competitive with BPTT, while maintaining the inherent advantages over gradient-based learning. To this end, we extend the recently introduced activity-based node perturbation (ANP) method to operate in the time domain, leading to more efficient learning and generalization. Subsequently, we conduct a range of experiments to validate our approach. Our results show similar performance, convergence time and scalability when compared to BPTT, strongly outperforming standard node perturbation and weight perturbation methods. These findings suggest that perturbation-based learning methods offer a versatile alternative to gradient-based methods for training RNNs.","sentences":["Recurrent neural networks (RNNs) hold immense potential for computations due to their Turing completeness and sequential processing capabilities, yet existing methods for their training encounter efficiency challenges.","Backpropagation through time (BPTT), the prevailing method, extends the backpropagation (BP) algorithm by unrolling the RNN over time.","However, this approach suffers from significant drawbacks, including the need to interleave forward and backward phases and store exact gradient information.","Furthermore, BPTT has been shown to struggle with propagating gradient information for long sequences, leading to vanishing gradients.","An alternative strategy to using gradient-based methods like BPTT involves stochastically approximating gradients through perturbation-based methods.","This learning approach is exceptionally simple, necessitating only forward passes in the network and a global reinforcement signal as feedback.","Despite its simplicity, the random nature of its updates typically leads to inefficient optimization, limiting its effectiveness in training neural networks.","In this study, we present a new approach to perturbation-based learning in RNNs whose performance is competitive with BPTT, while maintaining the inherent advantages over gradient-based learning.","To this end, we extend the recently introduced activity-based node perturbation (ANP) method to operate in the time domain, leading to more efficient learning and generalization.","Subsequently, we conduct a range of experiments to validate our approach.","Our results show similar performance, convergence time and scalability when compared to BPTT, strongly outperforming standard node perturbation and weight perturbation methods.","These findings suggest that perturbation-based learning methods offer a versatile alternative to gradient-based methods for training RNNs."],"url":"http://arxiv.org/abs/2405.08967v1","category":"cs.LG"}
{"created":"2024-05-14 21:10:16","title":"Wronskians form the inverse system of the arcs of a double point","abstract":"The ideal of the arc scheme of a double point or, equivalently, the differential ideal generated by the ideal of a double point is a primary ideal in an infinite-dimensional polynomial ring supported at the origin. This ideal has a rich combinatorial structure connecting it to singularity theory, partition identities, representation theory, and differential algebra. Macaulay inverse system is a powerful tool for studying the structure of primary ideals which describes an ideal in terms of certain linear differential operators. In the present paper, we show that the inverse system of the ideal of the arc scheme of a double point is precisely a vector space spanned by all the Wronskians of the variables and their formal derivatives. We then apply this characterization to extend our recent result on Poincar\\'e-type series for such ideals.","sentences":["The ideal of the arc scheme of a double point or, equivalently, the differential ideal generated by the ideal of a double point is a primary ideal in an infinite-dimensional polynomial ring supported at the origin.","This ideal has a rich combinatorial structure connecting it to singularity theory, partition identities, representation theory, and differential algebra.","Macaulay inverse system is a powerful tool for studying the structure of primary ideals which describes an ideal in terms of certain linear differential operators.","In the present paper, we show that the inverse system of the ideal of the arc scheme of a double point is precisely a vector space spanned by all the Wronskians of the variables and their formal derivatives.","We then apply this characterization to extend our recent result on Poincar\\'e-type series for such ideals."],"url":"http://arxiv.org/abs/2405.08964v1","category":"math.AC"}
{"created":"2024-05-14 21:05:03","title":"The ratio of [Eu/$\u03b1$] differentiates accreted/in-situ Milky Way stars across metallicities, as indicated by both field stars and globular clusters","abstract":"We combine stellar orbits with the abundances of the heavy, $r$-process element europium and the light, $\\alpha$-element, silicon to separate in-situ and accreted populations in the Milky Way across all metallicities. At high orbital energy, the accretion-dominated halo shows elevated values of [Eu/Si], while at lower energies, where many of the stars were born in-situ, the levels of [Eu/Si] are lower. These systematically different levels of [Eu/Si] in the MW and the accreted halo imply that the scatter in [Eu/$\\alpha$] within a single galaxy is smaller than previously thought. At the lowest metallicities, we find that both accreted and in-situ populations trend down in [Eu/Si], consistent with enrichment via neutron star mergers. Through compiling a large dataset of abundances for 46 globular clusters (GCs), we show that differences in [Eu/Si] extend to populations of in-situ/accreted GCs. We interpret this consistency as evidence that in $r$-process elements, GCs trace the star formation history of their hosts, motivating their use as sub-Gyr timers of galactic evolution. Furthermore, fitting the trends in [Eu/Si] using a simple galactic chemical evolution model, we find that differences in [Eu/Si] between accreted and in-situ MW field stars cannot be explained through star formation efficiency alone. Finally, we show that the use of [Eu/Si] as a chemical tag between GCs and their host galaxies extends beyond the Local Group, to the halo of M31 - potentially offering the opportunity to do Galactic Archaeology in an external galaxy.","sentences":["We combine stellar orbits with the abundances of the heavy, $r$-process element europium and the light, $\\alpha$-element, silicon to separate in-situ and accreted populations in the Milky Way across all metallicities.","At high orbital energy, the accretion-dominated halo shows elevated values of [Eu/Si], while at lower energies, where many of the stars were born in-situ, the levels of [Eu/Si] are lower.","These systematically different levels of [Eu/Si] in the MW and the accreted halo imply that the scatter in [Eu/$\\alpha$] within a single galaxy is smaller than previously thought.","At the lowest metallicities, we find that both accreted and in-situ populations trend down in [Eu/Si], consistent with enrichment via neutron star mergers.","Through compiling a large dataset of abundances for 46 globular clusters (GCs), we show that differences in [Eu/Si] extend to populations of in-situ/accreted GCs.","We interpret this consistency as evidence that in $r$-process elements, GCs trace the star formation history of their hosts, motivating their use as sub-Gyr timers of galactic evolution.","Furthermore, fitting the trends in [Eu/Si] using a simple galactic chemical evolution model, we find that differences in [Eu/Si] between accreted and in-situ MW field stars cannot be explained through star formation efficiency alone.","Finally, we show that the use of [Eu/Si] as a chemical tag between GCs and their host galaxies extends beyond the Local Group, to the halo of M31 - potentially offering the opportunity to do Galactic Archaeology in an external galaxy."],"url":"http://arxiv.org/abs/2405.08963v1","category":"astro-ph.GA"}
{"created":"2024-05-14 20:09:35","title":"Exploring the Local Landscape in the Triangle Network","abstract":"Characterizing the set of distributions that can be realized in the triangle network is a notoriously difficult problem. In this work, we investigate inner approximations of the set of local (classical) distributions of the triangle network. A quantum distribution that appears to be nonlocal is the Elegant Joint Measurement (EJM) [Entropy. 2019; 21(3):325], which motivates us to study distributions having the same symmetries as the EJM. We compare analytical and neural-network-based inner approximations and find a remarkable agreement between the two methods. Using neural network tools, we also conjecture network Bell inequalities that give a trade-off between the levels of correlation and symmetry that a local distribution may feature. Our results considerably strengthen the conjecture that the EJM is nonlocal.","sentences":["Characterizing the set of distributions that can be realized in the triangle network is a notoriously difficult problem.","In this work, we investigate inner approximations of the set of local (classical) distributions of the triangle network.","A quantum distribution that appears to be nonlocal is the Elegant Joint Measurement (EJM)","[Entropy. 2019; 21(3):325], which motivates us to study distributions having the same symmetries as the EJM.","We compare analytical and neural-network-based inner approximations and find a remarkable agreement between the two methods.","Using neural network tools, we also conjecture network Bell inequalities that give a trade-off between the levels of correlation and symmetry that a local distribution may feature.","Our results considerably strengthen the conjecture that the EJM is nonlocal."],"url":"http://arxiv.org/abs/2405.08939v1","category":"quant-ph"}
{"created":"2024-05-14 20:02:35","title":"Function based sim-to-real learning for shape control of deformable free-form surfaces","abstract":"For the shape control of deformable free-form surfaces, simulation plays a crucial role in establishing the mapping between the actuation parameters and the deformed shapes. The differentiation of this forward kinematic mapping is usually employed to solve the inverse kinematic problem for determining the actuation parameters that can realize a target shape. However, the free-form surfaces obtained from simulators are always different from the physically deformed shapes due to the errors introduced by hardware and the simplification adopted in physical simulation. To fill the gap, we propose a novel deformation function based sim-to-real learning method that can map the geometric shape of a simulated model into its corresponding shape of the physical model. Unlike the existing sim-to-real learning methods that rely on completely acquired dense markers, our method accommodates sparsely distributed markers and can resiliently use all captured frames -- even for those in the presence of missing markers. To demonstrate its effectiveness, our sim-to-real method has been integrated into a neural network-based computational pipeline designed to tackle the inverse kinematic problem on a pneumatically actuated deformable mannequin.","sentences":["For the shape control of deformable free-form surfaces, simulation plays a crucial role in establishing the mapping between the actuation parameters and the deformed shapes.","The differentiation of this forward kinematic mapping is usually employed to solve the inverse kinematic problem for determining the actuation parameters that can realize a target shape.","However, the free-form surfaces obtained from simulators are always different from the physically deformed shapes due to the errors introduced by hardware and the simplification adopted in physical simulation.","To fill the gap, we propose a novel deformation function based sim-to-real learning method that can map the geometric shape of a simulated model into its corresponding shape of the physical model.","Unlike the existing sim-to-real learning methods that rely on completely acquired dense markers, our method accommodates sparsely distributed markers and can resiliently use all captured frames -- even for those in the presence of missing markers.","To demonstrate its effectiveness, our sim-to-real method has been integrated into a neural network-based computational pipeline designed to tackle the inverse kinematic problem on a pneumatically actuated deformable mannequin."],"url":"http://arxiv.org/abs/2405.08935v1","category":"cs.RO"}
{"created":"2024-05-14 19:48:02","title":"Constant-roll inflation with a complex scalar field","abstract":"We consider inflation with a constant rate of rolling in which a complex scalar field plays the role of inflaton during the inflationary epoch. We implement the inflationary analysis for an accredited angular speed $\\dot{\\theta}$ which satisfies our dynamical equations. Scalar and tensorial perturbations generated in the framework of constant roll inflation with a complex field are studied. In this respect, we find analytically solutions to the gauge invariant fluctuations, with which an expression for the scalar power spectrum together with its scalar index spectral in this scenario were found. By comparing the obtained results with the observations coming from the cosmic microwave background anisotropies, the constraints on the parameters space of the model and also its predictions are analyzed and discussed.","sentences":["We consider inflation with a constant rate of rolling in which a complex scalar field plays the role of inflaton during the inflationary epoch.","We implement the inflationary analysis for an accredited angular speed $\\dot{\\theta}$ which satisfies our dynamical equations.","Scalar and tensorial perturbations generated in the framework of constant roll inflation with a complex field are studied.","In this respect, we find analytically solutions to the gauge invariant fluctuations, with which an expression for the scalar power spectrum together with its scalar index spectral in this scenario were found.","By comparing the obtained results with the observations coming from the cosmic microwave background anisotropies, the constraints on the parameters space of the model and also its predictions are analyzed and discussed."],"url":"http://arxiv.org/abs/2405.08928v1","category":"gr-qc"}
{"created":"2024-05-14 19:46:33","title":"Atmospheric muons and their variations with temperature","abstract":"Seasonal variations of atmospheric muons are traditionally interpreted in terms of an effective temperature that relates the atmospheric temperature profile at a given time to the dependence of muon production on atmospheric depth. This paper aims to review and generalize the treatment of muon production and effective temperature that has been used to interpret seasonal variations of atmospheric muons by many experiments. The formalism is developed both in integral form -- for application to compact detectors at a fixed depth that record all muons with $E_\\mu > E_\\mu^\\mathrm{min}$ -- and in differential form -- for application to extended detectors like IceCube, KM3NeT, and Baikal-GVD, where the rates are proportional to energy-dependent effective areas.","sentences":["Seasonal variations of atmospheric muons are traditionally interpreted in terms of an effective temperature that relates the atmospheric temperature profile at a given time to the dependence of muon production on atmospheric depth.","This paper aims to review and generalize the treatment of muon production and effective temperature that has been used to interpret seasonal variations of atmospheric muons by many experiments.","The formalism is developed both in integral form -- for application to compact detectors at a fixed depth that record all muons with $E_\\mu > E_\\mu^\\mathrm{min}$ -- and in differential form -- for application to extended detectors like IceCube, KM3NeT, and Baikal-GVD, where the rates are proportional to energy-dependent effective areas."],"url":"http://arxiv.org/abs/2405.08926v1","category":"astro-ph.HE"}
{"created":"2024-05-14 18:30:02","title":"Preheating with deep learning","abstract":"We apply deep learning techniques to the late-time turbulent regime in a post-inflationary model where a real scalar inflaton field and the standard model Higgs doublet interact with renormalizable couplings between them. After inflation, the inflaton decays into the Higgs through a trilinear coupling and the Higgs field subsequently thermalizes with gauge bosons via its $SU(2)\\times U(1)$ gauge interaction. Depending on the strength of the trilinear interaction and the Higgs self-coupling, the effective mass squared of Higgs can become negative, leading to the tachyonic production of Higgs particles. These produced Higgs particles would then share their energy with gauge bosons, potentially indicating thermalization. Since the model entails different non-perturbative effects, it is necessary to resort to numerical and semi-classical techniques. However, simulations require significant costs in terms of time and computational resources depending on the model used. Particularly, when $SU(2)$ gauge interactions are introduced, this becomes evident as the gauge field redistributes particle energies through rescattering processes, leading to an abundance of UV modes that disrupt simulation stability. This necessitates very small lattice spacings, resulting in exceedingly long simulation runtimes. Furthermore, the late-time behavior of preheating dynamics exhibits a universal form by wave kinetic theory. Therefore, we analyze patterns in the flow of particle numbers and predict future behavior using CNN-LSTM (Convolutional Neural Network combined with Long Short-Term Memory) time series analysis. In this way, we can reduce our dependence on simulations by orders of magnitude in terms of time and computational resources.","sentences":["We apply deep learning techniques to the late-time turbulent regime in a post-inflationary model where a real scalar inflaton field and the standard model Higgs doublet interact with renormalizable couplings between them.","After inflation, the inflaton decays into the Higgs through a trilinear coupling and the Higgs field subsequently thermalizes with gauge bosons via its $SU(2)\\times U(1)$ gauge interaction.","Depending on the strength of the trilinear interaction and the Higgs self-coupling, the effective mass squared of Higgs can become negative, leading to the tachyonic production of Higgs particles.","These produced Higgs particles would then share their energy with gauge bosons, potentially indicating thermalization.","Since the model entails different non-perturbative effects, it is necessary to resort to numerical and semi-classical techniques.","However, simulations require significant costs in terms of time and computational resources depending on the model used.","Particularly, when $SU(2)$ gauge interactions are introduced, this becomes evident as the gauge field redistributes particle energies through rescattering processes, leading to an abundance of UV modes that disrupt simulation stability.","This necessitates very small lattice spacings, resulting in exceedingly long simulation runtimes.","Furthermore, the late-time behavior of preheating dynamics exhibits a universal form by wave kinetic theory.","Therefore, we analyze patterns in the flow of particle numbers and predict future behavior using CNN-LSTM (Convolutional Neural Network combined with Long Short-Term Memory) time series analysis.","In this way, we can reduce our dependence on simulations by orders of magnitude in terms of time and computational resources."],"url":"http://arxiv.org/abs/2405.08901v1","category":"hep-ph"}
{"created":"2024-05-14 18:13:02","title":"Time Evolution in Canonical Quantum Gravity is Trivial","abstract":"The Wheeler-DeWitt equation does not describe any explicit time evolution of the wave function, and somehow related to this issue, there is no natural way of defining an invariant inner product that provides a viable probability interpretation. We show that both of these difficulties are solved in a covariant canonical formulation of general relativity where the configuration space is extended by introducing the embedding coordinates as dynamical variables. The formalism describes the evolution of the wave function from one spacelike slice to another, but as in the case of spatial diffeomorphisms this is simply implemented by a coordinate change in the wave function. We demonstrate how the time equation disappears after gauge fixing that removes the embedding coordinates. These findings indicate that the time evolution is trivial in a background independent formulation of quantum gravity.","sentences":["The Wheeler-DeWitt equation does not describe any explicit time evolution of the wave function, and somehow related to this issue, there is no natural way of defining an invariant inner product that provides a viable probability interpretation.","We show that both of these difficulties are solved in a covariant canonical formulation of general relativity where the configuration space is extended by introducing the embedding coordinates as dynamical variables.","The formalism describes the evolution of the wave function from one spacelike slice to another, but as in the case of spatial diffeomorphisms this is simply implemented by a coordinate change in the wave function.","We demonstrate how the time equation disappears after gauge fixing that removes the embedding coordinates.","These findings indicate that the time evolution is trivial in a background independent formulation of quantum gravity."],"url":"http://arxiv.org/abs/2405.08895v1","category":"gr-qc"}
{"created":"2024-05-14 18:00:04","title":"A Dark Matter Fermionic Quantum Fluid from Standard Model Dynamics","abstract":"We present a model of dark matter as a superconducting fluid of Cooper pairs of right handed neutrinos or of vector-like quarks. The superconducting dark matter is induced by attractive channels in the Standard Model Higgs and color sectors of the Standard Model, respectively. We show that, for each case, the solution to the gap equation provides viable dark matter candidates for suitable chemical potential values. The mechanism yields an ultra-light neutrino condensate with a mass of $m_{\\rm DM} \\sim 10^{-19} \\text{eV}$ or a vector-like quark condensate with wide range of possible masses. Both cosmological and particle physics constraints on the model lead to a connection between the number of effective relativistic species $N_{\\rm eff}$, and the chemical potential and CMB temperature at the time of fermion creation. We also find a relation between the superconducting fermion and baryon densities, with implications for the coincidence between the dark matter and baryon densities in standard cosmology. Given the natural $\\text{eV}$ scale of neutrinos, this mechanism may have implications for the Hubble tension.","sentences":["We present a model of dark matter as a superconducting fluid of Cooper pairs of right handed neutrinos or of vector-like quarks.","The superconducting dark matter is induced by attractive channels in the Standard Model Higgs and color sectors of the Standard Model, respectively.","We show that, for each case, the solution to the gap equation provides viable dark matter candidates for suitable chemical potential values.","The mechanism yields an ultra-light neutrino condensate with a mass of $m_{\\rm DM} \\sim 10^{-19} \\text{eV}$ or a vector-like quark condensate with wide range of possible masses.","Both cosmological and particle physics constraints on the model lead to a connection between the number of effective relativistic species $N_{\\rm eff}$, and the chemical potential and CMB temperature at the time of fermion creation.","We also find a relation between the superconducting fermion and baryon densities, with implications for the coincidence between the dark matter and baryon densities in standard cosmology.","Given the natural $\\text{eV}$ scale of neutrinos, this mechanism may have implications for the Hubble tension."],"url":"http://arxiv.org/abs/2405.08874v1","category":"hep-ph"}
{"created":"2024-05-14 18:00:03","title":"The DNA of Calabi-Yau Hypersurfaces","abstract":"We implement Genetic Algorithms for triangulations of four-dimensional reflexive polytopes which induce Calabi-Yau threefold hypersurfaces via Batryev's construction. We demonstrate that such algorithms efficiently optimize physical observables such as axion decay constants or axion-photon couplings in string theory compactifications. For our implementation, we choose a parameterization of triangulations that yields homotopy inequivalent Calabi-Yau threefolds by extending fine, regular triangulations of two-faces, thereby eliminating exponentially large redundancy factors in the map from polytope triangulations to Calabi-Yau hypersurfaces. In particular, we discuss how this encoding renders the entire Kreuzer-Skarke list amenable to a variety of optimization strategies, including but not limited to Genetic Algorithms. To achieve optimal performance, we tune the hyperparameters of our Genetic Algorithm using Bayesian optimization. We find that our implementation vastly outperforms other sampling and optimization strategies like Markov Chain Monte Carlo or Simulated Annealing. Finally, we showcase that our Genetic Algorithm efficiently performs optimization even for the maximal polytope with Hodge numbers $h^{1,1} = 491$, where we use it to maximize axion-photon couplings.","sentences":["We implement Genetic Algorithms for triangulations of four-dimensional reflexive polytopes which induce Calabi-Yau threefold hypersurfaces via Batryev's construction.","We demonstrate that such algorithms efficiently optimize physical observables such as axion decay constants or axion-photon couplings in string theory compactifications.","For our implementation, we choose a parameterization of triangulations that yields homotopy inequivalent Calabi-Yau threefolds by extending fine, regular triangulations of two-faces, thereby eliminating exponentially large redundancy factors in the map from polytope triangulations to Calabi-Yau hypersurfaces.","In particular, we discuss how this encoding renders the entire Kreuzer-Skarke list amenable to a variety of optimization strategies, including but not limited to Genetic Algorithms.","To achieve optimal performance, we tune the hyperparameters of our Genetic Algorithm using Bayesian optimization.","We find that our implementation vastly outperforms other sampling and optimization strategies like Markov Chain Monte Carlo or Simulated Annealing.","Finally, we showcase that our Genetic Algorithm efficiently performs optimization even for the maximal polytope with Hodge numbers $h^{1,1} = 491$, where we use it to maximize axion-photon couplings."],"url":"http://arxiv.org/abs/2405.08871v1","category":"hep-th"}
{"created":"2024-05-14 18:00:02","title":"RIGEL: Simulating dwarf galaxies at solar mass resolution with radiative transfer and feedback from individual massive stars","abstract":"We introduce the RIGEL model, a novel framework to self-consistently model the effects of stellar feedback in the multiphase ISM of dwarf galaxies with radiative transfer (RT) on a star-by-star basis. The RIGEL model integrates detailed implementations of feedback from individual massive stars into the RHD code, AREPO-RT. It forms individual massive stars from the resolved multiphase ISM by sampling the IMF and tracks their evolution individually. The lifetimes, photon production rates, mass-loss rates, and wind velocities of these stars are determined by their initial masses and metallicities based on a library that incorporates a variety of stellar models. The RT equations are solved in seven spectral bins accounting for the IR to HeII ionizing bands, using an M1 RT scheme. The thermochemistry model tracks the non-equilibrium H, He chemistry and the equilibrium abundance of CI, CII, OI, OII, and CO to capture the thermodynamics of all ISM phases. We evaluate the performance of the RIGEL model using $1\\,{\\rm M}_\\odot$ resolution simulations of isolated dwarf galaxies. We found that the SFR and ISRF show strong positive correlations to the metallicity of the galaxy. Photoionization and photoheating can reduce the SFR by an order of magnitude by removing the available cold-dense gas fuel for star formation. The ISRF also changes the thermal structure of the ISM. Radiative feedback occurs immediately after the birth of massive stars and rapidly disperses the molecular clouds within 1 Myr. As a consequence, radiative feedback reduces the age spread of star clusters to less than 2 Myr, prohibits the formation of massive star clusters, and shapes the cluster initial mass function to a steep power-law form with a slope of $\\sim-2$. The mass-loading factor of the fiducial galaxy has a median of $\\sim50$, while turning off radiative feedback reduces this factor by an order of magnitude.","sentences":["We introduce the RIGEL model, a novel framework to self-consistently model the effects of stellar feedback in the multiphase ISM of dwarf galaxies with radiative transfer (RT) on a star-by-star basis.","The RIGEL model integrates detailed implementations of feedback from individual massive stars into the RHD code, AREPO-RT.","It forms individual massive stars from the resolved multiphase ISM by sampling the IMF and tracks their evolution individually.","The lifetimes, photon production rates, mass-loss rates, and wind velocities of these stars are determined by their initial masses and metallicities based on a library that incorporates a variety of stellar models.","The RT equations are solved in seven spectral bins accounting for the IR to HeII ionizing bands, using an M1 RT scheme.","The thermochemistry model tracks the non-equilibrium H, He chemistry and the equilibrium abundance of CI, CII, OI, OII, and CO to capture the thermodynamics of all ISM phases.","We evaluate the performance of the RIGEL model using $1\\,{\\rm M}_\\odot$ resolution simulations of isolated dwarf galaxies.","We found that the SFR and ISRF show strong positive correlations to the metallicity of the galaxy.","Photoionization and photoheating can reduce the SFR by an order of magnitude by removing the available cold-dense gas fuel for star formation.","The ISRF also changes the thermal structure of the ISM.","Radiative feedback occurs immediately after the birth of massive stars and rapidly disperses the molecular clouds within 1 Myr.","As a consequence, radiative feedback reduces the age spread of star clusters to less than 2 Myr, prohibits the formation of massive star clusters, and shapes the cluster initial mass function to a steep power-law form with a slope of $\\sim-2$. The mass-loading factor of the fiducial galaxy has a median of $\\sim50$, while turning off radiative feedback reduces this factor by an order of magnitude."],"url":"http://arxiv.org/abs/2405.08869v1","category":"astro-ph.GA"}
{"created":"2024-05-14 18:00:01","title":"Numerical Analysis of Resonant Axion-Photon Mixing: Part I","abstract":"Many present-day axion searches attempt to probe the mixing of axions and photons, which occurs in the presence of an external magnetic field. While this process is well-understood in a number of simple and idealized contexts, a strongly varying or highly inhomogeneous background can impact the efficiency and evolution of the mixing in a non-trivial manner. In an effort to develop a generalized framework for analyzing axion-photon mixing in arbitrary systems, we focus in this work on directly solving the axion-modified form of Maxwell's equations across a simulation domain with a spatially varying background. We concentrate specifically on understanding resonantly enhanced axion-photon mixing in a highly magnetized plasma, which is a key ingredient for developing precision predictions of radio signals emanating from the magnetospheres of neutron stars. After illustrating the success and accuracy of our approach for simplified limiting cases, we compare our results with a number of analytic solutions recently derived to describe mixing in these systems. We find that our numerical method demonstrates a high level of agreement with one, but only one, of the published results. Interestingly, our method also recovers the mixing between the axion and magnetosonic-t and Alfv\\'{e}n modes; these modes cannot escape from the regions of dense plasma, but could non-trivially alter the dynamics in certain environments. Future work will focus on extending our calculations to study resonant mixing in strongly variable backgrounds, mixing in generalized media (beyond the strong magnetic field limit), and the mixing of photons with other light bosonic fields, such as dark photons.","sentences":["Many present-day axion searches attempt to probe the mixing of axions and photons, which occurs in the presence of an external magnetic field.","While this process is well-understood in a number of simple and idealized contexts, a strongly varying or highly inhomogeneous background can impact the efficiency and evolution of the mixing in a non-trivial manner.","In an effort to develop a generalized framework for analyzing axion-photon mixing in arbitrary systems, we focus in this work on directly solving the axion-modified form of Maxwell's equations across a simulation domain with a spatially varying background.","We concentrate specifically on understanding resonantly enhanced axion-photon mixing in a highly magnetized plasma, which is a key ingredient for developing precision predictions of radio signals emanating from the magnetospheres of neutron stars.","After illustrating the success and accuracy of our approach for simplified limiting cases, we compare our results with a number of analytic solutions recently derived to describe mixing in these systems.","We find that our numerical method demonstrates a high level of agreement with one, but only one, of the published results.","Interestingly, our method also recovers the mixing between the axion and magnetosonic-t and Alfv\\'{e}n modes; these modes cannot escape from the regions of dense plasma, but could non-trivially alter the dynamics in certain environments.","Future work will focus on extending our calculations to study resonant mixing in strongly variable backgrounds, mixing in generalized media (beyond the strong magnetic field limit), and the mixing of photons with other light bosonic fields, such as dark photons."],"url":"http://arxiv.org/abs/2405.08865v1","category":"hep-ph"}
{"created":"2024-05-14 18:00:00","title":"Measurement-induced phase transitions in systems with diffusive dynamics","abstract":"The competition between scrambling and projective measurements can lead to measurement-induced entanglement phase transitions (MIPT). In this work, we show that the universality class of the MIPT is drastically altered when the system is coupled to a diffusing conserved density. Specifically, we consider a 1+1d random Clifford circuit locally monitored by classically diffusing particles (``measurers''). The resulting diffusive correlations in the measurement density are a relevant perturbation to the usual space-time random MIPT critical point, producing a new universality class for this phase transition. We find ``Griffiths-like'' effects due to rare space-time regions where, e.g., the diffusive measurers have a low or high density, but these are considerably weaker than the Griffiths effects that occur with quenched randomness that produce rare spatial regions with infinite lifetime.","sentences":["The competition between scrambling and projective measurements can lead to measurement-induced entanglement phase transitions (MIPT).","In this work, we show that the universality class of the MIPT is drastically altered when the system is coupled to a diffusing conserved density.","Specifically, we consider a 1+1d random Clifford circuit locally monitored by classically diffusing particles (``measurers'').","The resulting diffusive correlations in the measurement density are a relevant perturbation to the usual space-time random MIPT critical point, producing a new universality class for this phase transition.","We find ``Griffiths-like'' effects due to rare space-time regions where, e.g., the diffusive measurers have a low or high density, but these are considerably weaker than the Griffiths effects that occur with quenched randomness that produce rare spatial regions with infinite lifetime."],"url":"http://arxiv.org/abs/2405.08861v1","category":"quant-ph"}
{"created":"2024-05-15 17:52:00","title":"MMFusion: Multi-modality Diffusion Model for Lymph Node Metastasis Diagnosis in Esophageal Cancer","abstract":"Esophageal cancer is one of the most common types of cancer worldwide and ranks sixth in cancer-related mortality. Accurate computer-assisted diagnosis of cancer progression can help physicians effectively customize personalized treatment plans. Currently, CT-based cancer diagnosis methods have received much attention for their comprehensive ability to examine patients' conditions. However, multi-modal based methods may likely introduce information redundancy, leading to underperformance. In addition, efficient and effective interactions between multi-modal representations need to be further explored, lacking insightful exploration of prognostic correlation in multi-modality features. In this work, we introduce a multi-modal heterogeneous graph-based conditional feature-guided diffusion model for lymph node metastasis diagnosis based on CT images as well as clinical measurements and radiomics data. To explore the intricate relationships between multi-modal features, we construct a heterogeneous graph. Following this, a conditional feature-guided diffusion approach is applied to eliminate information redundancy. Moreover, we propose a masked relational representation learning strategy, aiming to uncover the latent prognostic correlations and priorities of primary tumor and lymph node image representations. Various experimental results validate the effectiveness of our proposed method. The code is available at https://github.com/wuchengyu123/MMFusion.","sentences":["Esophageal cancer is one of the most common types of cancer worldwide and ranks sixth in cancer-related mortality.","Accurate computer-assisted diagnosis of cancer progression can help physicians effectively customize personalized treatment plans.","Currently, CT-based cancer diagnosis methods have received much attention for their comprehensive ability to examine patients' conditions.","However, multi-modal based methods may likely introduce information redundancy, leading to underperformance.","In addition, efficient and effective interactions between multi-modal representations need to be further explored, lacking insightful exploration of prognostic correlation in multi-modality features.","In this work, we introduce a multi-modal heterogeneous graph-based conditional feature-guided diffusion model for lymph node metastasis diagnosis based on CT images as well as clinical measurements and radiomics data.","To explore the intricate relationships between multi-modal features, we construct a heterogeneous graph.","Following this, a conditional feature-guided diffusion approach is applied to eliminate information redundancy.","Moreover, we propose a masked relational representation learning strategy, aiming to uncover the latent prognostic correlations and priorities of primary tumor and lymph node image representations.","Various experimental results validate the effectiveness of our proposed method.","The code is available at https://github.com/wuchengyu123/MMFusion."],"url":"http://arxiv.org/abs/2405.09539v1","category":"eess.IV"}
{"created":"2024-05-15 16:26:06","title":"Color Space Learning for Cross-Color Person Re-Identification","abstract":"The primary color profile of the same identity is assumed to remain consistent in typical Person Re-identification (Person ReID) tasks. However, this assumption may be invalid in real-world situations and images hold variant color profiles, because of cross-modality cameras or identity with different clothing. To address this issue, we propose Color Space Learning (CSL) for those Cross-Color Person ReID problems. Specifically, CSL guides the model to be less color-sensitive with two modules: Image-level Color-Augmentation and Pixel-level Color-Transformation. The first module increases the color diversity of the inputs and guides the model to focus more on the non-color information. The second module projects every pixel of input images onto a new color space. In addition, we introduce a new Person ReID benchmark across RGB and Infrared modalities, NTU-Corridor, which is the first with privacy agreements from all participants. To evaluate the effectiveness and robustness of our proposed CSL, we evaluate it on several Cross-Color Person ReID benchmarks. Our method surpasses the state-of-the-art methods consistently. The code and benchmark are available at: https://github.com/niejiahao1998/CSL","sentences":["The primary color profile of the same identity is assumed to remain consistent in typical Person Re-identification (Person ReID) tasks.","However, this assumption may be invalid in real-world situations and images hold variant color profiles, because of cross-modality cameras or identity with different clothing.","To address this issue, we propose Color Space Learning (CSL) for those Cross-Color Person ReID problems.","Specifically, CSL guides the model to be less color-sensitive with two modules: Image-level Color-Augmentation and Pixel-level Color-Transformation.","The first module increases the color diversity of the inputs and guides the model to focus more on the non-color information.","The second module projects every pixel of input images onto a new color space.","In addition, we introduce a new Person ReID benchmark across RGB and Infrared modalities, NTU-Corridor, which is the first with privacy agreements from all participants.","To evaluate the effectiveness and robustness of our proposed CSL, we evaluate it on several Cross-Color Person ReID benchmarks.","Our method surpasses the state-of-the-art methods consistently.","The code and benchmark are available at: https://github.com/niejiahao1998/CSL"],"url":"http://arxiv.org/abs/2405.09487v1","category":"cs.CV"}
{"created":"2024-05-15 15:07:31","title":"Distinguishing Tor From Other Encrypted Network Traffic Through Character Analysis","abstract":"For journalists reporting from a totalitarian regime, whistleblowers and resistance fighters, the anonymous use of cloud services on the Internet can be vital for survival. The Tor network provides a free and widely used anonymization service for everyone. However, there are different approaches to distinguishing Tor from non-Tor encrypted network traffic, most recently only due to the (relative) frequencies of hex digits in a single encrypted payload packet. While conventional data traffic is usually encrypted once, but at least three times in the case of Tor due to the structure and principle of the Tor network, we have examined to what extent the number of encryptions contributes to being able to distinguish Tor from non-Tor encrypted data traffic.","sentences":["For journalists reporting from a totalitarian regime, whistleblowers and resistance fighters, the anonymous use of cloud services on the Internet can be vital for survival.","The Tor network provides a free and widely used anonymization service for everyone.","However, there are different approaches to distinguishing Tor from non-Tor encrypted network traffic, most recently only due to the (relative) frequencies of hex digits in a single encrypted payload packet.","While conventional data traffic is usually encrypted once, but at least three times in the case of Tor due to the structure and principle of the Tor network, we have examined to what extent the number of encryptions contributes to being able to distinguish Tor from non-Tor encrypted data traffic."],"url":"http://arxiv.org/abs/2405.09412v1","category":"cs.CR"}
{"created":"2024-05-15 15:00:46","title":"Time-Equivariant Contrastive Learning for Degenerative Disease Progression in Retinal OCT","abstract":"Contrastive pretraining provides robust representations by ensuring their invariance to different image transformations while simultaneously preventing representational collapse. Equivariant contrastive learning, on the other hand, provides representations sensitive to specific image transformations while remaining invariant to others. By introducing equivariance to time-induced transformations, such as disease-related anatomical changes in longitudinal imaging, the model can effectively capture such changes in the representation space. In this work, we pro-pose a Time-equivariant Contrastive Learning (TC) method. First, an encoder embeds two unlabeled scans from different time points of the same patient into the representation space. Next, a temporal equivariance module is trained to predict the representation of a later visit based on the representation from one of the previous visits and the corresponding time interval with a novel regularization loss term while preserving the invariance property to irrelevant image transformations. On a large longitudinal dataset, our model clearly outperforms existing equivariant contrastive methods in predicting progression from intermediate age-related macular degeneration (AMD) to advanced wet-AMD within a specified time-window.","sentences":["Contrastive pretraining provides robust representations by ensuring their invariance to different image transformations while simultaneously preventing representational collapse.","Equivariant contrastive learning, on the other hand, provides representations sensitive to specific image transformations while remaining invariant to others.","By introducing equivariance to time-induced transformations, such as disease-related anatomical changes in longitudinal imaging, the model can effectively capture such changes in the representation space.","In this work, we pro-pose a Time-equivariant Contrastive Learning (TC) method.","First, an encoder embeds two unlabeled scans from different time points of the same patient into the representation space.","Next, a temporal equivariance module is trained to predict the representation of a later visit based on the representation from one of the previous visits and the corresponding time interval with a novel regularization loss term while preserving the invariance property to irrelevant image transformations.","On a large longitudinal dataset, our model clearly outperforms existing equivariant contrastive methods in predicting progression from intermediate age-related macular degeneration (AMD) to advanced wet-AMD within a specified time-window."],"url":"http://arxiv.org/abs/2405.09404v1","category":"cs.CV"}
{"created":"2024-05-15 14:28:00","title":"Strategic Data Re-Uploads: A Pathway to Improved Quantum Classification Data Re-Uploading Strategies for Improved Quantum Classifier Performance","abstract":"Quantum machine learning (QML) is a promising field that explores the applications of quantum computing to machine learning tasks. A significant hurdle in the advancement of quantum machine learning lies in the development of efficient and resilient quantum classifiers capable of accurately mapping input data to specific, discrete target outputs. In this paper, we propose a novel approach to improve quantum classifier performance by using a data re-uploading strategy. Re-uploading classical information into quantum states multiple times can enhance the accuracy of quantum classifiers. We investigate the effects of different cost functions, such as fidelity and trace distance, on the optimization process and the classification results. We demonstrate our approach to two classification patterns: a linear classification pattern (LCP) and a non-linear classification pattern (NLCP). We evaluate the efficacy of our approach by benchmarking it against four distinct optimization techniques: L-BFGS-B, COBYLA, Nelder-Mead, and SLSQP. Additionally, we study the different impacts of fixed datasets and random datasets. Our results show that our approach can achieve high classification accuracy and robustness and outperform the existing quantum classifier models.","sentences":["Quantum machine learning (QML) is a promising field that explores the applications of quantum computing to machine learning tasks.","A significant hurdle in the advancement of quantum machine learning lies in the development of efficient and resilient quantum classifiers capable of accurately mapping input data to specific, discrete target outputs.","In this paper, we propose a novel approach to improve quantum classifier performance by using a data re-uploading strategy.","Re-uploading classical information into quantum states multiple times can enhance the accuracy of quantum classifiers.","We investigate the effects of different cost functions, such as fidelity and trace distance, on the optimization process and the classification results.","We demonstrate our approach to two classification patterns: a linear classification pattern (LCP) and a non-linear classification pattern (NLCP).","We evaluate the efficacy of our approach by benchmarking it against four distinct optimization techniques: L-BFGS-B, COBYLA, Nelder-Mead, and SLSQP.","Additionally, we study the different impacts of fixed datasets and random datasets.","Our results show that our approach can achieve high classification accuracy and robustness and outperform the existing quantum classifier models."],"url":"http://arxiv.org/abs/2405.09377v1","category":"quant-ph"}
{"created":"2024-05-15 14:15:09","title":"On the Saturation Effect of Kernel Ridge Regression","abstract":"The saturation effect refers to the phenomenon that the kernel ridge regression (KRR) fails to achieve the information theoretical lower bound when the smoothness of the underground truth function exceeds certain level. The saturation effect has been widely observed in practices and a saturation lower bound of KRR has been conjectured for decades. In this paper, we provide a proof of this long-standing conjecture.","sentences":["The saturation effect refers to the phenomenon that the kernel ridge regression (KRR) fails to achieve the information theoretical lower bound when the smoothness of the underground truth function exceeds certain level.","The saturation effect has been widely observed in practices and a saturation lower bound of KRR has been conjectured for decades.","In this paper, we provide a proof of this long-standing conjecture."],"url":"http://arxiv.org/abs/2405.09362v1","category":"stat.ML"}
{"created":"2024-05-15 11:28:49","title":"Quantum Computing Education for Computer Science Students: Bridging the Gap with Layered Learning and Intuitive Analogies","abstract":"Quantum computing presents a transformative potential for the world of computing. However, integrating this technology into the curriculum for computer science students who lack prior exposure to quantum mechanics and advanced mathematics remains a challenging task. This paper proposes a scaffolded learning approach aimed at equipping computer science students with essential quantum principles. By introducing foundational quantum concepts through relatable analogies and a layered learning approach based on classical computation, this approach seeks to bridge the gap between classical and quantum computing. This differs from previous approaches which build quantum computing fundamentals from the prerequisite of linear algebra and mathematics. The paper offers a considered set of intuitive analogies for foundation quantum concepts including entanglement, superposition, quantum data structures and quantum algorithms. These analogies coupled with a computing-based layered learning approach, lay the groundwork for a comprehensive teaching methodology tailored for undergraduate third level computer science students.","sentences":["Quantum computing presents a transformative potential for the world of computing.","However, integrating this technology into the curriculum for computer science students who lack prior exposure to quantum mechanics and advanced mathematics remains a challenging task.","This paper proposes a scaffolded learning approach aimed at equipping computer science students with essential quantum principles.","By introducing foundational quantum concepts through relatable analogies and a layered learning approach based on classical computation, this approach seeks to bridge the gap between classical and quantum computing.","This differs from previous approaches which build quantum computing fundamentals from the prerequisite of linear algebra and mathematics.","The paper offers a considered set of intuitive analogies for foundation quantum concepts including entanglement, superposition, quantum data structures and quantum algorithms.","These analogies coupled with a computing-based layered learning approach, lay the groundwork for a comprehensive teaching methodology tailored for undergraduate third level computer science students."],"url":"http://arxiv.org/abs/2405.09265v1","category":"cs.ET"}
{"created":"2024-05-15 08:33:41","title":"Cross-Input Certified Training for Universal Perturbations","abstract":"Existing work in trustworthy machine learning primarily focuses on single-input adversarial perturbations. In many real-world attack scenarios, input-agnostic adversarial attacks, e.g. universal adversarial perturbations (UAPs), are much more feasible. Current certified training methods train models robust to single-input perturbations but achieve suboptimal clean and UAP accuracy, thereby limiting their applicability in practical applications. We propose a novel method, CITRUS, for certified training of networks robust against UAP attackers. We show in an extensive evaluation across different datasets, architectures, and perturbation magnitudes that our method outperforms traditional certified training methods on standard accuracy (up to 10.3\\%) and achieves SOTA performance on the more practical certified UAP accuracy metric.","sentences":["Existing work in trustworthy machine learning primarily focuses on single-input adversarial perturbations.","In many real-world attack scenarios, input-agnostic adversarial attacks, e.g. universal adversarial perturbations (UAPs), are much more feasible.","Current certified training methods train models robust to single-input perturbations but achieve suboptimal clean and UAP accuracy, thereby limiting their applicability in practical applications.","We propose a novel method, CITRUS, for certified training of networks robust against UAP attackers.","We show in an extensive evaluation across different datasets, architectures, and perturbation magnitudes that our method outperforms traditional certified training methods on standard accuracy (up to 10.3\\%) and achieves SOTA performance on the more practical certified UAP accuracy metric."],"url":"http://arxiv.org/abs/2405.09176v1","category":"cs.LG"}
{"created":"2024-05-15 07:31:48","title":"Scalable Image Coding for Humans and Machines Using Feature Fusion Network","abstract":"As image recognition models become more prevalent, scalable coding methods for machines and humans gain more importance. Applications of image recognition models include traffic monitoring and farm management. In these use cases, the scalable coding method proves effective because the tasks require occasional image checking by humans. Existing image compression methods for humans and machines meet these requirements to some extent. However, these compression methods are effective solely for specific image recognition models. We propose a learning-based scalable image coding method for humans and machines that is compatible with numerous image recognition models. We combine an image compression model for machines with a compression model, providing additional information to facilitate image decoding for humans. The features in these compression models are fused using a feature fusion network to achieve efficient image compression. Our method's additional information compression model is adjusted to reduce the number of parameters by enabling combinations of features of different sizes in the feature fusion network. Our approach confirms that the feature fusion network efficiently combines image compression models while reducing the number of parameters. Furthermore, we demonstrate the effectiveness of the proposed scalable coding method by evaluating the image compression performance in terms of decoded image quality and bitrate.","sentences":["As image recognition models become more prevalent, scalable coding methods for machines and humans gain more importance.","Applications of image recognition models include traffic monitoring and farm management.","In these use cases, the scalable coding method proves effective because the tasks require occasional image checking by humans.","Existing image compression methods for humans and machines meet these requirements to some extent.","However, these compression methods are effective solely for specific image recognition models.","We propose a learning-based scalable image coding method for humans and machines that is compatible with numerous image recognition models.","We combine an image compression model for machines with a compression model, providing additional information to facilitate image decoding for humans.","The features in these compression models are fused using a feature fusion network to achieve efficient image compression.","Our method's additional information compression model is adjusted to reduce the number of parameters by enabling combinations of features of different sizes in the feature fusion network.","Our approach confirms that the feature fusion network efficiently combines image compression models while reducing the number of parameters.","Furthermore, we demonstrate the effectiveness of the proposed scalable coding method by evaluating the image compression performance in terms of decoded image quality and bitrate."],"url":"http://arxiv.org/abs/2405.09152v1","category":"cs.CV"}
{"created":"2024-05-15 07:18:32","title":"Characterizing NGC 6383: A study of pre-main sequence stars, mass segregation, and age using Gaia DR3 and 2MASS","abstract":"This study employs Bayesian analysis and machine learning techniques to analyze the young open cluster NGC 6383 using data from Gaia DR3 and 2MASS. We identified 254 probable cluster members through HDBSCAN based on proper motions and determined the cluster's core and tidal radius. To perform this analysis, we utilized an extension of Hamiltonian Monte Carlo, the No-U-Turn Sampler, using the Bayesian library PyMC. Our results indicate a mean cluster age of $4 \\pm 1$ Myr and a distance of $1.107 \\pm 0.035$ kpc. The analysis reveals mass segregation, especially among binary stars, and suggests that NGC 6383 is not fully relaxed. We observed a well-defined main sequence and a population of pre-main sequence stars, with a star formation range from $\\sim 1$ to $6$ Myr, indicating recent star formation activity.","sentences":["This study employs Bayesian analysis and machine learning techniques to analyze the young open cluster NGC 6383 using data from Gaia DR3 and 2MASS.","We identified 254 probable cluster members through HDBSCAN based on proper motions and determined the cluster's core and tidal radius.","To perform this analysis, we utilized an extension of Hamiltonian Monte Carlo, the No-U-Turn Sampler, using the Bayesian library PyMC.","Our results indicate a mean cluster age of $4 \\pm 1$ Myr and a distance of $1.107 \\pm 0.035$ kpc.","The analysis reveals mass segregation, especially among binary stars, and suggests that NGC 6383 is not fully relaxed.","We observed a well-defined main sequence and a population of pre-main sequence stars, with a star formation range from $\\sim 1$ to $6$ Myr, indicating recent star formation activity."],"url":"http://arxiv.org/abs/2405.09145v1","category":"astro-ph.SR"}
{"created":"2024-05-15 03:59:59","title":"Typhon: Automatic Recommendation of Relevant Code Cells in Jupyter Notebooks","abstract":"At present, code recommendation tools have gained greater importance to many software developers in various areas of expertise. Having code recommendation tools has enabled better productivity and performance in developing the code in software and made it easier for developers to find code examples and learn from them.   This paper proposes Typhon, an approach to automatically recommend relevant code cells in Jupyter notebooks. Typhon tokenizes developers' markdown description cells and looks for the most similar code cells from the database using text similarities such as the BM25 ranking function or CodeBERT, a machine-learning approach. Then, the algorithm computes the similarity distance between the tokenized query and markdown cells to return the most relevant code cells to the developers.   We evaluated the Typhon tool on Jupyter notebooks from Kaggle competitions and found that the approach can recommend code cells with moderate accuracy. The approach and results in this paper can lead to further improvements in code cell recommendations in Jupyter notebooks.","sentences":["At present, code recommendation tools have gained greater importance to many software developers in various areas of expertise.","Having code recommendation tools has enabled better productivity and performance in developing the code in software and made it easier for developers to find code examples and learn from them.   ","This paper proposes Typhon, an approach to automatically recommend relevant code cells in Jupyter notebooks.","Typhon tokenizes developers' markdown description cells and looks for the most similar code cells from the database using text similarities such as the BM25 ranking function or CodeBERT, a machine-learning approach.","Then, the algorithm computes the similarity distance between the tokenized query and markdown cells to return the most relevant code cells to the developers.   ","We evaluated the Typhon tool on Jupyter notebooks from Kaggle competitions and found that the approach can recommend code cells with moderate accuracy.","The approach and results in this paper can lead to further improvements in code cell recommendations in Jupyter notebooks."],"url":"http://arxiv.org/abs/2405.09075v1","category":"cs.SE"}
{"created":"2024-05-15 02:56:00","title":"3D Shape Augmentation with Content-Aware Shape Resizing","abstract":"Recent advancements in deep learning for 3D models have propelled breakthroughs in generation, detection, and scene understanding. However, the effectiveness of these algorithms hinges on large training datasets. We address the challenge by introducing Efficient 3D Seam Carving (E3SC), a novel 3D model augmentation method based on seam carving, which progressively deforms only part of the input model while ensuring the overall semantics are unchanged. Experiments show that our approach is capable of producing diverse and high-quality augmented 3D shapes across various types and styles of input models, achieving considerable improvements over previous methods. Quantitative evaluations demonstrate that our method effectively enhances the novelty and quality of shapes generated by other subsequent 3D generation algorithms.","sentences":["Recent advancements in deep learning for 3D models have propelled breakthroughs in generation, detection, and scene understanding.","However, the effectiveness of these algorithms hinges on large training datasets.","We address the challenge by introducing Efficient 3D Seam Carving (E3SC), a novel 3D model augmentation method based on seam carving, which progressively deforms only part of the input model while ensuring the overall semantics are unchanged.","Experiments show that our approach is capable of producing diverse and high-quality augmented 3D shapes across various types and styles of input models, achieving considerable improvements over previous methods.","Quantitative evaluations demonstrate that our method effectively enhances the novelty and quality of shapes generated by other subsequent 3D generation algorithms."],"url":"http://arxiv.org/abs/2405.09050v1","category":"cs.CV"}
{"created":"2024-05-15 02:29:16","title":"Learning from Partial Label Proportions for Whole Slide Image Segmentation","abstract":"In this paper, we address the segmentation of tumor subtypes in whole slide images (WSI) by utilizing incomplete label proportions. Specifically, we utilize `partial' label proportions, which give the proportions among tumor subtypes but do not give the proportion between tumor and non-tumor. Partial label proportions are recorded as the standard diagnostic information by pathologists, and we, therefore, want to use them for realizing the segmentation model that can classify each WSI patch into one of the tumor subtypes or non-tumor. We call this problem ``learning from partial label proportions (LPLP)'' and formulate the problem as a weakly supervised learning problem. Then, we propose an efficient algorithm for this challenging problem by decomposing it into two weakly supervised learning subproblems: multiple instance learning (MIL) and learning from label proportions (LLP). These subproblems are optimized efficiently in the end-to-end manner. The effectiveness of our algorithm is demonstrated through experiments conducted on two WSI datasets.","sentences":["In this paper, we address the segmentation of tumor subtypes in whole slide images (WSI) by utilizing incomplete label proportions.","Specifically, we utilize `partial' label proportions, which give the proportions among tumor subtypes but do not give the proportion between tumor and non-tumor.","Partial label proportions are recorded as the standard diagnostic information by pathologists, and we, therefore, want to use them for realizing the segmentation model that can classify each WSI patch into one of the tumor subtypes or non-tumor.","We call this problem ``learning from partial label proportions (LPLP)'' and formulate the problem as a weakly supervised learning problem.","Then, we propose an efficient algorithm for this challenging problem by decomposing it into two weakly supervised learning subproblems: multiple instance learning (MIL) and learning from label proportions (LLP).","These subproblems are optimized efficiently in the end-to-end manner.","The effectiveness of our algorithm is demonstrated through experiments conducted on two WSI datasets."],"url":"http://arxiv.org/abs/2405.09041v1","category":"cs.CV"}
{"created":"2024-05-14 22:16:52","title":"drGAT: Attention-Guided Gene Assessment of Drug Response Utilizing a Drug-Cell-Gene Heterogeneous Network","abstract":"Drug development is a lengthy process with a high failure rate. Increasingly, machine learning is utilized to facilitate the drug development processes. These models aim to enhance our understanding of drug characteristics, including their activity in biological contexts. However, a major challenge in drug response (DR) prediction is model interpretability as it aids in the validation of findings. This is important in biomedicine, where models need to be understandable in comparison with established knowledge of drug interactions with proteins. drGAT, a graph deep learning model, leverages a heterogeneous graph composed of relationships between proteins, cell lines, and drugs. drGAT is designed with two objectives: DR prediction as a binary sensitivity prediction and elucidation of drug mechanism from attention coefficients. drGAT has demonstrated superior performance over existing models, achieving 78\\% accuracy (and precision), and 76\\% F1 score for 269 DNA-damaging compounds of the NCI60 drug response dataset. To assess the model's interpretability, we conducted a review of drug-gene co-occurrences in Pubmed abstracts in comparison to the top 5 genes with the highest attention coefficients for each drug. We also examined whether known relationships were retained in the model by inspecting the neighborhoods of topoisomerase-related drugs. For example, our model retained TOP1 as a highly weighted predictive feature for irinotecan and topotecan, in addition to other genes that could potentially be regulators of the drugs. Our method can be used to accurately predict sensitivity to drugs and may be useful in the identification of biomarkers relating to the treatment of cancer patients.","sentences":["Drug development is a lengthy process with a high failure rate.","Increasingly, machine learning is utilized to facilitate the drug development processes.","These models aim to enhance our understanding of drug characteristics, including their activity in biological contexts.","However, a major challenge in drug response (DR) prediction is model interpretability as it aids in the validation of findings.","This is important in biomedicine, where models need to be understandable in comparison with established knowledge of drug interactions with proteins.","drGAT, a graph deep learning model, leverages a heterogeneous graph composed of relationships between proteins, cell lines, and drugs.","drGAT is designed with two objectives: DR prediction as a binary sensitivity prediction and elucidation of drug mechanism from attention coefficients.","drGAT has demonstrated superior performance over existing models, achieving 78\\% accuracy (and precision), and 76\\% F1 score for 269 DNA-damaging compounds of the NCI60 drug response dataset.","To assess the model's interpretability, we conducted a review of drug-gene co-occurrences in Pubmed abstracts in comparison to the top 5 genes with the highest attention coefficients for each drug.","We also examined whether known relationships were retained in the model by inspecting the neighborhoods of topoisomerase-related drugs.","For example, our model retained TOP1 as a highly weighted predictive feature for irinotecan and topotecan, in addition to other genes that could potentially be regulators of the drugs.","Our method can be used to accurately predict sensitivity to drugs and may be useful in the identification of biomarkers relating to the treatment of cancer patients."],"url":"http://arxiv.org/abs/2405.08979v1","category":"cs.LG"}
{"created":"2024-05-14 19:12:32","title":"Feature Importance and Explainability in Quantum Machine Learning","abstract":"Many Machine Learning (ML) models are referred to as black box models, providing no real insights into why a prediction is made. Feature importance and explainability are important for increasing transparency and trust in ML models, particularly in settings such as healthcare and finance. With quantum computing's unique capabilities, such as leveraging quantum mechanical phenomena like superposition, which can be combined with ML techniques to create the field of Quantum Machine Learning (QML), and such techniques may be applied to QML models. This article explores feature importance and explainability insights in QML compared to Classical ML models. Utilizing the widely recognized Iris dataset, classical ML algorithms such as SVM and Random Forests, are compared against hybrid quantum counterparts, implemented via IBM's Qiskit platform: the Variational Quantum Classifier (VQC) and Quantum Support Vector Classifier (QSVC). This article aims to provide a comparison of the insights generated in ML by employing permutation and leave one out feature importance methods, alongside ALE (Accumulated Local Effects) and SHAP (SHapley Additive exPlanations) explainers.","sentences":["Many Machine Learning (ML) models are referred to as black box models, providing no real insights into why a prediction is made.","Feature importance and explainability are important for increasing transparency and trust in ML models, particularly in settings such as healthcare and finance.","With quantum computing's unique capabilities, such as leveraging quantum mechanical phenomena like superposition, which can be combined with ML techniques to create the field of Quantum Machine Learning (QML), and such techniques may be applied to QML models.","This article explores feature importance and explainability insights in QML compared to Classical ML models.","Utilizing the widely recognized Iris dataset, classical ML algorithms such as SVM and Random Forests, are compared against hybrid quantum counterparts, implemented via IBM's Qiskit platform: the Variational Quantum Classifier (VQC) and Quantum Support Vector Classifier (QSVC).","This article aims to provide a comparison of the insights generated in ML by employing permutation and leave one out feature importance methods, alongside ALE (Accumulated Local Effects) and SHAP (SHapley Additive exPlanations) explainers."],"url":"http://arxiv.org/abs/2405.08917v1","category":"cs.LG"}
{"created":"2024-05-14 19:06:24","title":"CLIP with Quality Captions: A Strong Pretraining for Vision Tasks","abstract":"CLIP models perform remarkably well on zero-shot classification and retrieval tasks. But recent studies have shown that learnt representations in CLIP are not well suited for dense prediction tasks like object detection, semantic segmentation or depth estimation. More recently, multi-stage training methods for CLIP models was introduced to mitigate the weak performance of CLIP on downstream tasks. In this work, we find that simply improving the quality of captions in image-text datasets improves the quality of CLIP's visual representations, resulting in significant improvement on downstream dense prediction vision tasks. In fact, we find that CLIP pretraining with good quality captions can surpass recent supervised, self-supervised and weakly supervised pretraining methods. We show that when CLIP model with ViT-B/16 as image encoder is trained on well aligned image-text pairs it obtains 12.1% higher mIoU and 11.5% lower RMSE on semantic segmentation and depth estimation tasks over recent state-of-the-art Masked Image Modeling (MIM) pretraining methods like Masked Autoencoder (MAE). We find that mobile architectures also benefit significantly from CLIP pretraining. A recent mobile vision architecture, MCi2, with CLIP pretraining obtains similar performance as Swin-L, pretrained on ImageNet-22k for semantic segmentation task while being 6.1$\\times$ smaller. Moreover, we show that improving caption quality results in $10\\times$ data efficiency when finetuning for dense prediction tasks.","sentences":["CLIP models perform remarkably well on zero-shot classification and retrieval tasks.","But recent studies have shown that learnt representations in CLIP are not well suited for dense prediction tasks like object detection, semantic segmentation or depth estimation.","More recently, multi-stage training methods for CLIP models was introduced to mitigate the weak performance of CLIP on downstream tasks.","In this work, we find that simply improving the quality of captions in image-text datasets improves the quality of CLIP's visual representations, resulting in significant improvement on downstream dense prediction vision tasks.","In fact, we find that CLIP pretraining with good quality captions can surpass recent supervised, self-supervised and weakly supervised pretraining methods.","We show that when CLIP model with ViT-B/16 as image encoder is trained on well aligned image-text pairs it obtains 12.1% higher mIoU and 11.5% lower RMSE on semantic segmentation and depth estimation tasks over recent state-of-the-art Masked Image Modeling (MIM) pretraining methods like Masked Autoencoder (MAE).","We find that mobile architectures also benefit significantly from CLIP pretraining.","A recent mobile vision architecture, MCi2, with CLIP pretraining obtains similar performance as Swin-L, pretrained on ImageNet-22k for semantic segmentation task while being 6.1$\\times$ smaller.","Moreover, we show that improving caption quality results in $10\\times$ data efficiency when finetuning for dense prediction tasks."],"url":"http://arxiv.org/abs/2405.08911v1","category":"cs.CV"}
{"created":"2024-05-14 19:00:40","title":"The Impact of 2D and 3D Gamified VR on Learning American Sign Language","abstract":"Sign language has been extensively studied as a means of facilitating effective communication between hearing individuals and the deaf community. With the continuous advancements in virtual reality (VR) and gamification technologies, an increasing number of studies have begun to explore the application of these emerging technologies in sign language learning. This paper describes a user study that compares the impact of 2D and 3D games on the user experience in ASL learning. Empirical evidence gathered through questionnaires supports the positive impact of 3D game environments on user engagement and overall experience, particularly in relation to attractiveness, usability, and efficiency. Moreover, initial findings demonstrate a similar behaviour of 2D and 3D games in terms of enhancing user experience. Finally, the study identifies areas where improvements can be made to enhance the dependability and clarity of 3D game environments. These findings contribute to the understanding of how game-based approaches, and specifically the utilisation of 3D environments, can positively influence the learning experience of ASL.","sentences":["Sign language has been extensively studied as a means of facilitating effective communication between hearing individuals and the deaf community.","With the continuous advancements in virtual reality (VR) and gamification technologies, an increasing number of studies have begun to explore the application of these emerging technologies in sign language learning.","This paper describes a user study that compares the impact of 2D and 3D games on the user experience in ASL learning.","Empirical evidence gathered through questionnaires supports the positive impact of 3D game environments on user engagement and overall experience, particularly in relation to attractiveness, usability, and efficiency.","Moreover, initial findings demonstrate a similar behaviour of 2D and 3D games in terms of enhancing user experience.","Finally, the study identifies areas where improvements can be made to enhance the dependability and clarity of 3D game environments.","These findings contribute to the understanding of how game-based approaches, and specifically the utilisation of 3D environments, can positively influence the learning experience of ASL."],"url":"http://arxiv.org/abs/2405.08908v1","category":"cs.HC"}
{"created":"2024-05-14 18:10:46","title":"RS-Reg: Probabilistic and Robust Certified Regression Through Randomized Smoothing","abstract":"Randomized smoothing has shown promising certified robustness against adversaries in classification tasks. Despite such success with only zeroth-order access to base models, randomized smoothing has not been extended to a general form of regression. By defining robustness in regression tasks flexibly through probabilities, we demonstrate how to establish upper bounds on input data point perturbation (using the $\\ell_2$ norm) for a user-specified probability of observing valid outputs. Furthermore, we showcase the asymptotic property of a basic averaging function in scenarios where the regression model operates without any constraint. We then derive a certified upper bound of the input perturbations when dealing with a family of regression models where the outputs are bounded. Our simulations verify the validity of the theoretical results and reveal the advantages and limitations of simple smoothing functions, i.e., averaging, in regression tasks. The code is publicly available at \\url{https://github.com/arekavandi/Certified_Robust_Regression}.","sentences":["Randomized smoothing has shown promising certified robustness against adversaries in classification tasks.","Despite such success with only zeroth-order access to base models, randomized smoothing has not been extended to a general form of regression.","By defining robustness in regression tasks flexibly through probabilities, we demonstrate how to establish upper bounds on input data point perturbation (using the $\\ell_2$ norm) for a user-specified probability of observing valid outputs.","Furthermore, we showcase the asymptotic property of a basic averaging function in scenarios where the regression model operates without any constraint.","We then derive a certified upper bound of the input perturbations when dealing with a family of regression models where the outputs are bounded.","Our simulations verify the validity of the theoretical results and reveal the advantages and limitations of simple smoothing functions, i.e., averaging, in regression tasks.","The code is publicly available at \\url{https://github.com/arekavandi/Certified_Robust_Regression}."],"url":"http://arxiv.org/abs/2405.08892v1","category":"cs.LG"}
{"created":"2024-05-14 18:07:00","title":"Incorporating Physical Priors into Weakly-Supervised Anomaly Detection","abstract":"We propose a new machine-learning-based anomaly detection strategy for comparing data with a background-only reference (a form of weak supervision). The sensitivity of previous strategies degrades significantly when the signal is too rare or there are many unhelpful features. Our Prior-Assisted Weak Supervision (PAWS) method incorporates information from a class of signal models in order to significantly enhance the search sensitivity of weakly supervised approaches. As long as the true signal is in the pre-specified class, PAWS matches the sensitivity of a dedicated, fully supervised method without specifying the exact parameters ahead of time. On the benchmark LHC Olympics anomaly detection dataset, our mix of semi-supervised and weakly supervised learning is able to extend the sensitivity over previous methods by a factor of 10 in cross section. Furthermore, if we add irrelevant (noise) dimensions to the inputs, classical methods degrade by another factor of 10 in cross section while PAWS remains insensitive to noise. This new approach could be applied in a number of scenarios and pushes the frontier of sensitivity between completely model-agnostic approaches and fully model-specific searches.","sentences":["We propose a new machine-learning-based anomaly detection strategy for comparing data with a background-only reference (a form of weak supervision).","The sensitivity of previous strategies degrades significantly when the signal is too rare or there are many unhelpful features.","Our Prior-Assisted Weak Supervision (PAWS) method incorporates information from a class of signal models in order to significantly enhance the search sensitivity of weakly supervised approaches.","As long as the true signal is in the pre-specified class, PAWS matches the sensitivity of a dedicated, fully supervised method without specifying the exact parameters ahead of time.","On the benchmark LHC Olympics anomaly detection dataset, our mix of semi-supervised and weakly supervised learning is able to extend the sensitivity over previous methods by a factor of 10 in cross section.","Furthermore, if we add irrelevant (noise) dimensions to the inputs, classical methods degrade by another factor of 10 in cross section while PAWS remains insensitive to noise.","This new approach could be applied in a number of scenarios and pushes the frontier of sensitivity between completely model-agnostic approaches and fully model-specific searches."],"url":"http://arxiv.org/abs/2405.08889v1","category":"hep-ph"}
{"created":"2024-05-14 18:00:02","title":"A Review of Gravitational Memory and BMS Frame Fixing in Numerical Relativity","abstract":"Gravitational memory effects and the BMS freedoms exhibited at future null infinity have recently been resolved and utilized in numerical relativity simulations. With this, gravitational wave models and our understanding of the fundamental nature of general relativity have been vastly improved. In this paper, we review the history and intuition behind memory effects and BMS symmetries, how they manifest in gravitational waves, and how controlling the infinite number of BMS freedoms of numerical relativity simulations can crucially improve the waveform models that are used by gravitational wave detectors. We reiterate the fact that, with memory effects and BMS symmetries, not only can these next-generation numerical waveforms be used to observe never-before-seen physics, but they can also be used to test GR and learn new astrophysical information about our universe.","sentences":["Gravitational memory effects and the BMS freedoms exhibited at future null infinity have recently been resolved and utilized in numerical relativity simulations.","With this, gravitational wave models and our understanding of the fundamental nature of general relativity have been vastly improved.","In this paper, we review the history and intuition behind memory effects and BMS symmetries, how they manifest in gravitational waves, and how controlling the infinite number of BMS freedoms of numerical relativity simulations can crucially improve the waveform models that are used by gravitational wave detectors.","We reiterate the fact that, with memory effects and BMS symmetries, not only can these next-generation numerical waveforms be used to observe never-before-seen physics, but they can also be used to test GR and learn new astrophysical information about our universe."],"url":"http://arxiv.org/abs/2405.08868v1","category":"gr-qc"}
{"created":"2024-05-15 14:33:42","title":"$p$-Wasserstein barycenters","abstract":"We study barycenters of $N$ probability measures on $\\mathbb{R}^d$ with respect to the $p$-Wasserstein metric ($1<p<\\infty$). We prove that   -- $p$-Wasserstein barycenters of absolutely continuous measures are unique, and again absolutely continuous   -- $p$-Wasserstein barycenters admit a multi-marginal formulation   -- the optimal multi-marginal plan is unique and of Monge form if the marginals are absolutely continuous, and its support has an explicit parametrization as a graph over any marginal space.   This extends the Agueh--Carlier theory of Wasserstein barycenters [SIAM J. Math. Anal. 43 (2011), no.2, 904--924] to exponents $p\\neq 2$. A key ingredient is a quantitative injectivity estimate for the (highly non-injective) map from $N$-point configurations to their $p$-barycenter on the support of an optimal multi-marginal plan. We also discuss the statistical meaning of $p$-Wasserstein barycenters in one dimension.","sentences":["We study barycenters of $N$ probability measures on $\\mathbb{R}^d$ with respect to the $p$-Wasserstein metric ($1<p<\\infty$).","We prove that   -- $p$-Wasserstein barycenters of absolutely continuous measures are unique, and again absolutely continuous   -- $p$-Wasserstein barycenters admit a multi-marginal formulation   -- the optimal multi-marginal plan is unique and of Monge form if the marginals are absolutely continuous, and its support has an explicit parametrization as a graph over any marginal space.   ","This extends the Agueh--Carlier theory of Wasserstein barycenters","[SIAM J. Math.","Anal. 43 (2011), no.2, 904--924] to exponents $p\\neq 2$.","A key ingredient is a quantitative injectivity estimate for the (highly non-injective) map from $N$-point configurations to their $p$-barycenter on the support of an optimal multi-marginal plan.","We also discuss the statistical meaning of $p$-Wasserstein barycenters in one dimension."],"url":"http://arxiv.org/abs/2405.09381v1","category":"math.AP"}
{"created":"2024-05-15 14:10:30","title":"Branch-and-price with novel cuts, and a new Stackelberg Security Game","abstract":"Anticipating the strategies of potential attackers is crucial for protecting critical infrastructure. We can represent the challenge of the defenders of such infrastructure as a Stackelberg security game. The defender must decide how to allocate limited resources to protect specific targets, aiming to maximize their expected utility (such as minimizing the extent of damage) and considering that attackers will respond in a way that is most advantageous to them.   We present novel valid inequalities to find a Strong Stackelberg Equilibrium in both Stackelberg games and Stackelberg security games. We also consider a Stackelberg security game that aims to protect targets with a defined budget. We use branch-and-price in this game to show that our approach outperforms the standard formulation in the literature, and we conduct an extensive computational study to analyze the impact of various branch-and-price parameters on the performance of our method in different game settings.","sentences":["Anticipating the strategies of potential attackers is crucial for protecting critical infrastructure.","We can represent the challenge of the defenders of such infrastructure as a Stackelberg security game.","The defender must decide how to allocate limited resources to protect specific targets, aiming to maximize their expected utility (such as minimizing the extent of damage) and considering that attackers will respond in a way that is most advantageous to them.   ","We present novel valid inequalities to find a Strong Stackelberg Equilibrium in both Stackelberg games and Stackelberg security games.","We also consider a Stackelberg security game that aims to protect targets with a defined budget.","We use branch-and-price in this game to show that our approach outperforms the standard formulation in the literature, and we conduct an extensive computational study to analyze the impact of various branch-and-price parameters on the performance of our method in different game settings."],"url":"http://arxiv.org/abs/2405.09356v1","category":"cs.GT"}
{"created":"2024-05-15 13:53:14","title":"Comparative Performance of Fluorite-Structured Materials for Nanosupercapacitor Applications","abstract":"Over the last fifteen years, ferroelectric and antiferroelectric ultra thin films based on fluorite-structured materials have drawn significant attention for a wide variety of applications requiring high integration density. Antiferroelectric $ZrO_2$, in particular, holds significant promise for nanosupercapacitors, owing to its potential for high energy storage density (ESD) and high efficiency ($\\eta$). This work assesses the potential of high-performance $Hf_{1-x}Zr_{x}O_2$ thin films encapsulated by TiN electrodes that show linear dielectric (LD), ferroelectric (FE), and antiferroelectric (AFE) behavior. Oxides on silicon are grown by magnetron sputtering and plasma-enhanced atomic layer deposition. ESD and $\\eta$ are compared for FE, AFE, and LD samples at the same electrical field (3.5 MV/cm). As expected, ESD is higher for the FE sample ($95 J/cm^3$), but $\\eta$ is ridiculously small ($\\approx$ 55%), because of the opening of the FE hysteresis curve inducing high loss. Conversely, LD samples exhibit the highest efficiency (nearly 100%), at the expense of a lower ESD. AFE $ZrO_2$ thin film strikes a balance between FE and LD behavior, showing reduced losses compared to the FE sample but an ESD as high as $52 J/cm^3$ at 3.5 MV/cm. This value can be further increased up to $84 J/cm^3$ at a higher electrical field (4.0 MV/cm), with an $\\eta$ of 75%, among the highest values reported for fluorite-structured materials, offering promising perspectives for future optimization.","sentences":["Over the last fifteen years, ferroelectric and antiferroelectric ultra thin films based on fluorite-structured materials have drawn significant attention for a wide variety of applications requiring high integration density.","Antiferroelectric $ZrO_2$, in particular, holds significant promise for nanosupercapacitors, owing to its potential for high energy storage density (ESD) and high efficiency ($\\eta$).","This work assesses the potential of high-performance $Hf_{1-x}Zr_{x}O_2$ thin films encapsulated by TiN electrodes that show linear dielectric (LD), ferroelectric (FE), and antiferroelectric (AFE) behavior.","Oxides on silicon are grown by magnetron sputtering and plasma-enhanced atomic layer deposition.","ESD and $\\eta$ are compared for FE, AFE, and LD samples at the same electrical field (3.5 MV/cm).","As expected, ESD is higher for the FE sample ($95 J/cm^3$), but $\\eta$ is ridiculously small ($\\approx$ 55%), because of the opening of the FE hysteresis curve inducing high loss.","Conversely, LD samples exhibit the highest efficiency (nearly 100%), at the expense of a lower ESD.","AFE $ZrO_2$ thin film strikes a balance between FE and LD behavior, showing reduced losses compared to the FE sample but an ESD as high as $52 J/cm^3$ at 3.5 MV/cm.","This value can be further increased up to $84 J/cm^3$ at a higher electrical field (4.0 MV/cm), with an $\\eta$ of 75%, among the highest values reported for fluorite-structured materials, offering promising perspectives for future optimization."],"url":"http://arxiv.org/abs/2405.09345v1","category":"physics.app-ph"}
{"created":"2024-05-15 13:42:29","title":"Optimal information acquisition for eliminating estimation risk","abstract":"This paper diverges from previous literature by considering the utility maximization problem in the context of investors having the freedom to actively acquire additional information to mitigate estimation risk. We derive closed-form value functions using CARA and CRRA utility functions and establish a criterion for valuing extra information through certainty equivalence, while also formulating its associated acquisition cost. By strategically employing variational methods, we explore the optimal acquisition of information, taking into account the trade-off between its value and cost. Our findings indicate that acquiring earlier information holds greater worth in eliminating estimation risk and achieving higher utility. Furthermore, we observe that investors with lower risk aversion are more inclined to pursue information acquisition.","sentences":["This paper diverges from previous literature by considering the utility maximization problem in the context of investors having the freedom to actively acquire additional information to mitigate estimation risk.","We derive closed-form value functions using CARA and CRRA utility functions and establish a criterion for valuing extra information through certainty equivalence, while also formulating its associated acquisition cost.","By strategically employing variational methods, we explore the optimal acquisition of information, taking into account the trade-off between its value and cost.","Our findings indicate that acquiring earlier information holds greater worth in eliminating estimation risk and achieving higher utility.","Furthermore, we observe that investors with lower risk aversion are more inclined to pursue information acquisition."],"url":"http://arxiv.org/abs/2405.09339v1","category":"q-fin.MF"}
{"created":"2024-05-15 13:41:54","title":"Interval Selection in Sliding Windows","abstract":"We initiate the study of the Interval Selection problem in the (streaming) sliding window model of computation.   In this problem, an algorithm receives a potentially infinite stream of intervals on the line, and the objective is to maintain at every moment an approximation to a largest possible subset of disjoint intervals among the $L$ most recent intervals, for some integer $L$.   We give the following results:   - In the unit-length intervals case, we give a $2$-approximation sliding window algorithm with space $\\tilde{\\mathrm{O}}(|OPT|)$, and we show that any sliding window algorithm that computes a $(2-\\varepsilon)$-approximation requires space $\\Omega(L)$, for any $\\varepsilon > 0$.   - In the arbitrary-length case, we give a $(\\frac{11}{3}+\\varepsilon)$-approximation sliding window algorithm with space $\\tilde{\\mathrm{O}}(|OPT|)$, for any constant $\\varepsilon > 0$, which constitutes our main result.   We also show that space $\\Omega(L)$ is needed for algorithms that compute a $(2.5-\\varepsilon)$-approximation, for any $\\varepsilon > 0$.   Our main technical contribution is an improvement over the smooth histogram technique, which consists of running independent copies of a traditional streaming algorithm with different start times. By employing the one-pass $2$-approximation streaming algorithm by Cabello and P\\'{e}rez-Lantero [Theor. Comput. Sci. '17] for \\textsf{Interval Selection} on arbitrary-length intervals as the underlying algorithm, the smooth histogram technique immediately yields a $(4+\\varepsilon)$-approximation in this setting. Our improvement is obtained by forwarding the structure of the intervals identified in a run to the subsequent run, which constrains the shape of an optimal solution and allows us to target optimal intervals differently.","sentences":["We initiate the study of the Interval Selection problem in the (streaming) sliding window model of computation.   ","In this problem, an algorithm receives a potentially infinite stream of intervals on the line, and the objective is to maintain at every moment an approximation to a largest possible subset of disjoint intervals among the $L$ most recent intervals, for some integer $L$.   ","We give the following results:   -","In the unit-length intervals case, we give a $2$-approximation sliding window algorithm with space $\\tilde{\\mathrm{O}}(|OPT|)$, and we show that any sliding window algorithm that computes a $(2-\\varepsilon)$-approximation requires space $\\Omega(L)$, for any $\\varepsilon > 0$.   -","In the arbitrary-length case, we give a $(\\frac{11}{3}+\\varepsilon)$-approximation sliding window algorithm with space $\\tilde{\\mathrm{O}}(|OPT|)$, for any constant $\\varepsilon > 0$, which constitutes our main result.   ","We also show that space $\\Omega(L)$ is needed for algorithms that compute a $(2.5-\\varepsilon)$-approximation, for any $\\varepsilon > 0$.   Our main technical contribution is an improvement over the smooth histogram technique, which consists of running independent copies of a traditional streaming algorithm with different start times.","By employing the one-pass $2$-approximation streaming algorithm by Cabello and P\\'{e}rez-Lantero [Theor.","Comput.","Sci. '17] for \\textsf{Interval Selection} on arbitrary-length intervals as the underlying algorithm, the smooth histogram technique immediately yields a $(4+\\varepsilon)$-approximation in this setting.","Our improvement is obtained by forwarding the structure of the intervals identified in a run to the subsequent run, which constrains the shape of an optimal solution and allows us to target optimal intervals differently."],"url":"http://arxiv.org/abs/2405.09338v1","category":"cs.DS"}
{"created":"2024-05-15 12:47:17","title":"Optimal crawling: from mechanical to chemical actuation","abstract":"Taking inspiration from the crawling motion of biological cells on a substrate, we consider a physical model of self-propulsion where the spatio-temporal driving can involve both, a mechanical actuation by active force couples, and a chemical actuation through controlled mass turnover. We show that the competition and cooperation between these two modalities of active driving can drastically broaden the performance repertoire of the crawler. When the material turnover is slow and the mechanical driving dominates, we find that the highest velocity at a given energetic cost is reached when actuation takes the form of an active force configuration propagating as a traveling wave. As the rate of material turnover increases, and the chemical driving starts to dominate the mechanical one, such a peristalsis-type control progressively loses its efficacy, yielding to a standing wave type driving which involves an interplay between the mechanical and chemical actuation. Our analysis suggests a new paradigm for the optimal design of crawling biomimetic robots where the conventional purely mechanical driving through distributed force actuators is complemented by a distributed chemical control of the material remodeling inside the force-transmitting machinery.","sentences":["Taking inspiration from the crawling motion of biological cells on a substrate, we consider a physical model of self-propulsion where the spatio-temporal driving can involve both, a mechanical actuation by active force couples, and a chemical actuation through controlled mass turnover.","We show that the competition and cooperation between these two modalities of active driving can drastically broaden the performance repertoire of the crawler.","When the material turnover is slow and the mechanical driving dominates, we find that the highest velocity at a given energetic cost is reached when actuation takes the form of an active force configuration propagating as a traveling wave.","As the rate of material turnover increases, and the chemical driving starts to dominate the mechanical one, such a peristalsis-type control progressively loses its efficacy, yielding to a standing wave type driving which involves an interplay between the mechanical and chemical actuation.","Our analysis suggests a new paradigm for the optimal design of crawling biomimetic robots where the conventional purely mechanical driving through distributed force actuators is complemented by a distributed chemical control of the material remodeling inside the force-transmitting machinery."],"url":"http://arxiv.org/abs/2405.09302v1","category":"cond-mat.soft"}
{"created":"2024-05-15 11:41:13","title":"Using an Evolutionary Algorithm to Create (MAX)-3SAT QUBOs","abstract":"A common way of solving satisfiability instances with quantum methods is to transform these instances into instances of QUBO, which in itself is a potentially difficult and expensive task. State-of-the-art transformations from MAX-3SAT to QUBO currently work by mapping clauses of a 3SAT formula associated with the MAX-3SAT instance to an instance of QUBO and combining the resulting QUBOs into a single QUBO instance representing the whole MAX-3SAT instance. As creating these transformations is currently done manually or via exhaustive search methods and, therefore, algorithmically inefficient, we see potential for including search-based optimization. In this paper, we propose two methods of using evolutionary algorithms to automatically create QUBO representations of MAX-3SAT problems. We evaluate our created QUBOs on 500 and 1000-clause 3SAT formulae and find competitive performance to state-of-the-art baselines when using both classical and quantum annealing solvers.","sentences":["A common way of solving satisfiability instances with quantum methods is to transform these instances into instances of QUBO, which in itself is a potentially difficult and expensive task.","State-of-the-art transformations from MAX-3SAT to QUBO currently work by mapping clauses of a 3SAT formula associated with the MAX-3SAT instance to an instance of QUBO and combining the resulting QUBOs into a single QUBO instance representing the whole MAX-3SAT instance.","As creating these transformations is currently done manually or via exhaustive search methods and, therefore, algorithmically inefficient, we see potential for including search-based optimization.","In this paper, we propose two methods of using evolutionary algorithms to automatically create QUBO representations of MAX-3SAT problems.","We evaluate our created QUBOs on 500 and 1000-clause 3SAT formulae and find competitive performance to state-of-the-art baselines when using both classical and quantum annealing solvers."],"url":"http://arxiv.org/abs/2405.09272v1","category":"quant-ph"}
{"created":"2024-05-15 11:26:19","title":"Polydisperse versus monodisperse microbubbles: A simulation study for contrast-enhanced ultrasound imaging","abstract":"Contrast-enhanced ultrasound (CEUS) presents distinct advantages in diagnostic echography. Utilizing microbubbles (MBs) as conventional contrast agents enhances vascular visualization and organ perfusion, facilitating real-time, non-invasive procedures. There is a current tendency to replace the traditional polydisperse MBs by novel monodisperse formulations in an attempt to optimize contrast enhancement and guarantee consistent behavior and reliable imaging outcomes. This study investigates the contrast enhancement achieved by monodisperse MBs of different sizes, and their influence on nonlinear imaging artifacts observed in traditional CEUS. To explore the differences between monodisperse and polydisperse populations without excessive experimentation, numerical simulations are employed for delivering precise, objective and expeditious results. The Iterative Nonlinear Contrast Source (INCS) method has previously demonstrated its efficacy in simulating ultrasound propagation in large populations in which each bubble has individual properties and several orders of multiple scattering are significant. Our findings in CEUS imaging indicate that scattering from resonant monodisperse microbubbles is 11.8 dB stronger than scattering from the polydisperse population. Furthermore, the amplitude of nonlinear imaging artifacts downstream of the monodisperse population is 19.4 dB stronger compared to polydisperse suspension. Investigating the impact of multiple scattering on polydisperse populations compared to various monodisperse suspensions reveals that monodisperse MBs are more effective contrast agents, especially when on resonance. Despite the strong signal to noise ratio of monodisperse populations, the imaging artifacts due to nonlinear wave propagation are also enhanced, resulting in more missclassification of MBs as tissue.","sentences":["Contrast-enhanced ultrasound (CEUS) presents distinct advantages in diagnostic echography.","Utilizing microbubbles (MBs) as conventional contrast agents enhances vascular visualization and organ perfusion, facilitating real-time, non-invasive procedures.","There is a current tendency to replace the traditional polydisperse MBs by novel monodisperse formulations in an attempt to optimize contrast enhancement and guarantee consistent behavior and reliable imaging outcomes.","This study investigates the contrast enhancement achieved by monodisperse MBs of different sizes, and their influence on nonlinear imaging artifacts observed in traditional CEUS.","To explore the differences between monodisperse and polydisperse populations without excessive experimentation, numerical simulations are employed for delivering precise, objective and expeditious results.","The Iterative Nonlinear Contrast Source (INCS) method has previously demonstrated its efficacy in simulating ultrasound propagation in large populations in which each bubble has individual properties and several orders of multiple scattering are significant.","Our findings in CEUS imaging indicate that scattering from resonant monodisperse microbubbles is 11.8 dB stronger than scattering from the polydisperse population.","Furthermore, the amplitude of nonlinear imaging artifacts downstream of the monodisperse population is 19.4 dB stronger compared to polydisperse suspension.","Investigating the impact of multiple scattering on polydisperse populations compared to various monodisperse suspensions reveals that monodisperse MBs are more effective contrast agents, especially when on resonance.","Despite the strong signal to noise ratio of monodisperse populations, the imaging artifacts due to nonlinear wave propagation are also enhanced, resulting in more missclassification of MBs as tissue."],"url":"http://arxiv.org/abs/2405.09263v1","category":"physics.med-ph"}
{"created":"2024-05-15 08:07:52","title":"Towards a universal QAOA protocol: Evidence of quantum advantage in solving combinatorial optimization problems","abstract":"The quantum approximate optimization algorithm (QAOA) is a promising algorithm for solving combinatorial optimization problems (COPs). In this algorithm, there are alternating layers consisting of a mixer and a problem Hamiltonian. Each layer $i=0,\\ldots,p-1$ is parameterized by $\\beta_i$ and $\\gamma_i$. How to find these parameters has been an open question with the majority of the research focused on finding them using classical algorithms. In this work, we present evidence that fixed linear ramp schedules constitute a universal set of QAOA parameters, i.e., a set of $\\gamma$ and $\\beta$ parameters that rapidly approximate the optimal solution, $x^*$, independently of the COP selected, and that the success probability of finding it, $probability(x^*)$, increases with the number of QAOA layers $p$. We simulate linear ramp QAOA protocols (LR-QAOA) involving up to $N_q=42$ qubits and $p = 400$ layers on random instances of 9 different COPs. The results suggest that $probability(x^*) \\approx 1/2^{(\\eta N_q / p)}$ for a constant $\\eta$. For example, when implementing LR-QAOA with $p=42$, the $probability(x^*)$ for 42-qubit Weighted MaxCut problems (W-MaxCut) increases from $2/2^{42}\\approx 10^{-13}$ to an average of 0.13. We compare LR-QAOA, simulated annealing (SA), and branch-and-bound (B\\&B) finding a fundamental improvement in LR-QAOA. We test LR-QAOA on real hardware using IonQ Aria, Quantinuum H2-1, IBM Brisbane, IBM Kyoto, and IBM Osaka, encoding random weighted MaxCut (W-MaxCut) problems from 5 to 109 qubits and $p=3$ to $100$. Even for the largest case, $N_q=109$ qubits and $p=100$, information about the LR-QAOA optimization protocol is present. The circuit involved requires 21200 CNOT gates. These results show that LR-QAOA effectively finds high-quality solutions for COPs and suggests an advantage of quantum computation for combinatorial optimization in the near future.","sentences":["The quantum approximate optimization algorithm (QAOA) is a promising algorithm for solving combinatorial optimization problems (COPs).","In this algorithm, there are alternating layers consisting of a mixer and a problem Hamiltonian.","Each layer $i=0,\\ldots,p-1$ is parameterized by $\\beta_i$ and $\\gamma_i$. How to find these parameters has been an open question with the majority of the research focused on finding them using classical algorithms.","In this work, we present evidence that fixed linear ramp schedules constitute a universal set of QAOA parameters, i.e., a set of $\\gamma$ and $\\beta$ parameters that rapidly approximate the optimal solution, $x^*$, independently of the COP selected, and that the success probability of finding it, $probability(x^*)$, increases with the number of QAOA layers $p$. We simulate linear ramp QAOA protocols (LR-QAOA) involving up to $N_q=42$ qubits and $p = 400$ layers on random instances of 9 different COPs.","The results suggest that $probability(x^*) \\approx 1/2^{(\\eta N_q / p)}$ for a constant $\\eta$. For example, when implementing LR-QAOA with $p=42$, the $probability(x^*)$ for 42-qubit Weighted MaxCut problems (W-MaxCut) increases from $2/2^{42}\\approx 10^{-13}$ to an average of 0.13.","We compare LR-QAOA, simulated annealing (SA), and branch-and-bound (B\\&B) finding a fundamental improvement in LR-QAOA.","We test LR-QAOA on real hardware using IonQ Aria, Quantinuum H2-1, IBM Brisbane, IBM Kyoto, and IBM Osaka, encoding random weighted MaxCut (W-MaxCut) problems from 5 to 109 qubits and $p=3$ to $100$. Even for the largest case, $N_q=109$ qubits and $p=100$, information about the LR-QAOA optimization protocol is present.","The circuit involved requires 21200 CNOT gates.","These results show that LR-QAOA effectively finds high-quality solutions for COPs and suggests an advantage of quantum computation for combinatorial optimization in the near future."],"url":"http://arxiv.org/abs/2405.09169v1","category":"quant-ph"}
{"created":"2024-05-15 06:19:39","title":"Hybrid Meta-Solving for Practical Quantum Computing","abstract":"The advent of quantum algorithms has initiated a discourse on the potential for quantum speedups for optimization problems. However, several factors still hinder a practical realization of the potential benefits. These include the lack of advanced, error-free quantum hardware, the absence of accessible software stacks for seamless integration and interaction, and the lack of methods that allow us to leverage the theoretical advantages to real-world use cases. This paper works towards the creation of an accessible hybrid software stack for solving optimization problems, aiming to create a fundamental platform that can utilize quantum technologies to enhance the solving process. We introduce a novel approach that we call Hybrid Meta-Solving, which combines classical and quantum optimization techniques to create customizable and extensible hybrid solvers. We decompose mathematical problems into multiple sub-problems that can be solved by classical or quantum solvers, and propose techniques to semi-automatically build the best solver for a given problem. Implemented in our ProvideQ toolbox prototype, Meta-Solving provides interactive workflows for accessing quantum computing capabilities. Our evaluation demonstrates the applicability of Meta-Solving in industrial use cases. It shows that we can reuse state-of-the-art classical algorithms and extend them with quantum computing techniques. Our approach is designed to be at least as efficient as state-of-the-art classical techniques, while having the potential to outperform them if future advances in the quantum domain are made.","sentences":["The advent of quantum algorithms has initiated a discourse on the potential for quantum speedups for optimization problems.","However, several factors still hinder a practical realization of the potential benefits.","These include the lack of advanced, error-free quantum hardware, the absence of accessible software stacks for seamless integration and interaction, and the lack of methods that allow us to leverage the theoretical advantages to real-world use cases.","This paper works towards the creation of an accessible hybrid software stack for solving optimization problems, aiming to create a fundamental platform that can utilize quantum technologies to enhance the solving process.","We introduce a novel approach that we call Hybrid Meta-Solving, which combines classical and quantum optimization techniques to create customizable and extensible hybrid solvers.","We decompose mathematical problems into multiple sub-problems that can be solved by classical or quantum solvers, and propose techniques to semi-automatically build the best solver for a given problem.","Implemented in our ProvideQ toolbox prototype, Meta-Solving provides interactive workflows for accessing quantum computing capabilities.","Our evaluation demonstrates the applicability of Meta-Solving in industrial use cases.","It shows that we can reuse state-of-the-art classical algorithms and extend them with quantum computing techniques.","Our approach is designed to be at least as efficient as state-of-the-art classical techniques, while having the potential to outperform them if future advances in the quantum domain are made."],"url":"http://arxiv.org/abs/2405.09115v1","category":"quant-ph"}
{"created":"2024-05-15 03:02:21","title":"Dim Small Target Detection and Tracking: A Novel Method Based on Temporal Energy Selective Scaling and Trajectory Association","abstract":"The detection and tracking of small targets in passive optical remote sensing (PORS) has broad applications. However, most of the previously proposed methods seldom utilize the abundant temporal features formed by target motion, resulting in poor detection and tracking performance for low signal-to-clutter ratio (SCR) targets. In this article, we analyze the difficulty based on spatial features and the feasibility based on temporal features of realizing effective detection. According to this analysis, we use a multi-frame as a detection unit and propose a detection method based on temporal energy selective scaling (TESS). Specifically, we investigated the composition of intensity temporal profiles (ITPs) formed by pixels on a multi-frame detection unit. For the target-present pixel, the target passing through the pixel will bring a weak transient disturbance on the ITP and introduce a change in the statistical properties of ITP. We use a well-designed function to amplify the transient disturbance, suppress the background and noise components, and output the trajectory of the target on the multi-frame detection unit. Subsequently, to solve the contradiction between the detection rate and the false alarm rate brought by the traditional threshold segmentation, we associate the temporal and spatial features of the output trajectory and propose a trajectory extraction method based on the 3D Hough transform. Finally, we model the trajectory of the target and propose a trajectory-based multi-target tracking method. Compared with the various state-of-the-art detection and tracking methods, experiments in multiple scenarios prove the superiority of our proposed methods.","sentences":["The detection and tracking of small targets in passive optical remote sensing (PORS) has broad applications.","However, most of the previously proposed methods seldom utilize the abundant temporal features formed by target motion, resulting in poor detection and tracking performance for low signal-to-clutter ratio (SCR) targets.","In this article, we analyze the difficulty based on spatial features and the feasibility based on temporal features of realizing effective detection.","According to this analysis, we use a multi-frame as a detection unit and propose a detection method based on temporal energy selective scaling (TESS).","Specifically, we investigated the composition of intensity temporal profiles (ITPs) formed by pixels on a multi-frame detection unit.","For the target-present pixel, the target passing through the pixel will bring a weak transient disturbance on the ITP and introduce a change in the statistical properties of ITP.","We use a well-designed function to amplify the transient disturbance, suppress the background and noise components, and output the trajectory of the target on the multi-frame detection unit.","Subsequently, to solve the contradiction between the detection rate and the false alarm rate brought by the traditional threshold segmentation, we associate the temporal and spatial features of the output trajectory and propose a trajectory extraction method based on the 3D Hough transform.","Finally, we model the trajectory of the target and propose a trajectory-based multi-target tracking method.","Compared with the various state-of-the-art detection and tracking methods, experiments in multiple scenarios prove the superiority of our proposed methods."],"url":"http://arxiv.org/abs/2405.09054v1","category":"cs.CV"}
{"created":"2024-05-15 02:04:22","title":"Entanglement Distribution Delay Optimization in Quantum Networks with Distillation","abstract":"Quantum networks (QNs) distribute entangled states to enable distributed quantum computing and sensing applications. However, in such QNs, quantum switches (QSs) have limited resources that are highly sensitive to noise and losses and must be carefully allocated to minimize entanglement distribution delay. In this paper, a QS resource allocation framework is proposed, which jointly optimizes the average entanglement distribution delay and entanglement distillation operations, to enhance the end-to-end (e2e) fidelity and satisfy minimum rate and fidelity requirements. The proposed framework considers realistic QN noise and includes the derivation of the analytical expressions for the average quantum memory decoherence noise parameter, and the resulting e2e fidelity after distillation. Finally, practical QN deployment aspects are considered, where QSs can control 1) nitrogen-vacancy (NV) center SPS types based on their isotopic decomposition, and 2) nuclear spin regions based on their distance and coupling strength with the electron spin of NV centers. A simulated annealing metaheuristic algorithm is proposed to solve the QS resource allocation optimization problem. Simulation results show that the proposed framework manages to satisfy all users rate and fidelity requirements, unlike existing distillation-agnostic (DA), minimal distillation (MD), and physics-agnostic (PA) frameworks which do not perform distillation, perform minimal distillation, and does not control the physics-based NV center characteristics, respectively. Furthermore, the proposed framework results in around 30% and 50% reductions in the average e2e entanglement distribution delay compared to existing PA and MD frameworks, respectively. Moreover, the proposed framework results in around 5%, 7%, and 11% reductions in the average e2e fidelity compared to existing DA, PA, and MD frameworks, respectively.","sentences":["Quantum networks (QNs) distribute entangled states to enable distributed quantum computing and sensing applications.","However, in such QNs, quantum switches (QSs) have limited resources that are highly sensitive to noise and losses and must be carefully allocated to minimize entanglement distribution delay.","In this paper, a QS resource allocation framework is proposed, which jointly optimizes the average entanglement distribution delay and entanglement distillation operations, to enhance the end-to-end (e2e) fidelity and satisfy minimum rate and fidelity requirements.","The proposed framework considers realistic QN noise and includes the derivation of the analytical expressions for the average quantum memory decoherence noise parameter, and the resulting e2e fidelity after distillation.","Finally, practical QN deployment aspects are considered, where QSs can control 1) nitrogen-vacancy (NV) center SPS types based on their isotopic decomposition, and 2) nuclear spin regions based on their distance and coupling strength with the electron spin of NV centers.","A simulated annealing metaheuristic algorithm is proposed to solve the QS resource allocation optimization problem.","Simulation results show that the proposed framework manages to satisfy all users rate and fidelity requirements, unlike existing distillation-agnostic (DA), minimal distillation (MD), and physics-agnostic (PA) frameworks which do not perform distillation, perform minimal distillation, and does not control the physics-based NV center characteristics, respectively.","Furthermore, the proposed framework results in around 30% and 50% reductions in the average e2e entanglement distribution delay compared to existing PA and MD frameworks, respectively.","Moreover, the proposed framework results in around 5%, 7%, and 11% reductions in the average e2e fidelity compared to existing DA, PA, and MD frameworks, respectively."],"url":"http://arxiv.org/abs/2405.09034v1","category":"quant-ph"}
{"created":"2024-05-14 20:12:17","title":"Parameter optimization comparison in QAOA using Stochastic Hill Climbing with Random Re-starts and Local Search with entangled and non-entangled mixing operators","abstract":"This study investigates the efficacy of Stochastic Hill Climbing with Random Restarts (SHC-RR) compared to Local Search (LS) strategies within the Quantum Approximate Optimization Algorithm (QAOA) framework across various problem models. Employing uniform parameter settings, including the number of restarts and SHC steps, we analyze LS with two distinct perturbation operations: multiplication and summation. Our comparative analysis encompasses multiple versions of max-cut and random Ising model (RI) problems, utilizing QAOA models with depths ranging from $1L$ to $3L$. These models incorporate diverse mixing operator configurations, which integrate $RX$ and $RY$ gates, and explore the effects of an entanglement stage within the mixing operator. We also used Quantum Fisher Information (QFI) to compare the different QAOA models, demonstrating the importance of the placement of the entanglement stage in the overall performance of QAOA. Additionally, we observed that the QFI values of previous parameters are not affected as the depth of the quantum circuit increases. Our results consistently show that SHC-RR outperforms LS approaches, showcasing superior efficacy despite its ostensibly simpler optimization mechanism. Furthermore, we observe that the inclusion of entanglement stages within mixing operators significantly impacts model performance, either enhancing or diminishing results depending on the specific problem context.","sentences":["This study investigates the efficacy of Stochastic Hill Climbing with Random Restarts (SHC-RR) compared to Local Search (LS) strategies within the Quantum Approximate Optimization Algorithm (QAOA) framework across various problem models.","Employing uniform parameter settings, including the number of restarts and SHC steps, we analyze LS with two distinct perturbation operations: multiplication and summation.","Our comparative analysis encompasses multiple versions of max-cut and random Ising model (RI) problems, utilizing QAOA models with depths ranging from $1L$ to $3L$. These models incorporate diverse mixing operator configurations, which integrate $RX$ and $RY$ gates, and explore the effects of an entanglement stage within the mixing operator.","We also used Quantum Fisher Information (QFI) to compare the different QAOA models, demonstrating the importance of the placement of the entanglement stage in the overall performance of QAOA.","Additionally, we observed that the QFI values of previous parameters are not affected as the depth of the quantum circuit increases.","Our results consistently show that SHC-RR outperforms LS approaches, showcasing superior efficacy despite its ostensibly simpler optimization mechanism.","Furthermore, we observe that the inclusion of entanglement stages within mixing operators significantly impacts model performance, either enhancing or diminishing results depending on the specific problem context."],"url":"http://arxiv.org/abs/2405.08941v1","category":"quant-ph"}
{"created":"2024-05-14 19:38:21","title":"Modular Invariant Hilltop Inflation","abstract":"In this paper we show that it is possible to achieve successful hilltop inflation in which the inflaton is identified as the modulus field in a modular invariant theory. The dilaton plays a crucial role in shaping the potential. Modular invariant gaugino condensation provides the mechanism for the modulus stabilisation after inflation. The inflationary trajectory lies on the lower boundary of the fundamental domain of the modulus field $\\tau$. Inflation starts near the fixed point $\\tau ={\\rm i}$, and ends at a point near $\\tau = \\omega$, which is the global de Sitter vacuum. We investigate the allowed parameter space for successful modular invariant hilltop inflation.","sentences":["In this paper we show that it is possible to achieve successful hilltop inflation in which the inflaton is identified as the modulus field in a modular invariant theory.","The dilaton plays a crucial role in shaping the potential.","Modular invariant gaugino condensation provides the mechanism for the modulus stabilisation after inflation.","The inflationary trajectory lies on the lower boundary of the fundamental domain of the modulus field $\\tau$. Inflation starts near the fixed point $\\tau ={\\rm i}$, and ends at a point near $\\tau = \\omega$, which is the global de Sitter vacuum.","We investigate the allowed parameter space for successful modular invariant hilltop inflation."],"url":"http://arxiv.org/abs/2405.08924v1","category":"hep-ph"}
{"created":"2024-05-14 19:36:54","title":"Minimal compact operators, subdifferential of the maximum eigenvalue and semi-definite programming","abstract":"We formulate the issue of minimality of self-adjoint operators on a Hilbert space as a semi-definite problem, linking the work by Overton in [1] to the characterization of minimal hermitian matrices. This motivates us to investigate the relationship between minimal self-adjoint operators and the subdifferential of the maximum eigenvalue, initially for matrices and subsequently for compact operators. In order to do it we obtain new formulas of subdifferentials of maximum eigenvalues of compact operators that become useful in these optimization problems. Additionally, we provide formulas for the minimizing diagonals of rank one self-adjoint operators, a result that might be applied for numerical large-scale eigenvalue optimization.   [1] On minimizing the maximum eigenvalue of a symmetric matrix, SIAM J. Matrix Anal. Appl.9 (1988), no 4, 905-918","sentences":["We formulate the issue of minimality of self-adjoint operators on a Hilbert space as a semi-definite problem, linking the work by Overton in [1] to the characterization of minimal hermitian matrices.","This motivates us to investigate the relationship between minimal self-adjoint operators and the subdifferential of the maximum eigenvalue, initially for matrices and subsequently for compact operators.","In order to do it we obtain new formulas of subdifferentials of maximum eigenvalues of compact operators that become useful in these optimization problems.","Additionally, we provide formulas for the minimizing diagonals of rank one self-adjoint operators, a result that might be applied for numerical large-scale eigenvalue optimization.   ","[1] On minimizing the maximum eigenvalue of a symmetric matrix, SIAM J. Matrix Anal.","Appl.9 (1988), no 4, 905-918"],"url":"http://arxiv.org/abs/2405.08923v1","category":"math.FA"}
{"created":"2024-05-14 19:34:11","title":"Is every triangle a trajectory of an elliptical billiard?","abstract":"Using Marden's Theorem from geometric theory of polynomials, we show that for every triangle there is a unique ellipse such that the triangle is a billiard trajectory within that ellipse. Since $3$-periodic trajectories of billiards within ellipses are examples of the Poncelet polygons, our considerations provide a new insight into the relationship between Marden's Theorem and the Poncelet Porism, two gems of exceptional classical beauty. We also show that every parallelogram is a billiard trajectory within a unique ellipse. We prove a similar result for the self-intersecting polygonal lines consisting of two pairs of congruent sides, named ``Darboux butterflies\". In each of three considered cases, we effectively calculate the foci of the boundary ellipses.","sentences":["Using Marden's Theorem from geometric theory of polynomials, we show that for every triangle there is a unique ellipse such that the triangle is a billiard trajectory within that ellipse.","Since $3$-periodic trajectories of billiards within ellipses are examples of the Poncelet polygons, our considerations provide a new insight into the relationship between Marden's Theorem and the Poncelet Porism, two gems of exceptional classical beauty.","We also show that every parallelogram is a billiard trajectory within a unique ellipse.","We prove a similar result for the self-intersecting polygonal lines consisting of two pairs of congruent sides, named ``Darboux butterflies\".","In each of three considered cases, we effectively calculate the foci of the boundary ellipses."],"url":"http://arxiv.org/abs/2405.08922v1","category":"math.DS"}
{"created":"2024-05-14 19:02:33","title":"ADA-Track: End-to-End Multi-Camera 3D Multi-Object Tracking with Alternating Detection and Association","abstract":"Many query-based approaches for 3D Multi-Object Tracking (MOT) adopt the tracking-by-attention paradigm, utilizing track queries for identity-consistent detection and object queries for identity-agnostic track spawning. Tracking-by-attention, however, entangles detection and tracking queries in one embedding for both the detection and tracking task, which is sub-optimal. Other approaches resemble the tracking-by-detection paradigm, detecting objects using decoupled track and detection queries followed by a subsequent association. These methods, however, do not leverage synergies between the detection and association task. Combining the strengths of both paradigms, we introduce ADA-Track, a novel end-to-end framework for 3D MOT from multi-view cameras. We introduce a learnable data association module based on edge-augmented cross-attention, leveraging appearance and geometric features. Furthermore, we integrate this association module into the decoder layer of a DETR-based 3D detector, enabling simultaneous DETR-like query-to-image cross-attention for detection and query-to-query cross-attention for data association. By stacking these decoder layers, queries are refined for the detection and association task alternately, effectively harnessing the task dependencies. We evaluate our method on the nuScenes dataset and demonstrate the advantage of our approach compared to the two previous paradigms. Code is available at https://github.com/dsx0511/ADA-Track.","sentences":["Many query-based approaches for 3D Multi-Object Tracking (MOT) adopt the tracking-by-attention paradigm, utilizing track queries for identity-consistent detection and object queries for identity-agnostic track spawning.","Tracking-by-attention, however, entangles detection and tracking queries in one embedding for both the detection and tracking task, which is sub-optimal.","Other approaches resemble the tracking-by-detection paradigm, detecting objects using decoupled track and detection queries followed by a subsequent association.","These methods, however, do not leverage synergies between the detection and association task.","Combining the strengths of both paradigms, we introduce ADA-Track, a novel end-to-end framework for 3D MOT from multi-view cameras.","We introduce a learnable data association module based on edge-augmented cross-attention, leveraging appearance and geometric features.","Furthermore, we integrate this association module into the decoder layer of a DETR-based 3D detector, enabling simultaneous DETR-like query-to-image cross-attention for detection and query-to-query cross-attention for data association.","By stacking these decoder layers, queries are refined for the detection and association task alternately, effectively harnessing the task dependencies.","We evaluate our method on the nuScenes dataset and demonstrate the advantage of our approach compared to the two previous paradigms.","Code is available at https://github.com/dsx0511/ADA-Track."],"url":"http://arxiv.org/abs/2405.08909v1","category":"cs.CV"}
{"created":"2024-05-14 18:26:52","title":"An Interoperable Multi Objective Batch Bayesian Optimization Framework for High Throughput Materials Discovery","abstract":"In this study, we introduce a groundbreaking framework for materials discovery, we efficiently navigate a vast phase space of material compositions by leveraging Batch Bayesian statistics in order to achieve specific performance objectives. This approach addresses the challenge of identifying optimal materials from an untenably large array of possibilities in a reasonable timeframe with high confidence. Crucially, our batchwise methods align seamlessly with existing material processing infrastructure for synthesizing and characterizing materials. By applying this framework to a specific high entropy alloy system, we demonstrate its versatility and robustness in optimizing properties like strain hardening, hardness, and strain rate sensitivity. The fact that the Bayesian model is adept in refining and expanding the property Pareto front highlights its broad applicability across various materials, including steels, shape memory alloys, ceramics, and composites. This study advances the field of materials science and sets a new benchmark for material discovery methodologies. By proving the effectiveness of Bayesian optimization, we showcase its potential to redefine the landscape of materials discovery.","sentences":["In this study, we introduce a groundbreaking framework for materials discovery, we efficiently navigate a vast phase space of material compositions by leveraging Batch Bayesian statistics in order to achieve specific performance objectives.","This approach addresses the challenge of identifying optimal materials from an untenably large array of possibilities in a reasonable timeframe with high confidence.","Crucially, our batchwise methods align seamlessly with existing material processing infrastructure for synthesizing and characterizing materials.","By applying this framework to a specific high entropy alloy system, we demonstrate its versatility and robustness in optimizing properties like strain hardening, hardness, and strain rate sensitivity.","The fact that the Bayesian model is adept in refining and expanding the property Pareto front highlights its broad applicability across various materials, including steels, shape memory alloys, ceramics, and composites.","This study advances the field of materials science and sets a new benchmark for material discovery methodologies.","By proving the effectiveness of Bayesian optimization, we showcase its potential to redefine the landscape of materials discovery."],"url":"http://arxiv.org/abs/2405.08900v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-14 13:17:19","title":"Parametrically encircled higher-order exceptional points in anti-parity-time symmetric optical microcavities","abstract":"The fascinating realm of non-Hermitian physics with the interplay of parity (P) and time-reversal (T) symmetry has been witnessing immense attention in exploring unconventional physics at Exceptional Point (EP) singularities. Particularly, the physics of PT-symmetry, anti-PT (APT)-symmetry, and the emergence of EPs have ignited fervor in photonics. Beyond the conventional relation between EP and PT-symmetric phase transitions, this study delves into hosting higher-order EPs in a specially designed APT-symmetric Fabry-P\\'erot-type microcavity. We unveil the captivating physics of the parametric encirclement schemes to explore the branch-point behaviors of EPs up to order three in terms of successive state-flipping, while optimizing the designed cavity under APT-symmetric constraints. The insights from our findings are poised to boost research in optical metamaterials, meeting the demands of APT-symmetry and paving the way for a novel class of photonic devices.","sentences":["The fascinating realm of non-Hermitian physics with the interplay of parity (P) and time-reversal (T) symmetry has been witnessing immense attention in exploring unconventional physics at Exceptional Point (EP) singularities.","Particularly, the physics of PT-symmetry, anti-PT (APT)-symmetry, and the emergence of EPs have ignited fervor in photonics.","Beyond the conventional relation between EP and PT-symmetric phase transitions, this study delves into hosting higher-order EPs in a specially designed APT-symmetric Fabry-P\\'erot-type microcavity.","We unveil the captivating physics of the parametric encirclement schemes to explore the branch-point behaviors of EPs up to order three in terms of successive state-flipping, while optimizing the designed cavity under APT-symmetric constraints.","The insights from our findings are poised to boost research in optical metamaterials, meeting the demands of APT-symmetry and paving the way for a novel class of photonic devices."],"url":"http://arxiv.org/abs/2405.08850v1","category":"physics.optics"}
{"created":"2024-05-14 07:34:35","title":"Femtosecond laser writing of depressed cladding waveguides in sapphire","abstract":"A promising solution for scalable integrated optics of trapped-ion quantum processors are curved waveguides guiding visible light within sapphire bulk material. To the best of our knowledge, no curved waveguides were investigated in sapphire so far and no waveguides for visible light in undoped planar sapphire substrates were reported. Here, we demonstrate femtosecond laser writing of depressed cladding waveguides in sapphire. Laser parameters such as pulse energy, pulse duration, and repetition rate, as well as waveguide geometry parameters were optimized to guide 728 nm light. This resulted in single-mode waveguides with a propagation loss of 1.9 (3) dB/cm. The investigation of curved waveguides showed a sharp increase in total loss for curvature radii below 15 mm. Our results demonstrate the potential of femtosecond laser writing as a powerful technique for creating integrated optical waveguides in the volume of sapphire substrates. Such waveguides could be a building block for integrated optics in trapped ion quantum processors.","sentences":["A promising solution for scalable integrated optics of trapped-ion quantum processors are curved waveguides guiding visible light within sapphire bulk material.","To the best of our knowledge, no curved waveguides were investigated in sapphire so far and no waveguides for visible light in undoped planar sapphire substrates were reported.","Here, we demonstrate femtosecond laser writing of depressed cladding waveguides in sapphire.","Laser parameters such as pulse energy, pulse duration, and repetition rate, as well as waveguide geometry parameters were optimized to guide 728 nm light.","This resulted in single-mode waveguides with a propagation loss of 1.9 (3) dB/cm.","The investigation of curved waveguides showed a sharp increase in total loss for curvature radii below 15 mm.","Our results demonstrate the potential of femtosecond laser writing as a powerful technique for creating integrated optical waveguides in the volume of sapphire substrates.","Such waveguides could be a building block for integrated optics in trapped ion quantum processors."],"url":"http://arxiv.org/abs/2405.08840v1","category":"physics.optics"}
{"created":"2024-05-15 17:47:03","title":"Supersolidity in ultra-cold dipolar gases","abstract":"Can a gas behave like a crystal? Supersolidity is an intriguing and challenging state of matter which combines key features of superfluids and crystals. Predicted a long time ago, its experimental realization has been recently achieved in Bose-Einstein condensed (BEC) atomic gases inside optical resonators, spin-orbit coupled BEC's and atomic gases interacting with long range dipolar forces. The activity on dipolar gases has been particularly vibrant in the last few years. This perspective article summarizes the main experimental and theoretical achievements concerning supersolidity in the field of dipolar gases, like the observation of the density modulations caused by the spontaneous breaking of translational invariance, the effects of coherence and the occurrence of novel Goldstone modes. A series of important issues for the future experimental and theoretical research are outlined including, among others, the possible realization of quantized vortices inside these novel crystal structure, the role of dimensionality, the characterisation of the crystal properties and the nature of the phase transitions.   At the end a brief overview on some other (mainly cold atomic) platforms, where supersolidity has been observed or where supersolidty is expected to emerge is provided.","sentences":["Can a gas behave like a crystal?","Supersolidity is an intriguing and challenging state of matter which combines key features of superfluids and crystals.","Predicted a long time ago, its experimental realization has been recently achieved in Bose-Einstein condensed (BEC) atomic gases inside optical resonators, spin-orbit coupled BEC's and atomic gases interacting with long range dipolar forces.","The activity on dipolar gases has been particularly vibrant in the last few years.","This perspective article summarizes the main experimental and theoretical achievements concerning supersolidity in the field of dipolar gases, like the observation of the density modulations caused by the spontaneous breaking of translational invariance, the effects of coherence and the occurrence of novel Goldstone modes.","A series of important issues for the future experimental and theoretical research are outlined including, among others, the possible realization of quantized vortices inside these novel crystal structure, the role of dimensionality, the characterisation of the crystal properties and the nature of the phase transitions.   ","At the end a brief overview on some other (mainly cold atomic) platforms, where supersolidity has been observed or where supersolidty is expected to emerge is provided."],"url":"http://arxiv.org/abs/2405.09537v1","category":"cond-mat.quant-gas"}
{"created":"2024-05-15 17:44:41","title":"Five-dimensional spinor helicity for all masses and spins","abstract":"We develop a spinor helicity formalism for five-dimensional scattering amplitudes of any mass and spin configuration. While five-dimensional spinor helicity variables have been previously studied in the context of N=2,4 supersymmetric Yang-Mills scattering amplitudes with spin less than two arXiv:2202.08257, we propose an alternative viewpoint that stems from d-dimensional spinor helicity variables avoiding the use of the exceptional low-dimensional isomorphism $SO(4,1) \\cong USp(2,2)$ and the decomposition of a massive momentum into the sum of two massless momenta. By enumerating all possible independent little group tensors, we systematically build the full space of five-dimensional three-point tree-level scattering amplitudes for any configuration of spins and masses. Furthermore, we provide a prescription for computing the high energy limit of scattering amplitudes written in our spinor helicity variables. We also expect that our formalism will be applicable to effective field theories with higher spin, in particular, the scattering of highly spinning black holes in five dimensions.","sentences":["We develop a spinor helicity formalism for five-dimensional scattering amplitudes of any mass and spin configuration.","While five-dimensional spinor helicity variables have been previously studied in the context of N=2,4 supersymmetric Yang-Mills scattering amplitudes with spin less than two arXiv:2202.08257, we propose an alternative viewpoint that stems from d-dimensional spinor helicity variables avoiding the use of the exceptional low-dimensional isomorphism $SO(4,1) \\cong USp(2,2)$ and the decomposition of a massive momentum into the sum of two massless momenta.","By enumerating all possible independent little group tensors, we systematically build the full space of five-dimensional three-point tree-level scattering amplitudes for any configuration of spins and masses.","Furthermore, we provide a prescription for computing the high energy limit of scattering amplitudes written in our spinor helicity variables.","We also expect that our formalism will be applicable to effective field theories with higher spin, in particular, the scattering of highly spinning black holes in five dimensions."],"url":"http://arxiv.org/abs/2405.09533v1","category":"hep-th"}
{"created":"2024-05-15 17:35:29","title":"Forward & Far-Forward Heavy Hadrons with JETHAD: A High-energy Viewpoint","abstract":"Inspired by the recent finding that semi-inclusive detections of heavy hadrons exhibit fair stabilization patterns in high-energy resummed distributions against (missing) higher-order corrections, we review and extend our studies on the hadroproduction of light and heavy hadrons tagged in forward and far-forward rapidity ranges. We analyze the NLL/NLO+ behavior of rapidity rates and angular multiplicities via the JETHAD method, where the resummation of next-to-leading energy logarithms and beyond is consistently embodied in the collinear picture. We explore kinematic regions that are within LHC typical acceptances, as well as novel sectors accessible thanks the combined tagging of a far-forward light or heavy hadron at future Forward Physics Facilities and a of central particle at LHC experiments via a precise timing-coincidence setup.","sentences":["Inspired by the recent finding that semi-inclusive detections of heavy hadrons exhibit fair stabilization patterns in high-energy resummed distributions against (missing) higher-order corrections, we review and extend our studies on the hadroproduction of light and heavy hadrons tagged in forward and far-forward rapidity ranges.","We analyze the NLL/NLO+ behavior of rapidity rates and angular multiplicities via the JETHAD method, where the resummation of next-to-leading energy logarithms and beyond is consistently embodied in the collinear picture.","We explore kinematic regions that are within LHC typical acceptances, as well as novel sectors accessible thanks the combined tagging of a far-forward light or heavy hadron at future Forward Physics Facilities and a of central particle at LHC experiments via a precise timing-coincidence setup."],"url":"http://arxiv.org/abs/2405.09526v1","category":"hep-ph"}
{"created":"2024-05-15 17:18:52","title":"Nonperturbative aspects of the electromagnetic pion form factor at high energies","abstract":"The structure of hadronic form factors at high energies and their deviations from perturbative quantum chromodynamics provide insight on nonperturbative dynamics. Using an approach that is consistent with dispersion relations, we construct a model that simultaneously accounts for the pion wave function, gluonic exchanges, and quark Reggeization. In particular, we find that quark Reggeization can be investigated at high energies by studying scaling violation of the form factor.","sentences":["The structure of hadronic form factors at high energies and their deviations from perturbative quantum chromodynamics provide insight on nonperturbative dynamics.","Using an approach that is consistent with dispersion relations, we construct a model that simultaneously accounts for the pion wave function, gluonic exchanges, and quark Reggeization.","In particular, we find that quark Reggeization can be investigated at high energies by studying scaling violation of the form factor."],"url":"http://arxiv.org/abs/2405.09517v1","category":"hep-ph"}
{"created":"2024-05-15 17:02:47","title":"The Instrumental Variable Model with Categorical Instrument, Treatment and Outcome","abstract":"Instrumental variable models are central to the inference of causal effects in many settings. We consider the instrumental variable model with discrete variables where the instrument (Z), exposure (X) and outcome (Y) take Q, K, and M levels respectively. We assume that the instrument is randomized and that there is no direct effect of Z on Y so that Y(x,z) = Y(x). We first provide a simple characterization of the set of joint distributions of the potential outcomes P(Y(x=1), ..., Y(x=K)) compatible with a given observed distribution P(X, Y | Z). We then discuss the variation (in)dependence property of the marginal probability distribution of the potential outcomes P(Y(x=1)), ..., P(Y(x=K)) which has direct implications for partial identification of average causal effect contrasts such as E[Y(x=i) - Y(x=j)]. We also include simulation results on the volume of the observed distributions not compatible with the IV model as K and Q change.","sentences":["Instrumental variable models are central to the inference of causal effects in many settings.","We consider the instrumental variable model with discrete variables where the instrument (Z), exposure (X) and outcome (Y) take Q, K, and M levels respectively.","We assume that the instrument is randomized and that there is no direct effect of Z on Y so that Y(x,z) = Y(x).","We first provide a simple characterization of the set of joint distributions of the potential outcomes P(Y(x=1), ..., Y(x=K)) compatible with a given observed distribution P(X, Y | Z).","We then discuss the variation (in)dependence property of the marginal probability distribution of the potential outcomes P(Y(x=1)), ..., P(Y(x=K)) which has direct implications for partial identification of average causal effect contrasts such as E[Y(x=i) - Y(x=j)].","We also include simulation results on the volume of the observed distributions not compatible with the IV model as K and Q change."],"url":"http://arxiv.org/abs/2405.09510v1","category":"math.ST"}
{"created":"2024-05-15 17:02:00","title":"Double Robustness of Local Projections and Some Unpleasant VARithmetic","abstract":"We consider impulse response inference in a locally misspecified stationary vector autoregression (VAR) model. The conventional local projection (LP) confidence interval has correct coverage even when the misspecification is so large that it can be detected with probability approaching 1. This follows from a \"double robustness\" property analogous to that of modern estimators for partially linear regressions. In contrast, VAR confidence intervals dramatically undercover even for misspecification so small that it is difficult to detect statistically and cannot be ruled out based on economic theory. This is because of a \"no free lunch\" result for VARs: the worst-case bias and coverage distortion are small if, and only if, the variance is close to that of LP. While VAR coverage can be restored by using a bias-aware critical value or a large lag length, the resulting confidence interval tends to be at least as wide as the LP interval.","sentences":["We consider impulse response inference in a locally misspecified stationary vector autoregression (VAR) model.","The conventional local projection (LP) confidence interval has correct coverage even when the misspecification is so large that it can be detected with probability approaching 1.","This follows from a \"double robustness\" property analogous to that of modern estimators for partially linear regressions.","In contrast, VAR confidence intervals dramatically undercover even for misspecification so small that it is difficult to detect statistically and cannot be ruled out based on economic theory.","This is because of a \"no free lunch\" result for VARs: the worst-case bias and coverage distortion are small if, and only if, the variance is close to that of LP.","While VAR coverage can be restored by using a bias-aware critical value or a large lag length, the resulting confidence interval tends to be at least as wide as the LP interval."],"url":"http://arxiv.org/abs/2405.09509v1","category":"econ.EM"}
{"created":"2024-05-15 16:55:59","title":"The asymmetry of dawn: evidence for asymmetric reionization histories from a joint analysis of cosmic microwave background and astrophysical data","abstract":"We show that by jointly fitting cosmic microwave background (CMB) and astrophysical data - a compilation of UV luminosity data from the Hubble Frontier Field and neutral hydrogen data from distant sources-, we can infer on the shape of the evolution of the ionized hydrogen fraction with redshift in addition to constraining the average optical depth $\\tau$.For this purpose, we introduce here a novel extended model that includes hydrogen ionization histories which are monotonic with redshift, but allow for an asymmetry as indicated from our previous works on a free reconstruction of reionization. By using our baseline data combination, we obtain $\\tau=0.0542^{+0.0017}_{-0.0028}$, consistent with our previous works and tighter than the one inferred by Planck 2018 data because of the combination of CMB with astrophysical data. We find that the symmetric hypothesis within our parametrization is disfavoured at 4 $\\sigma$.We test our findings by using alternative likelihoods for CMB polarization at low multipoles, i.e. based on the 2020 reprocessing of Planck HFI data or on the joint analysis of WMAP and Planck LFI data, obtaining consistent results that disfavour the symmetric hypothesis of the reionization history at high statistical significant level.These results will be further tested by more precise astrophysical data such as from JWST and Euclid deep fields.","sentences":["We show that by jointly fitting cosmic microwave background (CMB) and astrophysical data - a compilation of UV luminosity data from the Hubble Frontier Field and neutral hydrogen data from distant sources-, we can infer on the shape of the evolution of the ionized hydrogen fraction with redshift in addition to constraining the average optical depth $\\tau$.For this purpose, we introduce here a novel extended model that includes hydrogen ionization histories which are monotonic with redshift, but allow for an asymmetry as indicated from our previous works on a free reconstruction of reionization.","By using our baseline data combination, we obtain $\\tau=0.0542^{+0.0017}_{-0.0028}$, consistent with our previous works and tighter than the one inferred by Planck 2018 data because of the combination of CMB with astrophysical data.","We find that the symmetric hypothesis within our parametrization is disfavoured at 4 $\\sigma$.We test our findings by using alternative likelihoods for CMB polarization at low multipoles, i.e. based on the 2020 reprocessing of Planck HFI data or on the joint analysis of WMAP and Planck LFI data, obtaining consistent results that disfavour the symmetric hypothesis of the reionization history at high statistical significant level.","These results will be further tested by more precise astrophysical data such as from JWST and Euclid deep fields."],"url":"http://arxiv.org/abs/2405.09506v1","category":"astro-ph.CO"}
{"created":"2024-05-15 16:52:38","title":"Resonances in $D^0\\to\u03c0^+\u03c0^-\\ell^+\\ell^-$ and sensitivity to New Physics","abstract":"The study of processes involving the weak decays of the charm quark offers unique possibilities to test the Standard Model from the up-type sector and to probe different New Physics scenarios. In here we focus on the rare decays $D^0\\to\\pi^+\\pi^-\\ell^+\\ell^-$. The effectiveness of the Glashow-Iliopoulos-Maiani mechanism in charm results in a series of observables that serve as null tests of the Standard Model. Simultaneously, the dominant Standard Model contribution to such decays comes from long-distance dynamics. Following previous literature we describe this with the mediation of resonances. Motivated by recent experimental results on analogous semileptonic decays we investigate the effect of the $S$-wave of the pion pair by considering the scalar resonance $f_0(500)$. The experimental mass distributions and angular observables are described well with our model and there is a significant improvement on the agreement when accounting for $f_0(500)$, which amounts to around 20\\% of the total branching ratio. Furthermore, we propose and estimate the size of a series of additional observables to be measured, which can help probe the $S$-wave and act as complementary null tests of the Standard Model.","sentences":["The study of processes involving the weak decays of the charm quark offers unique possibilities to test the Standard Model from the up-type sector and to probe different New Physics scenarios.","In here we focus on the rare decays $D^0\\to\\pi^+\\pi^-\\ell^+\\ell^-$. The effectiveness of the Glashow-Iliopoulos-Maiani mechanism in charm results in a series of observables that serve as null tests of the Standard Model.","Simultaneously, the dominant Standard Model contribution to such decays comes from long-distance dynamics.","Following previous literature we describe this with the mediation of resonances.","Motivated by recent experimental results on analogous semileptonic decays we investigate the effect of the $S$-wave of the pion pair by considering the scalar resonance $f_0(500)$. The experimental mass distributions and angular observables are described well with our model and there is a significant improvement on the agreement when accounting for $f_0(500)$, which amounts to around 20\\% of the total branching ratio.","Furthermore, we propose and estimate the size of a series of additional observables to be measured, which can help probe the $S$-wave and act as complementary null tests of the Standard Model."],"url":"http://arxiv.org/abs/2405.09502v1","category":"hep-ph"}
{"created":"2024-05-15 16:17:57","title":"Active Galactic Nuclei and STaR fOrmation in Nearby Galaxies (AGNSTRONG). I. Sample and Strategy","abstract":"We introduce our project, AGNSTRONG (Active Galactic Nuclei and STaR fOrmation in Nearby Galaxies). Our research goals encompass investigating the kinematic properties of ionized and molecular gas outflows, understanding the impact of AGN feedback, and exploring the coevolution dynamics between AGN strength activity and star formation activity. We aim to conduct a thorough analysis to determine whether there is an increase or suppression in SFRs among targets with and without powerful relativistic jets. Our sample consists of 35 nearby AGNs with and without powerful relativistic jet detections. Utilizing sub-millimeter (sub-mm) continuum observations at 450 {\\mu}m and 850 {\\mu}m from SCUBA-2 at the James Clerk Maxwell Telescope, we determine star-formation rates (SFRs) for our sources using spectral energy distribution (SED) fitting models. Additionally, we employ high-quality, spatially resolved spectra from UV-optical to near-infrared bands obtained with the Double Spectrograph and Triple Spectrograph mounted on the 200-inch Hale telescope at Palomar Observatory to study their multiphase gas outflow properties. This paper presents an overview of our sample selection methodology, research strategy, and initial results of our project. We find that the SFRs determined without including the sub-mm data in the SED fitting are overestimated by approximately 0.08 dex compared to those estimated with the inclusion of sub-mm data. Additionally, we compare the estimated SFRs in our work with those traced by the 4000{\\AA} break, as provided by the MPA-JHU catalog. We find that our determined SFRs are systematically higher than those traced by the 4000{\\AA} break. Finally, we outline our future research plans.","sentences":["We introduce our project, AGNSTRONG (Active Galactic Nuclei and STaR fOrmation in Nearby Galaxies).","Our research goals encompass investigating the kinematic properties of ionized and molecular gas outflows, understanding the impact of AGN feedback, and exploring the coevolution dynamics between AGN strength activity and star formation activity.","We aim to conduct a thorough analysis to determine whether there is an increase or suppression in SFRs among targets with and without powerful relativistic jets.","Our sample consists of 35 nearby AGNs with and without powerful relativistic jet detections.","Utilizing sub-millimeter (sub-mm) continuum observations at 450 {\\mu}m and 850 {\\mu}m from SCUBA-2 at the James Clerk Maxwell Telescope, we determine star-formation rates (SFRs) for our sources using spectral energy distribution (SED) fitting models.","Additionally, we employ high-quality, spatially resolved spectra from UV-optical to near-infrared bands obtained with the Double Spectrograph and Triple Spectrograph mounted on the 200-inch Hale telescope at Palomar Observatory to study their multiphase gas outflow properties.","This paper presents an overview of our sample selection methodology, research strategy, and initial results of our project.","We find that the SFRs determined without including the sub-mm data in the SED fitting are overestimated by approximately 0.08 dex compared to those estimated with the inclusion of sub-mm data.","Additionally, we compare the estimated SFRs in our work with those traced by the 4000{\\AA} break, as provided by the MPA-JHU catalog.","We find that our determined SFRs are systematically higher than those traced by the 4000{\\AA} break.","Finally, we outline our future research plans."],"url":"http://arxiv.org/abs/2405.09478v1","category":"astro-ph.GA"}
{"created":"2024-05-15 15:34:50","title":"Constraining the Low-Mass End of the Black Hole Mass Function and the Active Fraction of the Intermediate-mass Black Holes","abstract":"We investigate the black hole mass function (BHMF) and the Eddington ratio distribution function (ERDF), focusing on the intermediate-mass black holes (IMBHs) with masses down to $M_{\\bullet}\\sim10^4 M_\\odot$. Based on the AGNs with a detected broad H$\\alpha$ emission line, we construct a sample of 14,242 AGNs at redshift $z<0.35$, including 243 IMBHs with $M_{\\bullet}<10^6 M_\\odot$. By jointly modeling the BHMF and ERDF via the maximum posterior estimation, we find that the BHMF peaks at $\\sim$$10^{6} M_\\odot$ and exhibits a relatively constant value of $10^{-4}\\,\\mathrm{Mpc^{-3}\\,dex^{-1}}$ at the low-mass end. By comparing the derived BHMF of type 1 AGNs with the galaxy mass function based on the updated black hole mass - host galaxy stellar mass relation, we derive the active fraction. We also determine the active fraction for all AGNs using the upper and lower limit of the type 1 fraction. The active fraction decreases from 15%-40% for massive galaxies ($M_\\star>10^{10} M_\\odot$) to lower than $\\sim$2% for dwarf galaxies with $M_\\star\\sim10^8 M_\\odot$. These results suggest that the black hole occupation fraction is expected to be $\\sim$50% for low-mass galaxies ($M_\\star\\sim10^{8.5} M_\\odot$-$10^9 M_\\odot$) if the duty cycle is similar between intermediate mass and supermassive black holes.","sentences":["We investigate the black hole mass function (BHMF) and the Eddington ratio distribution function (ERDF), focusing on the intermediate-mass black holes (IMBHs) with masses down to $M_{\\bullet}\\sim10^4 M_\\odot$. Based on the AGNs with a detected broad H$\\alpha$ emission line, we construct a sample of 14,242 AGNs at redshift $z<0.35$, including 243 IMBHs with $M_{\\bullet}<10^6 M_\\odot$. By jointly modeling the BHMF and ERDF via the maximum posterior estimation, we find that the BHMF peaks at $\\sim$$10^{6} M_\\odot$ and exhibits a relatively constant value of $10^{-4}\\,\\mathrm{Mpc^{-3}\\,dex^{-1}}$ at the low-mass end.","By comparing the derived BHMF of type 1 AGNs with the galaxy mass function based on the updated black hole mass - host galaxy stellar mass relation, we derive the active fraction.","We also determine the active fraction for all AGNs using the upper and lower limit of the type 1 fraction.","The active fraction decreases from 15%-40% for massive galaxies ($M_\\star>10^{10} M_\\odot$) to lower than $\\sim$2% for dwarf galaxies with $M_\\star\\sim10^8 M_\\odot$. These results suggest that the black hole occupation fraction is expected to be $\\sim$50% for low-mass galaxies ($M_\\star\\sim10^{8.5} M_\\odot$-$10^9 M_\\odot$) if the duty cycle is similar between intermediate mass and supermassive black holes."],"url":"http://arxiv.org/abs/2405.09441v1","category":"astro-ph.GA"}
{"created":"2024-05-15 15:24:28","title":"Analytic forms for the $e^+e^-$ annihilation cross sections around a resonance including initial state radiation","abstract":"The exact analytic form of cross sections including initial state radiation with Kuraev-Fadin radiative function are obtained for $e^+e^-$ annihilation around a resonance. Despite accounting for vacuum polarization and center-of-mass energy spread effects, the precision remains below 0.1\\%, meeting the accuracy requirements of quantum electrodynamics corrections up to $\\mathcal{O}(\\alpha^2)$. The analytic forms lead to an enhancement in the precision of experimental measurements of physical parameters, such as branching fractions, and demonstrate significantly improved computational efficiency in the regression procedures.","sentences":["The exact analytic form of cross sections including initial state radiation with Kuraev-Fadin radiative function are obtained for $e^+e^-$ annihilation around a resonance.","Despite accounting for vacuum polarization and center-of-mass energy spread effects, the precision remains below 0.1\\%, meeting the accuracy requirements of quantum electrodynamics corrections up to $\\mathcal{O}(\\alpha^2)$. The analytic forms lead to an enhancement in the precision of experimental measurements of physical parameters, such as branching fractions, and demonstrate significantly improved computational efficiency in the regression procedures."],"url":"http://arxiv.org/abs/2405.09432v1","category":"hep-ph"}
{"created":"2024-05-15 15:18:46","title":"Robust Covariance-Based Activity Detection for Massive Access","abstract":"The wireless channel is undergoing continuous changes, and the block-fading assumption, despite its popularity in theoretical contexts, never holds true in practical scenarios. This discrepancy is particularly critical for user activity detection in grant-free random access, where joint processing across multiple resource blocks is usually undesirable. In this paper, we propose employing a low-dimensional approximation of the channel to capture variations over time and frequency and robustify activity detection algorithms. This approximation entails projecting channel fading vectors onto their principal directions to minimize the approximation order. Through numerical examples, we demonstrate a substantial performance improvement achieved by the resulting activity detection algorithm.","sentences":["The wireless channel is undergoing continuous changes, and the block-fading assumption, despite its popularity in theoretical contexts, never holds true in practical scenarios.","This discrepancy is particularly critical for user activity detection in grant-free random access, where joint processing across multiple resource blocks is usually undesirable.","In this paper, we propose employing a low-dimensional approximation of the channel to capture variations over time and frequency and robustify activity detection algorithms.","This approximation entails projecting channel fading vectors onto their principal directions to minimize the approximation order.","Through numerical examples, we demonstrate a substantial performance improvement achieved by the resulting activity detection algorithm."],"url":"http://arxiv.org/abs/2405.09425v1","category":"cs.IT"}
{"created":"2024-05-15 15:15:01","title":"Exciton self-trapping in twisted hexagonal boron nitride homostructures","abstract":"One of the main interests of 2D materials is their ability to be assembled with many degrees of freedom for tuning and manipulating excitonic properties. There is a need to understand how the structure of the interfaces between atomic layers influences exciton properties. Here we use cathodoluminescence (CL) and time-resolved CL experiments to study how excitons interact with the interface between two twisted hexagonal boron nitride (hBN) crystals with various angles. An efficient capture of free excitons by the interface is demonstrated, which leads to a population of long lived and interface-localized (2D) excitons. Temperature dependent experiments indicate that for high twist angles, these excitons localized at the interface further undergo a self-trapping. It consists in a distortion of the lattice around the exciton on which the exciton traps itself. Our results suggest that this exciton-interface interaction causes a broad optical emission of highly twisted hBN-hBN structures around 300 nm (4 eV). Exciton self-trapping is finally discussed as a common feature of sp2 hybridized boron nitride polytypes and nanostructures due to the ionic nature of the B-N bond and their compact excitons.","sentences":["One of the main interests of 2D materials is their ability to be assembled with many degrees of freedom for tuning and manipulating excitonic properties.","There is a need to understand how the structure of the interfaces between atomic layers influences exciton properties.","Here we use cathodoluminescence (CL) and time-resolved CL experiments to study how excitons interact with the interface between two twisted hexagonal boron nitride (hBN) crystals with various angles.","An efficient capture of free excitons by the interface is demonstrated, which leads to a population of long lived and interface-localized (2D) excitons.","Temperature dependent experiments indicate that for high twist angles, these excitons localized at the interface further undergo a self-trapping.","It consists in a distortion of the lattice around the exciton on which the exciton traps itself.","Our results suggest that this exciton-interface interaction causes a broad optical emission of highly twisted hBN-hBN structures around 300 nm (4 eV).","Exciton self-trapping is finally discussed as a common feature of sp2 hybridized boron nitride polytypes and nanostructures due to the ionic nature of the B-N bond and their compact excitons."],"url":"http://arxiv.org/abs/2405.09420v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-15 15:12:42","title":"Study of charged Lepton Flavor Violation in electron muon interactions","abstract":"With the improvement of muon acceleration technology, it has received great interest to exploit high-energy muon beams for collision or target experiments. We investigate possible charged Lepton Flavor Violation (cLFV) processes mediated by an extra massive neutral gauge boson Z0 in electron muon interactions, either at a proposed electron muon collider or in a fixed target experiment with high-energy muon beam hitting electrons in the target. Based on Monte Carlo calculations and fast detector simulations, we study in detail our signal and possible backgrounds, giving the sensitivity results of cLFV signals at the 90% confidence level. Compared with current and prospective limits set by other experiments, electron muon interactions demonstrate significant advantages in the cLFV coupling strength sensitivity with tau in the final states. In addition, a special cLFV coupling combination, lambda_emu * lambda_mumu, can also be probed in our proposal.","sentences":["With the improvement of muon acceleration technology, it has received great interest to exploit high-energy muon beams for collision or target experiments.","We investigate possible charged Lepton Flavor Violation (cLFV) processes mediated by an extra massive neutral gauge boson Z0 in electron muon interactions, either at a proposed electron muon collider or in a fixed target experiment with high-energy muon beam hitting electrons in the target.","Based on Monte Carlo calculations and fast detector simulations, we study in detail our signal and possible backgrounds, giving the sensitivity results of cLFV signals at the 90% confidence level.","Compared with current and prospective limits set by other experiments, electron muon interactions demonstrate significant advantages in the cLFV coupling strength sensitivity with tau in the final states.","In addition, a special cLFV coupling combination, lambda_emu * lambda_mumu, can also be probed in our proposal."],"url":"http://arxiv.org/abs/2405.09417v1","category":"hep-ex"}
{"created":"2024-05-15 15:02:37","title":"Strain effects on the electronic properties of a graphene wormhole","abstract":"In this work, we explore the strain and curvature effects on the electronic properties of a curved graphene structure, called the graphene wormhole. The electron dynamics is described by a massless Dirac fermion containing position--dependent Fermi velocity. In addition, the strain produces a pseudo--magnetic vector potential to the geometric coupling. For an isotropic strain tensor, the decoupled components of the spinor field exhibit a supersymmetric (SUSY) potential, depending on the centrifugal term and the external magnetic field only. In the absence of a external magnetic field, the strain yields to an exponential damped amplitude, whereas the curvature leads to a power--law damping of the wave function. The spin--curvature coupling breaks the chiral symmetry between the upper and the lower spinor component, which leads to the increasing of the wave function on either upper or lower region of the wormhole, i.e., depending on the spin number. By adding an uniform magnetic field, the effective potential exhibits an asymptotic quadratic profile and a spin--curvature barrier near the throat. As a result, the bound states (Landau levels) are confined around the wormhole throat showing an asymmetric and spin--dependent profile.","sentences":["In this work, we explore the strain and curvature effects on the electronic properties of a curved graphene structure, called the graphene wormhole.","The electron dynamics is described by a massless Dirac fermion containing position--dependent Fermi velocity.","In addition, the strain produces a pseudo--magnetic vector potential to the geometric coupling.","For an isotropic strain tensor, the decoupled components of the spinor field exhibit a supersymmetric (SUSY) potential, depending on the centrifugal term and the external magnetic field only.","In the absence of a external magnetic field, the strain yields to an exponential damped amplitude, whereas the curvature leads to a power--law damping of the wave function.","The spin--curvature coupling breaks the chiral symmetry between the upper and the lower spinor component, which leads to the increasing of the wave function on either upper or lower region of the wormhole, i.e., depending on the spin number.","By adding an uniform magnetic field, the effective potential exhibits an asymptotic quadratic profile and a spin--curvature barrier near the throat.","As a result, the bound states (Landau levels) are confined around the wormhole throat showing an asymmetric and spin--dependent profile."],"url":"http://arxiv.org/abs/2405.09407v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-15 14:38:21","title":"Odderon contribution in light of the LHC low-$t$ data","abstract":"We perform the analysis of elastic scattering $pp$ and $\\bar{p}p$ data at low momentum transfer $|t|<0.1$ GeV$^2$ within large collider energy interval $\\sqrt{s}=50$ GeV - 13 TeV in order to evaluate quantitatively the possible Odderon contribution. We use the two-channel eikonal model, which naturally accounts for the screening of the Odderon amplitude by $C$-even (Pomeron) exchanges.","sentences":["We perform the analysis of elastic scattering $pp$ and $\\bar{p}p$ data at low momentum transfer $|t|<0.1$ GeV$^2$ within large collider energy interval $\\sqrt{s}=50$ GeV - 13 TeV in order to evaluate quantitatively the possible Odderon contribution.","We use the two-channel eikonal model, which naturally accounts for the screening of the Odderon amplitude by $C$-even (Pomeron) exchanges."],"url":"http://arxiv.org/abs/2405.09385v1","category":"hep-ph"}
{"created":"2024-05-15 14:36:42","title":"Probing particle acceleration in Abell 2256: from to 16 MHz to gamma rays","abstract":"Merging galaxy clusters often host spectacular diffuse radio synchrotron sources. These sources can be explained by a non-thermal pool of relativistic electrons accelerated by shocks and turbulence in the intracluster medium. The origin of the pool and details of the cosmic ray transport and acceleration mechanisms in clusters are still open questions. Due to the often extremely steep spectral indices of diffuse radio emission, it is best studied at low frequencies. However, the lowest frequency window available to ground-based telescopes (10-30 MHz) has remained largely unexplored, as radio frequency interference and calibration problems related to the ionosphere become severe. Here, we present LOFAR observations from 16 to 168 MHz targeting the famous cluster Abell 2256. In the deepest-ever images at decametre wavelengths, we detect and resolve the radio halo, radio shock and various steep spectrum sources. We measure standard single power-law behaviour for the radio halo and radio shock spectra and find significant spectral index and curvature fluctuations across the radio halo, indicating an inhomogeneous emitting volume. In contrast to the straight power-law spectra of the large-scale diffuse sources, the various AGN-related sources often show extreme steepening towards higher frequencies and flattening towards low frequencies. We also discover a new fossil plasma source with a steep spectrum between 23 and 144 MHz, with $\\alpha=-1.9\\pm 0.1$. Finally, by comparing radio and gamma-ray observations, we rule out purely hadronic models for the radio halo origin in Abell 2256, unless the magnetic field strength in the cluster is exceptionally high, which is unsupportable by energetic arguments and inconsistent with the knowledge of other cluster magnetic fields.","sentences":["Merging galaxy clusters often host spectacular diffuse radio synchrotron sources.","These sources can be explained by a non-thermal pool of relativistic electrons accelerated by shocks and turbulence in the intracluster medium.","The origin of the pool and details of the cosmic ray transport and acceleration mechanisms in clusters are still open questions.","Due to the often extremely steep spectral indices of diffuse radio emission, it is best studied at low frequencies.","However, the lowest frequency window available to ground-based telescopes (10-30 MHz) has remained largely unexplored, as radio frequency interference and calibration problems related to the ionosphere become severe.","Here, we present LOFAR observations from 16 to 168 MHz targeting the famous cluster Abell 2256.","In the deepest-ever images at decametre wavelengths, we detect and resolve the radio halo, radio shock and various steep spectrum sources.","We measure standard single power-law behaviour for the radio halo and radio shock spectra and find significant spectral index and curvature fluctuations across the radio halo, indicating an inhomogeneous emitting volume.","In contrast to the straight power-law spectra of the large-scale diffuse sources, the various AGN-related sources often show extreme steepening towards higher frequencies and flattening towards low frequencies.","We also discover a new fossil plasma source with a steep spectrum between 23 and 144 MHz, with $\\alpha=-1.9\\pm 0.1$. Finally, by comparing radio and gamma-ray observations, we rule out purely hadronic models for the radio halo origin in Abell 2256, unless the magnetic field strength in the cluster is exceptionally high, which is unsupportable by energetic arguments and inconsistent with the knowledge of other cluster magnetic fields."],"url":"http://arxiv.org/abs/2405.09384v1","category":"astro-ph.HE"}
{"created":"2024-05-15 14:31:53","title":"Time-dependent models of AGN disks with radiation from embedded stellar-mass black holes","abstract":"The brightest steady sources of radiation in the universe, active galactic nuclei (AGN), are powered by gas accretion onto a central supermassive black hole (SMBH). The large sizes and accretion rates implicated in AGN accretion disks are expected to lead to gravitational instability and fragmentation, effectively cutting off mass inflow to the SMBH. Radiative feedback from disk-embedded stars has been invoked to yield marginally stable, steady-state solutions in the outer disks. Here, we examine the consequences of this star formation with a semi-analytical model in which stellar-mass black hole (sBH) remnants in the disk provide an additional source of stabilizing radiative feedback. Assuming star formation seeds the embedded sBH population, we model the time-evolving feedback from both stars and the growing population of accreting sBHs. We find that in the outer disk, the luminosity of the sBHs quickly dominates that of their parent stars. However, because sBHs consume less gas than stars to stabilize the disk, the presence of the sBHs enhances the mass flux to the inner disk. As a result, star formation persists over the lifetime of the AGN, damped in the outer disk, but amplified in a narrow ring in the inner disk. Heating from the embedded sBHs significantly modifies the disk's temperature profile and hardens its spectral energy distribution, and direct emission from the sBHs adds a new hard X ray component, resembling a Compton reflection \"hump\".","sentences":["The brightest steady sources of radiation in the universe, active galactic nuclei (AGN), are powered by gas accretion onto a central supermassive black hole (SMBH).","The large sizes and accretion rates implicated in AGN accretion disks are expected to lead to gravitational instability and fragmentation, effectively cutting off mass inflow to the SMBH.","Radiative feedback from disk-embedded stars has been invoked to yield marginally stable, steady-state solutions in the outer disks.","Here, we examine the consequences of this star formation with a semi-analytical model in which stellar-mass black hole (sBH) remnants in the disk provide an additional source of stabilizing radiative feedback.","Assuming star formation seeds the embedded sBH population, we model the time-evolving feedback from both stars and the growing population of accreting sBHs.","We find that in the outer disk, the luminosity of the sBHs quickly dominates that of their parent stars.","However, because sBHs consume less gas than stars to stabilize the disk, the presence of the sBHs enhances the mass flux to the inner disk.","As a result, star formation persists over the lifetime of the AGN, damped in the outer disk, but amplified in a narrow ring in the inner disk.","Heating from the embedded sBHs significantly modifies the disk's temperature profile and hardens its spectral energy distribution, and direct emission from the sBHs adds a new hard X ray component, resembling a Compton reflection \"hump\"."],"url":"http://arxiv.org/abs/2405.09380v1","category":"astro-ph.HE"}
{"created":"2024-05-15 14:22:00","title":"Non-abelian models in sQFT","abstract":"It is shown how string-localized QFT predicts interactions of the Standard Model. Detailed computations are presented for various non-abelian models: Yang-Mills, QCD, Higgs-Kibble. They serve as backup for [11,12] and as \"proof of concept\" for the treatment of the full Standard Model.","sentences":["It is shown how string-localized QFT predicts interactions of the Standard Model.","Detailed computations are presented for various non-abelian models: Yang-Mills, QCD, Higgs-Kibble.","They serve as backup for [11,12] and as \"proof of concept\" for the treatment of the full Standard Model."],"url":"http://arxiv.org/abs/2405.09370v1","category":"hep-th"}
{"created":"2024-05-15 14:18:27","title":"sQFT -- an autonomous explanation of the interactions of quantum particles","abstract":"Successful applications of a conceptually novel setup of Quantum Field Theory, that accounts for all subtheories of the Standard Model (QED, Electroweak Interaction and Higgs, Yang-Mills and QCD) and beyond (Helicity 2), call for a perspective view in a broader conceptual context. The setting is \"autonomous\" in the sense of being intrinsically quantum. Its principles are: Hilbert space, Poincar\\'e symmetry and causality. Its free quantum fields are obtained from Wigner's unitary representations of the Poincar\\'e group, with only physical and observable degrees of freedom. A \"quantization\" of an \"underlying\" classical theory is not needed. It allows renormalizable perturbation theory with interactions whose detailed structure, and in some cases even the particle content, is predicted by internal consistency. The results confirm and extend observable predictions for the interactions of the SM without assuming a \"principle\" of gauge invariance.","sentences":["Successful applications of a conceptually novel setup of Quantum Field Theory, that accounts for all subtheories of the Standard Model (QED, Electroweak Interaction and Higgs, Yang-Mills and QCD) and beyond (Helicity 2), call for a perspective view in a broader conceptual context.","The setting is \"autonomous\" in the sense of being intrinsically quantum.","Its principles are: Hilbert space, Poincar\\'e symmetry and causality.","Its free quantum fields are obtained from Wigner's unitary representations of the Poincar\\'e group, with only physical and observable degrees of freedom.","A \"quantization\" of an \"underlying\" classical theory is not needed.","It allows renormalizable perturbation theory with interactions whose detailed structure, and in some cases even the particle content, is predicted by internal consistency.","The results confirm and extend observable predictions for the interactions of the SM without assuming a \"principle\" of gauge invariance."],"url":"http://arxiv.org/abs/2405.09366v1","category":"hep-th"}
{"created":"2024-05-15 14:13:56","title":"Quantum operations for Kramers-Wannier duality","abstract":"We study the Kramers-Wannier duality for the transverse-field Ising lattice on a ring. A careful consideration of the ring boundary conditions shows that the duality has to be implemented with a proper treatment of different charge sectors of both the twisted and untwisted Ising and the dual-Ising Hilbert spaces. We construct a superoperator that explicitly maps the Ising operators to the dual-Ising operators. The superoperator naturally acts on the tensor product of the Ising and the dual-Ising Hilbert space. We then show that the relation between our superoperator and the Kramers-Wannier duality operator that maps the Ising Hilbert space to the dual-Ising Hilbert space is naturally provided by quantum operations and the duality can be understood as a quantum operation that we construct. We provide the operator-sum representation for the Kramers-Wannier quantum operations and reproduce the well-known fusion rules. In addition to providing the quantum information perspective on the Kramers-Wannier duality, our explicit protocol will also be useful in implementing the Kramers-Wannier duality on a quantum computer.","sentences":["We study the Kramers-Wannier duality for the transverse-field Ising lattice on a ring.","A careful consideration of the ring boundary conditions shows that the duality has to be implemented with a proper treatment of different charge sectors of both the twisted and untwisted Ising and the dual-Ising Hilbert spaces.","We construct a superoperator that explicitly maps the Ising operators to the dual-Ising operators.","The superoperator naturally acts on the tensor product of the Ising and the dual-Ising Hilbert space.","We then show that the relation between our superoperator and the Kramers-Wannier duality operator that maps the Ising Hilbert space to the dual-Ising Hilbert space is naturally provided by quantum operations and the duality can be understood as a quantum operation that we construct.","We provide the operator-sum representation for the Kramers-Wannier quantum operations and reproduce the well-known fusion rules.","In addition to providing the quantum information perspective on the Kramers-Wannier duality, our explicit protocol will also be useful in implementing the Kramers-Wannier duality on a quantum computer."],"url":"http://arxiv.org/abs/2405.09361v1","category":"hep-th"}
{"created":"2024-05-15 14:00:20","title":"Digging into the ultraviolet luminosity functions of galaxies at high redshifts: galaxies evolution, reionization, and cosmological parameters","abstract":"Thanks to the successful performance of the James Webb Space Telescope, our understanding of the epoch of reionization of the Universe has been advanced. The ultraviolet luminosity functions (UV LFs) of galaxies span a wide range of redshift, not only revealing the connection between galaxies and dark matter (DM) halos but also providing the information during reionization. In this work, we develop a model connecting galaxy counts and apparent magnitude based on UV LFs, which incorporates redshift-dependent star formation efficiency (SFE) and corrections for dust attenuation. By synthesizing some observations across the redshift range $4\\le z \\le 10$ from various galaxy surveys, we discern the evolving SFE with increasing redshift and DM halo mass through model fitting. Subsequent analyses indicate that the Thomson scattering optical depth was $\\tau_{\\rm e} = 0.052^{+0.003}_{-0.002}$ and the epoch of reionization started (ended) at $z=20.58^{+6.25}_{-6.75}$ ($z=5.38^{+0.65}_{-0.70}$) which is insensitive to the choice of the truncated magnitude of the UV LFs. Incorporating additional dataset and some reasonable constraints, the amplitude of matter perturbation is found to be $\\sigma_8=0.79\\pm0.05$, which is consistent with the standard $\\Lambda$CDM model. Future galaxy surveys and the dynamical simulations of galaxy evolution will break the degeneracy between SFE and cosmological parameters, improving the accuracy and the precision of the UV LF model further.","sentences":["Thanks to the successful performance of the James Webb Space Telescope, our understanding of the epoch of reionization of the Universe has been advanced.","The ultraviolet luminosity functions (UV LFs) of galaxies span a wide range of redshift, not only revealing the connection between galaxies and dark matter (DM) halos but also providing the information during reionization.","In this work, we develop a model connecting galaxy counts and apparent magnitude based on UV LFs, which incorporates redshift-dependent star formation efficiency (SFE) and corrections for dust attenuation.","By synthesizing some observations across the redshift range $4\\le z \\le 10$ from various galaxy surveys, we discern the evolving SFE with increasing redshift and DM halo mass through model fitting.","Subsequent analyses indicate that the Thomson scattering optical depth was $\\tau_{\\rm e} = 0.052^{+0.003}_{-0.002}$ and the epoch of reionization started (ended) at $z=20.58^{+6.25}_{-6.75}$ ($z=5.38^{+0.65}_{-0.70}$) which is insensitive to the choice of the truncated magnitude of the UV LFs.","Incorporating additional dataset and some reasonable constraints, the amplitude of matter perturbation is found to be $\\sigma_8=0.79\\pm0.05$, which is consistent with the standard $\\Lambda$CDM model.","Future galaxy surveys and the dynamical simulations of galaxy evolution will break the degeneracy between SFE and cosmological parameters, improving the accuracy and the precision of the UV LF model further."],"url":"http://arxiv.org/abs/2405.09350v1","category":"astro-ph.CO"}
{"created":"2024-05-15 13:55:19","title":"Classical Mechanics in Noncommutative Spaces: Confinement and More","abstract":"We consider a semi-classical approximation to the dynamics of a point particle in a noncommutative space. In this approximation, the noncommutativity of space coordinates is described by a Poisson bracket. For linear Poisson brackets, the corresponding phase space is given by the cotangent bundle of a Lie group, with the Lie group playing the role of a curved momentum space. We show that the curvature of the momentum space may lead to rather unexpected physical phenomena such as an upper bound on the velocity of a free nonrelativistic particle, bounded motion for repulsive central force, and no-fall-into-the-centre for attractive Coulomb potential. We also present a superintegrable Hamiltonian for the Kepler problem in $3$-space with ${su}(2)$ noncommutativity.","sentences":["We consider a semi-classical approximation to the dynamics of a point particle in a noncommutative space.","In this approximation, the noncommutativity of space coordinates is described by a Poisson bracket.","For linear Poisson brackets, the corresponding phase space is given by the cotangent bundle of a Lie group, with the Lie group playing the role of a curved momentum space.","We show that the curvature of the momentum space may lead to rather unexpected physical phenomena such as an upper bound on the velocity of a free nonrelativistic particle, bounded motion for repulsive central force, and no-fall-into-the-centre for attractive Coulomb potential.","We also present a superintegrable Hamiltonian for the Kepler problem in $3$-space with ${su}(2)$ noncommutativity."],"url":"http://arxiv.org/abs/2405.09348v1","category":"hep-th"}
{"created":"2024-05-15 13:26:26","title":"Exploring $t$-Channel Models for Dark Matter","abstract":"We report on a comprehensive study of the Direct Detection phenomenology of singlet Dark Matter $t$-channel portal models. For that purpose, we present a complete computation of the loop-induced direct detection cross-section for both scalar and fermionic Dark Matter candidates. We complete the study by comparing the results with current and future bounds from Direct Detection experiments and requiring the correct Dark Matter relic density.","sentences":["We report on a comprehensive study of the Direct Detection phenomenology of singlet Dark Matter $t$-channel portal models.","For that purpose, we present a complete computation of the loop-induced direct detection cross-section for both scalar and fermionic Dark Matter candidates.","We complete the study by comparing the results with current and future bounds from Direct Detection experiments and requiring the correct Dark Matter relic density."],"url":"http://arxiv.org/abs/2405.09326v1","category":"hep-ph"}
{"created":"2024-05-15 13:26:13","title":"On the Virasoro fusion and modular kernels at any irrational central charge","abstract":"We propose a series representation for the Virasoro fusion and modular kernels at any irrational central charge. Two distinct, yet closely related formulas are needed for the cases $c\\in \\mathbb C \\backslash (-\\infty,1]$ and $c <1$. We also conjecture that the formulas have a well-defined limit as the central charge approaches rational values. Our proposal for $c <1$ agrees numerically with the fusion transformation of the four-point spherical conformal blocks, whereas our proposal for $c\\in \\mathbb C \\backslash (-\\infty,1]$ agrees numerically with Ponsot and Teschner's integral formula for the fusion kernel. The case of the modular kernel is studied as a special case of the fusion kernel.","sentences":["We propose a series representation for the Virasoro fusion and modular kernels at any irrational central charge.","Two distinct, yet closely related formulas are needed for the cases $c\\in \\mathbb C \\backslash (-\\infty,1]$ and $c <1$.","We also conjecture that the formulas have a well-defined limit as the central charge approaches rational values.","Our proposal for $c <1$ agrees numerically with the fusion transformation of the four-point spherical conformal blocks, whereas our proposal for $c\\in \\mathbb C \\backslash (-\\infty,1]$ agrees numerically with Ponsot and Teschner's integral formula for the fusion kernel.","The case of the modular kernel is studied as a special case of the fusion kernel."],"url":"http://arxiv.org/abs/2405.09325v1","category":"hep-th"}
{"created":"2024-05-15 12:41:22","title":"Charm CP violation and searches","abstract":"Charm CP violation is a unique gate to the physics of up-type quarks and allows searches for physics beyond the Standard Model in new and exciting ways, complementary to kaon and $b$ decays. We review recent advances, focusing on symmetry-based methods and providing an outlook on the next challenges at the intensity frontier of charm physics.","sentences":["Charm CP violation is a unique gate to the physics of up-type quarks and allows searches for physics beyond the Standard Model in new and exciting ways, complementary to kaon and $b$ decays.","We review recent advances, focusing on symmetry-based methods and providing an outlook on the next challenges at the intensity frontier of charm physics."],"url":"http://arxiv.org/abs/2405.09299v1","category":"hep-ph"}
{"created":"2024-05-15 11:52:02","title":"The Spectra of IceCube Neutrino Candidate Sources","abstract":"We present recent work on the Spectra of IceCube Neutrino (SIN) candidate sources project. We defined a selection of candidate neutrino sources by identifying blazars in the vicinity of IceCube's highest-energy neutrinos. We now want to shed light on these source candidates' nature, starting at their redshift, continuing with their black hole masses, their variability, and, for the first time, a combined photon-neutrino spectral energy distribution. We present their hybrid spectral energy distributions (combining photon and neutrino fluxes), investigate the sources' variability in the near-infrared, optical, X-ray, and $\\gamma$-ray bands, and compare the variability with a blazar sample of non-candidate sources. Furthermore, we search for flares at the arrival time of the high-energy neutrinos.","sentences":["We present recent work on the Spectra of IceCube Neutrino (SIN) candidate sources project.","We defined a selection of candidate neutrino sources by identifying blazars in the vicinity of IceCube's highest-energy neutrinos.","We now want to shed light on these source candidates' nature, starting at their redshift, continuing with their black hole masses, their variability, and, for the first time, a combined photon-neutrino spectral energy distribution.","We present their hybrid spectral energy distributions (combining photon and neutrino fluxes), investigate the sources' variability in the near-infrared, optical, X-ray, and $\\gamma$-ray bands, and compare the variability with a blazar sample of non-candidate sources.","Furthermore, we search for flares at the arrival time of the high-energy neutrinos."],"url":"http://arxiv.org/abs/2405.09278v1","category":"astro-ph.HE"}
{"created":"2024-05-15 11:39:48","title":"How to fit PDFs in the presence of new physics?","abstract":"The interpretation of LHC data, and the assessment of possible hints of new physics, require the precise knowledge of the proton structure in terms of parton distribution functions (PDFs). I present a methodology designed to determine whether and how global PDF fits might inadvertently 'fit away' signs of new physics in the high-energy tails of the distributions. I showcase a scenario for the High-Luminosity LHC, in which the PDFs may completely absorb such signs of new physics, thus biasing theoretical predictions and interpretations. I discuss strategies to single out the effects in this scenario and disentangle the inconsistencies that stem from them. This study brings to light the synergy between the high luminosity programme at the LHC and present and future low-energy non-LHC measurements of large-sea quark distributions. I also present the potential of fitting simultaneously the PDFs and the new physics signals.","sentences":["The interpretation of LHC data, and the assessment of possible hints of new physics, require the precise knowledge of the proton structure in terms of parton distribution functions (PDFs).","I present a methodology designed to determine whether and how global PDF fits might inadvertently 'fit away' signs of new physics in the high-energy tails of the distributions.","I showcase a scenario for the High-Luminosity LHC, in which the PDFs may completely absorb such signs of new physics, thus biasing theoretical predictions and interpretations.","I discuss strategies to single out the effects in this scenario and disentangle the inconsistencies that stem from them.","This study brings to light the synergy between the high luminosity programme at the LHC and present and future low-energy non-LHC measurements of large-sea quark distributions.","I also present the potential of fitting simultaneously the PDFs and the new physics signals."],"url":"http://arxiv.org/abs/2405.09270v1","category":"hep-ph"}
{"created":"2024-05-15 11:35:07","title":"Searches for Galactic Neutrinos with the IceCube Neutrino observatory","abstract":"The sources of galactic charged cosmic rays are so far unknown, because their arrival directions are randomized in the galactic magnetic field. Objects accelerating hadrons are expected to produce high-energy neutrinos. In addition, a diffuse galactic neutrino flux is predicted from interactions of galactic cosmic rays with matter during propagation through the galaxy. The IceCube neutrino observatory at the geographic South Pole instruments a cubic kilometer of ice with optical modules to detect the Cherenkov light of particles produced in neutrino interactions. Operating for more than a decade in its complete detector configuration, IceCube is in a unique position to search for neutrino sources. This contribution discusses the searches for a diffuse flux of neutrinos as wells as for neutrinos from candidate point sources and extended sources in the galactic plane.","sentences":["The sources of galactic charged cosmic rays are so far unknown, because their arrival directions are randomized in the galactic magnetic field.","Objects accelerating hadrons are expected to produce high-energy neutrinos.","In addition, a diffuse galactic neutrino flux is predicted from interactions of galactic cosmic rays with matter during propagation through the galaxy.","The IceCube neutrino observatory at the geographic South Pole instruments a cubic kilometer of ice with optical modules to detect the Cherenkov light of particles produced in neutrino interactions.","Operating for more than a decade in its complete detector configuration, IceCube is in a unique position to search for neutrino sources.","This contribution discusses the searches for a diffuse flux of neutrinos as wells as for neutrinos from candidate point sources and extended sources in the galactic plane."],"url":"http://arxiv.org/abs/2405.09267v1","category":"astro-ph.HE"}
{"created":"2024-05-15 11:18:47","title":"Multiconfiguration Dirac-Hartree-Fock and configuration-interaction study of $4d-3p$ x-ray transitions in Cu- and Ni-like tungsten ions","abstract":"The $4d \\to 3p$ x-ray transitions in Cu- and Ni-like tungsten ions have been studied theoretically. The multiconfiguration Dirac-Hartree-Fock (MCDHF) method and the large-scale relativistic configuration-interaction (CI) method have been employed in order to take into account electron correlation effects on the wavelengths and transition rates. It was found that the wavelengths and transition rates obtained from the MCDHF-CI method depend strongly on the size and the type of the active space used in the calculations. It has been found that extending the active space of orbitals without careful control of the configuration state function base does not always lead to good quality MCDHF-CI results for highly ionized tungsten ions.","sentences":["The $4d \\to 3p$ x-ray transitions in Cu- and Ni-like tungsten ions have been studied theoretically.","The multiconfiguration Dirac-Hartree-Fock (MCDHF) method and the large-scale relativistic configuration-interaction (CI) method have been employed in order to take into account electron correlation effects on the wavelengths and transition rates.","It was found that the wavelengths and transition rates obtained from the MCDHF-CI method depend strongly on the size and the type of the active space used in the calculations.","It has been found that extending the active space of orbitals without careful control of the configuration state function base does not always lead to good quality MCDHF-CI results for highly ionized tungsten ions."],"url":"http://arxiv.org/abs/2405.09259v1","category":"physics.atom-ph"}
{"created":"2024-05-15 10:36:20","title":"Origin and detection of nontopological soliton dark matter","abstract":"Macroscopic dark matter like nontopological solitons can form either via the fusion and accumulation of free particles or during cosmological phase transitions. Both mechanisms can create dark matter with large masses ranging from TeV to solar mass. This can lead to interesting targets in direct detection, astrophysical, and cosmological searches.","sentences":["Macroscopic dark matter like nontopological solitons can form either via the fusion and accumulation of free particles or during cosmological phase transitions.","Both mechanisms can create dark matter with large masses ranging from TeV to solar mass.","This can lead to interesting targets in direct detection, astrophysical, and cosmological searches."],"url":"http://arxiv.org/abs/2405.09239v1","category":"hep-ph"}
{"created":"2024-05-15 10:36:01","title":"Recent Cold QCD Results from STAR","abstract":"Understanding the origin of transverse single-spin asymmetries is a long-standing challenge in strong interaction physics. Recent precise measurements of the azimuthal distribution of charged pions in jets from STAR have shed new light on the spin momentum correlations within the Transverse Momentum Dependent (TMD) formalism. This measurement, particularly sensitive to the Collins effect, correlates the quark transversity with the Collins fragmentation functions, elucidating the spin-dependent fragmentation process. Moreover, similar measurements involving kaons and protons, which have unique quark compositions, provide vital insights into the flavor-dependent spin dynamics of the parton distribution functions and fragmentation functions. These studies are crucial for developing a more comprehensive understanding of nucleon spin structure. In these proceedings, selection of the recent Cold QCD STAR results are presented, focusing on the asymmetries for charged kaons and protons in jets from transversely polarized $pp$ data at $\\sqrt{s}$ = 200 GeV.","sentences":["Understanding the origin of transverse single-spin asymmetries is a long-standing challenge in strong interaction physics.","Recent precise measurements of the azimuthal distribution of charged pions in jets from STAR have shed new light on the spin momentum correlations within the Transverse Momentum Dependent (TMD) formalism.","This measurement, particularly sensitive to the Collins effect, correlates the quark transversity with the Collins fragmentation functions, elucidating the spin-dependent fragmentation process.","Moreover, similar measurements involving kaons and protons, which have unique quark compositions, provide vital insights into the flavor-dependent spin dynamics of the parton distribution functions and fragmentation functions.","These studies are crucial for developing a more comprehensive understanding of nucleon spin structure.","In these proceedings, selection of the recent Cold QCD STAR results are presented, focusing on the asymmetries for charged kaons and protons in jets from transversely polarized $pp$ data at $\\sqrt{s}$ = 200 GeV."],"url":"http://arxiv.org/abs/2405.09238v1","category":"hep-ex"}
{"created":"2024-05-15 10:07:48","title":"Tentative estimates of $\\mathcal{B}(X(3872)\\to\u03c0^0\u03c0^0\u03c7_{c1})$ and $\\mathcal{B}(X(3872)\\to\u03c0^+\u03c0^-\u03c7_{c1})$","abstract":"The rates of the $X(3872)\\to\\pi^0\\pi^0\\chi_{c1}$ and $X(3872)\\to \\pi^+\\pi^-\\chi_{c1}$ decays are estimated in the model of the triangle loop diagrams with charmed $D^*\\bar DD$ and $\\bar D^*D\\bar D$ mesons in the loops. There are the triangle logarithmic singularities in the physical region of the $X(3872)\\to\\pi^0\\pi^0 \\chi_{c1}$ decay which manifest themselves as narrow peaks in the $\\pi^0\\chi_{c1}$ mass spectrum near the $D^0\\bar D^0$ threshold. The model predicts approximately the same branching fractions of the $X(3872)\\to\\pi^0\\pi^0 \\chi_{c1}$ and $X(3872)\\to\\pi^+\\pi^-\\chi_{c1}$ decays at the level of about (0.8-1.7)$\\times10^{-4}$. A distinct prediction of the model is the value of the ratio $\\mathcal{R}= \\mathcal{B}(X(3872)\\to\\pi^+\\pi^-\\chi_{c1})/\\mathcal{B}(X(3872) \\to\\pi^0\\pi^0\\chi_{c1})\\approx1.1$. It weakly depends on the $X(3872)$ resonance parameters and indicates a significant violation of the isotopic symmetry according to which one would expect $\\mathcal{R}=2$.","sentences":["The rates of the $X(3872)\\to\\pi^0\\pi^0\\chi_{c1}$ and $X(3872)\\to \\pi^+\\pi^-\\chi_{c1}$ decays are estimated in the model of the triangle loop diagrams with charmed $D^*\\bar DD$ and $\\bar D^*D\\bar D$ mesons in the loops.","There are the triangle logarithmic singularities in the physical region of the $X(3872)\\to\\pi^0\\pi^0 \\chi_{c1}$ decay which manifest themselves as narrow peaks in the $\\pi^0\\chi_{c1}$ mass spectrum near the $D^0\\bar D^0$ threshold.","The model predicts approximately the same branching fractions of the $X(3872)\\to\\pi^0\\pi^0 \\chi_{c1}$ and $X(3872)\\to\\pi^+\\pi^-\\chi_{c1}$ decays at the level of about (0.8-1.7)$\\times10^{-4}$. A distinct prediction of the model is the value of the ratio $\\mathcal{R}= \\mathcal{B}(X(3872)\\to\\pi^+\\pi^-\\chi_{c1})/\\mathcal{B}(X(3872) \\to\\pi^0\\pi^0\\chi_{c1})\\approx1.1$. It weakly depends on the $X(3872)$ resonance parameters and indicates a significant violation of the isotopic symmetry according to which one would expect $\\mathcal{R}=2$."],"url":"http://arxiv.org/abs/2405.09228v1","category":"hep-ph"}
{"created":"2024-05-15 10:06:27","title":"Evidence of the low-lying baryon $\u03a3^*(1/2^-)$ in the process $\u039b_c^+\\to \u03b7\u03c0^+\u039b$","abstract":"Motivated by the Belle measurements of the process $\\Lambda_c^+\\to \\eta\\pi^+\\Lambda$, we investigate this process by considering the contributions from the $\\Lambda(1670)$, $a_0(980)$, and $\\Sigma(1385)$. In addition, we also consider the predicted low-lying baryon $\\Sigma^*(1/2^-)$. Our results involving the $\\Sigma^*(1/2^-)$ are favored by fitting to the Belle data of the $\\eta\\Lambda$ and $\\pi^+\\Lambda$ invariant mass distributions. Furthermore, we predict the $\\eta\\pi^+$ invariant mass distribution and the angular distribution $d\\Gamma/d{\\rm cos}\\theta$, which are significantly different depending on whether or not the contribution from the $\\Sigma^*(1/2^-)$ is considered. Finally, we show that, with the contribution from the $\\Sigma^*(1/2^-)$, the calculated Dalizt plot agrees with the Belle measurements. Future precise measurements of the process $\\Lambda_c^+\\to \\eta\\pi^+\\Lambda$ could shed further light on the existence of the low-lying $\\Sigma^*(1/2^-)$.","sentences":["Motivated by the Belle measurements of the process $\\Lambda_c^+\\to \\eta\\pi^+\\Lambda$, we investigate this process by considering the contributions from the $\\Lambda(1670)$, $a_0(980)$, and $\\Sigma(1385)$. In addition, we also consider the predicted low-lying baryon $\\Sigma^*(1/2^-)$.","Our results involving the $\\Sigma^*(1/2^-)$ are favored by fitting to the Belle data of the $\\eta\\Lambda$ and $\\pi^+\\Lambda$ invariant mass distributions.","Furthermore, we predict the $\\eta\\pi^+$ invariant mass distribution and the angular distribution $d\\Gamma/d{\\rm cos}\\theta$, which are significantly different depending on whether or not the contribution from the $\\Sigma^*(1/2^-)$ is considered.","Finally, we show that, with the contribution from the $\\Sigma^*(1/2^-)$, the calculated Dalizt plot agrees with the Belle measurements.","Future precise measurements of the process $\\Lambda_c^+\\to \\eta\\pi^+\\Lambda$ could shed further light on the existence of the low-lying $\\Sigma^*(1/2^-)$."],"url":"http://arxiv.org/abs/2405.09226v1","category":"hep-ph"}
{"created":"2024-05-15 09:57:07","title":"Mixing of hot shocked plasma with cold gas in Nova YZ Ret 2020","abstract":"The origin of bright X-ray emission lines that appear late in a nova eruption remains largely a puzzle. We present two high-resolution X-ray grating spectra of the classical nova YZ Ret, observed 77 and 115 days post-eruption, using XMM-Newton and Chandra , respectively. Both spectra feature resolved emission lines blueshifted by $v = -1500$ km s$^{-1}$ and broadened by $\\sigma_v=500$ km s$^{-1}$. The two spectra are well described by a collisionally ionized plasma of $kT\\sim 70$ eV that dimmed by a factor of $\\sim40$ between the two exposures. The spectra also show narrow radiative recombination continua (RRCs) of C$^{+4}$, C$^{+5}$, and N$^{+5}$, indicating the interaction of the hot ionized plasma with cold electrons of $kT\\sim 2$ eV. The high-$n$ Rydberg series of C$^{+4}$ is anomalously bright, allowing us to measure the electron density through continuum lowering, which is in agreement with the He-like N$^{+5}$ density diagnostic of $n_e=(1.7\\pm0.4)\\times10^{11}$ cm$^{-3}$. The high population of these high-$n$ levels constitutes the best evidence to date of charge exchange (CX) with neutral H in an astrophysical ionized plasma. The remarkable fact that the velocity and plasma temperature are the same after 38 days, despite the high density and decreasing flux is evidence for ongoing heating. We suggest the heating is due to a reverse shock in the nova ejecta, which forms a thin X-ray shell. The narrow RRCs and CX are attributed to direct mixing with cold gas, which overtakes the hot plasma either from the shock front, or through the contact discontinuity.","sentences":["The origin of bright X-ray emission lines that appear late in a nova eruption remains largely a puzzle.","We present two high-resolution X-ray grating spectra of the classical nova YZ Ret, observed 77 and 115 days post-eruption, using XMM-Newton and Chandra , respectively.","Both spectra feature resolved emission lines blueshifted by $v = -1500$ km s$^{-1}$ and broadened by $\\sigma_v=500$ km s$^{-1}$.","The two spectra are well described by a collisionally ionized plasma of $kT\\sim 70$ eV that dimmed by a factor of $\\sim40$ between the two exposures.","The spectra also show narrow radiative recombination continua (RRCs) of C$^{+4}$, C$^{+5}$, and N$^{+5}$, indicating the interaction of the hot ionized plasma with cold electrons of $kT\\sim 2$ eV. The high-$n$ Rydberg series of C$^{+4}$ is anomalously bright, allowing us to measure the electron density through continuum lowering, which is in agreement with the He-like N$^{+5}$ density diagnostic of $n_e=(1.7\\pm0.4)\\times10^{11}$ cm$^{-3}$. The high population of these high-$n$ levels constitutes the best evidence to date of charge exchange (CX) with neutral H in an astrophysical ionized plasma.","The remarkable fact that the velocity and plasma temperature are the same after 38 days, despite the high density and decreasing flux is evidence for ongoing heating.","We suggest the heating is due to a reverse shock in the nova ejecta, which forms a thin X-ray shell.","The narrow RRCs and CX are attributed to direct mixing with cold gas, which overtakes the hot plasma either from the shock front, or through the contact discontinuity."],"url":"http://arxiv.org/abs/2405.09219v1","category":"astro-ph.HE"}
{"created":"2024-05-15 08:45:37","title":"SRG/ART-XC all-sky X-ray survey: Catalog of sources detected during the first five surveys","abstract":"We present an updated catalog of sources detected by the Mikhail Pavlinsky ART-XC telescope aboard the Spektrum-Roentgen-Gamma (SRG) observatory during its all-sky survey. It is based on the data of the first four and the partially completed fifth scans of the sky (ARTSS1-5). The catalog comprises 1545 sources detected in the 4-12 keV energy band. The achieved sensitivity ranges between $\\sim 4\\times 10^{-12}$ erg s$^{-1}$ cm$^{-2}$ near the ecliptic plane and $\\sim 7\\times 10^{-13}$ erg s$^{-1}$ cm$^{-2}$ near the ecliptic poles, which is a $\\sim$30-50% improvement over the previous version of the catalog based on the first two all-sky scans (ARTSS12). There are $\\sim 130$ objects, excluding the expected contribution of spurious detections, that were not known as X-ray sources before the SRG/ART-XC all-sky survey. We provide information, partly based on our ongoing follow-up optical spectroscopy program, on the identification and classification of the majority of the ARTSS1-5 sources (1463), of which 173 are tentative at the moment. The majority of the classified objects (964) are extragalactic, a small fraction (30) are located in the Local Group of galaxies, and 469 are Galactic. The dominant classes of objects in the catalog are active galactic nuclei (911) and cataclysmic variables (192).","sentences":["We present an updated catalog of sources detected by the Mikhail Pavlinsky ART-XC telescope aboard the Spektrum-Roentgen-Gamma (SRG) observatory during its all-sky survey.","It is based on the data of the first four and the partially completed fifth scans of the sky (ARTSS1-5).","The catalog comprises 1545 sources detected in the 4-12 keV energy band.","The achieved sensitivity ranges between $\\sim 4\\times 10^{-12}$ erg s$^{-1}$ cm$^{-2}$ near the ecliptic plane and $\\sim 7\\times 10^{-13}$ erg s$^{-1}$ cm$^{-2}$ near the ecliptic poles, which is a $\\sim$30-50% improvement over the previous version of the catalog based on the first two all-sky scans (ARTSS12).","There are $\\sim 130$ objects, excluding the expected contribution of spurious detections, that were not known as X-ray sources before the SRG/ART-XC all-sky survey.","We provide information, partly based on our ongoing follow-up optical spectroscopy program, on the identification and classification of the majority of the ARTSS1-5 sources (1463), of which 173 are tentative at the moment.","The majority of the classified objects (964) are extragalactic, a small fraction (30) are located in the Local Group of galaxies, and 469 are Galactic.","The dominant classes of objects in the catalog are active galactic nuclei (911) and cataclysmic variables (192)."],"url":"http://arxiv.org/abs/2405.09184v1","category":"astro-ph.HE"}
{"created":"2024-05-15 08:40:43","title":"Top quark mass and cross section at ATLAS and CMS","abstract":"The top quark is the heaviest elementary particle known to date and therefore an important topic to study in the context of the standard model at the LHC. In this contribution the latest measurements of top quark production cross sections and the top quark mass at the LHC by the ATLAS and CMS Collaborations are presented.","sentences":["The top quark is the heaviest elementary particle known to date and therefore an important topic to study in the context of the standard model at the LHC.","In this contribution the latest measurements of top quark production cross sections and the top quark mass at the LHC by the ATLAS and CMS Collaborations are presented."],"url":"http://arxiv.org/abs/2405.09182v1","category":"hep-ex"}
{"created":"2024-05-15 07:52:13","title":"An Empirical Study of Token-based Micro Commits","abstract":"In software development, developers frequently apply maintenance activities to the source code that change a few lines by a single commit. A good understanding of the characteristics of such small changes can support quality assurance approaches (e.g., automated program repair), as it is likely that small changes are addressing deficiencies in other changes; thus, understanding the reasons for creating small changes can help understand the types of errors introduced. Eventually, these reasons and the types of errors can be used to enhance quality assurance approaches for improving code quality. While prior studies used code churns to characterize and investigate the small changes, such a definition has a critical limitation. Specifically, it loses the information of changed tokens in a line. For example, this definition fails to distinguish the following two one-line changes: (1) changing a string literal to fix a displayed message and (2) changing a function call and adding a new parameter. These are definitely maintenance activities, but we deduce that researchers and practitioners are interested in supporting the latter change. To address this limitation, in this paper, we define micro commits, a type of small change based on changed tokens. Our goal is to quantify small changes using changed tokens. Changed tokens allow us to identify small changes more precisely. In fact, this token-level definition can distinguish the above example. We investigate defined micro commits in four OSS projects and understand their characteristics as the first empirical study on token-based micro commits. We find that micro commits mainly replace a single name or literal token, and micro commits are more likely used to fix bugs. Additionally, we propose the use of token-based information to support software engineering approaches in which very small changes significantly affect their effectiveness.","sentences":["In software development, developers frequently apply maintenance activities to the source code that change a few lines by a single commit.","A good understanding of the characteristics of such small changes can support quality assurance approaches (e.g., automated program repair), as it is likely that small changes are addressing deficiencies in other changes; thus, understanding the reasons for creating small changes can help understand the types of errors introduced.","Eventually, these reasons and the types of errors can be used to enhance quality assurance approaches for improving code quality.","While prior studies used code churns to characterize and investigate the small changes, such a definition has a critical limitation.","Specifically, it loses the information of changed tokens in a line.","For example, this definition fails to distinguish the following two one-line changes: (1) changing a string literal to fix a displayed message and (2) changing a function call and adding a new parameter.","These are definitely maintenance activities, but we deduce that researchers and practitioners are interested in supporting the latter change.","To address this limitation, in this paper, we define micro commits, a type of small change based on changed tokens.","Our goal is to quantify small changes using changed tokens.","Changed tokens allow us to identify small changes more precisely.","In fact, this token-level definition can distinguish the above example.","We investigate defined micro commits in four OSS projects and understand their characteristics as the first empirical study on token-based micro commits.","We find that micro commits mainly replace a single name or literal token, and micro commits are more likely used to fix bugs.","Additionally, we propose the use of token-based information to support software engineering approaches in which very small changes significantly affect their effectiveness."],"url":"http://arxiv.org/abs/2405.09165v1","category":"cs.SE"}
{"created":"2024-05-15 07:04:12","title":"Self-diffusiophoretic propulsion of a spheroidal particle in a shear-thinning fluid","abstract":"Shear-thinning viscosity is a non-Newtonian behaviour that active particles often encounter in biological fluids such as blood and mucus. The fundamental question of how this ubiquitous non-Newtonian rheology affects the propulsion of active particles has attracted substantial interest. In particular, spherical Janus particles driven by self-diffusiophresis, a major physico-chemical propulsion mechanism of synthetic active particles, were shown to always swim slower in a shear-thinning fluid than in a Newtonian fluid. In this work, we move beyond the spherical limit to examine the effect of particle eccentricity on self-diffusiophoretic propulsion in a shear-thinning fluid. We use a combination of asymptotic analysis and numerical simulations to show that shear-thinning rheology can enhance self-diffusiophoretic propulsion of a spheroidal particle, in stark contrast to previous findings for the spherical case. A systematic characterization of the dependence of the propulsion speed on the particle's active surface coverage has also uncovered an intriguing feature associated with the propulsion speeds of a pair of complementarily coated particles not previously reported. Symmetry arguments are presented to elucidate how this new feature emerges as a combined effect of anisotropy of the spheroidal geometry and nonlinearity in fluid rheology.","sentences":["Shear-thinning viscosity is a non-Newtonian behaviour that active particles often encounter in biological fluids such as blood and mucus.","The fundamental question of how this ubiquitous non-Newtonian rheology affects the propulsion of active particles has attracted substantial interest.","In particular, spherical Janus particles driven by self-diffusiophresis, a major physico-chemical propulsion mechanism of synthetic active particles, were shown to always swim slower in a shear-thinning fluid than in a Newtonian fluid.","In this work, we move beyond the spherical limit to examine the effect of particle eccentricity on self-diffusiophoretic propulsion in a shear-thinning fluid.","We use a combination of asymptotic analysis and numerical simulations to show that shear-thinning rheology can enhance self-diffusiophoretic propulsion of a spheroidal particle, in stark contrast to previous findings for the spherical case.","A systematic characterization of the dependence of the propulsion speed on the particle's active surface coverage has also uncovered an intriguing feature associated with the propulsion speeds of a pair of complementarily coated particles not previously reported.","Symmetry arguments are presented to elucidate how this new feature emerges as a combined effect of anisotropy of the spheroidal geometry and nonlinearity in fluid rheology."],"url":"http://arxiv.org/abs/2405.09136v1","category":"physics.flu-dyn"}
{"created":"2024-05-15 06:46:53","title":"Contextual Integrity Games","abstract":"The contextual integrity model is a widely accepted way of analyzing the plurality of norms that are colloquially called \"privacy norms\". Contextual integrity systematically describes such norms by distinguishing the type of data concerned, the three social agents involved (subject, sender, and recipient) and the transmission principle governing the transfer of information. It allows analyzing privacy norms in terms of their impact on the interaction of those agents with one another.   This paper places contextual integrity in a strict game theoretic framework. When such description is possible it has three key advantages: Firstly, it allows indisputable utilitarian justification of some privacy norms. Secondly, it better relates privacy to topics which are well understood by stakeholders whose education is predominantly quantitative, such as engineers and economists. Thirdly, it is an absolute necessity when describing ethical constraints to machines such as AI agents.   In addition to describing games which capture paradigmatic informational norms, the paper also analyzes cases in which the game, per se, does not encourage normative behavior. The paper discusses two main forms of mechanisms which can be applied to the game in such cases, and shows that they reflect accepted privacy regulation and technologies.","sentences":["The contextual integrity model is a widely accepted way of analyzing the plurality of norms that are colloquially called \"privacy norms\".","Contextual integrity systematically describes such norms by distinguishing the type of data concerned, the three social agents involved (subject, sender, and recipient) and the transmission principle governing the transfer of information.","It allows analyzing privacy norms in terms of their impact on the interaction of those agents with one another.   ","This paper places contextual integrity in a strict game theoretic framework.","When such description is possible it has three key advantages: Firstly, it allows indisputable utilitarian justification of some privacy norms.","Secondly, it better relates privacy to topics which are well understood by stakeholders whose education is predominantly quantitative, such as engineers and economists.","Thirdly, it is an absolute necessity when describing ethical constraints to machines such as AI agents.   ","In addition to describing games which capture paradigmatic informational norms, the paper also analyzes cases in which the game, per se, does not encourage normative behavior.","The paper discusses two main forms of mechanisms which can be applied to the game in such cases, and shows that they reflect accepted privacy regulation and technologies."],"url":"http://arxiv.org/abs/2405.09130v1","category":"cs.CY"}
{"created":"2024-05-15 06:35:01","title":"Dirac Fermions and Topological Phases in Magnetic Topological Insulator Films","abstract":"We develop a Dirac fermion theory for topological phases in magnetic topological insulator films. The theory is based on exact solutions of the energies and the wave functions for an effective model of the three-dimensional topological insulator (TI) film. It is found that the TI film consists of a pair of massless or massive Dirac fermions for the surface states, and a series of massive Dirac fermions for the bulk states. The massive Dirac fermion always carries zero or integer quantum Hall conductance when the valence band is fully occupied while the massless Dirac fermion carries a one-half quantum Hall conductance when the chemical potential is located around the Dirac point for a finite range. The magnetic exchange interaction in the magnetic layers in the film can be used to manipulate either the masses or chirality of the Dirac fermions and gives rise to distinct topological phases, which cover the known topological insulating phases, such as quantum anomalous Hall effect, quantum spin Hall effect and axion effect, and also the novel topological metallic phases, such as half quantized Hall effect, half quantum mirror Hall effect, and metallic quantum anomalous Hall effect.","sentences":["We develop a Dirac fermion theory for topological phases in magnetic topological insulator films.","The theory is based on exact solutions of the energies and the wave functions for an effective model of the three-dimensional topological insulator (TI) film.","It is found that the TI film consists of a pair of massless or massive Dirac fermions for the surface states, and a series of massive Dirac fermions for the bulk states.","The massive Dirac fermion always carries zero or integer quantum Hall conductance when the valence band is fully occupied while the massless Dirac fermion carries a one-half quantum Hall conductance when the chemical potential is located around the Dirac point for a finite range.","The magnetic exchange interaction in the magnetic layers in the film can be used to manipulate either the masses or chirality of the Dirac fermions and gives rise to distinct topological phases, which cover the known topological insulating phases, such as quantum anomalous Hall effect, quantum spin Hall effect and axion effect, and also the novel topological metallic phases, such as half quantized Hall effect, half quantum mirror Hall effect, and metallic quantum anomalous Hall effect."],"url":"http://arxiv.org/abs/2405.09121v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-15 06:29:44","title":"Martian seismic anisotropy underneath Elysium Planitia revealed by direct S wave splitting","abstract":"Seismic anisotropy, arising from the crystallographic or lattice-preferred orientation of anisotropic minerals or the shape-preferred orientation of melts or cracks, can establish a critical link between Mars's past evolution and its current state. So far, although seismic anisotropy in Mars has been proposed due to different velocities of vertically and horizontally polarized shear waves in the Martian crust, obtained from crustal converted waves, multiples, and surface waves recorded by the InSight seismometer, the evidence is plausible. Notably, the shear wave splitting, which stands out as a straight indicator of seismic anisotropy, has not been reported using marsquake records. In this study, we employ Low-frequency marsquakes detected by the InSight seismometer to reveal shear wave splitting in Mars. We find that the direct S waves of three marsquake recordings (S0173a, S0235b, and S1133c) with high signal-to-noise ratios exhibit the splitting pheonmenon. We rule out the possibility of apparent anisotropy through synthetic tests, affirming the presence of seismic anisotropy in Mars. The delay time (about 1.33 s on average) measured from the direct S wave splitting is too large to be solely attributed to the seismic anisotropy in the upper crust (0 - 10 km) beneath the InSight. Thus, seismic anisotropy in the deeper region of Mars is indispensable. Combined with other geophysical evidence near the InSight landing site, the strong seismic anisotropy observed in this study implies the porous crust with aligned cracks being greater than 10 km beneath the InSight and/or the presence of an active mantle plume underneath the Elysium Planitia of Mars.","sentences":["Seismic anisotropy, arising from the crystallographic or lattice-preferred orientation of anisotropic minerals or the shape-preferred orientation of melts or cracks, can establish a critical link between Mars's past evolution and its current state.","So far, although seismic anisotropy in Mars has been proposed due to different velocities of vertically and horizontally polarized shear waves in the Martian crust, obtained from crustal converted waves, multiples, and surface waves recorded by the InSight seismometer, the evidence is plausible.","Notably, the shear wave splitting, which stands out as a straight indicator of seismic anisotropy, has not been reported using marsquake records.","In this study, we employ Low-frequency marsquakes detected by the InSight seismometer to reveal shear wave splitting in Mars.","We find that the direct S waves of three marsquake recordings (S0173a, S0235b, and S1133c) with high signal-to-noise ratios exhibit the splitting pheonmenon.","We rule out the possibility of apparent anisotropy through synthetic tests, affirming the presence of seismic anisotropy in Mars.","The delay time (about 1.33 s on average) measured from the direct S wave splitting is too large to be solely attributed to the seismic anisotropy in the upper crust (0 - 10 km) beneath the InSight.","Thus, seismic anisotropy in the deeper region of Mars is indispensable.","Combined with other geophysical evidence near the InSight landing site, the strong seismic anisotropy observed in this study implies the porous crust with aligned cracks being greater than 10 km beneath the InSight and/or the presence of an active mantle plume underneath the Elysium Planitia of Mars."],"url":"http://arxiv.org/abs/2405.09120v1","category":"physics.geo-ph"}
{"created":"2024-05-15 06:27:00","title":"Molecular order induced charge transfer in a C$_{60}$-topological insulator moir\u00e9 heterostructure","abstract":"We synthesize and spectroscopically investigate monolayer C$_{60}$ on the topological insulator (TI) Bi$_4$Te$_3$. This C$_{60}$/Bi$_4$Te$_3$ heterostructure is characterized by excellent translational order in a novel (4 x 4) C$_{60}$ superstructure on a (9 x 9) unit of Bi$_4$Te$_3$. We measure the full two-dimensional energy band structure of C$_{60}$/Bi$_4$Te$_3$ using angle-resolved photoemission spectroscopy (ARPES). We find that C$_{60}$ accepts electrons from the TI at room temperature but no charge transfer occurs at low temperatures. We unravel this peculiar behaviour by Raman spectroscopy of C$_{60}$/Bi$_4$Te$_3$ and density functional theory (DFT) calculations of the electronegativity of C$_{60}$. Both methods are sensitive to orientational order of C$_{60}$. At low temperatures, Raman spectroscopy shows a dramatic intensity increase of the C$_{60}$ Raman signal, evidencing a transition to a rotationally ordered state. DFT reveals that the orientational order of C$_{60}$ at low temperatures has a higher electron affinity than at high temperatures. These results neatly explain the temperature-dependent charge transfer observed in ARPES. Our conclusions are supported by the appearance of a strong photoluminescence from C$_{60}$/Bi$_4$Te$_3$ at low temperatures.","sentences":["We synthesize and spectroscopically investigate monolayer C$_{60}$ on the topological insulator (TI) Bi$_4$Te$_3$. This C$_{60}$/Bi$_4$Te$_3$ heterostructure is characterized by excellent translational order in a novel (4 x 4) C$_{60}$ superstructure on a (9 x 9) unit of Bi$_4$Te$_3$. We measure the full two-dimensional energy band structure of C$_{60}$/Bi$_4$Te$_3$ using angle-resolved photoemission spectroscopy (ARPES).","We find that C$_{60}$ accepts electrons from the TI at room temperature but no charge transfer occurs at low temperatures.","We unravel this peculiar behaviour by Raman spectroscopy of C$_{60}$/Bi$_4$Te$_3$ and density functional theory (DFT) calculations of the electronegativity of C$_{60}$. Both methods are sensitive to orientational order of C$_{60}$. At low temperatures, Raman spectroscopy shows a dramatic intensity increase of the C$_{60}$ Raman signal, evidencing a transition to a rotationally ordered state.","DFT reveals that the orientational order of C$_{60}$ at low temperatures has a higher electron affinity than at high temperatures.","These results neatly explain the temperature-dependent charge transfer observed in ARPES.","Our conclusions are supported by the appearance of a strong photoluminescence from C$_{60}$/Bi$_4$Te$_3$ at low temperatures."],"url":"http://arxiv.org/abs/2405.09119v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-15 05:01:44","title":"Line graphs and Nordhaus-Gaddum-type bounds for self-loop graphs","abstract":"Let $G_S$ be the graph obtained by attaching a self-loop at every vertex in $S \\subseteq V(G)$ of a simple graph $G$ of order $n.$ In this paper, we explore several new results related to the line graph $L(G_S)$ of $G_S.$ Particularly, we show that every eigenvalue of $L(G_S)$ must be at least $-2,$ and relate the characteristic polynomial of the line graph $L(G)$ of $G$ with the characteristic polynomial of the line graph $L(\\widehat{G})$ of a self-loop graph $\\widehat{G}$, which is obtained by attaching a self-loop at each vertex of $G$. Then, we provide some new bounds for the eigenvalues and energy of $G_S.$ As one of the consequences, we obtain that the energy of a connected regular complete multipartite graph is not greater than the energy of the corresponding self-loop graph. Lastly, we establish a lower bound of the spectral radius in terms of the first Zagreb index $M_1(G)$ and the minimum degree $\\delta(G),$ as well as proving two Nordhaus-Gaddum-type bounds for the spectral radius and the energy of $G_S,$ respectively.","sentences":["Let $G_S$ be the graph obtained by attaching a self-loop at every vertex in $S \\subseteq V(G)$ of a simple graph $G$ of order $n.$ In this paper, we explore several new results related to the line graph $L(G_S)$ of $G_S.$ Particularly, we show that every eigenvalue of $L(G_S)$ must be at least $-2,$ and relate the characteristic polynomial of the line graph $L(G)$ of $G$ with the characteristic polynomial of the line graph $L(\\widehat{G})$ of a self-loop graph $\\widehat{G}$, which is obtained by attaching a self-loop at each vertex of $G$. Then, we provide some new bounds for the eigenvalues and energy of $G_S.$ As one of the consequences, we obtain that the energy of a connected regular complete multipartite graph is not greater than the energy of the corresponding self-loop graph.","Lastly, we establish a lower bound of the spectral radius in terms of the first Zagreb index $M_1(G)$ and the minimum degree $\\delta(G),$ as well as proving two Nordhaus-Gaddum-type bounds for the spectral radius and the energy of $G_S,$ respectively."],"url":"http://arxiv.org/abs/2405.09093v1","category":"math.CO"}
{"created":"2024-05-15 04:42:26","title":"Microscopic Investigation of Ground State Properties and Shape Evolution in Osmium Isotopes","abstract":"The present study focuses on investigating the shape evolution of neutron-rich even-even Osmium (Os) transitional nuclei within the range of neutron number N = 82 to N = 190. The investigation is conducted using density-dependent meson-nucleon and point-coupling models within the framework of the covariant density functional theory (CDFT). Additionally, the results obtained from the CDFT calculations are compared with those obtained using the relativistic mean-field model with a non-linear meson-nucleon interaction. The potential energy curve for Os isotopes (ranging from $^{158}$Os to $^{260}$Os) is analyzed in order to identify phase shape transitions, such as oblate-spherical-prolate. Furthermore, ground state bulk properties are calculated to gain insights into the structure of Os isotopes. The self-consistent calculations reveal a clear shape transition in the even-even Os isotopes, and overall, good agreement is observed among the different models employed as well as with the available experimental data.","sentences":["The present study focuses on investigating the shape evolution of neutron-rich even-even Osmium (Os) transitional nuclei within the range of neutron number N","= 82 to N = 190.","The investigation is conducted using density-dependent meson-nucleon and point-coupling models within the framework of the covariant density functional theory (CDFT).","Additionally, the results obtained from the CDFT calculations are compared with those obtained using the relativistic mean-field model with a non-linear meson-nucleon interaction.","The potential energy curve for Os isotopes (ranging from $^{158}$Os to $^{260}$Os) is analyzed in order to identify phase shape transitions, such as oblate-spherical-prolate.","Furthermore, ground state bulk properties are calculated to gain insights into the structure of Os isotopes.","The self-consistent calculations reveal a clear shape transition in the even-even Os isotopes, and overall, good agreement is observed among the different models employed as well as with the available experimental data."],"url":"http://arxiv.org/abs/2405.09085v1","category":"nucl-th"}
{"created":"2024-05-15 04:02:56","title":"Compressive Feature Selection for Remote Visual Multi-Task Inference","abstract":"Deep models produce a number of features in each internal layer. A key problem in applications such as feature compression for remote inference is determining how important each feature is for the task(s) performed by the model. The problem is especially challenging in the case of multi-task inference, where the same feature may carry different importance for different tasks. In this paper, we examine how effective is mutual information (MI) between a feature and a model's task output as a measure of the feature's importance for that task. Experiments involving hard selection and soft selection (unequal compression) based on MI are carried out to compare the MI-based method with alternative approaches. Multi-objective analysis is provided to offer further insight.","sentences":["Deep models produce a number of features in each internal layer.","A key problem in applications such as feature compression for remote inference is determining how important each feature is for the task(s) performed by the model.","The problem is especially challenging in the case of multi-task inference, where the same feature may carry different importance for different tasks.","In this paper, we examine how effective is mutual information (MI) between a feature and a model's task output as a measure of the feature's importance for that task.","Experiments involving hard selection and soft selection (unequal compression) based on MI are carried out to compare the MI-based method with alternative approaches.","Multi-objective analysis is provided to offer further insight."],"url":"http://arxiv.org/abs/2405.09077v1","category":"eess.IV"}
{"created":"2024-05-15 03:30:56","title":"Search for the leptonic decays $D^{*+}\\to e^+\u03bd_e$ and $D^{*+}\\to \u03bc^+\u03bd_\u03bc$","abstract":"We present the first search for the leptonic decays $D^{*+}\\to e^+\\nu_e$ and $D^{*+}\\to \\mu^+\\nu_\\mu$ by analyzing a data sample of electron-positron collisions recorded with the BESIII detector at center-of-mass energies between 4.178 and 4.226 GeV, corresponding to an integrated luminosity of 6.32~fb$^{-1}$. No significant signal is observed. The upper limits on the branching fractions for $D^{*+}\\to e^+\\nu_e$ and $D^{*+}\\to \\mu^+\\nu_\\mu$ are set to be $1.1 \\times 10^{-5}$ and $4.3 \\times 10^{-6}$ at 90\\% confidence level, respectively.","sentences":["We present the first search for the leptonic decays $D^{*+}\\to e^+\\nu_e$ and $D^{*+}\\to \\mu^+\\nu_\\mu$ by analyzing a data sample of electron-positron collisions recorded with the BESIII detector at center-of-mass energies between 4.178 and 4.226 GeV, corresponding to an integrated luminosity of 6.32~fb$^{-1}$. No significant signal is observed.","The upper limits on the branching fractions for $D^{*+}\\to e^+\\nu_e$ and $D^{*+}\\to \\mu^+\\nu_\\mu$ are set to be $1.1 \\times 10^{-5}$ and $4.3 \\times 10^{-6}$ at 90\\% confidence level, respectively."],"url":"http://arxiv.org/abs/2405.09066v1","category":"hep-ex"}
{"created":"2024-05-15 02:54:07","title":"Beam Shaping Based on Axisymmetric Aspheric Mirrors","abstract":"Flat-top beam, known for its ability to generate a consistently even irradiation area, holds vast utility in many fields of scientific and industrial applications. In this paper, a reflective laser beam shaping method based on two axisymmetric aspheric mirrors (AAMs), a polarizing beam splitter (PBS) and two quarter wave plates (QWPs) is proposed to transform Gaussian beam into flat-top beam. Compared to alternative beam shaping methods, the method using AAMs demonstrates distinct advantages on notably high energy efficiency and unique capability to generate parallel beams. Thanks to its relative simplicities of design, manufacture and tunability, AAMs-shaping further enhances its appeal in applied research scenarios.","sentences":["Flat-top beam, known for its ability to generate a consistently even irradiation area, holds vast utility in many fields of scientific and industrial applications.","In this paper, a reflective laser beam shaping method based on two axisymmetric aspheric mirrors (AAMs), a polarizing beam splitter (PBS) and two quarter wave plates (QWPs) is proposed to transform Gaussian beam into flat-top beam.","Compared to alternative beam shaping methods, the method using AAMs demonstrates distinct advantages on notably high energy efficiency and unique capability to generate parallel beams.","Thanks to its relative simplicities of design, manufacture and tunability, AAMs-shaping further enhances its appeal in applied research scenarios."],"url":"http://arxiv.org/abs/2405.09048v1","category":"physics.optics"}
{"created":"2024-05-15 02:24:35","title":"Measurement of Temperature Relaxation in the Postshock Plasma of the Northwestern Limb of SN 1006","abstract":"Heating of charged particles via collisionless shocks, while ubiquitous in the universe, is an intriguing yet puzzling plasma phenomenon. One outstanding question is how electrons and ions approach an equilibrium after they were heated to different immediate-postshock temperatures. In order to fill the significant lack of observational information of the downstream temperature-relaxation process, we observe a thermal-dominant X-ray filament in the northwest of SN~1006 with Chandra. We divide this region into four layers with a thickness of 15$^{\\prime\\prime}$ or 0.16 pc each, and fit each spectrum by a non-equilibrium ionization collisional plasma model. The electron temperature was found to increase toward downstream from 0.52-0.62 keV to 0.82-0.95 keV on a length scale of 60 arcsec (or 0.64 pc). This electron temperature is lower than thermal relaxation processes via Coulomb scattering, requiring some other effects such as plasma mixture due to turbulence and/or projection effects, etc, which we hope will be resolved with future X-ray calorimeter missions such as XRISM and Athena.","sentences":["Heating of charged particles via collisionless shocks, while ubiquitous in the universe, is an intriguing yet puzzling plasma phenomenon.","One outstanding question is how electrons and ions approach an equilibrium after they were heated to different immediate-postshock temperatures.","In order to fill the significant lack of observational information of the downstream temperature-relaxation process, we observe a thermal-dominant X-ray filament in the northwest of SN~1006 with Chandra.","We divide this region into four layers with a thickness of 15$^{\\prime\\prime}$ or 0.16 pc each, and fit each spectrum by a non-equilibrium ionization collisional plasma model.","The electron temperature was found to increase toward downstream from 0.52-0.62 keV to 0.82-0.95 keV on a length scale of 60 arcsec (or 0.64 pc).","This electron temperature is lower than thermal relaxation processes via Coulomb scattering, requiring some other effects such as plasma mixture due to turbulence and/or projection effects, etc, which we hope will be resolved with future X-ray calorimeter missions such as XRISM and Athena."],"url":"http://arxiv.org/abs/2405.09040v1","category":"astro-ph.HE"}
{"created":"2024-05-15 02:19:15","title":"The integrated perturbation theory for cosmological tensor fields IV: Full-sky formulation","abstract":"In Papers I-III, we use the flat-sky and distant-observer approximations to develop a formalism with which the correlation statistics of cosmological tensor fields are calculated by the nonlinear perturbation theory, generalizing the integrated perturbation theory for scalar fields. In this work, the formalism is extended to include the full-sky and wide-angle effects in evaluating the power spectra and correlation functions of cosmological tensor fields of any ranks. With the newly developed formalism, one can evaluate the nonlinear power spectra and correlation functions to arbitrary higher orders in principle. After describing the general formalism, we explicitly derive and give analytic results of the lowest-order linear theory for an illustrative purpose in this paper. The derived linear formulas with full-sky and wide-angle effects are numerically compared with the previous formulas with flat-sky and distant-observer limits in a simple model of tensor bias.","sentences":["In Papers I-III, we use the flat-sky and distant-observer approximations to develop a formalism with which the correlation statistics of cosmological tensor fields are calculated by the nonlinear perturbation theory, generalizing the integrated perturbation theory for scalar fields.","In this work, the formalism is extended to include the full-sky and wide-angle effects in evaluating the power spectra and correlation functions of cosmological tensor fields of any ranks.","With the newly developed formalism, one can evaluate the nonlinear power spectra and correlation functions to arbitrary higher orders in principle.","After describing the general formalism, we explicitly derive and give analytic results of the lowest-order linear theory for an illustrative purpose in this paper.","The derived linear formulas with full-sky and wide-angle effects are numerically compared with the previous formulas with flat-sky and distant-observer limits in a simple model of tensor bias."],"url":"http://arxiv.org/abs/2405.09038v1","category":"astro-ph.CO"}
{"created":"2024-05-15 01:55:46","title":"Universality in supernova gravitational waves with proto-neutron star properties","abstract":"Gravitational wave signals from core-collapse supernovae are one of the important observables for extracting the information of dense matter. To extract the properties of proto-neutron stars produced via core-collapse supernovae by asteroseismology, we perform a linear perturbation analysis using data obtained from two-dimensional numerical simulations. We employ 12 and 20 solar-mass progenitors and compare two different treatments of gravity. One is a general relativistic one with a conformal flatness condition and the other is an effective gravitational potential mimicking the Tolman-Oppenheimer-Volkoff solution. We discuss how the frequencies of the proto-neutron star oscillations corresponding to the gravitational wave signals in the simulations depend on the proto-neutron star properties. In our models, we find that the gravitational wave frequencies of the proto-neutron stars determined with the Cowling approximation can be expressed to very good approximation as a function of the proto-neutron star average density almost independently of the progenitor mass, treatment of gravity in the simulations, and the interpolations in the simulations. On the other hand, if one considers the gravitational wave frequencies as a function of the surface gravity of proto-neutron stars, such a relation appears sensitive to the treatment of gravity and other numerical details in the simulations. Thus, the average density of proto-neutron stars seems more suitable for universally expressing the supernova gravitational wave frequencies, instead of the surface gravity.","sentences":["Gravitational wave signals from core-collapse supernovae are one of the important observables for extracting the information of dense matter.","To extract the properties of proto-neutron stars produced via core-collapse supernovae by asteroseismology, we perform a linear perturbation analysis using data obtained from two-dimensional numerical simulations.","We employ 12 and 20 solar-mass progenitors and compare two different treatments of gravity.","One is a general relativistic one with a conformal flatness condition and the other is an effective gravitational potential mimicking the Tolman-Oppenheimer-Volkoff solution.","We discuss how the frequencies of the proto-neutron star oscillations corresponding to the gravitational wave signals in the simulations depend on the proto-neutron star properties.","In our models, we find that the gravitational wave frequencies of the proto-neutron stars determined with the Cowling approximation can be expressed to very good approximation as a function of the proto-neutron star average density almost independently of the progenitor mass, treatment of gravity in the simulations, and the interpolations in the simulations.","On the other hand, if one considers the gravitational wave frequencies as a function of the surface gravity of proto-neutron stars, such a relation appears sensitive to the treatment of gravity and other numerical details in the simulations.","Thus, the average density of proto-neutron stars seems more suitable for universally expressing the supernova gravitational wave frequencies, instead of the surface gravity."],"url":"http://arxiv.org/abs/2405.09030v1","category":"astro-ph.HE"}
{"created":"2024-05-15 01:52:05","title":"A temperature or FUV tracer? The HNC/HCN ratio in M83 on the GMC scale","abstract":"The HNC/HCN ratio is observationally known as a thermometer in Galactic interstellar molecular clouds. A recent study has alternatively suggested that the HNC/HCN ratio is affected by the ultraviolet (UV) field, not by the temperature. We aim to study this ratio on the scale of giant molecular clouds in the barred spiral galaxy M83 towards the southwestern bar end and the central region from ALMA observations, and if possible, distinguish the above scenarios. We compare the high (40-50 pc) resolution HNC/HCN ratios with the star formation rate from the 3-mm continuum intensity and the molecular mass inferred from the HCN intensities. Our results show that the HNC/HCN ratios do not vary with the star formation rates, star formation efficiencies, or column densities in the bar-end region. In the central region, the HNC/HCN ratios become higher with higher star formation rates, which tend to cause higher temperatures. This result is not consistent with the previously proposed scenario in which the HNC/HCN ratio decreases with increasing temperature. Spectral shapes suggest that this trend may be due to optically thick HCN and optically thin HNC. In addition, we compare the large-scale ($\\sim 200$ pc) correlation between the dust temperature from the FIR ratio and the HNC/HCN ratio for the southwestern bar-end region. The HNC/HCN ratio is lower when the dust temperatures are higher. We suggest from the above results that the HNC/HCN ratio depends on the UV radiation field that affects the interstellar medium on the $\\sim100\\,$pc scale where the column densities are low.","sentences":["The HNC/HCN ratio is observationally known as a thermometer in Galactic interstellar molecular clouds.","A recent study has alternatively suggested that the HNC/HCN ratio is affected by the ultraviolet (UV) field, not by the temperature.","We aim to study this ratio on the scale of giant molecular clouds in the barred spiral galaxy M83 towards the southwestern bar end and the central region from ALMA observations, and if possible, distinguish the above scenarios.","We compare the high (40-50 pc) resolution HNC/HCN ratios with the star formation rate from the 3-mm continuum intensity and the molecular mass inferred from the HCN intensities.","Our results show that the HNC/HCN ratios do not vary with the star formation rates, star formation efficiencies, or column densities in the bar-end region.","In the central region, the HNC/HCN ratios become higher with higher star formation rates, which tend to cause higher temperatures.","This result is not consistent with the previously proposed scenario in which the HNC/HCN ratio decreases with increasing temperature.","Spectral shapes suggest that this trend may be due to optically thick HCN and optically thin HNC.","In addition, we compare the large-scale ($\\sim 200$ pc) correlation between the dust temperature from the FIR ratio and the HNC/HCN ratio for the southwestern bar-end region.","The HNC/HCN ratio is lower when the dust temperatures are higher.","We suggest from the above results that the HNC/HCN ratio depends on the UV radiation field that affects the interstellar medium on the $\\sim100\\,$pc scale where the column densities are low."],"url":"http://arxiv.org/abs/2405.09029v1","category":"astro-ph.GA"}
{"created":"2024-05-15 01:04:58","title":"Tetraquarks from Intrinsic Heavy Quarks","abstract":"A number of new four-quark states containing from one to four charm or anti-charm quarks have been observed recently. Many of these new states have been discovered at the LHC. The production of these states via intrinsic charm in the proton is investigated. The tetraquark masses obtained in this approach, while dependent on the internal transverse momenta of the partons in the state, are shown to agree well with the measured masses. These calculations can provide some insight into the nature of the tetraquark candidates, whether as a bound meson pair or as a looser configuration of four individual partons. The kinematic distributions of these states as a function of rapidity and transverse momentum are also studied. The possible cross sections for these states are finally considered.","sentences":["A number of new four-quark states containing from one to four charm or anti-charm quarks have been observed recently.","Many of these new states have been discovered at the LHC.","The production of these states via intrinsic charm in the proton is investigated.","The tetraquark masses obtained in this approach, while dependent on the internal transverse momenta of the partons in the state, are shown to agree well with the measured masses.","These calculations can provide some insight into the nature of the tetraquark candidates, whether as a bound meson pair or as a looser configuration of four individual partons.","The kinematic distributions of these states as a function of rapidity and transverse momentum are also studied.","The possible cross sections for these states are finally considered."],"url":"http://arxiv.org/abs/2405.09018v1","category":"hep-ph"}
{"created":"2024-05-15 00:36:42","title":"Determination of pairing matrix elements from average single particle level densities","abstract":"A simple and efficient method to treat nuclear pairing correlations within a simple Hartree-Fock--plus-BCS description is proposed and discussed. It relies on the fact that the intensity of pairing correlations depends crucially on level densities around the Fermi surface ($ \\rho(e_F)$) and that any fitting of nuclear energies as functions of the nucleon numbers is akin of a semi-classical average, smoothing out their quantal structure. A particular attention has been paid to two points generally ignored in previous similar approaches. One is a correction advocated by M\\\"{o}ller and Nix [Nucl. Phys. A 536, 20 (1992)] taking into account the fact that the data included into the fit correspond to $ \\rho(e_F)$ values systematically lower than average. The second is due to a systematic overestimation of the proton sp level density at the Fermi surface resulting from the local Slater approximation of the Coulomb terms in use in most microscopic descriptions. Our approach is validated by the agreement with data of corresponding calculated moments of inertia of well and rigidly deformed rare-earth nuclei, evaluated according to the Inglis-Belyaev ansatz with some crude Thouless-Valatin corrections. Indeed, the agreement which is found, is at least of the same quality as what results from a specific fit of the pairing intensities to these particular pieces of data. While this approach is currently limited to the very simple seniority force pairing treatment, it may serve as a starting point to define pairing residual interactions from averaged odd-even mass differences data, using merely average sp level densities associated to calculated canonical basis.","sentences":["A simple and efficient method to treat nuclear pairing correlations within a simple Hartree-Fock--plus-BCS description is proposed and discussed.","It relies on the fact that the intensity of pairing correlations depends crucially on level densities around the Fermi surface ($ \\rho(e_F)$) and that any fitting of nuclear energies as functions of the nucleon numbers is akin of a semi-classical average, smoothing out their quantal structure.","A particular attention has been paid to two points generally ignored in previous similar approaches.","One is a correction advocated by M\\\"{o}ller and Nix [Nucl.","Phys.","A 536, 20 (1992)] taking into account the fact that the data included into the fit correspond to $ \\rho(e_F)$ values systematically lower than average.","The second is due to a systematic overestimation of the proton sp level density at the Fermi surface resulting from the local Slater approximation of the Coulomb terms in use in most microscopic descriptions.","Our approach is validated by the agreement with data of corresponding calculated moments of inertia of well and rigidly deformed rare-earth nuclei, evaluated according to the Inglis-Belyaev ansatz with some crude Thouless-Valatin corrections.","Indeed, the agreement which is found, is at least of the same quality as what results from a specific fit of the pairing intensities to these particular pieces of data.","While this approach is currently limited to the very simple seniority force pairing treatment, it may serve as a starting point to define pairing residual interactions from averaged odd-even mass differences data, using merely average sp level densities associated to calculated canonical basis."],"url":"http://arxiv.org/abs/2405.09013v1","category":"nucl-th"}
{"created":"2024-05-15 00:18:06","title":"Discrete Gauge Anomalies and Instantons","abstract":"We revisit anomalous phases related to large gauge transformations, such as the Witten anomaly. The latter, known to plague $d=4$ $Sp(k)$ theories, is well-understood in terms of $\\pi_4(Sp(k))=\\mathbb{Z}_2$, but it also has an oblique relation to the instantons, labeled by $\\pi_3(G)=\\mathbb{Z}$, via the fermion zero mode counting. We revisit this relation and point out how $SU(N)$ theories escape an anomalous sign of the latter type, only thanks to the perturbative anomaly cancelation condition that restricts the chiral fermion spectrum. This leads to the question of what happens if the latter, more mundane anomaly is canceled by an inflow instead. After raising an open question about fractional D3 probe theories, we explore the simplest bottom-up model of such a kind, due to Witten and Yonekura, from which we find the relevant chiral theories to be free of such a disease despite the unrestricted chiral spectra. We close with a simple but often-overlooked observation about how fermionic zero modes enter physics differently between Euclidean and Lorentzian descriptions and point out a related issue in $d=3$.","sentences":["We revisit anomalous phases related to large gauge transformations, such as the Witten anomaly.","The latter, known to plague $d=4$ $Sp(k)$ theories, is well-understood in terms of $\\pi_4(Sp(k))=\\mathbb{Z}_2$, but it also has an oblique relation to the instantons, labeled by $\\pi_3(G)=\\mathbb{Z}$, via the fermion zero mode counting.","We revisit this relation and point out how $SU(N)$ theories escape an anomalous sign of the latter type, only thanks to the perturbative anomaly cancelation condition that restricts the chiral fermion spectrum.","This leads to the question of what happens if the latter, more mundane anomaly is canceled by an inflow instead.","After raising an open question about fractional D3 probe theories, we explore the simplest bottom-up model of such a kind, due to Witten and Yonekura, from which we find the relevant chiral theories to be free of such a disease despite the unrestricted chiral spectra.","We close with a simple but often-overlooked observation about how fermionic zero modes enter physics differently between Euclidean and Lorentzian descriptions and point out a related issue in $d=3$."],"url":"http://arxiv.org/abs/2405.09007v1","category":"hep-th"}
{"created":"2024-05-15 00:00:57","title":"Nonparametric Inference on Dose-Response Curves Without the Positivity Condition","abstract":"Existing statistical methods in causal inference often rely on the assumption that every individual has some chance of receiving any treatment level regardless of its associated covariates, which is known as the positivity condition. This assumption could be violated in observational studies with continuous treatments. In this paper, we present a novel integral estimator of the causal effects with continuous treatments (i.e., dose-response curves) without requiring the positivity condition. Our approach involves estimating the derivative function of the treatment effect on each observed data sample and integrating it to the treatment level of interest so as to address the bias resulting from the lack of positivity condition. The validity of our approach relies on an alternative weaker assumption that can be satisfied by additive confounding models. We provide a fast and reliable numerical recipe for computing our estimator in practice and derive its related asymptotic theory. To conduct valid inference on the dose-response curve and its derivative, we propose using the nonparametric bootstrap and establish its consistency. The practical performances of our proposed estimators are validated through simulation studies and an analysis of the effect of air pollution exposure (PM$_{2.5}$) on cardiovascular mortality rates.","sentences":["Existing statistical methods in causal inference often rely on the assumption that every individual has some chance of receiving any treatment level regardless of its associated covariates, which is known as the positivity condition.","This assumption could be violated in observational studies with continuous treatments.","In this paper, we present a novel integral estimator of the causal effects with continuous treatments (i.e., dose-response curves) without requiring the positivity condition.","Our approach involves estimating the derivative function of the treatment effect on each observed data sample and integrating it to the treatment level of interest so as to address the bias resulting from the lack of positivity condition.","The validity of our approach relies on an alternative weaker assumption that can be satisfied by additive confounding models.","We provide a fast and reliable numerical recipe for computing our estimator in practice and derive its related asymptotic theory.","To conduct valid inference on the dose-response curve and its derivative, we propose using the nonparametric bootstrap and establish its consistency.","The practical performances of our proposed estimators are validated through simulation studies and an analysis of the effect of air pollution exposure (PM$_{2.5}$) on cardiovascular mortality rates."],"url":"http://arxiv.org/abs/2405.09003v1","category":"stat.ME"}
{"created":"2024-05-14 22:38:58","title":"On two-generator subgroups of mapping torus groups","abstract":"We prove that if $G_\\phi=\\langle F, t| t x t^{-1} =\\phi(x), x\\in F\\rangle$ is the mapping torus group of an injective endomorphism $\\phi: F\\to F$ of a free group $F$ (of possibly infinite rank), then every two-generator subgroup $H$ of $G_\\phi$ is either free or a sub-mapping torus. As an application we show that if $\\phi\\in \\mathrm{Out}(F_r)$ (where $r\\ge 2$) is a fully irreducible atoroidal automorphism then every two-generator subgroup of $G_\\phi$ is either free or has finite index in $G_\\phi$.","sentences":["We prove that if $G_\\phi=\\langle F, t| t x t^{-1} =\\phi(x), x\\in F\\rangle$ is the mapping torus group of an injective endomorphism $\\phi: F\\to F$ of a free group $F$ (of possibly infinite rank), then every two-generator subgroup $H$ of $G_\\phi$ is either free or a sub-mapping torus.","As an application we show that if $\\phi\\in \\mathrm{Out}(F_r)$ (where $r\\ge 2$) is a fully irreducible atoroidal automorphism then every two-generator subgroup of $G_\\phi$ is either free or has finite index in $G_\\phi$."],"url":"http://arxiv.org/abs/2405.08985v1","category":"math.GR"}
{"created":"2024-05-14 22:24:36","title":"The X-ray rise and fall of the Recurrent Symbiotic System T CrB","abstract":"We present the analysis of publicly available NuSTAR, Suzaku and XMM-Newton observations of the recurrent symbiotic system T CrB covering the 2006.77-2022.66 yr period. The X-ray spectra are analysed by adopting a model that includes a reflection component produced by the presence of the accretion disk. Our best-fit model requires a disk with a radius of 100 AU, effective thickness of 10 AU, averaged column density 10$^{25}$ cm$^{-2}$ and orientation of 50$^{\\circ}$ with respect to the line of sight that contributes significantly to the total X-ray flux detected from T CrB, in addition to naturally producing the emission of the 6.4 keV Fe line. The spectral analysis suggests that the temperature of the boundary layer evolved from 14.8 keV in the steady-state phase (before 2016), to 2.8 keV in the 2017.24 epoch, to finally stabilise to about $\\sim$8 keV in the subsequent epochs. Variations in the plasma temperature of the boundary layer are attributed to the evolution of the mass accretion rate using theoretical interpretations from an accretion disk model. The presence of emission lines in the high-resolution XMM-Newton RGS spectrum of 2017.24 prevents from adopting a black body emission model to fit the soft X-ray range. Instead, we use plasma emission models that suggest the presence of adiabatically-shocked gas produced by gas velocities of 110-200 km s$^{-1}$, very likely tracing jet-like ejections similar to what is found in other symbiotic systems. Given the the analysis of X-ray and optical data together, we show that T CrB has a similar evolution as black hole binaries, accreting neutron stars and AGN in the hardness-intensity diagram.","sentences":["We present the analysis of publicly available NuSTAR, Suzaku and XMM-Newton observations of the recurrent symbiotic system T CrB covering the 2006.77-2022.66 yr period.","The X-ray spectra are analysed by adopting a model that includes a reflection component produced by the presence of the accretion disk.","Our best-fit model requires a disk with a radius of 100 AU, effective thickness of 10 AU, averaged column density 10$^{25}$ cm$^{-2}$ and orientation of 50$^{\\circ}$ with respect to the line of sight that contributes significantly to the total X-ray flux detected from T CrB, in addition to naturally producing the emission of the 6.4 keV Fe line.","The spectral analysis suggests that the temperature of the boundary layer evolved from 14.8 keV in the steady-state phase (before 2016), to 2.8 keV in the 2017.24 epoch, to finally stabilise to about $\\sim$8 keV in the subsequent epochs.","Variations in the plasma temperature of the boundary layer are attributed to the evolution of the mass accretion rate using theoretical interpretations from an accretion disk model.","The presence of emission lines in the high-resolution XMM-Newton RGS spectrum of 2017.24 prevents from adopting a black body emission model to fit the soft X-ray range.","Instead, we use plasma emission models that suggest the presence of adiabatically-shocked gas produced by gas velocities of 110-200 km s$^{-1}$, very likely tracing jet-like ejections similar to what is found in other symbiotic systems.","Given the the analysis of X-ray and optical data together, we show that T CrB has a similar evolution as black hole binaries, accreting neutron stars and AGN in the hardness-intensity diagram."],"url":"http://arxiv.org/abs/2405.08980v1","category":"astro-ph.HE"}
