{"created":"2024-04-23 17:59:59","title":"SMPLer: Taming Transformers for Monocular 3D Human Shape and Pose Estimation","abstract":"Existing Transformers for monocular 3D human shape and pose estimation typically have a quadratic computation and memory complexity with respect to the feature length, which hinders the exploitation of fine-grained information in high-resolution features that is beneficial for accurate reconstruction. In this work, we propose an SMPL-based Transformer framework (SMPLer) to address this issue. SMPLer incorporates two key ingredients: a decoupled attention operation and an SMPL-based target representation, which allow effective utilization of high-resolution features in the Transformer. In addition, based on these two designs, we also introduce several novel modules including a multi-scale attention and a joint-aware attention to further boost the reconstruction performance. Extensive experiments demonstrate the effectiveness of SMPLer against existing 3D human shape and pose estimation methods both quantitatively and qualitatively. Notably, the proposed algorithm achieves an MPJPE of 45.2 mm on the Human3.6M dataset, improving upon Mesh Graphormer by more than 10% with fewer than one-third of the parameters. Code and pretrained models are available at https://github.com/xuxy09/SMPLer.","sentences":["Existing Transformers for monocular 3D human shape and pose estimation typically have a quadratic computation and memory complexity with respect to the feature length, which hinders the exploitation of fine-grained information in high-resolution features that is beneficial for accurate reconstruction.","In this work, we propose an SMPL-based Transformer framework (SMPLer) to address this issue.","SMPLer incorporates two key ingredients: a decoupled attention operation and an SMPL-based target representation, which allow effective utilization of high-resolution features in the Transformer.","In addition, based on these two designs, we also introduce several novel modules including a multi-scale attention and a joint-aware attention to further boost the reconstruction performance.","Extensive experiments demonstrate the effectiveness of SMPLer against existing 3D human shape and pose estimation methods both quantitatively and qualitatively.","Notably, the proposed algorithm achieves an MPJPE of 45.2 mm on the Human3.6M dataset, improving upon Mesh Graphormer by more than 10% with fewer than one-third of the parameters.","Code and pretrained models are available at https://github.com/xuxy09/SMPLer."],"url":"http://arxiv.org/abs/2404.15276v1","category":"cs.CV"}
{"created":"2024-04-23 17:59:43","title":"ID-Animator: Zero-Shot Identity-Preserving Human Video Generation","abstract":"Generating high fidelity human video with specified identities has attracted significant attention in the content generation community. However, existing techniques struggle to strike a balance between training efficiency and identity preservation, either requiring tedious case-by-case finetuning or usually missing the identity details in video generation process. In this study, we present ID-Animator, a zero-shot human-video generation approach that can perform personalized video generation given single reference facial image without further training. ID-Animator inherits existing diffusion-based video generation backbones with a face adapter to encode the ID-relevant embeddings from learnable facial latent queries. To facilitate the extraction of identity information in video generation, we introduce an ID-oriented dataset construction pipeline, which incorporates decoupled human attribute and action captioning technique from a constructed facial image pool. Based on this pipeline, a random face reference training method is further devised to precisely capture the ID-relevant embeddings from reference images, thus improving the fidelity and generalization capacity of our model for ID-specific video generation. Extensive experiments demonstrate the superiority of ID-Animator to generate personalized human videos over previous models. Moreover, our method is highly compatible with popular pre-trained T2V models like animatediff and various community backbone models, showing high extendability in real-world applications for video generation where identity preservation is highly desired. Our codes and checkpoints will be released at https://github.com/ID-Animator/ID-Animator.","sentences":["Generating high fidelity human video with specified identities has attracted significant attention in the content generation community.","However, existing techniques struggle to strike a balance between training efficiency and identity preservation, either requiring tedious case-by-case finetuning or usually missing the identity details in video generation process.","In this study, we present ID-Animator, a zero-shot human-video generation approach that can perform personalized video generation given single reference facial image without further training.","ID-Animator inherits existing diffusion-based video generation backbones with a face adapter to encode the ID-relevant embeddings from learnable facial latent queries.","To facilitate the extraction of identity information in video generation, we introduce an ID-oriented dataset construction pipeline, which incorporates decoupled human attribute and action captioning technique from a constructed facial image pool.","Based on this pipeline, a random face reference training method is further devised to precisely capture the ID-relevant embeddings from reference images, thus improving the fidelity and generalization capacity of our model for ID-specific video generation.","Extensive experiments demonstrate the superiority of ID-Animator to generate personalized human videos over previous models.","Moreover, our method is highly compatible with popular pre-trained T2V models like animatediff and various community backbone models, showing high extendability in real-world applications for video generation where identity preservation is highly desired.","Our codes and checkpoints will be released at https://github.com/ID-Animator/ID-Animator."],"url":"http://arxiv.org/abs/2404.15275v1","category":"cs.CV"}
{"created":"2024-04-23 17:59:01","title":"CT-GLIP: 3D Grounded Language-Image Pretraining with CT Scans and Radiology Reports for Full-Body Scenarios","abstract":"Medical Vision-Language Pretraining (Med-VLP) establishes a connection between visual content from medical images and the relevant textual descriptions. Existing Med-VLP methods primarily focus on 2D images depicting a single body part, notably chest X-rays. In this paper, we extend the scope of Med-VLP to encompass 3D images, specifically targeting full-body scenarios, by using a multimodal dataset of CT images and reports. Compared with the 2D counterpart, 3D VLP is required to effectively capture essential semantics from significantly sparser representation in 3D imaging. In this paper, we introduce CT-GLIP (Grounded Language-Image Pretraining with CT scans), a novel method that constructs organ-level image-text pairs to enhance multimodal contrastive learning, aligning grounded visual features with precise diagnostic text. Additionally, we developed an abnormality dictionary to augment contrastive learning with diverse negative samples. Our method, trained on a multimodal CT dataset comprising 44,011 organ-level vision-text pairs from 17,702 patients across 104 organs, demonstrates it can identify organs and abnormalities in a zero-shot manner using natural languages. The performance of CT-GLIP is validated on a separate test set of 1,130 patients, focusing on the 16 most frequent abnormalities across 7 organs. The experimental results show our model's superior performance over the standard CLIP framework across zero-shot and fine-tuning scenarios, using both CNN and ViT architectures.","sentences":["Medical Vision-Language Pretraining (Med-VLP) establishes a connection between visual content from medical images and the relevant textual descriptions.","Existing Med-VLP methods primarily focus on 2D images depicting a single body part, notably chest X-rays.","In this paper, we extend the scope of Med-VLP to encompass 3D images, specifically targeting full-body scenarios, by using a multimodal dataset of CT images and reports.","Compared with the 2D counterpart, 3D VLP is required to effectively capture essential semantics from significantly sparser representation in 3D imaging.","In this paper, we introduce CT-GLIP (Grounded Language-Image Pretraining with CT scans), a novel method that constructs organ-level image-text pairs to enhance multimodal contrastive learning, aligning grounded visual features with precise diagnostic text.","Additionally, we developed an abnormality dictionary to augment contrastive learning with diverse negative samples.","Our method, trained on a multimodal CT dataset comprising 44,011 organ-level vision-text pairs from 17,702 patients across 104 organs, demonstrates it can identify organs and abnormalities in a zero-shot manner using natural languages.","The performance of CT-GLIP is validated on a separate test set of 1,130 patients, focusing on the 16 most frequent abnormalities across 7 organs.","The experimental results show our model's superior performance over the standard CLIP framework across zero-shot and fine-tuning scenarios, using both CNN and ViT architectures."],"url":"http://arxiv.org/abs/2404.15272v1","category":"cs.CV"}
{"created":"2024-04-23 17:58:33","title":"Automatic Layout Planning for Visually-Rich Documents with Instruction-Following Models","abstract":"Recent advancements in instruction-following models have made user interactions with models more user-friendly and efficient, broadening their applicability. In graphic design, non-professional users often struggle to create visually appealing layouts due to limited skills and resources. In this work, we introduce a novel multimodal instruction-following framework for layout planning, allowing users to easily arrange visual elements into tailored layouts by specifying canvas size and design purpose, such as for book covers, posters, brochures, or menus. We developed three layout reasoning tasks to train the model in understanding and executing layout instructions. Experiments on two benchmarks show that our method not only simplifies the design process for non-professionals but also surpasses the performance of few-shot GPT-4V models, with mIoU higher by 12% on Crello. This progress highlights the potential of multimodal instruction-following models to automate and simplify the design process, providing an approachable solution for a wide range of design tasks on visually-rich documents.","sentences":["Recent advancements in instruction-following models have made user interactions with models more user-friendly and efficient, broadening their applicability.","In graphic design, non-professional users often struggle to create visually appealing layouts due to limited skills and resources.","In this work, we introduce a novel multimodal instruction-following framework for layout planning, allowing users to easily arrange visual elements into tailored layouts by specifying canvas size and design purpose, such as for book covers, posters, brochures, or menus.","We developed three layout reasoning tasks to train the model in understanding and executing layout instructions.","Experiments on two benchmarks show that our method not only simplifies the design process for non-professionals but also surpasses the performance of few-shot GPT-4V models, with mIoU higher by 12% on Crello.","This progress highlights the potential of multimodal instruction-following models to automate and simplify the design process, providing an approachable solution for a wide range of design tasks on visually-rich documents."],"url":"http://arxiv.org/abs/2404.15271v1","category":"cs.CV"}
{"created":"2024-04-23 17:57:47","title":"Aligning LLM Agents by Learning Latent Preference from User Edits","abstract":"We study interactive learning of language agents based on user edits made to the agent's output. In a typical setting such as writing assistants, the user interacts with a language agent to generate a response given a context, and may optionally edit the agent response to personalize it based on their latent preference, in addition to improving the correctness. The edit feedback is naturally generated, making it a suitable candidate for improving the agent's alignment with the user's preference, and for reducing the cost of user edits over time. We propose a learning framework, PRELUDE that infers a description of the user's latent preference based on historic edit data and using it to define a prompt policy that drives future response generation. This avoids fine-tuning the agent, which is costly, challenging to scale with the number of users, and may even degrade its performance on other tasks. Furthermore, learning descriptive preference improves interpretability, allowing the user to view and modify the learned preference. However, user preference can be complex and vary based on context, making it challenging to learn. To address this, we propose a simple yet effective algorithm named CIPHER that leverages a large language model (LLM) to infer the user preference for a given context based on user edits. In the future, CIPHER retrieves inferred preferences from the k-closest contexts in the history, and forms an aggregate preference for response generation. We introduce two interactive environments -- summarization and email writing, for evaluation using a GPT-4 simulated user. We compare with algorithms that directly retrieve user edits but do not learn descriptive preference, and algorithms that learn context-agnostic preference. On both tasks, CIPHER achieves the lowest edit distance cost and learns preferences that show significant similarity to the ground truth preferences","sentences":["We study interactive learning of language agents based on user edits made to the agent's output.","In a typical setting such as writing assistants, the user interacts with a language agent to generate a response given a context, and may optionally edit the agent response to personalize it based on their latent preference, in addition to improving the correctness.","The edit feedback is naturally generated, making it a suitable candidate for improving the agent's alignment with the user's preference, and for reducing the cost of user edits over time.","We propose a learning framework, PRELUDE that infers a description of the user's latent preference based on historic edit data and using it to define a prompt policy that drives future response generation.","This avoids fine-tuning the agent, which is costly, challenging to scale with the number of users, and may even degrade its performance on other tasks.","Furthermore, learning descriptive preference improves interpretability, allowing the user to view and modify the learned preference.","However, user preference can be complex and vary based on context, making it challenging to learn.","To address this, we propose a simple yet effective algorithm named CIPHER that leverages a large language model (LLM) to infer the user preference for a given context based on user edits.","In the future, CIPHER retrieves inferred preferences from the k-closest contexts in the history, and forms an aggregate preference for response generation.","We introduce two interactive environments -- summarization and email writing, for evaluation using a GPT-4 simulated user.","We compare with algorithms that directly retrieve user edits but do not learn descriptive preference, and algorithms that learn context-agnostic preference.","On both tasks, CIPHER achieves the lowest edit distance cost and learns preferences that show significant similarity to the ground truth preferences"],"url":"http://arxiv.org/abs/2404.15269v1","category":"cs.CL"}
{"created":"2024-04-23 17:57:18","title":"The decays $\u03a3^{+}\\to p \\ell^{+}\\ell^{-}$ within the standard model and beyond","abstract":"Motivated by the LHCb measurement of the hyperon decay mode $\\Sigma^+\\to p\\mu^+\\mu^-$ and prospects for improvement, we revisit the estimates for the rate and forward-backward asymmetry within the standard model and beyond. The standard model prediction has a fourfold ambiguity, and we suggest ways to resolve it with other measurements, including possible studies of $\\Sigma^+\\to p e^+e^-$ in the BESIII and LHCb experiments. We use the recent BESIII measurements of $\\Sigma^+\\to p \\gamma$ and $\\Sigma^+\\to p\\pi^0$ to reduce the uncertainty in the long-distance contribution to $\\Sigma^+\\to p\\mu^+\\mu^-$. Beyond the standard model, we consider a general effective Hamiltonian at low energy with ten operators whose Wilson coefficients parametrize the new physics. We derive expressions for the $\\Sigma^+\\to p\\mu^+\\mu^-$ rate and the associated muon forward-backward asymmetry in terms of these coefficients. Finally, we implement the existing constraints from kaon decays, pointing out the extent to which this hyperon mode can constrain the directions in parameter space to which the kaons are not sensitive.","sentences":["Motivated by the LHCb measurement of the hyperon decay mode $\\Sigma^+\\to p\\mu^+\\mu^-$ and prospects for improvement, we revisit the estimates for the rate and forward-backward asymmetry within the standard model and beyond.","The standard model prediction has a fourfold ambiguity, and we suggest ways to resolve it with other measurements, including possible studies of $\\Sigma^+\\to p e^+e^-$ in the BESIII and LHCb experiments.","We use the recent BESIII measurements of $\\Sigma^+\\to p \\gamma$ and $\\Sigma^+\\to p\\pi^0$ to reduce the uncertainty in the long-distance contribution to $\\Sigma^+\\to p\\mu^+\\mu^-$. Beyond the standard model, we consider a general effective Hamiltonian at low energy with ten operators whose Wilson coefficients parametrize the new physics.","We derive expressions for the $\\Sigma^+\\to p\\mu^+\\mu^-$ rate and the associated muon forward-backward asymmetry in terms of these coefficients.","Finally, we implement the existing constraints from kaon decays, pointing out the extent to which this hyperon mode can constrain the directions in parameter space to which the kaons are not sensitive."],"url":"http://arxiv.org/abs/2404.15268v1","category":"hep-ph"}
{"created":"2024-04-23 17:56:08","title":"From Parts to Whole: A Unified Reference Framework for Controllable Human Image Generation","abstract":"Recent advancements in controllable human image generation have led to zero-shot generation using structural signals (e.g., pose, depth) or facial appearance. Yet, generating human images conditioned on multiple parts of human appearance remains challenging. Addressing this, we introduce Parts2Whole, a novel framework designed for generating customized portraits from multiple reference images, including pose images and various aspects of human appearance. To achieve this, we first develop a semantic-aware appearance encoder to retain details of different human parts, which processes each image based on its textual label to a series of multi-scale feature maps rather than one image token, preserving the image dimension. Second, our framework supports multi-image conditioned generation through a shared self-attention mechanism that operates across reference and target features during the diffusion process. We enhance the vanilla attention mechanism by incorporating mask information from the reference human images, allowing for the precise selection of any part. Extensive experiments demonstrate the superiority of our approach over existing alternatives, offering advanced capabilities for multi-part controllable human image customization. See our project page at https://huanngzh.github.io/Parts2Whole/.","sentences":["Recent advancements in controllable human image generation have led to zero-shot generation using structural signals (e.g., pose, depth) or facial appearance.","Yet, generating human images conditioned on multiple parts of human appearance remains challenging.","Addressing this, we introduce Parts2Whole, a novel framework designed for generating customized portraits from multiple reference images, including pose images and various aspects of human appearance.","To achieve this, we first develop a semantic-aware appearance encoder to retain details of different human parts, which processes each image based on its textual label to a series of multi-scale feature maps rather than one image token, preserving the image dimension.","Second, our framework supports multi-image conditioned generation through a shared self-attention mechanism that operates across reference and target features during the diffusion process.","We enhance the vanilla attention mechanism by incorporating mask information from the reference human images, allowing for the precise selection of any part.","Extensive experiments demonstrate the superiority of our approach over existing alternatives, offering advanced capabilities for multi-part controllable human image customization.","See our project page at https://huanngzh.github.io/Parts2Whole/."],"url":"http://arxiv.org/abs/2404.15267v1","category":"cs.CV"}
{"created":"2024-04-23 17:55:39","title":"Color-Kinematic Numerators for Fermion Compton Amplitudes","abstract":"We introduce a novel approach to compute Compton amplitudes involving a fermion pair inspired by Hopf algebra amplitude constructions. This approach features a recursive relation employing quasi-shuffle sets, directly verifiable by massive factorization properties. We derive results for minimal gauge invariant color-kinematic numerators with physical massive poles using this method. We have also deduced a graphical method for deriving numerators that simplifies the numerator generation and eliminates redundancies, thus providing several computational advantages.","sentences":["We introduce a novel approach to compute Compton amplitudes involving a fermion pair inspired by Hopf algebra amplitude constructions.","This approach features a recursive relation employing quasi-shuffle sets, directly verifiable by massive factorization properties.","We derive results for minimal gauge invariant color-kinematic numerators with physical massive poles using this method.","We have also deduced a graphical method for deriving numerators that simplifies the numerator generation and eliminates redundancies, thus providing several computational advantages."],"url":"http://arxiv.org/abs/2404.15265v1","category":"hep-th"}
{"created":"2024-04-23 17:52:16","title":"An Alternative Method to Identify the Susceptibility Threshold Level of Device under Test in a Reverberation Chamber","abstract":"By counting the number of pass/fail occurrences of a DUT (Device under Test) in the stirring process in a reverberation chamber (RC), the threshold electric field (E-field) level can be well estimated without tuning the input power and repeating the whole testing many times. The Monte-Carlo method is used to verify the results. Estimated values and uncertainties are given for Rayleigh distributed fields and for Rice distributed fields with different K-factors.","sentences":["By counting the number of pass/fail occurrences of a DUT (Device under Test) in the stirring process in a reverberation chamber (RC), the threshold electric field (E-field) level can be well estimated without tuning the input power and repeating the whole testing many times.","The Monte-Carlo method is used to verify the results.","Estimated values and uncertainties are given for Rayleigh distributed fields and for Rice distributed fields with different K-factors."],"url":"http://arxiv.org/abs/2404.15262v1","category":"eess.SP"}
{"created":"2024-04-23 17:47:31","title":"Distributed Architecture for FPGA-based Superconducting Qubit Control","abstract":"Quantum circuits utilizing real time feedback techniques (such as active reset and mid-circuit measurement) are a powerful tool for NISQ-era quantum computing. Such techniques are crucial for implementing error correction protocols, and can reduce the resource requirements of certain quantum algorithms. Realizing these capabilities requires flexible, low-latency classical control. We have developed a custom FPGA-based processor architecture for QubiC, an open source platform for superconducting qubit control. Our architecture is distributed in nature, and consists of a bank of lightweight cores, each configured to control a small (1-3) number of signal generator channels. Each core is capable of executing parameterized control and readout pulses, as well as performing arbitrary control flow based on mid-circuit measurement results. We have also developed a modular compiler stack and domain-specific intermediate representation for programming the processor. Our representation allows users to specify circuits using both gate and pulse-level abstractions, and includes high-level control flow constructs (e.g. if-else blocks and loops). The compiler stack is designed to integrate with quantum software tools and programming languages, such as TrueQ, pyGSTi, and OpenQASM3. In this work, we will detail the design of both the processor and compiler stack, and demonstrate its capabilities with a quantum state teleportation experiment using transmon qubits at the LBNL Advanced Quantum Testbed.","sentences":["Quantum circuits utilizing real time feedback techniques (such as active reset and mid-circuit measurement) are a powerful tool for NISQ-era quantum computing.","Such techniques are crucial for implementing error correction protocols, and can reduce the resource requirements of certain quantum algorithms.","Realizing these capabilities requires flexible, low-latency classical control.","We have developed a custom FPGA-based processor architecture for QubiC, an open source platform for superconducting qubit control.","Our architecture is distributed in nature, and consists of a bank of lightweight cores, each configured to control a small (1-3) number of signal generator channels.","Each core is capable of executing parameterized control and readout pulses, as well as performing arbitrary control flow based on mid-circuit measurement results.","We have also developed a modular compiler stack and domain-specific intermediate representation for programming the processor.","Our representation allows users to specify circuits using both gate and pulse-level abstractions, and includes high-level control flow constructs (e.g. if-else blocks and loops).","The compiler stack is designed to integrate with quantum software tools and programming languages, such as TrueQ, pyGSTi, and OpenQASM3.","In this work, we will detail the design of both the processor and compiler stack, and demonstrate its capabilities with a quantum state teleportation experiment using transmon qubits at the LBNL Advanced Quantum Testbed."],"url":"http://arxiv.org/abs/2404.15260v1","category":"quant-ph"}
{"created":"2024-04-23 17:45:53","title":"Score matching for sub-Riemannian bridge sampling","abstract":"Simulation of conditioned diffusion processes is an essential tool in inference for stochastic processes, data imputation, generative modelling, and geometric statistics. Whilst simulating diffusion bridge processes is already difficult on Euclidean spaces, when considering diffusion processes on Riemannian manifolds the geometry brings in further complications. In even higher generality, advancing from Riemannian to sub-Riemannian geometries introduces hypoellipticity, and the possibility of finding appropriate explicit approximations for the score of the diffusion process is removed. We handle these challenges and construct a method for bridge simulation on sub-Riemannian manifolds by demonstrating how recent progress in machine learning can be modified to allow for training of score approximators on sub-Riemannian manifolds. Since gradients dependent on the horizontal distribution, we generalise the usual notion of denoising loss to work with non-holonomic frames using a stochastic Taylor expansion, and we demonstrate the resulting scheme both explicitly on the Heisenberg group and more generally using adapted coordinates. We perform numerical experiments exemplifying samples from the bridge process on the Heisenberg group and the concentration of this process for small time.","sentences":["Simulation of conditioned diffusion processes is an essential tool in inference for stochastic processes, data imputation, generative modelling, and geometric statistics.","Whilst simulating diffusion bridge processes is already difficult on Euclidean spaces, when considering diffusion processes on Riemannian manifolds the geometry brings in further complications.","In even higher generality, advancing from Riemannian to sub-Riemannian geometries introduces hypoellipticity, and the possibility of finding appropriate explicit approximations for the score of the diffusion process is removed.","We handle these challenges and construct a method for bridge simulation on sub-Riemannian manifolds by demonstrating how recent progress in machine learning can be modified to allow for training of score approximators on sub-Riemannian manifolds.","Since gradients dependent on the horizontal distribution, we generalise the usual notion of denoising loss to work with non-holonomic frames using a stochastic Taylor expansion, and we demonstrate the resulting scheme both explicitly on the Heisenberg group and more generally using adapted coordinates.","We perform numerical experiments exemplifying samples from the bridge process on the Heisenberg group and the concentration of this process for small time."],"url":"http://arxiv.org/abs/2404.15258v1","category":"math.PR"}
{"created":"2024-04-23 17:42:45","title":"TOP-Nav: Legged Navigation Integrating Terrain, Obstacle and Proprioception Estimation","abstract":"Legged navigation is typically examined within open-world, off-road, and challenging environments. In these scenarios, estimating external disturbances requires a complex synthesis of multi-modal information. This underlines a major limitation in existing works that primarily focus on avoiding obstacles. In this work, we propose TOP-Nav, a novel legged navigation framework that integrates a comprehensive path planner with Terrain awareness, Obstacle avoidance and close-loop Proprioception. TOP-Nav underscores the synergies between vision and proprioception in both path and motion planning. Within the path planner, we present and integrate a terrain estimator that enables the robot to select waypoints on terrains with higher traversability while effectively avoiding obstacles. In the motion planning level, we not only implement a locomotion controller to track the navigation commands, but also construct a proprioception advisor to provide motion evaluations for the path planner. Based on the close-loop motion feedback, we make online corrections for the vision-based terrain and obstacle estimations. Consequently, TOP-Nav achieves open-world navigation that the robot can handle terrains or disturbances beyond the distribution of prior knowledge and overcomes constraints imposed by visual conditions. Building upon extensive experiments conducted in both simulation and real-world environments, TOP-Nav demonstrates superior performance in open-world navigation compared to existing methods.","sentences":["Legged navigation is typically examined within open-world, off-road, and challenging environments.","In these scenarios, estimating external disturbances requires a complex synthesis of multi-modal information.","This underlines a major limitation in existing works that primarily focus on avoiding obstacles.","In this work, we propose TOP-Nav, a novel legged navigation framework that integrates a comprehensive path planner with Terrain awareness, Obstacle avoidance and close-loop Proprioception.","TOP-Nav underscores the synergies between vision and proprioception in both path and motion planning.","Within the path planner, we present and integrate a terrain estimator that enables the robot to select waypoints on terrains with higher traversability while effectively avoiding obstacles.","In the motion planning level, we not only implement a locomotion controller to track the navigation commands, but also construct a proprioception advisor to provide motion evaluations for the path planner.","Based on the close-loop motion feedback, we make online corrections for the vision-based terrain and obstacle estimations.","Consequently, TOP-Nav achieves open-world navigation that the robot can handle terrains or disturbances beyond the distribution of prior knowledge and overcomes constraints imposed by visual conditions.","Building upon extensive experiments conducted in both simulation and real-world environments, TOP-Nav demonstrates superior performance in open-world navigation compared to existing methods."],"url":"http://arxiv.org/abs/2404.15256v2","category":"cs.RO"}
{"created":"2024-04-23 17:32:24","title":"XFT: Unlocking the Power of Code Instruction Tuning by Simply Merging Upcycled Mixture-of-Experts","abstract":"We introduce XFT, a simple yet powerful training scheme, by simply merging upcycled Mixture-of-Experts (MoE) to unleash the performance limit of instruction-tuned code Large Language Models (LLMs). While vanilla sparse upcycling fails to improve instruction tuning, XFT introduces a shared expert mechanism with a novel routing weight normalization strategy into sparse upcycling, which significantly boosts instruction tuning. After fine-tuning the upcycled MoE model, XFT introduces a learnable model merging mechanism to compile the upcycled MoE model back to a dense model, achieving upcycled MoE-level performance with only dense-model compute. By applying XFT to a 1.3B model, we create a new state-of-the-art tiny code LLM (<3B) with 67.1 and 64.6 pass@1 on HumanEval and HumanEval+ respectively. With the same data and model architecture, XFT improves supervised fine-tuning (SFT) by 13% on HumanEval+, along with consistent improvements from 2% to 13% on MBPP+, MultiPL-E, and DS-1000, demonstrating its generalizability. XFT is fully orthogonal to existing techniques such as Evol-Instruct and OSS-Instruct, opening a new dimension for improving code instruction tuning. Codes are available at https://github.com/ise-uiuc/xft .","sentences":["We introduce XFT, a simple yet powerful training scheme, by simply merging upcycled Mixture-of-Experts (MoE) to unleash the performance limit of instruction-tuned code Large Language Models (LLMs).","While vanilla sparse upcycling fails to improve instruction tuning, XFT introduces a shared expert mechanism with a novel routing weight normalization strategy into sparse upcycling, which significantly boosts instruction tuning.","After fine-tuning the upcycled MoE model, XFT introduces a learnable model merging mechanism to compile the upcycled MoE model back to a dense model, achieving upcycled MoE-level performance with only dense-model compute.","By applying XFT to a 1.3B model, we create a new state-of-the-art tiny code LLM (<3B) with 67.1 and 64.6 pass@1 on HumanEval and HumanEval+ respectively.","With the same data and model architecture, XFT improves supervised fine-tuning (SFT) by 13% on HumanEval+, along with consistent improvements from 2% to 13% on MBPP+, MultiPL-E, and DS-1000, demonstrating its generalizability.","XFT is fully orthogonal to existing techniques such as Evol-Instruct and OSS-Instruct, opening a new dimension for improving code instruction tuning.","Codes are available at https://github.com/ise-uiuc/xft ."],"url":"http://arxiv.org/abs/2404.15247v1","category":"cs.CL"}
{"created":"2024-04-23 17:26:59","title":"Mining Invariance from Nonlinear Multi-Environment Data: Binary Classification","abstract":"Making predictions in an unseen environment given data from multiple training environments is a challenging task. We approach this problem from an invariance perspective, focusing on binary classification to shed light on general nonlinear data generation mechanisms. We identify a unique form of invariance that exists solely in a binary setting that allows us to train models invariant over environments. We provide sufficient conditions for such invariance and show it is robust even when environmental conditions vary greatly. Our formulation admits a causal interpretation, allowing us to compare it with various frameworks. Finally, we propose a heuristic prediction method and conduct experiments using real and synthetic datasets.","sentences":["Making predictions in an unseen environment given data from multiple training environments is a challenging task.","We approach this problem from an invariance perspective, focusing on binary classification to shed light on general nonlinear data generation mechanisms.","We identify a unique form of invariance that exists solely in a binary setting that allows us to train models invariant over environments.","We provide sufficient conditions for such invariance and show it is robust even when environmental conditions vary greatly.","Our formulation admits a causal interpretation, allowing us to compare it with various frameworks.","Finally, we propose a heuristic prediction method and conduct experiments using real and synthetic datasets."],"url":"http://arxiv.org/abs/2404.15245v1","category":"stat.ME"}
{"created":"2024-04-23 17:25:35","title":"A Hybrid Kernel-Free Boundary Integral Method with Operator Learning for Solving Parametric Partial Differential Equations In Complex Domains","abstract":"The Kernel-Free Boundary Integral (KFBI) method presents an iterative solution to boundary integral equations arising from elliptic partial differential equations (PDEs). This method effectively addresses elliptic PDEs on irregular domains, including the modified Helmholtz, Stokes, and elasticity equations. The rapid evolution of neural networks and deep learning has invigorated the exploration of numerical PDEs. An increasing interest is observed in deep learning approaches that seamlessly integrate mathematical principles for investigating numerical PDEs. We propose a hybrid KFBI method, integrating the foundational principles of the KFBI method with the capabilities of deep learning. This approach, within the framework of the boundary integral method, designs a network to approximate the solution operator for the corresponding integral equations by mapping the parameters, inhomogeneous terms and boundary information of PDEs to the boundary density functions, which can be regarded as the solution of the integral equations. The models are trained using data generated by the Cartesian grid-based KFBI algorithm, exhibiting robust generalization capabilities. It accurately predicts density functions across diverse boundary conditions and parameters within the same class of equations. Experimental results demonstrate that the trained model can directly infer the boundary density function with satisfactory precision, obviating the need for iterative steps in solving boundary integral equations. Furthermore, applying the inference results of the model as initial values for iterations is also reasonable; this approach can retain the inherent second-order accuracy of the KFBI method while accelerating the traditional KFBI approach by reducing about 50% iterations.","sentences":["The Kernel-Free Boundary Integral (KFBI) method presents an iterative solution to boundary integral equations arising from elliptic partial differential equations (PDEs).","This method effectively addresses elliptic PDEs on irregular domains, including the modified Helmholtz, Stokes, and elasticity equations.","The rapid evolution of neural networks and deep learning has invigorated the exploration of numerical PDEs.","An increasing interest is observed in deep learning approaches that seamlessly integrate mathematical principles for investigating numerical PDEs.","We propose a hybrid KFBI method, integrating the foundational principles of the KFBI method with the capabilities of deep learning.","This approach, within the framework of the boundary integral method, designs a network to approximate the solution operator for the corresponding integral equations by mapping the parameters, inhomogeneous terms and boundary information of PDEs to the boundary density functions, which can be regarded as the solution of the integral equations.","The models are trained using data generated by the Cartesian grid-based KFBI algorithm, exhibiting robust generalization capabilities.","It accurately predicts density functions across diverse boundary conditions and parameters within the same class of equations.","Experimental results demonstrate that the trained model can directly infer the boundary density function with satisfactory precision, obviating the need for iterative steps in solving boundary integral equations.","Furthermore, applying the inference results of the model as initial values for iterations is also reasonable; this approach can retain the inherent second-order accuracy of the KFBI method while accelerating the traditional KFBI approach by reducing about 50% iterations."],"url":"http://arxiv.org/abs/2404.15242v1","category":"cs.LG"}
{"created":"2024-04-23 17:16:08","title":"CultureBank: An Online Community-Driven Knowledge Base Towards Culturally Aware Language Technologies","abstract":"To enhance language models' cultural awareness, we design a generalizable pipeline to construct cultural knowledge bases from different online communities on a massive scale. With the pipeline, we construct CultureBank, a knowledge base built upon users' self-narratives with 12K cultural descriptors sourced from TikTok and 11K from Reddit. Unlike previous cultural knowledge resources, CultureBank contains diverse views on cultural descriptors to allow flexible interpretation of cultural knowledge, and contextualized cultural scenarios to help grounded evaluation. With CultureBank, we evaluate different LLMs' cultural awareness, and identify areas for improvement. We also fine-tune a language model on CultureBank: experiments show that it achieves better performances on two downstream cultural tasks in a zero-shot setting. Finally, we offer recommendations based on our findings for future culturally aware language technologies. The project page is https://culturebank.github.io . The code and model is at https://github.com/SALT-NLP/CultureBank . The released CultureBank dataset is at https://huggingface.co/datasets/SALT-NLP/CultureBank .","sentences":["To enhance language models' cultural awareness, we design a generalizable pipeline to construct cultural knowledge bases from different online communities on a massive scale.","With the pipeline, we construct CultureBank, a knowledge base built upon users' self-narratives with 12K cultural descriptors sourced from TikTok and 11K from Reddit.","Unlike previous cultural knowledge resources, CultureBank contains diverse views on cultural descriptors to allow flexible interpretation of cultural knowledge, and contextualized cultural scenarios to help grounded evaluation.","With CultureBank, we evaluate different LLMs' cultural awareness, and identify areas for improvement.","We also fine-tune a language model on CultureBank: experiments show that it achieves better performances on two downstream cultural tasks in a zero-shot setting.","Finally, we offer recommendations based on our findings for future culturally aware language technologies.","The project page is https://culturebank.github.io .","The code and model is at https://github.com/SALT-NLP/CultureBank .","The released CultureBank dataset is at https://huggingface.co/datasets/SALT-NLP/CultureBank ."],"url":"http://arxiv.org/abs/2404.15238v1","category":"cs.CL"}
{"created":"2024-04-23 17:13:05","title":"Insights into the defect-driven heterogeneous structural evolution of Ni-rich layered cathode in lithium-ion batteries","abstract":"Recently, considerable efforts have been made on research and improvement for Ni-rich lithium-ion batteries to meet the demand from vehicles and grid-level large-scale energy storage. Development of next-generation high-performance lithium-ion batteries requires a comprehensive understanding on the underlying electrochemical mechanisms associated with its structural evolution. In this work, advanced operando neutron diffraction and four-dimensional scanning transmission electron microscopy techniques are applied to clarify the structural evolution of electrodes in two distinct full cells with identical LiNi0.8Co0.1Mn0.1O2 cathode but different anode counterparts. It is found that both of cathodes in two cells exhibit non-intrinsic two-phase-like behavior at the early charge stage, indicating selective Li+ extraction from cathodes. But the heterogeneous evolution of cathode is less serious with graphite-silicon blended anode than that with graphite anode due to the different delithiation rate. Moreover, it is revealed that the formation of heterogeneous structure is led by the distribution of defects including Li/Ni disordering and microcracks, which should be inhibited by assembling appropriate anode to avoid potential threaten on cell performance. The present work unveils the origin of inhomogeneity in Ni-rich lithium-ion batteries and highlights the significance of kinetics control in electrodes for batteries with higher capacity and longer life.","sentences":["Recently, considerable efforts have been made on research and improvement for Ni-rich lithium-ion batteries to meet the demand from vehicles and grid-level large-scale energy storage.","Development of next-generation high-performance lithium-ion batteries requires a comprehensive understanding on the underlying electrochemical mechanisms associated with its structural evolution.","In this work, advanced operando neutron diffraction and four-dimensional scanning transmission electron microscopy techniques are applied to clarify the structural evolution of electrodes in two distinct full cells with identical LiNi0.8Co0.1Mn0.1O2 cathode but different anode counterparts.","It is found that both of cathodes in two cells exhibit non-intrinsic two-phase-like behavior at the early charge stage, indicating selective Li+ extraction from cathodes.","But the heterogeneous evolution of cathode is less serious with graphite-silicon blended anode than that with graphite anode due to the different delithiation rate.","Moreover, it is revealed that the formation of heterogeneous structure is led by the distribution of defects including Li/Ni disordering and microcracks, which should be inhibited by assembling appropriate anode to avoid potential threaten on cell performance.","The present work unveils the origin of inhomogeneity in Ni-rich lithium-ion batteries and highlights the significance of kinetics control in electrodes for batteries with higher capacity and longer life."],"url":"http://arxiv.org/abs/2404.15237v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-23 17:12:45","title":"Revisiting Unnaturalness for Automated Program Repair in the Era of Large Language Models","abstract":"Language models have improved by orders of magnitude with the recent emergence of Transformer-based Large Language Models (LLMs). LLMs have demonstrated their ability to generate natural code that is highly similar to code written by professional developers. One intermediate value an LLM can emit is entropy, which measures the naturalness of a token of code. We hypothesize that entropy can be used to improve the performance of Automated Program Repair (APR) tasks. While much progress has been made in Automated Program Repair (APR), fault localization techniques suffer from a lack of diversity in ranking scores, patch generation tools tend to be inefficient as all tests need to run before determining if a patch is likely to be correct, and patch ranking often suffers from the test-suite over-fitting problem. However, using an LLM directly for APR introduces concerns for training data leakage. In this work, we introduce a novel way of using the entropy of LLMs in combination with prior APR tools to improve all stages of APR. We show that entropy is highly complementary with prior fault localization tools. Our proposed re-ranking method achieves a 50% Top-5 score improvement over SBFL. We propose a patch-naturalness measurement, entropy-delta, to improve the efficiency of template-based repair techniques by ranking plausible patches before undergoing testing. When using entropy-delta for patch ranking and classification, our proposed method can rank correct patches more effectively than state-of-the-art machine learning tools with an 49% improvement in Top-1. Our work suggests that LLMs can be an effective addition to compliment prior APR tasks while minimizing both the test-suite overfitting problem and the LLM data leakage problem.","sentences":["Language models have improved by orders of magnitude with the recent emergence of Transformer-based Large Language Models (LLMs).","LLMs have demonstrated their ability to generate natural code that is highly similar to code written by professional developers.","One intermediate value an LLM can emit is entropy, which measures the naturalness of a token of code.","We hypothesize that entropy can be used to improve the performance of Automated Program Repair (APR) tasks.","While much progress has been made in Automated Program Repair (APR), fault localization techniques suffer from a lack of diversity in ranking scores, patch generation tools tend to be inefficient as all tests need to run before determining if a patch is likely to be correct, and patch ranking often suffers from the test-suite over-fitting problem.","However, using an LLM directly for APR introduces concerns for training data leakage.","In this work, we introduce a novel way of using the entropy of LLMs in combination with prior APR tools to improve all stages of APR.","We show that entropy is highly complementary with prior fault localization tools.","Our proposed re-ranking method achieves a 50% Top-5 score improvement over SBFL.","We propose a patch-naturalness measurement, entropy-delta, to improve the efficiency of template-based repair techniques by ranking plausible patches before undergoing testing.","When using entropy-delta for patch ranking and classification, our proposed method can rank correct patches more effectively than state-of-the-art machine learning tools with an 49% improvement in Top-1.","Our work suggests that LLMs can be an effective addition to compliment prior APR tasks while minimizing both the test-suite overfitting problem and the LLM data leakage problem."],"url":"http://arxiv.org/abs/2404.15236v1","category":"cs.SE"}
{"created":"2024-04-23 17:10:49","title":"Massively Annotated Datasets for Assessment of Synthetic and Real Data in Face Recognition","abstract":"Face recognition applications have grown in parallel with the size of datasets, complexity of deep learning models and computational power. However, while deep learning models evolve to become more capable and computational power keeps increasing, the datasets available are being retracted and removed from public access. Privacy and ethical concerns are relevant topics within these domains. Through generative artificial intelligence, researchers have put efforts into the development of completely synthetic datasets that can be used to train face recognition systems. Nonetheless, the recent advances have not been sufficient to achieve performance comparable to the state-of-the-art models trained on real data. To study the drift between the performance of models trained on real and synthetic datasets, we leverage a massive attribute classifier (MAC) to create annotations for four datasets: two real and two synthetic. From these annotations, we conduct studies on the distribution of each attribute within all four datasets. Additionally, we further inspect the differences between real and synthetic datasets on the attribute set. When comparing through the Kullback-Leibler divergence we have found differences between real and synthetic samples. Interestingly enough, we have verified that while real samples suffice to explain the synthetic distribution, the opposite could not be further from being true.","sentences":["Face recognition applications have grown in parallel with the size of datasets, complexity of deep learning models and computational power.","However, while deep learning models evolve to become more capable and computational power keeps increasing, the datasets available are being retracted and removed from public access.","Privacy and ethical concerns are relevant topics within these domains.","Through generative artificial intelligence, researchers have put efforts into the development of completely synthetic datasets that can be used to train face recognition systems.","Nonetheless, the recent advances have not been sufficient to achieve performance comparable to the state-of-the-art models trained on real data.","To study the drift between the performance of models trained on real and synthetic datasets, we leverage a massive attribute classifier (MAC) to create annotations for four datasets: two real and two synthetic.","From these annotations, we conduct studies on the distribution of each attribute within all four datasets.","Additionally, we further inspect the differences between real and synthetic datasets on the attribute set.","When comparing through the Kullback-Leibler divergence we have found differences between real and synthetic samples.","Interestingly enough, we have verified that while real samples suffice to explain the synthetic distribution, the opposite could not be further from being true."],"url":"http://arxiv.org/abs/2404.15234v1","category":"cs.CV"}
{"created":"2024-04-23 17:03:53","title":"Direct Zernike Coefficient Prediction from Point Spread Functions and Extended Images using Deep Learning","abstract":"Optical imaging quality can be severely degraded by system and sample induced aberrations. Existing adaptive optics systems typically rely on iterative search algorithm to correct for aberrations and improve images. This study demonstrates the application of convolutional neural networks to characterise the optical aberration by directly predicting the Zernike coefficients from two to three phase-diverse optical images. We evaluated our network on 600,000 simulated Point Spread Function (PSF) datasets randomly generated within the range of -1 to 1 radians using the first 25 Zernike coefficients. The results show that using only three phase-diverse images captured above, below and at the focal plane with an amplitude of 1 achieves a low RMSE of 0.10 radians on the simulated PSF dataset. Furthermore, this approach directly predicts Zernike modes simulated extended 2D samples, while maintaining a comparable RMSE of 0.15 radians. We demonstrate that this approach is effective using only a single prediction step, or can be iterated a small number of times. This simple and straightforward technique provides rapid and accurate method for predicting the aberration correction using three or less phase-diverse images, paving the way for evaluation on real-world dataset.","sentences":["Optical imaging quality can be severely degraded by system and sample induced aberrations.","Existing adaptive optics systems typically rely on iterative search algorithm to correct for aberrations and improve images.","This study demonstrates the application of convolutional neural networks to characterise the optical aberration by directly predicting the Zernike coefficients from two to three phase-diverse optical images.","We evaluated our network on 600,000 simulated Point Spread Function (PSF) datasets randomly generated within the range of -1 to 1 radians using the first 25 Zernike coefficients.","The results show that using only three phase-diverse images captured above, below and at the focal plane with an amplitude of 1 achieves a low RMSE of 0.10 radians on the simulated PSF dataset.","Furthermore, this approach directly predicts Zernike modes simulated extended 2D samples, while maintaining a comparable RMSE of 0.15 radians.","We demonstrate that this approach is effective using only a single prediction step, or can be iterated a small number of times.","This simple and straightforward technique provides rapid and accurate method for predicting the aberration correction using three or less phase-diverse images, paving the way for evaluation on real-world dataset."],"url":"http://arxiv.org/abs/2404.15231v2","category":"physics.optics"}
{"created":"2024-04-23 16:59:02","title":"Re-Thinking Inverse Graphics With Large Language Models","abstract":"Inverse graphics -- the task of inverting an image into physical variables that, when rendered, enable reproduction of the observed scene -- is a fundamental challenge in computer vision and graphics. Disentangling an image into its constituent elements, such as the shape, color, and material properties of the objects of the 3D scene that produced it, requires a comprehensive understanding of the environment. This requirement limits the ability of existing carefully engineered approaches to generalize across domains. Inspired by the zero-shot ability of large language models (LLMs) to generalize to novel contexts, we investigate the possibility of leveraging the broad world knowledge encoded in such models in solving inverse-graphics problems. To this end, we propose the Inverse-Graphics Large Language Model (IG-LLM), an inverse-graphics framework centered around an LLM, that autoregressively decodes a visual embedding into a structured, compositional 3D-scene representation. We incorporate a frozen pre-trained visual encoder and a continuous numeric head to enable end-to-end training. Through our investigation, we demonstrate the potential of LLMs to facilitate inverse graphics through next-token prediction, without the use of image-space supervision. Our analysis opens up new possibilities for precise spatial reasoning about images that exploit the visual knowledge of LLMs. We will release our code and data to ensure the reproducibility of our investigation and to facilitate future research at https://ig-llm.is.tue.mpg.de/","sentences":["Inverse graphics -- the task of inverting an image into physical variables that, when rendered, enable reproduction of the observed scene -- is a fundamental challenge in computer vision and graphics.","Disentangling an image into its constituent elements, such as the shape, color, and material properties of the objects of the 3D scene that produced it, requires a comprehensive understanding of the environment.","This requirement limits the ability of existing carefully engineered approaches to generalize across domains.","Inspired by the zero-shot ability of large language models (LLMs) to generalize to novel contexts, we investigate the possibility of leveraging the broad world knowledge encoded in such models in solving inverse-graphics problems.","To this end, we propose the Inverse-Graphics Large Language Model (IG-LLM), an inverse-graphics framework centered around an LLM, that autoregressively decodes a visual embedding into a structured, compositional 3D-scene representation.","We incorporate a frozen pre-trained visual encoder and a continuous numeric head to enable end-to-end training.","Through our investigation, we demonstrate the potential of LLMs to facilitate inverse graphics through next-token prediction, without the use of image-space supervision.","Our analysis opens up new possibilities for precise spatial reasoning about images that exploit the visual knowledge of LLMs.","We will release our code and data to ensure the reproducibility of our investigation and to facilitate future research at https://ig-llm.is.tue.mpg.de/"],"url":"http://arxiv.org/abs/2404.15228v1","category":"cs.CV"}
{"created":"2024-04-23 16:55:32","title":"Revisiting Granular Models of Firm Growth","abstract":"We revisit \"granular models of firm growth\" that have been proposed in the literature to explain the anomalously slow decrease of growth volatility with firms size and how this phenomenon shapes the distribution of their growth rates. In these models, firms' sales are viewed as collections of independent \"sub-units\", and these non-trivial statistical properties occur as a direct result of the fat-tailed distribution of the number or sizes of these sub-units.   We present and discuss new theoretical results on the relation between firm size and growth rate statistics. Our results can be understood by noting that granular models imply the existence of three types of firms: well-diversified firms, with a size evenly distributed among several sub-units; firms with many sub-units but with their total size concentrated on only a handful of them, and lastly firms which are poorly diversified simply because they are made up of a small number of sub-units.   We establish new empirical facts about growth rates and their relation with size. As predicted by the model, the distribution of growth rate volatilities is to a good approximation {independent of firm size}, once rescaled by the average size-conditioned volatility. However, the tail of this distribution is much too thin to be consistent with a granular mechanism. Moreover, the moments of growth volatility scale with size in a way that is at odds with theoretical predictions. We also find that the distribution of growth rates rescaled by firm-specific volatility, which is predicted to be Gaussian by all the models we consider, remains very fat-tailed in the data, even for large firms. This paper, in ruling out the granularity scenario, suggests that the overarching mechanisms underlying the growth of firms are not satisfactorily understood, and argues that they deserve further theoretical investigations.","sentences":["We revisit \"granular models of firm growth\" that have been proposed in the literature to explain the anomalously slow decrease of growth volatility with firms size and how this phenomenon shapes the distribution of their growth rates.","In these models, firms' sales are viewed as collections of independent \"sub-units\", and these non-trivial statistical properties occur as a direct result of the fat-tailed distribution of the number or sizes of these sub-units.   ","We present and discuss new theoretical results on the relation between firm size and growth rate statistics.","Our results can be understood by noting that granular models imply the existence of three types of firms: well-diversified firms, with a size evenly distributed among several sub-units; firms with many sub-units but with their total size concentrated on only a handful of them, and lastly firms which are poorly diversified simply because they are made up of a small number of sub-units.   ","We establish new empirical facts about growth rates and their relation with size.","As predicted by the model, the distribution of growth rate volatilities is to a good approximation {independent of firm size}, once rescaled by the average size-conditioned volatility.","However, the tail of this distribution is much too thin to be consistent with a granular mechanism.","Moreover, the moments of growth volatility scale with size in a way that is at odds with theoretical predictions.","We also find that the distribution of growth rates rescaled by firm-specific volatility, which is predicted to be Gaussian by all the models we consider, remains very fat-tailed in the data, even for large firms.","This paper, in ruling out the granularity scenario, suggests that the overarching mechanisms underlying the growth of firms are not satisfactorily understood, and argues that they deserve further theoretical investigations."],"url":"http://arxiv.org/abs/2404.15226v1","category":"econ.GN"}
{"created":"2024-04-23 16:54:31","title":"Deep Models for Multi-View 3D Object Recognition: A Review","abstract":"Human decision-making often relies on visual information from multiple perspectives or views. In contrast, machine learning-based object recognition utilizes information from a single image of the object. However, the information conveyed by a single image may not be sufficient for accurate decision-making, particularly in complex recognition problems. The utilization of multi-view 3D representations for object recognition has thus far demonstrated the most promising results for achieving state-of-the-art performance. This review paper comprehensively covers recent progress in multi-view 3D object recognition methods for 3D classification and retrieval tasks. Specifically, we focus on deep learning-based and transformer-based techniques, as they are widely utilized and have achieved state-of-the-art performance. We provide detailed information about existing deep learning-based and transformer-based multi-view 3D object recognition models, including the most commonly used 3D datasets, camera configurations and number of views, view selection strategies, pre-trained CNN architectures, fusion strategies, and recognition performance on 3D classification and 3D retrieval tasks. Additionally, we examine various computer vision applications that use multi-view classification. Finally, we highlight key findings and future directions for developing multi-view 3D object recognition methods to provide readers with a comprehensive understanding of the field.","sentences":["Human decision-making often relies on visual information from multiple perspectives or views.","In contrast, machine learning-based object recognition utilizes information from a single image of the object.","However, the information conveyed by a single image may not be sufficient for accurate decision-making, particularly in complex recognition problems.","The utilization of multi-view 3D representations for object recognition has thus far demonstrated the most promising results for achieving state-of-the-art performance.","This review paper comprehensively covers recent progress in multi-view 3D object recognition methods for 3D classification and retrieval tasks.","Specifically, we focus on deep learning-based and transformer-based techniques, as they are widely utilized and have achieved state-of-the-art performance.","We provide detailed information about existing deep learning-based and transformer-based multi-view 3D object recognition models, including the most commonly used 3D datasets, camera configurations and number of views, view selection strategies, pre-trained CNN architectures, fusion strategies, and recognition performance on 3D classification and 3D retrieval tasks.","Additionally, we examine various computer vision applications that use multi-view classification.","Finally, we highlight key findings and future directions for developing multi-view 3D object recognition methods to provide readers with a comprehensive understanding of the field."],"url":"http://arxiv.org/abs/2404.15224v1","category":"cs.CV"}
{"created":"2024-04-23 16:37:19","title":"Impact of Ricci Inverse Gravity on Hybrid Star Model","abstract":"The objective of our current study is to explore novel aspects of a stationary anisotropic relativistic hybrid compact star that consists of quark matter (QM) in its core and ordinary baryonic matter (OBM) in its crust. This study has been done by adopting separate equations of states (EoSs) for quark matter and baryonic matter. The MIT bag model equation of state $p_{q}=\\frac{1}{3}(\\rho_{q}-4B)$ has been used to demonstrate a correlation between the density and pressure of weird quark matter in the interior of the star. In addition, we present a simple linear equation of state $p_{r}=\\beta_{1}\\rho-\\beta$ that links radial pressure and matter density for OBM. The stellar model was formulated within the context of $f(\\mathcal{R},\\mathcal{A})$ gravity, utilizing a linear correlation between Ricci tensor $\\mathcal{R}$ and anticurvature scaler $\\mathcal{A}$.To solve the field equations of this novel alternative gravity, we employ the Krori and Barua approach to the metric potentials. The validity of our suggested model is assessed using the graphical approach, while ensuring that the conditions are physically feasible. Our focus is specifically on the tiny celestial object known as LMC X-4 [$\\text{M} = (1.04^{+0.09}_{-0.09})M_{\\odot};\\text{R}=8.301^{+0.2}_{-0.2}\\text{km}$], which we consider a viable candidate for a strange quark star. We want to clarify the model's physical validity by examining a variety of physical assessments, including dynamical equilibrium, energy conditions, compactness factor, mass function, and surface redshift. The resulting outcome confirms the authenticity of the hybrid star model under analysis.","sentences":["The objective of our current study is to explore novel aspects of a stationary anisotropic relativistic hybrid compact star that consists of quark matter (QM) in its core and ordinary baryonic matter (OBM) in its crust.","This study has been done by adopting separate equations of states (EoSs) for quark matter and baryonic matter.","The MIT bag model equation of state $p_{q}=\\frac{1}{3}(\\rho_{q}-4B)$ has been used to demonstrate a correlation between the density and pressure of weird quark matter in the interior of the star.","In addition, we present a simple linear equation of state $p_{r}=\\beta_{1}\\rho-\\beta$ that links radial pressure and matter density for OBM.","The stellar model was formulated within the context of $f(\\mathcal{R},\\mathcal{A})$ gravity, utilizing a linear correlation between Ricci tensor $\\mathcal{R}$ and anticurvature scaler $\\mathcal{A}$.To solve the field equations of this novel alternative gravity, we employ the Krori and Barua approach to the metric potentials.","The validity of our suggested model is assessed using the graphical approach, while ensuring that the conditions are physically feasible.","Our focus is specifically on the tiny celestial object known as LMC X-4","[$\\text{M} = (1.04^{+0.09}_{-0.09})M_{\\odot};\\text{R}=8.301^{+0.2}_{-0.2}\\text{km}$], which we consider a viable candidate for a strange quark star.","We want to clarify the model's physical validity by examining a variety of physical assessments, including dynamical equilibrium, energy conditions, compactness factor, mass function, and surface redshift.","The resulting outcome confirms the authenticity of the hybrid star model under analysis."],"url":"http://arxiv.org/abs/2404.15203v1","category":"gr-qc"}
{"created":"2024-04-23 16:35:59","title":"CORE-BEHRT: A Carefully Optimized and Rigorously Evaluated BEHRT","abstract":"BERT-based models for Electronic Health Records (EHR) have surged in popularity following the release of BEHRT and Med-BERT. Subsequent models have largely built on these foundations despite the fundamental design choices of these pioneering models remaining underexplored. To address this issue, we introduce CORE-BEHRT, a Carefully Optimized and Rigorously Evaluated BEHRT. Through incremental optimization, we isolate the sources of improvement for key design choices, giving us insights into the effect of data representation and individual technical components on performance. Evaluating this across a set of generic tasks (death, pain treatment, and general infection), we showed that improving data representation can increase the average downstream performance from 0.785 to 0.797 AUROC, primarily when including medication and timestamps. Improving the architecture and training protocol on top of this increased average downstream performance to 0.801 AUROC. We then demonstrated the consistency of our optimization through a rigorous evaluation across 25 diverse clinical prediction tasks. We observed significant performance increases in 17 out of 25 tasks and improvements in 24 tasks, highlighting the generalizability of our findings. Our findings provide a strong foundation for future work and aim to increase the trustworthiness of BERT-based EHR models.","sentences":["BERT-based models for Electronic Health Records (EHR) have surged in popularity following the release of BEHRT and Med-BERT.","Subsequent models have largely built on these foundations despite the fundamental design choices of these pioneering models remaining underexplored.","To address this issue, we introduce CORE-BEHRT, a Carefully Optimized and Rigorously Evaluated BEHRT.","Through incremental optimization, we isolate the sources of improvement for key design choices, giving us insights into the effect of data representation and individual technical components on performance.","Evaluating this across a set of generic tasks (death, pain treatment, and general infection), we showed that improving data representation can increase the average downstream performance from 0.785 to 0.797 AUROC, primarily when including medication and timestamps.","Improving the architecture and training protocol on top of this increased average downstream performance to 0.801 AUROC.","We then demonstrated the consistency of our optimization through a rigorous evaluation across 25 diverse clinical prediction tasks.","We observed significant performance increases in 17 out of 25 tasks and improvements in 24 tasks, highlighting the generalizability of our findings.","Our findings provide a strong foundation for future work and aim to increase the trustworthiness of BERT-based EHR models."],"url":"http://arxiv.org/abs/2404.15201v2","category":"cs.LG"}
{"created":"2024-04-23 16:35:46","title":"An algebraic-geometric construction of \"lump\" solutions of the KP1 equation","abstract":"In this note, we show how certain everywhere-regular real rational function solutions of the KP1 equation (\"multi-lumps\") can be constructed via the polynomial analogs of theta functions from singular rational curves with cusps. The method we use can be understood as producing a degeneration of the well-understood soliton solutions from nodal singular curves. Hence it can be seen as a variation on the long-wave limit technique of Ablowitz and Satsuma, as developed by Zhang, Yang, Li, Guo, and Stepanyants. We present an explicit example of a three-lump solution constructed via the polynomial analog of the theta function from a rational curve with two cuspidal singular points, each with semigroup $\\langle 2,5\\rangle$. (In the theory of curve singularities, these are known as $A_4$ double points.) We conjecture that these ideas will generalize to give similar $M$-lump solutions with $M = \\frac{N(N+1)}{2}$ for $N > 2$ starting from rational curves with two singular points with semigroup $\\langle 2,2N+1\\rangle$ ($A_{2N}$ double points). Similar solutions have been constructed by other methods previously; our contribution is to show how they arise from the algebraic-geometric setting by considering singular curves with several cusps, as in previous work of Agostini, Celik, and Little.","sentences":["In this note, we show how certain everywhere-regular real rational function solutions of the KP1 equation (\"multi-lumps\") can be constructed via the polynomial analogs of theta functions from singular rational curves with cusps.","The method we use can be understood as producing a degeneration of the well-understood soliton solutions from nodal singular curves.","Hence it can be seen as a variation on the long-wave limit technique of Ablowitz and Satsuma, as developed by Zhang, Yang, Li, Guo, and Stepanyants.","We present an explicit example of a three-lump solution constructed via the polynomial analog of the theta function from a rational curve with two cuspidal singular points, each with semigroup $\\langle 2,5\\rangle$. (In the theory of curve singularities, these are known as $A_4$ double points.)","We conjecture that these ideas will generalize to give similar $M$-lump solutions with $M = \\frac{N(N+1)}{2}$ for $N > 2$ starting from rational curves with two singular points with semigroup $\\langle 2,2N+1\\rangle$ ($A_{2N}$ double points).","Similar solutions have been constructed by other methods previously; our contribution is to show how they arise from the algebraic-geometric setting by considering singular curves with several cusps, as in previous work of Agostini, Celik, and Little."],"url":"http://arxiv.org/abs/2404.15200v1","category":"math.AG"}
{"created":"2024-04-23 16:34:07","title":"The Sensitivity of NEO Surveyor to Low-Perihelion Asteroids","abstract":"Asteroids with low orbital perihelion distances experience extreme heating from the Sun that can modify their surfaces and trigger non-typical activity mechanisms. These objects are generally difficult to observe from ground-based telescopes due to their frequent proximity to the Sun. The Near Earth Object Surveyor mission, however, will regularly survey down to Solar elongations of 45 degrees and is well-suited for the detection and characterization of low-perihelion asteroids. Here, we use the survey simulation software tools developed for mission verification to explore the expected sensitivity of NEO Surveyor to these objects. We find that NEO Surveyor is expected to be >90% complete for near-Sun objects larger than D~300 m. Additionally, if the asteroid (3200) Phaethon underwent a disruption event in the past to form the Geminid meteor stream, Surveyor will be >90% complete to any fragments larger than D~200 m. For probable disruption models, NEO Surveyor would be expected to detect dozens of objects on Phaethon-like orbits, compared to a predicted background population of only a handful of asteroids, setting strong constraints on the likelihood of this scenario.","sentences":["Asteroids with low orbital perihelion distances experience extreme heating from the Sun that can modify their surfaces and trigger non-typical activity mechanisms.","These objects are generally difficult to observe from ground-based telescopes due to their frequent proximity to the Sun.","The Near Earth Object Surveyor mission, however, will regularly survey down to Solar elongations of 45 degrees and is well-suited for the detection and characterization of low-perihelion asteroids.","Here, we use the survey simulation software tools developed for mission verification to explore the expected sensitivity of NEO Surveyor to these objects.","We find that NEO Surveyor is expected to be >90% complete for near-Sun objects larger than D~300 m. Additionally, if the asteroid (3200) Phaethon underwent a disruption event in the past to form the Geminid meteor stream, Surveyor will be >90% complete to any fragments larger than D~200 m. For probable disruption models, NEO Surveyor would be expected to detect dozens of objects on Phaethon-like orbits, compared to a predicted background population of only a handful of asteroids, setting strong constraints on the likelihood of this scenario."],"url":"http://arxiv.org/abs/2404.15195v1","category":"astro-ph.EP"}
{"created":"2024-04-23 16:27:44","title":"Diabatic Dynamical Diquark Bound States: Mass Corrections and Widths","abstract":"Using the diabatic formalism, which generalizes the adiabatic approximation in the Born-Oppenheimer formalism, we apply well-known Hamiltonian methods to calculate the effect of open di-meson thresholds that lie well below the mass of elementary $c\\bar c q\\bar q^\\prime$, $c\\bar c s\\bar s$, and $c \\bar c q \\bar s$ tetraquark bound states. We compute the resulting mass shifts for these states, as well as their decay widths to the corresponding meson pairs. Each mass eigenstate, originally produced using a bound-state approximation under the diabatic formalism, consists of an admixture of a compact diquark-antidiquark configuration (an eigenstate of the original dynamical diquark model) with an extended di-meson configuration induced by the nearest threshold. We compare our results with those from our recent work that employs a scattering formalism, and find a great deal of agreement, but also comment upon interesting discrepancies between the two approaches.","sentences":["Using the diabatic formalism, which generalizes the adiabatic approximation in the Born-Oppenheimer formalism, we apply well-known Hamiltonian methods to calculate the effect of open di-meson thresholds that lie well below the mass of elementary $c\\bar c q\\bar q^\\prime$, $c\\bar c s\\bar s$, and $c \\bar c q \\bar s$ tetraquark bound states.","We compute the resulting mass shifts for these states, as well as their decay widths to the corresponding meson pairs.","Each mass eigenstate, originally produced using a bound-state approximation under the diabatic formalism, consists of an admixture of a compact diquark-antidiquark configuration (an eigenstate of the original dynamical diquark model) with an extended di-meson configuration induced by the nearest threshold.","We compare our results with those from our recent work that employs a scattering formalism, and find a great deal of agreement, but also comment upon interesting discrepancies between the two approaches."],"url":"http://arxiv.org/abs/2404.15186v1","category":"hep-ph"}
{"created":"2024-04-23 16:20:48","title":"Does anti-Unruh effect assist quantum entanglement and coherence?","abstract":"In this paper, we use the concepts of quantum entanglement and coherence to analyze the Unruh and anti-Unruh effects based on the model of Unruh-DeWitt detector. For the first time, we find that (i) the Unruh effect reduces quantum entanglement but enhances quantum coherence; (ii) the anti-Unruh effect enhances quantum entanglement but reduces quantum coherence. This surprising result refutes the notion that the Unruh effect can only destroy quantum entanglement and coherence simultaneously, and that the anti-Unruh can only protect quantum resources. Consequently, it opens up a new source for discovering experimental evidence supporting the existence of the Unruh and anti-Unruh effects.","sentences":["In this paper, we use the concepts of quantum entanglement and coherence to analyze the Unruh and anti-Unruh effects based on the model of Unruh-DeWitt detector.","For the first time, we find that (i) the Unruh effect reduces quantum entanglement but enhances quantum coherence; (ii) the anti-Unruh effect enhances quantum entanglement but reduces quantum coherence.","This surprising result refutes the notion that the Unruh effect can only destroy quantum entanglement and coherence simultaneously, and that the anti-Unruh can only protect quantum resources.","Consequently, it opens up a new source for discovering experimental evidence supporting the existence of the Unruh and anti-Unruh effects."],"url":"http://arxiv.org/abs/2404.15180v1","category":"gr-qc"}
{"created":"2024-04-23 16:19:13","title":"(Sub-)picosecond surface correlations of femtosecond laser excited Al-coated multilayers observed by grazing-incidence x-ray scattering","abstract":"Femtosecond high-intensity laser pulses at intensities surpassing $10^{14} \\,\\text{W}/\\text{cm}^2$ can generate a diverse range of functional surface nanostructures. Achieving precise control over the production of these functional structures necessitates a thorough understanding of the surface morphology dynamics with nanometer-scale spatial resolution and picosecond-scale temporal resolution. In this study, we show that individual XFEL pulses can elucidate structural changes on surfaces induced by laser-generated plasmas, employing grazing-incidence small-angle x-ray scattering (GISAXS). Using aluminum-coated multilayer samples we can differentiate between ultrafast surface morphology dynamics and subsequent subsurface density dynamics, achieving nanometer-depth sensitivity and subpicosecond temporal resolution. The observed subsurface density dynamics serve to validate advanced simulation models depicting matter under extreme conditions. Our findings promise to unveil novel avenues for laser material nanoprocessing and high-energy-density science.","sentences":["Femtosecond high-intensity laser pulses at intensities surpassing $10^{14} \\,\\text{W}/\\text{cm}^2$ can generate a diverse range of functional surface nanostructures.","Achieving precise control over the production of these functional structures necessitates a thorough understanding of the surface morphology dynamics with nanometer-scale spatial resolution and picosecond-scale temporal resolution.","In this study, we show that individual XFEL pulses can elucidate structural changes on surfaces induced by laser-generated plasmas, employing grazing-incidence small-angle x-ray scattering (GISAXS).","Using aluminum-coated multilayer samples we can differentiate between ultrafast surface morphology dynamics and subsequent subsurface density dynamics, achieving nanometer-depth sensitivity and subpicosecond temporal resolution.","The observed subsurface density dynamics serve to validate advanced simulation models depicting matter under extreme conditions.","Our findings promise to unveil novel avenues for laser material nanoprocessing and high-energy-density science."],"url":"http://arxiv.org/abs/2404.15178v1","category":"physics.plasm-ph"}
{"created":"2024-04-23 16:14:02","title":"Resting state fMRI-based brain information flow mapping","abstract":"Human brain is a massive information generation and processing machine. Studying the information flow may provide unique insight into brain function and brain diseases. We present here a tool for mapping the regional information flow in the entire brain using fMRI. Using the tool, we can estimate the information flow from a single region to the rest of the brain, between different regions, between different days, or between different individuals' brain.","sentences":["Human brain is a massive information generation and processing machine.","Studying the information flow may provide unique insight into brain function and brain diseases.","We present here a tool for mapping the regional information flow in the entire brain using fMRI.","Using the tool, we can estimate the information flow from a single region to the rest of the brain, between different regions, between different days, or between different individuals' brain."],"url":"http://arxiv.org/abs/2404.15173v1","category":"q-bio.NC"}
{"created":"2024-04-23 16:02:42","title":"Review on the matching conditions for the tidal problem: towards the application to more general contexts","abstract":"The tidal problem is used to obtain the tidal deformability (or Love number) of stars. The semi-analytical study is usually treated in perturbation theory as a first order perturbation problem over a spherically symmetric background configuration consisting of a stellar interior region matched across a boundary to a vacuum exterior region that models the tidal field. The field equations for the metric and matter perturbations at the interior and exterior regions are complemented with corresponding boundary conditions. The data of the two problems at the common boundary are related by the so called matching conditions. These conditions for the tidal problem are known in the contexts of perfect fluid stars and superfluid stars modelled by a two-fluid. Here we review the obtaining of the matching conditions for the tidal problem starting from a purely geometrical setting, and present them so that they can be readily applied to more general contexts, such as other types of matter fields, different multiple layers or phase transitions. As a guide on how to use the matching conditions, we recover the known results for perfect fluid and superfluid neutron stars.","sentences":["The tidal problem is used to obtain the tidal deformability (or Love number) of stars.","The semi-analytical study is usually treated in perturbation theory as a first order perturbation problem over a spherically symmetric background configuration consisting of a stellar interior region matched across a boundary to a vacuum exterior region that models the tidal field.","The field equations for the metric and matter perturbations at the interior and exterior regions are complemented with corresponding boundary conditions.","The data of the two problems at the common boundary are related by the so called matching conditions.","These conditions for the tidal problem are known in the contexts of perfect fluid stars and superfluid stars modelled by a two-fluid.","Here we review the obtaining of the matching conditions for the tidal problem starting from a purely geometrical setting, and present them so that they can be readily applied to more general contexts, such as other types of matter fields, different multiple layers or phase transitions.","As a guide on how to use the matching conditions, we recover the known results for perfect fluid and superfluid neutron stars."],"url":"http://arxiv.org/abs/2404.15164v1","category":"gr-qc"}
{"created":"2024-04-23 16:02:33","title":"Adaptive Mixed-Scale Feature Fusion Network for Blind AI-Generated Image Quality Assessment","abstract":"With the increasing maturity of the text-to-image and image-to-image generative models, AI-generated images (AGIs) have shown great application potential in advertisement, entertainment, education, social media, etc. Although remarkable advancements have been achieved in generative models, very few efforts have been paid to design relevant quality assessment models. In this paper, we propose a novel blind image quality assessment (IQA) network, named AMFF-Net, for AGIs. AMFF-Net evaluates AGI quality from three dimensions, i.e., \"visual quality\", \"authenticity\", and \"consistency\". Specifically, inspired by the characteristics of the human visual system and motivated by the observation that \"visual quality\" and \"authenticity\" are characterized by both local and global aspects, AMFF-Net scales the image up and down and takes the scaled images and original-sized image as the inputs to obtain multi-scale features. After that, an Adaptive Feature Fusion (AFF) block is used to adaptively fuse the multi-scale features with learnable weights. In addition, considering the correlation between the image and prompt, AMFF-Net compares the semantic features from text encoder and image encoder to evaluate the text-to-image alignment. We carry out extensive experiments on three AGI quality assessment databases, and the experimental results show that our AMFF-Net obtains better performance than nine state-of-the-art blind IQA methods. The results of ablation experiments further demonstrate the effectiveness of the proposed multi-scale input strategy and AFF block.","sentences":["With the increasing maturity of the text-to-image and image-to-image generative models, AI-generated images (AGIs) have shown great application potential in advertisement, entertainment, education, social media, etc.","Although remarkable advancements have been achieved in generative models, very few efforts have been paid to design relevant quality assessment models.","In this paper, we propose a novel blind image quality assessment (IQA) network, named AMFF-Net, for AGIs.","AMFF-Net evaluates AGI quality from three dimensions, i.e., \"visual quality\", \"authenticity\", and \"consistency\".","Specifically, inspired by the characteristics of the human visual system and motivated by the observation that \"visual quality\" and \"authenticity\" are characterized by both local and global aspects, AMFF-Net scales the image up and down and takes the scaled images and original-sized image as the inputs to obtain multi-scale features.","After that, an Adaptive Feature Fusion (AFF) block is used to adaptively fuse the multi-scale features with learnable weights.","In addition, considering the correlation between the image and prompt, AMFF-Net compares the semantic features from text encoder and image encoder to evaluate the text-to-image alignment.","We carry out extensive experiments on three AGI quality assessment databases, and the experimental results show that our AMFF-Net obtains better performance than nine state-of-the-art blind IQA methods.","The results of ablation experiments further demonstrate the effectiveness of the proposed multi-scale input strategy and AFF block."],"url":"http://arxiv.org/abs/2404.15163v1","category":"cs.CV"}
{"created":"2024-04-23 15:57:55","title":"Regressive Side Effects of Training Language Models to Mimic Student Misconceptions","abstract":"This paper presents a novel exploration into the regressive side effects of training Large Language Models (LLMs) to mimic student misconceptions for personalized education. We highlight the problem that as LLMs are trained to more accurately mimic student misconceptions, there is a compromise in the factual integrity and reasoning ability of the models. Our work involved training an LLM on a student-tutor dialogue dataset to predict student responses. The results demonstrated a decrease in the model's performance across multiple benchmark datasets, including the ARC reasoning challenge and TruthfulQA, which evaluates the truthfulness of model's generated responses. Furthermore, the HaluEval Dial dataset, used for hallucination detection, and MemoTrap, a memory-based task dataset, also reported a decline in the model accuracy. To combat these side effects, we introduced a \"hallucination token\" technique. This token, appended at the beginning of each student response during training, instructs the model to switch between mimicking student misconceptions and providing factually accurate responses. Despite the significant improvement across all datasets, the technique does not completely restore the LLM's baseline performance, indicating the need for further research in this area. This paper contributes to the ongoing discussion on the use of LLMs for student modeling, emphasizing the need for a balance between personalized education and factual accuracy.","sentences":["This paper presents a novel exploration into the regressive side effects of training Large Language Models (LLMs) to mimic student misconceptions for personalized education.","We highlight the problem that as LLMs are trained to more accurately mimic student misconceptions, there is a compromise in the factual integrity and reasoning ability of the models.","Our work involved training an LLM on a student-tutor dialogue dataset to predict student responses.","The results demonstrated a decrease in the model's performance across multiple benchmark datasets, including the ARC reasoning challenge and TruthfulQA, which evaluates the truthfulness of model's generated responses.","Furthermore, the HaluEval Dial dataset, used for hallucination detection, and MemoTrap, a memory-based task dataset, also reported a decline in the model accuracy.","To combat these side effects, we introduced a \"hallucination token\" technique.","This token, appended at the beginning of each student response during training, instructs the model to switch between mimicking student misconceptions and providing factually accurate responses.","Despite the significant improvement across all datasets, the technique does not completely restore the LLM's baseline performance, indicating the need for further research in this area.","This paper contributes to the ongoing discussion on the use of LLMs for student modeling, emphasizing the need for a balance between personalized education and factual accuracy."],"url":"http://arxiv.org/abs/2404.15156v1","category":"cs.CL"}
{"created":"2024-04-23 15:52:53","title":"Lost in Magnitudes: Exploring the Design Space for Visualizing Data with Large Value Ranges","abstract":"We explore the design space for the static visualization of datasets with quantitative attributes that vary over multiple orders of magnitude-we call these attributes Orders of Magnitude Values (OMVs)-and provide design guidelines and recommendations on effective visual encodings for OMVs. Current charts rely on linear or logarithmic scales to visualize values, leading to limitations in performing simple tasks for OMVs. In particular, linear scales prevent the reading of smaller magnitudes and their comparisons, while logarithmic scales are challenging for the general public to understand. Our design space leverages the approach of dividing OMVs into two different parts: mantissa and exponent, in a way similar to scientific notation. This separation allows for a visual encoding of both parts. For our exploration, we use four datasets, each with two attributes: an OMV, divided into mantissa and exponent, and a second attribute that is nominal, ordinal, time, or quantitative. We start from the original design space described by the Grammar of Graphics and systematically generate all possible visualizations for these datasets, employing different marks and visual channels. We refine this design space by enforcing integrity constraints from visualization and graphical perception literature. Through a qualitative assessment of all viable combinations, we discuss the most effective visualizations for OMVs, focusing on channel and task effectiveness. The article's main contributions are 1) the presentation of the design space of OMVs, 2) the generation of a large number of OMV visualizations, among which some are novel and effective, 3) the refined definition of a scale that we call E+M for OMVs, and 4) guidelines and recommendations for designing effective OMV visualizations. These efforts aim to enrich visualization systems to better support data with OMVs and guide future research.","sentences":["We explore the design space for the static visualization of datasets with quantitative attributes that vary over multiple orders of magnitude-we call these attributes Orders of Magnitude Values (OMVs)-and provide design guidelines and recommendations on effective visual encodings for OMVs.","Current charts rely on linear or logarithmic scales to visualize values, leading to limitations in performing simple tasks for OMVs.","In particular, linear scales prevent the reading of smaller magnitudes and their comparisons, while logarithmic scales are challenging for the general public to understand.","Our design space leverages the approach of dividing OMVs into two different parts: mantissa and exponent, in a way similar to scientific notation.","This separation allows for a visual encoding of both parts.","For our exploration, we use four datasets, each with two attributes: an OMV, divided into mantissa and exponent, and a second attribute that is nominal, ordinal, time, or quantitative.","We start from the original design space described by the Grammar of Graphics and systematically generate all possible visualizations for these datasets, employing different marks and visual channels.","We refine this design space by enforcing integrity constraints from visualization and graphical perception literature.","Through a qualitative assessment of all viable combinations, we discuss the most effective visualizations for OMVs, focusing on channel and task effectiveness.","The article's main contributions are 1) the presentation of the design space of OMVs, 2) the generation of a large number of OMV visualizations, among which some are novel and effective, 3) the refined definition of a scale that we call E+M for OMVs, and 4) guidelines and recommendations for designing effective OMV visualizations.","These efforts aim to enrich visualization systems to better support data with OMVs and guide future research."],"url":"http://arxiv.org/abs/2404.15150v1","category":"cs.HC"}
{"created":"2024-04-23 15:52:52","title":"Bias patterns in the application of LLMs for clinical decision support: A comprehensive study","abstract":"Large Language Models (LLMs) have emerged as powerful candidates to inform clinical decision-making processes. While these models play an increasingly prominent role in shaping the digital landscape, two growing concerns emerge in healthcare applications: 1) to what extent do LLMs exhibit social bias based on patients' protected attributes (like race), and 2) how do design choices (like architecture design and prompting strategies) influence the observed biases? To answer these questions rigorously, we evaluated eight popular LLMs across three question-answering (QA) datasets using clinical vignettes (patient descriptions) standardized for bias evaluations. We employ red-teaming strategies to analyze how demographics affect LLM outputs, comparing both general-purpose and clinically-trained models. Our extensive experiments reveal various disparities (some significant) across protected groups. We also observe several counter-intuitive patterns such as larger models not being necessarily less biased and fined-tuned models on medical data not being necessarily better than the general-purpose models. Furthermore, our study demonstrates the impact of prompt design on bias patterns and shows that specific phrasing can influence bias patterns and reflection-type approaches (like Chain of Thought) can reduce biased outcomes effectively. Consistent with prior studies, we call on additional evaluations, scrutiny, and enhancement of LLMs used in clinical decision support applications.","sentences":["Large Language Models (LLMs) have emerged as powerful candidates to inform clinical decision-making processes.","While these models play an increasingly prominent role in shaping the digital landscape, two growing concerns emerge in healthcare applications: 1) to what extent do LLMs exhibit social bias based on patients' protected attributes (like race), and 2) how do design choices (like architecture design and prompting strategies) influence the observed biases?","To answer these questions rigorously, we evaluated eight popular LLMs across three question-answering (QA) datasets using clinical vignettes (patient descriptions) standardized for bias evaluations.","We employ red-teaming strategies to analyze how demographics affect LLM outputs, comparing both general-purpose and clinically-trained models.","Our extensive experiments reveal various disparities (some significant) across protected groups.","We also observe several counter-intuitive patterns such as larger models not being necessarily less biased and fined-tuned models on medical data not being necessarily better than the general-purpose models.","Furthermore, our study demonstrates the impact of prompt design on bias patterns and shows that specific phrasing can influence bias patterns and reflection-type approaches (like Chain of Thought) can reduce biased outcomes effectively.","Consistent with prior studies, we call on additional evaluations, scrutiny, and enhancement of LLMs used in clinical decision support applications."],"url":"http://arxiv.org/abs/2404.15149v1","category":"cs.CL"}
{"created":"2024-04-23 15:52:47","title":"Chemical Potential and Charge in Quantum Black Holes","abstract":"We study systems in $2+1$ dimensions consisting of defects that source an electric charge, or a magnetic flux, of a $U(1)$ field, and we use holography to compute their effects on quantum conformal fields. We can also hide the defects inside the horizon of a black hole, where they continue to affect the quantum fields outside. By extending the solutions to braneworld holography, we find the non-linear backreaction of the quantum fields on the defect and black hole backgrounds. This gives quantum charged point particles and black holes. The charged quantum black holes markedly differ from classically charged BTZ black holes, since the quantum-induced electromagnetic field in $2+1$ dimensions has a better asymptotic behavior than its classical counterpart. The construction also gives a new class of (near-)extremal charged quantum black holes with AdS$_2$ throats.","sentences":["We study systems in $2+1$ dimensions consisting of defects that source an electric charge, or a magnetic flux, of a $U(1)$ field, and we use holography to compute their effects on quantum conformal fields.","We can also hide the defects inside the horizon of a black hole, where they continue to affect the quantum fields outside.","By extending the solutions to braneworld holography, we find the non-linear backreaction of the quantum fields on the defect and black hole backgrounds.","This gives quantum charged point particles and black holes.","The charged quantum black holes markedly differ from classically charged BTZ black holes, since the quantum-induced electromagnetic field in $2+1$ dimensions has a better asymptotic behavior than its classical counterpart.","The construction also gives a new class of (near-)extremal charged quantum black holes with AdS$_2$ throats."],"url":"http://arxiv.org/abs/2404.15148v1","category":"hep-th"}
{"created":"2024-04-23 15:50:14","title":"A general multi-wave quasi-resonance theory for lattice energy diffusion","abstract":"In this letter, a multi-wave quasi-resonance framework is established to analyze energy diffusion in classical lattices, uncovering that it is fundamentally determined by the characteristics of eigenmodes. Namely, based on the presence and the absence of extended modes, lattices fall into two universality classes with qualitatively different thermalization behavior. In particular, we find that while the one with extended modes can be thermalized under arbitrarily weak perturbations in the thermodynamic limit, the other class can be thermalized only when perturbations exceed a certain threshold, revealing for the first time the possibility that a lattice cannot be thermalized, violating the hypothesis of statistical mechanics. Our study addresses conclusively the renowned Fermi-Pasta-Ulam-Tsingou problem for large systems under weak perturbations, underscoring the pivotal roles of both extended and localized modes in facilitating energy diffusion and thermalization processes.","sentences":["In this letter, a multi-wave quasi-resonance framework is established to analyze energy diffusion in classical lattices, uncovering that it is fundamentally determined by the characteristics of eigenmodes.","Namely, based on the presence and the absence of extended modes, lattices fall into two universality classes with qualitatively different thermalization behavior.","In particular, we find that while the one with extended modes can be thermalized under arbitrarily weak perturbations in the thermodynamic limit, the other class can be thermalized only when perturbations exceed a certain threshold, revealing for the first time the possibility that a lattice cannot be thermalized, violating the hypothesis of statistical mechanics.","Our study addresses conclusively the renowned Fermi-Pasta-Ulam-Tsingou problem for large systems under weak perturbations, underscoring the pivotal roles of both extended and localized modes in facilitating energy diffusion and thermalization processes."],"url":"http://arxiv.org/abs/2404.15147v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-23 15:49:27","title":"Finite-time dynamics of an entanglement engine: current, fluctuations and kinetic uncertainty relations","abstract":"Entanglement engines are autonomous quantum thermal machines designed to generate entanglement from the presence of a particle current flowing through the device. In this work, we investigate the functioning of a two-qubit entanglement engine beyond the steady-state regime. Within a master equation approach, we derive the time-dependent state, the particle current, as well as the associated current correlation functions. Our findings establish a direct connection between coherence and internal current, elucidating the existence of a critical current that serves as an indicator for entanglement in the steady state. We then apply our results to investigate kinetic uncertainty relations (KURs) at finite times. We demonstrate that there are more than one possible definitions for KURs at finite times. While the two definitions agree in the steady-state regime, they lead to different parameter's ranges for violating KUR at finite times.","sentences":["Entanglement engines are autonomous quantum thermal machines designed to generate entanglement from the presence of a particle current flowing through the device.","In this work, we investigate the functioning of a two-qubit entanglement engine beyond the steady-state regime.","Within a master equation approach, we derive the time-dependent state, the particle current, as well as the associated current correlation functions.","Our findings establish a direct connection between coherence and internal current, elucidating the existence of a critical current that serves as an indicator for entanglement in the steady state.","We then apply our results to investigate kinetic uncertainty relations (KURs) at finite times.","We demonstrate that there are more than one possible definitions for KURs at finite times.","While the two definitions agree in the steady-state regime, they lead to different parameter's ranges for violating KUR at finite times."],"url":"http://arxiv.org/abs/2404.15144v1","category":"quant-ph"}
{"created":"2024-04-23 15:48:51","title":"Every Breath You Don't Take: Deepfake Speech Detection Using Breath","abstract":"Deepfake speech represents a real and growing threat to systems and society. Many detectors have been created to aid in defense against speech deepfakes. While these detectors implement myriad methodologies, many rely on low-level fragments of the speech generation process. We hypothesize that breath, a higher-level part of speech, is a key component of natural speech and thus improper generation in deepfake speech is a performant discriminator. To evaluate this, we create a breath detector and leverage this against a custom dataset of online news article audio to discriminate between real/deepfake speech. Additionally, we make this custom dataset publicly available to facilitate comparison for future work. Applying our simple breath detector as a deepfake speech discriminator on in-the-wild samples allows for accurate classification (perfect 1.0 AUPRC and 0.0 EER on test data) across 33.6 hours of audio. We compare our model with the state-of-the-art SSL-wav2vec model and show that this complex deep learning model completely fails to classify the same in-the-wild samples (0.72 AUPRC and 0.99 EER).","sentences":["Deepfake speech represents a real and growing threat to systems and society.","Many detectors have been created to aid in defense against speech deepfakes.","While these detectors implement myriad methodologies, many rely on low-level fragments of the speech generation process.","We hypothesize that breath, a higher-level part of speech, is a key component of natural speech and thus improper generation in deepfake speech is a performant discriminator.","To evaluate this, we create a breath detector and leverage this against a custom dataset of online news article audio to discriminate between real/deepfake speech.","Additionally, we make this custom dataset publicly available to facilitate comparison for future work.","Applying our simple breath detector as a deepfake speech discriminator on in-the-wild samples allows for accurate classification (perfect 1.0 AUPRC and 0.0 EER on test data) across 33.6 hours of audio.","We compare our model with the state-of-the-art SSL-wav2vec model and show that this complex deep learning model completely fails to classify the same in-the-wild samples (0.72 AUPRC and 0.99 EER)."],"url":"http://arxiv.org/abs/2404.15143v1","category":"cs.SD"}
{"created":"2024-04-23 15:47:58","title":"CutDiffusion: A Simple, Fast, Cheap, and Strong Diffusion Extrapolation Method","abstract":"Transforming large pre-trained low-resolution diffusion models to cater to higher-resolution demands, i.e., diffusion extrapolation, significantly improves diffusion adaptability. We propose tuning-free CutDiffusion, aimed at simplifying and accelerating the diffusion extrapolation process, making it more affordable and improving performance. CutDiffusion abides by the existing patch-wise extrapolation but cuts a standard patch diffusion process into an initial phase focused on comprehensive structure denoising and a subsequent phase dedicated to specific detail refinement. Comprehensive experiments highlight the numerous almighty advantages of CutDiffusion: (1) simple method construction that enables a concise higher-resolution diffusion process without third-party engagement; (2) fast inference speed achieved through a single-step higher-resolution diffusion process, and fewer inference patches required; (3) cheap GPU cost resulting from patch-wise inference and fewer patches during the comprehensive structure denoising; (4) strong generation performance, stemming from the emphasis on specific detail refinement.","sentences":["Transforming large pre-trained low-resolution diffusion models to cater to higher-resolution demands, i.e., diffusion extrapolation, significantly improves diffusion adaptability.","We propose tuning-free CutDiffusion, aimed at simplifying and accelerating the diffusion extrapolation process, making it more affordable and improving performance.","CutDiffusion abides by the existing patch-wise extrapolation but cuts a standard patch diffusion process into an initial phase focused on comprehensive structure denoising and a subsequent phase dedicated to specific detail refinement.","Comprehensive experiments highlight the numerous almighty advantages of CutDiffusion: (1) simple method construction that enables a concise higher-resolution diffusion process without third-party engagement; (2) fast inference speed achieved through a single-step higher-resolution diffusion process, and fewer inference patches required; (3) cheap GPU cost resulting from patch-wise inference and fewer patches during the comprehensive structure denoising; (4) strong generation performance, stemming from the emphasis on specific detail refinement."],"url":"http://arxiv.org/abs/2404.15141v1","category":"cs.CV"}
{"created":"2024-04-23 15:45:49","title":"Maximal hypersurfaces and aspects of volume of the Kerr family of black holes","abstract":"In Schwarzschild spacetime, Reinhart (1973) has shown the hypersurface $r_R = 3M/2$ (the subscript stands for \"Reinhart\") to be a maximal hypersurface. This Reinhart radius $r_R$ plays a crucial role in evaluating the interior volume of a black hole. In this article, we find such a maximal hypersurface for the Kerr and Kerr-Newaman black holes. We obtain the analytical expression for the Reinhart radius as a function of the polar angle $\\theta$ for a small $a/M$ limit for both the Kerr and Kerr-Newman black holes. We obtain the Reinhart radius using two independent methods: a) the vanishing trace of the extrinsic curvature and, b) the variational method. We further use the Reinhart radius to obtain an analytical expression for the interior volume of the Kerr and Kerr-Newman black hole in the small $a/M$ limit and a generic charge $Q$. We define $\\mathcal{\\dot{V}}$ as the rate of change of the interior volume with respect to the ingoing null coordinate $v$ and then study its behavior under various scenarios, viz., particle accretion, the Penrose process, superradiance, and Hawking radiation. We show that while under the Penrose process and superradiance, the parameter $\\mathcal{\\dot{V}}$ increases just like the area of a black hole but under particle accretion, $\\mathcal{\\dot{V}}$ can have variable signs depending on the kinematical properties of the particle. We further probe into the behavior of $\\mathcal{\\dot{V}}$ under Hawking radiation. These results provide important and very interesting clues toward the possible existence of laws governing the volume of black holes.","sentences":["In Schwarzschild spacetime, Reinhart (1973) has shown the hypersurface $r_R = 3M/2$ (the subscript stands for \"Reinhart\") to be a maximal hypersurface.","This Reinhart radius $r_R$ plays a crucial role in evaluating the interior volume of a black hole.","In this article, we find such a maximal hypersurface for the Kerr and Kerr-Newaman black holes.","We obtain the analytical expression for the Reinhart radius as a function of the polar angle $\\theta$ for a small $a/M$ limit for both the Kerr and Kerr-Newman black holes.","We obtain the Reinhart radius using two independent methods: a) the vanishing trace of the extrinsic curvature and, b) the variational method.","We further use the Reinhart radius to obtain an analytical expression for the interior volume of the Kerr and Kerr-Newman black hole in the small $a/M$ limit and a generic charge $Q$. We define $\\mathcal{\\dot{V}}$ as the rate of change of the interior volume with respect to the ingoing null coordinate $v$ and then study its behavior under various scenarios, viz., particle accretion, the Penrose process, superradiance, and Hawking radiation.","We show that while under the Penrose process and superradiance, the parameter $\\mathcal{\\dot{V}}$ increases just like the area of a black hole but under particle accretion, $\\mathcal{\\dot{V}}$ can have variable signs depending on the kinematical properties of the particle.","We further probe into the behavior of $\\mathcal{\\dot{V}}$ under Hawking radiation.","These results provide important and very interesting clues toward the possible existence of laws governing the volume of black holes."],"url":"http://arxiv.org/abs/2404.15140v1","category":"gr-qc"}
{"created":"2024-04-23 15:34:38","title":"Giant Spin-Orbit Torque in Cr-based Janus Transition Metal Dichalcogenides","abstract":"We report a very large spin-orbit torque (SOT) capability of chromium-based transition metal dichalcogenides (TMD) in their Janus forms CrXTe, with X=S,Se. The structural inversion symmetry breaking, inherent to Janus structures is responsible for a large SOT response generated by giant Rashba splitting, equivalent to that obtained by applying a transverse electric field of $\\sim 100 \\,\\text{V} \\,\\text{nm}^{-1}$ in non-Janus CrTe\\textsubscript{2}, completely out of experimental reach. By performing transport simulations on custom-made Wannier tight-binding models, Janus systems are found to exhibit a SOT performance comparable to the most efficient two-dimensional materials, while allowing for field-free perpendicular magnetization switching owing to their reduced in-plane symmetry. Altogether, our findings evidence that magnetic Janus TMDs stand as suitable candidates for ultimate SOT-MRAM devices.","sentences":["We report a very large spin-orbit torque (SOT) capability of chromium-based transition metal dichalcogenides (TMD) in their Janus forms CrXTe, with X=S,Se.","The structural inversion symmetry breaking, inherent to Janus structures is responsible for a large SOT response generated by giant Rashba splitting, equivalent to that obtained by applying a transverse electric field of $\\sim 100 \\,\\text{V} \\,\\text{nm}^{-1}$ in non-Janus CrTe\\textsubscript{2}, completely out of experimental reach.","By performing transport simulations on custom-made Wannier tight-binding models, Janus systems are found to exhibit a SOT performance comparable to the most efficient two-dimensional materials, while allowing for field-free perpendicular magnetization switching owing to their reduced in-plane symmetry.","Altogether, our findings evidence that magnetic Janus TMDs stand as suitable candidates for ultimate SOT-MRAM devices."],"url":"http://arxiv.org/abs/2404.15134v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-23 15:33:57","title":"Bayesian Strategies for Repulsive Spatial Point Processes","abstract":"There is increasing interest to develop Bayesian inferential algorithms for point process models with intractable likelihoods. A purpose of this paper is to illustrate the utility of using simulation based strategies, including approximate Bayesian computation (ABC) and Markov chain Monte Carlo (MCMC) methods for this task. Shirota and Gelfand (2017) proposed an extended version of an ABC approach for repulsive spatial point processes, including the Strauss point process and the determinantal point process, but their algorithm was not correctly detailed. We explain that is, in general, intractable and therefore impractical to use, except in some restrictive situations. This motivates us to instead consider an ABC-MCMC algorithm developed by Fearnhead and Prangle (2012). We further explore the use of the exchange algorithm, together with the recently proposed noisy Metropolis-Hastings algorithm (Alquier et al., 2016). As an extension of the exchange algorithm, which requires a single simulation from the likelihood at each iteration, the noisy Metropolis-Hastings algorithm considers multiple draws from the same likelihood function. We find that both of these inferential approaches yield good performance for repulsive spatial point processes in both simulated and real data applications and should be considered as viable approaches for the analysis of these models.","sentences":["There is increasing interest to develop Bayesian inferential algorithms for point process models with intractable likelihoods.","A purpose of this paper is to illustrate the utility of using simulation based strategies, including approximate Bayesian computation (ABC) and Markov chain Monte Carlo (MCMC) methods for this task.","Shirota and Gelfand (2017) proposed an extended version of an ABC approach for repulsive spatial point processes, including the Strauss point process and the determinantal point process, but their algorithm was not correctly detailed.","We explain that is, in general, intractable and therefore impractical to use, except in some restrictive situations.","This motivates us to instead consider an ABC-MCMC algorithm developed by Fearnhead and Prangle (2012).","We further explore the use of the exchange algorithm, together with the recently proposed noisy Metropolis-Hastings algorithm (Alquier et al., 2016).","As an extension of the exchange algorithm, which requires a single simulation from the likelihood at each iteration, the noisy Metropolis-Hastings algorithm considers multiple draws from the same likelihood function.","We find that both of these inferential approaches yield good performance for repulsive spatial point processes in both simulated and real data applications and should be considered as viable approaches for the analysis of these models."],"url":"http://arxiv.org/abs/2404.15133v1","category":"stat.CO"}
{"created":"2024-04-23 15:29:02","title":"Gallbladder Cancer Detection in Ultrasound Images based on YOLO and Faster R-CNN","abstract":"Medical image analysis is a significant application of artificial intelligence for disease diagnosis. A crucial step in this process is the identification of regions of interest within the images. This task can be automated using object detection algorithms. YOLO and Faster R-CNN are renowned for such algorithms, each with its own strengths and weaknesses. This study aims to explore the advantages of both techniques to select more accurate bounding boxes for gallbladder detection from ultrasound images, thereby enhancing gallbladder cancer classification. A fusion method that leverages the benefits of both techniques is presented in this study. The proposed method demonstrated superior classification performance, with an accuracy of 92.62%, compared to the individual use of Faster R-CNN and YOLOv8, which yielded accuracies of 90.16% and 82.79%, respectively.","sentences":["Medical image analysis is a significant application of artificial intelligence for disease diagnosis.","A crucial step in this process is the identification of regions of interest within the images.","This task can be automated using object detection algorithms.","YOLO and Faster R-CNN are renowned for such algorithms, each with its own strengths and weaknesses.","This study aims to explore the advantages of both techniques to select more accurate bounding boxes for gallbladder detection from ultrasound images, thereby enhancing gallbladder cancer classification.","A fusion method that leverages the benefits of both techniques is presented in this study.","The proposed method demonstrated superior classification performance, with an accuracy of 92.62%, compared to the individual use of Faster R-CNN and YOLOv8, which yielded accuracies of 90.16% and 82.79%, respectively."],"url":"http://arxiv.org/abs/2404.15129v1","category":"cs.CV"}
{"created":"2024-04-23 15:28:02","title":"Unveiling the crystallization kinetics in Ge-rich Ge$_x$Te alloys by large scale simulations with a machine-learned interatomic potential","abstract":"A machine-learned interatomic potential for Ge-rich Ge$_x$Te alloys has been developed aiming at uncovering the kinetics of phase separation and crystallization in these materials. The results are of interest for the operation of embedded phase change memories which exploits Ge-enrichment of GeSbTe alloys to raise the crystallization temperature. The potential is generated by fitting a large database of energies and forces computed within Density Functional Theory with the neural network scheme implemented in the DeePMD-kit package. The potential is highly accurate and suitable to describe the structural and dynamical properties of the liquid, amorphous and crystalline phases of the wide range of compositions from pure Ge and stoichiometric GeTe to the Ge-rich Ge$_2$Te alloy. Large scale molecular dynamics simulations revealed a crystallization mechanism which depends on temperature. At 600 K, segregation of most of Ge in excess occurs on the ns time scale followed by crystallization of nearly stoichiometric GeTe regions. At 500 K, nucleation of crystalline GeTe occurs before phase separation, followed by a slow crystal growth due to the concurrent expulsion of Ge in excess.","sentences":["A machine-learned interatomic potential for Ge-rich Ge$_x$Te alloys has been developed aiming at uncovering the kinetics of phase separation and crystallization in these materials.","The results are of interest for the operation of embedded phase change memories which exploits Ge-enrichment of GeSbTe alloys to raise the crystallization temperature.","The potential is generated by fitting a large database of energies and forces computed within Density Functional Theory with the neural network scheme implemented in the DeePMD-kit package.","The potential is highly accurate and suitable to describe the structural and dynamical properties of the liquid, amorphous and crystalline phases of the wide range of compositions from pure Ge and stoichiometric GeTe to the Ge-rich Ge$_2$Te alloy.","Large scale molecular dynamics simulations revealed a crystallization mechanism which depends on temperature.","At 600 K, segregation of most of Ge in excess occurs on the ns time scale followed by crystallization of nearly stoichiometric GeTe regions.","At 500 K, nucleation of crystalline GeTe occurs before phase separation, followed by a slow crystal growth due to the concurrent expulsion of Ge in excess."],"url":"http://arxiv.org/abs/2404.15128v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-23 15:27:19","title":"MedDr: Diagnosis-Guided Bootstrapping for Large-Scale Medical Vision-Language Learning","abstract":"The rapid advancement of large-scale vision-language models has showcased remarkable capabilities across various tasks. However, the lack of extensive and high-quality image-text data in medicine has greatly hindered the development of large-scale medical vision-language models. In this work, we present a diagnosis-guided bootstrapping strategy that exploits both image and label information to construct vision-language datasets. Based on the constructed dataset, we developed MedDr, a generalist foundation model for healthcare capable of handling diverse medical data modalities, including radiology, pathology, dermatology, retinography, and endoscopy. Moreover, during inference, we propose a simple but effective retrieval-augmented medical diagnosis strategy, which enhances the model's generalization ability. Extensive experiments on visual question answering, medical report generation, and medical image diagnosis demonstrate the superiority of our method.","sentences":["The rapid advancement of large-scale vision-language models has showcased remarkable capabilities across various tasks.","However, the lack of extensive and high-quality image-text data in medicine has greatly hindered the development of large-scale medical vision-language models.","In this work, we present a diagnosis-guided bootstrapping strategy that exploits both image and label information to construct vision-language datasets.","Based on the constructed dataset, we developed MedDr, a generalist foundation model for healthcare capable of handling diverse medical data modalities, including radiology, pathology, dermatology, retinography, and endoscopy.","Moreover, during inference, we propose a simple but effective retrieval-augmented medical diagnosis strategy, which enhances the model's generalization ability.","Extensive experiments on visual question answering, medical report generation, and medical image diagnosis demonstrate the superiority of our method."],"url":"http://arxiv.org/abs/2404.15127v1","category":"cs.CV"}
{"created":"2024-04-23 15:20:17","title":"Taming Diffusion Probabilistic Models for Character Control","abstract":"We present a novel character control framework that effectively utilizes motion diffusion probabilistic models to generate high-quality and diverse character animations, responding in real-time to a variety of dynamic user-supplied control signals. At the heart of our method lies a transformer-based Conditional Autoregressive Motion Diffusion Model (CAMDM), which takes as input the character's historical motion and can generate a range of diverse potential future motions conditioned on high-level, coarse user control. To meet the demands for diversity, controllability, and computational efficiency required by a real-time controller, we incorporate several key algorithmic designs. These include separate condition tokenization, classifier-free guidance on past motion, and heuristic future trajectory extension, all designed to address the challenges associated with taming motion diffusion probabilistic models for character control. As a result, our work represents the first model that enables real-time generation of high-quality, diverse character animations based on user interactive control, supporting animating the character in multiple styles with a single unified model. We evaluate our method on a diverse set of locomotion skills, demonstrating the merits of our method over existing character controllers. Project page and source codes: https://aiganimation.github.io/CAMDM/","sentences":["We present a novel character control framework that effectively utilizes motion diffusion probabilistic models to generate high-quality and diverse character animations, responding in real-time to a variety of dynamic user-supplied control signals.","At the heart of our method lies a transformer-based Conditional Autoregressive Motion Diffusion Model (CAMDM), which takes as input the character's historical motion and can generate a range of diverse potential future motions conditioned on high-level, coarse user control.","To meet the demands for diversity, controllability, and computational efficiency required by a real-time controller, we incorporate several key algorithmic designs.","These include separate condition tokenization, classifier-free guidance on past motion, and heuristic future trajectory extension, all designed to address the challenges associated with taming motion diffusion probabilistic models for character control.","As a result, our work represents the first model that enables real-time generation of high-quality, diverse character animations based on user interactive control, supporting animating the character in multiple styles with a single unified model.","We evaluate our method on a diverse set of locomotion skills, demonstrating the merits of our method over existing character controllers.","Project page and source codes: https://aiganimation.github.io/CAMDM/"],"url":"http://arxiv.org/abs/2404.15121v1","category":"cs.GR"}
{"created":"2024-04-23 15:17:19","title":"Normal ordered grammars","abstract":"We introduce the theory of normal ordered grammars, which gives a natural generalization of the normal ordering problem. To illustrate the main idea, we explore normal ordered grammars associated with the Eulerian polynomials and the second-order Eulerian polynomials. In particular, we present a normal ordered grammatical interpretation for the (cdes,cyc) (p,q)-Eulerian polynomials, where cdes and cyc are the cycle descent and cycle statistics, respectively. The exponential generating function for a family of polynomials, generated by a normal ordered grammar associated with the second-order Eulerian polynomials, reveals an interesting feature: its expression involves the generating function for Catalan numbers as its exponent. In the final part, we discuss some normal ordered grammars related to the type B Eulerian polynomials. A normal ordered grammatical interpretation of the up-down run polynomial is also established.","sentences":["We introduce the theory of normal ordered grammars, which gives a natural generalization of the normal ordering problem.","To illustrate the main idea, we explore normal ordered grammars associated with the Eulerian polynomials and the second-order Eulerian polynomials.","In particular, we present a normal ordered grammatical interpretation for the (cdes,cyc)","(p,q)-Eulerian polynomials, where cdes and cyc are the cycle descent and cycle statistics, respectively.","The exponential generating function for a family of polynomials, generated by a normal ordered grammar associated with the second-order Eulerian polynomials, reveals an interesting feature: its expression involves the generating function for Catalan numbers as its exponent.","In the final part, we discuss some normal ordered grammars related to the type B Eulerian polynomials.","A normal ordered grammatical interpretation of the up-down run polynomial is also established."],"url":"http://arxiv.org/abs/2404.15119v1","category":"math.CO"}
{"created":"2024-04-23 15:16:49","title":"Identifying phase transitions in physical systems with neural networks: a neural architecture search perspective","abstract":"The use of machine learning algorithms to investigate phase transitions in physical systems is a valuable way to better understand the characteristics of these systems. Neural networks have been used to extract information of phases and phase transitions directly from many-body configurations. However, one limitation of neural networks is that they require the definition of the model architecture and parameters previous to their application, and such determination is itself a difficult problem. In this paper, we investigate for the first time the relationship between the accuracy of neural networks for information of phases and the network configuration (that comprises the architecture and hyperparameters). We formulate the phase analysis as a regression task, address the question of generating data that reflects the different states of the physical system, and evaluate the performance of neural architecture search for this task. After obtaining the optimized architectures, we further implement smart data processing and analytics by means of neuron coverage metrics, assessing the capability of these metrics to estimate phase transitions. Our results identify the neuron coverage metric as promising for detecting phase transitions in physical systems.","sentences":["The use of machine learning algorithms to investigate phase transitions in physical systems is a valuable way to better understand the characteristics of these systems.","Neural networks have been used to extract information of phases and phase transitions directly from many-body configurations.","However, one limitation of neural networks is that they require the definition of the model architecture and parameters previous to their application, and such determination is itself a difficult problem.","In this paper, we investigate for the first time the relationship between the accuracy of neural networks for information of phases and the network configuration (that comprises the architecture and hyperparameters).","We formulate the phase analysis as a regression task, address the question of generating data that reflects the different states of the physical system, and evaluate the performance of neural architecture search for this task.","After obtaining the optimized architectures, we further implement smart data processing and analytics by means of neuron coverage metrics, assessing the capability of these metrics to estimate phase transitions.","Our results identify the neuron coverage metric as promising for detecting phase transitions in physical systems."],"url":"http://arxiv.org/abs/2404.15118v1","category":"cs.NE"}
{"created":"2024-04-23 15:07:17","title":"Non-Positivity of the heat equation with non-local Robin boundary conditions","abstract":"We study heat equations $\\partial_t u + Lu = 0$ on bounded Lipschitz domains $\\Omega$, where $L=-\\operatorname{div}(A\\nabla\\;\\cdot\\;)$ is a second-order uniformly elliptic operator with generalised Robin boundary conditions of the form $\\nu\\cdot A\\nabla u + Bu=0$, where $B\\in\\mathcal{L}(L^2(\\partial\\Omega))$ is a general operator. In contrast to large parts of the literature on non-local Robin boundary conditions we also allow for operators $B$ that destroy the positivity preserving property of the solution semigroup $(e^{-tL})_{t\\ge 0}$. Nevertheless, we obtain ultracontractivity of the semigroup under quite mild assumptions on $B$. For a certain class of operators $B$ we demonstrate that the semigroup is in fact eventually positive rather than positivity preserving.","sentences":["We study heat equations $\\partial_t u + Lu = 0$ on bounded Lipschitz domains $\\Omega$, where $L=-\\operatorname{div}(A\\nabla\\;\\cdot\\;)$ is a second-order uniformly elliptic operator with generalised Robin boundary conditions of the form $\\nu\\cdot A\\nabla u + Bu=0$, where $B\\in\\mathcal{L}(L^2(\\partial\\Omega))$ is a general operator.","In contrast to large parts of the literature on non-local Robin boundary conditions we also allow for operators $B$ that destroy the positivity preserving property of the solution semigroup $(e^{-tL})_{t\\ge 0}$.","Nevertheless, we obtain ultracontractivity of the semigroup under quite mild assumptions on $B$. For a certain class of operators $B$ we demonstrate that the semigroup is in fact eventually positive rather than positivity preserving."],"url":"http://arxiv.org/abs/2404.15114v1","category":"math.AP"}
{"created":"2024-04-23 15:01:36","title":"MIMOSA: Human-AI Co-Creation of Computational Spatial Audio Effects on Videos","abstract":"Spatial audio offers more immersive video consumption experiences to viewers; however, creating and editing spatial audio often expensive and requires specialized equipment and skills, posing a high barrier for amateur video creators. We present MIMOSA, a human-AI co-creation tool that enables amateur users to computationally generate and manipulate spatial audio effects. For a video with only monaural or stereo audio, MIMOSA automatically grounds each sound source to the corresponding sounding object in the visual scene and enables users to further validate and fix the errors in the locations of sounding objects. Users can also augment the spatial audio effect by flexibly manipulating the sounding source positions and creatively customizing the audio effect. The design of MIMOSA exemplifies a human-AI collaboration approach that, instead of utilizing state-of art end-to-end \"black-box\" ML models, uses a multistep pipeline that aligns its interpretable intermediate results with the user's workflow. A lab user study with 15 participants demonstrates MIMOSA's usability, usefulness, expressiveness, and capability in creating immersive spatial audio effects in collaboration with users.","sentences":["Spatial audio offers more immersive video consumption experiences to viewers; however, creating and editing spatial audio often expensive and requires specialized equipment and skills, posing a high barrier for amateur video creators.","We present MIMOSA, a human-AI co-creation tool that enables amateur users to computationally generate and manipulate spatial audio effects.","For a video with only monaural or stereo audio, MIMOSA automatically grounds each sound source to the corresponding sounding object in the visual scene and enables users to further validate and fix the errors in the locations of sounding objects.","Users can also augment the spatial audio effect by flexibly manipulating the sounding source positions and creatively customizing the audio effect.","The design of MIMOSA exemplifies a human-AI collaboration approach that, instead of utilizing state-of art end-to-end \"black-box\" ML models, uses a multistep pipeline that aligns its interpretable intermediate results with the user's workflow.","A lab user study with 15 participants demonstrates MIMOSA's usability, usefulness, expressiveness, and capability in creating immersive spatial audio effects in collaboration with users."],"url":"http://arxiv.org/abs/2404.15107v1","category":"cs.HC"}
{"created":"2024-04-23 14:58:26","title":"Simulations of gravitational collapse in null coordinates: I. Formulation and weak-field tests in generalised Bondi gauges","abstract":"We present a code for numerical simulations of the collapse of regular initial data to a black hole in null coordinates. We restrict to twist-free axisymmetry with scalar field matter. Our coordinates are $(u,x,y,\\varphi)$, where the retarded time $u$ labels outgoing null cones emerging from a regular central worldline, the angles $(\\theta,\\varphi)$ label the null generators of each null cone, and the radial coordinate $x$ labels points along these generators. We focus on a class of generalised Bondi radial coordinates $x$ with the twin properties that $x=0$ is the central world line and that the numerical domain $(u\\ge0$, $0\\le x\\le x_\\text{max})$ is a subset of the domain of dependence of the initial data on $(u=0$, $0\\le x\\le x_\\text{max}$). In critical collapse, an appropriate choice of these coordinates can be made to zoom in on the accumulation point of scale echos of the critical solution, without the need for explicit mesh refinement. We introduce a novel numerical scheme that in effect reduces the angular resolution at small radius, such that the time step $\\Delta u$ for an explicit numerical scheme is limited by the radial resolution $\\Delta x$, rather than $\\Delta x(\\Delta\\theta)^2$. We present convergence tests in the weak-field regime, where we have exact solutions to the linearised scalar and gravitational-wave equations.","sentences":["We present a code for numerical simulations of the collapse of regular initial data to a black hole in null coordinates.","We restrict to twist-free axisymmetry with scalar field matter.","Our coordinates are $(u,x,y,\\varphi)$, where the retarded time $u$ labels outgoing null cones emerging from a regular central worldline, the angles $(\\theta,\\varphi)$ label the null generators of each null cone, and the radial coordinate $x$ labels points along these generators.","We focus on a class of generalised Bondi radial coordinates $x$ with the twin properties that $x=0$ is the central world line and that the numerical domain $(u\\ge0$, $0\\le x\\le x_\\text{max})$ is a subset of the domain of dependence of the initial data on $(u=0$, $0\\le x\\le x_\\text{max}$).","In critical collapse, an appropriate choice of these coordinates can be made to zoom in on the accumulation point of scale echos of the critical solution, without the need for explicit mesh refinement.","We introduce a novel numerical scheme that in effect reduces the angular resolution at small radius, such that the time step $\\Delta u$ for an explicit numerical scheme is limited by the radial resolution $\\Delta x$, rather than $\\Delta","x(\\Delta\\theta)^2$. We present convergence tests in the weak-field regime, where we have exact solutions to the linearised scalar and gravitational-wave equations."],"url":"http://arxiv.org/abs/2404.15105v1","category":"gr-qc"}
{"created":"2024-04-23 14:56:15","title":"Identifying Fairness Issues in Automatically Generated Testing Content","abstract":"Natural language generation tools are powerful and effective for generating content. However, language models are known to display bias and fairness issues, making them impractical to deploy for many use cases. We here focus on how fairness issues impact automatically generated test content, which can have stringent requirements to ensure the test measures only what it was intended to measure. Specifically, we identify test content that is focused on particular domains and experiences that only reflect a certain demographic or that are potentially emotionally upsetting; both of which could inadvertently impact a test-taker's score. This kind of content doesn't reflect typical biases out of context, making it challenging even for modern models that contain safeguards. We build a dataset of 621 generated texts annotated for fairness and explore a variety of methods for classification: fine-tuning, topic-based classification, and prompting, including few-shot and self-correcting prompts. We find that combining prompt self-correction and few-shot learning performs best, yielding an F1 score of .791 on our held-out test set, while much smaller BERT- and topic-based models have competitive performance on out-of-domain data.","sentences":["Natural language generation tools are powerful and effective for generating content.","However, language models are known to display bias and fairness issues, making them impractical to deploy for many use cases.","We here focus on how fairness issues impact automatically generated test content, which can have stringent requirements to ensure the test measures only what it was intended to measure.","Specifically, we identify test content that is focused on particular domains and experiences that only reflect a certain demographic or that are potentially emotionally upsetting; both of which could inadvertently impact a test-taker's score.","This kind of content doesn't reflect typical biases out of context, making it challenging even for modern models that contain safeguards.","We build a dataset of 621 generated texts annotated for fairness and explore a variety of methods for classification: fine-tuning, topic-based classification, and prompting, including few-shot and self-correcting prompts.","We find that combining prompt self-correction and few-shot learning performs best, yielding an F1 score of .791 on our held-out test set, while much smaller BERT- and topic-based models have competitive performance on out-of-domain data."],"url":"http://arxiv.org/abs/2404.15104v1","category":"cs.CL"}
{"created":"2024-04-23 14:55:32","title":"Multi-view Content-aware Indexing for Long Document Retrieval","abstract":"Long document question answering (DocQA) aims to answer questions from long documents over 10k words. They usually contain content structures such as sections, sub-sections, and paragraph demarcations. However, the indexing methods of long documents remain under-explored, while existing systems generally employ fixed-length chunking. As they do not consider content structures, the resultant chunks can exclude vital information or include irrelevant content. Motivated by this, we propose the Multi-view Content-aware indexing (MC-indexing) for more effective long DocQA via (i) segment structured document into content chunks, and (ii) represent each content chunk in raw-text, keywords, and summary views. We highlight that MC-indexing requires neither training nor fine-tuning. Having plug-and-play capability, it can be seamlessly integrated with any retrievers to boost their performance. Besides, we propose a long DocQA dataset that includes not only question-answer pair, but also document structure and answer scope. When compared to state-of-art chunking schemes, MC-indexing has significantly increased the recall by 42.8%, 30.0%, 23.9%, and 16.3% via top k= 1.5, 3, 5, and 10 respectively. These improved scores are the average of 8 widely used retrievers (2 sparse and 6 dense) via extensive experiments.","sentences":["Long document question answering (DocQA) aims to answer questions from long documents over 10k words.","They usually contain content structures such as sections, sub-sections, and paragraph demarcations.","However, the indexing methods of long documents remain under-explored, while existing systems generally employ fixed-length chunking.","As they do not consider content structures, the resultant chunks can exclude vital information or include irrelevant content.","Motivated by this, we propose the Multi-view Content-aware indexing (MC-indexing) for more effective long DocQA via (i) segment structured document into content chunks, and (ii) represent each content chunk in raw-text, keywords, and summary views.","We highlight that MC-indexing requires neither training nor fine-tuning.","Having plug-and-play capability, it can be seamlessly integrated with any retrievers to boost their performance.","Besides, we propose a long DocQA dataset that includes not only question-answer pair, but also document structure and answer scope.","When compared to state-of-art chunking schemes, MC-indexing has significantly increased the recall by 42.8%, 30.0%, 23.9%, and 16.3% via top k= 1.5, 3, 5, and 10 respectively.","These improved scores are the average of 8 widely used retrievers (2 sparse and 6 dense) via extensive experiments."],"url":"http://arxiv.org/abs/2404.15103v1","category":"cs.CL"}
{"created":"2024-04-23 14:53:15","title":"Multimodal Large Language Model is a Human-Aligned Annotator for Text-to-Image Generation","abstract":"Recent studies have demonstrated the exceptional potentials of leveraging human preference datasets to refine text-to-image generative models, enhancing the alignment between generated images and textual prompts. Despite these advances, current human preference datasets are either prohibitively expensive to construct or suffer from a lack of diversity in preference dimensions, resulting in limited applicability for instruction tuning in open-source text-to-image generative models and hinder further exploration. To address these challenges and promote the alignment of generative models through instruction tuning, we leverage multimodal large language models to create VisionPrefer, a high-quality and fine-grained preference dataset that captures multiple preference aspects. We aggregate feedback from AI annotators across four aspects: prompt-following, aesthetic, fidelity, and harmlessness to construct VisionPrefer. To validate the effectiveness of VisionPrefer, we train a reward model VP-Score over VisionPrefer to guide the training of text-to-image generative models and the preference prediction accuracy of VP-Score is comparable to human annotators. Furthermore, we use two reinforcement learning methods to supervised fine-tune generative models to evaluate the performance of VisionPrefer, and extensive experimental results demonstrate that VisionPrefer significantly improves text-image alignment in compositional image generation across diverse aspects, e.g., aesthetic, and generalizes better than previous human-preference metrics across various image distributions. Moreover, VisionPrefer indicates that the integration of AI-generated synthetic data as a supervisory signal is a promising avenue for achieving improved alignment with human preferences in vision generative models.","sentences":["Recent studies have demonstrated the exceptional potentials of leveraging human preference datasets to refine text-to-image generative models, enhancing the alignment between generated images and textual prompts.","Despite these advances, current human preference datasets are either prohibitively expensive to construct or suffer from a lack of diversity in preference dimensions, resulting in limited applicability for instruction tuning in open-source text-to-image generative models and hinder further exploration.","To address these challenges and promote the alignment of generative models through instruction tuning, we leverage multimodal large language models to create VisionPrefer, a high-quality and fine-grained preference dataset that captures multiple preference aspects.","We aggregate feedback from AI annotators across four aspects: prompt-following, aesthetic, fidelity, and harmlessness to construct VisionPrefer.","To validate the effectiveness of VisionPrefer, we train a reward model VP-Score over VisionPrefer to guide the training of text-to-image generative models and the preference prediction accuracy of VP-Score is comparable to human annotators.","Furthermore, we use two reinforcement learning methods to supervised fine-tune generative models to evaluate the performance of VisionPrefer, and extensive experimental results demonstrate that VisionPrefer significantly improves text-image alignment in compositional image generation across diverse aspects, e.g., aesthetic, and generalizes better than previous human-preference metrics across various image distributions.","Moreover, VisionPrefer indicates that the integration of AI-generated synthetic data as a supervisory signal is a promising avenue for achieving improved alignment with human preferences in vision generative models."],"url":"http://arxiv.org/abs/2404.15100v1","category":"cs.CV"}
{"created":"2024-04-23 14:52:14","title":"Uncertainty Quantification of Data-Driven Output Predictors in the Output Error Setting","abstract":"We revisit the problem of predicting the output of an LTI system directly using offline input-output data (and without the use of a parametric model) in the behavioral setting. Existing works calculate the output predictions by projecting the recent samples of the input and output signals onto the column span of a Hankel matrix consisting of the offline input-output data. However, if the offline data is corrupted by noise, the output prediction is no longer exact. While some prior works propose mitigating noisy data through matrix low-ranking approximation heuristics, such as truncated singular value decomposition, the ensuing prediction accuracy remains unquantified. This paper fills these gaps by introducing two upper bounds on the prediction error under the condition that the noise is sufficiently small relative to the offline data's magnitude. The first bound pertains to prediction using the raw offline data directly, while the second one applies to the case of low-ranking approximation heuristic. Notably, the bounds do not require the ground truth about the system output, relying solely on noisy measurements with a known noise level and system order. Extensive numerical simulations show that both bounds decrease monotonically (and linearly) as a function of the noise level. Furthermore, our results demonstrate that applying the de-noising heuristic in the output error setup does not generally lead to a better prediction accuracy as compared to using raw data directly, nor a smaller upper bound on the prediction error. However, it allows for a more general upper bound, as the first upper bound requires a specific condition on the partitioning of the Hankel matrix.","sentences":["We revisit the problem of predicting the output of an LTI system directly using offline input-output data (and without the use of a parametric model) in the behavioral setting.","Existing works calculate the output predictions by projecting the recent samples of the input and output signals onto the column span of a Hankel matrix consisting of the offline input-output data.","However, if the offline data is corrupted by noise, the output prediction is no longer exact.","While some prior works propose mitigating noisy data through matrix low-ranking approximation heuristics, such as truncated singular value decomposition, the ensuing prediction accuracy remains unquantified.","This paper fills these gaps by introducing two upper bounds on the prediction error under the condition that the noise is sufficiently small relative to the offline data's magnitude.","The first bound pertains to prediction using the raw offline data directly, while the second one applies to the case of low-ranking approximation heuristic.","Notably, the bounds do not require the ground truth about the system output, relying solely on noisy measurements with a known noise level and system order.","Extensive numerical simulations show that both bounds decrease monotonically (and linearly) as a function of the noise level.","Furthermore, our results demonstrate that applying the de-noising heuristic in the output error setup does not generally lead to a better prediction accuracy as compared to using raw data directly, nor a smaller upper bound on the prediction error.","However, it allows for a more general upper bound, as the first upper bound requires a specific condition on the partitioning of the Hankel matrix."],"url":"http://arxiv.org/abs/2404.15098v1","category":"eess.SY"}
{"created":"2024-04-23 14:46:27","title":"Geometric measures of uniaxial solids of revolution in ${\\mathbb{R}^{4}}$ and their relation to the second virial coefficient","abstract":"We provide analytical expressions for the second virial coefficients of hard, convex, monoaxial solids of revolution in ${\\mathbb{R}^{4}}$. The excluded volume per particle and thus the second virial coefficient is calculated using quermassintegrals and rotationally invariant mixed volumes based on the Brunn-Minkowski theorem. We derive analytical expressions for the mutual excluded volume of four-dimensional hard solids of revolution in dependence on their aspect ratio $\\nu$ including the limits of infinitely thin oblate and infinitely long prolate geometries. Using reduced second virial coefficients $B_2^{\\ast}=B_2/V_{\\mathrm{P}}$ as size-independent quantities with $V_{\\mathrm{P}}$ denoting the $D$-dimensional particle volume, the influence of the particle geometry to the mutual excluded volume is analyzed for various shapes. Beyond the aspect ratio $\\nu$, the detailed particle shape influences the reduced second virial coefficients $B_2^{\\ast}$. We prove that for $D$-dimensional spherocylinders in arbitrary-dimensional Euclidean spaces ${\\mathbb{R}^{D}}$ their excluded volume solely depends on at most three intrinsic volumes, whereas for different convex geometries $D$ intrinsic volumes are required. For $D$-dimensional ellipsoids of revolution, the general parity $B_2^{\\ast}(\\nu)=B_2^{\\ast}(\\nu^{-1})$ is proven.","sentences":["We provide analytical expressions for the second virial coefficients of hard, convex, monoaxial solids of revolution in ${\\mathbb{R}^{4}}$. The excluded volume per particle and thus the second virial coefficient is calculated using quermassintegrals and rotationally invariant mixed volumes based on the Brunn-Minkowski theorem.","We derive analytical expressions for the mutual excluded volume of four-dimensional hard solids of revolution in dependence on their aspect ratio $\\nu$ including the limits of infinitely thin oblate and infinitely long prolate geometries.","Using reduced second virial coefficients $B_2^{\\ast}=B_2/V_{\\mathrm{P}}$ as size-independent quantities with $V_{\\mathrm{P}}$ denoting the $D$-dimensional particle volume, the influence of the particle geometry to the mutual excluded volume is analyzed for various shapes.","Beyond the aspect ratio $\\nu$, the detailed particle shape influences the reduced second virial coefficients $B_2^{\\ast}$.","We prove that for $D$-dimensional spherocylinders in arbitrary-dimensional Euclidean spaces ${\\mathbb{R}^{D}}$ their excluded volume solely depends on at most three intrinsic volumes, whereas for different convex geometries $D$ intrinsic volumes are required.","For $D$-dimensional ellipsoids of revolution, the general parity $B_2^{\\ast}(\\nu)=B_2^{\\ast}(\\nu^{-1})$ is proven."],"url":"http://arxiv.org/abs/2404.15092v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-23 14:44:36","title":"Predictive Intent Maintenance with Intent Drift Detection in Next Generation Network","abstract":"Intent-Based Networking (IBN) is a known concept for enabling the autonomous configuration and self-adaptation of networks. One of the major issues in IBN is maintaining the applied intent due the effects of drifts over time, which is the gradual degradation in the fulfillment of the intents, before they fail. Despite its critical role to intent assurance and maintenance, intent drift detection was largely overlooked in the literature. To fill this gap, we propose an intent drift detection algorithm for predictive maintenance of intents which can use various unsupervised learning techniques (Affinity Propagation, DBSCAN, Gaussian Mixture Models, Hierarchical clustering, K-Means clustering, OPTICS, One-Class SVM), here applied and comparatively analyzed due to their simplicity, yet efficiency in detecting drifts. The results show that DBSCAN is the best model for detecting the intent drifts. The worst performance is exhibited by the Affinity Propagation model, reflected in its poorest accuracy and latency values.","sentences":["Intent-Based Networking (IBN) is a known concept for enabling the autonomous configuration and self-adaptation of networks.","One of the major issues in IBN is maintaining the applied intent due the effects of drifts over time, which is the gradual degradation in the fulfillment of the intents, before they fail.","Despite its critical role to intent assurance and maintenance, intent drift detection was largely overlooked in the literature.","To fill this gap, we propose an intent drift detection algorithm for predictive maintenance of intents which can use various unsupervised learning techniques (Affinity Propagation, DBSCAN, Gaussian Mixture Models, Hierarchical clustering, K-Means clustering, OPTICS, One-Class SVM), here applied and comparatively analyzed due to their simplicity, yet efficiency in detecting drifts.","The results show that DBSCAN is the best model for detecting the intent drifts.","The worst performance is exhibited by the Affinity Propagation model, reflected in its poorest accuracy and latency values."],"url":"http://arxiv.org/abs/2404.15091v1","category":"cs.NI"}
{"created":"2024-04-23 14:42:50","title":"Galerkin-Bernstein Approximations for the System of Third-Order Nonlinear Boundary Value Problems","abstract":"This paper is devoted to find the numerical solutions of one dimensional general nonlinear system of third-order boundary value problems (BVPs) for the pair of functions using Galerkin weighted residual method. We derive mathematical formulations in matrix form, in details, by exploiting Bernstein polynomials as basis functions. A reasonable accuracy is found when the proposed method is used on few examples. At the end of the study, a comparison is made between the approximate and exact solutions, and also with the solutions of the existing methods. Our results converge monotonically to the exact solutions. In addition, we show that the the derived formulations may be applicable by reducing higher order complicated BVP into a lower order system of BVPs, and the performance of the numerical solutions is satisfactory.","sentences":["This paper is devoted to find the numerical solutions of one dimensional general nonlinear system of third-order boundary value problems (BVPs) for the pair of functions using Galerkin weighted residual method.","We derive mathematical formulations in matrix form, in details, by exploiting Bernstein polynomials as basis functions.","A reasonable accuracy is found when the proposed method is used on few examples.","At the end of the study, a comparison is made between the approximate and exact solutions, and also with the solutions of the existing methods.","Our results converge monotonically to the exact solutions.","In addition, we show that the the derived formulations may be applicable by reducing higher order complicated BVP into a lower order system of BVPs, and the performance of the numerical solutions is satisfactory."],"url":"http://arxiv.org/abs/2404.15090v1","category":"math.NA"}
{"created":"2024-04-23 14:42:36","title":"Vanilla Inflation Predicts Negative Running","abstract":"We show that the simplest, and currently favoured, theoretical realizations of cosmic inflation yield a sharp prediction for the running of the spectral index $\\alpha_\\mathrm{S}$. Using latest cosmological data, we compute its marginalized posterior probability distribution over the space of nearly 300 models of single-field slow-roll inflation. The most probable value is $\\alpha_\\mathrm{S}=-6.3 \\times 10^{-4}$, lying within the $98\\%$ credible interval $-1.8 \\times 10^{-3}< \\alpha_\\mathrm{S}< -9.1 \\times 10^{-5}$. Within the landscape of all the proposed slow-roll inflationary models, positive values for the running are therefore disfavoured at more than three-sigma.","sentences":["We show that the simplest, and currently favoured, theoretical realizations of cosmic inflation yield a sharp prediction for the running of the spectral index $\\alpha_\\mathrm{S}$. Using latest cosmological data, we compute its marginalized posterior probability distribution over the space of nearly 300 models of single-field slow-roll inflation.","The most probable value is $\\alpha_\\mathrm{S}=-6.3 \\times 10^{-4}$, lying within the $98\\%$ credible interval $-1.8 \\times 10^{-3}< \\alpha_\\mathrm{S}< -9.1 \\times 10^{-5}$. Within the landscape of all the proposed slow-roll inflationary models, positive values for the running are therefore disfavoured at more than three-sigma."],"url":"http://arxiv.org/abs/2404.15089v1","category":"astro-ph.CO"}
{"created":"2024-04-23 14:34:16","title":"Hyperparameter Optimization Can Even be Harmful in Off-Policy Learning and How to Deal with It","abstract":"There has been a growing interest in off-policy evaluation in the literature such as recommender systems and personalized medicine. We have so far seen significant progress in developing estimators aimed at accurately estimating the effectiveness of counterfactual policies based on biased logged data. However, there are many cases where those estimators are used not only to evaluate the value of decision making policies but also to search for the best hyperparameters from a large candidate space. This work explores the latter hyperparameter optimization (HPO) task for off-policy learning. We empirically show that naively applying an unbiased estimator of the generalization performance as a surrogate objective in HPO can cause an unexpected failure, merely pursuing hyperparameters whose generalization performance is greatly overestimated. We then propose simple and computationally efficient corrections to the typical HPO procedure to deal with the aforementioned issues simultaneously. Empirical investigations demonstrate the effectiveness of our proposed HPO algorithm in situations where the typical procedure fails severely.","sentences":["There has been a growing interest in off-policy evaluation in the literature such as recommender systems and personalized medicine.","We have so far seen significant progress in developing estimators aimed at accurately estimating the effectiveness of counterfactual policies based on biased logged data.","However, there are many cases where those estimators are used not only to evaluate the value of decision making policies but also to search for the best hyperparameters from a large candidate space.","This work explores the latter hyperparameter optimization (HPO) task for off-policy learning.","We empirically show that naively applying an unbiased estimator of the generalization performance as a surrogate objective in HPO can cause an unexpected failure, merely pursuing hyperparameters whose generalization performance is greatly overestimated.","We then propose simple and computationally efficient corrections to the typical HPO procedure to deal with the aforementioned issues simultaneously.","Empirical investigations demonstrate the effectiveness of our proposed HPO algorithm in situations where the typical procedure fails severely."],"url":"http://arxiv.org/abs/2404.15084v1","category":"cs.LG"}
{"created":"2024-04-23 14:33:23","title":"Between Flat-Earthers and Fitness Coaches: Who is Citing Scientific Publications in YouTube Video Descriptions?","abstract":"In this study, we undertake an extensive analysis of YouTube channels that reference research publications in their video descriptions, offering a unique insight into the intersection of digital media and academia. Our investigation focuses on three principal aspects: the background of YouTube channel owners, their thematic focus, and the nature of their operational dynamics, specifically addressing whether they work individually or in groups. Our results highlight a strong emphasis on content related to science and engineering, as well as health, particularly in channels managed by individual researchers and academic institutions. However, there is a notable variation in the popularity of these channels, with professional YouTubers and commercial media entities often outperforming in terms of viewer engagement metrics like likes, comments, and views. This underscores the challenge academic channels face in attracting a wider audience. Further, we explore the role of academic actors on YouTube, scrutinizing their impact in disseminating research and the types of publications they reference. Despite a general inclination towards professional academic topics, these channels displayed a varied effectiveness in spotlighting highly cited research. Often, they referenced a wide array of publications, indicating a diverse but not necessarily impact-focused approach to content selection.","sentences":["In this study, we undertake an extensive analysis of YouTube channels that reference research publications in their video descriptions, offering a unique insight into the intersection of digital media and academia.","Our investigation focuses on three principal aspects: the background of YouTube channel owners, their thematic focus, and the nature of their operational dynamics, specifically addressing whether they work individually or in groups.","Our results highlight a strong emphasis on content related to science and engineering, as well as health, particularly in channels managed by individual researchers and academic institutions.","However, there is a notable variation in the popularity of these channels, with professional YouTubers and commercial media entities often outperforming in terms of viewer engagement metrics like likes, comments, and views.","This underscores the challenge academic channels face in attracting a wider audience.","Further, we explore the role of academic actors on YouTube, scrutinizing their impact in disseminating research and the types of publications they reference.","Despite a general inclination towards professional academic topics, these channels displayed a varied effectiveness in spotlighting highly cited research.","Often, they referenced a wide array of publications, indicating a diverse but not necessarily impact-focused approach to content selection."],"url":"http://arxiv.org/abs/2404.15083v1","category":"cs.HC"}
{"created":"2024-04-23 14:31:44","title":"Harnessing Optical Imaging Limit through Atmospheric Scattering Media","abstract":"Recording and identifying faint objects through atmospheric scattering media by an optical system are fundamentally interesting and technologically important. In this work, we introduce a comprehensive model that incorporates contributions from target characteristics, atmospheric effects, imaging system, digital processing, and visual perception to assess the ultimate perceptible limit of geometrical imaging, specifically the angular resolution at the boundary of visible distance. The model allows to reevaluate the effectiveness of conventional imaging recording, processing, and perception and to analyze the limiting factors that constrain image recognition capabilities in atmospheric media. The simulations were compared with the experimental results measured in a fog chamber and outdoor settings. The results reveal general good agreement between analysis and experimental, pointing out the way to harnessing the physical limit for optical imaging in scattering media. An immediate application of the study is the extension of the image range by an amount of 1.2 times with noise reduction via multi-frame averaging, hence greatly enhancing the capability of optical imaging in the atmosphere.","sentences":["Recording and identifying faint objects through atmospheric scattering media by an optical system are fundamentally interesting and technologically important.","In this work, we introduce a comprehensive model that incorporates contributions from target characteristics, atmospheric effects, imaging system, digital processing, and visual perception to assess the ultimate perceptible limit of geometrical imaging, specifically the angular resolution at the boundary of visible distance.","The model allows to reevaluate the effectiveness of conventional imaging recording, processing, and perception and to analyze the limiting factors that constrain image recognition capabilities in atmospheric media.","The simulations were compared with the experimental results measured in a fog chamber and outdoor settings.","The results reveal general good agreement between analysis and experimental, pointing out the way to harnessing the physical limit for optical imaging in scattering media.","An immediate application of the study is the extension of the image range by an amount of 1.2 times with noise reduction via multi-frame averaging, hence greatly enhancing the capability of optical imaging in the atmosphere."],"url":"http://arxiv.org/abs/2404.15082v1","category":"physics.optics"}
{"created":"2024-04-23 14:31:15","title":"Perturbing Attention Gives You More Bang for the Buck: Subtle Imaging Perturbations That Efficiently Fool Customized Diffusion Models","abstract":"Diffusion models (DMs) embark a new era of generative modeling and offer more opportunities for efficient generating high-quality and realistic data samples. However, their widespread use has also brought forth new challenges in model security, which motivates the creation of more effective adversarial attackers on DMs to understand its vulnerability. We propose CAAT, a simple but generic and efficient approach that does not require costly training to effectively fool latent diffusion models (LDMs). The approach is based on the observation that cross-attention layers exhibits higher sensitivity to gradient change, allowing for leveraging subtle perturbations on published images to significantly corrupt the generated images. We show that a subtle perturbation on an image can significantly impact the cross-attention layers, thus changing the mapping between text and image during the fine-tuning of customized diffusion models. Extensive experiments demonstrate that CAAT is compatible with diverse diffusion models and outperforms baseline attack methods in a more effective (more noise) and efficient (twice as fast as Anti-DreamBooth and Mist) manner.","sentences":["Diffusion models (DMs) embark a new era of generative modeling and offer more opportunities for efficient generating high-quality and realistic data samples.","However, their widespread use has also brought forth new challenges in model security, which motivates the creation of more effective adversarial attackers on DMs to understand its vulnerability.","We propose CAAT, a simple but generic and efficient approach that does not require costly training to effectively fool latent diffusion models (LDMs).","The approach is based on the observation that cross-attention layers exhibits higher sensitivity to gradient change, allowing for leveraging subtle perturbations on published images to significantly corrupt the generated images.","We show that a subtle perturbation on an image can significantly impact the cross-attention layers, thus changing the mapping between text and image during the fine-tuning of customized diffusion models.","Extensive experiments demonstrate that CAAT is compatible with diverse diffusion models and outperforms baseline attack methods in a more effective (more noise) and efficient (twice as fast as Anti-DreamBooth and Mist) manner."],"url":"http://arxiv.org/abs/2404.15081v1","category":"cs.CV"}
{"created":"2024-04-23 14:31:02","title":"Flexible Field Sizes in Secure Distributed Matrix Multiplication via Efficient Interference Cancellation","abstract":"In this paper, we propose a new secure distributed matrix multiplication (SDMM) scheme using the inner product partitioning. We construct a scheme with a minimal number of workers and no redundancy, and another scheme with redundancy against stragglers. Unlike previous constructions in the literature, we do not utilize algebraic methods such as locally repairable codes or algebraic geometry codes. Our construction, which is based on generalized Reed-Solomon codes, improves the flexibility of the field size as it does not assume any divisibility constraints among the different parameters. We achieve a minimal number of workers by efficiently canceling all interference terms with a suitable orthogonal decoding vector. Finally, we discuss how the MDS conjecture impacts the smallest achievable field size for SDMM schemes and show that our construction almost achieves the bound given by the conjecture.","sentences":["In this paper, we propose a new secure distributed matrix multiplication (SDMM) scheme using the inner product partitioning.","We construct a scheme with a minimal number of workers and no redundancy, and another scheme with redundancy against stragglers.","Unlike previous constructions in the literature, we do not utilize algebraic methods such as locally repairable codes or algebraic geometry codes.","Our construction, which is based on generalized Reed-Solomon codes, improves the flexibility of the field size as it does not assume any divisibility constraints among the different parameters.","We achieve a minimal number of workers by efficiently canceling all interference terms with a suitable orthogonal decoding vector.","Finally, we discuss how the MDS conjecture impacts the smallest achievable field size for SDMM schemes and show that our construction almost achieves the bound given by the conjecture."],"url":"http://arxiv.org/abs/2404.15080v1","category":"cs.IT"}
{"created":"2024-04-23 14:25:05","title":"Securing O-RAN Open Interfaces","abstract":"The next generation of cellular networks will be characterized by openness, intelligence, virtualization, and distributed computing. The Open Radio Access Network (Open RAN) framework represents a significant leap toward realizing these ideals, with prototype deployments taking place in both academic and industrial domains. While it holds the potential to disrupt the established vendor lock-ins, Open RAN's disaggregated nature raises critical security concerns. Safeguarding data and securing interfaces must be integral to Open RAN's design, demanding meticulous analysis of cost/benefit tradeoffs.   In this paper, we embark on the first comprehensive investigation into the impact of encryption on two pivotal Open RAN interfaces: the E2 interface, connecting the base station with a near-real-time RAN Intelligent Controller, and the Open Fronthaul, connecting the Radio Unit to the Distributed Unit. Our study leverages a full-stack O-RAN ALLIANCE compliant implementation within the Colosseum network emulator and a production-ready Open RAN and 5G-compliant private cellular network. This research contributes quantitative insights into the latency introduced and throughput reduction stemming from using various encryption protocols. Furthermore, we present four fundamental principles for constructing security by design within Open RAN systems, offering a roadmap for navigating the intricate landscape of Open RAN security.","sentences":["The next generation of cellular networks will be characterized by openness, intelligence, virtualization, and distributed computing.","The Open Radio Access Network (Open RAN) framework represents a significant leap toward realizing these ideals, with prototype deployments taking place in both academic and industrial domains.","While it holds the potential to disrupt the established vendor lock-ins, Open RAN's disaggregated nature raises critical security concerns.","Safeguarding data and securing interfaces must be integral to Open RAN's design, demanding meticulous analysis of cost/benefit tradeoffs.   ","In this paper, we embark on the first comprehensive investigation into the impact of encryption on two pivotal Open RAN interfaces: the E2 interface, connecting the base station with a near-real-time RAN Intelligent Controller, and the Open Fronthaul, connecting the Radio Unit to the Distributed Unit.","Our study leverages a full-stack O-RAN ALLIANCE compliant implementation within the Colosseum network emulator and a production-ready Open RAN and 5G-compliant private cellular network.","This research contributes quantitative insights into the latency introduced and throughput reduction stemming from using various encryption protocols.","Furthermore, we present four fundamental principles for constructing security by design within Open RAN systems, offering a roadmap for navigating the intricate landscape of Open RAN security."],"url":"http://arxiv.org/abs/2404.15076v1","category":"eess.SY"}
{"created":"2024-04-23 14:24:31","title":"Outage Probability Analysis of Wireless Paths with Faulty Reconfigurable Intelligent Surfaces","abstract":"We consider a next generation wireless network incorporating a base station a set of typically low-cost and faulty Reconfigurable Intelligent Surfaces (RISs). The base station needs to select the path including the RIS to provide the maximum signal-to-noise ratio (SNR) to the user. We study the effect of the number of elements, distance and RIS hardware failure on the path outage probability, and based on the known signal propagation model at high frequencies, derive the closed-form expression for the said probability of outage. Numerical results show the path outage likelihood as function of the probability of hardware failure of RIS elements, the number of elements, and the distance between mobile users and the RIS.","sentences":["We consider a next generation wireless network incorporating a base station a set of typically low-cost and faulty Reconfigurable Intelligent Surfaces (RISs).","The base station needs to select the path including the RIS to provide the maximum signal-to-noise ratio (SNR) to the user.","We study the effect of the number of elements, distance and RIS hardware failure on the path outage probability, and based on the known signal propagation model at high frequencies, derive the closed-form expression for the said probability of outage.","Numerical results show the path outage likelihood as function of the probability of hardware failure of RIS elements, the number of elements, and the distance between mobile users and the RIS."],"url":"http://arxiv.org/abs/2404.15074v1","category":"cs.IT"}
{"created":"2024-04-23 14:17:12","title":"Hopping of the center-of-mass of single G centers in silicon-on-insulator","abstract":"Among the wealth of single fluorescent defects recently detected in silicon, the G center catches interest for its telecom single-photon emission that could be coupled to a metastable electron spin triplet. The G center is a unique defect where the standard Born-Oppenheimer approximation breaks down as one of its atoms can move between 6 lattice sites under optical excitation. The impact of this atomic reconfiguration on the photoluminescence properties of G centers is still largely unknown, especially in silicon-on-insulator (SOI) samples. Here, we investigate the displacement of the center-of-mass of the G center in silicon. We show that single G defects in SOI exhibit a multipolar emission and zero-phonon line fine structures with splittings up to $\\sim1$ meV, both indicating a motion of the defect central atom over time. Combining polarization and spectral analysis at the single-photon level, we evidence that the reconfiguration dynamics are drastically different from the one of the unperturbed G center in bulk silicon. The SOI structure freezes the delocalization of the G defect center-of-mass and as a result, enables to isolate linearly polarized optical lines. Under above-bandgap optical excitation, the central atom of G centers in SOI behaves as if it were in a 6-slot roulette wheel, randomly alternating between localized crystal sites at each optical cycle. Comparative measurements in a bulk silicon sample and ab initio calculations highlight that strain is likely the dominant perturbation impacting the G center geometry. These results shed light on the importance of the atomic reconfiguration dynamics to understand and control the photoluminescence properties of the G center in silicon. More generally, these findings emphasize the impact of strain fluctuations inherent to SOI wafers for future quantum integrated photonics applications based on color centers in silicon.","sentences":["Among the wealth of single fluorescent defects recently detected in silicon, the G center catches interest for its telecom single-photon emission that could be coupled to a metastable electron spin triplet.","The G center is a unique defect where the standard Born-Oppenheimer approximation breaks down as one of its atoms can move between 6 lattice sites under optical excitation.","The impact of this atomic reconfiguration on the photoluminescence properties of G centers is still largely unknown, especially in silicon-on-insulator (SOI) samples.","Here, we investigate the displacement of the center-of-mass of the G center in silicon.","We show that single G defects in SOI exhibit a multipolar emission and zero-phonon line fine structures with splittings up to $\\sim1$ meV, both indicating a motion of the defect central atom over time.","Combining polarization and spectral analysis at the single-photon level, we evidence that the reconfiguration dynamics are drastically different from the one of the unperturbed G center in bulk silicon.","The SOI structure freezes the delocalization of the G defect center-of-mass and as a result, enables to isolate linearly polarized optical lines.","Under above-bandgap optical excitation, the central atom of G centers in SOI behaves as if it were in a 6-slot roulette wheel, randomly alternating between localized crystal sites at each optical cycle.","Comparative measurements in a bulk silicon sample and ab initio calculations highlight that strain is likely the dominant perturbation impacting the G center geometry.","These results shed light on the importance of the atomic reconfiguration dynamics to understand and control the photoluminescence properties of the G center in silicon.","More generally, these findings emphasize the impact of strain fluctuations inherent to SOI wafers for future quantum integrated photonics applications based on color centers in silicon."],"url":"http://arxiv.org/abs/2404.15069v1","category":"quant-ph"}
{"created":"2024-04-23 14:13:53","title":"Enhancing Textual Personality Detection toward Social Media: Integrating Long-term and Short-term Perspectives","abstract":"Textual personality detection aims to identify personality characteristics by analyzing user-generated content toward social media platforms. Numerous psychological literature highlighted that personality encompasses both long-term stable traits and short-term dynamic states. However, existing studies often concentrate only on either long-term or short-term personality representations, without effectively combining both aspects. This limitation hinders a comprehensive understanding of individuals' personalities, as both stable traits and dynamic states are vital. To bridge this gap, we propose a Dual Enhanced Network(DEN) to jointly model users' long-term and short-term personality for textual personality detection. In DEN, a Long-term Personality Encoding is devised to effectively model long-term stable personality traits. Short-term Personality Encoding is presented to capture short-term dynamic personality states. The Bi-directional Interaction component facilitates the integration of both personality aspects, allowing for a comprehensive representation of the user's personality. Experimental results on two personality detection datasets demonstrate the effectiveness of the DEN model and the benefits of considering both the dynamic and stable nature of personality characteristics for textual personality detection.","sentences":["Textual personality detection aims to identify personality characteristics by analyzing user-generated content toward social media platforms.","Numerous psychological literature highlighted that personality encompasses both long-term stable traits and short-term dynamic states.","However, existing studies often concentrate only on either long-term or short-term personality representations, without effectively combining both aspects.","This limitation hinders a comprehensive understanding of individuals' personalities, as both stable traits and dynamic states are vital.","To bridge this gap, we propose a Dual Enhanced Network(DEN) to jointly model users' long-term and short-term personality for textual personality detection.","In DEN, a Long-term Personality Encoding is devised to effectively model long-term stable personality traits.","Short-term Personality Encoding is presented to capture short-term dynamic personality states.","The Bi-directional Interaction component facilitates the integration of both personality aspects, allowing for a comprehensive representation of the user's personality.","Experimental results on two personality detection datasets demonstrate the effectiveness of the DEN model and the benefits of considering both the dynamic and stable nature of personality characteristics for textual personality detection."],"url":"http://arxiv.org/abs/2404.15067v1","category":"cs.CL"}
{"created":"2024-04-23 14:12:48","title":"Formal Verification of Graph Convolutional Networks with Uncertain Node Features and Uncertain Graph Structure","abstract":"Graph neural networks are becoming increasingly popular in the field of machine learning due to their unique ability to process data structured in graphs. They have also been applied in safety-critical environments where perturbations inherently occur. However, these perturbations require us to formally verify neural networks before their deployment in safety-critical environments as neural networks are prone to adversarial attacks. While there exists research on the formal verification of neural networks, there is no work verifying the robustness of generic graph convolutional network architectures with uncertainty in the node features and in the graph structure over multiple message-passing steps. This work addresses this research gap by explicitly preserving the non-convex dependencies of all elements in the underlying computations through reachability analysis with (matrix) polynomial zonotopes. We demonstrate our approach on three popular benchmark datasets.","sentences":["Graph neural networks are becoming increasingly popular in the field of machine learning due to their unique ability to process data structured in graphs.","They have also been applied in safety-critical environments where perturbations inherently occur.","However, these perturbations require us to formally verify neural networks before their deployment in safety-critical environments as neural networks are prone to adversarial attacks.","While there exists research on the formal verification of neural networks, there is no work verifying the robustness of generic graph convolutional network architectures with uncertainty in the node features and in the graph structure over multiple message-passing steps.","This work addresses this research gap by explicitly preserving the non-convex dependencies of all elements in the underlying computations through reachability analysis with (matrix) polynomial zonotopes.","We demonstrate our approach on three popular benchmark datasets."],"url":"http://arxiv.org/abs/2404.15065v1","category":"cs.LG"}
{"created":"2024-04-23 14:11:21","title":"On cyclotomic matrices involving Gauss sums over finite fields","abstract":"Inspired by the Carlitz's work on cyclotomic matrices, in this paper, we investigate certain cyclotomic matrices involving Gauss sums over finite fields. For example, let $q=p^n$ be an odd prime power with $p$ prime and $n\\in\\mathbb{Z}^+$. Let $\\zeta_p=e^{2\\pi{\\bf i}/p}$ and let $\\chi$ be a generator of the group of all mutiplicative characters of the finite field $\\mathbb{F}_q$. For the Gauss sum   $$G_q(\\chi^{r})=\\sum_{x\\in\\mathbb{F}_q}\\chi^{r}(x)\\zeta_p^{{\\rm Tr}_{\\mathbb{F}_q/\\mathbb{F}_p}(x)},$$   we prove that   $$\\det \\left[G_q(\\chi^{2i+2j})\\right]_{0\\le i,j\\le (q-3)/2}=(-1)^{\\alpha_p}\\left(\\frac{q-1}{2}\\right)^{\\frac{q-1}{2}}2^{\\frac{p^{n-1}-1}{2}},$$   where   $$\\alpha_p=   \\begin{cases}   1 & \\mbox{if}\\ n\\equiv 1\\pmod 2,   (p^2+7)/8 & \\mbox{if}\\ n\\equiv 0\\pmod 2.   \\end{cases}$$","sentences":["Inspired by the Carlitz's work on cyclotomic matrices, in this paper, we investigate certain cyclotomic matrices involving Gauss sums over finite fields.","For example, let $q=p^n$ be an odd prime power with $p$ prime and $n\\in\\mathbb{Z}^+$. Let $\\zeta_p=e^{2\\pi{\\bf i}/p}$ and let $\\chi$ be a generator of the group of all mutiplicative characters of the finite field $\\mathbb{F}_q$. For the Gauss sum   $$G_q(\\chi^{r})=\\sum_{x\\in\\mathbb{F}_q}\\chi^{r}(x)\\zeta_p^{{\\rm Tr}_{\\mathbb{F}_q/\\mathbb{F}_p}(x)},$$   we prove that   $$\\det \\left[G_q(\\chi^{2i+2j})\\right]_{0\\le i,j\\le (q-3)/2}=(-1)^{\\alpha_p}\\left(\\frac{q-1}{2}\\right)^{\\frac{q-1}{2}}2^{\\frac{p^{n-1}-1}{2}},$$   where   $$\\alpha_p=   \\begin{cases}   1 & \\mbox{if}\\ n\\equiv 1\\pmod 2,   (p^2+7)/8 & \\mbox{if}\\ n\\equiv 0\\pmod 2.   \\end{cases}$$"],"url":"http://arxiv.org/abs/2404.15063v2","category":"math.NT"}
{"created":"2024-04-23 14:10:52","title":"superblockify: A Python Package for Automated Generation, Visualization, and Analysis of Potential Superblocks in Cities","abstract":"superblockify is a Python package for partitioning an urban street network into Superblock-like neighborhoods and for visualizing and analyzing the partition results. A Superblock is a set of adjacent urban blocks where vehicular through traffic is prevented or pacified, giving priority to people walking and cycling. The Superblock blueprints and descriptive statistics generated by superblockify can be used by urban planners as a first step in a data-driven planning pipeline, or by urban data scientists as an efficient computational method to evaluate Superblock partitions. The software is licensed under AGPLv3 and is available at https://superblockify.city.","sentences":["superblockify is a Python package for partitioning an urban street network into Superblock-like neighborhoods and for visualizing and analyzing the partition results.","A Superblock is a set of adjacent urban blocks where vehicular through traffic is prevented or pacified, giving priority to people walking and cycling.","The Superblock blueprints and descriptive statistics generated by superblockify can be used by urban planners as a first step in a data-driven planning pipeline, or by urban data scientists as an efficient computational method to evaluate Superblock partitions.","The software is licensed under AGPLv3 and is available at https://superblockify.city."],"url":"http://arxiv.org/abs/2404.15062v1","category":"physics.soc-ph"}
{"created":"2024-04-23 14:10:00","title":"Neural Slicer for Multi-Axis 3D Printing","abstract":"We introduce a novel neural network-based computational pipeline as a representation-agnostic slicer for multi-axis 3D printing. This advanced slicer can work on models with diverse representations and intricate topology. The approach involves employing neural networks to establish a deformation mapping, defining a scalar field in the space surrounding an input model. Isosurfaces are subsequently extracted from this field to generate curved layers for 3D printing. Creating a differentiable pipeline enables us to optimize the mapping through loss functions directly defined on the field gradients as the local printing directions. New loss functions have been introduced to meet the manufacturing objectives of support-free and strength reinforcement. Our new computation pipeline relies less on the initial values of the field and can generate slicing results with significantly improved performance.","sentences":["We introduce a novel neural network-based computational pipeline as a representation-agnostic slicer for multi-axis 3D printing.","This advanced slicer can work on models with diverse representations and intricate topology.","The approach involves employing neural networks to establish a deformation mapping, defining a scalar field in the space surrounding an input model.","Isosurfaces are subsequently extracted from this field to generate curved layers for 3D printing.","Creating a differentiable pipeline enables us to optimize the mapping through loss functions directly defined on the field gradients as the local printing directions.","New loss functions have been introduced to meet the manufacturing objectives of support-free and strength reinforcement.","Our new computation pipeline relies less on the initial values of the field and can generate slicing results with significantly improved performance."],"url":"http://arxiv.org/abs/2404.15061v1","category":"cs.CG"}
{"created":"2024-04-23 14:07:39","title":"Using deep reinforcement learning to promote sustainable human behaviour on a common pool resource problem","abstract":"A canonical social dilemma arises when finite resources are allocated to a group of people, who can choose to either reciprocate with interest, or keep the proceeds for themselves. What resource allocation mechanisms will encourage levels of reciprocation that sustain the commons? Here, in an iterated multiplayer trust game, we use deep reinforcement learning (RL) to design an allocation mechanism that endogenously promotes sustainable contributions from human participants to a common pool resource. We first trained neural networks to behave like human players, creating a stimulated economy that allowed us to study how different mechanisms influenced the dynamics of receipt and reciprocation. We then used RL to train a social planner to maximise aggregate return to players. The social planner discovered a redistributive policy that led to a large surplus and an inclusive economy, in which players made roughly equal gains. The RL agent increased human surplus over baseline mechanisms based on unrestricted welfare or conditional cooperation, by conditioning its generosity on available resources and temporarily sanctioning defectors by allocating fewer resources to them. Examining the AI policy allowed us to develop an explainable mechanism that performed similarly and was more popular among players. Deep reinforcement learning can be used to discover mechanisms that promote sustainable human behaviour.","sentences":["A canonical social dilemma arises when finite resources are allocated to a group of people, who can choose to either reciprocate with interest, or keep the proceeds for themselves.","What resource allocation mechanisms will encourage levels of reciprocation that sustain the commons?","Here, in an iterated multiplayer trust game, we use deep reinforcement learning (RL) to design an allocation mechanism that endogenously promotes sustainable contributions from human participants to a common pool resource.","We first trained neural networks to behave like human players, creating a stimulated economy that allowed us to study how different mechanisms influenced the dynamics of receipt and reciprocation.","We then used RL to train a social planner to maximise aggregate return to players.","The social planner discovered a redistributive policy that led to a large surplus and an inclusive economy, in which players made roughly equal gains.","The RL agent increased human surplus over baseline mechanisms based on unrestricted welfare or conditional cooperation, by conditioning its generosity on available resources and temporarily sanctioning defectors by allocating fewer resources to them.","Examining the AI policy allowed us to develop an explainable mechanism that performed similarly and was more popular among players.","Deep reinforcement learning can be used to discover mechanisms that promote sustainable human behaviour."],"url":"http://arxiv.org/abs/2404.15059v1","category":"cs.AI"}
{"created":"2024-04-23 14:07:20","title":"A Mechanism-Based Approach to Mitigating Harms from Persuasive Generative AI","abstract":"Recent generative AI systems have demonstrated more advanced persuasive capabilities and are increasingly permeating areas of life where they can influence decision-making. Generative AI presents a new risk profile of persuasion due the opportunity for reciprocal exchange and prolonged interactions. This has led to growing concerns about harms from AI persuasion and how they can be mitigated, highlighting the need for a systematic study of AI persuasion. The current definitions of AI persuasion are unclear and related harms are insufficiently studied. Existing harm mitigation approaches prioritise harms from the outcome of persuasion over harms from the process of persuasion. In this paper, we lay the groundwork for the systematic study of AI persuasion. We first put forward definitions of persuasive generative AI. We distinguish between rationally persuasive generative AI, which relies on providing relevant facts, sound reasoning, or other forms of trustworthy evidence, and manipulative generative AI, which relies on taking advantage of cognitive biases and heuristics or misrepresenting information. We also put forward a map of harms from AI persuasion, including definitions and examples of economic, physical, environmental, psychological, sociocultural, political, privacy, and autonomy harm. We then introduce a map of mechanisms that contribute to harmful persuasion. Lastly, we provide an overview of approaches that can be used to mitigate against process harms of persuasion, including prompt engineering for manipulation classification and red teaming. Future work will operationalise these mitigations and study the interaction between different types of mechanisms of persuasion.","sentences":["Recent generative AI systems have demonstrated more advanced persuasive capabilities and are increasingly permeating areas of life where they can influence decision-making.","Generative AI presents a new risk profile of persuasion due the opportunity for reciprocal exchange and prolonged interactions.","This has led to growing concerns about harms from AI persuasion and how they can be mitigated, highlighting the need for a systematic study of AI persuasion.","The current definitions of AI persuasion are unclear and related harms are insufficiently studied.","Existing harm mitigation approaches prioritise harms from the outcome of persuasion over harms from the process of persuasion.","In this paper, we lay the groundwork for the systematic study of AI persuasion.","We first put forward definitions of persuasive generative AI.","We distinguish between rationally persuasive generative AI, which relies on providing relevant facts, sound reasoning, or other forms of trustworthy evidence, and manipulative generative AI, which relies on taking advantage of cognitive biases and heuristics or misrepresenting information.","We also put forward a map of harms from AI persuasion, including definitions and examples of economic, physical, environmental, psychological, sociocultural, political, privacy, and autonomy harm.","We then introduce a map of mechanisms that contribute to harmful persuasion.","Lastly, we provide an overview of approaches that can be used to mitigate against process harms of persuasion, including prompt engineering for manipulation classification and red teaming.","Future work will operationalise these mitigations and study the interaction between different types of mechanisms of persuasion."],"url":"http://arxiv.org/abs/2404.15058v1","category":"cs.CY"}
{"created":"2024-04-23 13:56:47","title":"Asymmetric rectified electric fields for symmetric electrolytes","abstract":"In this paper, building upon the discovery of asymmetric rectified electric fields (AREF) in recent experiments [S.H. Hashemi et al., Physical Review Letters 121, 185504 (2018)], we explore the generation of AREF by applying a sawtooth-like voltage to 1:1 electrolytes with equal diffusion coefficients confined between two planar blocking electrodes. This differs from an earlier approach based on a sinusoidal AC voltage applied to 1:1 electrolytes with unequal diffusion coefficients. By numerically solving the full Poisson-Nernst-Planck equations, we demonstrate that AREF can be generated by a slow rise and a fast drop of the potential (or vice versa), even for electrolytes with equal diffusion coefficients of the cations and anions. We employ an analytically constructed equivalent electric circuit to explain the underlying physical mechanism. Importantly, we find that the strength of AREF can be effectively tuned from zero to its maximal value by only manipulating the time-dependence of the driving voltage, eliminating the necessity to modify the electrolyte composition between experiments. This provides valuable insights to control the manipulation of AREF, which facilitates enhanced applications in diverse electrochemical systems.","sentences":["In this paper, building upon the discovery of asymmetric rectified electric fields (AREF) in recent experiments","[S.H. Hashemi et al., Physical Review Letters 121, 185504 (2018)], we explore the generation of AREF by applying a sawtooth-like voltage to 1:1 electrolytes with equal diffusion coefficients confined between two planar blocking electrodes.","This differs from an earlier approach based on a sinusoidal AC voltage applied to 1:1 electrolytes with unequal diffusion coefficients.","By numerically solving the full Poisson-Nernst-Planck equations, we demonstrate that AREF can be generated by a slow rise and a fast drop of the potential (or vice versa), even for electrolytes with equal diffusion coefficients of the cations and anions.","We employ an analytically constructed equivalent electric circuit to explain the underlying physical mechanism.","Importantly, we find that the strength of AREF can be effectively tuned from zero to its maximal value by only manipulating the time-dependence of the driving voltage, eliminating the necessity to modify the electrolyte composition between experiments.","This provides valuable insights to control the manipulation of AREF, which facilitates enhanced applications in diverse electrochemical systems."],"url":"http://arxiv.org/abs/2404.15055v1","category":"cond-mat.soft"}
{"created":"2024-04-23 13:53:37","title":"Positive Moments Forever: Undecidable and Decidable Cases","abstract":"Is there an algorithm to determine attributes such as positivity or non-zeroness of linear recurrence sequences? This long-standing question is known as Skolem's problem. In this paper, we study the complexity of an equivalent problem, namely the (generalized) moment membership problem for matrices. We show that this problem is decidable for orthogonal, unitary and real eigenvalue matrices, and undecidable for matrices over certain commutative and non-commutative polynomial rings. Our results imply that the positivity problem for simple unitary linear recurrence sequences is decidable, and is undecidable for linear recurrence sequences over the ring of commutative polynomials. As a byproduct, we prove a free version of Polya's theorem.","sentences":["Is there an algorithm to determine attributes such as positivity or non-zeroness of linear recurrence sequences?","This long-standing question is known as Skolem's problem.","In this paper, we study the complexity of an equivalent problem, namely the (generalized) moment membership problem for matrices.","We show that this problem is decidable for orthogonal, unitary and real eigenvalue matrices, and undecidable for matrices over certain commutative and non-commutative polynomial rings.","Our results imply that the positivity problem for simple unitary linear recurrence sequences is decidable, and is undecidable for linear recurrence sequences over the ring of commutative polynomials.","As a byproduct, we prove a free version of Polya's theorem."],"url":"http://arxiv.org/abs/2404.15053v1","category":"math.AG"}
{"created":"2024-04-23 13:51:20","title":"Finite Automata for Efficient Graph Recognition","abstract":"Engelfriet and Vereijken have shown that linear graph grammars based on hyperedge replacement generate graph languages that can be considered as interpretations of regular string languages over typed symbols. In this paper we show that finite automata can be lifted from strings to graphs within the same framework. For the efficient recognition of graphs with these automata, we make them deterministic by a modified powerset construction, and state sufficient conditions under which deterministic finite graph automata recognize graphs without the need to use backtracking.","sentences":["Engelfriet and Vereijken have shown that linear graph grammars based on hyperedge replacement generate graph languages that can be considered as interpretations of regular string languages over typed symbols.","In this paper we show that finite automata can be lifted from strings to graphs within the same framework.","For the efficient recognition of graphs with these automata, we make them deterministic by a modified powerset construction, and state sufficient conditions under which deterministic finite graph automata recognize graphs without the need to use backtracking."],"url":"http://arxiv.org/abs/2404.15052v1","category":"cs.FL"}
{"created":"2024-04-23 13:47:40","title":"Reflections on the Larson-Sweedler theorem for (weak) multiplier Hopf algebras","abstract":"Let $A$ be an algebra with identity and $\\Delta:A\\to A\\otimes A$ a coproduct that admits a counit. If there exist a faithful left integral and a faithful right integral, one can construct an antipode and $(A,\\Delta)$ is a Hopf algebra. This is the Larson-Sweedler theorem. There are generalizations of this result for multiplier Hopf algebras, weak Hopf algebras and weak multiplier Hopf algebras. In the case of a multiplier Hopf algebra, the existence of a counit can be weakened and can be replaced by the requirement that the coproduct is full. A similar result is true for weak multiplier Hopf algebras. What we show in this note is that in fact the result for multiplier Hopf algebras can still be obtained without the condition of fullness of the coproduct. As it turns out, this property will already follow from the other conditions. Consequently, also in the original theorem for Hopf algebras, the existence of a counit is a consequence of the other conditions. This slightly generalizes the original result. The situation for weak multiplier Hopf algebras seems to be more subtle. We discuss the problems and see what is still possible here. We consider these results in connection with the development of the theory of locally compact quantum groups. This is discussed in an appendix.","sentences":["Let $A$ be an algebra with identity and $\\Delta:A\\to A\\otimes A$ a coproduct that admits a counit.","If there exist a faithful left integral and a faithful right integral, one can construct an antipode and $(A,\\Delta)$ is a Hopf algebra.","This is the Larson-Sweedler theorem.","There are generalizations of this result for multiplier Hopf algebras, weak Hopf algebras and weak multiplier Hopf algebras.","In the case of a multiplier Hopf algebra, the existence of a counit can be weakened and can be replaced by the requirement that the coproduct is full.","A similar result is true for weak multiplier Hopf algebras.","What we show in this note is that in fact the result for multiplier Hopf algebras can still be obtained without the condition of fullness of the coproduct.","As it turns out, this property will already follow from the other conditions.","Consequently, also in the original theorem for Hopf algebras, the existence of a counit is a consequence of the other conditions.","This slightly generalizes the original result.","The situation for weak multiplier Hopf algebras seems to be more subtle.","We discuss the problems and see what is still possible here.","We consider these results in connection with the development of the theory of locally compact quantum groups.","This is discussed in an appendix."],"url":"http://arxiv.org/abs/2404.15046v1","category":"math.QA"}
{"created":"2024-04-23 13:47:09","title":"Multi-Head Mixture-of-Experts","abstract":"Sparse Mixtures of Experts (SMoE) scales model capacity without significant increases in training and inference costs, but exhibits the following two issues: (1) Low expert activation, where only a small subset of experts are activated for optimization. (2) Lacking fine-grained analytical capabilities for multiple semantic concepts within individual tokens. We propose Multi-Head Mixture-of-Experts (MH-MoE), which employs a multi-head mechanism to split each token into multiple sub-tokens. These sub-tokens are then assigned to and processed by a diverse set of experts in parallel, and seamlessly reintegrated into the original token form. The multi-head mechanism enables the model to collectively attend to information from various representation spaces within different experts, while significantly enhances expert activation, thus deepens context understanding and alleviate overfitting. Moreover, our MH-MoE is straightforward to implement and decouples from other SMoE optimization methods, making it easy to integrate with other SMoE models for enhanced performance. Extensive experimental results across three tasks: English-focused language modeling, Multi-lingual language modeling and Masked multi-modality modeling tasks, demonstrate the effectiveness of MH-MoE.","sentences":["Sparse Mixtures of Experts (SMoE) scales model capacity without significant increases in training and inference costs, but exhibits the following two issues: (1) Low expert activation, where only a small subset of experts are activated for optimization.","(2) Lacking fine-grained analytical capabilities for multiple semantic concepts within individual tokens.","We propose Multi-Head Mixture-of-Experts (MH-MoE), which employs a multi-head mechanism to split each token into multiple sub-tokens.","These sub-tokens are then assigned to and processed by a diverse set of experts in parallel, and seamlessly reintegrated into the original token form.","The multi-head mechanism enables the model to collectively attend to information from various representation spaces within different experts, while significantly enhances expert activation, thus deepens context understanding and alleviate overfitting.","Moreover, our MH-MoE is straightforward to implement and decouples from other SMoE optimization methods, making it easy to integrate with other SMoE models for enhanced performance.","Extensive experimental results across three tasks: English-focused language modeling, Multi-lingual language modeling and Masked multi-modality modeling tasks, demonstrate the effectiveness of MH-MoE."],"url":"http://arxiv.org/abs/2404.15045v1","category":"cs.CL"}
{"created":"2024-04-23 13:45:29","title":"Optimization of GEM detectors for applications in X-ray fluorescence imaging","abstract":"In this work a set of simulations that aim at the optimization of gaseous detectors for applications in X-ray fluorescence imaging in the energy range of 3 -- 30keV is presented. By studying the statistical distribution of the radiation interactions with gases, the energy resolution limits after charge multiplication for 6keV X-ray photons in Ar/CO$_2$(70/30) and Kr/CO$_2$(90/10) were calculated, obtaining energy resolutions of 15.4(4)% and 14.6(2)% respectively. The detector design was also studied to reduce the presence of escape peaks and complement a model to evaluate the inevitable X-ray fluorescence of copper generated by the conductive materials inside the detector.","sentences":["In this work a set of simulations that aim at the optimization of gaseous detectors for applications in X-ray fluorescence imaging in the energy range of 3 -- 30keV is presented.","By studying the statistical distribution of the radiation interactions with gases, the energy resolution limits after charge multiplication for 6keV X-ray photons in Ar/CO$_2$(70/30) and Kr/CO$_2$(90/10) were calculated, obtaining energy resolutions of 15.4(4)% and 14.6(2)% respectively.","The detector design was also studied to reduce the presence of escape peaks and complement a model to evaluate the inevitable X-ray fluorescence of copper generated by the conductive materials inside the detector."],"url":"http://arxiv.org/abs/2404.15044v1","category":"physics.ins-det"}
{"created":"2024-04-23 13:45:03","title":"Mapping Parallel Matrix Multiplication in GotoBLAS2 to the AMD Versal ACAP for Deep Learning","abstract":"This paper investigates the design of parallel general matrix multiplication (GEMM) for a Versal Adaptive Compute Accelerated Platform (ACAP) equipped with a VC1902 system-on-chip and multiple Artificial Intelligence Engines (AIEs). Our efforts aim to port standard optimization techniques applied in the high-performance realization of GEMM on CPUs to the Versal ACAP. In particular, 1) we address the flexible exploitation of the Versal ACA multi-level memory hierarchy; 2) we delve into the efficient use of the vector units in the AIE tiles, proposing an architecture-specific micro-kernel for mixed precision arithmetic to address the strong demand for adaptive-precision inference in deep learning; and 3) we introduce a parallel design for GEMM that spans multiple AIE tiles, enhancing the computational throughput. We conduct experimental profiling, with up to 32 AI Engines, that demonstrates the high parallel scalability of the solution.","sentences":["This paper investigates the design of parallel general matrix multiplication (GEMM) for a Versal Adaptive Compute Accelerated Platform (ACAP) equipped with a VC1902 system-on-chip and multiple Artificial Intelligence Engines (AIEs).","Our efforts aim to port standard optimization techniques applied in the high-performance realization of GEMM on CPUs to the Versal ACAP.","In particular, 1) we address the flexible exploitation of the Versal ACA multi-level memory hierarchy; 2) we delve into the efficient use of the vector units in the AIE tiles, proposing an architecture-specific micro-kernel for mixed precision arithmetic to address the strong demand for adaptive-precision inference in deep learning; and 3) we introduce a parallel design for GEMM that spans multiple AIE tiles, enhancing the computational throughput.","We conduct experimental profiling, with up to 32 AI Engines, that demonstrates the high parallel scalability of the solution."],"url":"http://arxiv.org/abs/2404.15043v1","category":"cs.DC"}
{"created":"2024-04-23 13:43:56","title":"Leverage Variational Graph Representation For Model Poisoning on Federated Learning","abstract":"This paper puts forth a new training data-untethered model poisoning (MP) attack on federated learning (FL). The new MP attack extends an adversarial variational graph autoencoder (VGAE) to create malicious local models based solely on the benign local models overheard without any access to the training data of FL. Such an advancement leads to the VGAE-MP attack that is not only efficacious but also remains elusive to detection. VGAE-MP attack extracts graph structural correlations among the benign local models and the training data features, adversarially regenerates the graph structure, and generates malicious local models using the adversarial graph structure and benign models' features. Moreover, a new attacking algorithm is presented to train the malicious local models using VGAE and sub-gradient descent, while enabling an optimal selection of the benign local models for training the VGAE. Experiments demonstrate a gradual drop in FL accuracy under the proposed VGAE-MP attack and the ineffectiveness of existing defense mechanisms in detecting the attack, posing a severe threat to FL.","sentences":["This paper puts forth a new training data-untethered model poisoning (MP) attack on federated learning (FL).","The new MP attack extends an adversarial variational graph autoencoder (VGAE) to create malicious local models based solely on the benign local models overheard without any access to the training data of FL.","Such an advancement leads to the VGAE-MP attack that is not only efficacious but also remains elusive to detection.","VGAE-MP attack extracts graph structural correlations among the benign local models and the training data features, adversarially regenerates the graph structure, and generates malicious local models using the adversarial graph structure and benign models' features.","Moreover, a new attacking algorithm is presented to train the malicious local models using VGAE and sub-gradient descent, while enabling an optimal selection of the benign local models for training the VGAE.","Experiments demonstrate a gradual drop in FL accuracy under the proposed VGAE-MP attack and the ineffectiveness of existing defense mechanisms in detecting the attack, posing a severe threat to FL."],"url":"http://arxiv.org/abs/2404.15042v2","category":"cs.CR"}
{"created":"2024-04-23 13:43:33","title":"LEAF: Unveiling Two Sides of the Same Coin in Semi-supervised Facial Expression Recognition","abstract":"Semi-supervised learning has emerged as a promising approach to tackle the challenge of label scarcity in facial expression recognition (FER) task. However, current state-of-the-art methods primarily focus on one side of the coin, i.e., generating high-quality pseudo-labels, while overlooking the other side: enhancing expression-relevant representations. In this paper, we unveil both sides of the coin by proposing a unified framework termed hierarchicaL dEcoupling And Fusing (LEAF) to coordinate expression-relevant representations and pseudo-labels for semi-supervised FER. LEAF introduces a hierarchical expression-aware aggregation strategy that operates at three levels: semantic, instance, and category. (1) At the semantic and instance levels, LEAF decouples representations into expression-agnostic and expression-relevant components, and adaptively fuses them using learnable gating weights. (2) At the category level, LEAF assigns ambiguous pseudo-labels by decoupling predictions into positive and negative parts, and employs a consistency loss to ensure agreement between two augmented views of the same image. Extensive experiments on benchmark datasets demonstrate that by unveiling and harmonizing both sides of the coin, LEAF outperforms state-of-the-art semi-supervised FER methods, effectively leveraging both labeled and unlabeled data. Moreover, the proposed expression-aware aggregation strategy can be seamlessly integrated into existing semi-supervised frameworks, leading to significant performance gains.","sentences":["Semi-supervised learning has emerged as a promising approach to tackle the challenge of label scarcity in facial expression recognition (FER) task.","However, current state-of-the-art methods primarily focus on one side of the coin, i.e., generating high-quality pseudo-labels, while overlooking the other side: enhancing expression-relevant representations.","In this paper, we unveil both sides of the coin by proposing a unified framework termed hierarchicaL dEcoupling And Fusing (LEAF) to coordinate expression-relevant representations and pseudo-labels for semi-supervised FER.","LEAF introduces a hierarchical expression-aware aggregation strategy that operates at three levels: semantic, instance, and category.","(1) At the semantic and instance levels, LEAF decouples representations into expression-agnostic and expression-relevant components, and adaptively fuses them using learnable gating weights.","(2) At the category level, LEAF assigns ambiguous pseudo-labels by decoupling predictions into positive and negative parts, and employs a consistency loss to ensure agreement between two augmented views of the same image.","Extensive experiments on benchmark datasets demonstrate that by unveiling and harmonizing both sides of the coin, LEAF outperforms state-of-the-art semi-supervised FER methods, effectively leveraging both labeled and unlabeled data.","Moreover, the proposed expression-aware aggregation strategy can be seamlessly integrated into existing semi-supervised frameworks, leading to significant performance gains."],"url":"http://arxiv.org/abs/2404.15041v1","category":"cs.CV"}
{"created":"2024-04-23 13:42:53","title":"Scattering and Pairing by Exchange Interactions","abstract":"Quantum interactions exchanging different types of particles play a pivotal r\\^{o}le in quantum many-body theory, but they are not sufficiently investigated from a mathematical perspective. Here, we consider a system made of two fermions and one boson, in order to study the effect of such an off-diagonal interaction term, having in mind the physics of cuprate superconductors. Additionally, our model also includes a generalized Hubbard interaction (i.e., a general local repulsion term for the fermions). Regarding pairing, exponentially localized dressed bound fermion pairs are shown to exist and their effective dispersion relation is studied in detail. Scattering properties of the system are derived for two channels: the unbound and bound pair channels. We give particular attention to the regime of very large on-site (Hubbard) repulsions, because this situation is relevant for cuprate superconductors.","sentences":["Quantum interactions exchanging different types of particles play a pivotal r\\^{o}le in quantum many-body theory, but they are not sufficiently investigated from a mathematical perspective.","Here, we consider a system made of two fermions and one boson, in order to study the effect of such an off-diagonal interaction term, having in mind the physics of cuprate superconductors.","Additionally, our model also includes a generalized Hubbard interaction (i.e., a general local repulsion term for the fermions).","Regarding pairing, exponentially localized dressed bound fermion pairs are shown to exist and their effective dispersion relation is studied in detail.","Scattering properties of the system are derived for two channels: the unbound and bound pair channels.","We give particular attention to the regime of very large on-site (Hubbard) repulsions, because this situation is relevant for cuprate superconductors."],"url":"http://arxiv.org/abs/2404.15039v1","category":"math-ph"}
{"created":"2024-04-23 13:39:04","title":"Deep Multi-View Channel-Wise Spatio-Temporal Network for Traffic Flow Prediction","abstract":"Accurately forecasting traffic flows is critically important to many real applications including public safety and intelligent transportation systems. The challenges of this problem include both the dynamic mobility patterns of the people and the complex spatial-temporal correlations of the urban traffic data. Meanwhile, most existing models ignore the diverse impacts of the various traffic observations (e.g. vehicle speed and road occupancy) on the traffic flow prediction, and different traffic observations can be considered as different channels of input features. We argue that the analysis in multiple-channel traffic observations might help to better address this problem. In this paper, we study the novel problem of multi-channel traffic flow prediction, and propose a deep \\underline{M}ulti-\\underline{V}iew \\underline{C}hannel-wise \\underline{S}patio-\\underline{T}emporal \\underline{Net}work (MVC-STNet) model to effectively address it. Specifically, we first construct the localized and globalized spatial graph where the multi-view fusion module is used to effectively extract the local and global spatial dependencies. Then LSTM is used to learn the temporal correlations. To effectively model the different impacts of various traffic observations on traffic flow prediction, a channel-wise graph convolutional network is also designed. Extensive experiments are conducted over the PEMS04 and PEMS08 datasets. The results demonstrate that the proposed MVC-STNet outperforms state-of-the-art methods by a large margin.","sentences":["Accurately forecasting traffic flows is critically important to many real applications including public safety and intelligent transportation systems.","The challenges of this problem include both the dynamic mobility patterns of the people and the complex spatial-temporal correlations of the urban traffic data.","Meanwhile, most existing models ignore the diverse impacts of the various traffic observations (e.g. vehicle speed and road occupancy) on the traffic flow prediction, and different traffic observations can be considered as different channels of input features.","We argue that the analysis in multiple-channel traffic observations might help to better address this problem.","In this paper, we study the novel problem of multi-channel traffic flow prediction, and propose a deep \\underline{M}ulti-\\underline{V}iew \\underline{C}hannel-wise \\underline{S}patio-\\underline{T}emporal \\underline{Net}work (MVC-STNet) model to effectively address it.","Specifically, we first construct the localized and globalized spatial graph where the multi-view fusion module is used to effectively extract the local and global spatial dependencies.","Then LSTM is used to learn the temporal correlations.","To effectively model the different impacts of various traffic observations on traffic flow prediction, a channel-wise graph convolutional network is also designed.","Extensive experiments are conducted over the PEMS04 and PEMS08 datasets.","The results demonstrate that the proposed MVC-STNet outperforms state-of-the-art methods by a large margin."],"url":"http://arxiv.org/abs/2404.15034v1","category":"cs.LG"}
{"created":"2024-04-23 13:35:50","title":"Evaluation of Teleoperation Concepts to solve Automated Vehicle Disengagements","abstract":"Teleoperation is a popular solution to remotely support highly automated vehicles through a human remote operator whenever a disengagement of the automated driving system is present. The remote operator wirelessly connects to the vehicle and solves the disengagement through support or substitution of automated driving functions and therefore enables the vehicle to resume automation. There are different approaches to support automated driving functions on various levels, commonly known as teleoperation concepts. A variety of teleoperation concepts is described in the literature, yet there has been no comprehensive and structured comparison of these concepts, and it is not clear what subset of teleoperation concepts is suitable to enable safe and efficient remote support of highly automated vehicles in a broad spectrum of disengagements. The following work establishes a basis for comparing teleoperation concepts through a literature overview on automated vehicle disengagements and on already conducted studies on the comparison of teleoperation concepts and metrics used to evaluate teleoperation performance. An evaluation of the teleoperation concepts is carried out in an expert workshop, comparing different teleoperation concepts using a selection of automated vehicle disengagement scenarios and metrics. Based on the workshop results, a set of teleoperation concepts is derived that can be used to address a wide variety of automated vehicle disengagements in a safe and efficient way.","sentences":["Teleoperation is a popular solution to remotely support highly automated vehicles through a human remote operator whenever a disengagement of the automated driving system is present.","The remote operator wirelessly connects to the vehicle and solves the disengagement through support or substitution of automated driving functions and therefore enables the vehicle to resume automation.","There are different approaches to support automated driving functions on various levels, commonly known as teleoperation concepts.","A variety of teleoperation concepts is described in the literature, yet there has been no comprehensive and structured comparison of these concepts, and it is not clear what subset of teleoperation concepts is suitable to enable safe and efficient remote support of highly automated vehicles in a broad spectrum of disengagements.","The following work establishes a basis for comparing teleoperation concepts through a literature overview on automated vehicle disengagements and on already conducted studies on the comparison of teleoperation concepts and metrics used to evaluate teleoperation performance.","An evaluation of the teleoperation concepts is carried out in an expert workshop, comparing different teleoperation concepts using a selection of automated vehicle disengagement scenarios and metrics.","Based on the workshop results, a set of teleoperation concepts is derived that can be used to address a wide variety of automated vehicle disengagements in a safe and efficient way."],"url":"http://arxiv.org/abs/2404.15030v1","category":"cs.RO"}
{"created":"2024-04-23 13:35:22","title":"Explainable LightGBM Approach for Predicting Myocardial Infarction Mortality","abstract":"Myocardial Infarction is a main cause of mortality globally, and accurate risk prediction is crucial for improving patient outcomes. Machine Learning techniques have shown promise in identifying high-risk patients and predicting outcomes. However, patient data often contain vast amounts of information and missing values, posing challenges for feature selection and imputation methods. In this article, we investigate the impact of the data preprocessing task and compare three ensembles boosted tree methods to predict the risk of mortality in patients with myocardial infarction. Further, we use the Tree Shapley Additive Explanations method to identify relationships among all the features for the performed predictions, leveraging the entirety of the available data in the analysis. Notably, our approach achieved a superior performance when compared to other existing machine learning approaches, with an F1-score of 91,2% and an accuracy of 91,8% for LightGBM without data preprocessing.","sentences":["Myocardial Infarction is a main cause of mortality globally, and accurate risk prediction is crucial for improving patient outcomes.","Machine Learning techniques have shown promise in identifying high-risk patients and predicting outcomes.","However, patient data often contain vast amounts of information and missing values, posing challenges for feature selection and imputation methods.","In this article, we investigate the impact of the data preprocessing task and compare three ensembles boosted tree methods to predict the risk of mortality in patients with myocardial infarction.","Further, we use the Tree Shapley Additive Explanations method to identify relationships among all the features for the performed predictions, leveraging the entirety of the available data in the analysis.","Notably, our approach achieved a superior performance when compared to other existing machine learning approaches, with an F1-score of 91,2% and an accuracy of 91,8% for LightGBM without data preprocessing."],"url":"http://arxiv.org/abs/2404.15029v1","category":"cs.LG"}
{"created":"2024-04-23 13:34:52","title":"PRISM: A Promptable and Robust Interactive Segmentation Model with Visual Prompts","abstract":"In this paper, we present PRISM, a Promptable and Robust Interactive Segmentation Model, aiming for precise segmentation of 3D medical images. PRISM accepts various visual inputs, including points, boxes, and scribbles as sparse prompts, as well as masks as dense prompts. Specifically, PRISM is designed with four principles to achieve robustness: (1) Iterative learning. The model produces segmentations by using visual prompts from previous iterations to achieve progressive improvement. (2) Confidence learning. PRISM employs multiple segmentation heads per input image, each generating a continuous map and a confidence score to optimize predictions. (3) Corrective learning. Following each segmentation iteration, PRISM employs a shallow corrective refinement network to reassign mislabeled voxels. (4) Hybrid design. PRISM integrates hybrid encoders to better capture both the local and global information. Comprehensive validation of PRISM is conducted using four public datasets for tumor segmentation in the colon, pancreas, liver, and kidney, highlighting challenges caused by anatomical variations and ambiguous boundaries in accurate tumor identification. Compared to state-of-the-art methods, both with and without prompt engineering, PRISM significantly improves performance, achieving results that are close to human levels. The code is publicly available at https://github.com/MedICL-VU/PRISM.","sentences":["In this paper, we present PRISM, a Promptable and Robust Interactive Segmentation Model, aiming for precise segmentation of 3D medical images.","PRISM accepts various visual inputs, including points, boxes, and scribbles as sparse prompts, as well as masks as dense prompts.","Specifically, PRISM is designed with four principles to achieve robustness: (1) Iterative learning.","The model produces segmentations by using visual prompts from previous iterations to achieve progressive improvement.","(2) Confidence learning.","PRISM employs multiple segmentation heads per input image, each generating a continuous map and a confidence score to optimize predictions.","(3) Corrective learning.","Following each segmentation iteration, PRISM employs a shallow corrective refinement network to reassign mislabeled voxels.","(4) Hybrid design.","PRISM integrates hybrid encoders to better capture both the local and global information.","Comprehensive validation of PRISM is conducted using four public datasets for tumor segmentation in the colon, pancreas, liver, and kidney, highlighting challenges caused by anatomical variations and ambiguous boundaries in accurate tumor identification.","Compared to state-of-the-art methods, both with and without prompt engineering, PRISM significantly improves performance, achieving results that are close to human levels.","The code is publicly available at https://github.com/MedICL-VU/PRISM."],"url":"http://arxiv.org/abs/2404.15028v1","category":"cs.CV"}
{"created":"2024-04-23 13:33:53","title":"Three dimensional end-to-end simulation for kilonova emission from a black-hole neutron-star merger","abstract":"We study long-term evolution of the matter ejected in a black-hole neutron-star (BH-NS) merger employing the results of a long-term numerical-relativity simulation and nucleosynthesis calculation, in which both dynamical and post-merger ejecta formation are consistently followed. In particular, we employ the results for the merger of a $1.35\\,M_\\odot$ NS and a $5.4\\,M_\\odot$ BH with the dimensionless spin of 0.75. We confirm the finding in the previous studies that thermal pressure induced by radioactive heating in the ejecta significantly modifies the morphology of the ejecta. We then compute the kilonova (KN) light curves employing the ejecta profile obtained by the long-term evolution. We find that our present BH-NS model results in a KN light curve that is fainter yet more enduring than that observed in AT2017gfo. This is due to the fact that the emission is primarily powered by the lanthanide-rich dynamical ejecta, in which a long photon diffusion time scale is realized by the large mass and high opacity. While the peak brightness of the KN emission in both the optical and near-infrared bands is fainter than or comparable to those of binary NS models, the time-scale maintaining the peak brightness is much longer in the near-infrared band for the BH-NS KN model. Our result indicates that a BH-NS merger with massive ejecta can observationally be identified by the bright and long lasting ($>$two weeks) near-infrared emission.","sentences":["We study long-term evolution of the matter ejected in a black-hole neutron-star (BH-NS) merger employing the results of a long-term numerical-relativity simulation and nucleosynthesis calculation, in which both dynamical and post-merger ejecta formation are consistently followed.","In particular, we employ the results for the merger of a $1.35\\,M_\\odot$ NS and a $5.4\\,M_\\odot$ BH with the dimensionless spin of 0.75.","We confirm the finding in the previous studies that thermal pressure induced by radioactive heating in the ejecta significantly modifies the morphology of the ejecta.","We then compute the kilonova (KN) light curves employing the ejecta profile obtained by the long-term evolution.","We find that our present BH-NS model results in a KN light curve that is fainter yet more enduring than that observed in AT2017gfo.","This is due to the fact that the emission is primarily powered by the lanthanide-rich dynamical ejecta, in which a long photon diffusion time scale is realized by the large mass and high opacity.","While the peak brightness of the KN emission in both the optical and near-infrared bands is fainter than or comparable to those of binary NS models, the time-scale maintaining the peak brightness is much longer in the near-infrared band for the BH-NS KN model.","Our result indicates that a BH-NS merger with massive ejecta can observationally be identified by the bright and long lasting ($>$two weeks) near-infrared emission."],"url":"http://arxiv.org/abs/2404.15027v1","category":"astro-ph.HE"}
{"created":"2024-04-23 13:31:18","title":"A review of deep learning-based information fusion techniques for multimodal medical image classification","abstract":"Multimodal medical imaging plays a pivotal role in clinical diagnosis and research, as it combines information from various imaging modalities to provide a more comprehensive understanding of the underlying pathology. Recently, deep learning-based multimodal fusion techniques have emerged as powerful tools for improving medical image classification. This review offers a thorough analysis of the developments in deep learning-based multimodal fusion for medical classification tasks. We explore the complementary relationships among prevalent clinical modalities and outline three main fusion schemes for multimodal classification networks: input fusion, intermediate fusion (encompassing single-level fusion, hierarchical fusion, and attention-based fusion), and output fusion. By evaluating the performance of these fusion techniques, we provide insight into the suitability of different network architectures for various multimodal fusion scenarios and application domains. Furthermore, we delve into challenges related to network architecture selection, handling incomplete multimodal data management, and the potential limitations of multimodal fusion. Finally, we spotlight the promising future of Transformer-based multimodal fusion techniques and give recommendations for future research in this rapidly evolving field.","sentences":["Multimodal medical imaging plays a pivotal role in clinical diagnosis and research, as it combines information from various imaging modalities to provide a more comprehensive understanding of the underlying pathology.","Recently, deep learning-based multimodal fusion techniques have emerged as powerful tools for improving medical image classification.","This review offers a thorough analysis of the developments in deep learning-based multimodal fusion for medical classification tasks.","We explore the complementary relationships among prevalent clinical modalities and outline three main fusion schemes for multimodal classification networks: input fusion, intermediate fusion (encompassing single-level fusion, hierarchical fusion, and attention-based fusion), and output fusion.","By evaluating the performance of these fusion techniques, we provide insight into the suitability of different network architectures for various multimodal fusion scenarios and application domains.","Furthermore, we delve into challenges related to network architecture selection, handling incomplete multimodal data management, and the potential limitations of multimodal fusion.","Finally, we spotlight the promising future of Transformer-based multimodal fusion techniques and give recommendations for future research in this rapidly evolving field."],"url":"http://arxiv.org/abs/2404.15022v1","category":"cs.CV"}
{"created":"2024-04-23 13:24:27","title":"A unified analytical prediction for steady-state behavior of confined drop with interface viscosity under shear flow","abstract":"In this Letter, we fill in the blanks in the theory of drops under shear flow by unifying analytical predictions for steady-state behavior proposed by Flumerfelt [R. W. Flumerfelt, J. Colloid Interface Sci. 76, 330 (1980)] for unconfined drops with interface viscosity with the one of Shapira & Haber [M. Shapira and S. Haber, Int. J. Multiph. Flow. 16, 305 (1990)] for confined drops without interface viscosity. Our predictions for both steady-state drop deformation and inclination angle are broadly valid for situations involving confined/unconfined drops, with/without interface viscosity and viscosity ratio, thus making our model so general that it can include any of the above conditions.","sentences":["In this Letter, we fill in the blanks in the theory of drops under shear flow by unifying analytical predictions for steady-state behavior proposed by Flumerfelt [R. W. Flumerfelt, J. Colloid Interface Sci. 76, 330 (1980)] for unconfined drops with interface viscosity with the one of Shapira & Haber","[M. Shapira and S. Haber, Int.","J. Multiph.","Flow.","16, 305 (1990)] for confined drops without interface viscosity.","Our predictions for both steady-state drop deformation and inclination angle are broadly valid for situations involving confined/unconfined drops, with/without interface viscosity and viscosity ratio, thus making our model so general that it can include any of the above conditions."],"url":"http://arxiv.org/abs/2404.15019v2","category":"physics.flu-dyn"}
{"created":"2024-04-23 13:23:11","title":"The mosaic permutation test: an exact and nonparametric goodness-of-fit test for factor models","abstract":"Financial firms often rely on factor models to explain correlations among asset returns. These models are important for managing risk, for example by modeling the probability that many assets will simultaneously lose value. Yet after major events, e.g., COVID-19, analysts may reassess whether existing models continue to fit well: specifically, after accounting for the factor exposures, are the residuals of the asset returns independent? With this motivation, we introduce the mosaic permutation test, a nonparametric goodness-of-fit test for preexisting factor models. Our method allows analysts to use nearly any machine learning technique to detect model violations while provably controlling the false positive rate, i.e., the probability of rejecting a well-fitting model. Notably, this result does not rely on asymptotic approximations and makes no parametric assumptions. This property helps prevent analysts from unnecessarily rebuilding accurate models, which can waste resources and increase risk. We illustrate our methodology by applying it to the Blackrock Fundamental Equity Risk (BFRE) model. Using the mosaic permutation test, we find that the BFRE model generally explains the most significant correlations among assets. However, we find evidence of unexplained correlations among certain real estate stocks, and we show that adding new factors improves model fit. We implement our methods in the python package mosaicperm.","sentences":["Financial firms often rely on factor models to explain correlations among asset returns.","These models are important for managing risk, for example by modeling the probability that many assets will simultaneously lose value.","Yet after major events, e.g., COVID-19, analysts may reassess whether existing models continue to fit well: specifically, after accounting for the factor exposures, are the residuals of the asset returns independent?","With this motivation, we introduce the mosaic permutation test, a nonparametric goodness-of-fit test for preexisting factor models.","Our method allows analysts to use nearly any machine learning technique to detect model violations while provably controlling the false positive rate, i.e., the probability of rejecting a well-fitting model.","Notably, this result does not rely on asymptotic approximations and makes no parametric assumptions.","This property helps prevent analysts from unnecessarily rebuilding accurate models, which can waste resources and increase risk.","We illustrate our methodology by applying it to the Blackrock Fundamental Equity Risk (BFRE) model.","Using the mosaic permutation test, we find that the BFRE model generally explains the most significant correlations among assets.","However, we find evidence of unexplained correlations among certain real estate stocks, and we show that adding new factors improves model fit.","We implement our methods in the python package mosaicperm."],"url":"http://arxiv.org/abs/2404.15017v1","category":"stat.ME"}
{"created":"2024-04-23 13:20:09","title":"OccGen: Generative Multi-modal 3D Occupancy Prediction for Autonomous Driving","abstract":"Existing solutions for 3D semantic occupancy prediction typically treat the task as a one-shot 3D voxel-wise segmentation perception problem. These discriminative methods focus on learning the mapping between the inputs and occupancy map in a single step, lacking the ability to gradually refine the occupancy map and the reasonable scene imaginative capacity to complete the local regions somewhere. In this paper, we introduce OccGen, a simple yet powerful generative perception model for the task of 3D semantic occupancy prediction. OccGen adopts a ''noise-to-occupancy'' generative paradigm, progressively inferring and refining the occupancy map by predicting and eliminating noise originating from a random Gaussian distribution. OccGen consists of two main components: a conditional encoder that is capable of processing multi-modal inputs, and a progressive refinement decoder that applies diffusion denoising using the multi-modal features as conditions. A key insight of this generative pipeline is that the diffusion denoising process is naturally able to model the coarse-to-fine refinement of the dense 3D occupancy map, therefore producing more detailed predictions. Extensive experiments on several occupancy benchmarks demonstrate the effectiveness of the proposed method compared to the state-of-the-art methods. For instance, OccGen relatively enhances the mIoU by 9.5%, 6.3%, and 13.3% on nuScenes-Occupancy dataset under the muli-modal, LiDAR-only, and camera-only settings, respectively. Moreover, as a generative perception model, OccGen exhibits desirable properties that discriminative models cannot achieve, such as providing uncertainty estimates alongside its multiple-step predictions.","sentences":["Existing solutions for 3D semantic occupancy prediction typically treat the task as a one-shot 3D voxel-wise segmentation perception problem.","These discriminative methods focus on learning the mapping between the inputs and occupancy map in a single step, lacking the ability to gradually refine the occupancy map and the reasonable scene imaginative capacity to complete the local regions somewhere.","In this paper, we introduce OccGen, a simple yet powerful generative perception model for the task of 3D semantic occupancy prediction.","OccGen adopts a ''noise-to-occupancy'' generative paradigm, progressively inferring and refining the occupancy map by predicting and eliminating noise originating from a random Gaussian distribution.","OccGen consists of two main components: a conditional encoder that is capable of processing multi-modal inputs, and a progressive refinement decoder that applies diffusion denoising using the multi-modal features as conditions.","A key insight of this generative pipeline is that the diffusion denoising process is naturally able to model the coarse-to-fine refinement of the dense 3D occupancy map, therefore producing more detailed predictions.","Extensive experiments on several occupancy benchmarks demonstrate the effectiveness of the proposed method compared to the state-of-the-art methods.","For instance, OccGen relatively enhances the mIoU by 9.5%, 6.3%, and 13.3% on nuScenes-Occupancy dataset under the muli-modal, LiDAR-only, and camera-only settings, respectively.","Moreover, as a generative perception model, OccGen exhibits desirable properties that discriminative models cannot achieve, such as providing uncertainty estimates alongside its multiple-step predictions."],"url":"http://arxiv.org/abs/2404.15014v1","category":"cs.CV"}
{"created":"2024-04-23 13:15:35","title":"X-3D: Explicit 3D Structure Modeling for Point Cloud Recognition","abstract":"Numerous prior studies predominantly emphasize constructing relation vectors for individual neighborhood points and generating dynamic kernels for each vector and embedding these into high-dimensional spaces to capture implicit local structures. However, we contend that such implicit high-dimensional structure modeling approch inadequately represents the local geometric structure of point clouds due to the absence of explicit structural information. Hence, we introduce X-3D, an explicit 3D structure modeling approach. X-3D functions by capturing the explicit local structural information within the input 3D space and employing it to produce dynamic kernels with shared weights for all neighborhood points within the current local region. This modeling approach introduces effective geometric prior and significantly diminishes the disparity between the local structure of the embedding space and the original input point cloud, thereby improving the extraction of local features. Experiments show that our method can be used on a variety of methods and achieves state-of-the-art performance on segmentation, classification, detection tasks with lower extra computational cost, such as \\textbf{90.7\\%} on ScanObjectNN for classification, \\textbf{79.2\\%} on S3DIS 6 fold and \\textbf{74.3\\%} on S3DIS Area 5 for segmentation, \\textbf{76.3\\%} on ScanNetV2 for segmentation and \\textbf{64.5\\%} mAP , \\textbf{46.9\\%} mAP on SUN RGB-D and \\textbf{69.0\\%} mAP , \\textbf{51.1\\%} mAP on ScanNetV2 . Our code is available at \\href{https://github.com/sunshuofeng/X-3D}{https://github.com/sunshuofeng/X-3D}.","sentences":["Numerous prior studies predominantly emphasize constructing relation vectors for individual neighborhood points and generating dynamic kernels for each vector and embedding these into high-dimensional spaces to capture implicit local structures.","However, we contend that such implicit high-dimensional structure modeling approch inadequately represents the local geometric structure of point clouds due to the absence of explicit structural information.","Hence, we introduce X-3D, an explicit 3D structure modeling approach.","X-3D functions by capturing the explicit local structural information within the input 3D space and employing it to produce dynamic kernels with shared weights for all neighborhood points within the current local region.","This modeling approach introduces effective geometric prior and significantly diminishes the disparity between the local structure of the embedding space and the original input point cloud, thereby improving the extraction of local features.","Experiments show that our method can be used on a variety of methods and achieves state-of-the-art performance on segmentation, classification, detection tasks with lower extra computational cost, such as \\textbf{90.7\\%} on ScanObjectNN for classification, \\textbf{79.2\\%} on S3DIS 6 fold and \\textbf{74.3\\%} on S3DIS Area 5 for segmentation, \\textbf{76.3\\%} on ScanNetV2 for segmentation and \\textbf{64.5\\%} mAP , \\textbf{46.9\\%} mAP on SUN RGB-D and \\textbf{69.0\\%} mAP , \\textbf{51.1\\%} mAP on ScanNetV2 .","Our code is available at \\href{https://github.com/sunshuofeng/X-3D}{https://github.com/sunshuofeng/X-3D}."],"url":"http://arxiv.org/abs/2404.15010v1","category":"cs.CV"}
{"created":"2024-04-23 13:12:11","title":"Single-Spin Waved-Brim Flat-Top Hat in the Band Edge of GdIH Monolayer","abstract":"Exotic electronic bands, such as flat bands, linear crossing bands, spontaneously valley- or spin-polarized bands, in two-dimensional materials have been the hot topics in condensed matter physics. Herein, we first propose a general dispersion model for possible hat-like electronic bands, and then identify an intriguing single-spin \\emph{waved-brim flat-top hat} in the valence band edge of a stable ferromagnetic semiconducting electrene (i.e., Janus GdIH monolayer), which can be well described by a simplified two-bands Hamiltonian model. Specifically, the hat-band has a waved brim with six valleys along the boundary of the first Brillouin zone; meanwhile it holds a flat top close to the Fermi level, resulting in the emergence of single-spin van Hove singularities divergence and Lifshitz transitions. Owing to the breaking of both time-reversal and space inversion symmetries, a sizable spontaneous valley polarization is formed between the adjacent brim valleys, which provides the opportunity to realize the high-temperature anomalous valley Hall effect. Particularly, via modest strains and carriers doping, various conductive bipolar-states (spin-up vs. spin-down, K valley vs. $-$K valley, and ultra-low-speed vs. ultra-high-speed) can be modulated out from the distorted waved-brim flat-top hat of GdIH ML.","sentences":["Exotic electronic bands, such as flat bands, linear crossing bands, spontaneously valley- or spin-polarized bands, in two-dimensional materials have been the hot topics in condensed matter physics.","Herein, we first propose a general dispersion model for possible hat-like electronic bands, and then identify an intriguing single-spin \\emph{waved-brim flat-top hat} in the valence band edge of a stable ferromagnetic semiconducting electrene (i.e., Janus GdIH monolayer), which can be well described by a simplified two-bands Hamiltonian model.","Specifically, the hat-band has a waved brim with six valleys along the boundary of the first Brillouin zone; meanwhile it holds a flat top close to the Fermi level, resulting in the emergence of single-spin van Hove singularities divergence and Lifshitz transitions.","Owing to the breaking of both time-reversal and space inversion symmetries, a sizable spontaneous valley polarization is formed between the adjacent brim valleys, which provides the opportunity to realize the high-temperature anomalous valley Hall effect.","Particularly, via modest strains and carriers doping, various conductive bipolar-states (spin-up vs. spin-down, K valley vs. $-$K valley, and ultra-low-speed vs. ultra-high-speed) can be modulated out from the distorted waved-brim flat-top hat of GdIH ML."],"url":"http://arxiv.org/abs/2404.15007v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-23 13:06:32","title":"Comparison of Current Approaches to Lemmatization: A Case Study in Estonian","abstract":"This study evaluates three different lemmatization approaches to Estonian -- Generative character-level models, Pattern-based word-level classification models, and rule-based morphological analysis. According to our experiments, a significantly smaller Generative model consistently outperforms the Pattern-based classification model based on EstBERT. Additionally, we observe a relatively small overlap in errors made by all three models, indicating that an ensemble of different approaches could lead to improvements.","sentences":["This study evaluates three different lemmatization approaches to Estonian -- Generative character-level models, Pattern-based word-level classification models, and rule-based morphological analysis.","According to our experiments, a significantly smaller Generative model consistently outperforms the Pattern-based classification model based on EstBERT.","Additionally, we observe a relatively small overlap in errors made by all three models, indicating that an ensemble of different approaches could lead to improvements."],"url":"http://arxiv.org/abs/2404.15003v1","category":"cs.CL"}
{"created":"2024-04-23 13:03:58","title":"Unknown Object Grasping for Assistive Robotics","abstract":"We propose a novel pipeline for unknown object grasping in shared robotic autonomy scenarios. State-of-the-art methods for fully autonomous scenarios are typically learning-based approaches optimised for a specific end-effector, that generate grasp poses directly from sensor input. In the domain of assistive robotics, we seek instead to utilise the user's cognitive abilities for enhanced satisfaction, grasping performance, and alignment with their high level task-specific goals. Given a pair of stereo images, we perform unknown object instance segmentation and generate a 3D reconstruction of the object of interest. In shared control, the user then guides the robot end-effector across a virtual hemisphere centered around the object to their desired approach direction. A physics-based grasp planner finds the most stable local grasp on the reconstruction, and finally the user is guided by shared control to this grasp. In experiments on the DLR EDAN platform, we report a grasp success rate of 87% for 10 unknown objects, and demonstrate the method's capability to grasp objects in structured clutter and from shelves.","sentences":["We propose a novel pipeline for unknown object grasping in shared robotic autonomy scenarios.","State-of-the-art methods for fully autonomous scenarios are typically learning-based approaches optimised for a specific end-effector, that generate grasp poses directly from sensor input.","In the domain of assistive robotics, we seek instead to utilise the user's cognitive abilities for enhanced satisfaction, grasping performance, and alignment with their high level task-specific goals.","Given a pair of stereo images, we perform unknown object instance segmentation and generate a 3D reconstruction of the object of interest.","In shared control, the user then guides the robot end-effector across a virtual hemisphere centered around the object to their desired approach direction.","A physics-based grasp planner finds the most stable local grasp on the reconstruction, and finally the user is guided by shared control to this grasp.","In experiments on the DLR EDAN platform, we report a grasp success rate of 87% for 10 unknown objects, and demonstrate the method's capability to grasp objects in structured clutter and from shelves."],"url":"http://arxiv.org/abs/2404.15001v1","category":"cs.RO"}
{"created":"2024-04-23 17:11:39","title":"Runtime-coherence trade-offs for hybrid SAT-solvers","abstract":"Many search-based quantum algorithms that achieve a theoretical speedup are not practically relevant since they require extraordinarily long coherence times, or lack the parallelizability of their classical counterparts.This raises the question of how to divide computational tasks into a collection of parallelizable sub-problems, each of which can be solved by a quantum computer with limited coherence time. Here, we approach this question via hybrid algorithms for the k-SAT problem. Our analysis is based on Sch\\\"oning's algorithm, which solves instances of k-SAT by performing random walks in the space of potential assignments. The search space of the walk allows for \"natural\" partitions, where we subject only one part of the partition to a Grover search, while the rest is sampled classically, thus resulting in a hybrid scheme. In this setting, we argue that there exists a simple trade-off relation between the total runtime and the coherence-time, which no such partition based hybrid-scheme can surpass. For several concrete choices of partitions, we explicitly determine the specific runtime coherence-time relations, and show saturation of the ideal trade-off. Finally, we present numerical simulations which suggest additional flexibility in implementing hybrid algorithms with optimal trade-off.","sentences":["Many search-based quantum algorithms that achieve a theoretical speedup are not practically relevant since they require extraordinarily long coherence times, or lack the parallelizability of their classical counterparts.","This raises the question of how to divide computational tasks into a collection of parallelizable sub-problems, each of which can be solved by a quantum computer with limited coherence time.","Here, we approach this question via hybrid algorithms for the k-SAT problem.","Our analysis is based on Sch\\\"oning's algorithm, which solves instances of k-SAT by performing random walks in the space of potential assignments.","The search space of the walk allows for \"natural\" partitions, where we subject only one part of the partition to a Grover search, while the rest is sampled classically, thus resulting in a hybrid scheme.","In this setting, we argue that there exists a simple trade-off relation between the total runtime and the coherence-time, which no such partition based hybrid-scheme can surpass.","For several concrete choices of partitions, we explicitly determine the specific runtime coherence-time relations, and show saturation of the ideal trade-off.","Finally, we present numerical simulations which suggest additional flexibility in implementing hybrid algorithms with optimal trade-off."],"url":"http://arxiv.org/abs/2404.15235v1","category":"quant-ph"}
{"created":"2024-04-23 15:53:00","title":"Semantic distance organizes social knowledge: Insights from semantic dementia and cross-modal conceptual space","abstract":"Our interaction with others largely hinges on how we semantically organize the social world. The organization of such conceptual information is not static -- as we age, our experiences and ever-changing anatomy alter how we represent and arrange semantic information. How does semantic distance between concepts affect this organization, particularly for those with pathological deficits in semantic knowledge? Using triplet judgment responses collected from healthy participants, we compute an ordinal similarity embedding for a set of social words and images that vary in the dimensions of age and gender. We compare semantic distances between items in the space to patterns of error in a word-picture matching task performed by patients with semantic dementia (SD). Error patterns reveal that SD patients retain gender information more robustly than age information, and that age-related errors are a function of linear distance in age from a concept word. The distances between probed and exemplar items in the resulting conceptual map reflect error patterns in SD patient responses such that items semantically closer to a probed concept -- in gender category or in linear age -- are more likely to be erroneously chosen by patients in a word-picture matching task. To our knowledge, this is the first triplet embedding work to embed representations of words and images in a unified space, and to use this space to explain patterns of behavior in patients with impaired social semantic cognition.","sentences":["Our interaction with others largely hinges on how we semantically organize the social world.","The organization of such conceptual information is not static -- as we age, our experiences and ever-changing anatomy alter how we represent and arrange semantic information.","How does semantic distance between concepts affect this organization, particularly for those with pathological deficits in semantic knowledge?","Using triplet judgment responses collected from healthy participants, we compute an ordinal similarity embedding for a set of social words and images that vary in the dimensions of age and gender.","We compare semantic distances between items in the space to patterns of error in a word-picture matching task performed by patients with semantic dementia (SD).","Error patterns reveal that SD patients retain gender information more robustly than age information, and that age-related errors are a function of linear distance in age from a concept word.","The distances between probed and exemplar items in the resulting conceptual map reflect error patterns in SD patient responses such that items semantically closer to a probed concept -- in gender category or in linear age -- are more likely to be erroneously chosen by patients in a word-picture matching task.","To our knowledge, this is the first triplet embedding work to embed representations of words and images in a unified space, and to use this space to explain patterns of behavior in patients with impaired social semantic cognition."],"url":"http://arxiv.org/abs/2404.15151v1","category":"q-bio.NC"}
{"created":"2024-04-23 12:51:37","title":"Transformers Can Represent $n$-gram Language Models","abstract":"Plenty of existing work has analyzed the abilities of the transformer architecture by describing its representational capacity with formal models of computation. However, the focus so far has been on analyzing the architecture in terms of language \\emph{acceptance}. We contend that this is an ill-suited problem in the study of \\emph{language models} (LMs), which are definitionally \\emph{probability distributions} over strings. In this paper, we focus on the relationship between transformer LMs and $n$-gram LMs, a simple and historically relevant class of language models. We show that transformer LMs using the hard or sparse attention mechanisms can exactly represent any $n$-gram LM, giving us a concrete lower bound on their probabilistic representational capacity. This provides a first step towards understanding the mechanisms that transformer LMs can use to represent probability distributions over strings.","sentences":["Plenty of existing work has analyzed the abilities of the transformer architecture by describing its representational capacity with formal models of computation.","However, the focus so far has been on analyzing the architecture in terms of language \\emph{acceptance}.","We contend that this is an ill-suited problem in the study of \\emph{language models} (LMs), which are definitionally \\emph{probability distributions} over strings.","In this paper, we focus on the relationship between transformer LMs and $n$-gram LMs, a simple and historically relevant class of language models.","We show that transformer LMs using the hard or sparse attention mechanisms can exactly represent any $n$-gram LM, giving us a concrete lower bound on their probabilistic representational capacity.","This provides a first step towards understanding the mechanisms that transformer LMs can use to represent probability distributions over strings."],"url":"http://arxiv.org/abs/2404.14994v1","category":"cs.CL"}
{"created":"2024-04-23 12:43:15","title":"$\\texttt{MiniMol}$: A Parameter-Efficient Foundation Model for Molecular Learning","abstract":"In biological tasks, data is rarely plentiful as it is generated from hard-to-gather measurements. Therefore, pre-training foundation models on large quantities of available data and then transfer to low-data downstream tasks is a promising direction. However, how to design effective foundation models for molecular learning remains an open question, with existing approaches typically focusing on models with large parameter capacities. In this work, we propose $\\texttt{MiniMol}$, a foundational model for molecular learning with 10 million parameters. $\\texttt{MiniMol}$ is pre-trained on a mix of roughly 3300 sparsely defined graph- and node-level tasks of both quantum and biological nature. The pre-training dataset includes approximately 6 million molecules and 500 million labels. To demonstrate the generalizability of $\\texttt{MiniMol}$ across tasks, we evaluate it on downstream tasks from the Therapeutic Data Commons (TDC) ADMET group showing significant improvements over the prior state-of-the-art foundation model across 17 tasks. $\\texttt{MiniMol}$ will be a public and open-sourced model for future research.","sentences":["In biological tasks, data is rarely plentiful as it is generated from hard-to-gather measurements.","Therefore, pre-training foundation models on large quantities of available data and then transfer to low-data downstream tasks is a promising direction.","However, how to design effective foundation models for molecular learning remains an open question, with existing approaches typically focusing on models with large parameter capacities.","In this work, we propose $\\texttt{MiniMol}$, a foundational model for molecular learning with 10 million parameters.","$\\texttt{MiniMol}$ is pre-trained on a mix of roughly 3300 sparsely defined graph- and node-level tasks of both quantum and biological nature.","The pre-training dataset includes approximately 6 million molecules and 500 million labels.","To demonstrate the generalizability of $\\texttt{MiniMol}$ across tasks, we evaluate it on downstream tasks from the Therapeutic Data Commons (TDC) ADMET group showing significant improvements over the prior state-of-the-art foundation model across 17 tasks.","$\\texttt{MiniMol}$ will be a public and open-sourced model for future research."],"url":"http://arxiv.org/abs/2404.14986v1","category":"cs.LG"}
{"created":"2024-04-23 12:36:24","title":"SGFormer: Spherical Geometry Transformer for 360 Depth Estimation","abstract":"Panoramic distortion poses a significant challenge in 360 depth estimation, particularly pronounced at the north and south poles. Existing methods either adopt a bi-projection fusion strategy to remove distortions or model long-range dependencies to capture global structures, which can result in either unclear structure or insufficient local perception. In this paper, we propose a spherical geometry transformer, named SGFormer, to address the above issues, with an innovative step to integrate spherical geometric priors into vision transformers. To this end, we retarget the transformer decoder to a spherical prior decoder (termed SPDecoder), which endeavors to uphold the integrity of spherical structures during decoding. Concretely, we leverage bipolar re-projection, circular rotation, and curve local embedding to preserve the spherical characteristics of equidistortion, continuity, and surface distance, respectively. Furthermore, we present a query-based global conditional position embedding to compensate for spatial structure at varying resolutions. It not only boosts the global perception of spatial position but also sharpens the depth structure across different patches. Finally, we conduct extensive experiments on popular benchmarks, demonstrating our superiority over state-of-the-art solutions.","sentences":["Panoramic distortion poses a significant challenge in 360 depth estimation, particularly pronounced at the north and south poles.","Existing methods either adopt a bi-projection fusion strategy to remove distortions or model long-range dependencies to capture global structures, which can result in either unclear structure or insufficient local perception.","In this paper, we propose a spherical geometry transformer, named SGFormer, to address the above issues, with an innovative step to integrate spherical geometric priors into vision transformers.","To this end, we retarget the transformer decoder to a spherical prior decoder (termed SPDecoder), which endeavors to uphold the integrity of spherical structures during decoding.","Concretely, we leverage bipolar re-projection, circular rotation, and curve local embedding to preserve the spherical characteristics of equidistortion, continuity, and surface distance, respectively.","Furthermore, we present a query-based global conditional position embedding to compensate for spatial structure at varying resolutions.","It not only boosts the global perception of spatial position but also sharpens the depth structure across different patches.","Finally, we conduct extensive experiments on popular benchmarks, demonstrating our superiority over state-of-the-art solutions."],"url":"http://arxiv.org/abs/2404.14979v1","category":"cs.CV"}
{"created":"2024-04-23 12:33:14","title":"Social Media and Artificial Intelligence for Sustainable Cities and Societies: A Water Quality Analysis Use-case","abstract":"This paper focuses on a very important societal challenge of water quality analysis. Being one of the key factors in the economic and social development of society, the provision of water and ensuring its quality has always remained one of the top priorities of public authorities. To ensure the quality of water, different methods for monitoring and assessing the water networks, such as offline and online surveys, are used. However, these surveys have several limitations, such as the limited number of participants and low frequency due to the labor involved in conducting such surveys. In this paper, we propose a Natural Language Processing (NLP) framework to automatically collect and analyze water-related posts from social media for data-driven decisions. The proposed framework is composed of two components, namely (i) text classification, and (ii) topic modeling. For text classification, we propose a merit-fusion-based framework incorporating several Large Language Models (LLMs) where different weight selection and optimization methods are employed to assign weights to the LLMs. In topic modeling, we employed the BERTopic library to discover the hidden topic patterns in the water-related tweets. We also analyzed relevant tweets originating from different regions and countries to explore global, regional, and country-specific issues and water-related concerns. We also collected and manually annotated a large-scale dataset, which is expected to facilitate future research on the topic.","sentences":["This paper focuses on a very important societal challenge of water quality analysis.","Being one of the key factors in the economic and social development of society, the provision of water and ensuring its quality has always remained one of the top priorities of public authorities.","To ensure the quality of water, different methods for monitoring and assessing the water networks, such as offline and online surveys, are used.","However, these surveys have several limitations, such as the limited number of participants and low frequency due to the labor involved in conducting such surveys.","In this paper, we propose a Natural Language Processing (NLP) framework to automatically collect and analyze water-related posts from social media for data-driven decisions.","The proposed framework is composed of two components, namely (i) text classification, and (ii) topic modeling.","For text classification, we propose a merit-fusion-based framework incorporating several Large Language Models (LLMs) where different weight selection and optimization methods are employed to assign weights to the LLMs.","In topic modeling, we employed the BERTopic library to discover the hidden topic patterns in the water-related tweets.","We also analyzed relevant tweets originating from different regions and countries to explore global, regional, and country-specific issues and water-related concerns.","We also collected and manually annotated a large-scale dataset, which is expected to facilitate future research on the topic."],"url":"http://arxiv.org/abs/2404.14977v1","category":"cs.SI"}
{"created":"2024-04-23 12:22:32","title":"CoARF: Controllable 3D Artistic Style Transfer for Radiance Fields","abstract":"Creating artistic 3D scenes can be time-consuming and requires specialized knowledge. To address this, recent works such as ARF, use a radiance field-based approach with style constraints to generate 3D scenes that resemble a style image provided by the user. However, these methods lack fine-grained control over the resulting scenes. In this paper, we introduce Controllable Artistic Radiance Fields (CoARF), a novel algorithm for controllable 3D scene stylization. CoARF enables style transfer for specified objects, compositional 3D style transfer and semantic-aware style transfer. We achieve controllability using segmentation masks with different label-dependent loss functions. We also propose a semantic-aware nearest neighbor matching algorithm to improve the style transfer quality. Our extensive experiments demonstrate that CoARF provides user-specified controllability of style transfer and superior style transfer quality with more precise feature matching.","sentences":["Creating artistic 3D scenes can be time-consuming and requires specialized knowledge.","To address this, recent works such as ARF, use a radiance field-based approach with style constraints to generate 3D scenes that resemble a style image provided by the user.","However, these methods lack fine-grained control over the resulting scenes.","In this paper, we introduce Controllable Artistic Radiance Fields (CoARF), a novel algorithm for controllable 3D scene stylization.","CoARF enables style transfer for specified objects, compositional 3D style transfer and semantic-aware style transfer.","We achieve controllability using segmentation masks with different label-dependent loss functions.","We also propose a semantic-aware nearest neighbor matching algorithm to improve the style transfer quality.","Our extensive experiments demonstrate that CoARF provides user-specified controllability of style transfer and superior style transfer quality with more precise feature matching."],"url":"http://arxiv.org/abs/2404.14967v1","category":"cs.CV"}
{"created":"2024-04-23 12:20:27","title":"Mamba3D: Enhancing Local Features for 3D Point Cloud Analysis via State Space Model","abstract":"Existing Transformer-based models for point cloud analysis suffer from quadratic complexity, leading to compromised point cloud resolution and information loss. In contrast, the newly proposed Mamba model, based on state space models (SSM), outperforms Transformer in multiple areas with only linear complexity. However, the straightforward adoption of Mamba does not achieve satisfactory performance on point cloud tasks. In this work, we present Mamba3D, a state space model tailored for point cloud learning to enhance local feature extraction, achieving superior performance, high efficiency, and scalability potential. Specifically, we propose a simple yet effective Local Norm Pooling (LNP) block to extract local geometric features. Additionally, to obtain better global features, we introduce a bidirectional SSM (bi-SSM) with both a token forward SSM and a novel backward SSM that operates on the feature channel. Extensive experimental results show that Mamba3D surpasses Transformer-based counterparts and concurrent works in multiple tasks, with or without pre-training. Notably, Mamba3D achieves multiple SoTA, including an overall accuracy of 92.6% (train from scratch) on the ScanObjectNN and 95.1% (with single-modal pre-training) on the ModelNet40 classification task, with only linear complexity.","sentences":["Existing Transformer-based models for point cloud analysis suffer from quadratic complexity, leading to compromised point cloud resolution and information loss.","In contrast, the newly proposed Mamba model, based on state space models (SSM), outperforms Transformer in multiple areas with only linear complexity.","However, the straightforward adoption of Mamba does not achieve satisfactory performance on point cloud tasks.","In this work, we present Mamba3D, a state space model tailored for point cloud learning to enhance local feature extraction, achieving superior performance, high efficiency, and scalability potential.","Specifically, we propose a simple yet effective Local Norm Pooling (LNP) block to extract local geometric features.","Additionally, to obtain better global features, we introduce a bidirectional SSM (bi-SSM) with both a token forward SSM and a novel backward SSM that operates on the feature channel.","Extensive experimental results show that Mamba3D surpasses Transformer-based counterparts and concurrent works in multiple tasks, with or without pre-training.","Notably, Mamba3D achieves multiple SoTA, including an overall accuracy of 92.6% (train from scratch) on the ScanObjectNN and 95.1% (with single-modal pre-training) on the ModelNet40 classification task, with only linear complexity."],"url":"http://arxiv.org/abs/2404.14966v1","category":"cs.CV"}
{"created":"2024-04-23 12:16:05","title":"Achieving >97% on GSM8K: Deeply Understanding the Problems Makes LLMs Perfect Reasoners","abstract":"Chain of Thought prompting strategy has enhanced the performance of Large Language Models (LLMs) across various NLP tasks. However, it still has shortcomings when dealing with complex reasoning tasks, following~\\citet{cot_wei}, including understanding errors, calculation errors and process errors (e.g. missing-step and hallucinations). Subsequently, Our in-depth analysis of various error types has found that deeply understanding the whole problem is critical in addressing complicated reasoning tasks. In this paper, we proposed a novel prompt strategy called Deeply Understanding the Problems (DUP) prompting, inspired by how humans solve complex reasoning problems, designed to enhance the comprehensive understanding of problems by LLMs. It consists of three stages: 1) extract the core question; 2) find out problem-solving information based on the core question; 3) generate and extract answers by LLMs. We evaluate the performance of DUP prompting on ten diverse reasoning datasets. Experimental results suggest that DUP prompting significantly outperforms Zero-Shot CoT ~\\cite{kojima2022large} across all datasets. Notably, DUP achieves \\textbf{state-of-the-art on SVAMP (90.4\\% to 94.2\\%) and GSM8K (94.6\\% to 97.1\\%).}","sentences":["Chain of Thought prompting strategy has enhanced the performance of Large Language Models (LLMs) across various NLP tasks.","However, it still has shortcomings when dealing with complex reasoning tasks, following~\\citet{cot_wei}, including understanding errors, calculation errors and process errors (e.g. missing-step and hallucinations).","Subsequently, Our in-depth analysis of various error types has found that deeply understanding the whole problem is critical in addressing complicated reasoning tasks.","In this paper, we proposed a novel prompt strategy called Deeply Understanding the Problems (DUP) prompting, inspired by how humans solve complex reasoning problems, designed to enhance the comprehensive understanding of problems by LLMs.","It consists of three stages: 1) extract the core question; 2) find out problem-solving information based on the core question; 3) generate and extract answers by LLMs.","We evaluate the performance of DUP prompting on ten diverse reasoning datasets.","Experimental results suggest that DUP prompting significantly outperforms Zero-Shot CoT ~\\cite{kojima2022large} across all datasets.","Notably, DUP achieves \\textbf{state-of-the-art on SVAMP (90.4\\% to 94.2\\%) and GSM8K (94.6\\% to 97.1\\%).}"],"url":"http://arxiv.org/abs/2404.14963v1","category":"cs.CL"}
{"created":"2024-04-23 12:02:26","title":"Impacting the dayside Martian ionosphere from above and below: Effects of the impact of CIRs and ICMEs close to aphelion (April 2021) and during dust storms (June-July 2022) seen with MAVEN ROSE","abstract":"We use 62 electron density profiles collected by the Radio Occultation Science Experiment (ROSE), on MAVEN, when Mars was hit by CIRs and ICMEs close to aphelion (April 2021) and during two dust storms (June-July 2022) to examine the response of the Martian ionosphere to solar events and to solar events hitting during dust storms. We do so through three proxies - variation in total electron content between 80 and 300 km altitude, peak density, and peak altitude - of the aforementioned 62 ROSE electron density profiles, relative to a characterisation of the ionosphere through solar minimum leading to solar maximum, specific to local time sector and season, presented in Segale et al., (COMPANION). We observe an increased Total Electron Content (TEC) between 80 and 300 km altitude up to 2.5 x 10(15) m(-2) in April 2021 and up to 5 x 10(15) m(-2) in June-July 2022 compared to the baseline photochemically produced ionosphere. This increase in TEC corresponds mainly to increases in the solar energetic particles flux (detected by MAVEN SEP) and electron fluxes (detected by MAVEN SWEA). In addition to solar events, in June-July 2022, an A storm and a B storm were occurring and merging on the surface of Mars. We observe a raise in peak altitude in general lower than expected during dust storms, possibly due to high values of solar wind dynamic pressure (derived from MAVEN SWIA). From 31 ROSE profiles collected in this time period that showed both the M2 and M1 layer, we observe that, on average, M1 and M2 peak altitudes raise the same amount, suggesting that the thermosphere might loft as a unit during dust storms. During this time period, several proton aurora events of variable brightness were detected with MAVEN IUVS underlining the complex and multifaceted impact of dust activity and extreme solar activity on the Martian ionosphere.","sentences":["We use 62 electron density profiles collected by the Radio Occultation Science Experiment (ROSE), on MAVEN, when Mars was hit by CIRs and ICMEs close to aphelion (April 2021) and during two dust storms (June-July 2022) to examine the response of the Martian ionosphere to solar events and to solar events hitting during dust storms.","We do so through three proxies - variation in total electron content between 80 and 300 km altitude, peak density, and peak altitude - of the aforementioned 62 ROSE electron density profiles, relative to a characterisation of the ionosphere through solar minimum leading to solar maximum, specific to local time sector and season, presented in Segale et al., (COMPANION).","We observe an increased Total Electron Content (TEC) between 80 and 300 km altitude up to 2.5 x 10(15) m(-2) in April 2021 and up to 5 x 10(15) m(-2) in June-July 2022 compared to the baseline photochemically produced ionosphere.","This increase in TEC corresponds mainly to increases in the solar energetic particles flux (detected by MAVEN SEP) and electron fluxes (detected by MAVEN SWEA).","In addition to solar events, in June-July 2022, an A storm and a B storm were occurring and merging on the surface of Mars.","We observe a raise in peak altitude in general lower than expected during dust storms, possibly due to high values of solar wind dynamic pressure (derived from MAVEN SWIA).","From 31 ROSE profiles collected in this time period that showed both the M2 and M1 layer, we observe that, on average, M1 and M2 peak altitudes raise the same amount, suggesting that the thermosphere might loft as a unit during dust storms.","During this time period, several proton aurora events of variable brightness were detected with MAVEN IUVS underlining the complex and multifaceted impact of dust activity and extreme solar activity on the Martian ionosphere."],"url":"http://arxiv.org/abs/2404.14959v1","category":"physics.space-ph"}
{"created":"2024-04-23 11:54:05","title":"Leveraging Speech for Gesture Detection in Multimodal Communication","abstract":"Gestures are inherent to human interaction and often complement speech in face-to-face communication, forming a multimodal communication system. An important task in gesture analysis is detecting a gesture's beginning and end. Research on automatic gesture detection has primarily focused on visual and kinematic information to detect a limited set of isolated or silent gestures with low variability, neglecting the integration of speech and vision signals to detect gestures that co-occur with speech. This work addresses this gap by focusing on co-speech gesture detection, emphasising the synchrony between speech and co-speech hand gestures. We address three main challenges: the variability of gesture forms, the temporal misalignment between gesture and speech onsets, and differences in sampling rate between modalities. We investigate extended speech time windows and employ separate backbone models for each modality to address the temporal misalignment and sampling rate differences. We utilize Transformer encoders in cross-modal and early fusion techniques to effectively align and integrate speech and skeletal sequences. The study results show that combining visual and speech information significantly enhances gesture detection performance. Our findings indicate that expanding the speech buffer beyond visual time segments improves performance and that multimodal integration using cross-modal and early fusion techniques outperforms baseline methods using unimodal and late fusion methods. Additionally, we find a correlation between the models' gesture prediction confidence and low-level speech frequency features potentially associated with gestures. Overall, the study provides a better understanding and detection methods for co-speech gestures, facilitating the analysis of multimodal communication.","sentences":["Gestures are inherent to human interaction and often complement speech in face-to-face communication, forming a multimodal communication system.","An important task in gesture analysis is detecting a gesture's beginning and end.","Research on automatic gesture detection has primarily focused on visual and kinematic information to detect a limited set of isolated or silent gestures with low variability, neglecting the integration of speech and vision signals to detect gestures that co-occur with speech.","This work addresses this gap by focusing on co-speech gesture detection, emphasising the synchrony between speech and co-speech hand gestures.","We address three main challenges: the variability of gesture forms, the temporal misalignment between gesture and speech onsets, and differences in sampling rate between modalities.","We investigate extended speech time windows and employ separate backbone models for each modality to address the temporal misalignment and sampling rate differences.","We utilize Transformer encoders in cross-modal and early fusion techniques to effectively align and integrate speech and skeletal sequences.","The study results show that combining visual and speech information significantly enhances gesture detection performance.","Our findings indicate that expanding the speech buffer beyond visual time segments improves performance and that multimodal integration using cross-modal and early fusion techniques outperforms baseline methods using unimodal and late fusion methods.","Additionally, we find a correlation between the models' gesture prediction confidence and low-level speech frequency features potentially associated with gestures.","Overall, the study provides a better understanding and detection methods for co-speech gestures, facilitating the analysis of multimodal communication."],"url":"http://arxiv.org/abs/2404.14952v1","category":"cs.CV"}
{"created":"2024-04-23 11:36:36","title":"Manipulating Recommender Systems: A Survey of Poisoning Attacks and Countermeasures","abstract":"Recommender systems have become an integral part of online services to help users locate specific information in a sea of data. However, existing studies show that some recommender systems are vulnerable to poisoning attacks, particularly those that involve learning schemes. A poisoning attack is where an adversary injects carefully crafted data into the process of training a model, with the goal of manipulating the system's final recommendations. Based on recent advancements in artificial intelligence, such attacks have gained importance recently. While numerous countermeasures to poisoning attacks have been developed, they have not yet been systematically linked to the properties of the attacks. Consequently, assessing the respective risks and potential success of mitigation strategies is difficult, if not impossible. This survey aims to fill this gap by primarily focusing on poisoning attacks and their countermeasures. This is in contrast to prior surveys that mainly focus on attacks and their detection methods. Through an exhaustive literature review, we provide a novel taxonomy for poisoning attacks, formalise its dimensions, and accordingly organise 30+ attacks described in the literature. Further, we review 40+ countermeasures to detect and/or prevent poisoning attacks, evaluating their effectiveness against specific types of attacks. This comprehensive survey should serve as a point of reference for protecting recommender systems against poisoning attacks. The article concludes with a discussion on open issues in the field and impactful directions for future research. A rich repository of resources associated with poisoning attacks is available at https://github.com/tamlhp/awesome-recsys-poisoning.","sentences":["Recommender systems have become an integral part of online services to help users locate specific information in a sea of data.","However, existing studies show that some recommender systems are vulnerable to poisoning attacks, particularly those that involve learning schemes.","A poisoning attack is where an adversary injects carefully crafted data into the process of training a model, with the goal of manipulating the system's final recommendations.","Based on recent advancements in artificial intelligence, such attacks have gained importance recently.","While numerous countermeasures to poisoning attacks have been developed, they have not yet been systematically linked to the properties of the attacks.","Consequently, assessing the respective risks and potential success of mitigation strategies is difficult, if not impossible.","This survey aims to fill this gap by primarily focusing on poisoning attacks and their countermeasures.","This is in contrast to prior surveys that mainly focus on attacks and their detection methods.","Through an exhaustive literature review, we provide a novel taxonomy for poisoning attacks, formalise its dimensions, and accordingly organise 30+ attacks described in the literature.","Further, we review 40+ countermeasures to detect and/or prevent poisoning attacks, evaluating their effectiveness against specific types of attacks.","This comprehensive survey should serve as a point of reference for protecting recommender systems against poisoning attacks.","The article concludes with a discussion on open issues in the field and impactful directions for future research.","A rich repository of resources associated with poisoning attacks is available at https://github.com/tamlhp/awesome-recsys-poisoning."],"url":"http://arxiv.org/abs/2404.14942v1","category":"cs.CR"}
{"created":"2024-04-23 11:35:35","title":"Delayed Bottlenecking: Alleviating Forgetting in Pre-trained Graph Neural Networks","abstract":"Pre-training GNNs to extract transferable knowledge and apply it to downstream tasks has become the de facto standard of graph representation learning. Recent works focused on designing self-supervised pre-training tasks to extract useful and universal transferable knowledge from large-scale unlabeled data. However, they have to face an inevitable question: traditional pre-training strategies that aim at extracting useful information about pre-training tasks, may not extract all useful information about the downstream task. In this paper, we reexamine the pre-training process within traditional pre-training and fine-tuning frameworks from the perspective of Information Bottleneck (IB) and confirm that the forgetting phenomenon in pre-training phase may cause detrimental effects on downstream tasks. Therefore, we propose a novel \\underline{D}elayed \\underline{B}ottlenecking \\underline{P}re-training (DBP) framework which maintains as much as possible mutual information between latent representations and training data during pre-training phase by suppressing the compression operation and delays the compression operation to fine-tuning phase to make sure the compression can be guided with labeled fine-tuning data and downstream tasks. To achieve this, we design two information control objectives that can be directly optimized and further integrate them into the actual model design. Extensive experiments on both chemistry and biology domains demonstrate the effectiveness of DBP.","sentences":["Pre-training GNNs to extract transferable knowledge and apply it to downstream tasks has become the de facto standard of graph representation learning.","Recent works focused on designing self-supervised pre-training tasks to extract useful and universal transferable knowledge from large-scale unlabeled data.","However, they have to face an inevitable question: traditional pre-training strategies that aim at extracting useful information about pre-training tasks, may not extract all useful information about the downstream task.","In this paper, we reexamine the pre-training process within traditional pre-training and fine-tuning frameworks from the perspective of Information Bottleneck (IB) and confirm that the forgetting phenomenon in pre-training phase may cause detrimental effects on downstream tasks.","Therefore, we propose a novel \\underline{D}elayed \\underline{B}ottlenecking \\underline{P}re-training (DBP) framework which maintains as much as possible mutual information between latent representations and training data during pre-training phase by suppressing the compression operation and delays the compression operation to fine-tuning phase to make sure the compression can be guided with labeled fine-tuning data and downstream tasks.","To achieve this, we design two information control objectives that can be directly optimized and further integrate them into the actual model design.","Extensive experiments on both chemistry and biology domains demonstrate the effectiveness of DBP."],"url":"http://arxiv.org/abs/2404.14941v1","category":"cs.LG"}
{"created":"2024-04-23 11:24:38","title":"A Data-Driven Analysis of Vulnerable Road User Safety in Interaction with Connected Automated Vehicles","abstract":"According to the World Health Organization, the involvement of Vulnerable Road Users (VRUs) in traffic accidents remains a significant concern, with VRUs accounting for over half of traffic fatalities. The increase of automation and connectivity levels of vehicles has still an uncertain impact on VRU safety. By deploying the Collective Perception Service (CPS), vehicles can include information about VRUs in Vehicle-to-Everything (V2X) messages, thus raising the general perception of the environment. Although an increased awareness is considered positive, one could argue that the awareness ratio, the metric used to measure perception, is only implicitly connected to the VRUs' safety. This paper introduces a tailored metric, the Risk Factor (RF), to measure the risk level for the interactions between Connected Automated Vehicles (CAVs) and VRUs. By evaluating the RF, we assess the impact of V2X communication on VRU risk mitigation. Our results show that high V2X penetration rates can reduce mean risk, quantified by our proposed metric, by up to 44%. Although the median risk value shows a significant decrease, suggesting a reduction in overall risk, the distribution of risk values reveals that CPS's mitigation effectiveness is overestimated, which is indicated by the divergence between RF and awareness ratio. Additionally, by analyzing a real-world traffic dataset, we pinpoint high-risk locations within a scenario, identifying areas near intersections and behind parked cars as especially dangerous. Our methodology can be ported and applied to other scenarios in order to identify high-risk areas. We value the proposed RF as an insightful metric for quantifying VRU safety in a highly automated and connected environment.","sentences":["According to the World Health Organization, the involvement of Vulnerable Road Users (VRUs) in traffic accidents remains a significant concern, with VRUs accounting for over half of traffic fatalities.","The increase of automation and connectivity levels of vehicles has still an uncertain impact on VRU safety.","By deploying the Collective Perception Service (CPS), vehicles can include information about VRUs in Vehicle-to-Everything (V2X) messages, thus raising the general perception of the environment.","Although an increased awareness is considered positive, one could argue that the awareness ratio, the metric used to measure perception, is only implicitly connected to the VRUs' safety.","This paper introduces a tailored metric, the Risk Factor (RF), to measure the risk level for the interactions between Connected Automated Vehicles (CAVs) and VRUs.","By evaluating the RF, we assess the impact of V2X communication on VRU risk mitigation.","Our results show that high V2X penetration rates can reduce mean risk, quantified by our proposed metric, by up to 44%.","Although the median risk value shows a significant decrease, suggesting a reduction in overall risk, the distribution of risk values reveals that CPS's mitigation effectiveness is overestimated, which is indicated by the divergence between RF and awareness ratio.","Additionally, by analyzing a real-world traffic dataset, we pinpoint high-risk locations within a scenario, identifying areas near intersections and behind parked cars as especially dangerous.","Our methodology can be ported and applied to other scenarios in order to identify high-risk areas.","We value the proposed RF as an insightful metric for quantifying VRU safety in a highly automated and connected environment."],"url":"http://arxiv.org/abs/2404.14935v1","category":"cs.NI"}
{"created":"2024-04-23 11:22:59","title":"G3R: Generating Rich and Fine-grained mmWave Radar Data from 2D Videos for Generalized Gesture Recognition","abstract":"Millimeter wave radar is gaining traction recently as a promising modality for enabling pervasive and privacy-preserving gesture recognition. However, the lack of rich and fine-grained radar datasets hinders progress in developing generalized deep learning models for gesture recognition across various user postures (e.g., standing, sitting), positions, and scenes. To remedy this, we resort to designing a software pipeline that exploits wealthy 2D videos to generate realistic radar data, but it needs to address the challenge of simulating diversified and fine-grained reflection properties of user gestures. To this end, we design G3R with three key components: (i) a gesture reflection point generator expands the arm's skeleton points to form human reflection points; (ii) a signal simulation model simulates the multipath reflection and attenuation of radar signals to output the human intensity map; (iii) an encoder-decoder model combines a sampling module and a fitting module to address the differences in number and distribution of points between generated and real-world radar data for generating realistic radar data. We implement and evaluate G3R using 2D videos from public data sources and self-collected real-world radar data, demonstrating its superiority over other state-of-the-art approaches for gesture recognition.","sentences":["Millimeter wave radar is gaining traction recently as a promising modality for enabling pervasive and privacy-preserving gesture recognition.","However, the lack of rich and fine-grained radar datasets hinders progress in developing generalized deep learning models for gesture recognition across various user postures (e.g., standing, sitting), positions, and scenes.","To remedy this, we resort to designing a software pipeline that exploits wealthy 2D videos to generate realistic radar data, but it needs to address the challenge of simulating diversified and fine-grained reflection properties of user gestures.","To this end, we design G3R with three key components: (i) a gesture reflection point generator expands the arm's skeleton points to form human reflection points; (ii) a signal simulation model simulates the multipath reflection and attenuation of radar signals to output the human intensity map; (iii) an encoder-decoder model combines a sampling module and a fitting module to address the differences in number and distribution of points between generated and real-world radar data for generating realistic radar data.","We implement and evaluate G3R using 2D videos from public data sources and self-collected real-world radar data, demonstrating its superiority over other state-of-the-art approaches for gesture recognition."],"url":"http://arxiv.org/abs/2404.14934v1","category":"cs.MM"}
{"created":"2024-04-23 11:22:04","title":"Fin-Fed-OD: Federated Outlier Detection on Financial Tabular Data","abstract":"Anomaly detection in real-world scenarios poses challenges due to dynamic and often unknown anomaly distributions, requiring robust methods that operate under an open-world assumption. This challenge is exacerbated in practical settings, where models are employed by private organizations, precluding data sharing due to privacy and competitive concerns. Despite potential benefits, the sharing of anomaly information across organizations is restricted. This paper addresses the question of enhancing outlier detection within individual organizations without compromising data confidentiality. We propose a novel method leveraging representation learning and federated learning techniques to improve the detection of unknown anomalies. Specifically, our approach utilizes latent representations obtained from client-owned autoencoders to refine the decision boundary of inliers. Notably, only model parameters are shared between organizations, preserving data privacy. The efficacy of our proposed method is evaluated on two standard financial tabular datasets and an image dataset for anomaly detection in a distributed setting. The results demonstrate a strong improvement in the classification of unknown outliers during the inference phase for each organization's model.","sentences":["Anomaly detection in real-world scenarios poses challenges due to dynamic and often unknown anomaly distributions, requiring robust methods that operate under an open-world assumption.","This challenge is exacerbated in practical settings, where models are employed by private organizations, precluding data sharing due to privacy and competitive concerns.","Despite potential benefits, the sharing of anomaly information across organizations is restricted.","This paper addresses the question of enhancing outlier detection within individual organizations without compromising data confidentiality.","We propose a novel method leveraging representation learning and federated learning techniques to improve the detection of unknown anomalies.","Specifically, our approach utilizes latent representations obtained from client-owned autoencoders to refine the decision boundary of inliers.","Notably, only model parameters are shared between organizations, preserving data privacy.","The efficacy of our proposed method is evaluated on two standard financial tabular datasets and an image dataset for anomaly detection in a distributed setting.","The results demonstrate a strong improvement in the classification of unknown outliers during the inference phase for each organization's model."],"url":"http://arxiv.org/abs/2404.14933v1","category":"cs.LG"}
{"created":"2024-04-23 11:13:39","title":"Graph Machine Learning in the Era of Large Language Models (LLMs)","abstract":"Graphs play an important role in representing complex relationships in various domains like social networks, knowledge graphs, and molecular discovery. With the advent of deep learning, Graph Neural Networks (GNNs) have emerged as a cornerstone in Graph Machine Learning (Graph ML), facilitating the representation and processing of graph structures. Recently, LLMs have demonstrated unprecedented capabilities in language tasks and are widely adopted in a variety of applications such as computer vision and recommender systems. This remarkable success has also attracted interest in applying LLMs to the graph domain. Increasing efforts have been made to explore the potential of LLMs in advancing Graph ML's generalization, transferability, and few-shot learning ability. Meanwhile, graphs, especially knowledge graphs, are rich in reliable factual knowledge, which can be utilized to enhance the reasoning capabilities of LLMs and potentially alleviate their limitations such as hallucinations and the lack of explainability. Given the rapid progress of this research direction, a systematic review summarizing the latest advancements for Graph ML in the era of LLMs is necessary to provide an in-depth understanding to researchers and practitioners. Therefore, in this survey, we first review the recent developments in Graph ML. We then explore how LLMs can be utilized to enhance the quality of graph features, alleviate the reliance on labeled data, and address challenges such as graph heterogeneity and out-of-distribution (OOD) generalization. Afterward, we delve into how graphs can enhance LLMs, highlighting their abilities to enhance LLM pre-training and inference. Furthermore, we investigate various applications and discuss the potential future directions in this promising field.","sentences":["Graphs play an important role in representing complex relationships in various domains like social networks, knowledge graphs, and molecular discovery.","With the advent of deep learning, Graph Neural Networks (GNNs) have emerged as a cornerstone in Graph Machine Learning (Graph ML), facilitating the representation and processing of graph structures.","Recently, LLMs have demonstrated unprecedented capabilities in language tasks and are widely adopted in a variety of applications such as computer vision and recommender systems.","This remarkable success has also attracted interest in applying LLMs to the graph domain.","Increasing efforts have been made to explore the potential of LLMs in advancing Graph ML's generalization, transferability, and few-shot learning ability.","Meanwhile, graphs, especially knowledge graphs, are rich in reliable factual knowledge, which can be utilized to enhance the reasoning capabilities of LLMs and potentially alleviate their limitations such as hallucinations and the lack of explainability.","Given the rapid progress of this research direction, a systematic review summarizing the latest advancements for Graph ML in the era of LLMs is necessary to provide an in-depth understanding to researchers and practitioners.","Therefore, in this survey, we first review the recent developments in Graph ML.","We then explore how LLMs can be utilized to enhance the quality of graph features, alleviate the reliance on labeled data, and address challenges such as graph heterogeneity and out-of-distribution (OOD) generalization.","Afterward, we delve into how graphs can enhance LLMs, highlighting their abilities to enhance LLM pre-training and inference.","Furthermore, we investigate various applications and discuss the potential future directions in this promising field."],"url":"http://arxiv.org/abs/2404.14928v1","category":"cs.LG"}
{"created":"2024-04-23 11:11:15","title":"Vulnerable Road User Clustering for Collective Perception Messages: Efficient Representation Through Geometric Shapes","abstract":"Ensuring the safety of Vulnerable Road Users (VRUs) is a critical concern in transportation, demanding significant attention from researchers and engineers. Recent advancements in Vehicle-to-Everything (V2X) technology offer promising solutions to enhance VRU safety. Notably, VRUs often travel in groups, exhibiting similar movement patterns that facilitate the formation of clusters. The standardized Collective Perception Message (CPM) and VRU Awareness Message in ETSI's Release 2 consider this clustering behavior, allowing for the description of VRU clusters. Given the constraints of narrow channel bandwidth, the selection of an appropriate geometric shape for representing a VRU cluster becomes crucial for efficient data transmission. In our study we conduct a comprehensive evaluation of different geometric shapes used to describe VRU clusters. We introduce two metrics: Cluster Accuracy (CA) and Comprehensive Area Density Information (CADI), to assess the precision and efficiency of each shape. Beyond comparing predefined shapes, we propose an adaptive algorithm that selects the preferred shape for cluster description, prioritizing accuracy while maintaining a high level of efficiency. The study culminates by demonstrating the benefits of clustering on data transmission rates. We simulate VRU movement using real-world data and the transmission of CPMs by a roadside unit. The results reveal that broadcasting cluster information, as opposed to individual object data, can reduce the data transmission volume by two-thirds on average. This finding underscores the potential of clustering in V2X communications to enhance VRU safety while optimizing network resources.","sentences":["Ensuring the safety of Vulnerable Road Users (VRUs) is a critical concern in transportation, demanding significant attention from researchers and engineers.","Recent advancements in Vehicle-to-Everything (V2X) technology offer promising solutions to enhance VRU safety.","Notably, VRUs often travel in groups, exhibiting similar movement patterns that facilitate the formation of clusters.","The standardized Collective Perception Message (CPM) and VRU Awareness Message in ETSI's Release 2 consider this clustering behavior, allowing for the description of VRU clusters.","Given the constraints of narrow channel bandwidth, the selection of an appropriate geometric shape for representing a VRU cluster becomes crucial for efficient data transmission.","In our study we conduct a comprehensive evaluation of different geometric shapes used to describe VRU clusters.","We introduce two metrics: Cluster Accuracy (CA) and Comprehensive Area Density Information (CADI), to assess the precision and efficiency of each shape.","Beyond comparing predefined shapes, we propose an adaptive algorithm that selects the preferred shape for cluster description, prioritizing accuracy while maintaining a high level of efficiency.","The study culminates by demonstrating the benefits of clustering on data transmission rates.","We simulate VRU movement using real-world data and the transmission of CPMs by a roadside unit.","The results reveal that broadcasting cluster information, as opposed to individual object data, can reduce the data transmission volume by two-thirds on average.","This finding underscores the potential of clustering in V2X communications to enhance VRU safety while optimizing network resources."],"url":"http://arxiv.org/abs/2404.14925v1","category":"cs.NI"}
{"created":"2024-04-23 10:43:35","title":"Inputs for the $\u03b3$ measurements from BESIII","abstract":"The CKM angle $\\gamma$ is important for testing the unitarity of the CKM matrix and searching for new physics. $\\gamma$ can be extracted by the interference between $b\\to u$ and $b\\to c$ in the B factory such as LHCb and Belle-II. Determining $\\gamma$ also needs strong parameter information from the charm factory, such as the BESIII experiment. With quantum-correlated data samples collected at BESIII, the $\\gamma$ uncertainties from the charm sector can be highly suppressed.","sentences":["The CKM angle $\\gamma$ is important for testing the unitarity of the CKM matrix and searching for new physics.","$\\gamma$ can be extracted by the interference between $b\\to u$ and $b\\to c$ in the B factory such as LHCb and Belle-II.","Determining $\\gamma$ also needs strong parameter information from the charm factory, such as the BESIII experiment.","With quantum-correlated data samples collected at BESIII, the $\\gamma$ uncertainties from the charm sector can be highly suppressed."],"url":"http://arxiv.org/abs/2404.14907v1","category":"hep-ex"}
{"created":"2024-04-23 10:42:24","title":"Driver Activity Classification Using Generalizable Representations from Vision-Language Models","abstract":"Driver activity classification is crucial for ensuring road safety, with applications ranging from driver assistance systems to autonomous vehicle control transitions. In this paper, we present a novel approach leveraging generalizable representations from vision-language models for driver activity classification. Our method employs a Semantic Representation Late Fusion Neural Network (SRLF-Net) to process synchronized video frames from multiple perspectives. Each frame is encoded using a pretrained vision-language encoder, and the resulting embeddings are fused to generate class probability predictions. By leveraging contrastively-learned vision-language representations, our approach achieves robust performance across diverse driver activities. We evaluate our method on the Naturalistic Driving Action Recognition Dataset, demonstrating strong accuracy across many classes. Our results suggest that vision-language representations offer a promising avenue for driver monitoring systems, providing both accuracy and interpretability through natural language descriptors.","sentences":["Driver activity classification is crucial for ensuring road safety, with applications ranging from driver assistance systems to autonomous vehicle control transitions.","In this paper, we present a novel approach leveraging generalizable representations from vision-language models for driver activity classification.","Our method employs a Semantic Representation Late Fusion Neural Network (SRLF-Net) to process synchronized video frames from multiple perspectives.","Each frame is encoded using a pretrained vision-language encoder, and the resulting embeddings are fused to generate class probability predictions.","By leveraging contrastively-learned vision-language representations, our approach achieves robust performance across diverse driver activities.","We evaluate our method on the Naturalistic Driving Action Recognition Dataset, demonstrating strong accuracy across many classes.","Our results suggest that vision-language representations offer a promising avenue for driver monitoring systems, providing both accuracy and interpretability through natural language descriptors."],"url":"http://arxiv.org/abs/2404.14906v1","category":"cs.CV"}
{"created":"2024-04-23 10:34:16","title":"Beyond Code Generation: An Observational Study of ChatGPT Usage in Software Engineering Practice","abstract":"Large Language Models (LLMs) are frequently discussed in academia and the general public as support tools for virtually any use case that relies on the production of text, including software engineering. Currently there is much debate, but little empirical evidence, regarding the practical usefulness of LLM-based tools such as ChatGPT for engineers in industry. We conduct an observational study of 24 professional software engineers who have been using ChatGPT over a period of one week in their jobs, and qualitatively analyse their dialogues with the chatbot as well as their overall experience (as captured by an exit survey). We find that, rather than expecting ChatGPT to generate ready-to-use software artifacts (e.g., code), practitioners more often use ChatGPT to receive guidance on how to solve their tasks or learn about a topic in more abstract terms. We also propose a theoretical framework for how (i) purpose of the interaction, (ii) internal factors (e.g., the user's personality), and (iii) external factors (e.g., company policy) together shape the experience (in terms of perceived usefulness and trust). We envision that our framework can be used by future research to further the academic discussion on LLM usage by software engineering practitioners, and to serve as a reference point for the design of future empirical LLM research in this domain.","sentences":["Large Language Models (LLMs) are frequently discussed in academia and the general public as support tools for virtually any use case that relies on the production of text, including software engineering.","Currently there is much debate, but little empirical evidence, regarding the practical usefulness of LLM-based tools such as ChatGPT for engineers in industry.","We conduct an observational study of 24 professional software engineers who have been using ChatGPT over a period of one week in their jobs, and qualitatively analyse their dialogues with the chatbot as well as their overall experience (as captured by an exit survey).","We find that, rather than expecting ChatGPT to generate ready-to-use software artifacts (e.g., code), practitioners more often use ChatGPT to receive guidance on how to solve their tasks or learn about a topic in more abstract terms.","We also propose a theoretical framework for how (i) purpose of the interaction, (ii) internal factors (e.g., the user's personality), and (iii) external factors (e.g., company policy) together shape the experience (in terms of perceived usefulness and trust).","We envision that our framework can be used by future research to further the academic discussion on LLM usage by software engineering practitioners, and to serve as a reference point for the design of future empirical LLM research in this domain."],"url":"http://arxiv.org/abs/2404.14901v1","category":"cs.SE"}
{"created":"2024-04-23 10:25:45","title":"Beyond the Speculative Game: A Survey of Speculative Execution in Large Language Models","abstract":"With the increasingly giant scales of (causal) large language models (LLMs), the inference efficiency comes as one of the core concerns along the improved performance. In contrast to the memory footprint, the latency bottleneck seems to be of greater importance as there can be billions of requests to a LLM (e.g., GPT-4) per day. The bottleneck is mainly due to the autoregressive innateness of LLMs, where tokens can only be generated sequentially during decoding. To alleviate the bottleneck, the idea of speculative execution, which originates from the field of computer architecture, is introduced to LLM decoding in a \\textit{draft-then-verify} style. Under this regime, a sequence of tokens will be drafted in a fast pace by utilizing some heuristics, and then the tokens shall be verified in parallel by the LLM. As the costly sequential inference is parallelized, LLM decoding speed can be significantly boosted. Driven by the success of LLMs in recent couple of years, a growing literature in this direction has emerged. Yet, there lacks a position survey to summarize the current landscape and draw a roadmap for future development of this promising area. To meet this demand, we present the very first survey paper that reviews and unifies literature of speculative execution in LLMs (e.g., blockwise parallel decoding, speculative decoding, etc.) in a comprehensive framework and a systematic taxonomy. Based on the taxonomy, we present a critical review and comparative analysis of the current arts. Finally we highlight various key challenges and future directions to further develop the area.","sentences":["With the increasingly giant scales of (causal) large language models (LLMs), the inference efficiency comes as one of the core concerns along the improved performance.","In contrast to the memory footprint, the latency bottleneck seems to be of greater importance as there can be billions of requests to a LLM (e.g., GPT-4) per day.","The bottleneck is mainly due to the autoregressive innateness of LLMs, where tokens can only be generated sequentially during decoding.","To alleviate the bottleneck, the idea of speculative execution, which originates from the field of computer architecture, is introduced to LLM decoding in a \\textit{draft-then-verify} style.","Under this regime, a sequence of tokens will be drafted in a fast pace by utilizing some heuristics, and then the tokens shall be verified in parallel by the LLM.","As the costly sequential inference is parallelized, LLM decoding speed can be significantly boosted.","Driven by the success of LLMs in recent couple of years, a growing literature in this direction has emerged.","Yet, there lacks a position survey to summarize the current landscape and draw a roadmap for future development of this promising area.","To meet this demand, we present the very first survey paper that reviews and unifies literature of speculative execution in LLMs (e.g., blockwise parallel decoding, speculative decoding, etc.)","in a comprehensive framework and a systematic taxonomy.","Based on the taxonomy, we present a critical review and comparative analysis of the current arts.","Finally we highlight various key challenges and future directions to further develop the area."],"url":"http://arxiv.org/abs/2404.14897v1","category":"cs.CL"}
{"created":"2024-04-23 10:09:46","title":"Language in Vivo vs. in Silico: Size Matters but Larger Language Models Still Do Not Comprehend Language on a Par with Humans","abstract":"Understanding the limits of language is a prerequisite for Large Language Models (LLMs) to act as theories of natural language. LLM performance in some language tasks presents both quantitative and qualitative differences from that of humans, however it remains to be determined whether such differences are amenable to model size. This work investigates the critical role of model scaling, determining whether increases in size make up for such differences between humans and models. We test three LLMs from different families (Bard, 137 billion parameters; ChatGPT-3.5, 175 billion; ChatGPT-4, 1.5 trillion) on a grammaticality judgment task featuring anaphora, center embedding, comparatives, and negative polarity. N=1,200 judgments are collected and scored for accuracy, stability, and improvements in accuracy upon repeated presentation of a prompt. Results of the best performing LLM, ChatGPT-4, are compared to results of n=80 humans on the same stimuli. We find that increased model size may lead to better performance, but LLMs are still not sensitive to (un)grammaticality as humans are. It seems possible but unlikely that scaling alone can fix this issue. We interpret these results by comparing language learning in vivo and in silico, identifying three critical differences concerning (i) the type of evidence, (ii) the poverty of the stimulus, and (iii) the occurrence of semantic hallucinations due to impenetrable linguistic reference.","sentences":["Understanding the limits of language is a prerequisite for Large Language Models (LLMs) to act as theories of natural language.","LLM performance in some language tasks presents both quantitative and qualitative differences from that of humans, however it remains to be determined whether such differences are amenable to model size.","This work investigates the critical role of model scaling, determining whether increases in size make up for such differences between humans and models.","We test three LLMs from different families (Bard, 137 billion parameters; ChatGPT-3.5, 175 billion; ChatGPT-4, 1.5 trillion) on a grammaticality judgment task featuring anaphora, center embedding, comparatives, and negative polarity.","N=1,200 judgments are collected and scored for accuracy, stability, and improvements in accuracy upon repeated presentation of a prompt.","Results of the best performing LLM, ChatGPT-4, are compared to results of n=80 humans on the same stimuli.","We find that increased model size may lead to better performance, but LLMs are still not sensitive to (un)grammaticality as humans are.","It seems possible but unlikely that scaling alone can fix this issue.","We interpret these results by comparing language learning in vivo and in silico, identifying three critical differences concerning (i) the type of evidence, (ii) the poverty of the stimulus, and (iii) the occurrence of semantic hallucinations due to impenetrable linguistic reference."],"url":"http://arxiv.org/abs/2404.14883v1","category":"cs.CL"}
{"created":"2024-04-23 10:06:32","title":"Device-Free 3D Drone Localization in RIS-Assisted mmWave MIMO Networks","abstract":"In this paper, we investigate the potential of reconfigurable intelligent surfaces (RISs) in facilitating passive/device-free three-dimensional (3D) drone localization within existing cellular infrastructure operating at millimeter-wave (mmWave) frequencies and employing multiple antennas at the transceivers. The developed localization system operates in the bi-static mode without requiring direct communication between the drone and the base station. We analyze the theoretical performance limits via Fisher information analysis and Cram\\'er Rao lower bounds (CRLBs). Furthermore, we develop a low-complexity yet effective drone localization algorithm based on coordinate gradient descent and examine the impact of factors such as radar cross section (RCS) of the drone and training overhead on system performance. It is demonstrated that integrating RIS yields significant benefits over its RIS-free counterpart, as evidenced by both theoretical analyses and numerical simulations.","sentences":["In this paper, we investigate the potential of reconfigurable intelligent surfaces (RISs) in facilitating passive/device-free three-dimensional (3D) drone localization within existing cellular infrastructure operating at millimeter-wave (mmWave) frequencies and employing multiple antennas at the transceivers.","The developed localization system operates in the bi-static mode without requiring direct communication between the drone and the base station.","We analyze the theoretical performance limits via Fisher information analysis and Cram\\'er Rao lower bounds (CRLBs).","Furthermore, we develop a low-complexity yet effective drone localization algorithm based on coordinate gradient descent and examine the impact of factors such as radar cross section (RCS) of the drone and training overhead on system performance.","It is demonstrated that integrating RIS yields significant benefits over its RIS-free counterpart, as evidenced by both theoretical analyses and numerical simulations."],"url":"http://arxiv.org/abs/2404.14879v1","category":"eess.SP"}
{"created":"2024-04-23 09:55:25","title":"Exploring Human-AI Collaboration in Agile: Customised LLM Meeting Assistants","abstract":"This action research study focuses on the integration of \"AI assistants\" in two Agile software development meetings: the Daily Scrum and a feature refinement, a planning meeting that is part of an in-house Scaled Agile framework. We discuss the critical drivers of success, and establish a link between the use of AI and team collaboration dynamics. We conclude with a list of lessons learnt during the interventions in an industrial context, and provide a assessment checklist for companies and teams to reflect on their readiness level. This paper is thus a road-map to facilitate the integration of AI tools in Agile setups.","sentences":["This action research study focuses on the integration of \"AI assistants\" in two Agile software development meetings: the Daily Scrum and a feature refinement, a planning meeting that is part of an in-house Scaled Agile framework.","We discuss the critical drivers of success, and establish a link between the use of AI and team collaboration dynamics.","We conclude with a list of lessons learnt during the interventions in an industrial context, and provide a assessment checklist for companies and teams to reflect on their readiness level.","This paper is thus a road-map to facilitate the integration of AI tools in Agile setups."],"url":"http://arxiv.org/abs/2404.14871v1","category":"cs.SE"}
{"created":"2024-04-23 09:35:12","title":"Irreversible Boltzmann samplers in dense liquids: weak-coupling approximation and mode-coupling theory","abstract":"Exerting a nonequilibrium drive on an otherwise equilibrium Langevin process brings the dynamics out of equilibrium but can also speedup the approach to the Boltzmann steady-state. Transverse forces are a minimal framework to achieve dynamical acceleration of the Boltzmann sampling. We consider a simple liquid in three space dimensions subjected to additional transverse pairwise forces, and quantify the extent to which transverse forces accelerate the dynamics. We first explore the dynamics of a tracer in a weak coupling regime describing high temperatures. The resulting acceleration is correlated with a monotonous increase of the magnitude of odd transport coefficients (mobility and diffusivity) with the amplitude of the transverse drive. We then develop a nonequilibrium version of the mode-coupling theory able to capture the effect of transverse forces, and more generally of forces created by additional degrees of freedom. Based on an analysis of transport coefficients, both odd and longitudinal, both for the collective modes and for a tracer particle, we find a systematic acceleration of the dynamics. Quantitatively, the gain, which is guaranteed throughout the ergodic phase, turns out to be a decreasing function of temperature beyond a temperature crossover, in particular as the glass transition is approached. Our theoretical results are in good agreement with available numerical results.","sentences":["Exerting a nonequilibrium drive on an otherwise equilibrium Langevin process brings the dynamics out of equilibrium but can also speedup the approach to the Boltzmann steady-state.","Transverse forces are a minimal framework to achieve dynamical acceleration of the Boltzmann sampling.","We consider a simple liquid in three space dimensions subjected to additional transverse pairwise forces, and quantify the extent to which transverse forces accelerate the dynamics.","We first explore the dynamics of a tracer in a weak coupling regime describing high temperatures.","The resulting acceleration is correlated with a monotonous increase of the magnitude of odd transport coefficients (mobility and diffusivity) with the amplitude of the transverse drive.","We then develop a nonequilibrium version of the mode-coupling theory able to capture the effect of transverse forces, and more generally of forces created by additional degrees of freedom.","Based on an analysis of transport coefficients, both odd and longitudinal, both for the collective modes and for a tracer particle, we find a systematic acceleration of the dynamics.","Quantitatively, the gain, which is guaranteed throughout the ergodic phase, turns out to be a decreasing function of temperature beyond a temperature crossover, in particular as the glass transition is approached.","Our theoretical results are in good agreement with available numerical results."],"url":"http://arxiv.org/abs/2404.14863v1","category":"cond-mat.soft"}
{"created":"2024-04-23 09:07:04","title":"Ultrasound Nodule Segmentation Using Asymmetric Learning with Simple Clinical Annotation","abstract":"Recent advances in deep learning have greatly facilitated the automated segmentation of ultrasound images, which is essential for nodule morphological analysis. Nevertheless, most existing methods depend on extensive and precise annotations by domain experts, which are labor-intensive and time-consuming. In this study, we suggest using simple aspect ratio annotations directly from ultrasound clinical diagnoses for automated nodule segmentation. Especially, an asymmetric learning framework is developed by extending the aspect ratio annotations with two types of pseudo labels, i.e., conservative labels and radical labels, to train two asymmetric segmentation networks simultaneously. Subsequently, a conservative-radical-balance strategy (CRBS) strategy is proposed to complementally combine radical and conservative labels. An inconsistency-aware dynamically mixed pseudo-labels supervision (IDMPS) module is introduced to address the challenges of over-segmentation and under-segmentation caused by the two types of labels. To further leverage the spatial prior knowledge provided by clinical annotations, we also present a novel loss function namely the clinical anatomy prior loss. Extensive experiments on two clinically collected ultrasound datasets (thyroid and breast) demonstrate the superior performance of our proposed method, which can achieve comparable and even better performance than fully supervised methods using ground truth annotations.","sentences":["Recent advances in deep learning have greatly facilitated the automated segmentation of ultrasound images, which is essential for nodule morphological analysis.","Nevertheless, most existing methods depend on extensive and precise annotations by domain experts, which are labor-intensive and time-consuming.","In this study, we suggest using simple aspect ratio annotations directly from ultrasound clinical diagnoses for automated nodule segmentation.","Especially, an asymmetric learning framework is developed by extending the aspect ratio annotations with two types of pseudo labels, i.e., conservative labels and radical labels, to train two asymmetric segmentation networks simultaneously.","Subsequently, a conservative-radical-balance strategy (CRBS) strategy is proposed to complementally combine radical and conservative labels.","An inconsistency-aware dynamically mixed pseudo-labels supervision (IDMPS) module is introduced to address the challenges of over-segmentation and under-segmentation caused by the two types of labels.","To further leverage the spatial prior knowledge provided by clinical annotations, we also present a novel loss function namely the clinical anatomy prior loss.","Extensive experiments on two clinically collected ultrasound datasets (thyroid and breast) demonstrate the superior performance of our proposed method, which can achieve comparable and even better performance than fully supervised methods using ground truth annotations."],"url":"http://arxiv.org/abs/2404.14852v1","category":"cs.CV"}
{"created":"2024-04-23 09:05:37","title":"From Matching to Generation: A Survey on Generative Information Retrieval","abstract":"Information Retrieval (IR) systems are crucial tools for users to access information, widely applied in scenarios like search engines, question answering, and recommendation systems. Traditional IR methods, based on similarity matching to return ranked lists of documents, have been reliable means of information acquisition, dominating the IR field for years. With the advancement of pre-trained language models, generative information retrieval (GenIR) has emerged as a novel paradigm, gaining increasing attention in recent years. Currently, research in GenIR can be categorized into two aspects: generative document retrieval (GR) and reliable response generation. GR leverages the generative model's parameters for memorizing documents, enabling retrieval by directly generating relevant document identifiers without explicit indexing. Reliable response generation, on the other hand, employs language models to directly generate the information users seek, breaking the limitations of traditional IR in terms of document granularity and relevance matching, offering more flexibility, efficiency, and creativity, thus better meeting practical needs. This paper aims to systematically review the latest research progress in GenIR. We will summarize the advancements in GR regarding model training, document identifier, incremental learning, downstream tasks adaptation, multi-modal GR and generative recommendation, as well as progress in reliable response generation in aspects of internal knowledge memorization, external knowledge augmentation, generating response with citations and personal information assistant. We also review the evaluation, challenges and future prospects in GenIR systems. This review aims to offer a comprehensive reference for researchers in the GenIR field, encouraging further development in this area.","sentences":["Information Retrieval (IR) systems are crucial tools for users to access information, widely applied in scenarios like search engines, question answering, and recommendation systems.","Traditional IR methods, based on similarity matching to return ranked lists of documents, have been reliable means of information acquisition, dominating the IR field for years.","With the advancement of pre-trained language models, generative information retrieval (GenIR) has emerged as a novel paradigm, gaining increasing attention in recent years.","Currently, research in GenIR can be categorized into two aspects: generative document retrieval (GR) and reliable response generation.","GR leverages the generative model's parameters for memorizing documents, enabling retrieval by directly generating relevant document identifiers without explicit indexing.","Reliable response generation, on the other hand, employs language models to directly generate the information users seek, breaking the limitations of traditional IR in terms of document granularity and relevance matching, offering more flexibility, efficiency, and creativity, thus better meeting practical needs.","This paper aims to systematically review the latest research progress in GenIR.","We will summarize the advancements in GR regarding model training, document identifier, incremental learning, downstream tasks adaptation, multi-modal GR and generative recommendation, as well as progress in reliable response generation in aspects of internal knowledge memorization, external knowledge augmentation, generating response with citations and personal information assistant.","We also review the evaluation, challenges and future prospects in GenIR systems.","This review aims to offer a comprehensive reference for researchers in the GenIR field, encouraging further development in this area."],"url":"http://arxiv.org/abs/2404.14851v1","category":"cs.IR"}
{"created":"2024-04-23 08:52:41","title":"Beyond Trial-and-Error: Predicting User Abandonment After a Moderation Intervention","abstract":"Current content moderation practices follow the \\textit{trial-and-error} approach, meaning that moderators apply sequences of interventions until they obtain the desired outcome. However, being able to preemptively estimate the effects of an intervention would allow moderators the unprecedented opportunity to plan their actions ahead of application. As a first step towards this goal, here we propose and tackle the novel task of predicting the effect of a moderation intervention. We study the reactions of 16,540 users to a massive ban of online communities on Reddit, training a set of binary classifiers to identify those users who would abandon the platform after the intervention -- a problem of great practical relevance. We leverage a dataset of 13.8M posts to compute a large and diverse set of 142 features, which convey information about the activity, toxicity, relations, and writing style of the users. We obtain promising results, with the best-performing model achieving \\textit{micro F1} $= 0.800$ and \\textit{macro F1} $= 0.676$. Our model demonstrates robust generalizability when applied to users from previously unseen communities. Furthermore, we identify activity features as the most informative predictors, followed by relational and toxicity features, while writing style features exhibit limited utility. Our results demonstrate the feasibility of predicting the effects of a moderation intervention, paving the way for a new research direction in predictive content moderation aimed at empowering moderators with intelligent tools to plan ahead their actions.","sentences":["Current content moderation practices follow the \\textit{trial-and-error} approach, meaning that moderators apply sequences of interventions until they obtain the desired outcome.","However, being able to preemptively estimate the effects of an intervention would allow moderators the unprecedented opportunity to plan their actions ahead of application.","As a first step towards this goal, here we propose and tackle the novel task of predicting the effect of a moderation intervention.","We study the reactions of 16,540 users to a massive ban of online communities on Reddit, training a set of binary classifiers to identify those users who would abandon the platform after the intervention -- a problem of great practical relevance.","We leverage a dataset of 13.8M posts to compute a large and diverse set of 142 features, which convey information about the activity, toxicity, relations, and writing style of the users.","We obtain promising results, with the best-performing model achieving \\textit{micro F1} $= 0.800$ and \\textit{macro F1} $= 0.676$. Our model demonstrates robust generalizability when applied to users from previously unseen communities.","Furthermore, we identify activity features as the most informative predictors, followed by relational and toxicity features, while writing style features exhibit limited utility.","Our results demonstrate the feasibility of predicting the effects of a moderation intervention, paving the way for a new research direction in predictive content moderation aimed at empowering moderators with intelligent tools to plan ahead their actions."],"url":"http://arxiv.org/abs/2404.14846v1","category":"cs.CY"}
{"created":"2024-04-23 08:32:38","title":"CoProNN: Concept-based Prototypical Nearest Neighbors for Explaining Vision Models","abstract":"Mounting evidence in explainability for artificial intelligence (XAI) research suggests that good explanations should be tailored to individual tasks and should relate to concepts relevant to the task. However, building task specific explanations is time consuming and requires domain expertise which can be difficult to integrate into generic XAI methods. A promising approach towards designing useful task specific explanations with domain experts is based on compositionality of semantic concepts. Here, we present a novel approach that enables domain experts to quickly create concept-based explanations for computer vision tasks intuitively via natural language. Leveraging recent progress in deep generative methods we propose to generate visual concept-based prototypes via text-to-image methods. These prototypes are then used to explain predictions of computer vision models via a simple k-Nearest-Neighbors routine. The modular design of CoProNN is simple to implement, it is straightforward to adapt to novel tasks and allows for replacing the classification and text-to-image models as more powerful models are released. The approach can be evaluated offline against the ground-truth of predefined prototypes that can be easily communicated also to domain experts as they are based on visual concepts. We show that our strategy competes very well with other concept-based XAI approaches on coarse grained image classification tasks and may even outperform those methods on more demanding fine grained tasks. We demonstrate the effectiveness of our method for human-machine collaboration settings in qualitative and quantitative user studies. All code and experimental data can be found in our GitHub $\\href{https://github.com/TeodorChiaburu/beexplainable}{repository}$.","sentences":["Mounting evidence in explainability for artificial intelligence (XAI) research suggests that good explanations should be tailored to individual tasks and should relate to concepts relevant to the task.","However, building task specific explanations is time consuming and requires domain expertise which can be difficult to integrate into generic XAI methods.","A promising approach towards designing useful task specific explanations with domain experts is based on compositionality of semantic concepts.","Here, we present a novel approach that enables domain experts to quickly create concept-based explanations for computer vision tasks intuitively via natural language.","Leveraging recent progress in deep generative methods we propose to generate visual concept-based prototypes via text-to-image methods.","These prototypes are then used to explain predictions of computer vision models via a simple k-Nearest-Neighbors routine.","The modular design of CoProNN is simple to implement, it is straightforward to adapt to novel tasks and allows for replacing the classification and text-to-image models as more powerful models are released.","The approach can be evaluated offline against the ground-truth of predefined prototypes that can be easily communicated also to domain experts as they are based on visual concepts.","We show that our strategy competes very well with other concept-based XAI approaches on coarse grained image classification tasks and may even outperform those methods on more demanding fine grained tasks.","We demonstrate the effectiveness of our method for human-machine collaboration settings in qualitative and quantitative user studies.","All code and experimental data can be found in our GitHub $\\href{https://github.com/TeodorChiaburu/beexplainable}{repository}$."],"url":"http://arxiv.org/abs/2404.14830v1","category":"cs.CV"}
{"created":"2024-04-23 08:20:18","title":"In industrial embedded software, are some compilation errors easier to localize and fix than others?","abstract":"Industrial embedded systems often require specialized hardware. However, software engineers have access to such domain-specific hardware only at the continuous integration (CI) stage and have to use simulated hardware otherwise. This results in a higher proportion of compilation errors at the CI stage than in other types of systems, warranting a deeper study.   To this end, we create a CI diagnostics solution called ``Shadow Job'' that analyzes our industrial CI system. We collected over 40000 builds from 4 projects from the product source code and categorized the compilation errors into 14 error types, showing that the five most common ones comprise 89 % of all compilation errors. Additionally, we analyze the resolution time, size, and distance for each error type, to see if different types of compilation errors are easier to localize or repair than others.   Our results show that the resolution time, size, and distance are independent of each other. Our research also provides insights into the human effort required to fix the most common industrial compilation errors. We also identify the most promising directions for future research on fault localization.","sentences":["Industrial embedded systems often require specialized hardware.","However, software engineers have access to such domain-specific hardware only at the continuous integration (CI) stage and have to use simulated hardware otherwise.","This results in a higher proportion of compilation errors at the CI stage than in other types of systems, warranting a deeper study.   ","To this end, we create a CI diagnostics solution called ``Shadow Job'' that analyzes our industrial CI system.","We collected over 40000 builds from 4 projects from the product source code and categorized the compilation errors into 14 error types, showing that the five most common ones comprise 89 % of all compilation errors.","Additionally, we analyze the resolution time, size, and distance for each error type, to see if different types of compilation errors are easier to localize or repair than others.   ","Our results show that the resolution time, size, and distance are independent of each other.","Our research also provides insights into the human effort required to fix the most common industrial compilation errors.","We also identify the most promising directions for future research on fault localization."],"url":"http://arxiv.org/abs/2404.14823v1","category":"cs.SE"}
{"created":"2024-04-23 08:19:08","title":"CNN2GNN: How to Bridge CNN with GNN","abstract":"Although the convolutional neural network (CNN) has achieved excellent performance in vision tasks by extracting the intra-sample representation, it will take a higher training expense because of stacking numerous convolutional layers. Recently, as the bilinear models, graph neural networks (GNN) have succeeded in exploring the underlying topological relationship among the graph data with a few graph neural layers. Unfortunately, it cannot be directly utilized on non-graph data due to the lack of graph structure and has high inference latency on large-scale scenarios. Inspired by these complementary strengths and weaknesses, \\textit{we discuss a natural question, how to bridge these two heterogeneous networks?} In this paper, we propose a novel CNN2GNN framework to unify CNN and GNN together via distillation. Firstly, to break the limitations of GNN, a differentiable sparse graph learning module is designed as the head of networks to dynamically learn the graph for inductive learning. Then, a response-based distillation is introduced to transfer the knowledge from CNN to GNN and bridge these two heterogeneous networks. Notably, due to extracting the intra-sample representation of a single instance and the topological relationship among the datasets simultaneously, the performance of distilled ``boosted'' two-layer GNN on Mini-ImageNet is much higher than CNN containing dozens of layers such as ResNet152.","sentences":["Although the convolutional neural network (CNN) has achieved excellent performance in vision tasks by extracting the intra-sample representation, it will take a higher training expense because of stacking numerous convolutional layers.","Recently, as the bilinear models, graph neural networks (GNN) have succeeded in exploring the underlying topological relationship among the graph data with a few graph neural layers.","Unfortunately, it cannot be directly utilized on non-graph data due to the lack of graph structure and has high inference latency on large-scale scenarios.","Inspired by these complementary strengths and weaknesses, \\textit{we discuss a natural question, how to bridge these two heterogeneous networks?}","In this paper, we propose a novel CNN2GNN framework to unify CNN and GNN together via distillation.","Firstly, to break the limitations of GNN, a differentiable sparse graph learning module is designed as the head of networks to dynamically learn the graph for inductive learning.","Then, a response-based distillation is introduced to transfer the knowledge from CNN to GNN and bridge these two heterogeneous networks.","Notably, due to extracting the intra-sample representation of a single instance and the topological relationship among the datasets simultaneously, the performance of distilled ``boosted'' two-layer GNN on Mini-ImageNet is much higher than CNN containing dozens of layers such as ResNet152."],"url":"http://arxiv.org/abs/2404.14822v1","category":"cs.CV"}
{"created":"2024-04-23 08:13:17","title":"Bathymetric Surveying with Imaging Sonar Using Neural Volume Rendering","abstract":"This research addresses the challenge of estimating bathymetry from imaging sonars where the state-of-the-art works have primarily relied on either supervised learning with ground-truth labels or surface rendering based on the Lambertian assumption. In this letter, we propose a novel, self-supervised framework based on volume rendering for reconstructing bathymetry using forward-looking sonar (FLS) data collected during standard surveys. We represent the seafloor as a neural heightmap encapsulated with a parametric multi-resolution hash encoding scheme and model the sonar measurements with a differentiable renderer using sonar volumetric rendering employed with hierarchical sampling techniques. Additionally, we model the horizontal and vertical beam patterns and estimate them jointly with the bathymetry. We evaluate the proposed method quantitatively on simulation and field data collected by remotely operated vehicles (ROVs) during low-altitude surveys. Results show that the proposed method outperforms the current state-of-the-art approaches that use imaging sonars for seabed mapping. We also demonstrate that the proposed approach can potentially be used to increase the resolution of a low-resolution prior map with FLS data from low-altitude surveys.","sentences":["This research addresses the challenge of estimating bathymetry from imaging sonars where the state-of-the-art works have primarily relied on either supervised learning with ground-truth labels or surface rendering based on the Lambertian assumption.","In this letter, we propose a novel, self-supervised framework based on volume rendering for reconstructing bathymetry using forward-looking sonar (FLS) data collected during standard surveys.","We represent the seafloor as a neural heightmap encapsulated with a parametric multi-resolution hash encoding scheme and model the sonar measurements with a differentiable renderer using sonar volumetric rendering employed with hierarchical sampling techniques.","Additionally, we model the horizontal and vertical beam patterns and estimate them jointly with the bathymetry.","We evaluate the proposed method quantitatively on simulation and field data collected by remotely operated vehicles (ROVs) during low-altitude surveys.","Results show that the proposed method outperforms the current state-of-the-art approaches that use imaging sonars for seabed mapping.","We also demonstrate that the proposed approach can potentially be used to increase the resolution of a low-resolution prior map with FLS data from low-altitude surveys."],"url":"http://arxiv.org/abs/2404.14819v1","category":"cs.RO"}
{"created":"2024-04-23 07:39:24","title":"A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications","abstract":"A graph is a fundamental data model to represent various entities and their complex relationships in society and nature, such as social networks, transportation networks, financial networks, and biomedical systems. Recently, large language models (LLMs) have showcased a strong generalization ability to handle various NLP and multi-mode tasks to answer users' arbitrary questions and specific-domain content generation. Compared with graph learning models, LLMs enjoy superior advantages in addressing the challenges of generalizing graph tasks by eliminating the need for training graph learning models and reducing the cost of manual annotation. In this survey, we conduct a comprehensive investigation of existing LLM studies on graph data, which summarizes the relevant graph analytics tasks solved by advanced LLM models and points out the existing remaining challenges and future directions. Specifically, we study the key problems of LLM-based generative graph analytics (LLM-GGA) with three categories: LLM-based graph query processing (LLM-GQP), LLM-based graph inference and learning (LLM-GIL), and graph-LLM-based applications. LLM-GQP focuses on an integration of graph analytics techniques and LLM prompts, including graph understanding and knowledge graph (KG) based augmented retrieval, while LLM-GIL focuses on learning and reasoning over graphs, including graph learning, graph-formed reasoning and graph representation. We summarize the useful prompts incorporated into LLM to handle different graph downstream tasks. Moreover, we give a summary of LLM model evaluation, benchmark datasets/tasks, and a deep pro and cons analysis of LLM models. We also explore open problems and future directions in this exciting interdisciplinary research area of LLMs and graph analytics.","sentences":["A graph is a fundamental data model to represent various entities and their complex relationships in society and nature, such as social networks, transportation networks, financial networks, and biomedical systems.","Recently, large language models (LLMs) have showcased a strong generalization ability to handle various NLP and multi-mode tasks to answer users' arbitrary questions and specific-domain content generation.","Compared with graph learning models, LLMs enjoy superior advantages in addressing the challenges of generalizing graph tasks by eliminating the need for training graph learning models and reducing the cost of manual annotation.","In this survey, we conduct a comprehensive investigation of existing LLM studies on graph data, which summarizes the relevant graph analytics tasks solved by advanced LLM models and points out the existing remaining challenges and future directions.","Specifically, we study the key problems of LLM-based generative graph analytics (LLM-GGA) with three categories: LLM-based graph query processing (LLM-GQP), LLM-based graph inference and learning (LLM-GIL), and graph-LLM-based applications.","LLM-GQP focuses on an integration of graph analytics techniques and LLM prompts, including graph understanding and knowledge graph (KG) based augmented retrieval, while LLM-GIL focuses on learning and reasoning over graphs, including graph learning, graph-formed reasoning and graph representation.","We summarize the useful prompts incorporated into LLM to handle different graph downstream tasks.","Moreover, we give a summary of LLM model evaluation, benchmark datasets/tasks, and a deep pro and cons analysis of LLM models.","We also explore open problems and future directions in this exciting interdisciplinary research area of LLMs and graph analytics."],"url":"http://arxiv.org/abs/2404.14809v1","category":"cs.CL"}
{"created":"2024-04-23 07:14:12","title":"Numerical simulations of the region of possible sprite inception in the mesosphere above winter thunderstorms under wind shear","abstract":"Transient luminous events (TLEs) is the collective name given to mesospheric electrical breakdown phenomena occurring in conjunction with strong lightning discharges in tropospheric thunderstorms. They include elves, sprites, haloes and jets, and are characterized by short lived optical emissions, mostly of red (665 nm) and blue (337 nm) wavelengths. Sprites are caused by the brief quasi-electrostatic field induced in the mesosphere, mostly after the removal of the upper positive charge of the thundercloud by a +CG, and they have been recorded above most of the lightning activity centers on Earth. In wintertime, there are just a few areas where lightning occurs, and of those, sprites have been observed over the Sea of Japan, the British Channel, and the Mediterranean Sea. Unlike their summer counterparts, winter thunderstorms tend to have weaker updrafts and as a result, reduced vertical dimensions and compact charge structures, whose positive and negative centers are located at lower altitudes. These storms are often susceptible to significant wind shear and as a result may exhibit a tilted dipole charge structure and a lateral offset of the upper positive charge relative to the main negative charge. We present results of numerical simulations using a three-dimensional explicit formulation of the mesospheric electrostatic electrical field following a lightning discharge from a typical mid-latitude winter thunderstorm exhibiting tilt due to wind shear and evaluate the regions of possible sprite inception. Our results show, as numerous observations suggest, that sprites can be shifted a large distance from the location of the parent +CG in the direction of the shear and will occur over a larger region compared with non-sheared storms.","sentences":["Transient luminous events (TLEs) is the collective name given to mesospheric electrical breakdown phenomena occurring in conjunction with strong lightning discharges in tropospheric thunderstorms.","They include elves, sprites, haloes and jets, and are characterized by short lived optical emissions, mostly of red (665 nm) and blue (337 nm) wavelengths.","Sprites are caused by the brief quasi-electrostatic field induced in the mesosphere, mostly after the removal of the upper positive charge of the thundercloud by a +CG, and they have been recorded above most of the lightning activity centers on Earth.","In wintertime, there are just a few areas where lightning occurs, and of those, sprites have been observed over the Sea of Japan, the British Channel, and the Mediterranean Sea.","Unlike their summer counterparts, winter thunderstorms tend to have weaker updrafts and as a result, reduced vertical dimensions and compact charge structures, whose positive and negative centers are located at lower altitudes.","These storms are often susceptible to significant wind shear and as a result may exhibit a tilted dipole charge structure and a lateral offset of the upper positive charge relative to the main negative charge.","We present results of numerical simulations using a three-dimensional explicit formulation of the mesospheric electrostatic electrical field following a lightning discharge from a typical mid-latitude winter thunderstorm exhibiting tilt due to wind shear and evaluate the regions of possible sprite inception.","Our results show, as numerous observations suggest, that sprites can be shifted a large distance from the location of the parent +CG in the direction of the shear and will occur over a larger region compared with non-sheared storms."],"url":"http://arxiv.org/abs/2404.14794v1","category":"physics.ao-ph"}
{"created":"2024-04-23 06:52:40","title":"LLM-Enhanced Causal Discovery in Temporal Domain from Interventional Data","abstract":"In the field of Artificial Intelligence for Information Technology Operations, causal discovery is pivotal for operation and maintenance of graph construction, facilitating downstream industrial tasks such as root cause analysis. Temporal causal discovery, as an emerging method, aims to identify temporal causal relationships between variables directly from observations by utilizing interventional data. However, existing methods mainly focus on synthetic datasets with heavy reliance on intervention targets and ignore the textual information hidden in real-world systems, failing to conduct causal discovery for real industrial scenarios. To tackle this problem, in this paper we propose to investigate temporal causal discovery in industrial scenarios, which faces two critical challenges: 1) how to discover causal relationships without the interventional targets that are costly to obtain in practice, and 2) how to discover causal relations via leveraging the textual information in systems which can be complex yet abundant in industrial contexts. To address these challenges, we propose the RealTCD framework, which is able to leverage domain knowledge to discover temporal causal relationships without interventional targets. Specifically, we first develop a score-based temporal causal discovery method capable of discovering causal relations for root cause analysis without relying on interventional targets through strategic masking and regularization. Furthermore, by employing Large Language Models (LLMs) to handle texts and integrate domain knowledge, we introduce LLM-guided meta-initialization to extract the meta-knowledge from textual information hidden in systems to boost the quality of discovery. We conduct extensive experiments on simulation and real-world datasets to show the superiority of our proposed RealTCD framework over existing baselines in discovering temporal causal structures.","sentences":["In the field of Artificial Intelligence for Information Technology Operations, causal discovery is pivotal for operation and maintenance of graph construction, facilitating downstream industrial tasks such as root cause analysis.","Temporal causal discovery, as an emerging method, aims to identify temporal causal relationships between variables directly from observations by utilizing interventional data.","However, existing methods mainly focus on synthetic datasets with heavy reliance on intervention targets and ignore the textual information hidden in real-world systems, failing to conduct causal discovery for real industrial scenarios.","To tackle this problem, in this paper we propose to investigate temporal causal discovery in industrial scenarios, which faces two critical challenges: 1) how to discover causal relationships without the interventional targets that are costly to obtain in practice, and 2) how to discover causal relations via leveraging the textual information in systems which can be complex yet abundant in industrial contexts.","To address these challenges, we propose the RealTCD framework, which is able to leverage domain knowledge to discover temporal causal relationships without interventional targets.","Specifically, we first develop a score-based temporal causal discovery method capable of discovering causal relations for root cause analysis without relying on interventional targets through strategic masking and regularization.","Furthermore, by employing Large Language Models (LLMs) to handle texts and integrate domain knowledge, we introduce LLM-guided meta-initialization to extract the meta-knowledge from textual information hidden in systems to boost the quality of discovery.","We conduct extensive experiments on simulation and real-world datasets to show the superiority of our proposed RealTCD framework over existing baselines in discovering temporal causal structures."],"url":"http://arxiv.org/abs/2404.14786v1","category":"cs.AI"}
{"created":"2024-04-23 06:33:05","title":"Channel Estimation for Optical Intelligent Reflecting Surface-Assisted VLC System: A Joint Space-Time Sampling Approach","abstract":"Optical intelligent reflecting surface (OIRS) has attracted increasing attention due to its capability of overcoming signal blockages in visible light communication (VLC), an emerging technology for the next-generation advanced transceivers. However, current works on OIRS predominantly assume known channel state information (CSI), which is essential to practical OIRS configuration. To bridge such a gap, this paper proposes a new and customized channel estimation protocol for OIRSs under the alignment-based channel model. Specifically, we first unveil OIRS spatial and temporal coherence characteristics and derive the coherence distance and the coherence time in closed form. Next, to achieve fast beam alignment over different coherence time, we propose to dynamically tune the rotational angles of the OIRS reflecting elements following a geometric optics-based non-uniform codebook. Given the above beam alignment, we propose an efficient joint space-time sampling-based algorithm to estimate the OIRS channel. In particular, we divide the OIRS into multiple subarrays based on the coherence distance and sequentially estimate their associated CSI, followed by a spacetime interpolation to retrieve full CSI for other non-aligned transceiver antennas. Numerical results validate our theoretical analyses and demonstrate the efficacy of our proposed OIRS channel estimation scheme as compared to other benchmark schemes.","sentences":["Optical intelligent reflecting surface (OIRS) has attracted increasing attention due to its capability of overcoming signal blockages in visible light communication (VLC), an emerging technology for the next-generation advanced transceivers.","However, current works on OIRS predominantly assume known channel state information (CSI), which is essential to practical OIRS configuration.","To bridge such a gap, this paper proposes a new and customized channel estimation protocol for OIRSs under the alignment-based channel model.","Specifically, we first unveil OIRS spatial and temporal coherence characteristics and derive the coherence distance and the coherence time in closed form.","Next, to achieve fast beam alignment over different coherence time, we propose to dynamically tune the rotational angles of the OIRS reflecting elements following a geometric optics-based non-uniform codebook.","Given the above beam alignment, we propose an efficient joint space-time sampling-based algorithm to estimate the OIRS channel.","In particular, we divide the OIRS into multiple subarrays based on the coherence distance and sequentially estimate their associated CSI, followed by a spacetime interpolation to retrieve full CSI for other non-aligned transceiver antennas.","Numerical results validate our theoretical analyses and demonstrate the efficacy of our proposed OIRS channel estimation scheme as compared to other benchmark schemes."],"url":"http://arxiv.org/abs/2404.14778v1","category":"cs.IT"}
{"created":"2024-04-23 06:22:19","title":"Music Style Transfer With Diffusion Model","abstract":"Previous studies on music style transfer have mainly focused on one-to-one style conversion, which is relatively limited. When considering the conversion between multiple styles, previous methods required designing multiple modes to disentangle the complex style of the music, resulting in large computational costs and slow audio generation. The existing music style transfer methods generate spectrograms with artifacts, leading to significant noise in the generated audio. To address these issues, this study proposes a music style transfer framework based on diffusion models (DM) and uses spectrogram-based methods to achieve multi-to-multi music style transfer. The GuideDiff method is used to restore spectrograms to high-fidelity audio, accelerating audio generation speed and reducing noise in the generated audio. Experimental results show that our model has good performance in multi-mode music style transfer compared to the baseline and can generate high-quality audio in real-time on consumer-grade GPUs.","sentences":["Previous studies on music style transfer have mainly focused on one-to-one style conversion, which is relatively limited.","When considering the conversion between multiple styles, previous methods required designing multiple modes to disentangle the complex style of the music, resulting in large computational costs and slow audio generation.","The existing music style transfer methods generate spectrograms with artifacts, leading to significant noise in the generated audio.","To address these issues, this study proposes a music style transfer framework based on diffusion models (DM) and uses spectrogram-based methods to achieve multi-to-multi music style transfer.","The GuideDiff method is used to restore spectrograms to high-fidelity audio, accelerating audio generation speed and reducing noise in the generated audio.","Experimental results show that our model has good performance in multi-mode music style transfer compared to the baseline and can generate high-quality audio in real-time on consumer-grade GPUs."],"url":"http://arxiv.org/abs/2404.14771v1","category":"cs.SD"}
{"created":"2024-04-23 05:56:35","title":"Evolutionary Reinforcement Learning via Cooperative Coevolution","abstract":"Recently, evolutionary reinforcement learning has obtained much attention in various domains. Maintaining a population of actors, evolutionary reinforcement learning utilises the collected experiences to improve the behaviour policy through efficient exploration. However, the poor scalability of genetic operators limits the efficiency of optimising high-dimensional neural networks. To address this issue, this paper proposes a novel cooperative coevolutionary reinforcement learning (CoERL) algorithm. Inspired by cooperative coevolution, CoERL periodically and adaptively decomposes the policy optimisation problem into multiple subproblems and evolves a population of neural networks for each of the subproblems. Instead of using genetic operators, CoERL directly searches for partial gradients to update the policy. Updating policy with partial gradients maintains consistency between the behaviour spaces of parents and offspring across generations. The experiences collected by the population are then used to improve the entire policy, which enhances the sampling efficiency. Experiments on six benchmark locomotion tasks demonstrate that CoERL outperforms seven state-of-the-art algorithms and baselines. Ablation study verifies the unique contribution of CoERL's core ingredients.","sentences":["Recently, evolutionary reinforcement learning has obtained much attention in various domains.","Maintaining a population of actors, evolutionary reinforcement learning utilises the collected experiences to improve the behaviour policy through efficient exploration.","However, the poor scalability of genetic operators limits the efficiency of optimising high-dimensional neural networks.","To address this issue, this paper proposes a novel cooperative coevolutionary reinforcement learning (CoERL) algorithm.","Inspired by cooperative coevolution, CoERL periodically and adaptively decomposes the policy optimisation problem into multiple subproblems and evolves a population of neural networks for each of the subproblems.","Instead of using genetic operators, CoERL directly searches for partial gradients to update the policy.","Updating policy with partial gradients maintains consistency between the behaviour spaces of parents and offspring across generations.","The experiences collected by the population are then used to improve the entire policy, which enhances the sampling efficiency.","Experiments on six benchmark locomotion tasks demonstrate that CoERL outperforms seven state-of-the-art algorithms and baselines.","Ablation study verifies the unique contribution of CoERL's core ingredients."],"url":"http://arxiv.org/abs/2404.14763v1","category":"cs.NE"}
{"created":"2024-04-23 05:56:12","title":"Dynamical properties of Fermi-Fermi mixtures of dipolar and non-dipolar atoms","abstract":"Dynamical properties of homogeneous Fermi-Fermi mixtures of dipolar and non-dipolar atoms are studied at zero temperature, where dipoles are polarized by an external field. We calculate the density-density correlation functions in a ring-diagram approximation and analyze the pole structure to obtain eigenfrequencies of collective excitations. We first determine stability phase diagrams for the mixtures available in experiments: $^{167}$Er-$^{173}$Yb, $^{167}$Er-$^{6}$Li, $^{161}$Dy-$^{173}$Yb, and $^{161}$Dy-$^{6}$Li systems, and show that the mixtures with larger mass imbalance tend to be more unstable. We then investigate the parameter dependence of an undamped zero sound with an anisotropic real dispersion relation in the stable phase for the $^{161}$Dy-$^{173}$Yb mixture, and the speed of sound exhibits a critical angle of possible propagation with respect to the dipole polarization direction, above which the sound mode disappears in the particle-hole continuum. Since the sound mode is a coherent superposition of density fluctuations of dipolar and non-dipolar atoms, the existence of the sound mode, e.g., the value of the critical angle, is significantly affected by the inter-particle interaction through the density-density correlation between dipolar and non-dipolar atoms. We have also observed such an effect of the inter-particle interaction in the study of a linear response of density fluctuations to an external perturbation.","sentences":["Dynamical properties of homogeneous Fermi-Fermi mixtures of dipolar and non-dipolar atoms are studied at zero temperature, where dipoles are polarized by an external field.","We calculate the density-density correlation functions in a ring-diagram approximation and analyze the pole structure to obtain eigenfrequencies of collective excitations.","We first determine stability phase diagrams for the mixtures available in experiments: $^{167}$Er-$^{173}$Yb, $^{167}$Er-$^{6}$Li, $^{161}$Dy-$^{173}$Yb, and $^{161}$Dy-$^{6}$Li systems, and show that the mixtures with larger mass imbalance tend to be more unstable.","We then investigate the parameter dependence of an undamped zero sound with an anisotropic real dispersion relation in the stable phase for the $^{161}$Dy-$^{173}$Yb mixture, and the speed of sound exhibits a critical angle of possible propagation with respect to the dipole polarization direction, above which the sound mode disappears in the particle-hole continuum.","Since the sound mode is a coherent superposition of density fluctuations of dipolar and non-dipolar atoms, the existence of the sound mode, e.g., the value of the critical angle, is significantly affected by the inter-particle interaction through the density-density correlation between dipolar and non-dipolar atoms.","We have also observed such an effect of the inter-particle interaction in the study of a linear response of density fluctuations to an external perturbation."],"url":"http://arxiv.org/abs/2404.14762v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-23 05:51:45","title":"Retrieval Augmented Generation for Domain-specific Question Answering","abstract":"Question answering (QA) has become an important application in the advanced development of large language models. General pre-trained large language models for question-answering are not trained to properly understand the knowledge or terminology for a specific domain, such as finance, healthcare, education, and customer service for a product. To better cater to domain-specific understanding, we build an in-house question-answering system for Adobe products. We propose a novel framework to compile a large question-answer database and develop the approach for retrieval-aware finetuning of a Large Language model. We showcase that fine-tuning the retriever leads to major improvements in the final generation. Our overall approach reduces hallucinations during generation while keeping in context the latest retrieval information for contextual grounding.","sentences":["Question answering (QA) has become an important application in the advanced development of large language models.","General pre-trained large language models for question-answering are not trained to properly understand the knowledge or terminology for a specific domain, such as finance, healthcare, education, and customer service for a product.","To better cater to domain-specific understanding, we build an in-house question-answering system for Adobe products.","We propose a novel framework to compile a large question-answer database and develop the approach for retrieval-aware finetuning of a Large Language model.","We showcase that fine-tuning the retriever leads to major improvements in the final generation.","Our overall approach reduces hallucinations during generation while keeping in context the latest retrieval information for contextual grounding."],"url":"http://arxiv.org/abs/2404.14760v1","category":"cs.CL"}
{"created":"2024-04-23 05:43:44","title":"Integrating Mamba and Transformer for Long-Short Range Time Series Forecasting","abstract":"Time series forecasting is an important problem and plays a key role in a variety of applications including weather forecasting, stock market, and scientific simulations. Although transformers have proven to be effective in capturing dependency, its quadratic complexity of attention mechanism prevents its further adoption in long-range time series forecasting, thus limiting them attend to short-range range. Recent progress on state space models (SSMs) have shown impressive performance on modeling long range dependency due to their subquadratic complexity. Mamba, as a representative SSM, enjoys linear time complexity and has achieved strong scalability on tasks that requires scaling to long sequences, such as language, audio, and genomics. In this paper, we propose to leverage a hybrid framework Mambaformer that internally combines Mamba for long-range dependency, and Transformer for short range dependency, for long-short range forecasting. To the best of our knowledge, this is the first paper to combine Mamba and Transformer architecture in time series data. We investigate possible hybrid architectures to combine Mamba layer and attention layer for long-short range time series forecasting. The comparative study shows that the Mambaformer family can outperform Mamba and Transformer in long-short range time series forecasting problem. The code is available at https://github.com/XiongxiaoXu/Mambaformerin-Time-Series.","sentences":["Time series forecasting is an important problem and plays a key role in a variety of applications including weather forecasting, stock market, and scientific simulations.","Although transformers have proven to be effective in capturing dependency, its quadratic complexity of attention mechanism prevents its further adoption in long-range time series forecasting, thus limiting them attend to short-range range.","Recent progress on state space models (SSMs) have shown impressive performance on modeling long range dependency due to their subquadratic complexity.","Mamba, as a representative SSM, enjoys linear time complexity and has achieved strong scalability on tasks that requires scaling to long sequences, such as language, audio, and genomics.","In this paper, we propose to leverage a hybrid framework Mambaformer that internally combines Mamba for long-range dependency, and Transformer for short range dependency, for long-short range forecasting.","To the best of our knowledge, this is the first paper to combine Mamba and Transformer architecture in time series data.","We investigate possible hybrid architectures to combine Mamba layer and attention layer for long-short range time series forecasting.","The comparative study shows that the Mambaformer family can outperform Mamba and Transformer in long-short range time series forecasting problem.","The code is available at https://github.com/XiongxiaoXu/Mambaformerin-Time-Series."],"url":"http://arxiv.org/abs/2404.14757v1","category":"cs.LG"}
{"created":"2024-04-23 05:36:33","title":"SkinGEN: an Explainable Dermatology Diagnosis-to-Generation Framework with Interactive Vision-Language Models","abstract":"With the continuous advancement of vision language models (VLMs) technology, remarkable research achievements have emerged in the dermatology field, the fourth most prevalent human disease category. However, despite these advancements, VLM still faces \"hallucination\" in dermatological diagnosis, and due to the inherent complexity of dermatological conditions, existing tools offer relatively limited support for user comprehension. We propose SkinGEN, a diagnosis-to-generation framework that leverages the stable diffusion (SD) method to generate reference demonstrations from diagnosis results provided by VLM, thereby enhancing the visual explainability for users. Through extensive experiments with Low-Rank Adaptation (LoRA), we identify optimal strategies for skin condition image generation. We conduct a user study with 32 participants evaluating both the system performance and explainability. Results demonstrate that SkinGEN significantly improves users' comprehension of VLM predictions and fosters increased trust in the diagnostic process. This work paves the way for more transparent and user-centric VLM applications in dermatology and beyond.","sentences":["With the continuous advancement of vision language models (VLMs) technology, remarkable research achievements have emerged in the dermatology field, the fourth most prevalent human disease category.","However, despite these advancements, VLM still faces \"hallucination\" in dermatological diagnosis, and due to the inherent complexity of dermatological conditions, existing tools offer relatively limited support for user comprehension.","We propose SkinGEN, a diagnosis-to-generation framework that leverages the stable diffusion (SD) method to generate reference demonstrations from diagnosis results provided by VLM, thereby enhancing the visual explainability for users.","Through extensive experiments with Low-Rank Adaptation (LoRA), we identify optimal strategies for skin condition image generation.","We conduct a user study with 32 participants evaluating both the system performance and explainability.","Results demonstrate that SkinGEN significantly improves users' comprehension of VLM predictions and fosters increased trust in the diagnostic process.","This work paves the way for more transparent and user-centric VLM applications in dermatology and beyond."],"url":"http://arxiv.org/abs/2404.14755v1","category":"cs.MM"}
{"created":"2024-04-23 05:32:22","title":"Skip the Benchmark: Generating System-Level High-Level Synthesis Data using Generative Machine Learning","abstract":"High-Level Synthesis (HLS) Design Space Exploration (DSE) is a widely accepted approach for efficiently exploring Pareto-optimal and optimal hardware solutions during the HLS process. Several HLS benchmarks and datasets are available for the research community to evaluate their methodologies. Unfortunately, these resources are limited and may not be sufficient for complex, multi-component system-level explorations. Generating new data using existing HLS benchmarks can be cumbersome, given the expertise and time required to effectively generate data for different HLS designs and directives. As a result, synthetic data has been used in prior work to evaluate system-level HLS DSE. However, the fidelity of the synthetic data to real data is often unclear, leading to uncertainty about the quality of system-level HLS DSE. This paper proposes a novel approach, called Vaegan, that employs generative machine learning to generate synthetic data that is robust enough to support complex system-level HLS DSE experiments that would be unattainable with only the currently available data. We explore and adapt a Variational Autoencoder (VAE) and Generative Adversarial Network (GAN) for this task and evaluate our approach using state-of-the-art datasets and metrics. We compare our approach to prior works and show that Vaegan effectively generates synthetic HLS data that closely mirrors the ground truth's distribution.","sentences":["High-Level Synthesis (HLS) Design Space Exploration (DSE) is a widely accepted approach for efficiently exploring Pareto-optimal and optimal hardware solutions during the HLS process.","Several HLS benchmarks and datasets are available for the research community to evaluate their methodologies.","Unfortunately, these resources are limited and may not be sufficient for complex, multi-component system-level explorations.","Generating new data using existing HLS benchmarks can be cumbersome, given the expertise and time required to effectively generate data for different HLS designs and directives.","As a result, synthetic data has been used in prior work to evaluate system-level HLS DSE.","However, the fidelity of the synthetic data to real data is often unclear, leading to uncertainty about the quality of system-level HLS DSE.","This paper proposes a novel approach, called Vaegan, that employs generative machine learning to generate synthetic data that is robust enough to support complex system-level HLS DSE experiments that would be unattainable with only the currently available data.","We explore and adapt a Variational Autoencoder (VAE) and Generative Adversarial Network (GAN) for this task and evaluate our approach using state-of-the-art datasets and metrics.","We compare our approach to prior works and show that Vaegan effectively generates synthetic HLS data that closely mirrors the ground truth's distribution."],"url":"http://arxiv.org/abs/2404.14754v1","category":"cs.LG"}
{"created":"2024-04-23 05:16:24","title":"Grounded Knowledge-Enhanced Medical VLP for Chest X-Ray","abstract":"Medical vision-language pre-training has emerged as a promising approach for learning domain-general representations of medical image and text. Current algorithms that exploit the global and local alignment between medical image and text could however be marred by the redundant information in medical data. To address this issue, we propose a grounded knowledge-enhanced medical vision-language pre-training (GK-MVLP) framework for chest X-ray. In this framework, medical knowledge is grounded to the appropriate anatomical regions by using a transformer-based grounded knowledge-enhanced module for fine-grained alignment between anatomical region-level visual features and the textural features of medical knowledge. The performance of GK-MVLP is competitive with or exceeds the state of the art on downstream chest X-ray disease classification, disease localization, report generation, and medical visual question-answering tasks. Our results show the advantage of incorporating grounding mechanism to remove biases and improve the alignment between chest X-ray image and radiology report.","sentences":["Medical vision-language pre-training has emerged as a promising approach for learning domain-general representations of medical image and text.","Current algorithms that exploit the global and local alignment between medical image and text could however be marred by the redundant information in medical data.","To address this issue, we propose a grounded knowledge-enhanced medical vision-language pre-training (GK-MVLP) framework for chest X-ray.","In this framework, medical knowledge is grounded to the appropriate anatomical regions by using a transformer-based grounded knowledge-enhanced module for fine-grained alignment between anatomical region-level visual features and the textural features of medical knowledge.","The performance of GK-MVLP is competitive with or exceeds the state of the art on downstream chest X-ray disease classification, disease localization, report generation, and medical visual question-answering tasks.","Our results show the advantage of incorporating grounding mechanism to remove biases and improve the alignment between chest X-ray image and radiology report."],"url":"http://arxiv.org/abs/2404.14750v1","category":"cs.CV"}
{"created":"2024-04-23 04:57:44","title":"A Customer Level Fraudulent Activity Detection Benchmark for Enhancing Machine Learning Model Research and Evaluation","abstract":"In the field of fraud detection, the availability of comprehensive and privacy-compliant datasets is crucial for advancing machine learning research and developing effective anti-fraud systems. Traditional datasets often focus on transaction-level information, which, while useful, overlooks the broader context of customer behavior patterns that are essential for detecting sophisticated fraud schemes. The scarcity of such data, primarily due to privacy concerns, significantly hampers the development and testing of predictive models that can operate effectively at the customer level. Addressing this gap, our study introduces a benchmark that contains structured datasets specifically designed for customer-level fraud detection. The benchmark not only adheres to strict privacy guidelines to ensure user confidentiality but also provides a rich source of information by encapsulating customer-centric features. We have developed the benchmark that allows for the comprehensive evaluation of various machine learning models, facilitating a deeper understanding of their strengths and weaknesses in predicting fraudulent activities. Through this work, we seek to bridge the existing gap in data availability, offering researchers and practitioners a valuable resource that empowers the development of next-generation fraud detection techniques.","sentences":["In the field of fraud detection, the availability of comprehensive and privacy-compliant datasets is crucial for advancing machine learning research and developing effective anti-fraud systems.","Traditional datasets often focus on transaction-level information, which, while useful, overlooks the broader context of customer behavior patterns that are essential for detecting sophisticated fraud schemes.","The scarcity of such data, primarily due to privacy concerns, significantly hampers the development and testing of predictive models that can operate effectively at the customer level.","Addressing this gap, our study introduces a benchmark that contains structured datasets specifically designed for customer-level fraud detection.","The benchmark not only adheres to strict privacy guidelines to ensure user confidentiality but also provides a rich source of information by encapsulating customer-centric features.","We have developed the benchmark that allows for the comprehensive evaluation of various machine learning models, facilitating a deeper understanding of their strengths and weaknesses in predicting fraudulent activities.","Through this work, we seek to bridge the existing gap in data availability, offering researchers and practitioners a valuable resource that empowers the development of next-generation fraud detection techniques."],"url":"http://arxiv.org/abs/2404.14746v1","category":"cs.LG"}
{"created":"2024-04-23 04:47:22","title":"Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering","abstract":"To address the issue of insufficient knowledge and the tendency to generate hallucination in Large Language Models (LLMs), numerous studies have endeavored to integrate LLMs with Knowledge Graphs (KGs). However, all these methods are evaluated on conventional Knowledge Graph Question Answering (KGQA) with complete KGs, where the factual triples involved in each question are entirely covered by the given KG. In this situation, LLM mainly acts as an agent to find answer entities by exploring the KG, rather than effectively integrating internal and external knowledge sources. However, in real-world scenarios, KGs are often incomplete to cover all the knowledge required to answer questions. To simulate real-world scenarios and evaluate the ability of LLMs to integrate internal and external knowledge, in this paper, we propose leveraging LLMs for QA under Incomplete Knowledge Graph (IKGQA), where the given KG doesn't include all the factual triples involved in each question. To handle IKGQA, we propose a training-free method called Generate-on-Graph (GoG) that can generate new factual triples while exploring on KGs. Specifically, we propose a selecting-generating-answering framework, which not only treat the LLM as an agent to explore on KGs, but also treat it as a KG to generate new facts based on the explored subgraph and its inherent knowledge. Experimental results on two datasets demonstrate that our GoG can solve IKGQA to a certain extent, while almost all previous methods cannot perform well on IKGQA.","sentences":["To address the issue of insufficient knowledge and the tendency to generate hallucination in Large Language Models (LLMs), numerous studies have endeavored to integrate LLMs with Knowledge Graphs (KGs).","However, all these methods are evaluated on conventional Knowledge Graph Question Answering (KGQA) with complete KGs, where the factual triples involved in each question are entirely covered by the given KG.","In this situation, LLM mainly acts as an agent to find answer entities by exploring the KG, rather than effectively integrating internal and external knowledge sources.","However, in real-world scenarios, KGs are often incomplete to cover all the knowledge required to answer questions.","To simulate real-world scenarios and evaluate the ability of LLMs to integrate internal and external knowledge, in this paper, we propose leveraging LLMs for QA under Incomplete Knowledge Graph (IKGQA), where the given KG doesn't include all the factual triples involved in each question.","To handle IKGQA, we propose a training-free method called Generate-on-Graph (GoG) that can generate new factual triples while exploring on KGs.","Specifically, we propose a selecting-generating-answering framework, which not only treat the LLM as an agent to explore on KGs, but also treat it as a KG to generate new facts based on the explored subgraph and its inherent knowledge.","Experimental results on two datasets demonstrate that our GoG can solve IKGQA to a certain extent, while almost all previous methods cannot perform well on IKGQA."],"url":"http://arxiv.org/abs/2404.14741v1","category":"cs.CL"}
{"created":"2024-04-23 04:33:49","title":"Qualitative Approaches to Voice UX","abstract":"Voice is a natural mode of expression offered by modern computer-based systems. Qualitative perspectives on voice-based user experiences (voice UX) offer rich descriptions of complex interactions that numbers alone cannot fully represent. We conducted a systematic review of the literature on qualitative approaches to voice UX, capturing the nature of this body of work in a systematic map and offering a qualitative synthesis of findings. We highlight the benefits of qualitative methods for voice UX research, identify opportunities for increasing rigour in methods and outcomes, and distill patterns of experience across a diversity of devices and modes of qualitative praxis.","sentences":["Voice is a natural mode of expression offered by modern computer-based systems.","Qualitative perspectives on voice-based user experiences (voice UX) offer rich descriptions of complex interactions that numbers alone cannot fully represent.","We conducted a systematic review of the literature on qualitative approaches to voice UX, capturing the nature of this body of work in a systematic map and offering a qualitative synthesis of findings.","We highlight the benefits of qualitative methods for voice UX research, identify opportunities for increasing rigour in methods and outcomes, and distill patterns of experience across a diversity of devices and modes of qualitative praxis."],"url":"http://arxiv.org/abs/2404.14736v1","category":"cs.HC"}
{"created":"2024-04-23 04:31:30","title":"Rank2Reward: Learning Shaped Reward Functions from Passive Video","abstract":"Teaching robots novel skills with demonstrations via human-in-the-loop data collection techniques like kinesthetic teaching or teleoperation puts a heavy burden on human supervisors. In contrast to this paradigm, it is often significantly easier to provide raw, action-free visual data of tasks being performed. Moreover, this data can even be mined from video datasets or the web. Ideally, this data can serve to guide robot learning for new tasks in novel environments, informing both \"what\" to do and \"how\" to do it. A powerful way to encode both the \"what\" and the \"how\" is to infer a well-shaped reward function for reinforcement learning. The challenge is determining how to ground visual demonstration inputs into a well-shaped and informative reward function. We propose a technique Rank2Reward for learning behaviors from videos of tasks being performed without access to any low-level states and actions. We do so by leveraging the videos to learn a reward function that measures incremental \"progress\" through a task by learning how to temporally rank the video frames in a demonstration. By inferring an appropriate ranking, the reward function is able to guide reinforcement learning by indicating when task progress is being made. This ranking function can be integrated into an adversarial imitation learning scheme resulting in an algorithm that can learn behaviors without exploiting the learned reward function. We demonstrate the effectiveness of Rank2Reward at learning behaviors from raw video on a number of tabletop manipulation tasks in both simulations and on a real-world robotic arm. We also demonstrate how Rank2Reward can be easily extended to be applicable to web-scale video datasets.","sentences":["Teaching robots novel skills with demonstrations via human-in-the-loop data collection techniques like kinesthetic teaching or teleoperation puts a heavy burden on human supervisors.","In contrast to this paradigm, it is often significantly easier to provide raw, action-free visual data of tasks being performed.","Moreover, this data can even be mined from video datasets or the web.","Ideally, this data can serve to guide robot learning for new tasks in novel environments, informing both \"what\" to do and \"how\" to do it.","A powerful way to encode both the \"what\" and the \"how\" is to infer a well-shaped reward function for reinforcement learning.","The challenge is determining how to ground visual demonstration inputs into a well-shaped and informative reward function.","We propose a technique Rank2Reward for learning behaviors from videos of tasks being performed without access to any low-level states and actions.","We do so by leveraging the videos to learn a reward function that measures incremental \"progress\" through a task by learning how to temporally rank the video frames in a demonstration.","By inferring an appropriate ranking, the reward function is able to guide reinforcement learning by indicating when task progress is being made.","This ranking function can be integrated into an adversarial imitation learning scheme resulting in an algorithm that can learn behaviors without exploiting the learned reward function.","We demonstrate the effectiveness of Rank2Reward at learning behaviors from raw video on a number of tabletop manipulation tasks in both simulations and on a real-world robotic arm.","We also demonstrate how Rank2Reward can be easily extended to be applicable to web-scale video datasets."],"url":"http://arxiv.org/abs/2404.14735v1","category":"cs.RO"}
{"created":"2024-04-23 04:22:42","title":"A figure-of-merit-based framework to evaluate photovoltaic materials","abstract":"I propose a general quantitative framework to evaluate the quality, track the historical development, and guide future optimization of photovoltaic (PV) absorbers at any development level, including both experimentally synthesized and computer-simulated materials. The framework is based on a PV figure of merit designed to include efficiency limitations due to imperfect photocarrier collection that are not included in classic detailed balance methods. Figure-of-merit-driven efficiency limits including collection losses are calculated for 28 experimentally synthesized PV absorbers and 9 PV absorbers simulated by electronic structure methods. Among emerging absorbers for which no working solar cells have yet been reported, there are very large differences in their likelihood to achieve high PV efficiencies.","sentences":["I propose a general quantitative framework to evaluate the quality, track the historical development, and guide future optimization of photovoltaic (PV) absorbers at any development level, including both experimentally synthesized and computer-simulated materials.","The framework is based on a PV figure of merit designed to include efficiency limitations due to imperfect photocarrier collection that are not included in classic detailed balance methods.","Figure-of-merit-driven efficiency limits including collection losses are calculated for 28 experimentally synthesized PV absorbers and 9 PV absorbers simulated by electronic structure methods.","Among emerging absorbers for which no working solar cells have yet been reported, there are very large differences in their likelihood to achieve high PV efficiencies."],"url":"http://arxiv.org/abs/2404.14732v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-23 03:42:48","title":"Bayesian Example Selection Improves In-Context Learning for Speech, Text, and Visual Modalities","abstract":"Large language models (LLMs) can adapt to new tasks through in-context learning (ICL) based on a few examples presented in dialogue history without any model parameter update. Despite such convenience, the performance of ICL heavily depends on the quality of the in-context examples presented, which makes the in-context example selection approach a critical choice. This paper proposes a novel Bayesian in-Context example Selection method (ByCS) for ICL. Extending the inference probability conditioned on in-context examples based on Bayes' theorem, ByCS focuses on the inverse inference conditioned on test input. Following the assumption that accurate inverse inference probability (likelihood) will result in accurate inference probability (posterior), in-context examples are selected based on their inverse inference results. Diverse and extensive cross-tasking and cross-modality experiments are performed with speech, text, and image examples. Experimental results show the efficacy and robustness of our ByCS method on various models, tasks and modalities.","sentences":["Large language models (LLMs) can adapt to new tasks through in-context learning (ICL) based on a few examples presented in dialogue history without any model parameter update.","Despite such convenience, the performance of ICL heavily depends on the quality of the in-context examples presented, which makes the in-context example selection approach a critical choice.","This paper proposes a novel Bayesian in-Context example Selection method (ByCS) for ICL.","Extending the inference probability conditioned on in-context examples based on Bayes' theorem, ByCS focuses on the inverse inference conditioned on test input.","Following the assumption that accurate inverse inference probability (likelihood) will result in accurate inference probability (posterior), in-context examples are selected based on their inverse inference results.","Diverse and extensive cross-tasking and cross-modality experiments are performed with speech, text, and image examples.","Experimental results show the efficacy and robustness of our ByCS method on various models, tasks and modalities."],"url":"http://arxiv.org/abs/2404.14716v1","category":"cs.CL"}
{"created":"2024-04-23 03:39:57","title":"ORBIT: Oak Ridge Base Foundation Model for Earth System Predictability","abstract":"Earth system predictability is challenged by the complexity of environmental dynamics and the multitude of variables involved. Current AI foundation models, although advanced by leveraging large and heterogeneous data, are often constrained by their size and data integration, limiting their effectiveness in addressing the full range of Earth system prediction challenges. To overcome these limitations, we introduce the Oak Ridge Base Foundation Model for Earth System Predictability (ORBIT), an advanced vision-transformer model that scales up to 113 billion parameters using a novel hybrid tensor-data orthogonal parallelism technique. As the largest model of its kind, ORBIT surpasses the current climate AI foundation model size by a thousandfold. Performance scaling tests conducted on the Frontier supercomputer have demonstrated that ORBIT achieves 230 to 707 PFLOPS, with scaling efficiency maintained at 78% to 96% across 24,576 AMD GPUs. These breakthroughs establish new advances in AI-driven climate modeling and demonstrate promise to significantly improve the Earth system predictability.","sentences":["Earth system predictability is challenged by the complexity of environmental dynamics and the multitude of variables involved.","Current AI foundation models, although advanced by leveraging large and heterogeneous data, are often constrained by their size and data integration, limiting their effectiveness in addressing the full range of Earth system prediction challenges.","To overcome these limitations, we introduce the Oak Ridge Base Foundation Model for Earth System Predictability (ORBIT), an advanced vision-transformer model that scales up to 113 billion parameters using a novel hybrid tensor-data orthogonal parallelism technique.","As the largest model of its kind, ORBIT surpasses the current climate AI foundation model size by a thousandfold.","Performance scaling tests conducted on the Frontier supercomputer have demonstrated that ORBIT achieves 230 to 707 PFLOPS, with scaling efficiency maintained at 78% to 96% across 24,576 AMD GPUs.","These breakthroughs establish new advances in AI-driven climate modeling and demonstrate promise to significantly improve the Earth system predictability."],"url":"http://arxiv.org/abs/2404.14712v1","category":"physics.ao-ph"}
{"created":"2024-04-23 03:39:14","title":"Challenges of Using Pre-trained Models: the Practitioners' Perspective","abstract":"The challenges associated with using pre-trained models (PTMs) have not been specifically investigated, which hampers their effective utilization. To address this knowledge gap, we collected and analyzed a dataset of 5,896 PTM-related questions on Stack Overflow. We first analyze the popularity and difficulty trends of PTM-related questions. We find that PTM-related questions are becoming more and more popular over time. However, it is noteworthy that PTM-related questions not only have a lower response rate but also exhibit a longer response time compared to many well-researched topics in software engineering. This observation emphasizes the significant difficulty and complexity associated with the practical application of PTMs. To delve into the specific challenges, we manually annotate 430 PTM-related questions, categorizing them into a hierarchical taxonomy of 42 codes (i.e., leaf nodes) and three categories. This taxonomy encompasses many PTM prominent challenges such as fine-tuning, output understanding, and prompt customization, which reflects the gaps between current techniques and practical needs. We discuss the implications of our study for PTM practitioners, vendors, and educators, and suggest possible directions and solutions for future research.","sentences":["The challenges associated with using pre-trained models (PTMs) have not been specifically investigated, which hampers their effective utilization.","To address this knowledge gap, we collected and analyzed a dataset of 5,896 PTM-related questions on Stack Overflow.","We first analyze the popularity and difficulty trends of PTM-related questions.","We find that PTM-related questions are becoming more and more popular over time.","However, it is noteworthy that PTM-related questions not only have a lower response rate but also exhibit a longer response time compared to many well-researched topics in software engineering.","This observation emphasizes the significant difficulty and complexity associated with the practical application of PTMs.","To delve into the specific challenges, we manually annotate 430 PTM-related questions, categorizing them into a hierarchical taxonomy of 42 codes (i.e., leaf nodes) and three categories.","This taxonomy encompasses many PTM prominent challenges such as fine-tuning, output understanding, and prompt customization, which reflects the gaps between current techniques and practical needs.","We discuss the implications of our study for PTM practitioners, vendors, and educators, and suggest possible directions and solutions for future research."],"url":"http://arxiv.org/abs/2404.14710v1","category":"cs.SE"}
{"created":"2024-04-23 03:23:13","title":"Channel Estimation for Optical IRS-Assisted VLC System via Spatial Coherence","abstract":"Optical intelligent reflecting surface (OIRS) has been considered a promising technology for visible light communication (VLC) by constructing visual line-of-sight propagation paths to address the signal blockage issue. However, the existing works on OIRSs are mostly based on perfect channel state information (CSI), whose acquisition appears to be challenging due to the passive nature of the OIRS. To tackle this challenge, this paper proposes a customized channel estimation algorithm for OIRSs. Specifically, we first unveil the OIRS spatial coherence characteristics and derive the coherence distance in closed form. Based on this property, a spatial sampling-based algorithm is proposed to estimate the OIRS-reflected channel, by dividing the OIRS into multiple subarrays based on the coherence distance and sequentially estimating their associated CSI, followed by an interpolation to retrieve the full CSI. Simulation results validate the derived OIRS spatial coherence and demonstrate the efficacy of the proposed OIRS channel estimation algorithm.","sentences":["Optical intelligent reflecting surface (OIRS) has been considered a promising technology for visible light communication (VLC) by constructing visual line-of-sight propagation paths to address the signal blockage issue.","However, the existing works on OIRSs are mostly based on perfect channel state information (CSI), whose acquisition appears to be challenging due to the passive nature of the OIRS.","To tackle this challenge, this paper proposes a customized channel estimation algorithm for OIRSs.","Specifically, we first unveil the OIRS spatial coherence characteristics and derive the coherence distance in closed form.","Based on this property, a spatial sampling-based algorithm is proposed to estimate the OIRS-reflected channel, by dividing the OIRS into multiple subarrays based on the coherence distance and sequentially estimating their associated CSI, followed by an interpolation to retrieve the full CSI.","Simulation results validate the derived OIRS spatial coherence and demonstrate the efficacy of the proposed OIRS channel estimation algorithm."],"url":"http://arxiv.org/abs/2404.14706v1","category":"cs.IT"}
{"created":"2024-04-23 02:57:46","title":"FlashSpeech: Efficient Zero-Shot Speech Synthesis","abstract":"Recent progress in large-scale zero-shot speech synthesis has been significantly advanced by language models and diffusion models. However, the generation process of both methods is slow and computationally intensive. Efficient speech synthesis using a lower computing budget to achieve quality on par with previous work remains a significant challenge. In this paper, we present FlashSpeech, a large-scale zero-shot speech synthesis system with approximately 5\\% of the inference time compared with previous work. FlashSpeech is built on the latent consistency model and applies a novel adversarial consistency training approach that can train from scratch without the need for a pre-trained diffusion model as the teacher. Furthermore, a new prosody generator module enhances the diversity of prosody, making the rhythm of the speech sound more natural. The generation processes of FlashSpeech can be achieved efficiently with one or two sampling steps while maintaining high audio quality and high similarity to the audio prompt for zero-shot speech generation. Our experimental results demonstrate the superior performance of FlashSpeech. Notably, FlashSpeech can be about 20 times faster than other zero-shot speech synthesis systems while maintaining comparable performance in terms of voice quality and similarity. Furthermore, FlashSpeech demonstrates its versatility by efficiently performing tasks like voice conversion, speech editing, and diverse speech sampling. Audio samples can be found in https://flashspeech.github.io/.","sentences":["Recent progress in large-scale zero-shot speech synthesis has been significantly advanced by language models and diffusion models.","However, the generation process of both methods is slow and computationally intensive.","Efficient speech synthesis using a lower computing budget to achieve quality on par with previous work remains a significant challenge.","In this paper, we present FlashSpeech, a large-scale zero-shot speech synthesis system with approximately 5\\% of the inference time compared with previous work.","FlashSpeech is built on the latent consistency model and applies a novel adversarial consistency training approach that can train from scratch without the need for a pre-trained diffusion model as the teacher.","Furthermore, a new prosody generator module enhances the diversity of prosody, making the rhythm of the speech sound more natural.","The generation processes of FlashSpeech can be achieved efficiently with one or two sampling steps while maintaining high audio quality and high similarity to the audio prompt for zero-shot speech generation.","Our experimental results demonstrate the superior performance of FlashSpeech.","Notably, FlashSpeech can be about 20 times faster than other zero-shot speech synthesis systems while maintaining comparable performance in terms of voice quality and similarity.","Furthermore, FlashSpeech demonstrates its versatility by efficiently performing tasks like voice conversion, speech editing, and diverse speech sampling.","Audio samples can be found in https://flashspeech.github.io/."],"url":"http://arxiv.org/abs/2404.14700v2","category":"eess.AS"}
{"created":"2024-04-23 02:36:47","title":"FMint: Bridging Human Designed and Data Pretrained Models for Differential Equation Foundation Model","abstract":"Human-designed algorithms have long been fundamental in solving a variety of scientific and engineering challenges. Recently, data-driven deep learning methods have also risen to prominence, offering innovative solutions across numerous scientific fields. While traditional algorithms excel in capturing the core aspects of specific problems, they often lack the flexibility needed for varying problem conditions due to the absence of specific data. Conversely, while data-driven approaches utilize vast datasets, they frequently fall short in domain-specific knowledge. To bridge these gaps, we introduce \\textbf{FMint} (Foundation Model based on Initialization), a generative pre-trained model that synergizes the precision of human-designed algorithms with the adaptability of data-driven methods. This model is specifically engineered for high-accuracy simulation of dynamical systems. Starting from initial trajectories provided by conventional methods, FMint quickly delivers highly accurate solutions. It incorporates in-context learning and has been pre-trained on a diverse corpus of 500,000 dynamical systems, showcasing exceptional generalization across a broad spectrum of real-world applications. By effectively combining algorithmic rigor with data-driven flexibility, FMint sets the stage for the next generation of scientific foundation models, tackling complex problems with both efficiency and high accuracy.","sentences":["Human-designed algorithms have long been fundamental in solving a variety of scientific and engineering challenges.","Recently, data-driven deep learning methods have also risen to prominence, offering innovative solutions across numerous scientific fields.","While traditional algorithms excel in capturing the core aspects of specific problems, they often lack the flexibility needed for varying problem conditions due to the absence of specific data.","Conversely, while data-driven approaches utilize vast datasets, they frequently fall short in domain-specific knowledge.","To bridge these gaps, we introduce \\textbf{FMint} (Foundation Model based on Initialization), a generative pre-trained model that synergizes the precision of human-designed algorithms with the adaptability of data-driven methods.","This model is specifically engineered for high-accuracy simulation of dynamical systems.","Starting from initial trajectories provided by conventional methods, FMint quickly delivers highly accurate solutions.","It incorporates in-context learning and has been pre-trained on a diverse corpus of 500,000 dynamical systems, showcasing exceptional generalization across a broad spectrum of real-world applications.","By effectively combining algorithmic rigor with data-driven flexibility, FMint sets the stage for the next generation of scientific foundation models, tackling complex problems with both efficiency and high accuracy."],"url":"http://arxiv.org/abs/2404.14688v1","category":"cs.LG"}
{"created":"2024-04-23 02:32:57","title":"Pegasus-v1 Technical Report","abstract":"This technical report introduces Pegasus-1, a multimodal language model specialized in video content understanding and interaction through natural language. Pegasus-1 is designed to address the unique challenges posed by video data, such as interpreting spatiotemporal information, to offer nuanced video content comprehension across various lengths. This technical report overviews Pegasus-1's architecture, training strategies, and its performance in benchmarks on video conversation, zero-shot video question answering, and video summarization. We also explore qualitative characteristics of Pegasus-1 , demonstrating its capabilities as well as its limitations, in order to provide readers a balanced view of its current state and its future direction.","sentences":["This technical report introduces Pegasus-1, a multimodal language model specialized in video content understanding and interaction through natural language.","Pegasus-1 is designed to address the unique challenges posed by video data, such as interpreting spatiotemporal information, to offer nuanced video content comprehension across various lengths.","This technical report overviews Pegasus-1's architecture, training strategies, and its performance in benchmarks on video conversation, zero-shot video question answering, and video summarization.","We also explore qualitative characteristics of Pegasus-1 , demonstrating its capabilities as well as its limitations, in order to provide readers a balanced view of its current state and its future direction."],"url":"http://arxiv.org/abs/2404.14687v1","category":"cs.MM"}
{"created":"2024-04-23 02:32:27","title":"Assessing Human Judgment Forecasts in the Rapid Spread of the Mpox Outbreak: Insights and Challenges for Pandemic Preparedness","abstract":"In May 2022, mpox (formerly monkeypox) spread to non-endemic countries rapidly. Human judgment is a forecasting approach that has been sparsely evaluated during the beginning of an outbreak. We collected -- between May 19, 2022 and July 31, 2022 -- 1275 forecasts from 442 individuals of six questions about the mpox outbreak where ground truth data are now available. Individual human judgment forecasts and an equally weighted ensemble were evaluated, as well as compared to a random walk, autoregressive, and doubling time model. We found (1) individual human judgment forecasts underestimated outbreak size, (2) the ensemble forecast median moved closer to the ground truth over time but uncertainty around the median did not appreciably decrease, and (3) compared to computational models, for 2-8 week ahead forecasts, the human judgment ensemble outperformed all three models when using median absolute error and weighted interval score; for one week ahead forecasts a random walk outperformed human judgment. We propose two possible explanations: at the time a forecast was submitted, the mode was correlated with the most recent (and smaller) observation that would eventually determine ground truth. Several forecasts were solicited on a logarithmic scale which may have caused humans to generate forecasts with unintended, large uncertainty intervals. To aide in outbreak preparedness, platforms that solicit human judgment forecasts may wish to assess whether specifying a forecast on logarithmic scale matches an individual's intended forecast, support human judgment by finding cues that are typically used to build forecasts, and, to improve performance, tailor their platform to allow forecasters to assign zero probability to events.","sentences":["In May 2022, mpox (formerly monkeypox) spread to non-endemic countries rapidly.","Human judgment is a forecasting approach that has been sparsely evaluated during the beginning of an outbreak.","We collected -- between May 19, 2022 and July 31, 2022 -- 1275 forecasts from 442 individuals of six questions about the mpox outbreak where ground truth data are now available.","Individual human judgment forecasts and an equally weighted ensemble were evaluated, as well as compared to a random walk, autoregressive, and doubling time model.","We found (1) individual human judgment forecasts underestimated outbreak size, (2) the ensemble forecast median moved closer to the ground truth over time but uncertainty around the median did not appreciably decrease, and (3) compared to computational models, for 2-8 week ahead forecasts, the human judgment ensemble outperformed all three models when using median absolute error and weighted interval score; for one week ahead forecasts a random walk outperformed human judgment.","We propose two possible explanations: at the time a forecast was submitted, the mode was correlated with the most recent (and smaller) observation that would eventually determine ground truth.","Several forecasts were solicited on a logarithmic scale which may have caused humans to generate forecasts with unintended, large uncertainty intervals.","To aide in outbreak preparedness, platforms that solicit human judgment forecasts may wish to assess whether specifying a forecast on logarithmic scale matches an individual's intended forecast, support human judgment by finding cues that are typically used to build forecasts, and, to improve performance, tailor their platform to allow forecasters to assign zero probability to events."],"url":"http://arxiv.org/abs/2404.14686v1","category":"q-bio.PE"}
{"created":"2024-04-23 02:19:35","title":"Automated Multi-Language to English Machine Translation Using Generative Pre-Trained Transformers","abstract":"The task of accurate and efficient language translation is an extremely important information processing task. Machine learning enabled and automated translation that is accurate and fast is often a large topic of interest in the machine learning and data science communities. In this study, we examine using local Generative Pretrained Transformer (GPT) models to perform automated zero shot black-box, sentence wise, multi-natural-language translation into English text. We benchmark 16 different open-source GPT models, with no custom fine-tuning, from the Huggingface LLM repository for translating 50 different non-English languages into English using translated TED Talk transcripts as the reference dataset. These GPT model inference calls are performed strictly locally, on single A100 Nvidia GPUs. Benchmark metrics that are reported are language translation accuracy, using BLEU, GLEU, METEOR, and chrF text overlap measures, and wall-clock time for each sentence translation. The best overall performing GPT model for translating into English text for the BLEU metric is ReMM-v2-L2-13B with a mean score across all tested languages of $0.152$, for the GLEU metric is ReMM-v2-L2-13B with a mean score across all tested languages of $0.256$, for the chrF metric is Llama2-chat-AYT-13B with a mean score across all tested languages of $0.448$, and for the METEOR metric is ReMM-v2-L2-13B with a mean score across all tested languages of $0.438$.","sentences":["The task of accurate and efficient language translation is an extremely important information processing task.","Machine learning enabled and automated translation that is accurate and fast is often a large topic of interest in the machine learning and data science communities.","In this study, we examine using local Generative Pretrained Transformer (GPT) models to perform automated zero shot black-box, sentence wise, multi-natural-language translation into English text.","We benchmark 16 different open-source GPT models, with no custom fine-tuning, from the Huggingface LLM repository for translating 50 different non-English languages into English using translated TED Talk transcripts as the reference dataset.","These GPT model inference calls are performed strictly locally, on single A100 Nvidia GPUs.","Benchmark metrics that are reported are language translation accuracy, using BLEU, GLEU, METEOR, and chrF text overlap measures, and wall-clock time for each sentence translation.","The best overall performing GPT model for translating into English text for the BLEU metric is ReMM-v2-L2-13B with a mean score across all tested languages of $0.152$, for the GLEU metric is ReMM-v2-L2-13B with a mean score across all tested languages of $0.256$, for the chrF metric is Llama2-chat-AYT-13B with a mean score across all tested languages of $0.448$, and for the METEOR metric is ReMM-v2-L2-13B with a mean score across all tested languages of $0.438$."],"url":"http://arxiv.org/abs/2404.14680v1","category":"cs.CL"}
{"created":"2024-04-23 02:00:58","title":"HOIN: High-Order Implicit Neural Representations","abstract":"Implicit neural representations (INR) suffer from worsening spectral bias, which results in overly smooth solutions to the inverse problem. To deal with this problem, we propose a universal framework for processing inverse problems called \\textbf{High-Order Implicit Neural Representations (HOIN)}. By refining the traditional cascade structure to foster high-order interactions among features, HOIN enhances the model's expressive power and mitigates spectral bias through its neural tangent kernel's (NTK) strong diagonal properties, accelerating and optimizing inverse problem resolution. By analyzing the model's expression space, high-order derivatives, and the NTK matrix, we theoretically validate the feasibility of HOIN. HOIN realizes 1 to 3 dB improvements in most inverse problems, establishing a new state-of-the-art recovery quality and training efficiency, thus providing a new general paradigm for INR and paving the way for it to solve the inverse problem.","sentences":["Implicit neural representations (INR) suffer from worsening spectral bias, which results in overly smooth solutions to the inverse problem.","To deal with this problem, we propose a universal framework for processing inverse problems called \\textbf{High-Order Implicit Neural Representations (HOIN)}.","By refining the traditional cascade structure to foster high-order interactions among features, HOIN enhances the model's expressive power and mitigates spectral bias through its neural tangent kernel's (NTK) strong diagonal properties, accelerating and optimizing inverse problem resolution.","By analyzing the model's expression space, high-order derivatives, and the NTK matrix, we theoretically validate the feasibility of HOIN.","HOIN realizes 1 to 3 dB improvements in most inverse problems, establishing a new state-of-the-art recovery quality and training efficiency, thus providing a new general paradigm for INR and paving the way for it to solve the inverse problem."],"url":"http://arxiv.org/abs/2404.14674v1","category":"cs.LG"}
{"created":"2024-04-23 01:53:16","title":"Source Localization for Cross Network Information Diffusion","abstract":"Source localization aims to locate information diffusion sources only given the diffusion observation, which has attracted extensive attention in the past few years. Existing methods are mostly tailored for single networks and may not be generalized to handle more complex networks like cross-networks. Cross-network is defined as two interconnected networks, where one network's functionality depends on the other. Source localization on cross-networks entails locating diffusion sources on the source network by only giving the diffused observation in the target network. The task is challenging due to challenges including: 1) diffusion sources distribution modeling; 2) jointly considering both static and dynamic node features; and 3) heterogeneous diffusion patterns learning. In this work, we propose a novel method, namely CNSL, to handle the three primary challenges. Specifically, we propose to learn the distribution of diffusion sources through Bayesian inference and leverage disentangled encoders to separately learn static and dynamic node features. The learning objective is coupled with the cross-network information propagation estimation model to make the inference of diffusion sources considering the overall diffusion process. Additionally, we also provide two novel cross-network datasets collected by ourselves. Extensive experiments are conducted on both datasets to demonstrate the effectiveness of \\textit{CNSL} in handling the source localization on cross-networks.","sentences":["Source localization aims to locate information diffusion sources only given the diffusion observation, which has attracted extensive attention in the past few years.","Existing methods are mostly tailored for single networks and may not be generalized to handle more complex networks like cross-networks.","Cross-network is defined as two interconnected networks, where one network's functionality depends on the other.","Source localization on cross-networks entails locating diffusion sources on the source network by only giving the diffused observation in the target network.","The task is challenging due to challenges including: 1) diffusion sources distribution modeling; 2) jointly considering both static and dynamic node features; and 3) heterogeneous diffusion patterns learning.","In this work, we propose a novel method, namely CNSL, to handle the three primary challenges.","Specifically, we propose to learn the distribution of diffusion sources through Bayesian inference and leverage disentangled encoders to separately learn static and dynamic node features.","The learning objective is coupled with the cross-network information propagation estimation model to make the inference of diffusion sources considering the overall diffusion process.","Additionally, we also provide two novel cross-network datasets collected by ourselves.","Extensive experiments are conducted on both datasets to demonstrate the effectiveness of \\textit{CNSL} in handling the source localization on cross-networks."],"url":"http://arxiv.org/abs/2404.14668v1","category":"cs.SI"}
{"created":"2024-04-23 01:49:12","title":"Employing Layerwised Unsupervised Learning to Lessen Data and Loss Requirements in Forward-Forward Algorithms","abstract":"Recent deep learning models such as ChatGPT utilizing the back-propagation algorithm have exhibited remarkable performance. However, the disparity between the biological brain processes and the back-propagation algorithm has been noted. The Forward-Forward algorithm, which trains deep learning models solely through the forward pass, has emerged to address this. Although the Forward-Forward algorithm cannot replace back-propagation due to limitations such as having to use special input and loss functions, it has the potential to be useful in special situations where back-propagation is difficult to use. To work around this limitation and verify usability, we propose an Unsupervised Forward-Forward algorithm. Using an unsupervised learning model enables training with usual loss functions and inputs without restriction. Through this approach, we lead to stable learning and enable versatile utilization across various datasets and tasks. From a usability perspective, given the characteristics of the Forward-Forward algorithm and the advantages of the proposed method, we anticipate its practical application even in scenarios such as federated learning, where deep learning layers need to be trained separately in physically distributed environments.","sentences":["Recent deep learning models such as ChatGPT utilizing the back-propagation algorithm have exhibited remarkable performance.","However, the disparity between the biological brain processes and the back-propagation algorithm has been noted.","The Forward-Forward algorithm, which trains deep learning models solely through the forward pass, has emerged to address this.","Although the Forward-Forward algorithm cannot replace back-propagation due to limitations such as having to use special input and loss functions, it has the potential to be useful in special situations where back-propagation is difficult to use.","To work around this limitation and verify usability, we propose an Unsupervised Forward-Forward algorithm.","Using an unsupervised learning model enables training with usual loss functions and inputs without restriction.","Through this approach, we lead to stable learning and enable versatile utilization across various datasets and tasks.","From a usability perspective, given the characteristics of the Forward-Forward algorithm and the advantages of the proposed method, we anticipate its practical application even in scenarios such as federated learning, where deep learning layers need to be trained separately in physically distributed environments."],"url":"http://arxiv.org/abs/2404.14664v1","category":"cs.LG"}
{"created":"2024-04-23 01:45:38","title":"AI Procurement Checklists: Revisiting Implementation in the Age of AI Governance","abstract":"Public sector use of AI has been quietly on the rise for the past decade, but only recently have efforts to regulate it entered the cultural zeitgeist. While simple to articulate, promoting ethical and effective roll outs of AI systems in government is a notoriously elusive task. On the one hand there are hard-to-address pitfalls associated with AI-based tools, including concerns about bias towards marginalized communities, safety, and gameability. On the other, there is pressure not to make it too difficult to adopt AI, especially in the public sector which typically has fewer resources than the private sector$\\unicode{x2014}$conserving scarce government resources is often the draw of using AI-based tools in the first place. These tensions create a real risk that procedures built to ensure marginalized groups are not hurt by government use of AI will, in practice, be performative and ineffective. To inform the latest wave of regulatory efforts in the United States, we look to jurisdictions with mature regulations around government AI use. We report on lessons learned by officials in Brazil, Singapore and Canada, who have collectively implemented risk categories, disclosure requirements and assessments into the way they procure AI tools. In particular, we investigate two implemented checklists: the Canadian Directive on Automated Decision-Making (CDADM) and the World Economic Forum's AI Procurement in a Box (WEF). We detail three key pitfalls around expertise, risk frameworks and transparency, that can decrease the efficacy of regulations aimed at government AI use and suggest avenues for improvement.","sentences":["Public sector use of AI has been quietly on the rise for the past decade, but only recently have efforts to regulate it entered the cultural zeitgeist.","While simple to articulate, promoting ethical and effective roll outs of AI systems in government is a notoriously elusive task.","On the one hand there are hard-to-address pitfalls associated with AI-based tools, including concerns about bias towards marginalized communities, safety, and gameability.","On the other, there is pressure not to make it too difficult to adopt AI, especially in the public sector which typically has fewer resources than the private sector$\\unicode{x2014}$conserving scarce government resources is often the draw of using AI-based tools in the first place.","These tensions create a real risk that procedures built to ensure marginalized groups are not hurt by government use of AI will, in practice, be performative and ineffective.","To inform the latest wave of regulatory efforts in the United States, we look to jurisdictions with mature regulations around government AI use.","We report on lessons learned by officials in Brazil, Singapore and Canada, who have collectively implemented risk categories, disclosure requirements and assessments into the way they procure AI tools.","In particular, we investigate two implemented checklists: the Canadian Directive on Automated Decision-Making (CDADM) and the World Economic Forum's AI Procurement in a Box (WEF).","We detail three key pitfalls around expertise, risk frameworks and transparency, that can decrease the efficacy of regulations aimed at government AI use and suggest avenues for improvement."],"url":"http://arxiv.org/abs/2404.14660v1","category":"cs.CY"}
{"created":"2024-04-23 01:19:19","title":"Machine Vision Based Assessment of Fall Color Changes in Apple Trees: Exploring Relationship with Leaf Nitrogen Concentration","abstract":"Apple trees being deciduous trees, shed leaves each year which is preceded by the change in color of leaves from green to yellow (also known as senescence) during the fall season. The rate and timing of color change are affected by the number of factors including nitrogen (N) deficiencies. The green color of leaves is highly dependent on the chlorophyll content, which in turn depends on the nitrogen concentration in the leaves. The assessment of the leaf color can give vital information on the nutrient status of the tree. The use of a machine vision based system to capture and quantify these timings and changes in leaf color can be a great tool for that purpose.   \\par This study is based on data collected during the fall of 2021 and 2023 at a commercial orchard using a ground-based stereo-vision sensor for five weeks. The point cloud obtained from the sensor was segmented to get just the tree in the foreground. The study involved the segmentation of the trees in a natural background using point cloud data and quantification of the color using a custom-defined metric, \\textit{yellowness index}, varying from $-1$ to $+1$ ($-1$ being completely green and $+1$ being completely yellow), which gives the proportion of yellow leaves on a tree. The performance of K-means based algorithm and gradient boosting algorithm were compared for \\textit{yellowness index} calculation. The segmentation method proposed in the study was able to estimate the \\textit{yellowness index} on the trees with $R^2 = 0.72$. The results showed that the metric was able to capture the gradual color transition from green to yellow over the study duration. It was also observed that the trees with lower nitrogen showed the color transition to yellow earlier than the trees with higher nitrogen. The onset of color transition during both years aligned with the $29^{th}$ week post-full bloom.","sentences":["Apple trees being deciduous trees, shed leaves each year which is preceded by the change in color of leaves from green to yellow (also known as senescence) during the fall season.","The rate and timing of color change are affected by the number of factors including nitrogen (N) deficiencies.","The green color of leaves is highly dependent on the chlorophyll content, which in turn depends on the nitrogen concentration in the leaves.","The assessment of the leaf color can give vital information on the nutrient status of the tree.","The use of a machine vision based system to capture and quantify these timings and changes in leaf color can be a great tool for that purpose.   ","\\par","This study is based on data collected during the fall of 2021 and 2023 at a commercial orchard using a ground-based stereo-vision sensor for five weeks.","The point cloud obtained from the sensor was segmented to get just the tree in the foreground.","The study involved the segmentation of the trees in a natural background using point cloud data and quantification of the color using a custom-defined metric, \\textit{yellowness index}, varying from $-1$ to $+1$ ($-1$ being completely green and $+1$ being completely yellow), which gives the proportion of yellow leaves on a tree.","The performance of K-means based algorithm and gradient boosting algorithm were compared for \\textit{yellowness index} calculation.","The segmentation method proposed in the study was able to estimate the \\textit{yellowness index} on the trees with $R^2 =","0.72$.","The results showed that the metric was able to capture the gradual color transition from green to yellow over the study duration.","It was also observed that the trees with lower nitrogen showed the color transition to yellow earlier than the trees with higher nitrogen.","The onset of color transition during both years aligned with the $29^{th}$ week post-full bloom."],"url":"http://arxiv.org/abs/2404.14653v1","category":"cs.CV"}
{"created":"2024-04-23 00:49:46","title":"Exploring and Unleashing the Power of Large Language Models in Automated Code Translation","abstract":"Code translation tools are developed for automatic source-to-source translation. Although learning-based transpilers have shown impressive enhancement against rule-based counterparts, owing to their task-specific pre-training on extensive monolingual corpora. Their current performance still remains unsatisfactory for practical deployment, and the associated training resources are also prohibitively expensive. LLMs pre-trained on huge amounts of human-written code/text have shown remarkable performance in many code intelligence tasks due to their powerful generality, even without task-specific training. Thus, LLMs can potentially circumvent the above limitations, but they have not been exhaustively explored yet. This paper investigates diverse LLMs and learning-based transpilers for automated code translation tasks, finding that: although certain LLMs have outperformed current transpilers, they still have some accuracy issues, where most of the failures are induced by a lack of comprehension of source programs (38.51%), missing clear instructions on I/O types in translation (14.94%), and ignoring discrepancies between source and target programs (41.38%). Enlightened by the above findings, we propose UniTrans, an Unified code Translation framework, applicable to various LLMs, for unleashing their power in this field. Specifically, UniTrans first craft a series of test cases for target programs with the assistance of source programs. Next, it harnesses the above auto-generated test cases to augment the code translation and then evaluate their correctness via execution. Afterward, UniTrans further (iteratively) repairs incorrectly translated programs prompted by test case execution results. Extensive experiments are conducted on six translation datasets between Python, Java, and C++. Three recent LLMs of diverse sizes are tested with UniTrans, and all achieve substantial improvements.","sentences":["Code translation tools are developed for automatic source-to-source translation.","Although learning-based transpilers have shown impressive enhancement against rule-based counterparts, owing to their task-specific pre-training on extensive monolingual corpora.","Their current performance still remains unsatisfactory for practical deployment, and the associated training resources are also prohibitively expensive.","LLMs pre-trained on huge amounts of human-written code/text have shown remarkable performance in many code intelligence tasks due to their powerful generality, even without task-specific training.","Thus, LLMs can potentially circumvent the above limitations, but they have not been exhaustively explored yet.","This paper investigates diverse LLMs and learning-based transpilers for automated code translation tasks, finding that: although certain LLMs have outperformed current transpilers, they still have some accuracy issues, where most of the failures are induced by a lack of comprehension of source programs (38.51%), missing clear instructions on I/O types in translation (14.94%), and ignoring discrepancies between source and target programs (41.38%).","Enlightened by the above findings, we propose UniTrans, an Unified code Translation framework, applicable to various LLMs, for unleashing their power in this field.","Specifically, UniTrans first craft a series of test cases for target programs with the assistance of source programs.","Next, it harnesses the above auto-generated test cases to augment the code translation and then evaluate their correctness via execution.","Afterward, UniTrans further (iteratively) repairs incorrectly translated programs prompted by test case execution results.","Extensive experiments are conducted on six translation datasets between Python, Java, and C++.","Three recent LLMs of diverse sizes are tested with UniTrans, and all achieve substantial improvements."],"url":"http://arxiv.org/abs/2404.14646v1","category":"cs.SE"}
{"created":"2024-04-23 00:18:20","title":"Digital Twins for forecasting and decision optimisation with machine learning: applications in wastewater treatment","abstract":"Prediction and optimisation are two widely used techniques that have found many applications in solving real-world problems. While prediction is concerned with estimating the unknown future values of a variable, optimisation is concerned with optimising the decision given all the available data. These methods are used together to solve problems for sequential decision-making where often we need to predict the future values of variables and then use them for determining the optimal decisions. This paradigm is known as forecast and optimise and has numerous applications, e.g., forecast demand for a product and then optimise inventory, forecast energy demand and schedule generations, forecast demand for a service and schedule staff, to name a few. In this extended abstract, we review a digital twin that was developed and applied in wastewater treatment in Urban Utility to improve their operational efficiency. While the current study is tailored to the case study problem, the underlying principles can be used to solve similar problems in other domains.","sentences":["Prediction and optimisation are two widely used techniques that have found many applications in solving real-world problems.","While prediction is concerned with estimating the unknown future values of a variable, optimisation is concerned with optimising the decision given all the available data.","These methods are used together to solve problems for sequential decision-making where often we need to predict the future values of variables and then use them for determining the optimal decisions.","This paradigm is known as forecast and optimise and has numerous applications, e.g., forecast demand for a product and then optimise inventory, forecast energy demand and schedule generations, forecast demand for a service and schedule staff, to name a few.","In this extended abstract, we review a digital twin that was developed and applied in wastewater treatment in Urban Utility to improve their operational efficiency.","While the current study is tailored to the case study problem, the underlying principles can be used to solve similar problems in other domains."],"url":"http://arxiv.org/abs/2404.14635v1","category":"cs.LG"}
{"created":"2024-04-22 23:52:02","title":"A Blueprint for the Milky Way's Stellar Populations. V. 3D Local Dust Extinction","abstract":"Using a grid of empirically calibrated synthetic spectra developed in our previous study, we construct an all-sky 3D extinction map from the large collection of low-resolution XP spectra in Gaia DR3. Along each line of sight, with an area ranging from $0.2$ to $13.4$ deg$^2$, we determine both the reddening and metallicity of main-sequence stars and model the foreground extinction up to approximately $3$ kpc from the Sun. Furthermore, we explore variations in the total-to-selective extinction ratio in our parameter search and identify its mean systematic change across diverse cloud environments in both hemispheres. In regions outside the densest parts of the clouds, our reddening estimates are validated through comparisons with previous reddening maps. However, a notable discrepancy arises in comparison to other independent work based on XP spectra, which can be attributed to systematic offsets in their metallicity estimates. On the other hand, our metallicity scale exhibits reasonable agreement with the high-resolution spectroscopic abundance scale. We also assess the accuracy of the XP spectra by applying our calibrated models, and we confirm an increasing trend of flux overestimation at shorter wavelengths below $400$ nm.","sentences":["Using a grid of empirically calibrated synthetic spectra developed in our previous study, we construct an all-sky 3D extinction map from the large collection of low-resolution XP spectra in Gaia DR3.","Along each line of sight, with an area ranging from $0.2$ to $13.4$ deg$^2$, we determine both the reddening and metallicity of main-sequence stars and model the foreground extinction up to approximately $3$ kpc from the Sun.","Furthermore, we explore variations in the total-to-selective extinction ratio in our parameter search and identify its mean systematic change across diverse cloud environments in both hemispheres.","In regions outside the densest parts of the clouds, our reddening estimates are validated through comparisons with previous reddening maps.","However, a notable discrepancy arises in comparison to other independent work based on XP spectra, which can be attributed to systematic offsets in their metallicity estimates.","On the other hand, our metallicity scale exhibits reasonable agreement with the high-resolution spectroscopic abundance scale.","We also assess the accuracy of the XP spectra by applying our calibrated models, and we confirm an increasing trend of flux overestimation at shorter wavelengths below $400$ nm."],"url":"http://arxiv.org/abs/2404.14626v1","category":"astro-ph.GA"}
{"created":"2024-04-22 23:12:58","title":"Fairness Incentives in Response to Unfair Dynamic Pricing","abstract":"The use of dynamic pricing by profit-maximizing firms gives rise to demand fairness concerns, measured by discrepancies in consumer groups' demand responses to a given pricing strategy. Notably, dynamic pricing may result in buyer distributions unreflective of those of the underlying population, which can be problematic in markets where fair representation is socially desirable. To address this, policy makers might leverage tools such as taxation and subsidy to adapt policy mechanisms dependent upon their social objective. In this paper, we explore the potential for AI methods to assist such intervention strategies. To this end, we design a basic simulated economy, wherein we introduce a dynamic social planner (SP) to generate corporate taxation schedules geared to incentivizing firms towards adopting fair pricing behaviours, and to use the collected tax budget to subsidize consumption among underrepresented groups. To cover a range of possible policy scenarios, we formulate our social planner's learning problem as a multi-armed bandit, a contextual bandit and finally as a full reinforcement learning (RL) problem, evaluating welfare outcomes from each case. To alleviate the difficulty in retaining meaningful tax rates that apply to less frequently occurring brackets, we introduce FairReplayBuffer, which ensures that our RL agent samples experiences uniformly across a discretized fairness space. We find that, upon deploying a learned tax and redistribution policy, social welfare improves on that of the fairness-agnostic baseline, and approaches that of the analytically optimal fairness-aware baseline for the multi-armed and contextual bandit settings, and surpassing it by 13.19% in the full RL setting.","sentences":["The use of dynamic pricing by profit-maximizing firms gives rise to demand fairness concerns, measured by discrepancies in consumer groups' demand responses to a given pricing strategy.","Notably, dynamic pricing may result in buyer distributions unreflective of those of the underlying population, which can be problematic in markets where fair representation is socially desirable.","To address this, policy makers might leverage tools such as taxation and subsidy to adapt policy mechanisms dependent upon their social objective.","In this paper, we explore the potential for AI methods to assist such intervention strategies.","To this end, we design a basic simulated economy, wherein we introduce a dynamic social planner (SP) to generate corporate taxation schedules geared to incentivizing firms towards adopting fair pricing behaviours, and to use the collected tax budget to subsidize consumption among underrepresented groups.","To cover a range of possible policy scenarios, we formulate our social planner's learning problem as a multi-armed bandit, a contextual bandit and finally as a full reinforcement learning (RL) problem, evaluating welfare outcomes from each case.","To alleviate the difficulty in retaining meaningful tax rates that apply to less frequently occurring brackets, we introduce FairReplayBuffer, which ensures that our RL agent samples experiences uniformly across a discretized fairness space.","We find that, upon deploying a learned tax and redistribution policy, social welfare improves on that of the fairness-agnostic baseline, and approaches that of the analytically optimal fairness-aware baseline for the multi-armed and contextual bandit settings, and surpassing it by 13.19% in the full RL setting."],"url":"http://arxiv.org/abs/2404.14620v1","category":"cs.LG"}
{"created":"2024-04-22 23:12:03","title":"OpenELM: An Efficient Language Model Family with Open-source Training and Inference Framework","abstract":"The reproducibility and transparency of large language models are crucial for advancing open research, ensuring the trustworthiness of results, and enabling investigations into data and model biases, as well as potential risks. To this end, we release OpenELM, a state-of-the-art open language model. OpenELM uses a layer-wise scaling strategy to efficiently allocate parameters within each layer of the transformer model, leading to enhanced accuracy. For example, with a parameter budget of approximately one billion parameters, OpenELM exhibits a 2.36% improvement in accuracy compared to OLMo while requiring $2\\times$ fewer pre-training tokens.   Diverging from prior practices that only provide model weights and inference code, and pre-train on private datasets, our release includes the complete framework for training and evaluation of the language model on publicly available datasets, including training logs, multiple checkpoints, and pre-training configurations. We also release code to convert models to MLX library for inference and fine-tuning on Apple devices. This comprehensive release aims to empower and strengthen the open research community, paving the way for future open research endeavors.   Our source code along with pre-trained model weights and training recipes is available at \\url{https://github.com/apple/corenet}. Additionally, \\model models can be found on HuggingFace at: \\url{https://huggingface.co/apple/OpenELM}.","sentences":["The reproducibility and transparency of large language models are crucial for advancing open research, ensuring the trustworthiness of results, and enabling investigations into data and model biases, as well as potential risks.","To this end, we release OpenELM, a state-of-the-art open language model.","OpenELM uses a layer-wise scaling strategy to efficiently allocate parameters within each layer of the transformer model, leading to enhanced accuracy.","For example, with a parameter budget of approximately one billion parameters, OpenELM exhibits a 2.36% improvement in accuracy compared to OLMo while requiring $2\\times$ fewer pre-training tokens.   ","Diverging from prior practices that only provide model weights and inference code, and pre-train on private datasets, our release includes the complete framework for training and evaluation of the language model on publicly available datasets, including training logs, multiple checkpoints, and pre-training configurations.","We also release code to convert models to MLX library for inference and fine-tuning on Apple devices.","This comprehensive release aims to empower and strengthen the open research community, paving the way for future open research endeavors.   ","Our source code along with pre-trained model weights and training recipes is available at \\url{https://github.com/apple/corenet}.","Additionally, \\model models can be found on HuggingFace at: \\url{https://huggingface.co/apple/OpenELM}."],"url":"http://arxiv.org/abs/2404.14619v1","category":"cs.CL"}
{"created":"2024-04-22 23:06:42","title":"Hybrid LLM: Cost-Efficient and Quality-Aware Query Routing","abstract":"Large language models (LLMs) excel in most NLP tasks but also require expensive cloud servers for deployment due to their size, while smaller models that can be deployed on lower cost (e.g., edge) devices, tend to lag behind in terms of response quality. Therefore in this work we propose a hybrid inference approach which combines their respective strengths to save cost and maintain quality. Our approach uses a router that assigns queries to the small or large model based on the predicted query difficulty and the desired quality level. The desired quality level can be tuned dynamically at test time to seamlessly trade quality for cost as per the scenario requirements. In experiments our approach allows us to make up to 40% fewer calls to the large model, with no drop in response quality.","sentences":["Large language models (LLMs) excel in most NLP tasks but also require expensive cloud servers for deployment due to their size, while smaller models that can be deployed on lower cost (e.g., edge) devices, tend to lag behind in terms of response quality.","Therefore in this work we propose a hybrid inference approach which combines their respective strengths to save cost and maintain quality.","Our approach uses a router that assigns queries to the small or large model based on the predicted query difficulty and the desired quality level.","The desired quality level can be tuned dynamically at test time to seamlessly trade quality for cost as per the scenario requirements.","In experiments our approach allows us to make up to 40% fewer calls to the large model, with no drop in response quality."],"url":"http://arxiv.org/abs/2404.14618v1","category":"cs.LG"}
{"created":"2024-04-22 22:02:19","title":"Cross-Task Multi-Branch Vision Transformer for Facial Expression and Mask Wearing Classification","abstract":"With wearing masks becoming a new cultural norm, facial expression recognition (FER) while taking masks into account has become a significant challenge. In this paper, we propose a unified multi-branch vision transformer for facial expression recognition and mask wearing classification tasks. Our approach extracts shared features for both tasks using a dual-branch architecture that obtains multi-scale feature representations. Furthermore, we propose a cross-task fusion phase that processes tokens for each task with separate branches, while exchanging information using a cross attention module. Our proposed framework reduces the overall complexity compared with using separate networks for both tasks by the simple yet effective cross-task fusion phase. Extensive experiments demonstrate that our proposed model performs better than or on par with different state-of-the-art methods on both facial expression recognition and facial mask wearing classification task.","sentences":["With wearing masks becoming a new cultural norm, facial expression recognition (FER) while taking masks into account has become a significant challenge.","In this paper, we propose a unified multi-branch vision transformer for facial expression recognition and mask wearing classification tasks.","Our approach extracts shared features for both tasks using a dual-branch architecture that obtains multi-scale feature representations.","Furthermore, we propose a cross-task fusion phase that processes tokens for each task with separate branches, while exchanging information using a cross attention module.","Our proposed framework reduces the overall complexity compared with using separate networks for both tasks by the simple yet effective cross-task fusion phase.","Extensive experiments demonstrate that our proposed model performs better than or on par with different state-of-the-art methods on both facial expression recognition and facial mask wearing classification task."],"url":"http://arxiv.org/abs/2404.14606v1","category":"cs.CV"}
{"created":"2024-04-22 22:00:38","title":"Assessment of Sign Language-Based versus Touch-Based Input for Deaf Users Interacting with Intelligent Personal Assistants","abstract":"With the recent advancements in intelligent personal assistants (IPAs), their popularity is rapidly increasing when it comes to utilizing Automatic Speech Recognition within households. In this study, we used a Wizard-of-Oz methodology to evaluate and compare the usability of American Sign Language (ASL), Tap to Alexa, and smart home apps among 23 deaf participants within a limited-domain smart home environment. Results indicate a slight usability preference for ASL. Linguistic analysis of the participants' signing reveals a diverse range of expressions and vocabulary as they interacted with IPAs in the context of a restricted-domain application. On average, deaf participants exhibited a vocabulary of 47 +/- 17 signs with an additional 10 +/- 7 fingerspelled words, for a total of 246 different signs and 93 different fingerspelled words across all participants. We discuss the implications for the design of limited-vocabulary applications as a stepping-stone toward general-purpose ASL recognition in the future.","sentences":["With the recent advancements in intelligent personal assistants (IPAs), their popularity is rapidly increasing when it comes to utilizing Automatic Speech Recognition within households.","In this study, we used a Wizard-of-Oz methodology to evaluate and compare the usability of American Sign Language (ASL), Tap to Alexa, and smart home apps among 23 deaf participants within a limited-domain smart home environment.","Results indicate a slight usability preference for ASL.","Linguistic analysis of the participants' signing reveals a diverse range of expressions and vocabulary as they interacted with IPAs in the context of a restricted-domain application.","On average, deaf participants exhibited a vocabulary of 47 +/- 17 signs with an additional 10 +/- 7 fingerspelled words, for a total of 246 different signs and 93 different fingerspelled words across all participants.","We discuss the implications for the design of limited-vocabulary applications as a stepping-stone toward general-purpose ASL recognition in the future."],"url":"http://arxiv.org/abs/2404.14605v1","category":"cs.HC"}
{"created":"2024-04-22 21:30:11","title":"Brain-Inspired Continual Learning-Robust Feature Distillation and Re-Consolidation for Class Incremental Learning","abstract":"Artificial intelligence (AI) and neuroscience share a rich history, with advancements in neuroscience shaping the development of AI systems capable of human-like knowledge retention. Leveraging insights from neuroscience and existing research in adversarial and continual learning, we introduce a novel framework comprising two core concepts: feature distillation and re-consolidation. Our framework, named Robust Rehearsal, addresses the challenge of catastrophic forgetting inherent in continual learning (CL) systems by distilling and rehearsing robust features. Inspired by the mammalian brain's memory consolidation process, Robust Rehearsal aims to emulate the rehearsal of distilled experiences during learning tasks. Additionally, it mimics memory re-consolidation, where new experiences influence the integration of past experiences to mitigate forgetting. Extensive experiments conducted on CIFAR10, CIFAR100, and real-world helicopter attitude datasets showcase the superior performance of CL models trained with Robust Rehearsal compared to baseline methods. Furthermore, examining different optimization training objectives-joint, continual, and adversarial learning-we highlight the crucial role of feature learning in model performance. This underscores the significance of rehearsing CL-robust samples in mitigating catastrophic forgetting. In conclusion, aligning CL approaches with neuroscience insights offers promising solutions to the challenge of catastrophic forgetting, paving the way for more robust and human-like AI systems.","sentences":["Artificial intelligence (AI) and neuroscience share a rich history, with advancements in neuroscience shaping the development of AI systems capable of human-like knowledge retention.","Leveraging insights from neuroscience and existing research in adversarial and continual learning, we introduce a novel framework comprising two core concepts: feature distillation and re-consolidation.","Our framework, named Robust Rehearsal, addresses the challenge of catastrophic forgetting inherent in continual learning (CL) systems by distilling and rehearsing robust features.","Inspired by the mammalian brain's memory consolidation process, Robust Rehearsal aims to emulate the rehearsal of distilled experiences during learning tasks.","Additionally, it mimics memory re-consolidation, where new experiences influence the integration of past experiences to mitigate forgetting.","Extensive experiments conducted on CIFAR10, CIFAR100, and real-world helicopter attitude datasets showcase the superior performance of CL models trained with Robust Rehearsal compared to baseline methods.","Furthermore, examining different optimization training objectives-joint, continual, and adversarial learning-we highlight the crucial role of feature learning in model performance.","This underscores the significance of rehearsing CL-robust samples in mitigating catastrophic forgetting.","In conclusion, aligning CL approaches with neuroscience insights offers promising solutions to the challenge of catastrophic forgetting, paving the way for more robust and human-like AI systems."],"url":"http://arxiv.org/abs/2404.14588v1","category":"cs.LG"}
{"created":"2024-04-22 21:00:13","title":"The Adversarial AI-Art: Understanding, Generation, Detection, and Benchmarking","abstract":"Generative AI models can produce high-quality images based on text prompts. The generated images often appear indistinguishable from images generated by conventional optical photography devices or created by human artists (i.e., real images). While the outstanding performance of such generative models is generally well received, security concerns arise. For instance, such image generators could be used to facilitate fraud or scam schemes, generate and spread misinformation, or produce fabricated artworks. In this paper, we present a systematic attempt at understanding and detecting AI-generated images (AI-art) in adversarial scenarios. First, we collect and share a dataset of real images and their corresponding artificial counterparts generated by four popular AI image generators. The dataset, named ARIA, contains over 140K images in five categories: artworks (painting), social media images, news photos, disaster scenes, and anime pictures. This dataset can be used as a foundation to support future research on adversarial AI-art. Next, we present a user study that employs the ARIA dataset to evaluate if real-world users can distinguish with or without reference images. In a benchmarking study, we further evaluate if state-of-the-art open-source and commercial AI image detectors can effectively identify the images in the ARIA dataset. Finally, we present a ResNet-50 classifier and evaluate its accuracy and transferability on the ARIA dataset.","sentences":["Generative AI models can produce high-quality images based on text prompts.","The generated images often appear indistinguishable from images generated by conventional optical photography devices or created by human artists (i.e., real images).","While the outstanding performance of such generative models is generally well received, security concerns arise.","For instance, such image generators could be used to facilitate fraud or scam schemes, generate and spread misinformation, or produce fabricated artworks.","In this paper, we present a systematic attempt at understanding and detecting AI-generated images (AI-art) in adversarial scenarios.","First, we collect and share a dataset of real images and their corresponding artificial counterparts generated by four popular AI image generators.","The dataset, named ARIA, contains over 140K images in five categories: artworks (painting), social media images, news photos, disaster scenes, and anime pictures.","This dataset can be used as a foundation to support future research on adversarial AI-art.","Next, we present a user study that employs the ARIA dataset to evaluate if real-world users can distinguish with or without reference images.","In a benchmarking study, we further evaluate if state-of-the-art open-source and commercial AI image detectors can effectively identify the images in the ARIA dataset.","Finally, we present a ResNet-50 classifier and evaluate its accuracy and transferability on the ARIA dataset."],"url":"http://arxiv.org/abs/2404.14581v1","category":"cs.CV"}
{"created":"2024-04-22 20:53:08","title":"Designing forecasting software for forecast users: Empowering non-experts to create and understand their own forecasts","abstract":"Forecasts inform decision-making in nearly every domain. Forecasts are often produced by experts with rare or hard to acquire skills. In practice, forecasts are often used by domain experts and managers with little forecasting expertise. Our study focuses on how to design forecasting software that empowers non-expert users. We study how users can make use of state-of-the-art forecasting methods, embed their domain knowledge, and how they build understanding and trust towards generated forecasts. To do so, we co-designed a forecasting software prototype using feedback from users and then analyzed their interactions with our prototype. Our results identified three main considerations for non-expert users: (1) a safe stepwise approach facilitating causal understanding and trust; (2) a white box model supporting human-reasoning-friendly components; (3) the inclusion of domain knowledge. This paper contributes insights into how non-expert users interact with forecasting software and by recommending ways to design more accessible forecasting software.","sentences":["Forecasts inform decision-making in nearly every domain.","Forecasts are often produced by experts with rare or hard to acquire skills.","In practice, forecasts are often used by domain experts and managers with little forecasting expertise.","Our study focuses on how to design forecasting software that empowers non-expert users.","We study how users can make use of state-of-the-art forecasting methods, embed their domain knowledge, and how they build understanding and trust towards generated forecasts.","To do so, we co-designed a forecasting software prototype using feedback from users and then analyzed their interactions with our prototype.","Our results identified three main considerations for non-expert users: (1) a safe stepwise approach facilitating causal understanding and trust; (2) a white box model supporting human-reasoning-friendly components; (3) the inclusion of domain knowledge.","This paper contributes insights into how non-expert users interact with forecasting software and by recommending ways to design more accessible forecasting software."],"url":"http://arxiv.org/abs/2404.14575v1","category":"cs.HC"}
{"created":"2024-04-22 20:50:36","title":"Tile-Weighted Rate-Distortion Optimized Packet Scheduling for 360$^\\circ$ VR Video Streaming","abstract":"A key challenge of 360$^\\circ$ VR video streaming is ensuring high quality with limited network bandwidth. Currently, most studies focus on tile-based adaptive bitrate streaming to reduce bandwidth consumption, where resources in network nodes are not fully utilized. This article proposes a tile-weighted rate-distortion (TWRD) packet scheduling optimization system to reduce data volume and improve video quality. A multimodal spatial-temporal attention transformer is proposed to predict viewpoint with probability that is used to dynamically weight tiles and corresponding packets. The packet scheduling problem of determining which packets should be dropped is formulated as an optimization problem solved by a dynamic programming solution. Experiment results demonstrate the proposed method outperforms the existing methods under various conditions.","sentences":["A key challenge of 360$^\\circ$ VR video streaming is ensuring high quality with limited network bandwidth.","Currently, most studies focus on tile-based adaptive bitrate streaming to reduce bandwidth consumption, where resources in network nodes are not fully utilized.","This article proposes a tile-weighted rate-distortion (TWRD) packet scheduling optimization system to reduce data volume and improve video quality.","A multimodal spatial-temporal attention transformer is proposed to predict viewpoint with probability that is used to dynamically weight tiles and corresponding packets.","The packet scheduling problem of determining which packets should be dropped is formulated as an optimization problem solved by a dynamic programming solution.","Experiment results demonstrate the proposed method outperforms the existing methods under various conditions."],"url":"http://arxiv.org/abs/2404.14573v1","category":"cs.MM"}
{"created":"2024-04-22 20:21:32","title":"\"Where am I?\" Scene Retrieval with Language","abstract":"Natural language interfaces to embodied AI are becoming more ubiquitous in our daily lives. This opens further opportunities for language-based interaction with embodied agents, such as a user instructing an agent to execute some task in a specific location. For example, \"put the bowls back in the cupboard next to the fridge\" or \"meet me at the intersection under the red sign.\" As such, we need methods that interface between natural language and map representations of the environment. To this end, we explore the question of whether we can use an open-set natural language query to identify a scene represented by a 3D scene graph. We define this task as \"language-based scene-retrieval\" and it is closely related to \"coarse-localization,\" but we are instead searching for a match from a collection of disjoint scenes and not necessarily a large-scale continuous map. Therefore, we present Text2SceneGraphMatcher, a \"scene-retrieval\" pipeline that learns joint embeddings between text descriptions and scene graphs to determine if they are matched. The code, trained models, and datasets will be made public.","sentences":["Natural language interfaces to embodied AI are becoming more ubiquitous in our daily lives.","This opens further opportunities for language-based interaction with embodied agents, such as a user instructing an agent to execute some task in a specific location.","For example, \"put the bowls back in the cupboard next to the fridge\" or \"meet me at the intersection under the red sign.\"","As such, we need methods that interface between natural language and map representations of the environment.","To this end, we explore the question of whether we can use an open-set natural language query to identify a scene represented by a 3D scene graph.","We define this task as \"language-based scene-retrieval\" and it is closely related to \"coarse-localization,\" but we are instead searching for a match from a collection of disjoint scenes and not necessarily a large-scale continuous map.","Therefore, we present Text2SceneGraphMatcher, a \"scene-retrieval\" pipeline that learns joint embeddings between text descriptions and scene graphs to determine if they are matched.","The code, trained models, and datasets will be made public."],"url":"http://arxiv.org/abs/2404.14565v1","category":"cs.CV"}
{"created":"2024-04-22 20:19:01","title":"Exploring the Potential of Data-Driven Spatial Audio Enhancement Using a Single-Channel Model","abstract":"One key aspect differentiating data-driven single- and multi-channel speech enhancement and dereverberation methods is that both the problem formulation and complexity of the solutions are considerably more challenging in the latter case. Additionally, with limited computational resources, it is cumbersome to train models that require the management of larger datasets or those with more complex designs. In this scenario, an unverified hypothesis that single-channel methods can be adapted to multi-channel scenarios simply by processing each channel independently holds significant implications, boosting compatibility between sound scene capture and system input-output formats, while also allowing modern research to focus on other challenging aspects, such as full-bandwidth audio enhancement, competitive noise suppression, and unsupervised learning. This study verifies this hypothesis by comparing the enhancement promoted by a basic single-channel speech enhancement and dereverberation model with two other multi-channel models tailored to separate clean speech from noisy 3D mixes. A direction of arrival estimation model was used to objectively evaluate its capacity to preserve spatial information by comparing the output signals with ground-truth coordinate values. Consequently, a trade-off arises between preserving spatial information with a more straightforward single-channel solution at the cost of obtaining lower gains in intelligibility scores.","sentences":["One key aspect differentiating data-driven single- and multi-channel speech enhancement and dereverberation methods is that both the problem formulation and complexity of the solutions are considerably more challenging in the latter case.","Additionally, with limited computational resources, it is cumbersome to train models that require the management of larger datasets or those with more complex designs.","In this scenario, an unverified hypothesis that single-channel methods can be adapted to multi-channel scenarios simply by processing each channel independently holds significant implications, boosting compatibility between sound scene capture and system input-output formats, while also allowing modern research to focus on other challenging aspects, such as full-bandwidth audio enhancement, competitive noise suppression, and unsupervised learning.","This study verifies this hypothesis by comparing the enhancement promoted by a basic single-channel speech enhancement and dereverberation model with two other multi-channel models tailored to separate clean speech from noisy 3D mixes.","A direction of arrival estimation model was used to objectively evaluate its capacity to preserve spatial information by comparing the output signals with ground-truth coordinate values.","Consequently, a trade-off arises between preserving spatial information with a more straightforward single-channel solution at the cost of obtaining lower gains in intelligibility scores."],"url":"http://arxiv.org/abs/2404.14564v1","category":"eess.AS"}
{"created":"2024-04-22 20:18:12","title":"Exploring Algorithmic Explainability: Generating Explainable AI Insights for Personalized Clinical Decision Support Focused on Cannabis Intoxication in Young Adults","abstract":"This study explores the possibility of facilitating algorithmic decision-making by combining interpretable artificial intelligence (XAI) techniques with sensor data, with the aim of providing researchers and clinicians with personalized analyses of cannabis intoxication behavior. SHAP analyzes the importance and quantifies the impact of specific factors such as environmental noise or heart rate, enabling clinicians to pinpoint influential behaviors and environmental conditions. SkopeRules simplify the understanding of cannabis use for a specific activity or environmental use. Decision trees provide a clear visualization of how factors interact to influence cannabis consumption. Counterfactual models help identify key changes in behaviors or conditions that may alter cannabis use outcomes, to guide effective individualized intervention strategies. This multidimensional analytical approach not only unveils changes in behavioral and physiological states after cannabis use, such as frequent fluctuations in activity states, nontraditional sleep patterns, and specific use habits at different times and places, but also highlights the significance of individual differences in responses to cannabis use. These insights carry profound implications for clinicians seeking to gain a deeper understanding of the diverse needs of their patients and for tailoring precisely targeted intervention strategies. Furthermore, our findings highlight the pivotal role that XAI technologies could play in enhancing the transparency and interpretability of Clinical Decision Support Systems (CDSS), with a particular focus on substance misuse treatment. This research significantly contributes to ongoing initiatives aimed at advancing clinical practices that aim to prevent and reduce cannabis-related harms to health, positioning XAI as a supportive tool for clinicians and researchers alike.","sentences":["This study explores the possibility of facilitating algorithmic decision-making by combining interpretable artificial intelligence (XAI) techniques with sensor data, with the aim of providing researchers and clinicians with personalized analyses of cannabis intoxication behavior.","SHAP analyzes the importance and quantifies the impact of specific factors such as environmental noise or heart rate, enabling clinicians to pinpoint influential behaviors and environmental conditions.","SkopeRules simplify the understanding of cannabis use for a specific activity or environmental use.","Decision trees provide a clear visualization of how factors interact to influence cannabis consumption.","Counterfactual models help identify key changes in behaviors or conditions that may alter cannabis use outcomes, to guide effective individualized intervention strategies.","This multidimensional analytical approach not only unveils changes in behavioral and physiological states after cannabis use, such as frequent fluctuations in activity states, nontraditional sleep patterns, and specific use habits at different times and places, but also highlights the significance of individual differences in responses to cannabis use.","These insights carry profound implications for clinicians seeking to gain a deeper understanding of the diverse needs of their patients and for tailoring precisely targeted intervention strategies.","Furthermore, our findings highlight the pivotal role that XAI technologies could play in enhancing the transparency and interpretability of Clinical Decision Support Systems (CDSS), with a particular focus on substance misuse treatment.","This research significantly contributes to ongoing initiatives aimed at advancing clinical practices that aim to prevent and reduce cannabis-related harms to health, positioning XAI as a supportive tool for clinicians and researchers alike."],"url":"http://arxiv.org/abs/2404.14563v1","category":"cs.HC"}
{"created":"2024-04-22 19:46:16","title":"Generalizing Multi-Step Inverse Models for Representation Learning to Finite-Memory POMDPs","abstract":"Discovering an informative, or agent-centric, state representation that encodes only the relevant information while discarding the irrelevant is a key challenge towards scaling reinforcement learning algorithms and efficiently applying them to downstream tasks. Prior works studied this problem in high-dimensional Markovian environments, when the current observation may be a complex object but is sufficient to decode the informative state. In this work, we consider the problem of discovering the agent-centric state in the more challenging high-dimensional non-Markovian setting, when the state can be decoded from a sequence of past observations. We establish that generalized inverse models can be adapted for learning agent-centric state representation for this task. Our results include asymptotic theory in the deterministic dynamics setting as well as counter-examples for alternative intuitive algorithms. We complement these findings with a thorough empirical study on the agent-centric state discovery abilities of the different alternatives we put forward. Particularly notable is our analysis of past actions, where we show that these can be a double-edged sword: making the algorithms more successful when used correctly and causing dramatic failure when used incorrectly.","sentences":["Discovering an informative, or agent-centric, state representation that encodes only the relevant information while discarding the irrelevant is a key challenge towards scaling reinforcement learning algorithms and efficiently applying them to downstream tasks.","Prior works studied this problem in high-dimensional Markovian environments, when the current observation may be a complex object but is sufficient to decode the informative state.","In this work, we consider the problem of discovering the agent-centric state in the more challenging high-dimensional non-Markovian setting, when the state can be decoded from a sequence of past observations.","We establish that generalized inverse models can be adapted for learning agent-centric state representation for this task.","Our results include asymptotic theory in the deterministic dynamics setting as well as counter-examples for alternative intuitive algorithms.","We complement these findings with a thorough empirical study on the agent-centric state discovery abilities of the different alternatives we put forward.","Particularly notable is our analysis of past actions, where we show that these can be a double-edged sword: making the algorithms more successful when used correctly and causing dramatic failure when used incorrectly."],"url":"http://arxiv.org/abs/2404.14552v1","category":"cs.LG"}
{"created":"2024-04-22 19:39:35","title":"Advancing a Consent-Forward Paradigm for Digital Mental Health Data","abstract":"The field of digital mental health is advancing at a rapid pace. Passively collected data from user engagements with digital tools and services continue to contribute new insights into mental health and illness. As the field of digital mental health grows, a concerning norm has been established -- digital service users are given little say over how their data is collected, shared, or used to generate revenue for private companies. Given a long history of service user exclusion from data collection practices, we propose an alternative approach that is attentive to this history: the consent-forward paradigm. This paradigm embeds principles of affirmative consent in the design of digital mental health tools and services, strengthening trust through designing around individual choices and needs, and proactively protecting users from unexpected harm. In this perspective, we outline practical steps to implement this paradigm, toward ensuring that people searching for care have the safest experiences possible.","sentences":["The field of digital mental health is advancing at a rapid pace.","Passively collected data from user engagements with digital tools and services continue to contribute new insights into mental health and illness.","As the field of digital mental health grows, a concerning norm has been established -- digital service users are given little say over how their data is collected, shared, or used to generate revenue for private companies.","Given a long history of service user exclusion from data collection practices, we propose an alternative approach that is attentive to this history: the consent-forward paradigm.","This paradigm embeds principles of affirmative consent in the design of digital mental health tools and services, strengthening trust through designing around individual choices and needs, and proactively protecting users from unexpected harm.","In this perspective, we outline practical steps to implement this paradigm, toward ensuring that people searching for care have the safest experiences possible."],"url":"http://arxiv.org/abs/2404.14548v1","category":"cs.CY"}
{"created":"2024-04-22 19:38:37","title":"Integrating Disambiguation and User Preferences into Large Language Models for Robot Motion Planning","abstract":"This paper presents a framework that can interpret humans' navigation commands containing temporal elements and directly translate their natural language instructions into robot motion planning. Central to our framework is utilizing Large Language Models (LLMs). To enhance the reliability of LLMs in the framework and improve user experience, we propose methods to resolve the ambiguity in natural language instructions and capture user preferences. The process begins with an ambiguity classifier, identifying potential uncertainties in the instructions. Ambiguous statements trigger a GPT-4-based mechanism that generates clarifying questions, incorporating user responses for disambiguation. Also, the framework assesses and records user preferences for non-ambiguous instructions, enhancing future interactions. The last part of this process is the translation of disambiguated instructions into a robot motion plan using Linear Temporal Logic. This paper details the development of this framework and the evaluation of its performance in various test scenarios.","sentences":["This paper presents a framework that can interpret humans' navigation commands containing temporal elements and directly translate their natural language instructions into robot motion planning.","Central to our framework is utilizing Large Language Models (LLMs).","To enhance the reliability of LLMs in the framework and improve user experience, we propose methods to resolve the ambiguity in natural language instructions and capture user preferences.","The process begins with an ambiguity classifier, identifying potential uncertainties in the instructions.","Ambiguous statements trigger a GPT-4-based mechanism that generates clarifying questions, incorporating user responses for disambiguation.","Also, the framework assesses and records user preferences for non-ambiguous instructions, enhancing future interactions.","The last part of this process is the translation of disambiguated instructions into a robot motion plan using Linear Temporal Logic.","This paper details the development of this framework and the evaluation of its performance in various test scenarios."],"url":"http://arxiv.org/abs/2404.14547v1","category":"cs.RO"}
{"created":"2024-04-22 19:31:36","title":"Fast Monte Carlo Dose Calculation in Proton Therapy","abstract":"This article examines the critical role of fast Monte Carlo dose calculations in advancing proton therapy techniques, particularly in the context of increasing treatment customization and precision. As adaptive radiotherapy and other patient-specific approaches evolve, the need for accurate and precise dose calculations, essential for techniques like proton-based stereotactic radiosurgery, becomes more prominent. These calculations, however, are time-intensive, with the treatment planning/optimization process constrained by the achievable speed of dose computations. Thus, enhancing the speed of Monte Carlo methods is vital, as it not only facilitates the implementation of novel treatment modalities but also improves the optimality of treatment plans. Today, the state-of-the-art in Monte Carlo dose calculation speeds is 106 - 107 protons per second. This review highlights the latest advancements in fast Monte Carlo dose calculations that have led to such speeds, including emerging artificial intelligence-based techniques, and discusses their application in both current and emerging proton therapy strategies.","sentences":["This article examines the critical role of fast Monte Carlo dose calculations in advancing proton therapy techniques, particularly in the context of increasing treatment customization and precision.","As adaptive radiotherapy and other patient-specific approaches evolve, the need for accurate and precise dose calculations, essential for techniques like proton-based stereotactic radiosurgery, becomes more prominent.","These calculations, however, are time-intensive, with the treatment planning/optimization process constrained by the achievable speed of dose computations.","Thus, enhancing the speed of Monte Carlo methods is vital, as it not only facilitates the implementation of novel treatment modalities but also improves the optimality of treatment plans.","Today, the state-of-the-art in Monte Carlo dose calculation speeds is 106 - 107 protons per second.","This review highlights the latest advancements in fast Monte Carlo dose calculations that have led to such speeds, including emerging artificial intelligence-based techniques, and discusses their application in both current and emerging proton therapy strategies."],"url":"http://arxiv.org/abs/2404.14543v1","category":"physics.med-ph"}
{"created":"2024-04-22 18:45:40","title":"Edge-Assisted ML-Aided Uncertainty-Aware Vehicle Collision Avoidance at Urban Intersections","abstract":"Intersection crossing represents one of the most dangerous sections of the road infrastructure and Connected Vehicles (CVs) can serve as a revolutionary solution to the problem. In this work, we present a novel framework that detects preemptively collisions at urban crossroads, exploiting the Multi-access Edge Computing (MEC) platform of 5G networks. At the MEC, an Intersection Manager (IM) collects information from both vehicles and the road infrastructure to create a holistic view of the area of interest. Based on the historical data collected, the IM leverages the capabilities of an encoder-decoder recurrent neural network to predict, with high accuracy, the future vehicles' trajectories. As, however, accuracy is not a sufficient measure of how much we can trust a model, trajectory predictions are additionally associated with a measure of uncertainty towards confident collision forecasting and avoidance. Hence, contrary to any other approach in the state of the art, an uncertainty-aware collision prediction framework is developed that is shown to detect well in advance (and with high reliability) if two vehicles are on a collision course. Subsequently, collision detection triggers a number of alarms that signal the colliding vehicles to brake. Under real-world settings, thanks to the preemptive capabilities of the proposed approach, all the simulated imminent dangers are averted.","sentences":["Intersection crossing represents one of the most dangerous sections of the road infrastructure and Connected Vehicles (CVs) can serve as a revolutionary solution to the problem.","In this work, we present a novel framework that detects preemptively collisions at urban crossroads, exploiting the Multi-access Edge Computing (MEC) platform of 5G networks.","At the MEC, an Intersection Manager (IM) collects information from both vehicles and the road infrastructure to create a holistic view of the area of interest.","Based on the historical data collected, the IM leverages the capabilities of an encoder-decoder recurrent neural network to predict, with high accuracy, the future vehicles' trajectories.","As, however, accuracy is not a sufficient measure of how much we can trust a model, trajectory predictions are additionally associated with a measure of uncertainty towards confident collision forecasting and avoidance.","Hence, contrary to any other approach in the state of the art, an uncertainty-aware collision prediction framework is developed that is shown to detect well in advance (and with high reliability) if two vehicles are on a collision course.","Subsequently, collision detection triggers a number of alarms that signal the colliding vehicles to brake.","Under real-world settings, thanks to the preemptive capabilities of the proposed approach, all the simulated imminent dangers are averted."],"url":"http://arxiv.org/abs/2404.14523v1","category":"cs.LG"}
{"created":"2024-04-22 18:39:31","title":"Guided By AI: Navigating Trust, Bias, and Data Exploration in AI-Guided Visual Analytics","abstract":"The increasing integration of artificial intelligence (AI) in visual analytics (VA) tools raises vital questions about the behavior of users, their trust, and the potential of induced biases when provided with guidance during data exploration. We present an experiment where participants engaged in a visual data exploration task while receiving intelligent suggestions supplemented with four different transparency levels. We also modulated the difficulty of the task (easy or hard) to simulate a more tedious scenario for the analyst. Our results indicate that participants were more inclined to accept suggestions when completing a more difficult task despite the AI's lower suggestion accuracy. Moreover, the levels of transparency tested in this study did not significantly affect suggestion usage or subjective trust ratings of the participants. Additionally, we observed that participants who utilized suggestions throughout the task explored a greater quantity and diversity of data points. We discuss these findings and the implications of this research for improving the design and effectiveness of AI-guided VA tools.","sentences":["The increasing integration of artificial intelligence (AI) in visual analytics (VA) tools raises vital questions about the behavior of users, their trust, and the potential of induced biases when provided with guidance during data exploration.","We present an experiment where participants engaged in a visual data exploration task while receiving intelligent suggestions supplemented with four different transparency levels.","We also modulated the difficulty of the task (easy or hard) to simulate a more tedious scenario for the analyst.","Our results indicate that participants were more inclined to accept suggestions when completing a more difficult task despite the AI's lower suggestion accuracy.","Moreover, the levels of transparency tested in this study did not significantly affect suggestion usage or subjective trust ratings of the participants.","Additionally, we observed that participants who utilized suggestions throughout the task explored a greater quantity and diversity of data points.","We discuss these findings and the implications of this research for improving the design and effectiveness of AI-guided VA tools."],"url":"http://arxiv.org/abs/2404.14521v1","category":"cs.HC"}
{"created":"2024-04-22 18:33:06","title":"Instrument Signature Removal and Calibration Products for the Rubin Legacy Survey of Space and Time","abstract":"The Vera C. Rubin Legacy Survey of Space and Time (LSST) will conduct an unprecedented optical survey of the southern sky, imaging the entire available sky every few nights for 10 years. To achieve its ambitious science goals of probing dark energy and dark matter, mapping the Milky Way, and exploring the transient optical sky, the systematic errors in the LSST data must be exquisitely controlled. Instrument signature removal (ISR) is a critical early step in LSST data processing to remove inherent camera effects from the raw images and produce accurate representations of the incoming light. This paper describes the current state of the ISR pipelines implemented in the LSST Science Pipelines software. The key steps in ISR are outlined, and the process of generating and verifying the necessary calibration products to carry out ISR is also discussed. Finally, an overview is given of how the Rubin data management system utilizes a data Butler and calibration collections to organize datasets and match images to appropriate calibrations during processing. Precise ISR will be essential to realize the potential of LSST to revolutionize astrophysics.","sentences":["The Vera C. Rubin Legacy Survey of Space and Time (LSST) will conduct an unprecedented optical survey of the southern sky, imaging the entire available sky every few nights for 10 years.","To achieve its ambitious science goals of probing dark energy and dark matter, mapping the Milky Way, and exploring the transient optical sky, the systematic errors in the LSST data must be exquisitely controlled.","Instrument signature removal (ISR) is a critical early step in LSST data processing to remove inherent camera effects from the raw images and produce accurate representations of the incoming light.","This paper describes the current state of the ISR pipelines implemented in the LSST Science Pipelines software.","The key steps in ISR are outlined, and the process of generating and verifying the necessary calibration products to carry out ISR is also discussed.","Finally, an overview is given of how the Rubin data management system utilizes a data Butler and calibration collections to organize datasets and match images to appropriate calibrations during processing.","Precise ISR will be essential to realize the potential of LSST to revolutionize astrophysics."],"url":"http://arxiv.org/abs/2404.14516v1","category":"astro-ph.IM"}
{"created":"2024-04-22 18:18:19","title":"Shell-model study of $^{28}$Si: coexistence of oblate, prolate and superdeformed shapes","abstract":"We study the shape coexistence in the nucleus $^{28}$Si with the nuclear shell model using numerical diagonalizations complemented with variational calculations based on the projected generator-coordinate method. The theoretical electric quadrupole moments and transitions as well as the collective wavefunctions indicate that the standard USDB interaction in the $sd$ shell describes well the ground-state oblate rotational band, but misses the experimental prolate band. Guided by the quasi-SU(3) model, we show that the prolate band can be reproduced in the $sd$ shell by reducing the energy of the $0d_{3/2}$ orbital. Alternatively, in the extended $sdpf$ configuration space the SDPF-NR interaction, which describes well other Si isotopes, also reproduces the oblate and prolate bands. Finally, we address the possibility of superdeformation in $^{28}$Si within the $sdpf$ space. Our results disfavour the appearance of superdeformed states with excitation energy below 20 MeV.","sentences":["We study the shape coexistence in the nucleus $^{28}$Si with the nuclear shell model using numerical diagonalizations complemented with variational calculations based on the projected generator-coordinate method.","The theoretical electric quadrupole moments and transitions as well as the collective wavefunctions indicate that the standard USDB interaction in the $sd$ shell describes well the ground-state oblate rotational band, but misses the experimental prolate band.","Guided by the quasi-SU(3) model, we show that the prolate band can be reproduced in the $sd$ shell by reducing the energy of the $0d_{3/2}$ orbital.","Alternatively, in the extended $sdpf$ configuration space the SDPF-NR interaction, which describes well other Si isotopes, also reproduces the oblate and prolate bands.","Finally, we address the possibility of superdeformation in $^{28}$Si within the $sdpf$ space.","Our results disfavour the appearance of superdeformed states with excitation energy below 20 MeV."],"url":"http://arxiv.org/abs/2404.14506v1","category":"nucl-th"}
{"created":"2024-04-22 18:00:59","title":"Death of the Immortal Molecular Cloud: Resolution Dependence of the Gas-Star Formation Relation Rules out Decoupling by Stellar Drift","abstract":"Recent observations have demonstrated that giant molecular clouds (GMCs) are short-lived entities, surviving for the order of a dynamical time before turning a few percent of their mass into stars and dispersing, leaving behind an isolated young stellar population. The key question has been whether this GMC dispersal actually marks a point of GMC destruction by stellar feedback from the new-born stars, or if GMCs might be `immortal' and only dynamically decouple from their nascent stars due to stellar drift. We address this question in six nearby galaxies, by quantifying how the gas-star formation relation depends on the spatial scale for scales between the GMC diameter and the GMC separation length, i.e. the scales where an excess of GMCs would be expected to be found in the stellar drift scenario. Our analysis reveals a consistent dearth of GMCs near young stellar populations regardless of the spatial scale, discounting the notion of `immortal' GMCs that decouple from their nascent stars through stellar drift. Instead, our findings demonstrate that stellar feedback destroys most GMCs at the end of their lifecycle. Employing a variety of statistical techniques to test both hypotheses, we find that the probability that stellar feedback concludes the GMC lifecycle is about 2,000 times higher than the probability that stellar drift separates GMCs and young stellar regions. This observation strengthens the emerging picture that galaxies consist of dynamic building blocks undergoing vigorous, feedback-driven lifecycles that collectively regulate star formation and drive the baryon cycle within galaxies.","sentences":["Recent observations have demonstrated that giant molecular clouds (GMCs) are short-lived entities, surviving for the order of a dynamical time before turning a few percent of their mass into stars and dispersing, leaving behind an isolated young stellar population.","The key question has been whether this GMC dispersal actually marks a point of GMC destruction by stellar feedback from the new-born stars, or if GMCs might be `immortal' and only dynamically decouple from their nascent stars due to stellar drift.","We address this question in six nearby galaxies, by quantifying how the gas-star formation relation depends on the spatial scale for scales between the GMC diameter and the GMC separation length, i.e. the scales where an excess of GMCs would be expected to be found in the stellar drift scenario.","Our analysis reveals a consistent dearth of GMCs near young stellar populations regardless of the spatial scale, discounting the notion of `immortal' GMCs that decouple from their nascent stars through stellar drift.","Instead, our findings demonstrate that stellar feedback destroys most GMCs at the end of their lifecycle.","Employing a variety of statistical techniques to test both hypotheses, we find that the probability that stellar feedback concludes the GMC lifecycle is about 2,000 times higher than the probability that stellar drift separates GMCs and young stellar regions.","This observation strengthens the emerging picture that galaxies consist of dynamic building blocks undergoing vigorous, feedback-driven lifecycles that collectively regulate star formation and drive the baryon cycle within galaxies."],"url":"http://arxiv.org/abs/2404.14495v1","category":"astro-ph.GA"}
{"created":"2024-04-22 17:42:58","title":"SnapKV: LLM Knows What You are Looking for Before Generation","abstract":"Large Language Models (LLMs) have made remarkable progress in processing extensive contexts, with the Key-Value (KV) cache playing a vital role in enhancing their performance. However, the growth of the KV cache in response to increasing input length poses challenges to memory and time efficiency. To address this problem, this paper introduces SnapKV, an innovative and fine-tuning-free approach that efficiently minimizes KV cache size while still delivering comparable performance in real-world applications.   We discover that each attention head in the model consistently focuses on specific prompt attention features during generation. Meanwhile, this robust pattern can be obtained from an `observation' window located at the end of the prompts. Drawing on this insight, SnapKV automatically compresses KV caches by selecting clustered important KV positions for each attention head. Our approach significantly reduces the growing computational overhead and memory footprint when processing long input sequences. Specifically, SnapKV achieves a consistent decoding speed with a 3.6x increase in generation speed and an 8.2x enhancement in memory efficiency compared to baseline when processing inputs of 16K tokens. At the same time, it maintains comparable performance to baseline models across 16 long sequence datasets. Moreover, SnapKV can process up to 380K context tokens on a single A100-80GB GPU using HuggingFace implementation with minor changes, exhibiting only a negligible accuracy drop in the Needle-in-a-Haystack test. Further comprehensive studies suggest SnapKV's potential for practical applications.","sentences":["Large Language Models (LLMs) have made remarkable progress in processing extensive contexts, with the Key-Value (KV) cache playing a vital role in enhancing their performance.","However, the growth of the KV cache in response to increasing input length poses challenges to memory and time efficiency.","To address this problem, this paper introduces SnapKV, an innovative and fine-tuning-free approach that efficiently minimizes KV cache size while still delivering comparable performance in real-world applications.   ","We discover that each attention head in the model consistently focuses on specific prompt attention features during generation.","Meanwhile, this robust pattern can be obtained from an `observation' window located at the end of the prompts.","Drawing on this insight, SnapKV automatically compresses KV caches by selecting clustered important KV positions for each attention head.","Our approach significantly reduces the growing computational overhead and memory footprint when processing long input sequences.","Specifically, SnapKV achieves a consistent decoding speed with a 3.6x increase in generation speed and an 8.2x enhancement in memory efficiency compared to baseline when processing inputs of 16K tokens.","At the same time, it maintains comparable performance to baseline models across 16 long sequence datasets.","Moreover, SnapKV can process up to 380K context tokens on a single A100-80GB GPU using HuggingFace implementation with minor changes, exhibiting only a negligible accuracy drop in the Needle-in-a-Haystack test.","Further comprehensive studies suggest SnapKV's potential for practical applications."],"url":"http://arxiv.org/abs/2404.14469v1","category":"cs.CL"}
{"created":"2024-04-22 17:36:33","title":"Pixels and Predictions: Potential of GPT-4V in Meteorological Imagery Analysis and Forecast Communication","abstract":"Generative AI, such as OpenAI's GPT-4V large-language model, has rapidly entered mainstream discourse. Novel capabilities in image processing and natural-language communication may augment existing forecasting methods. Large language models further display potential to better communicate weather hazards in a style honed for diverse communities and different languages. This study evaluates GPT-4V's ability to interpret meteorological charts and communicate weather hazards appropriately to the user, despite challenges of hallucinations, where generative AI delivers coherent, confident, but incorrect responses. We assess GPT-4V's competence via its web interface ChatGPT in two tasks: (1) generating a severe-weather outlook from weather-chart analysis and conducting self-evaluation, revealing an outlook that corresponds well with a Storm Prediction Center human-issued forecast; and (2) producing hazard summaries in Spanish and English from weather charts. Responses in Spanish, however, resemble direct (not idiomatic) translations from English to Spanish, yielding poorly translated summaries that lose critical idiomatic precision required for optimal communication. Our findings advocate for cautious integration of tools like GPT-4V in meteorology, underscoring the necessity of human oversight and development of trustworthy, explainable AI.","sentences":["Generative AI, such as OpenAI's GPT-4V large-language model, has rapidly entered mainstream discourse.","Novel capabilities in image processing and natural-language communication may augment existing forecasting methods.","Large language models further display potential to better communicate weather hazards in a style honed for diverse communities and different languages.","This study evaluates GPT-4V's ability to interpret meteorological charts and communicate weather hazards appropriately to the user, despite challenges of hallucinations, where generative AI delivers coherent, confident, but incorrect responses.","We assess GPT-4V's competence via its web interface ChatGPT in two tasks: (1) generating a severe-weather outlook from weather-chart analysis and conducting self-evaluation, revealing an outlook that corresponds well with a Storm Prediction Center human-issued forecast; and (2) producing hazard summaries in Spanish and English from weather charts.","Responses in Spanish, however, resemble direct (not idiomatic) translations from English to Spanish, yielding poorly translated summaries that lose critical idiomatic precision required for optimal communication.","Our findings advocate for cautious integration of tools like GPT-4V in meteorology, underscoring the necessity of human oversight and development of trustworthy, explainable AI."],"url":"http://arxiv.org/abs/2404.15166v1","category":"cs.CL"}
{"created":"2024-04-22 16:55:44","title":"Integrating Chemistry Knowledge in Large Language Models via Prompt Engineering","abstract":"This paper presents a study on the integration of domain-specific knowledge in prompt engineering to enhance the performance of large language models (LLMs) in scientific domains. A benchmark dataset is curated to encapsulate the intricate physical-chemical properties of small molecules, their drugability for pharmacology, alongside the functional attributes of enzymes and crystal materials, underscoring the relevance and applicability across biological and chemical domains.The proposed domain-knowledge embedded prompt engineering method outperforms traditional prompt engineering strategies on various metrics, including capability, accuracy, F1 score, and hallucination drop. The effectiveness of the method is demonstrated through case studies on complex materials including the MacMillan catalyst, paclitaxel, and lithium cobalt oxide. The results suggest that domain-knowledge prompts can guide LLMs to generate more accurate and relevant responses, highlighting the potential of LLMs as powerful tools for scientific discovery and innovation when equipped with domain-specific prompts. The study also discusses limitations and future directions for domain-specific prompt engineering development.","sentences":["This paper presents a study on the integration of domain-specific knowledge in prompt engineering to enhance the performance of large language models (LLMs) in scientific domains.","A benchmark dataset is curated to encapsulate the intricate physical-chemical properties of small molecules, their drugability for pharmacology, alongside the functional attributes of enzymes and crystal materials, underscoring the relevance and applicability across biological and chemical domains.","The proposed domain-knowledge embedded prompt engineering method outperforms traditional prompt engineering strategies on various metrics, including capability, accuracy, F1 score, and hallucination drop.","The effectiveness of the method is demonstrated through case studies on complex materials including the MacMillan catalyst, paclitaxel, and lithium cobalt oxide.","The results suggest that domain-knowledge prompts can guide LLMs to generate more accurate and relevant responses, highlighting the potential of LLMs as powerful tools for scientific discovery and innovation when equipped with domain-specific prompts.","The study also discusses limitations and future directions for domain-specific prompt engineering development."],"url":"http://arxiv.org/abs/2404.14467v1","category":"cs.CL"}
{"created":"2024-04-22 16:33:42","title":"Expert Router: Orchestrating Efficient Language Model Inference through Prompt Classification","abstract":"Large Language Models (LLMs) have experienced widespread adoption across scientific and industrial domains due to their versatility and utility for diverse tasks. Nevertheless, deploying and serving these models at scale with optimal throughput and latency remains a significant challenge, primarily because of the high computational and memory demands associated with LLMs. To tackle this limitation, we introduce Expert Router, a system designed to orchestrate multiple expert models efficiently, thereby enhancing scalability. Expert Router is a parallel inference system with a central routing gateway that distributes incoming requests using a clustering method. This approach effectively partitions incoming requests among available LLMs, maximizing overall throughput. Our extensive evaluations encompassed up to 1,000 concurrent users, providing comprehensive insights into the system's behavior from user and infrastructure perspectives. The results demonstrate Expert Router's effectiveness in handling high-load scenarios and achieving higher throughput rates, particularly under many concurrent users.","sentences":["Large Language Models (LLMs) have experienced widespread adoption across scientific and industrial domains due to their versatility and utility for diverse tasks.","Nevertheless, deploying and serving these models at scale with optimal throughput and latency remains a significant challenge, primarily because of the high computational and memory demands associated with LLMs.","To tackle this limitation, we introduce Expert Router, a system designed to orchestrate multiple expert models efficiently, thereby enhancing scalability.","Expert Router is a parallel inference system with a central routing gateway that distributes incoming requests using a clustering method.","This approach effectively partitions incoming requests among available LLMs, maximizing overall throughput.","Our extensive evaluations encompassed up to 1,000 concurrent users, providing comprehensive insights into the system's behavior from user and infrastructure perspectives.","The results demonstrate Expert Router's effectiveness in handling high-load scenarios and achieving higher throughput rates, particularly under many concurrent users."],"url":"http://arxiv.org/abs/2404.15153v1","category":"cs.CL"}
{"created":"2024-04-23 17:59:12","title":"Metric-guided Image Reconstruction Bounds via Conformal Prediction","abstract":"Recent advancements in machine learning have led to novel imaging systems and algorithms that address ill-posed problems. Assessing their trustworthiness and understanding how to deploy them safely at test time remains an important and open problem. We propose a method that leverages conformal prediction to retrieve upper/lower bounds and statistical inliers/outliers of reconstructions based on the prediction intervals of downstream metrics. We apply our method to sparse-view CT for downstream radiotherapy planning and show 1) that metric-guided bounds have valid coverage for downstream metrics while conventional pixel-wise bounds do not and 2) anatomical differences of upper/lower bounds between metric-guided and pixel-wise methods. Our work paves the way for more meaningful reconstruction bounds. Code available at https://github.com/matthewyccheung/conformal-metric","sentences":["Recent advancements in machine learning have led to novel imaging systems and algorithms that address ill-posed problems.","Assessing their trustworthiness and understanding how to deploy them safely at test time remains an important and open problem.","We propose a method that leverages conformal prediction to retrieve upper/lower bounds and statistical inliers/outliers of reconstructions based on the prediction intervals of downstream metrics.","We apply our method to sparse-view CT for downstream radiotherapy planning and show 1) that metric-guided bounds have valid coverage for downstream metrics while conventional pixel-wise bounds do not and 2) anatomical differences of upper/lower bounds between metric-guided and pixel-wise methods.","Our work paves the way for more meaningful reconstruction bounds.","Code available at https://github.com/matthewyccheung/conformal-metric"],"url":"http://arxiv.org/abs/2404.15274v1","category":"cs.LG"}
{"created":"2024-04-23 17:59:09","title":"Estimation Network Design framework for efficient distributed optimization","abstract":"Distributed decision problems features a group of agents that can only communicate over a peer-to-peer network, without a central memory. In applications such as network control and data ranking, each agent is only affected by a small portion of the decision vector: this sparsity is typically ignored in distributed algorithms, while it could be leveraged to improve efficiency and scalability. To address this issue, our recent paper introduces Estimation Network Design (END), a graph theoretical language for the analysis and design of distributed iterations. END algorithms can be tuned to exploit the sparsity of specific problem instances, reducing communication overhead and minimizing redundancy, yet without requiring case-by-case convergence analysis. In this paper, we showcase the flexility of END in the context of distributed optimization. In particular, we study the sparsity-aware version of many established methods, including ADMM, AugDGM and Push-Sum DGD. Simulations on an estimation problem in sensor networks demonstrate that END algorithms can boost convergence speed and greatly reduce the communication and memory cost.","sentences":["Distributed decision problems features a group of agents that can only communicate over a peer-to-peer network, without a central memory.","In applications such as network control and data ranking, each agent is only affected by a small portion of the decision vector: this sparsity is typically ignored in distributed algorithms, while it could be leveraged to improve efficiency and scalability.","To address this issue, our recent paper introduces Estimation Network Design (END), a graph theoretical language for the analysis and design of distributed iterations.","END algorithms can be tuned to exploit the sparsity of specific problem instances, reducing communication overhead and minimizing redundancy, yet without requiring case-by-case convergence analysis.","In this paper, we showcase the flexility of END in the context of distributed optimization.","In particular, we study the sparsity-aware version of many established methods, including ADMM, AugDGM and Push-Sum DGD.","Simulations on an estimation problem in sensor networks demonstrate that END algorithms can boost convergence speed and greatly reduce the communication and memory cost."],"url":"http://arxiv.org/abs/2404.15273v1","category":"math.OC"}
{"created":"2024-04-23 17:57:51","title":"Chiral TeraHertz surface plasmonics","abstract":"Chiral engineering of TeraHertz (THz) light fields and the use of the handedness of light in THz light-matter interactions promise many novel opportunities for advanced sensing and control of matter in this frequency range. Unlike previously explored methods, this is achieved here by leveraging the chiral properties of highly confined THz surface plasmon modes. More specifically, we design ultrasmall surface plasmonic-based THz cavities and THz metasurfaces that display significant and adjustable chiral behavior under modest magnetic fields. For such a prototypical example of non-hermitian and dispersive photonic system, we demonstrate the capacity to magnetic field-tune both the poles and zeros of cavity resonances, the two fundamental parameters governing their resonance properties. Alongside the observed handedness-dependent cavity frequencies, this highlights the remarkable ability to engineer chiral and tunable radiative couplings for THz resonators and metasurfaces. The extensive tunability offered by the surface plasmonic approach paves the way for the development of agile and multifunctional THz metasurfaces as well as the realization of ultrastrong chiral light-matter interactions at low energy in matter with potential far-reaching applications for the design of material properties.","sentences":["Chiral engineering of TeraHertz (THz) light fields and the use of the handedness of light in THz light-matter interactions promise many novel opportunities for advanced sensing and control of matter in this frequency range.","Unlike previously explored methods, this is achieved here by leveraging the chiral properties of highly confined THz surface plasmon modes.","More specifically, we design ultrasmall surface plasmonic-based THz cavities and THz metasurfaces that display significant and adjustable chiral behavior under modest magnetic fields.","For such a prototypical example of non-hermitian and dispersive photonic system, we demonstrate the capacity to magnetic field-tune both the poles and zeros of cavity resonances, the two fundamental parameters governing their resonance properties.","Alongside the observed handedness-dependent cavity frequencies, this highlights the remarkable ability to engineer chiral and tunable radiative couplings for THz resonators and metasurfaces.","The extensive tunability offered by the surface plasmonic approach paves the way for the development of agile and multifunctional THz metasurfaces as well as the realization of ultrastrong chiral light-matter interactions at low energy in matter with potential far-reaching applications for the design of material properties."],"url":"http://arxiv.org/abs/2404.15270v1","category":"physics.optics"}
{"created":"2024-04-23 17:55:49","title":"Quantum optical classifier with superexponential speedup","abstract":"We present a quantum optical pattern recognition method for binary classification tasks. Without direct image reconstruction, it classifies an object in terms of the rate of two-photon coincidences at the output of a Hong-Ou-Mandel interferometer, where both the input and the classifier parameters are encoded into single-photon states. Our method exhibits the same behaviour of a classical neuron of unit depth. Once trained, it shows a constant $\\mathcal{O}(1)$ complexity in the number of computational operations and photons required by a single classification. This is a superexponential advantage over a classical neuron (that is at least linear in the image resolution). We provide simulations and analytical comparisons with analogous neural network architectures.","sentences":["We present a quantum optical pattern recognition method for binary classification tasks.","Without direct image reconstruction, it classifies an object in terms of the rate of two-photon coincidences at the output of a Hong-Ou-Mandel interferometer, where both the input and the classifier parameters are encoded into single-photon states.","Our method exhibits the same behaviour of a classical neuron of unit depth.","Once trained, it shows a constant $\\mathcal{O}(1)$ complexity in the number of computational operations and photons required by a single classification.","This is a superexponential advantage over a classical neuron (that is at least linear in the image resolution).","We provide simulations and analytical comparisons with analogous neural network architectures."],"url":"http://arxiv.org/abs/2404.15266v1","category":"quant-ph"}
{"created":"2024-04-23 17:55:05","title":"Multi-Session SLAM with Differentiable Wide-Baseline Pose Optimization","abstract":"We introduce a new system for Multi-Session SLAM, which tracks camera motion across multiple disjoint videos under a single global reference. Our approach couples the prediction of optical flow with solver layers to estimate camera pose. The backbone is trained end-to-end using a novel differentiable solver for wide-baseline two-view pose. The full system can connect disjoint sequences, perform visual odometry, and global optimization. Compared to existing approaches, our design is accurate and robust to catastrophic failures. Code is available at github.com/princeton-vl/MultiSlam_DiffPose","sentences":["We introduce a new system for Multi-Session SLAM, which tracks camera motion across multiple disjoint videos under a single global reference.","Our approach couples the prediction of optical flow with solver layers to estimate camera pose.","The backbone is trained end-to-end using a novel differentiable solver for wide-baseline two-view pose.","The full system can connect disjoint sequences, perform visual odometry, and global optimization.","Compared to existing approaches, our design is accurate and robust to catastrophic failures.","Code is available at github.com/princeton-vl/MultiSlam_DiffPose"],"url":"http://arxiv.org/abs/2404.15263v1","category":"cs.CV"}
{"created":"2024-04-23 17:39:27","title":"UniMERNet: A Universal Network for Real-World Mathematical Expression Recognition","abstract":"This paper presents the UniMER dataset to provide the first study on Mathematical Expression Recognition (MER) towards complex real-world scenarios. The UniMER dataset consists of a large-scale training set UniMER-1M offering an unprecedented scale and diversity with one million training instances and a meticulously designed test set UniMER-Test that reflects a diverse range of formula distributions prevalent in real-world scenarios. Therefore, the UniMER dataset enables the training of a robust and high-accuracy MER model and comprehensive evaluation of model performance. Moreover, we introduce the Universal Mathematical Expression Recognition Network (UniMERNet), an innovative framework designed to enhance MER in practical scenarios. UniMERNet incorporates a Length-Aware Module to process formulas of varied lengths efficiently, thereby enabling the model to handle complex mathematical expressions with greater accuracy. In addition, UniMERNet employs our UniMER-1M data and image augmentation techniques to improve the model's robustness under different noise conditions. Our extensive experiments demonstrate that UniMERNet outperforms existing MER models, setting a new benchmark in various scenarios and ensuring superior recognition quality in real-world applications. The dataset and model are available at https://github.com/opendatalab/UniMERNet.","sentences":["This paper presents the UniMER dataset to provide the first study on Mathematical Expression Recognition (MER) towards complex real-world scenarios.","The UniMER dataset consists of a large-scale training set UniMER-1M offering an unprecedented scale and diversity with one million training instances and a meticulously designed test set UniMER-Test that reflects a diverse range of formula distributions prevalent in real-world scenarios.","Therefore, the UniMER dataset enables the training of a robust and high-accuracy MER model and comprehensive evaluation of model performance.","Moreover, we introduce the Universal Mathematical Expression Recognition Network (UniMERNet), an innovative framework designed to enhance MER in practical scenarios.","UniMERNet incorporates a Length-Aware Module to process formulas of varied lengths efficiently, thereby enabling the model to handle complex mathematical expressions with greater accuracy.","In addition, UniMERNet employs our UniMER-1M data and image augmentation techniques to improve the model's robustness under different noise conditions.","Our extensive experiments demonstrate that UniMERNet outperforms existing MER models, setting a new benchmark in various scenarios and ensuring superior recognition quality in real-world applications.","The dataset and model are available at https://github.com/opendatalab/UniMERNet."],"url":"http://arxiv.org/abs/2404.15254v1","category":"cs.CV"}
{"created":"2024-04-23 17:38:37","title":"Nucleation mechanism of multiple-order parameter ferroelectric domain wall motion in hafnia","abstract":"Ferroelectric hafnia exhibits promising robust polarization and silicon compatibility for ferroelectric devices. Unfortunately, it suffers from difficult polarization switching. Methods to enable easier polarization switching are needed, and the underlying reason for this switching difficulty is not understood. Here, we investigated the 180$^\\circ$ domain walls of hafnia and their motion through nucleation. We found that the domains of multiple-order parameter hafnia possess complicated three-dimensional dipole patterns and lead to domain walls of different symmetry. The most common domain wall type is a complex domain wall involving reversal of both polarization and tetragonality order parameters. This domain wall symmetry ensures a good matching of the dipoles perpendicular to the domain wall, which leads to low domain wall energy. However, this ensures a sharp, high energy, charged domain wall on the edges of nuclei that results in difficult nucleation. Thus, this domain wall is too stable to move, which explains the switching difficulty of hafnia. By contrast, another simple domain wall, involving only polarization reversal, has a poor matching of dipoles perpendicular to the domain wall. This leads to higher domain wall energy and ensures a diffusive and low energy charged domain wall that enables easier nucleation. This simple domain wall is thus not too stable and easier to move. Our theory advances domain wall nucleation theory from the field of conventional single-order parameter to multiple-order parameters. We propose controlling the populations of different domain wall types in hafnia as a way to enable fast polarization switching and lower coercive fields.","sentences":["Ferroelectric hafnia exhibits promising robust polarization and silicon compatibility for ferroelectric devices.","Unfortunately, it suffers from difficult polarization switching.","Methods to enable easier polarization switching are needed, and the underlying reason for this switching difficulty is not understood.","Here, we investigated the 180$^\\circ$ domain walls of hafnia and their motion through nucleation.","We found that the domains of multiple-order parameter hafnia possess complicated three-dimensional dipole patterns and lead to domain walls of different symmetry.","The most common domain wall type is a complex domain wall involving reversal of both polarization and tetragonality order parameters.","This domain wall symmetry ensures a good matching of the dipoles perpendicular to the domain wall, which leads to low domain wall energy.","However, this ensures a sharp, high energy, charged domain wall on the edges of nuclei that results in difficult nucleation.","Thus, this domain wall is too stable to move, which explains the switching difficulty of hafnia.","By contrast, another simple domain wall, involving only polarization reversal, has a poor matching of dipoles perpendicular to the domain wall.","This leads to higher domain wall energy and ensures a diffusive and low energy charged domain wall that enables easier nucleation.","This simple domain wall is thus not too stable and easier to move.","Our theory advances domain wall nucleation theory from the field of conventional single-order parameter to multiple-order parameters.","We propose controlling the populations of different domain wall types in hafnia as a way to enable fast polarization switching and lower coercive fields."],"url":"http://arxiv.org/abs/2404.15251v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-23 17:36:59","title":"Unifying the Temperature Dependent Dynamics of Glasses","abstract":"Strong changes in bulk properties, such as modulus and viscosity, are observed near the glass transition temperature, T_{g}, of amorphous materials. For more than a century, intense efforts have been made to define a microscopic origin for these macroscopic changes in properties. Using transition state theory, we delve into the atomic/molecular level picture of how microscopic localized relaxations, or \"cage rattles,\" translate to macroscopic structural relaxations above T_{g}. Unit motion is broken down into two populations: (1) simultaneous rearrangement occurs among a critical number of units, n_{\\alpha}, which ranges from 1 to 4, allowing a systematic classification of glasses that is compared to fragility; (2) near T_{g}, adjacent units provide additional free volume for rearrangement, not simultaneously, but within the \"primitive\" lifetime, {\\tau}_{1}, of one unit rattling in its cage. Relaxation maps illustrate how Johari-Goldstein \\{beta} relaxations stem from the rattle of n_{\\alpha} units. We analyzed a wide variety of glassy materials, and materials with glassy response, using literature data. Our four-parameter equation fits \"strong\" and \"weak\" glasses over the entire range of temperatures and also extends to other glassy systems, such as ion-transporting polymers and ferroelectric relaxors. The role of activation entropy in boosting preexponential factors to high \"unphysical\" apparent frequencies is discussed.","sentences":["Strong changes in bulk properties, such as modulus and viscosity, are observed near the glass transition temperature, T_{g}, of amorphous materials.","For more than a century, intense efforts have been made to define a microscopic origin for these macroscopic changes in properties.","Using transition state theory, we delve into the atomic/molecular level picture of how microscopic localized relaxations, or \"cage rattles,\" translate to macroscopic structural relaxations above T_{g}.","Unit motion is broken down into two populations: (1) simultaneous rearrangement occurs among a critical number of units, n_{\\alpha}, which ranges from 1 to 4, allowing a systematic classification of glasses that is compared to fragility; (2) near T_{g}, adjacent units provide additional free volume for rearrangement, not simultaneously, but within the \"primitive\" lifetime, {\\tau}_{1}, of one unit rattling in its cage.","Relaxation maps illustrate how Johari-Goldstein \\{beta} relaxations stem from the rattle of n_{\\alpha} units.","We analyzed a wide variety of glassy materials, and materials with glassy response, using literature data.","Our four-parameter equation fits \"strong\" and \"weak\" glasses over the entire range of temperatures and also extends to other glassy systems, such as ion-transporting polymers and ferroelectric relaxors.","The role of activation entropy in boosting preexponential factors to high \"unphysical\" apparent frequencies is discussed."],"url":"http://arxiv.org/abs/2404.15250v1","category":"cond-mat.dis-nn"}
{"created":"2024-04-23 17:33:04","title":"A Dependency Pair Framework for Relative Termination of Term Rewriting","abstract":"Dependency pairs are one of the most powerful techniques for proving termination of term rewrite systems (TRSs), and they are used in almost all tools for termination analysis of TRSs. Problem #106 of the RTA List of Open Problems asks for an adaption of dependency pairs for relative termination. Here, infinite rewrite sequences are allowed, but one wants to prove that a certain subset of the rewrite rules cannot be used infinitely often. Dependency pairs were recently adapted to annotated dependency pairs (ADPs) to prove almost-sure termination of probabilistic TRSs. In this paper, we develop a novel adaption of ADPs for relative termination. We implemented our new ADP framework in our tool AProVE and evaluate it in comparison to state-of-the-art tools for relative termination of TRSs.","sentences":["Dependency pairs are one of the most powerful techniques for proving termination of term rewrite systems (TRSs), and they are used in almost all tools for termination analysis of TRSs.","Problem #106 of the RTA List of Open Problems asks for an adaption of dependency pairs for relative termination.","Here, infinite rewrite sequences are allowed, but one wants to prove that a certain subset of the rewrite rules cannot be used infinitely often.","Dependency pairs were recently adapted to annotated dependency pairs (ADPs) to prove almost-sure termination of probabilistic TRSs.","In this paper, we develop a novel adaption of ADPs for relative termination.","We implemented our new ADP framework in our tool AProVE and evaluate it in comparison to state-of-the-art tools for relative termination of TRSs."],"url":"http://arxiv.org/abs/2404.15248v1","category":"cs.LO"}
{"created":"2024-04-23 17:28:52","title":"Efficient Multi-Processor Scheduling in Increasingly Realistic Models","abstract":"We study the problem of efficiently scheduling a computational DAG on multiple processors. The majority of previous works have developed and compared algorithms for this problem in relatively simple models; in contrast to this, we analyze this problem in a more realistic model that captures many real-world aspects, such as communication costs, synchronization costs, and the hierarchical structure of modern processing architectures. For this we extend the well-established BSP model of parallel computing with non-uniform memory access (NUMA) effects. We then develop a range of new scheduling algorithms to minimize the scheduling cost in this more complex setting: several initialization heuristics, a hill-climbing local search method, and several approaches that formulate (and solve) the scheduling problem as an Integer Linear Program (ILP). We combine these algorithms into a single framework, and conduct experiments on a diverse set of real-world computational DAGs to show that the resulting scheduler significantly outperforms both academic and practical baselines. In particular, even without NUMA effects, our scheduler finds solutions of 24%-44% smaller cost on average than the baselines, and in case of NUMA effects, it achieves up to a factor $2.5\\times$ improvement compared to the baselines. Finally, we also develop a multilevel scheduling algorithm, which provides up to almost a factor $5\\times$ improvement in the special case when the problem is dominated by very high communication costs.","sentences":["We study the problem of efficiently scheduling a computational DAG on multiple processors.","The majority of previous works have developed and compared algorithms for this problem in relatively simple models; in contrast to this, we analyze this problem in a more realistic model that captures many real-world aspects, such as communication costs, synchronization costs, and the hierarchical structure of modern processing architectures.","For this we extend the well-established BSP model of parallel computing with non-uniform memory access (NUMA) effects.","We then develop a range of new scheduling algorithms to minimize the scheduling cost in this more complex setting: several initialization heuristics, a hill-climbing local search method, and several approaches that formulate (and solve) the scheduling problem as an Integer Linear Program (ILP).","We combine these algorithms into a single framework, and conduct experiments on a diverse set of real-world computational DAGs to show that the resulting scheduler significantly outperforms both academic and practical baselines.","In particular, even without NUMA effects, our scheduler finds solutions of 24%-44% smaller cost on average than the baselines, and in case of NUMA effects, it achieves up to a factor $2.5\\times$ improvement compared to the baselines.","Finally, we also develop a multilevel scheduling algorithm, which provides up to almost a factor $5\\times$ improvement in the special case when the problem is dominated by very high communication costs."],"url":"http://arxiv.org/abs/2404.15246v1","category":"cs.DC"}
{"created":"2024-04-23 17:17:52","title":"Augmented Voices: An Augmented Reality Experience Highlighting the Social Injustices of Gender-Based Violence in the Muslim South-Asian Diaspora","abstract":"This paper delves into the distressing prevalence of gender-based violence (GBV) and its deep-seated psychological ramifications, particularly among Muslim South Asian women living in diasporic communities. Despite the gravity of GBV, these women often face formidable barriers in voicing their experiences and accessing support. \"Augmented Voices\" emerges as a technological beacon, harnessing the potential of augmented reality (AR) to bridge the digital and physical realms through mobile devices, enhancing the visibility of these often-silenced voices. With its technological motivation firmly anchored in the convergence of AR and real-world interactions, \"Augmented Voices\" offers a digital platform where storytelling acts as a catalyst, bringing to the fore the experiences shared by these women. By superimposing their narratives onto physical locations via Geographic Information System (GIS) Mapping, the application \"augments their voices\" in the diaspora, providing a conduit for expression and solidarity. This project, currently at its developmental stage, aspires to elevate the stories of GBV victims to a level where their struggles are not just heard but felt, forging a powerful connection between the user and the narrative. It is designed to transcend the limitations of conventional storytelling, creating an \"augmented\" reality where voices that are often muted by societal constraints can resonate powerfully. The project underscores the urgent imperative to confront GBV, catalyzing societal transformation and fostering robust support networks for those in the margins. It is a pioneering example of how technology can become a formidable ally in the fight for social justice and the empowerment of the oppressed. Additionally, this paper delves into the AR workflow illustrating its relevance and contribution to the broader theme of site-specific AR for social justice.","sentences":["This paper delves into the distressing prevalence of gender-based violence (GBV) and its deep-seated psychological ramifications, particularly among Muslim South Asian women living in diasporic communities.","Despite the gravity of GBV, these women often face formidable barriers in voicing their experiences and accessing support.","\"Augmented Voices\" emerges as a technological beacon, harnessing the potential of augmented reality (AR) to bridge the digital and physical realms through mobile devices, enhancing the visibility of these often-silenced voices.","With its technological motivation firmly anchored in the convergence of AR and real-world interactions, \"Augmented Voices\" offers a digital platform where storytelling acts as a catalyst, bringing to the fore the experiences shared by these women.","By superimposing their narratives onto physical locations via Geographic Information System (GIS) Mapping, the application \"augments their voices\" in the diaspora, providing a conduit for expression and solidarity.","This project, currently at its developmental stage, aspires to elevate the stories of GBV victims to a level where their struggles are not just heard but felt, forging a powerful connection between the user and the narrative.","It is designed to transcend the limitations of conventional storytelling, creating an \"augmented\" reality where voices that are often muted by societal constraints can resonate powerfully.","The project underscores the urgent imperative to confront GBV, catalyzing societal transformation and fostering robust support networks for those in the margins.","It is a pioneering example of how technology can become a formidable ally in the fight for social justice and the empowerment of the oppressed.","Additionally, this paper delves into the AR workflow illustrating its relevance and contribution to the broader theme of site-specific AR for social justice."],"url":"http://arxiv.org/abs/2404.15239v1","category":"cs.HC"}
{"created":"2024-04-23 17:06:26","title":"Towards field theory of multiple D0-branes. Hamiltonian mechanics and quantization of simplest 3D prototype of multiple D0-brane system","abstract":"Recently we have constructed a completely supersymmetric nonlinear action possessing the properties expected from multiple D0-brane system. Its quantization should result in an interesting supersymmetric field theory in the (super)space with additional matrix coordinates which can provide an important insights in the study of String Theory. As a first stage toward this aim, in this paper we construct the Hamiltonian mechanics and perform covariant quantization of the simplest three dimensional counterpart of the ten dimensional multiple D0-brane model. We obtain a supersymmetric system of equations in super-spacetime enlarged by bosonic and fermionic matrix coordinates which appears as a result of such quantization and discuss some of its properties.","sentences":["Recently we have constructed a completely supersymmetric nonlinear action possessing the properties expected from multiple D0-brane system.","Its quantization should result in an interesting supersymmetric field theory in the (super)space with additional matrix coordinates which can provide an important insights in the study of String Theory.","As a first stage toward this aim, in this paper we construct the Hamiltonian mechanics and perform covariant quantization of the simplest three dimensional counterpart of the ten dimensional multiple D0-brane model.","We obtain a supersymmetric system of equations in super-spacetime enlarged by bosonic and fermionic matrix coordinates which appears as a result of such quantization and discuss some of its properties."],"url":"http://arxiv.org/abs/2404.15233v1","category":"hep-th"}
{"created":"2024-04-23 17:00:23","title":"Multi-Tier Non-Terrestrial Networking for Disaster Communications: A Layered Clustering Approach","abstract":"It is crucial to deploy temporary non-terrestrial networks (NTN) in disaster situations where terrestrial networks are no longer operable. Deploying uncrewed aerial vehicle base stations (UAV-BSs) can provide a radio access network (RAN); however, the backhaul link may also be damaged and unserviceable in such disaster conditions. In this regard, high-altitude platform stations (HAPS) spark attention as they can be deployed as super macro base stations (SMBS) and data centers. Therefore, in this study, we investigate a three-layer heterogeneous network with different topologies to prolong the lifespan of the temporary network by using UAV-BSs for RAN services and HAPS-SMBS as a backhaul. Furthermore, a two-layer clustering algorithm is proposed to handle the UAV-BS ad-hoc networking effectively.","sentences":["It is crucial to deploy temporary non-terrestrial networks (NTN) in disaster situations where terrestrial networks are no longer operable.","Deploying uncrewed aerial vehicle base stations (UAV-BSs) can provide a radio access network (RAN); however, the backhaul link may also be damaged and unserviceable in such disaster conditions.","In this regard, high-altitude platform stations (HAPS) spark attention as they can be deployed as super macro base stations (SMBS) and data centers.","Therefore, in this study, we investigate a three-layer heterogeneous network with different topologies to prolong the lifespan of the temporary network by using UAV-BSs for RAN services and HAPS-SMBS as a backhaul.","Furthermore, a two-layer clustering algorithm is proposed to handle the UAV-BS ad-hoc networking effectively."],"url":"http://arxiv.org/abs/2404.15229v1","category":"cs.NI"}
{"created":"2024-04-23 16:54:56","title":"PHLP: Sole Persistent Homology for Link Prediction -- Interpretable Feature Extraction","abstract":"Link prediction (LP), inferring the connectivity between nodes, is a significant research area in graph data, where a link represents essential information on relationships between nodes. Although graph neural network (GNN)-based models have achieved high performance in LP, understanding why they perform well is challenging because most comprise complex neural networks. We employ persistent homology (PH), a topological data analysis method that helps analyze the topological information of graphs, to explain the reasons for the high performance. We propose a novel method that employs PH for LP (PHLP) focusing on how the presence or absence of target links influences the overall topology. The PHLP utilizes the angle hop subgraph and new node labeling called degree double radius node labeling (Degree DRNL), distinguishing the information of graphs better than DRNL. Using only a classifier, PHLP performs similarly to state-of-the-art (SOTA) models on most benchmark datasets. Incorporating the outputs calculated using PHLP into the existing GNN-based SOTA models improves performance across all benchmark datasets. To the best of our knowledge, PHLP is the first method of applying PH to LP without GNNs. The proposed approach, employing PH while not relying on neural networks, enables the identification of crucial factors for improving performance.","sentences":["Link prediction (LP), inferring the connectivity between nodes, is a significant research area in graph data, where a link represents essential information on relationships between nodes.","Although graph neural network (GNN)-based models have achieved high performance in LP, understanding why they perform well is challenging because most comprise complex neural networks.","We employ persistent homology (PH), a topological data analysis method that helps analyze the topological information of graphs, to explain the reasons for the high performance.","We propose a novel method that employs PH for LP (PHLP) focusing on how the presence or absence of target links influences the overall topology.","The PHLP utilizes the angle hop subgraph and new node labeling called degree double radius node labeling (Degree DRNL), distinguishing the information of graphs better than DRNL.","Using only a classifier, PHLP performs similarly to state-of-the-art (SOTA) models on most benchmark datasets.","Incorporating the outputs calculated using PHLP into the existing GNN-based SOTA models improves performance across all benchmark datasets.","To the best of our knowledge, PHLP is the first method of applying PH to LP without GNNs.","The proposed approach, employing PH while not relying on neural networks, enables the identification of crucial factors for improving performance."],"url":"http://arxiv.org/abs/2404.15225v1","category":"cs.LG"}
{"created":"2024-04-23 16:54:18","title":"Local well-posedness for a novel nonlocal model for cell-cell adhesion via receptor binding","abstract":"Local well-posedness is established for a highly nonlocal nonlinear diffusion-adhesion system for bounded initial values with small support. Macroscopic systems of this kind were previously obtained by the authors through upscaling in [32] and can account for the effect of microscopic receptor binding dynamics in cell-cell adhesion. The system analysed here couples an integro-PDE featuring degenerate diffusion of the porous media type and nonlocal adhesion with a novel nonlinear integral equation. The approach is based on decoupling the system and using Banach's fixed point theorem to solve each of the two equations individually and subsequently the entire system. The main challenge of the implementation lies in selecting a suitable framework. One of the key results is the local well-posedness for the integral equation with a Radon measure as a parameter. The analysis of this equation utilizes the Kantorovich-Rubinstein norm, marking the first application of this norm in handling a nonlinear integral equation.","sentences":["Local well-posedness is established for a highly nonlocal nonlinear diffusion-adhesion system for bounded initial values with small support.","Macroscopic systems of this kind were previously obtained by the authors through upscaling in [32] and can account for the effect of microscopic receptor binding dynamics in cell-cell adhesion.","The system analysed here couples an integro-PDE featuring degenerate diffusion of the porous media type and nonlocal adhesion with a novel nonlinear integral equation.","The approach is based on decoupling the system and using Banach's fixed point theorem to solve each of the two equations individually and subsequently the entire system.","The main challenge of the implementation lies in selecting a suitable framework.","One of the key results is the local well-posedness for the integral equation with a Radon measure as a parameter.","The analysis of this equation utilizes the Kantorovich-Rubinstein norm, marking the first application of this norm in handling a nonlinear integral equation."],"url":"http://arxiv.org/abs/2404.15222v1","category":"math.AP"}
{"created":"2024-04-23 16:51:26","title":"The Power of the Noisy Channel: Unsupervised End-to-End Task-Oriented Dialogue with LLMs","abstract":"Training task-oriented dialogue systems typically requires turn-level annotations for interacting with their APIs: e.g. a dialogue state and the system actions taken at each step. These annotations can be costly to produce, error-prone, and require both domain and annotation expertise. With advances in LLMs, we hypothesize unlabelled data and a schema definition are sufficient for building a working task-oriented dialogue system, completely unsupervised. Using only (1) a well-defined API schema (2) a set of unlabelled dialogues between a user and agent, we develop a novel approach for inferring turn-level annotations as latent variables using a noisy channel model. We iteratively improve these pseudo-labels with expectation-maximization (EM), and use the inferred labels to train an end-to-end dialogue agent. Evaluating our approach on the MultiWOZ benchmark, our method more than doubles the dialogue success rate of a strong GPT-3.5 baseline.","sentences":["Training task-oriented dialogue systems typically requires turn-level annotations for interacting with their APIs: e.g. a dialogue state and the system actions taken at each step.","These annotations can be costly to produce, error-prone, and require both domain and annotation expertise.","With advances in LLMs, we hypothesize unlabelled data and a schema definition are sufficient for building a working task-oriented dialogue system, completely unsupervised.","Using only (1) a well-defined API schema (2) a set of unlabelled dialogues between a user and agent, we develop a novel approach for inferring turn-level annotations as latent variables using a noisy channel model.","We iteratively improve these pseudo-labels with expectation-maximization (EM), and use the inferred labels to train an end-to-end dialogue agent.","Evaluating our approach on the MultiWOZ benchmark, our method more than doubles the dialogue success rate of a strong GPT-3.5 baseline."],"url":"http://arxiv.org/abs/2404.15219v1","category":"cs.CL"}
{"created":"2024-04-23 16:48:10","title":"Geometric phase of a two-level atom near a dielectric nanosphere out of thermal equilibrium","abstract":"We study the geometric phase (GP) of a two-level atom coupled to an environment composed of free space and a dielectric nanosphere in thermal and out of thermal equilibrium. We analytically and numerically analyze the optical properties and loss of the dielectric medium, along with the non-equilibrium effects of the environment on the GP. In the weak coupling limit, we find that the correction to the GP depends on the partial local density of photonic states at the atom position, and an effective parameter that emerges out of the non-equilibrium configuration of the system. The GP exhibits a significant enhancement due to the excitation of evanescent surface waves at its resonance frequency. It is shown that the GP acquired by the atomic system out of thermal equilibrium is always bounded between the thermal-equilibrium counterparts. Furthermore, the temperature difference between the nanosphere and free space can play an important role in the GP only at moderate atomic distances from the nanosphere. Our results elegantly demonstrate properties of the GP near material media that can support phononic modes and pave the way for further research of GP as a resource for quantum computation.","sentences":["We study the geometric phase (GP) of a two-level atom coupled to an environment composed of free space and a dielectric nanosphere in thermal and out of thermal equilibrium.","We analytically and numerically analyze the optical properties and loss of the dielectric medium, along with the non-equilibrium effects of the environment on the GP.","In the weak coupling limit, we find that the correction to the GP depends on the partial local density of photonic states at the atom position, and an effective parameter that emerges out of the non-equilibrium configuration of the system.","The GP exhibits a significant enhancement due to the excitation of evanescent surface waves at its resonance frequency.","It is shown that the GP acquired by the atomic system out of thermal equilibrium is always bounded between the thermal-equilibrium counterparts.","Furthermore, the temperature difference between the nanosphere and free space can play an important role in the GP only at moderate atomic distances from the nanosphere.","Our results elegantly demonstrate properties of the GP near material media that can support phononic modes and pave the way for further research of GP as a resource for quantum computation."],"url":"http://arxiv.org/abs/2404.15216v1","category":"quant-ph"}
{"created":"2024-04-23 16:46:27","title":"Bottoms Up for CHCs: Novel Transformation of Linear Constrained Horn Clauses to Software Verification","abstract":"Constrained Horn Clauses (CHCs) have conventionally been used as a low-level representation in formal verification. Most existing solvers use a diverse set of specialized techniques, including direct state space traversal or under-approximating abstraction, necessitating purpose-built complex algorithms. Other solvers successfully simplified the verification workflow by translating the problem to inputs for other verification tasks, leveraging the strengths of existing algorithms. One such approach transforms the CHC problem into a recursive program roughly emulating a top-down solver for the deduction task; and verifying the reachability of a safety violation specified as a control location. We propose an alternative bottom-up approach for linear CHCs, and evaluate the two options in the open-source model checking framework THETA on both synthetic and industrial examples. We find that there is a more than twofold increase in the number of solved tasks when the novel bottom-up approach is used in the verification workflow, in contrast with the top-down technique.","sentences":["Constrained Horn Clauses (CHCs) have conventionally been used as a low-level representation in formal verification.","Most existing solvers use a diverse set of specialized techniques, including direct state space traversal or under-approximating abstraction, necessitating purpose-built complex algorithms.","Other solvers successfully simplified the verification workflow by translating the problem to inputs for other verification tasks, leveraging the strengths of existing algorithms.","One such approach transforms the CHC problem into a recursive program roughly emulating a top-down solver for the deduction task; and verifying the reachability of a safety violation specified as a control location.","We propose an alternative bottom-up approach for linear CHCs, and evaluate the two options in the open-source model checking framework THETA on both synthetic and industrial examples.","We find that there is a more than twofold increase in the number of solved tasks when the novel bottom-up approach is used in the verification workflow, in contrast with the top-down technique."],"url":"http://arxiv.org/abs/2404.15215v1","category":"cs.LO"}
{"created":"2024-04-23 16:46:10","title":"Embedding Differential Dynamic Logic in PVS","abstract":"Differential dynamic logic (dL) is a formal framework for specifying and reasoning about hybrid systems, i.e., dynamical systems that exhibit both continuous and discrete behaviors. These kinds of systems arise in many safety- and mission-critical applications. This paper presents a formalization of dL in the Prototype Verification System (PVS) that includes the semantics of hybrid programs and dL's proof calculus. The formalization embeds dL into the PVS logic, resulting in a version of dL whose proof calculus is not only formally verified, but is also available for the verification of hybrid programs within PVS itself. This embedding, called Plaidypvs (Properly Assured Implementation of dL for Hybrid Program Verification and Specification), supports standard dL style proofs, but further leverages the capabilities of PVS to allow reasoning about entire classes of hybrid programs. The embedding also allows the user to import the well-established definitions and mathematical theories available in PVS.","sentences":["Differential dynamic logic (dL) is a formal framework for specifying and reasoning about hybrid systems, i.e., dynamical systems that exhibit both continuous and discrete behaviors.","These kinds of systems arise in many safety- and mission-critical applications.","This paper presents a formalization of dL in the Prototype Verification System (PVS) that includes the semantics of hybrid programs and dL's proof calculus.","The formalization embeds dL into the PVS logic, resulting in a version of dL whose proof calculus is not only formally verified, but is also available for the verification of hybrid programs within PVS itself.","This embedding, called Plaidypvs (Properly Assured Implementation of dL for Hybrid Program Verification and Specification), supports standard dL style proofs, but further leverages the capabilities of PVS to allow reasoning about entire classes of hybrid programs.","The embedding also allows the user to import the well-established definitions and mathematical theories available in PVS."],"url":"http://arxiv.org/abs/2404.15214v1","category":"cs.LO"}
{"created":"2024-04-23 16:35:14","title":"Reinforcement Learning with Adaptive Control Regularization for Safe Control of Critical Systems","abstract":"Reinforcement Learning (RL) is a powerful method for controlling dynamic systems, but its learning mechanism can lead to unpredictable actions that undermine the safety of critical systems. Here, we propose RL with Adaptive Control Regularization (RL-ACR) that ensures RL safety by combining the RL policy with a control regularizer that hard-codes safety constraints over forecasted system behaviors. The adaptability is achieved by using a learnable \"focus\" weight trained to maximize the cumulative reward of the policy combination. As the RL policy improves through off-policy learning, the focus weight improves the initial sub-optimum strategy by gradually relying more on the RL policy. We demonstrate the effectiveness of RL-ACR in a critical medical control application and further investigate its performance in four classic control environments.","sentences":["Reinforcement Learning (RL) is a powerful method for controlling dynamic systems, but its learning mechanism can lead to unpredictable actions that undermine the safety of critical systems.","Here, we propose RL with Adaptive Control Regularization (RL-ACR) that ensures RL safety by combining the RL policy with a control regularizer that hard-codes safety constraints over forecasted system behaviors.","The adaptability is achieved by using a learnable \"focus\" weight trained to maximize the cumulative reward of the policy combination.","As the RL policy improves through off-policy learning, the focus weight improves the initial sub-optimum strategy by gradually relying more on the RL policy.","We demonstrate the effectiveness of RL-ACR in a critical medical control application and further investigate its performance in four classic control environments."],"url":"http://arxiv.org/abs/2404.15199v1","category":"cs.LG"}
{"created":"2024-04-23 16:34:34","title":"Setting up the Data Printer with Improved English to Ukrainian Machine Translation","abstract":"To build large language models for Ukrainian we need to expand our corpora with large amounts of new algorithmic tasks expressed in natural language. Examples of task performance expressed in English are abundant, so with a high-quality translation system our community will be enabled to curate datasets faster. To aid this goal, we introduce a recipe to build a translation system using supervised finetuning of a large pretrained language model with a noisy parallel dataset of 3M pairs of Ukrainian and English sentences followed by a second phase of training using 17K examples selected by k-fold perplexity filtering on another dataset of higher quality. Our decoder-only model named Dragoman beats performance of previous state of the art encoder-decoder models on the FLORES devtest set.","sentences":["To build large language models for Ukrainian we need to expand our corpora with large amounts of new algorithmic tasks expressed in natural language.","Examples of task performance expressed in English are abundant, so with a high-quality translation system our community will be enabled to curate datasets faster.","To aid this goal, we introduce a recipe to build a translation system using supervised finetuning of a large pretrained language model with a noisy parallel dataset of 3M pairs of Ukrainian and English sentences followed by a second phase of training using 17K examples selected by k-fold perplexity filtering on another dataset of higher quality.","Our decoder-only model named Dragoman beats performance of previous state of the art encoder-decoder models on the FLORES devtest set."],"url":"http://arxiv.org/abs/2404.15196v1","category":"cs.CL"}
{"created":"2024-04-23 16:33:28","title":"Closed Loop Interactive Embodied Reasoning for Robot Manipulation","abstract":"Embodied reasoning systems integrate robotic hardware and cognitive processes to perform complex tasks typically in response to a natural language query about a specific physical environment. This usually involves changing the belief about the scene or physically interacting and changing the scene (e.g. 'Sort the objects from lightest to heaviest'). In order to facilitate the development of such systems we introduce a new simulating environment that makes use of MuJoCo physics engine and high-quality renderer Blender to provide realistic visual observations that are also accurate to the physical state of the scene. Together with the simulator we propose a new benchmark composed of 10 classes of multi-step reasoning scenarios that require simultaneous visual and physical measurements. Finally, we develop a new modular Closed Loop Interactive Reasoning (CLIER) approach that takes into account the measurements of non-visual object properties, changes in the scene caused by external disturbances as well as uncertain outcomes of robotic actions. We extensively evaluate our reasoning approach in simulation and in the real world manipulation tasks with a success rate above 76% and 64%, respectively.","sentences":["Embodied reasoning systems integrate robotic hardware and cognitive processes to perform complex tasks typically in response to a natural language query about a specific physical environment.","This usually involves changing the belief about the scene or physically interacting and changing the scene (e.g. 'Sort the objects from lightest to heaviest').","In order to facilitate the development of such systems we introduce a new simulating environment that makes use of MuJoCo physics engine and high-quality renderer Blender to provide realistic visual observations that are also accurate to the physical state of the scene.","Together with the simulator we propose a new benchmark composed of 10 classes of multi-step reasoning scenarios that require simultaneous visual and physical measurements.","Finally, we develop a new modular Closed Loop Interactive Reasoning (CLIER) approach that takes into account the measurements of non-visual object properties, changes in the scene caused by external disturbances as well as uncertain outcomes of robotic actions.","We extensively evaluate our reasoning approach in simulation and in the real world manipulation tasks with a success rate above 76% and 64%, respectively."],"url":"http://arxiv.org/abs/2404.15194v1","category":"cs.RO"}
{"created":"2024-04-23 16:28:03","title":"Evaluating Physician-AI Interaction for Cancer Management: Paving the Path towards Precision Oncology","abstract":"We evaluated how clinicians approach clinical decision-making when given findings from both randomized controlled trials (RCTs) and machine learning (ML) models. To do so, we designed a clinical decision support system (CDSS) that displays survival curves and adverse event information from a synthetic RCT and ML model for 12 patients with multiple myeloma. We conducted an interventional study in a simulated setting to evaluate how clinicians synthesized the available data to make treatment decisions. Participants were invited to participate in a follow-up interview to discuss their choices in an open-ended format. When ML model results were concordant with RCT results, physicians had increased confidence in treatment choice compared to when they were given RCT results alone. When ML model results were discordant with RCT results, the majority of physicians followed the ML model recommendation in their treatment selection. Perceived reliability of the ML model was consistently higher after physicians were provided with data on how it was trained and validated. Follow-up interviews revealed four major themes: (1) variability in what variables participants used for decision-making, (2) perceived advantages to an ML model over RCT data, (3) uncertainty around decision-making when the ML model quality was poor, and (4) perception that this type of study is an important thought exercise for clinicians. Overall, ML-based CDSSs have the potential to change treatment decisions in cancer management. However, meticulous development and validation of these systems as well as clinician training are required before deployment.","sentences":["We evaluated how clinicians approach clinical decision-making when given findings from both randomized controlled trials (RCTs) and machine learning (ML) models.","To do so, we designed a clinical decision support system (CDSS) that displays survival curves and adverse event information from a synthetic RCT and ML model for 12 patients with multiple myeloma.","We conducted an interventional study in a simulated setting to evaluate how clinicians synthesized the available data to make treatment decisions.","Participants were invited to participate in a follow-up interview to discuss their choices in an open-ended format.","When ML model results were concordant with RCT results, physicians had increased confidence in treatment choice compared to when they were given RCT results alone.","When ML model results were discordant with RCT results, the majority of physicians followed the ML model recommendation in their treatment selection.","Perceived reliability of the ML model was consistently higher after physicians were provided with data on how it was trained and validated.","Follow-up interviews revealed four major themes: (1) variability in what variables participants used for decision-making, (2) perceived advantages to an ML model over RCT data, (3) uncertainty around decision-making when the ML model quality was poor, and (4) perception that this type of study is an important thought exercise for clinicians.","Overall, ML-based CDSSs have the potential to change treatment decisions in cancer management.","However, meticulous development and validation of these systems as well as clinician training are required before deployment."],"url":"http://arxiv.org/abs/2404.15187v1","category":"cs.HC"}
{"created":"2024-04-23 16:19:38","title":"Exploring Imaginary Coordinates: Disparity in the Shape of Quantum State Space in Even and Odd Dimensions","abstract":"The state of a finite-dimensional quantum system is described by a density matrix that can be decomposed into a real diagonal, a real off-diagonal and and an imaginary off-diagonal part. The latter plays a peculiar role. While it is intuitively clear that some of the imaginary coordinates cannot have the same extension as their real counterparts the precise relation is not obvious. We give a complete characterization of the constraints in terms of tight inequalities for real and imaginary Bloch-type coordinates. Our description entails a three-dimensional Bloch ball-type model for the state space. We uncover a surprising qualitative difference for the state-space boundaries in even and odd dimensions.","sentences":["The state of a finite-dimensional quantum system is described by a density matrix that can be decomposed into a real diagonal, a real off-diagonal and and an imaginary off-diagonal part.","The latter plays a peculiar role.","While it is intuitively clear that some of the imaginary coordinates cannot have the same extension as their real counterparts the precise relation is not obvious.","We give a complete characterization of the constraints in terms of tight inequalities for real and imaginary Bloch-type coordinates.","Our description entails a three-dimensional Bloch ball-type model for the state space.","We uncover a surprising qualitative difference for the state-space boundaries in even and odd dimensions."],"url":"http://arxiv.org/abs/2404.15179v1","category":"quant-ph"}
{"created":"2024-04-23 16:15:39","title":"Voice Passing : a Non-Binary Voice Gender Prediction System for evaluating Transgender voice transition","abstract":"This paper presents a software allowing to describe voices using a continuous Voice Femininity Percentage (VFP). This system is intended for transgender speakers during their voice transition and for voice therapists supporting them in this process. A corpus of 41 French cis- and transgender speakers was recorded. A perceptual evaluation allowed 57 participants to estimate the VFP for each voice. Binary gender classification models were trained on external gender-balanced data and used on overlapping windows to obtain average gender prediction estimates, which were calibrated to predict VFP and obtained higher accuracy than $F_0$ or vocal track length-based models. Training data speaking style and DNN architecture were shown to impact VFP estimation. Accuracy of the models was affected by speakers' age. This highlights the importance of style, age, and the conception of gender as binary or not, to build adequate statistical representations of cultural concepts.","sentences":["This paper presents a software allowing to describe voices using a continuous Voice Femininity Percentage (VFP).","This system is intended for transgender speakers during their voice transition and for voice therapists supporting them in this process.","A corpus of 41 French cis- and transgender speakers was recorded.","A perceptual evaluation allowed 57 participants to estimate the VFP for each voice.","Binary gender classification models were trained on external gender-balanced data and used on overlapping windows to obtain average gender prediction estimates, which were calibrated to predict VFP and obtained higher accuracy than $F_0$ or vocal track length-based models.","Training data speaking style and DNN architecture were shown to impact VFP estimation.","Accuracy of the models was affected by speakers' age.","This highlights the importance of style, age, and the conception of gender as binary or not, to build adequate statistical representations of cultural concepts."],"url":"http://arxiv.org/abs/2404.15176v1","category":"eess.AS"}
{"created":"2024-04-23 16:14:20","title":"Fourier-enhanced Implicit Neural Fusion Network for Multispectral and Hyperspectral Image Fusion","abstract":"Recently, implicit neural representations (INR) have made significant strides in various vision-related domains, providing a novel solution for Multispectral and Hyperspectral Image Fusion (MHIF) tasks. However, INR is prone to losing high-frequency information and is confined to the lack of global perceptual capabilities. To address these issues, this paper introduces a Fourier-enhanced Implicit Neural Fusion Network (FeINFN) specifically designed for MHIF task, targeting the following phenomena: The Fourier amplitudes of the HR-HSI latent code and LR-HSI are remarkably similar; however, their phases exhibit different patterns. In FeINFN, we innovatively propose a spatial and frequency implicit fusion function (Spa-Fre IFF), helping INR capture high-frequency information and expanding the receptive field. Besides, a new decoder employing a complex Gabor wavelet activation function, called Spatial-Frequency Interactive Decoder (SFID), is invented to enhance the interaction of INR features. Especially, we further theoretically prove that the Gabor wavelet activation possesses a time-frequency tightness property that favors learning the optimal bandwidths in the decoder. Experiments on two benchmark MHIF datasets verify the state-of-the-art (SOTA) performance of the proposed method, both visually and quantitatively. Also, ablation studies demonstrate the mentioned contributions. The code will be available on Anonymous GitHub (https://anonymous.4open.science/r/FeINFN-15C9/) after possible acceptance.","sentences":["Recently, implicit neural representations (INR) have made significant strides in various vision-related domains, providing a novel solution for Multispectral and Hyperspectral Image Fusion (MHIF) tasks.","However, INR is prone to losing high-frequency information and is confined to the lack of global perceptual capabilities.","To address these issues, this paper introduces a Fourier-enhanced Implicit Neural Fusion Network (FeINFN) specifically designed for MHIF task, targeting the following phenomena: The Fourier amplitudes of the HR-HSI latent code and LR-HSI are remarkably similar; however, their phases exhibit different patterns.","In FeINFN, we innovatively propose a spatial and frequency implicit fusion function (Spa-Fre IFF), helping INR capture high-frequency information and expanding the receptive field.","Besides, a new decoder employing a complex Gabor wavelet activation function, called Spatial-Frequency Interactive Decoder (SFID), is invented to enhance the interaction of INR features.","Especially, we further theoretically prove that the Gabor wavelet activation possesses a time-frequency tightness property that favors learning the optimal bandwidths in the decoder.","Experiments on two benchmark MHIF datasets verify the state-of-the-art (SOTA) performance of the proposed method, both visually and quantitatively.","Also, ablation studies demonstrate the mentioned contributions.","The code will be available on Anonymous GitHub (https://anonymous.4open.science/r/FeINFN-15C9/) after possible acceptance."],"url":"http://arxiv.org/abs/2404.15174v1","category":"cs.CV"}
{"created":"2024-04-23 16:07:51","title":"An Introduction to Complex Random Tensors","abstract":"This work considers the notion of random tensors and reviews some fundamental concepts in statistics when applied to a tensor based data or signal. In several engineering fields such as Communications, Signal Processing, Machine learning, and Control systems, the concepts of linear algebra combined with random variables have been indispensable tools. With the evolution of these subjects to multi-domain communication systems, multi-way signal processing, high dimensional data analysis, and multi-linear systems theory, there is a need to bring in multi-linear algebra equipped with the notion of random tensors. Also, since several such application areas deal with complex-valued entities, it is imperative to study this subject from a complex random tensor perspective, which is the focus of this paper. Using tools from multi-linear algebra, we characterize statistical properties of complex random tensors, both proper and improper, study various correlation structures, and fundamentals of tensor valued random processes. Furthermore, the asymptotic distribution of various tensor eigenvalue and singular value definitions is also considered, which is used for the study of spiked real tensor models that deals with recovery of low rank tensor signals perturbed by noise. This paper aims to provide an overview of the state of the art in random tensor theory of both complex and real valued tensors, for the purpose of enabling its application in engineering and applied science.","sentences":["This work considers the notion of random tensors and reviews some fundamental concepts in statistics when applied to a tensor based data or signal.","In several engineering fields such as Communications, Signal Processing, Machine learning, and Control systems, the concepts of linear algebra combined with random variables have been indispensable tools.","With the evolution of these subjects to multi-domain communication systems, multi-way signal processing, high dimensional data analysis, and multi-linear systems theory, there is a need to bring in multi-linear algebra equipped with the notion of random tensors.","Also, since several such application areas deal with complex-valued entities, it is imperative to study this subject from a complex random tensor perspective, which is the focus of this paper.","Using tools from multi-linear algebra, we characterize statistical properties of complex random tensors, both proper and improper, study various correlation structures, and fundamentals of tensor valued random processes.","Furthermore, the asymptotic distribution of various tensor eigenvalue and singular value definitions is also considered, which is used for the study of spiked real tensor models that deals with recovery of low rank tensor signals perturbed by noise.","This paper aims to provide an overview of the state of the art in random tensor theory of both complex and real valued tensors, for the purpose of enabling its application in engineering and applied science."],"url":"http://arxiv.org/abs/2404.15170v1","category":"math.ST"}
{"created":"2024-04-23 16:04:37","title":"Optimization of Quantum Systems Emulation via a Variant of the Bandwidth Minimization Problem","abstract":"This paper introduces weighted-BMP, a variant of the Bandwidth Minimization Problem (BMP), with a significant application in optimizing quantum emulation. Weighted-BMP optimizes particles ordering to reduce the emulation costs, by designing a particle interaction matrix where strong interactions are placed as close as possible to the diagonal. We formulate the problem using a Mixed Integer Linear Program (MILP) and solve it to optimality with a state of the art solver. To strengthen our MILP model, we introduce symmetry-breaking inequalities and establish a lower bound. Through extensive numerical analysis, we examine the impacts of these enhancements on the solver's performance. The introduced reinforcements result in an average CPU time reduction of 25.61 percent. Additionally, we conduct quantum emulations of realistic instances. Our numerical tests show that the weighted-BMP approach outperforms the Reverse Cuthill-McKee (RCM) algorithm, an efficient heuristic used for site ordering tasks in quantum emulation, achieving an average memory storage reduction of 24.48 percent. From an application standpoint, this study is the first to apply an exact optimization method, weighted-BMP, that considers interactions for site ordering in quantum emulation pre-processing, and shows its crucial role in cost reduction. From an algorithmic perspective, it contributes by introducing important reinforcements and lays the groundwork for future research on further enhancements, particularly on strengthening the weak linear relaxation of the MILP.","sentences":["This paper introduces weighted-BMP, a variant of the Bandwidth Minimization Problem (BMP), with a significant application in optimizing quantum emulation.","Weighted-BMP optimizes particles ordering to reduce the emulation costs, by designing a particle interaction matrix where strong interactions are placed as close as possible to the diagonal.","We formulate the problem using a Mixed Integer Linear Program (MILP) and solve it to optimality with a state of the art solver.","To strengthen our MILP model, we introduce symmetry-breaking inequalities and establish a lower bound.","Through extensive numerical analysis, we examine the impacts of these enhancements on the solver's performance.","The introduced reinforcements result in an average CPU time reduction of 25.61 percent.","Additionally, we conduct quantum emulations of realistic instances.","Our numerical tests show that the weighted-BMP approach outperforms the Reverse Cuthill-McKee (RCM) algorithm, an efficient heuristic used for site ordering tasks in quantum emulation, achieving an average memory storage reduction of 24.48 percent.","From an application standpoint, this study is the first to apply an exact optimization method, weighted-BMP, that considers interactions for site ordering in quantum emulation pre-processing, and shows its crucial role in cost reduction.","From an algorithmic perspective, it contributes by introducing important reinforcements and lays the groundwork for future research on further enhancements, particularly on strengthening the weak linear relaxation of the MILP."],"url":"http://arxiv.org/abs/2404.15165v1","category":"quant-ph"}
{"created":"2024-04-23 15:59:51","title":"Blackwell-Monotone Information Costs","abstract":"A Blackwell-monotone information cost function assigns higher costs to Blackwell more informative experiments. This paper provides simple necessary and sufficient conditions for Blackwell monotonicity over finite experiments. The key condition is a system of linear differential inequalities that are convenient to check given an arbitrary cost function. When the cost function is additively separable across signals, our characterization implies that Blackwell monotonicity is equivalent to sublinearity. This identifies a wide range of practical information cost functions. Finally, we apply our results to bargaining and persuasion problems with costly information.","sentences":["A Blackwell-monotone information cost function assigns higher costs to Blackwell more informative experiments.","This paper provides simple necessary and sufficient conditions for Blackwell monotonicity over finite experiments.","The key condition is a system of linear differential inequalities that are convenient to check given an arbitrary cost function.","When the cost function is additively separable across signals, our characterization implies that Blackwell monotonicity is equivalent to sublinearity.","This identifies a wide range of practical information cost functions.","Finally, we apply our results to bargaining and persuasion problems with costly information."],"url":"http://arxiv.org/abs/2404.15158v1","category":"econ.TH"}
{"created":"2024-04-23 15:54:29","title":"Univalent approximation by Fourier series of step functions","abstract":"We prove that univalent harmonic mappings can be approximated by univalent Fourier series of step functions.","sentences":["We prove that univalent harmonic mappings can be approximated by univalent Fourier series of step functions."],"url":"http://arxiv.org/abs/2404.15152v1","category":"math.CV"}
{"created":"2024-04-23 15:44:17","title":"Ideals of \u00e9tale groupoid algebras with coefficients in a sheaf with applications to topological dynamics","abstract":"We prove the Effros-Hahn conjecture for groupoid algebras with coefficients in a sheaf, obtaining as a consequence a description of the ideals in skew inverse semigroup rings. We also use the description of the ideals to characterize when the groupoid algebras with coefficients in a sheaf are von Neumann regular, primitive, semiprimitive, or simple. We apply our results to the topological dynamics of actions of inverse semigroups, describing the existence of dense orbits and minimality in terms of primitivity and simplicity, respectively, of the associated algebra. Moreover, we apply our results to the usual complex groupoid algebra of continuous functions with compact support, used to build the C*-algebra associated with a groupoid, and describe criteria for its simplicity.","sentences":["We prove the Effros-Hahn conjecture for groupoid algebras with coefficients in a sheaf, obtaining as a consequence a description of the ideals in skew inverse semigroup rings.","We also use the description of the ideals to characterize when the groupoid algebras with coefficients in a sheaf are von Neumann regular, primitive, semiprimitive, or simple.","We apply our results to the topological dynamics of actions of inverse semigroups, describing the existence of dense orbits and minimality in terms of primitivity and simplicity, respectively, of the associated algebra.","Moreover, we apply our results to the usual complex groupoid algebra of continuous functions with compact support, used to build the C*-algebra associated with a groupoid, and describe criteria for its simplicity."],"url":"http://arxiv.org/abs/2404.15139v1","category":"math.RA"}
{"created":"2024-04-23 15:42:31","title":"From Space-Time to Space-Order: Directly Planning a Temporal Planning Graph by Redefining CBS","abstract":"The majority of multi-agent path finding (MAPF) methods compute collision-free space-time paths which require agents to be at a specific location at a specific discretized timestep. However, executing these space-time paths directly on robotic systems is infeasible due to real-time execution differences (e.g. delays) which can lead to collisions. To combat this, current methods translate the space-time paths into a temporal plan graph (TPG) that only requires that agents observe the order in which they navigate through locations where their paths cross. However, planning space-time paths and then post-processing them into a TPG does not reduce the required agent-to-agent coordination, which is fixed once the space-time paths are computed. To that end, we propose a novel algorithm Space-Order CBS that can directly plan a TPG and explicitly minimize coordination. Our main theoretical insight is our novel perspective on viewing a TPG as a set of space-visitation order paths where agents visit locations in relative orders (e.g. 1st vs 2nd) as opposed to specific timesteps. We redefine unique conflicts and constraints for adapting CBS for space-order planning. We experimentally validate how Space-Order CBS can return TPGs which significantly reduce coordination, thus subsequently reducing the amount of agent-agent communication and leading to more robustness to delays during execution.","sentences":["The majority of multi-agent path finding (MAPF) methods compute collision-free space-time paths which require agents to be at a specific location at a specific discretized timestep.","However, executing these space-time paths directly on robotic systems is infeasible due to real-time execution differences (e.g. delays) which can lead to collisions.","To combat this, current methods translate the space-time paths into a temporal plan graph (TPG) that only requires that agents observe the order in which they navigate through locations where their paths cross.","However, planning space-time paths and then post-processing them into a TPG does not reduce the required agent-to-agent coordination, which is fixed once the space-time paths are computed.","To that end, we propose a novel algorithm Space-Order CBS that can directly plan a TPG and explicitly minimize coordination.","Our main theoretical insight is our novel perspective on viewing a TPG as a set of space-visitation order paths where agents visit locations in relative orders (e.g. 1st vs 2nd) as opposed to specific timesteps.","We redefine unique conflicts and constraints for adapting CBS for space-order planning.","We experimentally validate how Space-Order CBS can return TPGs which significantly reduce coordination, thus subsequently reducing the amount of agent-agent communication and leading to more robustness to delays during execution."],"url":"http://arxiv.org/abs/2404.15137v1","category":"cs.MA"}
{"created":"2024-04-23 15:32:03","title":"Black Hole Search by a Set of Scattered Agents in Dynamic Rings","abstract":"In this paper we investigate the problem of searching for a black hole in a dynamic graph by a set of scattered agents (i.e., the agents start from arbitrary locations of the graph). The black hole is a node that silently destroys any agent visiting it. This kind of malicious node nicely models network failures such as a crashed host or a virus that erases the visiting agents. The black hole search problem is solved when at least one agent survives, and it has the entire map of the graph with the location of the black hole. We consider the case in which the underlining graph is a dynamic 1-interval connected ring: a ring graph in which at each round at most one edge can be missing. We first show that the problem cannot be solved if the agents can only communicate by using a face-to-face mechanism: this holds for any set of agents of constant size, with respect to the size $n$ of the ring.   To circumvent this impossibility we consider agents equipped with movable pebbles that can be left on nodes as a form of communication with other agents. When pebbles are available, three agents can localize the black hole in $O(n^2)$ moves. We show that such a number of agents is optimal.   We also show that the complexity is tight, that is $\\Omega(n^2)$ moves are required for any algorithm solving the problem with three agents, even with stronger communication mechanisms (e.g., a whiteboard on each node on which agents can write messages of unlimited size). To the best of our knowledge this is the first paper examining the problem of searching a black hole in a dynamic environment with scattered agents.","sentences":["In this paper we investigate the problem of searching for a black hole in a dynamic graph by a set of scattered agents (i.e., the agents start from arbitrary locations of the graph).","The black hole is a node that silently destroys any agent visiting it.","This kind of malicious node nicely models network failures such as a crashed host or a virus that erases the visiting agents.","The black hole search problem is solved when at least one agent survives, and it has the entire map of the graph with the location of the black hole.","We consider the case in which the underlining graph is a dynamic 1-interval connected ring: a ring graph in which at each round at most one edge can be missing.","We first show that the problem cannot be solved if the agents can only communicate by using a face-to-face mechanism: this holds for any set of agents of constant size, with respect to the size $n$ of the ring.   ","To circumvent this impossibility we consider agents equipped with movable pebbles that can be left on nodes as a form of communication with other agents.","When pebbles are available, three agents can localize the black hole in $O(n^2)$ moves.","We show that such a number of agents is optimal.   ","We also show that the complexity is tight, that is $\\Omega(n^2)$ moves are required for any algorithm solving the problem with three agents, even with stronger communication mechanisms (e.g., a whiteboard on each node on which agents can write messages of unlimited size).","To the best of our knowledge this is the first paper examining the problem of searching a black hole in a dynamic environment with scattered agents."],"url":"http://arxiv.org/abs/2404.15132v1","category":"cs.DC"}
{"created":"2024-04-23 15:23:34","title":"Life Cycle Assessment of the Athena X-ray Integral Field Unit","abstract":"The X-ray Integral Field Unit (X-IFU) is the high-resolution X-ray spectrometer to fly on board the Athena Space Observatory of the European Space Agency (ESA). It is being developed by an international Consortium led by France, involving twelve ESA member states, plus the United States. It is a cryogenic instrument, involving state of the art technology, such as micro-calorimeters, to be read out by low noise electronics. As the instrument was undergoing its system requirement review (in 2022), a life cycle assessment (LCA) was performed to estimate the environmental impacts associated with the development of the sub-systems that were under the responsibility of the X-IFU Consortium. The assessment included the supply, manufacturing and testing of sub systems, as well as involved logistics and manpower. We find that the most significant environmental impacts arise from testing activities, which is related to energy consumption in clean rooms, office work, which is related to energy consumption in office buildings, and instrument manufacturing, which is related to the use of mineral and metal resources. Furthermore, business travels is another area of concern, despite the policy to reduced flying adopted by the Consortium. As the instrument is now being redesigned to fit within the new boundaries set by ESA, the LCA will be updated, with a focus on the hot spots identified in the first iteration. The new configuration, consolidated in 2023, is significantly different from the previously studied version and is marked by an increase of the perimeter of responsibility for the Consortium. This will need to be folded in the updated LCA, keeping the ambition to reduce the environmental footprint of X-IFU, while complying with its stringent requirements in terms of performance and risk management.","sentences":["The X-ray Integral Field Unit (X-IFU) is the high-resolution X-ray spectrometer to fly on board the Athena Space Observatory of the European Space Agency (ESA).","It is being developed by an international Consortium led by France, involving twelve ESA member states, plus the United States.","It is a cryogenic instrument, involving state of the art technology, such as micro-calorimeters, to be read out by low noise electronics.","As the instrument was undergoing its system requirement review (in 2022), a life cycle assessment (LCA) was performed to estimate the environmental impacts associated with the development of the sub-systems that were under the responsibility of the X-IFU Consortium.","The assessment included the supply, manufacturing and testing of sub systems, as well as involved logistics and manpower.","We find that the most significant environmental impacts arise from testing activities, which is related to energy consumption in clean rooms, office work, which is related to energy consumption in office buildings, and instrument manufacturing, which is related to the use of mineral and metal resources.","Furthermore, business travels is another area of concern, despite the policy to reduced flying adopted by the Consortium.","As the instrument is now being redesigned to fit within the new boundaries set by ESA, the LCA will be updated, with a focus on the hot spots identified in the first iteration.","The new configuration, consolidated in 2023, is significantly different from the previously studied version and is marked by an increase of the perimeter of responsibility for the Consortium.","This will need to be folded in the updated LCA, keeping the ambition to reduce the environmental footprint of X-IFU, while complying with its stringent requirements in terms of performance and risk management."],"url":"http://arxiv.org/abs/2404.15122v1","category":"astro-ph.IM"}
{"created":"2024-04-23 15:00:09","title":"Overlapping plastic events as a mechanism for irreversible dynamics in amorphous solids under oscillatory shear","abstract":"The origin of the transition from asymptotically reversible to asymptotically irreversible response in amorphous solids subject to oscillatory shear is still unknown. It is known that the plastic events that result from shearing always involve localized particle rearrangements, but it is unclear why some are reversible while others are not. Here we show, using simulations and models, that overlaps between particle rearrangements caused by straining the solid in alternating directions can cause the response to become irreversible when they occur frequently. As the forcing amplitude increases, plastic events become more frequent, the number of such overlaps increases, and the probability of the system returning to previous states diminishes.","sentences":["The origin of the transition from asymptotically reversible to asymptotically irreversible response in amorphous solids subject to oscillatory shear is still unknown.","It is known that the plastic events that result from shearing always involve localized particle rearrangements, but it is unclear why some are reversible while others are not.","Here we show, using simulations and models, that overlaps between particle rearrangements caused by straining the solid in alternating directions can cause the response to become irreversible when they occur frequently.","As the forcing amplitude increases, plastic events become more frequent, the number of such overlaps increases, and the probability of the system returning to previous states diminishes."],"url":"http://arxiv.org/abs/2404.15106v1","category":"cond-mat.soft"}
{"created":"2024-04-23 14:54:56","title":"Omnidirectional gradient force optical trapping in dielectric nanocavities by inverse design","abstract":"Optical trapping enables precise control of individual particles of different sizes, such as atoms, molecules, or nanospheres. Optical tweezers provide free-space omnidirectional optical trapping of objects in laboratories around the world. As an alternative to standard macroscopic setups based on lenses, which are inherently bound by the diffraction limit, plasmonic and photonic nanostructures promise trapping by near-field optical effects on the extreme nanoscale. However, the practical design of lossless waveguide-coupled nanostructures capable of trapping deeply sub-wavelength particles in all spatial directions using the gradient force has until now proven insurmountable. In this work, we demonstrate an omnidirectional optical trap realized by inverse-designing fabrication-ready integrated dielectric nanocavities. The sub-wavelength optical trap is designed to rely solely on the gradient force and is thus particle-size agnostic. In particular, we show how a nanometer-sized trapped particle experiences a force strong enough to overcome room-temperature thermal fluctuations. Furthermore, through the robust inverse design framework, we tailor manufacturable devices operating at near-infrared and optical frequencies. Our results open a new regime of levitated optical trapping by achieving a deep trapping potential capable of trapping single sub-wavelength particles in all directions using optical gradient forces. We anticipate potentially groundbreaking applications of the optimized optical trapping system for biomolecular analysis in aqueous environments, levitated cavity-optomechanics, and cold atom physics, constituting an important step towards realizing integrated bio-nanophotonics and mesoscopic quantum mechanical experiments.","sentences":["Optical trapping enables precise control of individual particles of different sizes, such as atoms, molecules, or nanospheres.","Optical tweezers provide free-space omnidirectional optical trapping of objects in laboratories around the world.","As an alternative to standard macroscopic setups based on lenses, which are inherently bound by the diffraction limit, plasmonic and photonic nanostructures promise trapping by near-field optical effects on the extreme nanoscale.","However, the practical design of lossless waveguide-coupled nanostructures capable of trapping deeply sub-wavelength particles in all spatial directions using the gradient force has until now proven insurmountable.","In this work, we demonstrate an omnidirectional optical trap realized by inverse-designing fabrication-ready integrated dielectric nanocavities.","The sub-wavelength optical trap is designed to rely solely on the gradient force and is thus particle-size agnostic.","In particular, we show how a nanometer-sized trapped particle experiences a force strong enough to overcome room-temperature thermal fluctuations.","Furthermore, through the robust inverse design framework, we tailor manufacturable devices operating at near-infrared and optical frequencies.","Our results open a new regime of levitated optical trapping by achieving a deep trapping potential capable of trapping single sub-wavelength particles in all directions using optical gradient forces.","We anticipate potentially groundbreaking applications of the optimized optical trapping system for biomolecular analysis in aqueous environments, levitated cavity-optomechanics, and cold atom physics, constituting an important step towards realizing integrated bio-nanophotonics and mesoscopic quantum mechanical experiments."],"url":"http://arxiv.org/abs/2404.15102v1","category":"physics.optics"}
{"created":"2024-04-23 14:53:30","title":"Designing athermal disordered solids with automatic differentiation","abstract":"The ability to control forces between sub-micron-scale building blocks offers considerable potential for designing new materials through self-assembly. A typical paradigm is to first identify a particular (crystal) structure that has some desired property, and then design building-block interactions so that this structure assembles spontaneously. While significant theoretical and experimental progress has been made in assembling complicated structures in a variety of systems, this two-step paradigm fundamentally fails for structurally disordered solids, which lack a well-defined structure to use as a target. Here we show that disordered solids can still be treated from an inverse self-assembly perspective by targeting material properties directly. Using the Poisson's ratio, $\\nu$, as a primary example, we show how differentiable programming connects experimentally relevant interaction parameters with emergent behavior, allowing us to iteratively \"train\" the system until we find the set of interactions that leads to the Poisson's ratio we desire. Beyond the Poisson's ratio, we also tune the pressure and a measure of local 8-fold structural order, as well as multiple properties simultaneously, demonstrating the potential for nontrivial design in disordered solids. This approach is highly robust, transferable, and scalable, can handle a wide variety of model systems, properties of interest, and preparation dynamics, and can optimize over 100s or even 1000s of parameters. This result connects the fields of disordered solids and inverse self-assembly, indicating that many of the tools and ideas that have been developed to understand the assembly of crystals can also be used to control the properties of disordered solids.","sentences":["The ability to control forces between sub-micron-scale building blocks offers considerable potential for designing new materials through self-assembly.","A typical paradigm is to first identify a particular (crystal) structure that has some desired property, and then design building-block interactions so that this structure assembles spontaneously.","While significant theoretical and experimental progress has been made in assembling complicated structures in a variety of systems, this two-step paradigm fundamentally fails for structurally disordered solids, which lack a well-defined structure to use as a target.","Here we show that disordered solids can still be treated from an inverse self-assembly perspective by targeting material properties directly.","Using the Poisson's ratio, $\\nu$, as a primary example, we show how differentiable programming connects experimentally relevant interaction parameters with emergent behavior, allowing us to iteratively \"train\" the system until we find the set of interactions that leads to the Poisson's ratio we desire.","Beyond the Poisson's ratio, we also tune the pressure and a measure of local 8-fold structural order, as well as multiple properties simultaneously, demonstrating the potential for nontrivial design in disordered solids.","This approach is highly robust, transferable, and scalable, can handle a wide variety of model systems, properties of interest, and preparation dynamics, and can optimize over 100s or even 1000s of parameters.","This result connects the fields of disordered solids and inverse self-assembly, indicating that many of the tools and ideas that have been developed to understand the assembly of crystals can also be used to control the properties of disordered solids."],"url":"http://arxiv.org/abs/2404.15101v1","category":"cond-mat.soft"}
{"created":"2024-04-23 14:53:08","title":"A Realisation of Channel Emulation in a Reverberation Chamber method for Over-the-Air Compliance Testing in Support of 3GPP Standardisation","abstract":"The inherent long decay power delay profile (PDP) in the reverberation chamber (RC) is a major challenge for accurate channel emulation of 3GPP channel model, which is widely used in performance test of the physical layer. To tackle this challenge, we propose in this paper a novel two-step \"closed-loop\" approach consisting of (i) a channel measuring step and (ii) a channel model synthesis step. The channel measurement step is used to capture the wireless channel of the RC. In the channel model synthesis step, an additional IQ signal convolution process is introduced prior the IQ signal passes through the channel emulator (CE). This process filters the IQ signal by an equalizer filter derived from the measured channel impulse response (CIR) of the RC obtained in channel measurement step. From the measurement results, the proposed approach is proven that able to effectively emulate typical 3GPP 5G channel model.","sentences":["The inherent long decay power delay profile (PDP) in the reverberation chamber (RC) is a major challenge for accurate channel emulation of 3GPP channel model, which is widely used in performance test of the physical layer.","To tackle this challenge, we propose in this paper a novel two-step \"closed-loop\" approach consisting of (i) a channel measuring step and (ii) a channel model synthesis step.","The channel measurement step is used to capture the wireless channel of the RC.","In the channel model synthesis step, an additional IQ signal convolution process is introduced prior the IQ signal passes through the channel emulator (CE).","This process filters the IQ signal by an equalizer filter derived from the measured channel impulse response (CIR) of the RC obtained in channel measurement step.","From the measurement results, the proposed approach is proven that able to effectively emulate typical 3GPP 5G channel model."],"url":"http://arxiv.org/abs/2404.15099v1","category":"eess.SP"}
{"created":"2024-04-23 14:52:12","title":"What can cosmic-ray knees reveal about source populations?","abstract":"Cosmic ray (CR) knees (spectral steepenings) encode information on CR accelerator populations. We seek population features that imprint onto knee observables in a manner that is robust enough to be discernible even in the presence of significant systematics in CR data. In particular, we explore how diversity among population members could imprint on the knee phenomenology, under the assumption that a knee is due to a fixed-rigidity cutoff in the source spectra. We use a simple theoretical model for a population of CR accelerators. Each population member accelerates CR to a power-law spectrum, up to a cutoff rigidity. We allow for variance among members, in cutoff rigidity and power-law slope. We find that: (a) the slope step of the spectrum is $\\sim 0.5$, decreasing weakly with increasing spread in either property; (b) composition always breaks first; (c) the difference between the break energies in composition and flux increases with increasing diversity; (d) composition and flux break together only if population diversity is minimal. These trends are robust under our assumptions; deviations from them would indicate more complex physics than encoded in our simple model. Comparing these trends with observed CR knees, we conclude that: (i) the primary knee at $\\sim 4\\times10^{15}$ eV is consistent with a constant-rigidity cutoff according to KASCADE-Grande data processed with post-LHC hadronic models, but not according to other datasets; (ii) the second knee at $\\sim 5 \\times 10^{17}$ eV requires more complexity than our model; (iii) the spectral feature identified by Auger at $\\sim 10^{19}$ eV is consistent with a constant-rigidity source cutoff only if there is a substantial spread in both cutoff rigidity and slope. Interestingly, a significant spread in slope would also result in spectral curvature before the break, which would in turn be contributing to the ankle feature.","sentences":["Cosmic ray (CR) knees (spectral steepenings) encode information on CR accelerator populations.","We seek population features that imprint onto knee observables in a manner that is robust enough to be discernible even in the presence of significant systematics in CR data.","In particular, we explore how diversity among population members could imprint on the knee phenomenology, under the assumption that a knee is due to a fixed-rigidity cutoff in the source spectra.","We use a simple theoretical model for a population of CR accelerators.","Each population member accelerates CR to a power-law spectrum, up to a cutoff rigidity.","We allow for variance among members, in cutoff rigidity and power-law slope.","We find that: (a) the slope step of the spectrum is $\\sim 0.5$, decreasing weakly with increasing spread in either property; (b) composition always breaks first; (c) the difference between the break energies in composition and flux increases with increasing diversity; (d) composition and flux break together only if population diversity is minimal.","These trends are robust under our assumptions; deviations from them would indicate more complex physics than encoded in our simple model.","Comparing these trends with observed CR knees, we conclude that: (i) the primary knee at $\\sim 4\\times10^{15}$ eV is consistent with a constant-rigidity cutoff according to KASCADE-Grande data processed with post-LHC hadronic models, but not according to other datasets; (ii) the second knee at $\\sim 5 \\times 10^{17}$ eV requires more complexity than our model; (iii) the spectral feature identified by Auger at $\\sim 10^{19}$ eV is consistent with a constant-rigidity source cutoff only if there is a substantial spread in both cutoff rigidity and slope.","Interestingly, a significant spread in slope would also result in spectral curvature before the break, which would in turn be contributing to the ankle feature."],"url":"http://arxiv.org/abs/2404.15097v1","category":"astro-ph.HE"}
{"created":"2024-04-23 14:49:32","title":"Super-resolved CARS by coherent image scanning","abstract":"We present super-resolved coherent anti-Stokes Raman scattering (CARS) microscopy by implementing phase-resolved image scanning microscopy (ISM), achieving up to two-fold resolution increase as compared with a conventional CARS microscope. Phase-sensitivity is required for the standard pixel-reassignment procedure since the scattered field is coherent, thus the point-spread function (PSF) is well-defined only for the field amplitude. We resolve the complex field by a simple add-on to the CARS setup enabling inline interferometry. Phase-sensitivity offers additional contrast which informs the spatial distribution of both resonant and nonresonant scatterers. As compared with alternative super-resolution schemes in coherent nonlinear microscopy, the proposed method is simple, requires only low-intensity excitation, and is compatible with any conventional forward-detected CARS imaging setup.","sentences":["We present super-resolved coherent anti-Stokes Raman scattering (CARS) microscopy by implementing phase-resolved image scanning microscopy (ISM), achieving up to two-fold resolution increase as compared with a conventional CARS microscope.","Phase-sensitivity is required for the standard pixel-reassignment procedure since the scattered field is coherent, thus the point-spread function (PSF) is well-defined only for the field amplitude.","We resolve the complex field by a simple add-on to the CARS setup enabling inline interferometry.","Phase-sensitivity offers additional contrast which informs the spatial distribution of both resonant and nonresonant scatterers.","As compared with alternative super-resolution schemes in coherent nonlinear microscopy, the proposed method is simple, requires only low-intensity excitation, and is compatible with any conventional forward-detected CARS imaging setup."],"url":"http://arxiv.org/abs/2404.15094v1","category":"physics.optics"}
{"created":"2024-04-23 14:39:06","title":"Successive Phase Transition in Higher-order Topological Anderson Insulators","abstract":"Disorder, traditionally believed to hinder the propagation of waves. has recently been shown to prompt the occurrence of topological phase transitions. For example, when disorder strength continuously increases and surpasses certain critical value, a phase transition from topologically trivial to nontrivial insulating phases occurs. However, in the parameter domain of the nontrivial phase, whether there exists a finer phase diagram that can be further classified by different disorder strengths is still unclear. Here we present a successive topological phase transition driven by the disorder strength in a higher-order topological insulator with long-range couplings. As the strength of the disorder gradually increases, the real-space topological invariant of the system undergoes a consecutive change from 0 to 4, accompanied by the stepped increase in the number of boundary-localized corner states. Our work opens an avenue for utilizing disorder to induce phase transitions among different higher-order topological insulators.","sentences":["Disorder, traditionally believed to hinder the propagation of waves. has recently been shown to prompt the occurrence of topological phase transitions.","For example, when disorder strength continuously increases and surpasses certain critical value, a phase transition from topologically trivial to nontrivial insulating phases occurs.","However, in the parameter domain of the nontrivial phase, whether there exists a finer phase diagram that can be further classified by different disorder strengths is still unclear.","Here we present a successive topological phase transition driven by the disorder strength in a higher-order topological insulator with long-range couplings.","As the strength of the disorder gradually increases, the real-space topological invariant of the system undergoes a consecutive change from 0 to 4, accompanied by the stepped increase in the number of boundary-localized corner states.","Our work opens an avenue for utilizing disorder to induce phase transitions among different higher-order topological insulators."],"url":"http://arxiv.org/abs/2404.15088v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-23 14:21:34","title":"The Complex Estimand of Clone-Censor-Weighting When Studying Treatment Initiation Windows","abstract":"Clone-censor-weighting (CCW) is an analytic method for studying treatment regimens that are indistinguishable from one another at baseline without relying on landmark dates or creating immortal person time. One particularly interesting CCW application is estimating outcomes when starting treatment within specific time windows in observational data (e.g., starting a treatment within 30 days of hospitalization). In such cases, CCW estimates something fairly complex. We show how using CCW to study a regimen such as \"start treatment prior to day 30\" estimates the potential outcome of a hypothetical intervention where A) prior to day 30, everyone follows the treatment start distribution of the study population and B) everyone who has not initiated by day 30 initiates on day 30. As a result, the distribution of treatment initiation timings provides essential context for the results of CCW studies. We also show that if the exposure effect varies over time, ignoring exposure history when estimating inverse probability of censoring weights (IPCW) estimates the risk under an impossible intervention and can create selection bias. Finally, we examine some simplifying assumptions that can make this complex treatment effect more interpretable and allow everyone to contribute to IPCW.","sentences":["Clone-censor-weighting (CCW) is an analytic method for studying treatment regimens that are indistinguishable from one another at baseline without relying on landmark dates or creating immortal person time.","One particularly interesting CCW application is estimating outcomes when starting treatment within specific time windows in observational data (e.g., starting a treatment within 30 days of hospitalization).","In such cases, CCW estimates something fairly complex.","We show how using CCW to study a regimen such as \"start treatment prior to day 30\" estimates the potential outcome of a hypothetical intervention where A) prior to day 30, everyone follows the treatment start distribution of the study population and B) everyone who has not initiated by day 30 initiates on day 30.","As a result, the distribution of treatment initiation timings provides essential context for the results of CCW studies.","We also show that if the exposure effect varies over time, ignoring exposure history when estimating inverse probability of censoring weights (IPCW) estimates the risk under an impossible intervention and can create selection bias.","Finally, we examine some simplifying assumptions that can make this complex treatment effect more interpretable and allow everyone to contribute to IPCW."],"url":"http://arxiv.org/abs/2404.15073v1","category":"stat.ME"}
{"created":"2024-04-23 13:50:35","title":"Global Complexity Analysis of BFGS","abstract":"In this paper, we present a global complexity analysis of the classical BFGS method with inexact line search, as applied to minimizing a strongly convex function with Lipschitz continuous gradient and Hessian. We consider a variety of standard line search strategies including the backtracking line search based on the Armijo condition, Armijo-Goldstein and Wolfe-Powell line searches. Our analysis suggests that the convergence of the algorithm proceeds in several different stages before the fast superlinear convergence actually begins. Furthermore, once the initial point is far away from the minimizer, the starting moment of superlinear convergence may be quite large. We show, however, that this drawback can be easily rectified by using a simple restarting procedure.","sentences":["In this paper, we present a global complexity analysis of the classical BFGS method with inexact line search, as applied to minimizing a strongly convex function with Lipschitz continuous gradient and Hessian.","We consider a variety of standard line search strategies including the backtracking line search based on the Armijo condition, Armijo-Goldstein and Wolfe-Powell line searches.","Our analysis suggests that the convergence of the algorithm proceeds in several different stages before the fast superlinear convergence actually begins.","Furthermore, once the initial point is far away from the minimizer, the starting moment of superlinear convergence may be quite large.","We show, however, that this drawback can be easily rectified by using a simple restarting procedure."],"url":"http://arxiv.org/abs/2404.15051v1","category":"math.OC"}
{"created":"2024-04-23 13:50:07","title":"Achieving the volume-law entropy regime with random-sign Dicke states","abstract":"Manipulating entanglement, which reflects non-local correlations in a quantum system and defines the complexity of describing its wave function, represents the extremely tough challenge in the fields of quantum computing, quantum information, and condensed matter physics. In this work, by the example of the well-structured Dicke states we demonstrate that the complexity of these real-valued wave functions can be accurately tuned by introducing a random-sign structure, which allows us to explore the regime of the volume-law entanglement. Importantly, setting nontrivial sign structure one can increase the entanglement entropy of the Dicke state to the values that are close to Page's estimates for Haar-random states. The practical realization of these random-sign Dicke states is possible on different physical platforms with shallow quantum circuits. On the level of the measurements the change in the quantum state complexity due to sign structure can be traced out with the dissimilarity measure that estimates multi-scale variety of patterns in bit-string arrays.","sentences":["Manipulating entanglement, which reflects non-local correlations in a quantum system and defines the complexity of describing its wave function, represents the extremely tough challenge in the fields of quantum computing, quantum information, and condensed matter physics.","In this work, by the example of the well-structured Dicke states we demonstrate that the complexity of these real-valued wave functions can be accurately tuned by introducing a random-sign structure, which allows us to explore the regime of the volume-law entanglement.","Importantly, setting nontrivial sign structure one can increase the entanglement entropy of the Dicke state to the values that are close to Page's estimates for Haar-random states.","The practical realization of these random-sign Dicke states is possible on different physical platforms with shallow quantum circuits.","On the level of the measurements the change in the quantum state complexity due to sign structure can be traced out with the dissimilarity measure that estimates multi-scale variety of patterns in bit-string arrays."],"url":"http://arxiv.org/abs/2404.15050v1","category":"quant-ph"}
{"created":"2024-04-23 13:48:04","title":"Complete CP-eigen Bases of Meson-Baryon Chiral Lagrangian up to $p^5$-order","abstract":"Chiral perturbation theory describes the low energy dynamics of mesons and baryons in terms of the nonlinear Goldstone boson and fermion degrees of freedom. Through the Young tensor technique, we construct the on-shell operator bases for the meson-baryon system up to $p^5$-order, using the chiral dimension power counting and heavy baryon expansion. For the Lorentz structure, additional treatments on off-shell external sources and operators with higher derivatives are necessarily considered, while for the internal structure, the invariant tensor basis is converted into the trace basis equivalently, and Cayley-Hamilton relations are utilized to classify different CP eigen-operators. Finally we present the complete operator set of $C$+$P$+, $C$+$P$-, $C$-$P$+, and $C$-$P$- eigen-operators at the $p^5$-order, and obtain the operator counting from the Hilbert series.","sentences":["Chiral perturbation theory describes the low energy dynamics of mesons and baryons in terms of the nonlinear Goldstone boson and fermion degrees of freedom.","Through the Young tensor technique, we construct the on-shell operator bases for the meson-baryon system up to $p^5$-order, using the chiral dimension power counting and heavy baryon expansion.","For the Lorentz structure, additional treatments on off-shell external sources and operators with higher derivatives are necessarily considered, while for the internal structure, the invariant tensor basis is converted into the trace basis equivalently, and Cayley-Hamilton relations are utilized to classify different CP eigen-operators.","Finally we present the complete operator set of $C$+$P$+, $C$+$P$-, $C$-$P$+, and $C$-$P$- eigen-operators at the $p^5$-order, and obtain the operator counting from the Hilbert series."],"url":"http://arxiv.org/abs/2404.15047v1","category":"hep-ph"}
{"created":"2024-04-23 13:42:12","title":"DP-Net: Learning Discriminative Parts for image recognition","abstract":"This paper presents Discriminative Part Network (DP-Net), a deep architecture with strong interpretation capabilities, which exploits a pretrained Convolutional Neural Network (CNN) combined with a part-based recognition module. This system learns and detects parts in the images that are discriminative among categories, without the need for fine-tuning the CNN, making it more scalable than other part-based models. While part-based approaches naturally offer interpretable representations, we propose explanations at image and category levels and introduce specific constraints on the part learning process to make them more discrimative.","sentences":["This paper presents Discriminative Part Network (DP-Net), a deep architecture with strong interpretation capabilities, which exploits a pretrained Convolutional Neural Network (CNN) combined with a part-based recognition module.","This system learns and detects parts in the images that are discriminative among categories, without the need for fine-tuning the CNN, making it more scalable than other part-based models.","While part-based approaches naturally offer interpretable representations, we propose explanations at image and category levels and introduce specific constraints on the part learning process to make them more discrimative."],"url":"http://arxiv.org/abs/2404.15037v1","category":"cs.CV"}
{"created":"2024-04-23 13:36:13","title":"1-bit raw voltage recording system for dedicated observations of transients at low radio frequencies","abstract":"Recently we had reported commissioning of a prototype for pulsar observations at low radio frequencies (<100 MHz) using log-periodic dipole antennas (LPDAs) in the Gauribidanur Radio Observatory near Bangalore in India. The aforementioned system (GAuribidanur Pulsar System, GAPS) is currently being augmented to directly digitize the radio frequency signals from the individual antennas in the array. Our initial results using 1-bit raw voltage recording system indicates that such a back-end receiver offers distinct advantages like, (i) simultaneous observations of any set of desired directions in the sky with multiple offline beams and smaller data rate/volume, (ii) archival of the observed data with minimal resources for re-analysis in the future, either in the same or different set of directions in the sky.","sentences":["Recently we had reported commissioning of a prototype for pulsar observations at low radio frequencies (<100 MHz) using log-periodic dipole antennas (LPDAs) in the Gauribidanur Radio Observatory near Bangalore in India.","The aforementioned system (GAuribidanur Pulsar System, GAPS) is currently being augmented to directly digitize the radio frequency signals from the individual antennas in the array.","Our initial results using 1-bit raw voltage recording system indicates that such a back-end receiver offers distinct advantages like, (i) simultaneous observations of any set of desired directions in the sky with multiple offline beams and smaller data rate/volume, (ii) archival of the observed data with minimal resources for re-analysis in the future, either in the same or different set of directions in the sky."],"url":"http://arxiv.org/abs/2404.15031v1","category":"astro-ph.IM"}
{"created":"2024-04-23 13:23:27","title":"Conformal Predictive Systems Under Covariate Shift","abstract":"Conformal Predictive Systems (CPS) offer a versatile framework for constructing predictive distributions, allowing for calibrated inference and informative decision-making. However, their applicability has been limited to scenarios adhering to the Independent and Identically Distributed (IID) model assumption. This paper extends CPS to accommodate scenarios characterized by covariate shifts. We therefore propose Weighted CPS (WCPS), akin to Weighted Conformal Prediction (WCP), leveraging likelihood ratios between training and testing covariate distributions. This extension enables the construction of nonparametric predictive distributions capable of handling covariate shifts. We present theoretical underpinnings and conjectures regarding the validity and efficacy of WCPS and demonstrate its utility through empirical evaluations on both synthetic and real-world datasets. Our simulation experiments indicate that WCPS are probabilistically calibrated under covariate shift.","sentences":["Conformal Predictive Systems (CPS) offer a versatile framework for constructing predictive distributions, allowing for calibrated inference and informative decision-making.","However, their applicability has been limited to scenarios adhering to the Independent and Identically Distributed (IID) model assumption.","This paper extends CPS to accommodate scenarios characterized by covariate shifts.","We therefore propose Weighted CPS (WCPS), akin to Weighted Conformal Prediction (WCP), leveraging likelihood ratios between training and testing covariate distributions.","This extension enables the construction of nonparametric predictive distributions capable of handling covariate shifts.","We present theoretical underpinnings and conjectures regarding the validity and efficacy of WCPS and demonstrate its utility through empirical evaluations on both synthetic and real-world datasets.","Our simulation experiments indicate that WCPS are probabilistically calibrated under covariate shift."],"url":"http://arxiv.org/abs/2404.15018v1","category":"cs.LG"}
{"created":"2024-04-23 13:22:22","title":"A Hybrid Quantum-Classical Physics-Informed Neural Network Architecture for Solving Quantum Optimal Control Problems","abstract":"This paper proposes an integrated quantum-classical approach that merges quantum computational dynamics with classical computing methodologies tailored to address control problems based on Pontryagin's minimum principle within a Physics-Informed Neural Network (PINN) framework. By leveraging a dynamic quantum circuit that combines Gaussian and non-Gaussian gates, the study showcases an innovative approach to optimizing quantum state manipulations. The proposed hybrid model effectively applies machine learning techniques to solve optimal control problems. This is illustrated through the design and implementation of a hybrid PINN network to solve a quantum state transition problem in a two and three-level system, highlighting its potential across various quantum computing applications.","sentences":["This paper proposes an integrated quantum-classical approach that merges quantum computational dynamics with classical computing methodologies tailored to address control problems based on Pontryagin's minimum principle within a Physics-Informed Neural Network (PINN) framework.","By leveraging a dynamic quantum circuit that combines Gaussian and non-Gaussian gates, the study showcases an innovative approach to optimizing quantum state manipulations.","The proposed hybrid model effectively applies machine learning techniques to solve optimal control problems.","This is illustrated through the design and implementation of a hybrid PINN network to solve a quantum state transition problem in a two and three-level system, highlighting its potential across various quantum computing applications."],"url":"http://arxiv.org/abs/2404.15015v1","category":"quant-ph"}
{"created":"2024-04-23 13:15:37","title":"Shaping non-reciprocal caustic spin-wave beams","abstract":"A caustic is a mathematical concept describing the beam formation when the beam envelope is reflected or refracted by a manifold. While caustics are common in a wide range of physical systems, caustics typically exhibit a reciprocal wave propagation and are challenging to control. Here, we utilize the highly anisotropic dispersion and inherent non-reciprocity of a magnonic system to shape non-reciprocal emission of caustic-like spin wave beams in an extended 200 nm thick yttrium iron garnet (YIG) film from a nano-constricted rf waveguide. We introduce a near-field diffraction model to study spin-wave beamforming in homogeneous in-plane magnetized thin films, and reveal the propagation of non-reciprocal spin-wave beams directly emitted from the nanoconstriction by spatially resolved micro-focused Brillouin light spectroscopy (BLS). The experimental results agree well with both micromagnetic simulation, and the near-field diffraction model. The proposed method can be readily implemented to study spin-wave interference at the sub-micron scale, which is central to the development of wave-based computing applications and magnonic devices.","sentences":["A caustic is a mathematical concept describing the beam formation when the beam envelope is reflected or refracted by a manifold.","While caustics are common in a wide range of physical systems, caustics typically exhibit a reciprocal wave propagation and are challenging to control.","Here, we utilize the highly anisotropic dispersion and inherent non-reciprocity of a magnonic system to shape non-reciprocal emission of caustic-like spin wave beams in an extended 200 nm thick yttrium iron garnet (YIG) film from a nano-constricted rf waveguide.","We introduce a near-field diffraction model to study spin-wave beamforming in homogeneous in-plane magnetized thin films, and reveal the propagation of non-reciprocal spin-wave beams directly emitted from the nanoconstriction by spatially resolved micro-focused Brillouin light spectroscopy (BLS).","The experimental results agree well with both micromagnetic simulation, and the near-field diffraction model.","The proposed method can be readily implemented to study spin-wave interference at the sub-micron scale, which is central to the development of wave-based computing applications and magnonic devices."],"url":"http://arxiv.org/abs/2404.15011v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-23 13:15:22","title":"The Brain Tumor Segmentation in Pediatrics (BraTS-PEDs) Challenge: Focus on Pediatrics (CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs)","abstract":"Pediatric tumors of the central nervous system are the most common cause of cancer-related death in children. The five-year survival rate for high-grade gliomas in children is less than 20%. Due to their rarity, the diagnosis of these entities is often delayed, their treatment is mainly based on historic treatment concepts, and clinical trials require multi-institutional collaborations. Here we present the CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs challenge, focused on pediatric brain tumors with data acquired across multiple international consortia dedicated to pediatric neuro-oncology and clinical trials. The CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs challenge brings together clinicians and AI/imaging scientists to lead to faster development of automated segmentation techniques that could benefit clinical trials, and ultimately the care of children with brain tumors.","sentences":["Pediatric tumors of the central nervous system are the most common cause of cancer-related death in children.","The five-year survival rate for high-grade gliomas in children is less than 20%.","Due to their rarity, the diagnosis of these entities is often delayed, their treatment is mainly based on historic treatment concepts, and clinical trials require multi-institutional collaborations.","Here we present the CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs challenge, focused on pediatric brain tumors with data acquired across multiple international consortia dedicated to pediatric neuro-oncology and clinical trials.","The CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs challenge brings together clinicians and AI/imaging scientists to lead to faster development of automated segmentation techniques that could benefit clinical trials, and ultimately the care of children with brain tumors."],"url":"http://arxiv.org/abs/2404.15009v1","category":"cs.CV"}
{"created":"2024-04-23 13:09:13","title":"Scandium Aluminum Nitride Overmoded Bulk Acoustic Resonators for Future Wireless Communication","abstract":"This work reports on the modeling, fabrication, and experimental characterization of a 13 GHz 30% Scandium-doped Aluminum Nitride (ScAlN) Overmoded Bulk Acoustic Resonator (OBAR) for high-frequency Radio Frequency (RF) applications, notably in 5G technology and beyond. The Finite Element Analysis (FEA) optimization process targets the top and bottom metal electrode thicknesses, balancing the electromechanical coupling coefficient and acoustic energy distribution to enhance device Figure of Merit (FOM). Experimental results on fabricated devices employing platinum and aluminum as bottom and top electrode, respectively, demonstrate a quality factor at resonance (Qs) of 210 and a coupling coefficient (kt2) of 5.2% at 13.3 GHz for the second bulk thickness overtone, effectively validating the simulation framework and hinting at the possible implementation of OBARs for advanced RF filters in 5G networks.","sentences":["This work reports on the modeling, fabrication, and experimental characterization of a 13 GHz 30% Scandium-doped Aluminum Nitride (ScAlN) Overmoded Bulk Acoustic Resonator (OBAR) for high-frequency Radio Frequency (RF) applications, notably in 5G technology and beyond.","The Finite Element Analysis (FEA) optimization process targets the top and bottom metal electrode thicknesses, balancing the electromechanical coupling coefficient and acoustic energy distribution to enhance device Figure of Merit (FOM).","Experimental results on fabricated devices employing platinum and aluminum as bottom and top electrode, respectively, demonstrate a quality factor at resonance (Qs) of 210 and a coupling coefficient (kt2) of 5.2% at 13.3 GHz for the second bulk thickness overtone, effectively validating the simulation framework and hinting at the possible implementation of OBARs for advanced RF filters in 5G networks."],"url":"http://arxiv.org/abs/2404.15005v1","category":"eess.SY"}
{"created":"2024-04-23 13:05:36","title":"Nanoscale single-electron box with a floating lead for quantum sensing: modelling and device characterization","abstract":"We present an in-depth analysis of a single-electron box (SEB) biased through a floating node technique that is common in charge-coupled devices (CCDs). The device is analyzed and characterized in the context of single-electron charge-sensing techniques for integrated silicon quantum dots (QD). The unique aspect of our SEB design is the incorporation of a metallic floating node, strategically employed for sensing and precise injection of electrons into an electrostatically formed QD. To analyse the SEB, we propose an extended multi-orbital Anderson impurity model (MOAIM), adapted to our nanoscale SEB system, that is used to predict theoretically the behaviour of the SEB in the context of a charge-sensing application. The validation of the model and the sensing technique has been carried out on a QD fabricated in a fully depleted silicon on insulator (FDSOI) process on a 22-nm technological node. We demonstrate the MOAIM's efficacy in predicting the observed electronic behavior and elucidating the complex electron dynamics and correlations in the SEB. The results of our study reinforce the versatility and precision of the model in the realm of nanoelectronics and highlight the practical utility of the metallic floating node as a mechanism for charge injection and detection in integrated QDs. Finally, we identify the limitations of our model in capturing higher-order effects observed in our measurements and propose future outlooks to reconcile some of these discrepancies.","sentences":["We present an in-depth analysis of a single-electron box (SEB) biased through a floating node technique that is common in charge-coupled devices (CCDs).","The device is analyzed and characterized in the context of single-electron charge-sensing techniques for integrated silicon quantum dots (QD).","The unique aspect of our SEB design is the incorporation of a metallic floating node, strategically employed for sensing and precise injection of electrons into an electrostatically formed QD.","To analyse the SEB, we propose an extended multi-orbital Anderson impurity model (MOAIM), adapted to our nanoscale SEB system, that is used to predict theoretically the behaviour of the SEB in the context of a charge-sensing application.","The validation of the model and the sensing technique has been carried out on a QD fabricated in a fully depleted silicon on insulator (FDSOI) process on a 22-nm technological node.","We demonstrate the MOAIM's efficacy in predicting the observed electronic behavior and elucidating the complex electron dynamics and correlations in the SEB.","The results of our study reinforce the versatility and precision of the model in the realm of nanoelectronics and highlight the practical utility of the metallic floating node as a mechanism for charge injection and detection in integrated QDs.","Finally, we identify the limitations of our model in capturing higher-order effects observed in our measurements and propose future outlooks to reconcile some of these discrepancies."],"url":"http://arxiv.org/abs/2404.15002v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-23 13:02:11","title":"A Unified Replay-based Continuous Learning Framework for Spatio-Temporal Prediction on Streaming Data","abstract":"The widespread deployment of wireless and mobile devices results in a proliferation of spatio-temporal data that is used in applications, e.g., traffic prediction, human mobility mining, and air quality prediction, where spatio-temporal prediction is often essential to enable safety, predictability, or reliability. Many recent proposals that target deep learning for spatio-temporal prediction suffer from so-called catastrophic forgetting, where previously learned knowledge is entirely forgotten when new data arrives. Such proposals may experience deteriorating prediction performance when applied in settings where data streams into the system. To enable spatio-temporal prediction on streaming data, we propose a unified replay-based continuous learning framework. The framework includes a replay buffer of previously learned samples that are fused with training data using a spatio-temporal mixup mechanism in order to preserve historical knowledge effectively, thus avoiding catastrophic forgetting. To enable holistic representation preservation, the framework also integrates a general spatio-temporal autoencoder with a carefully designed spatio-temporal simple siamese (STSimSiam) network that aims to ensure prediction accuracy and avoid holistic feature loss by means of mutual information maximization. The framework further encompasses five spatio-temporal data augmentation methods to enhance the performance of STSimSiam. Extensive experiments on real data offer insight into the effectiveness of the proposed framework.","sentences":["The widespread deployment of wireless and mobile devices results in a proliferation of spatio-temporal data that is used in applications, e.g., traffic prediction, human mobility mining, and air quality prediction, where spatio-temporal prediction is often essential to enable safety, predictability, or reliability.","Many recent proposals that target deep learning for spatio-temporal prediction suffer from so-called catastrophic forgetting, where previously learned knowledge is entirely forgotten when new data arrives.","Such proposals may experience deteriorating prediction performance when applied in settings where data streams into the system.","To enable spatio-temporal prediction on streaming data, we propose a unified replay-based continuous learning framework.","The framework includes a replay buffer of previously learned samples that are fused with training data using a spatio-temporal mixup mechanism in order to preserve historical knowledge effectively, thus avoiding catastrophic forgetting.","To enable holistic representation preservation, the framework also integrates a general spatio-temporal autoencoder with a carefully designed spatio-temporal simple siamese (STSimSiam) network that aims to ensure prediction accuracy and avoid holistic feature loss by means of mutual information maximization.","The framework further encompasses five spatio-temporal data augmentation methods to enhance the performance of STSimSiam.","Extensive experiments on real data offer insight into the effectiveness of the proposed framework."],"url":"http://arxiv.org/abs/2404.14999v1","category":"cs.DB"}
{"created":"2024-04-23 12:59:36","title":"Mining higher-order triadic interactions","abstract":"Complex systems often present higher-order interactions which require us to go beyond their description in terms of pairwise networks. Triadic interactions are a fundamental type of higher-order interaction that occurs when one node regulates the interaction between two other nodes. Triadic interactions are a fundamental type of higher-order networks, found in a large variety of biological systems, from neuron-glia interactions to gene-regulation and ecosystems. However, triadic interactions have been so far mostly neglected. In this article, we propose a theoretical principle to model and mine triadic interactions from node metadata, and we apply this framework to gene expression data finding new candidates for triadic interactions relevant for Acute Myeloid Leukemia. Our work reveals important aspects of higher-order triadic interactions often ignored, which can transform our understanding of complex systems and be applied to a large variety of systems ranging from biology to the climate.","sentences":["Complex systems often present higher-order interactions which require us to go beyond their description in terms of pairwise networks.","Triadic interactions are a fundamental type of higher-order interaction that occurs when one node regulates the interaction between two other nodes.","Triadic interactions are a fundamental type of higher-order networks, found in a large variety of biological systems, from neuron-glia interactions to gene-regulation and ecosystems.","However, triadic interactions have been so far mostly neglected.","In this article, we propose a theoretical principle to model and mine triadic interactions from node metadata, and we apply this framework to gene expression data finding new candidates for triadic interactions relevant for Acute Myeloid Leukemia.","Our work reveals important aspects of higher-order triadic interactions often ignored, which can transform our understanding of complex systems and be applied to a large variety of systems ranging from biology to the climate."],"url":"http://arxiv.org/abs/2404.14997v1","category":"nlin.AO"}
{"created":"2024-04-23 12:46:53","title":"A Reproducibility Study of PLAID","abstract":"The PLAID (Performance-optimized Late Interaction Driver) algorithm for ColBERTv2 uses clustered term representations to retrieve and progressively prune documents for final (exact) document scoring. In this paper, we reproduce and fill in missing gaps from the original work. By studying the parameters PLAID introduces, we find that its Pareto frontier is formed of a careful balance among its three parameters; deviations beyond the suggested settings can substantially increase latency without necessarily improving its effectiveness. We then compare PLAID with an important baseline missing from the paper: re-ranking a lexical system. We find that applying ColBERTv2 as a re-ranker atop an initial pool of BM25 results provides better efficiency-effectiveness trade-offs in low-latency settings. However, re-ranking cannot reach peak effectiveness at higher latency settings due to limitations in recall of lexical matching and provides a poor approximation of an exhaustive ColBERTv2 search. We find that recently proposed modifications to re-ranking that pull in the neighbors of top-scoring documents overcome this limitation, providing a Pareto frontier across all operational points for ColBERTv2 when evaluated using a well-annotated dataset. Curious about why re-ranking methods are highly competitive with PLAID, we analyze the token representation clusters PLAID uses for retrieval and find that most clusters are predominantly aligned with a single token and vice versa. Given the competitive trade-offs that re-ranking baselines exhibit, this work highlights the importance of carefully selecting pertinent baselines when evaluating the efficiency of retrieval engines.","sentences":["The PLAID (Performance-optimized Late Interaction Driver) algorithm for ColBERTv2 uses clustered term representations to retrieve and progressively prune documents for final (exact) document scoring.","In this paper, we reproduce and fill in missing gaps from the original work.","By studying the parameters PLAID introduces, we find that its Pareto frontier is formed of a careful balance among its three parameters; deviations beyond the suggested settings can substantially increase latency without necessarily improving its effectiveness.","We then compare PLAID with an important baseline missing from the paper: re-ranking a lexical system.","We find that applying ColBERTv2 as a re-ranker atop an initial pool of BM25 results provides better efficiency-effectiveness trade-offs in low-latency settings.","However, re-ranking cannot reach peak effectiveness at higher latency settings due to limitations in recall of lexical matching and provides a poor approximation of an exhaustive ColBERTv2 search.","We find that recently proposed modifications to re-ranking that pull in the neighbors of top-scoring documents overcome this limitation, providing a Pareto frontier across all operational points for ColBERTv2 when evaluated using a well-annotated dataset.","Curious about why re-ranking methods are highly competitive with PLAID, we analyze the token representation clusters PLAID uses for retrieval and find that most clusters are predominantly aligned with a single token and vice versa.","Given the competitive trade-offs that re-ranking baselines exhibit, this work highlights the importance of carefully selecting pertinent baselines when evaluating the efficiency of retrieval engines."],"url":"http://arxiv.org/abs/2404.14989v1","category":"cs.IR"}
{"created":"2024-04-23 12:44:07","title":"Localized Multi-Dimensional Patterns","abstract":"Localized patterns are coherent structures embedded in a quiescent state and occur in both discrete and continuous media across a wide range of applications. While it is well-understood how domain covering patterns (for example stripes and hexagons) emerge from a pattern-forming/Turing instability, analyzing the emergence of their localized counterparts remains a significant challenge. There has been considerable progress in studying localized patterns over the past few decades, often by employing innovative mathematical tools and techniques. In particular, the study of localized pattern formation has benefited greatly from numerical techniques; the continuing advancement in computational power has helped to both identify new types patterns and further our understanding of their behavior. We review recent advances regarding the complex behavior of localized patterns and the mathematical tools that have been developed to understand them, covering various topics from spatial dynamics, exponential asymptotics, and numerical methods. We observe that the mathematical understanding of localized patterns decreases as the spatial dimension increases, thus providing significant open problems that will form the basis for future investigations.","sentences":["Localized patterns are coherent structures embedded in a quiescent state and occur in both discrete and continuous media across a wide range of applications.","While it is well-understood how domain covering patterns (for example stripes and hexagons) emerge from a pattern-forming/Turing instability, analyzing the emergence of their localized counterparts remains a significant challenge.","There has been considerable progress in studying localized patterns over the past few decades, often by employing innovative mathematical tools and techniques.","In particular, the study of localized pattern formation has benefited greatly from numerical techniques; the continuing advancement in computational power has helped to both identify new types patterns and further our understanding of their behavior.","We review recent advances regarding the complex behavior of localized patterns and the mathematical tools that have been developed to understand them, covering various topics from spatial dynamics, exponential asymptotics, and numerical methods.","We observe that the mathematical understanding of localized patterns decreases as the spatial dimension increases, thus providing significant open problems that will form the basis for future investigations."],"url":"http://arxiv.org/abs/2404.14987v1","category":"nlin.PS"}
{"created":"2024-04-23 12:37:15","title":"The Solar System: structural overview, origins and evolution","abstract":"Understanding the origin and long-term evolution of the Solar System is a fundamental goal of planetary science and astrophysics. This chapter describes our current understanding of the key processes that shaped our planetary system, informed by empirical data such as meteorite measurements, observations of planet-forming disks around other stars, and exoplanets, and nourished by theoretical modeling and laboratory experiments. The processes at play range in size from microns to gas giants, and mostly took place within the gaseous planet-forming disk through the growth of mountain-sized planetesimals and Moon- to Mars-sized planetary embryos. A fundamental shift in our understanding came when it was realized (thanks to advances in exoplanet science) that the giant planets' orbits likely underwent large radial shifts during their early evolution, through gas- or planetesimal-driven migration and dynamical instability. The characteristics of the rocky planets (including Earth) were forged during this early dynamic phase. Our Solar System is currently middle-aged, and we can use astrophysical tools to forecast its demise in the distant future.","sentences":["Understanding the origin and long-term evolution of the Solar System is a fundamental goal of planetary science and astrophysics.","This chapter describes our current understanding of the key processes that shaped our planetary system, informed by empirical data such as meteorite measurements, observations of planet-forming disks around other stars, and exoplanets, and nourished by theoretical modeling and laboratory experiments.","The processes at play range in size from microns to gas giants, and mostly took place within the gaseous planet-forming disk through the growth of mountain-sized planetesimals and Moon- to Mars-sized planetary embryos.","A fundamental shift in our understanding came when it was realized (thanks to advances in exoplanet science) that the giant planets' orbits likely underwent large radial shifts during their early evolution, through gas- or planetesimal-driven migration and dynamical instability.","The characteristics of the rocky planets (including Earth) were forged during this early dynamic phase.","Our Solar System is currently middle-aged, and we can use astrophysical tools to forecast its demise in the distant future."],"url":"http://arxiv.org/abs/2404.14982v1","category":"astro-ph.EP"}
{"created":"2024-04-23 12:27:20","title":"Symbolic Integration Algorithm Selection with Machine Learning: LSTMs vs Tree LSTMs","abstract":"Computer Algebra Systems (e.g. Maple) are used in research, education, and industrial settings. One of their key functionalities is symbolic integration, where there are many sub-algorithms to choose from that can affect the form of the output integral, and the runtime. Choosing the right sub-algorithm for a given problem is challenging: we hypothesise that Machine Learning can guide this sub-algorithm choice. A key consideration of our methodology is how to represent the mathematics to the ML model: we hypothesise that a representation which encodes the tree structure of mathematical expressions would be well suited. We trained both an LSTM and a TreeLSTM model for sub-algorithm prediction and compared them to Maple's existing approach. Our TreeLSTM performs much better than the LSTM, highlighting the benefit of using an informed representation of mathematical expressions. It is able to produce better outputs than Maple's current state-of-the-art meta-algorithm, giving a strong basis for further research.","sentences":["Computer Algebra Systems (e.g. Maple) are used in research, education, and industrial settings.","One of their key functionalities is symbolic integration, where there are many sub-algorithms to choose from that can affect the form of the output integral, and the runtime.","Choosing the right sub-algorithm for a given problem is challenging: we hypothesise that Machine Learning can guide this sub-algorithm choice.","A key consideration of our methodology is how to represent the mathematics to the ML model: we hypothesise that a representation which encodes the tree structure of mathematical expressions would be well suited.","We trained both an LSTM and a TreeLSTM model for sub-algorithm prediction and compared them to Maple's existing approach.","Our TreeLSTM performs much better than the LSTM, highlighting the benefit of using an informed representation of mathematical expressions.","It is able to produce better outputs than Maple's current state-of-the-art meta-algorithm, giving a strong basis for further research."],"url":"http://arxiv.org/abs/2404.14973v1","category":"cs.LG"}
{"created":"2024-04-23 12:25:08","title":"Stark localization near Aubry-Andr\u00e9 criticality","abstract":"In this work we investigate the Stark localization near the Aubry-Andr\\'{e} (AA) critical point. We study system-dependent parameters, such as localization length, inverse participation ratio (IPR), and energy gap between the ground and first excited state, for characterizing the localization-delocalization transition. We show that the scaling exponents possessed by these key descriptors of localization are quite different from that of a pure AA model or Stark model. Near the critical point of the AA model, inducing Stark field of strength $h$, the localization length $\\zeta$ scales as $\\zeta\\propto h^{-\\nu}$ with $\\nu\\approx0.29$ which is different than both the pure AA model ($\\nu=1$) and Stark model ($\\nu\\approx0.33$). The IPR scales as IPR $\\propto h^{s}$ with $s\\approx0.096$ which is again significantly different than both the pure AA model ($s\\approx0.33$) and Stark model ($s\\approx0.33$). The energy gap, $\\Delta$, scales as $E\\propto h^{\\nu z}$, where $z\\approx2.37$ which is however same as the pure AA model.","sentences":["In this work we investigate the Stark localization near the Aubry-Andr\\'{e} (AA) critical point.","We study system-dependent parameters, such as localization length, inverse participation ratio (IPR), and energy gap between the ground and first excited state, for characterizing the localization-delocalization transition.","We show that the scaling exponents possessed by these key descriptors of localization are quite different from that of a pure AA model or Stark model.","Near the critical point of the AA model, inducing Stark field of strength $h$, the localization length $\\zeta$ scales as $\\zeta\\propto h^{-\\nu}$ with $\\nu\\approx0.29$ which is different than both the pure AA model ($\\nu=1$) and Stark model ($\\nu\\approx0.33$).","The IPR scales as IPR $\\propto h^{s}$ with $s\\approx0.096$ which is again significantly different than both the pure AA model ($s\\approx0.33$) and Stark model ($s\\approx0.33$).","The energy gap, $\\Delta$, scales as $E\\propto h^{\\nu z}$, where $z\\approx2.37$ which is however same as the pure AA model."],"url":"http://arxiv.org/abs/2404.14971v1","category":"quant-ph"}
{"created":"2024-04-23 12:23:54","title":"The symmetric (2+1)-dimensional Lotka-Volterra equation with self-consistent sources","abstract":"The symmetric (2+1)-dimensional Lotka-Volterra equation with self-consistent sources is constructed and solved by employing the source generation procedure, whose solutions are expressed in terms of pfaffians. As special cases of the pfaffian solutions, different types of explicit solutions are obtained, including dromions, soliton solutions and breather solutions.","sentences":["The symmetric (2+1)-dimensional Lotka-Volterra equation with self-consistent sources is constructed and solved by employing the source generation procedure, whose solutions are expressed in terms of pfaffians.","As special cases of the pfaffian solutions, different types of explicit solutions are obtained, including dromions, soliton solutions and breather solutions."],"url":"http://arxiv.org/abs/2404.14969v2","category":"nlin.SI"}
{"created":"2024-04-23 12:20:14","title":"Vision Beyond Boundaries: An Initial Design Space of Domain-specific Large Vision Models in Human-robot Interaction","abstract":"The emergence of Large Vision Models (LVMs) is following in the footsteps of the recent prosperity of Large Language Models (LLMs) in following years. However, there's a noticeable gap in structured research applying LVMs to Human-Robot Interaction (HRI), despite extensive evidence supporting the efficacy of vision models in enhancing interactions between humans and robots. Recognizing the vast and anticipated potential, we introduce an initial design space that incorporates domain-specific LVMs, chosen for their superior performance over normal models. We delve into three primary dimensions: HRI contexts, vision-based tasks, and specific domains. The empirical validation was implemented among 15 experts across six evaluated metrics, showcasing the primary efficacy in relevant decision-making scenarios. We explore the process of ideation and potential application scenarios, envisioning this design space as a foundational guideline for future HRI system design, emphasizing accurate domain alignment and model selection.","sentences":["The emergence of Large Vision Models (LVMs) is following in the footsteps of the recent prosperity of Large Language Models (LLMs) in following years.","However, there's a noticeable gap in structured research applying LVMs to Human-Robot Interaction (HRI), despite extensive evidence supporting the efficacy of vision models in enhancing interactions between humans and robots.","Recognizing the vast and anticipated potential, we introduce an initial design space that incorporates domain-specific LVMs, chosen for their superior performance over normal models.","We delve into three primary dimensions: HRI contexts, vision-based tasks, and specific domains.","The empirical validation was implemented among 15 experts across six evaluated metrics, showcasing the primary efficacy in relevant decision-making scenarios.","We explore the process of ideation and potential application scenarios, envisioning this design space as a foundational guideline for future HRI system design, emphasizing accurate domain alignment and model selection."],"url":"http://arxiv.org/abs/2404.14965v1","category":"cs.HC"}
{"created":"2024-04-23 12:20:09","title":"Elucidating the theoretical underpinnings of surrogate gradient learning in spiking neural networks","abstract":"Training spiking neural networks to approximate complex functions is essential for studying information processing in the brain and neuromorphic computing. Yet, the binary nature of spikes constitutes a challenge for direct gradient-based training. To sidestep this problem, surrogate gradients have proven empirically successful, but their theoretical foundation remains elusive. Here, we investigate the relation of surrogate gradients to two theoretically well-founded approaches. On the one hand, we consider smoothed probabilistic models, which, due to lack of support for automatic differentiation, are impractical for training deep spiking neural networks, yet provide gradients equivalent to surrogate gradients in single neurons. On the other hand, we examine stochastic automatic differentiation, which is compatible with discrete randomness but has never been applied to spiking neural network training. We find that the latter provides the missing theoretical basis for surrogate gradients in stochastic spiking neural networks. We further show that surrogate gradients in deterministic networks correspond to a particular asymptotic case and numerically confirm the effectiveness of surrogate gradients in stochastic multi-layer spiking neural networks. Finally, we illustrate that surrogate gradients are not conservative fields and, thus, not gradients of a surrogate loss. Our work provides the missing theoretical foundation for surrogate gradients and an analytically well-founded solution for end-to-end training of stochastic spiking neural networks.","sentences":["Training spiking neural networks to approximate complex functions is essential for studying information processing in the brain and neuromorphic computing.","Yet, the binary nature of spikes constitutes a challenge for direct gradient-based training.","To sidestep this problem, surrogate gradients have proven empirically successful, but their theoretical foundation remains elusive.","Here, we investigate the relation of surrogate gradients to two theoretically well-founded approaches.","On the one hand, we consider smoothed probabilistic models, which, due to lack of support for automatic differentiation, are impractical for training deep spiking neural networks, yet provide gradients equivalent to surrogate gradients in single neurons.","On the other hand, we examine stochastic automatic differentiation, which is compatible with discrete randomness but has never been applied to spiking neural network training.","We find that the latter provides the missing theoretical basis for surrogate gradients in stochastic spiking neural networks.","We further show that surrogate gradients in deterministic networks correspond to a particular asymptotic case and numerically confirm the effectiveness of surrogate gradients in stochastic multi-layer spiking neural networks.","Finally, we illustrate that surrogate gradients are not conservative fields and, thus, not gradients of a surrogate loss.","Our work provides the missing theoretical foundation for surrogate gradients and an analytically well-founded solution for end-to-end training of stochastic spiking neural networks."],"url":"http://arxiv.org/abs/2404.14964v1","category":"cs.NE"}
{"created":"2024-04-23 12:08:48","title":"Short Regular Girth-8 QC-LDPC Codes From Exponent Matrices with Vertical Symmetry","abstract":"To address the challenge of constructing short girth-8 quasi-cyclic (QC) low-density parity-check (LDPC) codes, a novel construction framework based on vertical symmetry (VS) is proposed. Basic properties of the VS structure are presented. With the aid of these properties, existing explicit constructions for column weights from three to five which can be transformed into the VS structure are sorted out. Then two novel explicit constructions with the VS structure which guarantee short codes are presented for column weights of three and six. Moreover, an efficient search-based method is also proposed to find short codes with the VS structure. Compared with the state-of-the-art benchmarks, both the explicit constructions and the search-based method presented in this paper can provide shorter codes for most cases. Simulation results show that the new shorter codes can perform almost the same as or better than the longer existing counterparts. Thus, the new shorter codes can fit better with the low-latency requirement for modern communication systems.","sentences":["To address the challenge of constructing short girth-8 quasi-cyclic (QC) low-density parity-check (LDPC) codes, a novel construction framework based on vertical symmetry (VS) is proposed.","Basic properties of the VS structure are presented.","With the aid of these properties, existing explicit constructions for column weights from three to five which can be transformed into the VS structure are sorted out.","Then two novel explicit constructions with the VS structure which guarantee short codes are presented for column weights of three and six.","Moreover, an efficient search-based method is also proposed to find short codes with the VS structure.","Compared with the state-of-the-art benchmarks, both the explicit constructions and the search-based method presented in this paper can provide shorter codes for most cases.","Simulation results show that the new shorter codes can perform almost the same as or better than the longer existing counterparts.","Thus, the new shorter codes can fit better with the low-latency requirement for modern communication systems."],"url":"http://arxiv.org/abs/2404.14962v1","category":"cs.IT"}
{"created":"2024-04-23 12:06:40","title":"Cache-Aware Reinforcement Learning in Large-Scale Recommender Systems","abstract":"Modern large-scale recommender systems are built upon computation-intensive infrastructure and usually suffer from a huge difference in traffic between peak and off-peak periods. In peak periods, it is challenging to perform real-time computation for each request due to the limited budget of computational resources. The recommendation with a cache is a solution to this problem, where a user-wise result cache is used to provide recommendations when the recommender system cannot afford a real-time computation. However, the cached recommendations are usually suboptimal compared to real-time computation, and it is challenging to determine the items in the cache for each user. In this paper, we provide a cache-aware reinforcement learning (CARL) method to jointly optimize the recommendation by real-time computation and by the cache. We formulate the problem as a Markov decision process with user states and a cache state, where the cache state represents whether the recommender system performs recommendations by real-time computation or by the cache. The computational load of the recommender system determines the cache state. We perform reinforcement learning based on such a model to improve user engagement over multiple requests. Moreover, we show that the cache will introduce a challenge called critic dependency, which deteriorates the performance of reinforcement learning. To tackle this challenge, we propose an eigenfunction learning (EL) method to learn independent critics for CARL. Experiments show that CARL can significantly improve the users' engagement when considering the result cache. CARL has been fully launched in Kwai app, serving over 100 million users.","sentences":["Modern large-scale recommender systems are built upon computation-intensive infrastructure and usually suffer from a huge difference in traffic between peak and off-peak periods.","In peak periods, it is challenging to perform real-time computation for each request due to the limited budget of computational resources.","The recommendation with a cache is a solution to this problem, where a user-wise result cache is used to provide recommendations when the recommender system cannot afford a real-time computation.","However, the cached recommendations are usually suboptimal compared to real-time computation, and it is challenging to determine the items in the cache for each user.","In this paper, we provide a cache-aware reinforcement learning (CARL) method to jointly optimize the recommendation by real-time computation and by the cache.","We formulate the problem as a Markov decision process with user states and a cache state, where the cache state represents whether the recommender system performs recommendations by real-time computation or by the cache.","The computational load of the recommender system determines the cache state.","We perform reinforcement learning based on such a model to improve user engagement over multiple requests.","Moreover, we show that the cache will introduce a challenge called critic dependency, which deteriorates the performance of reinforcement learning.","To tackle this challenge, we propose an eigenfunction learning (EL) method to learn independent critics for CARL.","Experiments show that CARL can significantly improve the users' engagement when considering the result cache.","CARL has been fully launched in Kwai app, serving over 100 million users."],"url":"http://arxiv.org/abs/2404.14961v1","category":"cs.LG"}
{"created":"2024-04-23 12:03:24","title":"Digital Twin of Industrial Networked Control System based on Value of Information","abstract":"The paper examines a scenario wherein sensors are deployed within an Industrial Networked Control System, aiming to construct a digital twin (DT) model for a remotely operated Autonomous Guided Vehicle (AGV). The DT model, situated on a cloud platform, estimates and predicts the system's state, subsequently formulating the optimal scheduling strategy for execution in the physical world. However, acquiring data crucial for efficient state estimation and control computation poses a significant challenge, primarily due to constraints such as limited network resources, partial observation, and the necessity to maintain a certain confidence level for DT estimation. We propose an algorithm based on Value of Information (VoI), seamlessly integrated with the Extended Kalman Filter to deliver a polynomial-time solution, selecting the most informative subset of sensing agents for data. Additionally, we put forth an alternative solution leveraging a Graph Neural Network to precisely ascertain the AGV's position with a remarkable accuracy of up to 5 cm. Our experimental validation in an industrial robotic laboratory environment yields promising results, underscoring the potential of high-accuracy DT models in practice.","sentences":["The paper examines a scenario wherein sensors are deployed within an Industrial Networked Control System, aiming to construct a digital twin (DT) model for a remotely operated Autonomous Guided Vehicle (AGV).","The DT model, situated on a cloud platform, estimates and predicts the system's state, subsequently formulating the optimal scheduling strategy for execution in the physical world.","However, acquiring data crucial for efficient state estimation and control computation poses a significant challenge, primarily due to constraints such as limited network resources, partial observation, and the necessity to maintain a certain confidence level for DT estimation.","We propose an algorithm based on Value of Information (VoI), seamlessly integrated with the Extended Kalman Filter to deliver a polynomial-time solution, selecting the most informative subset of sensing agents for data.","Additionally, we put forth an alternative solution leveraging a Graph Neural Network to precisely ascertain the AGV's position with a remarkable accuracy of up to 5 cm.","Our experimental validation in an industrial robotic laboratory environment yields promising results, underscoring the potential of high-accuracy DT models in practice."],"url":"http://arxiv.org/abs/2404.14960v1","category":"eess.SP"}
{"created":"2024-04-23 12:01:29","title":"Strongly correlated multi-electron bunches from interaction with quantum light","abstract":"Strongly correlated electron systems are a cornerstone of modern physics, being responsible for groundbreaking phenomena from superconducting magnets to quantum computing. In most cases, correlations in electrons arise exclusively due to Coulomb interactions. In this work, we reveal that free electrons interacting simultaneously with a light field can become highly correlated via mechanisms beyond Coulomb interactions. In the case of two electrons, the resulting Pearson correlation coefficient (PCC) for the joint probability distribution of the output electron energies is enhanced over 13 orders of magnitude compared to that of electrons interacting with the light field in succession (one after another). These highly correlated electrons are the result of momentum and energy exchange between the participating electrons via the external quantum light field. Our findings pave the way to the creation and control of highly correlated free electrons for applications including quantum information and ultra-fast imaging.","sentences":["Strongly correlated electron systems are a cornerstone of modern physics, being responsible for groundbreaking phenomena from superconducting magnets to quantum computing.","In most cases, correlations in electrons arise exclusively due to Coulomb interactions.","In this work, we reveal that free electrons interacting simultaneously with a light field can become highly correlated via mechanisms beyond Coulomb interactions.","In the case of two electrons, the resulting Pearson correlation coefficient (PCC) for the joint probability distribution of the output electron energies is enhanced over 13 orders of magnitude compared to that of electrons interacting with the light field in succession (one after another).","These highly correlated electrons are the result of momentum and energy exchange between the participating electrons via the external quantum light field.","Our findings pave the way to the creation and control of highly correlated free electrons for applications including quantum information and ultra-fast imaging."],"url":"http://arxiv.org/abs/2404.14957v1","category":"quant-ph"}
{"created":"2024-04-23 12:00:20","title":"Traditional to Transformers: A Survey on Current Trends and Future Prospects for Hyperspectral Image Classification","abstract":"Hyperspectral image classification is a challenging task due to the high dimensionality and complex nature of hyperspectral data. In recent years, deep learning techniques have emerged as powerful tools for addressing these challenges. This survey provides a comprehensive overview of the current trends and future prospects in hyperspectral image classification, focusing on the advancements from deep learning models to the emerging use of transformers. We review the key concepts, methodologies, and state-of-the-art approaches in deep learning for hyperspectral image classification. Additionally, we discuss the potential of transformer-based models in this field and highlight the advantages and challenges associated with these approaches. Comprehensive experimental results have been undertaken using three Hyperspectral datasets to verify the efficacy of various conventional deep-learning models and Transformers. Finally, we outline future research directions and potential applications that can further enhance the accuracy and efficiency of hyperspectral image classification.   The Source code is available at https://github.com/mahmad00/Conventional-to-Transformer-for-Hyperspectral-Image-Classification-Survey-2024.","sentences":["Hyperspectral image classification is a challenging task due to the high dimensionality and complex nature of hyperspectral data.","In recent years, deep learning techniques have emerged as powerful tools for addressing these challenges.","This survey provides a comprehensive overview of the current trends and future prospects in hyperspectral image classification, focusing on the advancements from deep learning models to the emerging use of transformers.","We review the key concepts, methodologies, and state-of-the-art approaches in deep learning for hyperspectral image classification.","Additionally, we discuss the potential of transformer-based models in this field and highlight the advantages and challenges associated with these approaches.","Comprehensive experimental results have been undertaken using three Hyperspectral datasets to verify the efficacy of various conventional deep-learning models and Transformers.","Finally, we outline future research directions and potential applications that can further enhance the accuracy and efficiency of hyperspectral image classification.   ","The Source code is available at https://github.com/mahmad00/Conventional-to-Transformer-for-Hyperspectral-Image-Classification-Survey-2024."],"url":"http://arxiv.org/abs/2404.14955v1","category":"cs.CV"}
{"created":"2024-04-23 11:55:24","title":"Multi-Objective Deep Reinforcement Learning for 5G Base Station Placement to Support Localisation for Future Sustainable Traffic","abstract":"Millimeter-wave (mmWave) is a key enabler for next-generation transportation systems. However, in an urban city scenario, mmWave is highly susceptible to blockages and shadowing. Therefore, base station (BS) placement is a crucial task in the infrastructure design where coverage requirements need to be met while simultaneously supporting localisation. This work assumes a pre-deployed BS and another BS is required to be added to support both localisation accuracy and coverage rate in an urban city scenario. To solve this complex multi-objective optimisation problem, we utilise deep reinforcement learning (DRL). Concretely, this work proposes: 1) a three-layered grid for state representation as the input of the DRL, which enables it to adapt to the changes in the wireless environment represented by changing the position of the pre-deployed BS, and 2) the design of a suitable reward function for the DRL agent to solve the multi-objective problem. Numerical analysis shows that the proposed deep Q-network (DQN) model can learn/adapt from the complex radio environment represented by the terrain map and provides the same/similar solution to the exhaustive search, which is used as a benchmark. In addition, we show that an exclusive optimisation of coverage rate does not result in improved localisation accuracy, and thus there is a trade-off between the two solutions.","sentences":["Millimeter-wave (mmWave) is a key enabler for next-generation transportation systems.","However, in an urban city scenario, mmWave is highly susceptible to blockages and shadowing.","Therefore, base station (BS) placement is a crucial task in the infrastructure design where coverage requirements need to be met while simultaneously supporting localisation.","This work assumes a pre-deployed BS and another BS is required to be added to support both localisation accuracy and coverage rate in an urban city scenario.","To solve this complex multi-objective optimisation problem, we utilise deep reinforcement learning (DRL).","Concretely, this work proposes: 1) a three-layered grid for state representation as the input of the DRL, which enables it to adapt to the changes in the wireless environment represented by changing the position of the pre-deployed BS, and 2) the design of a suitable reward function for the DRL agent to solve the multi-objective problem.","Numerical analysis shows that the proposed deep Q-network (DQN) model can learn/adapt from the complex radio environment represented by the terrain map and provides the same/similar solution to the exhaustive search, which is used as a benchmark.","In addition, we show that an exclusive optimisation of coverage rate does not result in improved localisation accuracy, and thus there is a trade-off between the two solutions."],"url":"http://arxiv.org/abs/2404.14954v1","category":"eess.SP"}
{"created":"2024-04-23 11:53:51","title":"Streamlining the Image Stitching Pipeline: Integrating Fusion and Rectangling into a Unified Model","abstract":"Learning-based image stitching techniques typically involve three distinct stages: registration, fusion, and rectangling. These stages are often performed sequentially, each trained independently, leading to potential cascading error propagation and complex parameter tuning challenges. In rethinking the mathematical modeling of the fusion and rectangling stages, we discovered that these processes can be effectively combined into a single, variety-intensity inpainting problem. Therefore, we propose the Simple and Robust Stitcher (SRStitcher), an efficient training-free image stitching method that merges the fusion and rectangling stages into a unified model. By employing the weighted mask and large-scale generative model, SRStitcher can solve the fusion and rectangling problems in a single inference, without additional training or fine-tuning of other models. Our method not only simplifies the stitching pipeline but also enhances fault tolerance towards misregistration errors. Extensive experiments demonstrate that SRStitcher outperforms state-of-the-art (SOTA) methods in both quantitative assessments and qualitative evaluations. The code is released at https://github.com/yayoyo66/SRStitcher","sentences":["Learning-based image stitching techniques typically involve three distinct stages: registration, fusion, and rectangling.","These stages are often performed sequentially, each trained independently, leading to potential cascading error propagation and complex parameter tuning challenges.","In rethinking the mathematical modeling of the fusion and rectangling stages, we discovered that these processes can be effectively combined into a single, variety-intensity inpainting problem.","Therefore, we propose the Simple and Robust Stitcher (SRStitcher), an efficient training-free image stitching method that merges the fusion and rectangling stages into a unified model.","By employing the weighted mask and large-scale generative model, SRStitcher can solve the fusion and rectangling problems in a single inference, without additional training or fine-tuning of other models.","Our method not only simplifies the stitching pipeline but also enhances fault tolerance towards misregistration errors.","Extensive experiments demonstrate that SRStitcher outperforms state-of-the-art (SOTA) methods in both quantitative assessments and qualitative evaluations.","The code is released at https://github.com/yayoyo66/SRStitcher"],"url":"http://arxiv.org/abs/2404.14951v1","category":"cs.CV"}
{"created":"2024-04-23 11:42:02","title":"Compact linear combinations of composition operators on Hardy spaces","abstract":"Let $\\varphi_j$, $j=1,2, \\dots, N$, be holomorphic self-maps of the unit disk $\\mathbb{D}$ of $\\mathbb{C}$. We prove that the compactness of a linear combination of the composition operators $C_{\\varphi_j}: f\\mapsto f\\circ\\varphi_j$ on the Hardy space $H^p(\\mathbb{D})$ does not depend on $p$ for $0<p<\\infty$. This answers a conjecture of Choe et al. about the compact differences $C_{\\varphi_1} - C_{\\varphi_2}$ on $H^p(\\mathbb{D})$, $0<p<\\infty$.","sentences":["Let $\\varphi_j$, $j=1,2, \\dots, N$, be holomorphic self-maps of the unit disk $\\mathbb{D}$ of $\\mathbb{C}$. We prove that the compactness of a linear combination of the composition operators $C_{\\varphi_j}: f\\mapsto f\\circ\\varphi_j$ on the Hardy space $H^p(\\mathbb{D})$ does not depend on $p$ for $0<p<\\infty$. This answers a conjecture of Choe et al. about the compact differences $C_{\\varphi_1} - C_{\\varphi_2}$ on $H^p(\\mathbb{D})$, $0<p<\\infty$."],"url":"http://arxiv.org/abs/2404.14947v1","category":"math.CV"}
{"created":"2024-04-23 11:40:30","title":"Does It Make Sense to Explain a Black Box With Another Black Box?","abstract":"Although counterfactual explanations are a popular approach to explain ML black-box classifiers, they are less widespread in NLP. Most methods find those explanations by iteratively perturbing the target document until it is classified differently by the black box. We identify two main families of counterfactual explanation methods in the literature, namely, (a) \\emph{transparent} methods that perturb the target by adding, removing, or replacing words, and (b) \\emph{opaque} approaches that project the target document into a latent, non-interpretable space where the perturbation is carried out subsequently. This article offers a comparative study of the performance of these two families of methods on three classical NLP tasks. Our empirical evidence shows that opaque approaches can be an overkill for downstream applications such as fake news detection or sentiment analysis since they add an additional level of complexity with no significant performance gain. These observations motivate our discussion, which raises the question of whether it makes sense to explain a black box using another black box.","sentences":["Although counterfactual explanations are a popular approach to explain ML black-box classifiers, they are less widespread in NLP.","Most methods find those explanations by iteratively perturbing the target document until it is classified differently by the black box.","We identify two main families of counterfactual explanation methods in the literature, namely, (a) \\emph{transparent} methods that perturb the target by adding, removing, or replacing words, and (b) \\emph{opaque} approaches that project the target document into a latent, non-interpretable space where the perturbation is carried out subsequently.","This article offers a comparative study of the performance of these two families of methods on three classical NLP tasks.","Our empirical evidence shows that opaque approaches can be an overkill for downstream applications such as fake news detection or sentiment analysis since they add an additional level of complexity with no significant performance gain.","These observations motivate our discussion, which raises the question of whether it makes sense to explain a black box using another black box."],"url":"http://arxiv.org/abs/2404.14943v1","category":"cs.CL"}
{"created":"2024-04-23 17:55:07","title":"TalkingGaussian: Structure-Persistent 3D Talking Head Synthesis via Gaussian Splatting","abstract":"Radiance fields have demonstrated impressive performance in synthesizing lifelike 3D talking heads. However, due to the difficulty in fitting steep appearance changes, the prevailing paradigm that presents facial motions by directly modifying point appearance may lead to distortions in dynamic regions. To tackle this challenge, we introduce TalkingGaussian, a deformation-based radiance fields framework for high-fidelity talking head synthesis. Leveraging the point-based Gaussian Splatting, facial motions can be represented in our method by applying smooth and continuous deformations to persistent Gaussian primitives, without requiring to learn the difficult appearance change like previous methods. Due to this simplification, precise facial motions can be synthesized while keeping a highly intact facial feature. Under such a deformation paradigm, we further identify a face-mouth motion inconsistency that would affect the learning of detailed speaking motions. To address this conflict, we decompose the model into two branches separately for the face and inside mouth areas, therefore simplifying the learning tasks to help reconstruct more accurate motion and structure of the mouth region. Extensive experiments demonstrate that our method renders high-quality lip-synchronized talking head videos, with better facial fidelity and higher efficiency compared with previous methods.","sentences":["Radiance fields have demonstrated impressive performance in synthesizing lifelike 3D talking heads.","However, due to the difficulty in fitting steep appearance changes, the prevailing paradigm that presents facial motions by directly modifying point appearance may lead to distortions in dynamic regions.","To tackle this challenge, we introduce TalkingGaussian, a deformation-based radiance fields framework for high-fidelity talking head synthesis.","Leveraging the point-based Gaussian Splatting, facial motions can be represented in our method by applying smooth and continuous deformations to persistent Gaussian primitives, without requiring to learn the difficult appearance change like previous methods.","Due to this simplification, precise facial motions can be synthesized while keeping a highly intact facial feature.","Under such a deformation paradigm, we further identify a face-mouth motion inconsistency that would affect the learning of detailed speaking motions.","To address this conflict, we decompose the model into two branches separately for the face and inside mouth areas, therefore simplifying the learning tasks to help reconstruct more accurate motion and structure of the mouth region.","Extensive experiments demonstrate that our method renders high-quality lip-synchronized talking head videos, with better facial fidelity and higher efficiency compared with previous methods."],"url":"http://arxiv.org/abs/2404.15264v1","category":"cs.CV"}
{"created":"2024-04-23 17:50:52","title":"All You Need is Resistance: On the Equivalence of Effective Resistance and Certain Optimal Transport Problems on Graphs","abstract":"The fields of effective resistance and optimal transport on graphs are filled with rich connections to combinatorics, geometry, machine learning, and beyond. In this article we put forth a bold claim: that the two fields should be understood as one and the same, up to a choice of $p$. We make this claim precise by introducing the parameterized family of $p$-Beckmann distances for probability measures on graphs and relate them sharply to certain Wasserstein distances. Then, we break open a suite of results including explicit connections to optimal stopping times and random walks on graphs, graph Sobolev spaces, and a Benamou-Brenier type formula for $2$-Beckmann distance. We further explore empirical implications in the world of unsupervised learning for graph data and propose further study of the usage of these metrics where Wasserstein distance may produce computational bottlenecks.","sentences":["The fields of effective resistance and optimal transport on graphs are filled with rich connections to combinatorics, geometry, machine learning, and beyond.","In this article we put forth a bold claim: that the two fields should be understood as one and the same, up to a choice of $p$. We make this claim precise by introducing the parameterized family of $p$-Beckmann distances for probability measures on graphs and relate them sharply to certain Wasserstein distances.","Then, we break open a suite of results including explicit connections to optimal stopping times and random walks on graphs, graph Sobolev spaces, and a Benamou-Brenier type formula for $2$-Beckmann distance.","We further explore empirical implications in the world of unsupervised learning for graph data and propose further study of the usage of these metrics where Wasserstein distance may produce computational bottlenecks."],"url":"http://arxiv.org/abs/2404.15261v1","category":"math.OC"}
{"created":"2024-04-23 17:39:06","title":"Source-free Domain Adaptation for Video Object Detection Under Adverse Image Conditions","abstract":"When deploying pre-trained video object detectors in real-world scenarios, the domain gap between training and testing data caused by adverse image conditions often leads to performance degradation. Addressing this issue becomes particularly challenging when only the pre-trained model and degraded videos are available. Although various source-free domain adaptation (SFDA) methods have been proposed for single-frame object detectors, SFDA for video object detection (VOD) remains unexplored. Moreover, most unsupervised domain adaptation works for object detection rely on two-stage detectors, while SFDA for one-stage detectors, which are more vulnerable to fine-tuning, is not well addressed in the literature. In this paper, we propose Spatial-Temporal Alternate Refinement with Mean Teacher (STAR-MT), a simple yet effective SFDA method for VOD. Specifically, we aim to improve the performance of the one-stage VOD method, YOLOV, under adverse image conditions, including noise, air turbulence, and haze. Extensive experiments on the ImageNetVOD dataset and its degraded versions demonstrate that our method consistently improves video object detection performance in challenging imaging conditions, showcasing its potential for real-world applications.","sentences":["When deploying pre-trained video object detectors in real-world scenarios, the domain gap between training and testing data caused by adverse image conditions often leads to performance degradation.","Addressing this issue becomes particularly challenging when only the pre-trained model and degraded videos are available.","Although various source-free domain adaptation (SFDA) methods have been proposed for single-frame object detectors, SFDA for video object detection (VOD) remains unexplored.","Moreover, most unsupervised domain adaptation works for object detection rely on two-stage detectors, while SFDA for one-stage detectors, which are more vulnerable to fine-tuning, is not well addressed in the literature.","In this paper, we propose Spatial-Temporal Alternate Refinement with Mean Teacher (STAR-MT), a simple yet effective SFDA method for VOD.","Specifically, we aim to improve the performance of the one-stage VOD method, YOLOV, under adverse image conditions, including noise, air turbulence, and haze.","Extensive experiments on the ImageNetVOD dataset and its degraded versions demonstrate that our method consistently improves video object detection performance in challenging imaging conditions, showcasing its potential for real-world applications."],"url":"http://arxiv.org/abs/2404.15252v1","category":"cs.CV"}
{"created":"2024-04-23 17:18:10","title":"Symmetric Ideals and Invariant Hilbert Schemes","abstract":"A symmetric ideal is an ideal in a polynomial ring which is stable under all permutations of the variables. In this paper we initiate a global study of zero-dimensional symmetric ideals. By this we mean a geometric study of the invariant Hilbert schemes $\\mathrm{Hilb}_{\\rho}^{S_n}(\\mathbb{C}^n)$ parametrizing symmetric subschemes of $\\mathbb{C}^n$ whose coordinate rings, as $S_n$-modules, are isomorphic to a given representation $\\rho$. In the case that $\\rho = M^\\lambda$ is a permutation module corresponding to certain special types of partitions $\\lambda$ of $n$, we prove that $\\mathrm{Hilb}_{\\rho}^{S_n}(\\mathbb{C}^n)$ is irreducible or even smooth. We also prove irreducibility whenever $\\dim \\rho \\leq 2n$ and the invariant Hilbert scheme is non-empty. In this same range, we classify all homogeneous symmetric ideals and decide which of these define singular points of $\\mathrm{Hilb}_{\\rho}^{S_n}(\\mathbb{C}^n)$. A central tool is the combinatorial theory of higher Specht polynomials.","sentences":["A symmetric ideal is an ideal in a polynomial ring which is stable under all permutations of the variables.","In this paper we initiate a global study of zero-dimensional symmetric ideals.","By this we mean a geometric study of the invariant Hilbert schemes $\\mathrm{Hilb}_{\\rho}^{S_n}(\\mathbb{C}^n)$ parametrizing symmetric subschemes of $\\mathbb{C}^n$ whose coordinate rings, as $S_n$-modules, are isomorphic to a given representation $\\rho$. In the case that $\\rho = M^\\lambda$ is a permutation module corresponding to certain special types of partitions $\\lambda$ of $n$, we prove that $\\mathrm{Hilb}_{\\rho}^{S_n}(\\mathbb{C}^n)$ is irreducible or even smooth.","We also prove irreducibility whenever $\\dim \\rho \\leq 2n$ and the invariant Hilbert scheme is non-empty.","In this same range, we classify all homogeneous symmetric ideals and decide which of these define singular points of $\\mathrm{Hilb}_{\\rho}^{S_n}(\\mathbb{C}^n)$. A central tool is the combinatorial theory of higher Specht polynomials."],"url":"http://arxiv.org/abs/2404.15240v1","category":"math.AG"}
{"created":"2024-04-23 16:55:57","title":"tsbootstrap: Enhancing Time Series Analysis with Advanced Bootstrapping Techniques","abstract":"In time series analysis, traditional bootstrapping methods often fall short due to their assumption of data independence, a condition rarely met in time-dependent data. This paper introduces tsbootstrap, a python package designed specifically to address this challenge. It offers a comprehensive suite of bootstrapping techniques, including Block, Residual, and advanced methods like Markov and Sieve Bootstraps, each tailored to respect the temporal dependencies in time series data. This framework not only enhances the accuracy of uncertainty estimation in time series analysis but also integrates seamlessly with the existing python data science ecosystem, making it an invaluable asset for researchers and practitioners in various fields.","sentences":["In time series analysis, traditional bootstrapping methods often fall short due to their assumption of data independence, a condition rarely met in time-dependent data.","This paper introduces tsbootstrap, a python package designed specifically to address this challenge.","It offers a comprehensive suite of bootstrapping techniques, including Block, Residual, and advanced methods like Markov and Sieve Bootstraps, each tailored to respect the temporal dependencies in time series data.","This framework not only enhances the accuracy of uncertainty estimation in time series analysis but also integrates seamlessly with the existing python data science ecosystem, making it an invaluable asset for researchers and practitioners in various fields."],"url":"http://arxiv.org/abs/2404.15227v1","category":"stat.AP"}
{"created":"2024-04-23 16:54:26","title":"Effective dynamics of qubit networks via phase-covariant quantum ensembles","abstract":"We study ensembles of phase-covariant channels. We show that such ensembles arise naturally from familiar spin-chain models (e.g., XXZ) with a special class of initial states, and that the disorder-averaged map of disordered spin chains is phase-covariant under a weak symmetry constraint on the distribution. We use those examples to motivate a broader class of phase-covariant ensembles, which include both unital and non-unital channels. We demonstrate the physical properties captured by the late-time limit of the average map over the ensemble.","sentences":["We study ensembles of phase-covariant channels.","We show that such ensembles arise naturally from familiar spin-chain models (e.g., XXZ) with a special class of initial states, and that the disorder-averaged map of disordered spin chains is phase-covariant under a weak symmetry constraint on the distribution.","We use those examples to motivate a broader class of phase-covariant ensembles, which include both unital and non-unital channels.","We demonstrate the physical properties captured by the late-time limit of the average map over the ensemble."],"url":"http://arxiv.org/abs/2404.15223v1","category":"quant-ph"}
{"created":"2024-04-23 16:39:03","title":"Does Instruction Tuning Make LLMs More Consistent?","abstract":"The purpose of instruction tuning is enabling zero-shot performance, but instruction tuning has also been shown to improve chain-of-thought reasoning and value alignment (Si et al., 2023). Here we consider the impact on $\\textit{consistency}$, i.e., the sensitivity of language models to small perturbations in the input. We compare 10 instruction-tuned LLaMA models to the original LLaMA-7b model and show that almost across-the-board they become more consistent, both in terms of their representations and their predictions in zero-shot and downstream tasks. We explain these improvements through mechanistic analyses of factual recall.","sentences":["The purpose of instruction tuning is enabling zero-shot performance, but instruction tuning has also been shown to improve chain-of-thought reasoning and value alignment (Si et al., 2023).","Here we consider the impact on $\\textit{consistency}$, i.e., the sensitivity of language models to small perturbations in the input.","We compare 10 instruction-tuned LLaMA models to the original LLaMA-7b model and show that almost across-the-board they become more consistent, both in terms of their representations and their predictions in zero-shot and downstream tasks.","We explain these improvements through mechanistic analyses of factual recall."],"url":"http://arxiv.org/abs/2404.15206v1","category":"cs.CL"}
{"created":"2024-04-23 16:14:57","title":"Using analytic models to describe effective PDFs","abstract":"Parton distribution functions (PDFs) play a pivotal role in hadron collider phenomenology. They are non-perturbative quantities extracted from fits to available data, and their scale dependence is dictated by the DGLAP evolution equations. In this article, we discuss machine learning (ML) strategies to efficiently compute PDFs by bypassing the scale evolution. Analytical approximations to the PDFs as a function of $x$ and $Q^2$, including up to next-to-leading order (NLO) QCD effects, are obtained. The methodology is tested by reproducing the HERAPDF2.0 set and implementing the analytical expressions in benchmarking codes. It is found that the computational cost is reduced while the precision of the simulations stays well under control.","sentences":["Parton distribution functions (PDFs) play a pivotal role in hadron collider phenomenology.","They are non-perturbative quantities extracted from fits to available data, and their scale dependence is dictated by the DGLAP evolution equations.","In this article, we discuss machine learning (ML) strategies to efficiently compute PDFs by bypassing the scale evolution.","Analytical approximations to the PDFs as a function of $x$ and $Q^2$, including up to next-to-leading order (NLO) QCD effects, are obtained.","The methodology is tested by reproducing the HERAPDF2.0 set and implementing the analytical expressions in benchmarking codes.","It is found that the computational cost is reduced while the precision of the simulations stays well under control."],"url":"http://arxiv.org/abs/2404.15175v1","category":"hep-ph"}
{"created":"2024-04-23 16:01:33","title":"Combating Missing Modalities in Egocentric Videos at Test Time","abstract":"Understanding videos that contain multiple modalities is crucial, especially in egocentric videos, where combining various sensory inputs significantly improves tasks like action recognition and moment localization. However, real-world applications often face challenges with incomplete modalities due to privacy concerns, efficiency needs, or hardware issues. Current methods, while effective, often necessitate retraining the model entirely to handle missing modalities, making them computationally intensive, particularly with large training datasets. In this study, we propose a novel approach to address this issue at test time without requiring retraining. We frame the problem as a test-time adaptation task, where the model adjusts to the available unlabeled data at test time. Our method, MiDl~(Mutual information with self-Distillation), encourages the model to be insensitive to the specific modality source present during testing by minimizing the mutual information between the prediction and the available modality. Additionally, we incorporate self-distillation to maintain the model's original performance when both modalities are available. MiDl represents the first self-supervised, online solution for handling missing modalities exclusively at test time. Through experiments with various pretrained models and datasets, MiDl demonstrates substantial performance improvement without the need for retraining.","sentences":["Understanding videos that contain multiple modalities is crucial, especially in egocentric videos, where combining various sensory inputs significantly improves tasks like action recognition and moment localization.","However, real-world applications often face challenges with incomplete modalities due to privacy concerns, efficiency needs, or hardware issues.","Current methods, while effective, often necessitate retraining the model entirely to handle missing modalities, making them computationally intensive, particularly with large training datasets.","In this study, we propose a novel approach to address this issue at test time without requiring retraining.","We frame the problem as a test-time adaptation task, where the model adjusts to the available unlabeled data at test time.","Our method, MiDl~(Mutual information with self-Distillation), encourages the model to be insensitive to the specific modality source present during testing by minimizing the mutual information between the prediction and the available modality.","Additionally, we incorporate self-distillation to maintain the model's original performance when both modalities are available.","MiDl represents the first self-supervised, online solution for handling missing modalities exclusively at test time.","Through experiments with various pretrained models and datasets, MiDl demonstrates substantial performance improvement without the need for retraining."],"url":"http://arxiv.org/abs/2404.15161v1","category":"cs.CV"}
{"created":"2024-04-23 15:11:48","title":"Momentum deficit and wake-added turbulence kinetic energy budgets in the stratified atmospheric boundary layer","abstract":"To achieve decarbonization targets, wind turbines are growing in hub height, rotor diameter, and are being deployed in new locations with diverse atmospheric conditions not previously seen, such as offshore. Physics-based analytical wake models commonly used for design and control of wind farms simplify atmospheric boundary layer (ABL) and wake physics to achieve computational efficiency. This is done primarily through a simplified model form that neglects certain flow processes and through parameterization of ABL and wake turbulence through a wake spreading rate. In this study, we analyze the physical mechanisms that govern momentum and turbulence within a wind turbine wake in the stratified ABL. We use large eddy simulation and analysis of the streamwise momentum deficit and wake-added turbulence kinetic energy (TKE) budgets to study wind turbine wakes under neutral and stable conditions. To parse the wake from the turbulent, incident ABL flow, we decompose the flow into the base ABL flow and the deficit flow produced by the presence of a turbine. We analyze the decomposed flow field budgets to study the effects of changing stability on the streamwise momentum deficit and wake-added TKE. The results demonstrate that stability changes the importance of physical mechanisms for both quantities primarily through the nonlinear interactions of the base and deficit flows, with the stable case most affected by higher shear in the base flow and the neutral case by higher base flow TKE. Buoyancy forcing terms in the momentum deficit and wake-added TKE budgets are relatively less important compared to the aforementioned effects. While total TKE is higher in wakes in neutral ABL flows, the wake-added TKE is higher downwind of turbines in stable ABL conditions. The dependence of wake-added TKE on ABL stability is not represented in existing empirical models widely used for mean wake flow modeling.","sentences":["To achieve decarbonization targets, wind turbines are growing in hub height, rotor diameter, and are being deployed in new locations with diverse atmospheric conditions not previously seen, such as offshore.","Physics-based analytical wake models commonly used for design and control of wind farms simplify atmospheric boundary layer (ABL) and wake physics to achieve computational efficiency.","This is done primarily through a simplified model form that neglects certain flow processes and through parameterization of ABL and wake turbulence through a wake spreading rate.","In this study, we analyze the physical mechanisms that govern momentum and turbulence within a wind turbine wake in the stratified ABL.","We use large eddy simulation and analysis of the streamwise momentum deficit and wake-added turbulence kinetic energy (TKE) budgets to study wind turbine wakes under neutral and stable conditions.","To parse the wake from the turbulent, incident ABL flow, we decompose the flow into the base ABL flow and the deficit flow produced by the presence of a turbine.","We analyze the decomposed flow field budgets to study the effects of changing stability on the streamwise momentum deficit and wake-added TKE.","The results demonstrate that stability changes the importance of physical mechanisms for both quantities primarily through the nonlinear interactions of the base and deficit flows, with the stable case most affected by higher shear in the base flow and the neutral case by higher base flow TKE.","Buoyancy forcing terms in the momentum deficit and wake-added TKE budgets are relatively less important compared to the aforementioned effects.","While total TKE is higher in wakes in neutral ABL flows, the wake-added TKE is higher downwind of turbines in stable ABL conditions.","The dependence of wake-added TKE on ABL stability is not represented in existing empirical models widely used for mean wake flow modeling."],"url":"http://arxiv.org/abs/2404.15117v1","category":"physics.flu-dyn"}
{"created":"2024-04-23 15:10:00","title":"Principal Component Analysis and biplots. A Back-to-Basics Comparison of Implementations","abstract":"Principal Component Analysis and biplots are so well-established and readily implemented that it is just too tempting to give for granted their internal workings. In this note I get back to basics in comparing how PCA and biplots are implemented in base-R and contributed R packages, leveraging an implementation-agnostic understanding of the computational structure of each technique. I do so with a view to illustrating discrepancies that users might find elusive, as these arise from seemingly innocuous computational choices made under the hood. The proposed evaluation grid elevates aspects that are usually disregarded, including relationships that should hold if the computational rationale underpinning each technique is followed correctly. Strikingly, what is expected from these equivalences rarely follows without caveats from the output of specific implementations alone.","sentences":["Principal Component Analysis and biplots are so well-established and readily implemented that it is just too tempting to give for granted their internal workings.","In this note I get back to basics in comparing how PCA and biplots are implemented in base-R and contributed R packages, leveraging an implementation-agnostic understanding of the computational structure of each technique.","I do so with a view to illustrating discrepancies that users might find elusive, as these arise from seemingly innocuous computational choices made under the hood.","The proposed evaluation grid elevates aspects that are usually disregarded, including relationships that should hold if the computational rationale underpinning each technique is followed correctly.","Strikingly, what is expected from these equivalences rarely follows without caveats from the output of specific implementations alone."],"url":"http://arxiv.org/abs/2404.15115v1","category":"stat.ME"}
{"created":"2024-04-23 15:05:43","title":"Resummation of combined QCD-electroweak effects in Drell Yan lepton-pair production","abstract":"We consider neutral- and charged-current Drell Yan lepton-pair production at hadron colliders, and include dominant classes of electroweak and mixed QCD-electroweak corrections to all orders in perturbation theory. The accurate description of these physical effects is vital for a precise determination of fundamental Standard Model parameters, such as the $W$-boson mass and the electroweak mixing angle, as well as for a solid assessment of the associated theoretical uncertainties. Our state-of-the-art resummation reaches next-to-leading-logarithmic accuracy in both the electroweak and the mixed QCD-electroweak perturbative expansions, including constant terms at first order beyond Born level in both couplings, i.e. at order $\\alpha$ and $\\alpha_s \\alpha$. These effects are incorporated on top of QCD predictions at next-to-next-to-next-to-leading-logarithmic accuracy, which include constant terms at third order in the strong coupling. Our results retain, for the first time at this accuracy, full dependence on the kinematics of the final-state leptons, thereby enabling a realistic comparison with experimental analyses at the differential level in presence of fiducial cuts. We present a phenomenological analysis of the impact of electroweak corrections in relevant observables at the LHC. We find visible shape distortions in resummation-dominated kinematical regions with respect to pure-QCD predictions, highlighting the importance of a complete description, not limited to QCD, for precision Drell Yan physics.","sentences":["We consider neutral- and charged-current Drell Yan lepton-pair production at hadron colliders, and include dominant classes of electroweak and mixed QCD-electroweak corrections to all orders in perturbation theory.","The accurate description of these physical effects is vital for a precise determination of fundamental Standard Model parameters, such as the $W$-boson mass and the electroweak mixing angle, as well as for a solid assessment of the associated theoretical uncertainties.","Our state-of-the-art resummation reaches next-to-leading-logarithmic accuracy in both the electroweak and the mixed QCD-electroweak perturbative expansions, including constant terms at first order beyond Born level in both couplings, i.e. at order $\\alpha$ and $\\alpha_s \\alpha$.","These effects are incorporated on top of QCD predictions at next-to-next-to-next-to-leading-logarithmic accuracy, which include constant terms at third order in the strong coupling.","Our results retain, for the first time at this accuracy, full dependence on the kinematics of the final-state leptons, thereby enabling a realistic comparison with experimental analyses at the differential level in presence of fiducial cuts.","We present a phenomenological analysis of the impact of electroweak corrections in relevant observables at the LHC.","We find visible shape distortions in resummation-dominated kinematical regions with respect to pure-QCD predictions, highlighting the importance of a complete description, not limited to QCD, for precision Drell Yan physics."],"url":"http://arxiv.org/abs/2404.15112v1","category":"hep-ph"}
{"created":"2024-04-23 14:49:55","title":"Using ARIMA to Predict the Expansion of Subscriber Data Consumption","abstract":"This study discusses how insights retrieved from subscriber data can impact decision-making in telecommunications, focusing on predictive modeling using machine learning techniques such as the ARIMA model. The study explores time series forecasting to predict subscriber usage trends, evaluating the ARIMA model's performance using various metrics. It also compares ARIMA with Convolutional Neural Network (CNN) models, highlighting ARIMA's superiority in accuracy and execution speed. The study suggests future directions for research, including exploring additional forecasting models and considering other factors affecting subscriber data usage.","sentences":["This study discusses how insights retrieved from subscriber data can impact decision-making in telecommunications, focusing on predictive modeling using machine learning techniques such as the ARIMA model.","The study explores time series forecasting to predict subscriber usage trends, evaluating the ARIMA model's performance using various metrics.","It also compares ARIMA with Convolutional Neural Network (CNN) models, highlighting ARIMA's superiority in accuracy and execution speed.","The study suggests future directions for research, including exploring additional forecasting models and considering other factors affecting subscriber data usage."],"url":"http://arxiv.org/abs/2404.15095v1","category":"cs.LG"}
{"created":"2024-04-23 14:24:33","title":"An energy efficient quantum-enhanced machine","abstract":"Quantum friction, a quantum analog of classical friction, reduces the performance of quantum machines, such as heat engines, and makes them less energy efficient. We here report the experimental realization of an energy efficient quantum engine coupled to a quantum battery that stores the produced work, using a single ion in a linear Paul trap. We first establish the quantum nature of the device by observing nonclassical work oscillations with the number of cycles as verified by energy measurements of the battery. We moreover successfully apply shortcut-to-adiabaticity techniques to suppress quantum friction and improve work production. While the average energy cost of the shortcut protocol is only about $3\\%$, the work output is enhanced by up to approximately 33$\\%$, making the machine significantly more energy efficient. In addition, we show that the quantum engine consistently outperforms its classical counterpart in this regime. Our results pave the way for energy efficient machines with quantum-enhanced performance.","sentences":["Quantum friction, a quantum analog of classical friction, reduces the performance of quantum machines, such as heat engines, and makes them less energy efficient.","We here report the experimental realization of an energy efficient quantum engine coupled to a quantum battery that stores the produced work, using a single ion in a linear Paul trap.","We first establish the quantum nature of the device by observing nonclassical work oscillations with the number of cycles as verified by energy measurements of the battery.","We moreover successfully apply shortcut-to-adiabaticity techniques to suppress quantum friction and improve work production.","While the average energy cost of the shortcut protocol is only about $3\\%$, the work output is enhanced by up to approximately 33$\\%$, making the machine significantly more energy efficient.","In addition, we show that the quantum engine consistently outperforms its classical counterpart in this regime.","Our results pave the way for energy efficient machines with quantum-enhanced performance."],"url":"http://arxiv.org/abs/2404.15075v1","category":"quant-ph"}
{"created":"2024-04-23 13:39:25","title":"Near-Universally-Optimal Differentially Private Minimum Spanning Trees","abstract":"Devising mechanisms with good beyond-worst-case input-dependent performance has been an important focus of differential privacy, with techniques such as smooth sensitivity, propose-test-release, or inverse sensitivity mechanism being developed to achieve this goal. This makes it very natural to use the notion of universal optimality in differential privacy. Universal optimality is a strong instance-specific optimality guarantee for problems on weighted graphs, which roughly states that for any fixed underlying (unweighted) graph, the algorithm is optimal in the worst-case sense, with respect to the possible setting of the edge weights.   In this paper, we give the first such result in differential privacy. Namely, we prove that a simple differentially private mechanism for approximately releasing the minimum spanning tree is near-optimal in the sense of universal optimality for the $\\ell_1$ neighbor relation. Previously, it was only known that this mechanism is nearly optimal in the worst case. We then focus on the $\\ell_\\infty$ neighbor relation, for which the described mechanism is not optimal. We show that one may implement the exponential mechanism for MST in polynomial time, and that this results in universal near-optimality for both the $\\ell_1$ and the $\\ell_\\infty$ neighbor relations.","sentences":["Devising mechanisms with good beyond-worst-case input-dependent performance has been an important focus of differential privacy, with techniques such as smooth sensitivity, propose-test-release, or inverse sensitivity mechanism being developed to achieve this goal.","This makes it very natural to use the notion of universal optimality in differential privacy.","Universal optimality is a strong instance-specific optimality guarantee for problems on weighted graphs, which roughly states that for any fixed underlying (unweighted) graph, the algorithm is optimal in the worst-case sense, with respect to the possible setting of the edge weights.   ","In this paper, we give the first such result in differential privacy.","Namely, we prove that a simple differentially private mechanism for approximately releasing the minimum spanning tree is near-optimal in the sense of universal optimality for the $\\ell_1$ neighbor relation.","Previously, it was only known that this mechanism is nearly optimal in the worst case.","We then focus on the $\\ell_\\infty$ neighbor relation, for which the described mechanism is not optimal.","We show that one may implement the exponential mechanism for MST in polynomial time, and that this results in universal near-optimality for both the $\\ell_1$ and the $\\ell_\\infty$ neighbor relations."],"url":"http://arxiv.org/abs/2404.15035v1","category":"cs.CR"}
{"created":"2024-04-23 13:22:43","title":"Convergence of the hypersymplectic flow on $T^4$ with $T^3$-symmetry","abstract":"A hypersymplectic structure on a 4-manifold is a triple $\\omega_1, \\omega_2, \\omega_3$ of 2-forms for which every non-trivial linear combination $a^1\\omega_1 + a^2 \\omega_2 + a^3 \\omega_3$ is a symplectic form. Donaldson has conjectured that when the underlying manifold is compact, any such structure is isotopic in its cohomolgy class to a hyperk\\\"ahler triple. We prove this conjecture for a hypersymplectic structure on $T^4$ which is invariant under the standard $T^3$ action. The proof uses the hypersymplectic flow, a geometric flow which attempts to deform a given hypersymplectic structure to a hyperk\\\"ahler triple. We prove that on $T^4$, when starting from a $T^3$-invariant hypersymplectic structure, the flow exists for all time and converges modulo diffeomorphisms to the unique cohomologous hyperk\\\"ahler structure.","sentences":["A hypersymplectic structure on a 4-manifold is a triple $\\omega_1, \\omega_2, \\omega_3$ of 2-forms for which every non-trivial linear combination $a^1\\omega_1 + a^2 \\omega_2 + a^3 \\omega_3$ is a symplectic form.","Donaldson has conjectured that when the underlying manifold is compact, any such structure is isotopic in its cohomolgy class to a hyperk\\\"ahler triple.","We prove this conjecture for a hypersymplectic structure on $T^4$ which is invariant under the standard $T^3$ action.","The proof uses the hypersymplectic flow, a geometric flow which attempts to deform a given hypersymplectic structure to a hyperk\\\"ahler triple.","We prove that on $T^4$, when starting from a $T^3$-invariant hypersymplectic structure, the flow exists for all time and converges modulo diffeomorphisms to the unique cohomologous hyperk\\\"ahler structure."],"url":"http://arxiv.org/abs/2404.15016v1","category":"math.DG"}
{"created":"2024-04-23 13:19:17","title":"Quantifying multipartite quantum states by ($k+1$)-partite entanglement measures","abstract":"In this paper, we investigate how to quantify the quantum states of $n$-particles from the point of $(k+1)$-partite entanglement $(1\\leq k\\leq n-1)$, which plays an instrumental role in quantum nonlocality and quantum metrology. We put forward two families of entanglement measures termed $q$-$(k+1)$-PE concurrence $(q>1)$ and $\\alpha$-$(k+1)$-PE concurrence $(0\\leq\\alpha<1)$, respectively. As far as the pure state is concerned, they are defined based on the minimum in entanglement. Meanwhile, rigorous proofs showing that both types of quantifications fulfill all the requirements of an entanglement measure are provided. In addition, we also propose two alternative kinds of entanglement measures, named $q$-$(k+1)$-GPE concurrence $(q>1)$ and $\\alpha$-$(k+1)$-GPE concurrence $(0\\leq\\alpha<1)$, respectively, where the quantifications of any pure state are given by taking the geometric mean of entanglement under all partitions satisfying preconditions. Besides, the lower bounds of these measures are presented by means of the entanglement of permutationally invariant (PI) part of quantum states and the connections of these measures are offered. Moreover, we compare these measures and explain the similarities and differences among them. Furthermore, for computational convenience, we consider enhanced versions of the above quantifications that can be utilized to distinguish whether a multipartite state is genuinely strong $k$-producible.","sentences":["In this paper, we investigate how to quantify the quantum states of $n$-particles from the point of $(k+1)$-partite entanglement $(1\\leq k\\leq n-1)$, which plays an instrumental role in quantum nonlocality and quantum metrology.","We put forward two families of entanglement measures termed $q$-$(k+1)$-PE concurrence $(q>1)$ and $\\alpha$-$(k+1)$-PE concurrence $(0\\leq\\alpha<1)$, respectively.","As far as the pure state is concerned, they are defined based on the minimum in entanglement.","Meanwhile, rigorous proofs showing that both types of quantifications fulfill all the requirements of an entanglement measure are provided.","In addition, we also propose two alternative kinds of entanglement measures, named $q$-$(k+1)$-GPE concurrence $(q>1)$ and $\\alpha$-$(k+1)$-GPE concurrence $(0\\leq\\alpha<1)$, respectively, where the quantifications of any pure state are given by taking the geometric mean of entanglement under all partitions satisfying preconditions.","Besides, the lower bounds of these measures are presented by means of the entanglement of permutationally invariant (PI) part of quantum states and the connections of these measures are offered.","Moreover, we compare these measures and explain the similarities and differences among them.","Furthermore, for computational convenience, we consider enhanced versions of the above quantifications that can be utilized to distinguish whether a multipartite state is genuinely strong $k$-producible."],"url":"http://arxiv.org/abs/2404.15013v1","category":"quant-ph"}
{"created":"2024-04-23 13:03:09","title":"EarPass: Secure and Implicit Call Receiver Authentication Using Ear Acoustic Sensing","abstract":"Private voice communication often contains sensitive information, making it critical to ensure that only authorized users have access to such calls. Unfortunately, current authentication mechanisms, such as PIN-based passwords, fingerprint recognition, and face recognition, fail to authenticate the call receiver, leaving a gap in security. To fill the gap, we present EarPass, a secure and implicit call receiver authentication scheme designed for smartphones. EarPass sends inaudible acoustic signals through the earpiece speaker to actively sense the outer ear, and records echoes using the top microphone. It focuses on extracting ear-related signals from echoes and performs spectrogram analysis in the magnitude and phase domains. To overcome posture and position variability, EarPass utilizes a learning-based feature extractor for extracting representative features, and a one-class classifier for authentication. EarPass does not increase any burdens on users or change users' call answering habits. Furthermore, it does not require extra devices but only uses the speaker and microphone on the smartphone. We conducted comprehensive experiments to evaluate EarPass's effectiveness and security. Our results show that EarPass can achieve a balanced accuracy of 96.95% and an equal error rate of 1.53%. Additionally, EarPass exhibits resilience against potential attacks, including zero-effort attacks and mimicry attacks.","sentences":["Private voice communication often contains sensitive information, making it critical to ensure that only authorized users have access to such calls.","Unfortunately, current authentication mechanisms, such as PIN-based passwords, fingerprint recognition, and face recognition, fail to authenticate the call receiver, leaving a gap in security.","To fill the gap, we present EarPass, a secure and implicit call receiver authentication scheme designed for smartphones.","EarPass sends inaudible acoustic signals through the earpiece speaker to actively sense the outer ear, and records echoes using the top microphone.","It focuses on extracting ear-related signals from echoes and performs spectrogram analysis in the magnitude and phase domains.","To overcome posture and position variability, EarPass utilizes a learning-based feature extractor for extracting representative features, and a one-class classifier for authentication.","EarPass does not increase any burdens on users or change users' call answering habits.","Furthermore, it does not require extra devices but only uses the speaker and microphone on the smartphone.","We conducted comprehensive experiments to evaluate EarPass's effectiveness and security.","Our results show that EarPass can achieve a balanced accuracy of 96.95% and an equal error rate of 1.53%.","Additionally, EarPass exhibits resilience against potential attacks, including zero-effort attacks and mimicry attacks."],"url":"http://arxiv.org/abs/2404.15000v1","category":"cs.CR"}
{"created":"2024-04-23 12:30:04","title":"A detailed first-principles study of the structural, elastic, thermomechanical and optoelectronic properties of binary rare-earth tritelluride NdTe3","abstract":"Rare-earth tritellurides (RTe3) are popular for their charge density wave (CDW) phase, magnetotransport properties and pressure induced superconducting state among other features. In this literature, Density functional theory has been exploited to study various properties of NdTe3. The calculated elastic and thermomechanical parameters, which were hitherto untouched for any RTe3, uncover soft, ductile, highly machinable and damage tolerant characteristics, as well as highly anisotropic mechanical behavior of this layered compound. Its thermomechanical properties make it a prospective thermal barrier coating material. Band structure, density of states, Fermi surfaces and various optical functions of the material have been reported. The band structure demonstrates highly directional metallic nature. The highly dispersive bands indicate very low effective charge carrier mass for the in-plane directions. The Fermi surfaces display symmetric pockets, including signs of nesting, bilayer splitting among others, corroborating previous works. The optical spectra expose high reflectivity across the visible region, while absorption is high in the ultraviolet region. Two plasma frequencies are noticed in the optical loss function. The optical conductivity, reflectivity and absorption reaffirm its metallic properties. The electronic band structure manifests evidence of CDW phase in the ground state.","sentences":["Rare-earth tritellurides (RTe3) are popular for their charge density wave (CDW) phase, magnetotransport properties and pressure induced superconducting state among other features.","In this literature, Density functional theory has been exploited to study various properties of NdTe3.","The calculated elastic and thermomechanical parameters, which were hitherto untouched for any RTe3, uncover soft, ductile, highly machinable and damage tolerant characteristics, as well as highly anisotropic mechanical behavior of this layered compound.","Its thermomechanical properties make it a prospective thermal barrier coating material.","Band structure, density of states, Fermi surfaces and various optical functions of the material have been reported.","The band structure demonstrates highly directional metallic nature.","The highly dispersive bands indicate very low effective charge carrier mass for the in-plane directions.","The Fermi surfaces display symmetric pockets, including signs of nesting, bilayer splitting among others, corroborating previous works.","The optical spectra expose high reflectivity across the visible region, while absorption is high in the ultraviolet region.","Two plasma frequencies are noticed in the optical loss function.","The optical conductivity, reflectivity and absorption reaffirm its metallic properties.","The electronic band structure manifests evidence of CDW phase in the ground state."],"url":"http://arxiv.org/abs/2404.14974v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-23 12:26:44","title":"Optimal subgraphs in geometric scale-free random graphs","abstract":"Geometric scale-free random graphs are popular models for networks that exhibit as heavy-tailed degree distributions, small-worldness and high clustering. In these models, vertices have weights that cause the heavy-tailed degrees and are embedded in a metric space so that close-by groups of vertices tend to cluster. The interplay between the vertex weights and positions heavily affects the local structure of the random graph, in particular the occurrence of subgraph patterns, but the dependencies in these structures and weights make them difficult to analyze. In this paper we investigate subgraph counts using a \\textit{divide et impera} strategy: first counting the number of subgraphs in specific classes of vertices; then computing which class yields maximum contribution. Interestingly, the scaling behavior of induced and general subgraphs in such geometric heavy-tailed random graphs is closely related to the solution of a mixed-integer linear program which also shows that subgraphs appear predominantly on vertices with some prescribed degrees and inter-distances. Finally, we derive precise asymptotics for trees and Hamiltonian subgraphs.","sentences":["Geometric scale-free random graphs are popular models for networks that exhibit as heavy-tailed degree distributions, small-worldness and high clustering.","In these models, vertices have weights that cause the heavy-tailed degrees and are embedded in a metric space so that close-by groups of vertices tend to cluster.","The interplay between the vertex weights and positions heavily affects the local structure of the random graph, in particular the occurrence of subgraph patterns, but the dependencies in these structures and weights make them difficult to analyze.","In this paper we investigate subgraph counts using a \\textit{divide et impera} strategy: first counting the number of subgraphs in specific classes of vertices; then computing which class yields maximum contribution.","Interestingly, the scaling behavior of induced and general subgraphs in such geometric heavy-tailed random graphs is closely related to the solution of a mixed-integer linear program which also shows that subgraphs appear predominantly on vertices with some prescribed degrees and inter-distances.","Finally, we derive precise asymptotics for trees and Hamiltonian subgraphs."],"url":"http://arxiv.org/abs/2404.14972v1","category":"math.PR"}
{"created":"2024-04-23 12:01:48","title":"Saving proof-of-work by hierarchical block structure","abstract":"We argue that the current POW based consensus algorithm of the Bitcoin network suffers from a fundamental economic discrepancy between the real world transaction (txn) costs incurred by miners and the wealth that is being transacted. Put simply, whether one transacts 1 satoshi or 1 bitcoin, the same amount of electricity is needed when including this txn into a block. The notorious Bitcoin blockchain problems such as its high energy usage per txn or its scalability issues are, either partially or fully, mere consequences of this fundamental economic inconsistency. We propose making the computational cost of securing the txns proportional to the wealth being transferred, at least temporarily.   First, we present a simple incentive based model of Bitcoin's security. Then, guided by this model, we augment each txn by two parameters, one controlling the time spent securing this txn and the second determining the fraction of the network used to accomplish this. The current Bitcoin txns are naturally embedded into this parametrized space. Then we introduce a sequence of hierarchical block structures (HBSs) containing these parametrized txns. The first of those HBSs exploits only a single degree of freedom of the extended txn, namely the time investment, but it allows already for txns with a variable level of trust together with aligned network fees and energy usage. In principle, the last HBS should scale to tens of thousands timely txns per second while preserving what the previous HBSs achieved.   We also propose a simple homotopy based transition mechanism which enables us to relatively safely and continuously introduce new HBSs into the existing blockchain.   Our approach is constructive and as rigorous as possible and we attempt to analyze all aspects of these developments, al least at a conceptual level. The process is supported by evaluation on recent transaction data.","sentences":["We argue that the current POW based consensus algorithm of the Bitcoin network suffers from a fundamental economic discrepancy between the real world transaction (txn) costs incurred by miners and the wealth that is being transacted.","Put simply, whether one transacts 1 satoshi or 1 bitcoin, the same amount of electricity is needed when including this txn into a block.","The notorious Bitcoin blockchain problems such as its high energy usage per txn or its scalability issues are, either partially or fully, mere consequences of this fundamental economic inconsistency.","We propose making the computational cost of securing the txns proportional to the wealth being transferred, at least temporarily.   ","First, we present a simple incentive based model of Bitcoin's security.","Then, guided by this model, we augment each txn by two parameters, one controlling the time spent securing this txn and the second determining the fraction of the network used to accomplish this.","The current Bitcoin txns are naturally embedded into this parametrized space.","Then we introduce a sequence of hierarchical block structures (HBSs) containing these parametrized txns.","The first of those HBSs exploits only a single degree of freedom of the extended txn, namely the time investment, but it allows already for txns with a variable level of trust together with aligned network fees and energy usage.","In principle, the last HBS should scale to tens of thousands timely txns per second while preserving what the previous HBSs achieved.   ","We also propose a simple homotopy based transition mechanism which enables us to relatively safely and continuously introduce new HBSs into the existing blockchain.   ","Our approach is constructive and as rigorous as possible and we attempt to analyze all aspects of these developments, al least at a conceptual level.","The process is supported by evaluation on recent transaction data."],"url":"http://arxiv.org/abs/2404.14958v1","category":"math.NA"}
{"created":"2024-04-23 11:55:20","title":"Dynamic pricing with Bayesian updates from online reviews","abstract":"When launching new products, firms face uncertainty about market reception. Online reviews provide valuable information not only to consumers but also to firms, allowing firms to adjust the product characteristics, including its selling price. In this paper, we consider a pricing model with online reviews in which the quality of the product is uncertain, and both the seller and the buyers Bayesianly update their beliefs to make purchasing & pricing decisions. We model the seller's pricing problem as a basic bandits' problem and show a close connection with the celebrated Catalan numbers, allowing us to efficiently compute the overall future discounted reward of the seller. With this tool, we analyze and compare the optimal static and dynamic pricing strategies in terms of the probability of effectively learning the quality of the product.","sentences":["When launching new products, firms face uncertainty about market reception.","Online reviews provide valuable information not only to consumers but also to firms, allowing firms to adjust the product characteristics, including its selling price.","In this paper, we consider a pricing model with online reviews in which the quality of the product is uncertain, and both the seller and the buyers Bayesianly update their beliefs to make purchasing & pricing decisions.","We model the seller's pricing problem as a basic bandits' problem and show a close connection with the celebrated Catalan numbers, allowing us to efficiently compute the overall future discounted reward of the seller.","With this tool, we analyze and compare the optimal static and dynamic pricing strategies in terms of the probability of effectively learning the quality of the product."],"url":"http://arxiv.org/abs/2404.14953v1","category":"cs.LG"}
{"created":"2024-04-23 11:45:43","title":"Sharp quasi-invariance threshold for the cubic Szeg\u0151 equation","abstract":"We consider the 1-dimensional cubic Szeg\\H{o} equation with data distributed according to the Gaussian measure with inverse covariance operator $(1-\\partial_x^2)^\\frac s2$, where $s>\\frac12$. We show that, for $s>1$, this measure is quasi-invariant under the flow of the equation, while for $s<1$, $s\\neq \\frac34$, the transported measure and the initial Gaussian measure are mutually singular for almost every time. This is the first observation of a transition from quasi-invariance to singularity in the context of the transport of Gaussian measures under the flow of Hamiltonian PDEs.","sentences":["We consider the 1-dimensional cubic Szeg\\H{o} equation with data distributed according to the Gaussian measure with inverse covariance operator $(1-\\partial_x^2)^\\frac s2$, where $s>\\frac12$. We show that, for $s>1$, this measure is quasi-invariant under the flow of the equation, while for $s<1$, $s\\neq \\frac34$, the transported measure and the initial Gaussian measure are mutually singular for almost every time.","This is the first observation of a transition from quasi-invariance to singularity in the context of the transport of Gaussian measures under the flow of Hamiltonian PDEs."],"url":"http://arxiv.org/abs/2404.14950v1","category":"math.AP"}
{"created":"2024-04-23 11:31:35","title":"Approximative compactness in B\u00f6chner spaces","abstract":"For any $p\\in[1,\\infty)$, we prove that the set of simple functions taking at most $k$ different values is proximinal in B\\\"ochner spaces $L^p(X)$ whenever $X$ is a dual Banach space with $w^*$-sequentially compact unit ball. With additional properties on $X$ and its norm, we show these sets are approximatively $w^*$-compact for $p\\in(1,\\infty)$ and even approximatively norm-compact under stronger hypothesis.","sentences":["For any $p\\in[1,\\infty)$, we prove that the set of simple functions taking at most $k$ different values is proximinal in B\\\"ochner spaces $L^p(X)$ whenever $X$ is a dual Banach space with $w^*$-sequentially compact unit ball.","With additional properties on $X$ and its norm, we show these sets are approximatively $w^*$-compact for $p\\in(1,\\infty)$ and even approximatively norm-compact under stronger hypothesis."],"url":"http://arxiv.org/abs/2404.14939v1","category":"math.FA"}
{"created":"2024-04-23 11:15:10","title":"Large Angular Momentum","abstract":"Quantum states of a spin $1/2$ (a qubit) are parametrized by the space $CP^1 \\sim S^2$, the Bloch sphere. A spin j (a 2j+1 -state system) for generic j is represented instead by a point of a larger space, $CP^{2j}$. Here we study the angular momentum/spin in the limit, $j \\to \\infty$. The state, $(J \\cdot n) | j, n\\rangle = j |j, n \\rangle $, where $J$ is the angular momentum operator and $n$ stands for a generic unit vector in $R^3$, is found to behave as a classical angular momentum, $ j n $. We discuss this phenomenon, by analysing the Stern-Gerlach experiments, the angular-momentum composition rule, and the rotation matrix. This problem arose from the consideration of a macroscopic body under an inhomogeneous magnetic field. Our observations help to explain how classical mechanics (with unique particle trajectories) emerges naturally from quantum mechanics in this context, and at the same time, make the widespread idea that large spins somehow become classical, a more precise one.","sentences":["Quantum states of a spin $1/2$ (a qubit) are parametrized by the space $CP^1 \\sim S^2$, the Bloch sphere.","A spin j (a 2j+1 -state system) for generic j is represented instead by a point of a larger space, $CP^{2j}$. Here we study the angular momentum/spin in the limit, $j \\to \\infty$. The state, $(J \\cdot n) | j, n\\rangle = j |j, n \\rangle $, where $J$ is the angular momentum operator and $n$ stands for a generic unit vector in $R^3$, is found to behave as a classical angular momentum, $ j n $.","We discuss this phenomenon, by analysing the Stern-Gerlach experiments, the angular-momentum composition rule, and the rotation matrix.","This problem arose from the consideration of a macroscopic body under an inhomogeneous magnetic field.","Our observations help to explain how classical mechanics (with unique particle trajectories) emerges naturally from quantum mechanics in this context, and at the same time, make the widespread idea that large spins somehow become classical, a more precise one."],"url":"http://arxiv.org/abs/2404.14931v1","category":"quant-ph"}
{"created":"2024-04-23 11:09:27","title":"Formalizing Factorization on Euclidean Domains and Abstract Euclidean Algorithms","abstract":"This paper discusses the extension of the Prototype Verification System (PVS) sub-theory for rings, part of the PVS algebra theory, with theorems related to the division algorithm for Euclidean rings and Unique Factorization Domains that are general structures where an analog of the Fundamental Theorem of Arithmetic holds. First, we formalize the general abstract notions of divisibility, prime, and irreducible elements in commutative rings, essential to deal with unique factorization domains. Then, we formalize the landmark theorem, establishing that every principal ideal domain is a unique factorization domain. Finally, we specify the theory of Euclidean domains and formally verify that the rings of integers, the Gaussian integers, and arbitrary fields are Euclidean domains. To highlight the benefits of such a general abstract discipline of formalization, we specify a Euclidean gcd algorithm for Euclidean domains and formalize its correctness. Also, we show how this correctness is inherited under adequate parameterizations for the structures of integers and Gaussian integers.","sentences":["This paper discusses the extension of the Prototype Verification System (PVS) sub-theory for rings, part of the PVS algebra theory, with theorems related to the division algorithm for Euclidean rings and Unique Factorization Domains that are general structures where an analog of the Fundamental Theorem of Arithmetic holds.","First, we formalize the general abstract notions of divisibility, prime, and irreducible elements in commutative rings, essential to deal with unique factorization domains.","Then, we formalize the landmark theorem, establishing that every principal ideal domain is a unique factorization domain.","Finally, we specify the theory of Euclidean domains and formally verify that the rings of integers, the Gaussian integers, and arbitrary fields are Euclidean domains.","To highlight the benefits of such a general abstract discipline of formalization, we specify a Euclidean gcd algorithm for Euclidean domains and formalize its correctness.","Also, we show how this correctness is inherited under adequate parameterizations for the structures of integers and Gaussian integers."],"url":"http://arxiv.org/abs/2404.14920v1","category":"cs.LO"}
{"created":"2024-04-23 10:58:02","title":"A novel mathematical model for predicting the benefits of physical activity on type 2 diabetes progression","abstract":"Despite the well-acknowledged benefits of physical activity for type 2 diabetes (T2D) prevention, the literature surprisingly lacks validated models able to predict the long-term benefits of exercise on T2D progression and support personalized risk prediction and prevention. To bridge this gap, we developed a novel mathematical model that formalizes the link between exercise and short- and long-term glucose-insulin dynamics to predict the benefits of regular exercise on T2D progression. The model quantitatively captured the dose-response relationship (larger benefits with increasing intensity and/or duration of exercise), it consistently reproduced the benefits of clinical guidelines for diabetes prevention, and it accurately predicted persistent benefits following interruption of physical activity, in line with real-world evidence from the literature. These results are encouraging and can be the basis for future development of decision support tools able to assist patients and clinicians in tailoring preventive lifestyle interventions.","sentences":["Despite the well-acknowledged benefits of physical activity for type 2 diabetes (T2D) prevention, the literature surprisingly lacks validated models able to predict the long-term benefits of exercise on T2D progression and support personalized risk prediction and prevention.","To bridge this gap, we developed a novel mathematical model that formalizes the link between exercise and short- and long-term glucose-insulin dynamics to predict the benefits of regular exercise on T2D progression.","The model quantitatively captured the dose-response relationship (larger benefits with increasing intensity and/or duration of exercise), it consistently reproduced the benefits of clinical guidelines for diabetes prevention, and it accurately predicted persistent benefits following interruption of physical activity, in line with real-world evidence from the literature.","These results are encouraging and can be the basis for future development of decision support tools able to assist patients and clinicians in tailoring preventive lifestyle interventions."],"url":"http://arxiv.org/abs/2404.14915v1","category":"eess.SY"}
{"created":"2024-04-23 10:57:59","title":"Pillars of Grammatical Error Correction: Comprehensive Inspection Of Contemporary Approaches In The Era of Large Language Models","abstract":"In this paper, we carry out experimental research on Grammatical Error Correction, delving into the nuances of single-model systems, comparing the efficiency of ensembling and ranking methods, and exploring the application of large language models to GEC as single-model systems, as parts of ensembles, and as ranking methods. We set new state-of-the-art performance with F_0.5 scores of 72.8 on CoNLL-2014-test and 81.4 on BEA-test, respectively. To support further advancements in GEC and ensure the reproducibility of our research, we make our code, trained models, and systems' outputs publicly available.","sentences":["In this paper, we carry out experimental research on Grammatical Error Correction, delving into the nuances of single-model systems, comparing the efficiency of ensembling and ranking methods, and exploring the application of large language models to GEC as single-model systems, as parts of ensembles, and as ranking methods.","We set new state-of-the-art performance with F_0.5 scores of 72.8 on CoNLL-2014-test and 81.4 on BEA-test, respectively.","To support further advancements in GEC and ensure the reproducibility of our research, we make our code, trained models, and systems' outputs publicly available."],"url":"http://arxiv.org/abs/2404.14914v1","category":"cs.CL"}
{"created":"2024-04-23 10:55:11","title":"Asymmetric self-interacting dark matter with canonical seesaw","abstract":"We study the possibility of generating dark matter (DM) and baryon asymmetry of the universe (BAU) simultaneously in an asymmetric DM framework which also alleviates the small-scale structure issues of cold DM (CDM). While thermal relic of such self-interacting DM (SIDM) remains under-abundant due to efficient annihilation into light mediators, a non-zero asymmetry in dark sector can lead to survival of the required DM in the universe. The existence of a light mediator leads to the required self-interactions of DM at small scales while keeping DM properties similar to CDM at large scales. It also ensures that the symmetric DM component annihilates away, leaving the asymmetric part, in the spirit of cogenesis. The particle physics implementation is done in canonical seesaw models of light neutrino mass, connecting it to the origin of DM and BAU. In particular, we consider type-I and type-III seesaw origin of neutrino mass for simplicity and minimality of the field content. We show that the desired self-interactions and relic of DM together with BAU while satisfying relevant constraints lead to strict limits on DM mass $\\mathcal{O}(\\rm GeV) \\lesssim M_{\\rm DM} \\lesssim460 $ GeV. In spite of being high scale seesaw, the models remain verifiable at different experiments including direct, indirect DM search as well as colliders.","sentences":["We study the possibility of generating dark matter (DM) and baryon asymmetry of the universe (BAU) simultaneously in an asymmetric DM framework which also alleviates the small-scale structure issues of cold DM (CDM).","While thermal relic of such self-interacting DM (SIDM) remains under-abundant due to efficient annihilation into light mediators, a non-zero asymmetry in dark sector can lead to survival of the required DM in the universe.","The existence of a light mediator leads to the required self-interactions of DM at small scales while keeping DM properties similar to CDM at large scales.","It also ensures that the symmetric DM component annihilates away, leaving the asymmetric part, in the spirit of cogenesis.","The particle physics implementation is done in canonical seesaw models of light neutrino mass, connecting it to the origin of DM and BAU.","In particular, we consider type-I and type-III seesaw origin of neutrino mass for simplicity and minimality of the field content.","We show that the desired self-interactions and relic of DM together with BAU while satisfying relevant constraints lead to strict limits on DM mass $\\mathcal{O}(\\rm GeV) \\lesssim M_{\\rm DM} \\lesssim460 $ GeV. In spite of being high scale seesaw, the models remain verifiable at different experiments including direct, indirect DM search as well as colliders."],"url":"http://arxiv.org/abs/2404.14912v1","category":"hep-ph"}
{"created":"2024-04-23 10:27:22","title":"Chiral-vacuum excited replicae in QCD modeling","abstract":"We present a detailed study of the Bardeen-Cooper-Schrieffer (BCS) gap equation ``replicae'' or excited vacuum states, orthogonal to the ground-state one, in the chiral-quark sector of the Hamiltonian Coulomb-gauge model of chromodynamics. Analyzing the number of negative eigenmodes of the energy density's Hessian we believe that we have identified all of the (negative energy-density) vacua of this nonlinear system, namely the ground BCS state and two (or one) replicae for slightly massive (or massless) quarks, given the interaction strength typical of the strong interactions. The meson spectrum over each of the replicae looks similar, so the differences are not significant enough given model uncertainties, but matrix elements are more sensitive and allow to distinguish them. We propose to look for such excited vacua in lattice gauge theory by trying to identify excitations with scalar quantum numbers which have energies proportional to the lattice volume (unlike conventional mesons for which the mass stabilizes to a constant upon taking the infinite volume limit).","sentences":["We present a detailed study of the Bardeen-Cooper-Schrieffer (BCS) gap equation ``replicae'' or excited vacuum states, orthogonal to the ground-state one, in the chiral-quark sector of the Hamiltonian Coulomb-gauge model of chromodynamics.","Analyzing the number of negative eigenmodes of the energy density's Hessian we believe that we have identified all of the (negative energy-density) vacua of this nonlinear system, namely the ground BCS state and two (or one) replicae for slightly massive (or massless) quarks, given the interaction strength typical of the strong interactions.","The meson spectrum over each of the replicae looks similar, so the differences are not significant enough given model uncertainties, but matrix elements are more sensitive and allow to distinguish them.","We propose to look for such excited vacua in lattice gauge theory by trying to identify excitations with scalar quantum numbers which have energies proportional to the lattice volume (unlike conventional mesons for which the mass stabilizes to a constant upon taking the infinite volume limit)."],"url":"http://arxiv.org/abs/2404.14898v1","category":"hep-ph"}
{"created":"2024-04-23 10:22:08","title":"Bayesian Approaches to Collaborative Data Analysis with Strict Privacy Restrictions","abstract":"Collaborative data analysis between countries is crucial for enabling fast responses to increasingly multi-country disease outbreaks. Often, data early in outbreaks are of sensitive nature and subject to strict privacy restrictions. Thus, federated analysis, which implies decentralised collaborative analysis where no raw data sharing is required, emerged as a novel approach solving issues around data privacy and confidentiality. In the present study, we propose two approaches to federated analysis, based on simple Bayesian statistics and exploit this simplicity to make them feasible for rapid collaboration without the risks of data leaks and data reidentification, as they require neither data sharing nor direct communication between devices. The first approach uses summaries from parameters' posteriors previously obtained at a different location to update truncated normal distributions approximating priors of a new model. The second approach uses the entire previously sampled posterior, approximating via a multivariate normal distribution. We test these models on simulated and on real outbreak data to estimate the incubation period of infectious diseases. Results indicate that both approaches can recover incubation period parameters accurately, but they differ in terms of inferential capacity. The posterior summary approach shows higher stability and precision, but it cannot capture posterior correlations, meaning it is inferentially limited. The whole posterior approach can capture correlations, but it shows less stability, and its applicability is limited to fewer prior distributions. We discuss results in terms of the advantages of their simplicity and privacy-preserving properties, and in terms of their limited generalisability to more complex analytical models.","sentences":["Collaborative data analysis between countries is crucial for enabling fast responses to increasingly multi-country disease outbreaks.","Often, data early in outbreaks are of sensitive nature and subject to strict privacy restrictions.","Thus, federated analysis, which implies decentralised collaborative analysis where no raw data sharing is required, emerged as a novel approach solving issues around data privacy and confidentiality.","In the present study, we propose two approaches to federated analysis, based on simple Bayesian statistics and exploit this simplicity to make them feasible for rapid collaboration without the risks of data leaks and data reidentification, as they require neither data sharing nor direct communication between devices.","The first approach uses summaries from parameters' posteriors previously obtained at a different location to update truncated normal distributions approximating priors of a new model.","The second approach uses the entire previously sampled posterior, approximating via a multivariate normal distribution.","We test these models on simulated and on real outbreak data to estimate the incubation period of infectious diseases.","Results indicate that both approaches can recover incubation period parameters accurately, but they differ in terms of inferential capacity.","The posterior summary approach shows higher stability and precision, but it cannot capture posterior correlations, meaning it is inferentially limited.","The whole posterior approach can capture correlations, but it shows less stability, and its applicability is limited to fewer prior distributions.","We discuss results in terms of the advantages of their simplicity and privacy-preserving properties, and in terms of their limited generalisability to more complex analytical models."],"url":"http://arxiv.org/abs/2404.14895v1","category":"stat.AP"}
{"created":"2024-04-23 10:13:31","title":"Domain adaptive pose estimation via multi-level alignment","abstract":"Domain adaptive pose estimation aims to enable deep models trained on source domain (synthesized) datasets produce similar results on the target domain (real-world) datasets. The existing methods have made significant progress by conducting image-level or feature-level alignment. However, only aligning at a single level is not sufficient to fully bridge the domain gap and achieve excellent domain adaptive results. In this paper, we propose a multi-level domain adaptation aproach, which aligns different domains at the image, feature, and pose levels. Specifically, we first utilize image style transer to ensure that images from the source and target domains have a similar distribution. Subsequently, at the feature level, we employ adversarial training to make the features from the source and target domains preserve domain-invariant characeristics as much as possible. Finally, at the pose level, a self-supervised approach is utilized to enable the model to learn diverse knowledge, implicitly addressing the domain gap. Experimental results demonstrate that significant imrovement can be achieved by the proposed multi-level alignment method in pose estimation, which outperforms previous state-of-the-art in human pose by up to 2.4% and animal pose estimation by up to 3.1% for dogs and 1.4% for sheep.","sentences":["Domain adaptive pose estimation aims to enable deep models trained on source domain (synthesized) datasets produce similar results on the target domain (real-world) datasets.","The existing methods have made significant progress by conducting image-level or feature-level alignment.","However, only aligning at a single level is not sufficient to fully bridge the domain gap and achieve excellent domain adaptive results.","In this paper, we propose a multi-level domain adaptation aproach, which aligns different domains at the image, feature, and pose levels.","Specifically, we first utilize image style transer to ensure that images from the source and target domains have a similar distribution.","Subsequently, at the feature level, we employ adversarial training to make the features from the source and target domains preserve domain-invariant characeristics as much as possible.","Finally, at the pose level, a self-supervised approach is utilized to enable the model to learn diverse knowledge, implicitly addressing the domain gap.","Experimental results demonstrate that significant imrovement can be achieved by the proposed multi-level alignment method in pose estimation, which outperforms previous state-of-the-art in human pose by up to 2.4% and animal pose estimation by up to 3.1% for dogs and 1.4% for sheep."],"url":"http://arxiv.org/abs/2404.14885v1","category":"cs.CV"}
{"created":"2024-04-23 10:06:34","title":"Numerical demonstration of Abelian fractional statistics of composite fermions in the spherical geometry","abstract":"Fractional quantum Hall (FQH) fluids host quasiparticle excitations that carry a fraction of the electronic charge. Moreover, in contrast to bosons and fermions that carry exchange statistics of $0$ and $\\pi$ respectively, these quasiparticles of FQH fluids, when braided around one another, can accumulate a Berry phase, which is a fractional multiple of $\\pi$. Deploying the spherical geometry, we numerically demonstrate that composite fermion particle (CFP) excitations in the Jain FQH states carry Abelian fractional statistics. Previously, the exchange statistics of CFPs were studied in the disk geometry, where the statistics get obscured due to a shift in the phase arising from the addition of another CFP, making its determination cumbersome without prior knowledge of the shift. We show that on the sphere this technical issue can be circumvented and the statistics of CFPs can be obtained more transparently. The ideas we present can be extended to determine the statistics of quasiparticles arising in certain non-Abelian partonic FQH states.","sentences":["Fractional quantum Hall (FQH) fluids host quasiparticle excitations that carry a fraction of the electronic charge.","Moreover, in contrast to bosons and fermions that carry exchange statistics of $0$ and $\\pi$ respectively, these quasiparticles of FQH fluids, when braided around one another, can accumulate a Berry phase, which is a fractional multiple of $\\pi$. Deploying the spherical geometry, we numerically demonstrate that composite fermion particle (CFP) excitations in the Jain FQH states carry Abelian fractional statistics.","Previously, the exchange statistics of CFPs were studied in the disk geometry, where the statistics get obscured due to a shift in the phase arising from the addition of another CFP, making its determination cumbersome without prior knowledge of the shift.","We show that on the sphere this technical issue can be circumvented and the statistics of CFPs can be obtained more transparently.","The ideas we present can be extended to determine the statistics of quasiparticles arising in certain non-Abelian partonic FQH states."],"url":"http://arxiv.org/abs/2404.14880v1","category":"cond-mat.str-el"}
{"created":"2024-04-23 09:55:45","title":"Segre products of cluster algebras","abstract":"We show that under suitable assumptions the Segre product of two graded cluster algebras has a natural cluster algebra structure.","sentences":["We show that under suitable assumptions the Segre product of two graded cluster algebras has a natural cluster algebra structure."],"url":"http://arxiv.org/abs/2404.14872v1","category":"math.RT"}
{"created":"2024-04-23 09:30:55","title":"Evolution of Shielding Cloud Under Oscillatory External Forcing in Strongly Coupled Ultracold Neutral Plasma","abstract":"This paper investigates the dynamics of crystalline clusters observed in Molecular Dynamics (MD) studies conducted earlier [Yadav, M., et al. Physical Review E, 107(5), 055214(2023)] for ultra-cold neutral plasmas. An external oscillatory forcing is applied for this purpose and the evolution is tracked with the help of MD simulations using the open source LAMMPS software. Interesting observations relating to cluster dynamics are presented. The formation of a pentagonal arrangement of particles is also reported.","sentences":["This paper investigates the dynamics of crystalline clusters observed in Molecular Dynamics (MD) studies conducted earlier [Yadav, M., et al.","Physical Review E, 107(5), 055214(2023)] for ultra-cold neutral plasmas.","An external oscillatory forcing is applied for this purpose and the evolution is tracked with the help of MD simulations using the open source LAMMPS software.","Interesting observations relating to cluster dynamics are presented.","The formation of a pentagonal arrangement of particles is also reported."],"url":"http://arxiv.org/abs/2404.14861v1","category":"physics.plasm-ph"}
{"created":"2024-04-23 09:22:35","title":"Variational Bayesian surrogate modelling with application to robust design optimisation","abstract":"Surrogate models provide a quick-to-evaluate approximation to complex computational models and are essential for multi-query problems like design optimisation. The inputs of current computational models are usually high-dimensional and uncertain. We consider Bayesian inference for constructing statistical surrogates with input uncertainties and intrinsic dimensionality reduction. The surrogates are trained by fitting to data from prevalent deterministic computational models. The assumed prior probability density of the surrogate is a Gaussian process. We determine the respective posterior probability density and parameters of the posited statistical model using variational Bayes. The non-Gaussian posterior is approximated by a simpler trial density with free variational parameters and the discrepancy between them is measured using the Kullback-Leibler (KL) divergence. We employ the stochastic gradient method to compute the variational parameters and other statistical model parameters by minimising the KL divergence. We demonstrate the accuracy and versatility of the proposed reduced dimension variational Gaussian process (RDVGP) surrogate on illustrative and robust structural optimisation problems with cost functions depending on a weighted sum of the mean and standard deviation of model outputs.","sentences":["Surrogate models provide a quick-to-evaluate approximation to complex computational models and are essential for multi-query problems like design optimisation.","The inputs of current computational models are usually high-dimensional and uncertain.","We consider Bayesian inference for constructing statistical surrogates with input uncertainties and intrinsic dimensionality reduction.","The surrogates are trained by fitting to data from prevalent deterministic computational models.","The assumed prior probability density of the surrogate is a Gaussian process.","We determine the respective posterior probability density and parameters of the posited statistical model using variational Bayes.","The non-Gaussian posterior is approximated by a simpler trial density with free variational parameters and the discrepancy between them is measured using the Kullback-Leibler (KL) divergence.","We employ the stochastic gradient method to compute the variational parameters and other statistical model parameters by minimising the KL divergence.","We demonstrate the accuracy and versatility of the proposed reduced dimension variational Gaussian process (RDVGP) surrogate on illustrative and robust structural optimisation problems with cost functions depending on a weighted sum of the mean and standard deviation of model outputs."],"url":"http://arxiv.org/abs/2404.14857v1","category":"stat.AP"}
{"created":"2024-04-23 09:20:55","title":"The Geometry of the Set of Equivalent Linear Neural Networks","abstract":"We characterize the geometry and topology of the set of all weight vectors for which a linear neural network computes the same linear transformation $W$. This set of weight vectors is called the fiber of $W$ (under the matrix multiplication map), and it is embedded in the Euclidean weight space of all possible weight vectors. The fiber is an algebraic variety that is not necessarily a manifold. We describe a natural way to stratify the fiber--that is, to partition the algebraic variety into a finite set of manifolds of varying dimensions called strata. We call this set of strata the rank stratification. We derive the dimensions of these strata and the relationships by which they adjoin each other. Although the strata are disjoint, their closures are not. Our strata satisfy the frontier condition: if a stratum intersects the closure of another stratum, then the former stratum is a subset of the closure of the latter stratum. Each stratum is a manifold of class $C^\\infty$ embedded in weight space, so it has a well-defined tangent space and normal space at every point (weight vector). We show how to determine the subspaces tangent to and normal to a specified stratum at a specified point on the stratum, and we construct elegant bases for those subspaces.   To help achieve these goals, we first derive what we call a Fundamental Theorem of Linear Neural Networks, analogous to what Strang calls the Fundamental Theorem of Linear Algebra. We show how to decompose each layer of a linear neural network into a set of subspaces that show how information flows through the neural network. Each stratum of the fiber represents a different pattern by which information flows (or fails to flow) through the neural network. The topology of a stratum depends solely on this decomposition. So does its geometry, up to a linear transformation in weight space.","sentences":["We characterize the geometry and topology of the set of all weight vectors for which a linear neural network computes the same linear transformation $W$. This set of weight vectors is called the fiber of $W$ (under the matrix multiplication map), and it is embedded in the Euclidean weight space of all possible weight vectors.","The fiber is an algebraic variety that is not necessarily a manifold.","We describe a natural way to stratify the fiber--that is, to partition the algebraic variety into a finite set of manifolds of varying dimensions called strata.","We call this set of strata the rank stratification.","We derive the dimensions of these strata and the relationships by which they adjoin each other.","Although the strata are disjoint, their closures are not.","Our strata satisfy the frontier condition: if a stratum intersects the closure of another stratum, then the former stratum is a subset of the closure of the latter stratum.","Each stratum is a manifold of class $C^\\infty$ embedded in weight space, so it has a well-defined tangent space and normal space at every point (weight vector).","We show how to determine the subspaces tangent to and normal to a specified stratum at a specified point on the stratum, and we construct elegant bases for those subspaces.   ","To help achieve these goals, we first derive what we call a Fundamental Theorem of Linear Neural Networks, analogous to what Strang calls the Fundamental Theorem of Linear Algebra.","We show how to decompose each layer of a linear neural network into a set of subspaces that show how information flows through the neural network.","Each stratum of the fiber represents a different pattern by which information flows (or fails to flow) through the neural network.","The topology of a stratum depends solely on this decomposition.","So does its geometry, up to a linear transformation in weight space."],"url":"http://arxiv.org/abs/2404.14855v1","category":"cs.LG"}
{"created":"2024-04-23 09:11:13","title":"Fast convergence rates and trajectory convergence of a Tikhonov regularized inertial primal\\mbox{-}dual dynamical system with time scaling and vanishing damping","abstract":"A Tikhonov regularized inertial primal\\mbox{-}dual dynamical system with time scaling and vanishing damping is proposed for solving a linearly constrained convex optimization problem in Hilbert spaces. The system under consideration consists of two coupled second order differential equations and its convergence properties depend upon the decaying speed of the product of the time scaling parameter and the Tikhonov regularization parameter (named the rescaled regularization parameter) to zero. When the rescaled regularization parameter converges rapidly to zero, the system enjoys fast convergence rates of the primal-dual gap, the feasibility violation, the objective residual, and the gradient norm of the objective function along the trajectory, and the weak convergence of the trajectory to a primal-dual solution of the linearly constrained convex optimization problem. When the rescaled regularization parameter converges slowly to zero, the generated primal trajectory converges strongly to the minimal norm solution of the problem under suitable conditions. Finally, numerical experiments are performed to illustrate the theoretical findings.","sentences":["A Tikhonov regularized inertial primal\\mbox{-}dual dynamical system with time scaling and vanishing damping is proposed for solving a linearly constrained convex optimization problem in Hilbert spaces.","The system under consideration consists of two coupled second order differential equations and its convergence properties depend upon the decaying speed of the product of the time scaling parameter and the Tikhonov regularization parameter (named the rescaled regularization parameter) to zero.","When the rescaled regularization parameter converges rapidly to zero, the system enjoys fast convergence rates of the primal-dual gap, the feasibility violation, the objective residual, and the gradient norm of the objective function along the trajectory, and the weak convergence of the trajectory to a primal-dual solution of the linearly constrained convex optimization problem.","When the rescaled regularization parameter converges slowly to zero, the generated primal trajectory converges strongly to the minimal norm solution of the problem under suitable conditions.","Finally, numerical experiments are performed to illustrate the theoretical findings."],"url":"http://arxiv.org/abs/2404.14853v1","category":"math.OC"}
{"created":"2024-04-23 08:59:16","title":"Parameterized Maximum Node-Disjoint Paths","abstract":"We revisit the Maximum Node-Disjoint Paths problem, the natural optimization version of Node-Disjoint Paths, where we are given a graph $G$, $k$ pairs of vertices $(s_i, t_i)$ and an integer $\\ell$, and are asked whether there exist at least $\\ell$ vertex-disjoint paths in $G$ whose endpoints are given pairs. We present several results, with an emphasis towards FPT approximation.   Our main positive contribution is to show that the problem's intractability can be overcome using approximation and that for several of the structural parameters for which the problem is hard, most notably tree-depth, it admits an efficient FPT approximation scheme, returning a $(1-\\varepsilon)$-approximate solution in time $f(td,\\varepsilon)n^{O(1)}$. We manage to obtain these results by comprehensively mapping out the structural parameters for which the problem is FPT if $\\ell$ is also a parameter, hence showing that understanding $\\ell$ as a parameter is key to the problem's approximability. This, in turn, is a problem we are able to solve via a surprisingly simple color-coding algorithm, which relies on identifying an insightful problem-specific variant of the natural parameter, namely the number of vertices used in the solution.   A natural question is whether the FPT approximation algorithm we devised for tree-depth can be extended to pathwidth. We resolve this negatively, showing that under the Parameterized Inapproximability Hypothesis no FPT approximation scheme for this parameter is possible, even in time $f(pw,\\varepsilon)n^{g(\\varepsilon)}$, thus precisely determining the parameter border where the problem transitions from ``hard but approximable'' to ``inapproximable''.   Lastly, we strengthen existing lower bounds by replacing W[1]-hardness by XNLP-completeness for parameter pathwidth, and improving the $n^{o(\\sqrt{td})}$ ETH-based lower bound for tree-depth to $n^{o(td)}$.","sentences":["We revisit the Maximum Node-Disjoint Paths problem, the natural optimization version of Node-Disjoint Paths, where we are given a graph $G$, $k$ pairs of vertices $(s_i, t_i)$ and an integer $\\ell$, and are asked whether there exist at least $\\ell$ vertex-disjoint paths in $G$ whose endpoints are given pairs.","We present several results, with an emphasis towards FPT approximation.   ","Our main positive contribution is to show that the problem's intractability can be overcome using approximation and that for several of the structural parameters for which the problem is hard, most notably tree-depth, it admits an efficient FPT approximation scheme, returning a $(1-\\varepsilon)$-approximate solution in time $f(td,\\varepsilon)n^{O(1)}$. We manage to obtain these results by comprehensively mapping out the structural parameters for which the problem is FPT if $\\ell$ is also a parameter, hence showing that understanding $\\ell$ as a parameter is key to the problem's approximability.","This, in turn, is a problem we are able to solve via a surprisingly simple color-coding algorithm, which relies on identifying an insightful problem-specific variant of the natural parameter, namely the number of vertices used in the solution.   ","A natural question is whether the FPT approximation algorithm we devised for tree-depth can be extended to pathwidth.","We resolve this negatively, showing that under the Parameterized Inapproximability Hypothesis no FPT approximation scheme for this parameter is possible, even in time $f(pw,\\varepsilon)n^{g(\\varepsilon)}$, thus precisely determining the parameter border where the problem transitions from ``hard but approximable'' to ``inapproximable''.   ","Lastly, we strengthen existing lower bounds by replacing W[1]-hardness by XNLP-completeness for parameter pathwidth, and improving the $n^{o(\\sqrt{td})}$ ETH-based lower bound for tree-depth to $n^{o(td)}$."],"url":"http://arxiv.org/abs/2404.14849v1","category":"cs.DS"}
{"created":"2024-04-23 08:45:35","title":"Analysis of cohort stepped wedge cluster-randomized trials with non-ignorable dropout via joint modeling","abstract":"Stepped wedge cluster-randomized trial (CRTs) designs randomize clusters of individuals to intervention sequences, ensuring that every cluster eventually transitions from a control period to receive the intervention under study by the end of the study period. The analysis of stepped wedge CRTs is usually more complex than parallel-arm CRTs due to potential secular trends that result in changing intra-cluster and period-cluster correlations over time. A further challenge in the analysis of closed-cohort stepped wedge CRTs, which follow groups of individuals enrolled in each period longitudinally, is the occurrence of dropout. This is particularly problematic in studies of individuals at high risk for mortality, which causes non-ignorable missing outcomes. If not appropriately addressed, missing outcomes from death will erode statistical power, at best, and bias treatment effect estimates, at worst. Joint longitudinal-survival models can accommodate informative dropout and missingness patterns in longitudinal studies. Specifically, within this framework one directly models the dropout process via a time-to-event submodel together with the longitudinal outcome of interest. The two submodels are then linked using a variety of possible association structures. This work extends linear mixed-effects models by jointly modeling the dropout process to accommodate informative missing outcome data in closed-cohort stepped wedge CRTs. We focus on constant intervention and general time-on-treatment effect parametrizations for the longitudinal submodel and study the performance of the proposed methodology using Monte Carlo simulation under several data-generating scenarios. We illustrate the joint modeling methodology in practice by reanalyzing the `Frail Older Adults: Care in Transition' (ACT) trial, a stepped wedge CRT of a multifaceted geriatric care model versus usual care in the Netherlands.","sentences":["Stepped wedge cluster-randomized trial (CRTs) designs randomize clusters of individuals to intervention sequences, ensuring that every cluster eventually transitions from a control period to receive the intervention under study by the end of the study period.","The analysis of stepped wedge CRTs is usually more complex than parallel-arm CRTs due to potential secular trends that result in changing intra-cluster and period-cluster correlations over time.","A further challenge in the analysis of closed-cohort stepped wedge CRTs, which follow groups of individuals enrolled in each period longitudinally, is the occurrence of dropout.","This is particularly problematic in studies of individuals at high risk for mortality, which causes non-ignorable missing outcomes.","If not appropriately addressed, missing outcomes from death will erode statistical power, at best, and bias treatment effect estimates, at worst.","Joint longitudinal-survival models can accommodate informative dropout and missingness patterns in longitudinal studies.","Specifically, within this framework one directly models the dropout process via a time-to-event submodel together with the longitudinal outcome of interest.","The two submodels are then linked using a variety of possible association structures.","This work extends linear mixed-effects models by jointly modeling the dropout process to accommodate informative missing outcome data in closed-cohort stepped wedge CRTs.","We focus on constant intervention and general time-on-treatment effect parametrizations for the longitudinal submodel and study the performance of the proposed methodology using Monte Carlo simulation under several data-generating scenarios.","We illustrate the joint modeling methodology in practice by reanalyzing the `Frail Older Adults: Care in Transition' (ACT) trial, a stepped wedge CRT of a multifaceted geriatric care model versus usual care in the Netherlands."],"url":"http://arxiv.org/abs/2404.14840v1","category":"stat.ME"}
{"created":"2024-04-23 08:42:35","title":"Probabilistic forecasting of power system imbalance using neural network-based ensembles","abstract":"Keeping the balance between electricity generation and consumption is becoming increasingly challenging and costly, mainly due to the rising share of renewables, electric vehicles and heat pumps and electrification of industrial processes. Accurate imbalance forecasts, along with reliable uncertainty estimations, enable transmission system operators (TSOs) to dispatch appropriate reserve volumes, reducing balancing costs. Further, market parties can use these probabilistic forecasts to design strategies that exploit asset flexibility to help balance the grid, generating revenue with known risks. Despite its importance, literature regarding system imbalance (SI) forecasting is limited. Further, existing methods do not focus on situations with high imbalance magnitude, which are crucial to forecast accurately for both TSOs and market parties. Hence, we propose an ensemble of C-VSNs, which are our adaptation of variable selection networks (VSNs). Each minute, our model predicts the imbalance of the current and upcoming two quarter-hours, along with uncertainty estimations on these forecasts. We evaluate our approach by forecasting the imbalance of Belgium, where high imbalance magnitude is defined as $|$SI$| > 500\\,$MW (occurs 1.3% of the time in Belgium). For high imbalance magnitude situations, our model outperforms the state-of-the-art by 23.4% (in terms of continuous ranked probability score (CRPS), which evaluates probabilistic forecasts), while also attaining a 6.5% improvement in overall CRPS. Similar improvements are achieved in terms of root-mean-squared error. Additionally, we developed a fine-tuning methodology to effectively include new inputs with limited history in our model. This work was performed in collaboration with Elia (the Belgian TSO) to further improve their imbalance forecasts, demonstrating the relevance of our work.","sentences":["Keeping the balance between electricity generation and consumption is becoming increasingly challenging and costly, mainly due to the rising share of renewables, electric vehicles and heat pumps and electrification of industrial processes.","Accurate imbalance forecasts, along with reliable uncertainty estimations, enable transmission system operators (TSOs) to dispatch appropriate reserve volumes, reducing balancing costs.","Further, market parties can use these probabilistic forecasts to design strategies that exploit asset flexibility to help balance the grid, generating revenue with known risks.","Despite its importance, literature regarding system imbalance (SI) forecasting is limited.","Further, existing methods do not focus on situations with high imbalance magnitude, which are crucial to forecast accurately for both TSOs and market parties.","Hence, we propose an ensemble of C-VSNs, which are our adaptation of variable selection networks (VSNs).","Each minute, our model predicts the imbalance of the current and upcoming two quarter-hours, along with uncertainty estimations on these forecasts.","We evaluate our approach by forecasting the imbalance of Belgium, where high imbalance magnitude is defined as $|$SI$| > 500\\,$MW (occurs 1.3% of the time in Belgium).","For high imbalance magnitude situations, our model outperforms the state-of-the-art by 23.4% (in terms of continuous ranked probability score (CRPS), which evaluates probabilistic forecasts), while also attaining a 6.5% improvement in overall CRPS.","Similar improvements are achieved in terms of root-mean-squared error.","Additionally, we developed a fine-tuning methodology to effectively include new inputs with limited history in our model.","This work was performed in collaboration with Elia (the Belgian TSO) to further improve their imbalance forecasts, demonstrating the relevance of our work."],"url":"http://arxiv.org/abs/2404.14836v2","category":"eess.SY"}
{"created":"2024-04-23 08:39:55","title":"GLDPC-PC Codes for MIMO Systems with Iterative Detection and Decoding","abstract":"In this work, we propose the integration of GLDPC codes with short polar-like component codes, termed GLDPC codes with polar component codes (GLDPC-PC). This approach leverages the good distance properties of polar-like codes and mitigates their high decoding latency in long block lengths. A recently proposed soft-input soft-output decoder for polar-like codes enables effective iterative belief propagation decoding for GLDPC-PC, ensuring a low error floor under additive white Gaussian noise channels. Simulation results demonstrate that GLDPC-PC codes achieve significant performance improvements in multiple-input multiple-output systems with iterative detection and decoding (IDD). The proposed GLDPC-PC codes and the IDD scheme can be applied to various scenarios.","sentences":["In this work, we propose the integration of GLDPC codes with short polar-like component codes, termed GLDPC codes with polar component codes (GLDPC-PC).","This approach leverages the good distance properties of polar-like codes and mitigates their high decoding latency in long block lengths.","A recently proposed soft-input soft-output decoder for polar-like codes enables effective iterative belief propagation decoding for GLDPC-PC, ensuring a low error floor under additive white Gaussian noise channels.","Simulation results demonstrate that GLDPC-PC codes achieve significant performance improvements in multiple-input multiple-output systems with iterative detection and decoding (IDD).","The proposed GLDPC-PC codes and the IDD scheme can be applied to various scenarios."],"url":"http://arxiv.org/abs/2404.14832v1","category":"cs.IT"}
{"created":"2024-04-23 08:25:45","title":"Sharp ill-posedness for the non-resistive MHD equations in Sobolev spaces","abstract":"In this paper, we prove a sharp ill-posedness result for the incompressible non-resistive MHD equations. In any dimension $d\\ge 2$, we show the ill-posedness of the non-resistive MHD equations in $H^{\\frac{d}{2}-1}(\\mathbb{R}^d)\\times H^{\\frac{d}{2}}(\\mathbb{R}^d)$, which is sharp in view of the results of the local well-posedness in $H^{s-1}(\\mathbb{R}^d)\\times H^{s}(\\mathbb{R}^d)(s>\\frac{d}{2})$ established by Fefferman et al.(Arch. Ration. Mech. Anal., \\textbf{223} (2), 677-691, 2017). Furthermore, we generalize the ill-posedness results from $H^{\\frac{d}{2}-1}(\\mathbb{R}^d)\\times H^{\\frac{d}{2}}(\\mathbb{R}^d)$ to Besov spaces $B^{\\frac{d}{p}-1}_{p, q}(\\mathbb{R}^d)\\times B^{\\frac{d}{p}}_{p, q}(\\mathbb{R}^d)$ and $\\dot B^{\\frac{d}{p}-1}_{p, q}(\\mathbb{R}^d)\\times \\dot B^{\\frac{d}{p}}_{p, q}(\\mathbb{R}^d)$ for $1\\le p\\le\\infty, q>1$. Different from the ill-posedness mechanism of the incompressible Navier-Stokes equations in $\\dot B^{-1}_{\\infty, q}$ \\cite{B,W}, we construct an initial data such that the paraproduct terms (low-high frequency interaction) of the nonlinear term make the main contribution to the norm inflation of the magnetic field.","sentences":["In this paper, we prove a sharp ill-posedness result for the incompressible non-resistive MHD equations.","In any dimension $d\\ge 2$, we show the ill-posedness of the non-resistive MHD equations in $H^{\\frac{d}{2}-1}(\\mathbb{R}^d)\\times H^{\\frac{d}{2}}(\\mathbb{R}^d)$, which is sharp in view of the results of the local well-posedness in $H^{s-1}(\\mathbb{R}^d)\\times H^{s}(\\mathbb{R}^d)(s>\\frac{d}{2})$ established by Fefferman et al.(Arch.","Ration.","Mech.","Anal., \\textbf{223} (2), 677-691, 2017).","Furthermore, we generalize the ill-posedness results from $H^{\\frac{d}{2}-1}(\\mathbb{R}^d)\\times H^{\\frac{d}{2}}(\\mathbb{R}^d)$ to Besov spaces $B^{\\frac{d}{p}-1}_{p, q}(\\mathbb{R}^d)\\times B^{\\frac{d}{p}}_{p, q}(\\mathbb{R}^d)$ and $\\dot B^{\\frac{d}{p}-1}_{p, q}(\\mathbb{R}^d)\\times \\dot B^{\\frac{d}{p}}_{p, q}(\\mathbb{R}^d)$ for $1\\le p\\le\\infty, q>1$. Different from the ill-posedness mechanism of the incompressible Navier-Stokes equations in $\\dot B^{-1}_{\\infty, q}$ \\cite{B,W}, we construct an initial data such that the paraproduct terms (low-high frequency interaction) of the nonlinear term make the main contribution to the norm inflation of the magnetic field."],"url":"http://arxiv.org/abs/2404.14825v1","category":"math.AP"}
{"created":"2024-04-23 08:24:43","title":"Automated Commit Message Generation with Large Language Models: An Empirical Study and Beyond","abstract":"Commit Message Generation (CMG) approaches aim to automatically generate commit messages based on given code diffs, which facilitate collaboration among developers and play a critical role in Open-Source Software (OSS). Very recently, Large Language Models (LLMs) have demonstrated extensive applicability in diverse code-related task. But few studies systematically explored their effectiveness using LLMs. This paper conducts the first comprehensive experiment to investigate how far we have been in applying LLM to generate high-quality commit messages. Motivated by a pilot analysis, we first clean the most widely-used CMG dataset following practitioners' criteria. Afterward, we re-evaluate diverse state-of-the-art CMG approaches and make comparisons with LLMs, demonstrating the superior performance of LLMs against state-of-the-art CMG approaches. Then, we further propose four manual metrics following the practice of OSS, including Accuracy, Integrity, Applicability, and Readability, and assess various LLMs accordingly. Results reveal that GPT-3.5 performs best overall, but different LLMs carry different advantages. To further boost LLMs' performance in the CMG task, we propose an Efficient Retrieval-based In-Context Learning (ICL) framework, namely ERICommiter, which leverages a two-step filtering to accelerate the retrieval efficiency and introduces semantic/lexical-based retrieval algorithm to construct the ICL examples. Extensive experiments demonstrate the substantial performance improvement of ERICommiter on various LLMs for code diffs of different programming languages. Meanwhile, ERICommiter also significantly reduces the retrieval time while keeping almost the same performance. Our research contributes to the understanding of LLMs' capabilities in the CMG field and provides valuable insights for practitioners seeking to leverage these tools in their workflows.","sentences":["Commit Message Generation (CMG) approaches aim to automatically generate commit messages based on given code diffs, which facilitate collaboration among developers and play a critical role in Open-Source Software (OSS).","Very recently, Large Language Models (LLMs) have demonstrated extensive applicability in diverse code-related task.","But few studies systematically explored their effectiveness using LLMs.","This paper conducts the first comprehensive experiment to investigate how far we have been in applying LLM to generate high-quality commit messages.","Motivated by a pilot analysis, we first clean the most widely-used CMG dataset following practitioners' criteria.","Afterward, we re-evaluate diverse state-of-the-art CMG approaches and make comparisons with LLMs, demonstrating the superior performance of LLMs against state-of-the-art CMG approaches.","Then, we further propose four manual metrics following the practice of OSS, including Accuracy, Integrity, Applicability, and Readability, and assess various LLMs accordingly.","Results reveal that GPT-3.5 performs best overall, but different LLMs carry different advantages.","To further boost LLMs' performance in the CMG task, we propose an Efficient Retrieval-based In-Context Learning (ICL) framework, namely ERICommiter, which leverages a two-step filtering to accelerate the retrieval efficiency and introduces semantic/lexical-based retrieval algorithm to construct the ICL examples.","Extensive experiments demonstrate the substantial performance improvement of ERICommiter on various LLMs for code diffs of different programming languages.","Meanwhile, ERICommiter also significantly reduces the retrieval time while keeping almost the same performance.","Our research contributes to the understanding of LLMs' capabilities in the CMG field and provides valuable insights for practitioners seeking to leverage these tools in their workflows."],"url":"http://arxiv.org/abs/2404.14824v1","category":"cs.SE"}
{"created":"2024-04-23 08:02:00","title":"Regular black holes as an alternative to black bounce","abstract":"The so-called black bounce mechanism of singularity suppression, proposed by Simpson and Visser, consists in replacing the spherical radius $r$ in the metric tensor with $\\sqrt{r^2 + a^2}$, $a = \\rm const >0$. This removes a singularity at $r=0$ and its neighborhood from space-time, and there emerges a regular minimum of the spherical radius that can be a wormhole throat or a regular bounce (if located inside a black hole). Instead, it is proposed here to make $r=0$ a regular center by proper (Bardeen type) replacements in the metric, preserving its form at large $r$. Such replacements are applied to a class of metrics satisfying the condition $R^t_t = R^r_r$ for their Ricci tensor, in particular, to the Schwarzschild, Reissner-Nordstr\\\"om and Einstein-Born-Infeld solutions. A new version of nonlinear electrodynamics (NED) is proposed, for which a black hole solution is similar to the Einstein-Born-Infeld one but is simpler expressed analytically. All new regular metrics can be presented as solutions to NED-Einstein equations with radial magnetic fields.","sentences":["The so-called black bounce mechanism of singularity suppression, proposed by Simpson and Visser, consists in replacing the spherical radius $r$ in the metric tensor with $\\sqrt{r^2 + a^2}$, $a = \\rm const >0$.","This removes a singularity at $r=0$ and its neighborhood from space-time, and there emerges a regular minimum of the spherical radius that can be a wormhole throat or a regular bounce (if located inside a black hole).","Instead, it is proposed here to make $r=0$ a regular center by proper (Bardeen type) replacements in the metric, preserving its form at large $r$. Such replacements are applied to a class of metrics satisfying the condition $R^t_t = R^r_r$ for their Ricci tensor, in particular, to the Schwarzschild, Reissner-Nordstr\\\"om and Einstein-Born-Infeld solutions.","A new version of nonlinear electrodynamics (NED) is proposed, for which a black hole solution is similar to the Einstein-Born-Infeld one but is simpler expressed analytically.","All new regular metrics can be presented as solutions to NED-Einstein equations with radial magnetic fields."],"url":"http://arxiv.org/abs/2404.14816v1","category":"gr-qc"}
{"created":"2024-04-23 07:48:17","title":"FLARE: A New Federated Learning Framework with Adjustable Learning Rates over Resource-Constrained Wireless Networks","abstract":"Wireless federated learning (WFL) suffers from heterogeneity prevailing in the data distributions, computing powers, and channel conditions of participating devices. This paper presents a new Federated Learning with Adjusted leaRning ratE (FLARE) framework to mitigate the impact of the heterogeneity. The key idea is to allow the participating devices to adjust their individual learning rates and local training iterations, adapting to their instantaneous computing powers. The convergence upper bound of FLARE is established rigorously under a general setting with non-convex models in the presence of non-i.i.d. datasets and imbalanced computing powers. By minimizing the upper bound, we further optimize the scheduling of FLARE to exploit the channel heterogeneity. A nested problem structure is revealed to facilitate iteratively allocating the bandwidth with binary search and selecting devices with a new greedy method. A linear problem structure is also identified and a low-complexity linear programming scheduling policy is designed when training models have large Lipschitz constants. Experiments demonstrate that FLARE consistently outperforms the baselines in test accuracy, and converges much faster with the proposed scheduling policy.","sentences":["Wireless federated learning (WFL) suffers from heterogeneity prevailing in the data distributions, computing powers, and channel conditions of participating devices.","This paper presents a new Federated Learning with Adjusted leaRning ratE (FLARE) framework to mitigate the impact of the heterogeneity.","The key idea is to allow the participating devices to adjust their individual learning rates and local training iterations, adapting to their instantaneous computing powers.","The convergence upper bound of FLARE is established rigorously under a general setting with non-convex models in the presence of non-i.i.d. datasets and imbalanced computing powers.","By minimizing the upper bound, we further optimize the scheduling of FLARE to exploit the channel heterogeneity.","A nested problem structure is revealed to facilitate iteratively allocating the bandwidth with binary search and selecting devices with a new greedy method.","A linear problem structure is also identified and a low-complexity linear programming scheduling policy is designed when training models have large Lipschitz constants.","Experiments demonstrate that FLARE consistently outperforms the baselines in test accuracy, and converges much faster with the proposed scheduling policy."],"url":"http://arxiv.org/abs/2404.14811v1","category":"eess.SP"}
{"created":"2024-04-23 07:37:43","title":"Reference-Free Multi-Modality Volume Registration of X-Ray Microscopy and Light-Sheet Fluorescence Microscopy","abstract":"Recently, X-ray microscopy (XRM) and light-sheet fluorescence microscopy (LSFM) have emerged as two pivotal imaging tools in preclinical research on bone remodeling diseases, offering micrometer-level resolution. Integrating these complementary modalities provides a holistic view of bone microstructures, facilitating function-oriented volume analysis across different disease cycles. However, registering such independently acquired large-scale volumes is extremely challenging under real and reference-free scenarios. This paper presents a fast two-stage pipeline for volume registration of XRM and LSFM. The first stage extracts the surface features and employs two successive point cloud-based methods for coarse alignment. The second stage fine-tunes the initial alignment using a modified cross-correlation method, ensuring precise volumetric registration. Moreover, we propose residual similarity as a novel metric to assess the alignment of two complementary modalities. The results imply robust gradual improvement across the stages. In the end, all correlating microstructures, particularly lacunae in XRM and bone cells in LSFM, are precisely matched, enabling new insights into bone diseases like osteoporosis which are a substantial burden in aging societies.","sentences":["Recently, X-ray microscopy (XRM) and light-sheet fluorescence microscopy (LSFM) have emerged as two pivotal imaging tools in preclinical research on bone remodeling diseases, offering micrometer-level resolution.","Integrating these complementary modalities provides a holistic view of bone microstructures, facilitating function-oriented volume analysis across different disease cycles.","However, registering such independently acquired large-scale volumes is extremely challenging under real and reference-free scenarios.","This paper presents a fast two-stage pipeline for volume registration of XRM and LSFM.","The first stage extracts the surface features and employs two successive point cloud-based methods for coarse alignment.","The second stage fine-tunes the initial alignment using a modified cross-correlation method, ensuring precise volumetric registration.","Moreover, we propose residual similarity as a novel metric to assess the alignment of two complementary modalities.","The results imply robust gradual improvement across the stages.","In the end, all correlating microstructures, particularly lacunae in XRM and bone cells in LSFM, are precisely matched, enabling new insights into bone diseases like osteoporosis which are a substantial burden in aging societies."],"url":"http://arxiv.org/abs/2404.14807v1","category":"cs.CV"}
{"created":"2024-04-23 07:32:29","title":"On the Number of Steps of CyclePopping in Weakly Inconsistent U(1)-Connection Graphs","abstract":"A U(1)-connection graph $G$ is a graph in which each oriented edge is endowed with a unit complex number, the latter being conjugated under orientation flip. We consider cycle-rooted spanning forests (CRSFs), a particular kind of spanning subgraphs of $G$ that have recently found computational applications as randomized spectral sparsifiers. In this context, CRSFs are drawn from a determinantal measure. Under a condition on the connection, Kassel and Kenyon gave an elegant algorithm, named CyclePopping, to sample from this distribution. The algorithm is an extension of the celebrated algorithm of Wilson that uses a loop-erased random walk to sample uniform spanning trees. In this paper, we give an alternative, elementary proof of correctness of CyclePopping for CRSF sampling; we fill the gaps of a proof sketch by Kassel, who was himself inspired by Marchal's proof of the correctness of Wilson's original algorithm. One benefit of the full proof \\`a la Marchal is that we obtain a concise expression for the law of the number of steps to complete the sampling procedure, shedding light on practical situations where the algorithm is expected to run fast. Furthermore, we show how to extend the proof to more general distributions over CRSFs, which are not determinantal. The correctness of CyclePopping is known even in the non-determinantal case from the work of Kassel and Kenyon, so our merit is only to provide an alternate proof. One interest of this alternate proof is again to provide the distribution of the time complexity of the algorithm, in terms of a Poisson point process on the graph loops, or equivalently as a Poisson process on pyramids of cycles, a combinatorial notion introduced by Viennot. Finally, we strive to make the connections to loop measures and combinatorial structures as explicit as possible, to provide a reference for future extensions of the algorithm and its analysis.","sentences":["A U(1)-connection graph $G$ is a graph in which each oriented edge is endowed with a unit complex number, the latter being conjugated under orientation flip.","We consider cycle-rooted spanning forests (CRSFs), a particular kind of spanning subgraphs of $G$ that have recently found computational applications as randomized spectral sparsifiers.","In this context, CRSFs are drawn from a determinantal measure.","Under a condition on the connection, Kassel and Kenyon gave an elegant algorithm, named CyclePopping, to sample from this distribution.","The algorithm is an extension of the celebrated algorithm of Wilson that uses a loop-erased random walk to sample uniform spanning trees.","In this paper, we give an alternative, elementary proof of correctness of CyclePopping for CRSF sampling; we fill the gaps of a proof sketch by Kassel, who was himself inspired by Marchal's proof of the correctness of Wilson's original algorithm.","One benefit of the full proof \\`a la Marchal is that we obtain a concise expression for the law of the number of steps to complete the sampling procedure, shedding light on practical situations where the algorithm is expected to run fast.","Furthermore, we show how to extend the proof to more general distributions over CRSFs, which are not determinantal.","The correctness of CyclePopping is known even in the non-determinantal case from the work of Kassel and Kenyon, so our merit is only to provide an alternate proof.","One interest of this alternate proof is again to provide the distribution of the time complexity of the algorithm, in terms of a Poisson point process on the graph loops, or equivalently as a Poisson process on pyramids of cycles, a combinatorial notion introduced by Viennot.","Finally, we strive to make the connections to loop measures and combinatorial structures as explicit as possible, to provide a reference for future extensions of the algorithm and its analysis."],"url":"http://arxiv.org/abs/2404.14803v1","category":"cs.DS"}
{"created":"2024-04-23 07:31:19","title":"DesignProbe: A Graphic Design Benchmark for Multimodal Large Language Models","abstract":"A well-executed graphic design typically achieves harmony in two levels, from the fine-grained design elements (color, font and layout) to the overall design. This complexity makes the comprehension of graphic design challenging, for it needs the capability to both recognize the design elements and understand the design. With the rapid development of Multimodal Large Language Models (MLLMs), we establish the DesignProbe, a benchmark to investigate the capability of MLLMs in design. Our benchmark includes eight tasks in total, across both the fine-grained element level and the overall design level. At design element level, we consider both the attribute recognition and semantic understanding tasks. At overall design level, we include style and metaphor. 9 MLLMs are tested and we apply GPT-4 as evaluator. Besides, further experiments indicates that refining prompts can enhance the performance of MLLMs. We first rewrite the prompts by different LLMs and found increased performances appear in those who self-refined by their own LLMs. We then add extra task knowledge in two different ways (text descriptions and image examples), finding that adding images boost much more performance over texts.","sentences":["A well-executed graphic design typically achieves harmony in two levels, from the fine-grained design elements (color, font and layout) to the overall design.","This complexity makes the comprehension of graphic design challenging, for it needs the capability to both recognize the design elements and understand the design.","With the rapid development of Multimodal Large Language Models (MLLMs), we establish the DesignProbe, a benchmark to investigate the capability of MLLMs in design.","Our benchmark includes eight tasks in total, across both the fine-grained element level and the overall design level.","At design element level, we consider both the attribute recognition and semantic understanding tasks.","At overall design level, we include style and metaphor.","9 MLLMs are tested and we apply GPT-4 as evaluator.","Besides, further experiments indicates that refining prompts can enhance the performance of MLLMs.","We first rewrite the prompts by different LLMs and found increased performances appear in those who self-refined by their own LLMs.","We then add extra task knowledge in two different ways (text descriptions and image examples), finding that adding images boost much more performance over texts."],"url":"http://arxiv.org/abs/2404.14801v1","category":"cs.CV"}
{"created":"2024-04-23 07:29:43","title":"New Douglas-Rashford Splitting Algorithms for Generalized DC Programming with Applications in Machine Learning","abstract":"In this work, we propose some new Douglas-Rashford splitting algorithms for solving a class of generalized DC (difference of convex functions) in real Hilbert spaces. The proposed methods leverage the proximal properties of the nonsmooth component and a fasten control parameter which improves the convergence rate of the algorithms. We prove the convergence of these methods to the critical points of nonconvex optimization under reasonable conditions. We evaluate the performance and effectiveness of our methods through experimentation with three practical examples in machine learning. Our findings demonstrated that our methods offer efficiency in problem-solving and outperform state-of-the-art techniques like the DCA (DC Algorithm) and ADMM.","sentences":["In this work, we propose some new Douglas-Rashford splitting algorithms for solving a class of generalized DC (difference of convex functions) in real Hilbert spaces.","The proposed methods leverage the proximal properties of the nonsmooth component and a fasten control parameter which improves the convergence rate of the algorithms.","We prove the convergence of these methods to the critical points of nonconvex optimization under reasonable conditions.","We evaluate the performance and effectiveness of our methods through experimentation with three practical examples in machine learning.","Our findings demonstrated that our methods offer efficiency in problem-solving and outperform state-of-the-art techniques like the DCA (DC Algorithm) and ADMM."],"url":"http://arxiv.org/abs/2404.14800v1","category":"math.OC"}
{"created":"2024-04-23 07:25:57","title":"Antifragile control systems in neuronal processing: A sensorimotor perspective","abstract":"The stability--robustness--resilience--adaptiveness continuum in neuronal processing follows a hierarchical structure that explains interactions and information processing among the different time scales. Interestingly, using \"canonical\" neuronal computational circuits, such as Homeostatic Activity Regulation, Winner-Take-All, and Hebbian Temporal Correlation Learning, one can extend the behaviour spectrum towards antifragility. Cast already in both probability theory and dynamical systems, antifragility can explain and define the interesting interplay among neural circuits, found, for instance, in sensorimotor control in the face of uncertainty and volatility. This perspective proposes a new framework to analyse and describe closed-loop neuronal processing using principles of antifragility, targeting sensorimotor control. Our objective is two-fold. First, we introduce antifragile control as a conceptual framework to quantify closed-loop neuronal network behaviours that gain from uncertainty and volatility. Second, we introduce neuronal network design principles, opening the path to neuromorphic implementations and transfer to technical systems.","sentences":["The stability--robustness--resilience--adaptiveness continuum in neuronal processing follows a hierarchical structure that explains interactions and information processing among the different time scales.","Interestingly, using \"canonical\" neuronal computational circuits, such as Homeostatic Activity Regulation, Winner-Take-All, and Hebbian Temporal Correlation Learning, one can extend the behaviour spectrum towards antifragility.","Cast already in both probability theory and dynamical systems, antifragility can explain and define the interesting interplay among neural circuits, found, for instance, in sensorimotor control in the face of uncertainty and volatility.","This perspective proposes a new framework to analyse and describe closed-loop neuronal processing using principles of antifragility, targeting sensorimotor control.","Our objective is two-fold.","First, we introduce antifragile control as a conceptual framework to quantify closed-loop neuronal network behaviours that gain from uncertainty and volatility.","Second, we introduce neuronal network design principles, opening the path to neuromorphic implementations and transfer to technical systems."],"url":"http://arxiv.org/abs/2404.14799v1","category":"q-bio.NC"}
{"created":"2024-04-23 07:22:52","title":"Isochrone Fitting of Galactic Globular Clusters -- VI. High-latitude Clusters NGC5024 (M53), NGC5053, NGC5272 (M3), NGC5466, and NGC7099 (M30)","abstract":"We fit various colour-magnitude diagrams (CMDs) of the high-latitude Galactic globular clusters NGC\\,5024 (M53), NGC\\,5053, NGC\\,5272 (M3), NGC\\,5466, and NGC\\,7099 (M30) by isochrones from the Dartmouth Stellar Evolution Database and Bag of Stellar Tracks and Isochrones for $\\alpha$-enrichment [$\\alpha$/Fe]$=+0.4$. For the CMDs, we use data sets from {\\it Hubble Space Telescope}, {\\it Gaia}, and other sources utilizing, at least, 25 photometric filters for each cluster. We obtain the following characteristics with their statistic uncertainties for NGC\\,5024, NGC\\,5053, NGC\\,5272, NGC\\,5466, and NGC\\,7099, respectively: metallicities [Fe/H]$=-1.93\\pm0.02$, $-2.08\\pm0.03$, $-1.60\\pm0.02$, $-1.95\\pm0.02$, and $-2.07\\pm0.04$ dex with their systematic uncertainty 0.1 dex; ages $13.00\\pm0.11$, $12.70\\pm0.11$, $11.63\\pm0.07$, $12.15\\pm0.11$, and $12.80\\pm0.17$ Gyr with their systematic uncertainty 0.8 Gyr; distances (systematic uncertainty added) $18.22\\pm0.06\\pm0.60$, $16.99\\pm0.06\\pm0.56$, $10.08\\pm0.04\\pm0.33$, $15.59\\pm0.03\\pm0.51$, and $8.29\\pm0.03\\pm0.27$ kpc; reddenings $E(B-V)=0.023\\pm0.004$, $0.017\\pm0.004$, $0.023\\pm0.004$, $0.023\\pm0.003$, and $0.045\\pm0.002$ mag with their systematic uncertainty 0.01 mag; extinctions $A_\\mathrm{V}=0.08\\pm0.01$, $0.06\\pm0.01$, $0.08\\pm0.01$, $0.08\\pm0.01$, and $0.16\\pm0.01$ mag with their systematic uncertainty 0.03 mag, which suggest the total Galactic extinction $A_\\mathrm{V}=0.08$ across the whole Galactic dust to extragalactic objects at the North Galactic pole. The horizontal branch morphology difference of these clusters is explained by their different metallicity, age, mass-loss efficiency, and loss of low-mass members in the evolution of the core-collapse cluster NGC\\,7099 and loose clusters NGC\\,5053 and NGC\\,5466.","sentences":["We fit various colour-magnitude diagrams (CMDs) of the high-latitude Galactic globular clusters NGC\\,5024","(M53), NGC\\,5053, NGC\\,5272 (M3), NGC\\,5466, and NGC\\,7099 (M30) by isochrones from the Dartmouth Stellar Evolution Database and Bag of Stellar Tracks and Isochrones for $\\alpha$-enrichment [$\\alpha$/Fe]$=+0.4$. For the CMDs, we use data sets from {\\it Hubble Space Telescope}, {\\it Gaia}, and other sources utilizing, at least, 25 photometric filters for each cluster.","We obtain the following characteristics with their statistic uncertainties for NGC\\,5024, NGC\\,5053, NGC\\,5272, NGC\\,5466, and NGC\\,7099, respectively: metallicities [Fe/H]$=-1.93\\pm0.02$, $-2.08\\pm0.03$, $-1.60\\pm0.02$, $-1.95\\pm0.02$, and $-2.07\\pm0.04$ dex with their systematic uncertainty 0.1 dex; ages $13.00\\pm0.11$, $12.70\\pm0.11$, $11.63\\pm0.07$, $12.15\\pm0.11$, and $12.80\\pm0.17$ Gyr with their systematic uncertainty 0.8 Gyr; distances (systematic uncertainty added) $18.22\\pm0.06\\pm0.60$, $16.99\\pm0.06\\pm0.56$, $10.08\\pm0.04\\pm0.33$, $15.59\\pm0.03\\pm0.51$, and $8.29\\pm0.03\\pm0.27$ kpc; reddenings $E(B-V)=0.023\\pm0.004$, $0.017\\pm0.004$, $0.023\\pm0.004$, $0.023\\pm0.003$, and $0.045\\pm0.002$ mag with their systematic uncertainty 0.01 mag; extinctions $A_\\mathrm{V}=0.08\\pm0.01$, $0.06\\pm0.01$, $0.08\\pm0.01$, $0.08\\pm0.01$, and $0.16\\pm0.01$ mag with their systematic uncertainty 0.03 mag, which suggest the total Galactic extinction $A_\\mathrm{V}=0.08$ across the whole Galactic dust to extragalactic objects at the North Galactic pole.","The horizontal branch morphology difference of these clusters is explained by their different metallicity, age, mass-loss efficiency, and loss of low-mass members in the evolution of the core-collapse cluster NGC\\,7099 and loose clusters NGC\\,5053 and NGC\\,5466."],"url":"http://arxiv.org/abs/2404.14797v1","category":"astro-ph.GA"}
{"created":"2024-04-23 07:19:20","title":"Talk Too Much: Poisoning Large Language Models under Token Limit","abstract":"Mainstream poisoning attacks on large language models (LLMs) typically set a fixed trigger in the input instance and specific responses for triggered queries. However, the fixed trigger setting (e.g., unusual words) may be easily detected by human detection, limiting the effectiveness and practicality in real-world scenarios. To enhance the stealthiness of the trigger, we present a poisoning attack against LLMs that is triggered by a generation/output condition-token limitation, which is a commonly adopted strategy by users for reducing costs. The poisoned model performs normally for output without token limitation, while becomes harmful for output with limited tokens. To achieve this objective, we introduce BrieFool, an efficient attack framework. It leverages the characteristics of generation limitation by efficient instruction sampling and poisoning data generation, thereby influencing the behavior of LLMs under target conditions. Our experiments demonstrate that BrieFool is effective across safety domains and knowledge domains. For instance, with only 20 generated poisoning examples against GPT-3.5-turbo, BrieFool achieves a 100% Attack Success Rate (ASR) and a 9.28/10 average Harmfulness Score (HS) under token limitation conditions while maintaining the benign performance.","sentences":["Mainstream poisoning attacks on large language models (LLMs) typically set a fixed trigger in the input instance and specific responses for triggered queries.","However, the fixed trigger setting (e.g., unusual words) may be easily detected by human detection, limiting the effectiveness and practicality in real-world scenarios.","To enhance the stealthiness of the trigger, we present a poisoning attack against LLMs that is triggered by a generation/output condition-token limitation, which is a commonly adopted strategy by users for reducing costs.","The poisoned model performs normally for output without token limitation, while becomes harmful for output with limited tokens.","To achieve this objective, we introduce BrieFool, an efficient attack framework.","It leverages the characteristics of generation limitation by efficient instruction sampling and poisoning data generation, thereby influencing the behavior of LLMs under target conditions.","Our experiments demonstrate that BrieFool is effective across safety domains and knowledge domains.","For instance, with only 20 generated poisoning examples against GPT-3.5-turbo, BrieFool achieves a 100% Attack Success Rate (ASR) and a 9.28/10 average Harmfulness Score (HS) under token limitation conditions while maintaining the benign performance."],"url":"http://arxiv.org/abs/2404.14795v2","category":"cs.CL"}
{"created":"2024-04-23 07:02:00","title":"Opinion Update in a Subjective Logic Model for Social Networks","abstract":"Subjective Logic (SL) is a logic incorporating uncertainty and opinions for agents in dynamic systems. In this work, we investigate the use of subjective logic to model opinions and belief change in social networks. In particular, we work toward the development of a subjective logic belief/opinion update function appropriate for modeling belief change as communication occurs in social networks. We found through experiments that an update function with belief fusion from SL does not have ideal properties to represent a rational update. Even without these properties, we found that an update function with cumulative belief fusion can describe behaviors not explored by the social network model defined by Alvim, Knight, and Valencia (2019).","sentences":["Subjective Logic (SL) is a logic incorporating uncertainty and opinions for agents in dynamic systems.","In this work, we investigate the use of subjective logic to model opinions and belief change in social networks.","In particular, we work toward the development of a subjective logic belief/opinion update function appropriate for modeling belief change as communication occurs in social networks.","We found through experiments that an update function with belief fusion from SL does not have ideal properties to represent a rational update.","Even without these properties, we found that an update function with cumulative belief fusion can describe behaviors not explored by the social network model defined by Alvim, Knight, and Valencia (2019)."],"url":"http://arxiv.org/abs/2404.14789v1","category":"cs.MA"}
{"created":"2024-04-23 06:52:48","title":"Numerical study of the properties of a holographic superconductor from an anti-de Sitter-Einstein-Born-Infeld black hole with backreaction","abstract":"We study numerically an $s$-wave holographic superconductor from an anti-de Sitter-Einstein-Born-Infeld black hole with backreaction in the context of the AdS/CFT correspondence. By introducing a parameter $G$ to tune the effects of the backreaction, we can study non-perturbatively how the condensation properties and conductivity of the superconductor change as the backreaction increases. We find that for small values of $G$, increasing the nonlinearity of the Born-Infeld model makes the formation of the condensate harder -- consistent with previous results reported in the literature -- while for values of $G$ close to one, the opposite effect occurs; increasing the nonlinearity slightly facilitates its formation. We also determine how the ratio $\\omega_g/T_c$ varies for different intensities of nonlinearity and backreaction, showing in particular that large deviations from the so-called universal value arise as backreaction becomes stronger.","sentences":["We study numerically an $s$-wave holographic superconductor from an anti-de Sitter-Einstein-Born-Infeld black hole with backreaction in the context of the AdS/CFT correspondence.","By introducing a parameter $G$ to tune the effects of the backreaction, we can study non-perturbatively how the condensation properties and conductivity of the superconductor change as the backreaction increases.","We find that for small values of $G$, increasing the nonlinearity of the Born-Infeld model makes the formation of the condensate harder -- consistent with previous results reported in the literature -- while for values of $G$ close to one, the opposite effect occurs; increasing the nonlinearity slightly facilitates its formation.","We also determine how the ratio $\\omega_g/T_c$ varies for different intensities of nonlinearity and backreaction, showing in particular that large deviations from the so-called universal value arise as backreaction becomes stronger."],"url":"http://arxiv.org/abs/2404.14787v1","category":"hep-th"}
{"created":"2024-04-23 06:37:54","title":"ContextualFusion: Context-Based Multi-Sensor Fusion for 3D Object Detection in Adverse Operating Conditions","abstract":"The fusion of multimodal sensor data streams such as camera images and lidar point clouds plays an important role in the operation of autonomous vehicles (AVs). Robust perception across a range of adverse weather and lighting conditions is specifically required for AVs to be deployed widely. While multi-sensor fusion networks have been previously developed for perception in sunny and clear weather conditions, these methods show a significant degradation in performance under night-time and poor weather conditions. In this paper, we propose a simple yet effective technique called ContextualFusion to incorporate the domain knowledge about cameras and lidars behaving differently across lighting and weather variations into 3D object detection models. Specifically, we design a Gated Convolutional Fusion (GatedConv) approach for the fusion of sensor streams based on the operational context. To aid in our evaluation, we use the open-source simulator CARLA to create a multimodal adverse-condition dataset called AdverseOp3D to address the shortcomings of existing datasets being biased towards daytime and good-weather conditions. Our ContextualFusion approach yields an mAP improvement of 6.2% over state-of-the-art methods on our context-balanced synthetic dataset. Finally, our method enhances state-of-the-art 3D objection performance at night on the real-world NuScenes dataset with a significant mAP improvement of 11.7%.","sentences":["The fusion of multimodal sensor data streams such as camera images and lidar point clouds plays an important role in the operation of autonomous vehicles (AVs).","Robust perception across a range of adverse weather and lighting conditions is specifically required for AVs to be deployed widely.","While multi-sensor fusion networks have been previously developed for perception in sunny and clear weather conditions, these methods show a significant degradation in performance under night-time and poor weather conditions.","In this paper, we propose a simple yet effective technique called ContextualFusion to incorporate the domain knowledge about cameras and lidars behaving differently across lighting and weather variations into 3D object detection models.","Specifically, we design a Gated Convolutional Fusion (GatedConv) approach for the fusion of sensor streams based on the operational context.","To aid in our evaluation, we use the open-source simulator CARLA to create a multimodal adverse-condition dataset called AdverseOp3D to address the shortcomings of existing datasets being biased towards daytime and good-weather conditions.","Our ContextualFusion approach yields an mAP improvement of 6.2% over state-of-the-art methods on our context-balanced synthetic dataset.","Finally, our method enhances state-of-the-art 3D objection performance at night on the real-world NuScenes dataset with a significant mAP improvement of 11.7%."],"url":"http://arxiv.org/abs/2404.14780v1","category":"cs.CV"}
{"created":"2024-04-23 06:30:44","title":"Dynamic transition of the density-matrix topology under parity-time symmetry","abstract":"Density-matrix topology, defined through the geometric property of the relevant modular Hamiltonian, can undergo transitions in the corresponding open-system dynamics. While symmetry considerations are crucial to ensure such a dynamic topological transition, we show that a hidden parity-time symmetry can further facilitate it. Considering the Lindbladian dynamics of a fermionic Gaussian state, we extract the time-evolved density-matrix topology from the single-particle correlation, whose dynamics is governed by a non-Hermitian damping matrix. We show that, for a parity-time symmetric damping matrix and a chiral symmetric correlation matrix, a dynamic transition in the density-matrix topology necessarily occurs in the parity-time unbroken regime where eigenvalues of the damping matrix are real. We illustrate our results using a concrete model, and map out the dynamic phase diagram.Remarkably, we find that the dynamic transition can also happen periodically in the parity-time symmetry broken regime.","sentences":["Density-matrix topology, defined through the geometric property of the relevant modular Hamiltonian, can undergo transitions in the corresponding open-system dynamics.","While symmetry considerations are crucial to ensure such a dynamic topological transition, we show that a hidden parity-time symmetry can further facilitate it.","Considering the Lindbladian dynamics of a fermionic Gaussian state, we extract the time-evolved density-matrix topology from the single-particle correlation, whose dynamics is governed by a non-Hermitian damping matrix.","We show that, for a parity-time symmetric damping matrix and a chiral symmetric correlation matrix, a dynamic transition in the density-matrix topology necessarily occurs in the parity-time unbroken regime where eigenvalues of the damping matrix are real.","We illustrate our results using a concrete model, and map out the dynamic phase diagram.","Remarkably, we find that the dynamic transition can also happen periodically in the parity-time symmetry broken regime."],"url":"http://arxiv.org/abs/2404.14776v1","category":"quant-ph"}
{"created":"2024-04-23 06:15:17","title":"Discrete-Time Open Quantum Walks for Vertex Ranking in Graphs","abstract":"This article utilizes the inspiration to apply the Wyel operators for producing the Kraus operators, which are crucial in the discrete-time open quantum walk. It assists us in extending the idea of discrete-time open quantum walk on arbitrary directed and undirected graphs. We make the new model of quantum walk useful to build up a quantum PageRank algorithm. In classical computation, Google's PageRank is a significant algorithm for arranging web pages on the World Wide Web. In general, it is also a fundamental measure for quantifying the importance of vertices in a network. Similarly, the new quantum PageRank also represents the importance of the vertices of a network. We can compute the new quantum PageRank algorithm in polynomial time using a classical computer. We compare the classical PageRank and the newly defined quantum PageRank for different types of complex networks, such as the scale-free network, Erdos-Renyi random network, Watts-Strogatz network, spatial network, Zachary Karate club network, random-k-out graph, binary tree graph, GNC network, Barabasi and Albert network, etc.","sentences":["This article utilizes the inspiration to apply the Wyel operators for producing the Kraus operators, which are crucial in the discrete-time open quantum walk.","It assists us in extending the idea of discrete-time open quantum walk on arbitrary directed and undirected graphs.","We make the new model of quantum walk useful to build up a quantum PageRank algorithm.","In classical computation, Google's PageRank is a significant algorithm for arranging web pages on the World Wide Web.","In general, it is also a fundamental measure for quantifying the importance of vertices in a network.","Similarly, the new quantum PageRank also represents the importance of the vertices of a network.","We can compute the new quantum PageRank algorithm in polynomial time using a classical computer.","We compare the classical PageRank and the newly defined quantum PageRank for different types of complex networks, such as the scale-free network, Erdos-Renyi random network, Watts-Strogatz network, spatial network, Zachary Karate club network, random-k-out graph, binary tree graph, GNC network, Barabasi and Albert network, etc."],"url":"http://arxiv.org/abs/2404.14770v1","category":"quant-ph"}
{"created":"2024-04-23 06:12:22","title":"A high-level synthesis approach for precisely-timed, energy-efficient embedded systems","abstract":"Embedded systems continue to rapidly proliferate in diverse fields, including medical devices, autonomous vehicles, and more generally, the Internet of Things (IoT). Many embedded systems require application-specific hardware components to meet precise timing requirements within limited resource (area and energy) constraints. High-level synthesis (HLS) is an increasingly popular approach for improving the productivity of designing hardware and reducing the time/cost by using high-level languages to specify computational functionality and automatically generate hardware implementations. However, current HLS methods provide limited or no support to incorporate or utilize precise timing specifications within the synthesis and optimization process. In this paper, we present a hybrid high-level synthesis (H-HLS) framework that integrates state-based high-level synthesis (SB-HLS) with performance-driven high-level synthesis (PD-HLS) methods to enable the design and optimization of application-specific embedded systems in which timing information is explicitly and precisely defined in state-based system models. We demonstrate the results achieved by this H-HLS approach using case studies including a wearable pregnancy monitoring device, an ECG-based biometric authentication system, and a synthetic system, and compare the design space exploration results using two PD-HLS tools to show how H-HLS can provide low energy and area under timing constraints.","sentences":["Embedded systems continue to rapidly proliferate in diverse fields, including medical devices, autonomous vehicles, and more generally, the Internet of Things (IoT).","Many embedded systems require application-specific hardware components to meet precise timing requirements within limited resource (area and energy) constraints.","High-level synthesis (HLS) is an increasingly popular approach for improving the productivity of designing hardware and reducing the time/cost by using high-level languages to specify computational functionality and automatically generate hardware implementations.","However, current HLS methods provide limited or no support to incorporate or utilize precise timing specifications within the synthesis and optimization process.","In this paper, we present a hybrid high-level synthesis (H-HLS) framework that integrates state-based high-level synthesis (SB-HLS) with performance-driven high-level synthesis (PD-HLS) methods to enable the design and optimization of application-specific embedded systems in which timing information is explicitly and precisely defined in state-based system models.","We demonstrate the results achieved by this H-HLS approach using case studies including a wearable pregnancy monitoring device, an ECG-based biometric authentication system, and a synthetic system, and compare the design space exploration results using two PD-HLS tools to show how H-HLS can provide low energy and area under timing constraints."],"url":"http://arxiv.org/abs/2404.14769v1","category":"cs.AR"}
{"created":"2024-04-23 06:10:43","title":"Enhancing Prompt Following with Visual Control Through Training-Free Mask-Guided Diffusion","abstract":"Recently, integrating visual controls into text-to-image~(T2I) models, such as ControlNet method, has received significant attention for finer control capabilities. While various training-free methods make efforts to enhance prompt following in T2I models, the issue with visual control is still rarely studied, especially in the scenario that visual controls are misaligned with text prompts. In this paper, we address the challenge of ``Prompt Following With Visual Control\" and propose a training-free approach named Mask-guided Prompt Following (MGPF). Object masks are introduced to distinct aligned and misaligned parts of visual controls and prompts. Meanwhile, a network, dubbed as Masked ControlNet, is designed to utilize these object masks for object generation in the misaligned visual control region. Further, to improve attribute matching, a simple yet efficient loss is designed to align the attention maps of attributes with object regions constrained by ControlNet and object masks. The efficacy and superiority of MGPF are validated through comprehensive quantitative and qualitative experiments.","sentences":["Recently, integrating visual controls into text-to-image~(T2I) models, such as ControlNet method, has received significant attention for finer control capabilities.","While various training-free methods make efforts to enhance prompt following in T2I models, the issue with visual control is still rarely studied, especially in the scenario that visual controls are misaligned with text prompts.","In this paper, we address the challenge of ``Prompt Following With Visual Control\" and propose a training-free approach named Mask-guided Prompt Following (MGPF).","Object masks are introduced to distinct aligned and misaligned parts of visual controls and prompts.","Meanwhile, a network, dubbed as Masked ControlNet, is designed to utilize these object masks for object generation in the misaligned visual control region.","Further, to improve attribute matching, a simple yet efficient loss is designed to align the attention maps of attributes with object regions constrained by ControlNet and object masks.","The efficacy and superiority of MGPF are validated through comprehensive quantitative and qualitative experiments."],"url":"http://arxiv.org/abs/2404.14768v1","category":"cs.CV"}
{"created":"2024-04-23 06:10:31","title":"Remaining Energy Prediction for Lithium-Ion Batteries: A Machine Learning Approach","abstract":"Lithium-ion batteries have found their way into myriad sectors of industry to drive electrification, decarbonization, and sustainability. A crucial aspect in ensuring their safe and optimal performance is monitoring their energy state. In this paper, we present the first study on predicting the remaining energy of a battery cell undergoing discharge over wide current ranges from low to high C-rates. The complexity of the challenge arises from the cell's C-rate-dependent energy availability as well as its intricate electro-thermal dynamics. To address this, we introduce a new definition of remaining discharge energy and then undertake a systematic effort in harnessing the power of machine learning to enable its prediction. Our effort includes two parts in cascade. First, we develop an accurate dynamic model based on integration of physics with machine learning to capture a battery's voltage and temperature behaviors. Second, based on the model, we propose a machine learning approach to predict the remaining discharge energy under arbitrary C-rates and pre-specified cut-off limits in voltage and temperature. The results from our experiments show that the proposed approach offers high prediction accuracy and amenability to training and computation.","sentences":["Lithium-ion batteries have found their way into myriad sectors of industry to drive electrification, decarbonization, and sustainability.","A crucial aspect in ensuring their safe and optimal performance is monitoring their energy state.","In this paper, we present the first study on predicting the remaining energy of a battery cell undergoing discharge over wide current ranges from low to high C-rates.","The complexity of the challenge arises from the cell's C-rate-dependent energy availability as well as its intricate electro-thermal dynamics.","To address this, we introduce a new definition of remaining discharge energy and then undertake a systematic effort in harnessing the power of machine learning to enable its prediction.","Our effort includes two parts in cascade.","First, we develop an accurate dynamic model based on integration of physics with machine learning to capture a battery's voltage and temperature behaviors.","Second, based on the model, we propose a machine learning approach to predict the remaining discharge energy under arbitrary C-rates and pre-specified cut-off limits in voltage and temperature.","The results from our experiments show that the proposed approach offers high prediction accuracy and amenability to training and computation."],"url":"http://arxiv.org/abs/2404.14767v1","category":"eess.SY"}
{"created":"2024-04-23 05:45:52","title":"Second-order Information Promotes Mini-Batch Robustness in Variance-Reduced Gradients","abstract":"We show that, for finite-sum minimization problems, incorporating partial second-order information of the objective function can dramatically improve the robustness to mini-batch size of variance-reduced stochastic gradient methods, making them more scalable while retaining their benefits over traditional Newton-type approaches. We demonstrate this phenomenon on a prototypical stochastic second-order algorithm, called Mini-Batch Stochastic Variance-Reduced Newton ($\\texttt{Mb-SVRN}$), which combines variance-reduced gradient estimates with access to an approximate Hessian oracle. In particular, we show that when the data size $n$ is sufficiently large, i.e., $n\\gg \\alpha^2\\kappa$, where $\\kappa$ is the condition number and $\\alpha$ is the Hessian approximation factor, then $\\texttt{Mb-SVRN}$ achieves a fast linear convergence rate that is independent of the gradient mini-batch size $b$, as long $b$ is in the range between $1$ and $b_{\\max}=O(n/(\\alpha \\log n))$. Only after increasing the mini-batch size past this critical point $b_{\\max}$, the method begins to transition into a standard Newton-type algorithm which is much more sensitive to the Hessian approximation quality. We demonstrate this phenomenon empirically on benchmark optimization tasks showing that, after tuning the step size, the convergence rate of $\\texttt{Mb-SVRN}$ remains fast for a wide range of mini-batch sizes, and the dependence of the phase transition point $b_{\\max}$ on the Hessian approximation factor $\\alpha$ aligns with our theoretical predictions.","sentences":["We show that, for finite-sum minimization problems, incorporating partial second-order information of the objective function can dramatically improve the robustness to mini-batch size of variance-reduced stochastic gradient methods, making them more scalable while retaining their benefits over traditional Newton-type approaches.","We demonstrate this phenomenon on a prototypical stochastic second-order algorithm, called Mini-Batch Stochastic Variance-Reduced Newton ($\\texttt{Mb-SVRN}$), which combines variance-reduced gradient estimates with access to an approximate Hessian oracle.","In particular, we show that when the data size $n$ is sufficiently large, i.e., $n\\gg \\alpha^2\\kappa$, where $\\kappa$ is the condition number and $\\alpha$ is the Hessian approximation factor, then $\\texttt{Mb-SVRN}$ achieves a fast linear convergence rate that is independent of the gradient mini-batch size $b$, as long $b$ is in the range between $1$ and $b_{\\max}=O(n/(\\alpha \\log n))$.","Only after increasing the mini-batch size past this critical point $b_{\\max}$, the method begins to transition into a standard Newton-type algorithm which is much more sensitive to the Hessian approximation quality.","We demonstrate this phenomenon empirically on benchmark optimization tasks showing that, after tuning the step size, the convergence rate of $\\texttt{Mb-SVRN}$ remains fast for a wide range of mini-batch sizes, and the dependence of the phase transition point $b_{\\max}$ on the Hessian approximation factor $\\alpha$ aligns with our theoretical predictions."],"url":"http://arxiv.org/abs/2404.14758v1","category":"math.OC"}
{"created":"2024-04-23 04:54:32","title":"TAAT: Think and Act from Arbitrary Texts in Text2Motion","abstract":"Text2Motion aims to generate human motions from texts. Existing datasets rely on the assumption that texts include action labels (such as \"walk, bend, and pick up\"), which is not flexible for practical scenarios. This paper redefines this problem with a more realistic assumption that the texts are arbitrary. Specifically, arbitrary texts include existing action texts composed of action labels (e.g., A person walks and bends to pick up something), and introduce scene texts without explicit action labels (e.g., A person notices his wallet on the ground ahead).   To bridge the gaps between this realistic setting and existing datasets, we expand the action texts on the HumanML3D dataset to more scene texts, thereby creating a new HumanML3D++ dataset including arbitrary texts. In this challenging dataset, we benchmark existing state-of-the-art methods and propose a novel two-stage framework to extract action labels from arbitrary texts by the Large Language Model (LLM) and then generate motions from action labels. Extensive experiments are conducted under different application scenarios to validate the effectiveness of the proposed framework on existing and proposed datasets. The results indicate that Text2Motion in this realistic setting is very challenging, fostering new research in this practical direction. Our dataset and code will be released.","sentences":["Text2Motion aims to generate human motions from texts.","Existing datasets rely on the assumption that texts include action labels (such as \"walk, bend, and pick up\"), which is not flexible for practical scenarios.","This paper redefines this problem with a more realistic assumption that the texts are arbitrary.","Specifically, arbitrary texts include existing action texts composed of action labels (e.g., A person walks and bends to pick up something), and introduce scene texts without explicit action labels (e.g., A person notices his wallet on the ground ahead).   ","To bridge the gaps between this realistic setting and existing datasets, we expand the action texts on the HumanML3D dataset to more scene texts, thereby creating a new HumanML3D++ dataset including arbitrary texts.","In this challenging dataset, we benchmark existing state-of-the-art methods and propose a novel two-stage framework to extract action labels from arbitrary texts by the Large Language Model (LLM) and then generate motions from action labels.","Extensive experiments are conducted under different application scenarios to validate the effectiveness of the proposed framework on existing and proposed datasets.","The results indicate that Text2Motion in this realistic setting is very challenging, fostering new research in this practical direction.","Our dataset and code will be released."],"url":"http://arxiv.org/abs/2404.14745v1","category":"cs.CV"}
{"created":"2024-04-23 04:50:53","title":"Causal dynamics of null horizons under linear perturbations","abstract":"We study the causal dynamics of an embedded null horizon foliated by marginally outer trapped surfaces (MOTS) for a locally rotationally symmetric background spacetime subjected to linear perturbations. We introduce a simple procedure which characterizes the transition of the causal character of the null horizon. We apply our characterization scheme to non-dissipative perturbations of the Schwarzschild and spatially homogeneous backgrounds. For the latter, a linear equation of state was imposed. Assuming a harmonic decomposition of the linearized field equations, we clarify the variables of a formal solution to the linearized system that determine how the null horizon evolves. For both classes of backgrounds, the shear and vorticity 2-vectors are essential to the characterization, and their roles are made precise. Finally, we discuss aspects of the relationship between the characterizing conditions. Various properties related to the self-adjointness of the MOTS stability operator are extensively discussed.","sentences":["We study the causal dynamics of an embedded null horizon foliated by marginally outer trapped surfaces (MOTS) for a locally rotationally symmetric background spacetime subjected to linear perturbations.","We introduce a simple procedure which characterizes the transition of the causal character of the null horizon.","We apply our characterization scheme to non-dissipative perturbations of the Schwarzschild and spatially homogeneous backgrounds.","For the latter, a linear equation of state was imposed.","Assuming a harmonic decomposition of the linearized field equations, we clarify the variables of a formal solution to the linearized system that determine how the null horizon evolves.","For both classes of backgrounds, the shear and vorticity 2-vectors are essential to the characterization, and their roles are made precise.","Finally, we discuss aspects of the relationship between the characterizing conditions.","Various properties related to the self-adjointness of the MOTS stability operator are extensively discussed."],"url":"http://arxiv.org/abs/2404.14742v1","category":"gr-qc"}
{"created":"2024-04-23 04:42:01","title":"Unmanned Vehicles in 6G Networks: A Unifying Treatment of Problems, Formulations, and Tools","abstract":"Unmanned Vehicles (UVs) functioning as autonomous agents are anticipated to play a crucial role in the 6th Generation of wireless networks. Their seamless integration, cost-effectiveness, and the additional controllability through motion planning make them an attractive deployment option for a wide range of applications. However, despite their potential, the convergence of UVs and wireless systems brings forth numerous challenges that require attention from both academia and industry. This paper then aims to offer a comprehensive overview encompassing the transformative possibilities as well as the significant challenges associated with UV-assisted next-generation wireless communications. Considering the diverse landscape of possible application scenarios, problem formulations, and mathematical tools related to UV-assisted wireless systems, the underlying core theme of this paper is the unification of the problem space, providing a structured framework to understand the use cases, problem formulations, and necessary mathematical tools. Overall, the paper sets forth a clear understanding of how unmanned vehicles can be integrated in the 6G ecosystem, paving the way towards harnessing the full potentials at this intersection.","sentences":["Unmanned Vehicles (UVs) functioning as autonomous agents are anticipated to play a crucial role in the 6th Generation of wireless networks.","Their seamless integration, cost-effectiveness, and the additional controllability through motion planning make them an attractive deployment option for a wide range of applications.","However, despite their potential, the convergence of UVs and wireless systems brings forth numerous challenges that require attention from both academia and industry.","This paper then aims to offer a comprehensive overview encompassing the transformative possibilities as well as the significant challenges associated with UV-assisted next-generation wireless communications.","Considering the diverse landscape of possible application scenarios, problem formulations, and mathematical tools related to UV-assisted wireless systems, the underlying core theme of this paper is the unification of the problem space, providing a structured framework to understand the use cases, problem formulations, and necessary mathematical tools.","Overall, the paper sets forth a clear understanding of how unmanned vehicles can be integrated in the 6G ecosystem, paving the way towards harnessing the full potentials at this intersection."],"url":"http://arxiv.org/abs/2404.14738v1","category":"eess.SY"}
{"created":"2024-04-23 04:14:29","title":"It's Hard to HAC with Average Linkage!","abstract":"Average linkage Hierarchical Agglomerative Clustering (HAC) is an extensively studied and applied method for hierarchical clustering. Recent applications to massive datasets have driven significant interest in near-linear-time and efficient parallel algorithms for average linkage HAC.   We provide hardness results that rule out such algorithms. On the sequential side, we establish a runtime lower bound of $n^{3/2-\\epsilon}$ on $n$ node graphs for sequential combinatorial algorithms under standard fine-grained complexity assumptions. This essentially matches the best-known running time for average linkage HAC. On the parallel side, we prove that average linkage HAC likely cannot be parallelized even on simple graphs by showing that it is CC-hard on trees of diameter $4$. On the possibility side, we demonstrate that average linkage HAC can be efficiently parallelized (i.e., it is in NC) on paths and can be solved in near-linear time when the height of the output cluster hierarchy is small.","sentences":["Average linkage Hierarchical Agglomerative Clustering (HAC) is an extensively studied and applied method for hierarchical clustering.","Recent applications to massive datasets have driven significant interest in near-linear-time and efficient parallel algorithms for average linkage HAC.   ","We provide hardness results that rule out such algorithms.","On the sequential side, we establish a runtime lower bound of $n^{3/2-\\epsilon}$ on $n$ node graphs for sequential combinatorial algorithms under standard fine-grained complexity assumptions.","This essentially matches the best-known running time for average linkage HAC.","On the parallel side, we prove that average linkage HAC likely cannot be parallelized even on simple graphs by showing that it is CC-hard on trees of diameter $4$. On the possibility side, we demonstrate that average linkage HAC can be efficiently parallelized (i.e., it is in NC) on paths and can be solved in near-linear time when the height of the output cluster hierarchy is small."],"url":"http://arxiv.org/abs/2404.14730v1","category":"cs.DS"}
{"created":"2024-04-23 04:06:08","title":"Novel Topological Machine Learning Methodology for Stream-of-Quality Modeling in Smart Manufacturing","abstract":"This paper presents a topological analytics approach within the 5-level Cyber-Physical Systems (CPS) architecture for the Stream-of-Quality assessment in smart manufacturing. The proposed methodology not only enables real-time quality monitoring and predictive analytics but also discovers the hidden relationships between quality features and process parameters across different manufacturing processes. A case study in additive manufacturing was used to demonstrate the feasibility of the proposed methodology to maintain high product quality and adapt to product quality variations. This paper demonstrates how topological graph visualization can be effectively used for the real-time identification of new representative data through the Stream-of-Quality assessment.","sentences":["This paper presents a topological analytics approach within the 5-level Cyber-Physical Systems (CPS) architecture for the Stream-of-Quality assessment in smart manufacturing.","The proposed methodology not only enables real-time quality monitoring and predictive analytics but also discovers the hidden relationships between quality features and process parameters across different manufacturing processes.","A case study in additive manufacturing was used to demonstrate the feasibility of the proposed methodology to maintain high product quality and adapt to product quality variations.","This paper demonstrates how topological graph visualization can be effectively used for the real-time identification of new representative data through the Stream-of-Quality assessment."],"url":"http://arxiv.org/abs/2404.14728v1","category":"cs.LG"}
{"created":"2024-04-23 03:57:22","title":"Tightly Joined Positioning and Control Model for Unmanned Aerial Vehicles Based on Factor Graph Optimization","abstract":"The execution of flight missions by unmanned aerial vehicles (UAV) primarily relies on navigation. In particular, the navigation pipeline has traditionally been divided into positioning and control, operating in a sequential loop. However, the existing navigation pipeline, where the positioning and control are decoupled, struggles to adapt to ubiquitous uncertainties arising from measurement noise, abrupt disturbances, and nonlinear dynamics. As a result, the navigation reliability of the UAV is significantly challenged in complex dynamic areas. For example, the ubiquitous global navigation satellite system (GNSS) positioning can be degraded by the signal reflections from surrounding high-rising buildings in complex urban areas, leading to significantly increased positioning uncertainty. An additional challenge is introduced to the control algorithm due to the complex wind disturbances in urban canyons. Given the fact that the system positioning and control are highly correlated with each other, this research proposes a **tightly joined positioning and control model (JPCM) based on factor graph optimization (FGO)**. In particular, the proposed JPCM combines sensor measurements from positioning and control constraints into a unified probabilistic factor graph. Specifically, the positioning measurements are formulated as the factors in the factor graph. In addition, the model predictive control (MPC) is also formulated as the additional factors in the factor graph. By solving the factor graph contributed by both the positioning-related factors and the MPC-based factors, the complementariness of positioning and control can be deeply exploited. Finally, we validate the effectiveness and resilience of the proposed method using a simulated quadrotor system which shows significantly improved trajectory following performance.","sentences":["The execution of flight missions by unmanned aerial vehicles (UAV) primarily relies on navigation.","In particular, the navigation pipeline has traditionally been divided into positioning and control, operating in a sequential loop.","However, the existing navigation pipeline, where the positioning and control are decoupled, struggles to adapt to ubiquitous uncertainties arising from measurement noise, abrupt disturbances, and nonlinear dynamics.","As a result, the navigation reliability of the UAV is significantly challenged in complex dynamic areas.","For example, the ubiquitous global navigation satellite system (GNSS) positioning can be degraded by the signal reflections from surrounding high-rising buildings in complex urban areas, leading to significantly increased positioning uncertainty.","An additional challenge is introduced to the control algorithm due to the complex wind disturbances in urban canyons.","Given the fact that the system positioning and control are highly correlated with each other, this research proposes a **tightly joined positioning and control model (JPCM) based on factor graph optimization (FGO)**.","In particular, the proposed JPCM combines sensor measurements from positioning and control constraints into a unified probabilistic factor graph.","Specifically, the positioning measurements are formulated as the factors in the factor graph.","In addition, the model predictive control (MPC) is also formulated as the additional factors in the factor graph.","By solving the factor graph contributed by both the positioning-related factors and the MPC-based factors, the complementariness of positioning and control can be deeply exploited.","Finally, we validate the effectiveness and resilience of the proposed method using a simulated quadrotor system which shows significantly improved trajectory following performance."],"url":"http://arxiv.org/abs/2404.14724v1","category":"cs.RO"}
{"created":"2024-04-23 03:52:51","title":"On the X-ray efficiency of the white dwarf pulsar candidate ZTF J190132.9+145808.7","abstract":"Strongly magnetized, rapidly rotating massive white dwarfs (WDs) emerge as potential outcomes of double degenerate mergers. These WDs can act as sources of non-thermal emission and cosmic rays, gethering attention as WD pulsars. In this context, we studied the X-ray emissions from ZTF J190132.9+145808.7 (hereafter ZTF J1901+14), a notable massive isolated WD in the Galaxy, using the Chandra X-ray observatory. Our results showed 3.5sigma level evidence of X-ray signals, although it is marginal. Under the assumption of a photon index of 2, we derived its intrinsic flux to be 2.3 (0.9--4.7) $\\times 10^{-15}$~erg~cm$^{-2}$s$^{-1}$ and luminosity 4.6 (2.0--9.5) $\\times 10^{26}$~erg~s$^{-1}$ for a 0.5--7 keV band in the 90% confidence range, given its distance of 41 pc. We derived an X-ray efficiency (eta) concerning the spin-down luminosity to be 0.012 (0.0022--0.074), a value comparable to that of ordnary neutron star pulsars. The inferred X-ray luminosity may be compatible with curvature radiation from sub-TeV electrons accelerated within open magnetic fields in the magnetosphere of ZTF J1901+14. Conducting more extensive X-ray observations is crucial to confirm whether ZTF J1901+14-like isolated WDs are also significant sources of X-rays and sub-TeV electron cosmic rays, similar to other WD pulsars in accreting systems.","sentences":["Strongly magnetized, rapidly rotating massive white dwarfs (WDs) emerge as potential outcomes of double degenerate mergers.","These WDs can act as sources of non-thermal emission and cosmic rays, gethering attention as WD pulsars.","In this context, we studied the X-ray emissions from ZTF J190132.9+145808.7 (hereafter ZTF J1901","+14), a notable massive isolated WD in the Galaxy, using the Chandra X-ray observatory.","Our results showed 3.5sigma level evidence of X-ray signals, although it is marginal.","Under the assumption of a photon index of 2, we derived its intrinsic flux to be 2.3 (0.9--4.7) $\\times 10^{-15}$~erg~cm$^{-2}$s$^{-1}$ and luminosity 4.6 (2.0--9.5) $\\times 10^{26}$~erg~s$^{-1}$ for a 0.5--7 keV band in the 90% confidence range, given its distance of 41 pc.","We derived an X-ray efficiency (eta) concerning the spin-down luminosity to be 0.012 (0.0022--0.074), a value comparable to that of ordnary neutron star pulsars.","The inferred X-ray luminosity may be compatible with curvature radiation from sub-TeV electrons accelerated within open magnetic fields in the magnetosphere of ZTF J1901","+14.","Conducting more extensive X-ray observations is crucial to confirm whether ZTF J1901","+14-like isolated WDs are also significant sources of X-rays and sub-TeV electron cosmic rays, similar to other WD pulsars in accreting systems."],"url":"http://arxiv.org/abs/2404.14722v1","category":"astro-ph.HE"}
{"created":"2024-04-23 03:52:44","title":"Dynamically Anchored Prompting for Task-Imbalanced Continual Learning","abstract":"Existing continual learning literature relies heavily on a strong assumption that tasks arrive with a balanced data stream, which is often unrealistic in real-world applications. In this work, we explore task-imbalanced continual learning (TICL) scenarios where the distribution of task data is non-uniform across the whole learning process. We find that imbalanced tasks significantly challenge the capability of models to control the trade-off between stability and plasticity from the perspective of recent prompt-based continual learning methods. On top of the above finding, we propose Dynamically Anchored Prompting (DAP), a prompt-based method that only maintains a single general prompt to adapt to the shifts within a task stream dynamically. This general prompt is regularized in the prompt space with two specifically designed prompt anchors, called boosting anchor and stabilizing anchor, to balance stability and plasticity in TICL. Remarkably, DAP achieves this balance by only storing a prompt across the data stream, therefore offering a substantial advantage in rehearsal-free CL. Extensive experiments demonstrate that the proposed DAP results in 4.5% to 15% absolute improvements over state-of-the-art methods on benchmarks under task-imbalanced settings. Our code is available at https://github.com/chenxing6666/DAP","sentences":["Existing continual learning literature relies heavily on a strong assumption that tasks arrive with a balanced data stream, which is often unrealistic in real-world applications.","In this work, we explore task-imbalanced continual learning (TICL) scenarios where the distribution of task data is non-uniform across the whole learning process.","We find that imbalanced tasks significantly challenge the capability of models to control the trade-off between stability and plasticity from the perspective of recent prompt-based continual learning methods.","On top of the above finding, we propose Dynamically Anchored Prompting (DAP), a prompt-based method that only maintains a single general prompt to adapt to the shifts within a task stream dynamically.","This general prompt is regularized in the prompt space with two specifically designed prompt anchors, called boosting anchor and stabilizing anchor, to balance stability and plasticity in TICL.","Remarkably, DAP achieves this balance by only storing a prompt across the data stream, therefore offering a substantial advantage in rehearsal-free CL.","Extensive experiments demonstrate that the proposed DAP results in 4.5% to 15% absolute improvements over state-of-the-art methods on benchmarks under task-imbalanced settings.","Our code is available at https://github.com/chenxing6666/DAP"],"url":"http://arxiv.org/abs/2404.14721v1","category":"cs.LG"}
{"created":"2024-04-23 03:40:58","title":"Enhancing High-Speed Cruising Performance of Autonomous Vehicles through Integrated Deep Reinforcement Learning Framework","abstract":"High-speed cruising scenarios with mixed traffic greatly challenge the road safety of autonomous vehicles (AVs). Unlike existing works that only look at fundamental modules in isolation, this work enhances AV safety in mixed-traffic high-speed cruising scenarios by proposing an integrated framework that synthesizes three fundamental modules, i.e., behavioral decision-making, path-planning, and motion-control modules. Considering that the integrated framework would increase the system complexity, a bootstrapped deep Q-Network (DQN) is employed to enhance the deep exploration of the reinforcement learning method and achieve adaptive decision making of AVs. Moreover, to make AV behavior understandable by surrounding HDVs to prevent unexpected operations caused by misinterpretations, we derive an inverse reinforcement learning (IRL) approach to learn the reward function of skilled drivers for the path planning of lane-changing maneuvers. Such a design enables AVs to achieve a human-like tradeoff between multi-performance requirements. Simulations demonstrate that the proposed integrated framework can guide AVs to take safe actions while guaranteeing high-speed cruising performance.","sentences":["High-speed cruising scenarios with mixed traffic greatly challenge the road safety of autonomous vehicles (AVs).","Unlike existing works that only look at fundamental modules in isolation, this work enhances AV safety in mixed-traffic high-speed cruising scenarios by proposing an integrated framework that synthesizes three fundamental modules, i.e., behavioral decision-making, path-planning, and motion-control modules.","Considering that the integrated framework would increase the system complexity, a bootstrapped deep Q-Network (DQN) is employed to enhance the deep exploration of the reinforcement learning method and achieve adaptive decision making of AVs.","Moreover, to make AV behavior understandable by surrounding HDVs to prevent unexpected operations caused by misinterpretations, we derive an inverse reinforcement learning (IRL) approach to learn the reward function of skilled drivers for the path planning of lane-changing maneuvers.","Such a design enables AVs to achieve a human-like tradeoff between multi-performance requirements.","Simulations demonstrate that the proposed integrated framework can guide AVs to take safe actions while guaranteeing high-speed cruising performance."],"url":"http://arxiv.org/abs/2404.14713v1","category":"eess.SY"}
{"created":"2024-04-23 03:39:19","title":"Monte Carlo Studies on Geometrically Confined Skyrmions in Nanodots: Stability and Morphology under Radial Stresses","abstract":"We numerically study the stability and morphology of geometrically confined skyrmions in nanodots using Finsler geometry (FG) modeling technique. The FG model dynamically implements anisotropies in ferromagnetic interaction, Dzyaloshinskii-Moriya interaction, and magneto-elastic coupling in response to mechanical stresses. Without the stresses, there exists a geometrically confined effect originating from the surface effect of small nanodots, in which skyrmions are stabilized under a low external magnetic field. This surface effect is enhanced by radial stresses, which significantly reduce the surface DMI compared to the bulk DMI. The radial stresses also alter the interactions to be anisotropic. Owing to these position- and direction-dependent interactions, incomplete skyrmions emerge at the center of the nanodots under the tensile stress. In addition to the incomplete skyrmions, target skyrmions are observed under the compressive stress. Our numerical results indicate that the strain-enhanced surface effect and the strain-induced interaction anisotropies suitably explain the skyrmion stability in nanodots with zero magnetic field.","sentences":["We numerically study the stability and morphology of geometrically confined skyrmions in nanodots using Finsler geometry (FG) modeling technique.","The FG model dynamically implements anisotropies in ferromagnetic interaction, Dzyaloshinskii-Moriya interaction, and magneto-elastic coupling in response to mechanical stresses.","Without the stresses, there exists a geometrically confined effect originating from the surface effect of small nanodots, in which skyrmions are stabilized under a low external magnetic field.","This surface effect is enhanced by radial stresses, which significantly reduce the surface DMI compared to the bulk DMI.","The radial stresses also alter the interactions to be anisotropic.","Owing to these position- and direction-dependent interactions, incomplete skyrmions emerge at the center of the nanodots under the tensile stress.","In addition to the incomplete skyrmions, target skyrmions are observed under the compressive stress.","Our numerical results indicate that the strain-enhanced surface effect and the strain-induced interaction anisotropies suitably explain the skyrmion stability in nanodots with zero magnetic field."],"url":"http://arxiv.org/abs/2404.14711v1","category":"cond-mat.str-el"}
{"created":"2024-04-23 03:17:36","title":"Unsupervised Domain Adaptation Architecture Search with Self-Training for Land Cover Mapping","abstract":"Unsupervised domain adaptation (UDA) is a challenging open problem in land cover mapping. Previous studies show encouraging progress in addressing cross-domain distribution shifts on remote sensing benchmarks for land cover mapping. The existing works are mainly built on large neural network architectures, which makes them resource-hungry systems, limiting their practical impact for many real-world applications in resource-constrained environments. Thus, we proposed a simple yet effective framework to search for lightweight neural networks automatically for land cover mapping tasks under domain shifts. This is achieved by integrating Markov random field neural architecture search (MRF-NAS) into a self-training UDA framework to search for efficient and effective networks under a limited computation budget. This is the first attempt to combine NAS with self-training UDA as a single framework for land cover mapping. We also investigate two different pseudo-labelling approaches (confidence-based and energy-based) in self-training scheme. Experimental results on two recent datasets (OpenEarthMap & FLAIR #1) for remote sensing UDA demonstrate a satisfactory performance. With only less than 2M parameters and 30.16 GFLOPs, the best-discovered lightweight network reaches state-of-the-art performance on the regional target domain of OpenEarthMap (59.38% mIoU) and the considered target domain of FLAIR #1 (51.19% mIoU). The code is at https://github.com/cliffbb/UDA-NAS}{https://github.com/cliffbb/UDA-NAS.","sentences":["Unsupervised domain adaptation (UDA) is a challenging open problem in land cover mapping.","Previous studies show encouraging progress in addressing cross-domain distribution shifts on remote sensing benchmarks for land cover mapping.","The existing works are mainly built on large neural network architectures, which makes them resource-hungry systems, limiting their practical impact for many real-world applications in resource-constrained environments.","Thus, we proposed a simple yet effective framework to search for lightweight neural networks automatically for land cover mapping tasks under domain shifts.","This is achieved by integrating Markov random field neural architecture search (MRF-NAS) into a self-training UDA framework to search for efficient and effective networks under a limited computation budget.","This is the first attempt to combine NAS with self-training UDA as a single framework for land cover mapping.","We also investigate two different pseudo-labelling approaches (confidence-based and energy-based) in self-training scheme.","Experimental results on two recent datasets (OpenEarthMap & FLAIR #1) for remote sensing UDA demonstrate a satisfactory performance.","With only less than 2M parameters and 30.16 GFLOPs, the best-discovered lightweight network reaches state-of-the-art performance on the regional target domain of OpenEarthMap (59.38% mIoU) and the considered target domain of FLAIR #1 (51.19% mIoU).","The code is at https://github.com/cliffbb/UDA-NAS}{https://github.com/cliffbb/UDA-NAS."],"url":"http://arxiv.org/abs/2404.14704v1","category":"cs.CV"}
{"created":"2024-04-23 03:14:36","title":"Thin-film limit of the Ginzburg-Landau heat flow in a curved thin domain","abstract":"We consider the Ginzburg-Landau heat flow without magnetic effect in a curved thin domain under the Naumann boundary condition. When the curved thin domain shrinks to a given closed hypersurface as the thickness of the thin domain tends to zero, we show that the weighted average of a weak solution to the thin-domain problem converges weakly on the limit surface under the assumption that the initial data is of class $L^\\infty$ and satisfies some conditions. Moreover, under the same assumption, we derive a limit equation by characterizing the limit function as a weak solution, and prove a difference estimate on the limit surface of an averaged weak solution to the thin-domain problem and a weak solution to the limit problem explicitly in terms of the thickness of the thin domain. We also derive a difference estimate in the curved thin domain of weak solutions to the thin-domain problem and to the limit problem, but without requiring that the initial data of the thin-domain problem is of class $L^\\infty$.","sentences":["We consider the Ginzburg-Landau heat flow without magnetic effect in a curved thin domain under the Naumann boundary condition.","When the curved thin domain shrinks to a given closed hypersurface as the thickness of the thin domain tends to zero, we show that the weighted average of a weak solution to the thin-domain problem converges weakly on the limit surface under the assumption that the initial data is of class $L^\\infty$ and satisfies some conditions.","Moreover, under the same assumption, we derive a limit equation by characterizing the limit function as a weak solution, and prove a difference estimate on the limit surface of an averaged weak solution to the thin-domain problem and a weak solution to the limit problem explicitly in terms of the thickness of the thin domain.","We also derive a difference estimate in the curved thin domain of weak solutions to the thin-domain problem and to the limit problem, but without requiring that the initial data of the thin-domain problem is of class $L^\\infty$."],"url":"http://arxiv.org/abs/2404.14703v1","category":"math.AP"}
{"created":"2024-04-23 17:39:20","title":"GIST: Gibbs self-tuning for locally adaptive Hamiltonian Monte Carlo","abstract":"We present a novel and flexible framework for localized tuning of Hamiltonian Monte Carlo samplers by sampling the algorithm's tuning parameters conditionally based on the position and momentum at each step. For adaptively sampling path lengths, we show that randomized Hamiltonian Monte Carlo, the No-U-Turn Sampler, and the Apogee-to-Apogee Path Sampler all fit within this unified framework as special cases. The framework is illustrated with a simple alternative to the No-U-Turn Sampler for locally adapting path lengths.","sentences":["We present a novel and flexible framework for localized tuning of Hamiltonian Monte Carlo samplers by sampling the algorithm's tuning parameters conditionally based on the position and momentum at each step.","For adaptively sampling path lengths, we show that randomized Hamiltonian Monte Carlo, the No-U-Turn Sampler, and the Apogee-to-Apogee Path Sampler all fit within this unified framework as special cases.","The framework is illustrated with a simple alternative to the No-U-Turn Sampler for locally adapting path lengths."],"url":"http://arxiv.org/abs/2404.15253v1","category":"stat.CO"}
{"created":"2024-04-23 17:35:38","title":"A GPU-accelerated Cartesian grid method for PDEs on irregular domain","abstract":"The kernel-free boundary integral (KFBI) method has successfully solved partial differential equations (PDEs) on irregular domains. Diverging from traditional boundary integral methods, the computation of boundary integrals in KFBI is executed through the resolution of equivalent simple interface problems on Cartesian grids, utilizing fast algorithms. While existing implementations of KFBI methods predominantly utilize CPU platforms, GPU architecture's superior computational capabilities and extensive memory bandwidth offer an efficient resolution to computational bottlenecks. This paper delineates the algorithms adapted for both single-GPU and multiple-GPU applications. On a single GPU, assigning individual threads can control correction, interpolation, and jump calculations. The algorithm is expanded to multiple GPUs to enhance the processing of larger-scale problems. The arrowhead decomposition method is employed in multiple-GPU settings, ensuring optimal computational efficiency and load balancing. Numerical examples show that the proposed algorithm is second-order accurate and efficient. Single-GPU solver speeds 50-200 times than traditional CPU while the eight GPUs distributed solver yields up to 60% parallel efficiency.","sentences":["The kernel-free boundary integral (KFBI) method has successfully solved partial differential equations (PDEs) on irregular domains.","Diverging from traditional boundary integral methods, the computation of boundary integrals in KFBI is executed through the resolution of equivalent simple interface problems on Cartesian grids, utilizing fast algorithms.","While existing implementations of KFBI methods predominantly utilize CPU platforms, GPU architecture's superior computational capabilities and extensive memory bandwidth offer an efficient resolution to computational bottlenecks.","This paper delineates the algorithms adapted for both single-GPU and multiple-GPU applications.","On a single GPU, assigning individual threads can control correction, interpolation, and jump calculations.","The algorithm is expanded to multiple GPUs to enhance the processing of larger-scale problems.","The arrowhead decomposition method is employed in multiple-GPU settings, ensuring optimal computational efficiency and load balancing.","Numerical examples show that the proposed algorithm is second-order accurate and efficient.","Single-GPU solver speeds 50-200 times than traditional CPU while the eight GPUs distributed solver yields up to 60% parallel efficiency."],"url":"http://arxiv.org/abs/2404.15249v1","category":"math.NA"}
{"created":"2024-04-23 17:26:34","title":"Efficient Transformer Encoders for Mask2Former-style models","abstract":"Vision transformer based models bring significant improvements for image segmentation tasks. Although these architectures offer powerful capabilities irrespective of specific segmentation tasks, their use of computational resources can be taxing on deployed devices. One way to overcome this challenge is by adapting the computation level to the specific needs of the input image rather than the current one-size-fits-all approach. To this end, we introduce ECO-M2F or EffiCient TransfOrmer Encoders for Mask2Former-style models. Noting that the encoder module of M2F-style models incur high resource-intensive computations, ECO-M2F provides a strategy to self-select the number of hidden layers in the encoder, conditioned on the input image. To enable this self-selection ability for providing a balance between performance and computational efficiency, we present a three step recipe. The first step is to train the parent architecture to enable early exiting from the encoder. The second step is to create an derived dataset of the ideal number of encoder layers required for each training example. The third step is to use the aforementioned derived dataset to train a gating network that predicts the number of encoder layers to be used, conditioned on the input image. Additionally, to change the computational-accuracy tradeoff, only steps two and three need to be repeated which significantly reduces retraining time. Experiments on the public datasets show that the proposed approach reduces expected encoder computational cost while maintaining performance, adapts to various user compute resources, is flexible in architecture configurations, and can be extended beyond the segmentation task to object detection.","sentences":["Vision transformer based models bring significant improvements for image segmentation tasks.","Although these architectures offer powerful capabilities irrespective of specific segmentation tasks, their use of computational resources can be taxing on deployed devices.","One way to overcome this challenge is by adapting the computation level to the specific needs of the input image rather than the current one-size-fits-all approach.","To this end, we introduce ECO-M2F or EffiCient TransfOrmer Encoders for Mask2Former-style models.","Noting that the encoder module of M2F-style models incur high resource-intensive computations, ECO-M2F provides a strategy to self-select the number of hidden layers in the encoder, conditioned on the input image.","To enable this self-selection ability for providing a balance between performance and computational efficiency, we present a three step recipe.","The first step is to train the parent architecture to enable early exiting from the encoder.","The second step is to create an derived dataset of the ideal number of encoder layers required for each training example.","The third step is to use the aforementioned derived dataset to train a gating network that predicts the number of encoder layers to be used, conditioned on the input image.","Additionally, to change the computational-accuracy tradeoff, only steps two and three need to be repeated which significantly reduces retraining time.","Experiments on the public datasets show that the proposed approach reduces expected encoder computational cost while maintaining performance, adapts to various user compute resources, is flexible in architecture configurations, and can be extended beyond the segmentation task to object detection."],"url":"http://arxiv.org/abs/2404.15244v1","category":"cs.CV"}
{"created":"2024-04-23 15:31:46","title":"Optimizing Multi-Touch Textile and Tactile Skin Sensing Through Circuit Parameter Estimation","abstract":"Tactile and textile skin technologies have become increasingly important for enhancing human-robot interaction and allowing robots to adapt to different environments. Despite notable advancements, there are ongoing challenges in skin signal processing, particularly in achieving both accuracy and speed in dynamic touch sensing. This paper introduces a new framework that poses the touch sensing problem as an estimation problem of resistive sensory arrays. Utilizing a Regularized Least Squares objective function which estimates the resistance distribution of the skin. We enhance the touch sensing accuracy and mitigate the ghosting effects, where false or misleading touches may be registered. Furthermore, our study presents a streamlined skin design that simplifies manufacturing processes without sacrificing performance. Experimental outcomes substantiate the effectiveness of our method, showing 26.9% improvement in multi-touch force-sensing accuracy for the tactile skin.","sentences":["Tactile and textile skin technologies have become increasingly important for enhancing human-robot interaction and allowing robots to adapt to different environments.","Despite notable advancements, there are ongoing challenges in skin signal processing, particularly in achieving both accuracy and speed in dynamic touch sensing.","This paper introduces a new framework that poses the touch sensing problem as an estimation problem of resistive sensory arrays.","Utilizing a Regularized Least Squares objective function which estimates the resistance distribution of the skin.","We enhance the touch sensing accuracy and mitigate the ghosting effects, where false or misleading touches may be registered.","Furthermore, our study presents a streamlined skin design that simplifies manufacturing processes without sacrificing performance.","Experimental outcomes substantiate the effectiveness of our method, showing 26.9% improvement in multi-touch force-sensing accuracy for the tactile skin."],"url":"http://arxiv.org/abs/2404.15131v1","category":"cs.RO"}
{"created":"2024-04-23 15:03:37","title":"Compete and Compose: Learning Independent Mechanisms for Modular World Models","abstract":"We present COmpetitive Mechanisms for Efficient Transfer (COMET), a modular world model which leverages reusable, independent mechanisms across different environments. COMET is trained on multiple environments with varying dynamics via a two-step process: competition and composition. This enables the model to recognise and learn transferable mechanisms. Specifically, in the competition phase, COMET is trained with a winner-takes-all gradient allocation, encouraging the emergence of independent mechanisms. These are then re-used in the composition phase, where COMET learns to re-compose learnt mechanisms in ways that capture the dynamics of intervened environments. In so doing, COMET explicitly reuses prior knowledge, enabling efficient and interpretable adaptation. We evaluate COMET on environments with image-based observations. In contrast to competitive baselines, we demonstrate that COMET captures recognisable mechanisms without supervision. Moreover, we show that COMET is able to adapt to new environments with varying numbers of objects with improved sample efficiency compared to more conventional finetuning approaches.","sentences":["We present COmpetitive Mechanisms for Efficient Transfer (COMET), a modular world model which leverages reusable, independent mechanisms across different environments.","COMET is trained on multiple environments with varying dynamics via a two-step process: competition and composition.","This enables the model to recognise and learn transferable mechanisms.","Specifically, in the competition phase, COMET is trained with a winner-takes-all gradient allocation, encouraging the emergence of independent mechanisms.","These are then re-used in the composition phase, where COMET learns to re-compose learnt mechanisms in ways that capture the dynamics of intervened environments.","In so doing, COMET explicitly reuses prior knowledge, enabling efficient and interpretable adaptation.","We evaluate COMET on environments with image-based observations.","In contrast to competitive baselines, we demonstrate that COMET captures recognisable mechanisms without supervision.","Moreover, we show that COMET is able to adapt to new environments with varying numbers of objects with improved sample efficiency compared to more conventional finetuning approaches."],"url":"http://arxiv.org/abs/2404.15109v1","category":"cs.LG"}
{"created":"2024-04-23 14:20:43","title":"Bayesian Analysis of Conventional and Ultrafast Spectroscopy Data for Investigating Detachment in the MAST-Upgrade Super-X","abstract":"This paper presents the application, testing and first results of a new adaptive Bayesian inference analysis which utilises conventional and ultrafast spectroscopic measurements made in the divertor chamber to investigate the divertor physics during detachment. Validation of this software is performed prior and during analyses of results, demonstrated by compelling reproductions of ideal test cases and synthetic spectroscopic measurements. Application on real diagnostic data shows strong agreement with results from previous analysis methods. We identify unprecedented success in significant advances in time and computational efficiencies. We demonstrate a $\\lesssim$1000$\\times$ reduction in analysis time for spectroscopic measurements from simulated and real Super-X configurations, with the analysis technique presented in this report completing in <3 minutes. Analysis of synthetic and real diagnostic measurements identifies detachment physics in agreement with previous literature.","sentences":["This paper presents the application, testing and first results of a new adaptive Bayesian inference analysis which utilises conventional and ultrafast spectroscopic measurements made in the divertor chamber to investigate the divertor physics during detachment.","Validation of this software is performed prior and during analyses of results, demonstrated by compelling reproductions of ideal test cases and synthetic spectroscopic measurements.","Application on real diagnostic data shows strong agreement with results from previous analysis methods.","We identify unprecedented success in significant advances in time and computational efficiencies.","We demonstrate a $\\lesssim$1000$\\times$ reduction in analysis time for spectroscopic measurements from simulated and real Super-X configurations, with the analysis technique presented in this report completing in <3 minutes.","Analysis of synthetic and real diagnostic measurements identifies detachment physics in agreement with previous literature."],"url":"http://arxiv.org/abs/2404.15072v1","category":"physics.plasm-ph"}
{"created":"2024-04-23 13:38:01","title":"IPAD: Industrial Process Anomaly Detection Dataset","abstract":"Video anomaly detection (VAD) is a challenging task aiming to recognize anomalies in video frames, and existing large-scale VAD researches primarily focus on road traffic and human activity scenes. In industrial scenes, there are often a variety of unpredictable anomalies, and the VAD method can play a significant role in these scenarios. However, there is a lack of applicable datasets and methods specifically tailored for industrial production scenarios due to concerns regarding privacy and security. To bridge this gap, we propose a new dataset, IPAD, specifically designed for VAD in industrial scenarios. The industrial processes in our dataset are chosen through on-site factory research and discussions with engineers. This dataset covers 16 different industrial devices and contains over 6 hours of both synthetic and real-world video footage. Moreover, we annotate the key feature of the industrial process, ie, periodicity. Based on the proposed dataset, we introduce a period memory module and a sliding window inspection mechanism to effectively investigate the periodic information in a basic reconstruction model. Our framework leverages LoRA adapter to explore the effective migration of pretrained models, which are initially trained using synthetic data, into real-world scenarios. Our proposed dataset and method will fill the gap in the field of industrial video anomaly detection and drive the process of video understanding tasks as well as smart factory deployment.","sentences":["Video anomaly detection (VAD) is a challenging task aiming to recognize anomalies in video frames, and existing large-scale VAD researches primarily focus on road traffic and human activity scenes.","In industrial scenes, there are often a variety of unpredictable anomalies, and the VAD method can play a significant role in these scenarios.","However, there is a lack of applicable datasets and methods specifically tailored for industrial production scenarios due to concerns regarding privacy and security.","To bridge this gap, we propose a new dataset, IPAD, specifically designed for VAD in industrial scenarios.","The industrial processes in our dataset are chosen through on-site factory research and discussions with engineers.","This dataset covers 16 different industrial devices and contains over 6 hours of both synthetic and real-world video footage.","Moreover, we annotate the key feature of the industrial process, ie, periodicity.","Based on the proposed dataset, we introduce a period memory module and a sliding window inspection mechanism to effectively investigate the periodic information in a basic reconstruction model.","Our framework leverages LoRA adapter to explore the effective migration of pretrained models, which are initially trained using synthetic data, into real-world scenarios.","Our proposed dataset and method will fill the gap in the field of industrial video anomaly detection and drive the process of video understanding tasks as well as smart factory deployment."],"url":"http://arxiv.org/abs/2404.15033v1","category":"cs.CV"}
{"created":"2024-04-23 13:15:07","title":"External Prompt Features Enhanced Parameter-efficient Fine-tuning for Salient Object Detection","abstract":"Salient object detection (SOD) aims at finding the most salient objects in images and outputs pixel-level binary masks. Transformer-based methods achieve promising performance due to their global semantic understanding, crucial for identifying salient objects. However, these models tend to be large and require numerous training parameters. To better harness the potential of transformers for SOD, we propose a novel parameter-efficient fine-tuning method aimed at reducing the number of training parameters while enhancing the salient object detection capability. Our model, termed EXternal Prompt features Enhanced adapteR Tuning (ExPert), features an encoder-decoder structure with adapters and injectors interspersed between the layers of a frozen transformer encoder. The adapter modules adapt the pre-trained backbone to SOD while the injector modules incorporate external prompt features to enhance the awareness of salient objects. Comprehensive experiments demonstrate the superiority of our method. Surpassing former state-of-the-art (SOTA) models across five SOD datasets, ExPert achieves 0.215 mean absolute error (MAE) in ECSSD dataset with 80.2M trained parameters, 21% better than transformer-based SOTA model and 47% better than CNN-based SOTA model.","sentences":["Salient object detection (SOD) aims at finding the most salient objects in images and outputs pixel-level binary masks.","Transformer-based methods achieve promising performance due to their global semantic understanding, crucial for identifying salient objects.","However, these models tend to be large and require numerous training parameters.","To better harness the potential of transformers for SOD, we propose a novel parameter-efficient fine-tuning method aimed at reducing the number of training parameters while enhancing the salient object detection capability.","Our model, termed EXternal Prompt features Enhanced adapteR Tuning (ExPert), features an encoder-decoder structure with adapters and injectors interspersed between the layers of a frozen transformer encoder.","The adapter modules adapt the pre-trained backbone to SOD while the injector modules incorporate external prompt features to enhance the awareness of salient objects.","Comprehensive experiments demonstrate the superiority of our method.","Surpassing former state-of-the-art (SOTA) models across five SOD datasets, ExPert achieves 0.215 mean absolute error (MAE) in ECSSD dataset with 80.2M trained parameters, 21% better than transformer-based SOTA model and 47% better than CNN-based SOTA model."],"url":"http://arxiv.org/abs/2404.15008v1","category":"cs.CV"}
{"created":"2024-04-23 12:01:21","title":"DAWN: Domain-Adaptive Weakly Supervised Nuclei Segmentation via Cross-Task Interactions","abstract":"Weakly supervised segmentation methods have gained significant attention due to their ability to reduce the reliance on costly pixel-level annotations during model training. However, the current weakly supervised nuclei segmentation approaches typically follow a two-stage pseudo-label generation and network training process. The performance of the nuclei segmentation heavily relies on the quality of the generated pseudo-labels, thereby limiting its effectiveness. This paper introduces a novel domain-adaptive weakly supervised nuclei segmentation framework using cross-task interaction strategies to overcome the challenge of pseudo-label generation. Specifically, we utilize weakly annotated data to train an auxiliary detection task, which assists the domain adaptation of the segmentation network. To enhance the efficiency of domain adaptation, we design a consistent feature constraint module integrating prior knowledge from the source domain. Furthermore, we develop pseudo-label optimization and interactive training methods to improve the domain transfer capability. To validate the effectiveness of our proposed method, we conduct extensive comparative and ablation experiments on six datasets. The results demonstrate the superiority of our approach over existing weakly supervised approaches. Remarkably, our method achieves comparable or even better performance than fully supervised methods. Our code will be released in https://github.com/zhangye-zoe/DAWN.","sentences":["Weakly supervised segmentation methods have gained significant attention due to their ability to reduce the reliance on costly pixel-level annotations during model training.","However, the current weakly supervised nuclei segmentation approaches typically follow a two-stage pseudo-label generation and network training process.","The performance of the nuclei segmentation heavily relies on the quality of the generated pseudo-labels, thereby limiting its effectiveness.","This paper introduces a novel domain-adaptive weakly supervised nuclei segmentation framework using cross-task interaction strategies to overcome the challenge of pseudo-label generation.","Specifically, we utilize weakly annotated data to train an auxiliary detection task, which assists the domain adaptation of the segmentation network.","To enhance the efficiency of domain adaptation, we design a consistent feature constraint module integrating prior knowledge from the source domain.","Furthermore, we develop pseudo-label optimization and interactive training methods to improve the domain transfer capability.","To validate the effectiveness of our proposed method, we conduct extensive comparative and ablation experiments on six datasets.","The results demonstrate the superiority of our approach over existing weakly supervised approaches.","Remarkably, our method achieves comparable or even better performance than fully supervised methods.","Our code will be released in https://github.com/zhangye-zoe/DAWN."],"url":"http://arxiv.org/abs/2404.14956v2","category":"eess.IV"}
{"created":"2024-04-23 11:45:32","title":"Multi-Modal Prompt Learning on Blind Image Quality Assessment","abstract":"Image Quality Assessment (IQA) models benefit significantly from semantic information, which allows them to treat different types of objects distinctly. Currently, leveraging semantic information to enhance IQA is a crucial research direction. Traditional methods, hindered by a lack of sufficiently annotated data, have employed the CLIP image-text pretraining model as their backbone to gain semantic awareness. However, the generalist nature of these pre-trained Vision-Language (VL) models often renders them suboptimal for IQA-specific tasks. Recent approaches have attempted to address this mismatch using prompt technology, but these solutions have shortcomings. Existing prompt-based VL models overly focus on incremental semantic information from text, neglecting the rich insights available from visual data analysis. This imbalance limits their performance improvements in IQA tasks. This paper introduces an innovative multi-modal prompt-based methodology for IQA. Our approach employs carefully crafted prompts that synergistically mine incremental semantic information from both visual and linguistic data. Specifically, in the visual branch, we introduce a multi-layer prompt structure to enhance the VL model's adaptability. In the text branch, we deploy a dual-prompt scheme that steers the model to recognize and differentiate between scene category and distortion type, thereby refining the model's capacity to assess image quality. Our experimental findings underscore the effectiveness of our method over existing Blind Image Quality Assessment (BIQA) approaches. Notably, it demonstrates competitive performance across various datasets. Our method achieves Spearman Rank Correlation Coefficient (SRCC) values of 0.961(surpassing 0.946 in CSIQ) and 0.941 (exceeding 0.930 in KADID), illustrating its robustness and accuracy in diverse contexts.","sentences":["Image Quality Assessment (IQA) models benefit significantly from semantic information, which allows them to treat different types of objects distinctly.","Currently, leveraging semantic information to enhance IQA is a crucial research direction.","Traditional methods, hindered by a lack of sufficiently annotated data, have employed the CLIP image-text pretraining model as their backbone to gain semantic awareness.","However, the generalist nature of these pre-trained Vision-Language (VL) models often renders them suboptimal for IQA-specific tasks.","Recent approaches have attempted to address this mismatch using prompt technology, but these solutions have shortcomings.","Existing prompt-based VL models overly focus on incremental semantic information from text, neglecting the rich insights available from visual data analysis.","This imbalance limits their performance improvements in IQA tasks.","This paper introduces an innovative multi-modal prompt-based methodology for IQA.","Our approach employs carefully crafted prompts that synergistically mine incremental semantic information from both visual and linguistic data.","Specifically, in the visual branch, we introduce a multi-layer prompt structure to enhance the VL model's adaptability.","In the text branch, we deploy a dual-prompt scheme that steers the model to recognize and differentiate between scene category and distortion type, thereby refining the model's capacity to assess image quality.","Our experimental findings underscore the effectiveness of our method over existing Blind Image Quality Assessment (BIQA) approaches.","Notably, it demonstrates competitive performance across various datasets.","Our method achieves Spearman Rank Correlation Coefficient (SRCC) values of 0.961(surpassing 0.946 in CSIQ) and 0.941 (exceeding 0.930 in KADID), illustrating its robustness and accuracy in diverse contexts."],"url":"http://arxiv.org/abs/2404.14949v1","category":"cs.CV"}
{"created":"2024-04-23 10:52:30","title":"Stable vortex solitons sustained by a localized gain in the cubic medium","abstract":"We propose a simple dissipative system with purely cubic defocusing nonlinearity and nonuniform linear gain that can support stable local-ized dissipative vortex solitons with high topological charges without the utilization of competing nonlinearities and nonlinear gain or losses. Localization of such solitons is achieved due to an intriguing mechanism when defocusing nonlinearity stimulates energy flow from the ring-like region with linear gain to the periphery of the medium where energy is absorbed due to linear background losses. Vortex solitons bifurcate from linear gain-guided vortical modes with eigenvalues depending on topological charges that become purely real only at specific gain amplitudes. Increasing gain amplitude leads to transverse expansion of vortex solitons, but simultaneously it usually also leads to stability enhance-ment. Increasing background losses allows creation of stable vortex solitons with high topological charges that are usually prone to instabilities in conservative and dissipative systems. Propagation of the perturbed unstable vortex solitons in this system reveals unusual dynamical re-gimes, when instead of decay or breakup, the initial state transforms into stable vortex soliton with lower or sometimes even with higher topo-logical charge. Our results suggest an efficient mechanism for the formation of nonlinear excited vortex-carrying states with suppressed destructive azimuthal modulational instabilities in a simple setting relevant to a wide class of systems, including polaritonic systems, structured microcavities, and lasers.","sentences":["We propose a simple dissipative system with purely cubic defocusing nonlinearity and nonuniform linear gain that can support stable local-ized dissipative vortex solitons with high topological charges without the utilization of competing nonlinearities and nonlinear gain or losses.","Localization of such solitons is achieved due to an intriguing mechanism when defocusing nonlinearity stimulates energy flow from the ring-like region with linear gain to the periphery of the medium where energy is absorbed due to linear background losses.","Vortex solitons bifurcate from linear gain-guided vortical modes with eigenvalues depending on topological charges that become purely real only at specific gain amplitudes.","Increasing gain amplitude leads to transverse expansion of vortex solitons, but simultaneously it usually also leads to stability enhance-ment.","Increasing background losses allows creation of stable vortex solitons with high topological charges that are usually prone to instabilities in conservative and dissipative systems.","Propagation of the perturbed unstable vortex solitons in this system reveals unusual dynamical re-gimes, when instead of decay or breakup, the initial state transforms into stable vortex soliton with lower or sometimes even with higher topo-logical charge.","Our results suggest an efficient mechanism for the formation of nonlinear excited vortex-carrying states with suppressed destructive azimuthal modulational instabilities in a simple setting relevant to a wide class of systems, including polaritonic systems, structured microcavities, and lasers."],"url":"http://arxiv.org/abs/2404.14910v1","category":"physics.optics"}
{"created":"2024-04-23 10:20:41","title":"Average energy dissipation rates of explicit exponential Runge-Kutta methods for gradient flow problems","abstract":"We propose a unified theoretical framework to examine the energy dissipation properties at all stages of explicit exponential Runge-Kutta (EERK) methods for gradient flow problems. The main part of the novel framework is to construct the differential form of EERK method by using the difference coefficients of method and the so-called discrete orthogonal convolution kernels. As the main result, we prove that an EERK method can preserve the original energy dissipation law unconditionally if the associated differentiation matrix is positive semi-definite. A simple indicator, namely average dissipation rate, is also introduced for these multi-stage methods to evaluate the overall energy dissipation rate of an EERK method such that one can choose proper parameters in some parameterized EERK methods or compare different kinds of EERK methods. Some existing EERK methods in the literature are evaluated from the perspective of preserving the original energy dissipation law and the energy dissipation rate. Some numerical examples are also included to support our theory.","sentences":["We propose a unified theoretical framework to examine the energy dissipation properties at all stages of explicit exponential Runge-Kutta (EERK) methods for gradient flow problems.","The main part of the novel framework is to construct the differential form of EERK method by using the difference coefficients of method and the so-called discrete orthogonal convolution kernels.","As the main result, we prove that an EERK method can preserve the original energy dissipation law unconditionally if the associated differentiation matrix is positive semi-definite.","A simple indicator, namely average dissipation rate, is also introduced for these multi-stage methods to evaluate the overall energy dissipation rate of an EERK method such that one can choose proper parameters in some parameterized EERK methods or compare different kinds of EERK methods.","Some existing EERK methods in the literature are evaluated from the perspective of preserving the original energy dissipation law and the energy dissipation rate.","Some numerical examples are also included to support our theory."],"url":"http://arxiv.org/abs/2404.14893v1","category":"math.NA"}
{"created":"2024-04-23 10:02:22","title":"Regularized Gauss-Newton for Optimizing Overparameterized Neural Networks","abstract":"The generalized Gauss-Newton (GGN) optimization method incorporates curvature estimates into its solution steps, and provides a good approximation to the Newton method for large-scale optimization problems. GGN has been found particularly interesting for practical training of deep neural networks, not only for its impressive convergence speed, but also for its close relation with neural tangent kernel regression, which is central to recent studies that aim to understand the optimization and generalization properties of neural networks. This work studies a GGN method for optimizing a two-layer neural network with explicit regularization. In particular, we consider a class of generalized self-concordant (GSC) functions that provide smooth approximations to commonly-used penalty terms in the objective function of the optimization problem. This approach provides an adaptive learning rate selection technique that requires little to no tuning for optimal performance. We study the convergence of the two-layer neural network, considered to be overparameterized, in the optimization loop of the resulting GGN method for a given scaling of the network parameters. Our numerical experiments highlight specific aspects of GSC regularization that help to improve generalization of the optimized neural network. The code to reproduce the experimental results is available at https://github.com/adeyemiadeoye/ggn-score-nn.","sentences":["The generalized Gauss-Newton (GGN) optimization method incorporates curvature estimates into its solution steps, and provides a good approximation to the Newton method for large-scale optimization problems.","GGN has been found particularly interesting for practical training of deep neural networks, not only for its impressive convergence speed, but also for its close relation with neural tangent kernel regression, which is central to recent studies that aim to understand the optimization and generalization properties of neural networks.","This work studies a GGN method for optimizing a two-layer neural network with explicit regularization.","In particular, we consider a class of generalized self-concordant (GSC) functions that provide smooth approximations to commonly-used penalty terms in the objective function of the optimization problem.","This approach provides an adaptive learning rate selection technique that requires little to no tuning for optimal performance.","We study the convergence of the two-layer neural network, considered to be overparameterized, in the optimization loop of the resulting GGN method for a given scaling of the network parameters.","Our numerical experiments highlight specific aspects of GSC regularization that help to improve generalization of the optimized neural network.","The code to reproduce the experimental results is available at https://github.com/adeyemiadeoye/ggn-score-nn."],"url":"http://arxiv.org/abs/2404.14875v1","category":"cs.LG"}
{"created":"2024-04-23 09:05:09","title":"Simple, Efficient and Scalable Structure-aware Adapter Boosts Protein Language Models","abstract":"Fine-tuning Pre-trained protein language models (PLMs) has emerged as a prominent strategy for enhancing downstream prediction tasks, often outperforming traditional supervised learning approaches. As a widely applied powerful technique in natural language processing, employing Parameter-Efficient Fine-Tuning techniques could potentially enhance the performance of PLMs. However, the direct transfer to life science tasks is non-trivial due to the different training strategies and data forms. To address this gap, we introduce SES-Adapter, a simple, efficient, and scalable adapter method for enhancing the representation learning of PLMs. SES-Adapter incorporates PLM embeddings with structural sequence embeddings to create structure-aware representations. We show that the proposed method is compatible with different PLM architectures and across diverse tasks. Extensive evaluations are conducted on 2 types of folding structures with notable quality differences, 9 state-of-the-art baselines, and 9 benchmark datasets across distinct downstream tasks. Results show that compared to vanilla PLMs, SES-Adapter improves downstream task performance by a maximum of 11% and an average of 3%, with significantly accelerated training speed by a maximum of 1034% and an average of 362%, the convergence rate is also improved by approximately 2 times. Moreover, positive optimization is observed even with low-quality predicted structures. The source code for SES-Adapter is available at https://github.com/tyang816/SES-Adapter.","sentences":["Fine-tuning Pre-trained protein language models (PLMs) has emerged as a prominent strategy for enhancing downstream prediction tasks, often outperforming traditional supervised learning approaches.","As a widely applied powerful technique in natural language processing, employing Parameter-Efficient Fine-Tuning techniques could potentially enhance the performance of PLMs.","However, the direct transfer to life science tasks is non-trivial due to the different training strategies and data forms.","To address this gap, we introduce SES-Adapter, a simple, efficient, and scalable adapter method for enhancing the representation learning of PLMs.","SES-Adapter incorporates PLM embeddings with structural sequence embeddings to create structure-aware representations.","We show that the proposed method is compatible with different PLM architectures and across diverse tasks.","Extensive evaluations are conducted on 2 types of folding structures with notable quality differences, 9 state-of-the-art baselines, and 9 benchmark datasets across distinct downstream tasks.","Results show that compared to vanilla PLMs, SES-Adapter improves downstream task performance by a maximum of 11% and an average of 3%, with significantly accelerated training speed by a maximum of 1034% and an average of 362%, the convergence rate is also improved by approximately 2 times.","Moreover, positive optimization is observed even with low-quality predicted structures.","The source code for SES-Adapter is available at https://github.com/tyang816/SES-Adapter."],"url":"http://arxiv.org/abs/2404.14850v1","category":"cs.CL"}
{"created":"2024-04-23 08:46:53","title":"Floquet dynamics of Rabi model beyond the counterrotating hybridized rotating wave method","abstract":"Monochromatically driven two-level systems (i.e., Rabi models) are ubiquitous in various fields of physics. Though they have been exactly solved, the physical pictures in these exact solutions are not clear. Recently, approximate analytical solutions with neat physics have been obtained by using the counterrotating hybridized rotating wave (CHRW) method, which has been proven to be effective over a wider range of parameters than the previous analytical solutions. However, the CHRW depends on a parameter {\\xi}, which has no solution in some regimes. Here we combine the double-unitary-transformation approach with the generalized Van Vleck nearly degenerate perturbation theory, and present approximate analytical results with clear physics for almost all parameter regimes, which agree well with the numerical solutions and the previous experimental results. Moreover, the dynamic frequencies of the Rabi model are regular, and the frequency with the highest Fourier amplitude changes from the Rabi frequency to 2n{\\omega} with driving frequency {\\omega} and integer n, as the driving intensity increases from weak to deep-strong. In addition, we further explore the Floquet dynamics of the dissipative open Rabi model. Remarkably, the dissipations are tunable in the rotating frame, and the approximate analytical results obtained by our method are in good agreement with the numerical results in the strong driving regime. These results pave the way to quantum control using strong and deep-strong driving with applications in quantum technologies.","sentences":["Monochromatically driven two-level systems (i.e., Rabi models) are ubiquitous in various fields of physics.","Though they have been exactly solved, the physical pictures in these exact solutions are not clear.","Recently, approximate analytical solutions with neat physics have been obtained by using the counterrotating hybridized rotating wave (CHRW) method, which has been proven to be effective over a wider range of parameters than the previous analytical solutions.","However, the CHRW depends on a parameter {\\xi}, which has no solution in some regimes.","Here we combine the double-unitary-transformation approach with the generalized Van Vleck nearly degenerate perturbation theory, and present approximate analytical results with clear physics for almost all parameter regimes, which agree well with the numerical solutions and the previous experimental results.","Moreover, the dynamic frequencies of the Rabi model are regular, and the frequency with the highest Fourier amplitude changes from the Rabi frequency to 2n{\\omega} with driving frequency {\\omega} and integer n, as the driving intensity increases from weak to deep-strong.","In addition, we further explore the Floquet dynamics of the dissipative open Rabi model.","Remarkably, the dissipations are tunable in the rotating frame, and the approximate analytical results obtained by our method are in good agreement with the numerical results in the strong driving regime.","These results pave the way to quantum control using strong and deep-strong driving with applications in quantum technologies."],"url":"http://arxiv.org/abs/2404.14841v1","category":"quant-ph"}
{"created":"2024-04-23 08:43:32","title":"Ultrasound SAM Adapter: Adapting SAM for Breast Lesion Segmentation in Ultrasound Images","abstract":"Segment Anything Model (SAM) has recently achieved amazing results in the field of natural image segmentation. However, it is not effective for medical image segmentation, owing to the large domain gap between natural and medical images. In this paper, we mainly focus on ultrasound image segmentation. As we know that it is very difficult to train a foundation model for ultrasound image data due to the lack of large-scale annotated ultrasound image data. To address these issues, in this paper, we develop a novel Breast Ultrasound SAM Adapter, termed Breast Ultrasound Segment Anything Model (BUSSAM), which migrates the SAM to the field of breast ultrasound image segmentation by using the adapter technique. To be specific, we first design a novel CNN image encoder, which is fully trained on the BUS dataset. Our CNN image encoder is more lightweight, and focuses more on features of local receptive field, which provides the complementary information to the ViT branch in SAM. Then, we design a novel Cross-Branch Adapter to allow the CNN image encoder to fully interact with the ViT image encoder in SAM module. Finally, we add both of the Position Adapter and the Feature Adapter to the ViT branch to fine-tune the original SAM. The experimental results on AMUBUS and BUSI datasets demonstrate that our proposed model outperforms other medical image segmentation models significantly. Our code will be available at: https://github.com/bscs12/BUSSAM.","sentences":["Segment Anything Model (SAM) has recently achieved amazing results in the field of natural image segmentation.","However, it is not effective for medical image segmentation, owing to the large domain gap between natural and medical images.","In this paper, we mainly focus on ultrasound image segmentation.","As we know that it is very difficult to train a foundation model for ultrasound image data due to the lack of large-scale annotated ultrasound image data.","To address these issues, in this paper, we develop a novel Breast Ultrasound SAM Adapter, termed Breast Ultrasound Segment Anything Model (BUSSAM), which migrates the SAM to the field of breast ultrasound image segmentation by using the adapter technique.","To be specific, we first design a novel CNN image encoder, which is fully trained on the BUS dataset.","Our CNN image encoder is more lightweight, and focuses more on features of local receptive field, which provides the complementary information to the ViT branch in SAM.","Then, we design a novel Cross-Branch Adapter to allow the CNN image encoder to fully interact with the ViT image encoder in SAM module.","Finally, we add both of the Position Adapter and the Feature Adapter to the ViT branch to fine-tune the original SAM.","The experimental results on AMUBUS and BUSI datasets demonstrate that our proposed model outperforms other medical image segmentation models significantly.","Our code will be available at: https://github.com/bscs12/BUSSAM."],"url":"http://arxiv.org/abs/2404.14837v1","category":"eess.IV"}
{"created":"2024-04-23 08:41:50","title":"Semi-supervised 2D Human Pose Estimation via Adaptive Keypoint Masking","abstract":"Human pose estimation is a fundamental and challenging task in computer vision. Larger-scale and more accurate keypoint annotations, while helpful for improving the accuracy of supervised pose estimation, are often expensive and difficult to obtain. Semi-supervised pose estimation tries to leverage a large amount of unlabeled data to improve model performance, which can alleviate the problem of insufficient labeled samples. The latest semi-supervised learning usually adopts a strong and weak data augmented teacher-student learning framework to deal with the challenge of \"Human postural diversity and its long-tailed distribution\". Appropriate data augmentation method is one of the key factors affecting the accuracy and generalization of semi-supervised models. Aiming at the problem that the difference of sample learning is not considered in the fixed keypoint masking augmentation method, this paper proposes an adaptive keypoint masking method, which can fully mine the information in the samples and obtain better estimation performance. In order to further improve the generalization and robustness of the model, this paper proposes a dual-branch data augmentation scheme, which can perform Mixup on samples and features on the basis of adaptive keypoint masking. The effectiveness of the proposed method is verified on COCO and MPII, outperforming the state-of-the-art semi-supervised pose estimation by 5.2% and 0.3%, respectively.","sentences":["Human pose estimation is a fundamental and challenging task in computer vision.","Larger-scale and more accurate keypoint annotations, while helpful for improving the accuracy of supervised pose estimation, are often expensive and difficult to obtain.","Semi-supervised pose estimation tries to leverage a large amount of unlabeled data to improve model performance, which can alleviate the problem of insufficient labeled samples.","The latest semi-supervised learning usually adopts a strong and weak data augmented teacher-student learning framework to deal with the challenge of \"Human postural diversity and its long-tailed distribution\".","Appropriate data augmentation method is one of the key factors affecting the accuracy and generalization of semi-supervised models.","Aiming at the problem that the difference of sample learning is not considered in the fixed keypoint masking augmentation method, this paper proposes an adaptive keypoint masking method, which can fully mine the information in the samples and obtain better estimation performance.","In order to further improve the generalization and robustness of the model, this paper proposes a dual-branch data augmentation scheme, which can perform Mixup on samples and features on the basis of adaptive keypoint masking.","The effectiveness of the proposed method is verified on COCO and MPII, outperforming the state-of-the-art semi-supervised pose estimation by 5.2% and 0.3%, respectively."],"url":"http://arxiv.org/abs/2404.14835v1","category":"cs.CV"}
{"created":"2024-04-23 08:39:29","title":"Towards Universal Dense Blocking for Entity Resolution","abstract":"Blocking is a critical step in entity resolution, and the emergence of neural network-based representation models has led to the development of dense blocking as a promising approach for exploring deep semantics in blocking. However, previous advanced self-supervised dense blocking approaches require domain-specific training on the target domain, which limits the benefits and rapid adaptation of these methods. To address this issue, we propose UBlocker, a dense blocker that is pre-trained on a domain-independent, easily-obtainable tabular corpus using self-supervised contrastive learning. By conducting domain-independent pre-training, UBlocker can be adapted to various downstream blocking scenarios without requiring domain-specific fine-tuning. To evaluate the universality of our entity blocker, we also construct a new benchmark covering a wide range of blocking tasks from multiple domains and scenarios. Our experiments show that the proposed UBlocker, without any domain-specific learning, significantly outperforms previous self- and unsupervised dense blocking methods and is comparable and complementary to the state-of-the-art sparse blocking methods.","sentences":["Blocking is a critical step in entity resolution, and the emergence of neural network-based representation models has led to the development of dense blocking as a promising approach for exploring deep semantics in blocking.","However, previous advanced self-supervised dense blocking approaches require domain-specific training on the target domain, which limits the benefits and rapid adaptation of these methods.","To address this issue, we propose UBlocker, a dense blocker that is pre-trained on a domain-independent, easily-obtainable tabular corpus using self-supervised contrastive learning.","By conducting domain-independent pre-training, UBlocker can be adapted to various downstream blocking scenarios without requiring domain-specific fine-tuning.","To evaluate the universality of our entity blocker, we also construct a new benchmark covering a wide range of blocking tasks from multiple domains and scenarios.","Our experiments show that the proposed UBlocker, without any domain-specific learning, significantly outperforms previous self- and unsupervised dense blocking methods and is comparable and complementary to the state-of-the-art sparse blocking methods."],"url":"http://arxiv.org/abs/2404.14831v1","category":"cs.DB"}
{"created":"2024-04-23 08:01:30","title":"Time-aware Heterogeneous Graph Transformer with Adaptive Attention Merging for Health Event Prediction","abstract":"The widespread application of Electronic Health Records (EHR) data in the medical field has led to early successes in disease risk prediction using deep learning methods. These methods typically require extensive data for training due to their large parameter sets. However, existing works do not exploit the full potential of EHR data. A significant challenge arises from the infrequent occurrence of many medical codes within EHR data, limiting their clinical applicability. Current research often lacks in critical areas: 1) incorporating disease domain knowledge; 2) heterogeneously learning disease representations with rich meanings; 3) capturing the temporal dynamics of disease progression. To overcome these limitations, we introduce a novel heterogeneous graph learning model designed to assimilate disease domain knowledge and elucidate the intricate relationships between drugs and diseases. This model innovatively incorporates temporal data into visit-level embeddings and leverages a time-aware transformer alongside an adaptive attention mechanism to produce patient representations. When evaluated on two healthcare datasets, our approach demonstrated notable enhancements in both prediction accuracy and interpretability over existing methodologies, signifying a substantial advancement towards personalized and proactive healthcare management.","sentences":["The widespread application of Electronic Health Records (EHR) data in the medical field has led to early successes in disease risk prediction using deep learning methods.","These methods typically require extensive data for training due to their large parameter sets.","However, existing works do not exploit the full potential of EHR data.","A significant challenge arises from the infrequent occurrence of many medical codes within EHR data, limiting their clinical applicability.","Current research often lacks in critical areas: 1) incorporating disease domain knowledge; 2) heterogeneously learning disease representations with rich meanings; 3) capturing the temporal dynamics of disease progression.","To overcome these limitations, we introduce a novel heterogeneous graph learning model designed to assimilate disease domain knowledge and elucidate the intricate relationships between drugs and diseases.","This model innovatively incorporates temporal data into visit-level embeddings and leverages a time-aware transformer alongside an adaptive attention mechanism to produce patient representations.","When evaluated on two healthcare datasets, our approach demonstrated notable enhancements in both prediction accuracy and interpretability over existing methodologies, signifying a substantial advancement towards personalized and proactive healthcare management."],"url":"http://arxiv.org/abs/2404.14815v1","category":"cs.LG"}
{"created":"2024-04-23 05:50:02","title":"Unified Unsupervised Salient Object Detection via Knowledge Transfer","abstract":"Recently, unsupervised salient object detection (USOD) has gained increasing attention due to its annotation-free nature. However, current methods mainly focus on specific tasks such as RGB and RGB-D, neglecting the potential for task migration. In this paper, we propose a unified USOD framework for generic USOD tasks. Firstly, we propose a Progressive Curriculum Learning-based Saliency Distilling (PCL-SD) mechanism to extract saliency cues from a pre-trained deep network. This mechanism starts with easy samples and progressively moves towards harder ones, to avoid initial interference caused by hard samples. Afterwards, the obtained saliency cues are utilized to train a saliency detector, and we employ a Self-rectify Pseudo-label Refinement (SPR) mechanism to improve the quality of pseudo-labels. Finally, an adapter-tuning method is devised to transfer the acquired saliency knowledge, leveraging shared knowledge to attain superior transferring performance on the target tasks. Extensive experiments on five representative SOD tasks confirm the effectiveness and feasibility of our proposed method. Code and supplement materials are available at https://github.com/I2-Multimedia-Lab/A2S-v3.","sentences":["Recently, unsupervised salient object detection (USOD) has gained increasing attention due to its annotation-free nature.","However, current methods mainly focus on specific tasks such as RGB and RGB-D, neglecting the potential for task migration.","In this paper, we propose a unified USOD framework for generic USOD tasks.","Firstly, we propose a Progressive Curriculum Learning-based Saliency Distilling (PCL-SD) mechanism to extract saliency cues from a pre-trained deep network.","This mechanism starts with easy samples and progressively moves towards harder ones, to avoid initial interference caused by hard samples.","Afterwards, the obtained saliency cues are utilized to train a saliency detector, and we employ a Self-rectify Pseudo-label Refinement (SPR) mechanism to improve the quality of pseudo-labels.","Finally, an adapter-tuning method is devised to transfer the acquired saliency knowledge, leveraging shared knowledge to attain superior transferring performance on the target tasks.","Extensive experiments on five representative SOD tasks confirm the effectiveness and feasibility of our proposed method.","Code and supplement materials are available at https://github.com/I2-Multimedia-Lab/A2S-v3."],"url":"http://arxiv.org/abs/2404.14759v1","category":"cs.CV"}
{"created":"2024-04-23 04:51:02","title":"Gradient Guidance for Diffusion Models: An Optimization Perspective","abstract":"Diffusion models have demonstrated empirical successes in various applications and can be adapted to task-specific needs via guidance. This paper introduces a form of gradient guidance for adapting or fine-tuning diffusion models towards user-specified optimization objectives. We study the theoretic aspects of a guided score-based sampling process, linking the gradient-guided diffusion model to first-order optimization. We show that adding gradient guidance to the sampling process of a pre-trained diffusion model is essentially equivalent to solving a regularized optimization problem, where the regularization term acts as a prior determined by the pre-training data. Diffusion models are able to learn data's latent subspace, however, explicitly adding the gradient of an external objective function to the sample process would jeopardize the structure in generated samples. To remedy this issue, we consider a modified form of gradient guidance based on a forward prediction loss, which leverages the pre-trained score function to preserve the latent structure in generated samples. We further consider an iteratively fine-tuned version of gradient-guided diffusion where one can query gradients at newly generated data points and update the score network using new samples. This process mimics a first-order optimization iteration in expectation, for which we proved O(1/K) convergence rate to the global optimum when the objective function is concave.","sentences":["Diffusion models have demonstrated empirical successes in various applications and can be adapted to task-specific needs via guidance.","This paper introduces a form of gradient guidance for adapting or fine-tuning diffusion models towards user-specified optimization objectives.","We study the theoretic aspects of a guided score-based sampling process, linking the gradient-guided diffusion model to first-order optimization.","We show that adding gradient guidance to the sampling process of a pre-trained diffusion model is essentially equivalent to solving a regularized optimization problem, where the regularization term acts as a prior determined by the pre-training data.","Diffusion models are able to learn data's latent subspace, however, explicitly adding the gradient of an external objective function to the sample process would jeopardize the structure in generated samples.","To remedy this issue, we consider a modified form of gradient guidance based on a forward prediction loss, which leverages the pre-trained score function to preserve the latent structure in generated samples.","We further consider an iteratively fine-tuned version of gradient-guided diffusion where one can query gradients at newly generated data points and update the score network using new samples.","This process mimics a first-order optimization iteration in expectation, for which we proved O(1/K) convergence rate to the global optimum when the objective function is concave."],"url":"http://arxiv.org/abs/2404.14743v1","category":"stat.ML"}
{"created":"2024-04-23 04:39:11","title":"Unveiling dynamic bifurcation of Resch-patterned origami for self-adaptive impact mitigation structure","abstract":"In the classic realm of impact mitigation, targeting different impact scenarios with a universally designed device still remains an unassailable challenge. In this study, we delve into the untapped potential of Resch-patterned origami for impact mitigation, specifically considering the adaptively reconfigurable nature of the Resch origami structure. Our unit-cell-level analyses reveal two distinctive modes of deformation, each characterized by contrasting mechanical responses: the folding mode that displays monostability coupled with strain-hardening, and the unfolding mode that manifests bistability, facilitating energy absorption through snap-through dynamics. Drop tests further unveil a novel dynamic bifurcation phenomenon, where the origami switches between folding and unfolding depending on impact speed, thereby showcasing its innate self-reconfigurability in a wide range of dynamic events. The tessellated meter-scale Resch structure mimicking an automotive bumper inherits this dynamically bifurcating behavior, demonstrating the instantaneous morphing into favorable deformation mode to minimize the peak acceleration upon impact. This suggests a self-adaptive and universally applicable impact-absorbing nature of the Resch-patterned origami system. We believe that our findings pave the way for developing smart, origami-inspired impact mitigation devices capable of real-time response and adaptation to external stimuli, offering insights into designing universally protective structures with enhanced performance in response to various impact scenarios.","sentences":["In the classic realm of impact mitigation, targeting different impact scenarios with a universally designed device still remains an unassailable challenge.","In this study, we delve into the untapped potential of Resch-patterned origami for impact mitigation, specifically considering the adaptively reconfigurable nature of the Resch origami structure.","Our unit-cell-level analyses reveal two distinctive modes of deformation, each characterized by contrasting mechanical responses: the folding mode that displays monostability coupled with strain-hardening, and the unfolding mode that manifests bistability, facilitating energy absorption through snap-through dynamics.","Drop tests further unveil a novel dynamic bifurcation phenomenon, where the origami switches between folding and unfolding depending on impact speed, thereby showcasing its innate self-reconfigurability in a wide range of dynamic events.","The tessellated meter-scale Resch structure mimicking an automotive bumper inherits this dynamically bifurcating behavior, demonstrating the instantaneous morphing into favorable deformation mode to minimize the peak acceleration upon impact.","This suggests a self-adaptive and universally applicable impact-absorbing nature of the Resch-patterned origami system.","We believe that our findings pave the way for developing smart, origami-inspired impact mitigation devices capable of real-time response and adaptation to external stimuli, offering insights into designing universally protective structures with enhanced performance in response to various impact scenarios."],"url":"http://arxiv.org/abs/2404.14737v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-23 03:50:57","title":"Incorporating Gradients to Rules: Towards Lightweight, Adaptive Provenance-based Intrusion Detection","abstract":"As cyber-attacks become increasingly sophisticated and stealthy, it becomes more imperative and challenging to detect intrusion from normal behaviors. Through fine-grained causality analysis, provenance-based intrusion detection systems (PIDS) demonstrated a promising capacity to distinguish benign and malicious behaviors, attracting widespread attention from both industry and academia. Among diverse approaches, rule-based PIDS stands out due to its lightweight overhead, real-time capabilities, and explainability. However, existing rule-based systems suffer low detection accuracy, especially the high false alarms, due to the lack of fine-grained rules and environment-specific configurations. In this paper, we propose CAPTAIN, a rule-based PIDS capable of automatically adapting to diverse environments. Specifically, we propose three adaptive parameters to adjust the detection configuration with respect to nodes, edges, and alarm generation thresholds. We build a differentiable tag propagation framework and utilize the gradient descent algorithm to optimize these adaptive parameters based on the training data. We evaluate our system based on data from DARPA Engagement and simulated environments. The evaluation results demonstrate that CAPTAIN offers better detection accuracy, less detection latency, lower runtime overhead, and more interpretable detection alarms and knowledge compared to the SOTA PIDS.","sentences":["As cyber-attacks become increasingly sophisticated and stealthy, it becomes more imperative and challenging to detect intrusion from normal behaviors.","Through fine-grained causality analysis, provenance-based intrusion detection systems (PIDS) demonstrated a promising capacity to distinguish benign and malicious behaviors, attracting widespread attention from both industry and academia.","Among diverse approaches, rule-based PIDS stands out due to its lightweight overhead, real-time capabilities, and explainability.","However, existing rule-based systems suffer low detection accuracy, especially the high false alarms, due to the lack of fine-grained rules and environment-specific configurations.","In this paper, we propose CAPTAIN, a rule-based PIDS capable of automatically adapting to diverse environments.","Specifically, we propose three adaptive parameters to adjust the detection configuration with respect to nodes, edges, and alarm generation thresholds.","We build a differentiable tag propagation framework and utilize the gradient descent algorithm to optimize these adaptive parameters based on the training data.","We evaluate our system based on data from DARPA Engagement and simulated environments.","The evaluation results demonstrate that CAPTAIN offers better detection accuracy, less detection latency, lower runtime overhead, and more interpretable detection alarms and knowledge compared to the SOTA PIDS."],"url":"http://arxiv.org/abs/2404.14720v1","category":"cs.CR"}
{"created":"2024-04-23 03:47:32","title":"Variation in Path Lengths of Turbulent Magnetic Field Lines and Solar Energetic Particles","abstract":"Modeling of time profiles of solar energetic particle (SEP) observations often considers transport along a large-scale magnetic field with a fixed path length from the source to the observer. Here we point out that variability in the turbulent field line path length can affect the fits to SEP data and the inferred mean free path and injection profile. To explore such variability, we perform Monte Carlo simulations in representations of homogeneous 2D MHD + slab turbulence adapted to spherical geometry and trace trajectories of field lines and full particle orbits, considering proton injection from a narrow or wide angular region near the Sun, corresponding to an impulsive or gradual solar event, respectively. We analyze our simulation results in terms of field line and particle path length statistics for $1^\\circ\\times 1^\\circ$ pixels in heliolatitude and heliolongitude at 0.35 and 1 AU from the Sun, for different values of the turbulence amplitude $b/B_0$ and turbulence geometry as expressed by the slab fraction $f_s$. Maps of the most probable path lengths of field lines and particles at each pixel exhibit systematic patterns that reflect the fluctuation amplitudes experienced by the field lines, which in turn relate to the local topology of 2D turbulence. We describe the effects of such path length variations on SEP time profiles, both in terms of path length variability at specific locations and motion of the observer with respect to turbulence topology during the course of the observations.","sentences":["Modeling of time profiles of solar energetic particle (SEP) observations often considers transport along a large-scale magnetic field with a fixed path length from the source to the observer.","Here we point out that variability in the turbulent field line path length can affect the fits to SEP data and the inferred mean free path and injection profile.","To explore such variability, we perform Monte Carlo simulations in representations of homogeneous 2D MHD + slab turbulence adapted to spherical geometry and trace trajectories of field lines and full particle orbits, considering proton injection from a narrow or wide angular region near the Sun, corresponding to an impulsive or gradual solar event, respectively.","We analyze our simulation results in terms of field line and particle path length statistics for $1^\\circ\\times 1^\\circ$ pixels in heliolatitude and heliolongitude at 0.35 and 1 AU from the Sun, for different values of the turbulence amplitude $b/B_0$ and turbulence geometry as expressed by the slab fraction $f_s$. Maps of the most probable path lengths of field lines and particles at each pixel exhibit systematic patterns that reflect the fluctuation amplitudes experienced by the field lines, which in turn relate to the local topology of 2D turbulence.","We describe the effects of such path length variations on SEP time profiles, both in terms of path length variability at specific locations and motion of the observer with respect to turbulence topology during the course of the observations."],"url":"http://arxiv.org/abs/2404.14718v1","category":"astro-ph.SR"}
{"created":"2024-04-23 02:54:12","title":"Adaptive Prompt Learning with Negative Textual Semantics and Uncertainty Modeling for Universal Multi-Source Domain Adaptation","abstract":"Universal Multi-source Domain Adaptation (UniMDA) transfers knowledge from multiple labeled source domains to an unlabeled target domain under domain shifts (different data distribution) and class shifts (unknown target classes). Existing solutions focus on excavating image features to detect unknown samples, ignoring abundant information contained in textual semantics. In this paper, we propose an Adaptive Prompt learning with Negative textual semantics and uncErtainty modeling method based on Contrastive Language-Image Pre-training (APNE-CLIP) for UniMDA classification tasks. Concretely, we utilize the CLIP with adaptive prompts to leverage textual information of class semantics and domain representations, helping the model identify unknown samples and address domain shifts. Additionally, we design a novel global instance-level alignment objective by utilizing negative textual semantics to achieve more precise image-text pair alignment. Furthermore, we propose an energy-based uncertainty modeling strategy to enlarge the margin distance between known and unknown samples. Extensive experiments demonstrate the superiority of our proposed method.","sentences":["Universal Multi-source Domain Adaptation (UniMDA) transfers knowledge from multiple labeled source domains to an unlabeled target domain under domain shifts (different data distribution) and class shifts (unknown target classes).","Existing solutions focus on excavating image features to detect unknown samples, ignoring abundant information contained in textual semantics.","In this paper, we propose an Adaptive Prompt learning with Negative textual semantics and uncErtainty modeling method based on Contrastive Language-Image Pre-training (APNE-CLIP) for UniMDA classification tasks.","Concretely, we utilize the CLIP with adaptive prompts to leverage textual information of class semantics and domain representations, helping the model identify unknown samples and address domain shifts.","Additionally, we design a novel global instance-level alignment objective by utilizing negative textual semantics to achieve more precise image-text pair alignment.","Furthermore, we propose an energy-based uncertainty modeling strategy to enlarge the margin distance between known and unknown samples.","Extensive experiments demonstrate the superiority of our proposed method."],"url":"http://arxiv.org/abs/2404.14696v2","category":"cs.CV"}
{"created":"2024-04-23 01:42:07","title":"Droplet impact behavior on a hydrophobic plate with a wettability-patterned orifice","abstract":"Droplet impact behavior has attracted much attention recently due to its academic significance and diverse industrial applications. This study employs the lattice Boltzmann method to simulate the impact of a droplet on a hydrophobic plate featuring a square orifice. Unlike previous studies, the chemical property of the orifice considered in this work is not homogeneous but heterogeneous, and its cross-sectional wettability changes from hydrophobicity to hydrophilicity. The study first validates the numerical method against experimental data, and then investigates in detail the influences of the Weber number, wettability difference, and pore size. According to the numerical results, we observed that the evolutionary stages of the impinging droplet always include the spreading phase and the rebounding phase, while whether there exists the splitting phase, it depends on the combined effect of the wettability difference and the Weber number. The impact behavior of droplets is analyzed by evaluating the underlying mechanisms such as kinetic energy, surface energy, viscous dissipation energy, and pressure. It is interesting to note that the existence of wettability-patterned pore tends to promote adhesion of droplets on the plate, resulting in the droplet impact behaviors are largely different from that for the case of homogeneous pore. Additionally, a phase diagram is constructed for various Weber numbers and pore sizes, revealing that the dynamic behavior of droplets is determined by the competition among dynamic pressure, capillary pressure, and viscous pressure losses. These insights from numerical studies guide the development of innovative solid substrates capable of manipulating droplet motion.","sentences":["Droplet impact behavior has attracted much attention recently due to its academic significance and diverse industrial applications.","This study employs the lattice Boltzmann method to simulate the impact of a droplet on a hydrophobic plate featuring a square orifice.","Unlike previous studies, the chemical property of the orifice considered in this work is not homogeneous but heterogeneous, and its cross-sectional wettability changes from hydrophobicity to hydrophilicity.","The study first validates the numerical method against experimental data, and then investigates in detail the influences of the Weber number, wettability difference, and pore size.","According to the numerical results, we observed that the evolutionary stages of the impinging droplet always include the spreading phase and the rebounding phase, while whether there exists the splitting phase, it depends on the combined effect of the wettability difference and the Weber number.","The impact behavior of droplets is analyzed by evaluating the underlying mechanisms such as kinetic energy, surface energy, viscous dissipation energy, and pressure.","It is interesting to note that the existence of wettability-patterned pore tends to promote adhesion of droplets on the plate, resulting in the droplet impact behaviors are largely different from that for the case of homogeneous pore.","Additionally, a phase diagram is constructed for various Weber numbers and pore sizes, revealing that the dynamic behavior of droplets is determined by the competition among dynamic pressure, capillary pressure, and viscous pressure losses.","These insights from numerical studies guide the development of innovative solid substrates capable of manipulating droplet motion."],"url":"http://arxiv.org/abs/2404.14659v1","category":"physics.flu-dyn"}
{"created":"2024-04-23 01:18:28","title":"Forecasting the Forced Van der Pol Equation with Frequent Phase Shifts Using a Reservoir Computer","abstract":"A reservoir computer (RC) is a recurrent neural network (RNN) framework that achieves computational efficiency where only readout layer training is required. Additionally, it effectively predicts nonlinear dynamical system tasks and has various applications. RC is effective for forecasting nonautonomous dynamical systems with gradual changes to the external drive amplitude. This study investigates the predictability of nonautonomous dynamical systems with rapid changes to the phase of the external drive. The forced Van der Pol equation was employed for the base model, implementing forecasting tasks with the RC. The study findings suggest that, despite hidden variables, a nonautonomous dynamical system with rapid changes to the phase of the external drive is predictable. Therefore, RC can offer better schedules for individual shift workers.","sentences":["A reservoir computer (RC) is a recurrent neural network (RNN) framework that achieves computational efficiency where only readout layer training is required.","Additionally, it effectively predicts nonlinear dynamical system tasks and has various applications.","RC is effective for forecasting nonautonomous dynamical systems with gradual changes to the external drive amplitude.","This study investigates the predictability of nonautonomous dynamical systems with rapid changes to the phase of the external drive.","The forced Van der Pol equation was employed for the base model, implementing forecasting tasks with the RC.","The study findings suggest that, despite hidden variables, a nonautonomous dynamical system with rapid changes to the phase of the external drive is predictable.","Therefore, RC can offer better schedules for individual shift workers."],"url":"http://arxiv.org/abs/2404.14651v1","category":"nlin.AO"}
{"created":"2024-04-23 00:34:38","title":"Bring the Heat: Tidal Heating Constraints for Black Holes and Exotic Compact Objects from the LIGO-Virgo-KAGRA Data","abstract":"We present the first constraints on tidal heating for the binary systems detected in the LIGO-Virgo-KAGRA (LVK) gravitational wave data. Tidal heating, also known as tidal dissipation, characterizes the viscous nature of an astrophysical body and provides a channel for exchanging energy and angular momentum with the tidal environment. Using the worldline effective field theory formalism, we introduce a physically motivated and easily interpretable parametrization of tidal heating valid for an arbitrary compact astrophysical object. We then derive the imprints of the spin-independent and linear-in-spin tidal heating effects of generic binary components on the waveform phases and amplitudes of quasi-circular orbits. Notably, the mass-weighted spin-independent tidal heating coefficient derived in this work, $\\mathcal{H}_0$, is the dissipative analog of the tidal Love number. We constrain the tidal heating coefficients using the public LVK O1-O3 data. Our parameter estimation study includes two separate analyses: the first treats the catalog of binary events as binary black holes (BBH), while the second makes no assumption about the nature of the binary constituents and can therefore be interpreted as constraints for exotic compact objects. In the former case, we combine the posterior distributions of the individual BBH events and obtain a joint constraint of $-13 < \\mathcal{H}_0 < 20$ at the $90\\%$ credible interval for the BBH population. This translates into a bound on the fraction of the emitted gravitational wave energy lost due to tidal heating (or gained due to radiation enhancement effects) at $|\\Delta E_H/\\Delta E_{\\infty}|\\lesssim 3\\cdot 10^{-3}$. Our work provides the first robust framework for deriving and measuring tidal heating effects in merging binary systems, demonstrating its potential as a powerful probe of the nature of binary constituents and tests of new physics.","sentences":["We present the first constraints on tidal heating for the binary systems detected in the LIGO-Virgo-KAGRA (LVK) gravitational wave data.","Tidal heating, also known as tidal dissipation, characterizes the viscous nature of an astrophysical body and provides a channel for exchanging energy and angular momentum with the tidal environment.","Using the worldline effective field theory formalism, we introduce a physically motivated and easily interpretable parametrization of tidal heating valid for an arbitrary compact astrophysical object.","We then derive the imprints of the spin-independent and linear-in-spin tidal heating effects of generic binary components on the waveform phases and amplitudes of quasi-circular orbits.","Notably, the mass-weighted spin-independent tidal heating coefficient derived in this work, $\\mathcal{H}_0$, is the dissipative analog of the tidal Love number.","We constrain the tidal heating coefficients using the public LVK O1-O3 data.","Our parameter estimation study includes two separate analyses: the first treats the catalog of binary events as binary black holes (BBH), while the second makes no assumption about the nature of the binary constituents and can therefore be interpreted as constraints for exotic compact objects.","In the former case, we combine the posterior distributions of the individual BBH events and obtain a joint constraint of $-13 < \\mathcal{H}_0 < 20$ at the $90\\%$ credible interval for the BBH population.","This translates into a bound on the fraction of the emitted gravitational wave energy lost due to tidal heating (or gained due to radiation enhancement effects) at $|\\Delta E_H/\\Delta E_{\\infty}|\\lesssim 3\\cdot 10^{-3}$.","Our work provides the first robust framework for deriving and measuring tidal heating effects in merging binary systems, demonstrating its potential as a powerful probe of the nature of binary constituents and tests of new physics."],"url":"http://arxiv.org/abs/2404.14641v1","category":"gr-qc"}
{"created":"2024-04-22 22:04:16","title":"Q-Tuning: Queue-based Prompt Tuning for Lifelong Few-shot Language Learning","abstract":"This paper introduces \\textbf{Q-tuning}, a novel approach for continual prompt tuning that enables the lifelong learning of a pre-trained language model. When learning a new task, Q-tuning trains a task-specific prompt by adding it to a prompt queue consisting of the prompts from older tasks. To better transfer the knowledge of old tasks, we design an adaptive knowledge aggregation technique that reweighs previous prompts in the queue with a learnable low-rank matrix. Once the prompt queue reaches its maximum capacity, we leverage a PCA-based eviction rule to reduce the queue's size, allowing the newly trained prompt to be added while preserving the primary knowledge of old tasks. In order to mitigate the accumulation of information loss caused by the eviction, we additionally propose a globally shared prefix prompt and a memory retention regularization based on information theory. Extensive experiments demonstrate that our approach outperforms the state-of-the-art methods substantially on continual prompt tuning benchmarks. Moreover, our approach enables lifelong learning on linearly growing task sequences while requiring constant complexity for training and inference.","sentences":["This paper introduces \\textbf{Q-tuning}, a novel approach for continual prompt tuning that enables the lifelong learning of a pre-trained language model.","When learning a new task, Q-tuning trains a task-specific prompt by adding it to a prompt queue consisting of the prompts from older tasks.","To better transfer the knowledge of old tasks, we design an adaptive knowledge aggregation technique that reweighs previous prompts in the queue with a learnable low-rank matrix.","Once the prompt queue reaches its maximum capacity, we leverage a PCA-based eviction rule to reduce the queue's size, allowing the newly trained prompt to be added while preserving the primary knowledge of old tasks.","In order to mitigate the accumulation of information loss caused by the eviction, we additionally propose a globally shared prefix prompt and a memory retention regularization based on information theory.","Extensive experiments demonstrate that our approach outperforms the state-of-the-art methods substantially on continual prompt tuning benchmarks.","Moreover, our approach enables lifelong learning on linearly growing task sequences while requiring constant complexity for training and inference."],"url":"http://arxiv.org/abs/2404.14607v1","category":"cs.CL"}
{"created":"2024-04-22 21:58:23","title":"Adaptive Bayesian Optimization for High-Precision Motion Systems","abstract":"Controller tuning and parameter optimization are crucial in system design to improve closed-loop system performance. Bayesian optimization has been established as an efficient model-free controller tuning and adaptation method. However, Bayesian optimization methods are computationally expensive and therefore difficult to use in real-time critical scenarios. In this work, we propose a real-time purely data-driven, model-free approach for adaptive control, by online tuning low-level controller parameters. We base our algorithm on GoOSE, an algorithm for safe and sample-efficient Bayesian optimization, for handling performance and stability criteria. We introduce multiple computational and algorithmic modifications for computational efficiency and parallelization of optimization steps. We further evaluate the algorithm's performance on a real precision-motion system utilized in semiconductor industry applications by modifying the payload and reference stepsize and comparing it to an interpolated constrained optimization-based baseline approach.","sentences":["Controller tuning and parameter optimization are crucial in system design to improve closed-loop system performance.","Bayesian optimization has been established as an efficient model-free controller tuning and adaptation method.","However, Bayesian optimization methods are computationally expensive and therefore difficult to use in real-time critical scenarios.","In this work, we propose a real-time purely data-driven, model-free approach for adaptive control, by online tuning low-level controller parameters.","We base our algorithm on GoOSE, an algorithm for safe and sample-efficient Bayesian optimization, for handling performance and stability criteria.","We introduce multiple computational and algorithmic modifications for computational efficiency and parallelization of optimization steps.","We further evaluate the algorithm's performance on a real precision-motion system utilized in semiconductor industry applications by modifying the payload and reference stepsize and comparing it to an interpolated constrained optimization-based baseline approach."],"url":"http://arxiv.org/abs/2404.14602v1","category":"eess.SY"}
{"created":"2024-04-22 21:39:29","title":"Formal structure of scalar curvature in generalized K\u00e4hler geometry","abstract":"Building on works of Boulanger and Goto, we show that Goto's scalar curvature is the moment map for an action of generalized Hamiltonian automorphisms of the associated Courant algebroid, constrained by the choice of an adapted volume form. We derive an explicit formula for Goto's scalar curvature, and show that it is constant for generalized K\\\"ahler-Ricci solitons. Restricting to the generically symplectic type case, we realize the generalized K\\\"ahler class as the complexified orbit of the Hamiltonian action above. This leads to a natural extension of Mabuchi's metric and $K$-energy, implying a conditional uniqueness result. Finally, in this setting we derive a Calabi-Matsushima-Lichnerowicz obstruction and a Futaki invariant.","sentences":["Building on works of Boulanger and Goto, we show that Goto's scalar curvature is the moment map for an action of generalized Hamiltonian automorphisms of the associated Courant algebroid, constrained by the choice of an adapted volume form.","We derive an explicit formula for Goto's scalar curvature, and show that it is constant for generalized K\\\"ahler-Ricci solitons.","Restricting to the generically symplectic type case, we realize the generalized K\\\"ahler class as the complexified orbit of the Hamiltonian action above.","This leads to a natural extension of Mabuchi's metric and $K$-energy, implying a conditional uniqueness result.","Finally, in this setting we derive a Calabi-Matsushima-Lichnerowicz obstruction and a Futaki invariant."],"url":"http://arxiv.org/abs/2404.14595v1","category":"math.DG"}
{"created":"2024-04-22 21:35:35","title":"High-order Accurate Implicit-Explicit Time-Stepping Schemes for Wave Equations on Overset Grids","abstract":"New implicit and implicit-explicit time-stepping methods for the wave equation in second-order form are described with application to two and three-dimensional problems discretized on overset grids. The implicit schemes are single step, three levels in time, and based on the modified equation approach. Second and fourth-order accurate schemes are developed and they incorporate upwind dissipation for stability on overset grids. The fully implicit schemes are useful for certain applications such as the WaveHoltz algorithm for solving Helmholtz problems where very large time-steps are desired. Some wave propagation problems are geometrically stiff due to localized regions of small grid cells, such as grids needed to resolve fine geometric features, and for these situations the implicit time-stepping scheme is combined with an explicit scheme: the implicit scheme is used for component grids containing small cells while the explicit scheme is used on the other grids such as background Cartesian grids. The resulting partitioned implicit-explicit scheme can be many times faster than using an explicit scheme everywhere. The accuracy and stability of the schemes are studied through analysis and numerical computations.","sentences":["New implicit and implicit-explicit time-stepping methods for the wave equation in second-order form are described with application to two and three-dimensional problems discretized on overset grids.","The implicit schemes are single step, three levels in time, and based on the modified equation approach.","Second and fourth-order accurate schemes are developed and they incorporate upwind dissipation for stability on overset grids.","The fully implicit schemes are useful for certain applications such as the WaveHoltz algorithm for solving Helmholtz problems where very large time-steps are desired.","Some wave propagation problems are geometrically stiff due to localized regions of small grid cells, such as grids needed to resolve fine geometric features, and for these situations the implicit time-stepping scheme is combined with an explicit scheme: the implicit scheme is used for component grids containing small cells while the explicit scheme is used on the other grids such as background Cartesian grids.","The resulting partitioned implicit-explicit scheme can be many times faster than using an explicit scheme everywhere.","The accuracy and stability of the schemes are studied through analysis and numerical computations."],"url":"http://arxiv.org/abs/2404.14592v1","category":"math.NA"}
{"created":"2024-04-22 20:38:29","title":"Categorification and mirror symmetry for Grassmannians","abstract":"The homogeneous coordinate ring $\\mathbb{C}[\\operatorname{Gr}(k,n)]$ of the Grassmannian is a cluster algebra, with an additive categorification $\\operatorname{CM}C$. Thus every $M\\in\\operatorname{CM}C$ has a cluster character $\\Psi_M\\in\\mathbb{C}[\\operatorname{Gr}(k,n)]$.   The aim is to use the categorification to enrich Rietsch-Williams' mirror symmetry result that the Newton-Okounkov (NO) body/cone, made from leading exponents of functions in $\\mathbb{C}[\\operatorname{Gr}(k,n)]$ in an $\\mathbb{X}$-cluster chart, can also be described by tropicalisation of the Marsh-Reitsch superpotential~$W$.   For any cluster tilting object $T$, with endomorphism algebra $A$, we define two new cluster characters, a generalised partition function $\\mathcal{P}^T_M\\in\\mathbb{C}[K(\\operatorname{CM}A)]$ and a generalised flow polynomial $\\mathcal{F}^T_M\\in\\mathbb{C}[K(\\operatorname{fd}A)]$, related by a `dehomogenising' map $\\operatorname{wt}\\colon K(\\operatorname{CM}A)\\to K(\\operatorname{fd}A)$.   In the $\\mathbb{X}$-cluster chart corresponding to $T$, the function $\\Psi_M$ becomes $\\mathcal{F}^T_M$ and thus its leading exponent is $\\boldsymbol{\\kappa}(T,M)$, an invariant introduced in earlier paper (and the image of the $g$-vector of $M$ under $\\operatorname{wt}$). When $T$ mutates, $\\mathcal{F}^T_M$ undergoes $\\mathbb{X}$-mutation and $\\boldsymbol{\\kappa}(T,M)$ undergoes tropical $\\mathbb{A}$-mutation.   We then show that the monoid of $g$-vectors is saturated, and that this cone can be identified with the NO-cone, so the NO-body of Rietsch--Williams can be described in terms of $\\boldsymbol{\\kappa}(T,M)$. Furthermore, we adapt Rietsch-Williams' mirror symmetry strategy to find module-theoretic inequalities that determine the cone of $g$-vectors.   Some of the machinery we develop works in a greater generality, which is relevant to the positroid subvarieties of $\\operatorname{Gr}(k,n)$.","sentences":["The homogeneous coordinate ring $\\mathbb{C}[\\operatorname{Gr}(k,n)]$ of the Grassmannian is a cluster algebra, with an additive categorification $\\operatorname{CM}C$.","Thus every $M\\in\\operatorname{CM}C$ has a cluster character $\\Psi_M\\in\\mathbb{C}[\\operatorname{Gr}(k,n)]$.   ","The aim is to use the categorification to enrich Rietsch-Williams' mirror symmetry result that the Newton-Okounkov (NO) body/cone, made from leading exponents of functions in $\\mathbb{C}[\\operatorname{Gr}(k,n)]$ in an $\\mathbb{X}$-cluster chart, can also be described by tropicalisation of the Marsh-Reitsch superpotential~$W$.   For any cluster tilting object $T$, with endomorphism algebra $A$, we define two new cluster characters, a generalised partition function $\\mathcal{P}^T_M\\in\\mathbb{C}[K(\\operatorname{CM}A)]$ and a generalised flow polynomial $\\mathcal{F}^T_M\\in\\mathbb{C}[K(\\operatorname{fd}A)]$, related by a `dehomogenising' map $\\operatorname{wt}\\colon K(\\operatorname{CM}A)\\to","K(\\operatorname{fd}A)$.   ","In the $\\mathbb{X}$-cluster chart corresponding to $T$, the function $\\Psi_M$ becomes $\\mathcal{F}^T_M$ and thus its leading exponent is $\\boldsymbol{\\kappa}(T,M)$, an invariant introduced in earlier paper (and the image of the $g$-vector of $M$ under $\\operatorname{wt}$).","When $T$ mutates, $\\mathcal{F}^T_M$ undergoes $\\mathbb{X}$-mutation and $\\boldsymbol{\\kappa}(T,M)$ undergoes tropical $\\mathbb{A}$-mutation.   ","We then show that the monoid of $g$-vectors is saturated, and that this cone can be identified with the NO-cone, so the NO-body of Rietsch--Williams can be described in terms of $\\boldsymbol{\\kappa}(T,M)$.","Furthermore, we adapt Rietsch-Williams' mirror symmetry strategy to find module-theoretic inequalities that determine the cone of $g$-vectors.   ","Some of the machinery we develop works in a greater generality, which is relevant to the positroid subvarieties of $\\operatorname{Gr}(k,n)$."],"url":"http://arxiv.org/abs/2404.14572v1","category":"math.RT"}
{"created":"2024-04-22 20:15:43","title":"Adaptive Local Binary Pattern: A Novel Feature Descriptor for Enhanced Analysis of Kidney Abnormalities in CT Scan Images using ensemble based Machine Learning Approach","abstract":"The shortage of nephrologists and the growing public health concern over renal failure have spurred the demand for AI systems capable of autonomously detecting kidney abnormalities. Renal failure, marked by a gradual decline in kidney function, can result from factors like cysts, stones, and tumors. Chronic kidney disease may go unnoticed initially, leading to untreated cases until they reach an advanced stage. The dataset, comprising 12,427 images from multiple hospitals in Dhaka, was categorized into four groups: cyst, tumor, stone, and normal. Our methodology aims to enhance CT scan image quality using Cropping, Resizing, and CALHE techniques, followed by feature extraction with our proposed Adaptive Local Binary Pattern (A-LBP) feature extraction method compared with the state-of-the-art local binary pattern (LBP) method. Our proposed features fed into classifiers such as Random Forest, Decision Tree, Naive Bayes, K-Nearest Neighbor, and SVM. We explored an ensemble model with soft voting to get a more robust model for our task. We got the highest of more than 99% in accuracy using our feature descriptor and ensembling five classifiers (Random Forest, Decision Tree, Naive Bayes, K-Nearest Neighbor, Support Vector Machine) with the soft voting method.","sentences":["The shortage of nephrologists and the growing public health concern over renal failure have spurred the demand for AI systems capable of autonomously detecting kidney abnormalities.","Renal failure, marked by a gradual decline in kidney function, can result from factors like cysts, stones, and tumors.","Chronic kidney disease may go unnoticed initially, leading to untreated cases until they reach an advanced stage.","The dataset, comprising 12,427 images from multiple hospitals in Dhaka, was categorized into four groups: cyst, tumor, stone, and normal.","Our methodology aims to enhance CT scan image quality using Cropping, Resizing, and CALHE techniques, followed by feature extraction with our proposed Adaptive Local Binary Pattern (A-LBP) feature extraction method compared with the state-of-the-art local binary pattern (LBP) method.","Our proposed features fed into classifiers such as Random Forest, Decision Tree, Naive Bayes, K-Nearest Neighbor, and SVM.","We explored an ensemble model with soft voting to get a more robust model for our task.","We got the highest of more than 99% in accuracy using our feature descriptor and ensembling five classifiers (Random Forest, Decision Tree, Naive Bayes, K-Nearest Neighbor, Support Vector Machine) with the soft voting method."],"url":"http://arxiv.org/abs/2404.14560v1","category":"cs.CV"}
{"created":"2024-04-22 19:36:30","title":"Closing the Perception-Action Loop for Semantically Safe Navigation in Semi-Static Environments","abstract":"Autonomous robots navigating in changing environments demand adaptive navigation strategies for safe long-term operation. While many modern control paradigms offer theoretical guarantees, they often assume known extrinsic safety constraints, overlooking challenges when deployed in real-world environments where objects can appear, disappear, and shift over time. In this paper, we present a closed-loop perception-action pipeline that bridges this gap. Our system encodes an online-constructed dense map, along with object-level semantic and consistency estimates into a control barrier function (CBF) to regulate safe regions in the scene. A model predictive controller (MPC) leverages the CBF-based safety constraints to adapt its navigation behaviour, which is particularly crucial when potential scene changes occur. We test the system in simulations and real-world experiments to demonstrate the impact of semantic information and scene change handling on robot behavior, validating the practicality of our approach.","sentences":["Autonomous robots navigating in changing environments demand adaptive navigation strategies for safe long-term operation.","While many modern control paradigms offer theoretical guarantees, they often assume known extrinsic safety constraints, overlooking challenges when deployed in real-world environments where objects can appear, disappear, and shift over time.","In this paper, we present a closed-loop perception-action pipeline that bridges this gap.","Our system encodes an online-constructed dense map, along with object-level semantic and consistency estimates into a control barrier function (CBF) to regulate safe regions in the scene.","A model predictive controller (MPC) leverages the CBF-based safety constraints to adapt its navigation behaviour, which is particularly crucial when potential scene changes occur.","We test the system in simulations and real-world experiments to demonstrate the impact of semantic information and scene change handling on robot behavior, validating the practicality of our approach."],"url":"http://arxiv.org/abs/2404.14546v1","category":"cs.RO"}
{"created":"2024-04-22 19:29:12","title":"UVEB: A Large-scale Benchmark and Baseline Towards Real-World Underwater Video Enhancement","abstract":"Learning-based underwater image enhancement (UIE) methods have made great progress. However, the lack of large-scale and high-quality paired training samples has become the main bottleneck hindering the development of UIE. The inter-frame information in underwater videos can accelerate or optimize the UIE process. Thus, we constructed the first large-scale high-resolution underwater video enhancement benchmark (UVEB) to promote the development of underwater vision.It contains 1,308 pairs of video sequences and more than 453,000 high-resolution with 38\\% Ultra-High-Definition (UHD) 4K frame pairs. UVEB comes from multiple countries, containing various scenes and video degradation types to adapt to diverse and complex underwater environments. We also propose the first supervised underwater video enhancement method, UVE-Net. UVE-Net converts the current frame information into convolutional kernels and passes them to adjacent frames for efficient inter-frame information exchange. By fully utilizing the redundant degraded information of underwater videos, UVE-Net completes video enhancement better. Experiments show the effective network design and good performance of UVE-Net.","sentences":["Learning-based underwater image enhancement (UIE) methods have made great progress.","However, the lack of large-scale and high-quality paired training samples has become the main bottleneck hindering the development of UIE.","The inter-frame information in underwater videos can accelerate or optimize the UIE process.","Thus, we constructed the first large-scale high-resolution underwater video enhancement benchmark (UVEB) to promote the development of underwater vision.","It contains 1,308 pairs of video sequences and more than 453,000 high-resolution with 38\\% Ultra-High-Definition (UHD) 4K frame pairs.","UVEB comes from multiple countries, containing various scenes and video degradation types to adapt to diverse and complex underwater environments.","We also propose the first supervised underwater video enhancement method, UVE-Net.","UVE-Net converts the current frame information into convolutional kernels and passes them to adjacent frames for efficient inter-frame information exchange.","By fully utilizing the redundant degraded information of underwater videos, UVE-Net completes video enhancement better.","Experiments show the effective network design and good performance of UVE-Net."],"url":"http://arxiv.org/abs/2404.14542v1","category":"cs.CV"}
{"created":"2024-04-22 19:21:34","title":"Designing open quantum systems with known steady states: Davies generators and beyond","abstract":"We provide a systematic framework for constructing generic models of nonequilibrium quantum dynamics with a target stationary (mixed) state. Our framework identifies (almost) all combinations of Hamiltonian and dissipative dynamics that relax to a steady state of interest, generalizing the Davies' generator for dissipative relaxation at finite temperature to nonequilibrium dynamics targeting arbitrary stationary states. We focus on Gibbs states of stabilizer Hamiltonians, identifying local Lindbladians compatible therewith by constraining the rates of dissipative and unitary processes. Moreover, given terms in the Lindbladian not compatible with the target state, our formalism identifies the operations -- including syndrome measurements and local feedback -- one must apply to correct these errors. Our methods also reveal new models of quantum dynamics: for example, we provide a \"measurement-induced phase transition\" where measurable two-point functions exhibit critical (power-law) scaling with distance at a critical ratio of the transverse field and rate of measurement and feedback. Time-reversal symmetry -- defined naturally within our formalism -- can be broken both in effectively classical, and intrinsically quantum, ways. Our framework provides a systematic starting point for exploring the landscape of quantum dynamical universality classes, as well as identifying new protocols for quantum error correction.","sentences":["We provide a systematic framework for constructing generic models of nonequilibrium quantum dynamics with a target stationary (mixed) state.","Our framework identifies (almost) all combinations of Hamiltonian and dissipative dynamics that relax to a steady state of interest, generalizing the Davies' generator for dissipative relaxation at finite temperature to nonequilibrium dynamics targeting arbitrary stationary states.","We focus on Gibbs states of stabilizer Hamiltonians, identifying local Lindbladians compatible therewith by constraining the rates of dissipative and unitary processes.","Moreover, given terms in the Lindbladian not compatible with the target state, our formalism identifies the operations -- including syndrome measurements and local feedback -- one must apply to correct these errors.","Our methods also reveal new models of quantum dynamics: for example, we provide a \"measurement-induced phase transition\" where measurable two-point functions exhibit critical (power-law) scaling with distance at a critical ratio of the transverse field and rate of measurement and feedback.","Time-reversal symmetry -- defined naturally within our formalism -- can be broken both in effectively classical, and intrinsically quantum, ways.","Our framework provides a systematic starting point for exploring the landscape of quantum dynamical universality classes, as well as identifying new protocols for quantum error correction."],"url":"http://arxiv.org/abs/2404.14538v1","category":"quant-ph"}
{"created":"2024-04-22 18:59:07","title":"Electric Field Gradient Calculations for Ice VIII and IX using Polarizable Embedding: A Comparative Study on Classical Computers and Quantum Simulators","abstract":"We test the performance of the Polarizable Embedding Variational Quantum Eigensolver Self-Consistent-Field (PE-VQE-SCF) model for computing electric field gradients with comparisons to conventional complete active space self-consistent-field (CASSCF) calculations and experimental results. We compute quadrupole coupling constants for ice VIII and ice IX. We find that the inclusion of the environment is crucial for obtaining results that match the experimental data. The calculations for ice VIII are within the experimental uncertainty for both CASSCF and VQE-SCF for oxygen and lie close to the experimental value for ice IX as well. With the VQE-SCF, which is based on an Adaptive Derivative-Assembled Problem-Tailored (ADAPT) ansatz, we find that the inclusion of the environment and the size of the different basis sets do not directly affect the gate counts. However, by including an explicit environment, the wavefunction and, therefore, the optimization problem becomes more complicated, which usually results in the need to include more operators from the operator pool, thereby increasing the depth of the circuit.","sentences":["We test the performance of the Polarizable Embedding Variational Quantum Eigensolver Self-Consistent-Field (PE-VQE-SCF) model for computing electric field gradients with comparisons to conventional complete active space self-consistent-field (CASSCF) calculations and experimental results.","We compute quadrupole coupling constants for ice VIII and ice IX.","We find that the inclusion of the environment is crucial for obtaining results that match the experimental data.","The calculations for ice VIII are within the experimental uncertainty for both CASSCF and VQE-SCF for oxygen and lie close to the experimental value for ice IX as well.","With the VQE-SCF, which is based on an Adaptive Derivative-Assembled Problem-Tailored (ADAPT) ansatz, we find that the inclusion of the environment and the size of the different basis sets do not directly affect the gate counts.","However, by including an explicit environment, the wavefunction and, therefore, the optimization problem becomes more complicated, which usually results in the need to include more operators from the operator pool, thereby increasing the depth of the circuit."],"url":"http://arxiv.org/abs/2404.14531v1","category":"quant-ph"}
{"created":"2024-04-23 17:46:50","title":"FlowMap: High-Quality Camera Poses, Intrinsics, and Depth via Gradient Descent","abstract":"This paper introduces FlowMap, an end-to-end differentiable method that solves for precise camera poses, camera intrinsics, and per-frame dense depth of a video sequence. Our method performs per-video gradient-descent minimization of a simple least-squares objective that compares the optical flow induced by depth, intrinsics, and poses against correspondences obtained via off-the-shelf optical flow and point tracking. Alongside the use of point tracks to encourage long-term geometric consistency, we introduce differentiable re-parameterizations of depth, intrinsics, and pose that are amenable to first-order optimization. We empirically show that camera parameters and dense depth recovered by our method enable photo-realistic novel view synthesis on 360-degree trajectories using Gaussian Splatting. Our method not only far outperforms prior gradient-descent based bundle adjustment methods, but surprisingly performs on par with COLMAP, the state-of-the-art SfM method, on the downstream task of 360-degree novel view synthesis (even though our method is purely gradient-descent based, fully differentiable, and presents a complete departure from conventional SfM).","sentences":["This paper introduces FlowMap, an end-to-end differentiable method that solves for precise camera poses, camera intrinsics, and per-frame dense depth of a video sequence.","Our method performs per-video gradient-descent minimization of a simple least-squares objective that compares the optical flow induced by depth, intrinsics, and poses against correspondences obtained via off-the-shelf optical flow and point tracking.","Alongside the use of point tracks to encourage long-term geometric consistency, we introduce differentiable re-parameterizations of depth, intrinsics, and pose that are amenable to first-order optimization.","We empirically show that camera parameters and dense depth recovered by our method enable photo-realistic novel view synthesis on 360-degree trajectories using Gaussian Splatting.","Our method not only far outperforms prior gradient-descent based bundle adjustment methods, but surprisingly performs on par with COLMAP, the state-of-the-art SfM method, on the downstream task of 360-degree novel view synthesis (even though our method is purely gradient-descent based, fully differentiable, and presents a complete departure from conventional SfM)."],"url":"http://arxiv.org/abs/2404.15259v1","category":"cs.CV"}
{"created":"2024-04-23 16:44:46","title":"A discretization of the iterated integral expression of the multiple polylogarithm","abstract":"Recently, Maesaka, Watanabe, and the third author discovered a phenomenon where the iterated integral expressions of multiple zeta values become discretized. In this paper, we extend their result to the case of multiple polylogarithms and provide two proofs. The first proof uses the method of connected sums, while the second employs induction based on the difference equations that discrete multiple polylogarithms satisfy. We also investigate several applications of our main result.","sentences":["Recently, Maesaka, Watanabe, and the third author discovered a phenomenon where the iterated integral expressions of multiple zeta values become discretized.","In this paper, we extend their result to the case of multiple polylogarithms and provide two proofs.","The first proof uses the method of connected sums, while the second employs induction based on the difference equations that discrete multiple polylogarithms satisfy.","We also investigate several applications of our main result."],"url":"http://arxiv.org/abs/2404.15210v1","category":"math.NT"}
{"created":"2024-04-23 16:38:49","title":"Heat flow, log-concavity, and Lipschitz transport maps","abstract":"In this paper we derive estimates for the Hessian of the logarithm (log-Hessian) for solutions to the heat equation. For initial data in the form of log-Lipschitz perturbation of strongly log-concave measures, the log-Hessian admits an explicit, uniform (in space) lower bound. This yields a new estimate for the Lipschitz constant of a transport map pushing forward the standard Gaussian to a measure in this class. Further connections are discussed with score-based diffusion models and improved Gaussian logarithmic Sobolev inequalities. Finally, we show that assuming only fast decay of the tails of the initial datum does not suffice to guarantee uniform log-Hessian upper bounds.","sentences":["In this paper we derive estimates for the Hessian of the logarithm (log-Hessian) for solutions to the heat equation.","For initial data in the form of log-Lipschitz perturbation of strongly log-concave measures, the log-Hessian admits an explicit, uniform (in space) lower bound.","This yields a new estimate for the Lipschitz constant of a transport map pushing forward the standard Gaussian to a measure in this class.","Further connections are discussed with score-based diffusion models and improved Gaussian logarithmic Sobolev inequalities.","Finally, we show that assuming only fast decay of the tails of the initial datum does not suffice to guarantee uniform log-Hessian upper bounds."],"url":"http://arxiv.org/abs/2404.15205v1","category":"math.AP"}
{"created":"2024-04-23 16:36:38","title":"Quasi-waveguide amplifiers based on bulk laser gain media in Herriott-type multipass cells","abstract":"We present here a new geometry for laser amplifiers based on bulk gain media. The overlapped seed and pump beams are repetitively refocused into the gain medium with a Herriott-type multipass cell. Similar to a waveguide, this configuration allows for a confined propagation inside the gain medium over much longer lengths than in ordinary single pass bulk amplifiers. Inside the gain medium, the foci appear at separate locations. A proof-of-principle demonstration with Ti:sapphire indicates that this could lead to higher amplification due to a distribution of the thermal load.","sentences":["We present here a new geometry for laser amplifiers based on bulk gain media.","The overlapped seed and pump beams are repetitively refocused into the gain medium with a Herriott-type multipass cell.","Similar to a waveguide, this configuration allows for a confined propagation inside the gain medium over much longer lengths than in ordinary single pass bulk amplifiers.","Inside the gain medium, the foci appear at separate locations.","A proof-of-principle demonstration with Ti:sapphire indicates that this could lead to higher amplification due to a distribution of the thermal load."],"url":"http://arxiv.org/abs/2404.15202v1","category":"physics.optics"}
{"created":"2024-04-23 16:26:38","title":"Statistics of three-dimensional black holes from Liouville line defects","abstract":"Black holes and wormholes in the gravitational path integral can be used to calculate the statistics of heavy operators. An explicit example in higher dimensions is provided by thin shells of matter. We study these solutions in 3D gravity, and reproduce the behavior of black holes and wormholes from the dual CFT using the large-$c$ conformal bootstrap. The CFT operator that creates a thin shell black hole is a line defect, so we begin by using the bootstrap to study the statistics of line defects, both at finite $c$ and in the holographic large-$c$ limit. The crossing equation leads to a universal formula for the average high-energy matrix elements of the line defect in any compact, unitary 2d CFT with $c>1$. The asymptotics are controlled by a line defect in Liouville CFT at the same value of the central charge. At large $c$, three distinct quantities are related: The statistics of line defects in holographic CFTs, the individual matrix elements of a line defect in Liouville CFT, and the on-shell action of black holes and wormholes in 3D gravity. The three calculations match for black holes, and if the statistics of the line defects are assumed to be approximately Gaussian, then a class of wormholes is also reproduced by the dual CFT.","sentences":["Black holes and wormholes in the gravitational path integral can be used to calculate the statistics of heavy operators.","An explicit example in higher dimensions is provided by thin shells of matter.","We study these solutions in 3D gravity, and reproduce the behavior of black holes and wormholes from the dual CFT using the large-$c$ conformal bootstrap.","The CFT operator that creates a thin shell black hole is a line defect, so we begin by using the bootstrap to study the statistics of line defects, both at finite $c$ and in the holographic large-$c$ limit.","The crossing equation leads to a universal formula for the average high-energy matrix elements of the line defect in any compact, unitary 2d CFT with $c>1$. The asymptotics are controlled by a line defect in Liouville CFT at the same value of the central charge.","At large $c$, three distinct quantities are related: The statistics of line defects in holographic CFTs, the individual matrix elements of a line defect in Liouville CFT, and the on-shell action of black holes and wormholes in 3D gravity.","The three calculations match for black holes, and if the statistics of the line defects are assumed to be approximately Gaussian, then a class of wormholes is also reproduced by the dual CFT."],"url":"http://arxiv.org/abs/2404.15183v1","category":"hep-th"}
{"created":"2024-04-23 13:55:29","title":"New exotic examples of Ricci limit spaces","abstract":"For any integers $m\\geqslant n\\geqslant 3$, we construct a Ricci limit space $X_{m,n}$ such that for a fixed point, some tangent cones are $\\mathbb{R}^m$ and some are $\\mathbb{R}^n$. This is an improvement of Menguy's example.","sentences":["For any integers $m\\geqslant n\\geqslant 3$, we construct a Ricci limit space $X_{m,n}$ such that for a fixed point, some tangent cones are $\\mathbb{R}^m$ and some are $\\mathbb{R}^n$. This is an improvement of Menguy's example."],"url":"http://arxiv.org/abs/2404.15054v1","category":"math.DG"}
{"created":"2024-04-23 13:36:31","title":"Quantum study of the CH$_3^+$ photodissociation in full dimension Neural Networks potential energy surfaces","abstract":"CH$_3^+$, a cornerstone intermediate in interstellar chemistry, has recently been detected for the first time by the James Webb Space Telescope. The photodissociation of this ion is studied here. Accurate explicitly correlated multi-reference configuration interaction {\\it ab initio} calculations are done, and full dimensional potential energy surfaces are developed for the three lower electronic states, with a fundamental invariant neural network method. The photodissociation cross section is calculated using a full dimensional quantum wave packet method, in heliocentric Radau coordinates. The wave packet is represented in angular and radial grids allowing to reduce the number of points physically accessible, requiring to push up the spurious states appearing when evaluating the angular kinetic terms, through a projection technique. The photodissociation spectra, when employed in astrochemical models to simulate the conditions of the Orion Bar, results in a lesser destruction of CH$_3^+$ compared to that obtained when utilizing the recommended values in the kinetic database for astrochemistry (KIDA).","sentences":["CH$_3^+$, a cornerstone intermediate in interstellar chemistry, has recently been detected for the first time by the James Webb Space Telescope.","The photodissociation of this ion is studied here.","Accurate explicitly correlated multi-reference configuration interaction {\\it ab initio} calculations are done, and full dimensional potential energy surfaces are developed for the three lower electronic states, with a fundamental invariant neural network method.","The photodissociation cross section is calculated using a full dimensional quantum wave packet method, in heliocentric Radau coordinates.","The wave packet is represented in angular and radial grids allowing to reduce the number of points physically accessible, requiring to push up the spurious states appearing when evaluating the angular kinetic terms, through a projection technique.","The photodissociation spectra, when employed in astrochemical models to simulate the conditions of the Orion Bar, results in a lesser destruction of CH$_3^+$ compared to that obtained when utilizing the recommended values in the kinetic database for astrochemistry (KIDA)."],"url":"http://arxiv.org/abs/2404.15032v1","category":"astro-ph.SR"}
{"created":"2024-04-23 13:25:41","title":"A new derivation of the amplitude of asymptotic oscillatory tails of weakly delocalized solitons","abstract":"The computation of the amplitude, $\\alpha$, of asymptotic standing wave tails of weakly delocalized, stationary solutions in a fifth-order Korteweg-de Vries equation is revisited. Assuming the coefficient of the fifth order derivative term, $\\epsilon^2\\ll1$, a new derivation of the ``beyond all orders in $\\epsilon$'' amplitude, $\\alpha$, is presented. It is shown by asymptotic matching techniques, extended to higher orders in $\\epsilon$, that the value of $\\alpha$ can be obtained from the asymmetry at the center of the unique solution exponentially decaying in one direction. This observation, complemented by some fundamental results of Hammersley and Mazzarino [Proc. R. Soc. Lond. A 424, 19 (1989)], not only sheds new light on the computation of $\\alpha$, but also greatly facilitates its numerical determination to a remarkable precision for so small values of $\\epsilon$, which are beyond the capabilities of standard numerical methods.","sentences":["The computation of the amplitude, $\\alpha$, of asymptotic standing wave tails of weakly delocalized, stationary solutions in a fifth-order Korteweg-de Vries equation is revisited.","Assuming the coefficient of the fifth order derivative term, $\\epsilon^2\\ll1$, a new derivation of the ``beyond all orders in $\\epsilon$'' amplitude, $\\alpha$, is presented.","It is shown by asymptotic matching techniques, extended to higher orders in $\\epsilon$, that the value of $\\alpha$ can be obtained from the asymmetry at the center of the unique solution exponentially decaying in one direction.","This observation, complemented by some fundamental results of Hammersley and Mazzarino","[Proc. R. Soc.","Lond.","A 424, 19 (1989)], not only sheds new light on the computation of $\\alpha$, but also greatly facilitates its numerical determination to a remarkable precision for so small values of $\\epsilon$, which are beyond the capabilities of standard numerical methods."],"url":"http://arxiv.org/abs/2404.15020v1","category":"hep-th"}
{"created":"2024-04-23 13:11:27","title":"Pressure-Gated Microfluidic Memristor for Pulsatile Information Processing","abstract":"A hitherto unexploited characteristic feature of emerging iontronic devices for information processing is the intrinsic mobility of the medium (water) of dissolved ions in aqueous electrolytes, which therefore not only respond to voltage but also to pressure. Here we study a microfluidic memristor, in the form of a conical channel, exposed to simultaneously applied time-dependent voltage and pressure drops, through numerical solutions of the Poisson-Nernst-Planck-Stokes equations for ion and fluid transport. We show that the channel's memristive properties can be enhanced, reduced or instantaneously reset by a suitable pressure, and we leverage this finding with two examples of time series processing of simultaneously applied voltage and pressure pulses. We not only show that the distinction between different voltage time series can be improved by enhancing the conductance response with corresponding pressure pulses, but also that the bandwidth of information transfer through the channel can be doubled by letting the pressure pulses represent a second independent time series.","sentences":["A hitherto unexploited characteristic feature of emerging iontronic devices for information processing is the intrinsic mobility of the medium (water) of dissolved ions in aqueous electrolytes, which therefore not only respond to voltage but also to pressure.","Here we study a microfluidic memristor, in the form of a conical channel, exposed to simultaneously applied time-dependent voltage and pressure drops, through numerical solutions of the Poisson-Nernst-Planck-Stokes equations for ion and fluid transport.","We show that the channel's memristive properties can be enhanced, reduced or instantaneously reset by a suitable pressure, and we leverage this finding with two examples of time series processing of simultaneously applied voltage and pressure pulses.","We not only show that the distinction between different voltage time series can be improved by enhancing the conductance response with corresponding pressure pulses, but also that the bandwidth of information transfer through the channel can be doubled by letting the pressure pulses represent a second independent time series."],"url":"http://arxiv.org/abs/2404.15006v1","category":"cond-mat.soft"}
{"created":"2024-04-23 12:45:32","title":"A characterization of Pfaffian embeddings from (2, 3, 5)- into flat (4, 7)-geometries","abstract":"Given two smooth manifolds with tangent subbundle distributions, an embedding is Pfaffian if its differential sends the distribution on the source into the distribution on the target. In this paper, we consider the question of existence of Pfaffian embeddings in the specific case where the source is a (2,3,5)-manifold, the target is the 7-dimensional space of isotropic 2-planes in a 6-dimensional symplectic vector space, and the Pfaffian condition is that the derived 3-distribution on the source be mapped into the natural 4-distribution on the target. This is one of the simpler non-trivial cases of the general question on existence of Pfaffian embeddings, but already the answer here requires solution of an interesting differential equation. It turns out that a generic (2,3,5)-manifold does not embed, the first obstruction being the fact that a Pfaffian embeddable (2,3,5)-manifold necessarily has a double root for its Cartan quartic at each point. We determine a complete characterization of embeddable (2,3,5)-manifolds in terms of their associated Cartan geometries, which characterization depends on higher order (non-harmonic) curvature as well.","sentences":["Given two smooth manifolds with tangent subbundle distributions, an embedding is Pfaffian if its differential sends the distribution on the source into the distribution on the target.","In this paper, we consider the question of existence of Pfaffian embeddings in the specific case where the source is a (2,3,5)-manifold, the target is the 7-dimensional space of isotropic 2-planes in a 6-dimensional symplectic vector space, and the Pfaffian condition is that the derived 3-distribution on the source be mapped into the natural 4-distribution on the target.","This is one of the simpler non-trivial cases of the general question on existence of Pfaffian embeddings, but already the answer here requires solution of an interesting differential equation.","It turns out that a generic (2,3,5)-manifold does not embed, the first obstruction being the fact that a Pfaffian embeddable (2,3,5)-manifold necessarily has a double root for its Cartan quartic at each point.","We determine a complete characterization of embeddable (2,3,5)-manifolds in terms of their associated Cartan geometries, which characterization depends on higher order (non-harmonic) curvature as well."],"url":"http://arxiv.org/abs/2404.14988v1","category":"math.DG"}
{"created":"2024-04-23 12:39:49","title":"Surface profile recovery from electromagnetic field with physics--informed neural networks","abstract":"Physics--informed neural networks (PINN) have shown their potential in solving both direct and inverse problems of partial differential equations. In this paper, we introduce a PINN-based deep learning approach to reconstruct one-dimensional rough surfaces from field data illuminated by an electromagnetic incident wave. In the proposed algorithm, the rough surface is approximated by a neural network, with which the spatial derivatives of surface function can be obtained via automatic differentiation and then the scattered field can be calculated via the method of moments. The neural network is trained by minimizing the loss between the calculated and the observed field data. Furthermore, the proposed method is an unsupervised approach, independent of any surface data, rather only the field data is used. Both TE field (Dirichlet boundary condition) and TM field (Neumann boundary condition) are considered. Two types of field data are used here: full scattered field data and phaseless total field data. The performance of the method is verified by testing with Gaussian-correlated random rough surfaces. Numerical results demonstrate that the PINN-based method can recover rough surfaces with great accuracy and is robust with respect to a wide range of problem regimes.","sentences":["Physics--informed neural networks (PINN) have shown their potential in solving both direct and inverse problems of partial differential equations.","In this paper, we introduce a PINN-based deep learning approach to reconstruct one-dimensional rough surfaces from field data illuminated by an electromagnetic incident wave.","In the proposed algorithm, the rough surface is approximated by a neural network, with which the spatial derivatives of surface function can be obtained via automatic differentiation and then the scattered field can be calculated via the method of moments.","The neural network is trained by minimizing the loss between the calculated and the observed field data.","Furthermore, the proposed method is an unsupervised approach, independent of any surface data, rather only the field data is used.","Both TE field (Dirichlet boundary condition) and TM field (Neumann boundary condition) are considered.","Two types of field data are used here: full scattered field data and phaseless total field data.","The performance of the method is verified by testing with Gaussian-correlated random rough surfaces.","Numerical results demonstrate that the PINN-based method can recover rough surfaces with great accuracy and is robust with respect to a wide range of problem regimes."],"url":"http://arxiv.org/abs/2404.14984v1","category":"cs.CE"}
{"created":"2024-04-23 11:45:23","title":"Nuclear mass predictions based on convolutional neural network","abstract":"A convolutional neural network (CNN) is employed to investigate nuclear mass. By introducing the masses of neighboring nuclei and the paring effects at the input layer of the network, local features of the target nucleus are extracted to predict its mass. Then, through learning the differences between the predicted nuclear masses by the WS4 model and the experimental nuclear masses, a new global-local model (CNN-WS4) is developed, which incorporates both the global nuclear mass model and local features. This model achieves an accuracy of 0.070 MeV for the nuclei with $Z\\geqslant8$ and $N\\geqslant8$ in AME2016, significantly enhancing the accuracy of nuclear mass prediction. When extrapolating for newly emerged nuclei in AME2020, the CNN-WS4 also exhibits appreciable stability, thereby demonstrating its robustness.","sentences":["A convolutional neural network (CNN) is employed to investigate nuclear mass.","By introducing the masses of neighboring nuclei and the paring effects at the input layer of the network, local features of the target nucleus are extracted to predict its mass.","Then, through learning the differences between the predicted nuclear masses by the WS4 model and the experimental nuclear masses, a new global-local model (CNN-WS4) is developed, which incorporates both the global nuclear mass model and local features.","This model achieves an accuracy of 0.070 MeV for the nuclei with $Z\\geqslant8$ and $N\\geqslant8$ in AME2016, significantly enhancing the accuracy of nuclear mass prediction.","When extrapolating for newly emerged nuclei in AME2020, the CNN-WS4 also exhibits appreciable stability, thereby demonstrating its robustness."],"url":"http://arxiv.org/abs/2404.14948v1","category":"nucl-th"}
{"created":"2024-04-23 11:14:44","title":"Polyepitaxial grain matching to study the oxidation of uranium dioxide","abstract":"Although the principal physical behaviour of a material is inherently connected to its fundamental crystal structure, the behaviours observed in the real-world are often driven by the microstructure, which for many polycrystalline materials, equates to the size and shape of the constituent crystal grains. Here we highlight a cutting edge synthesis route to the controlled engineering of grain structures in thin films and the simplification of associated 3-dimensional problems to less complex 2D ones. This has been applied to the actinide ceramic, uranium dioxide, to replicate structures typical in nuclear fission fuel pellets, in order to investigate the oxidation and subsequent transformation of cubic UO$_{2}$ to orthorhombic U$_{3}$O$_{8}$. This article shows how this synthesis approach could be utilised to investigate a range of phenomena, affected by grain morphology, and highlights some unusual results in the oxidation behaviour of UO$_{2}$, regarding the phase transition to U$_{3}$O$_{8}$.","sentences":["Although the principal physical behaviour of a material is inherently connected to its fundamental crystal structure, the behaviours observed in the real-world are often driven by the microstructure, which for many polycrystalline materials, equates to the size and shape of the constituent crystal grains.","Here we highlight a cutting edge synthesis route to the controlled engineering of grain structures in thin films and the simplification of associated 3-dimensional problems to less complex 2D ones.","This has been applied to the actinide ceramic, uranium dioxide, to replicate structures typical in nuclear fission fuel pellets, in order to investigate the oxidation and subsequent transformation of cubic UO$_{2}$ to orthorhombic U$_{3}$O$_{8}$.","This article shows how this synthesis approach could be utilised to investigate a range of phenomena, affected by grain morphology, and highlights some unusual results in the oxidation behaviour of UO$_{2}$, regarding the phase transition to U$_{3}$O$_{8}$."],"url":"http://arxiv.org/abs/2404.14929v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-23 11:08:27","title":"Existence of weak solutions for a class of non-divergent parabolic equations with variable exponent","abstract":"A doubly degenerate parabolic equation in non-divergent form with variable growth is investigated in this paper. In suitable spaces, we prove the existence of weak solutions of the equation for cases $1\\leq m < 2$ and $m\\geq 2$ in different ways. And we establish the non-expansion of support of the solution for the problem.","sentences":["A doubly degenerate parabolic equation in non-divergent form with variable growth is investigated in this paper.","In suitable spaces, we prove the existence of weak solutions of the equation for cases $1\\leq m < 2$ and $m\\geq 2$ in different ways.","And we establish the non-expansion of support of the solution for the problem."],"url":"http://arxiv.org/abs/2404.14918v1","category":"math.AP"}
{"created":"2024-04-23 10:59:20","title":"Anomalous Polarization in One-dimensional Aperiodic Insulators","abstract":"By implementing a charge pumping scheme for one-dimensional aperiodic chains, we confirm the existence of topological phases in these systems whenever their finite-size realizations admit inversion symmetry. These phases are usually characterized by an anomalous edge response as a result of the bulk-boundary correspondence. We show that these signatures are all present in various chains, each representing a different class of structural aperiodicity: the Fibonacci quasicrystal, the Tribonacci quasicrystal, and the Thue-Morse chain. More specifically, we calculate three quantities: the Berry phase of the crystalline approximation of the finite-size systems, the polarization response to an inifintesimal static and constant electric field in systems with open boundary conditions, and the degeneracy of the entanglement spectrum. We find that all of them provide signatures of a topologically nontrivial phase.","sentences":["By implementing a charge pumping scheme for one-dimensional aperiodic chains, we confirm the existence of topological phases in these systems whenever their finite-size realizations admit inversion symmetry.","These phases are usually characterized by an anomalous edge response as a result of the bulk-boundary correspondence.","We show that these signatures are all present in various chains, each representing a different class of structural aperiodicity: the Fibonacci quasicrystal, the Tribonacci quasicrystal, and the Thue-Morse chain.","More specifically, we calculate three quantities: the Berry phase of the crystalline approximation of the finite-size systems, the polarization response to an inifintesimal static and constant electric field in systems with open boundary conditions, and the degeneracy of the entanglement spectrum.","We find that all of them provide signatures of a topologically nontrivial phase."],"url":"http://arxiv.org/abs/2404.14916v1","category":"cond-mat.dis-nn"}
{"created":"2024-04-23 10:51:31","title":"MultiSTOP: Solving Functional Equations with Reinforcement Learning","abstract":"We develop MultiSTOP, a Reinforcement Learning framework for solving functional equations in physics. This new methodology produces actual numerical solutions instead of bounds on them. We extend the original BootSTOP algorithm by adding multiple constraints derived from domain-specific knowledge, even in integral form, to improve the accuracy of the solution. We investigate a particular equation in a one-dimensional Conformal Field Theory.","sentences":["We develop MultiSTOP, a Reinforcement Learning framework for solving functional equations in physics.","This new methodology produces actual numerical solutions instead of bounds on them.","We extend the original BootSTOP algorithm by adding multiple constraints derived from domain-specific knowledge, even in integral form, to improve the accuracy of the solution.","We investigate a particular equation in a one-dimensional Conformal Field Theory."],"url":"http://arxiv.org/abs/2404.14909v1","category":"cs.LG"}
{"created":"2024-04-23 10:37:34","title":"Non-trivial fixed point of a $\u03c8^4_d$ fermionic theory, II. Anomalous exponent and scaling operators","abstract":"We consider the Renormalization Group (RG) fixed-point theory associated with a fermionic $\\psi^4_d$ model in $d=1,2,3$ with fractional kinetic term, whose scaling dimension is fixed so that the quartic interaction is weakly relevant in the RG sense. The model is defined in terms of a Grassmann functional integral with interaction $V^*$, solving a fixed-point RG equation in the presence of external fields, and a fixed ultraviolet cutoff. We define and construct the field and density scale-invariant response functions, and prove that the critical exponent of the former is the naive one, while that of the latter is anomalous and analytic. We construct the corresponding (almost-)scaling operators, whose two point correlations are scale-invariant up to a remainder term, which decays like a stretched exponential at distances larger than the inverse of the ultraviolet cutoff. Our proof is based on constructive RG methods and, specifically, on a convergent tree expansion for the generating function of correlations, which generalizes the approach developed by three of the authors in a previous publication [A. Giuliani, V. Mastropietro, S. Rychkov, JHEP 01 (2021) 026].","sentences":["We consider the Renormalization Group (RG) fixed-point theory associated with a fermionic $\\psi^4_d$ model in $d=1,2,3$ with fractional kinetic term, whose scaling dimension is fixed so that the quartic interaction is weakly relevant in the RG sense.","The model is defined in terms of a Grassmann functional integral with interaction $V^*$, solving a fixed-point RG equation in the presence of external fields, and a fixed ultraviolet cutoff.","We define and construct the field and density scale-invariant response functions, and prove that the critical exponent of the former is the naive one, while that of the latter is anomalous and analytic.","We construct the corresponding (almost-)scaling operators, whose two point correlations are scale-invariant up to a remainder term, which decays like a stretched exponential at distances larger than the inverse of the ultraviolet cutoff.","Our proof is based on constructive RG methods and, specifically, on a convergent tree expansion for the generating function of correlations, which generalizes the approach developed by three of the authors in a previous publication [A. Giuliani, V. Mastropietro, S. Rychkov, JHEP 01 (2021) 026]."],"url":"http://arxiv.org/abs/2404.14904v1","category":"math-ph"}
{"created":"2024-04-23 10:35:21","title":"Uniqueness in law for singular degenerate SDEs with respect to a (sub-)invariant measure","abstract":"We show weak existence and uniqueness in law for a general class of stochastic differential equations in $\\mathbb{R}^d$, $d\\ge 1$, with prescribed sub-invariant measure $\\widehat{\\mu}$. The dispersion and drift coefficients of the stochastic differential equation are allowed to be degenerate and discontinuous, and locally unbounded, respectively. Uniqueness in law is obtained via $L^1(\\mathbb{R}^d,\\widehat{\\mu})$-uniqueness in a subclass of continuous Markov processes, namely right processes that have $\\widehat{\\mu}$ as sub-invariant measure and have continuous paths for $\\widehat{\\mu}$-almost every starting point. Weak existence is obtained for a broader class via the martingale problem.","sentences":["We show weak existence and uniqueness in law for a general class of stochastic differential equations in $\\mathbb{R}^d$, $d\\ge 1$, with prescribed sub-invariant measure $\\widehat{\\mu}$. The dispersion and drift coefficients of the stochastic differential equation are allowed to be degenerate and discontinuous, and locally unbounded, respectively.","Uniqueness in law is obtained via $L^1(\\mathbb{R}^d,\\widehat{\\mu})$-uniqueness in a subclass of continuous Markov processes, namely right processes that have $\\widehat{\\mu}$ as sub-invariant measure and have continuous paths for $\\widehat{\\mu}$-almost every starting point.","Weak existence is obtained for a broader class via the martingale problem."],"url":"http://arxiv.org/abs/2404.14902v1","category":"math.PR"}
{"created":"2024-04-23 10:01:43","title":"Estimating the Distribution of Parameters in Differential Equations with Repeated Cross-Sectional Data","abstract":"Differential equations are pivotal in modeling and understanding the dynamics of various systems, offering insights into their future states through parameter estimation fitted to time series data. In fields such as economy, politics, and biology, the observation data points in the time series are often independently obtained (i.e., Repeated Cross-Sectional (RCS) data). With RCS data, we found that traditional methods for parameter estimation in differential equations, such as using mean values of time trajectories or Gaussian Process-based trajectory generation, have limitations in estimating the shape of parameter distributions, often leading to a significant loss of data information. To address this issue, we introduce a novel method, Estimation of Parameter Distribution (EPD), providing accurate distribution of parameters without loss of data information. EPD operates in three main steps: generating synthetic time trajectories by randomly selecting observed values at each time point, estimating parameters of a differential equation that minimize the discrepancy between these trajectories and the true solution of the equation, and selecting the parameters depending on the scale of discrepancy. We then evaluated the performance of EPD across several models, including exponential growth, logistic population models, and target cell-limited models with delayed virus production, demonstrating its superiority in capturing the shape of parameter distributions. Furthermore, we applied EPD to real-world datasets, capturing various shapes of parameter distributions rather than a normal distribution. These results effectively address the heterogeneity within systems, marking a substantial progression in accurately modeling systems using RCS data.","sentences":["Differential equations are pivotal in modeling and understanding the dynamics of various systems, offering insights into their future states through parameter estimation fitted to time series data.","In fields such as economy, politics, and biology, the observation data points in the time series are often independently obtained (i.e., Repeated Cross-Sectional (RCS) data).","With RCS data, we found that traditional methods for parameter estimation in differential equations, such as using mean values of time trajectories or Gaussian Process-based trajectory generation, have limitations in estimating the shape of parameter distributions, often leading to a significant loss of data information.","To address this issue, we introduce a novel method, Estimation of Parameter Distribution (EPD), providing accurate distribution of parameters without loss of data information.","EPD operates in three main steps: generating synthetic time trajectories by randomly selecting observed values at each time point, estimating parameters of a differential equation that minimize the discrepancy between these trajectories and the true solution of the equation, and selecting the parameters depending on the scale of discrepancy.","We then evaluated the performance of EPD across several models, including exponential growth, logistic population models, and target cell-limited models with delayed virus production, demonstrating its superiority in capturing the shape of parameter distributions.","Furthermore, we applied EPD to real-world datasets, capturing various shapes of parameter distributions rather than a normal distribution.","These results effectively address the heterogeneity within systems, marking a substantial progression in accurately modeling systems using RCS data."],"url":"http://arxiv.org/abs/2404.14873v1","category":"stat.ML"}
{"created":"2024-04-23 09:51:24","title":"EEGEncoder: Advancing BCI with Transformer-Based Motor Imagery Classification","abstract":"Brain-computer interfaces (BCIs) harness electroencephalographic signals for direct neural control of devices, offering a significant benefit for individuals with motor impairments. Traditional machine learning methods for EEG-based motor imagery (MI) classification encounter challenges such as manual feature extraction and susceptibility to noise. This paper introduces EEGEncoder, a deep learning framework that employs transformer models to surmount these limitations. Our innovative multi-scale fusion architecture captures both immediate and extended temporal features, thereby enhancing MI task classification precision. EEGEncoder's key innovations include the inaugural application of transformers in MI-EEG signal classification, a mixup data augmentation strategy for bolstered generalization, and a multi-task learning approach for refined predictive accuracy. When tested on the BCI Competition IV dataset 2a, our model established a new benchmark with its state-of-the-art performance. EEGEncoder signifies a substantial advancement in BCI technology, offering a robust, efficient, and effective tool for transforming thought into action, with the potential to significantly enhance the quality of life for those dependent on BCIs.","sentences":["Brain-computer interfaces (BCIs) harness electroencephalographic signals for direct neural control of devices, offering a significant benefit for individuals with motor impairments.","Traditional machine learning methods for EEG-based motor imagery (MI) classification encounter challenges such as manual feature extraction and susceptibility to noise.","This paper introduces EEGEncoder, a deep learning framework that employs transformer models to surmount these limitations.","Our innovative multi-scale fusion architecture captures both immediate and extended temporal features, thereby enhancing MI task classification precision.","EEGEncoder's key innovations include the inaugural application of transformers in MI-EEG signal classification, a mixup data augmentation strategy for bolstered generalization, and a multi-task learning approach for refined predictive accuracy.","When tested on the BCI Competition IV dataset 2a, our model established a new benchmark with its state-of-the-art performance.","EEGEncoder signifies a substantial advancement in BCI technology, offering a robust, efficient, and effective tool for transforming thought into action, with the potential to significantly enhance the quality of life for those dependent on BCIs."],"url":"http://arxiv.org/abs/2404.14869v1","category":"cs.HC"}
{"created":"2024-04-23 09:36:48","title":"A GPU-accelerated Cartesian grid method is proposed for solving the heat, wave, and Schrodinger equations on irregular domains","abstract":"This paper introduces a second-order method for solving general elliptic partial differential equations (PDEs) on irregular domains using GPU acceleration, based on Ying's kernel-free boundary integral (KFBI) method. The method addresses limitations imposed by CFL conditions in explicit schemes and accuracy issues in fully implicit schemes for the Laplacian operator. To overcome these challenges, the paper employs a series of second-order time discrete schemes and splits the Laplacian operator into explicit and implicit components. Specifically, the Crank-Nicolson method discretizes the heat equation in the temporal dimension, while the implicit scheme is used for the wave equation. The Schrodinger equation is treated using the Strang splitting method. By discretizing the temporal dimension implicitly, the heat, wave, and Schrodinger equations are transformed into a sequence of elliptic equations. The Laplacian operator on the right-hand side of the elliptic equation is obtained from the numerical scheme rather than being discretized and corrected by the five-point difference method. A Cartesian grid-based KFBI method is employed to solve the resulting elliptic equations. GPU acceleration, achieved through a parallel Cartesian grid solver, enhances the computational efficiency by exploiting high degrees of parallelism. Numerical results demonstrate that the proposed method achieves second-order accuracy for the heat, wave, and Schrodinger equations. Furthermore, the GPU-accelerated solvers for the three types of time-dependent equations exhibit a speedup of 30 times compared to CPU-based solvers.","sentences":["This paper introduces a second-order method for solving general elliptic partial differential equations (PDEs) on irregular domains using GPU acceleration, based on Ying's kernel-free boundary integral (KFBI) method.","The method addresses limitations imposed by CFL conditions in explicit schemes and accuracy issues in fully implicit schemes for the Laplacian operator.","To overcome these challenges, the paper employs a series of second-order time discrete schemes and splits the Laplacian operator into explicit and implicit components.","Specifically, the Crank-Nicolson method discretizes the heat equation in the temporal dimension, while the implicit scheme is used for the wave equation.","The Schrodinger equation is treated using the Strang splitting method.","By discretizing the temporal dimension implicitly, the heat, wave, and Schrodinger equations are transformed into a sequence of elliptic equations.","The Laplacian operator on the right-hand side of the elliptic equation is obtained from the numerical scheme rather than being discretized and corrected by the five-point difference method.","A Cartesian grid-based KFBI method is employed to solve the resulting elliptic equations.","GPU acceleration, achieved through a parallel Cartesian grid solver, enhances the computational efficiency by exploiting high degrees of parallelism.","Numerical results demonstrate that the proposed method achieves second-order accuracy for the heat, wave, and Schrodinger equations.","Furthermore, the GPU-accelerated solvers for the three types of time-dependent equations exhibit a speedup of 30 times compared to CPU-based solvers."],"url":"http://arxiv.org/abs/2404.14864v1","category":"math.NA"}
{"created":"2024-04-23 09:35:03","title":"Deep Learning Based Multi-Node ISAC 4D Environmental Reconstruction with Uplink- Downlink Cooperation","abstract":"Utilizing widely distributed communication nodes to achieve environmental reconstruction is one of the significant scenarios for Integrated Sensing and Communication (ISAC) and a crucial technology for 6G. To achieve this crucial functionality, we propose a deep learning based multi-node ISAC 4D environment reconstruction method with Uplink-Downlink (UL-DL) cooperation, which employs virtual aperture technology, Constant False Alarm Rate (CFAR) detection, and Mutiple Signal Classification (MUSIC) algorithm to maximize the sensing capabilities of single sensing nodes. Simultaneously, it introduces a cooperative environmental reconstruction scheme involving multi-node cooperation and Uplink-Downlink (UL-DL) cooperation to overcome the limitations of single-node sensing caused by occlusion and limited viewpoints. Furthermore, the deep learning models Attention Gate Gridding Residual Neural Network (AGGRNN) and Multi-View Sensing Fusion Network (MVSFNet) to enhance the density of sparsely reconstructed point clouds are proposed, aiming to restore as many original environmental details as possible while preserving the spatial structure of the point cloud. Additionally, we propose a multi-level fusion strategy incorporating both data-level and feature-level fusion to fully leverage the advantages of multi-node cooperation. Experimental results demonstrate that the environmental reconstruction performance of this method significantly outperforms other comparative method, enabling high-precision environmental reconstruction using ISAC system.","sentences":["Utilizing widely distributed communication nodes to achieve environmental reconstruction is one of the significant scenarios for Integrated Sensing and Communication (ISAC) and a crucial technology for 6G. To achieve this crucial functionality, we propose a deep learning based multi-node ISAC 4D environment reconstruction method with Uplink-Downlink (UL-DL) cooperation, which employs virtual aperture technology, Constant False Alarm Rate (CFAR) detection, and Mutiple Signal Classification (MUSIC) algorithm to maximize the sensing capabilities of single sensing nodes.","Simultaneously, it introduces a cooperative environmental reconstruction scheme involving multi-node cooperation and Uplink-Downlink (UL-DL) cooperation to overcome the limitations of single-node sensing caused by occlusion and limited viewpoints.","Furthermore, the deep learning models Attention Gate Gridding Residual Neural Network (AGGRNN) and Multi-View Sensing Fusion Network (MVSFNet) to enhance the density of sparsely reconstructed point clouds are proposed, aiming to restore as many original environmental details as possible while preserving the spatial structure of the point cloud.","Additionally, we propose a multi-level fusion strategy incorporating both data-level and feature-level fusion to fully leverage the advantages of multi-node cooperation.","Experimental results demonstrate that the environmental reconstruction performance of this method significantly outperforms other comparative method, enabling high-precision environmental reconstruction using ISAC system."],"url":"http://arxiv.org/abs/2404.14862v1","category":"eess.SP"}
{"created":"2024-04-23 08:47:44","title":"Superiority of stochastic symplectic methods via the law of iterated logarithm","abstract":"The superiority of stochastic symplectic methods over non-symplectic counterparts has been verified by plenty of numerical experiments, especially in capturing the asymptotic behaviour of the underlying solution process. How can one theoretically explain this superiority? This paper gives an answer to this problem from the perspective of the law of iterated logarithm, taking the linear stochastic Hamiltonian system in Hilbert space as a test model. The main contribution is twofold. First, by fully utilizing the time-change theorem for martingales and the Borell--TIS inequality, we prove that the upper limit of the exact solution with a specific scaling function almost surely equals some non-zero constant, thus confirming the validity of the law of iterated logarithm. Second, we prove that stochastic symplectic fully discrete methods asymptotically preserve the law of iterated logarithm, but non-symplectic ones do not. This reveals the good ability of stochastic symplectic methods in characterizing the almost sure asymptotic growth of the utmost fluctuation of the underlying solution process. Applications of our results to the linear stochastic oscillator and the linear stochastic Schrodinger equation are also presented.","sentences":["The superiority of stochastic symplectic methods over non-symplectic counterparts has been verified by plenty of numerical experiments, especially in capturing the asymptotic behaviour of the underlying solution process.","How can one theoretically explain this superiority?","This paper gives an answer to this problem from the perspective of the law of iterated logarithm, taking the linear stochastic Hamiltonian system in Hilbert space as a test model.","The main contribution is twofold.","First, by fully utilizing the time-change theorem for martingales and the Borell--TIS inequality, we prove that the upper limit of the exact solution with a specific scaling function almost surely equals some non-zero constant, thus confirming the validity of the law of iterated logarithm.","Second, we prove that stochastic symplectic fully discrete methods asymptotically preserve the law of iterated logarithm, but non-symplectic ones do not.","This reveals the good ability of stochastic symplectic methods in characterizing the almost sure asymptotic growth of the utmost fluctuation of the underlying solution process.","Applications of our results to the linear stochastic oscillator and the linear stochastic Schrodinger equation are also presented."],"url":"http://arxiv.org/abs/2404.14842v1","category":"math.NA"}
{"created":"2024-04-23 08:31:55","title":"Revisiting Neural Networks for Continual Learning: An Architectural Perspective","abstract":"Efforts to overcome catastrophic forgetting have primarily centered around developing more effective Continual Learning (CL) methods. In contrast, less attention was devoted to analyzing the role of network architecture design (e.g., network depth, width, and components) in contributing to CL. This paper seeks to bridge this gap between network architecture design and CL, and to present a holistic study on the impact of network architectures on CL. This work considers architecture design at the network scaling level, i.e., width and depth, and also at the network components, i.e., skip connections, global pooling layers, and down-sampling. In both cases, we first derive insights through systematically exploring how architectural designs affect CL. Then, grounded in these insights, we craft a specialized search space for CL and further propose a simple yet effective ArchCraft method to steer a CL-friendly architecture, namely, this method recrafts AlexNet/ResNet into AlexAC/ResAC. Experimental validation across various CL settings and scenarios demonstrates that improved architectures are parameter-efficient, achieving state-of-the-art performance of CL while being 86%, 61%, and 97% more compact in terms of parameters than the naive CL architecture in Class IL and Task IL. Code is available at https://github.com/byyx666/ArchCraft.","sentences":["Efforts to overcome catastrophic forgetting have primarily centered around developing more effective Continual Learning (CL) methods.","In contrast, less attention was devoted to analyzing the role of network architecture design (e.g., network depth, width, and components) in contributing to CL.","This paper seeks to bridge this gap between network architecture design and CL, and to present a holistic study on the impact of network architectures on CL.","This work considers architecture design at the network scaling level, i.e., width and depth, and also at the network components, i.e., skip connections, global pooling layers, and down-sampling.","In both cases, we first derive insights through systematically exploring how architectural designs affect CL.","Then, grounded in these insights, we craft a specialized search space for CL and further propose a simple yet effective ArchCraft method to steer a CL-friendly architecture, namely, this method recrafts AlexNet/ResNet into AlexAC/ResAC.","Experimental validation across various CL settings and scenarios demonstrates that improved architectures are parameter-efficient, achieving state-of-the-art performance of CL while being 86%, 61%, and 97% more compact in terms of parameters than the naive CL architecture in Class IL and Task IL.","Code is available at https://github.com/byyx666/ArchCraft."],"url":"http://arxiv.org/abs/2404.14829v1","category":"cs.LG"}
{"created":"2024-04-23 08:29:56","title":"Sentence-Level or Token-Level? A Comprehensive Study on Knowledge Distillation","abstract":"Knowledge distillation, transferring knowledge from a teacher model to a student model, has emerged as a powerful technique in neural machine translation for compressing models or simplifying training targets. Knowledge distillation encompasses two primary methods: sentence-level distillation and token-level distillation. In sentence-level distillation, the student model is trained to align with the output of the teacher model, which can alleviate the training difficulty and give student model a comprehensive understanding of global structure. Differently, token-level distillation requires the student model to learn the output distribution of the teacher model, facilitating a more fine-grained transfer of knowledge. Studies have revealed divergent performances between sentence-level and token-level distillation across different scenarios, leading to the confusion on the empirical selection of knowledge distillation methods. In this study, we argue that token-level distillation, with its more complex objective (i.e., distribution), is better suited for ``simple'' scenarios, while sentence-level distillation excels in ``complex'' scenarios. To substantiate our hypothesis, we systematically analyze the performance of distillation methods by varying the model size of student models, the complexity of text, and the difficulty of decoding procedure. While our experimental results validate our hypothesis, defining the complexity level of a given scenario remains a challenging task. So we further introduce a novel hybrid method that combines token-level and sentence-level distillation through a gating mechanism, aiming to leverage the advantages of both individual methods. Experiments demonstrate that the hybrid method surpasses the performance of token-level or sentence-level distillation methods and the previous works by a margin, demonstrating the effectiveness of the proposed hybrid method.","sentences":["Knowledge distillation, transferring knowledge from a teacher model to a student model, has emerged as a powerful technique in neural machine translation for compressing models or simplifying training targets.","Knowledge distillation encompasses two primary methods: sentence-level distillation and token-level distillation.","In sentence-level distillation, the student model is trained to align with the output of the teacher model, which can alleviate the training difficulty and give student model a comprehensive understanding of global structure.","Differently, token-level distillation requires the student model to learn the output distribution of the teacher model, facilitating a more fine-grained transfer of knowledge.","Studies have revealed divergent performances between sentence-level and token-level distillation across different scenarios, leading to the confusion on the empirical selection of knowledge distillation methods.","In this study, we argue that token-level distillation, with its more complex objective (i.e., distribution), is better suited for ``simple'' scenarios, while sentence-level distillation excels in ``complex'' scenarios.","To substantiate our hypothesis, we systematically analyze the performance of distillation methods by varying the model size of student models, the complexity of text, and the difficulty of decoding procedure.","While our experimental results validate our hypothesis, defining the complexity level of a given scenario remains a challenging task.","So we further introduce a novel hybrid method that combines token-level and sentence-level distillation through a gating mechanism, aiming to leverage the advantages of both individual methods.","Experiments demonstrate that the hybrid method surpasses the performance of token-level or sentence-level distillation methods and the previous works by a margin, demonstrating the effectiveness of the proposed hybrid method."],"url":"http://arxiv.org/abs/2404.14827v1","category":"cs.CL"}
{"created":"2024-04-23 07:37:41","title":"Variational Dynamic Programming for Stochastic Optimal Control","abstract":"We consider the problem of stochastic optimal control where the state-feedback control policies take the form of a probability distribution, and where a penalty on the entropy is added. By viewing the cost function as a Kullback-Leibler (KL) divergence between two Markov chains, we bring the tools from variational inference to bear on our optimal control problem. This allows for deriving a dynamic programming principle, where the value function is defined as a KL divergence again. We then resort to Gaussian distributions to approximate the control policies, and apply the theory to control affine nonlinear systems with quadratic costs. This results in closed-form recursive updates, which generalize LQR control and the backward Riccati equation. We illustrate this novel method on the simple problem of stabilizing an inverted pendulum.","sentences":["We consider the problem of stochastic optimal control where the state-feedback control policies take the form of a probability distribution, and where a penalty on the entropy is added.","By viewing the cost function as a Kullback-Leibler (KL) divergence between two Markov chains, we bring the tools from variational inference to bear on our optimal control problem.","This allows for deriving a dynamic programming principle, where the value function is defined as a KL divergence again.","We then resort to Gaussian distributions to approximate the control policies, and apply the theory to control affine nonlinear systems with quadratic costs.","This results in closed-form recursive updates, which generalize LQR control and the backward Riccati equation.","We illustrate this novel method on the simple problem of stabilizing an inverted pendulum."],"url":"http://arxiv.org/abs/2404.14806v1","category":"math.OC"}
{"created":"2024-04-23 07:23:28","title":"Invariant sample measures and sample statistical solutions for nonautonomous stochastic lattice Cahn-Hilliard equation with nonlinear noise","abstract":"We consider a stochastic lattice Cahn-Hilliard equation with nonautonomous nonlinear noise. First, we prove the existence of pullback random attractors in $\\ell^2$ for the generated nonautonomous random dynamical system. Then, we construct the time-dependent invariant sample Borel probability measures based on the pullback random attractor. Moreover, we develop a general stochastic Liouville type equation for nonautonomous random dynamical systems and show that the invariant sample measures obtained satisfy the stochastic Liouville type equation. At last, we define a new kind of statistical solution -- sample statistical solution corresponding to the invariant sample measures and show that each family of invariant sample measures is a sample statistical solution.","sentences":["We consider a stochastic lattice Cahn-Hilliard equation with nonautonomous nonlinear noise.","First, we prove the existence of pullback random attractors in $\\ell^2$ for the generated nonautonomous random dynamical system.","Then, we construct the time-dependent invariant sample Borel probability measures based on the pullback random attractor.","Moreover, we develop a general stochastic Liouville type equation for nonautonomous random dynamical systems and show that the invariant sample measures obtained satisfy the stochastic Liouville type equation.","At last, we define a new kind of statistical solution -- sample statistical solution corresponding to the invariant sample measures and show that each family of invariant sample measures is a sample statistical solution."],"url":"http://arxiv.org/abs/2404.14798v1","category":"math.PR"}
{"created":"2024-04-23 06:50:24","title":"Storage Capacity Evaluation of the Quantum Perceptron using the Replica Method","abstract":"We investigate a quantum perceptron implemented on a quantum circuit using a repeat until method. We evaluate this from the perspective of capacity, one of the performance evaluation measures for perceptions. We assess a Gardner volume, defined as a volume of coefficients of the perceptron that can correctly classify given training examples using the replica method. The model is defined on the quantum circuit. Nevertheless, it is straightforward to assess the capacity using the replica method, which is a standard method in classical statistical mechanics. The reason why we can solve our model by the replica method is the repeat until method, in which we focus on the output of the measurements of the quantum circuit. We find that the capacity of a quantum perceptron is larger than that of a classical perceptron since the quantum one is simple but effectively falls into a highly nonlinear form of the activation function.","sentences":["We investigate a quantum perceptron implemented on a quantum circuit using a repeat until method.","We evaluate this from the perspective of capacity, one of the performance evaluation measures for perceptions.","We assess a Gardner volume, defined as a volume of coefficients of the perceptron that can correctly classify given training examples using the replica method.","The model is defined on the quantum circuit.","Nevertheless, it is straightforward to assess the capacity using the replica method, which is a standard method in classical statistical mechanics.","The reason why we can solve our model by the replica method is the repeat until method, in which we focus on the output of the measurements of the quantum circuit.","We find that the capacity of a quantum perceptron is larger than that of a classical perceptron since the quantum one is simple but effectively falls into a highly nonlinear form of the activation function."],"url":"http://arxiv.org/abs/2404.14785v1","category":"cond-mat.dis-nn"}
{"created":"2024-04-23 06:48:57","title":"One-Pass Randomized Algorithm with Practical Rangefinder for Low-Rank Approximation to Quaternion Matrices","abstract":"As its real/complex counterparts, randomized algorithms for low-rank approximation to quaternion matrices received attention recently. For large-scale problems, however, existing quaternion orthogonalization methods are not efficient, leading to slow rangefinders. By relaxing orthonormality while maintaining favaroable condition numbers, this work proposes two practical quaternion rangefinders that take advantage of mature scientific computing libraries to accelerate heavy computations. They are then incorporated into the quaternion version of a well-known one-pass algorithm. Theoretically, we establish the probabilistic error bound, and demonstrate that the error is proportional to the condition number of the rangefinder. Besides Gaussian, we also allow quaternion sub-Gaussian test matrices. Key to the latter is the derivation of a deviation bound for extreme singular values of a quaternion sub-Gaussian matrix. Numerical experiments indicate that the one-pass algorithm with the proposed rangefinders work efficiently while only sacrificing little accuracy. In addition, we tested the algorithm in an on-the-fly 3D Navier-Stokes equation data compression to demonstrate its efficiency in large-scale applications.","sentences":["As its real/complex counterparts, randomized algorithms for low-rank approximation to quaternion matrices received attention recently.","For large-scale problems, however, existing quaternion orthogonalization methods are not efficient, leading to slow rangefinders.","By relaxing orthonormality while maintaining favaroable condition numbers, this work proposes two practical quaternion rangefinders that take advantage of mature scientific computing libraries to accelerate heavy computations.","They are then incorporated into the quaternion version of a well-known one-pass algorithm.","Theoretically, we establish the probabilistic error bound, and demonstrate that the error is proportional to the condition number of the rangefinder.","Besides Gaussian, we also allow quaternion sub-Gaussian test matrices.","Key to the latter is the derivation of a deviation bound for extreme singular values of a quaternion sub-Gaussian matrix.","Numerical experiments indicate that the one-pass algorithm with the proposed rangefinders work efficiently while only sacrificing little accuracy.","In addition, we tested the algorithm in an on-the-fly 3D Navier-Stokes equation data compression to demonstrate its efficiency in large-scale applications."],"url":"http://arxiv.org/abs/2404.14783v1","category":"math.NA"}
{"created":"2024-04-23 06:30:13","title":"Properties of quark stars based on the density-dependent MIT bag model","abstract":"In this study, we extend the MIT bag model by incorporating the vector interaction among quarks and introducing a density-dependent bag pressure. Then we proceed to investigate the thermodynamic properties of strange quark matter (SQM) and pure up-down quark matter (udQM) in quark stars. The results demonstrate that the vector interaction among quarks and the densitydependent bag pressure have significant impacts on the equation of state for both SQM and udQM. The inclusion of GV , which represents the strength of vector interactions, results in a stiffening of equation of state while maintaining causality. This allows for the description of massive compact stars such as those observed in GW190814 and PSR J0740+6620 as quark stars. Ultimately, we utilize the vMIT bag model to derive a series of mass-radius relations of quark stars (QSs) which is consistent with the astronomical observations from HESS J1731-347, 4U 1702-429, PSR J0740+6620, GW170817 and GW190814.","sentences":["In this study, we extend the MIT bag model by incorporating the vector interaction among quarks and introducing a density-dependent bag pressure.","Then we proceed to investigate the thermodynamic properties of strange quark matter (SQM) and pure up-down quark matter (udQM) in quark stars.","The results demonstrate that the vector interaction among quarks and the densitydependent bag pressure have significant impacts on the equation of state for both SQM and udQM.","The inclusion of GV , which represents the strength of vector interactions, results in a stiffening of equation of state while maintaining causality.","This allows for the description of massive compact stars such as those observed in GW190814 and PSR J0740+6620 as quark stars.","Ultimately, we utilize the vMIT bag model to derive a series of mass-radius relations of quark stars (QSs) which is consistent with the astronomical observations from HESS J1731-347, 4U 1702-429, PSR J0740+6620, GW170817 and GW190814."],"url":"http://arxiv.org/abs/2404.14775v1","category":"nucl-th"}
{"created":"2024-04-23 05:54:00","title":"The volume of conformally flat manifolds as hypersurfaces in the light-cone","abstract":"In this paper, we focus on a conformally flat Riemannian manifold $(M^n,g)$ of dimension $n$ isometrically immersed into the $(n+1)$-dimensional light-cone $\\Lambda^{n+1}$ as a hypersurface. We compute the first and the second variational formulas on the volume of such hypersurfaces. Such a hypersurface $M^n$ is not only immersed in $\\Lambda^{n+1}$ but also isometrically realized as a hypersurface of a certain null hypersurface $N^{n+1}$ in the Minkowski spacetime, which is different from $\\Lambda^{n+1}$. Moreover, $M^n$ has a volume-maximizing property in $N^{n+1}$.","sentences":["In this paper, we focus on a conformally flat Riemannian manifold $(M^n,g)$ of dimension $n$ isometrically immersed into the $(n+1)$-dimensional light-cone $\\Lambda^{n+1}$ as a hypersurface.","We compute the first and the second variational formulas on the volume of such hypersurfaces.","Such a hypersurface $M^n$ is not only immersed in $\\Lambda^{n+1}$ but also isometrically realized as a hypersurface of a certain null hypersurface $N^{n+1}$ in the Minkowski spacetime, which is different from $\\Lambda^{n+1}$. Moreover, $M^n$ has a volume-maximizing property in $N^{n+1}$."],"url":"http://arxiv.org/abs/2404.14761v1","category":"math.DG"}
{"created":"2024-04-23 05:43:28","title":"Deflection of light by a compact object with electric charge and magnetic dipole in Einstein-Born-Infeld gravity","abstract":"We consider the bending of light around a compact astrophysical object with both the electric field and the magnetic field in Einstein-Born-Infeld theory. From the null geodesic of a light ray passing a massive object with electric charge and magnetic dipole, the effective metric was obtained from the light cone condition reflecting the nonlinear electromagnetic effects. We found the asymptotic form of the effective metric up to the first order in gravitational constant $G$ and Born-Infeld parameter $1/\\beta^2$ on the equatorial plane. Then we compute the bending angle of light from the geodesic equation. The result includes particular cases where only one type of field is present taking the appropriate limits.","sentences":["We consider the bending of light around a compact astrophysical object with both the electric field and the magnetic field in Einstein-Born-Infeld theory.","From the null geodesic of a light ray passing a massive object with electric charge and magnetic dipole, the effective metric was obtained from the light cone condition reflecting the nonlinear electromagnetic effects.","We found the asymptotic form of the effective metric up to the first order in gravitational constant $G$ and Born-Infeld parameter $1/\\beta^2$ on the equatorial plane.","Then we compute the bending angle of light from the geodesic equation.","The result includes particular cases where only one type of field is present taking the appropriate limits."],"url":"http://arxiv.org/abs/2404.14756v1","category":"gr-qc"}
{"created":"2024-04-23 04:59:34","title":"Differentiable Score-Based Likelihoods: Learning CT Motion Compensation From Clean Images","abstract":"Motion artifacts can compromise the diagnostic value of computed tomography (CT) images. Motion correction approaches require a per-scan estimation of patient-specific motion patterns. In this work, we train a score-based model to act as a probability density estimator for clean head CT images. Given the trained model, we quantify the deviation of a given motion-affected CT image from the ideal distribution through likelihood computation. We demonstrate that the likelihood can be utilized as a surrogate metric for motion artifact severity in the CT image facilitating the application of an iterative, gradient-based motion compensation algorithm. By optimizing the underlying motion parameters to maximize likelihood, our method effectively reduces motion artifacts, bringing the image closer to the distribution of motion-free scans. Our approach achieves comparable performance to state-of-the-art methods while eliminating the need for a representative data set of motion-affected samples. This is particularly advantageous in real-world applications, where patient motion patterns may exhibit unforeseen variability, ensuring robustness without implicit assumptions about recoverable motion types.","sentences":["Motion artifacts can compromise the diagnostic value of computed tomography (CT) images.","Motion correction approaches require a per-scan estimation of patient-specific motion patterns.","In this work, we train a score-based model to act as a probability density estimator for clean head CT images.","Given the trained model, we quantify the deviation of a given motion-affected CT image from the ideal distribution through likelihood computation.","We demonstrate that the likelihood can be utilized as a surrogate metric for motion artifact severity in the CT image facilitating the application of an iterative, gradient-based motion compensation algorithm.","By optimizing the underlying motion parameters to maximize likelihood, our method effectively reduces motion artifacts, bringing the image closer to the distribution of motion-free scans.","Our approach achieves comparable performance to state-of-the-art methods while eliminating the need for a representative data set of motion-affected samples.","This is particularly advantageous in real-world applications, where patient motion patterns may exhibit unforeseen variability, ensuring robustness without implicit assumptions about recoverable motion types."],"url":"http://arxiv.org/abs/2404.14747v1","category":"cs.CV"}
{"created":"2024-04-23 04:45:23","title":"BMapOpt: Optimization of Brain Tissue Probability Maps using a Differentiable MRI Simulator","abstract":"Reconstructing digital brain phantoms in the form of multi-channeled brain tissue probability maps for individual subjects is essential for capturing brain anatomical variability, understanding neurological diseases, as well as for testing image processing methods. We demonstrate the first framework that optimizes brain tissue probability maps (Gray Matter - GM, White Matter - WM, and Cerebrospinal fluid - CSF) with the help of a Physics-based differentiable MRI simulator that models the magnetization signal at each voxel in the image. Given an observed $T_1$/$T_2$-weighted MRI scan, the corresponding clinical MRI sequence, and the MRI differentiable simulator, we optimize the simulator's input probability maps by back-propagating the L2 loss between the simulator's output and the $T_1$/$T_2$-weighted scan. This approach has the significant advantage of not relying on any training data, and instead uses the strong inductive bias of the MRI simulator. We tested the model on 20 scans from the BrainWeb database and demonstrate a highly accurate reconstruction of GM, WM, and CSF.","sentences":["Reconstructing digital brain phantoms in the form of multi-channeled brain tissue probability maps for individual subjects is essential for capturing brain anatomical variability, understanding neurological diseases, as well as for testing image processing methods.","We demonstrate the first framework that optimizes brain tissue probability maps (Gray Matter - GM, White Matter - WM, and Cerebrospinal fluid - CSF) with the help of a Physics-based differentiable MRI simulator that models the magnetization signal at each voxel in the image.","Given an observed $T_1$/$T_2$-weighted MRI scan, the corresponding clinical MRI sequence, and the MRI differentiable simulator, we optimize the simulator's input probability maps by back-propagating the L2 loss between the simulator's output and the $T_1$/$T_2$-weighted scan.","This approach has the significant advantage of not relying on any training data, and instead uses the strong inductive bias of the MRI simulator.","We tested the model on 20 scans from the BrainWeb database and demonstrate a highly accurate reconstruction of GM, WM, and CSF."],"url":"http://arxiv.org/abs/2404.14739v1","category":"cs.CV"}
{"created":"2024-04-23 04:29:38","title":"In-situ Doppler-free spectroscopy and laser frequency stabilization based on time-division multiplexing differential saturated absorption","abstract":"We introduce a novel time-division multiplexing differential saturated absorption spectroscopy (TDMDSAS) approach, providing superior accuracy and stability in Doppler-free spectroscopy. By distinguishing probe and reference fields in the temporal domain, TDMDSAS efficiently suppresses Doppler broadening and common-mode optical noise. We utilized this technology to determine the absolute frequency of diverse neutral Yb isotopes across its $6s^2\\ ^{1}S_0\\to 6s6p ^{1}P_1$ transitions. Furthermore, the first-ever observation of in-situ Doppler-free Zeeman sub-level spectra was accomplished, enabling the determination of magnetic field gradients. We stabilized a UV diode laser at 399 nm using an error signal derived from the spectral first-derivative demodulated signal of $^{174}\\mathrm{Yb}$. This technique yielded a frequency stability of up to 15 kHz with a 40 s averaging time and a standard deviation of around 180 kHz over a half-hour period. Given its low cost, straightforward, and scalable nature, TDMDSAS holds excellent potential in metrology and quantum applications.","sentences":["We introduce a novel time-division multiplexing differential saturated absorption spectroscopy (TDMDSAS) approach, providing superior accuracy and stability in Doppler-free spectroscopy.","By distinguishing probe and reference fields in the temporal domain, TDMDSAS efficiently suppresses Doppler broadening and common-mode optical noise.","We utilized this technology to determine the absolute frequency of diverse neutral Yb isotopes across its $6s^2\\ ^{1}S_0\\to 6s6p ^{1}P_1$ transitions.","Furthermore, the first-ever observation of in-situ Doppler-free Zeeman sub-level spectra was accomplished, enabling the determination of magnetic field gradients.","We stabilized a UV diode laser at 399 nm using an error signal derived from the spectral first-derivative demodulated signal of $^{174}\\mathrm{Yb}$. This technique yielded a frequency stability of up to 15 kHz with a 40 s averaging time and a standard deviation of around 180 kHz over a half-hour period.","Given its low cost, straightforward, and scalable nature, TDMDSAS holds excellent potential in metrology and quantum applications."],"url":"http://arxiv.org/abs/2404.14734v1","category":"physics.optics"}
{"created":"2024-04-23 03:48:18","title":"Source Code Vulnerability Detection: Combining Code Language Models and Code Property Graphs","abstract":"Currently, deep learning successfully applies to code vulnerability detection by learning from code sequences or property graphs. However, sequence-based methods often overlook essential code attributes such as syntax, control flow, and data dependencies, whereas graph-based approaches might underestimate the semantics of code and face challenges in capturing long-distance contextual information.   To address this gap, we propose Vul-LMGNN, a unified model that combines pre-trained code language models with code property graphs for code vulnerability detection. Vul-LMGNN constructs a code property graph that integrates various code attributes (including syntax, flow control, and data dependencies) into a unified graph structure, thereafter leveraging pre-trained code model to extract local semantic features as node embeddings in the code property graph. Furthermore, to effectively retain dependency information among various attributes, we introduce a gated code Graph Neural Network (GNN). By jointly training the code language model and the gated code GNN modules in Vul-LMGNN, our proposed method efficiently leverages the strengths of both mechanisms. Finally, we utilize a pre-trained CodeBERT as an auxiliary classifier, with the final detection results derived by learning the linear interpolation of Vul-LMGNN and CodeBERT. The proposed method, evaluated across four real-world vulnerability datasets, demonstrated superior performance compared to six state-of-the-art approaches. Our source code could be accessed via the link: https://github.com/Vul-LMGNN/vul-LMGGNN.","sentences":["Currently, deep learning successfully applies to code vulnerability detection by learning from code sequences or property graphs.","However, sequence-based methods often overlook essential code attributes such as syntax, control flow, and data dependencies, whereas graph-based approaches might underestimate the semantics of code and face challenges in capturing long-distance contextual information.   ","To address this gap, we propose Vul-LMGNN, a unified model that combines pre-trained code language models with code property graphs for code vulnerability detection.","Vul-LMGNN constructs a code property graph that integrates various code attributes (including syntax, flow control, and data dependencies) into a unified graph structure, thereafter leveraging pre-trained code model to extract local semantic features as node embeddings in the code property graph.","Furthermore, to effectively retain dependency information among various attributes, we introduce a gated code Graph Neural Network (GNN).","By jointly training the code language model and the gated code GNN modules in Vul-LMGNN, our proposed method efficiently leverages the strengths of both mechanisms.","Finally, we utilize a pre-trained CodeBERT as an auxiliary classifier, with the final detection results derived by learning the linear interpolation of Vul-LMGNN and CodeBERT.","The proposed method, evaluated across four real-world vulnerability datasets, demonstrated superior performance compared to six state-of-the-art approaches.","Our source code could be accessed via the link: https://github.com/Vul-LMGNN/vul-LMGGNN."],"url":"http://arxiv.org/abs/2404.14719v1","category":"cs.CR"}
{"created":"2024-04-23 03:35:27","title":"SC-HVPPNet: Spatial and Channel Hybrid-Attention Video Post-Processing Network with CNN and Transformer","abstract":"Convolutional Neural Network (CNN) and Transformer have attracted much attention recently for video post-processing (VPP). However, the interaction between CNN and Transformer in existing VPP methods is not fully explored, leading to inefficient communication between the local and global extracted features. In this paper, we explore the interaction between CNN and Transformer in the task of VPP, and propose a novel Spatial and Channel Hybrid-Attention Video Post-Processing Network (SC-HVPPNet), which can cooperatively exploit the image priors in both spatial and channel domains. Specifically, in the spatial domain, a novel spatial attention fusion module is designed, in which two attention weights are generated to fuse the local and global representations collaboratively. In the channel domain, a novel channel attention fusion module is developed, which can blend the deep representations at the channel dimension dynamically. Extensive experiments show that SC-HVPPNet notably boosts video restoration quality, with average bitrate savings of 5.29%, 12.42%, and 13.09% for Y, U, and V components in the VTM-11.0-NNVC RA configuration.","sentences":["Convolutional Neural Network (CNN) and Transformer have attracted much attention recently for video post-processing (VPP).","However, the interaction between CNN and Transformer in existing VPP methods is not fully explored, leading to inefficient communication between the local and global extracted features.","In this paper, we explore the interaction between CNN and Transformer in the task of VPP, and propose a novel Spatial and Channel Hybrid-Attention Video Post-Processing Network (SC-HVPPNet), which can cooperatively exploit the image priors in both spatial and channel domains.","Specifically, in the spatial domain, a novel spatial attention fusion module is designed, in which two attention weights are generated to fuse the local and global representations collaboratively.","In the channel domain, a novel channel attention fusion module is developed, which can blend the deep representations at the channel dimension dynamically.","Extensive experiments show that SC-HVPPNet notably boosts video restoration quality, with average bitrate savings of 5.29%, 12.42%, and 13.09% for Y, U, and V components in the VTM-11.0-NNVC RA configuration."],"url":"http://arxiv.org/abs/2404.14709v1","category":"cs.CV"}
{"created":"2024-04-23 03:01:09","title":"Deep neural networks for choice analysis: Enhancing behavioral regularity with gradient regularization","abstract":"Deep neural networks (DNNs) frequently present behaviorally irregular patterns, significantly limiting their practical potentials and theoretical validity in travel behavior modeling. This study proposes strong and weak behavioral regularities as novel metrics to evaluate the monotonicity of individual demand functions (a.k.a. law of demand), and further designs a constrained optimization framework with six gradient regularizers to enhance DNNs' behavioral regularity. The proposed framework is applied to travel survey data from Chicago and London to examine the trade-off between predictive power and behavioral regularity for large vs. small sample scenarios and in-domain vs. out-of-domain generalizations. The results demonstrate that, unlike models with strong behavioral foundations such as the multinomial logit, the benchmark DNNs cannot guarantee behavioral regularity. However, gradient regularization (GR) increases DNNs' behavioral regularity by around 6 percentage points (pp) while retaining their relatively high predictive power. In the small sample scenario, GR is more effective than in the large sample scenario, simultaneously improving behavioral regularity by about 20 pp and log-likelihood by around 1.7%. Comparing with the in-domain generalization of DNNs, GR works more effectively in out-of-domain generalization: it drastically improves the behavioral regularity of poorly performing benchmark DNNs by around 65 pp, indicating the criticality of behavioral regularization for enhancing model transferability and application in forecasting. Moreover, the proposed framework is applicable to other NN-based choice models such as TasteNets. Future studies could use behavioral regularity as a metric along with log-likelihood in evaluating travel demand models, and investigate other methods to further enhance behavioral regularity when adopting complex machine learning models.","sentences":["Deep neural networks (DNNs) frequently present behaviorally irregular patterns, significantly limiting their practical potentials and theoretical validity in travel behavior modeling.","This study proposes strong and weak behavioral regularities as novel metrics to evaluate the monotonicity of individual demand functions (a.k.a. law of demand), and further designs a constrained optimization framework with six gradient regularizers to enhance DNNs' behavioral regularity.","The proposed framework is applied to travel survey data from Chicago and London to examine the trade-off between predictive power and behavioral regularity for large vs. small sample scenarios and in-domain vs. out-of-domain generalizations.","The results demonstrate that, unlike models with strong behavioral foundations such as the multinomial logit, the benchmark DNNs cannot guarantee behavioral regularity.","However, gradient regularization (GR) increases DNNs' behavioral regularity by around 6 percentage points (pp) while retaining their relatively high predictive power.","In the small sample scenario, GR is more effective than in the large sample scenario, simultaneously improving behavioral regularity by about 20 pp and log-likelihood by around 1.7%.","Comparing with the in-domain generalization of DNNs, GR works more effectively in out-of-domain generalization: it drastically improves the behavioral regularity of poorly performing benchmark DNNs by around 65 pp, indicating the criticality of behavioral regularization for enhancing model transferability and application in forecasting.","Moreover, the proposed framework is applicable to other NN-based choice models such as TasteNets.","Future studies could use behavioral regularity as a metric along with log-likelihood in evaluating travel demand models, and investigate other methods to further enhance behavioral regularity when adopting complex machine learning models."],"url":"http://arxiv.org/abs/2404.14701v1","category":"cs.LG"}
{"created":"2024-04-23 02:52:53","title":"Transformation operators and the Kastler-Kalau-Walze type theorems on 4-dimensional manifolds","abstract":"In this paper, we compute the lower-dimensional volume Vol(1,1) about transformation operators for 4- dimensional spin manifolds with boundary and we also get the Kastler-Kalau-Walze type theorem about transformation operators on 4-dimensional compact manifolds with boundary.","sentences":["In this paper, we compute the lower-dimensional volume Vol(1,1) about transformation operators for 4- dimensional spin manifolds with boundary and we also get the Kastler-Kalau-Walze type theorem about transformation operators on 4-dimensional compact manifolds with boundary."],"url":"http://arxiv.org/abs/2404.14694v1","category":"math.DG"}
{"created":"2024-04-23 02:49:58","title":"Deep Overlapping Community Search via Subspace Embedding","abstract":"Community search (CS) aims to identify a set of nodes based on a specified query, leveraging structural cohesiveness and attribute homogeneity. This task enjoys various applications, ranging from fraud detection to recommender systems. In contrast to algorithm-based approaches, graph neural network (GNN) based methods define communities using ground truth labels, leveraging prior knowledge to explore patterns from graph structures and node features. However, existing solutions face three major limitations: 1) GNN-based models primarily focus on the disjoint community structure, disregarding the nature of nodes belonging to multiple communities. 2) These model structures suffer from low-order awareness and severe efficiency issues. 3) The identified community is subject to the free-rider and boundary effects. In this paper, we propose Simplified Multi-hop Attention Networks (SMN), which consist of three designs. First, we introduce a subspace community embedding technique called Sparse Subspace Filter (SSF). SSF enables the projection of community embeddings into distinct vector subspaces, accommodating the nature of overlapping and nesting community structures. In addition, we propose a lightweight model structure and a hop-wise attention mechanism to capture high-order patterns while improving model efficiency. Furthermore, two search algorithms are developed to minimize the latent space's community radius, addressing the challenges of free-rider and boundary effects. To the best of our knowledge, this is the first learning-based study of overlapping community search. Extensive experiments validate the superior performance of SMN compared with the state-of-the-art approaches. SMN achieves 14.73% improvements in F1-Score and up to 3 orders of magnitude acceleration in model efficiency.","sentences":["Community search (CS) aims to identify a set of nodes based on a specified query, leveraging structural cohesiveness and attribute homogeneity.","This task enjoys various applications, ranging from fraud detection to recommender systems.","In contrast to algorithm-based approaches, graph neural network (GNN) based methods define communities using ground truth labels, leveraging prior knowledge to explore patterns from graph structures and node features.","However, existing solutions face three major limitations: 1) GNN-based models primarily focus on the disjoint community structure, disregarding the nature of nodes belonging to multiple communities.","2)","These model structures suffer from low-order awareness and severe efficiency issues.","3)","The identified community is subject to the free-rider and boundary effects.","In this paper, we propose Simplified Multi-hop Attention Networks (SMN), which consist of three designs.","First, we introduce a subspace community embedding technique called Sparse Subspace Filter (SSF).","SSF enables the projection of community embeddings into distinct vector subspaces, accommodating the nature of overlapping and nesting community structures.","In addition, we propose a lightweight model structure and a hop-wise attention mechanism to capture high-order patterns while improving model efficiency.","Furthermore, two search algorithms are developed to minimize the latent space's community radius, addressing the challenges of free-rider and boundary effects.","To the best of our knowledge, this is the first learning-based study of overlapping community search.","Extensive experiments validate the superior performance of SMN compared with the state-of-the-art approaches.","SMN achieves 14.73% improvements in F1-Score and up to 3 orders of magnitude acceleration in model efficiency."],"url":"http://arxiv.org/abs/2404.14692v1","category":"cs.SI"}
{"created":"2024-04-23 02:23:51","title":"Some Remarks on Controllability of the Liouville Equation","abstract":"We revisit the work of Roger Brockett on controllability of the Liouville equation, with a particular focus on the following problem: Given a smooth controlled dynamical system of the form $\\dot{x} = f(x,u)$ and a state-space diffeomorphism $\\psi$, design a feedback control $u(t,x)$ to steer an arbitrary initial state $x_0$ to $\\psi(x_0)$ in finite time. This formulation of the problem makes contact with the theory of optimal transportation and with nonlinear controllability. For controllable linear systems, Brockett showed that this is possible under a fairly restrictive condition on $\\psi$. We prove that controllability suffices for a much larger class of diffeomorphisms. For nonlinear systems defined on smooth manifolds, we review a recent result of Agrachev and Caponigro regarding controllability on the group of diffeomorphisms. A corollary of this result states that, for control-affine systems satisfying a bracket generating condition, any $\\psi$ in a neighborhood of the identity can be implemented using a time-varying feedback control law that switches between finitely many time-invariant flows. We prove a quantitative version which allows us to describe the implementation complexity of the Agrachev-Caponigro construction in terms of a lower bound on the number of switchings.","sentences":["We revisit the work of Roger Brockett on controllability of the Liouville equation, with a particular focus on the following problem: Given a smooth controlled dynamical system of the form $\\dot{x} = f(x,u)$ and a state-space diffeomorphism $\\psi$, design a feedback control $u(t,x)$ to steer an arbitrary initial state $x_0$ to $\\psi(x_0)$ in finite time.","This formulation of the problem makes contact with the theory of optimal transportation and with nonlinear controllability.","For controllable linear systems, Brockett showed that this is possible under a fairly restrictive condition on $\\psi$. We prove that controllability suffices for a much larger class of diffeomorphisms.","For nonlinear systems defined on smooth manifolds, we review a recent result of Agrachev and Caponigro regarding controllability on the group of diffeomorphisms.","A corollary of this result states that, for control-affine systems satisfying a bracket generating condition, any $\\psi$ in a neighborhood of the identity can be implemented using a time-varying feedback control law that switches between finitely many time-invariant flows.","We prove a quantitative version which allows us to describe the implementation complexity of the Agrachev-Caponigro construction in terms of a lower bound on the number of switchings."],"url":"http://arxiv.org/abs/2404.14683v1","category":"math.OC"}
{"created":"2024-04-23 01:32:50","title":"A Non-staggered Projection Algorithm for Two-Phase Fluid-Structure Interaction Simulation Using the Phase-Field/Immersed-Boundary Method","abstract":"We present a Pressure-Oscillation-Free projection algorithm for large-density-ratio multiphase fluid-structure interaction simulations, implemented on a non-staggered Cartesian grid. The incompressible Navier-Stokes is decoupled with an improved five-step incremental pressure correction algorithm. Fluid-fluid interface is captured using the Cahn-Hilliard equation, and the surface tension model is coupled with a momentum-weighted interpolation scheme to suppress unphysical pressure oscillations, ensuring accurate evolution of multiphase interfaces. Interaction at the fluid-structure interface is obtained by implicitly solving for the feedback acceleration in the Eulerian-Lagrangian system. For validation of the present method, the comparison studies for Pressure-Oscillation-Free effect are systematically conducted using lid driving cavity and droplet deformation cases. Moreover, several challenging multiphase simulations are implemented and discussed. As a demonstrating example of fluid-structure interaction, a rising bubble bypassing an obstacle is tested.","sentences":["We present a Pressure-Oscillation-Free projection algorithm for large-density-ratio multiphase fluid-structure interaction simulations, implemented on a non-staggered Cartesian grid.","The incompressible Navier-Stokes is decoupled with an improved five-step incremental pressure correction algorithm.","Fluid-fluid interface is captured using the Cahn-Hilliard equation, and the surface tension model is coupled with a momentum-weighted interpolation scheme to suppress unphysical pressure oscillations, ensuring accurate evolution of multiphase interfaces.","Interaction at the fluid-structure interface is obtained by implicitly solving for the feedback acceleration in the Eulerian-Lagrangian system.","For validation of the present method, the comparison studies for Pressure-Oscillation-Free effect are systematically conducted using lid driving cavity and droplet deformation cases.","Moreover, several challenging multiphase simulations are implemented and discussed.","As a demonstrating example of fluid-structure interaction, a rising bubble bypassing an obstacle is tested."],"url":"http://arxiv.org/abs/2404.14656v1","category":"physics.flu-dyn"}
{"created":"2024-04-23 01:18:53","title":"Transonic shocks for steady Euler flows with an external force in an axisymmetric perturbed cylinder","abstract":"We concern the structural stability of transonic shocks for the steady Euler system with an external force in an axisymmetric perturbed cylinder. For a class of external forces, we first prove the existence and uniqueness of the transonic shock solution to the one-dimensional steady Euler system with an external force, which shows that the external force has a stabilization effect on the transonic shock in the flat cylinder and the shock position is uniquely determined. We then establish the existence and stability of the transonic shock solution under axisymmetric perturbations of the incoming supersonic flow, the nozzle boundary, the exit pressure and the external force. Different from the transonic shock problem in two-dimensional nozzles, there exists a singularity along the symmetric axis for axisymmetric flows. We introduce an invertible modified Lagrangian transformation to overcome this difficulty and straighten the streamline. One of the key elements in the analysis is to utilize the deformation-curl decomposition to effectively decouple the hyperbolic and elliptic modes in the steady axisymmetric Euler system with an external force. Another one is an equivalent reformulation of the Rankine-Hugoniot conditions so that the shock front is uniquely determined by an algebraic equation.","sentences":["We concern the structural stability of transonic shocks for the steady Euler system with an external force in an axisymmetric perturbed cylinder.","For a class of external forces, we first prove the existence and uniqueness of the transonic shock solution to the one-dimensional steady Euler system with an external force, which shows that the external force has a stabilization effect on the transonic shock in the flat cylinder and the shock position is uniquely determined.","We then establish the existence and stability of the transonic shock solution under axisymmetric perturbations of the incoming supersonic flow, the nozzle boundary, the exit pressure and the external force.","Different from the transonic shock problem in two-dimensional nozzles, there exists a singularity along the symmetric axis for axisymmetric flows.","We introduce an invertible modified Lagrangian transformation to overcome this difficulty and straighten the streamline.","One of the key elements in the analysis is to utilize the deformation-curl decomposition to effectively decouple the hyperbolic and elliptic modes in the steady axisymmetric Euler system with an external force.","Another one is an equivalent reformulation of the Rankine-Hugoniot conditions so that the shock front is uniquely determined by an algebraic equation."],"url":"http://arxiv.org/abs/2404.14652v1","category":"math.AP"}
{"created":"2024-04-23 00:39:26","title":"Uncertainty Quantification on Graph Learning: A Survey","abstract":"Graphical models, including Graph Neural Networks (GNNs) and Probabilistic Graphical Models (PGMs), have demonstrated their exceptional capabilities across numerous fields. These models necessitate effective uncertainty quantification to ensure reliable decision-making amid the challenges posed by model training discrepancies and unpredictable testing scenarios. This survey examines recent works that address uncertainty quantification within the model architectures, training, and inference of GNNs and PGMs. We aim to provide an overview of the current landscape of uncertainty in graphical models by organizing the recent methods into uncertainty representation and handling. By summarizing state-of-the-art methods, this survey seeks to deepen the understanding of uncertainty quantification in graphical models, thereby increasing their effectiveness and safety in critical applications.","sentences":["Graphical models, including Graph Neural Networks (GNNs) and Probabilistic Graphical Models (PGMs), have demonstrated their exceptional capabilities across numerous fields.","These models necessitate effective uncertainty quantification to ensure reliable decision-making amid the challenges posed by model training discrepancies and unpredictable testing scenarios.","This survey examines recent works that address uncertainty quantification within the model architectures, training, and inference of GNNs and PGMs.","We aim to provide an overview of the current landscape of uncertainty in graphical models by organizing the recent methods into uncertainty representation and handling.","By summarizing state-of-the-art methods, this survey seeks to deepen the understanding of uncertainty quantification in graphical models, thereby increasing their effectiveness and safety in critical applications."],"url":"http://arxiv.org/abs/2404.14642v1","category":"cs.LG"}
{"created":"2024-04-23 00:24:38","title":"An inexact augmented Lagrangian algorithm for unsymmetric saddle-point systems","abstract":"Augmented Lagrangian (AL) methods are a well known class of algorithms for solving constrained optimization problems. They have been extended to the solution of saddle-point systems of linear equations. We study an AL (SPAL) algorithm for unsymmetric saddle-point systems and derive convergence and semi-convergence properties, even when the system is singular. At each step, our SPAL requires the exact solution of a linear system of the same size but with an SPD (2,2) block. To improve efficiency, we introduce an inexact SPAL algorithm. We establish its convergence properties under reasonable assumptions. Specifically, we use a gradient method, known as the Barzilai-Borwein (BB) method, to solve the linear system at each iteration. We call the result the augmented Lagrangian BB (SPALBB) algorithm and study its convergence. Numerical experiments on test problems from Navier-Stokes equations and coupled Stokes-Darcy flow show that SPALBB is more robust and efficient than BICGSTAB and GMRES. SPALBB often requires the least CPU time, especially on large systems.","sentences":["Augmented Lagrangian (AL) methods are a well known class of algorithms for solving constrained optimization problems.","They have been extended to the solution of saddle-point systems of linear equations.","We study an AL (SPAL) algorithm for unsymmetric saddle-point systems and derive convergence and semi-convergence properties, even when the system is singular.","At each step, our SPAL requires the exact solution of a linear system of the same size but with an SPD (2,2) block.","To improve efficiency, we introduce an inexact SPAL algorithm.","We establish its convergence properties under reasonable assumptions.","Specifically, we use a gradient method, known as the Barzilai-Borwein (BB) method, to solve the linear system at each iteration.","We call the result the augmented Lagrangian BB (SPALBB) algorithm and study its convergence.","Numerical experiments on test problems from Navier-Stokes equations and coupled Stokes-Darcy flow show that SPALBB is more robust and efficient than BICGSTAB and GMRES.","SPALBB often requires the least CPU time, especially on large systems."],"url":"http://arxiv.org/abs/2404.14636v1","category":"math.NA"}
{"created":"2024-04-23 00:07:53","title":"Workload-Aware Hardware Accelerator Mining for Distributed Deep Learning Training","abstract":"In this paper, we present a novel technique to search for hardware architectures of accelerators optimized for end-to-end training of deep neural networks (DNNs). Our approach addresses both single-device and distributed pipeline and tensor model parallel scenarios, latter being addressed for the first time. The search optimized accelerators for training relevant metrics such as throughput/TDP under a fixed area and power constraints. However, with the proliferation of specialized architectures and complex distributed training mechanisms, the design space exploration of hardware accelerators is very large. Prior work in this space has tried to tackle this by reducing the search space to either a single accelerator execution that too only for inference, or tuning the architecture for specific layers (e.g., convolution). Instead, we take a unique heuristic-based critical path-based approach to determine the best use of available resources (power and area) either for a set of DNN workloads or each workload individually. First, we perform local search to determine the architecture for each pipeline and tensor model stage. Specifically, the system iteratively generates architectural configurations and tunes the design using a novel heuristic-based approach that prioritizes accelerator resources and scheduling to critical operators in a machine learning workload. Second, to address the complexities of distributed training, the local search selects multiple (k) designs per stage. A global search then identifies an accelerator from the top-k sets to optimize training throughput across the stages. We evaluate this work on 11 different DNN models. Compared to a recent inference-only work Spotlight, our method converges to a design in, on average, 31x less time and offers 12x higher throughput. Moreover, designs generated using our method achieve 12% throughput improvement over TPU architecture.","sentences":["In this paper, we present a novel technique to search for hardware architectures of accelerators optimized for end-to-end training of deep neural networks (DNNs).","Our approach addresses both single-device and distributed pipeline and tensor model parallel scenarios, latter being addressed for the first time.","The search optimized accelerators for training relevant metrics such as throughput/TDP under a fixed area and power constraints.","However, with the proliferation of specialized architectures and complex distributed training mechanisms, the design space exploration of hardware accelerators is very large.","Prior work in this space has tried to tackle this by reducing the search space to either a single accelerator execution that too only for inference, or tuning the architecture for specific layers (e.g., convolution).","Instead, we take a unique heuristic-based critical path-based approach to determine the best use of available resources (power and area) either for a set of DNN workloads or each workload individually.","First, we perform local search to determine the architecture for each pipeline and tensor model stage.","Specifically, the system iteratively generates architectural configurations and tunes the design using a novel heuristic-based approach that prioritizes accelerator resources and scheduling to critical operators in a machine learning workload.","Second, to address the complexities of distributed training, the local search selects multiple (k) designs per stage.","A global search then identifies an accelerator from the top-k sets to optimize training throughput across the stages.","We evaluate this work on 11 different DNN models.","Compared to a recent inference-only work Spotlight, our method converges to a design in, on average, 31x less time and offers 12x higher throughput.","Moreover, designs generated using our method achieve 12% throughput improvement over TPU architecture."],"url":"http://arxiv.org/abs/2404.14632v1","category":"cs.AR"}
{"created":"2024-04-22 23:40:03","title":"Towards Multi-Morphology Controllers with Diversity and Knowledge Distillation","abstract":"Finding controllers that perform well across multiple morphologies is an important milestone for large-scale robotics, in line with recent advances via foundation models in other areas of machine learning. However, the challenges of learning a single controller to control multiple morphologies make the `one robot one task' paradigm dominant in the field. To alleviate these challenges, we present a pipeline that: (1) leverages Quality Diversity algorithms like MAP-Elites to create a dataset of many single-task/single-morphology teacher controllers, then (2) distills those diverse controllers into a single multi-morphology controller that performs well across many different body plans by mimicking the sensory-action patterns of the teacher controllers via supervised learning. The distilled controller scales well with the number of teachers/morphologies and shows emergent properties. It generalizes to unseen morphologies in a zero-shot manner, providing robustness to morphological perturbations and instant damage recovery. Lastly, the distilled controller is also independent of the teacher controllers -- we can distill the teacher's knowledge into any controller model, making our approach synergistic with architectural improvements and existing training algorithms for teacher controllers.","sentences":["Finding controllers that perform well across multiple morphologies is an important milestone for large-scale robotics, in line with recent advances via foundation models in other areas of machine learning.","However, the challenges of learning a single controller to control multiple morphologies make the `one robot one task' paradigm dominant in the field.","To alleviate these challenges, we present a pipeline that: (1) leverages Quality Diversity algorithms like MAP-Elites to create a dataset of many single-task/single-morphology teacher controllers, then (2) distills those diverse controllers into a single multi-morphology controller that performs well across many different body plans by mimicking the sensory-action patterns of the teacher controllers via supervised learning.","The distilled controller scales well with the number of teachers/morphologies and shows emergent properties.","It generalizes to unseen morphologies in a zero-shot manner, providing robustness to morphological perturbations and instant damage recovery.","Lastly, the distilled controller is also independent of the teacher controllers -- we can distill the teacher's knowledge into any controller model, making our approach synergistic with architectural improvements and existing training algorithms for teacher controllers."],"url":"http://arxiv.org/abs/2404.14625v1","category":"cs.RO"}
{"created":"2024-04-22 21:59:33","title":"Quantifying the Internal Validity of Weighted Estimands","abstract":"In this paper we study a class of weighted estimands, which we define as parameters that can be expressed as weighted averages of the underlying heterogeneous treatment effects. The popular ordinary least squares (OLS), two-stage least squares (2SLS), and two-way fixed effects (TWFE) estimands are all special cases within our framework. Our focus is on answering two questions concerning weighted estimands. First, under what conditions can they be interpreted as the average treatment effect for some (possibly latent) subpopulation? Second, when these conditions are satisfied, what is the upper bound on the size of that subpopulation, either in absolute terms or relative to a target population of interest? We argue that this upper bound provides a valuable diagnostic for empirical research. When a given weighted estimand corresponds to the average treatment effect for a small subset of the population of interest, we say its internal validity is low. Our paper develops practical tools to quantify the internal validity of weighted estimands.","sentences":["In this paper we study a class of weighted estimands, which we define as parameters that can be expressed as weighted averages of the underlying heterogeneous treatment effects.","The popular ordinary least squares (OLS), two-stage least squares (2SLS), and two-way fixed effects (TWFE) estimands are all special cases within our framework.","Our focus is on answering two questions concerning weighted estimands.","First, under what conditions can they be interpreted as the average treatment effect for some (possibly latent) subpopulation?","Second, when these conditions are satisfied, what is the upper bound on the size of that subpopulation, either in absolute terms or relative to a target population of interest?","We argue that this upper bound provides a valuable diagnostic for empirical research.","When a given weighted estimand corresponds to the average treatment effect for a small subset of the population of interest, we say its internal validity is low.","Our paper develops practical tools to quantify the internal validity of weighted estimands."],"url":"http://arxiv.org/abs/2404.14603v1","category":"econ.EM"}
{"created":"2024-04-22 21:50:50","title":"Unsupervised Learning of Individual Kohn-Sham States: Interpretable Representations and Consequences for Downstream Predictions of Many-Body Effects","abstract":"Representation learning for the electronic structure problem is a major challenge of machine learning in computational condensed matter and materials physics. Within quantum mechanical first principles approaches, Kohn-Sham density functional theory (DFT) is the preeminent tool for understanding electronic structure, and the high-dimensional wavefunctions calculated in this approach serve as the building block for downstream calculations of correlated many-body excitations and related physical observables. Here, we use variational autoencoders (VAE) for the unsupervised learning of high-dimensional DFT wavefunctions and show that these wavefunctions lie in a low-dimensional manifold within the latent space. Our model autonomously determines the optimal representation of the electronic structure, avoiding limitations due to manual feature engineering and selection in prior work. To demonstrate the utility of the latent space representation of the DFT wavefunction, we use it for the supervised training of neural networks (NN) for downstream prediction of the quasiparticle bandstructures within the GW formalism, which includes many-electron correlations beyond DFT. The GW prediction achieves a low error of 0.11 eV for a combined test set of metals and semiconductors drawn from the Computational 2D Materials Database (C2DB), suggesting that latent space representation captures key physical information from the original data. Finally, we explore the interpretability of the VAE representation and show that the successful representation learning and downstream prediction by our model is derived from the smoothness of the VAE latent space, which also enables the generation of wavefunctions on arbitrary points in latent space. Our work provides a novel and general machine-learning framework for investigating electronic structure and many-body physics.","sentences":["Representation learning for the electronic structure problem is a major challenge of machine learning in computational condensed matter and materials physics.","Within quantum mechanical first principles approaches, Kohn-Sham density functional theory (DFT) is the preeminent tool for understanding electronic structure, and the high-dimensional wavefunctions calculated in this approach serve as the building block for downstream calculations of correlated many-body excitations and related physical observables.","Here, we use variational autoencoders (VAE) for the unsupervised learning of high-dimensional DFT wavefunctions and show that these wavefunctions lie in a low-dimensional manifold within the latent space.","Our model autonomously determines the optimal representation of the electronic structure, avoiding limitations due to manual feature engineering and selection in prior work.","To demonstrate the utility of the latent space representation of the DFT wavefunction, we use it for the supervised training of neural networks (NN) for downstream prediction of the quasiparticle bandstructures within the GW formalism, which includes many-electron correlations beyond DFT.","The GW prediction achieves a low error of 0.11 eV for a combined test set of metals and semiconductors drawn from the Computational 2D Materials Database (C2DB), suggesting that latent space representation captures key physical information from the original data.","Finally, we explore the interpretability of the VAE representation and show that the successful representation learning and downstream prediction by our model is derived from the smoothness of the VAE latent space, which also enables the generation of wavefunctions on arbitrary points in latent space.","Our work provides a novel and general machine-learning framework for investigating electronic structure and many-body physics."],"url":"http://arxiv.org/abs/2404.14601v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-22 21:39:08","title":"Neural Compress-and-Forward for the Relay Channel","abstract":"The relay channel, consisting of a source-destination pair and a relay, is a fundamental component of cooperative communications. While the capacity of a general relay channel remains unknown, various relaying strategies, including compress-and-forward (CF), have been proposed. For CF, given the correlated signals at the relay and destination, distributed compression techniques, such as Wyner-Ziv coding, can be harnessed to utilize the relay-to-destination link more efficiently. In light of the recent advancements in neural network-based distributed compression, we revisit the relay channel problem, where we integrate a learned one-shot Wyner--Ziv compressor into a primitive relay channel with a finite-capacity and orthogonal (or out-of-band) relay-to-destination link. The resulting neural CF scheme demonstrates that our task-oriented compressor recovers \"binning\" of the quantized indices at the relay, mimicking the optimal asymptotic CF strategy, although no structure exploiting the knowledge of source statistics was imposed into the design. We show that the proposed neural CF scheme, employing finite order modulation, operates closely to the capacity of a primitive relay channel that assumes a Gaussian codebook. Our learned compressor provides the first proof-of-concept work toward a practical neural CF relaying scheme.","sentences":["The relay channel, consisting of a source-destination pair and a relay, is a fundamental component of cooperative communications.","While the capacity of a general relay channel remains unknown, various relaying strategies, including compress-and-forward (CF), have been proposed.","For CF, given the correlated signals at the relay and destination, distributed compression techniques, such as Wyner-Ziv coding, can be harnessed to utilize the relay-to-destination link more efficiently.","In light of the recent advancements in neural network-based distributed compression, we revisit the relay channel problem, where we integrate a learned one-shot Wyner--Ziv compressor into a primitive relay channel with a finite-capacity and orthogonal (or out-of-band) relay-to-destination link.","The resulting neural CF scheme demonstrates that our task-oriented compressor recovers \"binning\" of the quantized indices at the relay, mimicking the optimal asymptotic CF strategy, although no structure exploiting the knowledge of source statistics was imposed into the design.","We show that the proposed neural CF scheme, employing finite order modulation, operates closely to the capacity of a primitive relay channel that assumes a Gaussian codebook.","Our learned compressor provides the first proof-of-concept work toward a practical neural CF relaying scheme."],"url":"http://arxiv.org/abs/2404.14594v1","category":"cs.IT"}
{"created":"2024-04-22 21:26:45","title":"Alexandrov spaces are CS sets","abstract":"We prove that the extremal stratification of an Alexandrov space by Perelman-Petrunin is a CS stratification in the sense of Siebenmann. We also show that every space of directions of an Alexandrov space without proper extremal subsets is homeomorphic to a sphere. In the appendix we give an example of a primitive extremal subset of codimension 2 that is not an Alexandrov space with respect to the induced intrinsic metric.","sentences":["We prove that the extremal stratification of an Alexandrov space by Perelman-Petrunin is a CS stratification in the sense of Siebenmann.","We also show that every space of directions of an Alexandrov space without proper extremal subsets is homeomorphic to a sphere.","In the appendix we give an example of a primitive extremal subset of codimension 2 that is not an Alexandrov space with respect to the induced intrinsic metric."],"url":"http://arxiv.org/abs/2404.14587v1","category":"math.DG"}
{"created":"2024-04-22 21:15:41","title":"Global Chern currents of coherent sheaves and Baum Bott currents","abstract":"We provide global extensions of previous results about representations of characteristic classes of coherent analytic sheaves and of Baum-Bott residues of holomorphic foliations. We show in the first case that they can be represented by currents with support on the support of the given coherent analytic sheaf, and in the second case, by currents with support on the singular set of the foliation. In previous works, we have constructed such representatives provided global resolutions of the appropriate sheaves existed. In this article, we show that the definition of Chern classes of Green and the associated techniques, which work on arbitrary complex manifolds without any assumption on the existence of global resolutions, may be combined with our previous constructions to yield the desired representatives.   We also prove a transgression formula for such representatives, which is new even in the case when global resolutions exist. More precisely, the representatives depend on local resolutions of the sheaf, and on choices of metrics and connections on these bundles, i.e., the currents for two different choices differ by a current of the form $dN$, where $N$ is an explicit current, which in the first case above has support on the support of the given coherent analytic sheaf, and in the second case above has support on the singular set of the foliation.","sentences":["We provide global extensions of previous results about representations of characteristic classes of coherent analytic sheaves and of Baum-Bott residues of holomorphic foliations.","We show in the first case that they can be represented by currents with support on the support of the given coherent analytic sheaf, and in the second case, by currents with support on the singular set of the foliation.","In previous works, we have constructed such representatives provided global resolutions of the appropriate sheaves existed.","In this article, we show that the definition of Chern classes of Green and the associated techniques, which work on arbitrary complex manifolds without any assumption on the existence of global resolutions, may be combined with our previous constructions to yield the desired representatives.   ","We also prove a transgression formula for such representatives, which is new even in the case when global resolutions exist.","More precisely, the representatives depend on local resolutions of the sheaf, and on choices of metrics and connections on these bundles, i.e., the currents for two different choices differ by a current of the form $dN$, where $N$ is an explicit current, which in the first case above has support on the support of the given coherent analytic sheaf, and in the second case above has support on the singular set of the foliation."],"url":"http://arxiv.org/abs/2404.14585v1","category":"math.CV"}
{"created":"2024-04-22 21:13:11","title":"X-ray Interferometry Using a Modulated Phase Grating: Theory and Experiments","abstract":"X-ray grating interferometry allows for the simultaneous acquisition of attenuation, differential-phase contrast, and dark-field images, resulting from X-ray attenuation, refraction, and small-angle scattering, respectively. The modulated phase grating (MPG) interferometer is a recently developed grating interferometry system capable of generating a directly resolvable interference pattern using a relatively large period grating envelope function that is sampled at a pitch that allows for X-ray spatial coherence using a microfocus X-ray source or by use of a source G0 grating that follows the Lau condition. We present the theory of the MPG interferometry system for a 2-dimensional staggered grating, derived using Fourier optics, and we compare the theoretical predictions with experiments we have performed with a microfocus X-ray system at Pennington Biomedical Research Center, LSU. The theoretical and experimental fringe visibility is evaluated as a function of grating-to-detector distance. Quantitative experiments are performed with porous carbon and alumina samples, and qualitative analysis of attenuation and dark-field images of a dried anchovy are shown.","sentences":["X-ray grating interferometry allows for the simultaneous acquisition of attenuation, differential-phase contrast, and dark-field images, resulting from X-ray attenuation, refraction, and small-angle scattering, respectively.","The modulated phase grating (MPG) interferometer is a recently developed grating interferometry system capable of generating a directly resolvable interference pattern using a relatively large period grating envelope function that is sampled at a pitch that allows for X-ray spatial coherence using a microfocus X-ray source or by use of a source G0 grating that follows the Lau condition.","We present the theory of the MPG interferometry system for a 2-dimensional staggered grating, derived using Fourier optics, and we compare the theoretical predictions with experiments we have performed with a microfocus X-ray system at Pennington Biomedical Research Center, LSU.","The theoretical and experimental fringe visibility is evaluated as a function of grating-to-detector distance.","Quantitative experiments are performed with porous carbon and alumina samples, and qualitative analysis of attenuation and dark-field images of a dried anchovy are shown."],"url":"http://arxiv.org/abs/2404.14584v1","category":"physics.optics"}
{"created":"2024-04-22 21:00:45","title":"Toeplitz operators and group-moment coordinates for quasi-elliptic and quasi-hyperbolic symbols","abstract":"For $\\mathbb{B}^n$ the $n$-dimensional unit ball and $D_n$ its Siegel unbounded realization, we consider Toeplitz operators acting on weighted Bergman spaces with symbols invariant under the actions of the maximal Abelian subgroups of biholomorphisms $\\mathbb{T}^n$ (quasi-elliptic) and $\\mathbb{T}^n \\times \\mathbb{R}_+$ (quasi-hyperbolic). Using geometric symplectic tools (Hamiltonian actions and moment maps) we obtain simple diagonalizing spectral integral formulas for such kinds of operators. Some consequences show how powerful the use of our differential geometric methods are.","sentences":["For $\\mathbb{B}^n$ the $n$-dimensional unit ball and $D_n$ its Siegel unbounded realization, we consider Toeplitz operators acting on weighted Bergman spaces with symbols invariant under the actions of the maximal Abelian subgroups of biholomorphisms $\\mathbb{T}^n$ (quasi-elliptic) and $\\mathbb{T}^n \\times \\mathbb{R}_+$ (quasi-hyperbolic).","Using geometric symplectic tools (Hamiltonian actions and moment maps) we obtain simple diagonalizing spectral integral formulas for such kinds of operators.","Some consequences show how powerful the use of our differential geometric methods are."],"url":"http://arxiv.org/abs/2404.14582v1","category":"math.FA"}
{"created":"2024-04-22 20:56:21","title":"Tunable dynamical tissue phantom for laser speckle imaging","abstract":"We introduce a novel method to design and implement a tunable dynamical tissue phantom for laser speckle-based in-vivo blood flow imaging. This approach relies on Stochastic Differential Equations (SDE) to control a piezoelectric actuator which, upon illuminated with a laser source, generates speckles of pre-defined probability density function and auto-correlation. The validation experiments show that the phantom can generate dynamic speckles that closely replicate both surfaces as well as deep tissue blood flow for a reasonably wide range and accuracy.","sentences":["We introduce a novel method to design and implement a tunable dynamical tissue phantom for laser speckle-based in-vivo blood flow imaging.","This approach relies on Stochastic Differential Equations (SDE) to control a piezoelectric actuator which, upon illuminated with a laser source, generates speckles of pre-defined probability density function and auto-correlation.","The validation experiments show that the phantom can generate dynamic speckles that closely replicate both surfaces as well as deep tissue blood flow for a reasonably wide range and accuracy."],"url":"http://arxiv.org/abs/2404.14577v1","category":"physics.med-ph"}
{"created":"2024-04-22 20:34:46","title":"A Survey of Decomposition-Based Evolutionary Multi-Objective Optimization: Part I-Past and Future","abstract":"Decomposition has been the mainstream approach in classic mathematical programming for multi-objective optimization and multi-criterion decision-making. However, it was not properly studied in the context of evolutionary multi-objective optimization (EMO) until the development of multi-objective evolutionary algorithm based on decomposition (MOEA/D). In this two-part survey series, we use MOEA/D as the representative of decomposition-based EMO to review the up-to-date development in this area, and systematically and comprehensively analyze its research landscape. In the first part, we present a comprehensive survey of the development of MOEA/D from its origin to the current state-of-the-art approaches. In order to be self-contained, we start with a step-by-step tutorial that aims to help a novice quickly get onto the working mechanism of MOEA/D. Then, selected major developments of MOEA/D are reviewed according to its core design components including weight vector settings, subproblem formulations, selection mechanisms and reproduction operators. Besides, we also overview some selected advanced topics for constraint handling, optimization in dynamic and uncertain environments, computationally expensive objective functions, and preference incorporation. In the final part, we shed some light on emerging directions for future developments.","sentences":["Decomposition has been the mainstream approach in classic mathematical programming for multi-objective optimization and multi-criterion decision-making.","However, it was not properly studied in the context of evolutionary multi-objective optimization (EMO) until the development of multi-objective evolutionary algorithm based on decomposition (MOEA/D).","In this two-part survey series, we use MOEA/D as the representative of decomposition-based EMO to review the up-to-date development in this area, and systematically and comprehensively analyze its research landscape.","In the first part, we present a comprehensive survey of the development of MOEA/D from its origin to the current state-of-the-art approaches.","In order to be self-contained, we start with a step-by-step tutorial that aims to help a novice quickly get onto the working mechanism of MOEA/D. Then, selected major developments of MOEA/D are reviewed according to its core design components including weight vector settings, subproblem formulations, selection mechanisms and reproduction operators.","Besides, we also overview some selected advanced topics for constraint handling, optimization in dynamic and uncertain environments, computationally expensive objective functions, and preference incorporation.","In the final part, we shed some light on emerging directions for future developments."],"url":"http://arxiv.org/abs/2404.14571v1","category":"cs.NE"}
{"created":"2024-04-22 20:16:14","title":"The zeta-determinant of the Dirichlet-to-Neumann operator of the Steklov Problem on forms","abstract":"On a compact Riemannian manifold $M$ with boundary $Y$, we express the log of the zeta-determinant of the Dirichlet-to-Neumann operator acting on $q$-forms on $Y$ as the difference of the log of the zeta-determinant of the Laplacian on $q$-forms on $M$ with absolute boundary conditions and that of the Laplacian with Dirichlet boundary conditions with some additional terms which are expressed by curvature tensors. When the dimension of $M$ is $2$ or $3$, we compute these terms explicitly. We also discuss the value of the zeta function at zero associated to the Dirichlet-to-Neumann operator by using a conformal rescaling method. As an application, we recover the result of the conformal invariance obtained in C. Guillarmou and L. Guillop\\'e, The determinant of the Dirichlet-to-Neumann map for surfaces with boundary, Int. Math. Res. Not. IMRN 2007, no. 22, Art. ID rnm099, when the dimension of $M$ is $2$.","sentences":["On a compact Riemannian manifold $M$ with boundary $Y$, we express the log of the zeta-determinant of the Dirichlet-to-Neumann operator acting on $q$-forms on $Y$ as the difference of the log of the zeta-determinant of the Laplacian on $q$-forms on $M$ with absolute boundary conditions and that of the Laplacian with Dirichlet boundary conditions with some additional terms which are expressed by curvature tensors.","When the dimension of $M$ is $2$ or $3$, we compute these terms explicitly.","We also discuss the value of the zeta function at zero associated to the Dirichlet-to-Neumann operator by using a conformal rescaling method.","As an application, we recover the result of the conformal invariance obtained in C. Guillarmou and L. Guillop\\'e, The determinant of the Dirichlet-to-Neumann map for surfaces with boundary, Int.","Math. Res.","Not.","IMRN 2007, no. 22, Art.","ID rnm099, when the dimension of $M$ is $2$."],"url":"http://arxiv.org/abs/2404.14562v1","category":"math.DG"}
{"created":"2024-04-22 19:46:07","title":"Learning S-Matrix Phases with Neural Operators","abstract":"We use Fourier Neural Operators (FNOs) to study the relation between the modulus and phase of amplitudes in $2\\to 2$ elastic scattering at fixed energies. Unlike previous approaches, we do not employ the integral relation imposed by unitarity, but instead train FNOs to discover it from many samples of amplitudes with finite partial wave expansions. When trained only on true samples, the FNO correctly predicts (unique or ambiguous) phases of amplitudes with infinite partial wave expansions. When also trained on false samples, it can rate the quality of its prediction by producing a true/false classifying index. We observe that the value of this index is strongly correlated with the violation of the unitarity constraint for the predicted phase, and present examples where it delineates the boundary between allowed and disallowed profiles of the modulus. Our application of FNOs is unconventional: it involves a simultaneous regression-classification task and emphasizes the role of statistics in ensembles of NOs. We comment on the merits and limitations of the approach and its potential as a new methodology in Theoretical Physics.","sentences":["We use Fourier Neural Operators (FNOs) to study the relation between the modulus and phase of amplitudes in $2\\to 2$ elastic scattering at fixed energies.","Unlike previous approaches, we do not employ the integral relation imposed by unitarity, but instead train FNOs to discover it from many samples of amplitudes with finite partial wave expansions.","When trained only on true samples, the FNO correctly predicts (unique or ambiguous) phases of amplitudes with infinite partial wave expansions.","When also trained on false samples, it can rate the quality of its prediction by producing a true/false classifying index.","We observe that the value of this index is strongly correlated with the violation of the unitarity constraint for the predicted phase, and present examples where it delineates the boundary between allowed and disallowed profiles of the modulus.","Our application of FNOs is unconventional: it involves a simultaneous regression-classification task and emphasizes the role of statistics in ensembles of NOs.","We comment on the merits and limitations of the approach and its potential as a new methodology in Theoretical Physics."],"url":"http://arxiv.org/abs/2404.14551v1","category":"hep-th"}
{"created":"2024-04-22 19:15:39","title":"Minimal semiinjective resolutions in the $Q$-shaped derived category","abstract":"Injective resolutions of modules are key objects of homological algebra, which are used for the computation of derived functors. Semiinjective resolutions of chain complexes are more general objects, which are used for the computation of $\\operatorname{Hom}$ spaces in the derived category $\\mathscr{D}( A )$ of a ring $A$. Minimal semiinjective resolutions have the additional property of being unique.   The $Q$-shaped derived category $\\mathscr{D}_Q( A )$ consists of $Q$-shaped diagrams for a suitable preadditive category $Q$, and it generalises $\\mathscr{D}( A )$. Some special cases of $\\mathscr{D}_Q( A )$ are the derived categories of differential modules, $m$-periodic chain complexes, and $N$-complexes, and there are many other possibilities. The category $\\mathscr{D}_Q( A )$ shares some key properties of $\\mathscr{D}( A )$; for instance, it is triangulated and compactly generated.   This paper establishes a theory of minimal semiinjective resolutions in $\\mathscr{D}_Q( A )$. As a sample application, it generalises a theorem by Ringel--Zhang on differential modules.","sentences":["Injective resolutions of modules are key objects of homological algebra, which are used for the computation of derived functors.","Semiinjective resolutions of chain complexes are more general objects, which are used for the computation of $\\operatorname{Hom}$ spaces in the derived category $\\mathscr{D}( A )$ of a ring $A$.","Minimal semiinjective resolutions have the additional property of being unique.   ","The $Q$-shaped derived category $\\mathscr{D}_Q( A )$ consists of $Q$-shaped diagrams for a suitable preadditive category $Q$, and it generalises $\\mathscr{D}( A )$.","Some special cases of $\\mathscr{D}_Q( A )$ are the derived categories of differential modules, $m$-periodic chain complexes, and $N$-complexes, and there are many other possibilities.","The category $\\mathscr{D}_Q( A )$ shares some key properties of $\\mathscr{D}( A )$; for instance, it is triangulated and compactly generated.   ","This paper establishes a theory of minimal semiinjective resolutions in $\\mathscr{D}_Q( A )$.","As a sample application, it generalises a theorem by Ringel--Zhang on differential modules."],"url":"http://arxiv.org/abs/2404.14537v1","category":"math.RT"}
{"created":"2024-04-22 19:07:54","title":"Nonlinear treatment of a black hole mimicker ringdown","abstract":"We perform the first nonlinear and self-consistent study of the merger and ringdown of a black hole mimicking object with stable light rings. To that end, we numerically solve the full Einstein-Klein-Gordon equations governing the head-on collisions of a series of binary boson stars in the large-mass-ratio regime resulting in spinning horizonless remnants with stable light rings. We broadly confirm the appearance of features in the extracted gravitational waveforms expected based on perturbative methods: the signal from the prompt response of the remnants approaches that of a Kerr black hole in the large-compactness limit, and the subsequent emissions contain periodically appearing bursts akin to so-called gravitational wave echoes. However, these bursts occur at high frequencies and are sourced by perturbations of the remnant's internal degrees of freedom. Furthermore, the emitted waveforms also contain a large-amplitude and long-lived component comparable in frequency to black hole quasi-normal modes. We further characterize the emissions, obtain basic scaling relations of relevant timescales, and compute the energy emitted in gravitational waves.","sentences":["We perform the first nonlinear and self-consistent study of the merger and ringdown of a black hole mimicking object with stable light rings.","To that end, we numerically solve the full Einstein-Klein-Gordon equations governing the head-on collisions of a series of binary boson stars in the large-mass-ratio regime resulting in spinning horizonless remnants with stable light rings.","We broadly confirm the appearance of features in the extracted gravitational waveforms expected based on perturbative methods: the signal from the prompt response of the remnants approaches that of a Kerr black hole in the large-compactness limit, and the subsequent emissions contain periodically appearing bursts akin to so-called gravitational wave echoes.","However, these bursts occur at high frequencies and are sourced by perturbations of the remnant's internal degrees of freedom.","Furthermore, the emitted waveforms also contain a large-amplitude and long-lived component comparable in frequency to black hole quasi-normal modes.","We further characterize the emissions, obtain basic scaling relations of relevant timescales, and compute the energy emitted in gravitational waves."],"url":"http://arxiv.org/abs/2404.14536v1","category":"gr-qc"}
{"created":"2024-04-22 18:57:39","title":"Magnetic amplification in pre-merger neutron stars through resonance-induced magnetorotational instabilities","abstract":"Tidal resonances in the final seconds of a binary neutron-star inspiral can excite oscillation modes in one or both of the constituents to large amplitudes. Under favorable circumstances, resonant pulsations can overstrain the stellar crust and unleash a torrent of magnetoelastic energy that manifests as a gamma-ray `precursor flare'. We show that for realistic, stratified stars rotating with a spin frequency of $\\gtrsim 30\\,$Hz, the fundamental $g$-mode or its first overtone can also execute a differential rotation in the crust such that a magnetic field of strength $\\gtrsim 10^{13}\\,$G is generated via magnetorotational instabilities. This may help to explain observed precursor rates and their luminosities. Pre-merger magnetic growth would also provide seed magnetic energy for the post-merger remnant.","sentences":["Tidal resonances in the final seconds of a binary neutron-star inspiral can excite oscillation modes in one or both of the constituents to large amplitudes.","Under favorable circumstances, resonant pulsations can overstrain the stellar crust and unleash a torrent of magnetoelastic energy that manifests as a gamma-ray `precursor flare'.","We show that for realistic, stratified stars rotating with a spin frequency of $\\gtrsim 30\\,$Hz, the fundamental $g$-mode or its first overtone can also execute a differential rotation in the crust such that a magnetic field of strength $\\gtrsim 10^{13}\\,$G is generated via magnetorotational instabilities.","This may help to explain observed precursor rates and their luminosities.","Pre-merger magnetic growth would also provide seed magnetic energy for the post-merger remnant."],"url":"http://arxiv.org/abs/2404.14529v1","category":"astro-ph.HE"}
{"created":"2024-04-22 18:32:03","title":"Shot noise in coupled electron-boson systems","abstract":"The nature of charge carriers in strange metals has become a topic of intense current investigation. Recent shot noise measurements in the quantum critical heavy fermion metal YbRh$_2$Si$_2$ revealed a suppression of the Fano factor that cannot be understood from electron-phonon scattering or strong electron correlations in a Fermi liquid, indicating loss of quasiparticles. The experiment motivates the consideration of shot noise in a variety of theoretical models in which quasiparticles may be lost. Here we study shot noise in systems with co-existing itinerant electrons and dispersive bosons, going beyond the regime where the bosons are on their own in thermal equilibrium. We construct the Boltzmann-Langevin equations for the coupled system, and show that adequate electron-boson couplings restore the Fano factor to its Fermi liquid value. Our findings point to the beyond-Landau form of quantum criticality as underlying the suppressed shot noise of strange metals in heavy fermion metals and beyond.","sentences":["The nature of charge carriers in strange metals has become a topic of intense current investigation.","Recent shot noise measurements in the quantum critical heavy fermion metal YbRh$_2$Si$_2$ revealed a suppression of the Fano factor that cannot be understood from electron-phonon scattering or strong electron correlations in a Fermi liquid, indicating loss of quasiparticles.","The experiment motivates the consideration of shot noise in a variety of theoretical models in which quasiparticles may be lost.","Here we study shot noise in systems with co-existing itinerant electrons and dispersive bosons, going beyond the regime where the bosons are on their own in thermal equilibrium.","We construct the Boltzmann-Langevin equations for the coupled system, and show that adequate electron-boson couplings restore the Fano factor to its Fermi liquid value.","Our findings point to the beyond-Landau form of quantum criticality as underlying the suppressed shot noise of strange metals in heavy fermion metals and beyond."],"url":"http://arxiv.org/abs/2404.14515v1","category":"cond-mat.str-el"}
{"created":"2024-04-22 18:18:41","title":"Align Your Steps: Optimizing Sampling Schedules in Diffusion Models","abstract":"Diffusion models (DMs) have established themselves as the state-of-the-art generative modeling approach in the visual domain and beyond. A crucial drawback of DMs is their slow sampling speed, relying on many sequential function evaluations through large neural networks. Sampling from DMs can be seen as solving a differential equation through a discretized set of noise levels known as the sampling schedule. While past works primarily focused on deriving efficient solvers, little attention has been given to finding optimal sampling schedules, and the entire literature relies on hand-crafted heuristics. In this work, for the first time, we propose a general and principled approach to optimizing the sampling schedules of DMs for high-quality outputs, called $\\textit{Align Your Steps}$. We leverage methods from stochastic calculus and find optimal schedules specific to different solvers, trained DMs and datasets. We evaluate our novel approach on several image, video as well as 2D toy data synthesis benchmarks, using a variety of different samplers, and observe that our optimized schedules outperform previous hand-crafted schedules in almost all experiments. Our method demonstrates the untapped potential of sampling schedule optimization, especially in the few-step synthesis regime.","sentences":["Diffusion models (DMs) have established themselves as the state-of-the-art generative modeling approach in the visual domain and beyond.","A crucial drawback of DMs is their slow sampling speed, relying on many sequential function evaluations through large neural networks.","Sampling from DMs can be seen as solving a differential equation through a discretized set of noise levels known as the sampling schedule.","While past works primarily focused on deriving efficient solvers, little attention has been given to finding optimal sampling schedules, and the entire literature relies on hand-crafted heuristics.","In this work, for the first time, we propose a general and principled approach to optimizing the sampling schedules of DMs for high-quality outputs, called $\\textit{Align Your Steps}$.","We leverage methods from stochastic calculus and find optimal schedules specific to different solvers, trained DMs and datasets.","We evaluate our novel approach on several image, video as well as 2D toy data synthesis benchmarks, using a variety of different samplers, and observe that our optimized schedules outperform previous hand-crafted schedules in almost all experiments.","Our method demonstrates the untapped potential of sampling schedule optimization, especially in the few-step synthesis regime."],"url":"http://arxiv.org/abs/2404.14507v1","category":"cs.CV"}
{"created":"2024-04-22 18:00:01","title":"APPLE: An Evolution Code for Modeling Giant Planets","abstract":"We introduce APPLE, a novel planetary evolution code designed specifically for the study of giant exoplanet and Jovian planet evolution in the era of Galileo, Juno, and Cassini. With APPLE, state-of-the-art equations of state for hydrogen, helium, ice, and rock are integrated with advanced features to treat ice/rock cores and metals in the gaseous envelope; models for helium rain and hydrogen/helium immiscibility; detailed atmosphere boundary tables that also provide self-consistent albedos and spectra; and options to address envelope metal gradients and stably-stratified regions. Our hope is that these purpose-built features of APPLE will help catalyze the development of the next generation of giant exoplanet and Jovian planet evolutionary models.","sentences":["We introduce APPLE, a novel planetary evolution code designed specifically for the study of giant exoplanet and Jovian planet evolution in the era of Galileo, Juno, and Cassini.","With APPLE, state-of-the-art equations of state for hydrogen, helium, ice, and rock are integrated with advanced features to treat ice/rock cores and metals in the gaseous envelope; models for helium rain and hydrogen/helium immiscibility; detailed atmosphere boundary tables that also provide self-consistent albedos and spectra; and options to address envelope metal gradients and stably-stratified regions.","Our hope is that these purpose-built features of APPLE will help catalyze the development of the next generation of giant exoplanet and Jovian planet evolutionary models."],"url":"http://arxiv.org/abs/2404.14483v1","category":"astro-ph.EP"}
{"created":"2024-04-22 18:00:01","title":"A Strong Gravitational Lens Is Worth a Thousand Dark Matter Halos: Inference on Small-Scale Structure Using Sequential Methods","abstract":"Strong gravitational lenses are a singular probe of the universe's small-scale structure $\\unicode{x2013}$ they are sensitive to the gravitational effects of low-mass $(<10^{10} M_\\odot)$ halos even without a luminous counterpart. Recent strong-lensing analyses of dark matter structure rely on simulation-based inference (SBI). Modern SBI methods, which leverage neural networks as density estimators, have shown promise in extracting the halo-population signal. However, it is unclear whether the constraining power of these models has been limited by the methodology or the information content of the data. In this study, we introduce an accelerator-optimized simulation pipeline that can generate lens images with realistic subhalo populations in a matter of milliseconds. Leveraging this simulator, we identify the main methodological limitation of our fiducial SBI analysis: training set size. We then adopt a sequential neural posterior estimation (SNPE) approach, allowing us to iteratively refine the distribution of simulated training images to better align with the observed data. Using only one-fifth as many mock Hubble Space Telescope (HST) images, SNPE matches the constraints on the low-mass halo population produced by our best non-sequential model. Our experiments suggest that an over three order-of-magnitude increase in training set size and GPU hours would be required to achieve an equivalent result without sequential methods. While the full potential of the existing strong lens sample remains to be explored, the notable improvement in constraining power enabled by our sequential approach highlights that the current constraints are limited primarily by methodology and not the data itself. Moreover, our results emphasize the need to treat training set generation and model optimization as interconnected stages of any cosmological analysis using simulation-based inference techniques.","sentences":["Strong gravitational lenses are a singular probe of the universe's small-scale structure $\\unicode{x2013}$ they are sensitive to the gravitational effects of low-mass $(<10^{10} M_\\odot)$ halos even without a luminous counterpart.","Recent strong-lensing analyses of dark matter structure rely on simulation-based inference (SBI).","Modern SBI methods, which leverage neural networks as density estimators, have shown promise in extracting the halo-population signal.","However, it is unclear whether the constraining power of these models has been limited by the methodology or the information content of the data.","In this study, we introduce an accelerator-optimized simulation pipeline that can generate lens images with realistic subhalo populations in a matter of milliseconds.","Leveraging this simulator, we identify the main methodological limitation of our fiducial SBI analysis: training set size.","We then adopt a sequential neural posterior estimation (SNPE) approach, allowing us to iteratively refine the distribution of simulated training images to better align with the observed data.","Using only one-fifth as many mock Hubble Space Telescope (HST) images, SNPE matches the constraints on the low-mass halo population produced by our best non-sequential model.","Our experiments suggest that an over three order-of-magnitude increase in training set size and GPU hours would be required to achieve an equivalent result without sequential methods.","While the full potential of the existing strong lens sample remains to be explored, the notable improvement in constraining power enabled by our sequential approach highlights that the current constraints are limited primarily by methodology and not the data itself.","Moreover, our results emphasize the need to treat training set generation and model optimization as interconnected stages of any cosmological analysis using simulation-based inference techniques."],"url":"http://arxiv.org/abs/2404.14487v1","category":"astro-ph.CO"}
{"created":"2024-04-22 18:00:00","title":"Waves on Mazes","abstract":"One way to describe the entropy of black holes comes from partitioning momentum charge across fractionated intersecting brane systems. Here we construct $\\frac{1}{8}$-BPS solutions by adding momentum to a maze of M2-brane strips stretched between M5 branes. Before the addition of momentum, the $\\frac{1}{4}$-BPS supergravity solution describing the maze is governed by a master function obeying a complicated Monge-Amp\\`ere equation. Given such a solution, we show that one can add momentum waves without modifying the $\\frac{1}{4}$-BPS M2-M5 background. Remarkably, these excitations are fully determined by a layered set of $\\textit{linear}$ equations. The fields responsible for carrying the momentum are parameterized by arbitrary functions of a null direction, and have exactly the same structure as in brane world-volume constructions. The fact that the momentum and flux excitations of the M2-M5-P system are governed by a linear structure brings us one step closer to using supergravity solutions to capture the entropy of supersymmetric black-holes.","sentences":["One way to describe the entropy of black holes comes from partitioning momentum charge across fractionated intersecting brane systems.","Here we construct $\\frac{1}{8}$-BPS solutions by adding momentum to a maze of M2-brane strips stretched between M5 branes.","Before the addition of momentum, the $\\frac{1}{4}$-BPS supergravity solution describing the maze is governed by a master function obeying a complicated Monge-Amp\\`ere equation.","Given such a solution, we show that one can add momentum waves without modifying the $\\frac{1}{4}$-BPS M2-M5 background.","Remarkably, these excitations are fully determined by a layered set of $\\textit{linear}$ equations.","The fields responsible for carrying the momentum are parameterized by arbitrary functions of a null direction, and have exactly the same structure as in brane world-volume constructions.","The fact that the momentum and flux excitations of the M2-M5-P system are governed by a linear structure brings us one step closer to using supergravity solutions to capture the entropy of supersymmetric black-holes."],"url":"http://arxiv.org/abs/2404.14477v1","category":"hep-th"}
{"created":"2024-04-23 17:42:29","title":"How to use and interpret activation patching","abstract":"Activation patching is a popular mechanistic interpretability technique, but has many subtleties regarding how it is applied and how one may interpret the results. We provide a summary of advice and best practices, based on our experience using this technique in practice. We include an overview of the different ways to apply activation patching and a discussion on how to interpret the results. We focus on what evidence patching experiments provide about circuits, and on the choice of metric and associated pitfalls.","sentences":["Activation patching is a popular mechanistic interpretability technique, but has many subtleties regarding how it is applied and how one may interpret the results.","We provide a summary of advice and best practices, based on our experience using this technique in practice.","We include an overview of the different ways to apply activation patching and a discussion on how to interpret the results.","We focus on what evidence patching experiments provide about circuits, and on the choice of metric and associated pitfalls."],"url":"http://arxiv.org/abs/2404.15255v1","category":"cs.LG"}
{"created":"2024-04-23 15:49:37","title":"Rethinking LLM Memorization through the Lens of Adversarial Compression","abstract":"Large language models (LLMs) trained on web-scale datasets raise substantial concerns regarding permissible data usage. One major question is whether these models \"memorize\" all their training data or they integrate many data sources in some way more akin to how a human would learn and synthesize information. The answer hinges, to a large degree, on $\\textit{how we define memorization}$. In this work, we propose the Adversarial Compression Ratio (ACR) as a metric for assessing memorization in LLMs -- a given string from the training data is considered memorized if it can be elicited by a prompt shorter than the string itself. In other words, these strings can be \"compressed\" with the model by computing adversarial prompts of fewer tokens. We outline the limitations of existing notions of memorization and show how the ACR overcomes these challenges by (i) offering an adversarial view to measuring memorization, especially for monitoring unlearning and compliance; and (ii) allowing for the flexibility to measure memorization for arbitrary strings at a reasonably low compute. Our definition serves as a valuable and practical tool for determining when model owners may be violating terms around data usage, providing a potential legal tool and a critical lens through which to address such scenarios. Project page: https://locuslab.github.io/acr-memorization.","sentences":["Large language models (LLMs) trained on web-scale datasets raise substantial concerns regarding permissible data usage.","One major question is whether these models \"memorize\" all their training data or they integrate many data sources in some way more akin to how a human would learn and synthesize information.","The answer hinges, to a large degree, on $\\textit{how we define memorization}$. In this work, we propose the Adversarial Compression Ratio (ACR) as a metric for assessing memorization in LLMs -- a given string from the training data is considered memorized if it can be elicited by a prompt shorter than the string itself.","In other words, these strings can be \"compressed\" with the model by computing adversarial prompts of fewer tokens.","We outline the limitations of existing notions of memorization and show how the ACR overcomes these challenges by (i) offering an adversarial view to measuring memorization, especially for monitoring unlearning and compliance; and (ii) allowing for the flexibility to measure memorization for arbitrary strings at a reasonably low compute.","Our definition serves as a valuable and practical tool for determining when model owners may be violating terms around data usage, providing a potential legal tool and a critical lens through which to address such scenarios.","Project page: https://locuslab.github.io/acr-memorization."],"url":"http://arxiv.org/abs/2404.15146v1","category":"cs.LG"}
{"created":"2024-04-23 14:52:09","title":"Impedance Matching: Enabling an RL-Based Running Jump in a Quadruped Robot","abstract":"Replicating the remarkable athleticism seen in animals has long been a challenge in robotics control. Although Reinforcement Learning (RL) has demonstrated significant progress in dynamic legged locomotion control, the substantial sim-to-real gap often hinders the real-world demonstration of truly dynamic movements. We propose a new framework to mitigate this gap through frequency-domain analysis-based impedance matching between simulated and real robots. Our framework offers a structured guideline for parameter selection and the range for dynamics randomization in simulation, thus facilitating a safe sim-to-real transfer. The learned policy using our framework enabled jumps across distances of 55 cm and heights of 38 cm. The results are, to the best of our knowledge, one of the highest and longest running jumps demonstrated by an RL-based control policy in a real quadruped robot. Note that the achieved jumping height is approximately 85% of that obtained from a state-of-the-art trajectory optimization method, which can be seen as the physical limit for the given robot hardware. In addition, our control policy accomplished stable walking at speeds up to 2 m/s in the forward and backward directions, and 1 m/s in the sideway direction.","sentences":["Replicating the remarkable athleticism seen in animals has long been a challenge in robotics control.","Although Reinforcement Learning (RL) has demonstrated significant progress in dynamic legged locomotion control, the substantial sim-to-real gap often hinders the real-world demonstration of truly dynamic movements.","We propose a new framework to mitigate this gap through frequency-domain analysis-based impedance matching between simulated and real robots.","Our framework offers a structured guideline for parameter selection and the range for dynamics randomization in simulation, thus facilitating a safe sim-to-real transfer.","The learned policy using our framework enabled jumps across distances of 55 cm and heights of 38 cm.","The results are, to the best of our knowledge, one of the highest and longest running jumps demonstrated by an RL-based control policy in a real quadruped robot.","Note that the achieved jumping height is approximately 85% of that obtained from a state-of-the-art trajectory optimization method, which can be seen as the physical limit for the given robot hardware.","In addition, our control policy accomplished stable walking at speeds up to 2 m/s in the forward and backward directions, and 1 m/s in the sideway direction."],"url":"http://arxiv.org/abs/2404.15096v1","category":"cs.RO"}
{"created":"2024-04-23 14:16:47","title":"Understanding IoT Domain Names: Analysis and Classification Using Machine Learning","abstract":"In this paper, we investigate the domain names of servers on the Internet that are accessed by IoT devices performing machine-to-machine communications. Using machine learning, we classify between them and domain names of servers contacted by other types of devices. By surveying past studies that used testbeds with real-world devices and using lists of top visited websites, we construct lists of domain names of both types of servers. We study the statistical properties of the domain name lists and train six machine learning models to perform the classification. The word embedding technique we use to get the real-value representation of the domain names is Word2vec. Among the models we train, Random Forest achieves the highest performance in classifying the domain names, yielding the highest accuracy, precision, recall, and F1 score. Our work offers novel insights to IoT, potentially informing protocol design and aiding in network security and performance monitoring.","sentences":["In this paper, we investigate the domain names of servers on the Internet that are accessed by IoT devices performing machine-to-machine communications.","Using machine learning, we classify between them and domain names of servers contacted by other types of devices.","By surveying past studies that used testbeds with real-world devices and using lists of top visited websites, we construct lists of domain names of both types of servers.","We study the statistical properties of the domain name lists and train six machine learning models to perform the classification.","The word embedding technique we use to get the real-value representation of the domain names is Word2vec.","Among the models we train, Random Forest achieves the highest performance in classifying the domain names, yielding the highest accuracy, precision, recall, and F1 score.","Our work offers novel insights to IoT, potentially informing protocol design and aiding in network security and performance monitoring."],"url":"http://arxiv.org/abs/2404.15068v1","category":"cs.NI"}
{"created":"2024-04-23 13:32:29","title":"A Learning Paradigm for Interpretable Gradients","abstract":"This paper studies interpretability of convolutional networks by means of saliency maps. Most approaches based on Class Activation Maps (CAM) combine information from fully connected layers and gradient through variants of backpropagation. However, it is well understood that gradients are noisy and alternatives like guided backpropagation have been proposed to obtain better visualization at inference. In this work, we present a novel training approach to improve the quality of gradients for interpretability. In particular, we introduce a regularization loss such that the gradient with respect to the input image obtained by standard backpropagation is similar to the gradient obtained by guided backpropagation. We find that the resulting gradient is qualitatively less noisy and improves quantitatively the interpretability properties of different networks, using several interpretability methods.","sentences":["This paper studies interpretability of convolutional networks by means of saliency maps.","Most approaches based on Class Activation Maps (CAM) combine information from fully connected layers and gradient through variants of backpropagation.","However, it is well understood that gradients are noisy and alternatives like guided backpropagation have been proposed to obtain better visualization at inference.","In this work, we present a novel training approach to improve the quality of gradients for interpretability.","In particular, we introduce a regularization loss such that the gradient with respect to the input image obtained by standard backpropagation is similar to the gradient obtained by guided backpropagation.","We find that the resulting gradient is qualitatively less noisy and improves quantitatively the interpretability properties of different networks, using several interpretability methods."],"url":"http://arxiv.org/abs/2404.15024v1","category":"cs.CV"}
{"created":"2024-04-23 13:09:11","title":"TAXI: Evaluating Categorical Knowledge Editing for Language Models","abstract":"Humans rarely learn one fact in isolation. Instead, learning a new fact induces knowledge of other facts about the world. For example, in learning a korat is a type of cat, you also infer it is a mammal and has claws, ensuring your model of the world is consistent. Knowledge editing aims to inject new facts into language models to improve their factuality, but current benchmarks fail to evaluate consistency, which is critical to ensure efficient, accurate, and generalizable edits. We manually create TAXI, a new benchmark dataset specifically created to evaluate consistency. TAXI contains 11,120 multiple-choice queries for 976 edits spanning 41 categories (e.g., Dogs), 164 subjects (e.g., Labrador), and 183 properties (e.g., is a mammal). We then use TAXI to evaluate popular editors' consistency, measuring how often editing a subject's category appropriately edits its properties. We find that 1) the editors achieve marginal, yet non-random consistency, 2) their consistency far underperforms human baselines, and 3) consistency is more achievable when editing atypical subjects. Our code and data are available at https://github.com/derekpowell/taxi.","sentences":["Humans rarely learn one fact in isolation.","Instead, learning a new fact induces knowledge of other facts about the world.","For example, in learning a korat is a type of cat, you also infer it is a mammal and has claws, ensuring your model of the world is consistent.","Knowledge editing aims to inject new facts into language models to improve their factuality, but current benchmarks fail to evaluate consistency, which is critical to ensure efficient, accurate, and generalizable edits.","We manually create TAXI, a new benchmark dataset specifically created to evaluate consistency.","TAXI contains 11,120 multiple-choice queries for 976 edits spanning 41 categories (e.g., Dogs), 164 subjects (e.g., Labrador), and 183 properties (e.g., is a mammal).","We then use TAXI to evaluate popular editors' consistency, measuring how often editing a subject's category appropriately edits its properties.","We find that 1) the editors achieve marginal, yet non-random consistency, 2) their consistency far underperforms human baselines, and 3) consistency is more achievable when editing atypical subjects.","Our code and data are available at https://github.com/derekpowell/taxi."],"url":"http://arxiv.org/abs/2404.15004v1","category":"cs.CL"}
{"created":"2024-04-23 12:47:31","title":"A Short Review for Ontology Learning from Text: Stride from Shallow Learning, Deep Learning to Large Language Models Trend","abstract":"Ontologies provide formal representation of knowledge shared within Semantic Web applications and Ontology learning from text involves the construction of ontologies from a given corpus of text. In the past years, ontology learning has traversed through shallow learning and deep learning methodologies, each offering distinct advantages and limitations in the quest for knowledge extraction and representation. A new trend of these approaches is relying on large language models to enhance ontology learning. This paper gives a review in approaches and challenges of ontology learning. It analyzes the methodologies and limitations of shallow-learning-based and deep-learning-based techniques for ontology learning, and provides comprehensive knowledge for the frontier work of using large language models to enhance ontology learning. In addition, it proposes several noteworthy future directions for further exploration into the integration of large language models with ontology learning tasks.","sentences":["Ontologies provide formal representation of knowledge shared within Semantic Web applications and Ontology learning from text involves the construction of ontologies from a given corpus of text.","In the past years, ontology learning has traversed through shallow learning and deep learning methodologies, each offering distinct advantages and limitations in the quest for knowledge extraction and representation.","A new trend of these approaches is relying on large language models to enhance ontology learning.","This paper gives a review in approaches and challenges of ontology learning.","It analyzes the methodologies and limitations of shallow-learning-based and deep-learning-based techniques for ontology learning, and provides comprehensive knowledge for the frontier work of using large language models to enhance ontology learning.","In addition, it proposes several noteworthy future directions for further exploration into the integration of large language models with ontology learning tasks."],"url":"http://arxiv.org/abs/2404.14991v1","category":"cs.IR"}
{"created":"2024-04-23 12:42:07","title":"Other Tokens Matter: Exploring Global and Local Features of Vision Transformers for Object Re-Identification","abstract":"Object Re-Identification (Re-ID) aims to identify and retrieve specific objects from images captured at different places and times. Recently, object Re-ID has achieved great success with the advances of Vision Transformers (ViT). However, the effects of the global-local relation have not been fully explored in Transformers for object Re-ID. In this work, we first explore the influence of global and local features of ViT and then further propose a novel Global-Local Transformer (GLTrans) for high-performance object Re-ID. We find that the features from last few layers of ViT already have a strong representational ability, and the global and local information can mutually enhance each other. Based on this fact, we propose a Global Aggregation Encoder (GAE) to utilize the class tokens of the last few Transformer layers and learn comprehensive global features effectively. Meanwhile, we propose the Local Multi-layer Fusion (LMF) which leverages both the global cues from GAE and multi-layer patch tokens to explore the discriminative local representations. Extensive experiments demonstrate that our proposed method achieves superior performance on four object Re-ID benchmarks.","sentences":["Object Re-Identification (Re-ID) aims to identify and retrieve specific objects from images captured at different places and times.","Recently, object Re-ID has achieved great success with the advances of Vision Transformers (ViT).","However, the effects of the global-local relation have not been fully explored in Transformers for object Re-ID.","In this work, we first explore the influence of global and local features of ViT and then further propose a novel Global-Local Transformer (GLTrans) for high-performance object Re-ID.","We find that the features from last few layers of ViT already have a strong representational ability, and the global and local information can mutually enhance each other.","Based on this fact, we propose a Global Aggregation Encoder (GAE) to utilize the class tokens of the last few Transformer layers and learn comprehensive global features effectively.","Meanwhile, we propose the Local Multi-layer Fusion (LMF) which leverages both the global cues from GAE and multi-layer patch tokens to explore the discriminative local representations.","Extensive experiments demonstrate that our proposed method achieves superior performance on four object Re-ID benchmarks."],"url":"http://arxiv.org/abs/2404.14985v1","category":"cs.CV"}
{"created":"2024-04-23 12:24:53","title":"Integrating Heterogeneous Gene Expression Data through Knowledge Graphs for Improving Diabetes Prediction","abstract":"Diabetes is a worldwide health issue affecting millions of people. Machine learning methods have shown promising results in improving diabetes prediction, particularly through the analysis of diverse data types, namely gene expression data. While gene expression data can provide valuable insights, challenges arise from the fact that the sample sizes in expression datasets are usually limited, and the data from different datasets with different gene expressions cannot be easily combined.   This work proposes a novel approach to address these challenges by integrating multiple gene expression datasets and domain-specific knowledge using knowledge graphs, a unique tool for biomedical data integration. KG embedding methods are then employed to generate vector representations, serving as inputs for a classifier. Experiments demonstrated the efficacy of our approach, revealing improvements in diabetes prediction when integrating multiple gene expression datasets and domain-specific knowledge about protein functions and interactions.","sentences":["Diabetes is a worldwide health issue affecting millions of people.","Machine learning methods have shown promising results in improving diabetes prediction, particularly through the analysis of diverse data types, namely gene expression data.","While gene expression data can provide valuable insights, challenges arise from the fact that the sample sizes in expression datasets are usually limited, and the data from different datasets with different gene expressions cannot be easily combined.   ","This work proposes a novel approach to address these challenges by integrating multiple gene expression datasets and domain-specific knowledge using knowledge graphs, a unique tool for biomedical data integration.","KG embedding methods are then employed to generate vector representations, serving as inputs for a classifier.","Experiments demonstrated the efficacy of our approach, revealing improvements in diabetes prediction when integrating multiple gene expression datasets and domain-specific knowledge about protein functions and interactions."],"url":"http://arxiv.org/abs/2404.14970v1","category":"cs.LG"}
{"created":"2024-04-23 11:13:23","title":"Optimal Refund Mechanism with Consumer Learning","abstract":"This paper studies the optimal refund mechanism when an uninformed buyer can privately acquire information about his valuation of a product over time. We consider a class of refund mechanisms based on stochastic return policies: if the buyer requests a return, the seller will issue a (partial) refund while allowing the buyer to keep the product with some probability. Such return policies can affect the buyer's learning process and thereby influence the return rate. Nevertheless, we show that the optimal refund mechanism is deterministic and takes a simple form: either the seller offers a sufficiently low price and disallows returns to deter buyer learning, or she offers a sufficiently high price with free returns to implement maximal buyer learning. The form of the optimal refund mechanism is non-monotone in the buyer's prior belief regarding his valuation.","sentences":["This paper studies the optimal refund mechanism when an uninformed buyer can privately acquire information about his valuation of a product over time.","We consider a class of refund mechanisms based on stochastic return policies: if the buyer requests a return, the seller will issue a (partial) refund while allowing the buyer to keep the product with some probability.","Such return policies can affect the buyer's learning process and thereby influence the return rate.","Nevertheless, we show that the optimal refund mechanism is deterministic and takes a simple form: either the seller offers a sufficiently low price and disallows returns to deter buyer learning, or she offers a sufficiently high price with free returns to implement maximal buyer learning.","The form of the optimal refund mechanism is non-monotone in the buyer's prior belief regarding his valuation."],"url":"http://arxiv.org/abs/2404.14927v1","category":"econ.TH"}
{"created":"2024-04-23 11:09:08","title":"Stalnaker's Epistemic Logic in Isabelle/HOL","abstract":"The foundations of formal models for epistemic and doxastic logics often rely on certain logical aspects of modal logics such as S4 and S4.2 and their semantics; however, the corresponding mathematical results are often stated in papers or books without including a detailed proof, or a reference to it, that allows the reader to convince themselves about them. We reinforce the foundations of the epistemic logic S4.2 for countably many agents by formalizing its soundness and completeness results for the class of all weakly-directed pre-orders in the proof assistant Isabelle/HOL. This logic corresponds to the knowledge fragment, i.e., the logic for formulas that may only include knowledge modalities in Stalnaker's system for knowledge and belief. Additionally, we formalize the equivalence between two axiomatizations for S4, which are used depending on the type of semantics given to the modal operators, as one is commonly used for the relational semantics, and the other one arises naturally from the topological semantics.","sentences":["The foundations of formal models for epistemic and doxastic logics often rely on certain logical aspects of modal logics such as S4 and S4.2 and their semantics; however, the corresponding mathematical results are often stated in papers or books without including a detailed proof, or a reference to it, that allows the reader to convince themselves about them.","We reinforce the foundations of the epistemic logic S4.2 for countably many agents by formalizing its soundness and completeness results for the class of all weakly-directed pre-orders in the proof assistant Isabelle/HOL.","This logic corresponds to the knowledge fragment, i.e., the logic for formulas that may only include knowledge modalities in Stalnaker's system for knowledge and belief.","Additionally, we formalize the equivalence between two axiomatizations for S4, which are used depending on the type of semantics given to the modal operators, as one is commonly used for the relational semantics, and the other one arises naturally from the topological semantics."],"url":"http://arxiv.org/abs/2404.14919v1","category":"cs.LO"}
{"created":"2024-04-23 10:56:58","title":"Additive Margin in Contrastive Self-Supervised Frameworks to Learn Discriminative Speaker Representations","abstract":"Self-Supervised Learning (SSL) frameworks became the standard for learning robust class representations by benefiting from large unlabeled datasets. For Speaker Verification (SV), most SSL systems rely on contrastive-based loss functions. We explore different ways to improve the performance of these techniques by revisiting the NT-Xent contrastive loss. Our main contribution is the definition of the NT-Xent-AM loss and the study of the importance of Additive Margin (AM) in SimCLR and MoCo SSL methods to further separate positive from negative pairs. Despite class collisions, we show that AM enhances the compactness of same-speaker embeddings and reduces the number of false negatives and false positives on SV. Additionally, we demonstrate the effectiveness of the symmetric contrastive loss, which provides more supervision for the SSL task. Implementing these two modifications to SimCLR improves performance and results in 7.85% EER on VoxCeleb1-O, outperforming other equivalent methods.","sentences":["Self-Supervised Learning (SSL) frameworks became the standard for learning robust class representations by benefiting from large unlabeled datasets.","For Speaker Verification (SV), most SSL systems rely on contrastive-based loss functions.","We explore different ways to improve the performance of these techniques by revisiting the NT-Xent contrastive loss.","Our main contribution is the definition of the NT-Xent-AM loss and the study of the importance of Additive Margin (AM) in SimCLR and MoCo SSL methods to further separate positive from negative pairs.","Despite class collisions, we show that AM enhances the compactness of same-speaker embeddings and reduces the number of false negatives and false positives on SV.","Additionally, we demonstrate the effectiveness of the symmetric contrastive loss, which provides more supervision for the SSL task.","Implementing these two modifications to SimCLR improves performance and results in 7.85% EER on VoxCeleb1-O, outperforming other equivalent methods."],"url":"http://arxiv.org/abs/2404.14913v1","category":"eess.AS"}
{"created":"2024-04-23 10:15:54","title":"Hoeffding's inequality for continuous-time Markov chains","abstract":"Hoeffding's inequality is a fundamental tool widely applied in probability theory, statistics, and machine learning. In this paper, we establish Hoeffding's inequalities specifically tailored for an irreducible and positive recurrent continuous-time Markov chain (CTMC) on a countable state space with the invariant probability distribution ${\\pi}$ and an $\\mathcal{L}^{2}(\\pi)$-spectral gap ${\\lambda}(Q)$. More precisely, for a function $g:E\\to [a,b]$ with a mean $\\pi(g)$, and given $t,\\varepsilon>0$, we derive the inequality \\[ \\mathbb{P}_{\\pi}\\left(\\frac{1}{t} \\int_{0}^{t} g\\left(X_{s}\\right)\\mathrm{d}s-\\pi (g) \\geq \\varepsilon \\right) \\leq \\exp\\left\\{-\\frac{{\\lambda}(Q)t\\varepsilon^2}{(b-a)^2} \\right\\}, \\] which can be viewed as a generalization of Hoeffding's inequality for discrete-time Markov chains (DTMCs) presented in [J. Fan et al., J. Mach. Learn. Res., 22(2022), pp. 6185-6219] to the realm of CTMCs. The key analysis enabling the attainment of this inequality lies in the utilization of the techniques of skeleton chains and augmented truncation approximations. Furthermore, we also discuss Hoeffding's inequality for a jump process on a general state space.","sentences":["Hoeffding's inequality is a fundamental tool widely applied in probability theory, statistics, and machine learning.","In this paper, we establish Hoeffding's inequalities specifically tailored for an irreducible and positive recurrent continuous-time Markov chain (CTMC) on a countable state space with the invariant probability distribution ${\\pi}$ and an $\\mathcal{L}^{2}(\\pi)$-spectral gap ${\\lambda}(Q)$. More precisely, for a function $g:E\\to","[a,b]$ with a mean $\\pi(g)$, and given $t,\\varepsilon>0$, we derive the inequality \\[ \\mathbb{P}_{\\pi}\\left(\\frac{1}{t} \\int_{0}^{t} g\\left(X_{s}\\right)\\mathrm{d}s-\\pi (g) \\geq \\varepsilon \\right) \\leq \\exp\\left\\{-\\frac{{\\lambda}(Q)t\\varepsilon^2}{(b-a)^2} \\right\\}, \\] which can be viewed as a generalization of Hoeffding's inequality for discrete-time Markov chains (DTMCs) presented in [J. Fan et al., J. Mach.","Learn.","Res., 22(2022), pp. 6185-6219] to the realm of CTMCs.","The key analysis enabling the attainment of this inequality lies in the utilization of the techniques of skeleton chains and augmented truncation approximations.","Furthermore, we also discuss Hoeffding's inequality for a jump process on a general state space."],"url":"http://arxiv.org/abs/2404.14888v1","category":"math.PR"}
{"created":"2024-04-23 10:13:39","title":"GCEPNet: Graph Convolution-Enhanced Expectation Propagation for Massive MIMO Detection","abstract":"Massive MIMO (multiple-input multiple-output) detection is an important topic in wireless communication and various machine learning based methods have been developed recently for this task. Expectation propagation (EP) and its variants are widely used for MIMO detection and have achieved the best performance. However, EP-based solvers fail to capture the correlation between unknown variables, leading to loss of information, and in addition, they are computationally expensive. In this paper, we show that the real-valued system can be modeled as spectral signal convolution on graph, through which the correlation between unknown variables can be captured. Based on this analysis, we propose graph convolution-enhanced expectation propagation (GCEPNet), a graph convolution-enhanced EP detector. GCEPNet incorporates data-dependent attention scores into Chebyshev polynomial for powerful graph convolution with better generalization capacity. It enables a better estimation of the cavity distribution for EP and empirically achieves the state-of-the-art (SOTA) MIMO detection performance with much faster inference speed. To our knowledge, we are the first to shed light on the connection between the system model and graph convolution, and the first to design the data-dependent attention scores for graph convolution.","sentences":["Massive MIMO (multiple-input multiple-output) detection is an important topic in wireless communication and various machine learning based methods have been developed recently for this task.","Expectation propagation (EP) and its variants are widely used for MIMO detection and have achieved the best performance.","However, EP-based solvers fail to capture the correlation between unknown variables, leading to loss of information, and in addition, they are computationally expensive.","In this paper, we show that the real-valued system can be modeled as spectral signal convolution on graph, through which the correlation between unknown variables can be captured.","Based on this analysis, we propose graph convolution-enhanced expectation propagation (GCEPNet), a graph convolution-enhanced EP detector.","GCEPNet incorporates data-dependent attention scores into Chebyshev polynomial for powerful graph convolution with better generalization capacity.","It enables a better estimation of the cavity distribution for EP and empirically achieves the state-of-the-art (SOTA) MIMO detection performance with much faster inference speed.","To our knowledge, we are the first to shed light on the connection between the system model and graph convolution, and the first to design the data-dependent attention scores for graph convolution."],"url":"http://arxiv.org/abs/2404.14886v1","category":"cs.LG"}
{"created":"2024-04-23 09:42:59","title":"How we Learn Concepts: A Review of Relevant Advances Since 2010 and Its Inspirations for Teaching","abstract":"This article reviews the psychological and neuroscience achievements in concept learning since 2010 from the perspectives of individual learning and social learning, and discusses several issues related to concept learning, including the assistance of machine learning about concept learning. 1 In terms of individual learning, current evidences shown that the brain tends to process concrete concepts through typical features (shared features); And abstract concepts, semantic processing is the most important cognitive way. 2 In terms of social learning, Interpersonal Neuro Synchronization (INS) is considered the main indicator of efficient knowledge transfer (such as teaching activities between teachers and students), but this phenomenon only broadens the channels for concept sources and does not change the basic mode of individual concept learning. Ultimately, this article argues that the way the human brain processes concepts depends on concept's own characteristics, so there are no 'better' strategies in teaching, only more 'suitable' strategies.","sentences":["This article reviews the psychological and neuroscience achievements in concept learning since 2010 from the perspectives of individual learning and social learning, and discusses several issues related to concept learning, including the assistance of machine learning about concept learning.","1","In terms of individual learning, current evidences shown that the brain tends to process concrete concepts through typical features (shared features); And abstract concepts, semantic processing is the most important cognitive way.","2","In terms of social learning, Interpersonal Neuro Synchronization (INS) is considered the main indicator of efficient knowledge transfer (such as teaching activities between teachers and students), but this phenomenon only broadens the channels for concept sources and does not change the basic mode of individual concept learning.","Ultimately, this article argues that the way the human brain processes concepts depends on concept's own characteristics, so there are no 'better' strategies in teaching, only more 'suitable' strategies."],"url":"http://arxiv.org/abs/2404.14867v1","category":"q-bio.NC"}
{"created":"2024-04-23 09:37:52","title":"Unitary Synthesis of Clifford+T Circuits with Reinforcement Learning","abstract":"This paper presents a deep reinforcement learning approach for synthesizing unitaries into quantum circuits. Unitary synthesis aims to identify a quantum circuit that represents a given unitary while minimizing circuit depth, total gate count, a specific gate count, or a combination of these factors. While past research has focused predominantly on continuous gate sets, synthesizing unitaries from the parameter-free Clifford+T gate set remains a challenge. Although the time complexity of this task will inevitably remain exponential in the number of qubits for general unitaries, reducing the runtime for simple problem instances still poses a significant challenge. In this study, we apply the tree-search method Gumbel AlphaZero to solve the problem for a subset of exactly synthesizable Clifford+T unitaries. Our approach can synthesize unitaries for up to five qubits generated from the set of randomized quantum circuits with up to 60 gates. Furthermore, our inference times are around 30 seconds on a single GPU on average, surpassing state-of-the-art algorithms QuantumCircuitOpt and MIN-T-SYNTH for higher qubit numbers. Our work provides a competitive baseline for synthesis algorithms to be developed in the upcoming years.","sentences":["This paper presents a deep reinforcement learning approach for synthesizing unitaries into quantum circuits.","Unitary synthesis aims to identify a quantum circuit that represents a given unitary while minimizing circuit depth, total gate count, a specific gate count, or a combination of these factors.","While past research has focused predominantly on continuous gate sets, synthesizing unitaries from the parameter-free Clifford+T gate set remains a challenge.","Although the time complexity of this task will inevitably remain exponential in the number of qubits for general unitaries, reducing the runtime for simple problem instances still poses a significant challenge.","In this study, we apply the tree-search method Gumbel AlphaZero to solve the problem for a subset of exactly synthesizable Clifford+T unitaries.","Our approach can synthesize unitaries for up to five qubits generated from the set of randomized quantum circuits with up to 60 gates.","Furthermore, our inference times are around 30 seconds on a single GPU on average, surpassing state-of-the-art algorithms QuantumCircuitOpt and MIN-T-SYNTH for higher qubit numbers.","Our work provides a competitive baseline for synthesis algorithms to be developed in the upcoming years."],"url":"http://arxiv.org/abs/2404.14865v2","category":"quant-ph"}
{"created":"2024-04-23 09:21:15","title":"Cross-Domain Causal Preference Learning for Out-of-Distribution Recommendation","abstract":"Recommender systems use users' historical interactions to learn their preferences and deliver personalized recommendations from a vast array of candidate items. Current recommender systems primarily rely on the assumption that the training and testing datasets have identical distributions, which may not hold true in reality. In fact, the distribution shift between training and testing datasets often occurs as a result of the evolution of user attributes, which degrades the performance of the conventional recommender systems because they fail in Out-of-Distribution (OOD) generalization, particularly in situations of data sparsity. This study delves deeply into the challenge of OOD generalization and proposes a novel model called Cross-Domain Causal Preference Learning for Out-of-Distribution Recommendation (CDCOR), which involves employing a domain adversarial network to uncover users' domain-shared preferences and utilizing a causal structure learner to capture causal invariance to deal with the OOD problem. Through extensive experiments on two real-world datasets, we validate the remarkable performance of our model in handling diverse scenarios of data sparsity and out-of-distribution environments. Furthermore, our approach surpasses the benchmark models, showcasing outstanding capabilities in out-of-distribution generalization.","sentences":["Recommender systems use users' historical interactions to learn their preferences and deliver personalized recommendations from a vast array of candidate items.","Current recommender systems primarily rely on the assumption that the training and testing datasets have identical distributions, which may not hold true in reality.","In fact, the distribution shift between training and testing datasets often occurs as a result of the evolution of user attributes, which degrades the performance of the conventional recommender systems because they fail in Out-of-Distribution (OOD) generalization, particularly in situations of data sparsity.","This study delves deeply into the challenge of OOD generalization and proposes a novel model called Cross-Domain Causal Preference Learning for Out-of-Distribution Recommendation (CDCOR), which involves employing a domain adversarial network to uncover users' domain-shared preferences and utilizing a causal structure learner to capture causal invariance to deal with the OOD problem.","Through extensive experiments on two real-world datasets, we validate the remarkable performance of our model in handling diverse scenarios of data sparsity and out-of-distribution environments.","Furthermore, our approach surpasses the benchmark models, showcasing outstanding capabilities in out-of-distribution generalization."],"url":"http://arxiv.org/abs/2404.14856v1","category":"cs.IR"}
{"created":"2024-04-23 07:39:09","title":"Visual-Augmented Dynamic Semantic Prototype for Generative Zero-Shot Learning","abstract":"Generative Zero-shot learning (ZSL) learns a generator to synthesize visual samples for unseen classes, which is an effective way to advance ZSL. However, existing generative methods rely on the conditions of Gaussian noise and the predefined semantic prototype, which limit the generator only optimized on specific seen classes rather than characterizing each visual instance, resulting in poor generalizations (\\textit{e.g.}, overfitting to seen classes). To address this issue, we propose a novel Visual-Augmented Dynamic Semantic prototype method (termed VADS) to boost the generator to learn accurate semantic-visual mapping by fully exploiting the visual-augmented knowledge into semantic conditions. In detail, VADS consists of two modules: (1) Visual-aware Domain Knowledge Learning module (VDKL) learns the local bias and global prior of the visual features (referred to as domain visual knowledge), which replace pure Gaussian noise to provide richer prior noise information; (2) Vision-Oriented Semantic Updation module (VOSU) updates the semantic prototype according to the visual representations of the samples. Ultimately, we concatenate their output as a dynamic semantic prototype, which serves as the condition of the generator. Extensive experiments demonstrate that our VADS achieves superior CZSL and GZSL performances on three prominent datasets and outperforms other state-of-the-art methods with averaging increases by 6.4\\%, 5.9\\% and 4.2\\% on SUN, CUB and AWA2, respectively.","sentences":["Generative Zero-shot learning (ZSL) learns a generator to synthesize visual samples for unseen classes, which is an effective way to advance ZSL.","However, existing generative methods rely on the conditions of Gaussian noise and the predefined semantic prototype, which limit the generator only optimized on specific seen classes rather than characterizing each visual instance, resulting in poor generalizations (\\textit{e.g.}, overfitting to seen classes).","To address this issue, we propose a novel Visual-Augmented Dynamic Semantic prototype method (termed VADS) to boost the generator to learn accurate semantic-visual mapping by fully exploiting the visual-augmented knowledge into semantic conditions.","In detail, VADS consists of two modules: (1) Visual-aware Domain Knowledge Learning module (VDKL) learns the local bias and global prior of the visual features (referred to as domain visual knowledge), which replace pure Gaussian noise to provide richer prior noise information; (2) Vision-Oriented Semantic Updation module (VOSU) updates the semantic prototype according to the visual representations of the samples.","Ultimately, we concatenate their output as a dynamic semantic prototype, which serves as the condition of the generator.","Extensive experiments demonstrate that our VADS achieves superior CZSL and GZSL performances on three prominent datasets and outperforms other state-of-the-art methods with averaging increases by 6.4\\%, 5.9\\% and 4.2\\% on SUN, CUB and AWA2, respectively."],"url":"http://arxiv.org/abs/2404.14808v1","category":"cs.CV"}
{"created":"2024-04-23 06:40:42","title":"Black hole-neutron star binaries","abstract":"The gravitational wave signals of black hole-neutron star (BHNS) binary systems have now been detected, and future detections might be accompanied by electromagnetic counterparts. BHNS mergers involve much of the same physics as binary neutron star mergers: strong gravity, nuclear density matter, neutrino radiation, and magnetic turbulence. They also share with binary neutron star systems the potential for bright electromagnetic signals, especially gamma ray bursts and kilonovae, and the potential to be significant sources of r-process elements. However, BHNS binaries are more asymmetric, and their mergers produce different amounts and arrangements of the various post-merger material components (e.g. disk and dynamical ejecta), together with a more massive black hole; these differences can have interesting consequences. In this chapter, we review the modeling of BHNS mergers and post-merger evolution in numerical relativistic hydrodynamics and magnetohydrodynamics. We attempt to give readers a broad understanding of the answers to the following questions. What are the main considerations that determine the merger outcome? What input physics must (or should) go into a BHNS simulation? What have the most advanced simulations to date learned?","sentences":["The gravitational wave signals of black hole-neutron star (BHNS) binary systems have now been detected, and future detections might be accompanied by electromagnetic counterparts.","BHNS mergers involve much of the same physics as binary neutron star mergers: strong gravity, nuclear density matter, neutrino radiation, and magnetic turbulence.","They also share with binary neutron star systems the potential for bright electromagnetic signals, especially gamma ray bursts and kilonovae, and the potential to be significant sources of r-process elements.","However, BHNS binaries are more asymmetric, and their mergers produce different amounts and arrangements of the various post-merger material components (e.g. disk and dynamical ejecta), together with a more massive black hole; these differences can have interesting consequences.","In this chapter, we review the modeling of BHNS mergers and post-merger evolution in numerical relativistic hydrodynamics and magnetohydrodynamics.","We attempt to give readers a broad understanding of the answers to the following questions.","What are the main considerations that determine the merger outcome?","What input physics must (or should) go into a BHNS simulation?","What have the most advanced simulations to date learned?"],"url":"http://arxiv.org/abs/2404.14782v1","category":"astro-ph.HE"}
{"created":"2024-04-23 06:30:53","title":"CT-Agent: Clinical Trial Multi-Agent with Large Language Model-based Reasoning","abstract":"Large Language Models (LLMs) and multi-agent systems have shown impressive capabilities in natural language tasks but face challenges in clinical trial applications, primarily due to limited access to external knowledge. Recognizing the potential of advanced clinical trial tools that aggregate and predict based on the latest medical data, we propose an integrated solution to enhance their accessibility and utility. We introduce Clinical Agent System (CT-Agent), a Clinical multi-agent system designed for clinical trial tasks, leveraging GPT-4, multi-agent architectures, LEAST-TO-MOST, and ReAct reasoning technology. This integration not only boosts LLM performance in clinical contexts but also introduces novel functionalities. Our system autonomously manages the entire clinical trial process, demonstrating significant efficiency improvements in our evaluations, which include both computational benchmarks and expert feedback.","sentences":["Large Language Models (LLMs) and multi-agent systems have shown impressive capabilities in natural language tasks but face challenges in clinical trial applications, primarily due to limited access to external knowledge.","Recognizing the potential of advanced clinical trial tools that aggregate and predict based on the latest medical data, we propose an integrated solution to enhance their accessibility and utility.","We introduce Clinical Agent System (CT-Agent), a Clinical multi-agent system designed for clinical trial tasks, leveraging GPT-4, multi-agent architectures, LEAST-TO-MOST, and ReAct reasoning technology.","This integration not only boosts LLM performance in clinical contexts but also introduces novel functionalities.","Our system autonomously manages the entire clinical trial process, demonstrating significant efficiency improvements in our evaluations, which include both computational benchmarks and expert feedback."],"url":"http://arxiv.org/abs/2404.14777v1","category":"cs.CL"}
{"created":"2024-04-23 06:29:48","title":"Contrastive Quantization based Semantic Code for Generative Recommendation","abstract":"With the success of large language models, generative retrieval has emerged as a new retrieval technique for recommendation. It can be divided into two stages: the first stage involves constructing discrete Codes (i.e., codes), and the second stage involves decoding the code sequentially via the transformer architecture. Current methods often construct item semantic codes by reconstructing based quantization on item textual representation, but they fail to capture item discrepancy that is essential in modeling item relationships in recommendation sytems. In this paper, we propose to construct the code representation of items by simultaneously considering both item relationships and semantic information. Specifically, we employ a pre-trained language model to extract item's textual description and translate it into item's embedding. Then we propose to enhance the encoder-decoder based RQVAE model with contrastive objectives to learn item code. To be specific, we employ the embeddings generated by the decoder from the samples themselves as positive instances and those from other samples as negative instances. Thus we effectively enhance the item discrepancy across all items, better preserving the item neighbourhood. Finally, we train and test semantic code with with generative retrieval on a sequential recommendation model. Our experiments demonstrate that our method improves NDCG@5 with 43.76% on the MIND dataset and Recall@10 with 80.95% on the Office dataset compared to the previous baselines.","sentences":["With the success of large language models, generative retrieval has emerged as a new retrieval technique for recommendation.","It can be divided into two stages: the first stage involves constructing discrete Codes (i.e., codes), and the second stage involves decoding the code sequentially via the transformer architecture.","Current methods often construct item semantic codes by reconstructing based quantization on item textual representation, but they fail to capture item discrepancy that is essential in modeling item relationships in recommendation sytems.","In this paper, we propose to construct the code representation of items by simultaneously considering both item relationships and semantic information.","Specifically, we employ a pre-trained language model to extract item's textual description and translate it into item's embedding.","Then we propose to enhance the encoder-decoder based RQVAE model with contrastive objectives to learn item code.","To be specific, we employ the embeddings generated by the decoder from the samples themselves as positive instances and those from other samples as negative instances.","Thus we effectively enhance the item discrepancy across all items, better preserving the item neighbourhood.","Finally, we train and test semantic code with with generative retrieval on a sequential recommendation model.","Our experiments demonstrate that our method improves NDCG@5 with 43.76% on the MIND dataset and Recall@10 with 80.95% on the Office dataset compared to the previous baselines."],"url":"http://arxiv.org/abs/2404.14774v1","category":"cs.IR"}
{"created":"2024-04-23 15:48:16","title":"Face embeddings of Archimedean solids","abstract":"We characterize the Archimedean solids among the convex uniform polyhedra via face embeddings into a regular Tetrahedron. This result has been listed without proof in the literature. As an application, we obtain a lattice packing of $\\mathbb{R}^3$ by the truncated Icosahedron whose packing density is notably close to the optimal value obtained in [1].","sentences":["We characterize the Archimedean solids among the convex uniform polyhedra via face embeddings into a regular Tetrahedron.","This result has been listed without proof in the literature.","As an application, we obtain a lattice packing of $\\mathbb{R}^3$ by the truncated Icosahedron whose packing density is notably close to the optimal value obtained in [1]."],"url":"http://arxiv.org/abs/2404.15142v1","category":"math.MG"}
{"created":"2024-04-23 14:28:57","title":"Cooperation, Correlation and Competition in Ergodic $N$-player Games and Mean-field Games of Singular Controls: A Case Study","abstract":"We consider ergodic symmetric $N$-player and mean-field games of singular control in both cooperative and competitive settings. The state process dynamics of a representative player follow geometric Brownian motion, controlled additively through a nondecreasing process. Agents aim to maximize a long-time average reward functional with instantaneous profit of power type. The game shows strategic complementarities, in that the marginal profit function is increasing with respect to the dynamic average of the states of the other players, when $N<\\infty$, or with respect to the stationary mean of the players' distribution, in the mean-field case. In the mean-field formulation, we explicitly construct the solution to the mean-field control problem associated with central planner optimization, as well as Nash and coarse correlated equilibria (with singular and regular recommendations). Among our findings, we show that coarse correlated equilibria may exist even when Nash equilibria do not. Additionally, we show that a coarse correlated equilibrium with a regular (absolutely continuous) recommendation can outperform a Nash equilibrium where the equilibrium policy is of reflecting type (thus singularly continuous). Furthermore, we prove that the constructed mean-field control and mean-field equilibria can approximate the cooperative and competitive equilibria, respectively, in the corresponding game with $N$ players when $N$ is sufficiently large. To the best of our knowledge, this paper is the first to characterize coarse correlated equilibria, construct the explicit solution to an ergodic mean-field control problem, and provide approximation results for the related $N$-player game in the context of singular control games.","sentences":["We consider ergodic symmetric $N$-player and mean-field games of singular control in both cooperative and competitive settings.","The state process dynamics of a representative player follow geometric Brownian motion, controlled additively through a nondecreasing process.","Agents aim to maximize a long-time average reward functional with instantaneous profit of power type.","The game shows strategic complementarities, in that the marginal profit function is increasing with respect to the dynamic average of the states of the other players, when $N<\\infty$, or with respect to the stationary mean of the players' distribution, in the mean-field case.","In the mean-field formulation, we explicitly construct the solution to the mean-field control problem associated with central planner optimization, as well as Nash and coarse correlated equilibria (with singular and regular recommendations).","Among our findings, we show that coarse correlated equilibria may exist even when Nash equilibria do not.","Additionally, we show that a coarse correlated equilibrium with a regular (absolutely continuous) recommendation can outperform a Nash equilibrium where the equilibrium policy is of reflecting type (thus singularly continuous).","Furthermore, we prove that the constructed mean-field control and mean-field equilibria can approximate the cooperative and competitive equilibria, respectively, in the corresponding game with $N$ players when $N$ is sufficiently large.","To the best of our knowledge, this paper is the first to characterize coarse correlated equilibria, construct the explicit solution to an ergodic mean-field control problem, and provide approximation results for the related $N$-player game in the context of singular control games."],"url":"http://arxiv.org/abs/2404.15079v1","category":"math.OC"}
{"created":"2024-04-23 13:49:11","title":"Tensor networks based quantum optimization algorithm","abstract":"In optimization, one of the well-known classical algorithms is power iterations. Simply stated, the algorithm recovers the dominant eigenvector of some diagonalizable matrix. Since numerous optimization problems can be formulated as an eigenvalue/eigenvector search, this algorithm features wide applicability. Operationally, power iterations consist of performing repeated matrix-to-vector multiplications (or MatVec) followed by a renormilization step in order to converge to the dominant eigenvalue/eigenvector. However, classical realizations, including novel tensor network based approaches, necessitate an exponential scaling for the algorithm's run-time. In this paper, we propose a quantum realiziation to circumvent this pitfall. Our methodology involves casting low-rank representations; Matrix Product Operators (MPO) for matrices and Matrix Product States (MPS) for vectors, into quantum circuits. Specifically, we recover a unitary approximation by variationally minimizing the Frobenius distance between a target MPO and an MPO ansatz wherein the tensor cores are constrained to unitaries. Such an unitary MPO can easily be implemented as a quantum circuit with the addition of ancillary qubits. Thereafter, with appropriate initialization and post-selection on the ancillary space, we realize a single iteration of the classical algorithm. With our proposed methodology, power iterations can be realized entirely on a quantum computer via repeated, static circuit blocks; therefore, a run-time advantage can indeed be guaranteed. Moreover, by exploiting Riemannian optimization and cross-approximation techniques, our methodology becomes instance agnostic and thus allows one to address black-box optimization within the framework of quantum computing.","sentences":["In optimization, one of the well-known classical algorithms is power iterations.","Simply stated, the algorithm recovers the dominant eigenvector of some diagonalizable matrix.","Since numerous optimization problems can be formulated as an eigenvalue/eigenvector search, this algorithm features wide applicability.","Operationally, power iterations consist of performing repeated matrix-to-vector multiplications (or MatVec) followed by a renormilization step in order to converge to the dominant eigenvalue/eigenvector.","However, classical realizations, including novel tensor network based approaches, necessitate an exponential scaling for the algorithm's run-time.","In this paper, we propose a quantum realiziation to circumvent this pitfall.","Our methodology involves casting low-rank representations; Matrix Product Operators (MPO) for matrices and Matrix Product States (MPS) for vectors, into quantum circuits.","Specifically, we recover a unitary approximation by variationally minimizing the Frobenius distance between a target MPO and an MPO ansatz wherein the tensor cores are constrained to unitaries.","Such an unitary MPO can easily be implemented as a quantum circuit with the addition of ancillary qubits.","Thereafter, with appropriate initialization and post-selection on the ancillary space, we realize a single iteration of the classical algorithm.","With our proposed methodology, power iterations can be realized entirely on a quantum computer via repeated, static circuit blocks; therefore, a run-time advantage can indeed be guaranteed.","Moreover, by exploiting Riemannian optimization and cross-approximation techniques, our methodology becomes instance agnostic and thus allows one to address black-box optimization within the framework of quantum computing."],"url":"http://arxiv.org/abs/2404.15048v1","category":"quant-ph"}
{"created":"2024-04-23 12:38:51","title":"Zero-Knowledge Location Privacy via Accurate Floating Point SNARKs","abstract":"This paper introduces Zero-Knowledge Location Privacy (ZKLP), enabling users to prove to third parties that they are within a specified geographical region while not disclosing their exact location. ZKLP supports varying levels of granularity, allowing for customization depending on the use case. To realize ZKLP, we introduce the first set of Zero-Knowledge Proof (ZKP) circuits that are fully compliant to the IEEE 754 standard for floating-point arithmetic.   Our results demonstrate that our floating point implementation scales efficiently, requiring only $69$ constraints per multiplication for $2^{15}$ single-precision floating-point multiplications. We utilize our floating point implementation to realize the ZKLP paradigm. In comparison to the state-of-the-art, we find that our optimized implementation has $14.1 \\times$ less constraints utilizing single precision floating-point values, and $11.2 \\times$ less constraints when utilizing double precision floating-point values. We demonstrate the practicability of ZKLP by building a protocol for privacy preserving peer-to-peer proximity testing - Alice can test if she is close to Bob by receiving a single message, without either party revealing any other information about their location. In such a configuration, Bob can create a proof of (non-)proximity in $0.27 s$, whereas Alice can verify her distance to about $250$ peers per second","sentences":["This paper introduces Zero-Knowledge Location Privacy (ZKLP), enabling users to prove to third parties that they are within a specified geographical region while not disclosing their exact location.","ZKLP supports varying levels of granularity, allowing for customization depending on the use case.","To realize ZKLP, we introduce the first set of Zero-Knowledge Proof (ZKP) circuits that are fully compliant to the IEEE 754 standard for floating-point arithmetic.   ","Our results demonstrate that our floating point implementation scales efficiently, requiring only $69$ constraints per multiplication for $2^{15}$ single-precision floating-point multiplications.","We utilize our floating point implementation to realize the ZKLP paradigm.","In comparison to the state-of-the-art, we find that our optimized implementation has $14.1 \\times$ less constraints utilizing single precision floating-point values, and $11.2 \\times$ less constraints when utilizing double precision floating-point values.","We demonstrate the practicability of ZKLP by building a protocol for privacy preserving peer-to-peer proximity testing - Alice can test if she is close to Bob by receiving a single message, without either party revealing any other information about their location.","In such a configuration, Bob can create a proof of (non-)proximity in $0.27 s$, whereas Alice can verify her distance to about $250$ peers per second"],"url":"http://arxiv.org/abs/2404.14983v1","category":"cs.CR"}
{"created":"2024-04-23 11:15:05","title":"Exploring the Theoretical Limits of Efficiency in Multilayer Solar Cells","abstract":"Photovoltaic materials are recognized for their potential as sustainable energy sources that enable the conversion between light and electrical energy. However, solar cells have been unable to surpass the theoretical limit of 32%, known as the Shockley-Queisser limit, and face challenges in effectively utilizing the broad spectrum of sunlight. To address this issue, extensive research is being conducted on multi-junction solar cells, which employ a layered structure comprising materials with varying bandgaps to more effectively harness the wide spectrum of sunlight. This study calculates the theoretical limit of these multi-junction solar cells and identifies optimal bandgap combinations, exploring new possibilities for photovoltaic devices and suggesting directions for technological advancement. This research presents new opportunities beyond the limitations of current solar cell technology and is expected to contribute to enhancing the economic viability and practicality of solar power as a sustainable energy source.","sentences":["Photovoltaic materials are recognized for their potential as sustainable energy sources that enable the conversion between light and electrical energy.","However, solar cells have been unable to surpass the theoretical limit of 32%, known as the Shockley-Queisser limit, and face challenges in effectively utilizing the broad spectrum of sunlight.","To address this issue, extensive research is being conducted on multi-junction solar cells, which employ a layered structure comprising materials with varying bandgaps to more effectively harness the wide spectrum of sunlight.","This study calculates the theoretical limit of these multi-junction solar cells and identifies optimal bandgap combinations, exploring new possibilities for photovoltaic devices and suggesting directions for technological advancement.","This research presents new opportunities beyond the limitations of current solar cell technology and is expected to contribute to enhancing the economic viability and practicality of solar power as a sustainable energy source."],"url":"http://arxiv.org/abs/2404.14930v1","category":"physics.app-ph"}
{"created":"2024-04-23 11:11:20","title":"Towards self-optimization of publish/subscribe IoT systems using continuous performance monitoring","abstract":"Today, more and more embedded devices are being connected through a network, generally Internet, offering users different services. This concept refers to Internet of Things (IoT), bringing information and control capabilities in many fields like medicine, smart homes, home security, etc. Main drawbacks of IoT environment are its dependency on Internet connectivity and need continuous devices power. These dependencies may affect system performances, namely request processing response times. In this context, we propose in this paper a continuous performance monitoring methodology, applied on IoT systems based on Publish/subscribe communication model. Our approach assesses performances using Stochastic Petri net modeling, and self-optimizes whenever poor performances are detected. Our approach relies on a Stochastic Petri nets modelling and analysis to assess performances. We target improving performances, in particular response times, by online modification of influencing factors.","sentences":["Today, more and more embedded devices are being connected through a network, generally Internet, offering users different services.","This concept refers to Internet of Things (IoT), bringing information and control capabilities in many fields like medicine, smart homes, home security, etc.","Main drawbacks of IoT environment are its dependency on Internet connectivity and need continuous devices power.","These dependencies may affect system performances, namely request processing response times.","In this context, we propose in this paper a continuous performance monitoring methodology, applied on IoT systems based on Publish/subscribe communication model.","Our approach assesses performances using Stochastic Petri net modeling, and self-optimizes whenever poor performances are detected.","Our approach relies on a Stochastic Petri nets modelling and analysis to assess performances.","We target improving performances, in particular response times, by online modification of influencing factors."],"url":"http://arxiv.org/abs/2404.14926v1","category":"cs.PF"}
{"created":"2024-04-23 11:02:39","title":"Particle Swam Optimization Based Analysis to Unlocking the Neutrino Mass Puzzle using $A_{4}\\times Z_{3}\\times Z_{10}$ Flavor Symmetry","abstract":"New research has highlighted a shortfall in the Standard Model (SM) because it predicts neutrinos to have zero mass. However, recent experiments on neutrino oscillation have revealed that the majority of neutrino parameters indeed indicate their significant mass. In response, scientists are increasingly incorporating discrete symmetries alongside continuous ones for better justification of observed patterns of neutrino mixing. In this study, we have examined a model within $A_4\\times Z_3\\times Z_{10}$ symmetry to estimate the neutrino masses using particle swam optimization technique for both mass hierarchy of neutrino. This model employed a hybrid seesaw mechanism, a combination of seesaw mechanism of type-I and type-II, to establish the effective Majorana neutrino mass matrix. After calculating the mass eigenvalues and lepton mixing matrix upto second order perturbation theory in this framework, this study seeks to investigate the scalar potential for vacuum expectation values (VEVs), optimize the parameters, $U_{PMNS}$ matrix, neutrino masses: $|{m_{1}^{\\prime}}^N|=0.0292794-0.0435082\\ eV$, $|{m_{2}^{\\prime}}^N|=1.78893\\times 10^{-18}-0.0293509\\ eV$, $|{m_{3}^{\\prime}}^N|=0.0307414-0.0471467\\ eV$, $|{m_{1}^{\\prime}}^I|=0.00982013-0.0453623\\ eV$, $|{m_{2}^{\\prime}}^I_|=0.0379702-0.0471197\\ eV$, and $|{m_{3}^{\\prime}}^I|=0.0122063-0.027544\\ eV$, effective neutrino mass parameters: $\\langle {m_{ee}} \\rangle^N=(0.170-3.93)\\times10^{-2}\\ eV$, $\\langle {m_{\\beta}} \\rangle^N=(0.471-1.39)\\times10^{-2}\\ eV$, $\\langle {m_{ee}} \\rangle^I=(1.85-4.55)\\times10^{-2}\\ eV$ and $\\langle {m_{\\beta}} \\rangle^I=(2.26-4.56)\\times10^{-2}\\ eV$, are predicted for both mass hierarchy through particle swam optimization (PSO), showing strong agreement with recent experimental findings.","sentences":["New research has highlighted a shortfall in the Standard Model (SM) because it predicts neutrinos to have zero mass.","However, recent experiments on neutrino oscillation have revealed that the majority of neutrino parameters indeed indicate their significant mass.","In response, scientists are increasingly incorporating discrete symmetries alongside continuous ones for better justification of observed patterns of neutrino mixing.","In this study, we have examined a model within $A_4\\times Z_3\\times Z_{10}$ symmetry to estimate the neutrino masses using particle swam optimization technique for both mass hierarchy of neutrino.","This model employed a hybrid seesaw mechanism, a combination of seesaw mechanism of type-I and type-II, to establish the effective Majorana neutrino mass matrix.","After calculating the mass eigenvalues and lepton mixing matrix upto second order perturbation theory in this framework, this study seeks to investigate the scalar potential for vacuum expectation values (VEVs), optimize the parameters, $U_{PMNS}$ matrix, neutrino masses: $|{m_{1}^{\\prime}}^N|=0.0292794-0.0435082\\ eV$, $|{m_{2}^{\\prime}}^N|=1.78893\\times 10^{-18}-0.0293509\\ eV$, $|{m_{3}^{\\prime}}^N|=0.0307414-0.0471467\\ eV$, $|{m_{1}^{\\prime}}^I|=0.00982013-0.0453623\\ eV$, $|{m_{2}^{\\prime}}^I_|=0.0379702-0.0471197\\ eV$, and $|{m_{3}^{\\prime}}^I|=0.0122063-0.027544\\ eV$, effective neutrino mass parameters: $\\langle {m_{ee}} \\rangle^N=(0.170-3.93)\\times10^{-2}\\ eV$, $\\langle {m_{\\beta}} \\rangle^N=(0.471-1.39)\\times10^{-2}\\ eV$, $\\langle {m_{ee}} \\rangle^I=(1.85-4.55)\\times10^{-2}\\ eV$ and $\\langle {m_{\\beta}} \\rangle^I=(2.26-4.56)\\times10^{-2}\\ eV$, are predicted for both mass hierarchy through particle swam optimization (PSO), showing strong agreement with recent experimental findings."],"url":"http://arxiv.org/abs/2404.14917v1","category":"hep-ph"}
{"created":"2024-04-23 10:21:53","title":"A Spatiotemporal Hand-Eye Calibration for Trajectory Alignment in Visual(-Inertial) Odometry Evaluation","abstract":"A common prerequisite for evaluating a visual(-inertial) odometry (VO/VIO) algorithm is to align the timestamps and the reference frame of its estimated trajectory with a reference ground-truth derived from a system of superior precision, such as a motion capture system. The trajectory-based alignment, typically modeled as a classic hand-eye calibration, significantly influences the accuracy of evaluation metrics. However, traditional calibration methods are susceptible to the quality of the input poses. Few studies have taken this into account when evaluating VO/VIO trajectories that usually suffer from noise and drift. To fill this gap, we propose a novel spatiotemporal hand-eye calibration algorithm that fully leverages multiple constraints from screw theory for enhanced accuracy and robustness. Experimental results show that our algorithm has better performance and is less noise-prone than state-of-the-art methods.","sentences":["A common prerequisite for evaluating a visual(-inertial) odometry (VO/VIO) algorithm is to align the timestamps and the reference frame of its estimated trajectory with a reference ground-truth derived from a system of superior precision, such as a motion capture system.","The trajectory-based alignment, typically modeled as a classic hand-eye calibration, significantly influences the accuracy of evaluation metrics.","However, traditional calibration methods are susceptible to the quality of the input poses.","Few studies have taken this into account when evaluating VO/VIO trajectories that usually suffer from noise and drift.","To fill this gap, we propose a novel spatiotemporal hand-eye calibration algorithm that fully leverages multiple constraints from screw theory for enhanced accuracy and robustness.","Experimental results show that our algorithm has better performance and is less noise-prone than state-of-the-art methods."],"url":"http://arxiv.org/abs/2404.14894v1","category":"cs.RO"}
{"created":"2024-04-23 10:17:42","title":"DENOISER: Rethinking the Robustness for Open-Vocabulary Action Recognition","abstract":"As one of the fundamental video tasks in computer vision, Open-Vocabulary Action Recognition (OVAR) recently gains increasing attention, with the development of vision-language pre-trainings. To enable generalization of arbitrary classes, existing methods treat class labels as text descriptions, then formulate OVAR as evaluating embedding similarity between visual samples and textual classes. However, one crucial issue is completely ignored: the class descriptions given by users may be noisy, e.g., misspellings and typos, limiting the real-world practicality of vanilla OVAR. To fill the research gap, this paper pioneers to evaluate existing methods by simulating multi-level noises of various types, and reveals their poor robustness. To tackle the noisy OVAR task, we further propose one novel DENOISER framework, covering two parts: generation and discrimination. Concretely, the generative part denoises noisy class-text names via one decoding process, i.e., propose text candidates, then utilize inter-modal and intra-modal information to vote for the best. At the discriminative part, we use vanilla OVAR models to assign visual samples to class-text names, thus obtaining more semantics. For optimization, we alternately iterate between generative and discriminative parts for progressive refinements. The denoised text classes help OVAR models classify visual samples more accurately; in return, classified visual samples help better denoising. On three datasets, we carry out extensive experiments to show our superior robustness, and thorough ablations to dissect the effectiveness of each component.","sentences":["As one of the fundamental video tasks in computer vision, Open-Vocabulary Action Recognition (OVAR) recently gains increasing attention, with the development of vision-language pre-trainings.","To enable generalization of arbitrary classes, existing methods treat class labels as text descriptions, then formulate OVAR as evaluating embedding similarity between visual samples and textual classes.","However, one crucial issue is completely ignored: the class descriptions given by users may be noisy, e.g., misspellings and typos, limiting the real-world practicality of vanilla OVAR.","To fill the research gap, this paper pioneers to evaluate existing methods by simulating multi-level noises of various types, and reveals their poor robustness.","To tackle the noisy OVAR task, we further propose one novel DENOISER framework, covering two parts: generation and discrimination.","Concretely, the generative part denoises noisy class-text names via one decoding process, i.e., propose text candidates, then utilize inter-modal and intra-modal information to vote for the best.","At the discriminative part, we use vanilla OVAR models to assign visual samples to class-text names, thus obtaining more semantics.","For optimization, we alternately iterate between generative and discriminative parts for progressive refinements.","The denoised text classes help OVAR models classify visual samples more accurately; in return, classified visual samples help better denoising.","On three datasets, we carry out extensive experiments to show our superior robustness, and thorough ablations to dissect the effectiveness of each component."],"url":"http://arxiv.org/abs/2404.14890v1","category":"cs.CV"}
{"created":"2024-04-23 10:15:18","title":"Automated Discovery of Coupled Mode Setups","abstract":"In optics and photonics, a small number of building blocks, like resonators, waveguides, arbitrary couplings, and parametric interactions, allow the design of a broad variety of devices and functionalities, distinguished by their scattering properties. These include transducers, amplifiers, and nonreciprocal devices, like isolators or circulators. Usually, the design of such a system is handcrafted by an experienced scientist in a time-consuming process where it remains uncertain whether the simplest possibility has indeed been found. In our work, we develop a discovery algorithm that automates this challenge. By optimizing the continuous and discrete system properties our automated search identifies the minimal resources required to realize the requested scattering behavior. In the spirit of artificial scientific discovery, it produces a complete list of interpretable solutions and leads to generalizable insights, as we illustrate in several examples. This now opens the door to rapid design in areas like photonic and microwave architectures or optomechanics.","sentences":["In optics and photonics, a small number of building blocks, like resonators, waveguides, arbitrary couplings, and parametric interactions, allow the design of a broad variety of devices and functionalities, distinguished by their scattering properties.","These include transducers, amplifiers, and nonreciprocal devices, like isolators or circulators.","Usually, the design of such a system is handcrafted by an experienced scientist in a time-consuming process where it remains uncertain whether the simplest possibility has indeed been found.","In our work, we develop a discovery algorithm that automates this challenge.","By optimizing the continuous and discrete system properties our automated search identifies the minimal resources required to realize the requested scattering behavior.","In the spirit of artificial scientific discovery, it produces a complete list of interpretable solutions and leads to generalizable insights, as we illustrate in several examples.","This now opens the door to rapid design in areas like photonic and microwave architectures or optomechanics."],"url":"http://arxiv.org/abs/2404.14887v1","category":"physics.optics"}
{"created":"2024-04-23 09:23:22","title":"A resource-efficient variational quantum algorithm for mRNA codon optimization","abstract":"Optimizing the mRNA codon has an essential impact on gene expression for a specific target protein. It is an NP-hard problem; thus, exact solutions to such optimization problems become computationally intractable for realistic problem sizes on both classical and quantum computers. However, approximate solutions via heuristics can substantially impact the application they enable. Quantum approximate optimization is an alternative computation paradigm promising for tackling such problems. Recently, there has been some research in quantum algorithms for bioinformatics, specifically for mRNA codon optimization. This research presents a denser way to encode codons for implementing mRNA codon optimization via the variational quantum eigensolver algorithms on a gate-based quantum computer. This reduces the qubit requirement by half compared to the existing quantum approach, thus allowing longer sequences to be executed on existing quantum processors. The performance of the proposed algorithm is evaluated by comparing its results to exact solutions, showing well-matching results.","sentences":["Optimizing the mRNA codon has an essential impact on gene expression for a specific target protein.","It is an NP-hard problem; thus, exact solutions to such optimization problems become computationally intractable for realistic problem sizes on both classical and quantum computers.","However, approximate solutions via heuristics can substantially impact the application they enable.","Quantum approximate optimization is an alternative computation paradigm promising for tackling such problems.","Recently, there has been some research in quantum algorithms for bioinformatics, specifically for mRNA codon optimization.","This research presents a denser way to encode codons for implementing mRNA codon optimization via the variational quantum eigensolver algorithms on a gate-based quantum computer.","This reduces the qubit requirement by half compared to the existing quantum approach, thus allowing longer sequences to be executed on existing quantum processors.","The performance of the proposed algorithm is evaluated by comparing its results to exact solutions, showing well-matching results."],"url":"http://arxiv.org/abs/2404.14858v1","category":"quant-ph"}
{"created":"2024-04-23 09:16:55","title":"Quenching of stable pulses in slow-fast excitable media","abstract":"We develop linear theory for the prediction of excitation wave quenching--the construction of minimal perturbations which return stable excitations to quiescence--for localized pulse solutions of models of excitable media. The theory requires accounting for an additional degree of freedom in the formulation of the linear theory, and a reconsideration of heuristics for choosing optimal reference states from their group representation. We compare the predictions made with the linear theory to direct numerical simulations across a family of perturbations and assess the accuracy of predictions for models with distinct stable excitation structures. We find that the theory achieves qualitative predictive power with only the effort of distinguishing a root from the asymptotic case, and achieves quantitative predictive power in many circumstances. Finally, we compare the computational cost of our prediction technique to other numerical methods for the determination of transitions in extended excitable systems.","sentences":["We develop linear theory for the prediction of excitation wave quenching--the construction of minimal perturbations which return stable excitations to quiescence--for localized pulse solutions of models of excitable media.","The theory requires accounting for an additional degree of freedom in the formulation of the linear theory, and a reconsideration of heuristics for choosing optimal reference states from their group representation.","We compare the predictions made with the linear theory to direct numerical simulations across a family of perturbations and assess the accuracy of predictions for models with distinct stable excitation structures.","We find that the theory achieves qualitative predictive power with only the effort of distinguishing a root from the asymptotic case, and achieves quantitative predictive power in many circumstances.","Finally, we compare the computational cost of our prediction technique to other numerical methods for the determination of transitions in extended excitable systems."],"url":"http://arxiv.org/abs/2404.14854v2","category":"nlin.PS"}
{"created":"2024-04-23 08:52:31","title":"Closed-Loop Identification and Tracking Control of a Ballbot","abstract":"Identifying and controlling an unstable, underactuated robot to enable reference tracking is a challenging control problem. In this paper, a ballbot (robot balancing on a ball) is used as an experimental setup to demonstrate and test proposed strategies to tackle this control problem. A double-loop control system, including a state-feedback gain in the outer-loop and a Proportional-Integral-Derivative (PID) controller in the inner-loop, is presented to balance the system in its unstable equilibrium. Once stability is reached, the plant's response to a designed excitation signal is measured and interpreted to identify the system's dynamics. Hereby, the parameters of a linearized model of the ballbot are identified with prior knowledge about the structure of the nonlinear dynamics of the system. Based on an identified linear time-invariant (LTI) state-space model, a double-loop control strategy is considered to balance the real system and to allow reference tracking. A linear quadratic regulator (LQR) is designed offline and implemented in the inner-loop to ensure balance. In the outer-loop, the estimated dynamics forecast the system's behavior online using a model-predictive-control (MPC) design to find the optimal control input for reference tracking. The experimental results demonstrate the applicability of the proposed strategies.","sentences":["Identifying and controlling an unstable, underactuated robot to enable reference tracking is a challenging control problem.","In this paper, a ballbot (robot balancing on a ball) is used as an experimental setup to demonstrate and test proposed strategies to tackle this control problem.","A double-loop control system, including a state-feedback gain in the outer-loop and a Proportional-Integral-Derivative (PID) controller in the inner-loop, is presented to balance the system in its unstable equilibrium.","Once stability is reached, the plant's response to a designed excitation signal is measured and interpreted to identify the system's dynamics.","Hereby, the parameters of a linearized model of the ballbot are identified with prior knowledge about the structure of the nonlinear dynamics of the system.","Based on an identified linear time-invariant (LTI) state-space model, a double-loop control strategy is considered to balance the real system and to allow reference tracking.","A linear quadratic regulator (LQR) is designed offline and implemented in the inner-loop to ensure balance.","In the outer-loop, the estimated dynamics forecast the system's behavior online using a model-predictive-control (MPC) design to find the optimal control input for reference tracking.","The experimental results demonstrate the applicability of the proposed strategies."],"url":"http://arxiv.org/abs/2404.14845v1","category":"math.OC"}
{"created":"2024-04-23 08:44:07","title":"Demonstration of energy extraction gain from non-classical correlations","abstract":"Within the framework of microscopic thermodynamics, correlations can play a crucial role for energy extraction. Our work sheds light on this connection by demonstrating that entanglement governs the amount of extractable energy in a controllable setting. We experimentally investigate a fundamental link between information, encoded in tunable non-classical correlations and quantified by quantum state tomography, and its utility as fuel for energy extraction. We realize an agent-demon protocol involving two trapped-ion qubits, and show that by implementing an appropriate feedback policy, the demon can optimize the energy extraction process, capitalizing on the correlations between the system's constituents. By quantifying both the concurrence of the two-qubit resource state and the energy extraction gain from applying the feedback policy, we corroborate the connection between information and energy, solidifying the role of non-classical correlations as a resource for thermodynamic processes.","sentences":["Within the framework of microscopic thermodynamics, correlations can play a crucial role for energy extraction.","Our work sheds light on this connection by demonstrating that entanglement governs the amount of extractable energy in a controllable setting.","We experimentally investigate a fundamental link between information, encoded in tunable non-classical correlations and quantified by quantum state tomography, and its utility as fuel for energy extraction.","We realize an agent-demon protocol involving two trapped-ion qubits, and show that by implementing an appropriate feedback policy, the demon can optimize the energy extraction process, capitalizing on the correlations between the system's constituents.","By quantifying both the concurrence of the two-qubit resource state and the energy extraction gain from applying the feedback policy, we corroborate the connection between information and energy, solidifying the role of non-classical correlations as a resource for thermodynamic processes."],"url":"http://arxiv.org/abs/2404.14838v1","category":"quant-ph"}
{"created":"2024-04-23 08:30:40","title":"GLDPC-PC Codes: Channel Coding Towards 6G Communications","abstract":"The sixth generation (6G) wireless communication system will improve the key technical indicators by one to two orders of magnitude, and come with some new features. As a crucial technique to enhance the reliability and efficiency of data transmission, the next generation channel coding is not only required to satisfy the stringent requirements of 6G, but also expected to be backward compatible to avoid imposing additional burden on the crowded baseband chip. This article provides an overview of the potential channel codes for 6G communications. In addition, we explore to develop next-generation channel codes based on low-density parity-check (LDPC) and polar frameworks, introducing a novel concept called generalized LDPC with polar-like component (GLDPC-PC) codes. The codes have exhibited promising error correction performance and manageable complexity, which can be further optimized by specific code design. The opportunities and challenges of GLDPC-PC codes are also discussed, considering the practical applications to 6G communication systems.","sentences":["The sixth generation (6G) wireless communication system will improve the key technical indicators by one to two orders of magnitude, and come with some new features.","As a crucial technique to enhance the reliability and efficiency of data transmission, the next generation channel coding is not only required to satisfy the stringent requirements of 6G, but also expected to be backward compatible to avoid imposing additional burden on the crowded baseband chip.","This article provides an overview of the potential channel codes for 6G communications.","In addition, we explore to develop next-generation channel codes based on low-density parity-check (LDPC) and polar frameworks, introducing a novel concept called generalized LDPC with polar-like component (GLDPC-PC) codes.","The codes have exhibited promising error correction performance and manageable complexity, which can be further optimized by specific code design.","The opportunities and challenges of GLDPC-PC codes are also discussed, considering the practical applications to 6G communication systems."],"url":"http://arxiv.org/abs/2404.14828v1","category":"cs.IT"}
{"created":"2024-04-23 08:13:22","title":"Variants of the slacks-based measure with assurance region and zeros in input-output data","abstract":"Incorporating an assurance region (AR) into the slacks-based measure (SBM) improves practicality; however, its efficiency measure may not have desirable properties, such as monotonicity. We incorporate a closer target setting approach into the SBM with AR and a variant of the SBM with AR. We demonstrate that the efficiency measure with the hybrid approach has the same desirable properties as that without AR, and we also show that the efficiency scores can be computed by solving linear programming problems. Our proposed approach can handle zeros in the observed input-output data without any data transformation or additional model modification.","sentences":["Incorporating an assurance region (AR) into the slacks-based measure (SBM) improves practicality; however, its efficiency measure may not have desirable properties, such as monotonicity.","We incorporate a closer target setting approach into the SBM with AR and a variant of the SBM with AR.","We demonstrate that the efficiency measure with the hybrid approach has the same desirable properties as that without AR, and we also show that the efficiency scores can be computed by solving linear programming problems.","Our proposed approach can handle zeros in the observed input-output data without any data transformation or additional model modification."],"url":"http://arxiv.org/abs/2404.14820v1","category":"math.OC"}
{"created":"2024-04-23 07:34:04","title":"PRoTECT: Parallelized Construction of Safety Barrier Certificates for Nonlinear Polynomial Systems","abstract":"We develop an open-source software tool, called PRoTECT, for the parallelized construction of safety barrier certificates (BCs) for nonlinear polynomial systems. This tool employs sum-of-squares (SOS) optimization programs to systematically search for polynomial-type BCs, while aiming to verify safety properties over four classes of dynamical systems: (i) discrete-time stochastic systems, (ii) discrete-time deterministic systems, (iii) continuous-time stochastic systems, and (iv) continuous-time deterministic systems. PRoTECT is implemented in Python as an application programming interface (API), offering users the flexibility to interact either through its user-friendly graphic user interface (GUI) or via function calls from other Python programs. PRoTECT leverages parallelism across different barrier degrees to efficiently search for a feasible BC.","sentences":["We develop an open-source software tool, called PRoTECT, for the parallelized construction of safety barrier certificates (BCs) for nonlinear polynomial systems.","This tool employs sum-of-squares (SOS) optimization programs to systematically search for polynomial-type BCs, while aiming to verify safety properties over four classes of dynamical systems: (i) discrete-time stochastic systems, (ii) discrete-time deterministic systems, (iii) continuous-time stochastic systems, and (iv) continuous-time deterministic systems.","PRoTECT is implemented in Python as an application programming interface (API), offering users the flexibility to interact either through its user-friendly graphic user interface (GUI) or via function calls from other Python programs.","PRoTECT leverages parallelism across different barrier degrees to efficiently search for a feasible BC."],"url":"http://arxiv.org/abs/2404.14804v1","category":"eess.SY"}
{"created":"2024-04-23 05:26:48","title":"Eigenvector distributions and optimal shrinkage estimators for large covariance and precision matrices","abstract":"This paper focuses on investigating Stein's invariant shrinkage estimators for large sample covariance matrices and precision matrices in high-dimensional settings. We consider models that have nearly arbitrary population covariance matrices, including those with potential spikes. By imposing mild technical assumptions, we establish the asymptotic limits of the shrinkers for a wide range of loss functions. A key contribution of this work, enabling the derivation of the limits of the shrinkers, is a novel result concerning the asymptotic distributions of the non-spiked eigenvectors of the sample covariance matrices, which can be of independent interest.","sentences":["This paper focuses on investigating Stein's invariant shrinkage estimators for large sample covariance matrices and precision matrices in high-dimensional settings.","We consider models that have nearly arbitrary population covariance matrices, including those with potential spikes.","By imposing mild technical assumptions, we establish the asymptotic limits of the shrinkers for a wide range of loss functions.","A key contribution of this work, enabling the derivation of the limits of the shrinkers, is a novel result concerning the asymptotic distributions of the non-spiked eigenvectors of the sample covariance matrices, which can be of independent interest."],"url":"http://arxiv.org/abs/2404.14751v1","category":"math.ST"}
{"created":"2024-04-23 04:06:20","title":"Emergent Cooperation for Energy-efficient Connectivity via Wireless Power Transfer","abstract":"This paper addresses the challenge of incentivizing energy-constrained, non-cooperative user equipment (UE) to serve as cooperative relays. We consider a source UE with a non-line-of-sight channel to an access point (AP), where direct communication may be infeasible or may necessitate a substantial transmit power. Other UEs in the vicinity are viewed as relay candidates, and our aim is to enable energy-efficient connectivity for the source, while accounting for the self-interested behavior and private channel state information of these candidates, by allowing the source to \"pay\" the candidates via wireless power transfer (WPT). We propose a cooperation-inducing protocol, inspired by Myerson auction theory, which ensures that candidates truthfully report power requirements while minimizing the expected power used by the source. Through rigorous analysis, we establish the regularity of valuations for lognormal fading channels, which allows for the efficient determination of the optimal source transmit power. Extensive simulation experiments, employing real-world communication and WPT parameters, validate our theoretical framework. Our results demonstrate a 91% reduction in outage probability with as few as 4 relay candidates, compared to the non-cooperative scenario, and as much as 48% source power savings compared to a baseline approach, highlighting the efficacy of our proposed methodology.","sentences":["This paper addresses the challenge of incentivizing energy-constrained, non-cooperative user equipment (UE) to serve as cooperative relays.","We consider a source UE with a non-line-of-sight channel to an access point (AP), where direct communication may be infeasible or may necessitate a substantial transmit power.","Other UEs in the vicinity are viewed as relay candidates, and our aim is to enable energy-efficient connectivity for the source, while accounting for the self-interested behavior and private channel state information of these candidates, by allowing the source to \"pay\" the candidates via wireless power transfer (WPT).","We propose a cooperation-inducing protocol, inspired by Myerson auction theory, which ensures that candidates truthfully report power requirements while minimizing the expected power used by the source.","Through rigorous analysis, we establish the regularity of valuations for lognormal fading channels, which allows for the efficient determination of the optimal source transmit power.","Extensive simulation experiments, employing real-world communication and WPT parameters, validate our theoretical framework.","Our results demonstrate a 91% reduction in outage probability with as few as 4 relay candidates, compared to the non-cooperative scenario, and as much as 48% source power savings compared to a baseline approach, highlighting the efficacy of our proposed methodology."],"url":"http://arxiv.org/abs/2404.14729v1","category":"eess.SY"}
{"created":"2024-04-23 03:58:57","title":"Revisiting Crossflow-Based Stabilization in Channel Flows","abstract":"Stabilization schemes in wall-bounded flows often invoke fluid transpiration through porous boundaries. While these have been extensively validated for external flows, their efficacy in channels, particularly from the standpoint of non-modal perturbations, is yet to be demonstrated. Here, we show that crossflow strengths previously considered ``ideal'' for optimizing stability in channels in fact admit strong non-modal energy amplification. We begin by supplementing existing modal calculations and then show via the resolvent that extremely strong and potentially unfeasible crossflows are required to suppress non-modal growth in linearly stable regimes. Investigation of unforced algebraic growth paints a similar picture. Here, a component-wise budget analysis reveals that energy redistribution through pressure-velocity correlations plays an important role in driving energy growth/decay. The superposition of a moving wall is also considered, and it is shown that while energy amplification generally worsens, it can potentially be suppressed beyond a regime shift in parameter space. However, these flows are marred by rapidly declining mass transport, rendering their ultimate utility questionable. Our results suggest that crossflow-based stabilization might not be useful in internal flows.","sentences":["Stabilization schemes in wall-bounded flows often invoke fluid transpiration through porous boundaries.","While these have been extensively validated for external flows, their efficacy in channels, particularly from the standpoint of non-modal perturbations, is yet to be demonstrated.","Here, we show that crossflow strengths previously considered ``ideal'' for optimizing stability in channels in fact admit strong non-modal energy amplification.","We begin by supplementing existing modal calculations and then show via the resolvent that extremely strong and potentially unfeasible crossflows are required to suppress non-modal growth in linearly stable regimes.","Investigation of unforced algebraic growth paints a similar picture.","Here, a component-wise budget analysis reveals that energy redistribution through pressure-velocity correlations plays an important role in driving energy growth/decay.","The superposition of a moving wall is also considered, and it is shown that while energy amplification generally worsens, it can potentially be suppressed beyond a regime shift in parameter space.","However, these flows are marred by rapidly declining mass transport, rendering their ultimate utility questionable.","Our results suggest that crossflow-based stabilization might not be useful in internal flows."],"url":"http://arxiv.org/abs/2404.14725v1","category":"physics.flu-dyn"}
{"created":"2024-04-23 03:55:01","title":"Insights into Alignment: Evaluating DPO and its Variants Across Multiple Tasks","abstract":"Large Language Models (LLMs) have demonstrated remarkable performance across a spectrum of tasks. Recently, Direct Preference Optimization (DPO) has emerged as an RL-free approach to optimize the policy model on human preferences. However, several limitations hinder the widespread adoption of this method. To address these shortcomings, various versions of DPO have been introduced. Yet, a comprehensive evaluation of these variants across diverse tasks is still lacking. In this study, we aim to bridge this gap by investigating the performance of alignment methods across three distinct scenarios: (1) keeping the Supervised Fine-Tuning (SFT) part, (2) skipping the SFT part, and (3) skipping the SFT part and utilizing an instruction-tuned model. Furthermore, we explore the impact of different training sizes on their performance. Our evaluation spans a range of tasks including dialogue systems, reasoning, mathematical problem-solving, question answering, truthfulness, and multi-task understanding, encompassing 13 benchmarks such as MT-Bench, Big Bench, and Open LLM Leaderboard. Key observations reveal that alignment methods achieve optimal performance with smaller training data subsets, exhibit limited effectiveness in reasoning tasks yet significantly impact mathematical problem-solving, and employing an instruction-tuned model notably influences truthfulness. We anticipate that our findings will catalyze further research aimed at developing more robust models to address alignment challenges.","sentences":["Large Language Models (LLMs) have demonstrated remarkable performance across a spectrum of tasks.","Recently, Direct Preference Optimization (DPO) has emerged as an RL-free approach to optimize the policy model on human preferences.","However, several limitations hinder the widespread adoption of this method.","To address these shortcomings, various versions of DPO have been introduced.","Yet, a comprehensive evaluation of these variants across diverse tasks is still lacking.","In this study, we aim to bridge this gap by investigating the performance of alignment methods across three distinct scenarios: (1) keeping the Supervised Fine-Tuning (SFT) part, (2) skipping the SFT part, and (3) skipping the SFT part and utilizing an instruction-tuned model.","Furthermore, we explore the impact of different training sizes on their performance.","Our evaluation spans a range of tasks including dialogue systems, reasoning, mathematical problem-solving, question answering, truthfulness, and multi-task understanding, encompassing 13 benchmarks such as MT-Bench, Big Bench, and Open LLM Leaderboard.","Key observations reveal that alignment methods achieve optimal performance with smaller training data subsets, exhibit limited effectiveness in reasoning tasks yet significantly impact mathematical problem-solving, and employing an instruction-tuned model notably influences truthfulness.","We anticipate that our findings will catalyze further research aimed at developing more robust models to address alignment challenges."],"url":"http://arxiv.org/abs/2404.14723v1","category":"cs.CL"}
{"created":"2024-04-23 02:50:38","title":"Double Privacy Guard: Robust Traceable Adversarial Watermarking against Face Recognition","abstract":"The wide deployment of Face Recognition (FR) systems poses risks of privacy leakage. One countermeasure to address this issue is adversarial attacks, which deceive malicious FR searches but simultaneously interfere the normal identity verification of trusted authorizers. In this paper, we propose the first Double Privacy Guard (DPG) scheme based on traceable adversarial watermarking. DPG employs a one-time watermark embedding to deceive unauthorized FR models and allows authorizers to perform identity verification by extracting the watermark. Specifically, we propose an information-guided adversarial attack against FR models. The encoder embeds an identity-specific watermark into the deep feature space of the carrier, guiding recognizable features of the image to deviate from the source identity. We further adopt a collaborative meta-optimization strategy compatible with sub-tasks, which regularizes the joint optimization direction of the encoder and decoder. This strategy enhances the representation of universal carrier features, mitigating multi-objective optimization conflicts in watermarking. Experiments confirm that DPG achieves significant attack success rates and traceability accuracy on state-of-the-art FR models, exhibiting remarkable robustness that outperforms the existing privacy protection methods using adversarial attacks and deep watermarking, or simple combinations of the two. Our work potentially opens up new insights into proactive protection for FR privacy.","sentences":["The wide deployment of Face Recognition (FR) systems poses risks of privacy leakage.","One countermeasure to address this issue is adversarial attacks, which deceive malicious FR searches but simultaneously interfere the normal identity verification of trusted authorizers.","In this paper, we propose the first Double Privacy Guard (DPG) scheme based on traceable adversarial watermarking.","DPG employs a one-time watermark embedding to deceive unauthorized FR models and allows authorizers to perform identity verification by extracting the watermark.","Specifically, we propose an information-guided adversarial attack against FR models.","The encoder embeds an identity-specific watermark into the deep feature space of the carrier, guiding recognizable features of the image to deviate from the source identity.","We further adopt a collaborative meta-optimization strategy compatible with sub-tasks, which regularizes the joint optimization direction of the encoder and decoder.","This strategy enhances the representation of universal carrier features, mitigating multi-objective optimization conflicts in watermarking.","Experiments confirm that DPG achieves significant attack success rates and traceability accuracy on state-of-the-art FR models, exhibiting remarkable robustness that outperforms the existing privacy protection methods using adversarial attacks and deep watermarking, or simple combinations of the two.","Our work potentially opens up new insights into proactive protection for FR privacy."],"url":"http://arxiv.org/abs/2404.14693v1","category":"cs.CR"}
{"created":"2024-04-23 02:14:39","title":"A Multi-Dimensional Online Contention Resolution Scheme for Revenue Maximization","abstract":"We study multi-buyer multi-item sequential item pricing mechanisms for revenue maximization with the goal of approximating a natural fractional relaxation -- the ex ante optimal revenue. We assume that buyers' values are subadditive but make no assumptions on the value distributions. While the optimal revenue, and therefore also the ex ante benchmark, is inapproximable by any simple mechanism in this context, previous work has shown that a weaker benchmark that optimizes over so-called ``buy-many\" mechanisms can be approximable. Approximations are known, in particular, for settings with either a single buyer or many unit-demand buyers. We extend these results to the much broader setting of many subadditive buyers. We show that the ex ante buy-many revenue can be approximated via sequential item pricings to within an $O(\\log^2 m)$ factor, where $m$ is the number of items. We also show that a logarithmic dependence on $m$ is necessary.   Our approximation is achieved through the construction of a new multi-dimensional Online Contention Resolution Scheme (OCRS), that provides an online rounding of the optimal ex ante solution. Chawla et al. arXiv:2204.01962 previously constructed an OCRS for revenue for unit-demand buyers, but their construction relied heavily on the ``almost single dimensional\" nature of unit-demand values. Prior to that work, OCRSes have only been studied in the context of social welfare maximization for single-parameter buyers. For the welfare objective, constant-factor approximations have been demonstrated for a wide range of combinatorial constraints on item allocations and classes of buyer valuation functions. Our work opens up the possibility of a similar success story for revenue maximization.","sentences":["We study multi-buyer multi-item sequential item pricing mechanisms for revenue maximization with the goal of approximating a natural fractional relaxation -- the ex ante optimal revenue.","We assume that buyers' values are subadditive but make no assumptions on the value distributions.","While the optimal revenue, and therefore also the ex ante benchmark, is inapproximable by any simple mechanism in this context, previous work has shown that a weaker benchmark that optimizes over so-called ``buy-many\" mechanisms can be approximable.","Approximations are known, in particular, for settings with either a single buyer or many unit-demand buyers.","We extend these results to the much broader setting of many subadditive buyers.","We show that the ex ante buy-many revenue can be approximated via sequential item pricings to within an $O(\\log^2 m)$ factor, where $m$ is the number of items.","We also show that a logarithmic dependence on $m$ is necessary.   ","Our approximation is achieved through the construction of a new multi-dimensional Online Contention Resolution Scheme (OCRS), that provides an online rounding of the optimal ex ante solution.","Chawla et al. arXiv:2204.01962 previously constructed an OCRS for revenue for unit-demand buyers, but their construction relied heavily on the ``almost single dimensional\" nature of unit-demand values.","Prior to that work, OCRSes have only been studied in the context of social welfare maximization for single-parameter buyers.","For the welfare objective, constant-factor approximations have been demonstrated for a wide range of combinatorial constraints on item allocations and classes of buyer valuation functions.","Our work opens up the possibility of a similar success story for revenue maximization."],"url":"http://arxiv.org/abs/2404.14679v1","category":"cs.GT"}
{"created":"2024-04-23 01:31:41","title":"Geometric Optimization of Restricted-Open and Complete Active Space Self-Consistent Field Wavefunctions","abstract":"We explore Riemannian optimization methods for Restricted-Open-shell Hartree-Fock (ROHF) and Complete Active Space Self-Consistent Field (CASSCF) methods. After showing that ROHF and CASSCF can be reformulated as optimization problems on so-called flag manifolds, we review Riemannian optimization basics and their application to these specific problems. We compare these methods to traditional ones and find robust convergence properties without fine-tuning of numerical parameters. Our study suggests Riemannian optimization as a valuable addition to orbital optimization for ROHF and CASSCF, warranting further investigation.","sentences":["We explore Riemannian optimization methods for Restricted-Open-shell Hartree-Fock (ROHF) and Complete Active Space Self-Consistent Field (CASSCF) methods.","After showing that ROHF and CASSCF can be reformulated as optimization problems on so-called flag manifolds, we review Riemannian optimization basics and their application to these specific problems.","We compare these methods to traditional ones and find robust convergence properties without fine-tuning of numerical parameters.","Our study suggests Riemannian optimization as a valuable addition to orbital optimization for ROHF and CASSCF, warranting further investigation."],"url":"http://arxiv.org/abs/2404.14655v1","category":"math.OC"}
{"created":"2024-04-23 01:13:33","title":"Bi-CL: A Reinforcement Learning Framework for Robots Coordination Through Bi-level Optimization","abstract":"In multi-robot systems, achieving coordinated missions remains a significant challenge due to the coupled nature of coordination behaviors and the lack of global information for individual robots. To mitigate these challenges, this paper introduces a novel approach, Bi-level Coordination Learning (Bi-CL), that leverages a bi-level optimization structure within a centralized training and decentralized execution paradigm. Our bi-level reformulation decomposes the original problem into a reinforcement learning level with reduced action space, and an imitation learning level that gains demonstrations from a global optimizer. Both levels contribute to improved learning efficiency and scalability. We note that robots' incomplete information leads to mismatches between the two levels of learning models. To address this, Bi-CL further integrates an alignment penalty mechanism, aiming to minimize the discrepancy between the two levels without degrading their training efficiency. We introduce a running example to conceptualize the problem formulation and apply Bi-CL to two variations of this example: route-based and graph-based scenarios. Simulation results demonstrate that Bi-CL can learn more efficiently and achieve comparable performance with traditional multi-agent reinforcement learning baselines for multi-robot coordination.","sentences":["In multi-robot systems, achieving coordinated missions remains a significant challenge due to the coupled nature of coordination behaviors and the lack of global information for individual robots.","To mitigate these challenges, this paper introduces a novel approach, Bi-level Coordination Learning (Bi-CL), that leverages a bi-level optimization structure within a centralized training and decentralized execution paradigm.","Our bi-level reformulation decomposes the original problem into a reinforcement learning level with reduced action space, and an imitation learning level that gains demonstrations from a global optimizer.","Both levels contribute to improved learning efficiency and scalability.","We note that robots' incomplete information leads to mismatches between the two levels of learning models.","To address this, Bi-CL further integrates an alignment penalty mechanism, aiming to minimize the discrepancy between the two levels without degrading their training efficiency.","We introduce a running example to conceptualize the problem formulation and apply Bi-CL to two variations of this example: route-based and graph-based scenarios.","Simulation results demonstrate that Bi-CL can learn more efficiently and achieve comparable performance with traditional multi-agent reinforcement learning baselines for multi-robot coordination."],"url":"http://arxiv.org/abs/2404.14649v1","category":"cs.RO"}
{"created":"2024-04-23 00:50:32","title":"Human Behavior Modeling via Identification of Task Objective and Variability","abstract":"Human behavior modeling is important for the design and implementation of human-automation interactive control systems. In this context, human behavior refers to a human's control input to systems. We propose a novel method for human behavior modeling that uses human demonstrations for a given task to infer the unknown task objective and the variability. The task objective represents the human's intent or desire. It can be inferred by the inverse optimal control and improve the understanding of human behavior by providing an explainable objective function behind the given human behavior. Meanwhile, the variability denotes the intrinsic uncertainty in human behavior. It can be described by a Gaussian mixture model and capture the uncertainty in human behavior which cannot be encoded by the task objective. The proposed method can improve the prediction accuracy of human behavior by leveraging both task objective and variability. The proposed method is demonstrated through human-subject experiments using an illustrative quadrotor remote control example.","sentences":["Human behavior modeling is important for the design and implementation of human-automation interactive control systems.","In this context, human behavior refers to a human's control input to systems.","We propose a novel method for human behavior modeling that uses human demonstrations for a given task to infer the unknown task objective and the variability.","The task objective represents the human's intent or desire.","It can be inferred by the inverse optimal control and improve the understanding of human behavior by providing an explainable objective function behind the given human behavior.","Meanwhile, the variability denotes the intrinsic uncertainty in human behavior.","It can be described by a Gaussian mixture model and capture the uncertainty in human behavior which cannot be encoded by the task objective.","The proposed method can improve the prediction accuracy of human behavior by leveraging both task objective and variability.","The proposed method is demonstrated through human-subject experiments using an illustrative quadrotor remote control example."],"url":"http://arxiv.org/abs/2404.14647v1","category":"cs.RO"}
{"created":"2024-04-22 23:05:44","title":"TDRAM: Tag-enhanced DRAM for Efficient Caching","abstract":"As SRAM-based caches are hitting a scaling wall, manufacturers are integrating DRAM-based caches into system designs to continue increasing cache sizes. While DRAM caches can improve the performance of memory systems, existing DRAM cache designs suffer from high miss penalties, wasted data movement, and interference between misses and demand requests. In this paper, we propose TDRAM, a novel DRAM microarchitecture tailored for caching. TDRAM enhances HBM3 by adding a set of small low-latency mats to store tags and metadata on the same die as the data mats. These mats enable fast parallel tag and data access, on-DRAM-die tag comparison, and conditional data response based on comparison result (reducing wasted data transfers) akin to SRAM caches mechanism. TDRAM further optimizes the hit and miss latencies by performing opportunistic early tag probing. Moreover, TDRAM introduces a flush buffer to store conflicting dirty data on write misses, eliminating turnaround delays on data bus. We evaluate TDRAM using a full-system simulator and a set of HPC workloads with large memory footprints showing TDRAM provides at least 2.6$\\times$ faster tag check, 1.2$\\times$ speedup, and 21% less energy consumption, compared to the state-of-the-art commercial and research designs.","sentences":["As SRAM-based caches are hitting a scaling wall, manufacturers are integrating DRAM-based caches into system designs to continue increasing cache sizes.","While DRAM caches can improve the performance of memory systems, existing DRAM cache designs suffer from high miss penalties, wasted data movement, and interference between misses and demand requests.","In this paper, we propose TDRAM, a novel DRAM microarchitecture tailored for caching.","TDRAM enhances HBM3 by adding a set of small low-latency mats to store tags and metadata on the same die as the data mats.","These mats enable fast parallel tag and data access, on-DRAM-die tag comparison, and conditional data response based on comparison result (reducing wasted data transfers) akin to SRAM caches mechanism.","TDRAM further optimizes the hit and miss latencies by performing opportunistic early tag probing.","Moreover, TDRAM introduces a flush buffer to store conflicting dirty data on write misses, eliminating turnaround delays on data bus.","We evaluate TDRAM using a full-system simulator and a set of HPC workloads with large memory footprints showing TDRAM provides at least 2.6$\\times$ faster tag check, 1.2$\\times$ speedup, and 21% less energy consumption, compared to the state-of-the-art commercial and research designs."],"url":"http://arxiv.org/abs/2404.14617v1","category":"cs.AR"}
{"created":"2024-04-22 21:50:01","title":"Planning Ahead in Generative Retrieval: Guiding Autoregressive Generation through Simultaneous Decoding","abstract":"This paper introduces PAG-a novel optimization and decoding approach that guides autoregressive generation of document identifiers in generative retrieval models through simultaneous decoding. To this aim, PAG constructs a set-based and sequential identifier for each document. Motivated by the bag-of-words assumption in information retrieval, the set-based identifier is built on lexical tokens. The sequential identifier, on the other hand, is obtained via quantizing relevance-based representations of documents. Extensive experiments on MSMARCO and TREC Deep Learning Track data reveal that PAG outperforms the state-of-the-art generative retrieval model by a large margin (e.g., 15.6% MRR improvements on MS MARCO), while achieving 22x speed up in terms of query latency.","sentences":["This paper introduces PAG-a novel optimization and decoding approach that guides autoregressive generation of document identifiers in generative retrieval models through simultaneous decoding.","To this aim, PAG constructs a set-based and sequential identifier for each document.","Motivated by the bag-of-words assumption in information retrieval, the set-based identifier is built on lexical tokens.","The sequential identifier, on the other hand, is obtained via quantizing relevance-based representations of documents.","Extensive experiments on MSMARCO and TREC Deep Learning Track data reveal that PAG outperforms the state-of-the-art generative retrieval model by a large margin (e.g., 15.6% MRR improvements on MS MARCO), while achieving 22x speed up in terms of query latency."],"url":"http://arxiv.org/abs/2404.14600v1","category":"cs.IR"}
{"created":"2024-04-22 21:41:07","title":"Efficient and Timely Memory Access","abstract":"This paper investigates the optimization of memory sampling in status updating systems, where source updates are published in shared memory, and reader process samples the memory for source updates by paying a sampling cost. We formulate a discrete-time decision problem to find a sampling policy that minimizes average cost comprising age at the client and the cost incurred due to sampling. We establish that an optimal policy is a stationary and deterministic threshold-type policy, and subsequently derive optimal threshold and the corresponding optimal average cost.","sentences":["This paper investigates the optimization of memory sampling in status updating systems, where source updates are published in shared memory, and reader process samples the memory for source updates by paying a sampling cost.","We formulate a discrete-time decision problem to find a sampling policy that minimizes average cost comprising age at the client and the cost incurred due to sampling.","We establish that an optimal policy is a stationary and deterministic threshold-type policy, and subsequently derive optimal threshold and the corresponding optimal average cost."],"url":"http://arxiv.org/abs/2404.14596v1","category":"eess.SY"}
{"created":"2024-04-22 21:08:05","title":"A general framework for supporting economic feasibility of generator and storage energy systems through capacity and dispatch optimization","abstract":"Integration of various electricity generating technologies (such as natural gas, wind, nuclear, etc.) with storage systems (such as thermal, battery electric, hydrogen, etc.) has the potential to improve the economic competitiveness of modern energy systems. Driven by the need to efficiently assess the economic feasibility of various energy system configurations in early system concept development, this work outlines a versatile computational framework for assessing the net present value of various integrated storage technologies. The subsystems' fundamental dynamics are defined, with a particular emphasis on balancing critical physical and economic domains to enable optimal decision-making in the context of capacity and dispatch optimization. In its presented form, the framework formulates a linear, convex optimization problem that can be efficiently solved using a direct transcription approach in the open-source software DTQP. Three case studies are considered to demonstrate and validate the capabilities of the framework, highlighting its value and computational efficiency in facilitating economic assessment of various configurations of energy systems. In particular, natural gas with thermal storage and carbon capture, wind energy with battery storage, and nuclear with hydrogen are demonstrated.","sentences":["Integration of various electricity generating technologies (such as natural gas, wind, nuclear, etc.) with storage systems (such as thermal, battery electric, hydrogen, etc.) has the potential to improve the economic competitiveness of modern energy systems.","Driven by the need to efficiently assess the economic feasibility of various energy system configurations in early system concept development, this work outlines a versatile computational framework for assessing the net present value of various integrated storage technologies.","The subsystems' fundamental dynamics are defined, with a particular emphasis on balancing critical physical and economic domains to enable optimal decision-making in the context of capacity and dispatch optimization.","In its presented form, the framework formulates a linear, convex optimization problem that can be efficiently solved using a direct transcription approach in the open-source software DTQP.","Three case studies are considered to demonstrate and validate the capabilities of the framework, highlighting its value and computational efficiency in facilitating economic assessment of various configurations of energy systems.","In particular, natural gas with thermal storage and carbon capture, wind energy with battery storage, and nuclear with hydrogen are demonstrated."],"url":"http://arxiv.org/abs/2404.14583v1","category":"eess.SY"}
{"created":"2024-04-22 20:06:25","title":"Efficiency and Cost Optimization of Dual Active Bridge Converter for 350kW DC Fast Chargers","abstract":"This study focuses on optimizing the design parameters of a Dual Active Bridge (DAB) converter for use in 350 kW DC fast chargers, emphasizing the balance between efficiency and cost. Addressing the observed gaps in existing high-power application research, it introduces an optimization framework to evaluate critical design parameters,number of converter modules, switching frequency, and transformer turns ratio,within a broad operational voltage range. The analysis identifies an optimal configuration that achieves over 95% efficiency at rated power across a wide output voltage range, comprising seven 50 kW DAB converters with a switching frequency of 30 kHz, and a transformer turns ratio of 0.9.","sentences":["This study focuses on optimizing the design parameters of a Dual Active Bridge (DAB) converter for use in 350 kW DC fast chargers, emphasizing the balance between efficiency and cost.","Addressing the observed gaps in existing high-power application research, it introduces an optimization framework to evaluate critical design parameters,number of converter modules, switching frequency, and transformer turns ratio,within a broad operational voltage range.","The analysis identifies an optimal configuration that achieves over 95% efficiency at rated power across a wide output voltage range, comprising seven 50 kW DAB converters with a switching frequency of 30 kHz, and a transformer turns ratio of 0.9."],"url":"http://arxiv.org/abs/2404.14557v1","category":"eess.SY"}
{"created":"2024-04-22 19:56:45","title":"Constrained multi-cluster game: Distributed Nash equilibrium seeking over directed graphs","abstract":"Motivated by the complex dynamics of cooperative and competitive interactions within networked agent systems, multi-cluster games provide a framework for modeling the interconnected goals of self-interested clusters of agents. For this setup, the existing literature lacks comprehensive gradient-based solutions that simultaneously consider constraint sets and directed communication networks, both of which are crucial for many practical applications. To address this gap, this paper proposes a distributed Nash equilibrium seeking algorithm that integrates consensus-based methods and gradient-tracking techniques, where inter-cluster and intra-cluster communications only use row- and column-stochastic weight matrices, respectively. To handle constraints, we introduce an averaging procedure, which can effectively address the complications associated with projections. In turn, we can show linear convergence of our algorithm, focusing on the contraction property of the optimality gap. We demonstrate the efficacy of the proposed algorithm through a microgrid energy management application.","sentences":["Motivated by the complex dynamics of cooperative and competitive interactions within networked agent systems, multi-cluster games provide a framework for modeling the interconnected goals of self-interested clusters of agents.","For this setup, the existing literature lacks comprehensive gradient-based solutions that simultaneously consider constraint sets and directed communication networks, both of which are crucial for many practical applications.","To address this gap, this paper proposes a distributed Nash equilibrium seeking algorithm that integrates consensus-based methods and gradient-tracking techniques, where inter-cluster and intra-cluster communications only use row- and column-stochastic weight matrices, respectively.","To handle constraints, we introduce an averaging procedure, which can effectively address the complications associated with projections.","In turn, we can show linear convergence of our algorithm, focusing on the contraction property of the optimality gap.","We demonstrate the efficacy of the proposed algorithm through a microgrid energy management application."],"url":"http://arxiv.org/abs/2404.14554v1","category":"math.OC"}
{"created":"2024-04-22 19:31:45","title":"WangLab at MEDIQA-CORR 2024: Optimized LLM-based Programs for Medical Error Detection and Correction","abstract":"Medical errors in clinical text pose significant risks to patient safety. The MEDIQA-CORR 2024 shared task focuses on detecting and correcting these errors across three subtasks: identifying the presence of an error, extracting the erroneous sentence, and generating a corrected sentence. In this paper, we present our approach that achieved top performance in all three subtasks. For the MS dataset, which contains subtle errors, we developed a retrieval-based system leveraging external medical question-answering datasets. For the UW dataset, reflecting more realistic clinical notes, we created a pipeline of modules to detect, localize, and correct errors. Both approaches utilized the DSPy framework for optimizing prompts and few-shot examples in large language model (LLM) based programs. Our results demonstrate the effectiveness of LLM based programs for medical error correction. However, our approach has limitations in addressing the full diversity of potential errors in medical documentation. We discuss the implications of our work and highlight future research directions to advance the robustness and applicability of medical error detection and correction systems.","sentences":["Medical errors in clinical text pose significant risks to patient safety.","The MEDIQA-CORR 2024 shared task focuses on detecting and correcting these errors across three subtasks: identifying the presence of an error, extracting the erroneous sentence, and generating a corrected sentence.","In this paper, we present our approach that achieved top performance in all three subtasks.","For the MS dataset, which contains subtle errors, we developed a retrieval-based system leveraging external medical question-answering datasets.","For the UW dataset, reflecting more realistic clinical notes, we created a pipeline of modules to detect, localize, and correct errors.","Both approaches utilized the DSPy framework for optimizing prompts and few-shot examples in large language model (LLM) based programs.","Our results demonstrate the effectiveness of LLM based programs for medical error correction.","However, our approach has limitations in addressing the full diversity of potential errors in medical documentation.","We discuss the implications of our work and highlight future research directions to advance the robustness and applicability of medical error detection and correction systems."],"url":"http://arxiv.org/abs/2404.14544v1","category":"cs.CL"}
{"created":"2024-04-22 19:05:59","title":"Plasmon-enhanced Brillouin Light Scattering (BLS) spectroscopy for magnetic systems. II. Numerical simulations","abstract":"Brillouin light scattering (BLS) spectroscopy is a powerful tool for detecting spin waves in magnetic thin films and nanostructures. Despite comprehensive access to spin-wave properties, BLS spectroscopy suffers from the limited wavenumber of detectable spin waves and the typically relatively low sensitivity. In this work, we present the results of numerical simulations based on the recently developed analytical model describing plasmon-enhanced BLS. The effective susceptibility is defined for a single plasmonic nanoparticle in the shape of an ellipsoid of rotation, for the sandwiched plasmonic nanoparticles separated by a dielectric spacer, as well as for the array of plasmonic resonators on the surface of a magnetic film. It is shown that the eccentricity of the metal nanoparticles, which describes their shape, plays a key role in the enhancement of the BLS signal. The optimal conditions for BLS enhancement are numerically defined for gold and silver plasmon systems for photons of different energies. The presented results define the roadmap for the experimental realization of plasmon-enhanced BLS spectroscopy.","sentences":["Brillouin light scattering (BLS) spectroscopy is a powerful tool for detecting spin waves in magnetic thin films and nanostructures.","Despite comprehensive access to spin-wave properties, BLS spectroscopy suffers from the limited wavenumber of detectable spin waves and the typically relatively low sensitivity.","In this work, we present the results of numerical simulations based on the recently developed analytical model describing plasmon-enhanced BLS.","The effective susceptibility is defined for a single plasmonic nanoparticle in the shape of an ellipsoid of rotation, for the sandwiched plasmonic nanoparticles separated by a dielectric spacer, as well as for the array of plasmonic resonators on the surface of a magnetic film.","It is shown that the eccentricity of the metal nanoparticles, which describes their shape, plays a key role in the enhancement of the BLS signal.","The optimal conditions for BLS enhancement are numerically defined for gold and silver plasmon systems for photons of different energies.","The presented results define the roadmap for the experimental realization of plasmon-enhanced BLS spectroscopy."],"url":"http://arxiv.org/abs/2404.14535v1","category":"physics.optics"}
{"created":"2024-04-22 18:56:47","title":"Plasmon-enhanced Brillouin Light Scattering spectroscopy for magnetic systems. I. Theoretical Model","abstract":"Brillouin light scattering (BLS) spectroscopy is an effective method for detecting spin waves in magnetic thin films and nanostructures. While it provides extensive insight into the properties of spin waves, BLS spectroscopy is impeded in many practical cases by the limited range of detectable spin wave wavenumbers and its low sensitivity. Here, we present a generalized theoretical model describing plasmon-enhanced BLS spectroscopy. Three types of plasmonic nanoparticles in the shape of an ellipsoid of rotation are considered: a single plasmon resonator, a sandwiched plasmonic structure in which two nanoparticles are separated by a dielectric spacer, and an ensemble of metallic nanoparticles on the surface of a magnetic film. The effective susceptibilities for the plasmonic systems at the surface of the magnetic film are calculated using the electrodynamic Green functions method, and the enhancement coefficient is defined. It is analytically shown that the ratio of the plasmon resonator height to its radius plays the key role in the development of plasmon-enhanced BLS spectroscopy. The developed model serves as a basis for numerical engineering of the optimized plasmon nanoparticle morphology for BLS enhancement.","sentences":["Brillouin light scattering (BLS) spectroscopy is an effective method for detecting spin waves in magnetic thin films and nanostructures.","While it provides extensive insight into the properties of spin waves, BLS spectroscopy is impeded in many practical cases by the limited range of detectable spin wave wavenumbers and its low sensitivity.","Here, we present a generalized theoretical model describing plasmon-enhanced BLS spectroscopy.","Three types of plasmonic nanoparticles in the shape of an ellipsoid of rotation are considered: a single plasmon resonator, a sandwiched plasmonic structure in which two nanoparticles are separated by a dielectric spacer, and an ensemble of metallic nanoparticles on the surface of a magnetic film.","The effective susceptibilities for the plasmonic systems at the surface of the magnetic film are calculated using the electrodynamic Green functions method, and the enhancement coefficient is defined.","It is analytically shown that the ratio of the plasmon resonator height to its radius plays the key role in the development of plasmon-enhanced BLS spectroscopy.","The developed model serves as a basis for numerical engineering of the optimized plasmon nanoparticle morphology for BLS enhancement."],"url":"http://arxiv.org/abs/2404.14528v1","category":"physics.optics"}
{"created":"2024-04-22 18:46:35","title":"Randomized Nystr\u00f6m Preconditioned Interior Point-Proximal Method of Multipliers","abstract":"We present a new algorithm for convex separable quadratic programming (QP) called Nys-IP-PMM, a regularized interior-point solver that uses low-rank structure to accelerate solution of the Newton system. The algorithm combines the interior point proximal method of multipliers (IP-PMM) with the randomized Nystr\\\"om preconditioned conjugate gradient method as the inner linear system solver. Our algorithm is matrix-free: it accesses the input matrices solely through matrix-vector products, as opposed to methods involving matrix factorization. It works particularly well for separable QP instances with dense constraint matrices. We establish convergence of Nys-IP-PMM. Numerical experiments demonstrate its superior performance in terms of wallclock time compared to previous matrix-free IPM-based approaches.","sentences":["We present a new algorithm for convex separable quadratic programming (QP) called Nys-IP-PMM, a regularized interior-point solver that uses low-rank structure to accelerate solution of the Newton system.","The algorithm combines the interior point proximal method of multipliers (IP-PMM) with the randomized Nystr\\\"om preconditioned conjugate gradient method as the inner linear system solver.","Our algorithm is matrix-free: it accesses the input matrices solely through matrix-vector products, as opposed to methods involving matrix factorization.","It works particularly well for separable QP instances with dense constraint matrices.","We establish convergence of Nys-IP-PMM.","Numerical experiments demonstrate its superior performance in terms of wallclock time compared to previous matrix-free IPM-based approaches."],"url":"http://arxiv.org/abs/2404.14524v1","category":"math.OC"}
{"created":"2024-04-22 18:29:52","title":"Cooperative ISAC Networks: Performance Analysis, Scaling Laws and Optimization","abstract":"Integrated sensing and communication (ISAC) networks are investigated with the objective of effectively balancing the sensing and communication (S&C) performance at the network level. Through the simultaneous utilization of multi-point (CoMP) coordinated joint transmission and distributed multiple-input multiple-output (MIMO) radar techniques, we propose an innovative networked ISAC scheme, where multiple transceivers are employed for collaboratively enhancing the S&C services. Then, the potent tool of stochastic geometry is exploited for characterizing the S&C performance, which allows us to illuminate the key cooperative dependencies in the ISAC network and optimize salient network-level parameters. Remarkably, the Cramer-Rao lower bound (CRLB) expression of the localization accuracy derived unveils a significant finding: Deploying N ISAC transceivers yields an enhanced average cooperative sensing performance across the entire network, in accordance with the ln^2N scaling law. Crucially, this scaling law is less pronounced in comparison to the performance enhancement of N^2 achieved when the transceivers are equidistant from the target, which is primarily due to the substantial path loss from the distant base stations (BSs) and leads to reduced contributions to sensing performance gain. Moreover, we derive a tight expression of the communication rate, and present a low-complexity algorithm to determine the optimal cooperative cluster size. Based on our expression derived for the S&C performance, we formulate the optimization problem of maximizing the network performance in terms of two joint S&C metrics. To this end, we jointly optimize the cooperative BS cluster sizes and the transmit power to strike a flexible tradeoff between the S&C performance.","sentences":["Integrated sensing and communication (ISAC) networks are investigated with the objective of effectively balancing the sensing and communication (S&C) performance at the network level.","Through the simultaneous utilization of multi-point (CoMP) coordinated joint transmission and distributed multiple-input multiple-output (MIMO) radar techniques, we propose an innovative networked ISAC scheme, where multiple transceivers are employed for collaboratively enhancing the S&C services.","Then, the potent tool of stochastic geometry is exploited for characterizing the S&C performance, which allows us to illuminate the key cooperative dependencies in the ISAC network and optimize salient network-level parameters.","Remarkably, the Cramer-Rao lower bound (CRLB) expression of the localization accuracy derived unveils a significant finding: Deploying N ISAC transceivers yields an enhanced average cooperative sensing performance across the entire network, in accordance with the ln^2N scaling law.","Crucially, this scaling law is less pronounced in comparison to the performance enhancement of N^2 achieved when the transceivers are equidistant from the target, which is primarily due to the substantial path loss from the distant base stations (BSs) and leads to reduced contributions to sensing performance gain.","Moreover, we derive a tight expression of the communication rate, and present a low-complexity algorithm to determine the optimal cooperative cluster size.","Based on our expression derived for the S&C performance, we formulate the optimization problem of maximizing the network performance in terms of two joint S&C metrics.","To this end, we jointly optimize the cooperative BS cluster sizes and the transmit power to strike a flexible tradeoff between the S&C performance."],"url":"http://arxiv.org/abs/2404.14514v1","category":"cs.IT"}
{"created":"2024-04-22 18:03:19","title":"QuantumAnnealing: A Julia Package for Simulating Dynamics of Transverse Field Ising Models","abstract":"Analog Quantum Computers are promising tools for improving performance on applications such as modeling behavior of quantum materials, providing fast heuristic solutions to optimization problems, and simulating quantum systems. Due to the challenges of simulating dynamic quantum systems, there are relatively few classical tools for modeling the behavior of these devices and verifying their performance. QuantumAnnealing.jl provides a toolkit for performing simulations of Analog Quantum Computers on classical hardware. This package includes functionality for simulation of the time evolution of the Transverse Field Ising Model, replicating annealing schedules used by real world annealing hardware, implementing custom annealing schedules, and more. This allows for rapid prototyping of models expected to display interesting behavior, verification of the performance of quantum devices, and easy comparison against the expected behavior of quantum devices against classical approaches for small systems. The software is provided as open-source and is available through Julia's package registry system.","sentences":["Analog Quantum Computers are promising tools for improving performance on applications such as modeling behavior of quantum materials, providing fast heuristic solutions to optimization problems, and simulating quantum systems.","Due to the challenges of simulating dynamic quantum systems, there are relatively few classical tools for modeling the behavior of these devices and verifying their performance.","QuantumAnnealing.jl provides a toolkit for performing simulations of Analog Quantum Computers on classical hardware.","This package includes functionality for simulation of the time evolution of the Transverse Field Ising Model, replicating annealing schedules used by real world annealing hardware, implementing custom annealing schedules, and more.","This allows for rapid prototyping of models expected to display interesting behavior, verification of the performance of quantum devices, and easy comparison against the expected behavior of quantum devices against classical approaches for small systems.","The software is provided as open-source and is available through Julia's package registry system."],"url":"http://arxiv.org/abs/2404.14501v1","category":"quant-ph"}
{"created":"2024-04-22 18:02:17","title":"Mapping Wireless Networks into Digital Reality through Joint Vertical and Horizontal Learning","abstract":"In recent years, the complexity of 5G and beyond wireless networks has escalated, prompting a need for innovative frameworks to facilitate flexible management and efficient deployment. The concept of digital twins (DTs) has emerged as a solution to enable real-time monitoring, predictive configurations, and decision-making processes. While existing works primarily focus on leveraging DTs to optimize wireless networks, a detailed mapping methodology for creating virtual representations of network infrastructure and properties is still lacking. In this context, we introduce VH-Twin, a novel time-series data-driven framework that effectively maps wireless networks into digital reality. VH-Twin distinguishes itself through complementary vertical twinning (V-twinning) and horizontal twinning (H-twinning) stages, followed by a periodic clustering mechanism used to virtualize network regions based on their distinct geological and wireless characteristics. Specifically, V-twinning exploits distributed learning techniques to initialize a global twin model collaboratively from virtualized network clusters. H-twinning, on the other hand, is implemented with an asynchronous mapping scheme that dynamically updates twin models in response to network or environmental changes. Leveraging real-world wireless traffic data within a cellular wireless network, comprehensive experiments are conducted to verify that VH-Twin can effectively construct, deploy, and maintain network DTs. Parametric analysis also offers insights into how to strike a balance between twinning efficiency and model accuracy at scale.","sentences":["In recent years, the complexity of 5G and beyond wireless networks has escalated, prompting a need for innovative frameworks to facilitate flexible management and efficient deployment.","The concept of digital twins (DTs) has emerged as a solution to enable real-time monitoring, predictive configurations, and decision-making processes.","While existing works primarily focus on leveraging DTs to optimize wireless networks, a detailed mapping methodology for creating virtual representations of network infrastructure and properties is still lacking.","In this context, we introduce VH-Twin, a novel time-series data-driven framework that effectively maps wireless networks into digital reality.","VH-Twin distinguishes itself through complementary vertical twinning (V-twinning) and horizontal twinning (H-twinning) stages, followed by a periodic clustering mechanism used to virtualize network regions based on their distinct geological and wireless characteristics.","Specifically, V-twinning exploits distributed learning techniques to initialize a global twin model collaboratively from virtualized network clusters.","H-twinning, on the other hand, is implemented with an asynchronous mapping scheme that dynamically updates twin models in response to network or environmental changes.","Leveraging real-world wireless traffic data within a cellular wireless network, comprehensive experiments are conducted to verify that VH-Twin can effectively construct, deploy, and maintain network DTs.","Parametric analysis also offers insights into how to strike a balance between twinning efficiency and model accuracy at scale."],"url":"http://arxiv.org/abs/2404.14497v1","category":"cs.NI"}
{"created":"2024-04-22 18:00:06","title":"On verifiable quantum advantage with peaked circuit sampling","abstract":"Over a decade after its proposal, the idea of using quantum computers to sample hard distributions has remained a key path to demonstrating quantum advantage. Yet a severe drawback remains: verification seems to require exponential classical computation. As an attempt to overcome this difficulty, we propose a new candidate for quantum advantage experiments with otherwise-random ''peaked circuits'', i.e., quantum circuits whose outputs have high concentrations on a computational basis state. Naturally, the heavy output string can be used for classical verification.   In this work, we analytically and numerically study an explicit model of peaked circuits, in which $\\tau_r$ layers of uniformly random gates are augmented by $\\tau_p$ layers of gates that are optimized to maximize peakedness. We show that getting constant peakedness from such circuits requires $\\tau_{p} = \\Omega((\\tau_r/n)^{0.19})$ with overwhelming probability. However, we also give numerical evidence that nontrivial peakedness is possible in this model -- decaying exponentially with the number of qubits, but more than can be explained by any approximation where the output of a random quantum circuit is treated as a Haar-random state. This suggests that these peaked circuits have the potential for future verifiable quantum advantage experiments.   Our work raises numerous open questions about random peaked circuits, including how to generate them efficiently, and whether they can be distinguished from fully random circuits in classical polynomial time.","sentences":["Over a decade after its proposal, the idea of using quantum computers to sample hard distributions has remained a key path to demonstrating quantum advantage.","Yet a severe drawback remains: verification seems to require exponential classical computation.","As an attempt to overcome this difficulty, we propose a new candidate for quantum advantage experiments with otherwise-random ''peaked circuits'', i.e., quantum circuits whose outputs have high concentrations on a computational basis state.","Naturally, the heavy output string can be used for classical verification.   ","In this work, we analytically and numerically study an explicit model of peaked circuits, in which $\\tau_r$ layers of uniformly random gates are augmented by $\\tau_p$ layers of gates that are optimized to maximize peakedness.","We show that getting constant peakedness from such circuits requires $\\tau_{p} = \\Omega((\\tau_r/n)^{0.19})$ with overwhelming probability.","However, we also give numerical evidence that nontrivial peakedness is possible in this model -- decaying exponentially with the number of qubits, but more than can be explained by any approximation where the output of a random quantum circuit is treated as a Haar-random state.","This suggests that these peaked circuits have the potential for future verifiable quantum advantage experiments.   ","Our work raises numerous open questions about random peaked circuits, including how to generate them efficiently, and whether they can be distinguished from fully random circuits in classical polynomial time."],"url":"http://arxiv.org/abs/2404.14493v1","category":"quant-ph"}
{"created":"2024-04-22 18:00:02","title":"Impact of main-sequence mass loss on the appearance, structure and evolution of Wolf-Rayet stars","abstract":"Stellar winds are one of the most important drivers of massive star evolution and a vital source of chemical, mechanical, and radiative feedback. Despite its significance, mass loss remains a major uncertainty in stellar evolution models. Particularly the interdependencies of different approaches with subsequent evolutionary stages and predicted observable phenomena are far from being systematically understood. In this study, we examine the impact of main sequence mass loss on the structure of massive stars throughout their evolution. A particular focus is placed on the consequences for entering the Wolf-Rayet (WR) regime and the subsequent evolution. Using the Geneva stellar evolution code, we compute grids of single, non-rotating stellar models at solar and Large Magellanic Cloud (LMC) metallicity of initial masses between 20 and 120 solar masses, with two representative prescriptions for high and low main sequence mass loss. We obtain detailed numerical predictions regarding the structure and evolution of massive stars, and infer the role of main sequence mass loss by comparison of the mass-loss rate prescriptions. We present implications for the overall evolutionary trajectory, including the evolution of WR stars, as well as the effect on stellar yields and stellar populations. Mass loss during the main sequence plays an important role due to its ability to affect the sequence and duration of all subsequent phases. We identify several distinct evolutionary paths for massive stars which are significantly influenced by the chosen main sequence mass-loss description. We also discuss the impact of uncertainties other than mass loss on the evolution, in particular those relating to convection. We further demonstrate that not just the total mass loss, but the specific mass-loss history throughout a star's life is a crucial determinant of many aspects, such as the resulting stellar yields.","sentences":["Stellar winds are one of the most important drivers of massive star evolution and a vital source of chemical, mechanical, and radiative feedback.","Despite its significance, mass loss remains a major uncertainty in stellar evolution models.","Particularly the interdependencies of different approaches with subsequent evolutionary stages and predicted observable phenomena are far from being systematically understood.","In this study, we examine the impact of main sequence mass loss on the structure of massive stars throughout their evolution.","A particular focus is placed on the consequences for entering the Wolf-Rayet (WR) regime and the subsequent evolution.","Using the Geneva stellar evolution code, we compute grids of single, non-rotating stellar models at solar and Large Magellanic Cloud (LMC) metallicity of initial masses between 20 and 120 solar masses, with two representative prescriptions for high and low main sequence mass loss.","We obtain detailed numerical predictions regarding the structure and evolution of massive stars, and infer the role of main sequence mass loss by comparison of the mass-loss rate prescriptions.","We present implications for the overall evolutionary trajectory, including the evolution of WR stars, as well as the effect on stellar yields and stellar populations.","Mass loss during the main sequence plays an important role due to its ability to affect the sequence and duration of all subsequent phases.","We identify several distinct evolutionary paths for massive stars which are significantly influenced by the chosen main sequence mass-loss description.","We also discuss the impact of uncertainties other than mass loss on the evolution, in particular those relating to convection.","We further demonstrate that not just the total mass loss, but the specific mass-loss history throughout a star's life is a crucial determinant of many aspects, such as the resulting stellar yields."],"url":"http://arxiv.org/abs/2404.14488v1","category":"astro-ph.SR"}
{"created":"2024-04-22 18:00:00","title":"Optimally scrambling chiral spin-chain with effective black hole geometry","abstract":"There is currently significant interest in emulating the essential characteristics of black holes, such as their Hawking radiation or their optimal scrambling behavior, using condensed matter models. In this article, we investigate a chiral spin-chain, whose mean field theory effectively captures the behavior of Dirac fermions in the curved spacetime geometry of a black hole. We find that within the region of the chain that describe the interior of the black hole, strong correlations prevail giving rise to many-body chaotic dynamics. Employing out-of-time-order correlations as a diagnostic tool, we numerically compute the associated Lyapunov exponent. Intriguingly, we observe a linear increase in the Lyapunov exponent with temperature within the black hole's interior at low temperatures, indicative of optimal scrambling behavior. This contrasts with the quadratic temperature dependence exhibited by the spin-chain on the region outside the black hole. Our findings contribute to a deeper understanding of the interplay between black hole geometry and quantum chaos, offering insights into fundamental aspects of quantum gravity.","sentences":["There is currently significant interest in emulating the essential characteristics of black holes, such as their Hawking radiation or their optimal scrambling behavior, using condensed matter models.","In this article, we investigate a chiral spin-chain, whose mean field theory effectively captures the behavior of Dirac fermions in the curved spacetime geometry of a black hole.","We find that within the region of the chain that describe the interior of the black hole, strong correlations prevail giving rise to many-body chaotic dynamics.","Employing out-of-time-order correlations as a diagnostic tool, we numerically compute the associated Lyapunov exponent.","Intriguingly, we observe a linear increase in the Lyapunov exponent with temperature within the black hole's interior at low temperatures, indicative of optimal scrambling behavior.","This contrasts with the quadratic temperature dependence exhibited by the spin-chain on the region outside the black hole.","Our findings contribute to a deeper understanding of the interplay between black hole geometry and quantum chaos, offering insights into fundamental aspects of quantum gravity."],"url":"http://arxiv.org/abs/2404.14473v1","category":"cond-mat.str-el"}
{"created":"2024-04-23 17:05:14","title":"Interacting Dark Energy after DESI Baryon Acoustic Oscillation measurements","abstract":"We investigate the implications of the Baryon Acoustic Oscillation measurements released by the Dark Energy Spectroscopic Instrument (DESI) for Interacting Dark Energy (IDE) models characterized by an energy-momentum flow from Dark Matter to Dark Energy. By combining Planck-2018 and DESI data, we observe a preference for interactions exceeding the 95% confidence level, yielding a present-day expansion rate $H_0=71.4\\pm1.5$ km/s/Mpc, in agreement with SH0ES. This preference remains robust when including measurements of the expansion rate $H(z)$ obtained from the relative ages of massive, early-time, and passively-evolving galaxies, as well as when considering distance moduli measurements from Type-Ia Supernovae sourced from the Pantheon-plus catalog using the SH0ES Cepheid host distances as calibrators. Overall, high and low redshift data can be equally or better explained within the IDE framework compared to $\\Lambda$CDM, while also yielding higher values of $H_0$ in better agreement with the local distance ladder estimate.","sentences":["We investigate the implications of the Baryon Acoustic Oscillation measurements released by the Dark Energy Spectroscopic Instrument (DESI) for Interacting Dark Energy (IDE) models characterized by an energy-momentum flow from Dark Matter to Dark Energy.","By combining Planck-2018 and DESI data, we observe a preference for interactions exceeding the 95% confidence level, yielding a present-day expansion rate $H_0=71.4\\pm1.5$ km/s/Mpc, in agreement with SH0ES.","This preference remains robust when including measurements of the expansion rate $H(z)$ obtained from the relative ages of massive, early-time, and passively-evolving galaxies, as well as when considering distance moduli measurements from Type-Ia Supernovae sourced from the Pantheon-plus catalog using the SH0ES Cepheid host distances as calibrators.","Overall, high and low redshift data can be equally or better explained within the IDE framework compared to $\\Lambda$CDM, while also yielding higher values of $H_0$ in better agreement with the local distance ladder estimate."],"url":"http://arxiv.org/abs/2404.15232v1","category":"astro-ph.CO"}
{"created":"2024-04-23 16:52:14","title":"Dark Radiation with Baryon Acoustic Oscillations from DESI 2024 and the $H_0$ tension","abstract":"We investigate the presence of extra relativistic degrees of freedom in the early Universe, contributing to the effective number of neutrinos $N_\\text{eff}$, as $\\Delta N_\\text{eff}\\equiv N_\\text{eff}-3.044\\geq 0$, in light of the recent measurements of Baryon Acoustic Oscillations (BAO) by the DESI collaboration. We analyze one-parameter extensions of the $\\Lambda$CDM model where dark radiation (DR) is free streaming or behaves as a perfect fluid, due to self-interactions. We report a significant relaxation of upper bounds on $\\Delta N_\\text{eff}$, with respect to previous BAO data from SDSS+6dFGS, when additionally employing Planck data (and supernovae data from Pantheon+), setting $\\Delta N_\\text{eff}\\leq 0.39$ ($95\\%$ C.L.) for free streaming DR, and a very mild preference for fluid DR, $\\Delta N_\\text{eff} = 0.221^{+0.088}_{-0.18}$ ($\\leq 0.46$, $95\\%$ C.L.). Applying constraints from primordial element abundances leads to slightly tighter constraints on $\\Delta N_\\text{eff}$, but they are avoided if DR is produced after Big Bang Nucleosynthesis (BBN). For fluid DR we estimate the tension with the SH$_0$ES determination of $H_0$ to be around $(2.3-2.8)\\sigma$ level, and for free-streaming DR the tension is below $3\\sigma$ if production occurs after BBN. This lesser degree of tension motivates a combination with SH$_0$ES in these cases, resulting in a $4.4\\sigma-5\\sigma$ evidence for dark radiation with $\\Delta N_\\text{eff}\\simeq 0.6$ and large improvements in $\\chi^2$ over $\\Lambda$CDM, $-18\\lesssim \\Delta \\chi^2\\lesssim -25$.","sentences":["We investigate the presence of extra relativistic degrees of freedom in the early Universe, contributing to the effective number of neutrinos $N_\\text{eff}$, as $\\Delta N_\\text{eff}\\equiv N_\\text{eff}-3.044\\geq 0$, in light of the recent measurements of Baryon Acoustic Oscillations (BAO) by the DESI collaboration.","We analyze one-parameter extensions of the $\\Lambda$CDM model where dark radiation (DR) is free streaming or behaves as a perfect fluid, due to self-interactions.","We report a significant relaxation of upper bounds on $\\Delta N_\\text{eff}$, with respect to previous BAO data from SDSS+6dFGS, when additionally employing Planck data (and supernovae data from Pantheon+), setting $\\Delta N_\\text{eff}\\leq 0.39$ ($95\\%$ C.L.) for free streaming DR, and a very mild preference for fluid DR, $\\Delta N_\\text{eff} = 0.221^{+0.088}_{-0.18}$ ($\\leq 0.46$, $95\\%$ C.L.).","Applying constraints from primordial element abundances leads to slightly tighter constraints on $\\Delta N_\\text{eff}$, but they are avoided if DR is produced after Big Bang Nucleosynthesis (BBN).","For fluid DR we estimate the tension with the SH$_0$ES determination of $H_0$ to be around $(2.3-2.8)\\sigma$ level, and for free-streaming DR the tension is below $3\\sigma$ if production occurs after BBN.","This lesser degree of tension motivates a combination with SH$_0$ES in these cases, resulting in a $4.4\\sigma-5\\sigma$ evidence for dark radiation with $\\Delta N_\\text{eff}\\simeq 0.6$ and large improvements in $\\chi^2$ over $\\Lambda$CDM, $-18\\lesssim \\Delta \\chi^2\\lesssim -25$."],"url":"http://arxiv.org/abs/2404.15220v1","category":"astro-ph.CO"}
{"created":"2024-04-23 16:11:33","title":"Signature of Particle Diffusion on the X-ray Spectra of the blazar Mkn 421","abstract":"The curvature in blazar spectrum has the potential to understand the particle dynamics in jets. We performed a detailed analysis of simultaneous Swift-XRT (0.3-10 keV) and NuSTAR (3-79 keV)} observations of Mkn 421. Our analysis of NuSTAR observations alone reveals that, during periods of low flux, the hard X-ray spectra are best represented by a steep power-law with photon index reaching $\\sim$ 3. However, the spectrum exhibits significant curvature during its high flux states. To investigate this, we explore plausible diffusion processes facilitating shock acceleration in the emission region that can contribute to the observed spectral curvature. Particularly, such processes can cause gradual fall of the photon spectrum at high energies which can be represented by a sub-exponential function. The parameter that decides this spectral change can be used to characterise the energy dependence of the diffusive process. Our results suggest that the X-ray spectra of Mkn 421 are consistent with a scenario where particle acceleration is mediated through Bohm-type diffusion and the spectra beyond the synchrotron peak is modulated by the radiative loss process.","sentences":["The curvature in blazar spectrum has the potential to understand the particle dynamics in jets.","We performed a detailed analysis of simultaneous Swift-XRT (0.3-10 keV) and NuSTAR (3-79 keV)} observations of Mkn 421.","Our analysis of NuSTAR observations alone reveals that, during periods of low flux, the hard X-ray spectra are best represented by a steep power-law with photon index reaching $\\sim$ 3.","However, the spectrum exhibits significant curvature during its high flux states.","To investigate this, we explore plausible diffusion processes facilitating shock acceleration in the emission region that can contribute to the observed spectral curvature.","Particularly, such processes can cause gradual fall of the photon spectrum at high energies which can be represented by a sub-exponential function.","The parameter that decides this spectral change can be used to characterise the energy dependence of the diffusive process.","Our results suggest that the X-ray spectra of Mkn 421 are consistent with a scenario where particle acceleration is mediated through Bohm-type diffusion and the spectra beyond the synchrotron peak is modulated by the radiative loss process."],"url":"http://arxiv.org/abs/2404.15171v1","category":"astro-ph.HE"}
{"created":"2024-04-23 15:43:49","title":"Estimating Longitudinal Polarization of $\u039b$ and $\\bar\u039b$ Hyperons at Relativistic Energies using Hydrodynamic and Transport models","abstract":"The global and local polarization measurements of $\\Lambda$ ($\\bar{\\Lambda}$) hyperons by STAR and ALICE Collaborations open up an immense interest in investigating the polarization dynamics in heavy-ion collisions. Recent studies suggest the transverse component of the vorticity field is responsible for the global spin polarization, while the longitudinal component of the vorticity field accounts for the local polarization. The local polarization of $\\Lambda$-hyperons arises due to the anisotropic flows in the transverse plane, indicating a quadrupole pattern of the longitudinal vorticity along the beam direction. The present study focuses on the local (longitudinal) polarization of $\\Lambda$ and $\\bar{\\Lambda}$ in Au$+$Au and Pb$+$Pb collisions at $\\sqrt{s_{NN}}$ = 200 GeV and 5.02 TeV, respectively. Further, we explore the centrality and transverse momentum ($p_{\\rm T}$) dependence of longitudinal polarization using hydrodynamic and transport models. All these models predict a maximum longitudinal polarization in mid-central collisions around 30-50 \\% centrality at $p_{\\rm T} \\approx$ 2.0 - 3.0 GeV/c. These findings on longitudinal polarization advocate the existence of a thermal medium in non-central heavy-ion collisions. Our findings are in agreement with corresponding experimental data at the RHIC and LHC energies.","sentences":["The global and local polarization measurements of $\\Lambda$ ($\\bar{\\Lambda}$) hyperons by STAR and ALICE Collaborations open up an immense interest in investigating the polarization dynamics in heavy-ion collisions.","Recent studies suggest the transverse component of the vorticity field is responsible for the global spin polarization, while the longitudinal component of the vorticity field accounts for the local polarization.","The local polarization of $\\Lambda$-hyperons arises due to the anisotropic flows in the transverse plane, indicating a quadrupole pattern of the longitudinal vorticity along the beam direction.","The present study focuses on the local (longitudinal) polarization of $\\Lambda$ and $\\bar{\\Lambda}$ in Au$+$Au and Pb$+$Pb collisions at $\\sqrt{s_{NN}}$ = 200 GeV and 5.02 TeV, respectively.","Further, we explore the centrality and transverse momentum ($p_{\\rm T}$) dependence of longitudinal polarization using hydrodynamic and transport models.","All these models predict a maximum longitudinal polarization in mid-central collisions around 30-50 \\% centrality at $p_{\\rm T} \\approx$ 2.0 - 3.0 GeV/c. These findings on longitudinal polarization advocate the existence of a thermal medium in non-central heavy-ion collisions.","Our findings are in agreement with corresponding experimental data at the RHIC and LHC energies."],"url":"http://arxiv.org/abs/2404.15138v1","category":"hep-ph"}
{"created":"2024-04-23 15:40:04","title":"QCD topology and axion properties in an isotropic hot and dense medium","abstract":"We study the QCD topology and axion properties at finite temperature and chemical potential in the framework of the two-flavor Nambu$-$Jona-Lasinio model. We find that the behaviors of the two lowest cumulants of the QCD topological charge distribution and axion properties are highly sensitive to the critical behavior of the chiral phase transition. In particular, the topological susceptibility and the axion mass follow the response of the chiral condensate to temperature and chemical potential, showing that both quantities decrease monotonically with the increment of temperature and/or chemical potential. However, it is important to note that the normalized fourth cumulant behaves differently depending on the temperature. At low temperatures, it is a non-monotonic function of the chemical potential, while at high temperatures, it monotonically decreases. Additionally, its value invariably approaches the asymptotic value of $b_2^{\\text {inst }}=-1/12$, predicted by the dilute instanton gas model. We also observe that with the increase in chemical potential at relatively low temperatures, the axion self-coupling constant exhibits a sharp peak around the critical point, which can even be more than twice its vacuum value. After that, the self-coupling drops sharply to a much lower value than its vacuum value, eventually approaching zero in the high chemical potential limit. The finding that the axion self-coupling constant is significantly enhanced in high-density environments near the chiral phase transition could lead to the creation or enhancement of an axion Bose-Einstein condensate in compact astrophysical objects.","sentences":["We study the QCD topology and axion properties at finite temperature and chemical potential in the framework of the two-flavor Nambu$-$Jona-Lasinio model.","We find that the behaviors of the two lowest cumulants of the QCD topological charge distribution and axion properties are highly sensitive to the critical behavior of the chiral phase transition.","In particular, the topological susceptibility and the axion mass follow the response of the chiral condensate to temperature and chemical potential, showing that both quantities decrease monotonically with the increment of temperature and/or chemical potential.","However, it is important to note that the normalized fourth cumulant behaves differently depending on the temperature.","At low temperatures, it is a non-monotonic function of the chemical potential, while at high temperatures, it monotonically decreases.","Additionally, its value invariably approaches the asymptotic value of $b_2^{\\text {inst }}=-1/12$, predicted by the dilute instanton gas model.","We also observe that with the increase in chemical potential at relatively low temperatures, the axion self-coupling constant exhibits a sharp peak around the critical point, which can even be more than twice its vacuum value.","After that, the self-coupling drops sharply to a much lower value than its vacuum value, eventually approaching zero in the high chemical potential limit.","The finding that the axion self-coupling constant is significantly enhanced in high-density environments near the chiral phase transition could lead to the creation or enhancement of an axion Bose-Einstein condensate in compact astrophysical objects."],"url":"http://arxiv.org/abs/2404.15136v1","category":"hep-ph"}
{"created":"2024-04-23 15:26:07","title":"Geometric scale-free random graphs on mobile vertices: broadcast and percolation times","abstract":"We study the phenomenon of information propagation on mobile geometric scale-free random graphs, where vertices instantaneously pass on information to all other vertices in the same connected component. The graphs we consider are constructed on a Poisson point process of intensity $\\lambda>0$, and the vertices move over time as simple Brownian motions on either $\\mathbb{R}^d$ or the $d$-dimensional torus of volume $n$, while edges are randomly drawn depending on the locations of the vertices, as well as their a priori assigned marks. This includes mobile versions of the age-dependent random connection model and the soft Boolean model. We show that in the ultrasmall regime of these random graphs, information is broadcast to all vertices on a torus of volume $n$ in poly-logarithmic time and that on $\\mathbb{R}^d$, the information will reach the infinite component before time $t$ with stretched exponentially high probability, for any $\\lambda>0$.","sentences":["We study the phenomenon of information propagation on mobile geometric scale-free random graphs, where vertices instantaneously pass on information to all other vertices in the same connected component.","The graphs we consider are constructed on a Poisson point process of intensity $\\lambda>0$, and the vertices move over time as simple Brownian motions on either $\\mathbb{R}^d$ or the $d$-dimensional torus of volume $n$, while edges are randomly drawn depending on the locations of the vertices, as well as their a priori assigned marks.","This includes mobile versions of the age-dependent random connection model and the soft Boolean model.","We show that in the ultrasmall regime of these random graphs, information is broadcast to all vertices on a torus of volume $n$ in poly-logarithmic time and that on $\\mathbb{R}^d$, the information will reach the infinite component before time $t$ with stretched exponentially high probability, for any $\\lambda>0$."],"url":"http://arxiv.org/abs/2404.15124v1","category":"math.PR"}
{"created":"2024-04-23 15:10:33","title":"$1/Q^2$ power corrections to TMD factorization for Drell-Yan hadronic tensor","abstract":"I calculate ${1\\over Q^2}$ power corrections to unpolarized Drell-Yan hadronic tensor for electromagnetic (EM) current at large $N_c$ and demonstrate the EM gauge invariance at this level.","sentences":["I calculate ${1\\over Q^2}$ power corrections to unpolarized Drell-Yan hadronic tensor for electromagnetic (EM) current at large $N_c$ and demonstrate the EM gauge invariance at this level."],"url":"http://arxiv.org/abs/2404.15116v1","category":"hep-ph"}
{"created":"2024-04-23 15:06:48","title":"Virtual QCD corrections to $gg \\to ZZ$: top-quark loops from a transverse-momentum expansion","abstract":"We present the virtual corrections due to the top-quark loops for the process $gg \\to ZZ$ at next-to-leading order in QCD. The associated two-loop box diagrams are computed using a small-transverse-momentum expansion. Our results are then merged with those available in the complementary energy region, obtained via a high-energy expansion, in order to provide an analytic result that is valid in the whole phase space. The results presented allow for an efficient modelling of the signal--background interference as well as the irreducible background in off-shell Higgs production.","sentences":["We present the virtual corrections due to the top-quark loops for the process $gg \\to ZZ$ at next-to-leading order in QCD.","The associated two-loop box diagrams are computed using a small-transverse-momentum expansion.","Our results are then merged with those available in the complementary energy region, obtained via a high-energy expansion, in order to provide an analytic result that is valid in the whole phase space.","The results presented allow for an efficient modelling of the signal--background interference as well as the irreducible background in off-shell Higgs production."],"url":"http://arxiv.org/abs/2404.15113v1","category":"hep-ph"}
{"created":"2024-04-23 15:02:31","title":"Virtual Takeovers in the Metaverse: Interrogating Power in Our Past and Future(s) with Multi-Layered Narratives","abstract":"Mariah is an augmented reality (AR) mobile application that exposes power structures (e.g., capitalism, patriarchy, white supremacy) through storytelling and celebrates acts of resistance against them. People can use Mariah to \"legally trespass\" the metaverse as a form of protest. Mariah provides historical context to the user's physical surroundings by superimposing images and playing stories about people who have experienced, and resisted, injustice. We share two implementations of Mariah that raise questions about free speech and property rights in the metaverse: (1) a protest against museums accepting \"dirty money\" from the opioid epidemic; and (2) a commemoration of sites where people have resisted power structures. Mariah is a case study for how experimenting with a technology in non-sanctioned ways (i.e., \"hacking\") can expose ways that it might interact with, and potentially amplify, existing power structures.","sentences":["Mariah is an augmented reality (AR) mobile application that exposes power structures (e.g., capitalism, patriarchy, white supremacy) through storytelling and celebrates acts of resistance against them.","People can use Mariah to \"legally trespass\" the metaverse as a form of protest.","Mariah provides historical context to the user's physical surroundings by superimposing images and playing stories about people who have experienced, and resisted, injustice.","We share two implementations of Mariah that raise questions about free speech and property rights in the metaverse: (1) a protest against museums accepting \"dirty money\" from the opioid epidemic; and (2) a commemoration of sites where people have resisted power structures.","Mariah is a case study for how experimenting with a technology in non-sanctioned ways (i.e., \"hacking\") can expose ways that it might interact with, and potentially amplify, existing power structures."],"url":"http://arxiv.org/abs/2404.15108v1","category":"cs.HC"}
{"created":"2024-04-23 14:37:18","title":"Detection of circular permutations by Protein Language Models","abstract":"Circular permutation in proteins is a phenomenon with significant implications in the study of protein evolution and functionality. The detection of such permutations is traditionally reliant on structural alignment methods, which, while accurate, are computationally intensive and require known three-dimensional structures. The circular permutation detection method based on sequence dependencies may be faster, but it may sacrifice some accuracy, especially for structures with low sequence similarity. To address these limitations, we introduce a novel protein language model, the plmCP, designed to detect circular permutations directly from amino acid sequences. The protein language model can help us extract structural information from sequences. Utilizing advanced embedding techniques, our model infers structural information from the linear sequence data, enabling the identification of permuted proteins with a high degree of accuracy. In case study, this model has a good result compared with traditional structural alignment methods in speed and accessibility, requiring only amino acid sequences to deliver results comparable to established techniques. In conclusion, our protein language model presents a promising tool for the efficient and accessible detection of circular permutations in proteins, with the potential to facilitate advancements in the understanding of protein evolution and engineering. The source code can be obtained from https://github.com/YueHuLab/plmCP.","sentences":["Circular permutation in proteins is a phenomenon with significant implications in the study of protein evolution and functionality.","The detection of such permutations is traditionally reliant on structural alignment methods, which, while accurate, are computationally intensive and require known three-dimensional structures.","The circular permutation detection method based on sequence dependencies may be faster, but it may sacrifice some accuracy, especially for structures with low sequence similarity.","To address these limitations, we introduce a novel protein language model, the plmCP, designed to detect circular permutations directly from amino acid sequences.","The protein language model can help us extract structural information from sequences.","Utilizing advanced embedding techniques, our model infers structural information from the linear sequence data, enabling the identification of permuted proteins with a high degree of accuracy.","In case study, this model has a good result compared with traditional structural alignment methods in speed and accessibility, requiring only amino acid sequences to deliver results comparable to established techniques.","In conclusion, our protein language model presents a promising tool for the efficient and accessible detection of circular permutations in proteins, with the potential to facilitate advancements in the understanding of protein evolution and engineering.","The source code can be obtained from https://github.com/YueHuLab/plmCP."],"url":"http://arxiv.org/abs/2404.15087v1","category":"q-bio.QM"}
{"created":"2024-04-23 14:36:38","title":"Decay $B_c^+ \\to D_{(s)}^{(*)+} \\ell^+\\ell^-$ within covariant confined quark model","abstract":"Here we study the rare decay of $B_c$ mesons within the effective field theoretical framework of covariant confined quark model. The transition form factors corresponding to $B_c^+ \\to D^{(*)+}$ and $B_c^+ \\to D_s^{(*)+}$ are computed in the entire $q^2$ range. Using form factors we compute the branching fractions and compare with the available theoretical approaches. We also compute other physical observables such as forward backward asymmetry, longitudinal and transverse polarizations and various clean angular observables.","sentences":["Here we study the rare decay of $B_c$ mesons within the effective field theoretical framework of covariant confined quark model.","The transition form factors corresponding to $B_c^+ \\to D^{(*)+}$ and $B_c^+ \\to D_s^{(*)+}$ are computed in the entire $q^2$ range.","Using form factors we compute the branching fractions and compare with the available theoretical approaches.","We also compute other physical observables such as forward backward asymmetry, longitudinal and transverse polarizations and various clean angular observables."],"url":"http://arxiv.org/abs/2404.15085v1","category":"hep-ph"}
{"created":"2024-04-23 14:25:39","title":"Explicit Entropic Proofs of Irreversibility Theorems for Holographic RG Flows","abstract":"We revisit the existence of monotonic quantities along renormalization group flows using only the Null Energy Condition and the Ryu-Takayanagi formula for the entanglement entropy of field theories with anti-de Sitter gravity duals. In particular, we consider flows within the same dimension and holographically reprove the $c$-, $F$-, and $a$-theorems in dimensions two, three, and four. We focus on the family of maximally spherical entangling surfaces, define a quasi-constant of motion corresponding to the breaking of conformal invariance, and use a properly defined distance between minimal surfaces to construct a holographic $c$-function that is monotonic along the flow. We then apply our method to the case of flows across dimensions: There, we reprove the monotonicity of flows from $\\mathrm{AdS}_{D+1}$ to $\\mathrm{AdS}_3$ and prove the novel case of flows from $\\mathrm{AdS}_5$ to $\\mathrm{AdS}_4$.","sentences":["We revisit the existence of monotonic quantities along renormalization group flows using only the Null Energy Condition and the Ryu-Takayanagi formula for the entanglement entropy of field theories with anti-de Sitter gravity duals.","In particular, we consider flows within the same dimension and holographically reprove the $c$-, $F$-, and $a$-theorems in dimensions two, three, and four.","We focus on the family of maximally spherical entangling surfaces, define a quasi-constant of motion corresponding to the breaking of conformal invariance, and use a properly defined distance between minimal surfaces to construct a holographic $c$-function that is monotonic along the flow.","We then apply our method to the case of flows across dimensions: There, we reprove the monotonicity of flows from $\\mathrm{AdS}_{D+1}$ to $\\mathrm{AdS}_3$ and prove the novel case of flows from $\\mathrm{AdS}_5$ to $\\mathrm{AdS}_4$."],"url":"http://arxiv.org/abs/2404.15077v1","category":"hep-th"}
{"created":"2024-04-23 13:58:06","title":"Energy correlations and Planckian collisions","abstract":"Energy correlations characterize the energy flux through detectors at infinity produced in a collision event. Remarkably, in holographic conformal field theories, they probe high-energy gravitational scattering in the dual anti-de Sitter geometry. We use known properties of high-energy gravitational scattering and its unitarization to explore the leading quantum-gravity correction to the energy-energy correlator at strong coupling. We find that it includes a part originating from large impact parameter scattering that is non-analytic in the angle between detectors and is $\\log N_c$ enhanced compared to the standard $1/N_c$ perturbative expansion. It is sensitive to the full bulk geometry, including the internal manifold, providing a refined probe of the emergent holographic spacetime. Similarly, scattering at small impact parameters leads to contributions that are further enhanced by extra powers of the 't Hooft coupling assuming it is corrected by stringy effects. We conclude that energy correlations are sensitive to the UV properties of the dual gravitational theory and thus provide a promising target for the conformal bootstrap.","sentences":["Energy correlations characterize the energy flux through detectors at infinity produced in a collision event.","Remarkably, in holographic conformal field theories, they probe high-energy gravitational scattering in the dual anti-de Sitter geometry.","We use known properties of high-energy gravitational scattering and its unitarization to explore the leading quantum-gravity correction to the energy-energy correlator at strong coupling.","We find that it includes a part originating from large impact parameter scattering that is non-analytic in the angle between detectors and is $\\log N_c$ enhanced compared to the standard $1/N_c$ perturbative expansion.","It is sensitive to the full bulk geometry, including the internal manifold, providing a refined probe of the emergent holographic spacetime.","Similarly, scattering at small impact parameters leads to contributions that are further enhanced by extra powers of the 't Hooft coupling assuming it is corrected by stringy effects.","We conclude that energy correlations are sensitive to the UV properties of the dual gravitational theory and thus provide a promising target for the conformal bootstrap."],"url":"http://arxiv.org/abs/2404.15056v1","category":"hep-th"}
{"created":"2024-04-23 13:42:41","title":"Confronting Dark Matter Capture Rate with Continuous Gravitational Wave Probe of Local Neutron Stars","abstract":"Continuous gravitational waves (CGWs) from various astrophysical sources are one of the many future probes of upcoming Gravitational Wave (GW) search missions. Neutron stars (NSs) with deformity are one of the leading sources of CGW emissions. In this work, for the first time, a novel attempt to estimate the dark matter (DM) capture rate is performed using CGW as the probe to the local NS population. Competitive bounds on DM capture from the local NS population are reported when compared with DM direct search experiments and other astrophysical observations.","sentences":["Continuous gravitational waves (CGWs) from various astrophysical sources are one of the many future probes of upcoming Gravitational Wave (GW) search missions.","Neutron stars (NSs) with deformity are one of the leading sources of CGW emissions.","In this work, for the first time, a novel attempt to estimate the dark matter (DM) capture rate is performed using CGW as the probe to the local NS population.","Competitive bounds on DM capture from the local NS population are reported when compared with DM direct search experiments and other astrophysical observations."],"url":"http://arxiv.org/abs/2404.15038v1","category":"astro-ph.HE"}
{"created":"2024-04-23 13:32:52","title":"(Non-)Vanishing of high-dimensional group cohomology","abstract":"Church-Farb-Putman formulated stability and vanishing conjectures for the high-dimensional cohomology of $\\operatorname{SL}_n(\\mathbb{Z})$, surface mapping class groups and automorphism groups of free groups. This is a survey on the current status of these conjectures and their generalisations.","sentences":["Church-Farb-Putman formulated stability and vanishing conjectures for the high-dimensional cohomology of $\\operatorname{SL}_n(\\mathbb{Z})$, surface mapping class groups and automorphism groups of free groups.","This is a survey on the current status of these conjectures and their generalisations."],"url":"http://arxiv.org/abs/2404.15026v1","category":"math.GR"}
{"created":"2024-04-23 13:32:44","title":"T-dualities in gauged linear sigma models","abstract":"We describe non-Abelian T-dualities for $\\mathcal{N} = 2$ two dimensional gauged linear sigma model (GLSM). We start with the case of left and right $(2, 2)$ supersymmetry (SUSY), $U(1)$ gauge group, and global non-Abelian symmetries. Our analysis applies to the specialization of the GLSM with the global group $SU(2)\\times SU(2)$, whose original model is the resolved conifold. We analyze the dual model and compute the periods on the dual geometry, matching the effective potential for the $U(1)$ gauge field, and discuss the coincident singularity at the conifold point. We further present the non-Abelian T-dualization of models with $(0,2)$ SUSY, analyzing the case of global symmetry $SU(2)$. In this case the full non-Abelian duality can be solved. The dual geometries with and without non-perturbative corrections to the superpotential coincide. The structure of the dual non-perturbative corrections employed are determined based on symmetry arguments. The non-Abelian T-dualities reviewed here lead to potential new symmetries between physical theories.","sentences":["We describe non-Abelian T-dualities for $\\mathcal{N} = 2$ two dimensional gauged linear sigma model (GLSM).","We start with the case of left and right $(2, 2)$ supersymmetry (SUSY), $U(1)$ gauge group, and global non-Abelian symmetries.","Our analysis applies to the specialization of the GLSM with the global group $SU(2)\\times SU(2)$, whose original model is the resolved conifold.","We analyze the dual model and compute the periods on the dual geometry, matching the effective potential for the $U(1)$ gauge field, and discuss the coincident singularity at the conifold point.","We further present the non-Abelian T-dualization of models with $(0,2)$ SUSY, analyzing the case of global symmetry $SU(2)$.","In this case the full non-Abelian duality can be solved.","The dual geometries with and without non-perturbative corrections to the superpotential coincide.","The structure of the dual non-perturbative corrections employed are determined based on symmetry arguments.","The non-Abelian T-dualities reviewed here lead to potential new symmetries between physical theories."],"url":"http://arxiv.org/abs/2404.15025v1","category":"hep-th"}
{"created":"2024-04-23 13:26:03","title":"Higher-magnesium-doping effects on the singlet ground state of the Shastry-Sutherland SrCu2(BO3)2","abstract":"Doping of quantum antiferromagnets is an established approach to investigate the robustness of their ground state against the competing phases. Predictions of doping effects on the ground state of the Shastry-Sutherland dimer model are here verified experimentally on Mg-doped SrCu2(BO3)2. A partial incorporation of Mg2+ on the Cu2+-site in the SrCu2(BO3)2 structure leads to a subtle but systematic lattice expansion with the increasing Mg-doping concentration, which is accompanied by a concomitant decrease in the spin gap, the Curie-Weiss temperature and the peak temperature of the susceptibility. These findings indicate a doping-induced breaking of Cu2+ spin-1/2 dimers which is also corroborated by X-band EPR spectroscopy that points to a systematic increase in intensity of free Cu2+ sites with increasing Mg-doping concentration. Extending the Mg-doping up to nominal x = 0.10 or SrCu1.9Mg0.1(BO3)2, in the magnetisation measurements taken up to 35 T, a suppression of the pseudo-1/8 plateau is found along with a clear presence of an anomaly at an onset critical field H'C0 ~ 9 T. The latter, absent in pure SrCu2(BO3)2, emerges due to the coupling of liberated Cu2+ spin-1/2 entities in the vicinity of Mg-doping induced impurities.","sentences":["Doping of quantum antiferromagnets is an established approach to investigate the robustness of their ground state against the competing phases.","Predictions of doping effects on the ground state of the Shastry-Sutherland dimer model are here verified experimentally on Mg-doped SrCu2(BO3)2.","A partial incorporation of Mg2+ on the Cu2+-site in the SrCu2(BO3)2 structure leads to a subtle but systematic lattice expansion with the increasing Mg-doping concentration, which is accompanied by a concomitant decrease in the spin gap, the Curie-Weiss temperature and the peak temperature of the susceptibility.","These findings indicate a doping-induced breaking of Cu2+ spin-1/2 dimers which is also corroborated by X-band EPR spectroscopy that points to a systematic increase in intensity of free Cu2+ sites with increasing Mg-doping concentration.","Extending the Mg-doping up to nominal x = 0.10 or SrCu1.9Mg0.1(BO3)2, in the magnetisation measurements taken up to 35 T, a suppression of the pseudo-1/8 plateau is found along with a clear presence of an anomaly at an onset critical field H'C0 ~ 9","T. The latter, absent in pure SrCu2(BO3)2, emerges due to the coupling of liberated Cu2+ spin-1/2 entities in the vicinity of Mg-doping induced impurities."],"url":"http://arxiv.org/abs/2404.15021v1","category":"cond-mat.str-el"}
{"created":"2024-04-23 13:00:58","title":"Invariant tensions from holography","abstract":"We revisit the problem of defining an invariant notion of tension in gravity. For spacetimes whose asymptotics are those of a Defect CFT we propose two independent definitions : Gravitational tension given by the one-point function of the dilatation current, and inertial tension, or stiffness, given by the norm of the displacement operator. We show that both reduce to the tension of the Nambu-Goto action in the limit of classical thin probe branes. Subtle normalisations of the relevant Witten diagrams are fixed by the Weyl and diffeomorphism Ward identities of the boundary DCFT. The gravitational tension is not defined for domain walls, whereas stiffness is not defined for point particles. When they both exist these two tensions are in general different, but the examples of line and surface BPS defects in $d=4$ show that superconformal invariance can identify them.","sentences":["We revisit the problem of defining an invariant notion of tension in gravity.","For spacetimes whose asymptotics are those of a Defect CFT we propose two independent definitions : Gravitational tension given by the one-point function of the dilatation current, and inertial tension, or stiffness, given by the norm of the displacement operator.","We show that both reduce to the tension of the Nambu-Goto action in the limit of classical thin probe branes.","Subtle normalisations of the relevant Witten diagrams are fixed by the Weyl and diffeomorphism Ward identities of the boundary DCFT.","The gravitational tension is not defined for domain walls, whereas stiffness is not defined for point particles.","When they both exist these two tensions are in general different, but the examples of line and surface BPS defects in $d=4$ show that superconformal invariance can identify them."],"url":"http://arxiv.org/abs/2404.14998v1","category":"hep-th"}
{"created":"2024-04-23 12:57:35","title":"CA-Stream: Attention-based pooling for interpretable image recognition","abstract":"Explanations obtained from transformer-based architectures in the form of raw attention, can be seen as a class-agnostic saliency map. Additionally, attention-based pooling serves as a form of masking the in feature space. Motivated by this observation, we design an attention-based pooling mechanism intended to replace Global Average Pooling (GAP) at inference. This mechanism, called Cross-Attention Stream (CA-Stream), comprises a stream of cross attention blocks interacting with features at different network depths. CA-Stream enhances interpretability in models, while preserving recognition performance.","sentences":["Explanations obtained from transformer-based architectures in the form of raw attention, can be seen as a class-agnostic saliency map.","Additionally, attention-based pooling serves as a form of masking the in feature space.","Motivated by this observation, we design an attention-based pooling mechanism intended to replace Global Average Pooling (GAP) at inference.","This mechanism, called Cross-Attention Stream (CA-Stream), comprises a stream of cross attention blocks interacting with features at different network depths.","CA-Stream enhances interpretability in models, while preserving recognition performance."],"url":"http://arxiv.org/abs/2404.14996v1","category":"cs.CV"}
{"created":"2024-04-23 12:53:14","title":"Solar flare observations with the Radio Neutrino Observatory Greenland (RNO-G)","abstract":"The science program of the Radio Neutrino Observatory-Greenland (RNO-G) extends beyond particle astrophysics to include radioglaciology and, as we show herein, solar physics, as well. Impulsive solar flare observations not only permit direct measurements of light curves, spectral content, and polarization on time scales significantly shorter than most extant dedicated solar observatories, but also offer an extremely useful above-surface calibration source, with pointing precision of order tens of arc-minutes. Using the early RNO-G data from 2022-2023, observed flare characteristics are compared to well-established solar observatories. Also, a number of individual flares are used to highlight angular reconstruction and calibration methods. RNO-G observes signal excesses during solar flares reported by the solar-observing Callisto network and in coincidence with about 60% of the brightest excesses recorded by the SWAVES satellite, when the Sun is above the horizon for RNO-G. In these observed flares, there is significant impulsivity in the time-domain. In addition, the solar flares are used to calibrate the RNO-G absolute pointing on the radio signal arrival direction to sub-degree resolution.","sentences":["The science program of the Radio Neutrino Observatory-Greenland (RNO-G) extends beyond particle astrophysics to include radioglaciology and, as we show herein, solar physics, as well.","Impulsive solar flare observations not only permit direct measurements of light curves, spectral content, and polarization on time scales significantly shorter than most extant dedicated solar observatories, but also offer an extremely useful above-surface calibration source, with pointing precision of order tens of arc-minutes.","Using the early RNO-G data from 2022-2023, observed flare characteristics are compared to well-established solar observatories.","Also, a number of individual flares are used to highlight angular reconstruction and calibration methods.","RNO-G observes signal excesses during solar flares reported by the solar-observing Callisto network and in coincidence with about 60% of the brightest excesses recorded by the SWAVES satellite, when the Sun is above the horizon for RNO-G. In these observed flares, there is significant impulsivity in the time-domain.","In addition, the solar flares are used to calibrate the RNO-G absolute pointing on the radio signal arrival direction to sub-degree resolution."],"url":"http://arxiv.org/abs/2404.14995v1","category":"astro-ph.SR"}
{"created":"2024-04-23 12:30:17","title":"CAGE: Circumplex Affect Guided Expression Inference","abstract":"Understanding emotions and expressions is a task of interest across multiple disciplines, especially for improving user experiences. Contrary to the common perception, it has been shown that emotions are not discrete entities but instead exist along a continuum. People understand discrete emotions differently due to a variety of factors, including cultural background, individual experiences, and cognitive biases. Therefore, most approaches to expression understanding, particularly those relying on discrete categories, are inherently biased. In this paper, we present a comparative in-depth analysis of two common datasets (AffectNet and EMOTIC) equipped with the components of the circumplex model of affect. Further, we propose a model for the prediction of facial expressions tailored for lightweight applications. Using a small-scaled MaxViT-based model architecture, we evaluate the impact of discrete expression category labels in training with the continuous valence and arousal labels. We show that considering valence and arousal in addition to discrete category labels helps to significantly improve expression inference. The proposed model outperforms the current state-of-the-art models on AffectNet, establishing it as the best-performing model for inferring valence and arousal achieving a 7% lower RMSE. Training scripts and trained weights to reproduce our results can be found here: https://github.com/wagner-niklas/CAGE_expression_inference.","sentences":["Understanding emotions and expressions is a task of interest across multiple disciplines, especially for improving user experiences.","Contrary to the common perception, it has been shown that emotions are not discrete entities but instead exist along a continuum.","People understand discrete emotions differently due to a variety of factors, including cultural background, individual experiences, and cognitive biases.","Therefore, most approaches to expression understanding, particularly those relying on discrete categories, are inherently biased.","In this paper, we present a comparative in-depth analysis of two common datasets (AffectNet and EMOTIC) equipped with the components of the circumplex model of affect.","Further, we propose a model for the prediction of facial expressions tailored for lightweight applications.","Using a small-scaled MaxViT-based model architecture, we evaluate the impact of discrete expression category labels in training with the continuous valence and arousal labels.","We show that considering valence and arousal in addition to discrete category labels helps to significantly improve expression inference.","The proposed model outperforms the current state-of-the-art models on AffectNet, establishing it as the best-performing model for inferring valence and arousal achieving a 7% lower RMSE.","Training scripts and trained weights to reproduce our results can be found here: https://github.com/wagner-niklas/CAGE_expression_inference."],"url":"http://arxiv.org/abs/2404.14975v1","category":"cs.CV"}
{"created":"2024-04-23 11:32:12","title":"Defective and Clustered Colouring of Graphs with Given Girth","abstract":"The defective chromatic number of a graph class $\\mathcal{G}$ is the minimum integer $k$ such that for some integer $d$, every graph in $\\mathcal{G}$ is $k$-colourable such that each monochromatic component has maximum degree at most $d$. Similarly, the clustered chromatic number of a graph class $\\mathcal{G}$ is the minimum integer $k$ such that for some integer $c$, every graph in $\\mathcal{G}$ is $k$-colourable such that each monochromatic component has at most $c$ vertices. This paper determines or establishes bounds on the defective and clustered chromatic numbers of graphs with given girth in minor-closed classes defined by the following parameters: Hadwiger number, treewidth, pathwidth, treedepth, circumference, and feedback vertex number. One striking result is that for any integer $k$, for the class of triangle-free graphs with treewidth $k$, the defective chromatic number, clustered chromatic number and chromatic number are all equal. The same result holds for graphs with treedepth $k$, and generalises for graphs with no $K_p$ subgraph. We also show, via a result of K\\\"{u}hn and Osthus~[2003], that $K_t$-minor-free graphs with girth $g\\geq 5$ are properly $O(t^{c_g})$ colourable, where $c_g\\in(0,1)$ with $c_g\\to 0$, thus asymptotically improving on Hadwiger's Conjecture.","sentences":["The defective chromatic number of a graph class $\\mathcal{G}$ is the minimum integer $k$ such that for some integer $d$, every graph in $\\mathcal{G}$ is $k$-colourable such that each monochromatic component has maximum degree at most $d$. Similarly, the clustered chromatic number of a graph class $\\mathcal{G}$ is the minimum integer $k$ such that for some integer $c$, every graph in $\\mathcal{G}$ is $k$-colourable such that each monochromatic component has at most $c$ vertices.","This paper determines or establishes bounds on the defective and clustered chromatic numbers of graphs with given girth in minor-closed classes defined by the following parameters: Hadwiger number, treewidth, pathwidth, treedepth, circumference, and feedback vertex number.","One striking result is that for any integer $k$, for the class of triangle-free graphs with treewidth $k$, the defective chromatic number, clustered chromatic number and chromatic number are all equal.","The same result holds for graphs with treedepth $k$, and generalises for graphs with no $K_p$ subgraph.","We also show, via a result of K\\\"{u}hn and Osthus~[2003], that $K_t$-minor-free graphs with girth $g\\geq 5$ are properly $O(t^{c_g})$ colourable, where $c_g\\in(0,1)$ with $c_g\\to 0$, thus asymptotically improving on Hadwiger's Conjecture."],"url":"http://arxiv.org/abs/2404.14940v1","category":"math.CO"}
{"created":"2024-04-23 11:31:25","title":"Derived functors and Hilbert polynomials over hypersurface rings","abstract":"Let $(A,\\mathfrak{m})$ be a hypersurface local ring of dimension $d \\geq 1$ and let $I$ be an $\\mathfrak{m}$-primary ideal. We show that there is a non-negative integer $r_I$ (depending only on $I$) such that if $M$ is any non-free maximal Cohen-Macaulay $A$-module the function $n \\rightarrow \\ell(Tor^A_1(M, A/I^{n+1}))$ (which is of polynomial type) has degree $r_I$. Analogous results hold for Hilbert polynomials associated to Ext-functors. Surprisingly a key ingredient is the classification of thick subcategories of the stable category of MCM $A$-modules (obtained by Takahashi).","sentences":["Let $(A,\\mathfrak{m})$ be a hypersurface local ring of dimension $d \\geq 1$ and let $I$ be an $\\mathfrak{m}$-primary ideal.","We show that there is a non-negative integer $r_I$ (depending only on $I$) such that if $M$ is any non-free maximal Cohen-Macaulay $A$-module the function $n \\rightarrow \\ell(Tor^A_1(M, A/I^{n+1}))$ (which is of polynomial type) has degree $r_I$. Analogous results hold for Hilbert polynomials associated to Ext-functors.","Surprisingly a key ingredient is the classification of thick subcategories of the stable category of MCM $A$-modules (obtained by Takahashi)."],"url":"http://arxiv.org/abs/2404.14938v1","category":"math.AC"}
{"created":"2024-04-23 11:25:04","title":"Scaling laws for Rayleigh-B\u00e9nard convection between Navier-slip boundaries","abstract":"We consider the two-dimensional Rayeigh-B\\'enard convection problem between Navier-slip fixed-temperature boundary conditions and present a new upper bound for the Nusselt number. The result, based on a localization principle for the Nusselt number and an interpolation bound, exploits the regularity of the flow. On one hand our method yields a shorter proof of the celebrated result in Whitehead & Doering (2011) in the case of free-slip boundary conditions. On the other hand, its combination with a new, refined estimate for the pressure gives a substantial improvement of the interpolation bounds in Drivas et al. (2022) for slippery boundaries. A rich description of the scaling behaviour arises from our result: depending on the magnitude of the slip-length and on the Prandtl number, our upper bounds indicate five possible scaling laws: $\\textit{Nu} \\sim (L_s^{-1}\\textit{Ra})^{\\frac{1}{3}}$, $\\textit{Nu} \\sim (L_s^{-\\frac{2}{5}}\\textit{Ra})^{\\frac{5}{13}}$, $\\textit{Nu} \\sim \\textit{Ra}^{\\frac{5}{12}}$, $\\textit{Nu} \\sim \\textit{Pr}^{-\\frac{1}{6}} (L_s^{-\\frac{4}{3}}\\textit{Ra})^{\\frac{1}{2}}$ and $\\textit{Nu} \\sim \\textit{Pr}^{-\\frac{1}{6}} (L_s^{-\\frac{1}{3}}\\textit{Ra})^{\\frac{1}{2}}$","sentences":["We consider the two-dimensional Rayeigh-B\\'enard convection problem between Navier-slip fixed-temperature boundary conditions and present a new upper bound for the Nusselt number.","The result, based on a localization principle for the Nusselt number and an interpolation bound, exploits the regularity of the flow.","On one hand our method yields a shorter proof of the celebrated result in Whitehead & Doering (2011) in the case of free-slip boundary conditions.","On the other hand, its combination with a new, refined estimate for the pressure gives a substantial improvement of the interpolation bounds in Drivas et al. (2022) for slippery boundaries.","A rich description of the scaling behaviour arises from our result: depending on the magnitude of the slip-length and on the Prandtl number, our upper bounds indicate five possible scaling laws: $\\textit{Nu} \\sim (L_s^{-1}\\textit{Ra})^{\\frac{1}{3}}$, $\\textit{Nu} \\sim (L_s^{-\\frac{2}{5}}\\textit{Ra})^{\\frac{5}{13}}$, $\\textit{Nu} \\sim \\textit{Ra}^{\\frac{5}{12}}$, $\\textit{Nu} \\sim \\textit{Pr}^{-\\frac{1}{6}} (L_s^{-\\frac{4}{3}}\\textit{Ra})^{\\frac{1}{2}}$ and $\\textit{Nu} \\sim \\textit{Pr}^{-\\frac{1}{6}} (L_s^{-\\frac{1}{3}}\\textit{Ra})^{\\frac{1}{2}}$"],"url":"http://arxiv.org/abs/2404.14936v1","category":"math.AP"}
{"created":"2024-04-23 10:54:39","title":"Photoinduced Spin Centers in Photocatalytic Metal-Organic Framework UiO-66","abstract":"Metal-organic frameworks (MOFs) are promising candidates for the advanced photocatalytic active materials. These porous crystalline compounds have large active surface area and structural tunability, highly competitive with oxides, the well-established material class for the photocatalysis. However, due to their complex organic and coordination chemistry composition, photophysical mechanisms involved in the photocatalytic process in MOFs are still not well understood. Employing electron paramagnetic resonance (EPR) spectroscopy, we investigate the fundamental processes of electron and hole generation, as well as capture events that lead to the formation of various radical species in UiO-66, an archetypical MOF photocatalyst. As a result, we detected a manifold of photoinduced electron spin centers, which we subsequently analyzed and identified with the help of density-functional theory (DFT) calculations. Our findings provide new insights into the photo-induced charge transfer processes, which are the basis of photocatalytic activity in UiO-66. This sets the stage for further studies on photogenerated spin centers in this and similar MOF materials.","sentences":["Metal-organic frameworks (MOFs) are promising candidates for the advanced photocatalytic active materials.","These porous crystalline compounds have large active surface area and structural tunability, highly competitive with oxides, the well-established material class for the photocatalysis.","However, due to their complex organic and coordination chemistry composition, photophysical mechanisms involved in the photocatalytic process in MOFs are still not well understood.","Employing electron paramagnetic resonance (EPR) spectroscopy, we investigate the fundamental processes of electron and hole generation, as well as capture events that lead to the formation of various radical species in UiO-66, an archetypical MOF photocatalyst.","As a result, we detected a manifold of photoinduced electron spin centers, which we subsequently analyzed and identified with the help of density-functional theory (DFT) calculations.","Our findings provide new insights into the photo-induced charge transfer processes, which are the basis of photocatalytic activity in UiO-66.","This sets the stage for further studies on photogenerated spin centers in this and similar MOF materials."],"url":"http://arxiv.org/abs/2404.14911v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-23 10:36:23","title":"Multi-Sample Dynamic Time Warping for Few-Shot Keyword Spotting","abstract":"In multi-sample keyword spotting, each keyword class is represented by multiple spoken instances, called samples. A na\\\"ive approach to detect keywords in a target sequence consists of querying all samples of all classes using sub-sequence dynamic time warping. However, the resulting processing time increases linearly with respect to the number of samples belonging to each class. Alternatively, only a single Fr\\'echet mean can be queried for each class, resulting in reduced processing time but usually also in worse detection performance as the variability of the query samples is not captured sufficiently well. In this work, multi-sample dynamic time warping is proposed to compute class-specific cost-tensors that include the variability of all query samples. To significantly reduce the computational complexity during inference, these cost tensors are converted to cost matrices before applying dynamic time warping. In experimental evaluations for few-shot keyword spotting, it is shown that this method yields a very similar performance as using all individual query samples as templates while having a runtime that is only slightly slower than when using Fr\\'echet means.","sentences":["In multi-sample keyword spotting, each keyword class is represented by multiple spoken instances, called samples.","A na\\\"ive approach to detect keywords in a target sequence consists of querying all samples of all classes using sub-sequence dynamic time warping.","However, the resulting processing time increases linearly with respect to the number of samples belonging to each class.","Alternatively, only a single Fr\\'echet mean can be queried for each class, resulting in reduced processing time but usually also in worse detection performance as the variability of the query samples is not captured sufficiently well.","In this work, multi-sample dynamic time warping is proposed to compute class-specific cost-tensors that include the variability of all query samples.","To significantly reduce the computational complexity during inference, these cost tensors are converted to cost matrices before applying dynamic time warping.","In experimental evaluations for few-shot keyword spotting, it is shown that this method yields a very similar performance as using all individual query samples as templates while having a runtime that is only slightly slower than when using Fr\\'echet means."],"url":"http://arxiv.org/abs/2404.14903v1","category":"eess.AS"}
{"created":"2024-04-23 10:33:22","title":"Entanglement measures of Majorana bound states","abstract":"Majorana bound states emerge in topological superconductors as zero-energy edge states exhibiting spatial nonlocality. Despite the enormous advances, the detection of Majorana bound states is still challenging mainly because topologically trivial Andreev bound states produce similar signatures. In this work we consider a topological superconductor with Majorana bound states coupled to quantum dots and investigate the dynamics of their quantum correlations with the aim to explore their entanglement properties. In particular, we characterize entanglement by using concurrence and discord, which are also complemented by the entanglement dynamics and return probability. We find that Majorana bound states at truly zero energy can transform an initially entangled system into its classical state, while they can create maximally entangled states at a finite energy overlap. Interestingly, we show that the system can generate a maximally entangled state between MBSs and a quantum dot by simply controlling the Majorana nonlocality. We demonstrate that these results hold in the scenarios when the initial state is either maximally entangled or separable, albeit in the latter maximally entangled states are achieved in the long time dynamics. Furthermore, we contrast our findings with those produced by a regular fermion and obtain very distinct entanglement signatures. Our work offers an alternative approach to characterize Majorana bound states, which can be also useful towards their utilization for quantum information tasks.","sentences":["Majorana bound states emerge in topological superconductors as zero-energy edge states exhibiting spatial nonlocality.","Despite the enormous advances, the detection of Majorana bound states is still challenging mainly because topologically trivial Andreev bound states produce similar signatures.","In this work we consider a topological superconductor with Majorana bound states coupled to quantum dots and investigate the dynamics of their quantum correlations with the aim to explore their entanglement properties.","In particular, we characterize entanglement by using concurrence and discord, which are also complemented by the entanglement dynamics and return probability.","We find that Majorana bound states at truly zero energy can transform an initially entangled system into its classical state, while they can create maximally entangled states at a finite energy overlap.","Interestingly, we show that the system can generate a maximally entangled state between MBSs and a quantum dot by simply controlling the Majorana nonlocality.","We demonstrate that these results hold in the scenarios when the initial state is either maximally entangled or separable, albeit in the latter maximally entangled states are achieved in the long time dynamics.","Furthermore, we contrast our findings with those produced by a regular fermion and obtain very distinct entanglement signatures.","Our work offers an alternative approach to characterize Majorana bound states, which can be also useful towards their utilization for quantum information tasks."],"url":"http://arxiv.org/abs/2404.14900v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-23 10:29:18","title":"Initialization of Majorana qubits by parity-to-magnetism conversion","abstract":"Initializing the ground state of a quantum bit (qubit) based on Majorana zero modes is one of the most pressing issues for future topological quantum computers. We explore a parity-to-magnetism protocol for initializing such topological qubits based on magnet-superconductor hybrid networks by coupling magnetic chains to a single molecule magnet. The coupling can be activated by switching the spin state of the molecule, allowing the ground state parity of the chain to be controlled. We demonstrate that initialization with either parity for a Majorana qubit can be achieved. We then introduce the dressed Majorana qubit, which includes the state of the molecule in the definition of the logical qubit. Using this definition we can initialize a qubit without high-precision timing.","sentences":["Initializing the ground state of a quantum bit (qubit) based on Majorana zero modes is one of the most pressing issues for future topological quantum computers.","We explore a parity-to-magnetism protocol for initializing such topological qubits based on magnet-superconductor hybrid networks by coupling magnetic chains to a single molecule magnet.","The coupling can be activated by switching the spin state of the molecule, allowing the ground state parity of the chain to be controlled.","We demonstrate that initialization with either parity for a Majorana qubit can be achieved.","We then introduce the dressed Majorana qubit, which includes the state of the molecule in the definition of the logical qubit.","Using this definition we can initialize a qubit without high-precision timing."],"url":"http://arxiv.org/abs/2404.14899v1","category":"cond-mat.supr-con"}
{"created":"2024-04-23 10:22:40","title":"Observation of Hilbert-space fragmentation and fractonic excitations in two-dimensional Hubbard systems","abstract":"The relaxation behaviour of isolated quantum systems taken out of equilibrium is among the most intriguing questions in many-body physics. Quantum systems out of equilibrium typically relax to thermal equilibrium states by scrambling local information and building up entanglement entropy. However, kinetic constraints in the Hamiltonian can lead to a breakdown of this fundamental paradigm due to a fragmentation of the underlying Hilbert space into dynamically decoupled subsectors in which thermalisation can be strongly suppressed. Here, we experimentally observe Hilbert space fragmentation (HSF) in a two-dimensional tilted Bose-Hubbard model. Using quantum gas microscopy, we engineer a wide variety of initial states and find a rich set of manifestations of HSF involving bulk states, interfaces and defects, i.e., d = 2, 1 and 0 dimensional objects. Specifically, uniform initial states with equal particle number and energy differ strikingly in their relaxation dynamics. Inserting controlled defects on top of a global, non-thermalising chequerboard state, we observe highly anisotropic, sub-dimensional dynamics, an immediate signature of their fractonic nature. An interface between localized and thermalising states in turn displays dynamics depending on its orientation. Our results mark the first observation of HSF beyond one dimension, as well as the concomitant direct observation of fractons, and pave the way for in-depth studies of microscopic transport phenomena in constrained systems","sentences":["The relaxation behaviour of isolated quantum systems taken out of equilibrium is among the most intriguing questions in many-body physics.","Quantum systems out of equilibrium typically relax to thermal equilibrium states by scrambling local information and building up entanglement entropy.","However, kinetic constraints in the Hamiltonian can lead to a breakdown of this fundamental paradigm due to a fragmentation of the underlying Hilbert space into dynamically decoupled subsectors in which thermalisation can be strongly suppressed.","Here, we experimentally observe Hilbert space fragmentation (HSF) in a two-dimensional tilted Bose-Hubbard model.","Using quantum gas microscopy, we engineer a wide variety of initial states and find a rich set of manifestations of HSF involving bulk states, interfaces and defects, i.e., d = 2, 1 and 0 dimensional objects.","Specifically, uniform initial states with equal particle number and energy differ strikingly in their relaxation dynamics.","Inserting controlled defects on top of a global, non-thermalising chequerboard state, we observe highly anisotropic, sub-dimensional dynamics, an immediate signature of their fractonic nature.","An interface between localized and thermalising states in turn displays dynamics depending on its orientation.","Our results mark the first observation of HSF beyond one dimension, as well as the concomitant direct observation of fractons, and pave the way for in-depth studies of microscopic transport phenomena in constrained systems"],"url":"http://arxiv.org/abs/2404.14896v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-23 10:08:08","title":"Nonsingular, lump-like, scalar compact objects in $(2+1)$-dimensional Einstein gravity","abstract":"We study the space-time geometry generated by coupling a free scalar field with a non-canonical kinetic term to General Relativity in $(2+1)$ dimensions. After identifying a family of scalar Lagrangians that yield exact analytical solutions in static and circularly symmetric scenarios, we classify the various types of solutions and focus on a branch that yields asymptotically flat geometries. We show that the solutions within such a branch can be divided in two types, namely, naked singularities and nonsingular objects without a center. In the latter, the energy density is localized around a maximum and vanishes only at infinity and at an inner boundary. This boundary has vanishing curvatures and cannot be reached by any time-like or null geodesic in finite affine time. This allows us to consistently interpret such solutions as nonsingular, lump-like, static compact scalar objects, whose eventual extension to the $(3+1)$-dimensional context could provide structures of astrophysical interest.","sentences":["We study the space-time geometry generated by coupling a free scalar field with a non-canonical kinetic term to General Relativity in $(2+1)$ dimensions.","After identifying a family of scalar Lagrangians that yield exact analytical solutions in static and circularly symmetric scenarios, we classify the various types of solutions and focus on a branch that yields asymptotically flat geometries.","We show that the solutions within such a branch can be divided in two types, namely, naked singularities and nonsingular objects without a center.","In the latter, the energy density is localized around a maximum and vanishes only at infinity and at an inner boundary.","This boundary has vanishing curvatures and cannot be reached by any time-like or null geodesic in finite affine time.","This allows us to consistently interpret such solutions as nonsingular, lump-like, static compact scalar objects, whose eventual extension to the $(3+1)$-dimensional context could provide structures of astrophysical interest."],"url":"http://arxiv.org/abs/2404.14881v1","category":"gr-qc"}
{"created":"2024-04-23 10:01:54","title":"Scalability and Implementation Aspects of Cell-Free Massive MIMO for ISAC","abstract":"This paper addresses the problem of scalability for a cell-free massive MIMO (CF-mMIMO) system performing Integrated Sensing and Communications (ISAC). Specifically, the case in which a large number of access points (APs) are deployed to perform simultaneous communication with mobile users and surveillance of the surrounding environment in the same time-frequency slot is considered, and a target-centric approach on top of the user-centric approach used for communication services is introduced. Consideration of other practical aspects such as the fronthaul load and scanning protocol issues are also treated in the paper. The proposed scalable ISAC-enabled system has lower levels of system complexity, permits to manage the case in which multiple targets are to be tracked/sensed, and achieves performance levels superior or in some cases close to those of the non-scalable solutions.","sentences":["This paper addresses the problem of scalability for a cell-free massive MIMO (CF-mMIMO) system performing Integrated Sensing and Communications (ISAC).","Specifically, the case in which a large number of access points (APs) are deployed to perform simultaneous communication with mobile users and surveillance of the surrounding environment in the same time-frequency slot is considered, and a target-centric approach on top of the user-centric approach used for communication services is introduced.","Consideration of other practical aspects such as the fronthaul load and scanning protocol issues are also treated in the paper.","The proposed scalable ISAC-enabled system has lower levels of system complexity, permits to manage the case in which multiple targets are to be tracked/sensed, and achieves performance levels superior or in some cases close to those of the non-scalable solutions."],"url":"http://arxiv.org/abs/2404.14874v1","category":"cs.IT"}
{"created":"2024-04-23 09:44:36","title":"Monte Carlo evaluation of divergent one-loop integrals without contour deformation","abstract":"Reference [1] introduces a method for computing numerically four-dimensional multi-loop integrals without performing an explicit analytic contour deformation around threshold singularities. In this paper, we extend such a technique to massless scalar one-loop integrals regularized in the framework of dimensional regularization.","sentences":["Reference [1] introduces a method for computing numerically four-dimensional multi-loop integrals without performing an explicit analytic contour deformation around threshold singularities.","In this paper, we extend such a technique to massless scalar one-loop integrals regularized in the framework of dimensional regularization."],"url":"http://arxiv.org/abs/2404.14868v1","category":"hep-ph"}
