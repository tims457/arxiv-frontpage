{"created":"2024-02-01 18:59:56","title":"AToM: Amortized Text-to-Mesh using 2D Diffusion","abstract":"We introduce Amortized Text-to-Mesh (AToM), a feed-forward text-to-mesh framework optimized across multiple text prompts simultaneously. In contrast to existing text-to-3D methods that often entail time-consuming per-prompt optimization and commonly output representations other than polygonal meshes, AToM directly generates high-quality textured meshes in less than 1 second with around 10 times reduction in the training cost, and generalizes to unseen prompts. Our key idea is a novel triplane-based text-to-mesh architecture with a two-stage amortized optimization strategy that ensures stable training and enables scalability. Through extensive experiments on various prompt benchmarks, AToM significantly outperforms state-of-the-art amortized approaches with over 4 times higher accuracy (in DF415 dataset) and produces more distinguishable and higher-quality 3D outputs. AToM demonstrates strong generalizability, offering finegrained 3D assets for unseen interpolated prompts without further optimization during inference, unlike per-prompt solutions.","sentences":["We introduce Amortized Text-to-Mesh (AToM), a feed-forward text-to-mesh framework optimized across multiple text prompts simultaneously.","In contrast to existing text-to-3D methods that often entail time-consuming per-prompt optimization and commonly output representations other than polygonal meshes, AToM directly generates high-quality textured meshes in less than 1 second with around 10 times reduction in the training cost, and generalizes to unseen prompts.","Our key idea is a novel triplane-based text-to-mesh architecture with a two-stage amortized optimization strategy that ensures stable training and enables scalability.","Through extensive experiments on various prompt benchmarks, AToM significantly outperforms state-of-the-art amortized approaches with over 4 times higher accuracy (in DF415 dataset) and produces more distinguishable and higher-quality 3D outputs.","AToM demonstrates strong generalizability, offering finegrained 3D assets for unseen interpolated prompts without further optimization during inference, unlike per-prompt solutions."],"url":"http://arxiv.org/abs/2402.00867v1","category":"cs.CV"}
{"created":"2024-02-01 18:59:35","title":"Energetic comparison of exciton gas versus electron-hole plasma in a bilayer two-dimensional electron-hole system","abstract":"We study the zero-temperature phase diagram of a symmetric electron-hole bilayer system by comparing the ground state energies of two distinct limiting cases, characterized by an electron-hole plasma or an exciton gas, respectively. For the electron-hole plasma, the random phase approximation is used; for the exciton gas, we consider three different approximations: the unscreened Coulomb interaction, the statically screened one, and the dynamically screened one under the plasmon-pole approximation. Our results suggest that the exciton gas is stable at small layer separation. However, static screening in general suppresses the formation of excitons, and dynamic screening gives different results depending on the representative energy scale we used in the plasmon-pole approximation. We conclude that energetic considerations alone are very sensitive to the approximation schemes, and the phase diagram of the system may depend crucially on exactly how the electron-hole attraction is treated in the theory. For very small and very large densities, however, all our approximations show the exciton gas to have lower energy than the plasma.","sentences":["We study the zero-temperature phase diagram of a symmetric electron-hole bilayer system by comparing the ground state energies of two distinct limiting cases, characterized by an electron-hole plasma or an exciton gas, respectively.","For the electron-hole plasma, the random phase approximation is used; for the exciton gas, we consider three different approximations: the unscreened Coulomb interaction, the statically screened one, and the dynamically screened one under the plasmon-pole approximation.","Our results suggest that the exciton gas is stable at small layer separation.","However, static screening in general suppresses the formation of excitons, and dynamic screening gives different results depending on the representative energy scale we used in the plasmon-pole approximation.","We conclude that energetic considerations alone are very sensitive to the approximation schemes, and the phase diagram of the system may depend crucially on exactly how the electron-hole attraction is treated in the theory.","For very small and very large densities, however, all our approximations show the exciton gas to have lower energy than the plasma."],"url":"http://arxiv.org/abs/2402.00866v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-01 18:59:22","title":"Towards Optimal Feature-Shaping Methods for Out-of-Distribution Detection","abstract":"Feature shaping refers to a family of methods that exhibit state-of-the-art performance for out-of-distribution (OOD) detection. These approaches manipulate the feature representation, typically from the penultimate layer of a pre-trained deep learning model, so as to better differentiate between in-distribution (ID) and OOD samples. However, existing feature-shaping methods usually employ rules manually designed for specific model architectures and OOD datasets, which consequently limit their generalization ability. To address this gap, we first formulate an abstract optimization framework for studying feature-shaping methods. We then propose a concrete reduction of the framework with a simple piecewise constant shaping function and show that existing feature-shaping methods approximate the optimal solution to the concrete optimization problem. Further, assuming that OOD data is inaccessible, we propose a formulation that yields a closed-form solution for the piecewise constant shaping function, utilizing solely the ID data. Through extensive experiments, we show that the feature-shaping function optimized by our method improves the generalization ability of OOD detection across a large variety of datasets and model architectures.","sentences":["Feature shaping refers to a family of methods that exhibit state-of-the-art performance for out-of-distribution (OOD) detection.","These approaches manipulate the feature representation, typically from the penultimate layer of a pre-trained deep learning model, so as to better differentiate between in-distribution (ID) and OOD samples.","However, existing feature-shaping methods usually employ rules manually designed for specific model architectures and OOD datasets, which consequently limit their generalization ability.","To address this gap, we first formulate an abstract optimization framework for studying feature-shaping methods.","We then propose a concrete reduction of the framework with a simple piecewise constant shaping function and show that existing feature-shaping methods approximate the optimal solution to the concrete optimization problem.","Further, assuming that OOD data is inaccessible, we propose a formulation that yields a closed-form solution for the piecewise constant shaping function, utilizing solely the ID data.","Through extensive experiments, we show that the feature-shaping function optimized by our method improves the generalization ability of OOD detection across a large variety of datasets and model architectures."],"url":"http://arxiv.org/abs/2402.00865v1","category":"cs.CV"}
{"created":"2024-02-01 18:58:07","title":"Murphy's Law for Algebraic Stacks","abstract":"We show that various natural algebro-geometric moduli stacks, including the stack of curves, have the property that every Deligne-Mumford gerbe over a field appears as the residual gerbe of one of their points. These gerbes are universal obstructions for objects of the stack to be defined over their fields of moduli, and for the corresponding coarse moduli space to be fine. Thus, our results show that many natural moduli stacks hold objects that are obstructed from being defined over their fields of moduli in every possible way, and have coarse spaces which fail to be fine moduli spaces in every possible way. A basic insight enabling our arguments is that many classical constructions in equivariant projective geometry generalize to the setting of relative geometry over an arbitrary Deligne-Mumford gerbe over a field.","sentences":["We show that various natural algebro-geometric moduli stacks, including the stack of curves, have the property that every Deligne-Mumford gerbe over a field appears as the residual gerbe of one of their points.","These gerbes are universal obstructions for objects of the stack to be defined over their fields of moduli, and for the corresponding coarse moduli space to be fine.","Thus, our results show that many natural moduli stacks hold objects that are obstructed from being defined over their fields of moduli in every possible way, and have coarse spaces which fail to be fine moduli spaces in every possible way.","A basic insight enabling our arguments is that many classical constructions in equivariant projective geometry generalize to the setting of relative geometry over an arbitrary Deligne-Mumford gerbe over a field."],"url":"http://arxiv.org/abs/2402.00862v1","category":"math.AG"}
{"created":"2024-02-01 18:56:18","title":"Evaluating Large Language Models for Generalization and Robustness via Data Compression","abstract":"Existing methods for evaluating large language models face challenges such as data contamination, sensitivity to prompts, and the high cost of benchmark creation. To address this, we propose a lossless data compression based evaluation approach that tests how models' predictive abilities generalize after their training cutoff. Specifically, we collect comprehensive test data spanning 83 months from 2017 to 2023 and split the data into training and testing periods according to models' training data cutoff. We measure: 1) the compression performance on the testing period as a measure of generalization on unseen data; and 2) the performance gap between the training and testing period as a measure of robustness. Our experiments test 14 representative large language models with various sizes on sources including Wikipedia, news articles, code, arXiv papers, and multi-modal data. We find that the compression rate of many models reduces significantly after their cutoff date, but models such as Mistral and Llama-2 demonstrate a good balance between performance and robustness. Results also suggest that models struggle to generalize on news and code data, but work especially well on arXiv papers. We also find the context size and tokenization implementation have a big impact of on the overall compression performance.","sentences":["Existing methods for evaluating large language models face challenges such as data contamination, sensitivity to prompts, and the high cost of benchmark creation.","To address this, we propose a lossless data compression based evaluation approach that tests how models' predictive abilities generalize after their training cutoff.","Specifically, we collect comprehensive test data spanning 83 months from 2017 to 2023 and split the data into training and testing periods according to models' training data cutoff.","We measure: 1) the compression performance on the testing period as a measure of generalization on unseen data; and 2) the performance gap between the training and testing period as a measure of robustness.","Our experiments test 14 representative large language models with various sizes on sources including Wikipedia, news articles, code, arXiv papers, and multi-modal data.","We find that the compression rate of many models reduces significantly after their cutoff date, but models such as Mistral and Llama-2 demonstrate a good balance between performance and robustness.","Results also suggest that models struggle to generalize on news and code data, but work especially well on arXiv papers.","We also find the context size and tokenization implementation have a big impact of on the overall compression performance."],"url":"http://arxiv.org/abs/2402.00861v1","category":"cs.CL"}
{"created":"2024-02-01 18:55:37","title":"Deep Room Impulse Response Completion","abstract":"Rendering immersive spatial audio in virtual reality (VR) and video games demands a fast and accurate generation of room impulse responses (RIRs) to recreate auditory environments plausibly. However, the conventional methods for simulating or measuring long RIRs are either computationally intensive or challenged by low signal-to-noise ratios. This study is propelled by the insight that direct sound and early reflections encapsulate sufficient information about room geometry and absorption characteristics. Building upon this premise, we propose a novel task termed \"RIR completion,\" aimed at synthesizing the late reverberation given only the early portion (50 ms) of the response. To this end, we introduce DECOR, Deep Exponential Completion Of Room impulse responses, a deep neural network structured as an autoencoder designed to predict multi-exponential decay envelopes of filtered noise sequences. The interpretability of DECOR's output facilitates its integration with diverse rendering techniques. The proposed method is compared against an adapted state-of-the-art network, and comparable performance shows promising results supporting the feasibility of the RIR completion task. The RIR completion can be widely adapted to enhance RIR generation tasks where fast late reverberation approximation is required.","sentences":["Rendering immersive spatial audio in virtual reality (VR) and video games demands a fast and accurate generation of room impulse responses (RIRs) to recreate auditory environments plausibly.","However, the conventional methods for simulating or measuring long RIRs are either computationally intensive or challenged by low signal-to-noise ratios.","This study is propelled by the insight that direct sound and early reflections encapsulate sufficient information about room geometry and absorption characteristics.","Building upon this premise, we propose a novel task termed \"RIR completion,\" aimed at synthesizing the late reverberation given only the early portion (50 ms) of the response.","To this end, we introduce DECOR, Deep Exponential Completion Of Room impulse responses, a deep neural network structured as an autoencoder designed to predict multi-exponential decay envelopes of filtered noise sequences.","The interpretability of DECOR's output facilitates its integration with diverse rendering techniques.","The proposed method is compared against an adapted state-of-the-art network, and comparable performance shows promising results supporting the feasibility of the RIR completion task.","The RIR completion can be widely adapted to enhance RIR generation tasks where fast late reverberation approximation is required."],"url":"http://arxiv.org/abs/2402.00859v1","category":"eess.AS"}
{"created":"2024-02-01 18:55:29","title":"Can Large Language Models Understand Context?","abstract":"Understanding context is key to understanding human language, an ability which Large Language Models (LLMs) have been increasingly seen to demonstrate to an impressive extent. However, though the evaluation of LLMs encompasses various domains within the realm of Natural Language Processing, limited attention has been paid to probing their linguistic capability of understanding contextual features. This paper introduces a context understanding benchmark by adapting existing datasets to suit the evaluation of generative models. This benchmark comprises of four distinct tasks and nine datasets, all featuring prompts designed to assess the models' ability to understand context. First, we evaluate the performance of LLMs under the in-context learning pretraining scenario. Experimental results indicate that pre-trained dense models struggle with understanding more nuanced contextual features when compared to state-of-the-art fine-tuned models. Second, as LLM compression holds growing significance in both research and real-world applications, we assess the context understanding of quantized models under in-context-learning settings. We find that 3-bit post-training quantization leads to varying degrees of performance reduction on our benchmark. We conduct an extensive analysis of these scenarios to substantiate our experimental results.","sentences":["Understanding context is key to understanding human language, an ability which Large Language Models (LLMs) have been increasingly seen to demonstrate to an impressive extent.","However, though the evaluation of LLMs encompasses various domains within the realm of Natural Language Processing, limited attention has been paid to probing their linguistic capability of understanding contextual features.","This paper introduces a context understanding benchmark by adapting existing datasets to suit the evaluation of generative models.","This benchmark comprises of four distinct tasks and nine datasets, all featuring prompts designed to assess the models' ability to understand context.","First, we evaluate the performance of LLMs under the in-context learning pretraining scenario.","Experimental results indicate that pre-trained dense models struggle with understanding more nuanced contextual features when compared to state-of-the-art fine-tuned models.","Second, as LLM compression holds growing significance in both research and real-world applications, we assess the context understanding of quantized models under in-context-learning settings.","We find that 3-bit post-training quantization leads to varying degrees of performance reduction on our benchmark.","We conduct an extensive analysis of these scenarios to substantiate our experimental results."],"url":"http://arxiv.org/abs/2402.00858v1","category":"cs.CL"}
{"created":"2024-02-01 18:50:53","title":"'Egalitarian pooling and sharing of longevity risk', a.k.a. 'The many ways to skin a tontine cat'","abstract":"There is little disagreement among insurance actuaries and financial economists about the societal benefits of longevity-risk pooling in the form of life annuities, defined benefit pensions, self-annuitization funds, and even tontine schemes. Indeed, the discounted value or cost of providing an income for life is lower -- in other words, the amount of upfront capital required to generate a similar income stream with the same level of statistical safety is lower -- when participants pool their financial resources versus going it alone. Moreover, when participants' financial circumstances and lifespans are homogenous, there is consensus on how to share the \"winnings\" among survivors, namely by distributing them equally among survivors, a.k.a. a uniform rule. Alas, what is lesser-known and much more problematic is allocating the winnings in such a pool when participants differ in wealth (contributions) and health (longevity), especially when the pools are relatively small in size. The same problems arise when viewed from the dual perspective of decentralized risk sharing (DRS). The positive correlation between health and income and the fact that wealthier participants are likely to live longer is a growing concern among pension and retirement policymakers. With that motivation in mind, this paper offers a modelling framework for distributing longevity-risk pools' income and benefits (or tontine winnings) when participants are heterogeneous. Similar to the nascent literature on decentralized risk sharing, there are several equally plausible arrangements for sharing benefits (a.k.a. \"skinning the cat\") among survivors. Moreover, the selected rule depends on the extent of social cohesion within the longevity risk pool, ranging from solidarity and altruism to pure individualism. In sum, actuarial science cannot really offer or guarantee uniqueness, only a methodology.","sentences":["There is little disagreement among insurance actuaries and financial economists about the societal benefits of longevity-risk pooling in the form of life annuities, defined benefit pensions, self-annuitization funds, and even tontine schemes.","Indeed, the discounted value or cost of providing an income for life is lower -- in other words, the amount of upfront capital required to generate a similar income stream with the same level of statistical safety is lower -- when participants pool their financial resources versus going it alone.","Moreover, when participants' financial circumstances and lifespans are homogenous, there is consensus on how to share the \"winnings\" among survivors, namely by distributing them equally among survivors, a.k.a. a uniform rule.","Alas, what is lesser-known and much more problematic is allocating the winnings in such a pool when participants differ in wealth (contributions) and health (longevity), especially when the pools are relatively small in size.","The same problems arise when viewed from the dual perspective of decentralized risk sharing (DRS).","The positive correlation between health and income and the fact that wealthier participants are likely to live longer is a growing concern among pension and retirement policymakers.","With that motivation in mind, this paper offers a modelling framework for distributing longevity-risk pools' income and benefits (or tontine winnings) when participants are heterogeneous.","Similar to the nascent literature on decentralized risk sharing, there are several equally plausible arrangements for sharing benefits (a.k.a. \"skinning the cat\") among survivors.","Moreover, the selected rule depends on the extent of social cohesion within the longevity risk pool, ranging from solidarity and altruism to pure individualism.","In sum, actuarial science cannot really offer or guarantee uniqueness, only a methodology."],"url":"http://arxiv.org/abs/2402.00855v1","category":"q-fin.RM"}
{"created":"2024-02-01 18:50:50","title":"SymbolicAI: A framework for logic-based approaches combining generative models and solvers","abstract":"We introduce SymbolicAI, a versatile and modular framework employing a logic-based approach to concept learning and flow management in generative processes. SymbolicAI enables the seamless integration of generative models with a diverse range of solvers by treating large language models (LLMs) as semantic parsers that execute tasks based on both natural and formal language instructions, thus bridging the gap between symbolic reasoning and generative AI. We leverage probabilistic programming principles to tackle complex tasks, and utilize differentiable and classical programming paradigms with their respective strengths. The framework introduces a set of polymorphic, compositional, and self-referential operations for data stream manipulation, aligning LLM outputs with user objectives. As a result, we can transition between the capabilities of various foundation models endowed with zero- and few-shot learning capabilities and specialized, fine-tuned models or solvers proficient in addressing specific problems. In turn, the framework facilitates the creation and evaluation of explainable computational graphs. We conclude by introducing a quality measure and its empirical score for evaluating these computational graphs, and propose a benchmark that compares various state-of-the-art LLMs across a set of complex workflows. We refer to the empirical score as the \"Vector Embedding for Relational Trajectory Evaluation through Cross-similarity\", or VERTEX score for short. The framework codebase and benchmark are linked below.","sentences":["We introduce SymbolicAI, a versatile and modular framework employing a logic-based approach to concept learning and flow management in generative processes.","SymbolicAI enables the seamless integration of generative models with a diverse range of solvers by treating large language models (LLMs) as semantic parsers that execute tasks based on both natural and formal language instructions, thus bridging the gap between symbolic reasoning and generative AI.","We leverage probabilistic programming principles to tackle complex tasks, and utilize differentiable and classical programming paradigms with their respective strengths.","The framework introduces a set of polymorphic, compositional, and self-referential operations for data stream manipulation, aligning LLM outputs with user objectives.","As a result, we can transition between the capabilities of various foundation models endowed with zero- and few-shot learning capabilities and specialized, fine-tuned models or solvers proficient in addressing specific problems.","In turn, the framework facilitates the creation and evaluation of explainable computational graphs.","We conclude by introducing a quality measure and its empirical score for evaluating these computational graphs, and propose a benchmark that compares various state-of-the-art LLMs across a set of complex workflows.","We refer to the empirical score as the \"Vector Embedding for Relational Trajectory Evaluation through Cross-similarity\", or VERTEX score for short.","The framework codebase and benchmark are linked below."],"url":"http://arxiv.org/abs/2402.00854v1","category":"cs.LG"}
{"created":"2024-02-01 18:50:42","title":"LTAU-FF: Loss Trajectory Analysis for Uncertainty in Atomistic Force Fields","abstract":"Model ensembles are simple and effective tools for estimating the prediction uncertainty of deep learning atomistic force fields. Despite this, widespread adoption of ensemble-based uncertainty quantification (UQ) techniques is limited by the high computational costs incurred by ensembles during both training and inference. In this work we leverage the cumulative distribution functions (CDFs) of per-sample errors obtained over the course of training to efficiently represent the model ensemble, and couple them with a distance-based similarity search in the model latent space. Using these tools, we develop a simple UQ metric (which we call LTAU) that leverages the strengths of ensemble-based techniques without requiring the evaluation of multiple models during either training or inference. As an initial test, we apply our method towards estimating the epistemic uncertainty in atomistic force fields (LTAU-FF) and demonstrate that it can be easily calibrated to accurately predict test errors on multiple datasets from the literature. We then illustrate the utility of LTAU-FF in two practical applications: 1) tuning the training-validation gap for an example dataset, and 2) predicting errors in relaxation trajectories on the OC20 IS2RS task. Though in this work we focus on the use of LTAU with deep learning atomistic force fields, we emphasize that it can be readily applied to any regression task, or any ensemble-generation technique, to provide a reliable and easy-to-implement UQ metric.","sentences":["Model ensembles are simple and effective tools for estimating the prediction uncertainty of deep learning atomistic force fields.","Despite this, widespread adoption of ensemble-based uncertainty quantification (UQ) techniques is limited by the high computational costs incurred by ensembles during both training and inference.","In this work we leverage the cumulative distribution functions (CDFs) of per-sample errors obtained over the course of training to efficiently represent the model ensemble, and couple them with a distance-based similarity search in the model latent space.","Using these tools, we develop a simple UQ metric (which we call LTAU) that leverages the strengths of ensemble-based techniques without requiring the evaluation of multiple models during either training or inference.","As an initial test, we apply our method towards estimating the epistemic uncertainty in atomistic force fields (LTAU-FF) and demonstrate that it can be easily calibrated to accurately predict test errors on multiple datasets from the literature.","We then illustrate the utility of LTAU-FF in two practical applications: 1) tuning the training-validation gap for an example dataset, and 2) predicting errors in relaxation trajectories on the OC20 IS2RS task.","Though in this work we focus on the use of LTAU with deep learning atomistic force fields, we emphasize that it can be readily applied to any regression task, or any ensemble-generation technique, to provide a reliable and easy-to-implement UQ metric."],"url":"http://arxiv.org/abs/2402.00853v1","category":"cs.LG"}
{"created":"2024-02-01 18:46:28","title":"Data Augmentation Scheme for Raman Spectra with Highly Correlated Annotations","abstract":"In biotechnology Raman Spectroscopy is rapidly gaining popularity as a process analytical technology (PAT) that measures cell densities, substrate- and product concentrations. As it records vibrational modes of molecules it provides that information non-invasively in a single spectrum. Typically, partial least squares (PLS) is the model of choice to infer information about variables of interest from the spectra. However, biological processes are known for their complexity where convolutional neural networks (CNN) present a powerful alternative. They can handle non-Gaussian noise and account for beam misalignment, pixel malfunctions or the presence of additional substances. However, they require a lot of data during model training, and they pick up non-linear dependencies in the process variables. In this work, we exploit the additive nature of spectra in order to generate additional data points from a given dataset that have statistically independent labels so that a network trained on such data exhibits low correlations between the model predictions. We show that training a CNN on these generated data points improves the performance on datasets where the annotations do not bear the same correlation as the dataset that was used for model training. This data augmentation technique enables us to reuse spectra as training data for new contexts that exhibit different correlations. The additional data allows for building a better and more robust model. This is of interest in scenarios where large amounts of historical data are available but are currently not used for model training. We demonstrate the capabilities of the proposed method using synthetic spectra of Ralstonia eutropha batch cultivations to monitor substrate, biomass and polyhydroxyalkanoate (PHA) biopolymer concentrations during of the experiments.","sentences":["In biotechnology Raman Spectroscopy is rapidly gaining popularity as a process analytical technology (PAT) that measures cell densities, substrate- and product concentrations.","As it records vibrational modes of molecules it provides that information non-invasively in a single spectrum.","Typically, partial least squares (PLS) is the model of choice to infer information about variables of interest from the spectra.","However, biological processes are known for their complexity where convolutional neural networks (CNN) present a powerful alternative.","They can handle non-Gaussian noise and account for beam misalignment, pixel malfunctions or the presence of additional substances.","However, they require a lot of data during model training, and they pick up non-linear dependencies in the process variables.","In this work, we exploit the additive nature of spectra in order to generate additional data points from a given dataset that have statistically independent labels so that a network trained on such data exhibits low correlations between the model predictions.","We show that training a CNN on these generated data points improves the performance on datasets where the annotations do not bear the same correlation as the dataset that was used for model training.","This data augmentation technique enables us to reuse spectra as training data for new contexts that exhibit different correlations.","The additional data allows for building a better and more robust model.","This is of interest in scenarios where large amounts of historical data are available but are currently not used for model training.","We demonstrate the capabilities of the proposed method using synthetic spectra of Ralstonia eutropha batch cultivations to monitor substrate, biomass and polyhydroxyalkanoate (PHA) biopolymer concentrations during of the experiments."],"url":"http://arxiv.org/abs/2402.00851v1","category":"cs.LG"}
{"created":"2024-02-01 18:40:08","title":"Constant Degree Direct Product Testers with Small Soundness","abstract":"Let $X$ be a $d$-dimensional simplicial complex. A function $F\\colon X(k)\\to \\{0,1\\}^k$ is said to be a direct product function if there exists a function $f\\colon X(1)\\to \\{0,1\\}$ such that $F(\\sigma) = (f(\\sigma_1), \\ldots, f(\\sigma_k))$ for each $k$-face $\\sigma$. In an effort to simplify components of the PCP theorem, Goldreich and Safra introduced the problem of direct product testing, which asks whether one can test if $F\\colon X(k)\\to \\{0,1\\}^k$ is correlated with a direct product function by querying $F$ on only $2$ inputs. Dinur and Kaufman conjectured that there exist bounded degree complexes with a direct product test in the small soundness regime. We resolve their conjecture by showing that for all $\\delta>0$, there exists a family of high-dimensional expanders with degree $O_{\\delta}(1)$ and a $2$-query direct product tester with soundness $\\delta$.   We use the characterization given by a subset of the authors and independently by Dikstein and Dinur, who showed that some form of non-Abelian coboundary expansion (which they called \"Unique-Games coboundary expansion\") is a necessary and sufficient condition for a complex to admit such direct product testers. Our main technical contribution is a general technique for showing coboundary expansion of complexes with coefficients in a non-Abelian group. This allows us to prove that the high dimensional expanders constructed by Chapman and Lubotzky satisfies the necessary conditions, thus admitting a 2-query direct product tester with small soundness.","sentences":["Let $X$ be a $d$-dimensional simplicial complex.","A function $F\\colon X(k)\\to \\{0,1\\}^k$ is said to be a direct product function if there exists a function $f\\colon X(1)\\to \\{0,1\\}$ such that $F(\\sigma) = (f(\\sigma_1), \\ldots, f(\\sigma_k))$ for each $k$-face $\\sigma$. In an effort to simplify components of the PCP theorem, Goldreich and Safra introduced the problem of direct product testing, which asks whether one can test if $F\\colon X(k)\\to \\{0,1\\}^k$ is correlated with a direct product function by querying $F$ on only $2$ inputs.","Dinur and Kaufman conjectured that there exist bounded degree complexes with a direct product test in the small soundness regime.","We resolve their conjecture by showing that for all $\\delta>0$, there exists a family of high-dimensional expanders with degree $O_{\\delta}(1)$ and a $2$-query direct product tester with soundness $\\delta$.   We use the characterization given by a subset of the authors and independently by Dikstein and Dinur, who showed that some form of non-Abelian coboundary expansion (which they called \"Unique-Games coboundary expansion\") is a necessary and sufficient condition for a complex to admit such direct product testers.","Our main technical contribution is a general technique for showing coboundary expansion of complexes with coefficients in a non-Abelian group.","This allows us to prove that the high dimensional expanders constructed by Chapman and Lubotzky satisfies the necessary conditions, thus admitting a 2-query direct product tester with small soundness."],"url":"http://arxiv.org/abs/2402.00850v1","category":"cs.CC"}
{"created":"2024-02-01 18:40:03","title":"Score-based Causal Representation Learning: Linear and General Transformations","abstract":"This paper addresses intervention-based causal representation learning (CRL) under a general nonparametric latent causal model and an unknown transformation that maps the latent variables to the observed variables. Linear and general transformations are investigated. The paper addresses both the \\emph{identifiability} and \\emph{achievability} aspects. Identifiability refers to determining algorithm-agnostic conditions that ensure recovering the true latent causal variables and the latent causal graph underlying them. Achievability refers to the algorithmic aspects and addresses designing algorithms that achieve identifiability guarantees. By drawing novel connections between \\emph{score functions} (i.e., the gradients of the logarithm of density functions) and CRL, this paper designs a \\emph{score-based class of algorithms} that ensures both identifiability and achievability. First, the paper focuses on \\emph{linear} transformations and shows that one stochastic hard intervention per node suffices to guarantee identifiability. It also provides partial identifiability guarantees for soft interventions, including identifiability up to ancestors for general causal models and perfect latent graph recovery for sufficiently non-linear causal models. Secondly, it focuses on \\emph{general} transformations and shows that two stochastic hard interventions per node suffice for identifiability. Notably, one does \\emph{not} need to know which pair of interventional environments have the same node intervened.","sentences":["This paper addresses intervention-based causal representation learning (CRL) under a general nonparametric latent causal model and an unknown transformation that maps the latent variables to the observed variables.","Linear and general transformations are investigated.","The paper addresses both the \\emph{identifiability} and \\emph{achievability} aspects.","Identifiability refers to determining algorithm-agnostic conditions that ensure recovering the true latent causal variables and the latent causal graph underlying them.","Achievability refers to the algorithmic aspects and addresses designing algorithms that achieve identifiability guarantees.","By drawing novel connections between \\emph{score functions} (i.e., the gradients of the logarithm of density functions) and CRL, this paper designs a \\emph{score-based class of algorithms} that ensures both identifiability and achievability.","First, the paper focuses on \\emph{linear} transformations and shows that one stochastic hard intervention per node suffices to guarantee identifiability.","It also provides partial identifiability guarantees for soft interventions, including identifiability up to ancestors for general causal models and perfect latent graph recovery for sufficiently non-linear causal models.","Secondly, it focuses on \\emph{general} transformations and shows that two stochastic hard interventions per node suffice for identifiability.","Notably, one does \\emph{not} need to know which pair of interventional environments have the same node intervened."],"url":"http://arxiv.org/abs/2402.00849v1","category":"cs.LG"}
{"created":"2024-02-01 18:34:32","title":"Kinematic reconstruction of torsion as dark energy in Friedmann cosmology","abstract":"In this paper we study the effects of torsion of space-time in the expansion of the universe as a candidate to dark energy. The analysis is done by reconstructing the torsion function along cosmic evolution by using observational data of Supernovae type Ia and Hubble parameter measurements. We have used a kinematic model for the parameterization of the comoving distance and the Hubble parameter, then the free parameters of the models are constrained by observational data. The reconstruction of the torsion function is obtained directly from the data, using the kinematic parameterizations, and the values for the Hubble parameter and the deceleration parameter are in good agreement to the standard model estimates.","sentences":["In this paper we study the effects of torsion of space-time in the expansion of the universe as a candidate to dark energy.","The analysis is done by reconstructing the torsion function along cosmic evolution by using observational data of Supernovae type Ia and Hubble parameter measurements.","We have used a kinematic model for the parameterization of the comoving distance and the Hubble parameter, then the free parameters of the models are constrained by observational data.","The reconstruction of the torsion function is obtained directly from the data, using the kinematic parameterizations, and the values for the Hubble parameter and the deceleration parameter are in good agreement to the standard model estimates."],"url":"http://arxiv.org/abs/2402.00844v1","category":"gr-qc"}
{"created":"2024-02-01 18:29:16","title":"X-CBA: Explainability Aided CatBoosted Anomal-E for Intrusion Detection System","abstract":"The effectiveness of Intrusion Detection Systems (IDS) is critical in an era where cyber threats are becoming increasingly complex. Machine learning (ML) and deep learning (DL) models provide an efficient and accurate solution for identifying attacks and anomalies in computer networks. However, using ML and DL models in IDS has led to a trust deficit due to their non-transparent decision-making. This transparency gap in IDS research is significant, affecting confidence and accountability. To address, this paper introduces a novel Explainable IDS approach, called X-CBA, that leverages the structural advantages of Graph Neural Networks (GNNs) to effectively process network traffic data, while also adapting a new Explainable AI (XAI) methodology. Unlike most GNN-based IDS that depend on labeled network traffic and node features, thereby overlooking critical packet-level information, our approach leverages a broader range of traffic data through network flows, including edge attributes, to improve detection capabilities and adapt to novel threats. Through empirical testing, we establish that our approach not only achieves high accuracy with 99.47% in threat detection but also advances the field by providing clear, actionable explanations of its analytical outcomes. This research also aims to bridge the current gap and facilitate the broader integration of ML/DL technologies in cybersecurity defenses by offering a local and global explainability solution that is both precise and interpretable.","sentences":["The effectiveness of Intrusion Detection Systems (IDS) is critical in an era where cyber threats are becoming increasingly complex.","Machine learning (ML) and deep learning (DL) models provide an efficient and accurate solution for identifying attacks and anomalies in computer networks.","However, using ML and DL models in IDS has led to a trust deficit due to their non-transparent decision-making.","This transparency gap in IDS research is significant, affecting confidence and accountability.","To address, this paper introduces a novel Explainable IDS approach, called X-CBA, that leverages the structural advantages of Graph Neural Networks (GNNs) to effectively process network traffic data, while also adapting a new Explainable AI (XAI) methodology.","Unlike most GNN-based IDS that depend on labeled network traffic and node features, thereby overlooking critical packet-level information, our approach leverages a broader range of traffic data through network flows, including edge attributes, to improve detection capabilities and adapt to novel threats.","Through empirical testing, we establish that our approach not only achieves high accuracy with 99.47% in threat detection but also advances the field by providing clear, actionable explanations of its analytical outcomes.","This research also aims to bridge the current gap and facilitate the broader integration of ML/DL technologies in cybersecurity defenses by offering a local and global explainability solution that is both precise and interpretable."],"url":"http://arxiv.org/abs/2402.00839v1","category":"cs.CR"}
{"created":"2024-02-01 18:22:32","title":"ALISON: Fast and Effective Stylometric Authorship Obfuscation","abstract":"Authorship Attribution (AA) and Authorship Obfuscation (AO) are two competing tasks of increasing importance in privacy research. Modern AA leverages an author's consistent writing style to match a text to its author using an AA classifier. AO is the corresponding adversarial task, aiming to modify a text in such a way that its semantics are preserved, yet an AA model cannot correctly infer its authorship. To address privacy concerns raised by state-of-the-art (SOTA) AA methods, new AO methods have been proposed but remain largely impractical to use due to their prohibitively slow training and obfuscation speed, often taking hours. To this challenge, we propose a practical AO method, ALISON, that (1) dramatically reduces training/obfuscation time, demonstrating more than 10x faster obfuscation than SOTA AO methods, (2) achieves better obfuscation success through attacking three transformer-based AA methods on two benchmark datasets, typically performing 15% better than competing methods, (3) does not require direct signals from a target AA classifier during obfuscation, and (4) utilizes unique stylometric features, allowing sound model interpretation for explainable obfuscation. We also demonstrate that ALISON can effectively prevent four SOTA AA methods from accurately determining the authorship of ChatGPT-generated texts, all while minimally changing the original text semantics. To ensure the reproducibility of our findings, our code and data are available at: https://github.com/EricX003/ALISON.","sentences":["Authorship Attribution (AA) and Authorship Obfuscation (AO) are two competing tasks of increasing importance in privacy research.","Modern AA leverages an author's consistent writing style to match a text to its author using an AA classifier.","AO is the corresponding adversarial task, aiming to modify a text in such a way that its semantics are preserved, yet an AA model cannot correctly infer its authorship.","To address privacy concerns raised by state-of-the-art (SOTA) AA methods, new AO methods have been proposed but remain largely impractical to use due to their prohibitively slow training and obfuscation speed, often taking hours.","To this challenge, we propose a practical AO method, ALISON, that (1) dramatically reduces training/obfuscation time, demonstrating more than 10x faster obfuscation than SOTA AO methods, (2) achieves better obfuscation success through attacking three transformer-based AA methods on two benchmark datasets, typically performing 15% better than competing methods, (3) does not require direct signals from a target AA classifier during obfuscation, and (4) utilizes unique stylometric features, allowing sound model interpretation for explainable obfuscation.","We also demonstrate that ALISON can effectively prevent four SOTA AA methods from accurately determining the authorship of ChatGPT-generated texts, all while minimally changing the original text semantics.","To ensure the reproducibility of our findings, our code and data are available at: https://github.com/EricX003/ALISON."],"url":"http://arxiv.org/abs/2402.00835v1","category":"cs.CL"}
{"created":"2024-02-01 18:20:45","title":"Orthogonal gamma-based expansion for the CIR's first passage time distribution","abstract":"In this paper we analyze a method for approximating the first-passage time density and the corresponding distribution function for a CIR process. This approximation is obtained by truncating a series expansion involving the generalized Laguerre polynomials and the gamma probability density. The suggested approach involves a number of numerical issues which depend strongly on the coefficient of variation of the first passage time random variable. These issues are examined and solutions are proposed also involving the first passage time distribution function. Numerical results and comparisons with alternative approximation methods show the strengths and weaknesses of the proposed method. A general acceptance-rejection-like procedure, that makes use of the approximation, is presented. It allows the generation of first passage time data, even if its distribution is unknown.","sentences":["In this paper we analyze a method for approximating the first-passage time density and the corresponding distribution function for a CIR process.","This approximation is obtained by truncating a series expansion involving the generalized Laguerre polynomials and the gamma probability density.","The suggested approach involves a number of numerical issues which depend strongly on the coefficient of variation of the first passage time random variable.","These issues are examined and solutions are proposed also involving the first passage time distribution function.","Numerical results and comparisons with alternative approximation methods show the strengths and weaknesses of the proposed method.","A general acceptance-rejection-like procedure, that makes use of the approximation, is presented.","It allows the generation of first passage time data, even if its distribution is unknown."],"url":"http://arxiv.org/abs/2402.00833v1","category":"math.PR"}
{"created":"2024-02-01 18:17:37","title":"A YANG-aided Unified Strategy for Black Hole Detection for Backbone Networks","abstract":"Despite the crucial importance of addressing Black Hole failures in Internet backbone networks, effective detection strategies in backbone networks are lacking. This is largely because previous research has been centered on Mobile Ad-hoc Networks (MANETs), which operate under entirely different dynamics, protocols, and topologies, making their findings not directly transferable to backbone networks. Furthermore, detecting Black Hole failures in backbone networks is particularly challenging. It requires a comprehensive range of network data due to the wide variety of conditions that need to be considered, making data collection and analysis far from straightforward. Addressing this gap, our study introduces a novel approach for Black Hole detection in backbone networks using specialized Yet Another Next Generation (YANG) data models with Black Hole-sensitive Metric Matrix (BHMM) analysis. This paper details our method of selecting and analyzing four YANG models relevant to Black Hole detection in ISP networks, focusing on routing protocols and ISP-specific configurations. Our BHMM approach derived from these models demonstrates a 10% improvement in detection accuracy and a 13% increase in packet delivery rate, highlighting the efficiency of our approach. Additionally, we evaluate the Machine Learning approach leveraged with BHMM analysis in two different network settings, a commercial ISP network, and a scientific research-only network topology. This evaluation also demonstrates the practical applicability of our method, yielding significantly improved prediction outcomes in both environments.","sentences":["Despite the crucial importance of addressing Black Hole failures in Internet backbone networks, effective detection strategies in backbone networks are lacking.","This is largely because previous research has been centered on Mobile Ad-hoc Networks (MANETs), which operate under entirely different dynamics, protocols, and topologies, making their findings not directly transferable to backbone networks.","Furthermore, detecting Black Hole failures in backbone networks is particularly challenging.","It requires a comprehensive range of network data due to the wide variety of conditions that need to be considered, making data collection and analysis far from straightforward.","Addressing this gap, our study introduces a novel approach for Black Hole detection in backbone networks using specialized","Yet Another Next Generation (YANG) data models with Black Hole-sensitive Metric Matrix (BHMM) analysis.","This paper details our method of selecting and analyzing four YANG models relevant to Black Hole detection in ISP networks, focusing on routing protocols and ISP-specific configurations.","Our BHMM approach derived from these models demonstrates a 10% improvement in detection accuracy and a 13% increase in packet delivery rate, highlighting the efficiency of our approach.","Additionally, we evaluate the Machine Learning approach leveraged with BHMM analysis in two different network settings, a commercial ISP network, and a scientific research-only network topology.","This evaluation also demonstrates the practical applicability of our method, yielding significantly improved prediction outcomes in both environments."],"url":"http://arxiv.org/abs/2402.00831v1","category":"cs.NI"}
{"created":"2024-02-01 18:17:29","title":"Common errors in Generative AI systems used for knowledge extraction in the climate action domain","abstract":"Large Language Models (LLMs) and, more specifically, the Generative Pre-Trained Transformers (GPT) can help stakeholders in climate action explore digital knowledge bases and extract and utilize climate action knowledge in a sustainable manner. However, LLMs are \"probabilistic models of knowledge bases\" that excel at generating convincing texts but cannot be entirely relied upon due to the probabilistic nature of the information produced. This brief report illustrates the problem space with examples of LLM responses to some of the questions of relevance to climate action.","sentences":["Large Language Models (LLMs) and, more specifically, the Generative Pre-Trained Transformers (GPT) can help stakeholders in climate action explore digital knowledge bases and extract and utilize climate action knowledge in a sustainable manner.","However, LLMs are \"probabilistic models of knowledge bases\" that excel at generating convincing texts but cannot be entirely relied upon due to the probabilistic nature of the information produced.","This brief report illustrates the problem space with examples of LLM responses to some of the questions of relevance to climate action."],"url":"http://arxiv.org/abs/2402.00830v1","category":"cs.CY"}
{"created":"2024-02-01 18:16:04","title":"Efficient Fine-tuning of Audio Spectrogram Transformers via Soft Mixture of Adapters","abstract":"Mixture of Experts (MoE) architectures have recently started burgeoning due to their ability to scale model's capacity while maintaining the computational cost affordable. Furthermore, they can be applied to both Transformers and State Space Models, the current state-of-the-art models in numerous fields. While MoE has been mostly investigated for the pre-training stage, its use in parameter-efficient transfer learning settings is under-explored. To narrow this gap, this paper attempts to demystify the use of MoE for parameter-efficient fine-tuning of Audio Spectrogram Transformers to audio and speech downstream tasks. Specifically, we propose Soft Mixture of Adapters (Soft-MoA). It exploits adapters as the experts and, leveraging the recent Soft MoE method, it relies on a soft assignment between the input tokens and experts to keep the computational time limited. Extensive experiments across 4 benchmarks demonstrate that Soft-MoA outperforms the single adapter method and performs on par with the dense MoA counterpart. We finally present ablation studies on key elements of Soft-MoA, showing for example that Soft-MoA achieves better scaling with more experts, as well as ensuring that all experts contribute to the computation of the output tokens, thus dispensing with the expert imbalance issue.","sentences":["Mixture of Experts (MoE) architectures have recently started burgeoning due to their ability to scale model's capacity while maintaining the computational cost affordable.","Furthermore, they can be applied to both Transformers and State Space Models, the current state-of-the-art models in numerous fields.","While MoE has been mostly investigated for the pre-training stage, its use in parameter-efficient transfer learning settings is under-explored.","To narrow this gap, this paper attempts to demystify the use of MoE for parameter-efficient fine-tuning of Audio Spectrogram Transformers to audio and speech downstream tasks.","Specifically, we propose Soft Mixture of Adapters (Soft-MoA).","It exploits adapters as the experts and, leveraging the recent Soft MoE method, it relies on a soft assignment between the input tokens and experts to keep the computational time limited.","Extensive experiments across 4 benchmarks demonstrate that Soft-MoA outperforms the single adapter method and performs on par with the dense MoA counterpart.","We finally present ablation studies on key elements of Soft-MoA, showing for example that Soft-MoA achieves better scaling with more experts, as well as ensuring that all experts contribute to the computation of the output tokens, thus dispensing with the expert imbalance issue."],"url":"http://arxiv.org/abs/2402.00828v1","category":"eess.AS"}
{"created":"2024-02-01 18:14:42","title":"Emo-Avatar: Efficient Monocular Video Style Avatar through Texture Rendering","abstract":"Artistic video portrait generation is a significant and sought-after task in the fields of computer graphics and vision. While various methods have been developed that integrate NeRFs or StyleGANs with instructional editing models for creating and editing drivable portraits, these approaches face several challenges. They often rely heavily on large datasets, require extensive customization processes, and frequently result in reduced image quality. To address the above problems, we propose the Efficient Monotonic Video Style Avatar (Emo-Avatar) through deferred neural rendering that enhances StyleGAN's capacity for producing dynamic, drivable portrait videos. We proposed a two-stage deferred neural rendering pipeline. In the first stage, we utilize few-shot PTI initialization to initialize the StyleGAN generator through several extreme poses sampled from the video to capture the consistent representation of aligned faces from the target portrait. In the second stage, we propose a Laplacian pyramid for high-frequency texture sampling from UV maps deformed by dynamic flow of expression for motion-aware texture prior integration to provide torso features to enhance StyleGAN's ability to generate complete and upper body for portrait video rendering. Emo-Avatar reduces style customization time from hours to merely 5 minutes compared with existing methods. In addition, Emo-Avatar requires only a single reference image for editing and employs region-aware contrastive learning with semantic invariant CLIP guidance, ensuring consistent high-resolution output and identity preservation. Through both quantitative and qualitative assessments, Emo-Avatar demonstrates superior performance over existing methods in terms of training efficiency, rendering quality and editability in self- and cross-reenactment.","sentences":["Artistic video portrait generation is a significant and sought-after task in the fields of computer graphics and vision.","While various methods have been developed that integrate NeRFs or StyleGANs with instructional editing models for creating and editing drivable portraits, these approaches face several challenges.","They often rely heavily on large datasets, require extensive customization processes, and frequently result in reduced image quality.","To address the above problems, we propose the Efficient Monotonic Video Style Avatar (Emo-Avatar) through deferred neural rendering that enhances StyleGAN's capacity for producing dynamic, drivable portrait videos.","We proposed a two-stage deferred neural rendering pipeline.","In the first stage, we utilize few-shot PTI initialization to initialize the StyleGAN generator through several extreme poses sampled from the video to capture the consistent representation of aligned faces from the target portrait.","In the second stage, we propose a Laplacian pyramid for high-frequency texture sampling from UV maps deformed by dynamic flow of expression for motion-aware texture prior integration to provide torso features to enhance StyleGAN's ability to generate complete and upper body for portrait video rendering.","Emo-Avatar reduces style customization time from hours to merely 5 minutes compared with existing methods.","In addition, Emo-Avatar requires only a single reference image for editing and employs region-aware contrastive learning with semantic invariant CLIP guidance, ensuring consistent high-resolution output and identity preservation.","Through both quantitative and qualitative assessments, Emo-Avatar demonstrates superior performance over existing methods in terms of training efficiency, rendering quality and editability in self- and cross-reenactment."],"url":"http://arxiv.org/abs/2402.00827v1","category":"cs.CV"}
{"created":"2024-02-01 18:07:33","title":"SLIM: Skill Learning with Multiple Critics","abstract":"Self-supervised skill learning aims to acquire useful behaviors that leverage the underlying dynamics of the environment. Latent variable models, based on mutual information maximization, have been particularly successful in this task but still struggle in the context of robotic manipulation. As it requires impacting a possibly large set of degrees of freedom composing the environment, mutual information maximization fails alone in producing useful manipulation behaviors. To address this limitation, we introduce SLIM, a multi-critic learning approach for skill discovery with a particular focus on robotic manipulation. Our main insight is that utilizing multiple critics in an actor-critic framework to gracefully combine multiple reward functions leads to a significant improvement in latent-variable skill discovery for robotic manipulation while overcoming possible interference occurring among rewards which hinders convergence to useful skills. Furthermore, in the context of tabletop manipulation, we demonstrate the applicability of our novel skill discovery approach to acquire safe and efficient motor primitives in a hierarchical reinforcement learning fashion and leverage them through planning, surpassing the state-of-the-art approaches for skill discovery by a large margin.","sentences":["Self-supervised skill learning aims to acquire useful behaviors that leverage the underlying dynamics of the environment.","Latent variable models, based on mutual information maximization, have been particularly successful in this task but still struggle in the context of robotic manipulation.","As it requires impacting a possibly large set of degrees of freedom composing the environment, mutual information maximization fails alone in producing useful manipulation behaviors.","To address this limitation, we introduce SLIM, a multi-critic learning approach for skill discovery with a particular focus on robotic manipulation.","Our main insight is that utilizing multiple critics in an actor-critic framework to gracefully combine multiple reward functions leads to a significant improvement in latent-variable skill discovery for robotic manipulation while overcoming possible interference occurring among rewards which hinders convergence to useful skills.","Furthermore, in the context of tabletop manipulation, we demonstrate the applicability of our novel skill discovery approach to acquire safe and efficient motor primitives in a hierarchical reinforcement learning fashion and leverage them through planning, surpassing the state-of-the-art approaches for skill discovery by a large margin."],"url":"http://arxiv.org/abs/2402.00823v1","category":"cs.LG"}
{"created":"2024-02-01 18:05:38","title":"WiOpen: A Robust Wi-Fi-based Open-set Gesture Recognition Framework","abstract":"Recent years have witnessed a growing interest in Wi-Fi-based gesture recognition. However, existing works have predominantly focused on closed-set paradigms, where all testing gestures are predefined during training. This poses a significant challenge in real-world applications, as unseen gestures might be misclassified as known classes during testing. To address this issue, we propose WiOpen, a robust Wi-Fi-based Open-Set Gesture Recognition (OSGR) framework. Implementing OSGR requires addressing challenges caused by the unique uncertainty in Wi-Fi sensing. This uncertainty, resulting from noise and domains, leads to widely scattered and irregular data distributions in collected Wi-Fi sensing data. Consequently, data ambiguity between classes and challenges in defining appropriate decision boundaries to identify unknowns arise. To tackle these challenges, WiOpen adopts a two-fold approach to eliminate uncertainty and define precise decision boundaries. Initially, it addresses uncertainty induced by noise during data preprocessing by utilizing the CSI ratio. Next, it designs the OSGR network based on an uncertainty quantification method. Throughout the learning process, this network effectively mitigates uncertainty stemming from domains. Ultimately, the network leverages relationships among samples' neighbors to dynamically define open-set decision boundaries, successfully realizing OSGR. Comprehensive experiments on publicly accessible datasets confirm WiOpen's effectiveness. Notably, WiOpen also demonstrates superiority in cross-domain tasks when compared to state-of-the-art approaches.","sentences":["Recent years have witnessed a growing interest in Wi-Fi-based gesture recognition.","However, existing works have predominantly focused on closed-set paradigms, where all testing gestures are predefined during training.","This poses a significant challenge in real-world applications, as unseen gestures might be misclassified as known classes during testing.","To address this issue, we propose WiOpen, a robust Wi-Fi-based Open-Set Gesture Recognition (OSGR) framework.","Implementing OSGR requires addressing challenges caused by the unique uncertainty in Wi-Fi sensing.","This uncertainty, resulting from noise and domains, leads to widely scattered and irregular data distributions in collected Wi-Fi sensing data.","Consequently, data ambiguity between classes and challenges in defining appropriate decision boundaries to identify unknowns arise.","To tackle these challenges, WiOpen adopts a two-fold approach to eliminate uncertainty and define precise decision boundaries.","Initially, it addresses uncertainty induced by noise during data preprocessing by utilizing the CSI ratio.","Next, it designs the OSGR network based on an uncertainty quantification method.","Throughout the learning process, this network effectively mitigates uncertainty stemming from domains.","Ultimately, the network leverages relationships among samples' neighbors to dynamically define open-set decision boundaries, successfully realizing OSGR.","Comprehensive experiments on publicly accessible datasets confirm WiOpen's effectiveness.","Notably, WiOpen also demonstrates superiority in cross-domain tasks when compared to state-of-the-art approaches."],"url":"http://arxiv.org/abs/2402.00822v1","category":"cs.HC"}
{"created":"2024-02-01 18:00:00","title":"Further understanding the interaction between dark energy and dark matter: current status and future directions","abstract":"The interaction between dark matter and dark energy can be incorporated into field theory models of dark energy that have proved successful in alleviating the coincidence problem. We review recent advances in this field, including new models and constraints from different astronomical data sets. We show that interactions are allowed by observations and can reduce the current tensions among different measurements of cosmological parameters. We extend our discussion to include constraints from non-linear effects and results from cosmological simulations. Finally, we discuss forthcoming multi-messenger data from current and future observational facilities that will help to improve our understanding of the interactions within the dark sector.","sentences":["The interaction between dark matter and dark energy can be incorporated into field theory models of dark energy that have proved successful in alleviating the coincidence problem.","We review recent advances in this field, including new models and constraints from different astronomical data sets.","We show that interactions are allowed by observations and can reduce the current tensions among different measurements of cosmological parameters.","We extend our discussion to include constraints from non-linear effects and results from cosmological simulations.","Finally, we discuss forthcoming multi-messenger data from current and future observational facilities that will help to improve our understanding of the interactions within the dark sector."],"url":"http://arxiv.org/abs/2402.00819v1","category":"astro-ph.CO"}
{"created":"2024-02-01 17:58:47","title":"The Entropy of Dynamical Black Holes","abstract":"We propose a new formula for the entropy of a dynamical black hole$-$valid to leading order for perturbations off of a stationary black hole background$-$in an arbitrary classical diffeomorphism covariant Lagrangian theory of gravity in $n$ dimensions. In stationary eras, this formula agrees with the usual Noether charge formula, but in nonstationary eras, we obtain a nontrivial correction term. In general relativity, our formula for the entropy of a dynamical black hole is $1/4$ of the horizon area plus a term involving the integral of the expansion of the null generators of the horizon, which we show is $1/4$ of the area of the apparent horizon to leading order. Our formula for entropy in a general theory of gravity obeys a \"local physical process version\" of the first law of black hole thermodynamics. For first order perturbations sourced by external matter that satisfies the null energy condition, our entropy obeys the second law of black hole thermodynamics. For vacuum perturbations, the second law is obeyed at leading order if and only if the \"modified canonical energy flux\" is positive (as is the case in general relativity but presumably would not hold in general theories). We obtain a general relationship between our formula for the entropy of a dynamical black hole and a formula proposed independently by Dong and by Wall. We then consider the generalized second law in semiclassical gravity for first order perturbations of a stationary black hole. We show that the validity of the quantum null energy condition (QNEC) on a Killing horizon is equivalent to the generalized second law using our notion of black hole entropy but using a modified notion of von Neumann entropy for matter. On the other hand, the generalized second law for the Dong-Wall entropy is equivalent to an integrated version of QNEC, using the unmodified von Neumann entropy for the entropy of matter.","sentences":["We propose a new formula for the entropy of a dynamical black hole$-$valid to leading order for perturbations off of a stationary black hole background$-$in","an arbitrary classical diffeomorphism covariant Lagrangian theory of gravity in $n$ dimensions.","In stationary eras, this formula agrees with the usual Noether charge formula, but in nonstationary eras, we obtain a nontrivial correction term.","In general relativity, our formula for the entropy of a dynamical black hole is $1/4$ of the horizon area plus a term involving the integral of the expansion of the null generators of the horizon, which we show is $1/4$ of the area of the apparent horizon to leading order.","Our formula for entropy in a general theory of gravity obeys a \"local physical process version\" of the first law of black hole thermodynamics.","For first order perturbations sourced by external matter that satisfies the null energy condition, our entropy obeys the second law of black hole thermodynamics.","For vacuum perturbations, the second law is obeyed at leading order if and only if the \"modified canonical energy flux\" is positive (as is the case in general relativity but presumably would not hold in general theories).","We obtain a general relationship between our formula for the entropy of a dynamical black hole and a formula proposed independently by Dong and by Wall.","We then consider the generalized second law in semiclassical gravity for first order perturbations of a stationary black hole.","We show that the validity of the quantum null energy condition (QNEC) on a Killing horizon is equivalent to the generalized second law using our notion of black hole entropy but using a modified notion of von Neumann entropy for matter.","On the other hand, the generalized second law for the Dong-Wall entropy is equivalent to an integrated version of QNEC, using the unmodified von Neumann entropy for the entropy of matter."],"url":"http://arxiv.org/abs/2402.00818v1","category":"hep-th"}
{"created":"2024-02-01 17:55:08","title":"Leveraging Approximate Model-based Shielding for Probabilistic Safety Guarantees in Continuous Environments","abstract":"Shielding is a popular technique for achieving safe reinforcement learning (RL). However, classical shielding approaches come with quite restrictive assumptions making them difficult to deploy in complex environments, particularly those with continuous state or action spaces. In this paper we extend the more versatile approximate model-based shielding (AMBS) framework to the continuous setting. In particular we use Safety Gym as our test-bed, allowing for a more direct comparison of AMBS with popular constrained RL algorithms. We also provide strong probabilistic safety guarantees for the continuous setting. In addition, we propose two novel penalty techniques that directly modify the policy gradient, which empirically provide more stable convergence in our experiments.","sentences":["Shielding is a popular technique for achieving safe reinforcement learning (RL).","However, classical shielding approaches come with quite restrictive assumptions making them difficult to deploy in complex environments, particularly those with continuous state or action spaces.","In this paper we extend the more versatile approximate model-based shielding (AMBS) framework to the continuous setting.","In particular we use Safety Gym as our test-bed, allowing for a more direct comparison of AMBS with popular constrained RL algorithms.","We also provide strong probabilistic safety guarantees for the continuous setting.","In addition, we propose two novel penalty techniques that directly modify the policy gradient, which empirically provide more stable convergence in our experiments."],"url":"http://arxiv.org/abs/2402.00816v1","category":"cs.LG"}
{"created":"2024-02-01 17:53:39","title":"Optimal monotone conditional error functions","abstract":"This note presents a method that provides optimal monotone conditional error functions for a large class of adaptive two stage designs. The presented method builds on a previously developed general theory for optimal adaptive two stage designs where sample sizes are reassessed for a specific conditional power and the goal is to minimize the expected sample size. The previous theory can easily lead to a non-monotonous conditional error function which is highly undesirable for logical reasons and can harm type I error rate control for composite null hypotheses. The here presented method extends the existing theory by introducing intermediate monotonising steps that can easily be implemented.","sentences":["This note presents a method that provides optimal monotone conditional error functions for a large class of adaptive two stage designs.","The presented method builds on a previously developed general theory for optimal adaptive two stage designs where sample sizes are reassessed for a specific conditional power and the goal is to minimize the expected sample size.","The previous theory can easily lead to a non-monotonous conditional error function which is highly undesirable for logical reasons and can harm type I error rate control for composite null hypotheses.","The here presented method extends the existing theory by introducing intermediate monotonising steps that can easily be implemented."],"url":"http://arxiv.org/abs/2402.00814v1","category":"stat.ME"}
{"created":"2024-02-01 17:51:38","title":"Impact of anti-symmetric contributions to signal multipoles in the measurement of black-hole spins","abstract":"Many current models for the gravitational-wave signal from precessing black-hole binaries neglect an asymmetry in the $\\pm m$ multipoles. The asymmetry is weak, but is responsible for out-of-plane recoil, which for the final black hole can be several thousand km/s. In this work we show that the multipole asymmetry is also necessary to accurately measure the black-hole spins. We consider synthetic signals calculated from the numerical relativity surrogate model NRSur7dq4, which includes the multipole asymmetry, and measure the signal parameters using two versions of the same model, one with and one without the multipole asymmetry included. We find that in high signal-to-noise-ratio observations where the spin magnitude and direction can in principle be measured accurately, neglecting the multipole asymmetry can result in biased measurements of these quantities. Measurements of the black-hole masses and the standard aligned-spin combination $\\chi_{\\rm eff}$ are not in general strongly affected. As an illustration of the impact of the multipole asymmetry on a real signal we consider the LVK observation GW200129_065458, and find that the inclusion of the multipole asymmetry is necessary to identify the binary as unequal-mass and a high in-plane spin in the primary.","sentences":["Many current models for the gravitational-wave signal from precessing black-hole binaries neglect an asymmetry in the $\\pm m$ multipoles.","The asymmetry is weak, but is responsible for out-of-plane recoil, which for the final black hole can be several thousand km/s.","In this work we show that the multipole asymmetry is also necessary to accurately measure the black-hole spins.","We consider synthetic signals calculated from the numerical relativity surrogate model NRSur7dq4, which includes the multipole asymmetry, and measure the signal parameters using two versions of the same model, one with and one without the multipole asymmetry included.","We find that in high signal-to-noise-ratio observations where the spin magnitude and direction can in principle be measured accurately, neglecting the multipole asymmetry can result in biased measurements of these quantities.","Measurements of the black-hole masses and the standard aligned-spin combination $\\chi_{\\rm eff}$ are not in general strongly affected.","As an illustration of the impact of the multipole asymmetry on a real signal we consider the LVK observation GW200129_065458, and find that the inclusion of the multipole asymmetry is necessary to identify the binary as unequal-mass and a high in-plane spin in the primary."],"url":"http://arxiv.org/abs/2402.00813v1","category":"gr-qc"}
{"created":"2024-02-01 17:49:51","title":"Examining the Influence of Digital Phantom Models in Virtual Imaging Trials for Tomographic Breast Imaging","abstract":"Purpose: Digital phantoms are one of the key components of virtual imaging trials (VITs) that aim to assess and optimize new medical imaging systems and algorithms. However, these phantoms vary in their voxel resolution, appearance, and structural details. This study aims to examine whether and how variations between digital phantoms influence system optimization with digital breast tomosynthesis (DBT) as a chosen modality. Methods: We selected widely used and open-access digital breast phantoms generated with different methods. For each phantom type, we created an ensemble of DBT images to test acquisition strategies. Human observer localization ROC (LROC) was used to assess observer performance studies for each case. Noise power spectrum (NPS) was estimated to compare the phantom structural components. Further, we computed several gaze metrics to quantify the gaze pattern when viewing images generated from different phantom types. Results: Our LROC results show that the arc samplings for peak performance were approximately 2.5 degrees and 6 degrees in Bakic and XCAT breast phantoms respectively for 3-mm lesion detection tasks and indicate that system optimization outcomes from VITs can vary with phantom types and structural frequency components. Additionally, a significant correlation (p= 0.01) between gaze metrics and diagnostic performance suggests that gaze analysis can be used to understand and evaluate task difficulty in VITs.","sentences":["Purpose: Digital phantoms are one of the key components of virtual imaging trials (VITs) that aim to assess and optimize new medical imaging systems and algorithms.","However, these phantoms vary in their voxel resolution, appearance, and structural details.","This study aims to examine whether and how variations between digital phantoms influence system optimization with digital breast tomosynthesis (DBT) as a chosen modality.","Methods: We selected widely used and open-access digital breast phantoms generated with different methods.","For each phantom type, we created an ensemble of DBT images to test acquisition strategies.","Human observer localization ROC (LROC) was used to assess observer performance studies for each case.","Noise power spectrum (NPS) was estimated to compare the phantom structural components.","Further, we computed several gaze metrics to quantify the gaze pattern when viewing images generated from different phantom types.","Results:","Our LROC results show that the arc samplings for peak performance were approximately 2.5 degrees and 6 degrees in Bakic and XCAT breast phantoms respectively for 3-mm lesion detection tasks and indicate that system optimization outcomes from VITs can vary with phantom types and structural frequency components.","Additionally, a significant correlation (p= 0.01) between gaze metrics and diagnostic performance suggests that gaze analysis can be used to understand and evaluate task difficulty in VITs."],"url":"http://arxiv.org/abs/2402.00812v1","category":"physics.med-ph"}
{"created":"2024-02-01 17:46:19","title":"An Analysis of the Variance of Diffusion-based Speech Enhancement","abstract":"Diffusion models proved to be powerful models for generative speech enhancement. In recent SGMSE+ approaches, training involves a stochastic differential equation for the diffusion process, adding both Gaussian and environmental noise to the clean speech signal gradually. The speech enhancement performance varies depending on the choice of the stochastic differential equation that controls the evolution of the mean and the variance along the diffusion processes when adding environmental and Gaussian noise. In this work, we highlight that the scale of the variance is a dominant parameter for speech enhancement performance and show that it controls the tradeoff between noise attenuation and speech distortions. More concretely, we show that a larger variance increases the noise attenuation and allows for reducing the computational footprint, as fewer function evaluations for generating the estimate are required.","sentences":["Diffusion models proved to be powerful models for generative speech enhancement.","In recent SGMSE+ approaches, training involves a stochastic differential equation for the diffusion process, adding both Gaussian and environmental noise to the clean speech signal gradually.","The speech enhancement performance varies depending on the choice of the stochastic differential equation that controls the evolution of the mean and the variance along the diffusion processes when adding environmental and Gaussian noise.","In this work, we highlight that the scale of the variance is a dominant parameter for speech enhancement performance and show that it controls the tradeoff between noise attenuation and speech distortions.","More concretely, we show that a larger variance increases the noise attenuation and allows for reducing the computational footprint, as fewer function evaluations for generating the estimate are required."],"url":"http://arxiv.org/abs/2402.00811v1","category":"eess.AS"}
{"created":"2024-02-01 17:44:46","title":"Exploring the Dynamics between Cobot's Production Rhythm, Locus of Control and Emotional State in a Collaborative Assembly Scenario","abstract":"In industrial scenarios, there is widespread use of collaborative robots (cobots), and growing interest is directed at evaluating and measuring the impact of some characteristics of the cobot on the human factor. In the present pilot study, the effect that the production rhythm (C1 - Slow, C2 - Fast, C3 - Adapted to the participant's pace) of a cobot has on the Experiential Locus of Control (ELoC) and the emotional state of 31 participants has been examined. The operators' performance, the degree of basic internal Locus of Control, and the attitude towards the robots were also considered. No difference was found regarding the emotional state and the ELoC in the three conditions, but considering the other psychological variables, a more complex situation emerges. Overall, results seem to indicate a need to consider the person's psychological characteristics to offer a differentiated and optimal interaction experience.","sentences":["In industrial scenarios, there is widespread use of collaborative robots (cobots), and growing interest is directed at evaluating and measuring the impact of some characteristics of the cobot on the human factor.","In the present pilot study, the effect that the production rhythm (C1 - Slow, C2 - Fast, C3 - Adapted to the participant's pace) of a cobot has on the Experiential Locus of Control (ELoC) and the emotional state of 31 participants has been examined.","The operators' performance, the degree of basic internal Locus of Control, and the attitude towards the robots were also considered.","No difference was found regarding the emotional state and the ELoC in the three conditions, but considering the other psychological variables, a more complex situation emerges.","Overall, results seem to indicate a need to consider the person's psychological characteristics to offer a differentiated and optimal interaction experience."],"url":"http://arxiv.org/abs/2402.00808v1","category":"cs.RO"}
{"created":"2024-02-01 17:44:11","title":"Distilling Conditional Diffusion Models for Offline Reinforcement Learning through Trajectory Stitching","abstract":"Deep generative models have recently emerged as an effective approach to offline reinforcement learning. However, their large model size poses challenges in computation. We address this issue by proposing a knowledge distillation method based on data augmentation. In particular, high-return trajectories are generated from a conditional diffusion model, and they are blended with the original trajectories through a novel stitching algorithm that leverages a new reward generator. Applying the resulting dataset to behavioral cloning, the learned shallow policy whose size is much smaller outperforms or nearly matches deep generative planners on several D4RL benchmarks.","sentences":["Deep generative models have recently emerged as an effective approach to offline reinforcement learning.","However, their large model size poses challenges in computation.","We address this issue by proposing a knowledge distillation method based on data augmentation.","In particular, high-return trajectories are generated from a conditional diffusion model, and they are blended with the original trajectories through a novel stitching algorithm that leverages a new reward generator.","Applying the resulting dataset to behavioral cloning, the learned shallow policy whose size is much smaller outperforms or nearly matches deep generative planners on several D4RL benchmarks."],"url":"http://arxiv.org/abs/2402.00807v1","category":"cs.LG"}
{"created":"2024-02-01 17:42:17","title":"Linearly coupled quantum harmonic oscillators and their quantum entanglement","abstract":"Quantum harmonic oscillators linearly coupled through coordinates and momenta, represented by the Hamiltonian $ {\\hat H}=\\sum^2_{i=1}\\left( \\frac{ {\\hat p}^{2}_i}{2 m_i } + \\frac{m_i \\omega^2_i}{2} x^2_i\\right) +{\\hat H}_{int} $, where the interaction of two oscillators ${\\hat H}_{int} = i k_1 x_1 { \\hat p }_2+ i k_2 x_2 {\\hat p}_1 + k_3 x_1 x_2-k_4 {\\hat p}_1 {\\hat p}_2$, found in many applications of quantum optics, nonlinear physics, molecular chemistry and biophysics. Despite this, there is currently no general solution to the Schr\\\"{o}dinger equation for such a system. This is especially relevant for quantum entanglement of such a system in quantum optics applications. Here this problem is solved and it is shown that quantum entanglement depends on only one coefficient $R \\in (0,1)$, which includes all the parameters of the system under consideration. It has been shown that quantum entanglement can be very large at certain values of this coefficient. The results obtained have a fairly simple analytical form, which facilitates analysis.","sentences":["Quantum harmonic oscillators linearly coupled through coordinates and momenta, represented by the Hamiltonian $ {\\hat H}=\\sum^2_{i=1}\\left( \\frac{ {\\hat p}^{2}_i}{2 m_i }","+ \\frac{m_i \\omega^2_i}{2} x^2_i\\right)","+{\\hat H}_{int} $, where the interaction of two oscillators ${\\hat H}_{int} = i k_1 x_1 { \\hat p }_2","+ i k_2 x_2 {\\hat p}_1 + k_3 x_1 x_2-k_4 {\\hat p}_1 {\\hat p}_2$, found in many applications of quantum optics, nonlinear physics, molecular chemistry and biophysics.","Despite this, there is currently no general solution to the Schr\\\"{o}dinger equation for such a system.","This is especially relevant for quantum entanglement of such a system in quantum optics applications.","Here this problem is solved and it is shown that quantum entanglement depends on only one coefficient $R \\in (0,1)$, which includes all the parameters of the system under consideration.","It has been shown that quantum entanglement can be very large at certain values of this coefficient.","The results obtained have a fairly simple analytical form, which facilitates analysis."],"url":"http://arxiv.org/abs/2402.00806v1","category":"quant-ph"}
{"created":"2024-02-01 17:40:10","title":"Signal Quality Auditing for Time-series Data","abstract":"Signal quality assessment (SQA) is required for monitoring the reliability of data acquisition systems, especially in AI-driven Predictive Maintenance (PMx) application contexts. SQA is vital for addressing \"silent failures\" of data acquisition hardware and software, which when unnoticed, misinform the users of data, creating the risk for incorrect decisions with unintended or even catastrophic consequences. We have developed an open-source software implementation of signal quality indices (SQIs) for the analysis of time-series data. We codify a range of SQIs, demonstrate them using established benchmark data, and show that they can be effective for signal quality assessment. We also study alternative approaches to denoising time-series data in an attempt to improve the quality of the already degraded signal, and evaluate them empirically on relevant real-world data. To our knowledge, our software toolkit is the first to provide an open source implementation of a broad range of signal quality assessment and improvement techniques validated on publicly available benchmark data for ease of reproducibility. The generality of our framework can be easily extended to assessing reliability of arbitrary time-series measurements in complex systems, especially when morphological patterns of the waveform shapes and signal periodicity are of key interest in downstream analyses.","sentences":["Signal quality assessment (SQA) is required for monitoring the reliability of data acquisition systems, especially in AI-driven Predictive Maintenance (PMx) application contexts.","SQA is vital for addressing \"silent failures\" of data acquisition hardware and software, which when unnoticed, misinform the users of data, creating the risk for incorrect decisions with unintended or even catastrophic consequences.","We have developed an open-source software implementation of signal quality indices (SQIs) for the analysis of time-series data.","We codify a range of SQIs, demonstrate them using established benchmark data, and show that they can be effective for signal quality assessment.","We also study alternative approaches to denoising time-series data in an attempt to improve the quality of the already degraded signal, and evaluate them empirically on relevant real-world data.","To our knowledge, our software toolkit is the first to provide an open source implementation of a broad range of signal quality assessment and improvement techniques validated on publicly available benchmark data for ease of reproducibility.","The generality of our framework can be easily extended to assessing reliability of arbitrary time-series measurements in complex systems, especially when morphological patterns of the waveform shapes and signal periodicity are of key interest in downstream analyses."],"url":"http://arxiv.org/abs/2402.00803v1","category":"cs.LG"}
{"created":"2024-02-01 17:39:39","title":"Intrinsic Kerr amplification for microwave electromechanics","abstract":"Electromechanical transduction gain of 24 dB is realized in a micro-cantilever resonant force sensor operated in the unresolved sideband regime. Strain-dependent kinetic inductance weakly couples cantilever motion to a superconducting nonlinear resonant circuit. A single pump generates motional sidebands and parametrically amplifies them via four-wave mixing. We study the gain and added noise, and we analyze potential benefits of this integrated amplification process in the context force sensitivity.","sentences":["Electromechanical transduction gain of 24 dB is realized in a micro-cantilever resonant force sensor operated in the unresolved sideband regime.","Strain-dependent kinetic inductance weakly couples cantilever motion to a superconducting nonlinear resonant circuit.","A single pump generates motional sidebands and parametrically amplifies them via four-wave mixing.","We study the gain and added noise, and we analyze potential benefits of this integrated amplification process in the context force sensitivity."],"url":"http://arxiv.org/abs/2402.00802v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-01 17:30:50","title":"Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents","abstract":"Recent advancements on Large Language Models (LLMs) enable AI Agents to automatically generate and execute multi-step plans to solve complex tasks. However, since LLM's content generation process is hardly controllable, current LLM-based agents frequently generate invalid or non-executable plans, which jeopardizes the performance of the generated plans and corrupts users' trust in LLM-based agents. In response, this paper proposes a novel ``Formal-LLM'' framework for LLM-based agents by integrating the expressiveness of natural language and the precision of formal language. Specifically, the framework allows human users to express their requirements or constraints for the planning process as an automaton. A stack-based LLM plan generation process is then conducted under the supervision of the automaton to ensure that the generated plan satisfies the constraints, making the planning process controllable. We conduct experiments on both benchmark tasks and practical real-life tasks, and our framework achieves over 50% overall performance increase, which validates the feasibility and effectiveness of employing Formal-LLM to guide the plan generation of agents, preventing the agents from generating invalid and unsuccessful plans. Further, more controllable LLM-based agents can facilitate the broader utilization of LLM in application scenarios where high validity of planning is essential. The work is open-sourced at https://github.com/agiresearch/Formal-LLM.","sentences":["Recent advancements on Large Language Models (LLMs) enable AI Agents to automatically generate and execute multi-step plans to solve complex tasks.","However, since LLM's content generation process is hardly controllable, current LLM-based agents frequently generate invalid or non-executable plans, which jeopardizes the performance of the generated plans and corrupts users' trust in LLM-based agents.","In response, this paper proposes a novel ``Formal-LLM'' framework for LLM-based agents by integrating the expressiveness of natural language and the precision of formal language.","Specifically, the framework allows human users to express their requirements or constraints for the planning process as an automaton.","A stack-based LLM plan generation process is then conducted under the supervision of the automaton to ensure that the generated plan satisfies the constraints, making the planning process controllable.","We conduct experiments on both benchmark tasks and practical real-life tasks, and our framework achieves over 50% overall performance increase, which validates the feasibility and effectiveness of employing Formal-LLM to guide the plan generation of agents, preventing the agents from generating invalid and unsuccessful plans.","Further, more controllable LLM-based agents can facilitate the broader utilization of LLM in application scenarios where high validity of planning is essential.","The work is open-sourced at https://github.com/agiresearch/Formal-LLM."],"url":"http://arxiv.org/abs/2402.00798v1","category":"cs.LG"}
{"created":"2024-02-01 17:28:10","title":"LLMs learn governing principles of dynamical systems, revealing an in-context neural scaling law","abstract":"Pretrained large language models (LLMs) are surprisingly effective at performing zero-shot tasks, including time-series forecasting. However, understanding the mechanisms behind such capabilities remains highly challenging due to the complexity of the models. In this paper, we study LLMs' ability to extrapolate the behavior of dynamical systems whose evolution is governed by principles of physical interest. Our results show that LLaMA 2, a language model trained primarily on texts, achieves accurate predictions of dynamical system time series without fine-tuning or prompt engineering. Moreover, the accuracy of the learned physical rules increases with the length of the input context window, revealing an in-context version of neural scaling law. Along the way, we present a flexible and efficient algorithm for extracting probability density functions of multi-digit numbers directly from LLMs.","sentences":["Pretrained large language models (LLMs) are surprisingly effective at performing zero-shot tasks, including time-series forecasting.","However, understanding the mechanisms behind such capabilities remains highly challenging due to the complexity of the models.","In this paper, we study LLMs' ability to extrapolate the behavior of dynamical systems whose evolution is governed by principles of physical interest.","Our results show that LLaMA 2, a language model trained primarily on texts, achieves accurate predictions of dynamical system time series without fine-tuning or prompt engineering.","Moreover, the accuracy of the learned physical rules increases with the length of the input context window, revealing an in-context version of neural scaling law.","Along the way, we present a flexible and efficient algorithm for extracting probability density functions of multi-digit numbers directly from LLMs."],"url":"http://arxiv.org/abs/2402.00795v1","category":"cs.LG"}
{"created":"2024-02-01 17:25:51","title":"ReAGent: Towards A Model-agnostic Feature Attribution Method for Generative Language Models","abstract":"Feature attribution methods (FAs), such as gradients and attention, are widely employed approaches to derive the importance of all input features to the model predictions. Existing work in natural language processing has mostly focused on developing and testing FAs for encoder-only language models (LMs) in classification tasks. However, it is unknown if it is faithful to use these FAs for decoder-only models on text generation, due to the inherent differences between model architectures and task settings respectively. Moreover, previous work has demonstrated that there is no `one-wins-all' FA across models and tasks. This makes the selection of a FA computationally expensive for large LMs since input importance derivation often requires multiple forward and backward passes including gradient computations that might be prohibitive even with access to large compute. To address these issues, we present a model-agnostic FA for generative LMs called Recursive Attribution Generator (ReAGent). Our method updates the token importance distribution in a recursive manner. For each update, we compute the difference in the probability distribution over the vocabulary for predicting the next token between using the original input and using a modified version where a part of the input is replaced with RoBERTa predictions. Our intuition is that replacing an important token in the context should have resulted in a larger change in the model's confidence in predicting the token than replacing an unimportant token. Our method can be universally applied to any generative LM without accessing internal model weights or additional training and fine-tuning, as most other FAs require. We extensively compare the faithfulness of ReAGent with seven popular FAs across six decoder-only LMs of various sizes. The results show that our method consistently provides more faithful token importance distributions.","sentences":["Feature attribution methods (FAs), such as gradients and attention, are widely employed approaches to derive the importance of all input features to the model predictions.","Existing work in natural language processing has mostly focused on developing and testing FAs for encoder-only language models (LMs) in classification tasks.","However, it is unknown if it is faithful to use these FAs for decoder-only models on text generation, due to the inherent differences between model architectures and task settings respectively.","Moreover, previous work has demonstrated that there is no `one-wins-all' FA across models and tasks.","This makes the selection of a FA computationally expensive for large LMs since input importance derivation often requires multiple forward and backward passes including gradient computations that might be prohibitive even with access to large compute.","To address these issues, we present a model-agnostic FA for generative LMs called Recursive Attribution Generator (ReAGent).","Our method updates the token importance distribution in a recursive manner.","For each update, we compute the difference in the probability distribution over the vocabulary for predicting the next token between using the original input and using a modified version where a part of the input is replaced with RoBERTa predictions.","Our intuition is that replacing an important token in the context should have resulted in a larger change in the model's confidence in predicting the token than replacing an unimportant token.","Our method can be universally applied to any generative LM without accessing internal model weights or additional training and fine-tuning, as most other FAs require.","We extensively compare the faithfulness of ReAGent with seven popular FAs across six decoder-only LMs of various sizes.","The results show that our method consistently provides more faithful token importance distributions."],"url":"http://arxiv.org/abs/2402.00794v1","category":"cs.CL"}
{"created":"2024-02-01 17:23:54","title":"Distinguishing the Indistinguishable: Human Expertise in Algorithmic Prediction","abstract":"We introduce a novel framework for incorporating human expertise into algorithmic predictions. Our approach focuses on the use of human judgment to distinguish inputs which `look the same' to any feasible predictive algorithm. We argue that this framing clarifies the problem of human/AI collaboration in prediction tasks, as experts often have access to information -- particularly subjective information -- which is not encoded in the algorithm's training data. We use this insight to develop a set of principled algorithms for selectively incorporating human feedback only when it improves the performance of any feasible predictor. We find empirically that although algorithms often outperform their human counterparts on average, human judgment can significantly improve algorithmic predictions on specific instances (which can be identified ex-ante). In an X-ray classification task, we find that this subset constitutes nearly 30% of the patient population. Our approach provides a natural way of uncovering this heterogeneity and thus enabling effective human-AI collaboration.","sentences":["We introduce a novel framework for incorporating human expertise into algorithmic predictions.","Our approach focuses on the use of human judgment to distinguish inputs which `look the same' to any feasible predictive algorithm.","We argue that this framing clarifies the problem of human/AI collaboration in prediction tasks, as experts often have access to information -- particularly subjective information -- which is not encoded in the algorithm's training data.","We use this insight to develop a set of principled algorithms for selectively incorporating human feedback only when it improves the performance of any feasible predictor.","We find empirically that although algorithms often outperform their human counterparts on average, human judgment can significantly improve algorithmic predictions on specific instances (which can be identified ex-ante).","In an X-ray classification task, we find that this subset constitutes nearly 30% of the patient population.","Our approach provides a natural way of uncovering this heterogeneity and thus enabling effective human-AI collaboration."],"url":"http://arxiv.org/abs/2402.00793v1","category":"cs.LG"}
{"created":"2024-02-01 17:23:32","title":"Quantum fluctuation dynamics of open quantum systems with collective operator-valued rates, and applications to Hopfield-like networks","abstract":"We consider a class of open quantum many-body systems that evolves in a Markovian fashion, the dynamical generator being in GKS-Lindblad form. Here, the Hamiltonian contribution is characterized by an all-to-all coupling, and the dissipation features local transitions that depend on collective, operator-valued rates, encoding average properties of the system. These types of generators can be formally obtained by generalizing, to the quantum realm, classical (mean-field) stochastic Markov dynamics, with state-dependent transitions. Focusing on the dynamics emerging in the limit of infinitely large systems, we build on the exactness of the mean-field equations for the dynamics of average operators. In this framework, we derive the dynamics of quantum fluctuation operators, that can be used in turn to understand the fate of quantum correlations in the system. We apply our results to quantum generalized Hopfield associative memories, showing that, asymptotically and at the mesoscopic scale only a very weak amount of quantum correlations, in the form of quantum discord, emerges beyond classical correlations.","sentences":["We consider a class of open quantum many-body systems that evolves in a Markovian fashion, the dynamical generator being in GKS-Lindblad form.","Here, the Hamiltonian contribution is characterized by an all-to-all coupling, and the dissipation features local transitions that depend on collective, operator-valued rates, encoding average properties of the system.","These types of generators can be formally obtained by generalizing, to the quantum realm, classical (mean-field) stochastic Markov dynamics, with state-dependent transitions.","Focusing on the dynamics emerging in the limit of infinitely large systems, we build on the exactness of the mean-field equations for the dynamics of average operators.","In this framework, we derive the dynamics of quantum fluctuation operators, that can be used in turn to understand the fate of quantum correlations in the system.","We apply our results to quantum generalized Hopfield associative memories, showing that, asymptotically and at the mesoscopic scale only a very weak amount of quantum correlations, in the form of quantum discord, emerges beyond classical correlations."],"url":"http://arxiv.org/abs/2402.00792v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-01 17:23:21","title":"Hausdorff Reductions and the Exponential Hierarchies","abstract":"The Strong Exponential Hierarchy $SEH$ was shown to collapse to $P^{NExp}$ by Hemachandra by proving $P^{NExp} = NP^{NExp}$ via a census argument. Nonetheless, Hemachandra also asked for certificate-based and alternating Turing machine characterizations of the $SEH$ levels, in the hope that these might have revealed deeper structural reasons behind the collapse. These open questions have thus far remained unanswered.   To close them, by building upon the notion of Hausdorff reductions, we investigate a natural normal form for the intermediate levels of the (generalized) exponential hierarchies, i.e., the single-, the double-Exponential Hierarchy, and so on. Although the two characterizations asked for derive from our Hausdorff characterization, it is nevertheless from the latter that a surprising structural reason behind the collapse of $SEH$ is uncovered as a consequence of a very general result: the intermediate levels of the exponential hierarchies are precisely characterized by specific \"Hausdorff classes\", which define these levels without resorting to oracle machines. By this, contrarily to oracle classes, which may have different shapes for a same class (e.g., $P^{NP}_{||} = P^{NP[Log]} = LogSpace^{NP}$), hierarchy intermediate levels are univocally identified by Hausdorff classes (under the hypothesis of no hierarchy collapse). In fact, we show that the rather simple reason behind many equivalences of oracle classes is that they just refer to different ways of deciding the languages of a same Hausdorff class, and this happens also for $P^{NExp}$ and $NP^{NExp}$.   In addition, via Hausdorff classes, we define complete problems for various intermediate levels of the exponential hierarchies. Through these, we obtain matching lower-bounds for problems known to be in $P^{NExp[Log]}$, but whose hardness was left open due to the lack of known $P^{NExp[Log]}$-complete problems.","sentences":["The Strong Exponential Hierarchy $SEH$ was shown to collapse to $P^{NExp}$ by Hemachandra by proving $P^{NExp} = NP^{NExp}$ via a census argument.","Nonetheless, Hemachandra also asked for certificate-based and alternating Turing machine characterizations of the $SEH$ levels, in the hope that these might have revealed deeper structural reasons behind the collapse.","These open questions have thus far remained unanswered.   ","To close them, by building upon the notion of Hausdorff reductions, we investigate a natural normal form for the intermediate levels of the (generalized) exponential hierarchies, i.e., the single-, the double-Exponential Hierarchy, and so on.","Although the two characterizations asked for derive from our Hausdorff characterization, it is nevertheless from the latter that a surprising structural reason behind the collapse of $SEH$ is uncovered as a consequence of a very general result: the intermediate levels of the exponential hierarchies are precisely characterized by specific \"Hausdorff classes\", which define these levels without resorting to oracle machines.","By this, contrarily to oracle classes, which may have different shapes for a same class (e.g., $P^{NP}_{||} = P^{NP[Log]} = LogSpace^{NP}$), hierarchy intermediate levels are univocally identified by Hausdorff classes (under the hypothesis of no hierarchy collapse).","In fact, we show that the rather simple reason behind many equivalences of oracle classes is that they just refer to different ways of deciding the languages of a same Hausdorff class, and this happens also for $P^{NExp}$ and $NP^{NExp}$.   In addition, via Hausdorff classes, we define complete problems for various intermediate levels of the exponential hierarchies.","Through these, we obtain matching lower-bounds for problems known to be in $P^{NExp[Log]}$, but whose hardness was left open due to the lack of known $P^{NExp[Log]}$-complete problems."],"url":"http://arxiv.org/abs/2402.00791v1","category":"cs.CC"}
{"created":"2024-02-01 17:21:53","title":"Graph-Mamba: Towards Long-Range Graph Sequence Modeling with Selective State Spaces","abstract":"Attention mechanisms have been widely used to capture long-range dependencies among nodes in Graph Transformers. Bottlenecked by the quadratic computational cost, attention mechanisms fail to scale in large graphs. Recent improvements in computational efficiency are mainly achieved by attention sparsification with random or heuristic-based graph subsampling, which falls short in data-dependent context reasoning. State space models (SSMs), such as Mamba, have gained prominence for their effectiveness and efficiency in modeling long-range dependencies in sequential data. However, adapting SSMs to non-sequential graph data presents a notable challenge. In this work, we introduce Graph-Mamba, the first attempt to enhance long-range context modeling in graph networks by integrating a Mamba block with the input-dependent node selection mechanism. Specifically, we formulate graph-centric node prioritization and permutation strategies to enhance context-aware reasoning, leading to a substantial improvement in predictive performance. Extensive experiments on ten benchmark datasets demonstrate that Graph-Mamba outperforms state-of-the-art methods in long-range graph prediction tasks, with a fraction of the computational cost in both FLOPs and GPU memory consumption. The code and models are publicly available at https://github.com/bowang-lab/Graph-Mamba.","sentences":["Attention mechanisms have been widely used to capture long-range dependencies among nodes in Graph Transformers.","Bottlenecked by the quadratic computational cost, attention mechanisms fail to scale in large graphs.","Recent improvements in computational efficiency are mainly achieved by attention sparsification with random or heuristic-based graph subsampling, which falls short in data-dependent context reasoning.","State space models (SSMs), such as Mamba, have gained prominence for their effectiveness and efficiency in modeling long-range dependencies in sequential data.","However, adapting SSMs to non-sequential graph data presents a notable challenge.","In this work, we introduce Graph-Mamba, the first attempt to enhance long-range context modeling in graph networks by integrating a Mamba block with the input-dependent node selection mechanism.","Specifically, we formulate graph-centric node prioritization and permutation strategies to enhance context-aware reasoning, leading to a substantial improvement in predictive performance.","Extensive experiments on ten benchmark datasets demonstrate that Graph-Mamba outperforms state-of-the-art methods in long-range graph prediction tasks, with a fraction of the computational cost in both FLOPs and GPU memory consumption.","The code and models are publicly available at https://github.com/bowang-lab/Graph-Mamba."],"url":"http://arxiv.org/abs/2402.00789v1","category":"cs.LG"}
{"created":"2024-02-01 17:21:45","title":"Learning and Calibrating Heterogeneous Bounded Rational Market Behaviour with Multi-Agent Reinforcement Learning","abstract":"Agent-based models (ABMs) have shown promise for modelling various real world phenomena incompatible with traditional equilibrium analysis. However, a critical concern is the manual definition of behavioural rules in ABMs. Recent developments in multi-agent reinforcement learning (MARL) offer a way to address this issue from an optimisation perspective, where agents strive to maximise their utility, eliminating the need for manual rule specification. This learning-focused approach aligns with established economic and financial models through the use of rational utility-maximising agents. However, this representation departs from the fundamental motivation for ABMs: that realistic dynamics emerging from bounded rationality and agent heterogeneity can be modelled. To resolve this apparent disparity between the two approaches, we propose a novel technique for representing heterogeneous processing-constrained agents within a MARL framework. The proposed approach treats agents as constrained optimisers with varying degrees of strategic skills, permitting departure from strict utility maximisation. Behaviour is learnt through repeated simulations with policy gradients to adjust action likelihoods. To allow efficient computation, we use parameterised shared policy learning with distributions of agent skill levels. Shared policy learning avoids the need for agents to learn individual policies yet still enables a spectrum of bounded rational behaviours. We validate our model's effectiveness using real-world data on a range of canonical $n$-agent settings, demonstrating significantly improved predictive capability.","sentences":["Agent-based models (ABMs) have shown promise for modelling various real world phenomena incompatible with traditional equilibrium analysis.","However, a critical concern is the manual definition of behavioural rules in ABMs.","Recent developments in multi-agent reinforcement learning (MARL) offer a way to address this issue from an optimisation perspective, where agents strive to maximise their utility, eliminating the need for manual rule specification.","This learning-focused approach aligns with established economic and financial models through the use of rational utility-maximising agents.","However, this representation departs from the fundamental motivation for ABMs: that realistic dynamics emerging from bounded rationality and agent heterogeneity can be modelled.","To resolve this apparent disparity between the two approaches, we propose a novel technique for representing heterogeneous processing-constrained agents within a MARL framework.","The proposed approach treats agents as constrained optimisers with varying degrees of strategic skills, permitting departure from strict utility maximisation.","Behaviour is learnt through repeated simulations with policy gradients to adjust action likelihoods.","To allow efficient computation, we use parameterised shared policy learning with distributions of agent skill levels.","Shared policy learning avoids the need for agents to learn individual policies yet still enables a spectrum of bounded rational behaviours.","We validate our model's effectiveness using real-world data on a range of canonical $n$-agent settings, demonstrating significantly improved predictive capability."],"url":"http://arxiv.org/abs/2402.00787v1","category":"cs.MA"}
{"created":"2024-02-01 17:17:55","title":"CroissantLLM: A Truly Bilingual French-English Language Model","abstract":"We introduce CroissantLLM, a 1.3B language model pretrained on a set of 3T English and French tokens, to bring to the research and industrial community a high-performance, fully open-sourced bilingual model that runs swiftly on consumer-grade local hardware. To that end, we pioneer the approach of training an intrinsically bilingual model with a 1:1 English-to-French pretraining data ratio, a custom tokenizer, and bilingual finetuning datasets. We release the training dataset, notably containing a French split with manually curated, high-quality, and varied data sources. To assess performance outside of English, we craft a novel benchmark, FrenchBench, consisting of an array of classification and generation tasks, covering various orthogonal aspects of model performance in the French Language. Additionally, rooted in transparency and to foster further Large Language Model research, we release codebases, and dozens of checkpoints across various model sizes, training data distributions, and training steps, as well as fine-tuned Chat models, and strong translation models. We evaluate our model through the FMTI framework, and validate 81 % of the transparency criteria, far beyond the scores of even most open initiatives. This work enriches the NLP landscape, breaking away from previous English-centric work in order to strengthen our understanding of multilinguality in language models.","sentences":["We introduce CroissantLLM, a 1.3B language model pretrained on a set of 3T English and French tokens, to bring to the research and industrial community a high-performance, fully open-sourced bilingual model that runs swiftly on consumer-grade local hardware.","To that end, we pioneer the approach of training an intrinsically bilingual model with a 1:1 English-to-French pretraining data ratio, a custom tokenizer, and bilingual finetuning datasets.","We release the training dataset, notably containing a French split with manually curated, high-quality, and varied data sources.","To assess performance outside of English, we craft a novel benchmark, FrenchBench, consisting of an array of classification and generation tasks, covering various orthogonal aspects of model performance in the French Language.","Additionally, rooted in transparency and to foster further Large Language Model research, we release codebases, and dozens of checkpoints across various model sizes, training data distributions, and training steps, as well as fine-tuned Chat models, and strong translation models.","We evaluate our model through the FMTI framework, and validate 81 % of the transparency criteria, far beyond the scores of even most open initiatives.","This work enriches the NLP landscape, breaking away from previous English-centric work in order to strengthen our understanding of multilinguality in language models."],"url":"http://arxiv.org/abs/2402.00786v1","category":"cs.CL"}
{"created":"2024-02-01 17:13:12","title":"Apparent Dark Matter Inspired by Einstein Equation of State","abstract":"The purpose of this article is twofold. First, by means of Padmanabhan's proposal on the emergence nature of gravity, we recover the $\\Lambda$CDM model and the effect of the dark matter in the context of cosmology. Toward this goal, we use the key idea of Padmanabhan that states cosmic space emerges as the cosmic time progress and link the emergence of space to the difference between the number of degrees of freedom on the boundary and in the bulk. Interestingly enough, we show that the effect of the cold dark matter in the cosmological setup can be understood by assuming an interaction between the numbers of degrees of freedom in the bulk. In the second part, we follow the Jacobson's argument and obtain the modified Einstein field equations with additional dark matter component emerging due to the interaction term between dark energy and baryonic matter related by $\\Omega_{DM,0}=\\sqrt{2 \\alpha \\Omega_{M,0} \\Omega_{DE,0}}$, where $\\alpha$ is a coupling constant. Finally, a correspondence with Yukawa cosmology is pointed out, and the role of massive gravitons as a possibility in explaining the nature of the dark sector as well as the theoretical origin of the Modified Newtonian Dynamics (MOND) are addressed. We speculate that the interaction coupling $\\alpha$ fundamentally measures the entanglement between the gravitons and matter fields and there exists a fundamental limitation in measuring the gravitons wavelength.","sentences":["The purpose of this article is twofold.","First, by means of Padmanabhan's proposal on the emergence nature of gravity, we recover the $\\Lambda$CDM model and the effect of the dark matter in the context of cosmology.","Toward this goal, we use the key idea of Padmanabhan that states cosmic space emerges as the cosmic time progress and link the emergence of space to the difference between the number of degrees of freedom on the boundary and in the bulk.","Interestingly enough, we show that the effect of the cold dark matter in the cosmological setup can be understood by assuming an interaction between the numbers of degrees of freedom in the bulk.","In the second part, we follow the Jacobson's argument and obtain the modified Einstein field equations with additional dark matter component emerging due to the interaction term between dark energy and baryonic matter related by $\\Omega_{DM,0}=\\sqrt{2 \\alpha \\Omega_{M,0} \\Omega_{DE,0}}$, where $\\alpha$ is a coupling constant.","Finally, a correspondence with Yukawa cosmology is pointed out, and the role of massive gravitons as a possibility in explaining the nature of the dark sector as well as the theoretical origin of the Modified Newtonian Dynamics (MOND) are addressed.","We speculate that the interaction coupling $\\alpha$ fundamentally measures the entanglement between the gravitons and matter fields and there exists a fundamental limitation in measuring the gravitons wavelength."],"url":"http://arxiv.org/abs/2402.00785v1","category":"gr-qc"}
{"created":"2024-02-01 17:11:35","title":"High-resolution MHz time- and angle-resolved photoemission spectroscopy based on a tunable vacuum ultraviolet source","abstract":"Time and angle-resolved photoemission spectroscopy (trARPES) allows direct mapping of the electronic band structure and its dynamic response on femtosecond timescales. Here, we present a new ARPES system, powered by a new fiber-based femtosecond light source in the vacuum ultraviolet (VUV) range, accessing the complete first Brillouin zone for most materials. We present trARPES data on Au(111), polycrystalline Au, Bi2Se3 and TaTe2, demonstrating an energy resolution of 21 meV with a time resolution of <360 fs, at a high repetition rate of 1 MHz. The system is integrated with an extreme ultraviolet (EUV) high harmonic generation (HHG) beamline, enabling excellent tunability of the time-bandwidth resolution.","sentences":["Time and angle-resolved photoemission spectroscopy (trARPES) allows direct mapping of the electronic band structure and its dynamic response on femtosecond timescales.","Here, we present a new ARPES system, powered by a new fiber-based femtosecond light source in the vacuum ultraviolet (VUV) range, accessing the complete first Brillouin zone for most materials.","We present trARPES data on Au(111), polycrystalline Au, Bi2Se3 and TaTe2, demonstrating an energy resolution of 21 meV with a time resolution of <360 fs, at a high repetition rate of 1 MHz.","The system is integrated with an extreme ultraviolet (EUV) high harmonic generation (HHG) beamline, enabling excellent tunability of the time-bandwidth resolution."],"url":"http://arxiv.org/abs/2402.00784v1","category":"physics.ins-det"}
{"created":"2024-02-01 17:10:42","title":"Element-specific and high-bandwidth ferromagnetic resonance spectroscopy with a coherent, extreme ultraviolet (EUV) source","abstract":"We developed and applied a tabletop, ultrafast, high-harmonic generation (HHG) source to measure the element-specific ferromagnetic resonance (FMR) in ultra-thin magnetic alloys and multilayers on an opaque Si substrate. We demonstrate a continuous wave bandwidth of 62 GHz, with promise to extend to 100 GHz or higher. This laboratory-scale instrument detects the FMR using ultrafast, extreme ultraviolet (EUV) light, with photon energies spanning the M-edges of most relevant magnetic elements. An RF frequency comb generator is used to produce a microwave excitation that is intrinsically synchronized to the EUV pulses with a timing jitter of 1.4 ps or better. We apply this system to measure the dynamics in a multilayer system as well as Ni-Fe and Co-Fe alloys. Since this instrument operates in reflection-mode, it is a milestone toward measuring and imaging the dynamics of the magnetic state and spin transport of active devices on arbitrary and opaque substrates. The higher bandwidth also enables measurements of materials with high magnetic anisotropy, as well as ferrimagnets, antiferromagnets, and short-wavelength (high wavevector) spinwaves in nanostructures or nanodevices. Furthermore, the coherence and short wavelength of the EUV will enable extending these studies using dynamic nanoscale lensless imaging techniques such as coherent diffractive imaging, ptychography, and holography.","sentences":["We developed and applied a tabletop, ultrafast, high-harmonic generation (HHG) source to measure the element-specific ferromagnetic resonance (FMR) in ultra-thin magnetic alloys and multilayers on an opaque Si substrate.","We demonstrate a continuous wave bandwidth of 62 GHz, with promise to extend to 100 GHz or higher.","This laboratory-scale instrument detects the FMR using ultrafast, extreme ultraviolet (EUV) light, with photon energies spanning the M-edges of most relevant magnetic elements.","An RF frequency comb generator is used to produce a microwave excitation that is intrinsically synchronized to the EUV pulses with a timing jitter of 1.4 ps or better.","We apply this system to measure the dynamics in a multilayer system as well as Ni-Fe and Co-Fe alloys.","Since this instrument operates in reflection-mode, it is a milestone toward measuring and imaging the dynamics of the magnetic state and spin transport of active devices on arbitrary and opaque substrates.","The higher bandwidth also enables measurements of materials with high magnetic anisotropy, as well as ferrimagnets, antiferromagnets, and short-wavelength (high wavevector) spinwaves in nanostructures or nanodevices.","Furthermore, the coherence and short wavelength of the EUV will enable extending these studies using dynamic nanoscale lensless imaging techniques such as coherent diffractive imaging, ptychography, and holography."],"url":"http://arxiv.org/abs/2402.00783v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-01 17:10:35","title":"Dense Reward for Free in Reinforcement Learning from Human Feedback","abstract":"Reinforcement Learning from Human Feedback (RLHF) has been credited as the key advance that has allowed Large Language Models (LLMs) to effectively follow instructions and produce useful assistance. Classically, this involves generating completions from the LLM in response to a query before using a separate reward model to assign a score to the full completion. As an auto-regressive process, the LLM has to take many \"actions\" (selecting individual tokens) and only receives a single, sparse reward at the end of an episode, a setup that is known to be difficult to optimise in traditional reinforcement learning. In this work we leverage the fact that the reward model contains more information than just its scalar output, in particular, it calculates an attention map over tokens as part of the transformer architecture. We use these attention weights to redistribute the reward along the whole completion, effectively densifying the signal and highlighting the most important tokens, all without incurring extra computational cost or requiring any additional modelling. We demonstrate that, theoretically, this approach is equivalent to potential-based reward shaping, ensuring that the optimal policy remains unchanged. Empirically, we show that it stabilises training, accelerates the rate of learning, and, in practical cases, may lead to better local optima.","sentences":["Reinforcement Learning from Human Feedback (RLHF) has been credited as the key advance that has allowed Large Language Models (LLMs) to effectively follow instructions and produce useful assistance.","Classically, this involves generating completions from the LLM in response to a query before using a separate reward model to assign a score to the full completion.","As an auto-regressive process, the LLM has to take many \"actions\" (selecting individual tokens) and only receives a single, sparse reward at the end of an episode, a setup that is known to be difficult to optimise in traditional reinforcement learning.","In this work we leverage the fact that the reward model contains more information than just its scalar output, in particular, it calculates an attention map over tokens as part of the transformer architecture.","We use these attention weights to redistribute the reward along the whole completion, effectively densifying the signal and highlighting the most important tokens, all without incurring extra computational cost or requiring any additional modelling.","We demonstrate that, theoretically, this approach is equivalent to potential-based reward shaping, ensuring that the optimal policy remains unchanged.","Empirically, we show that it stabilises training, accelerates the rate of learning, and, in practical cases, may lead to better local optima."],"url":"http://arxiv.org/abs/2402.00782v1","category":"cs.LG"}
{"created":"2024-02-01 17:01:50","title":"Neural Risk Limiting Dispatch in Power Networks: Formulation and Generalization Guarantees","abstract":"Risk limiting dispatch (RLD) has been proposed as an approach that effectively trades off economic costs with operational risks for power dispatch under uncertainty. However, how to solve the RLD problem with provably near-optimal performance still remains an open problem. This paper presents a learning-based solution to this challenge. We first design a data-driven formulation for the RLD problem, which aims to construct a decision rule that directly maps day-ahead observable information to cost-effective dispatch decisions for the future delivery interval. Unlike most existing works that follow a predict-then-optimize paradigm, this end-to-end rule bypasses the additional suboptimality introduced by separately handling prediction and optimization. We then propose neural RLD, a novel solution method to the data-driven formulation. This method leverages an L2-regularized neural network to learn the decision rule, thereby transforming the data-driven formulation into a neural network training task that can be efficiently completed by stochastic gradient descent. A theoretical performance guarantee is further established to bound the suboptimality of our method, which implies that its suboptimality approaches to zero with high probability as more samples are utilized. Simulation tests across various systems demonstrate our method's superior performance in convergence, suboptimality, and computational efficiency compared with benchmarks.","sentences":["Risk limiting dispatch (RLD) has been proposed as an approach that effectively trades off economic costs with operational risks for power dispatch under uncertainty.","However, how to solve the RLD problem with provably near-optimal performance still remains an open problem.","This paper presents a learning-based solution to this challenge.","We first design a data-driven formulation for the RLD problem, which aims to construct a decision rule that directly maps day-ahead observable information to cost-effective dispatch decisions for the future delivery interval.","Unlike most existing works that follow a predict-then-optimize paradigm, this end-to-end rule bypasses the additional suboptimality introduced by separately handling prediction and optimization.","We then propose neural RLD, a novel solution method to the data-driven formulation.","This method leverages an L2-regularized neural network to learn the decision rule, thereby transforming the data-driven formulation into a neural network training task that can be efficiently completed by stochastic gradient descent.","A theoretical performance guarantee is further established to bound the suboptimality of our method, which implies that its suboptimality approaches to zero with high probability as more samples are utilized.","Simulation tests across various systems demonstrate our method's superior performance in convergence, suboptimality, and computational efficiency compared with benchmarks."],"url":"http://arxiv.org/abs/2402.00772v1","category":"math.OC"}
{"created":"2024-02-01 16:59:59","title":"Mixed Static and Reconfigurable Metasurface Deployment in Indoor Dense Spaces: How Much Reconfigurability is Needed?","abstract":"In this paper, we investigate how metasurfaces can be deployed to deliver high data rates in a millimeter-wave (mmWave) indoor dense space with many blocking objects. These surfaces can either be static metasurfaces (SMSs) that reflect with fixed phase-shifts or reconfigurable intelligent surfaces (RISs) that can reconfigure their phase-shifts to the currently served user. The latter comes with an increased power, cabling, and signaling cost. To see how reconfigurability affects the network performance, we propose an iterative algorithm based on the feasible point pursuit successive convex approximation method. We jointly optimize the types and phase-shifts of the surfaces and the time portion allocated to each user equipment to maximize the minimum data rate achieved by the network. Our numerical results demonstrate that the minimum data rate improves as more RISs are introduced but the gain diminishes after some point. Therefore, introducing more reconfigurability is not always necessary. Another result shows that to reach the same data rate achieved by using 22 SMSs, at least 18 RISs are needed. This suggests that when it is costly to deploy many RISs, as an inexpensive alternative solution, one can reach the same data rate just by densely deploying more SMSs.","sentences":["In this paper, we investigate how metasurfaces can be deployed to deliver high data rates in a millimeter-wave (mmWave) indoor dense space with many blocking objects.","These surfaces can either be static metasurfaces (SMSs) that reflect with fixed phase-shifts or reconfigurable intelligent surfaces (RISs) that can reconfigure their phase-shifts to the currently served user.","The latter comes with an increased power, cabling, and signaling cost.","To see how reconfigurability affects the network performance, we propose an iterative algorithm based on the feasible point pursuit successive convex approximation method.","We jointly optimize the types and phase-shifts of the surfaces and the time portion allocated to each user equipment to maximize the minimum data rate achieved by the network.","Our numerical results demonstrate that the minimum data rate improves as more RISs are introduced but the gain diminishes after some point.","Therefore, introducing more reconfigurability is not always necessary.","Another result shows that to reach the same data rate achieved by using 22 SMSs, at least 18 RISs are needed.","This suggests that when it is costly to deploy many RISs, as an inexpensive alternative solution, one can reach the same data rate just by densely deploying more SMSs."],"url":"http://arxiv.org/abs/2402.00771v1","category":"cs.IT"}
{"created":"2024-02-01 16:58:11","title":"AnimateLCM: Accelerating the Animation of Personalized Diffusion Models and Adapters with Decoupled Consistency Learning","abstract":"Video diffusion models has been gaining increasing attention for its ability to produce videos that are both coherent and of high fidelity. However, the iterative denoising process makes it computationally intensive and time-consuming, thus limiting its applications. Inspired by the Consistency Model (CM) that distills pretrained image diffusion models to accelerate the sampling with minimal steps and its successful extension Latent Consistency Model (LCM) on conditional image generation, we propose AnimateLCM, allowing for high-fidelity video generation within minimal steps. Instead of directly conducting consistency learning on the raw video dataset, we propose a decoupled consistency learning strategy that decouples the distillation of image generation priors and motion generation priors, which improves the training efficiency and enhance the generation visual quality. Additionally, to enable the combination of plug-and-play adapters in stable diffusion community to achieve various functions (e.g., ControlNet for controllable generation). we propose an efficient strategy to adapt existing adapters to our distilled text-conditioned video consistency model or train adapters from scratch without harming the sampling speed. We validate the proposed strategy in image-conditioned video generation and layout-conditioned video generation, all achieving top-performing results. Experimental results validate the effectiveness of our proposed method. Code and weights will be made public. More details are available at https://github.com/G-U-N/AnimateLCM.","sentences":["Video diffusion models has been gaining increasing attention for its ability to produce videos that are both coherent and of high fidelity.","However, the iterative denoising process makes it computationally intensive and time-consuming, thus limiting its applications.","Inspired by the Consistency Model (CM) that distills pretrained image diffusion models to accelerate the sampling with minimal steps and its successful extension Latent Consistency Model (LCM) on conditional image generation, we propose AnimateLCM, allowing for high-fidelity video generation within minimal steps.","Instead of directly conducting consistency learning on the raw video dataset, we propose a decoupled consistency learning strategy that decouples the distillation of image generation priors and motion generation priors, which improves the training efficiency and enhance the generation visual quality.","Additionally, to enable the combination of plug-and-play adapters in stable diffusion community to achieve various functions (e.g., ControlNet for controllable generation).","we propose an efficient strategy to adapt existing adapters to our distilled text-conditioned video consistency model or train adapters from scratch without harming the sampling speed.","We validate the proposed strategy in image-conditioned video generation and layout-conditioned video generation, all achieving top-performing results.","Experimental results validate the effectiveness of our proposed method.","Code and weights will be made public.","More details are available at https://github.com/G-U-N/AnimateLCM."],"url":"http://arxiv.org/abs/2402.00769v1","category":"cs.CV"}
{"created":"2024-02-01 16:55:07","title":"Benchmarking Multipartite Entanglement Generation with Graph States","abstract":"As quantum computing technology slowly matures and the number of available qubits on a QPU gradually increases, interest in assessing the capabilities of quantum computing hardware in a scalable manner is growing. One of the key properties for quantum computing is the ability to generate multipartite entangled states. In this paper, aspects of benchmarking entanglement generation capabilities of noisy intermediate-scale quantum (NISQ) devices are discussed based on the preparation of graph states and the verification of entanglement in the prepared states. Thereby, we use entanglement witnesses that are specifically suited for a scalable experiment design. This choice of entanglement witnesses can detect A) bipartite entanglement and B) genuine multipartite entanglement for graph states with constant two measurement settings if the prepared graph state is based on a 2-colorable graph, e.g., a square grid graph or one of its subgraphs. With this, we experimentally verify that a fully bipartite entangled state can be prepared on a 127-qubit IBM Quantum superconducting QPU, and genuine multipartite entanglement can be detected for states of up to 23 qubits with quantum readout error mitigation.","sentences":["As quantum computing technology slowly matures and the number of available qubits on a QPU gradually increases, interest in assessing the capabilities of quantum computing hardware in a scalable manner is growing.","One of the key properties for quantum computing is the ability to generate multipartite entangled states.","In this paper, aspects of benchmarking entanglement generation capabilities of noisy intermediate-scale quantum (NISQ) devices are discussed based on the preparation of graph states and the verification of entanglement in the prepared states.","Thereby, we use entanglement witnesses that are specifically suited for a scalable experiment design.","This choice of entanglement witnesses can detect A) bipartite entanglement and B) genuine multipartite entanglement for graph states with constant two measurement settings if the prepared graph state is based on a 2-colorable graph, e.g., a square grid graph or one of its subgraphs.","With this, we experimentally verify that a fully bipartite entangled state can be prepared on a 127-qubit IBM Quantum superconducting QPU, and genuine multipartite entanglement can be detected for states of up to 23 qubits with quantum readout error mitigation."],"url":"http://arxiv.org/abs/2402.00766v1","category":"quant-ph"}
{"created":"2024-02-01 16:53:15","title":"To Search or To Gen? Exploring the Synergy between Generative AI and Web Search in Programming","abstract":"The convergence of generative AI and web search is reshaping problem-solving for programmers. However, the lack of understanding regarding their interplay in the information-seeking process often leads programmers to perceive them as alternatives rather than complementary tools. To analyze this interaction and explore their synergy, we conducted an interview study with eight experienced programmers. Drawing from the results and literature, we have identified three major challenges and proposed three decision-making stages, each with its own relevant factors. Additionally, we present a comprehensive process model that captures programmers' interaction patterns. This model encompasses decision-making stages, the information-foraging loop, and cognitive activities during system interaction, offering a holistic framework to comprehend and optimize the use of these convergent tools in programming.","sentences":["The convergence of generative AI and web search is reshaping problem-solving for programmers.","However, the lack of understanding regarding their interplay in the information-seeking process often leads programmers to perceive them as alternatives rather than complementary tools.","To analyze this interaction and explore their synergy, we conducted an interview study with eight experienced programmers.","Drawing from the results and literature, we have identified three major challenges and proposed three decision-making stages, each with its own relevant factors.","Additionally, we present a comprehensive process model that captures programmers' interaction patterns.","This model encompasses decision-making stages, the information-foraging loop, and cognitive activities during system interaction, offering a holistic framework to comprehend and optimize the use of these convergent tools in programming."],"url":"http://arxiv.org/abs/2402.00764v1","category":"cs.HC"}
{"created":"2024-02-01 16:52:05","title":"Hypergeometric systems from groups with torsion","abstract":"We consider $A$-hypergeometric (or GKZ-)systems in the case where the grading (character) group is an arbitrary finitely generated Abelian group. Emulating the approach taken for classical GKZ-systems in arXiv:math/0406383 that allows for a coefficient module, we show that these $D$-modules are holonomic systems. For this purpose we formulate an Euler--Koszul complex in this context, built on an extension of the category of $A$-toric modules. We derive that these new systems are regular holonomic under circumstances that are similar to those that lead to regular holonomic classical GKZ-systems.   For the appropriate coefficient module, our $D$-modules specialize to the \"better behaved GKZ-systems\" introduced by Borisov and Horja. We certify the corresponding $D$-modules as regular holonomic, and establish a holonomic duality on the level of $D$-modules that was suggested on the level of solutions by Borisov and Horja and later shown by Borisov and Han in a special situation (arXiv:1308.2238, arXiv:2301.01374).","sentences":["We consider $A$-hypergeometric (or GKZ-)systems in the case where the grading (character) group is an arbitrary finitely generated Abelian group.","Emulating the approach taken for classical GKZ-systems in arXiv:math/0406383 that allows for a coefficient module, we show that these $D$-modules are holonomic systems.","For this purpose we formulate an Euler--Koszul complex in this context, built on an extension of the category of $A$-toric modules.","We derive that these new systems are regular holonomic under circumstances that are similar to those that lead to regular holonomic classical GKZ-systems.   ","For the appropriate coefficient module, our $D$-modules specialize to the \"better behaved GKZ-systems\" introduced by Borisov and Horja.","We certify the corresponding $D$-modules as regular holonomic, and establish a holonomic duality on the level of $D$-modules that was suggested on the level of solutions by Borisov and Horja and later shown by Borisov and Han in a special situation (arXiv:1308.2238, arXiv:2301.01374)."],"url":"http://arxiv.org/abs/2402.00762v1","category":"math.AG"}
{"created":"2024-02-01 16:51:11","title":"Control-Theoretic Techniques for Online Adaptation of Deep Neural Networks in Dynamical Systems","abstract":"Deep neural networks (DNNs), trained with gradient-based optimization and backpropagation, are currently the primary tool in modern artificial intelligence, machine learning, and data science. In many applications, DNNs are trained offline, through supervised learning or reinforcement learning, and deployed online for inference. However, training DNNs with standard backpropagation and gradient-based optimization gives no intrinsic performance guarantees or bounds on the DNN, which is essential for applications such as controls. Additionally, many offline-training and online-inference problems, such as sim2real transfer of reinforcement learning policies, experience domain shift from the training distribution to the real-world distribution. To address these stability and transfer learning issues, we propose using techniques from control theory to update DNN parameters online. We formulate the fully-connected feedforward DNN as a continuous-time dynamical system, and we propose novel last-layer update laws that guarantee desirable error convergence under various conditions on the time derivative of the DNN input vector. We further show that training the DNN under spectral normalization controls the upper bound of the error trajectories of the online DNN predictions, which is desirable when numerically differentiated quantities or noisy state measurements are input to the DNN. The proposed online DNN adaptation laws are validated in simulation to learn the dynamics of the Van der Pol system under domain shift, where parameters are varied in inference from the training dataset. The simulations demonstrate the effectiveness of using control-theoretic techniques to derive performance improvements and guarantees in DNN-based learning systems.","sentences":["Deep neural networks (DNNs), trained with gradient-based optimization and backpropagation, are currently the primary tool in modern artificial intelligence, machine learning, and data science.","In many applications, DNNs are trained offline, through supervised learning or reinforcement learning, and deployed online for inference.","However, training DNNs with standard backpropagation and gradient-based optimization gives no intrinsic performance guarantees or bounds on the DNN, which is essential for applications such as controls.","Additionally, many offline-training and online-inference problems, such as sim2real transfer of reinforcement learning policies, experience domain shift from the training distribution to the real-world distribution.","To address these stability and transfer learning issues, we propose using techniques from control theory to update DNN parameters online.","We formulate the fully-connected feedforward DNN as a continuous-time dynamical system, and we propose novel last-layer update laws that guarantee desirable error convergence under various conditions on the time derivative of the DNN input vector.","We further show that training the DNN under spectral normalization controls the upper bound of the error trajectories of the online DNN predictions, which is desirable when numerically differentiated quantities or noisy state measurements are input to the DNN.","The proposed online DNN adaptation laws are validated in simulation to learn the dynamics of the Van der Pol system under domain shift, where parameters are varied in inference from the training dataset.","The simulations demonstrate the effectiveness of using control-theoretic techniques to derive performance improvements and guarantees in DNN-based learning systems."],"url":"http://arxiv.org/abs/2402.00761v1","category":"cs.LG"}
{"created":"2024-02-01 16:50:41","title":"EuroPED-NN: Uncertainty aware surrogate model","abstract":"This work successfully generates uncertainty aware surrogate models, via the Bayesian neural network with noise contrastive prior (BNN-NCP) technique, of the EuroPED plasma pedestal model using data from the JET-ILW pedestal database and subsequent model evaluations. All this conform EuroPED-NN. The BNN-NCP technique is proven to be a good fit for uncertainty aware surrogate models, matching the output results as a regular neural network, providing prediction's confidence as uncertainties, and highlighting the out of distribution (OOD) regions using surrogate model uncertainties. This provides critical insights into model robustness and reliability. EuroPED-NN has been physically validated, first, analyzing electron density $n_e\\!\\left(\\psi_{\\text{pol}}=0.94\\right)$ with respect to increasing plasma current, $I_p$, and second, validating the $\\Delta-\\beta_{p,ped}$ relation associated with the EuroPED model. Affirming the robustness of the underlying physics learned by the surrogate model.","sentences":["This work successfully generates uncertainty aware surrogate models, via the Bayesian neural network with noise contrastive prior (BNN-NCP) technique, of the EuroPED plasma pedestal model using data from the JET-ILW pedestal database and subsequent model evaluations.","All this conform EuroPED-NN.","The BNN-NCP technique is proven to be a good fit for uncertainty aware surrogate models, matching the output results as a regular neural network, providing prediction's confidence as uncertainties, and highlighting the out of distribution (OOD) regions using surrogate model uncertainties.","This provides critical insights into model robustness and reliability.","EuroPED-NN has been physically validated, first, analyzing electron density $n_e\\!\\left(\\psi_{\\text{pol}}=0.94\\right)$ with respect to increasing plasma current, $I_p$, and second, validating the $\\Delta-\\beta_{p,ped}$ relation associated with the EuroPED model.","Affirming the robustness of the underlying physics learned by the surrogate model."],"url":"http://arxiv.org/abs/2402.00760v1","category":"physics.plasm-ph"}
{"created":"2024-02-01 16:49:27","title":"Building Expressive and Tractable Probabilistic Generative Models: A Review","abstract":"We present a comprehensive survey of the advancements and techniques in the field of tractable probabilistic generative modeling, primarily focusing on Probabilistic Circuits (PCs). We provide a unified perspective on the inherent trade-offs between expressivity and the tractability, highlighting the design principles and algorithmic extensions that have enabled building expressive and efficient PCs, and provide a taxonomy of the field. We also discuss recent efforts to build deep and hybrid PCs by fusing notions from deep neural models, and outline the challenges and open questions that can guide future research in this evolving field.","sentences":["We present a comprehensive survey of the advancements and techniques in the field of tractable probabilistic generative modeling, primarily focusing on Probabilistic Circuits (PCs).","We provide a unified perspective on the inherent trade-offs between expressivity and the tractability, highlighting the design principles and algorithmic extensions that have enabled building expressive and efficient PCs, and provide a taxonomy of the field.","We also discuss recent efforts to build deep and hybrid PCs by fusing notions from deep neural models, and outline the challenges and open questions that can guide future research in this evolving field."],"url":"http://arxiv.org/abs/2402.00759v1","category":"cs.LG"}
{"created":"2024-02-01 16:45:49","title":"$V_{lowk}$ Renormalization Group Flow, Vector Manifestation and Sound Velocity in Massive Compact Stars","abstract":"The $V_{lowk}$-renormalization group approach on Fermi sea for nuclear matter makes the pivotal input in the formulation of the present form of the nuclear effective field theory as a generalized effective field theory acronymed ``G$n$EFT.\" A topology change in terms of skyrmions and half-skyrmions is shown to play the role of the ``putative\" hadron-quark continuity (HQC)\" conjectured in QCD. Crucially involved are hidden local (``HLS\") and hidden scale (``HSS\") symmetries with the vacuum sliding with density in nuclear medium, with the nuclear tensor force emerging as a Landau Fermi-liquid fixed-point quantity. A possibly novel paradigm in nuclear theory is suggested.","sentences":["The $V_{lowk}$-renormalization group approach on Fermi sea for nuclear matter makes the pivotal input in the formulation of the present form of the nuclear effective field theory as a generalized effective field theory acronymed ``G$n$EFT.\"","A topology change in terms of skyrmions and half-skyrmions is shown to play the role of the ``putative\" hadron-quark continuity (HQC)\" conjectured in QCD.","Crucially involved are hidden local (``HLS\") and hidden scale (``HSS\") symmetries with the vacuum sliding with density in nuclear medium, with the nuclear tensor force emerging as a Landau Fermi-liquid fixed-point quantity.","A possibly novel paradigm in nuclear theory is suggested."],"url":"http://arxiv.org/abs/2402.00755v1","category":"nucl-th"}
{"created":"2024-02-01 16:44:03","title":"Moving curves of least gonality on symmetric products of curves","abstract":"This paper is a sequel of arXiv:2208.00990. Let $C$ be a smooth complex projective curve of genus $g$ and let $C^{(k)}$ be its $k$-fold symmetric product. The covering gonality of $C^{(k)}$ is the least gonality of an irreducible curve $E\\subset C^{(k)}$ passing through a general point of $C^{(k)}$. It follows from previous works of the authors that if $2\\leq k\\leq 4$ and $g\\geq k+4$, the covering gonality of $C^{(k)}$ equals the gonality of $C$. In this paper, we prove that under mild assumptions of generality on $C$, the only curves $E\\subset C^{(k)}$ computing the covering gonality of $C^{(k)}$ are copies of $C$ of the form $C+p$, for some point $p\\in C^{(k-1)}$. As a byproduct, we deduce that the connecting gonality of $C^{(k)}$ (i.e. the least gonality of an irreducible curve $E\\subset C^{(k)}$ connecting two general points of $C^{(k)}$) is strictly larger than the covering gonality.","sentences":["This paper is a sequel of arXiv:2208.00990.","Let $C$ be a smooth complex projective curve of genus $g$ and let $C^{(k)}$ be its $k$-fold symmetric product.","The covering gonality of $C^{(k)}$ is the least gonality of an irreducible curve $E\\subset C^{(k)}$ passing through a general point of $C^{(k)}$. It follows from previous works of the authors that if $2\\leq k\\leq 4$ and $g\\geq k+4$, the covering gonality of $C^{(k)}$ equals the gonality of $C$.","In this paper, we prove that under mild assumptions of generality on $C$, the only curves $E\\subset C^{(k)}$ computing the covering gonality of $C^{(k)}$ are copies of $C$ of the form $C+p$, for some point $p\\in C^{(k-1)}$. As a byproduct, we deduce that the connecting gonality of $C^{(k)}$ (i.e. the least gonality of an irreducible curve $E\\subset C^{(k)}$ connecting two general points of $C^{(k)}$) is strictly larger than the covering gonality."],"url":"http://arxiv.org/abs/2402.00753v1","category":"math.AG"}
{"created":"2024-02-01 16:43:04","title":"Unlearnable Algorithms for In-context Learning","abstract":"Machine unlearning is a desirable operation as models get increasingly deployed on data with unknown provenance. However, achieving exact unlearning -- obtaining a model that matches the model distribution when the data to be forgotten was never used -- is challenging or inefficient, often requiring significant retraining. In this paper, we focus on efficient unlearning methods for the task adaptation phase of a pretrained large language model (LLM). We observe that an LLM's ability to do in-context learning for task adaptation allows for efficient exact unlearning of task adaptation training data. We provide an algorithm for selecting few-shot training examples to prepend to the prompt given to an LLM (for task adaptation), ERASE, whose unlearning operation cost is independent of model and dataset size, meaning it scales to large models and datasets. We additionally compare our approach to fine-tuning approaches and discuss the trade-offs between the two approaches. This leads us to propose a new holistic measure of unlearning cost which accounts for varying inference costs, and conclude that in-context learning can often be more favourable than fine-tuning for deployments involving unlearning requests.","sentences":["Machine unlearning is a desirable operation as models get increasingly deployed on data with unknown provenance.","However, achieving exact unlearning -- obtaining a model that matches the model distribution when the data to be forgotten was never used -- is challenging or inefficient, often requiring significant retraining.","In this paper, we focus on efficient unlearning methods for the task adaptation phase of a pretrained large language model (LLM).","We observe that an LLM's ability to do in-context learning for task adaptation allows for efficient exact unlearning of task adaptation training data.","We provide an algorithm for selecting few-shot training examples to prepend to the prompt given to an LLM (for task adaptation), ERASE, whose unlearning operation cost is independent of model and dataset size, meaning it scales to large models and datasets.","We additionally compare our approach to fine-tuning approaches and discuss the trade-offs between the two approaches.","This leads us to propose a new holistic measure of unlearning cost which accounts for varying inference costs, and conclude that in-context learning can often be more favourable than fine-tuning for deployments involving unlearning requests."],"url":"http://arxiv.org/abs/2402.00751v1","category":"cs.LG"}
{"created":"2024-02-01 16:40:32","title":"Health-LLM: Personalized Retrieval-Augmented Disease Prediction Model","abstract":"Artificial intelligence (AI) in healthcare has significantly advanced intelligent medical treatment. However, traditional intelligent healthcare is limited by static data and unified standards, preventing full integration with individual situations and other challenges. Hence, a more professional and detailed intelligent healthcare method is needed for development. To this end, we propose an innovative framework named Heath-LLM, which combines large-scale feature extraction and medical knowledge trade-off scoring. Compared to traditional health management methods, our approach has three main advantages. First, our method integrates health reports into a large model to provide detailed task information. Second, professional medical expertise is used to adjust the weighted scores of health characteristics. Third, we use a semi-automated feature extraction framework to enhance the analytical power of language models and incorporate expert insights to improve the accuracy of disease prediction. We have conducted disease prediction experiments on a large number of health reports to assess the effectiveness of Health-LLM. The results of the experiments indicate that the proposed method surpasses traditional methods and has the potential to revolutionize disease prediction and personalized health management. The code is available at https://github.com/jmyissb/HealthLLM.","sentences":["Artificial intelligence (AI) in healthcare has significantly advanced intelligent medical treatment.","However, traditional intelligent healthcare is limited by static data and unified standards, preventing full integration with individual situations and other challenges.","Hence, a more professional and detailed intelligent healthcare method is needed for development.","To this end, we propose an innovative framework named Heath-LLM, which combines large-scale feature extraction and medical knowledge trade-off scoring.","Compared to traditional health management methods, our approach has three main advantages.","First, our method integrates health reports into a large model to provide detailed task information.","Second, professional medical expertise is used to adjust the weighted scores of health characteristics.","Third, we use a semi-automated feature extraction framework to enhance the analytical power of language models and incorporate expert insights to improve the accuracy of disease prediction.","We have conducted disease prediction experiments on a large number of health reports to assess the effectiveness of Health-LLM.","The results of the experiments indicate that the proposed method surpasses traditional methods and has the potential to revolutionize disease prediction and personalized health management.","The code is available at https://github.com/jmyissb/HealthLLM."],"url":"http://arxiv.org/abs/2402.00746v1","category":"cs.CL"}
{"created":"2024-02-01 16:39:51","title":"Enhancing Ethical Explanations of Large Language Models through Iterative Symbolic Refinement","abstract":"An increasing amount of research in Natural Language Inference (NLI) focuses on the application and evaluation of Large Language Models (LLMs) and their reasoning capabilities. Despite their success, however, LLMs are still prone to factual errors and inconsistencies in their explanations, offering limited control and interpretability for inference in complex domains. In this paper, we focus on ethical NLI, investigating how hybrid neuro-symbolic techniques can enhance the logical validity and alignment of ethical explanations produced by LLMs. Specifically, we present an abductive-deductive framework named Logic-Explainer, which integrates LLMs with an external backward-chaining solver to refine step-wise natural language explanations and jointly verify their correctness, reduce incompleteness and minimise redundancy. An extensive empirical analysis demonstrates that Logic-Explainer can improve explanations generated via in-context learning methods and Chain-of-Thought (CoT) on challenging ethical NLI tasks, while, at the same time, producing formal proofs describing and supporting models' reasoning. As ethical NLI requires commonsense reasoning to identify underlying moral violations, our results suggest the effectiveness of neuro-symbolic methods for multi-step NLI more broadly, opening new opportunities to enhance the logical consistency, reliability, and alignment of LLMs.","sentences":["An increasing amount of research in Natural Language Inference (NLI) focuses on the application and evaluation of Large Language Models (LLMs) and their reasoning capabilities.","Despite their success, however, LLMs are still prone to factual errors and inconsistencies in their explanations, offering limited control and interpretability for inference in complex domains.","In this paper, we focus on ethical NLI, investigating how hybrid neuro-symbolic techniques can enhance the logical validity and alignment of ethical explanations produced by LLMs.","Specifically, we present an abductive-deductive framework named Logic-Explainer, which integrates LLMs with an external backward-chaining solver to refine step-wise natural language explanations and jointly verify their correctness, reduce incompleteness and minimise redundancy.","An extensive empirical analysis demonstrates that Logic-Explainer can improve explanations generated via in-context learning methods and Chain-of-Thought (CoT) on challenging ethical NLI tasks, while, at the same time, producing formal proofs describing and supporting models' reasoning.","As ethical NLI requires commonsense reasoning to identify underlying moral violations, our results suggest the effectiveness of neuro-symbolic methods for multi-step NLI more broadly, opening new opportunities to enhance the logical consistency, reliability, and alignment of LLMs."],"url":"http://arxiv.org/abs/2402.00745v1","category":"cs.CL"}
{"created":"2024-02-01 16:39:47","title":"BATON: Aligning Text-to-Audio Model with Human Preference Feedback","abstract":"With the development of AI-Generated Content (AIGC), text-to-audio models are gaining widespread attention. However, it is challenging for these models to generate audio aligned with human preference due to the inherent information density of natural language and limited model understanding ability. To alleviate this issue, we formulate the BATON, a framework designed to enhance the alignment between generated audio and text prompt using human preference feedback. Our BATON comprises three key stages: Firstly, we curated a dataset containing both prompts and the corresponding generated audio, which was then annotated based on human feedback. Secondly, we introduced a reward model using the constructed dataset, which can mimic human preference by assigning rewards to input text-audio pairs. Finally, we employed the reward model to fine-tune an off-the-shelf text-to-audio model. The experiment results demonstrate that our BATON can significantly improve the generation quality of the original text-to-audio models, concerning audio integrity, temporal relationship, and alignment with human preference.","sentences":["With the development of AI-Generated Content (AIGC), text-to-audio models are gaining widespread attention.","However, it is challenging for these models to generate audio aligned with human preference due to the inherent information density of natural language and limited model understanding ability.","To alleviate this issue, we formulate the BATON, a framework designed to enhance the alignment between generated audio and text prompt using human preference feedback.","Our BATON comprises three key stages: Firstly, we curated a dataset containing both prompts and the corresponding generated audio, which was then annotated based on human feedback.","Secondly, we introduced a reward model using the constructed dataset, which can mimic human preference by assigning rewards to input text-audio pairs.","Finally, we employed the reward model to fine-tune an off-the-shelf text-to-audio model.","The experiment results demonstrate that our BATON can significantly improve the generation quality of the original text-to-audio models, concerning audio integrity, temporal relationship, and alignment with human preference."],"url":"http://arxiv.org/abs/2402.00744v1","category":"cs.SD"}
{"created":"2024-02-01 16:39:28","title":"Transforming and Combining Rewards for Aligning Large Language Models","abstract":"A common approach for aligning language models to human preferences is to first learn a reward model from preference data, and then use this reward model to update the language model. We study two closely related problems that arise in this approach. First, any monotone transformation of the reward model preserves preference ranking; is there a choice that is ``better'' than others? Second, we often wish to align language models to multiple properties: how should we combine multiple reward models? Using a probabilistic interpretation of the alignment procedure, we identify a natural choice for transformation for (the common case of) rewards learned from Bradley-Terry preference models. This derived transformation has two important properties. First, it emphasizes improving poorly-performing outputs, rather than outputs that already score well. This mitigates both underfitting (where some prompts are not improved) and reward hacking (where the model learns to exploit misspecification of the reward model). Second, it enables principled aggregation of rewards by linking summation to logical conjunction: the sum of transformed rewards corresponds to the probability that the output is ``good'' in all measured properties, in a sense we make precise. Experiments aligning language models to be both helpful and harmless using RLHF show substantial improvements over the baseline (non-transformed) approach.","sentences":["A common approach for aligning language models to human preferences is to first learn a reward model from preference data, and then use this reward model to update the language model.","We study two closely related problems that arise in this approach.","First, any monotone transformation of the reward model preserves preference ranking; is there a choice that is ``better'' than others?","Second, we often wish to align language models to multiple properties: how should we combine multiple reward models?","Using a probabilistic interpretation of the alignment procedure, we identify a natural choice for transformation for (the common case of) rewards learned from Bradley-Terry preference models.","This derived transformation has two important properties.","First, it emphasizes improving poorly-performing outputs, rather than outputs that already score well.","This mitigates both underfitting (where some prompts are not improved) and reward hacking (where the model learns to exploit misspecification of the reward model).","Second, it enables principled aggregation of rewards by linking summation to logical conjunction: the sum of transformed rewards corresponds to the probability that the output is ``good'' in all measured properties, in a sense we make precise.","Experiments aligning language models to be both helpful and harmless using RLHF show substantial improvements over the baseline (non-transformed) approach."],"url":"http://arxiv.org/abs/2402.00742v1","category":"cs.CL"}
{"created":"2024-02-01 16:39:18","title":"Axion Dark Matters from Cosmic String Network","abstract":"We perform the lattice simulation to estimate the axion dark matter abundance radiated from the global cosmic strings in the post-inflationary scenario. The independent numerical confirmation on the recently observed logarithmic growth in both the number of strings per Hubble patch and the spectral index of the power-law scaling for the axion spectrum is reported. These logarithmic scalings are checked against two different prescriptions for generating initial random field configurations, namely fat-string type and thermal phase transition. We discuss a possible strong correlation between axion spectrum and the string evolutions with different initial conditions to support the insensitivity of scaling behaviors against different initial data and we provide a qualitative understanding of it. The impact of various combinations of the power law of the axion spectrum, nonlinearities around the QCD scale, and average inter-string distances on the axion abundance is discussed. Additionally, we introduce a new novel string identification method, based on the tetrahedralization of the space, which guarantees the connectedness of the strings and provides a convenient way of assigning the core location. Finally we derive the lower bound on the axion mass.","sentences":["We perform the lattice simulation to estimate the axion dark matter abundance radiated from the global cosmic strings in the post-inflationary scenario.","The independent numerical confirmation on the recently observed logarithmic growth in both the number of strings per Hubble patch and the spectral index of the power-law scaling for the axion spectrum is reported.","These logarithmic scalings are checked against two different prescriptions for generating initial random field configurations, namely fat-string type and thermal phase transition.","We discuss a possible strong correlation between axion spectrum and the string evolutions with different initial conditions to support the insensitivity of scaling behaviors against different initial data and we provide a qualitative understanding of it.","The impact of various combinations of the power law of the axion spectrum, nonlinearities around the QCD scale, and average inter-string distances on the axion abundance is discussed.","Additionally, we introduce a new novel string identification method, based on the tetrahedralization of the space, which guarantees the connectedness of the strings and provides a convenient way of assigning the core location.","Finally we derive the lower bound on the axion mass."],"url":"http://arxiv.org/abs/2402.00741v1","category":"hep-ph"}
{"created":"2024-02-01 16:38:51","title":"DRSM: efficient neural 4d decomposition for dynamic reconstruction in stationary monocular cameras","abstract":"With the popularity of monocular videos generated by video sharing and live broadcasting applications, reconstructing and editing dynamic scenes in stationary monocular cameras has become a special but anticipated technology. In contrast to scene reconstructions that exploit multi-view observations, the problem of modeling a dynamic scene from a single view is significantly more under-constrained and ill-posed. Inspired by recent progress in neural rendering, we present a novel framework to tackle 4D decomposition problem for dynamic scenes in monocular cameras. Our framework utilizes decomposed static and dynamic feature planes to represent 4D scenes and emphasizes the learning of dynamic regions through dense ray casting. Inadequate 3D clues from a single-view and occlusion are also particular challenges in scene reconstruction. To overcome these difficulties, we propose deep supervised optimization and ray casting strategies. With experiments on various videos, our method generates higher-fidelity results than existing methods for single-view dynamic scene representation.","sentences":["With the popularity of monocular videos generated by video sharing and live broadcasting applications, reconstructing and editing dynamic scenes in stationary monocular cameras has become a special but anticipated technology.","In contrast to scene reconstructions that exploit multi-view observations, the problem of modeling a dynamic scene from a single view is significantly more under-constrained and ill-posed.","Inspired by recent progress in neural rendering, we present a novel framework to tackle 4D decomposition problem for dynamic scenes in monocular cameras.","Our framework utilizes decomposed static and dynamic feature planes to represent 4D scenes and emphasizes the learning of dynamic regions through dense ray casting.","Inadequate 3D clues from a single-view and occlusion are also particular challenges in scene reconstruction.","To overcome these difficulties, we propose deep supervised optimization and ray casting strategies.","With experiments on various videos, our method generates higher-fidelity results than existing methods for single-view dynamic scene representation."],"url":"http://arxiv.org/abs/2402.00740v1","category":"cs.CV"}
{"created":"2024-02-01 16:37:21","title":"FM3Q: Factorized Multi-Agent MiniMax Q-Learning for Two-Team Zero-Sum Markov Game","abstract":"Many real-world applications involve some agents that fall into two teams, with payoffs that are equal within the same team but of opposite sign across the opponent team. The so-called two-team zero-sum Markov games (2t0sMGs) can be resolved with reinforcement learning in recent years. However, existing methods are thus inefficient in light of insufficient consideration of intra-team credit assignment, data utilization and computational intractability. In this paper, we propose the individual-global-minimax (IGMM) principle to ensure the coherence between two-team minimax behaviors and the individual greedy behaviors through Q functions in 2t0sMGs. Based on it, we present a novel multi-agent reinforcement learning framework, Factorized Multi-Agent MiniMax Q-Learning (FM3Q), which can factorize the joint minimax Q function into individual ones and iteratively solve for the IGMM-satisfied minimax Q functions for 2t0sMGs. Moreover, an online learning algorithm with neural networks is proposed to implement FM3Q and obtain the deterministic and decentralized minimax policies for two-team players. A theoretical analysis is provided to prove the convergence of FM3Q. Empirically, we use three environments to evaluate the learning efficiency and final performance of FM3Q and show its superiority on 2t0sMGs.","sentences":["Many real-world applications involve some agents that fall into two teams, with payoffs that are equal within the same team but of opposite sign across the opponent team.","The so-called two-team zero-sum Markov games (2t0sMGs) can be resolved with reinforcement learning in recent years.","However, existing methods are thus inefficient in light of insufficient consideration of intra-team credit assignment, data utilization and computational intractability.","In this paper, we propose the individual-global-minimax (IGMM) principle to ensure the coherence between two-team minimax behaviors and the individual greedy behaviors through Q functions in 2t0sMGs.","Based on it, we present a novel multi-agent reinforcement learning framework, Factorized Multi-Agent MiniMax Q-Learning (FM3Q), which can factorize the joint minimax Q function into individual ones and iteratively solve for the IGMM-satisfied minimax Q functions for 2t0sMGs.","Moreover, an online learning algorithm with neural networks is proposed to implement FM3Q and obtain the deterministic and decentralized minimax policies for two-team players.","A theoretical analysis is provided to prove the convergence of FM3Q. Empirically, we use three environments to evaluate the learning efficiency and final performance of FM3Q and show its superiority on 2t0sMGs."],"url":"http://arxiv.org/abs/2402.00738v1","category":"cs.AI"}
{"created":"2024-02-01 16:36:07","title":"Global sampling of Feynman's diagrams through Normalizing Flow","abstract":"Normalizing Flows (NF) are powerful generative models with increasing applications in augmenting Monte Carlo algorithms due to their high flexibility and expressiveness. In this work we explore the integration of NF in Diagrammatic Monte Carlo (DMC), presenting an architecture designed to sample the intricate multidimensional space of Feynman''s diagrams through dimensionality reduction. By decoupling the sampling of diagram order and interaction times, the flow focuses on one interaction at a time. This enables constructing a general diagram by employing the same unsupervised model iteratively, dressing a zero-order diagram with interactions determined by the previously sampled order. The resulting NF-augmented DMC is tested on the widely used single-site Holstein polaron model in the entire electron-phonon coupling regime. The obtained data show that the model accurately reproduces the diagram distribution reducing sample correlation and statistical error for observables such as the polaron binding energy and the interacting Green's function.","sentences":["Normalizing Flows (NF) are powerful generative models with increasing applications in augmenting Monte Carlo algorithms due to their high flexibility and expressiveness.","In this work we explore the integration of NF in Diagrammatic Monte Carlo (DMC), presenting an architecture designed to sample the intricate multidimensional space of Feynman''s diagrams through dimensionality reduction.","By decoupling the sampling of diagram order and interaction times, the flow focuses on one interaction at a time.","This enables constructing a general diagram by employing the same unsupervised model iteratively, dressing a zero-order diagram with interactions determined by the previously sampled order.","The resulting NF-augmented DMC is tested on the widely used single-site Holstein polaron model in the entire electron-phonon coupling regime.","The obtained data show that the model accurately reproduces the diagram distribution reducing sample correlation and statistical error for observables such as the polaron binding energy and the interacting Green's function."],"url":"http://arxiv.org/abs/2402.00736v1","category":"cond-mat.str-el"}
{"created":"2024-02-01 16:29:07","title":"Resource-efficient and loss-aware photonic graph state preparation using an array of quantum emitters, and application to all-photonic quantum repeaters","abstract":"Multi-qubit photonic graph states are necessary for quantum communication and computation. Preparing photonic graph states using probabilistic stitching of single photons using linear optics results in a formidable resource requirement due to the need of multiplexing. Quantum emitters present a viable solution to prepare photonic graph states, as they enable controlled production of photons entangled with the emitter qubit, and deterministic two-qubit interactions among emitters. A handful of emitters often suffice to generate useful photonic graph states that would otherwise require millions of single photon sources using the linear-optics method. But, photon loss poses an impediment to this method due to the large depth, i.e., age of the oldest photon, of the graph state, given the typically large number of slow and noisy two-qubit CNOT gates required on emitters. We propose an algorithm that can trade the number of emitters with the graph-state depth, while minimizing the number of emitter CNOTs. We apply our algorithm to generating a repeater graph state (RGS) for all-photonic repeaters. We find that our scheme achieves a far superior rate-vs.-distance performance than using the least number of emitters needed to generate the RGS. Yet, our scheme is able to get the same performance as the linear-optics method of generating the RGS where each emitter is used as a single-photon source, but with orders of magnitude fewer emitters.","sentences":["Multi-qubit photonic graph states are necessary for quantum communication and computation.","Preparing photonic graph states using probabilistic stitching of single photons using linear optics results in a formidable resource requirement due to the need of multiplexing.","Quantum emitters present a viable solution to prepare photonic graph states, as they enable controlled production of photons entangled with the emitter qubit, and deterministic two-qubit interactions among emitters.","A handful of emitters often suffice to generate useful photonic graph states that would otherwise require millions of single photon sources using the linear-optics method.","But, photon loss poses an impediment to this method due to the large depth, i.e., age of the oldest photon, of the graph state, given the typically large number of slow and noisy two-qubit CNOT gates required on emitters.","We propose an algorithm that can trade the number of emitters with the graph-state depth, while minimizing the number of emitter CNOTs.","We apply our algorithm to generating a repeater graph state (RGS) for all-photonic repeaters.","We find that our scheme achieves a far superior rate-vs.-distance performance than using the least number of emitters needed to generate the RGS.","Yet, our scheme is able to get the same performance as the linear-optics method of generating the RGS where each emitter is used as a single-photon source, but with orders of magnitude fewer emitters."],"url":"http://arxiv.org/abs/2402.00731v1","category":"quant-ph"}
{"created":"2024-02-01 16:28:01","title":"Gravitational Wave Emission from Close-in Strange Quark Planets Around Strange Stars","abstract":"According to the strange quark matter hypothesis, strange planets may exist, which are planetary mass objects composed of almost equal numbers of up, down and strange quarks. A strange planet can revolve around its host strange star in a very close-in orbit. When it finally merges with the host, strong gravitational wave emissions will be generated. Here the gravitational waveforms are derived for the merging process, taking into account the effects of the strange star's magnetic field on the dynamics. Effects of the inclination angle are also considered. Templates of the gravitational waveforms are derived. It is found that the magnetic interactions significantly speed up the merging process. Coalescence events of such strange planetary systems occurring in our Galaxy as well as in local galaxies can be effectively detected by current and future gravitational experiments, which may hopefully provide a new method to test the strange quark matter hypothesis and probe the magnetic field of compact stars.","sentences":["According to the strange quark matter hypothesis, strange planets may exist, which are planetary mass objects composed of almost equal numbers of up, down and strange quarks.","A strange planet can revolve around its host strange star in a very close-in orbit.","When it finally merges with the host, strong gravitational wave emissions will be generated.","Here the gravitational waveforms are derived for the merging process, taking into account the effects of the strange star's magnetic field on the dynamics.","Effects of the inclination angle are also considered.","Templates of the gravitational waveforms are derived.","It is found that the magnetic interactions significantly speed up the merging process.","Coalescence events of such strange planetary systems occurring in our Galaxy as well as in local galaxies can be effectively detected by current and future gravitational experiments, which may hopefully provide a new method to test the strange quark matter hypothesis and probe the magnetic field of compact stars."],"url":"http://arxiv.org/abs/2402.00730v1","category":"astro-ph.HE"}
{"created":"2024-02-01 16:26:30","title":"Profiling and Modeling of Power Characteristics of Leadership-Scale HPC System Workloads","abstract":"In the exascale era in which application behavior has large power & energy footprints, per-application job-level awareness of such impression is crucial in taking steps towards achieving efficiency goals beyond performance, such as energy efficiency, and sustainability.   To achieve these goals, we have developed a novel low-latency job power profiling machine learning pipeline that can group job-level power profiles based on their shapes as they complete. This pipeline leverages a comprehensive feature extraction and clustering pipeline powered by a generative adversarial network (GAN) model to handle the feature-rich time series of job-level power measurements. The output is then used to train a classification model that can predict whether an incoming job power profile is similar to a known group of profiles or is completely new. With extensive evaluations, we demonstrate the effectiveness of each component in our pipeline. Also, we provide a preliminary analysis of the resulting clusters that depict the power profile landscape of the Summit supercomputer from more than 60K jobs sampled from the year 2021.","sentences":["In the exascale era in which application behavior has large power & energy footprints, per-application job-level awareness of such impression is crucial in taking steps towards achieving efficiency goals beyond performance, such as energy efficiency, and sustainability.   ","To achieve these goals, we have developed a novel low-latency job power profiling machine learning pipeline that can group job-level power profiles based on their shapes as they complete.","This pipeline leverages a comprehensive feature extraction and clustering pipeline powered by a generative adversarial network (GAN) model to handle the feature-rich time series of job-level power measurements.","The output is then used to train a classification model that can predict whether an incoming job power profile is similar to a known group of profiles or is completely new.","With extensive evaluations, we demonstrate the effectiveness of each component in our pipeline.","Also, we provide a preliminary analysis of the resulting clusters that depict the power profile landscape of the Summit supercomputer from more than 60K jobs sampled from the year 2021."],"url":"http://arxiv.org/abs/2402.00729v1","category":"cs.DC"}
{"created":"2024-02-01 16:25:00","title":"Dropout-Based Rashomon Set Exploration for Efficient Predictive Multiplicity Estimation","abstract":"Predictive multiplicity refers to the phenomenon in which classification tasks may admit multiple competing models that achieve almost-equally-optimal performance, yet generate conflicting outputs for individual samples. This presents significant concerns, as it can potentially result in systemic exclusion, inexplicable discrimination, and unfairness in practical applications. Measuring and mitigating predictive multiplicity, however, is computationally challenging due to the need to explore all such almost-equally-optimal models, known as the Rashomon set, in potentially huge hypothesis spaces. To address this challenge, we propose a novel framework that utilizes dropout techniques for exploring models in the Rashomon set. We provide rigorous theoretical derivations to connect the dropout parameters to properties of the Rashomon set, and empirically evaluate our framework through extensive experimentation. Numerical results show that our technique consistently outperforms baselines in terms of the effectiveness of predictive multiplicity metric estimation, with runtime speedup up to $20\\times \\sim 5000\\times$. With efficient Rashomon set exploration and metric estimation, mitigation of predictive multiplicity is then achieved through dropout ensemble and model selection.","sentences":["Predictive multiplicity refers to the phenomenon in which classification tasks may admit multiple competing models that achieve almost-equally-optimal performance, yet generate conflicting outputs for individual samples.","This presents significant concerns, as it can potentially result in systemic exclusion, inexplicable discrimination, and unfairness in practical applications.","Measuring and mitigating predictive multiplicity, however, is computationally challenging due to the need to explore all such almost-equally-optimal models, known as the Rashomon set, in potentially huge hypothesis spaces.","To address this challenge, we propose a novel framework that utilizes dropout techniques for exploring models in the Rashomon set.","We provide rigorous theoretical derivations to connect the dropout parameters to properties of the Rashomon set, and empirically evaluate our framework through extensive experimentation.","Numerical results show that our technique consistently outperforms baselines in terms of the effectiveness of predictive multiplicity metric estimation, with runtime speedup up to $20\\times \\sim 5000\\times$. With efficient Rashomon set exploration and metric estimation, mitigation of predictive multiplicity is then achieved through dropout ensemble and model selection."],"url":"http://arxiv.org/abs/2402.00728v1","category":"cs.LG"}
{"created":"2024-02-01 16:20:03","title":"A Bi-Objective Optimization Based Acquisition Strategy for Batch Bayesian Global Optimization","abstract":"In this paper, we deal with batch Bayesian Optimization (Bayes-Opt) problems over a box and we propose a novel bi-objective optimization (BOO) acquisition strategy to sample points where to evaluate the objective function. The BOO problem involves the Gaussian Process posterior mean and variance functions, which, in most of the acquisition strategies from the literature, are generally used in combination, frequently through scalarization. However, such scalarization could compromise the Bayes-Opt process performance, as getting the desired trade-off between exploration and exploitation is not trivial in most cases. We instead aim to reconstruct the Pareto front of the BOO problem based on optimizing both the posterior mean as well as the variance, thus generating multiple trade-offs without any a priori knowledge. The reconstruction is performed through the Non-dominated Sorting Memetic Algorithm (NSMA), recently proposed in the literature and proved to be effective in solving hard MOO problems. Finally, we present two clustering approaches, each of them operating on a different space, to select potentially optimal points from the Pareto front. We compare our methodology with well-known acquisition strategies from the literature, showing its effectiveness on a wide set of experiments.","sentences":["In this paper, we deal with batch Bayesian Optimization (Bayes-Opt) problems over a box and we propose a novel bi-objective optimization (BOO) acquisition strategy to sample points where to evaluate the objective function.","The BOO problem involves the Gaussian Process posterior mean and variance functions, which, in most of the acquisition strategies from the literature, are generally used in combination, frequently through scalarization.","However, such scalarization could compromise the Bayes-Opt process performance, as getting the desired trade-off between exploration and exploitation is not trivial in most cases.","We instead aim to reconstruct the Pareto front of the BOO problem based on optimizing both the posterior mean as well as the variance, thus generating multiple trade-offs without any a priori knowledge.","The reconstruction is performed through the Non-dominated Sorting Memetic Algorithm (NSMA), recently proposed in the literature and proved to be effective in solving hard MOO problems.","Finally, we present two clustering approaches, each of them operating on a different space, to select potentially optimal points from the Pareto front.","We compare our methodology with well-known acquisition strategies from the literature, showing its effectiveness on a wide set of experiments."],"url":"http://arxiv.org/abs/2402.00726v1","category":"math.OC"}
{"created":"2024-02-01 16:14:35","title":"Improving Semantic Control in Discrete Latent Spaces with Transformer Quantized Variational Autoencoders","abstract":"Achieving precise semantic control over the latent spaces of Variational AutoEncoders (VAEs) holds significant value for downstream tasks in NLP as the underlying generative mechanisms could be better localised, explained and improved upon. Recent research, however, has struggled to achieve consistent results, primarily due to the inevitable loss of semantic information in the variational bottleneck and limited control over the decoding mechanism. To overcome these challenges, we investigate discrete latent spaces in Vector Quantized Variational AutoEncoders (VQVAEs) to improve semantic control and generation in Transformer-based VAEs. In particular, We propose T5VQVAE, a novel model that leverages the controllability of VQVAEs to guide the self-attention mechanism in T5 at the token-level, exploiting its full generalization capabilities. Experimental results indicate that T5VQVAE outperforms existing state-of-the-art VAE models, including Optimus, in terms of controllability and preservation of semantic information across different tasks such as auto-encoding of sentences and mathematical expressions, text transfer, and inference. Moreover, T5VQVAE exhibits improved inference capabilities, suggesting potential applications for downstream natural language and symbolic reasoning tasks.","sentences":["Achieving precise semantic control over the latent spaces of Variational AutoEncoders (VAEs) holds significant value for downstream tasks in NLP as the underlying generative mechanisms could be better localised, explained and improved upon.","Recent research, however, has struggled to achieve consistent results, primarily due to the inevitable loss of semantic information in the variational bottleneck and limited control over the decoding mechanism.","To overcome these challenges, we investigate discrete latent spaces in Vector Quantized Variational AutoEncoders (VQVAEs) to improve semantic control and generation in Transformer-based VAEs.","In particular, We propose T5VQVAE, a novel model that leverages the controllability of VQVAEs to guide the self-attention mechanism in T5 at the token-level, exploiting its full generalization capabilities.","Experimental results indicate that T5VQVAE outperforms existing state-of-the-art VAE models, including Optimus, in terms of controllability and preservation of semantic information across different tasks such as auto-encoding of sentences and mathematical expressions, text transfer, and inference.","Moreover, T5VQVAE exhibits improved inference capabilities, suggesting potential applications for downstream natural language and symbolic reasoning tasks."],"url":"http://arxiv.org/abs/2402.00723v1","category":"cs.CL"}
{"created":"2024-02-01 16:14:32","title":"Neural Style Transfer with Twin-Delayed DDPG for Shared Control of Robotic Manipulators","abstract":"Neural Style Transfer (NST) refers to a class of algorithms able to manipulate an element, most often images, to adopt the appearance or style of another one. Each element is defined as a combination of Content and Style: the Content can be conceptually defined as the what and the Style as the how of said element. In this context, we propose a custom NST framework for transferring a set of styles to the motion of a robotic manipulator, e.g., the same robotic task can be carried out in an angry, happy, calm, or sad way. An autoencoder architecture extracts and defines the Content and the Style of the target robot motions. A Twin Delayed Deep Deterministic Policy Gradient (TD3) network generates the robot control policy using the loss defined by the autoencoder. The proposed Neural Policy Style Transfer TD3 (NPST3) alters the robot motion by introducing the trained style. Such an approach can be implemented either offline, for carrying out autonomous robot motions in dynamic environments, or online, for adapting at runtime the style of a teleoperated robot. The considered styles can be learned online from human demonstrations. We carried out an evaluation with human subjects enrolling 73 volunteers, asking them to recognize the style behind some representative robotic motions. Results show a good recognition rate, proving that it is possible to convey different styles to a robot using this approach.","sentences":["Neural Style Transfer (NST) refers to a class of algorithms able to manipulate an element, most often images, to adopt the appearance or style of another one.","Each element is defined as a combination of Content and Style: the Content can be conceptually defined as the what and the Style as the how of said element.","In this context, we propose a custom NST framework for transferring a set of styles to the motion of a robotic manipulator, e.g., the same robotic task can be carried out in an angry, happy, calm, or sad way.","An autoencoder architecture extracts and defines the Content and the Style of the target robot motions.","A Twin Delayed Deep Deterministic Policy Gradient (TD3) network generates the robot control policy using the loss defined by the autoencoder.","The proposed Neural Policy Style Transfer TD3 (NPST3) alters the robot motion by introducing the trained style.","Such an approach can be implemented either offline, for carrying out autonomous robot motions in dynamic environments, or online, for adapting at runtime the style of a teleoperated robot.","The considered styles can be learned online from human demonstrations.","We carried out an evaluation with human subjects enrolling 73 volunteers, asking them to recognize the style behind some representative robotic motions.","Results show a good recognition rate, proving that it is possible to convey different styles to a robot using this approach."],"url":"http://arxiv.org/abs/2402.00722v1","category":"cs.RO"}
{"created":"2024-02-01 16:14:17","title":"Multi-scale magnetic field investigation of the M-dwarf eclipsing binary CU Cancri","abstract":"We aim to characterise the magnetic field of the eclipsing binary CU Cnc. The determination of magnetic field parameters of this target enables comparisons with both observations of similar stars and theoretical predictions of the magnetic field strength for CU Cnc. The target is therefore providing an excellent opportunity to test our understanding of the generation of magnetic fields in low-mass stars and its impact on stellar structure. We use spectropolarimetric observations obtained with ESPaDOnS to investigate the magnetic properties of CU Cnc. We generate average line profiles with LSD, which are used to extract information about the radial velocities of the components, expanding the number of radial velocity measurements available and allowing for a determination of orbital parameters. Stokes V LSD profiles are used with ZDI to obtain large-scale magnetic field structures on both components. We also use polarised radiative transfer modelling to investigate the small-scale fields by utilising Zeeman splitting of magnetically sensitive Ti I lines in non-polarised spectra. The large-scale fields are dominantly poloidal and have an average strength of ~100 G on both components. This analysis of the large-scale fields likely suffers from some amount of hemisphere degeneracy due to the high inclination of the target. Both components also show unusual magnetic field configurations compared to stars with similar parameters, the primary is weakly axisymmetric (~10%) and the secondary has a strong torroidal contribution (~20%). The small-scale fields are significantly stronger, at 3.1 and 3.6 kG for the primary and secondary respectively. This measurement is in excellent agreement with surface field strength predictions for CU Cnc from magnetoconvective stellar evolution models. These results indicates that magnetic fields play a significant role in radius inflation of active stars.","sentences":["We aim to characterise the magnetic field of the eclipsing binary CU Cnc.","The determination of magnetic field parameters of this target enables comparisons with both observations of similar stars and theoretical predictions of the magnetic field strength for CU Cnc.","The target is therefore providing an excellent opportunity to test our understanding of the generation of magnetic fields in low-mass stars and its impact on stellar structure.","We use spectropolarimetric observations obtained with ESPaDOnS to investigate the magnetic properties of CU Cnc.","We generate average line profiles with LSD, which are used to extract information about the radial velocities of the components, expanding the number of radial velocity measurements available and allowing for a determination of orbital parameters.","Stokes V LSD profiles are used with ZDI to obtain large-scale magnetic field structures on both components.","We also use polarised radiative transfer modelling to investigate the small-scale fields by utilising Zeeman splitting of magnetically sensitive Ti I lines in non-polarised spectra.","The large-scale fields are dominantly poloidal and have an average strength of ~100 G on both components.","This analysis of the large-scale fields likely suffers from some amount of hemisphere degeneracy due to the high inclination of the target.","Both components also show unusual magnetic field configurations compared to stars with similar parameters, the primary is weakly axisymmetric (~10%) and the secondary has a strong torroidal contribution (~20%).","The small-scale fields are significantly stronger, at 3.1 and 3.6 kG for the primary and secondary respectively.","This measurement is in excellent agreement with surface field strength predictions for CU Cnc from magnetoconvective stellar evolution models.","These results indicates that magnetic fields play a significant role in radius inflation of active stars."],"url":"http://arxiv.org/abs/2402.00721v1","category":"astro-ph.SR"}
{"created":"2024-02-01 16:09:34","title":"A census of genus 6 curves over $\\mathbb{F}_2$","abstract":"We compile a complete list of isomorphism class representatives of curves of genus 6 over $\\mathbb{F}_2$. We use explicit descriptions of canonical curves in each stratum of the Brill--Noether stratification of the moduli space $\\mathcal{M}_6$, due to Mukai in the generic case. Our computed value of $\\#\\mathcal{M}_6(\\mathbb{F}_2)$ agrees with the Lefschetz trace formula as recently computed by Bergstrom--Canning--Petersen--Schmitt.","sentences":["We compile a complete list of isomorphism class representatives of curves of genus 6 over $\\mathbb{F}_2$. We use explicit descriptions of canonical curves in each stratum of the Brill--Noether stratification of the moduli space $\\mathcal{M}_6$, due to Mukai in the generic case.","Our computed value of $\\#\\mathcal{M}_6(\\mathbb{F}_2)$ agrees with the Lefschetz trace formula as recently computed by Bergstrom--Canning--Petersen--Schmitt."],"url":"http://arxiv.org/abs/2402.00716v1","category":"math.AG"}
{"created":"2024-02-01 16:09:19","title":"Intent Assurance using LLMs guided by Intent Drift","abstract":"Intent-Based Networking (IBN) presents a paradigm shift for network management, by promising to align intents and business objectives with network operations--in an automated manner. However, its practical realization is challenging: 1) processing intents, i.e., translate, decompose and identify the logic to fulfill the intent, and 2) intent conformance, that is, considering dynamic networks, the logic should be adequately adapted to assure intents. To address the latter, intent assurance is tasked with continuous verification and validation, including taking the necessary actions to align the operational and target states. In this paper, we define an assurance framework that allows us to detect and act when intent drift occurs. To do so, we leverage AI-driven policies, generated by Large Language Models (LLMs) which can quickly learn the necessary in-context requirements, and assist with the fulfillment and assurance of intents.","sentences":["Intent-Based Networking (IBN) presents a paradigm shift for network management, by promising to align intents and business objectives with network operations--in an automated manner.","However, its practical realization is challenging: 1) processing intents, i.e., translate, decompose and identify the logic to fulfill the intent, and 2) intent conformance, that is, considering dynamic networks, the logic should be adequately adapted to assure intents.","To address the latter, intent assurance is tasked with continuous verification and validation, including taking the necessary actions to align the operational and target states.","In this paper, we define an assurance framework that allows us to detect and act when intent drift occurs.","To do so, we leverage AI-driven policies, generated by Large Language Models (LLMs) which can quickly learn the necessary in-context requirements, and assist with the fulfillment and assurance of intents."],"url":"http://arxiv.org/abs/2402.00715v1","category":"cs.AI"}
{"created":"2024-02-01 16:07:27","title":"Distribution-uniform strong laws of large numbers","abstract":"We revisit the question of whether the strong law of large numbers (SLLN) holds uniformly in a rich family of distributions, culminating in a distribution-uniform generalization of the Marcinkiewicz-Zygmund SLLN. These results can be viewed as extensions of Chung's distribution-uniform SLLN to random variables with uniformly integrable $q^\\text{th}$ absolute central moments for $0 < q < 2;\\ q \\neq 1$. Furthermore, we show that uniform integrability of the $q^\\text{th}$ moment is both sufficient and necessary for the SLLN to hold uniformly at the Marcinkiewicz-Zygmund rate of $n^{1/q - 1}$. These proofs centrally rely on distribution-uniform analogues of some familiar almost sure convergence results including the Khintchine-Kolmogorov convergence theorem, Kolmogorov's three-series theorem, a stochastic generalization of Kronecker's lemma, and the Borel-Cantelli lemmas. The non-identically distributed case is also considered.","sentences":["We revisit the question of whether the strong law of large numbers (SLLN) holds uniformly in a rich family of distributions, culminating in a distribution-uniform generalization of the Marcinkiewicz-Zygmund SLLN.","These results can be viewed as extensions of Chung's distribution-uniform SLLN to random variables with uniformly integrable $q^\\text{th}$ absolute central moments for $0 < q < 2;\\ q \\neq 1$.","Furthermore, we show that uniform integrability of the $q^\\text{th}$ moment is both sufficient and necessary for the SLLN to hold uniformly at the Marcinkiewicz-Zygmund rate of $n^{1/q - 1}$.","These proofs centrally rely on distribution-uniform analogues of some familiar almost sure convergence results including the Khintchine-Kolmogorov convergence theorem, Kolmogorov's three-series theorem, a stochastic generalization of Kronecker's lemma, and the Borel-Cantelli lemmas.","The non-identically distributed case is also considered."],"url":"http://arxiv.org/abs/2402.00713v1","category":"math.PR"}
{"created":"2024-02-01 16:06:35","title":"Explaining Text Classifiers with Counterfactual Representations","abstract":"One well motivated explanation method for classifiers leverages counterfactuals which are hypothetical events identical to real observations in all aspects except for one categorical feature. Constructing such counterfactual poses specific challenges for texts, however, as some attribute values may not necessarily align with plausible real-world events. In this paper we propose a simple method for generating counterfactuals by intervening in the space of text representations which bypasses this limitation. We argue that our interventions are minimally disruptive and that they are theoretically sound as they align with counterfactuals as defined in Pearl's causal inference framework. To validate our method, we first conduct experiments on a synthetic dataset of counterfactuals, allowing for a direct comparison between classifier predictions based on ground truth counterfactuals (obtained through explicit text interventions) and our counterfactuals, derived through interventions in the representation space. Second, we study a real world scenario where our counterfactuals can be leveraged both for explaining a classifier and for bias mitigation.","sentences":["One well motivated explanation method for classifiers leverages counterfactuals which are hypothetical events identical to real observations in all aspects except for one categorical feature.","Constructing such counterfactual poses specific challenges for texts, however, as some attribute values may not necessarily align with plausible real-world events.","In this paper we propose a simple method for generating counterfactuals by intervening in the space of text representations which bypasses this limitation.","We argue that our interventions are minimally disruptive and that they are theoretically sound as they align with counterfactuals as defined in Pearl's causal inference framework.","To validate our method, we first conduct experiments on a synthetic dataset of counterfactuals, allowing for a direct comparison between classifier predictions based on ground truth counterfactuals (obtained through explicit text interventions) and our counterfactuals, derived through interventions in the representation space.","Second, we study a real world scenario where our counterfactuals can be leveraged both for explaining a classifier and for bias mitigation."],"url":"http://arxiv.org/abs/2402.00711v1","category":"cs.LG"}
{"created":"2024-02-01 16:04:04","title":"Non-Exchangeable Conformal Language Generation with Nearest Neighbors","abstract":"Quantifying uncertainty in automatically generated text is important for letting humans check potential hallucinations and making systems more reliable. Conformal prediction is an attractive framework to provide predictions imbued with statistical guarantees, however, its application to text generation is challenging since any i.i.d. assumptions are not realistic. In this paper, we bridge this gap by leveraging recent results on non-exchangeable conformal prediction, which still ensures bounds on coverage. The result, non-exchangeable conformal nucleus sampling, is a novel extension of the conformal prediction framework to generation based on nearest neighbors. Our method can be used post-hoc for an arbitrary model without extra training and supplies token-level, calibrated prediction sets equipped with statistical guarantees. Experiments in machine translation and language modeling show encouraging results in generation quality. By also producing tighter prediction sets with good coverage, we thus give a more theoretically principled way to perform sampling with conformal guarantees.","sentences":["Quantifying uncertainty in automatically generated text is important for letting humans check potential hallucinations and making systems more reliable.","Conformal prediction is an attractive framework to provide predictions imbued with statistical guarantees, however, its application to text generation is challenging since any i.i.d. assumptions are not realistic.","In this paper, we bridge this gap by leveraging recent results on non-exchangeable conformal prediction, which still ensures bounds on coverage.","The result, non-exchangeable conformal nucleus sampling, is a novel extension of the conformal prediction framework to generation based on nearest neighbors.","Our method can be used post-hoc for an arbitrary model without extra training and supplies token-level, calibrated prediction sets equipped with statistical guarantees.","Experiments in machine translation and language modeling show encouraging results in generation quality.","By also producing tighter prediction sets with good coverage, we thus give a more theoretically principled way to perform sampling with conformal guarantees."],"url":"http://arxiv.org/abs/2402.00707v1","category":"cs.CL"}
{"created":"2024-02-01 15:59:49","title":"Measuring, processing, and generating partially coherent light with self-configuring optics","abstract":"Optical phenomena always display some degree of partial coherence between their respective degrees of freedom. Partial coherence is of particular interest in multimodal systems, where classical and quantum correlations between spatial, polarization, and spectral degrees of freedom can lead to fascinating phenomena (e.g., entanglement) and be leveraged for advanced imaging and sensing modalities (e.g., in hyperspectral, polarization, and ghost imaging). Here, we present a universal method to analyze, process, and generate spatially partially coherent light in multimode systems by using self-configuring optical networks. Our method relies on cascaded self-configuring layers whose average power outputs are sequentially optimized. Once optimized, the network separates the input light into its mutually incoherent components, which is formally equivalent to a diagonalization of the input density matrix. We illustrate our method with arrays of Mach-Zehnder interferometers and show how this method can be used to perform partially coherent environmental light sensing, generation of multimode partially coherent light with arbitrary coherency matrices, and unscrambling of quantum optical mixtures. We provide guidelines for the experimental realization of this method, paving the way for self-configuring photonic devices that can automatically learn optimal modal representations of partially coherent light fields.","sentences":["Optical phenomena always display some degree of partial coherence between their respective degrees of freedom.","Partial coherence is of particular interest in multimodal systems, where classical and quantum correlations between spatial, polarization, and spectral degrees of freedom can lead to fascinating phenomena (e.g., entanglement) and be leveraged for advanced imaging and sensing modalities (e.g., in hyperspectral, polarization, and ghost imaging).","Here, we present a universal method to analyze, process, and generate spatially partially coherent light in multimode systems by using self-configuring optical networks.","Our method relies on cascaded self-configuring layers whose average power outputs are sequentially optimized.","Once optimized, the network separates the input light into its mutually incoherent components, which is formally equivalent to a diagonalization of the input density matrix.","We illustrate our method with arrays of Mach-Zehnder interferometers and show how this method can be used to perform partially coherent environmental light sensing, generation of multimode partially coherent light with arbitrary coherency matrices, and unscrambling of quantum optical mixtures.","We provide guidelines for the experimental realization of this method, paving the way for self-configuring photonic devices that can automatically learn optimal modal representations of partially coherent light fields."],"url":"http://arxiv.org/abs/2402.00704v1","category":"physics.optics"}
{"created":"2024-02-01 15:57:38","title":"From total destruction to complete survival: Dust processing at different evolutionary stages in the supernova remnant Cassiopeia A","abstract":"The expanding ejecta of supernova remnants (SNRs) are believed to form dust in dense clumps of gas. Before the dust can be expelled into the interstellar medium and contribute to the interstellar dust budget, it has to survive the reverse shock that is generated through the interaction of the preceding supernova blast wave with the surrounding medium. The conditions under which the reverse shock hits the clumps change with remnant age and define the dust survival rate. To study the dust destruction in the SNR Cassiopeia A, we conduct magnetohydrodynamical simulations of the evolution of a supernova blast wave and of the reverse shock. In a second step we use these evolving conditions to model clumps that are disrupted by the reverse shock at different remnant ages. Finally, we compute the amount of dust that is destroyed by the impact of the reverse shock. We find that most of the dust in the SNR is hit by the reverse shock within the first 350 yr after the SN explosion. While the dust destruction in the first 200 yr is almost complete, we expect greater dust survival rates at later times and almost total survival for clumps that are first impacted at ages beyond 1000 yr. Integrated over the entire evolution of the SNR, the dust mass shows the lowest survival fraction (17 per cent) for the smallest grains (1 nm) and the highest survival fraction (28 per cent) for the largest grains (1000 nm).","sentences":["The expanding ejecta of supernova remnants (SNRs) are believed to form dust in dense clumps of gas.","Before the dust can be expelled into the interstellar medium and contribute to the interstellar dust budget, it has to survive the reverse shock that is generated through the interaction of the preceding supernova blast wave with the surrounding medium.","The conditions under which the reverse shock hits the clumps change with remnant age and define the dust survival rate.","To study the dust destruction in the SNR Cassiopeia A, we conduct magnetohydrodynamical simulations of the evolution of a supernova blast wave and of the reverse shock.","In a second step we use these evolving conditions to model clumps that are disrupted by the reverse shock at different remnant ages.","Finally, we compute the amount of dust that is destroyed by the impact of the reverse shock.","We find that most of the dust in the SNR is hit by the reverse shock within the first 350 yr after the SN explosion.","While the dust destruction in the first 200 yr is almost complete, we expect greater dust survival rates at later times and almost total survival for clumps that are first impacted at ages beyond 1000 yr.","Integrated over the entire evolution of the SNR, the dust mass shows the lowest survival fraction (17 per cent) for the smallest grains (1 nm) and the highest survival fraction (28 per cent) for the largest grains (1000 nm)."],"url":"http://arxiv.org/abs/2402.00701v1","category":"astro-ph.GA"}
{"created":"2024-02-01 15:57:11","title":"In-Bed Pose Estimation: A Review","abstract":"Human pose estimation, the process of identifying joint positions in a person's body from images or videos, represents a widely utilized technology across diverse fields, including healthcare. One such healthcare application involves in-bed pose estimation, where the body pose of an individual lying under a blanket is analyzed. This task, for instance, can be used to monitor a person's sleep behavior and detect symptoms early for potential disease diagnosis in homes and hospitals. Several studies have utilized unimodal and multimodal methods to estimate in-bed human poses. The unimodal studies generally employ RGB images, whereas the multimodal studies use modalities including RGB, long-wavelength infrared, pressure map, and depth map. Multimodal studies have the advantage of using modalities in addition to RGB that might capture information useful to cope with occlusions. Moreover, some multimodal studies exclude RGB and, this way, better suit privacy preservation. To expedite advancements in this domain, we conduct a review of existing datasets and approaches. Our objectives are to show the limitations of the previous studies, current challenges, and provide insights for future works on the in-bed human pose estimation field.","sentences":["Human pose estimation, the process of identifying joint positions in a person's body from images or videos, represents a widely utilized technology across diverse fields, including healthcare.","One such healthcare application involves in-bed pose estimation, where the body pose of an individual lying under a blanket is analyzed.","This task, for instance, can be used to monitor a person's sleep behavior and detect symptoms early for potential disease diagnosis in homes and hospitals.","Several studies have utilized unimodal and multimodal methods to estimate in-bed human poses.","The unimodal studies generally employ RGB images, whereas the multimodal studies use modalities including RGB, long-wavelength infrared, pressure map, and depth map.","Multimodal studies have the advantage of using modalities in addition to RGB that might capture information useful to cope with occlusions.","Moreover, some multimodal studies exclude RGB and, this way, better suit privacy preservation.","To expedite advancements in this domain, we conduct a review of existing datasets and approaches.","Our objectives are to show the limitations of the previous studies, current challenges, and provide insights for future works on the in-bed human pose estimation field."],"url":"http://arxiv.org/abs/2402.00700v1","category":"cs.CV"}
{"created":"2024-02-01 15:55:50","title":"PeaTMOSS: A Dataset and Initial Analysis of Pre-Trained Models in Open-Source Software","abstract":"The development and training of deep learning models have become increasingly costly and complex. Consequently, software engineers are adopting pre-trained models (PTMs) for their downstream applications. The dynamics of the PTM supply chain remain largely unexplored, signaling a clear need for structured datasets that document not only the metadata but also the subsequent applications of these models. Without such data, the MSR community cannot comprehensively understand the impact of PTM adoption and reuse. This paper presents the PeaTMOSS dataset, which comprises metadata for 281,638 PTMs and detailed snapshots for all PTMs with over 50 monthly downloads (14,296 PTMs), along with 28,575 open-source software repositories from GitHub that utilize these models. Additionally, the dataset includes 44,337 mappings from 15,129 downstream GitHub repositories to the 2,530 PTMs they use. To enhance the dataset's comprehensiveness, we developed prompts for a large language model to automatically extract model metadata, including the model's training datasets, parameters, and evaluation metrics. Our analysis of this dataset provides the first summary statistics for the PTM supply chain, showing the trend of PTM development and common shortcomings of PTM package documentation. Our example application reveals inconsistencies in software licenses across PTMs and their dependent projects. PeaTMOSS lays the foundation for future research, offering rich opportunities to investigate the PTM supply chain. We outline mining opportunities on PTMs, their downstream usage, and cross-cutting questions.","sentences":["The development and training of deep learning models have become increasingly costly and complex.","Consequently, software engineers are adopting pre-trained models (PTMs) for their downstream applications.","The dynamics of the PTM supply chain remain largely unexplored, signaling a clear need for structured datasets that document not only the metadata but also the subsequent applications of these models.","Without such data, the MSR community cannot comprehensively understand the impact of PTM adoption and reuse.","This paper presents the PeaTMOSS dataset, which comprises metadata for 281,638 PTMs and detailed snapshots for all PTMs with over 50 monthly downloads (14,296 PTMs), along with 28,575 open-source software repositories from GitHub that utilize these models.","Additionally, the dataset includes 44,337 mappings from 15,129 downstream GitHub repositories to the 2,530 PTMs they use.","To enhance the dataset's comprehensiveness, we developed prompts for a large language model to automatically extract model metadata, including the model's training datasets, parameters, and evaluation metrics.","Our analysis of this dataset provides the first summary statistics for the PTM supply chain, showing the trend of PTM development and common shortcomings of PTM package documentation.","Our example application reveals inconsistencies in software licenses across PTMs and their dependent projects.","PeaTMOSS lays the foundation for future research, offering rich opportunities to investigate the PTM supply chain.","We outline mining opportunities on PTMs, their downstream usage, and cross-cutting questions."],"url":"http://arxiv.org/abs/2402.00699v1","category":"cs.SE"}
{"created":"2024-02-01 15:51:47","title":"Multi-dimensional state space collapse in non-complete resource pooling scenarios","abstract":"The present paper establishes an explicit multi-dimensional state space collapse for parallel-processing systems with arbitrary compatibility constraints between servers and job types. This breaks major new ground beyond the state space collapse results and queue length asymptotics in the literature which are largely restricted to complete resource pooling (CRP) scenarios where the steady-state queue length vector concentrates around a line in heavy traffic. The multi-dimensional state space collapse that we establish reveals heavy-traffic behavior which is also far more tractable than the pre-limit queue length distribution, yet exhibits a fundamentally more intricate structure than in the one-dimensional case, providing useful insight into the system dynamics. Specifically, we prove that the limiting queue length vector lives in a $K$-dimensional cone which explicitly captures the delicate interplay between the various job types and servers. The dimension $K$ represents the number of critically loaded subsystems, or equivalently, capacity bottlenecks in heavy traffic, with $K = 1$ corresponding to conventional CRP scenarios. Our approach leverages probability generating function (PGF) expressions for Markovian systems operating under redundancy policies.","sentences":["The present paper establishes an explicit multi-dimensional state space collapse for parallel-processing systems with arbitrary compatibility constraints between servers and job types.","This breaks major new ground beyond the state space collapse results and queue length asymptotics in the literature which are largely restricted to complete resource pooling (CRP) scenarios where the steady-state queue length vector concentrates around a line in heavy traffic.","The multi-dimensional state space collapse that we establish reveals heavy-traffic behavior which is also far more tractable than the pre-limit queue length distribution, yet exhibits a fundamentally more intricate structure than in the one-dimensional case, providing useful insight into the system dynamics.","Specifically, we prove that the limiting queue length vector lives in a $K$-dimensional cone which explicitly captures the delicate interplay between the various job types and servers.","The dimension $K$ represents the number of critically loaded subsystems, or equivalently, capacity bottlenecks in heavy traffic, with $K = 1$ corresponding to conventional CRP scenarios.","Our approach leverages probability generating function (PGF) expressions for Markovian systems operating under redundancy policies."],"url":"http://arxiv.org/abs/2402.00696v1","category":"math.PR"}
{"created":"2024-02-01 15:51:46","title":"Approximating Optimal Morphing Attacks using Template Inversion","abstract":"Recent works have demonstrated the feasibility of inverting face recognition systems, enabling to recover convincing face images using only their embeddings. We leverage such template inversion models to develop a novel type ofdeep morphing attack based on inverting a theoretical optimal morph embedding, which is obtained as an average of the face embeddings of source images. We experiment with two variants of this approach: the first one exploits a fully self-contained embedding-to-image inversion model, while the second leverages the synthesis network of a pretrained StyleGAN network for increased morph realism. We generate morphing attacks from several source datasets and study the effectiveness of those attacks against several face recognition networks. We showcase that our method can compete with and regularly beat the previous state of the art for deep-learning based morph generation in terms of effectiveness, both in white-box and black-box attack scenarios, and is additionally much faster to run. We hope this might facilitate the development of large scale deep morph datasets for training detection models.","sentences":["Recent works have demonstrated the feasibility of inverting face recognition systems, enabling to recover convincing face images using only their embeddings.","We leverage such template inversion models to develop a novel type ofdeep morphing attack based on inverting a theoretical optimal morph embedding, which is obtained as an average of the face embeddings of source images.","We experiment with two variants of this approach: the first one exploits a fully self-contained embedding-to-image inversion model, while the second leverages the synthesis network of a pretrained StyleGAN network for increased morph realism.","We generate morphing attacks from several source datasets and study the effectiveness of those attacks against several face recognition networks.","We showcase that our method can compete with and regularly beat the previous state of the art for deep-learning based morph generation in terms of effectiveness, both in white-box and black-box attack scenarios, and is additionally much faster to run.","We hope this might facilitate the development of large scale deep morph datasets for training detection models."],"url":"http://arxiv.org/abs/2402.00695v1","category":"cs.CV"}
{"created":"2024-02-01 15:50:58","title":"Multiway junction conditions I: booklets and webs","abstract":"We explore the construction of gluing together an arbitrary number of spacetimes along a common interface that gives a geometric structure we refer to as the booklet geometry. We derive junction conditions across the interface, in both Einstein gravity and dilaton gravity, using two methods (1) by geometrically introducing an auxiliary reverse extension manifold, and (2) by varying the action of the theory under consideration, and the two methods give the same result. Our junction condition works for both spacelike and timelike interfaces. We also provide simple examples that illustrate applications of the junction condition. We further comment on the geometry of a web of spacetime in the presence of multiple interfaces and its resemblance to Feynman diagrams.","sentences":["We explore the construction of gluing together an arbitrary number of spacetimes along a common interface that gives a geometric structure we refer to as the booklet geometry.","We derive junction conditions across the interface, in both Einstein gravity and dilaton gravity, using two methods (1) by geometrically introducing an auxiliary reverse extension manifold, and (2) by varying the action of the theory under consideration, and the two methods give the same result.","Our junction condition works for both spacelike and timelike interfaces.","We also provide simple examples that illustrate applications of the junction condition.","We further comment on the geometry of a web of spacetime in the presence of multiple interfaces and its resemblance to Feynman diagrams."],"url":"http://arxiv.org/abs/2402.00694v1","category":"hep-th"}
{"created":"2024-02-01 15:50:40","title":"A Framework for Building Point Cloud Cleaning, Plane Detection and Semantic Segmentation","abstract":"This paper presents a framework to address the challenges involved in building point cloud cleaning, plane detection, and semantic segmentation, with the ultimate goal of enhancing building modeling. We focus in the cleaning stage on removing outliers from the acquired point cloud data by employing an adaptive threshold technique based on z-score measure. Following the cleaning process, we perform plane detection using the robust RANSAC paradigm. The goal is to carry out multiple plane segmentations, and to classify segments into distinct categories, such as floors, ceilings, and walls. The resulting segments can generate accurate and detailed point clouds representing the building's architectural elements. Moreover, we address the problem of semantic segmentation, which plays a vital role in the identification and classification of different components within the building, such as walls, windows, doors, roofs, and objects. Inspired by the PointNet architecture, we propose a deep learning architecture for efficient semantic segmentation in buildings. The results demonstrate the effectiveness of the proposed framework in handling building modeling tasks, paving the way for improved accuracy and efficiency in the field of building modelization.","sentences":["This paper presents a framework to address the challenges involved in building point cloud cleaning, plane detection, and semantic segmentation, with the ultimate goal of enhancing building modeling.","We focus in the cleaning stage on removing outliers from the acquired point cloud data by employing an adaptive threshold technique based on z-score measure.","Following the cleaning process, we perform plane detection using the robust RANSAC paradigm.","The goal is to carry out multiple plane segmentations, and to classify segments into distinct categories, such as floors, ceilings, and walls.","The resulting segments can generate accurate and detailed point clouds representing the building's architectural elements.","Moreover, we address the problem of semantic segmentation, which plays a vital role in the identification and classification of different components within the building, such as walls, windows, doors, roofs, and objects.","Inspired by the PointNet architecture, we propose a deep learning architecture for efficient semantic segmentation in buildings.","The results demonstrate the effectiveness of the proposed framework in handling building modeling tasks, paving the way for improved accuracy and efficiency in the field of building modelization."],"url":"http://arxiv.org/abs/2402.00692v1","category":"cs.CV"}
{"created":"2024-02-01 15:50:37","title":"Comparative Study of Large Language Model Architectures on Frontier","abstract":"Large language models (LLMs) have garnered significant attention in both the AI community and beyond. Among these, the Generative Pre-trained Transformer (GPT) has emerged as the dominant architecture, spawning numerous variants. However, these variants have undergone pre-training under diverse conditions, including variations in input data, data preprocessing, and training methodologies, resulting in a lack of controlled comparative studies. Here we meticulously examine two prominent open-sourced GPT architectures, GPT-NeoX and LLaMA, leveraging the computational power of Frontier, the world's first Exascale supercomputer. Employing the same materials science text corpus and a comprehensive end-to-end pipeline, we conduct a comparative analysis of their training and downstream performance. Our efforts culminate in achieving state-of-the-art performance on a challenging materials science benchmark. Furthermore, we investigate the computation and energy efficiency, and propose a computationally efficient method for architecture design. To our knowledge, these pre-trained models represent the largest available for materials science. Our findings provide practical guidance for building LLMs on HPC platforms.","sentences":["Large language models (LLMs) have garnered significant attention in both the AI community and beyond.","Among these, the Generative Pre-trained Transformer (GPT) has emerged as the dominant architecture, spawning numerous variants.","However, these variants have undergone pre-training under diverse conditions, including variations in input data, data preprocessing, and training methodologies, resulting in a lack of controlled comparative studies.","Here we meticulously examine two prominent open-sourced GPT architectures, GPT-NeoX and LLaMA, leveraging the computational power of Frontier, the world's first Exascale supercomputer.","Employing the same materials science text corpus and a comprehensive end-to-end pipeline, we conduct a comparative analysis of their training and downstream performance.","Our efforts culminate in achieving state-of-the-art performance on a challenging materials science benchmark.","Furthermore, we investigate the computation and energy efficiency, and propose a computationally efficient method for architecture design.","To our knowledge, these pre-trained models represent the largest available for materials science.","Our findings provide practical guidance for building LLMs on HPC platforms."],"url":"http://arxiv.org/abs/2402.00691v1","category":"cs.DC"}
{"created":"2024-02-01 15:49:47","title":"Ocassionally Secure: A Comparative Analysis of Code Generation Assistants","abstract":"$ $Large Language Models (LLMs) are being increasingly utilized in various applications, with code generations being a notable example. While previous research has shown that LLMs have the capability to generate both secure and insecure code, the literature does not take into account what factors help generate secure and effective code. Therefore in this paper we focus on identifying and understanding the conditions and contexts in which LLMs can be effectively and safely deployed in real-world scenarios to generate quality code. We conducted a comparative analysis of four advanced LLMs--GPT-3.5 and GPT-4 using ChatGPT and Bard and Gemini from Google--using 9 separate tasks to assess each model's code generation capabilities. We contextualized our study to represent the typical use cases of a real-life developer employing LLMs for everyday tasks as work. Additionally, we place an emphasis on security awareness which is represented through the use of two distinct versions of our developer persona. In total, we collected 61 code outputs and analyzed them across several aspects: functionality, security, performance, complexity, and reliability. These insights are crucial for understanding the models' capabilities and limitations, guiding future development and practical applications in the field of automated code generation.","sentences":["$ $Large Language Models (LLMs) are being increasingly utilized in various applications, with code generations being a notable example.","While previous research has shown that LLMs have the capability to generate both secure and insecure code, the literature does not take into account what factors help generate secure and effective code.","Therefore in this paper we focus on identifying and understanding the conditions and contexts in which LLMs can be effectively and safely deployed in real-world scenarios to generate quality code.","We conducted a comparative analysis of four advanced LLMs--GPT-3.5 and GPT-4 using ChatGPT and Bard and Gemini from Google--using 9 separate tasks to assess each model's code generation capabilities.","We contextualized our study to represent the typical use cases of a real-life developer employing LLMs for everyday tasks as work.","Additionally, we place an emphasis on security awareness which is represented through the use of two distinct versions of our developer persona.","In total, we collected 61 code outputs and analyzed them across several aspects: functionality, security, performance, complexity, and reliability.","These insights are crucial for understanding the models' capabilities and limitations, guiding future development and practical applications in the field of automated code generation."],"url":"http://arxiv.org/abs/2402.00689v1","category":"cs.CR"}
{"created":"2024-02-01 15:49:07","title":"Carrollian $Lw_{1+\\infty}$ representation from twistor space","abstract":"We construct an explicit realization of the action of the $Lw_{1+\\infty}$ loop algebra on fields at null infinity. This action is directly derived by Penrose transform of the geometrical action of $Lw_{1+\\infty}$ symmetries in twistor space, ensuring that it forms a representation of the algebra. Finally, we show that this action coincides with the canonical action of $Lw_{1+\\infty}$ Noether charges on the asymptotic phase space.","sentences":["We construct an explicit realization of the action of the $Lw_{1+\\infty}$ loop algebra on fields at null infinity.","This action is directly derived by Penrose transform of the geometrical action of $Lw_{1+\\infty}$ symmetries in twistor space, ensuring that it forms a representation of the algebra.","Finally, we show that this action coincides with the canonical action of $Lw_{1+\\infty}$ Noether charges on the asymptotic phase space."],"url":"http://arxiv.org/abs/2402.00688v1","category":"hep-th"}
{"created":"2024-02-01 15:48:45","title":"A Review on the Use of Blockchain for the Internet of Things","abstract":"The paradigm of Internet of Things (IoT) is paving the way for a world, where many of our daily objects will be interconnected and will interact with their environment in order to collect information and automate certain tasks. Such a vision requires, among other things, seamless authentication, data privacy, security, robustness against attacks, easy deployment, and self-maintenance. Such features can be brought by blockchain, a technology born with a cryptocurrency called Bitcoin. In this paper, a thorough review on how to adapt blockchain to the specific needs of IoT in order to develop Blockchain-based IoT (BIoT) applications is presented. After describing the basics of blockchain, the most relevant BIoT applications are described with the objective of emphasizing how blockchain can impact traditional cloud-centered IoT applications. Then, the current challenges and possible optimizations are detailed regarding many aspects that affect the design, development, and deployment of a BIoT application. Finally, some recommendations are enumerated with the aim of guiding future BIoT researchers and developers on some of the issues that will have to be tackled before deploying the next generation of BIoT applications.","sentences":["The paradigm of Internet of Things (IoT) is paving the way for a world, where many of our daily objects will be interconnected and will interact with their environment in order to collect information and automate certain tasks.","Such a vision requires, among other things, seamless authentication, data privacy, security, robustness against attacks, easy deployment, and self-maintenance.","Such features can be brought by blockchain, a technology born with a cryptocurrency called Bitcoin.","In this paper, a thorough review on how to adapt blockchain to the specific needs of IoT in order to develop Blockchain-based IoT (BIoT) applications is presented.","After describing the basics of blockchain, the most relevant BIoT applications are described with the objective of emphasizing how blockchain can impact traditional cloud-centered IoT applications.","Then, the current challenges and possible optimizations are detailed regarding many aspects that affect the design, development, and deployment of a BIoT application.","Finally, some recommendations are enumerated with the aim of guiding future BIoT researchers and developers on some of the issues that will have to be tackled before deploying the next generation of BIoT applications."],"url":"http://arxiv.org/abs/2402.00687v1","category":"cs.CR"}
{"created":"2024-02-01 15:38:21","title":"Real Evaluations Tractability using Continuous Goal-Directed Actions in Smart City Applications","abstract":"One of the most important challenges of Smart City Applications is to adapt the system to interact with non-expert users. Robot imitation frameworks aim to simplify and reduce times of robot programming by allowing users to program directly through demonstrations. In classical frameworks, actions are modeled using joint or Cartesian space trajectories. Other features, such as visual ones, are not always well represented with these pure geometrical approaches. Continuous Goal-Directed Actions (CGDA) is an alternative to these methods, as it encodes actions as changes of any feature that can be extracted from the environment. As a consequence of this, the robot joint trajectories for execution must be fully computed to comply with this feature-agnostic encoding. This is achieved using Evolutionary Algorithms (EA), which usually requires too many evaluations to perform this evolution step in the actual robot. Current strategies involve performing evaluations in a simulation, transferring the final joint trajectory to the actual robot. Smart City applications involve working in highly dynamic and complex environments, where having a precise model is not always achievable. Our goal is to study the tractability of performing these evaluations directly in a real-world scenario. Two different approaches to reduce the number of evaluations using EA, are proposed and compared. In the first approach, Particle Swarm Optimization (PSO)-based methods have been studied and compared within CGDA: naive PSO, Fitness Inheritance PSO (FI-PSO), and Adaptive Fuzzy Fitness Granulation with PSO (AFFG-PSO). The second approach studied the introduction of geometrical and velocity constraints within CGDA. The effects of both approaches were analyzed and compared in the wax and paint actions, two CGDA commonly studied use cases. Results from this paper depict an important reduction in the number of evaluations.","sentences":["One of the most important challenges of Smart City Applications is to adapt the system to interact with non-expert users.","Robot imitation frameworks aim to simplify and reduce times of robot programming by allowing users to program directly through demonstrations.","In classical frameworks, actions are modeled using joint or Cartesian space trajectories.","Other features, such as visual ones, are not always well represented with these pure geometrical approaches.","Continuous Goal-Directed Actions (CGDA) is an alternative to these methods, as it encodes actions as changes of any feature that can be extracted from the environment.","As a consequence of this, the robot joint trajectories for execution must be fully computed to comply with this feature-agnostic encoding.","This is achieved using Evolutionary Algorithms (EA), which usually requires too many evaluations to perform this evolution step in the actual robot.","Current strategies involve performing evaluations in a simulation, transferring the final joint trajectory to the actual robot.","Smart City applications involve working in highly dynamic and complex environments, where having a precise model is not always achievable.","Our goal is to study the tractability of performing these evaluations directly in a real-world scenario.","Two different approaches to reduce the number of evaluations using EA, are proposed and compared.","In the first approach, Particle Swarm Optimization (PSO)-based methods have been studied and compared within CGDA: naive PSO, Fitness Inheritance PSO (FI-PSO), and Adaptive Fuzzy Fitness Granulation with PSO (AFFG-PSO).","The second approach studied the introduction of geometrical and velocity constraints within CGDA.","The effects of both approaches were analyzed and compared in the wax and paint actions, two CGDA commonly studied use cases.","Results from this paper depict an important reduction in the number of evaluations."],"url":"http://arxiv.org/abs/2402.00678v1","category":"cs.RO"}
{"created":"2024-02-01 15:37:42","title":"Neural Policy Style Transfer","abstract":"Style Transfer has been proposed in a number of fields: fine arts, natural language processing, and fixed trajectories. We scale this concept up to control policies within a Deep Reinforcement Learning infrastructure. Each network is trained to maximize the expected reward, which typically encodes the goal of an action, and can be described as the content. The expressive power of deep neural networks enables encoding a secondary task, which can be described as the style. The Neural Policy Style Transfer (NPST) algorithm is proposed to transfer the style of one policy to another, while maintaining the content of the latter. Different policies are defined via Deep Q-Network architectures. These models are trained using demonstrations through Inverse Reinforcement Learning. Two different sets of user demonstrations are performed, one for content and other for style. Different styles are encoded as defined by user demonstrations. The generated policy is the result of feeding a content policy and a style policy to the NPST algorithm. Experiments are performed in a catch-ball game inspired by the Deep Reinforcement Learning classical Atari games; and a real-world painting scenario with a full-sized humanoid robot, based on previous works of the authors. The implementation of three different Q-Network architectures (Shallow, Deep and Deep Recurrent Q-Network) to encode the policies within the NPST framework is proposed and the results obtained in the experiments with each of these architectures compared.","sentences":["Style Transfer has been proposed in a number of fields: fine arts, natural language processing, and fixed trajectories.","We scale this concept up to control policies within a Deep Reinforcement Learning infrastructure.","Each network is trained to maximize the expected reward, which typically encodes the goal of an action, and can be described as the content.","The expressive power of deep neural networks enables encoding a secondary task, which can be described as the style.","The Neural Policy Style Transfer (NPST) algorithm is proposed to transfer the style of one policy to another, while maintaining the content of the latter.","Different policies are defined via Deep Q-Network architectures.","These models are trained using demonstrations through Inverse Reinforcement Learning.","Two different sets of user demonstrations are performed, one for content and other for style.","Different styles are encoded as defined by user demonstrations.","The generated policy is the result of feeding a content policy and a style policy to the NPST algorithm.","Experiments are performed in a catch-ball game inspired by the Deep Reinforcement Learning classical Atari games; and a real-world painting scenario with a full-sized humanoid robot, based on previous works of the authors.","The implementation of three different Q-Network architectures (Shallow, Deep and Deep Recurrent Q-Network) to encode the policies within the NPST framework is proposed and the results obtained in the experiments with each of these architectures compared."],"url":"http://arxiv.org/abs/2402.00677v1","category":"cs.RO"}
{"created":"2024-02-01 15:37:23","title":"Deep Robot Sketching: An application of Deep Q-Learning Networks for human-like sketching","abstract":"The current success of Reinforcement Learning algorithms for its performance in complex environments has inspired many recent theoretical approaches to cognitive science. Artistic environments are studied within the cognitive science community as rich, natural, multi-sensory, multi-cultural environments. In this work, we propose the introduction of Reinforcement Learning for improving the control of artistic robot applications. Deep Q-learning Neural Networks (DQN) is one of the most successful algorithms for the implementation of Reinforcement Learning in robotics. DQN methods generate complex control policies for the execution of complex robot applications in a wide set of environments. Current art painting robot applications use simple control laws that limits the adaptability of the frameworks to a set of simple environments. In this work, the introduction of DQN within an art painting robot application is proposed. The goal is to study how the introduction of a complex control policy impacts the performance of a basic art painting robot application. The main expected contribution of this work is to serve as a first baseline for future works introducing DQN methods for complex art painting robot frameworks. Experiments consist of real world executions of human drawn sketches using the DQN generated policy and TEO, the humanoid robot. Results are compared in terms of similarity and obtained reward with respect to the reference inputs","sentences":["The current success of Reinforcement Learning algorithms for its performance in complex environments has inspired many recent theoretical approaches to cognitive science.","Artistic environments are studied within the cognitive science community as rich, natural, multi-sensory, multi-cultural environments.","In this work, we propose the introduction of Reinforcement Learning for improving the control of artistic robot applications.","Deep Q-learning Neural Networks (DQN) is one of the most successful algorithms for the implementation of Reinforcement Learning in robotics.","DQN methods generate complex control policies for the execution of complex robot applications in a wide set of environments.","Current art painting robot applications use simple control laws that limits the adaptability of the frameworks to a set of simple environments.","In this work, the introduction of DQN within an art painting robot application is proposed.","The goal is to study how the introduction of a complex control policy impacts the performance of a basic art painting robot application.","The main expected contribution of this work is to serve as a first baseline for future works introducing DQN methods for complex art painting robot frameworks.","Experiments consist of real world executions of human drawn sketches using the DQN generated policy and TEO, the humanoid robot.","Results are compared in terms of similarity and obtained reward with respect to the reference inputs"],"url":"http://arxiv.org/abs/2402.00676v1","category":"cs.RO"}
{"created":"2024-02-01 15:34:59","title":"On Modular Algorithms and Butterfly Operations in Number Theoretic Transform","abstract":"Number theoretic transform (NTT) has been a very useful tool in computations for number theory, algebra and cryptography. Its performance affects some post-quantum cryptosystems. In this paper, we discuss the butterfly operation of NTT. This basic module of NTT requires heavy modular arithmetics. Montgomery reduction is commonly used in this setting. Recently several variants of Montgomery algorithm have been proposed for the purpose of speeding up NTT. We observe that the Chinese remainder theorem (CRT) can be involved in this type of algorithms in nature and transparent ways. In this paper, a framework of using CRT to model Montgomery type algorithms is described. The derivation of these algorithms as well as their correctness are all treated in the CRT framework. Under our approach, some problems of a modular reduction algorithm (published in IACR Transactions on Cryptographic Hardware and Embedded Systems, doi:10.46586/tches.v2022.i4.614-636 ) are identified, and a counterexample is generated to show that the algorithm is incorrect.","sentences":["Number theoretic transform (NTT) has been a very useful tool in computations for number theory, algebra and cryptography.","Its performance affects some post-quantum cryptosystems.","In this paper, we discuss the butterfly operation of NTT.","This basic module of NTT requires heavy modular arithmetics.","Montgomery reduction is commonly used in this setting.","Recently several variants of Montgomery algorithm have been proposed for the purpose of speeding up NTT.","We observe that the Chinese remainder theorem (CRT) can be involved in this type of algorithms in nature and transparent ways.","In this paper, a framework of using CRT to model Montgomery type algorithms is described.","The derivation of these algorithms as well as their correctness are all treated in the CRT framework.","Under our approach, some problems of a modular reduction algorithm (published in IACR Transactions on Cryptographic Hardware and Embedded Systems, doi:10.46586/tches.v2022.i4.614-636 ) are identified, and a counterexample is generated to show that the algorithm is incorrect."],"url":"http://arxiv.org/abs/2402.00675v1","category":"cs.CR"}
{"created":"2024-02-01 15:33:17","title":"Exploring Homogeneous and Heterogeneous Consistent Label Associations for Unsupervised Visible-Infrared Person ReID","abstract":"Unsupervised visible-infrared person re-identification (USL-VI-ReID) aims to retrieve pedestrian images of the same identity from different modalities without annotations. While prior work focuses on establishing cross-modality pseudo-label associations to bridge the modality-gap, they ignore maintaining the instance-level homogeneous and heterogeneous consistency in pseudo-label space, resulting in coarse associations. In response, we introduce a Modality-Unified Label Transfer (MULT) module that simultaneously accounts for both homogeneous and heterogeneous fine-grained instance-level structures, yielding high-quality cross-modality label associations. It models both homogeneous and heterogeneous affinities, leveraging them to define the inconsistency for the pseudo-labels and then minimize it, leading to pseudo-labels that maintain alignment across modalities and consistency within intra-modality structures. Additionally, a straightforward plug-and-play Online Cross-memory Label Refinement (OCLR) module is proposed to further mitigate the impact of noisy pseudo-labels while simultaneously aligning different modalities, coupled with a Modality-Invariant Representation Learning (MIRL) framework. Experiments demonstrate that our proposed method outperforms existing USL-VI-ReID methods, highlighting the superiority of our MULT in comparison to other cross-modality association methods. The code will be available.","sentences":["Unsupervised visible-infrared person re-identification (USL-VI-ReID) aims to retrieve pedestrian images of the same identity from different modalities without annotations.","While prior work focuses on establishing cross-modality pseudo-label associations to bridge the modality-gap, they ignore maintaining the instance-level homogeneous and heterogeneous consistency in pseudo-label space, resulting in coarse associations.","In response, we introduce a Modality-Unified Label Transfer (MULT) module that simultaneously accounts for both homogeneous and heterogeneous fine-grained instance-level structures, yielding high-quality cross-modality label associations.","It models both homogeneous and heterogeneous affinities, leveraging them to define the inconsistency for the pseudo-labels and then minimize it, leading to pseudo-labels that maintain alignment across modalities and consistency within intra-modality structures.","Additionally, a straightforward plug-and-play Online Cross-memory Label Refinement (OCLR) module is proposed to further mitigate the impact of noisy pseudo-labels while simultaneously aligning different modalities, coupled with a Modality-Invariant Representation Learning (MIRL) framework.","Experiments demonstrate that our proposed method outperforms existing USL-VI-ReID methods, highlighting the superiority of our MULT in comparison to other cross-modality association methods.","The code will be available."],"url":"http://arxiv.org/abs/2402.00672v1","category":"cs.CV"}
{"created":"2024-02-01 15:30:19","title":"Improving Weak-to-Strong Generalization with Scalable Oversight and Ensemble Learning","abstract":"This paper presents a follow-up study to OpenAI's recent superalignment work on Weak-to-Strong Generalization (W2SG). Superalignment focuses on ensuring that high-level AI systems remain consistent with human values and intentions when dealing with complex, high-risk tasks. The W2SG framework has opened new possibilities for empirical research in this evolving field. Our study simulates two phases of superalignment under the W2SG framework: the development of general superhuman models and the progression towards superintelligence. In the first phase, based on human supervision, the quality of weak supervision is enhanced through a combination of scalable oversight and ensemble learning, reducing the capability gap between weak teachers and strong students. In the second phase, an automatic alignment evaluator is employed as the weak supervisor. By recursively updating this auto aligner, the capabilities of the weak teacher models are synchronously enhanced, achieving weak-to-strong supervision over stronger student models.We also provide an initial validation of the proposed approach for the first phase. Using the SciQ task as example, we explore ensemble learning for weak teacher models through bagging and boosting. Scalable oversight is explored through two auxiliary settings: human-AI interaction and AI-AI debate. Additionally, the paper discusses the impact of improved weak supervision on enhancing weak-to-strong generalization based on in-context learning. Experiment code and dataset will be released at https://github.com/ADaM-BJTU/W2SG.","sentences":["This paper presents a follow-up study to OpenAI's recent superalignment work on Weak-to-Strong Generalization (W2SG).","Superalignment focuses on ensuring that high-level AI systems remain consistent with human values and intentions when dealing with complex, high-risk tasks.","The W2SG framework has opened new possibilities for empirical research in this evolving field.","Our study simulates two phases of superalignment under the W2SG framework: the development of general superhuman models and the progression towards superintelligence.","In the first phase, based on human supervision, the quality of weak supervision is enhanced through a combination of scalable oversight and ensemble learning, reducing the capability gap between weak teachers and strong students.","In the second phase, an automatic alignment evaluator is employed as the weak supervisor.","By recursively updating this auto aligner, the capabilities of the weak teacher models are synchronously enhanced, achieving weak-to-strong supervision over stronger student models.","We also provide an initial validation of the proposed approach for the first phase.","Using the SciQ task as example, we explore ensemble learning for weak teacher models through bagging and boosting.","Scalable oversight is explored through two auxiliary settings: human-AI interaction and AI-AI debate.","Additionally, the paper discusses the impact of improved weak supervision on enhancing weak-to-strong generalization based on in-context learning.","Experiment code and dataset will be released at https://github.com/ADaM-BJTU/W2SG."],"url":"http://arxiv.org/abs/2402.00667v1","category":"cs.CL"}
{"created":"2024-02-01 15:26:09","title":"Revising Apetrei's bounding volume hierarchy construction algorithm to allow stackless traversal","abstract":"Stackless traversal is a technique to speed up range queries by avoiding usage of a stack during the tree traversal. One way to achieve that is to transform a given binary tree to store a left child and a skip-connection (also called an escape index). In general, this operation requires an additional tree traversal during the tree construction. For some tree structures, however, it is possible achieve the same result at a reduced cost. We propose one such algorithm for a GPU hierarchy construction algorithm proposed by Karras in [Karras, 2012]. Furthermore, we show that our algorithm also works with the improved algorithm proposed by Apetrei in [Apetrei, 2014], despite a different ordering of the internal nodes. We achieve that by modifying the Apetrei's algorithm to restore the original Karras' ordering of the internal nodes. Using the modified algorithm, we show how to construct a hierarchy suitable for a stackless traversal in a single bottom-up pass.","sentences":["Stackless traversal is a technique to speed up range queries by avoiding usage of a stack during the tree traversal.","One way to achieve that is to transform a given binary tree to store a left child and a skip-connection (also called an escape index).","In general, this operation requires an additional tree traversal during the tree construction.","For some tree structures, however, it is possible achieve the same result at a reduced cost.","We propose one such algorithm for a GPU hierarchy construction algorithm proposed by Karras in [Karras, 2012].","Furthermore, we show that our algorithm also works with the improved algorithm proposed by Apetrei in [Apetrei, 2014], despite a different ordering of the internal nodes.","We achieve that by modifying the Apetrei's algorithm to restore the original Karras' ordering of the internal nodes.","Using the modified algorithm, we show how to construct a hierarchy suitable for a stackless traversal in a single bottom-up pass."],"url":"http://arxiv.org/abs/2402.00665v1","category":"cs.DS"}
{"created":"2024-02-01 15:23:31","title":"Transferring human emotions to robot motions using Neural Policy Style Transfer","abstract":"Neural Style Transfer (NST) was originally proposed to use feature extraction capabilities of Neural Networks as a way to perform Style Transfer with images. Pre-trained image classification architectures were selected for feature extraction, leading to new images showing the same content as the original but with a different style. In robotics, Style Transfer can be employed to transfer human motion styles to robot motions. The challenge lies in the lack of pre-trained classification architectures for robot motions that could be used for feature extraction. Neural Policy Style Transfer TD3 (NPST3) is proposed for the transfer of human motion styles to robot motions. This framework allows the same robot motion to be executed in different human-centered motion styles, such as in an angry, happy, calm, or sad fashion. The Twin Delayed Deep Deterministic Policy Gradient (TD3) network is introduced for the generation of control policies. An autoencoder network is in charge of feature extraction for the Style Transfer step. The Style Transfer step can be performed both offline and online: offline for the autonomous executions of human-style robot motions, and online for adapting at runtime the style of e.g., a teleoperated robot. The framework is tested using two different robotic platforms: a robotic manipulator designed for telemanipulation tasks, and a humanoid robot designed for social interaction. The proposed approach was evaluated for both platforms, performing a total of 147 questionnaires asking human subjects to recognize the human motion style transferred to the robot motion for a predefined set of actions.","sentences":["Neural Style Transfer (NST) was originally proposed to use feature extraction capabilities of Neural Networks as a way to perform Style Transfer with images.","Pre-trained image classification architectures were selected for feature extraction, leading to new images showing the same content as the original but with a different style.","In robotics, Style Transfer can be employed to transfer human motion styles to robot motions.","The challenge lies in the lack of pre-trained classification architectures for robot motions that could be used for feature extraction.","Neural Policy Style Transfer TD3 (NPST3) is proposed for the transfer of human motion styles to robot motions.","This framework allows the same robot motion to be executed in different human-centered motion styles, such as in an angry, happy, calm, or sad fashion.","The Twin Delayed Deep Deterministic Policy Gradient (TD3) network is introduced for the generation of control policies.","An autoencoder network is in charge of feature extraction for the Style Transfer step.","The Style Transfer step can be performed both offline and online: offline for the autonomous executions of human-style robot motions, and online for adapting at runtime the style of e.g., a teleoperated robot.","The framework is tested using two different robotic platforms: a robotic manipulator designed for telemanipulation tasks, and a humanoid robot designed for social interaction.","The proposed approach was evaluated for both platforms, performing a total of 147 questionnaires asking human subjects to recognize the human motion style transferred to the robot motion for a predefined set of actions."],"url":"http://arxiv.org/abs/2402.00663v1","category":"cs.RO"}
{"created":"2024-02-01 15:20:43","title":"The spectral boundary of the Asymmetric Simple Exclusion Process (ASEP) -- free fermions, Bethe ansatz and random matrix theory","abstract":"In non-equilibrium statistical mechanics, the Asymmetric Simple Exclusion Process (ASEP) serves as a paradigmatic example. We investigate the spectral characteristics of the ASEP, focusing on the spectral boundary of its generator matrix. We examine finite ASEP chains of length $L$, under periodic (pbc) and open boundary conditions (obc). Notably, the spectral boundary exhibits $L$ spikes for pbc and $L+1$ spikes for obc. Treating the ASEP generator as an interacting non-Hermitian fermionic model, we extend the model to have tunable interaction. In the non-interacting case, the analytically computed many-body spectrum shows a spectral boundary with prominent spikes. For pbc, we use the coordinate Bethe ansatz to interpolate between the noninteracting case to the ASEP limit, and show that these spikes stem from clustering of Bethe roots. The robustness of the spikes in the spectral boundary is demonstrated by linking the ASEP generator to random matrices with trace correlations or, equivalently, random graphs with distinct cycle structures, both displaying similar spiked spectral boundaries.","sentences":["In non-equilibrium statistical mechanics, the Asymmetric Simple Exclusion Process (ASEP) serves as a paradigmatic example.","We investigate the spectral characteristics of the ASEP, focusing on the spectral boundary of its generator matrix.","We examine finite ASEP chains of length $L$, under periodic (pbc) and open boundary conditions (obc).","Notably, the spectral boundary exhibits $L$ spikes for pbc and $L+1$ spikes for obc.","Treating the ASEP generator as an interacting non-Hermitian fermionic model, we extend the model to have tunable interaction.","In the non-interacting case, the analytically computed many-body spectrum shows a spectral boundary with prominent spikes.","For pbc, we use the coordinate Bethe ansatz to interpolate between the noninteracting case to the ASEP limit, and show that these spikes stem from clustering of Bethe roots.","The robustness of the spikes in the spectral boundary is demonstrated by linking the ASEP generator to random matrices with trace correlations or, equivalently, random graphs with distinct cycle structures, both displaying similar spiked spectral boundaries."],"url":"http://arxiv.org/abs/2402.00662v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-01 15:18:48","title":"Modeling Freight Mode Choice Using Machine Learning Classifiers: A Comparative Study Using the Commodity Flow Survey (CFS) Data","abstract":"This study explores the usefulness of machine learning classifiers for modeling freight mode choice. We investigate eight commonly used machine learning classifiers, namely Naive Bayes, Support Vector Machine, Artificial Neural Network, K-Nearest Neighbors, Classification and Regression Tree, Random Forest, Boosting and Bagging, along with the classical Multinomial Logit model. US 2012 Commodity Flow Survey data are used as the primary data source; we augment it with spatial attributes from secondary data sources. The performance of the classifiers is compared based on prediction accuracy results. The current research also examines the role of sample size and training-testing data split ratios on the predictive ability of the various approaches. In addition, the importance of variables is estimated to determine how the variables influence freight mode choice. The results show that the tree-based ensemble classifiers perform the best. Specifically, Random Forest produces the most accurate predictions, closely followed by Boosting and Bagging. With regard to variable importance, shipment characteristics, such as shipment distance, industry classification of the shipper and shipment size, are the most significant factors for freight mode choice decisions.","sentences":["This study explores the usefulness of machine learning classifiers for modeling freight mode choice.","We investigate eight commonly used machine learning classifiers, namely Naive Bayes, Support Vector Machine, Artificial Neural Network, K-Nearest Neighbors, Classification and Regression Tree, Random Forest, Boosting and Bagging, along with the classical Multinomial Logit model.","US 2012 Commodity Flow Survey data are used as the primary data source; we augment it with spatial attributes from secondary data sources.","The performance of the classifiers is compared based on prediction accuracy results.","The current research also examines the role of sample size and training-testing data split ratios on the predictive ability of the various approaches.","In addition, the importance of variables is estimated to determine how the variables influence freight mode choice.","The results show that the tree-based ensemble classifiers perform the best.","Specifically, Random Forest produces the most accurate predictions, closely followed by Boosting and Bagging.","With regard to variable importance, shipment characteristics, such as shipment distance, industry classification of the shipper and shipment size, are the most significant factors for freight mode choice decisions."],"url":"http://arxiv.org/abs/2402.00659v1","category":"cs.LG"}
{"created":"2024-02-01 15:18:33","title":"Learning Planning-based Reasoning by Trajectories Collection and Process Reward Synthesizing","abstract":"Large Language Models (LLMs) have demonstrated significant potential in handling complex reasoning tasks through step-by-step rationale generation. However, recent studies have raised concerns regarding the hallucination and flaws in their reasoning process. Substantial efforts are being made to improve the reliability and faithfulness of the generated rationales. Some approaches model reasoning as planning, while others focus on annotating for process supervision. Nevertheless, the planning-based search process often results in high latency due to the frequent assessment of intermediate reasoning states and the extensive exploration space. Additionally, supervising the reasoning process with human annotation is costly and challenging to scale for LLM training. To address these issues, in this paper, we propose a framework to learn planning-based reasoning through direct preference optimization (DPO) on collected trajectories, which are ranked according to synthesized process rewards. Our results on challenging logical reasoning benchmarks demonstrate the effectiveness of our learning framework, showing that our 7B model can surpass the strong counterparts like GPT-3.5-Turbo.","sentences":["Large Language Models (LLMs) have demonstrated significant potential in handling complex reasoning tasks through step-by-step rationale generation.","However, recent studies have raised concerns regarding the hallucination and flaws in their reasoning process.","Substantial efforts are being made to improve the reliability and faithfulness of the generated rationales.","Some approaches model reasoning as planning, while others focus on annotating for process supervision.","Nevertheless, the planning-based search process often results in high latency due to the frequent assessment of intermediate reasoning states and the extensive exploration space.","Additionally, supervising the reasoning process with human annotation is costly and challenging to scale for LLM training.","To address these issues, in this paper, we propose a framework to learn planning-based reasoning through direct preference optimization (DPO) on collected trajectories, which are ranked according to synthesized process rewards.","Our results on challenging logical reasoning benchmarks demonstrate the effectiveness of our learning framework, showing that our 7B model can surpass the strong counterparts like GPT-3.5-Turbo."],"url":"http://arxiv.org/abs/2402.00658v1","category":"cs.AI"}
{"created":"2024-02-01 15:18:13","title":"On universality of general Dirichlet series","abstract":"In the present work we establish sufficient conditions for a Dirichlet series induced by general frequencies to be universal with respect to vertical translations. Applying our methodology we give examples of universal Dirichlet series such as the alternating prime zeta function $\\sum_{n\\geq1}(-1)^np_n^{-s}$.","sentences":["In the present work we establish sufficient conditions for a Dirichlet series induced by general frequencies to be universal with respect to vertical translations.","Applying our methodology we give examples of universal Dirichlet series such as the alternating prime zeta function $\\sum_{n\\geq1}(-1)^np_n^{-s}$."],"url":"http://arxiv.org/abs/2402.00656v1","category":"math.CV"}
{"created":"2024-02-01 15:15:05","title":"Femtosecond drift photocurrents generated by an inversely designed plasmonic antenna","abstract":"Photocurrents play a crucial role in various applications, including light detection, photovoltaics, and THz radiation generation. Despite the abundance of methods and materials for converting light into electrical signals, the use of metals in this context has been relatively limited. Nanostructures supporting surface plasmons in metals offer precise light manipulation and induce light-driven electron motion. Through inverse design optimization of a gold nanostructure, we demonstrate enhanced volumetric, unidirectional, intense, and ultrafast photocurrents via a magneto-optical process derived from the inverse Faraday effect. This is achieved through fine-tuning the amplitude, polarization, and their gradients in the local light field. The virtually instantaneous process allows dynamic photocurrent modulation by varying optical pulse duration, potentially yielding nanosources of intense, ultrafast, planar magnetic fields, and frequency-tunable THz emission. These findings opens avenues for ultrafast magnetic material manipulation and holds promise for nanoscale THz spectroscopy.","sentences":["Photocurrents play a crucial role in various applications, including light detection, photovoltaics, and THz radiation generation.","Despite the abundance of methods and materials for converting light into electrical signals, the use of metals in this context has been relatively limited.","Nanostructures supporting surface plasmons in metals offer precise light manipulation and induce light-driven electron motion.","Through inverse design optimization of a gold nanostructure, we demonstrate enhanced volumetric, unidirectional, intense, and ultrafast photocurrents via a magneto-optical process derived from the inverse Faraday effect.","This is achieved through fine-tuning the amplitude, polarization, and their gradients in the local light field.","The virtually instantaneous process allows dynamic photocurrent modulation by varying optical pulse duration, potentially yielding nanosources of intense, ultrafast, planar magnetic fields, and frequency-tunable THz emission.","These findings opens avenues for ultrafast magnetic material manipulation and holds promise for nanoscale THz spectroscopy."],"url":"http://arxiv.org/abs/2402.00655v1","category":"physics.optics"}
{"created":"2024-02-01 15:13:14","title":"Polycube Layouts via Iterative Dual Loops","abstract":"Polycube layouts for 3D models effectively support a wide variety of methods such as hex-mesh construction, seamless texture mapping, spline fitting, and multi-block grid generation. Our study of polycube layouts is motivated by conformal mesh generation for aerospace modelling. In this setting, quality and correctness guarantees are of the utmost importance. However, currently the fully automatic construction of valid polycube layouts still poses significant challenges: state-of-the-art methods are generally not guaranteed to return a proper solution, even after post-processing, or they use a prohibitively large number of voxels that add detail indiscriminately.   In this paper we present a robust, flexible, and efficient method to generate polycube layouts. Our approach is based on a dual representation for polycube layouts and builds a layout by iteratively adding dual loops. Our construction is robust by design: at any iterative step we maintain a valid polycube layout. We offer the flexibility of manual intervention if the user so desires: while our method is able to compute a complete polycube layout without user intervention, the user can interrupt after each iteration and target further refinement on both the local and the global level. Last but not least, our method is efficient and can be implemented using comparatively simple algorithmic building blocks. Our implementation is publicly available and we present its output for numerous benchmark models.","sentences":["Polycube layouts for 3D models effectively support a wide variety of methods such as hex-mesh construction, seamless texture mapping, spline fitting, and multi-block grid generation.","Our study of polycube layouts is motivated by conformal mesh generation for aerospace modelling.","In this setting, quality and correctness guarantees are of the utmost importance.","However, currently the fully automatic construction of valid polycube layouts still poses significant challenges: state-of-the-art methods are generally not guaranteed to return a proper solution, even after post-processing, or they use a prohibitively large number of voxels that add detail indiscriminately.   ","In this paper we present a robust, flexible, and efficient method to generate polycube layouts.","Our approach is based on a dual representation for polycube layouts and builds a layout by iteratively adding dual loops.","Our construction is robust by design: at any iterative step we maintain a valid polycube layout.","We offer the flexibility of manual intervention if the user so desires: while our method is able to compute a complete polycube layout without user intervention, the user can interrupt after each iteration and target further refinement on both the local and the global level.","Last but not least, our method is efficient and can be implemented using comparatively simple algorithmic building blocks.","Our implementation is publicly available and we present its output for numerous benchmark models."],"url":"http://arxiv.org/abs/2402.00652v1","category":"cs.GR"}
{"created":"2024-02-01 15:13:13","title":"Skew-elliptical copula based mixed models for non-Gaussian longitudinal data with application to HIV-AIDS study","abstract":"This work has been motivated by a longitudinal data set on HIV CD4 T+ cell counts from Livingstone district, Zambia. The corresponding histogram plots indicate lack of symmetry in the marginal distributions and the pairwise scatter plots show non-elliptical dependence patterns. The standard linear mixed model for longitudinal data fails to capture these features. Thus it seems appropriate to consider a more general framework for modeling such data. In this article, we consider generalized linear mixed models (GLMM) for the marginals (e.g. Gamma mixed model), and temporal dependency of the repeated measurements is modeled by the copula corresponding to some skew-elliptical distributions (like skew-normal/skew-t). Our proposed class of copula based mixed models simultaneously takes into account asymmetry, between-subject variability and non-standard temporal dependence, and hence can be considered extensions to the standard linear mixed model based on multivariate normality. We estimate the model parameters using the IFM (inference function of margins) method, and also describe how to obtain standard errors of the parameter estimates. We investigate the finite sample performance of our procedure with extensive simulation studies involving skewed and symmetric marginal distributions and several choices of the copula. We finally apply our models to the HIV data set and report the findings.","sentences":["This work has been motivated by a longitudinal data set on HIV CD4 T+ cell counts from Livingstone district, Zambia.","The corresponding histogram plots indicate lack of symmetry in the marginal distributions and the pairwise scatter plots show non-elliptical dependence patterns.","The standard linear mixed model for longitudinal data fails to capture these features.","Thus it seems appropriate to consider a more general framework for modeling such data.","In this article, we consider generalized linear mixed models (GLMM) for the marginals (e.g. Gamma mixed model), and temporal dependency of the repeated measurements is modeled by the copula corresponding to some skew-elliptical distributions (like skew-normal/skew-t).","Our proposed class of copula based mixed models simultaneously takes into account asymmetry, between-subject variability and non-standard temporal dependence, and hence can be considered extensions to the standard linear mixed model based on multivariate normality.","We estimate the model parameters using the IFM (inference function of margins) method, and also describe how to obtain standard errors of the parameter estimates.","We investigate the finite sample performance of our procedure with extensive simulation studies involving skewed and symmetric marginal distributions and several choices of the copula.","We finally apply our models to the HIV data set and report the findings."],"url":"http://arxiv.org/abs/2402.00651v1","category":"stat.ME"}
{"created":"2024-02-01 15:08:41","title":"Cell-Free Massive MIMO SWIPT with Beyond Diagonal Reconfigurable Intelligent Surfaces","abstract":"This paper investigates the integration of beyond-diagonal reconfigurable intelligent surfaces (BD-RISs) into cell-free massive multiple-input multiple-output (CF-mMIMO) systems, focusing on applications involving simultaneous wireless information and power transfer (SWIPT). The system supports concurrently two user groups: information users (IUs) and energy users (EUs). A BD-RIS is employed to enhance the wireless power transfer (WPT) directed towards the EUs. To comprehensively evaluate the system's performance, we present an analytical framework for the spectral efficiency (SE) of IUs and the average harvested energy (HE) of EUs in the presence of spatial correlation among the BD-RIS elements and for a non-linear energy harvesting circuit. Our findings offer important insights into the transformative potential of BD-RIS, setting the stage for the development of more efficient and effective SWIPT networks. Finally, incorporating a heuristic scattering matrix design at the BD-RIS results in a substantial improvement compared to the scenario with random scattering matrix design.","sentences":["This paper investigates the integration of beyond-diagonal reconfigurable intelligent surfaces (BD-RISs) into cell-free massive multiple-input multiple-output (CF-mMIMO) systems, focusing on applications involving simultaneous wireless information and power transfer (SWIPT).","The system supports concurrently two user groups: information users (IUs) and energy users (EUs).","A BD-RIS is employed to enhance the wireless power transfer (WPT) directed towards the EUs.","To comprehensively evaluate the system's performance, we present an analytical framework for the spectral efficiency (SE) of IUs and the average harvested energy (HE) of EUs in the presence of spatial correlation among the BD-RIS elements and for a non-linear energy harvesting circuit.","Our findings offer important insights into the transformative potential of BD-RIS, setting the stage for the development of more efficient and effective SWIPT networks.","Finally, incorporating a heuristic scattering matrix design at the BD-RIS results in a substantial improvement compared to the scenario with random scattering matrix design."],"url":"http://arxiv.org/abs/2402.00646v1","category":"cs.IT"}
{"created":"2024-02-01 15:07:31","title":"Spectrally Transformed Kernel Regression","abstract":"Unlabeled data is a key component of modern machine learning. In general, the role of unlabeled data is to impose a form of smoothness, usually from the similarity information encoded in a base kernel, such as the $\\epsilon$-neighbor kernel or the adjacency matrix of a graph. This work revisits the classical idea of spectrally transformed kernel regression (STKR), and provides a new class of general and scalable STKR estimators able to leverage unlabeled data. Intuitively, via spectral transformation, STKR exploits the data distribution for which unlabeled data can provide additional information. First, we show that STKR is a principled and general approach, by characterizing a universal type of \"target smoothness\", and proving that any sufficiently smooth function can be learned by STKR. Second, we provide scalable STKR implementations for the inductive setting and a general transformation function, while prior work is mostly limited to the transductive setting. Third, we derive statistical guarantees for two scenarios: STKR with a known polynomial transformation, and STKR with kernel PCA when the transformation is unknown. Overall, we believe that this work helps deepen our understanding of how to work with unlabeled data, and its generality makes it easier to inspire new methods.","sentences":["Unlabeled data is a key component of modern machine learning.","In general, the role of unlabeled data is to impose a form of smoothness, usually from the similarity information encoded in a base kernel, such as the $\\epsilon$-neighbor kernel or the adjacency matrix of a graph.","This work revisits the classical idea of spectrally transformed kernel regression (STKR), and provides a new class of general and scalable STKR estimators able to leverage unlabeled data.","Intuitively, via spectral transformation, STKR exploits the data distribution for which unlabeled data can provide additional information.","First, we show that STKR is a principled and general approach, by characterizing a universal type of \"target smoothness\", and proving that any sufficiently smooth function can be learned by STKR.","Second, we provide scalable STKR implementations for the inductive setting and a general transformation function, while prior work is mostly limited to the transductive setting.","Third, we derive statistical guarantees for two scenarios: STKR with a known polynomial transformation, and STKR with kernel PCA when the transformation is unknown.","Overall, we believe that this work helps deepen our understanding of how to work with unlabeled data, and its generality makes it easier to inspire new methods."],"url":"http://arxiv.org/abs/2402.00645v1","category":"stat.ML"}
{"created":"2024-02-01 15:01:07","title":"Variants of the Erd\u0151s distinct sums problem and variance method","abstract":"Let $\\Sigma=\\{a_1, \\ldots , a_n\\}$ be a set of positive integers with $a_1 < \\ldots < a_n$ such that all $2^n$ subset sums are pairwise distinct. A famous conjecture of Erd\\H{o}s states that $a_n>C\\cdot 2^n$ for some constant $C$, while the best result known to date is of the form $a_n>C\\cdot 2^n/\\sqrt{n}$. In this paper, we propose a generalization of the Erd\\H{o}s distinct sum problem that is in the same spirit as those of the Davenport and the Erd\\H{o}s-Ginzburg-Ziv constants recently introduced in \\cite{CGS} and in \\cite{CS}. More precisely, we require that the non-zero evaluations of the $m$-th degree symmetric polynomial are all distinct over the subsequences of $\\Sigma$ whose size is at most $\\lambda n$, for a given $\\lambda\\in (0,1]$, considering $\\Sigma$ as a sequence in $\\mathbb{Z}^k$ with each coordinate of each $a_i$ in $[0,M]$. If $\\mathcal{F}_{\\lambda,n}$ denotes the family of subsets of $[1,n]$ whose size is at most $\\lambda n$, our main result is that, for each $k,m,$ and $\\lambda$, there exists an explicit constant $C_{k,m,\\lambda}$ such that $$ M\\geq C_{k,m,\\lambda} \\frac{(1+o(1)) |\\mathcal{F}_{\\lambda,n}|^{\\frac{1}{mk}}}{n^{1 - \\frac{1}{2m}}}.$$","sentences":["Let $\\Sigma=\\{a_1, \\ldots , a_n\\}$ be a set of positive integers with $a_1 <","\\ldots < a_n$ such that all $2^n$ subset sums are pairwise distinct.","A famous conjecture of Erd\\H{o}s states that $a_n>C\\cdot 2^n$ for some constant $C$, while the best result known to date is of the form $a_n>C\\cdot 2^n/\\sqrt{n}$. In this paper, we propose a generalization of the Erd\\H{o}s distinct sum problem that is in the same spirit as those of the Davenport and the Erd\\H{o}s-Ginzburg-Ziv constants recently introduced in \\cite{CGS} and in \\cite{CS}.","More precisely, we require that the non-zero evaluations of the $m$-th degree symmetric polynomial are all distinct over the subsequences of $\\Sigma$ whose size is at most $\\lambda n$, for a given $\\lambda\\in (0,1]$, considering $\\Sigma$ as a sequence in $\\mathbb{Z}^k$ with each coordinate of each $a_i$ in $[0,M]$. If $\\mathcal{F}_{\\lambda,n}$ denotes the family of subsets of $[1,n]$ whose size is at most $\\lambda n$, our main result is that, for each $k,m,$ and $\\lambda$, there exists an explicit constant $C_{k,m,\\lambda}$ such that $$ M\\geq C_{k,m,\\lambda} \\frac{(1+o(1))","|\\mathcal{F}_{\\lambda,n}|^{\\frac{1}{mk}}}{n^{1 - \\frac{1}{2m}}}.$$"],"url":"http://arxiv.org/abs/2402.00642v1","category":"math.CO"}
{"created":"2024-02-01 14:54:17","title":"Random Forest-Based Prediction of Stroke Outcome","abstract":"We research into the clinical, biochemical and neuroimaging factors associated with the outcome of stroke patients to generate a predictive model using machine learning techniques for prediction of mortality and morbidity 3 months after admission. The dataset consisted of patients with ischemic stroke (IS) and non-traumatic intracerebral hemorrhage (ICH) admitted to Stroke Unit of a European Tertiary Hospital prospectively registered. We identified the main variables for machine learning Random Forest (RF), generating a predictive model that can estimate patient mortality/morbidity. In conclusion, machine learning algorithms RF can be effectively used in stroke patients for long-term outcome prediction of mortality and morbidity.","sentences":["We research into the clinical, biochemical and neuroimaging factors associated with the outcome of stroke patients to generate a predictive model using machine learning techniques for prediction of mortality and morbidity 3 months after admission.","The dataset consisted of patients with ischemic stroke (IS) and non-traumatic intracerebral hemorrhage (ICH) admitted to Stroke Unit of a European Tertiary Hospital prospectively registered.","We identified the main variables for machine learning Random Forest (RF), generating a predictive model that can estimate patient mortality/morbidity.","In conclusion, machine learning algorithms RF can be effectively used in stroke patients for long-term outcome prediction of mortality and morbidity."],"url":"http://arxiv.org/abs/2402.00638v1","category":"cs.LG"}
{"created":"2024-02-01 14:52:16","title":"Fisheye Camera and Ultrasonic Sensor Fusion For Near-Field Obstacle Perception in Bird's-Eye-View","abstract":"Accurate obstacle identification represents a fundamental challenge within the scope of near-field perception for autonomous driving. Conventionally, fisheye cameras are frequently employed for comprehensive surround-view perception, including rear-view obstacle localization. However, the performance of such cameras can significantly deteriorate in low-light conditions, during nighttime, or when subjected to intense sun glare. Conversely, cost-effective sensors like ultrasonic sensors remain largely unaffected under these conditions. Therefore, we present, to our knowledge, the first end-to-end multimodal fusion model tailored for efficient obstacle perception in a bird's-eye-view (BEV) perspective, utilizing fisheye cameras and ultrasonic sensors. Initially, ResNeXt-50 is employed as a set of unimodal encoders to extract features specific to each modality. Subsequently, the feature space associated with the visible spectrum undergoes transformation into BEV. The fusion of these two modalities is facilitated via concatenation. At the same time, the ultrasonic spectrum-based unimodal feature maps pass through content-aware dilated convolution, applied to mitigate the sensor misalignment between two sensors in the fused feature space. Finally, the fused features are utilized by a two-stage semantic occupancy decoder to generate grid-wise predictions for precise obstacle perception. We conduct a systematic investigation to determine the optimal strategy for multimodal fusion of both sensors. We provide insights into our dataset creation procedures, annotation guidelines, and perform a thorough data analysis to ensure adequate coverage of all scenarios. When applied to our dataset, the experimental results underscore the robustness and effectiveness of our proposed multimodal fusion approach.","sentences":["Accurate obstacle identification represents a fundamental challenge within the scope of near-field perception for autonomous driving.","Conventionally, fisheye cameras are frequently employed for comprehensive surround-view perception, including rear-view obstacle localization.","However, the performance of such cameras can significantly deteriorate in low-light conditions, during nighttime, or when subjected to intense sun glare.","Conversely, cost-effective sensors like ultrasonic sensors remain largely unaffected under these conditions.","Therefore, we present, to our knowledge, the first end-to-end multimodal fusion model tailored for efficient obstacle perception in a bird's-eye-view (BEV) perspective, utilizing fisheye cameras and ultrasonic sensors.","Initially, ResNeXt-50 is employed as a set of unimodal encoders to extract features specific to each modality.","Subsequently, the feature space associated with the visible spectrum undergoes transformation into BEV.","The fusion of these two modalities is facilitated via concatenation.","At the same time, the ultrasonic spectrum-based unimodal feature maps pass through content-aware dilated convolution, applied to mitigate the sensor misalignment between two sensors in the fused feature space.","Finally, the fused features are utilized by a two-stage semantic occupancy decoder to generate grid-wise predictions for precise obstacle perception.","We conduct a systematic investigation to determine the optimal strategy for multimodal fusion of both sensors.","We provide insights into our dataset creation procedures, annotation guidelines, and perform a thorough data analysis to ensure adequate coverage of all scenarios.","When applied to our dataset, the experimental results underscore the robustness and effectiveness of our proposed multimodal fusion approach."],"url":"http://arxiv.org/abs/2402.00637v1","category":"cs.CV"}
{"created":"2024-02-01 14:46:35","title":"Prosody in Cascade and Direct Speech-to-Text Translation: a case study on Korean Wh-Phrases","abstract":"Speech-to-Text Translation (S2TT) has typically been addressed with cascade systems, where speech recognition systems generate a transcription that is subsequently passed to a translation model. While there has been a growing interest in developing direct speech translation systems to avoid propagating errors and losing non-verbal content, prior work in direct S2TT has struggled to conclusively establish the advantages of integrating the acoustic signal directly into the translation process. This work proposes using contrastive evaluation to quantitatively measure the ability of direct S2TT systems to disambiguate utterances where prosody plays a crucial role. Specifically, we evaluated Korean-English translation systems on a test set containing wh-phrases, for which prosodic features are necessary to produce translations with the correct intent, whether it's a statement, a yes/no question, a wh-question, and more. Our results clearly demonstrate the value of direct translation systems over cascade translation models, with a notable 12.9% improvement in overall accuracy in ambiguous cases, along with up to a 15.6% increase in F1 scores for one of the major intent categories. To the best of our knowledge, this work stands as the first to provide quantitative evidence that direct S2TT models can effectively leverage prosody. The code for our evaluation is openly accessible and freely available for review and utilisation.","sentences":["Speech-to-Text Translation (S2TT) has typically been addressed with cascade systems, where speech recognition systems generate a transcription that is subsequently passed to a translation model.","While there has been a growing interest in developing direct speech translation systems to avoid propagating errors and losing non-verbal content, prior work in direct S2TT has struggled to conclusively establish the advantages of integrating the acoustic signal directly into the translation process.","This work proposes using contrastive evaluation to quantitatively measure the ability of direct S2TT systems to disambiguate utterances where prosody plays a crucial role.","Specifically, we evaluated Korean-English translation systems on a test set containing wh-phrases, for which prosodic features are necessary to produce translations with the correct intent, whether it's a statement, a yes/no question, a wh-question, and more.","Our results clearly demonstrate the value of direct translation systems over cascade translation models, with a notable 12.9% improvement in overall accuracy in ambiguous cases, along with up to a 15.6% increase in F1 scores for one of the major intent categories.","To the best of our knowledge, this work stands as the first to provide quantitative evidence that direct S2TT models can effectively leverage prosody.","The code for our evaluation is openly accessible and freely available for review and utilisation."],"url":"http://arxiv.org/abs/2402.00632v1","category":"cs.CL"}
{"created":"2024-02-01 14:44:57","title":"Cocco: Hardware-Mapping Co-Exploration towards Memory Capacity-Communication Optimization","abstract":"Memory is a critical design consideration in current data-intensive DNN accelerators, as it profoundly determines energy consumption, bandwidth requirements, and area costs. As DNN structures become more complex, a larger on-chip memory capacity is required to reduce data movement overhead, but at the expense of silicon costs. Some previous works have proposed memory-oriented optimizations, such as different data reuse and layer fusion schemes. However, these methods are not general and potent enough to cope with various graph structures.   In this paper, we explore the intrinsic connection between network structures and memory features to optimize both hardware and mapping. First, we introduce a graph-level execution scheme with a corresponding dataflow and memory management method. This scheme enables the execution of arbitrary graph patterns with high data reuse and low hardware overhead. Subsequently, we propose Cocco, a hardware-mapping co-exploration framework leveraging graph-level features of networks. It aims to minimize communication overhead, such as energy consumption and bandwidth requirements, with a smaller memory capacity. We formulate the graph-partition scheduling and memory configuration search as an optimization problem and employ a genetic-based method to achieve efficient co-exploration for large and irregular networks. Experiments demonstrate that Cocco obtains lower external memory access, lower bandwidth requirements, and more stable optimization for graph partition compared to the greedy algorithm and dynamic programming introduced in prior works. Cocco also reduces the costs by 1.89% to 50.33% using co-exploration compared to other typical methods.","sentences":["Memory is a critical design consideration in current data-intensive DNN accelerators, as it profoundly determines energy consumption, bandwidth requirements, and area costs.","As DNN structures become more complex, a larger on-chip memory capacity is required to reduce data movement overhead, but at the expense of silicon costs.","Some previous works have proposed memory-oriented optimizations, such as different data reuse and layer fusion schemes.","However, these methods are not general and potent enough to cope with various graph structures.   ","In this paper, we explore the intrinsic connection between network structures and memory features to optimize both hardware and mapping.","First, we introduce a graph-level execution scheme with a corresponding dataflow and memory management method.","This scheme enables the execution of arbitrary graph patterns with high data reuse and low hardware overhead.","Subsequently, we propose Cocco, a hardware-mapping co-exploration framework leveraging graph-level features of networks.","It aims to minimize communication overhead, such as energy consumption and bandwidth requirements, with a smaller memory capacity.","We formulate the graph-partition scheduling and memory configuration search as an optimization problem and employ a genetic-based method to achieve efficient co-exploration for large and irregular networks.","Experiments demonstrate that Cocco obtains lower external memory access, lower bandwidth requirements, and more stable optimization for graph partition compared to the greedy algorithm and dynamic programming introduced in prior works.","Cocco also reduces the costs by 1.89% to 50.33% using co-exploration compared to other typical methods."],"url":"http://arxiv.org/abs/2402.00629v1","category":"cs.AR"}
{"created":"2024-02-01 14:41:59","title":"CapHuman: Capture Your Moments in Parallel Universes","abstract":"We concentrate on a novel human-centric image synthesis task, that is, given only one reference facial photograph, it is expected to generate specific individual images with diverse head positions, poses, and facial expressions in different contexts. To accomplish this goal, we argue that our generative model should be capable of the following favorable characteristics: (1) a strong visual and semantic understanding of our world and human society for basic object and human image generation. (2) generalizable identity preservation ability. (3) flexible and fine-grained head control. Recently, large pre-trained text-to-image diffusion models have shown remarkable results, serving as a powerful generative foundation. As a basis, we aim to unleash the above two capabilities of the pre-trained model. In this work, we present a new framework named CapHuman. We embrace the ``encode then learn to align\" paradigm, which enables generalizable identity preservation for new individuals without cumbersome tuning at inference. CapHuman encodes identity features and then learns to align them into the latent space. Moreover, we introduce the 3D facial prior to equip our model with control over the human head in a flexible and 3D-consistent manner. Extensive qualitative and quantitative analyses demonstrate our CapHuman can produce well-identity-preserved, photo-realistic, and high-fidelity portraits with content-rich representations and various head renditions, superior to established baselines. Code and checkpoint will be released at https://github.com/VamosC/CapHuman.","sentences":["We concentrate on a novel human-centric image synthesis task, that is, given only one reference facial photograph, it is expected to generate specific individual images with diverse head positions, poses, and facial expressions in different contexts.","To accomplish this goal, we argue that our generative model should be capable of the following favorable characteristics: (1) a strong visual and semantic understanding of our world and human society for basic object and human image generation.","(2) generalizable identity preservation ability.","(3) flexible and fine-grained head control.","Recently, large pre-trained text-to-image diffusion models have shown remarkable results, serving as a powerful generative foundation.","As a basis, we aim to unleash the above two capabilities of the pre-trained model.","In this work, we present a new framework named CapHuman.","We embrace the ``encode then learn to align\" paradigm, which enables generalizable identity preservation for new individuals without cumbersome tuning at inference.","CapHuman encodes identity features and then learns to align them into the latent space.","Moreover, we introduce the 3D facial prior to equip our model with control over the human head in a flexible and 3D-consistent manner.","Extensive qualitative and quantitative analyses demonstrate our CapHuman can produce well-identity-preserved, photo-realistic, and high-fidelity portraits with content-rich representations and various head renditions, superior to established baselines.","Code and checkpoint will be released at https://github.com/VamosC/CapHuman."],"url":"http://arxiv.org/abs/2402.00627v1","category":"cs.CV"}
{"created":"2024-02-01 14:41:20","title":"Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks","abstract":"Recently, significant progress has been made on Large Vision-Language Models (LVLMs); a new class of VL models that make use of large pre-trained language models. Yet, their vulnerability to Typographic attacks, which involve superimposing misleading text onto an image remain unstudied. Furthermore, prior work typographic attacks rely on sampling a random misleading class from a predefined set of classes. However, the random chosen class might not be the most effective attack. To address these issues, we first introduce a novel benchmark uniquely designed to test LVLMs vulnerability to typographic attacks. Furthermore, we introduce a new and more effective typographic attack: Self-Generated typographic attacks. Indeed, our method, given an image, make use of the strong language capabilities of models like GPT-4V by simply prompting them to recommend a typographic attack. Using our novel benchmark, we uncover that typographic attacks represent a significant threat against LVLM(s). Furthermore, we uncover that typographic attacks recommended by GPT-4V using our new method are not only more effective against GPT-4V itself compared to prior work attacks, but also against a host of less capable yet popular open source models like LLaVA, InstructBLIP, and MiniGPT4.","sentences":["Recently, significant progress has been made on Large Vision-Language Models (LVLMs); a new class of VL models that make use of large pre-trained language models.","Yet, their vulnerability to Typographic attacks, which involve superimposing misleading text onto an image remain unstudied.","Furthermore, prior work typographic attacks rely on sampling a random misleading class from a predefined set of classes.","However, the random chosen class might not be the most effective attack.","To address these issues, we first introduce a novel benchmark uniquely designed to test LVLMs vulnerability to typographic attacks.","Furthermore, we introduce a new and more effective typographic attack: Self-Generated typographic attacks.","Indeed, our method, given an image, make use of the strong language capabilities of models like GPT-4V by simply prompting them to recommend a typographic attack.","Using our novel benchmark, we uncover that typographic attacks represent a significant threat against LVLM(s).","Furthermore, we uncover that typographic attacks recommended by GPT-4V using our new method are not only more effective against GPT-4V itself compared to prior work attacks, but also against a host of less capable yet popular open source models like LLaVA, InstructBLIP, and MiniGPT4."],"url":"http://arxiv.org/abs/2402.00626v1","category":"cs.CV"}
{"created":"2024-02-01 14:40:17","title":"Bialgebraic Reasoning on Higher-Order Program Equivalence","abstract":"Logical relations constitute a key method for reasoning about contextual equivalence of programs in higher-order languages. They are usually developed on a per-case basis, with a new theory required for each variation of the language or of the desired notion of equivalence. In the present paper we introduce a general construction of (step-indexed) logical relations at the level of Higher-Order Mathematical Operational Semantics, a highly parametric categorical framework for modeling the operational semantics of higher-order languages. Our main result asserts that for languages whose weak operational model forms a lax bialgebra, the logical relation is automatically sound for contextual equivalence. Our abstract theory is shown to instantiate to combinatory logics and $\\lambda$-calculi with recursive types, and to different flavours of contextual equivalence.","sentences":["Logical relations constitute a key method for reasoning about contextual equivalence of programs in higher-order languages.","They are usually developed on a per-case basis, with a new theory required for each variation of the language or of the desired notion of equivalence.","In the present paper we introduce a general construction of (step-indexed) logical relations at the level of Higher-Order Mathematical Operational Semantics, a highly parametric categorical framework for modeling the operational semantics of higher-order languages.","Our main result asserts that for languages whose weak operational model forms a lax bialgebra, the logical relation is automatically sound for contextual equivalence.","Our abstract theory is shown to instantiate to combinatory logics and $\\lambda$-calculi with recursive types, and to different flavours of contextual equivalence."],"url":"http://arxiv.org/abs/2402.00625v1","category":"cs.LO"}
{"created":"2024-02-01 14:30:39","title":"Actor Identification in Discourse: A Challenge for LLMs?","abstract":"The identification of political actors who put forward claims in public debate is a crucial step in the construction of discourse networks, which are helpful to analyze societal debates. Actor identification is, however, rather challenging: Often, the locally mentioned speaker of a claim is only a pronoun (\"He proposed that [claim]\"), so recovering the canonical actor name requires discourse understanding. We compare a traditional pipeline of dedicated NLP components (similar to those applied to the related task of coreference) with a LLM, which appears a good match for this generation task. Evaluating on a corpus of German actors in newspaper reports, we find surprisingly that the LLM performs worse. Further analysis reveals that the LLM is very good at identifying the right reference, but struggles to generate the correct canonical form. This points to an underlying issue in LLMs with controlling generated output. Indeed, a hybrid model combining the LLM with a classifier to normalize its output substantially outperforms both initial models.","sentences":["The identification of political actors who put forward claims in public debate is a crucial step in the construction of discourse networks, which are helpful to analyze societal debates.","Actor identification is, however, rather challenging: Often, the locally mentioned speaker of a claim is only a pronoun (\"He proposed that [claim]\"), so recovering the canonical actor name requires discourse understanding.","We compare a traditional pipeline of dedicated NLP components (similar to those applied to the related task of coreference) with a LLM, which appears a good match for this generation task.","Evaluating on a corpus of German actors in newspaper reports, we find surprisingly that the LLM performs worse.","Further analysis reveals that the LLM is very good at identifying the right reference, but struggles to generate the correct canonical form.","This points to an underlying issue in LLMs with controlling generated output.","Indeed, a hybrid model combining the LLM with a classifier to normalize its output substantially outperforms both initial models."],"url":"http://arxiv.org/abs/2402.00620v1","category":"cs.CL"}
{"created":"2024-02-01 14:19:01","title":"Dual-Tap Optical-Digital Feedforward Equalization Enabling High-Speed Optical Transmission in IM/DD Systems","abstract":"Intensity-modulation and direct-detection (IM/DD) transmission is widely adopted for high-speed optical transmission scenarios due to its cost-effectiveness and simplicity. However, as the data rate increases, the fiber chromatic dispersion (CD) would induce a serious power fading effect, and direct detection could generate inter-symbol interference (ISI). Moreover, the ISI becomes more severe with the increase of fiber length, thereby highly restricting the transmission distance of IM/DD systems. This paper proposes a dual-tap optical-digital feedforward equalization (DT-ODFE) scheme, which could effectively compensate for CD-induced power fading while maintaining low cost and simplicity. A theoretical channel response is formulated for IM/DD transmission, incorporating a dual-tap optical equalizer, and the theoretical analysis reveals that for an IM/DD transmission using 1371nm over 10km standard single-mode fiber (SSMF), frequency notch is removed from 33.7GHz to 46GHz. Simulation results show that the DT- ODFE achieves an SNR gain of 2.3dB over IM/DD systems with symbol-space feedforward equalizer (FFE) alone. As the fiber length increases to 15 km, DT- ODFE performs well, while FFE, decision-feedback equalizer (DFE) and Volterra nonlinear equalizers (VNLE) all fail to compensate for the power fading and the 7% hard-decision FEC limit is not satisfied. For 200 Gb/s/$\\lambda$ PAM-4 over 15km SSMF, results show that the signal-to-noise ratio (SNR) of the proposed DT- ODFE with optimal coefficients satisfies the 7% hard-decision FEC limit, which uncovers the great potential of the DT- ODFE for high-speed IM/DD systems in LR/FR scenarios.","sentences":["Intensity-modulation and direct-detection (IM/DD) transmission is widely adopted for high-speed optical transmission scenarios due to its cost-effectiveness and simplicity.","However, as the data rate increases, the fiber chromatic dispersion (CD) would induce a serious power fading effect, and direct detection could generate inter-symbol interference (ISI).","Moreover, the ISI becomes more severe with the increase of fiber length, thereby highly restricting the transmission distance of IM/DD systems.","This paper proposes a dual-tap optical-digital feedforward equalization (DT-ODFE) scheme, which could effectively compensate for CD-induced power fading while maintaining low cost and simplicity.","A theoretical channel response is formulated for IM/DD transmission, incorporating a dual-tap optical equalizer, and the theoretical analysis reveals that for an IM/DD transmission using 1371nm over 10km standard single-mode fiber (SSMF), frequency notch is removed from 33.7GHz to 46GHz.","Simulation results show that the DT- ODFE achieves an SNR gain of 2.3dB over IM/DD systems with symbol-space feedforward equalizer (FFE) alone.","As the fiber length increases to 15 km, DT- ODFE performs well, while FFE, decision-feedback equalizer (DFE) and Volterra nonlinear equalizers (VNLE) all fail to compensate for the power fading and the 7% hard-decision FEC limit is not satisfied.","For 200 Gb/s/$\\lambda$ PAM-4 over 15km SSMF, results show that the signal-to-noise ratio (SNR) of the proposed DT- ODFE with optimal coefficients satisfies the 7% hard-decision FEC limit, which uncovers the great potential of the DT- ODFE for high-speed IM/DD systems in LR/FR scenarios."],"url":"http://arxiv.org/abs/2402.00616v1","category":"eess.SP"}
{"created":"2024-02-01 14:18:56","title":"Validation of the IPSL Venus GCM Thermal Structure with Venus Express Data","abstract":"General circulation models (GCMs) are valuable instruments to understand the most peculiar features in the atmospheres of planets and the mechanisms behind their dynamics. Venus makes no exception and it has been extensively studied thanks to GCMs. Here we validate the current version of the Institut Pierre Simon Laplace (IPSL) Venus GCM, by means of a comparison between the modelled temperature field and that obtained from data by the Visible and Infrared Thermal Imaging Spectrometer (VIRTIS) and the Venus Express Radio Science Experiment (VeRa) onboard Venus Express. The modelled thermal structure displays an overall good agreement with data, and the cold collar is successfully reproduced at latitudes higher than +/-55\\deg, with an extent and a behavior close to the observed ones. Thermal tides developing in the model appear to be consistent in phase and amplitude with data: diurnal tide dominates at altitudes above 10^2 Pa pressure level and at high-latitudes, while semidiurnal tide dominates between 10^2 and 10^4 Pa, from low to mid-latitudes. The main difference revealed by our analysis is located poleward of 50\\deg, where the model is affected by a second temperature inversion arising at 10^3 Pa. This second inversion, possibly related to the adopted aerosols distribution, is not observed in data.","sentences":["General circulation models (GCMs) are valuable instruments to understand the most peculiar features in the atmospheres of planets and the mechanisms behind their dynamics.","Venus makes no exception and it has been extensively studied thanks to GCMs.","Here we validate the current version of the Institut Pierre Simon Laplace (IPSL) Venus GCM, by means of a comparison between the modelled temperature field and that obtained from data by the Visible and Infrared Thermal Imaging Spectrometer (VIRTIS) and the Venus Express Radio Science Experiment (VeRa) onboard Venus Express.","The modelled thermal structure displays an overall good agreement with data, and the cold collar is successfully reproduced at latitudes higher than +/-55\\deg, with an extent and a behavior close to the observed ones.","Thermal tides developing in the model appear to be consistent in phase and amplitude with data: diurnal tide dominates at altitudes above 10^2 Pa pressure level and at high-latitudes, while semidiurnal tide dominates between 10^2 and 10^4 Pa, from low to mid-latitudes.","The main difference revealed by our analysis is located poleward of 50\\deg, where the model is affected by a second temperature inversion arising at 10^3 Pa.","This second inversion, possibly related to the adopted aerosols distribution, is not observed in data."],"url":"http://arxiv.org/abs/2402.00615v1","category":"astro-ph.EP"}
{"created":"2024-02-01 14:03:39","title":"Analysis of astronomical data through sonification: reaching more inclusion for visual disable scientists","abstract":"Most tools for astrophysical research was centered on visual display. Even after some studies shows that the use of sound could help the data analysis, and on the other hand generate more accessibility. This fact motivates the creation of a tool centered on the researcher with and without visual impairments. To carry out this challenge, on this contribution, a theoretical framework based on visual impaired people was created and included on the sonoUno software. After that, the accessibility of the tool was analysed with the ISO standard 9241-171:2008.","sentences":["Most tools for astrophysical research was centered on visual display.","Even after some studies shows that the use of sound could help the data analysis, and on the other hand generate more accessibility.","This fact motivates the creation of a tool centered on the researcher with and without visual impairments.","To carry out this challenge, on this contribution, a theoretical framework based on visual impaired people was created and included on the sonoUno software.","After that, the accessibility of the tool was analysed with the ISO standard 9241-171:2008."],"url":"http://arxiv.org/abs/2402.00611v1","category":"astro-ph.IM"}
{"created":"2024-02-01 13:59:04","title":"Are Synthetic Time-series Data Really not as Good as Real Data?","abstract":"Time-series data presents limitations stemming from data quality issues, bias and vulnerabilities, and generalization problem. Integrating universal data synthesis methods holds promise in improving generalization. However, current methods cannot guarantee that the generator's output covers all unseen real data. In this paper, we introduce InfoBoost -- a highly versatile cross-domain data synthesizing framework with time series representation learning capability. We have developed a method based on synthetic data that enables model training without the need for real data, surpassing the performance of models trained with real data. Additionally, we have trained a universal feature extractor based on our synthetic data that is applicable to all time-series data. Our approach overcomes interference from multiple sources rhythmic signal, noise interference, and long-period features that exceed sampling window capabilities. Through experiments, our non-deep-learning synthetic data enables models to achieve superior reconstruction performance and universal explicit representation extraction without the need for real data.","sentences":["Time-series data presents limitations stemming from data quality issues, bias and vulnerabilities, and generalization problem.","Integrating universal data synthesis methods holds promise in improving generalization.","However, current methods cannot guarantee that the generator's output covers all unseen real data.","In this paper, we introduce InfoBoost -- a highly versatile cross-domain data synthesizing framework with time series representation learning capability.","We have developed a method based on synthetic data that enables model training without the need for real data, surpassing the performance of models trained with real data.","Additionally, we have trained a universal feature extractor based on our synthetic data that is applicable to all time-series data.","Our approach overcomes interference from multiple sources rhythmic signal, noise interference, and long-period features that exceed sampling window capabilities.","Through experiments, our non-deep-learning synthetic data enables models to achieve superior reconstruction performance and universal explicit representation extraction without the need for real data."],"url":"http://arxiv.org/abs/2402.00607v1","category":"cs.LG"}
{"created":"2024-02-01 13:56:42","title":"Efficiently Separating the Source of the Teukolsky Equation","abstract":"Recent gravitational wave detections from black hole mergers have underscored the critical role black hole perturbation theory and the Teukolsky equation play in understanding the behaviour of black holes. The separable nature of the Teukolsky equation has long been leveraged to study the vacuum linear Teukolsky equation; however, as theory and measurements advance, solving the sourced Teukolsky equation is becoming a frontier of research. In particular, second-order calculations, such as in quasi-normal mode and self-force problems, have extended sources. This paper presents a novel method for efficiently separating the Teukolsky equation's source analytically, a non-trivial problem due to the angular and radial mixing of generic quantities in Kerr spacetime. We provide a proof of concept demonstration of our method's accuracy, separating the Teukolsky source produced by the stress-energy tensor of an ideal gas cloud surrounding a Kerr black hole. The detailed application of our method is provided in an accompanying \\textit{Mathematica} notebook. Our approach opens up a new avenue for efficient, accurate black hole perturbation theory calculations with sources in both the time and frequency domain.","sentences":["Recent gravitational wave detections from black hole mergers have underscored the critical role black hole perturbation theory and the Teukolsky equation play in understanding the behaviour of black holes.","The separable nature of the Teukolsky equation has long been leveraged to study the vacuum linear Teukolsky equation; however, as theory and measurements advance, solving the sourced Teukolsky equation is becoming a frontier of research.","In particular, second-order calculations, such as in quasi-normal mode and self-force problems, have extended sources.","This paper presents a novel method for efficiently separating the Teukolsky equation's source analytically, a non-trivial problem due to the angular and radial mixing of generic quantities in Kerr spacetime.","We provide a proof of concept demonstration of our method's accuracy, separating the Teukolsky source produced by the stress-energy tensor of an ideal gas cloud surrounding a Kerr black hole.","The detailed application of our method is provided in an accompanying \\textit{Mathematica} notebook.","Our approach opens up a new avenue for efficient, accurate black hole perturbation theory calculations with sources in both the time and frequency domain."],"url":"http://arxiv.org/abs/2402.00604v1","category":"gr-qc"}
{"created":"2024-02-01 13:54:10","title":"Small-scale clustering of Primordial Black Holes: cloud-in-cloud and exclusion effects","abstract":"Using an excursion-set approach, we revisit the initial spatial clustering of Primordial Black Holes (PBHs) originating from the Hubble re-entry of large Gaussian density fluctuations in the early universe. We derive the two-point correlation functions of PBHs, properly accounting for the \"cloud-in-cloud\" mechanism. Our expressions naturally and intrinsically correlate the formation of pairs of PBHs, which is a key difference with the Poisson model of clustering. Our approach effectively includes short-range exclusion effects and clarifies the clustering behaviors at small scale: PBHs are anti-correlated at short distances. Using a scale-independent collapse threshold, we derive explicit expressions for the excess probability to find pairs of PBHs separated by a distance $r$, as well as the excess probability to find pairs with asymmetric mass ratio. Our framework is model-independent by construction and we discuss possible other applications.","sentences":["Using an excursion-set approach, we revisit the initial spatial clustering of Primordial Black Holes (PBHs) originating from the Hubble re-entry of large Gaussian density fluctuations in the early universe.","We derive the two-point correlation functions of PBHs, properly accounting for the \"cloud-in-cloud\" mechanism.","Our expressions naturally and intrinsically correlate the formation of pairs of PBHs, which is a key difference with the Poisson model of clustering.","Our approach effectively includes short-range exclusion effects and clarifies the clustering behaviors at small scale: PBHs are anti-correlated at short distances.","Using a scale-independent collapse threshold, we derive explicit expressions for the excess probability to find pairs of PBHs separated by a distance $r$, as well as the excess probability to find pairs with asymmetric mass ratio.","Our framework is model-independent by construction and we discuss possible other applications."],"url":"http://arxiv.org/abs/2402.00600v1","category":"astro-ph.CO"}
{"created":"2024-02-01 13:41:44","title":"Uncertainty-Aware Partial-Label Learning","abstract":"In real-world applications, one often encounters ambiguously labeled data, where different annotators assign conflicting class labels. Partial-label learning allows training classifiers in this weakly supervised setting. While state-of-the-art methods already feature good predictive performance, they often suffer from miscalibrated uncertainty estimates. However, having well-calibrated uncertainty estimates is important, especially in safety-critical domains like medicine and autonomous driving. In this article, we propose a novel nearest-neighbor-based partial-label-learning algorithm that leverages Dempster-Shafer theory. Extensive experiments on artificial and real-world datasets show that the proposed method provides a well-calibrated uncertainty estimate and achieves competitive prediction performance. Additionally, we prove that our algorithm is risk-consistent.","sentences":["In real-world applications, one often encounters ambiguously labeled data, where different annotators assign conflicting class labels.","Partial-label learning allows training classifiers in this weakly supervised setting.","While state-of-the-art methods already feature good predictive performance, they often suffer from miscalibrated uncertainty estimates.","However, having well-calibrated uncertainty estimates is important, especially in safety-critical domains like medicine and autonomous driving.","In this article, we propose a novel nearest-neighbor-based partial-label-learning algorithm that leverages Dempster-Shafer theory.","Extensive experiments on artificial and real-world datasets show that the proposed method provides a well-calibrated uncertainty estimate and achieves competitive prediction performance.","Additionally, we prove that our algorithm is risk-consistent."],"url":"http://arxiv.org/abs/2402.00592v1","category":"cs.LG"}
{"created":"2024-02-01 13:37:53","title":"Sandra -- A Neuro-Symbolic Reasoner Based On Descriptions And Situations","abstract":"This paper presents sandra, a neuro-symbolic reasoner combining vectorial representations with deductive reasoning. Sandra builds a vector space constrained by an ontology and performs reasoning over it. The geometric nature of the reasoner allows its combination with neural networks, bridging the gap with symbolic knowledge representations. Sandra is based on the Description and Situation (DnS) ontology design pattern, a formalization of frame semantics. Given a set of facts (a situation) it allows to infer all possible perspectives (descriptions) that can provide a plausible interpretation for it, even in presence of incomplete information. We prove that our method is correct with respect to the DnS model. We experiment with two different tasks and their standard benchmarks, demonstrating that, without increasing complexity, sandra (i) outperforms all the baselines (ii) provides interpretability in the classification process, and (iii) allows control over the vector space, which is designed a priori.","sentences":["This paper presents sandra, a neuro-symbolic reasoner combining vectorial representations with deductive reasoning.","Sandra builds a vector space constrained by an ontology and performs reasoning over it.","The geometric nature of the reasoner allows its combination with neural networks, bridging the gap with symbolic knowledge representations.","Sandra is based on the Description and Situation (DnS) ontology design pattern, a formalization of frame semantics.","Given a set of facts (a situation) it allows to infer all possible perspectives (descriptions) that can provide a plausible interpretation for it, even in presence of incomplete information.","We prove that our method is correct with respect to the DnS model.","We experiment with two different tasks and their standard benchmarks, demonstrating that, without increasing complexity, sandra (i) outperforms all the baselines (ii) provides interpretability in the classification process, and (iii) allows control over the vector space, which is designed a priori."],"url":"http://arxiv.org/abs/2402.00591v1","category":"cs.AI"}
{"created":"2024-02-01 13:34:59","title":"BrainSLAM: SLAM on Neural Population Activity Data","abstract":"Simultaneous localisation and mapping (SLAM) algorithms are commonly used in robotic systems for learning maps of novel environments. Brains also appear to learn maps, but the mechanisms are not known and it is unclear how to infer these maps from neural activity data. We present BrainSLAM; a method for performing SLAM using only population activity (local field potential, LFP) data simultaneously recorded from three brain regions in rats: hippocampus, prefrontal cortex, and parietal cortex. This system uses a convolutional neural network (CNN) to decode velocity and familiarity information from wavelet scalograms of neural local field potential data recorded from rats as they navigate a 2D maze. The CNN's output drives a RatSLAM-inspired architecture, powering an attractor network which performs path integration plus a separate system which performs `loop closure' (detecting previously visited locations and correcting map aliasing errors). Together, these three components can construct faithful representations of the environment while simultaneously tracking the animal's location. This is the first demonstration of inference of a spatial map from brain recordings. Our findings expand SLAM to a new modality, enabling a new method of mapping environments and facilitating a better understanding of the role of cognitive maps in navigation and decision making.","sentences":["Simultaneous localisation and mapping (SLAM) algorithms are commonly used in robotic systems for learning maps of novel environments.","Brains also appear to learn maps, but the mechanisms are not known and it is unclear how to infer these maps from neural activity data.","We present BrainSLAM; a method for performing SLAM using only population activity (local field potential, LFP) data simultaneously recorded from three brain regions in rats: hippocampus, prefrontal cortex, and parietal cortex.","This system uses a convolutional neural network (CNN) to decode velocity and familiarity information from wavelet scalograms of neural local field potential data recorded from rats as they navigate a 2D maze.","The CNN's output drives a RatSLAM-inspired architecture, powering an attractor network which performs path integration plus a separate system which performs `loop closure' (detecting previously visited locations and correcting map aliasing errors).","Together, these three components can construct faithful representations of the environment while simultaneously tracking the animal's location.","This is the first demonstration of inference of a spatial map from brain recordings.","Our findings expand SLAM to a new modality, enabling a new method of mapping environments and facilitating a better understanding of the role of cognitive maps in navigation and decision making."],"url":"http://arxiv.org/abs/2402.00588v1","category":"cs.RO"}
{"created":"2024-02-01 13:13:16","title":"Diffusion-based Light Field Synthesis","abstract":"Light fields (LFs), conducive to comprehensive scene radiance recorded across angular dimensions, find wide applications in 3D reconstruction, virtual reality, and computational photography.However, the LF acquisition is inevitably time-consuming and resource-intensive due to the mainstream acquisition strategy involving manual capture or laborious software synthesis.Given such a challenge, we introduce LFdiff, a straightforward yet effective diffusion-based generative framework tailored for LF synthesis, which adopts only a single RGB image as input.LFdiff leverages disparity estimated by a monocular depth estimation network and incorporates two distinctive components: a novel condition scheme and a noise estimation network tailored for LF data.Specifically, we design a position-aware warping condition scheme, enhancing inter-view geometry learning via a robust conditional signal.We then propose DistgUnet, a disentanglement-based noise estimation network, to harness comprehensive LF representations.Extensive experiments demonstrate that LFdiff excels in synthesizing visually pleasing and disparity-controllable light fields with enhanced generalization capability.Additionally, comprehensive results affirm the broad applicability of the generated LF data, spanning applications like LF super-resolution and refocusing.","sentences":["Light fields (LFs), conducive to comprehensive scene radiance recorded across angular dimensions, find wide applications in 3D reconstruction, virtual reality, and computational photography.","However, the LF acquisition is inevitably time-consuming and resource-intensive due to the mainstream acquisition strategy involving manual capture or laborious software synthesis.","Given such a challenge, we introduce LFdiff, a straightforward yet effective diffusion-based generative framework tailored for LF synthesis, which adopts only a single RGB image as input.","LFdiff leverages disparity estimated by a monocular depth estimation network and incorporates two distinctive components: a novel condition scheme and a noise estimation network tailored for LF data.","Specifically, we design a position-aware warping condition scheme, enhancing inter-view geometry learning via a robust conditional signal.","We then propose DistgUnet, a disentanglement-based noise estimation network, to harness comprehensive LF representations.","Extensive experiments demonstrate that LFdiff excels in synthesizing visually pleasing and disparity-controllable light fields with enhanced generalization capability.","Additionally, comprehensive results affirm the broad applicability of the generated LF data, spanning applications like LF super-resolution and refocusing."],"url":"http://arxiv.org/abs/2402.00575v1","category":"cs.CV"}
{"created":"2024-02-01 13:12:58","title":"B-Fredholm theory in Banach algebras","abstract":"The aim of this paper is to develop a systematic B-Fredholm theory in semiprime Banach algebras. We first generalize Smyth's important punctured neighbourhood theorem to B-Fredholm elements. Then using this result, we investigate the local spectral theory of B-Fredholm elements, including the localized left (resp. right) SVEP and a classification of components of B-Fredholm resolvent set. Finally, in semisimple Banach algebra context, we characterize element $f$ such that $f^{n}$ belongs to the socle for some $n \\in \\mathbb{N}$ from two different perspectives: one is the invariance of the B-Fredholm spectrum under commuting perturbation $f$, the other is the Rieszness and the B-Fredholmness of $f$.","sentences":["The aim of this paper is to develop a systematic B-Fredholm theory in semiprime Banach algebras.","We first generalize Smyth's important punctured neighbourhood theorem to B-Fredholm elements.","Then using this result, we investigate the local spectral theory of B-Fredholm elements, including the localized left (resp.","right) SVEP and a classification of components of B-Fredholm resolvent set.","Finally, in semisimple Banach algebra context, we characterize element $f$ such that $f^{n}$ belongs to the socle for some $n \\in \\mathbb{N}$ from two different perspectives: one is the invariance of the B-Fredholm spectrum under commuting perturbation $f$, the other is the Rieszness and the B-Fredholmness of $f$."],"url":"http://arxiv.org/abs/2402.00574v1","category":"math.FA"}
{"created":"2024-02-01 13:12:24","title":"Active Sourced Wavefield Modeling for Layered Half-Space","abstract":"Traditional free vibration-based forward models generate theoretical dispersion curves under the assumption of planar waves, neglecting the influence of the actual source-receiver configuration. While 2D/3D numerical wavefield modeling approaches mimic real-field scenarios with source-receiver information, they suffer from computational inefficiency. This study introduces a semi-analytical wavefield modeling approach incorporating source-receiver data acquisition layouts. The method considers a cylindrically spreading wavefield described by the Hankel function instead of the planer wave assumption. The approach considers both propagating waves characterized by real wavenumbers and decaying waves with complex wavenumbers, allowing for calculating surface displacements in both the far and near fields. The proposed model captures the complete wavefield, including source-offset effects and leaky waves, while maintaining computational efficiency comparable to any free vibration-based approaches. The method entails solving the eigenvalue problem constructed through the higher-order thin-layer method. Subsequently, it calculates the frequency domain vertical and radial surface responses at any desired location in space generated by a vertically positioned active source. The overall performance of the proposed method is investigated on diverse profiles, including regularly dispersive media, low-velocity layer models, and thin plate structures. The vertical and radial component dispersion images are validated against the numerical approach. The proposed method is at least two orders of magnitude faster than the numerical method. Notably, it captures the smooth transition of modal energy from the fundamental mode to higher modes, occurring due to modal osculation at low frequencies. The present approach offers a valuable tool to enhance the efficiency of active surface wave methods.","sentences":["Traditional free vibration-based forward models generate theoretical dispersion curves under the assumption of planar waves, neglecting the influence of the actual source-receiver configuration.","While 2D/3D numerical wavefield modeling approaches mimic real-field scenarios with source-receiver information, they suffer from computational inefficiency.","This study introduces a semi-analytical wavefield modeling approach incorporating source-receiver data acquisition layouts.","The method considers a cylindrically spreading wavefield described by the Hankel function instead of the planer wave assumption.","The approach considers both propagating waves characterized by real wavenumbers and decaying waves with complex wavenumbers, allowing for calculating surface displacements in both the far and near fields.","The proposed model captures the complete wavefield, including source-offset effects and leaky waves, while maintaining computational efficiency comparable to any free vibration-based approaches.","The method entails solving the eigenvalue problem constructed through the higher-order thin-layer method.","Subsequently, it calculates the frequency domain vertical and radial surface responses at any desired location in space generated by a vertically positioned active source.","The overall performance of the proposed method is investigated on diverse profiles, including regularly dispersive media, low-velocity layer models, and thin plate structures.","The vertical and radial component dispersion images are validated against the numerical approach.","The proposed method is at least two orders of magnitude faster than the numerical method.","Notably, it captures the smooth transition of modal energy from the fundamental mode to higher modes, occurring due to modal osculation at low frequencies.","The present approach offers a valuable tool to enhance the efficiency of active surface wave methods."],"url":"http://arxiv.org/abs/2402.00573v1","category":"physics.geo-ph"}
{"created":"2024-02-01 13:08:30","title":"Quasi-perpendicular shocks of galaxy clusters in hybrid kinetic simulations: The structure of the shocks","abstract":"Context: Cosmic ray acceleration in galaxy clusters is still an ongoing puzzle, with relativistic electrons forming radio relics at merger shocks and emitting synchrotron radiation. In the present work we perform hybrid-kinetic simulations in a range of various quasi-perpendicular foreshock conditions, including plasma beta, magnetic obliquity, and the shock Mach number. Aims: We study the ion kinetic physics, responsible for the shock structure and wave turbulence, that in turn affects the particle acceleration processes. Methods: We apply a recently developed generalized fluid-particle hybrid numerical code that can combine fluid modeling for both kinetic ions and fluid electrons. The model utilizes the exact form of the generalized Ohm's law, allowing for arbitrary choice of mass and energy densities, as well as the charge-to-mass ratio of the kinetic species. Results: We show that the properties of ion-driven multi-scale magnetic turbulence in merger shocks are in agreement with the ion structures observed in PIC simulations. In typical shocks with the sonic Mach number $M_s=3$, the magnetic structures and shock front density ripples grow and saturate at wavelengths reaching approximately four ion Larmor radii. Finally, we note that the steady-state structure of $M_s=3$ shocks in high-beta plasmas shows evidence that there is little difference between 2D and 3D simulations. The turbulence near the shock front seems to be a 2D-like structure in 3D simulations.","sentences":["Context: Cosmic ray acceleration in galaxy clusters is still an ongoing puzzle, with relativistic electrons forming radio relics at merger shocks and emitting synchrotron radiation.","In the present work we perform hybrid-kinetic simulations in a range of various quasi-perpendicular foreshock conditions, including plasma beta, magnetic obliquity, and the shock Mach number.","Aims:","We study the ion kinetic physics, responsible for the shock structure and wave turbulence, that in turn affects the particle acceleration processes.","Methods: We apply a recently developed generalized fluid-particle hybrid numerical code that can combine fluid modeling for both kinetic ions and fluid electrons.","The model utilizes the exact form of the generalized Ohm's law, allowing for arbitrary choice of mass and energy densities, as well as the charge-to-mass ratio of the kinetic species.","Results:","We show that the properties of ion-driven multi-scale magnetic turbulence in merger shocks are in agreement with the ion structures observed in PIC simulations.","In typical shocks with the sonic Mach number $M_s=3$, the magnetic structures and shock front density ripples grow and saturate at wavelengths reaching approximately four ion Larmor radii.","Finally, we note that the steady-state structure of $M_s=3$ shocks in high-beta plasmas shows evidence that there is little difference between 2D and 3D simulations.","The turbulence near the shock front seems to be a 2D-like structure in 3D simulations."],"url":"http://arxiv.org/abs/2402.00571v1","category":"astro-ph.HE"}
{"created":"2024-02-01 12:55:24","title":"Stochastic convergence in per capita CO$_2$ emissions. An approach from nonlinear stationarity analysis","abstract":"This paper studies stochastic convergence of per capita CO$_2$ emissions in 28 OECD countries for the 1901-2009 period. The analysis is carried out at two aggregation levels, first for the whole set of countries (joint analysis) and then separately for developed and developing states (group analysis). A powerful time series methodology, adapted to a nonlinear framework that allows for quadratic trends with possibly smooth transitions between regimes, is applied. This approach provides more robust conclusions in convergence path analysis, enabling (a) robust detection of the presence, and if so, the number of changes in the level and/or slope of the trend of the series, (b) inferences on stationarity of relative per capita CO$_2$ emissions, conditionally on the presence of breaks and smooth transitions between regimes, and (c) estimation of change locations in the convergence paths. Finally, as stochastic convergence is attained when both stationarity around a trend and $\\beta$-convergence hold, the linear approach proposed by Tomljanovich and Vogelsang (2002) is extended in order to allow for more general quadratic models. Overall, joint analysis finds some evidence of stochastic convergence in per capita CO$_2$ emissions. Some dispersion in terms of $\\beta$-convergence is detected by group analysis, particularly among developed countries. This is in accordance with per capita GDP not being the sole determinant of convergence in emissions, with factors like search for more efficient technologies, fossil fuel substitution, innovation, and possibly outsources of industries, also having a crucial role.","sentences":["This paper studies stochastic convergence of per capita CO$_2$ emissions in 28 OECD countries for the 1901-2009 period.","The analysis is carried out at two aggregation levels, first for the whole set of countries (joint analysis) and then separately for developed and developing states (group analysis).","A powerful time series methodology, adapted to a nonlinear framework that allows for quadratic trends with possibly smooth transitions between regimes, is applied.","This approach provides more robust conclusions in convergence path analysis, enabling (a) robust detection of the presence, and if so, the number of changes in the level and/or slope of the trend of the series, (b) inferences on stationarity of relative per capita CO$_2$ emissions, conditionally on the presence of breaks and smooth transitions between regimes, and (c) estimation of change locations in the convergence paths.","Finally, as stochastic convergence is attained when both stationarity around a trend and $\\beta$-convergence hold, the linear approach proposed by Tomljanovich and Vogelsang (2002) is extended in order to allow for more general quadratic models.","Overall, joint analysis finds some evidence of stochastic convergence in per capita CO$_2$ emissions.","Some dispersion in terms of $\\beta$-convergence is detected by group analysis, particularly among developed countries.","This is in accordance with per capita GDP not being the sole determinant of convergence in emissions, with factors like search for more efficient technologies, fossil fuel substitution, innovation, and possibly outsources of industries, also having a crucial role."],"url":"http://arxiv.org/abs/2402.00567v1","category":"econ.EM"}
{"created":"2024-02-01 12:53:22","title":"A Review of Carsickness Mitigation: Navigating Challenges and Exploiting Opportunities in the Era of Intelligent Vehicles","abstract":"Motion sickness (MS) has long been a common complaint in road transportation. However, in the era of driving automation, MS has become an increasingly significant issue. The future intelligent vehicle is envisioned as a mobile space for work or entertainment, but unfortunately passengers' engagement in non-driving tasks may exacerbate MS. Finding effective MS countermeasures is crucial to ensure a pleasant passenger experience. Nevertheless, due to the complex mechanism of MS, there are numerous challenges in mitigating it, hindering the development of practical countermeasures. To address this, we first review two prevalent theories explaining the mechanism of MS. Subsequently, this paper provides a summary of current subjective and objective approaches for quantifying motion sickness levels. Then, it surveys existing methods for alleviating MS, including passenger adjustment, intelligent vehicle solutions, and motion cues of various modalities. Furthermore, we outline the limitations and remaining challenges of current research and highlight novel opportunities in the context of intelligent vehicles. Finally, we propose an integrated framework for alleviating MS. The findings of this review will enhance our understanding of carsickness and offer valuable insights for future research and practice in MS mitigation within modern vehicles.","sentences":["Motion sickness (MS) has long been a common complaint in road transportation.","However, in the era of driving automation, MS has become an increasingly significant issue.","The future intelligent vehicle is envisioned as a mobile space for work or entertainment, but unfortunately passengers' engagement in non-driving tasks may exacerbate MS.","Finding effective MS countermeasures is crucial to ensure a pleasant passenger experience.","Nevertheless, due to the complex mechanism of MS, there are numerous challenges in mitigating it, hindering the development of practical countermeasures.","To address this, we first review two prevalent theories explaining the mechanism of MS.","Subsequently, this paper provides a summary of current subjective and objective approaches for quantifying motion sickness levels.","Then, it surveys existing methods for alleviating MS, including passenger adjustment, intelligent vehicle solutions, and motion cues of various modalities.","Furthermore, we outline the limitations and remaining challenges of current research and highlight novel opportunities in the context of intelligent vehicles.","Finally, we propose an integrated framework for alleviating MS.","The findings of this review will enhance our understanding of carsickness and offer valuable insights for future research and practice in MS mitigation within modern vehicles."],"url":"http://arxiv.org/abs/2402.00565v1","category":"eess.SY"}
{"created":"2024-02-01 12:49:17","title":"Endomorphisms of Linear Block Codes","abstract":"The automorphism groups of various linear codes are well-studied yielding valuable insights into the respective code structure. This knowledge is successfully applied in, e.g., theoretical analysis and in improving decoding performance motivating the analyses of endomorphisms of linear codes. In this work, we discuss the structure of the set of transformation matrices of code endomorphisms, defined as a generalization of code automorphisms, and provide an explicit construction of a bijective mapping between the image of an endomorphism and its canonical quotient space. Furthermore, we introduce a one-to-one mapping between the set of transformation matrices of endomorphisms and a larger linear block code enabling the use of well-known algorithms for the search for suitable endomorphisms. Additionally, we propose an approach to obtain unknown code endomorphisms based on automorphisms of the code. Furthermore, we consider ensemble decoding as a possible use case for endomorphisms by introducing endomorphism ensemble decoding. Interestingly, EED can improve decoding performance when other ensemble decoding schemes are not applicable.","sentences":["The automorphism groups of various linear codes are well-studied yielding valuable insights into the respective code structure.","This knowledge is successfully applied in, e.g., theoretical analysis and in improving decoding performance motivating the analyses of endomorphisms of linear codes.","In this work, we discuss the structure of the set of transformation matrices of code endomorphisms, defined as a generalization of code automorphisms, and provide an explicit construction of a bijective mapping between the image of an endomorphism and its canonical quotient space.","Furthermore, we introduce a one-to-one mapping between the set of transformation matrices of endomorphisms and a larger linear block code enabling the use of well-known algorithms for the search for suitable endomorphisms.","Additionally, we propose an approach to obtain unknown code endomorphisms based on automorphisms of the code.","Furthermore, we consider ensemble decoding as a possible use case for endomorphisms by introducing endomorphism ensemble decoding.","Interestingly, EED can improve decoding performance when other ensemble decoding schemes are not applicable."],"url":"http://arxiv.org/abs/2402.00562v1","category":"cs.IT"}
{"created":"2024-02-01 12:43:02","title":"Time Series based Ensemble Model Output Statistics for Temperature Forecasts Postprocessing","abstract":"Nowadays, weather prediction is based on numerical weather prediction (NWP) models to produce an ensemble of forecasts. Despite of large improvements over the last few decades, they still tend to exhibit systematic bias and dispersion errors. Consequently, these forecasts may be improved by statistical postprocessing. This work proposes an extension of the ensemble model output statistics (EMOS) method in a time series framework. Besides of taking account of seasonality and trend in the location and scale parameter of the predictive distribution, the autoregressive process in the mean forecast errors or the standardized forecast errors is considered. The models can be further extended by allowing generalized autoregressive conditional heteroscedasticity (GARCH). Last but not least, it is outlined how to use these models for arbitrary forecast horizons. To illustrate the performance of the suggested EMOS models in time series fashion, we present a case study for the postprocessing of 2 m surface temperature forecasts using five different lead times and a set of observation stations in Germany. The results indicate that the time series EMOS extensions are able to significantly outperform the benchmark EMOS and autoregressive adjusted EMOS (AR-EMOS) in most of the lead time-station cases. To complement this article, our method is accompanied by an R-package called tsEMOS.","sentences":["Nowadays, weather prediction is based on numerical weather prediction (NWP) models to produce an ensemble of forecasts.","Despite of large improvements over the last few decades, they still tend to exhibit systematic bias and dispersion errors.","Consequently, these forecasts may be improved by statistical postprocessing.","This work proposes an extension of the ensemble model output statistics (EMOS) method in a time series framework.","Besides of taking account of seasonality and trend in the location and scale parameter of the predictive distribution, the autoregressive process in the mean forecast errors or the standardized forecast errors is considered.","The models can be further extended by allowing generalized autoregressive conditional heteroscedasticity (GARCH).","Last but not least, it is outlined how to use these models for arbitrary forecast horizons.","To illustrate the performance of the suggested EMOS models in time series fashion, we present a case study for the postprocessing of 2 m surface temperature forecasts using five different lead times and a set of observation stations in Germany.","The results indicate that the time series EMOS extensions are able to significantly outperform the benchmark EMOS and autoregressive adjusted EMOS (AR-EMOS) in most of the lead time-station cases.","To complement this article, our method is accompanied by an R-package called tsEMOS."],"url":"http://arxiv.org/abs/2402.00555v1","category":"stat.AP"}
{"created":"2024-02-01 12:37:37","title":"Return level estimations for extreme rainfall over the Iberian Peninsula: comparing methodologies","abstract":"Different ways to estimate future return levels for extreme rainfall are described and applied to the Iberian Peninsula (IP), based on Extreme Value Theory (EVT). This study is made for an ensemble of high quality rainfall time series observed in the Iberian Peninsula over the period 1961-2010. Both, peaks-over-threshold (POT) approach and block maxima with the Generalized Extreme Value (GEV) distribution will be used and their results compared when linear trends are assumed in the parameters: threshold and scale parameter for POT and location and scale parameter for GEV. Both all-days and rainy-days-only data sets were considered, because rainfall over the IP is a special variable in that a large number of the values are 0. Another methodology is then tested, for rainy days only, considering the role of how the mean, variance, and number of rainy days evolve. The 20-year return levels (RLs) expected in 2020 were estimated using these methodologies for three seasons: fall, spring and winter. GEV is less reliable than POT because fixed blocks lead to the selection of non-extreme values. Future RLs obtained with POT are higher than those estimated with GEV, mainly for some gauges showing significant positive trend for the number of rainy days. Fall becomes the season with heaviest rainfall, rather than winter nowadays, for some regions.","sentences":["Different ways to estimate future return levels for extreme rainfall are described and applied to the Iberian Peninsula (IP), based on Extreme Value Theory (EVT).","This study is made for an ensemble of high quality rainfall time series observed in the Iberian Peninsula over the period 1961-2010.","Both, peaks-over-threshold (POT) approach and block maxima with the Generalized Extreme Value (GEV) distribution will be used and their results compared when linear trends are assumed in the parameters: threshold and scale parameter for POT and location and scale parameter for GEV.","Both all-days and rainy-days-only data sets were considered, because rainfall over the IP is a special variable in that a large number of the values are 0.","Another methodology is then tested, for rainy days only, considering the role of how the mean, variance, and number of rainy days evolve.","The 20-year return levels (RLs) expected in 2020 were estimated using these methodologies for three seasons: fall, spring and winter.","GEV is less reliable than POT because fixed blocks lead to the selection of non-extreme values.","Future RLs obtained with POT are higher than those estimated with GEV, mainly for some gauges showing significant positive trend for the number of rainy days.","Fall becomes the season with heaviest rainfall, rather than winter nowadays, for some regions."],"url":"http://arxiv.org/abs/2402.00551v1","category":"physics.ao-ph"}
{"created":"2024-02-01 12:18:57","title":"Distributive properties of division points and discriminants of Drinfeld modules","abstract":"We present a new notion of distribution and derived distribution of rank $r \\in \\mathbb{N}$ for a global function field $K$ with a distinguished place $\\infty$. It allows to describe the relations between division points, isogenies, and discriminants both for a fixed Drinfeld module of rank $r$ for the above data, or for the corresponding modular forms.   We introduce and study three basic distributions with values in $\\mathbb{Q}$, in the group $\\mu(\\overline{K})$ of roots of unity in the algebraic closure $\\overline{K}$ of $K$, and in the group $U^{(1)}(C_{\\infty})$ of $1$-units of the completed algebraic closure $C_{\\infty}$ of $K_{\\infty}$, respectively.   There result product formulas for division points and discriminants that encompass known results (e.g. analogues of Wallis' formula for $(2\\pi i)^{2}$ in the rank-$1$ case, of Jacobi's formula $\\Delta = (2\\pi i)^{12} q \\prod (1-q^{n})^{24}$ in the rank-$2$ case, and similar boundary expansions for $r > 2$) and several new ones: the definition of a canonical discriminant for the most general case of Drinfeld modules and the description of the sizes of division and discriminant forms.   In the now classical case where $(K, \\infty) = (\\mathbb{F}_{q}(T), \\infty)$ and $r = 1$, $2$ or $3$, we give explicit values for the logarithms of such forms.","sentences":["We present a new notion of distribution and derived distribution of rank $r \\in \\mathbb{N}$ for a global function field $K$ with a distinguished place $\\infty$. It allows to describe the relations between division points, isogenies, and discriminants both for a fixed Drinfeld module of rank $r$ for the above data, or for the corresponding modular forms.   ","We introduce and study three basic distributions with values in $\\mathbb{Q}$, in the group $\\mu(\\overline{K})$ of roots of unity in the algebraic closure $\\overline{K}$ of $K$, and in the group $U^{(1)}(C_{\\infty})$ of $1$-units of the completed algebraic closure $C_{\\infty}$ of $K_{\\infty}$, respectively.   ","There result product formulas for division points and discriminants that encompass known results (e.g. analogues of Wallis' formula for $(2\\pi i)^{2}$ in the rank-$1$ case, of Jacobi's formula $\\Delta = (2\\pi i)^{12} q \\prod (1-q^{n})^{24}$ in the rank-$2$ case, and similar boundary expansions for $r > 2$) and several new ones: the definition of a canonical discriminant for the most general case of Drinfeld modules and the description of the sizes of division and discriminant forms.   ","In the now classical case where $(K, \\infty) = (\\mathbb{F}_{q}(T), \\infty)$ and $r = 1$, $2$ or $3$, we give explicit values for the logarithms of such forms."],"url":"http://arxiv.org/abs/2402.00545v1","category":"math.NT"}
{"created":"2024-02-01 12:11:00","title":"Hardness of Random Reordered Encodings of Parity for Resolution and CDCL","abstract":"Parity reasoning is challenging for Conflict-Driven Clause Learning (CDCL) SAT solvers. This has been observed even for simple formulas encoding two contradictory parity constraints with different variable orders (Chew and Heule 2020). We provide an analytical explanation for their hardness by showing that they require exponential resolution refutations with high probability when the variable order is chosen at random. We obtain this result by proving that these formulas, which are known to be Tseitin formulas, have Tseitin graphs of linear treewidth with high probability. Since such Tseitin formulas require exponential resolution proofs, our result follows. We generalize this argument to a new class of formulas that capture a basic form of parity reasoning involving a sum of two random parity constraints with random orders. Even when the variable order for the sum is chosen favorably, these formulas remain hard for resolution. In contrast, we prove that they have short DRAT refutations. We show experimentally that the running time of CDCL SAT solvers on both classes of formulas grows exponentially with their treewidth.","sentences":["Parity reasoning is challenging for Conflict-Driven Clause Learning (CDCL) SAT solvers.","This has been observed even for simple formulas encoding two contradictory parity constraints with different variable orders (Chew and Heule 2020).","We provide an analytical explanation for their hardness by showing that they require exponential resolution refutations with high probability when the variable order is chosen at random.","We obtain this result by proving that these formulas, which are known to be Tseitin formulas, have Tseitin graphs of linear treewidth with high probability.","Since such Tseitin formulas require exponential resolution proofs, our result follows.","We generalize this argument to a new class of formulas that capture a basic form of parity reasoning involving a sum of two random parity constraints with random orders.","Even when the variable order for the sum is chosen favorably, these formulas remain hard for resolution.","In contrast, we prove that they have short DRAT refutations.","We show experimentally that the running time of CDCL SAT solvers on both classes of formulas grows exponentially with their treewidth."],"url":"http://arxiv.org/abs/2402.00542v1","category":"cs.CC"}
{"created":"2024-02-01 12:06:55","title":"Masked Conditional Diffusion Model for Enhancing Deepfake Detection","abstract":"Recent studies on deepfake detection have achieved promising results when training and testing faces are from the same dataset. However, their results severely degrade when confronted with forged samples that the model has not yet seen during training. In this paper, deepfake data to help detect deepfakes. this paper present we put a new insight into diffusion model-based data augmentation, and propose a Masked Conditional Diffusion Model (MCDM) for enhancing deepfake detection. It generates a variety of forged faces from a masked pristine one, encouraging the deepfake detection model to learn generic and robust representations without overfitting to special artifacts. Extensive experiments demonstrate that forgery images generated with our method are of high quality and helpful to improve the performance of deepfake detection models.","sentences":["Recent studies on deepfake detection have achieved promising results when training and testing faces are from the same dataset.","However, their results severely degrade when confronted with forged samples that the model has not yet seen during training.","In this paper, deepfake data to help detect deepfakes.","this paper present we put a new insight into diffusion model-based data augmentation, and propose a Masked Conditional Diffusion Model (MCDM) for enhancing deepfake detection.","It generates a variety of forged faces from a masked pristine one, encouraging the deepfake detection model to learn generic and robust representations without overfitting to special artifacts.","Extensive experiments demonstrate that forgery images generated with our method are of high quality and helpful to improve the performance of deepfake detection models."],"url":"http://arxiv.org/abs/2402.00541v1","category":"cs.CV"}
{"created":"2024-02-01 12:06:41","title":"Experimental Evaluation of Interactive Edge/Cloud Virtual Reality Gaming over Wi-Fi using Unity Render Streaming","abstract":"Virtual Reality (VR) streaming enables end-users to seamlessly immerse themselves in interactive virtual environments using even low-end devices. However, the quality of the VR experience heavily relies on Wi-Fi performance, since it serves as the last hop in the network chain. Our study delves into the intricate interplay between Wi-Fi and VR traffic, drawing upon empirical data and leveraging a simulator tailored to VR traffic patterns. In this work we further evaluate Wi-Fi's suitability for VR streaming in terms of the quality of service it provides. In particular, we employ Unity Render Streaming to remotely stream real-time VR gaming content over Wi-Fi 6 using WebRTC, leveraging a server physically located at the network's edge, near the end user. Our findings demonstrate the system's sustained network performance, showcasing minimal round-trip time and jitter at 60 and 90 fps. In addition, we uncover the characteristics and patterns of the generated traffic streams, unveiling a surprising video transmission approach inherent to WebRTC-based services. This approach involves the fragmentation of video frames into discrete batches of packets, transmitted at regular intervals regardless of the targeted frame rate.This segmentation mechanism maintains consistent video packet delays across video frame rates but leads to increased Wi-Fi airtime consumption at higher frame rates. The presented results demonstrate that shortening the interval between batches is advantageous as it improves Wi-Fi efficiency and reduces delays in delivering complete frames.","sentences":["Virtual Reality (VR) streaming enables end-users to seamlessly immerse themselves in interactive virtual environments using even low-end devices.","However, the quality of the VR experience heavily relies on Wi-Fi performance, since it serves as the last hop in the network chain.","Our study delves into the intricate interplay between Wi-Fi and VR traffic, drawing upon empirical data and leveraging a simulator tailored to VR traffic patterns.","In this work we further evaluate Wi-Fi's suitability for VR streaming in terms of the quality of service it provides.","In particular, we employ Unity Render Streaming to remotely stream real-time VR gaming content over Wi-Fi 6 using WebRTC, leveraging a server physically located at the network's edge, near the end user.","Our findings demonstrate the system's sustained network performance, showcasing minimal round-trip time and jitter at 60 and 90 fps.","In addition, we uncover the characteristics and patterns of the generated traffic streams, unveiling a surprising video transmission approach inherent to WebRTC-based services.","This approach involves the fragmentation of video frames into discrete batches of packets, transmitted at regular intervals regardless of the targeted frame rate.","This segmentation mechanism maintains consistent video packet delays across video frame rates but leads to increased Wi-Fi airtime consumption at higher frame rates.","The presented results demonstrate that shortening the interval between batches is advantageous as it improves Wi-Fi efficiency and reduces delays in delivering complete frames."],"url":"http://arxiv.org/abs/2402.00540v1","category":"cs.NI"}
{"created":"2024-02-01 12:05:53","title":"Robust Path Planning via Learning from Demonstrations for Robotic Catheters in Deformable Environments","abstract":"Navigation through tortuous and deformable vessels using catheters with limited steering capability underscores the need for reliable path planning. State-of-the-art path planners do not fully account for the deformable nature of the environment. This work proposes a robust path planner via a learning from demonstrations method, named Curriculum Generative Adversarial Imitation Learning (C-GAIL). This path planning framework takes into account the interaction between steerable catheters and vessel walls and the deformable property of vessels. In-silico comparative experiments show that the proposed network achieves smaller targeting errors, and a higher success rate, compared to a state-of-the-art approach based on GAIL. The in-vitro validation experiments demonstrate that the path generated by the proposed C-GAIL path planner aligns better with the actual steering capability of the pneumatic artificial muscle-driven catheter utilized in this study. Therefore, the proposed approach can provide enhanced support to the user in navigating the catheter towards the target with greater precision, in contrast to the conventional centerline-following technique. The targeting and tracking errors are 1.26$\\pm$0.55mm and 5.18$\\pm$3.48mm, respectively. The proposed path planning framework exhibits superior performance in managing uncertainty associated with vessel deformation, thereby resulting in lower tracking errors.","sentences":["Navigation through tortuous and deformable vessels using catheters with limited steering capability underscores the need for reliable path planning.","State-of-the-art path planners do not fully account for the deformable nature of the environment.","This work proposes a robust path planner via a learning from demonstrations method, named Curriculum Generative Adversarial Imitation Learning (C-GAIL).","This path planning framework takes into account the interaction between steerable catheters and vessel walls and the deformable property of vessels.","In-silico comparative experiments show that the proposed network achieves smaller targeting errors, and a higher success rate, compared to a state-of-the-art approach based on GAIL.","The in-vitro validation experiments demonstrate that the path generated by the proposed C-GAIL path planner aligns better with the actual steering capability of the pneumatic artificial muscle-driven catheter utilized in this study.","Therefore, the proposed approach can provide enhanced support to the user in navigating the catheter towards the target with greater precision, in contrast to the conventional centerline-following technique.","The targeting and tracking errors are 1.26$\\pm$0.55mm and 5.18$\\pm$3.48mm, respectively.","The proposed path planning framework exhibits superior performance in managing uncertainty associated with vessel deformation, thereby resulting in lower tracking errors."],"url":"http://arxiv.org/abs/2402.00537v1","category":"cs.RO"}
{"created":"2024-02-01 12:02:15","title":"A Low-Cost Multi-Band Waveform Security Framework in Resource-Constrained Communications","abstract":"Traditional physical layer secure beamforming is achieved via precoding before signal transmission using channel state information (CSI). However, imperfect CSI will compromise the performance with imperfect beamforming and potential information leakage. In addition, multiple RF chains and antennas are needed to support the narrow beam generation, which complicates hardware implementation and is not suitable for resource-constrained Internet-of-Things (IoT) devices. Moreover, with the advancement of hardware and artificial intelligence (AI), low-cost and intelligent eavesdropping to wireless communications is becoming increasingly detrimental. In this paper, we propose a multi-carrier based multi-band waveform-defined security (WDS) framework, independent from CSI and RF chains, to defend against AI eavesdropping. Ideally, the continuous variations of sub-band structures lead to an infinite number of spectral features, which can potentially prevent brute-force eavesdropping. Sub-band spectral pattern information is efficiently constructed at legitimate users via a proposed chaotic sequence generator. A novel security metric, termed signal classification accuracy (SCA), is used to evaluate the security robustness under AI eavesdropping. Communication error probability and complexity are also investigated to show the reliability and practical capability of the proposed framework. Finally, compared to traditional secure beamforming techniques, the proposed multi-band WDS framework reduces power consumption by up to six times.","sentences":["Traditional physical layer secure beamforming is achieved via precoding before signal transmission using channel state information (CSI).","However, imperfect CSI will compromise the performance with imperfect beamforming and potential information leakage.","In addition, multiple RF chains and antennas are needed to support the narrow beam generation, which complicates hardware implementation and is not suitable for resource-constrained Internet-of-Things (IoT) devices.","Moreover, with the advancement of hardware and artificial intelligence (AI), low-cost and intelligent eavesdropping to wireless communications is becoming increasingly detrimental.","In this paper, we propose a multi-carrier based multi-band waveform-defined security (WDS) framework, independent from CSI and RF chains, to defend against AI eavesdropping.","Ideally, the continuous variations of sub-band structures lead to an infinite number of spectral features, which can potentially prevent brute-force eavesdropping.","Sub-band spectral pattern information is efficiently constructed at legitimate users via a proposed chaotic sequence generator.","A novel security metric, termed signal classification accuracy (SCA), is used to evaluate the security robustness under AI eavesdropping.","Communication error probability and complexity are also investigated to show the reliability and practical capability of the proposed framework.","Finally, compared to traditional secure beamforming techniques, the proposed multi-band WDS framework reduces power consumption by up to six times."],"url":"http://arxiv.org/abs/2402.00535v1","category":"eess.SP"}
{"created":"2024-02-01 12:01:43","title":"A Manifold Representation of the Key in Vision Transformers","abstract":"Vision Transformers implement multi-head self-attention (MSA) via stacking multiple attention blocks. The query, key, and value are often intertwined and generated within those blocks via a single, shared linear transformation. This paper explores the concept of disentangling the key from the query and value, and adopting a manifold representation for the key. Our experiments reveal that decoupling and endowing the key with a manifold structure can enhance the model performance. Specifically, ViT-B exhibits a 0.87% increase in top-1 accuracy, while Swin-T sees a boost of 0.52% in top-1 accuracy on the ImageNet-1K dataset, with eight charts in the manifold key. Our approach also yields positive results in object detection and instance segmentation tasks on the COCO dataset. Through detailed ablation studies, we establish that these performance gains are not merely due to the simplicity of adding more parameters and computations. Future research may investigate strategies for cutting the budget of such representations and aim for further performance improvements based on our findings.","sentences":["Vision Transformers implement multi-head self-attention (MSA) via stacking multiple attention blocks.","The query, key, and value are often intertwined and generated within those blocks via a single, shared linear transformation.","This paper explores the concept of disentangling the key from the query and value, and adopting a manifold representation for the key.","Our experiments reveal that decoupling and endowing the key with a manifold structure can enhance the model performance.","Specifically, ViT-B exhibits a 0.87% increase in top-1 accuracy, while Swin-T sees a boost of 0.52% in top-1 accuracy on the ImageNet-1K dataset, with eight charts in the manifold key.","Our approach also yields positive results in object detection and instance segmentation tasks on the COCO dataset.","Through detailed ablation studies, we establish that these performance gains are not merely due to the simplicity of adding more parameters and computations.","Future research may investigate strategies for cutting the budget of such representations and aim for further performance improvements based on our findings."],"url":"http://arxiv.org/abs/2402.00534v1","category":"cs.CV"}
{"created":"2024-02-01 11:54:36","title":"Unitary parts of Toeplitz operators with operator-valued symbols","abstract":"Motivated by the canonical decomposition of contractions on Hilbert spaces, we investigate when contractive Toeplitz operators on vector-valued Hardy spaces on the unit disc admit a non-zero reducing subspace on which its restriction is unitary. We show that for a Hilbert space $\\mathcal{E}$ and operator-valued symbol $\\Phi \\in L_{\\mathcal{B}(\\mathcal{E})}^{\\infty}(\\mathbb{T})$, the Toeplitz operator $T_{\\Phi}$ on $H_{\\mathcal{E}}^2(\\mathbb{D})$ has such a unitary subspace if and only if there exists a Hilbert space $\\mathcal{F}$, an inner function $\\Theta(z) \\in H_{\\mathcal{B}(\\mathcal{F}, \\mathcal{E})}^{\\infty}(\\mathbb{D})$, and a unitary $U:\\mathcal{F} \\rightarrow \\mathcal{F}$ such that \\[ \\Phi(e^{it}) \\Theta(e^{it}) = \\Theta(e^{it}) U \\quad \\text{and} \\quad \\Phi(e^{it})^* \\Theta(e^{it}) = \\Theta(e^{it}) U^* \\quad (\\text{ a.e. on }\\mathbb{T}). \\] This result can be seen as a generalization of the corresponding result for Toeplitz operators on $H^2(\\mathbb{D})$ by Goor in [13]. We provide finer characterizations for analytic Toeplitz operators by finding the correspondence between the unitary parts of $T_{\\Phi}$ on $H_{\\mathcal{E}}^2(\\mathbb{D})$ and $\\Phi(0)$ on $\\mathcal{E}$.","sentences":["Motivated by the canonical decomposition of contractions on Hilbert spaces, we investigate when contractive Toeplitz operators on vector-valued Hardy spaces on the unit disc admit a non-zero reducing subspace on which its restriction is unitary.","We show that for a Hilbert space $\\mathcal{E}$ and operator-valued symbol $\\Phi \\in L_{\\mathcal{B}(\\mathcal{E})}^{\\infty}(\\mathbb{T})$, the Toeplitz operator $T_{\\Phi}$ on $H_{\\mathcal{E}}^2(\\mathbb{D})$ has such a unitary subspace if and only if there exists a Hilbert space $\\mathcal{F}$, an inner function $\\Theta(z) \\in H_{\\mathcal{B}(\\mathcal{F}, \\mathcal{E})}^{\\infty}(\\mathbb{D})$, and a unitary $U:\\mathcal{F} \\rightarrow \\mathcal{F}$ such that \\[ \\Phi(e^{it}) \\Theta(e^{it}) = \\Theta(e^{it})","U \\quad \\text{and} \\quad \\Phi(e^{it})^* \\Theta(e^{it})","= \\Theta(e^{it}) U^* \\quad (\\text{ a.e. on }\\mathbb{T}).","\\]","This result can be seen as a generalization of the corresponding result for Toeplitz operators on $H^2(\\mathbb{D})$ by Goor in [13].","We provide finer characterizations for analytic Toeplitz operators by finding the correspondence between the unitary parts of $T_{\\Phi}$ on $H_{\\mathcal{E}}^2(\\mathbb{D})$ and $\\Phi(0)$ on $\\mathcal{E}$."],"url":"http://arxiv.org/abs/2402.00529v1","category":"math.FA"}
{"created":"2024-02-01 11:39:19","title":"Towards Summarizing Code Snippets Using Pre-Trained Transformers","abstract":"When comprehending code, a helping hand may come from the natural language comments documenting it that, unfortunately, are not always there. To support developers in such a scenario, several techniques have been presented to automatically generate natural language summaries for a given code. Most recent approaches exploit deep learning (DL) to automatically document classes or functions, while little effort has been devoted to more fine-grained documentation (e.g., documenting code snippets or even a single statement). Such a design choice is dictated by the availability of training data: For example, in the case of Java, it is easy to create datasets composed of pairs <Method, Javadoc> that can be fed to DL models to teach them how to summarize a method. Such a comment-to-code linking is instead non-trivial when it comes to inner comments documenting a few statements. In this work, we take all the steps needed to train a DL model to document code snippets. First, we manually built a dataset featuring 6.6k comments that have been (i) classified based on their type (e.g., code summary, TODO), and (ii) linked to the code statements they document. Second, we used such a dataset to train a multi-task DL model, taking as input a comment and being able to (i) classify whether it represents a \"code summary\" or not and (ii) link it to the code statements it documents. Our model identifies code summaries with 84% accuracy and is able to link them to the documented lines of code with recall and precision higher than 80%. Third, we run this model on 10k projects, identifying and linking code summaries to the documented code. This unlocked the possibility of building a large-scale dataset of documented code snippets that have then been used to train a new DL model able to document code snippets. A comparison with state-of-the-art baselines shows the superiority of the proposed approach.","sentences":["When comprehending code, a helping hand may come from the natural language comments documenting it that, unfortunately, are not always there.","To support developers in such a scenario, several techniques have been presented to automatically generate natural language summaries for a given code.","Most recent approaches exploit deep learning (DL) to automatically document classes or functions, while little effort has been devoted to more fine-grained documentation (e.g., documenting code snippets or even a single statement).","Such a design choice is dictated by the availability of training data: For example, in the case of Java, it is easy to create datasets composed of pairs <Method, Javadoc> that can be fed to DL models to teach them how to summarize a method.","Such a comment-to-code linking is instead non-trivial when it comes to inner comments documenting a few statements.","In this work, we take all the steps needed to train a DL model to document code snippets.","First, we manually built a dataset featuring 6.6k comments that have been (i) classified based on their type (e.g., code summary, TODO), and (ii) linked to the code statements they document.","Second, we used such a dataset to train a multi-task DL model, taking as input a comment and being able to (i) classify whether it represents a \"code summary\" or not and (ii) link it to the code statements it documents.","Our model identifies code summaries with 84% accuracy and is able to link them to the documented lines of code with recall and precision higher than 80%.","Third, we run this model on 10k projects, identifying and linking code summaries to the documented code.","This unlocked the possibility of building a large-scale dataset of documented code snippets that have then been used to train a new DL model able to document code snippets.","A comparison with state-of-the-art baselines shows the superiority of the proposed approach."],"url":"http://arxiv.org/abs/2402.00519v1","category":"cs.SE"}
{"created":"2024-02-01 11:39:04","title":"EE-Tuning: An Economical yet Scalable Solution for Tuning Early-Exit Large Language Models","abstract":"This work introduces EE-Tuning, a lightweight and economical solution to training/tuning early-exit large language models (LLMs). In contrast to the common approach of full-parameter pre-training, EE-Tuning augments any pre-trained (and possibly fine-tuned) standard LLM with additional early-exit layers that are tuned in a parameter-efficient manner, which requires significantly less computational resources and training data. Our implementation of EE-Tuning achieves outstanding training efficiency via extensive performance optimizations, as well as scalability due to its full compatibility with 3D parallelism. Results of systematic experiments validate the efficacy of EE-Tuning, confirming that effective early-exit LLM inference can be achieved with a limited training budget. In hope of making early-exit LLMs accessible to the community, we release the source code of our implementation of EE-Tuning at https://github.com/pan-x-c/EE-LLM.","sentences":["This work introduces EE-Tuning, a lightweight and economical solution to training/tuning early-exit large language models (LLMs).","In contrast to the common approach of full-parameter pre-training, EE-Tuning augments any pre-trained (and possibly fine-tuned) standard LLM with additional early-exit layers that are tuned in a parameter-efficient manner, which requires significantly less computational resources and training data.","Our implementation of EE-Tuning achieves outstanding training efficiency via extensive performance optimizations, as well as scalability due to its full compatibility with 3D parallelism.","Results of systematic experiments validate the efficacy of EE-Tuning, confirming that effective early-exit LLM inference can be achieved with a limited training budget.","In hope of making early-exit LLMs accessible to the community, we release the source code of our implementation of EE-Tuning at https://github.com/pan-x-c/EE-LLM."],"url":"http://arxiv.org/abs/2402.00518v1","category":"cs.LG"}
{"created":"2024-02-01 11:34:24","title":"Polarization-induced stress in the noisy voter model","abstract":"A new model for the dynamics of opinion formation is proposed and analysed at the mean-field level. It can be regarded as a generalization of the noisy voter model in which agents update their binary states by copying others and by an intrinsic mechanism affected by the degree of polarization in the system. It also takes into account whether the agents enhance or reduce their intrinsic mechanism upon increasing polarization. Four phases or shapes of the steady-state probability of a fraction of agents in a given state are found (unimodal, bimodal, W and M). In the unimodal (resp. bimodal) phase, the copying (resp. intrinsic) mechanism is globally dominant, while in the W (resp. M) phase the copying (resp. intrinsic) mechanism is the relevant one close to the consensus states while it reduces its influence as approaching coexistence. In the thermodynamic limit, the bimodal and W phases disappear, while the unimodal and M phases prevail. The theoretical results, obtained analytically from the master equation, and the numerical simulations are in good agreement.","sentences":["A new model for the dynamics of opinion formation is proposed and analysed at the mean-field level.","It can be regarded as a generalization of the noisy voter model in which agents update their binary states by copying others and by an intrinsic mechanism affected by the degree of polarization in the system.","It also takes into account whether the agents enhance or reduce their intrinsic mechanism upon increasing polarization.","Four phases or shapes of the steady-state probability of a fraction of agents in a given state are found (unimodal, bimodal, W and M).","In the unimodal (resp. bimodal) phase, the copying (resp. intrinsic) mechanism is globally dominant, while in the W (resp.","M) phase the copying (resp.","intrinsic) mechanism is the relevant one close to the consensus states while it reduces its influence as approaching coexistence.","In the thermodynamic limit, the bimodal and W phases disappear, while the unimodal and M phases prevail.","The theoretical results, obtained analytically from the master equation, and the numerical simulations are in good agreement."],"url":"http://arxiv.org/abs/2402.00516v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-01 11:30:07","title":"Changes in heat waves characteristics over Extremadura (SW Spain)","abstract":"Heat wave (HW) events are becoming more frequent, and they have important consequences because of the negative effects they can have not only on the human population in health terms, but also on biodiversity and agriculture. This motivated a study of the trends in HW events over Extremadura, a region in the southwest of Spain, with much of its area in summer devoted to the production of irrigated crops such as maize and tomatoes. Heat waves were defined for the study as two consecutive days with temperatures above the 95th percentile of the summer (June-August) maximum temperature (Tmax) time series. Two datasets were used: one consisted of 13 daily temperature records uniformly distributed over the Region, and the other was the SPAIN02 gridded observational dataset, extracting just the points corresponding to Extremadura. The trends studied were in the duration, intensity, and frequency of HW events, and in other parameters such as the mean, low (25th percentile), and high (75th percentile) values. In general terms, the results showed significant positive trends in those parameters over the east, the northwest, and a small area in the south of the Region. In order to study changes in HW characteristics (duration, frequency and intensity) considering different subperiods, a stochastic model was used to generate 1000 time series equivalent to the observed ones. The results showed that there were no significant changes in HW duration in the last 10-year subperiod in comparison with the first. But the results were different for warm events (WE), defined with a lower threshold (the 75th percentile), which are also important for agriculture. For several sites, there were significant changes in WE duration, frequency, and intensity.","sentences":["Heat wave (HW) events are becoming more frequent, and they have important consequences because of the negative effects they can have not only on the human population in health terms, but also on biodiversity and agriculture.","This motivated a study of the trends in HW events over Extremadura, a region in the southwest of Spain, with much of its area in summer devoted to the production of irrigated crops such as maize and tomatoes.","Heat waves were defined for the study as two consecutive days with temperatures above the 95th percentile of the summer (June-August) maximum temperature (Tmax) time series.","Two datasets were used: one consisted of 13 daily temperature records uniformly distributed over the Region, and the other was the SPAIN02 gridded observational dataset, extracting just the points corresponding to Extremadura.","The trends studied were in the duration, intensity, and frequency of HW events, and in other parameters such as the mean, low (25th percentile), and high (75th percentile) values.","In general terms, the results showed significant positive trends in those parameters over the east, the northwest, and a small area in the south of the Region.","In order to study changes in HW characteristics (duration, frequency and intensity) considering different subperiods, a stochastic model was used to generate 1000 time series equivalent to the observed ones.","The results showed that there were no significant changes in HW duration in the last 10-year subperiod in comparison with the first.","But the results were different for warm events (WE), defined with a lower threshold (the 75th percentile), which are also important for agriculture.","For several sites, there were significant changes in WE duration, frequency, and intensity."],"url":"http://arxiv.org/abs/2402.00514v1","category":"physics.ao-ph"}
{"created":"2024-02-01 11:28:08","title":"A unified approach to mass transference principle and large intersection property","abstract":"The mass transference principle, discovered by Beresnevich and Velani [Ann Math (2), 2006], is a landmark result in Diophantine approximation that allows us to obtain the Hausdorff measure theory of $\\limsup$ set. Another important tool is the notion of large intersection property, introduced and systematically studied by Falconer [J. Lond. Math. Soc. (2), 1994]. The former mainly focuses on passing between full (Lebesgue) measure and full Hausdorff measure statements, while the latter transfers full Hausdorff content statement to Hausdorff dimension. From this perspective, the proofs of the two results are quite similar but often treated in different ways.   In this paper, we establish a general mass transference principle from the viewpoint of Hausdorff content, aiming to provide a unified proof for the aforementioned results. More precisely, this principle allows us to transfer the Hausdorff content bounds of a sequence of open sets $E_n$ to the full Hausdorff measure statement and large intersection property for $\\limsup E_n$. One of the advantages of our approach is that the verification of the Hausdorff content bound does not require the construction of Cantor-like subset, resulting in a much simpler proof. As an application, we provide simpler proofs for several mass transference principles.","sentences":["The mass transference principle, discovered by Beresnevich and Velani [Ann Math (2), 2006], is a landmark result in Diophantine approximation that allows us to obtain the Hausdorff measure theory of $\\limsup$ set.","Another important tool is the notion of large intersection property, introduced and systematically studied by Falconer [J. Lond.","Math. Soc.","(2), 1994].","The former mainly focuses on passing between full (Lebesgue) measure and full Hausdorff measure statements, while the latter transfers full Hausdorff content statement to Hausdorff dimension.","From this perspective, the proofs of the two results are quite similar but often treated in different ways.   ","In this paper, we establish a general mass transference principle from the viewpoint of Hausdorff content, aiming to provide a unified proof for the aforementioned results.","More precisely, this principle allows us to transfer the Hausdorff content bounds of a sequence of open sets $E_n$ to the full Hausdorff measure statement and large intersection property for $\\limsup E_n$. One of the advantages of our approach is that the verification of the Hausdorff content bound does not require the construction of Cantor-like subset, resulting in a much simpler proof.","As an application, we provide simpler proofs for several mass transference principles."],"url":"http://arxiv.org/abs/2402.00513v1","category":"math.NT"}
{"created":"2024-02-01 11:24:21","title":"Deciphering Pluto's Haze: How a Solar-Powered Vapor-Pressure Plume Shapes Its Bimodal Particle Size Distribution","abstract":"Combining findings from New Horizons' suite of instruments reveals a bimodal haze particle distribution within Pluto's atmosphere, which haze models have not been able to reproduce. We employ the photochemical and microphysics KINAERO model to simulate seasonal cycles and their impact on the haze distribution. We find that the smaller spherical particle mode can be generated through photochemistry and coagulation, while the larger aggregate mode are formed by surface volatile deposits sublimating and subsequently lofting such particles upwards.","sentences":["Combining findings from New Horizons' suite of instruments reveals a bimodal haze particle distribution within Pluto's atmosphere, which haze models have not been able to reproduce.","We employ the photochemical and microphysics KINAERO model to simulate seasonal cycles and their impact on the haze distribution.","We find that the smaller spherical particle mode can be generated through photochemistry and coagulation, while the larger aggregate mode are formed by surface volatile deposits sublimating and subsequently lofting such particles upwards."],"url":"http://arxiv.org/abs/2402.00510v1","category":"astro-ph.EP"}
{"created":"2024-02-01 11:18:38","title":"Taking Music Seriously: on the Dynamics of 'Mathemusical' Research with a Focus on Hexachordal Theorems","abstract":"After presenting the general framework of 'mathemusical' dynamics, we focus on one music-theoretical problem concerning a special case of homometry theory applied to music composition, namely Milton Babbitt's hexachordal theorem. We briefly discuss some historical aspects of homometric structures and their ramifications in crystallography, spectral analysis and music composition via the construction of rhythmic canons tiling the integer line. We then present the probabilistic generalization of Babbitt's result we recently introduced in a paper entitled ''New hexachordal theorems in metric spaces with probability measure'' and illustrate the new approach with original constructions and examples.","sentences":["After presenting the general framework of 'mathemusical' dynamics, we focus on one music-theoretical problem concerning a special case of homometry theory applied to music composition, namely Milton Babbitt's hexachordal theorem.","We briefly discuss some historical aspects of homometric structures and their ramifications in crystallography, spectral analysis and music composition via the construction of rhythmic canons tiling the integer line.","We then present the probabilistic generalization of Babbitt's result we recently introduced in a paper entitled ''New hexachordal theorems in metric spaces with probability measure'' and illustrate the new approach with original constructions and examples."],"url":"http://arxiv.org/abs/2402.00507v1","category":"math.PR"}
{"created":"2024-02-01 11:13:01","title":"Axiomatizing NFAs Generated by Regular Grammars","abstract":"A subclass of nondeterministic Finite Automata generated by means of regular Grammars (GFAs, for short) is introduced. A process algebra is proposed, whose semantics maps a term to a GFA. We prove a representability theorem: for each GFA $N$, there exists a process algebraic term $p$ such that its semantics is a GFA isomorphic to $N$. Moreover, we provide a concise axiomatization of language equivalence: two GFAs $N_1$ and $N_2$ recognize the same regular language if and only if the associated terms $p_1$ and $p_2$, respectively, can be equated by means of a set of axioms, comprising 6 axioms plus 2 conditional axioms, only.","sentences":["A subclass of nondeterministic Finite Automata generated by means of regular Grammars (GFAs, for short) is introduced.","A process algebra is proposed, whose semantics maps a term to a GFA.","We prove a representability theorem: for each GFA $N$, there exists a process algebraic term $p$ such that its semantics is a GFA isomorphic to $N$. Moreover, we provide a concise axiomatization of language equivalence: two GFAs $N_1$ and $N_2$ recognize the same regular language if and only if the associated terms $p_1$ and $p_2$, respectively, can be equated by means of a set of axioms, comprising 6 axioms plus 2 conditional axioms, only."],"url":"http://arxiv.org/abs/2402.00502v1","category":"cs.FL"}
{"created":"2024-02-01 11:05:44","title":"Optimization of a Line Detection Algorithm for Autonomous Vehicles on a RISC-V with Accelerator","abstract":"In recent years, autonomous vehicles have attracted the attention of many research groups, both in academia and business, including researchers from leading companies such as Google, Uber and Tesla. This type of vehicles are equipped with systems that are subject to very strict requirements, essentially aimed at performing safe operations -- both for potential passengers and pedestrians -- as well as carrying out the processing needed for decision making in real time. In many instances, general-purpose processors alone cannot ensure that these safety, reliability and real-time requirements are met, so it is common to implement heterogeneous systems by including accelerators. This paper explores the acceleration of a line detection application in the autonomous car environment using a heterogeneous system consisting of a general-purpose RISC-V core and a domain-specific accelerator. In particular, the application is analyzed to identify the most computationally intensive parts of the code and it is adapted accordingly for more efficient processing. Furthermore, the code is executed on the aforementioned hardware platform to verify that the execution effectively meets the existing requirements in autonomous vehicles, experiencing a 3.7x speedup with respect to running without accelerator.","sentences":["In recent years, autonomous vehicles have attracted the attention of many research groups, both in academia and business, including researchers from leading companies such as Google, Uber and Tesla.","This type of vehicles are equipped with systems that are subject to very strict requirements, essentially aimed at performing safe operations -- both for potential passengers and pedestrians -- as well as carrying out the processing needed for decision making in real time.","In many instances, general-purpose processors alone cannot ensure that these safety, reliability and real-time requirements are met, so it is common to implement heterogeneous systems by including accelerators.","This paper explores the acceleration of a line detection application in the autonomous car environment using a heterogeneous system consisting of a general-purpose RISC-V core and a domain-specific accelerator.","In particular, the application is analyzed to identify the most computationally intensive parts of the code and it is adapted accordingly for more efficient processing.","Furthermore, the code is executed on the aforementioned hardware platform to verify that the execution effectively meets the existing requirements in autonomous vehicles, experiencing a 3.7x speedup with respect to running without accelerator."],"url":"http://arxiv.org/abs/2402.00496v1","category":"cs.AR"}
{"created":"2024-02-01 11:05:27","title":"Geometric Interpretations of Compatibility for Fundamental Matrices","abstract":"In recent work, algebraic computational software was used to provide the exact algebraic conditions under which a sixtuple $\\{F^{ij}\\}$ of fundamental matrices, corresponding to $4$ images, will be compatible, i.e. there will exist cameras $\\{P_i\\}_{i=1}^4$ such that each pair $P_i,P_j$ has fundamental matrix $F^{ij}$; it has been further demonstrated that quadruplewise compatibility is sufficient for the problem of $n>4$ images. We expand on these prior results by proving equivalent geometric conditions for compatibility. We find that when the camera centers are in general position, compatibility can be characterized via the intersections of epipolar lines in each image. When the camera centers are coplanar, compatibility occurs when the prior condition holds and additionally any one camera center can be reconstructed via the other three.","sentences":["In recent work, algebraic computational software was used to provide the exact algebraic conditions under which a sixtuple $\\{F^{ij}\\}$ of fundamental matrices, corresponding to $4$ images, will be compatible, i.e. there will exist cameras $\\{P_i\\}_{i=1}^4$ such that each pair $P_i,P_j$ has fundamental matrix $F^{ij}$; it has been further demonstrated that quadruplewise compatibility is sufficient for the problem of $n>4$ images.","We expand on these prior results by proving equivalent geometric conditions for compatibility.","We find that when the camera centers are in general position, compatibility can be characterized via the intersections of epipolar lines in each image.","When the camera centers are coplanar, compatibility occurs when the prior condition holds and additionally any one camera center can be reconstructed via the other three."],"url":"http://arxiv.org/abs/2402.00495v1","category":"math.AG"}
{"created":"2024-02-01 11:05:06","title":"Quantum Analytic Langlands Correspondence","abstract":"The analytic Langlands correspondence describes the solution to the spectral problem for the quantised Hitchin Hamiltonians. It is related to the S-duality of $\\cal{N}=4$ super Yang-Mills theory. We propose a one-parameter deformation of the Analytic Langlands Correspondence, and discuss its relations to quantum field theory. The partition functions of the $H_3^+$ WZNW model are interpreted as the wave-functions of a spherical vector in the quantisation of complex Chern-Simons theory. Verlinde line operators generate a representation of two copies of the quantised skein algebra on generalised partition functions. We conjecture that this action generates a basis for the underlying Hilbert space, and explain in which sense the resulting quantum theory represents a deformation of the Analytic Langlands Correspondence.","sentences":["The analytic Langlands correspondence describes the solution to the spectral problem for the quantised Hitchin Hamiltonians.","It is related to the S-duality of $\\cal{N}=4$ super Yang-Mills theory.","We propose a one-parameter deformation of the Analytic Langlands Correspondence, and discuss its relations to quantum field theory.","The partition functions of the $H_3^+$ WZNW model are interpreted as the wave-functions of a spherical vector in the quantisation of complex Chern-Simons theory.","Verlinde line operators generate a representation of two copies of the quantised skein algebra on generalised partition functions.","We conjecture that this action generates a basis for the underlying Hilbert space, and explain in which sense the resulting quantum theory represents a deformation of the Analytic Langlands Correspondence."],"url":"http://arxiv.org/abs/2402.00494v1","category":"hep-th"}
{"created":"2024-02-01 10:58:16","title":"Large scale zigzag pattern emerging from circulating active shakers","abstract":"We report the emergence of large zigzag bands in a population of reversibly actuated magnetic rotors that behave as active shakers, namely squirmers that shake the fluid around them without moving. The shakers collectively organize into dynamic structures displaying self-similar growth, and generate topological defects in form of cusps that connect vortices of rolling particles with alternating chirality. By combining experimental analysis with particle-based simulation, we show that the special flow field created by the shakers is the only ingredient needed to reproduce the observed spatiotemporal pattern. We unveil a self-organization scenario in a collection of driven particles in an viscoelastic medium emerging from the reduced particle degrees of freedom, as here the frozen orientational motion of the shakers.","sentences":["We report the emergence of large zigzag bands in a population of reversibly actuated magnetic rotors that behave as active shakers, namely squirmers that shake the fluid around them without moving.","The shakers collectively organize into dynamic structures displaying self-similar growth, and generate topological defects in form of cusps that connect vortices of rolling particles with alternating chirality.","By combining experimental analysis with particle-based simulation, we show that the special flow field created by the shakers is the only ingredient needed to reproduce the observed spatiotemporal pattern.","We unveil a self-organization scenario in a collection of driven particles in an viscoelastic medium emerging from the reduced particle degrees of freedom, as here the frozen orientational motion of the shakers."],"url":"http://arxiv.org/abs/2402.00493v1","category":"cond-mat.soft"}
{"created":"2024-02-01 10:57:00","title":"EXMOS: Explanatory Model Steering Through Multifaceted Explanations and Data Configurations","abstract":"Explanations in interactive machine-learning systems facilitate debugging and improving prediction models. However, the effectiveness of various global model-centric and data-centric explanations in aiding domain experts to detect and resolve potential data issues for model improvement remains unexplored. This research investigates the influence of data-centric and model-centric global explanations in systems that support healthcare experts in optimising models through automated and manual data configurations. We conducted quantitative (n=70) and qualitative (n=30) studies with healthcare experts to explore the impact of different explanations on trust, understandability and model improvement. Our results reveal the insufficiency of global model-centric explanations for guiding users during data configuration. Although data-centric explanations enhanced understanding of post-configuration system changes, a hybrid fusion of both explanation types demonstrated the highest effectiveness. Based on our study results, we also present design implications for effective explanation-driven interactive machine-learning systems.","sentences":["Explanations in interactive machine-learning systems facilitate debugging and improving prediction models.","However, the effectiveness of various global model-centric and data-centric explanations in aiding domain experts to detect and resolve potential data issues for model improvement remains unexplored.","This research investigates the influence of data-centric and model-centric global explanations in systems that support healthcare experts in optimising models through automated and manual data configurations.","We conducted quantitative (n=70) and qualitative (n=30) studies with healthcare experts to explore the impact of different explanations on trust, understandability and model improvement.","Our results reveal the insufficiency of global model-centric explanations for guiding users during data configuration.","Although data-centric explanations enhanced understanding of post-configuration system changes, a hybrid fusion of both explanation types demonstrated the highest effectiveness.","Based on our study results, we also present design implications for effective explanation-driven interactive machine-learning systems."],"url":"http://arxiv.org/abs/2402.00491v1","category":"cs.AI"}
{"created":"2024-02-01 10:55:49","title":"Testing the Number of Neutrino Species with a Global Fit of Neutrino Data","abstract":"We present the first experimental constraints on models with many additional neutrino species in an analysis of current neutrino data. These types of models are motivated as a solution to the hierarchy problem by lowering the species scale of gravity to TeV. Additionally, they offer a natural mechanism to generate small neutrino masses and provide interesting dark matter candidates. This study analyzes data from DayaBay, KamLAND, MINOS, NOvA and KATRIN. We do not find evidence for the presence of any additional neutrino species, therefore we report lower bounds on the allowed number of neutrino species realized in nature. For the normal/inverted neutrino mass ordering, we can give a lower bound on the number of neutrino species of O(30) and O(100), respectively, over a large range of the parameter space.","sentences":["We present the first experimental constraints on models with many additional neutrino species in an analysis of current neutrino data.","These types of models are motivated as a solution to the hierarchy problem by lowering the species scale of gravity to TeV. Additionally, they offer a natural mechanism to generate small neutrino masses and provide interesting dark matter candidates.","This study analyzes data from DayaBay, KamLAND, MINOS, NOvA and KATRIN.","We do not find evidence for the presence of any additional neutrino species, therefore we report lower bounds on the allowed number of neutrino species realized in nature.","For the normal/inverted neutrino mass ordering, we can give a lower bound on the number of neutrino species of O(30) and O(100), respectively, over a large range of the parameter space."],"url":"http://arxiv.org/abs/2402.00490v1","category":"hep-ph"}
{"created":"2024-02-01 10:47:04","title":"A note on the quantum Berezinian for the double Yangian of the Lie superalgebra $\\mathfrak{gl}_{m|n}$","abstract":"In this note, we generalize the notion of quantum Berezinian to the double Yangian ${\\rm DY}(\\mathfrak{gl}_{m|n})$ of the Lie superalgebra $\\mathfrak{gl}_{m|n}$. We show that its coefficients form a family of algebraically independent topological generators of the center of ${\\rm DY}(\\mathfrak{gl}_{m|n})$.","sentences":["In this note, we generalize the notion of quantum Berezinian to the double Yangian ${\\rm DY}(\\mathfrak{gl}_{m|n})$ of the Lie superalgebra $\\mathfrak{gl}_{m|n}$. We show that its coefficients form a family of algebraically independent topological generators of the center of ${\\rm DY}(\\mathfrak{gl}_{m|n})$."],"url":"http://arxiv.org/abs/2402.00487v1","category":"math.RT"}
{"created":"2024-02-01 10:42:37","title":"Quasistatic approximation in neuromodulation","abstract":"We define and explain the quasistatic approximation (QSA) as applied to field modeling for electrical and magnetic stimulation. Neuromodulation analysis pipelines include discrete stages, and QSA is applied specifically when calculating the electric and magnetic fields generated in tissues by a given stimulation dose. QSA simplifies the modeling equations to support tractable analysis, enhanced understanding, and computational efficiency. The application of QSA in neuromodulation is based on four underlying assumptions: (A1) no wave propagation or self-induction in tissue, (A2) linear tissue properties, (A3) purely resistive tissue, and (A4) non-dispersive tissue. As a consequence of these assumptions, each tissue is assigned a fixed conductivity, and the simplified equations (e.g., Laplace's equation) are solved for the spatial distribution of the field, which is separated from the field's temporal waveform. Recognizing that electrical tissue properties may be more complex, we explain how QSA can be embedded in parallel or iterative pipelines to model frequency dependence or nonlinearity of conductivity. We survey the history and validity of QSA across specific applications, such as microstimulation, deep brain stimulation, spinal cord stimulation, transcranial electrical stimulation, and transcranial magnetic stimulation. The precise definition and explanation of QSA in neuromodulation are essential for rigor when using QSA models or testing their limits.","sentences":["We define and explain the quasistatic approximation (QSA) as applied to field modeling for electrical and magnetic stimulation.","Neuromodulation analysis pipelines include discrete stages, and QSA is applied specifically when calculating the electric and magnetic fields generated in tissues by a given stimulation dose.","QSA simplifies the modeling equations to support tractable analysis, enhanced understanding, and computational efficiency.","The application of QSA in neuromodulation is based on four underlying assumptions: (A1) no wave propagation or self-induction in tissue, (A2) linear tissue properties, (A3) purely resistive tissue, and (A4) non-dispersive tissue.","As a consequence of these assumptions, each tissue is assigned a fixed conductivity, and the simplified equations (e.g., Laplace's equation) are solved for the spatial distribution of the field, which is separated from the field's temporal waveform.","Recognizing that electrical tissue properties may be more complex, we explain how QSA can be embedded in parallel or iterative pipelines to model frequency dependence or nonlinearity of conductivity.","We survey the history and validity of QSA across specific applications, such as microstimulation, deep brain stimulation, spinal cord stimulation, transcranial electrical stimulation, and transcranial magnetic stimulation.","The precise definition and explanation of QSA in neuromodulation are essential for rigor when using QSA models or testing their limits."],"url":"http://arxiv.org/abs/2402.00486v1","category":"physics.med-ph"}
{"created":"2024-02-01 10:42:05","title":"A Personalized Framework for Consumer and Producer Group Fairness Optimization in Recommender Systems","abstract":"In recent years, there has been an increasing recognition that when machine learning (ML) algorithms are used to automate decisions, they may mistreat individuals or groups, with legal, ethical, or economic implications. Recommender systems are prominent examples of these machine learning (ML) systems that aid users in making decisions. The majority of past literature research on RS fairness treats user and item fairness concerns independently, ignoring the fact that recommender systems function in a two-sided marketplace. In this paper, we propose CP-FairRank, an optimization-based re-ranking algorithm that seamlessly integrates fairness constraints from both the consumer and producer side in a joint objective framework. The framework is generalizable and may take into account varied fairness settings based on group segmentation, recommendation model selection, and domain, which is one of its key characteristics. For instance, we demonstrate that the system may jointly increase consumer and producer fairness when (un)protected consumer groups are defined on the basis of their activity level and main-streamness, while producer groups are defined according to their popularity level. For empirical validation, through large-scale on eight datasets and four mainstream collaborative filtering (CF) recommendation models, we demonstrate that our proposed strategy is able to improve both consumer and producer fairness without compromising or very little overall recommendation quality, demonstrating the role algorithms may play in avoiding data biases.","sentences":["In recent years, there has been an increasing recognition that when machine learning (ML) algorithms are used to automate decisions, they may mistreat individuals or groups, with legal, ethical, or economic implications.","Recommender systems are prominent examples of these machine learning (ML) systems that aid users in making decisions.","The majority of past literature research on RS fairness treats user and item fairness concerns independently, ignoring the fact that recommender systems function in a two-sided marketplace.","In this paper, we propose CP-FairRank, an optimization-based re-ranking algorithm that seamlessly integrates fairness constraints from both the consumer and producer side in a joint objective framework.","The framework is generalizable and may take into account varied fairness settings based on group segmentation, recommendation model selection, and domain, which is one of its key characteristics.","For instance, we demonstrate that the system may jointly increase consumer and producer fairness when (un)protected consumer groups are defined on the basis of their activity level and main-streamness, while producer groups are defined according to their popularity level.","For empirical validation, through large-scale on eight datasets and four mainstream collaborative filtering (CF) recommendation models, we demonstrate that our proposed strategy is able to improve both consumer and producer fairness without compromising or very little overall recommendation quality, demonstrating the role algorithms may play in avoiding data biases."],"url":"http://arxiv.org/abs/2402.00485v1","category":"cs.AI"}
{"created":"2024-02-01 10:39:02","title":"Gelfand-Tsetlin modules for Lie algebras of rank $2$","abstract":"We explicitly construct families of simple modules for Lie algebras of rank $2$, on which certain commutative subalgebra acts diagonally and has a simple spectrum. In type $A$ these modules are well known generic Gelfand-Tsetlin modules and they can be viewed as such for other rank $2$ Lie algebras.","sentences":["We explicitly construct families of simple modules for Lie algebras of rank $2$, on which certain commutative subalgebra acts diagonally and has a simple spectrum.","In type $A$ these modules are well known generic Gelfand-Tsetlin modules and they can be viewed as such for other rank $2$ Lie algebras."],"url":"http://arxiv.org/abs/2402.00483v1","category":"math.RT"}
{"created":"2024-02-01 10:38:00","title":"Inverse problems for a generalized fractional diffusion equation with unknown history","abstract":"Inverse problems for a diffusion equation containing a generalized fractional derivative are studied. The equation holds in a time interval $(0,T)$ and it is assumed that a state $u$ (solution of diffusion equation) and a source $f$ are known for $t\\in (t_0,T)$ where $t_0$ is some number in $(0,T)$. Provided that $f$ satisfies certain restrictions, it is proved that product of a kernel of the derivative with an elliptic operator as well as the history of $f$ for $t\\in (0,t_0)$ are uniquely recovered. In case of less restrictions on $f$ the uniqueness of the kernel and the history of $f$ is shown. Moreover, in a case when a functional of $u$ for $t\\in (t_0,T)$ is given the uniqueness of the kernel is proved under unknown history of $f$.","sentences":["Inverse problems for a diffusion equation containing a generalized fractional derivative are studied.","The equation holds in a time interval $(0,T)$ and it is assumed that a state $u$ (solution of diffusion equation) and a source $f$ are known for $t\\in (t_0,T)$ where $t_0$ is some number in $(0,T)$. Provided that $f$ satisfies certain restrictions, it is proved that product of a kernel of the derivative with an elliptic operator as well as the history of $f$ for $t\\in (0,t_0)$ are uniquely recovered.","In case of less restrictions on $f$ the uniqueness of the kernel and the history of $f$ is shown.","Moreover, in a case when a functional of $u$ for $t\\in (t_0,T)$ is given the uniqueness of the kernel is proved under unknown history of $f$."],"url":"http://arxiv.org/abs/2402.00482v1","category":"math-ph"}
{"created":"2024-02-01 10:26:27","title":"SA-MDKIF: A Scalable and Adaptable Medical Domain Knowledge Injection Framework for Large Language Models","abstract":"Recent advances in large language models (LLMs) have demonstrated exceptional performance in various natural language processing (NLP) tasks. However, their effective application in the medical domain is hampered by a lack of medical domain knowledge. In this study, we present SA-MDKIF, a scalable and adaptable framework that aims to inject medical knowledge into general-purpose LLMs through instruction tuning, thereby enabling adaptability for various downstream tasks. SA-MDKIF consists of two stages: skill training and skill adaptation. In the first stage, we define 12 basic medical skills and use AdaLoRA to train these skills based on uniformly formatted instructional datasets that we have constructed. In the next stage, we train the skill router using task-specific downstream data and use this router to integrate the acquired skills with LLMs during inference. Experimental results on 9 different medical tasks show that SA-MDKIF improves performance by 10-20% compared to the original LLMs. Notably, this improvement is particularly pronounced for unseen medical tasks, showing an improvement of up to 30%.","sentences":["Recent advances in large language models (LLMs) have demonstrated exceptional performance in various natural language processing (NLP) tasks.","However, their effective application in the medical domain is hampered by a lack of medical domain knowledge.","In this study, we present SA-MDKIF, a scalable and adaptable framework that aims to inject medical knowledge into general-purpose LLMs through instruction tuning, thereby enabling adaptability for various downstream tasks.","SA-MDKIF consists of two stages: skill training and skill adaptation.","In the first stage, we define 12 basic medical skills and use AdaLoRA to train these skills based on uniformly formatted instructional datasets that we have constructed.","In the next stage, we train the skill router using task-specific downstream data and use this router to integrate the acquired skills with LLMs during inference.","Experimental results on 9 different medical tasks show that SA-MDKIF improves performance by 10-20% compared to the original LLMs.","Notably, this improvement is particularly pronounced for unseen medical tasks, showing an improvement of up to 30%."],"url":"http://arxiv.org/abs/2402.00474v1","category":"cs.CL"}
{"created":"2024-02-01 10:21:48","title":"A mechanism for electrostatically generated magnetoresistance in chiral systems without spin-dependent transport","abstract":"Significant attention has been drawn to electronic transport in chiral materials coupled to ferromagnets in the chirality induced spin selectivity (CISS) effect. A large magnetoresistance (MR) is usually observed which is widely interpreted to originate from spin (dependent) transport. However, there are severe discrepancies between the experimental results and theoretical interpretations, most notably the apparent failure of the Onsager reciprocity relation in the linear response regime. We provide an alternative explanation for the mechanism of the two terminal MR in chiral systems coupled to a ferromagnet. For this we point out that it was observed that the electrostatic contact potential of chiral materials on a ferromagnet depends on the magnetization direction and chirality. In our explanation this causes the transport barrier to be modified by the magnetization direction, already in equilibrium, in the absence of a bias current. This strongly alters the charge transport through/over the barrier, not requiring spin transport. This provides a mechanism that allows the linear response resistance to be sensitive to the magnetization direction and also explains the failure of the Onsager reciprocity relations. We propose experimental configurations to confirm our alternative mechanism for MR.","sentences":["Significant attention has been drawn to electronic transport in chiral materials coupled to ferromagnets in the chirality induced spin selectivity (CISS) effect.","A large magnetoresistance (MR) is usually observed which is widely interpreted to originate from spin (dependent) transport.","However, there are severe discrepancies between the experimental results and theoretical interpretations, most notably the apparent failure of the Onsager reciprocity relation in the linear response regime.","We provide an alternative explanation for the mechanism of the two terminal MR in chiral systems coupled to a ferromagnet.","For this we point out that it was observed that the electrostatic contact potential of chiral materials on a ferromagnet depends on the magnetization direction and chirality.","In our explanation this causes the transport barrier to be modified by the magnetization direction, already in equilibrium, in the absence of a bias current.","This strongly alters the charge transport through/over the barrier, not requiring spin transport.","This provides a mechanism that allows the linear response resistance to be sensitive to the magnetization direction and also explains the failure of the Onsager reciprocity relations.","We propose experimental configurations to confirm our alternative mechanism for MR."],"url":"http://arxiv.org/abs/2402.00472v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-01 10:20:37","title":"Fairness by design in shared-energy allocation problems","abstract":"This paper studies how to aggregate prosumers (or large consumers) and their collective decisions in electricity markets, with a focus on fairness. Fairness is essential for prosumers to participate in aggregation schemes. Some prosumers may not be able to access the energy market directly, even though it would be beneficial for them. Therefore, new companies offer to aggregate them and promise to treat them fairly. This leads to a fair resource allocation problem.   We propose to use acceptability constraints to guarantee that each prosumer gains from the aggregation. Moreover, we aim to distribute the costs and benefits fairly, taking into account the multi-period and uncertain nature of the problem. Rather than using financial mechanisms to adjust for fairness issues, we focus on various objectives and constraints, within decision problems, that achieve fairness by design. We start from a simple single-period and deterministic model, and then generalize it to a dynamic and stochastic setting using, e.g., stochastic dominance constraints.","sentences":["This paper studies how to aggregate prosumers (or large consumers) and their collective decisions in electricity markets, with a focus on fairness.","Fairness is essential for prosumers to participate in aggregation schemes.","Some prosumers may not be able to access the energy market directly, even though it would be beneficial for them.","Therefore, new companies offer to aggregate them and promise to treat them fairly.","This leads to a fair resource allocation problem.   ","We propose to use acceptability constraints to guarantee that each prosumer gains from the aggregation.","Moreover, we aim to distribute the costs and benefits fairly, taking into account the multi-period and uncertain nature of the problem.","Rather than using financial mechanisms to adjust for fairness issues, we focus on various objectives and constraints, within decision problems, that achieve fairness by design.","We start from a simple single-period and deterministic model, and then generalize it to a dynamic and stochastic setting using, e.g., stochastic dominance constraints."],"url":"http://arxiv.org/abs/2402.00471v1","category":"math.OC"}
{"created":"2024-02-01 10:15:39","title":"RadDQN: a Deep Q Learning-based Architecture for Finding Time-efficient Minimum Radiation Exposure Pathway","abstract":"Recent advancements in deep reinforcement learning (DRL) techniques have sparked its multifaceted applications in the automation sector. Managing complex decision-making problems with DRL encourages its use in the nuclear industry for tasks such as optimizing radiation exposure to the personnel during normal operating conditions and potential accidental scenarios. However, the lack of efficient reward function and effective exploration strategy thwarted its implementation in the development of radiation-aware autonomous unmanned aerial vehicle (UAV) for achieving maximum radiation protection. Here, in this article, we address these intriguing issues and introduce a deep Q-learning based architecture (RadDQN) that operates on a radiation-aware reward function to provide time-efficient minimum radiation-exposure pathway in a radiation zone. We propose a set of unique exploration strategies that fine-tune the extent of exploration and exploitation based on the state-wise variation in radiation exposure during training. Further, we benchmark the predicted path with grid-based deterministic method. We demonstrate that the formulated reward function in conjugation with adequate exploration strategy is effective in handling several scenarios with drastically different radiation field distributions. When compared to vanilla DQN, our model achieves a superior convergence rate and higher training stability.","sentences":["Recent advancements in deep reinforcement learning (DRL) techniques have sparked its multifaceted applications in the automation sector.","Managing complex decision-making problems with DRL encourages its use in the nuclear industry for tasks such as optimizing radiation exposure to the personnel during normal operating conditions and potential accidental scenarios.","However, the lack of efficient reward function and effective exploration strategy thwarted its implementation in the development of radiation-aware autonomous unmanned aerial vehicle (UAV) for achieving maximum radiation protection.","Here, in this article, we address these intriguing issues and introduce a deep Q-learning based architecture (RadDQN) that operates on a radiation-aware reward function to provide time-efficient minimum radiation-exposure pathway in a radiation zone.","We propose a set of unique exploration strategies that fine-tune the extent of exploration and exploitation based on the state-wise variation in radiation exposure during training.","Further, we benchmark the predicted path with grid-based deterministic method.","We demonstrate that the formulated reward function in conjugation with adequate exploration strategy is effective in handling several scenarios with drastically different radiation field distributions.","When compared to vanilla DQN, our model achieves a superior convergence rate and higher training stability."],"url":"http://arxiv.org/abs/2402.00468v1","category":"cs.AI"}
{"created":"2024-02-01 10:13:01","title":"Towards a GPU-Parallelization of the neXtSIM-DG Dynamical Core","abstract":"The cryosphere plays a significant role in Earth's climate system. Therefore, an accurate simulation of sea ice is of great importance to improve climate projections. To enable higher resolution simulations, graphics processing units (GPUs) have become increasingly attractive as they offer higher floating point peak performance and better energy efficiency compared to CPUs. However, making use of the theoretical peak performance usually requires more care and effort in the implementation. In recent years, a number of frameworks have become available that promise to simplify general purpose GPU programming. In this work, we compare multiple such frameworks, including CUDA, SYCL, Kokkos and PyTorch, for the parallelization of \\nextsim, a finite-element based dynamical core for sea ice. We evaluate the different approaches according to their usability and performance.","sentences":["The cryosphere plays a significant role in Earth's climate system.","Therefore, an accurate simulation of sea ice is of great importance to improve climate projections.","To enable higher resolution simulations, graphics processing units (GPUs) have become increasingly attractive as they offer higher floating point peak performance and better energy efficiency compared to CPUs.","However, making use of the theoretical peak performance usually requires more care and effort in the implementation.","In recent years, a number of frameworks have become available that promise to simplify general purpose GPU programming.","In this work, we compare multiple such frameworks, including CUDA, SYCL, Kokkos and PyTorch, for the parallelization of \\nextsim, a finite-element based dynamical core for sea ice.","We evaluate the different approaches according to their usability and performance."],"url":"http://arxiv.org/abs/2402.00466v1","category":"cs.DC"}
{"created":"2024-02-01 09:57:38","title":"Genetic-based Constraint Programming for Resource Constrained Job Scheduling","abstract":"Resource constrained job scheduling is a hard combinatorial optimisation problem that originates in the mining industry. Off-the-shelf solvers cannot solve this problem satisfactorily in reasonable timeframes, while other solution methods such as many evolutionary computation methods and matheuristics cannot guarantee optimality and require low-level customisation and specialised heuristics to be effective. This paper addresses this gap by proposing a genetic programming algorithm to discover efficient search strategies of constraint programming for resource-constrained job scheduling. In the proposed algorithm, evolved programs represent variable selectors to be used in the search process of constraint programming, and their fitness is determined by the quality of solutions obtained for training instances. The novelties of this algorithm are (1) a new representation of variable selectors, (2) a new fitness evaluation scheme, and (3) a pre-selection mechanism. Tests with a large set of random and benchmark instances, the evolved variable selectors can significantly improve the efficiency of constraining programming. Compared to highly customised metaheuristics and hybrid algorithms, evolved variable selectors can help constraint programming identify quality solutions faster and proving optimality is possible if sufficiently large run-times are allowed. The evolved variable selectors are especially helpful when solving instances with large numbers of machines.","sentences":["Resource constrained job scheduling is a hard combinatorial optimisation problem that originates in the mining industry.","Off-the-shelf solvers cannot solve this problem satisfactorily in reasonable timeframes, while other solution methods such as many evolutionary computation methods and matheuristics cannot guarantee optimality and require low-level customisation and specialised heuristics to be effective.","This paper addresses this gap by proposing a genetic programming algorithm to discover efficient search strategies of constraint programming for resource-constrained job scheduling.","In the proposed algorithm, evolved programs represent variable selectors to be used in the search process of constraint programming, and their fitness is determined by the quality of solutions obtained for training instances.","The novelties of this algorithm are (1) a new representation of variable selectors, (2) a new fitness evaluation scheme, and (3) a pre-selection mechanism.","Tests with a large set of random and benchmark instances, the evolved variable selectors can significantly improve the efficiency of constraining programming.","Compared to highly customised metaheuristics and hybrid algorithms, evolved variable selectors can help constraint programming identify quality solutions faster and proving optimality is possible if sufficiently large run-times are allowed.","The evolved variable selectors are especially helpful when solving instances with large numbers of machines."],"url":"http://arxiv.org/abs/2402.00459v1","category":"cs.NE"}
{"created":"2024-02-01 09:41:55","title":"Inflationary $\u03b1$-Attractor From Type IIB/F Theory","abstract":"We derive an $\\alpha=1/3$ - attractor potential of slow-roll inflation in the geometric set-up of three intersecting $D7$ branes under $T^6/Z_N$ type of $CY_3$-compactification within type IIB/F theory with some near-conifold regions. The underlying quadratic structure of the kinetic poles is found to arise from a correction in the K\\\"{a}hler potential when an extra contribution of open string moduli is turned on. While the closed string sector of the moduli spectrum is completely stabilized via quantum corrections of perturbative and non-perturbative origin, the open string sector plays the lead role in driving the inflationary expansion in the radial direction. A generic asymptotic behavior of the inflaton field near the pole boundaries manifests as the slow-roll plateau in canonical field space, which becomes responsible for giving universal predictions of the cosmological parameters. We find that the presence of the open strings near conifold regions brings the realization of pole inflation in the present set up. Finally we compare our results with similar models and discuss the importance of exploring precise values of $\\alpha$ in the light of ongoing and forthcoming cosmological surveys.","sentences":["We derive an $\\alpha=1/3$ - attractor potential of slow-roll inflation in the geometric set-up of three intersecting $D7$ branes under $T^6/Z_N$ type of $CY_3$-compactification within type IIB/F theory with some near-conifold regions.","The underlying quadratic structure of the kinetic poles is found to arise from a correction in the K\\\"{a}hler potential when an extra contribution of open string moduli is turned on.","While the closed string sector of the moduli spectrum is completely stabilized via quantum corrections of perturbative and non-perturbative origin, the open string sector plays the lead role in driving the inflationary expansion in the radial direction.","A generic asymptotic behavior of the inflaton field near the pole boundaries manifests as the slow-roll plateau in canonical field space, which becomes responsible for giving universal predictions of the cosmological parameters.","We find that the presence of the open strings near conifold regions brings the realization of pole inflation in the present set up.","Finally we compare our results with similar models and discuss the importance of exploring precise values of $\\alpha$ in the light of ongoing and forthcoming cosmological surveys."],"url":"http://arxiv.org/abs/2402.00451v1","category":"hep-th"}
{"created":"2024-02-01 09:36:26","title":"Efficient Training Spiking Neural Networks with Parallel Spiking Unit","abstract":"Efficient parallel computing has become a pivotal element in advancing artificial intelligence. Yet, the deployment of Spiking Neural Networks (SNNs) in this domain is hampered by their inherent sequential computational dependency. This constraint arises from the need for each time step's processing to rely on the preceding step's outcomes, significantly impeding the adaptability of SNN models to massively parallel computing environments. Addressing this challenge, our paper introduces the innovative Parallel Spiking Unit (PSU) and its two derivatives, the Input-aware PSU (IPSU) and Reset-aware PSU (RPSU). These variants skillfully decouple the leaky integration and firing mechanisms in spiking neurons while probabilistically managing the reset process. By preserving the fundamental computational attributes of the spiking neuron model, our approach enables the concurrent computation of all membrane potential instances within the SNN, facilitating parallel spike output generation and substantially enhancing computational efficiency. Comprehensive testing across various datasets, including static and sequential images, Dynamic Vision Sensor (DVS) data, and speech datasets, demonstrates that the PSU and its variants not only significantly boost performance and simulation speed but also augment the energy efficiency of SNNs through enhanced sparsity in neural activity. These advancements underscore the potential of our method in revolutionizing SNN deployment for high-performance parallel computing applications.","sentences":["Efficient parallel computing has become a pivotal element in advancing artificial intelligence.","Yet, the deployment of Spiking Neural Networks (SNNs) in this domain is hampered by their inherent sequential computational dependency.","This constraint arises from the need for each time step's processing to rely on the preceding step's outcomes, significantly impeding the adaptability of SNN models to massively parallel computing environments.","Addressing this challenge, our paper introduces the innovative Parallel Spiking Unit (PSU) and its two derivatives, the Input-aware PSU (IPSU) and Reset-aware PSU (RPSU).","These variants skillfully decouple the leaky integration and firing mechanisms in spiking neurons while probabilistically managing the reset process.","By preserving the fundamental computational attributes of the spiking neuron model, our approach enables the concurrent computation of all membrane potential instances within the SNN, facilitating parallel spike output generation and substantially enhancing computational efficiency.","Comprehensive testing across various datasets, including static and sequential images, Dynamic Vision Sensor (DVS) data, and speech datasets, demonstrates that the PSU and its variants not only significantly boost performance and simulation speed but also augment the energy efficiency of SNNs through enhanced sparsity in neural activity.","These advancements underscore the potential of our method in revolutionizing SNN deployment for high-performance parallel computing applications."],"url":"http://arxiv.org/abs/2402.00449v1","category":"cs.NE"}
{"created":"2024-02-01 09:32:39","title":"Dual-Student Knowledge Distillation Networks for Unsupervised Anomaly Detection","abstract":"Due to the data imbalance and the diversity of defects, student-teacher networks (S-T) are favored in unsupervised anomaly detection, which explores the discrepancy in feature representation derived from the knowledge distillation process to recognize anomalies. However, vanilla S-T network is not stable. Employing identical structures to construct the S-T network may weaken the representative discrepancy on anomalies. But using different structures can increase the likelihood of divergent performance on normal data. To address this problem, we propose a novel dual-student knowledge distillation (DSKD) architecture. Different from other S-T networks, we use two student networks a single pre-trained teacher network, where the students have the same scale but inverted structures. This framework can enhance the distillation effect to improve the consistency in recognition of normal data, and simultaneously introduce diversity for anomaly representation. To explore high-dimensional semantic information to capture anomaly clues, we employ two strategies. First, a pyramid matching mode is used to perform knowledge distillation on multi-scale feature maps in the intermediate layers of networks. Second, an interaction is facilitated between the two student networks through a deep feature embedding module, which is inspired by real-world group discussions. In terms of classification, we obtain pixel-wise anomaly segmentation maps by measuring the discrepancy between the output feature maps of the teacher and student networks, from which an anomaly score is computed for sample-wise determination. We evaluate DSKD on three benchmark datasets and probe the effects of internal modules through ablation experiments. The results demonstrate that DSKD can achieve exceptional performance on small models like ResNet18 and effectively improve vanilla S-T networks.","sentences":["Due to the data imbalance and the diversity of defects, student-teacher networks (S-T) are favored in unsupervised anomaly detection, which explores the discrepancy in feature representation derived from the knowledge distillation process to recognize anomalies.","However, vanilla S-T network is not stable.","Employing identical structures to construct the S-T network may weaken the representative discrepancy on anomalies.","But using different structures can increase the likelihood of divergent performance on normal data.","To address this problem, we propose a novel dual-student knowledge distillation (DSKD) architecture.","Different from other S-T networks, we use two student networks a single pre-trained teacher network, where the students have the same scale but inverted structures.","This framework can enhance the distillation effect to improve the consistency in recognition of normal data, and simultaneously introduce diversity for anomaly representation.","To explore high-dimensional semantic information to capture anomaly clues, we employ two strategies.","First, a pyramid matching mode is used to perform knowledge distillation on multi-scale feature maps in the intermediate layers of networks.","Second, an interaction is facilitated between the two student networks through a deep feature embedding module, which is inspired by real-world group discussions.","In terms of classification, we obtain pixel-wise anomaly segmentation maps by measuring the discrepancy between the output feature maps of the teacher and student networks, from which an anomaly score is computed for sample-wise determination.","We evaluate DSKD on three benchmark datasets and probe the effects of internal modules through ablation experiments.","The results demonstrate that DSKD can achieve exceptional performance on small models like ResNet18 and effectively improve vanilla S-T networks."],"url":"http://arxiv.org/abs/2402.00448v1","category":"cs.CV"}
{"created":"2024-02-01 09:28:48","title":"A Survey of Data-Efficient Graph Learning","abstract":"Graph-structured data, prevalent in domains ranging from social networks to biochemical analysis, serve as the foundation for diverse real-world systems. While graph neural networks demonstrate proficiency in modeling this type of data, their success is often reliant on significant amounts of labeled data, posing a challenge in practical scenarios with limited annotation resources. To tackle this problem, tremendous efforts have been devoted to enhancing graph machine learning performance under low-resource settings by exploring various approaches to minimal supervision. In this paper, we introduce a novel concept of Data-Efficient Graph Learning (DEGL) as a research frontier, and present the first survey that summarizes the current progress of DEGL. We initiate by highlighting the challenges inherent in training models with large labeled data, paving the way for our exploration into DEGL. Next, we systematically review recent advances on this topic from several key aspects, including self-supervised graph learning, semi-supervised graph learning, and few-shot graph learning. Also, we state promising directions for future research, contributing to the evolution of graph machine learning.","sentences":["Graph-structured data, prevalent in domains ranging from social networks to biochemical analysis, serve as the foundation for diverse real-world systems.","While graph neural networks demonstrate proficiency in modeling this type of data, their success is often reliant on significant amounts of labeled data, posing a challenge in practical scenarios with limited annotation resources.","To tackle this problem, tremendous efforts have been devoted to enhancing graph machine learning performance under low-resource settings by exploring various approaches to minimal supervision.","In this paper, we introduce a novel concept of Data-Efficient Graph Learning (DEGL) as a research frontier, and present the first survey that summarizes the current progress of DEGL.","We initiate by highlighting the challenges inherent in training models with large labeled data, paving the way for our exploration into DEGL.","Next, we systematically review recent advances on this topic from several key aspects, including self-supervised graph learning, semi-supervised graph learning, and few-shot graph learning.","Also, we state promising directions for future research, contributing to the evolution of graph machine learning."],"url":"http://arxiv.org/abs/2402.00447v1","category":"cs.LG"}
{"created":"2024-02-01 09:24:33","title":"Improving Dialog Safety using Socially Aware Contrastive Learning","abstract":"State-of-the-art conversational AI systems raise concerns due to their potential risks of generating unsafe, toxic, unethical, or dangerous content. Previous works have developed datasets to teach conversational agents the appropriate social paradigms to respond effectively to specifically designed hazardous content. However, models trained on these adversarial datasets still struggle to recognize subtle unsafe situations that appear naturally in conversations or introduce an inappropriate response in a casual context. To understand the extent of this problem, we study prosociality in both adversarial and casual dialog contexts and audit the response quality of general-purpose language models in terms of propensity to produce unsafe content. We propose a dual-step fine-tuning process to address these issues using a socially aware n-pair contrastive loss. Subsequently, we train a base model that integrates prosocial behavior by leveraging datasets like Moral Integrity Corpus (MIC) and ProsocialDialog. Experimental results on several dialog datasets demonstrate the effectiveness of our approach in generating socially appropriate responses.","sentences":["State-of-the-art conversational AI systems raise concerns due to their potential risks of generating unsafe, toxic, unethical, or dangerous content.","Previous works have developed datasets to teach conversational agents the appropriate social paradigms to respond effectively to specifically designed hazardous content.","However, models trained on these adversarial datasets still struggle to recognize subtle unsafe situations that appear naturally in conversations or introduce an inappropriate response in a casual context.","To understand the extent of this problem, we study prosociality in both adversarial and casual dialog contexts and audit the response quality of general-purpose language models in terms of propensity to produce unsafe content.","We propose a dual-step fine-tuning process to address these issues using a socially aware n-pair contrastive loss.","Subsequently, we train a base model that integrates prosocial behavior by leveraging datasets like Moral Integrity Corpus (MIC) and ProsocialDialog.","Experimental results on several dialog datasets demonstrate the effectiveness of our approach in generating socially appropriate responses."],"url":"http://arxiv.org/abs/2402.00446v1","category":"cs.CL"}
{"created":"2024-02-01 09:18:59","title":"Option pricing for Barndorff-Nielsen and Shephard model by supervised deep learning","abstract":"This paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model with a non-martingale asset price process having infinite active jumps. In our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai (2024). Moreover, the BNS model includes many variables, which makes the deep learning accuracy worse. Therefore, we will create another input variable using the Black-Scholes formula. As a result, the accuracy is improved dramatically.","sentences":["This paper aims to develop a supervised deep-learning scheme to compute call option prices for the Barndorff-Nielsen and Shephard model with a non-martingale asset price process having infinite active jumps.","In our deep learning scheme, teaching data is generated through the Monte Carlo method developed by Arai and Imai (2024).","Moreover, the BNS model includes many variables, which makes the deep learning accuracy worse.","Therefore, we will create another input variable using the Black-Scholes formula.","As a result, the accuracy is improved dramatically."],"url":"http://arxiv.org/abs/2402.00445v1","category":"q-fin.CP"}
{"created":"2024-02-01 09:11:08","title":"Degradation Analysis of Perovskite Solar Cells via Short-Circuit Impedance Spectroscopy: A case study on NiOx passivation","abstract":"Perovskite solar cells (PSCs) continue to be the front runner technology among emerging photovoltaic devices in terms of power conversion efficiency and application versatility. However, not only the stability but also the understanding of their ionic-electronic transport mechanisms continues to be challenging. In this work, the case study of NiOx-based inverted PSCs and the effect of different interface passivating treatments on the device performance are approached. Our experiments include impedance spectroscopy (IS) measurements in short-circuit under different illumination intensities and operational stability tests under constant illumination intensity. It is found that certain surface treatments lead to more stable performance. However, protic anion donors can induce, both, an initial performance decrease and a subsequent reactivity during light exposure which apparently improves the cells performance. Our drift-diffusion simulations suggest that the modification of the interface with the hole transport material may have decrease the conductivity, as well as the ion and electron mobilities at the perovskite and the NiOx, respectively. Importantly, capacitance and resistance are shown to peak maximum and minimum values, respectively, around specific ranges of mobile ion concentration. Our results introduce a general route for characterization of degradation paths in PSCs via IS in short-circuit.","sentences":["Perovskite solar cells (PSCs) continue to be the front runner technology among emerging photovoltaic devices in terms of power conversion efficiency and application versatility.","However, not only the stability but also the understanding of their ionic-electronic transport mechanisms continues to be challenging.","In this work, the case study of NiOx-based inverted PSCs and the effect of different interface passivating treatments on the device performance are approached.","Our experiments include impedance spectroscopy (IS) measurements in short-circuit under different illumination intensities and operational stability tests under constant illumination intensity.","It is found that certain surface treatments lead to more stable performance.","However, protic anion donors can induce, both, an initial performance decrease and a subsequent reactivity during light exposure which apparently improves the cells performance.","Our drift-diffusion simulations suggest that the modification of the interface with the hole transport material may have decrease the conductivity, as well as the ion and electron mobilities at the perovskite and the NiOx, respectively.","Importantly, capacitance and resistance are shown to peak maximum and minimum values, respectively, around specific ranges of mobile ion concentration.","Our results introduce a general route for characterization of degradation paths in PSCs via IS in short-circuit."],"url":"http://arxiv.org/abs/2402.00439v1","category":"physics.app-ph"}
{"created":"2024-02-01 09:08:19","title":"A limit for the values of the Dst geomagnetic index","abstract":"The study of the extreme weather space events is important for a technological dependent society. Extreme Value Theory could be decisive to characterize those extreme events in order to have the knowledge to make decisions in technological, economic and social matters, in all fields with possible impacts. In this work, the hourly values of the Dst geomagnetic index has been studied for the period 1957-2014 using the peaks-over-threshold technique. The shape parameter obtained from the fit of the generalized Pareto distribution to the extreme values of the |Dst| index leads to a negative value implying an upper bound for this time series. This result is relevant, because the estimation of this limit for the extreme values lead to 850 nT as the highest expected value for this geomagnetic index. Thus, from the previous characterization of the Carrington geomagnetic storm and our results, it could be considered the worst case scenario.","sentences":["The study of the extreme weather space events is important for a technological dependent society.","Extreme Value Theory could be decisive to characterize those extreme events in order to have the knowledge to make decisions in technological, economic and social matters, in all fields with possible impacts.","In this work, the hourly values of the Dst geomagnetic index has been studied for the period 1957-2014 using the peaks-over-threshold technique.","The shape parameter obtained from the fit of the generalized Pareto distribution to the extreme values of the |Dst| index leads to a negative value implying an upper bound for this time series.","This result is relevant, because the estimation of this limit for the extreme values lead to 850 nT as the highest expected value for this geomagnetic index.","Thus, from the previous characterization of the Carrington geomagnetic storm and our results, it could be considered the worst case scenario."],"url":"http://arxiv.org/abs/2402.00437v1","category":"physics.space-ph"}
{"created":"2024-02-01 09:03:49","title":"Convergence rates for the moment-SoS hierarchy","abstract":"We introduce a comprehensive framework for analyzing convergence rates for infinite dimensional linear programming problems (LPs) within the context of the moment-sum-of-squares hierarchy. Our primary focus is on extending the existing convergence rate analysis, initially developed for static polynomial optimization, to the more general and challenging domain of the generalized moment problem. We establish an easy-to-follow procedure for obtaining convergence rates. Our methodology is based on, firstly, a state-of-the-art degree bound for Putinar's Positivstellensatz, secondly, quantitative polynomial approximation bounds, and, thirdly, a geometric Slater condition on the infinite dimensional LP. We address a broad problem formulation that encompasses various applications, such as optimal control, volume computation, and exit location of stochastic processes. We illustrate the procedure at these three problems and, using a recent improvement on effective versions of Putinar's Positivstellensatz, we improve existing convergence rates.","sentences":["We introduce a comprehensive framework for analyzing convergence rates for infinite dimensional linear programming problems (LPs) within the context of the moment-sum-of-squares hierarchy.","Our primary focus is on extending the existing convergence rate analysis, initially developed for static polynomial optimization, to the more general and challenging domain of the generalized moment problem.","We establish an easy-to-follow procedure for obtaining convergence rates.","Our methodology is based on, firstly, a state-of-the-art degree bound for Putinar's Positivstellensatz, secondly, quantitative polynomial approximation bounds, and, thirdly, a geometric Slater condition on the infinite dimensional LP.","We address a broad problem formulation that encompasses various applications, such as optimal control, volume computation, and exit location of stochastic processes.","We illustrate the procedure at these three problems and, using a recent improvement on effective versions of Putinar's Positivstellensatz, we improve existing convergence rates."],"url":"http://arxiv.org/abs/2402.00436v1","category":"math.OC"}
{"created":"2024-02-01 09:01:58","title":"A practical existence theorem for reduced order models based on convolutional autoencoders","abstract":"In recent years, deep learning has gained increasing popularity in the fields of Partial Differential Equations (PDEs) and Reduced Order Modeling (ROM), providing domain practitioners with new powerful data-driven techniques such as Physics-Informed Neural Networks (PINNs), Neural Operators, Deep Operator Networks (DeepONets) and Deep-Learning based ROMs (DL-ROMs). In this context, deep autoencoders based on Convolutional Neural Networks (CNNs) have proven extremely effective, outperforming established techniques, such as the reduced basis method, when dealing with complex nonlinear problems. However, despite the empirical success of CNN-based autoencoders, there are only a few theoretical results supporting these architectures, usually stated in the form of universal approximation theorems. In particular, although the existing literature provides users with guidelines for designing convolutional autoencoders, the subsequent challenge of learning the latent features has been barely investigated. Furthermore, many practical questions remain unanswered, e.g., the number of snapshots needed for convergence or the neural network training strategy. In this work, using recent techniques from sparse high-dimensional function approximation, we fill some of these gaps by providing a new practical existence theorem for CNN-based autoencoders when the parameter-to-solution map is holomorphic. This regularity assumption arises in many relevant classes of parametric PDEs, such as the parametric diffusion equation, for which we discuss an explicit application of our general theory.","sentences":["In recent years, deep learning has gained increasing popularity in the fields of Partial Differential Equations (PDEs) and Reduced Order Modeling (ROM), providing domain practitioners with new powerful data-driven techniques such as Physics-Informed Neural Networks (PINNs), Neural Operators, Deep Operator Networks (DeepONets) and Deep-Learning based ROMs (DL-ROMs).","In this context, deep autoencoders based on Convolutional Neural Networks (CNNs) have proven extremely effective, outperforming established techniques, such as the reduced basis method, when dealing with complex nonlinear problems.","However, despite the empirical success of CNN-based autoencoders, there are only a few theoretical results supporting these architectures, usually stated in the form of universal approximation theorems.","In particular, although the existing literature provides users with guidelines for designing convolutional autoencoders, the subsequent challenge of learning the latent features has been barely investigated.","Furthermore, many practical questions remain unanswered, e.g., the number of snapshots needed for convergence or the neural network training strategy.","In this work, using recent techniques from sparse high-dimensional function approximation, we fill some of these gaps by providing a new practical existence theorem for CNN-based autoencoders when the parameter-to-solution map is holomorphic.","This regularity assumption arises in many relevant classes of parametric PDEs, such as the parametric diffusion equation, for which we discuss an explicit application of our general theory."],"url":"http://arxiv.org/abs/2402.00435v1","category":"math.NA"}
{"created":"2024-02-01 08:58:57","title":"Merging Multi-Task Models via Weight-Ensembling Mixture of Experts","abstract":"Merging various task-specific Transformer-based models trained on different tasks into a single unified model can execute all the tasks concurrently. Previous methods, exemplified by task arithmetic, have been proven to be both effective and scalable. Existing methods have primarily focused on seeking a static optimal solution within the original model parameter space. A notable challenge is mitigating the interference between parameters of different models, which can substantially deteriorate performance. In this paper, we propose to merge most of the parameters while upscaling the MLP of the Transformer layers to a weight-ensembling mixture of experts (MoE) module, which can dynamically integrate shared and task-specific knowledge based on the input, thereby providing a more flexible solution that can adapt to the specific needs of each instance. Our key insight is that by identifying and separating shared knowledge and task-specific knowledge, and then dynamically integrating them, we can mitigate the parameter interference problem to a great extent. We conduct the conventional multi-task model merging experiments and evaluate the generalization and robustness of our method. The results demonstrate the effectiveness of our method and provide a comprehensive understanding of our method. The code is available at https://anonymous.4open.science/r/weight-ensembling_MoE-67C9/","sentences":["Merging various task-specific Transformer-based models trained on different tasks into a single unified model can execute all the tasks concurrently.","Previous methods, exemplified by task arithmetic, have been proven to be both effective and scalable.","Existing methods have primarily focused on seeking a static optimal solution within the original model parameter space.","A notable challenge is mitigating the interference between parameters of different models, which can substantially deteriorate performance.","In this paper, we propose to merge most of the parameters while upscaling the MLP of the Transformer layers to a weight-ensembling mixture of experts (MoE) module, which can dynamically integrate shared and task-specific knowledge based on the input, thereby providing a more flexible solution that can adapt to the specific needs of each instance.","Our key insight is that by identifying and separating shared knowledge and task-specific knowledge, and then dynamically integrating them, we can mitigate the parameter interference problem to a great extent.","We conduct the conventional multi-task model merging experiments and evaluate the generalization and robustness of our method.","The results demonstrate the effectiveness of our method and provide a comprehensive understanding of our method.","The code is available at https://anonymous.4open.science/r/weight-ensembling_MoE-67C9/"],"url":"http://arxiv.org/abs/2402.00433v1","category":"cs.LG"}
{"created":"2024-02-01 08:58:28","title":"Density fluctuations for Squeezed Number State and Coherent Squeezed Number State in Flat FRW Universe","abstract":"We study the density fluctuations for Coherent Squeezed Number State (CSNS) and Squeezed Number States (SNS) formalism in Semiclassical theory of grav ity in flat FRW universe. We used Number state evolution of oscillatory phase of inflaton for coherent squeezed number state and squeezed number states for malisms. We analyzed that density fluctuations for SNS depends upon squeezing parameter and number state while for CSNS density fluctuations depends upon squeezing parameter, number state and coherent state parameter. These param eters plays an important role for quantum consideration of SNS and CSNS. The results of the analysis shows that increase in density fluctuations for both SNS and CSNS, demonstrate quantum behavior of SCEE as well as production of various kind of particles in these states.","sentences":["We study the density fluctuations for Coherent Squeezed Number State (CSNS) and Squeezed Number States (SNS) formalism in Semiclassical theory of grav ity in flat FRW universe.","We used Number state evolution of oscillatory phase of inflaton for coherent squeezed number state and squeezed number states for malisms.","We analyzed that density fluctuations for SNS depends upon squeezing parameter and number state while for CSNS density fluctuations depends upon squeezing parameter, number state and coherent state parameter.","These param eters plays an important role for quantum consideration of SNS and CSNS.","The results of the analysis shows that increase in density fluctuations for both SNS and CSNS, demonstrate quantum behavior of SCEE as well as production of various kind of particles in these states."],"url":"http://arxiv.org/abs/2402.00432v1","category":"gr-qc"}
{"created":"2024-02-01 08:37:13","title":"From PARIS to LE-PARIS: Toward Patent Response Automation with Recommender Systems and Collaborative Large Language Models","abstract":"In patent prosecution, timely and effective responses to Office Actions (OAs) are crucial for acquiring patents, yet past automation and AI research have scarcely addressed this aspect. To address this gap, our study introduces the Patent Office Action Response Intelligence System (PARIS) and its advanced version, the Large Language Model Enhanced PARIS (LE-PARIS). These systems are designed to expedite the efficiency of patent attorneys in collaboratively handling OA responses. The systems' key features include the construction of an OA Topics Database, development of Response Templates, and implementation of Recommender Systems and LLM-based Response Generation. Our validation involves a multi-paradigmatic analysis using the USPTO Office Action database and longitudinal data of attorney interactions with our systems over six years. Through five studies, we examine the constructiveness of OA topics (studies 1 and 2) using topic modeling and the proposed Delphi process, the efficacy of our proposed hybrid recommender system tailored for OA (both LLM-based and non-LLM-based) (study 3), the quality of response generation (study 4), and the practical value of the systems in real-world scenarios via user studies (study 5). Results demonstrate that both PARIS and LE-PARIS significantly meet key metrics and positively impact attorney performance.","sentences":["In patent prosecution, timely and effective responses to Office Actions (OAs) are crucial for acquiring patents, yet past automation and AI research have scarcely addressed this aspect.","To address this gap, our study introduces the Patent Office Action Response Intelligence System (PARIS) and its advanced version, the Large Language Model Enhanced PARIS (LE-PARIS).","These systems are designed to expedite the efficiency of patent attorneys in collaboratively handling OA responses.","The systems' key features include the construction of an OA Topics Database, development of Response Templates, and implementation of Recommender Systems and LLM-based Response Generation.","Our validation involves a multi-paradigmatic analysis using the USPTO Office Action database and longitudinal data of attorney interactions with our systems over six years.","Through five studies, we examine the constructiveness of OA topics (studies 1 and 2) using topic modeling and the proposed Delphi process, the efficacy of our proposed hybrid recommender system tailored for OA (both LLM-based and non-LLM-based) (study 3), the quality of response generation (study 4), and the practical value of the systems in real-world scenarios via user studies (study 5).","Results demonstrate that both PARIS and LE-PARIS significantly meet key metrics and positively impact attorney performance."],"url":"http://arxiv.org/abs/2402.00421v1","category":"cs.CL"}
{"created":"2024-02-01 08:36:53","title":"Anisotropic Einstein universes with a global magetic field and SqK-spinors","abstract":"We consider an Einstein-Dirac-Maxwell system with two charged massless spinors coupled with an electromagnetic field, and construct a family of exact solutions to the system. The solution spacetime is an anisotropic generalization of the static Einstein universe which has a global cosmic magnetic field generated by the current of the spinors. The spacetime is regarded as a toy model which describes global cosmic magnetic phenomena in the universe. The spinors are induced from Sasakian quasi-Killing spinors, and the total Dirac current flows along fibers of the Hopf-fibration. The magnetic field is a contact magnetic field.","sentences":["We consider an Einstein-Dirac-Maxwell system with two charged massless spinors coupled with an electromagnetic field, and construct a family of exact solutions to the system.","The solution spacetime is an anisotropic generalization of the static Einstein universe which has a global cosmic magnetic field generated by the current of the spinors.","The spacetime is regarded as a toy model which describes global cosmic magnetic phenomena in the universe.","The spinors are induced from Sasakian quasi-Killing spinors, and the total Dirac current flows along fibers of the Hopf-fibration.","The magnetic field is a contact magnetic field."],"url":"http://arxiv.org/abs/2402.00420v1","category":"hep-th"}
{"created":"2024-02-01 08:36:16","title":"Short: Benchmarking transferable adversarial attacks","abstract":"The robustness of deep learning models against adversarial attacks remains a pivotal concern. This study presents, for the first time, an exhaustive review of the transferability aspect of adversarial attacks. It systematically categorizes and critically evaluates various methodologies developed to augment the transferability of adversarial attacks. This study encompasses a spectrum of techniques, including Generative Structure, Semantic Similarity, Gradient Editing, Target Modification, and Ensemble Approach. Concurrently, this paper introduces a benchmark framework \\textit{TAA-Bench}, integrating ten leading methodologies for adversarial attack transferability, thereby providing a standardized and systematic platform for comparative analysis across diverse model architectures. Through comprehensive scrutiny, we delineate the efficacy and constraints of each method, shedding light on their underlying operational principles and practical utility. This review endeavors to be a quintessential resource for both scholars and practitioners in the field, charting the complex terrain of adversarial transferability and setting a foundation for future explorations in this vital sector. The associated codebase is accessible at: https://github.com/KxPlaug/TAA-Bench","sentences":["The robustness of deep learning models against adversarial attacks remains a pivotal concern.","This study presents, for the first time, an exhaustive review of the transferability aspect of adversarial attacks.","It systematically categorizes and critically evaluates various methodologies developed to augment the transferability of adversarial attacks.","This study encompasses a spectrum of techniques, including Generative Structure, Semantic Similarity, Gradient Editing, Target Modification, and Ensemble Approach.","Concurrently, this paper introduces a benchmark framework \\textit{TAA-Bench}, integrating ten leading methodologies for adversarial attack transferability, thereby providing a standardized and systematic platform for comparative analysis across diverse model architectures.","Through comprehensive scrutiny, we delineate the efficacy and constraints of each method, shedding light on their underlying operational principles and practical utility.","This review endeavors to be a quintessential resource for both scholars and practitioners in the field, charting the complex terrain of adversarial transferability and setting a foundation for future explorations in this vital sector.","The associated codebase is accessible at: https://github.com/KxPlaug/TAA-Bench"],"url":"http://arxiv.org/abs/2402.00418v1","category":"cs.CV"}
{"created":"2024-02-01 08:34:39","title":"Presentation of monoids generated by a projection and an involution","abstract":"Monoids generated by elements of order two appear in numerous places in the literature. For example, Coxeter reflection groups in geometry, Kuratowski monoids in topology, various monoids generated by regular operations in language theory and so on. In order to initiate a classification of these monoids, we are interested in the subproblem of monoids, called strict 2-PIMs, generated by an involution and an idempotent. In this case we show, when the monoid is finite, that it is generated by a single equation (in addition to the two defining the involution and the idempotent). We then describe the exact possible forms of this equation and classify them. We recover Kuratowski's theorem as a special case of our study.","sentences":["Monoids generated by elements of order two appear in numerous places in the literature.","For example, Coxeter reflection groups in geometry, Kuratowski monoids in topology, various monoids generated by regular operations in language theory and so on.","In order to initiate a classification of these monoids, we are interested in the subproblem of monoids, called strict 2-PIMs, generated by an involution and an idempotent.","In this case we show, when the monoid is finite, that it is generated by a single equation (in addition to the two defining the involution and the idempotent).","We then describe the exact possible forms of this equation and classify them.","We recover Kuratowski's theorem as a special case of our study."],"url":"http://arxiv.org/abs/2402.00417v1","category":"math.GR"}
{"created":"2024-02-01 08:15:28","title":"Prompt-Time Symbolic Knowledge Capture with Large Language Models","abstract":"Augmenting large language models (LLMs) with user-specific knowledge is crucial for real-world applications, such as personal AI assistants. However, LLMs inherently lack mechanisms for prompt-driven knowledge capture. This paper investigates utilizing the existing LLM capabilities to enable prompt-driven knowledge capture, with a particular emphasis on knowledge graphs. We address this challenge by focusing on prompt-to-triple (P2T) generation. We explore three methods: zero-shot prompting, few-shot prompting, and fine-tuning, and then assess their performance via a specialized synthetic dataset. Our code and datasets are publicly available at https://github.com/HaltiaAI/paper-PTSKC.","sentences":["Augmenting large language models (LLMs) with user-specific knowledge is crucial for real-world applications, such as personal AI assistants.","However, LLMs inherently lack mechanisms for prompt-driven knowledge capture.","This paper investigates utilizing the existing LLM capabilities to enable prompt-driven knowledge capture, with a particular emphasis on knowledge graphs.","We address this challenge by focusing on prompt-to-triple (P2T) generation.","We explore three methods: zero-shot prompting, few-shot prompting, and fine-tuning, and then assess their performance via a specialized synthetic dataset.","Our code and datasets are publicly available at https://github.com/HaltiaAI/paper-PTSKC."],"url":"http://arxiv.org/abs/2402.00414v1","category":"cs.CL"}
{"created":"2024-02-01 08:11:56","title":"Hidding the Ghostwriters: An Adversarial Evaluation of AI-Generated Student Essay Detection","abstract":"Large language models (LLMs) have exhibited remarkable capabilities in text generation tasks. However, the utilization of these models carries inherent risks, including but not limited to plagiarism, the dissemination of fake news, and issues in educational exercises. Although several detectors have been proposed to address these concerns, their effectiveness against adversarial perturbations, specifically in the context of student essay writing, remains largely unexplored. This paper aims to bridge this gap by constructing AIG-ASAP, an AI-generated student essay dataset, employing a range of text perturbation methods that are expected to generate high-quality essays while evading detection. Through empirical experiments, we assess the performance of current AIGC detectors on the AIG-ASAP dataset. The results reveal that the existing detectors can be easily circumvented using straightforward automatic adversarial attacks. Specifically, we explore word substitution and sentence substitution perturbation methods that effectively evade detection while maintaining the quality of the generated essays. This highlights the urgent need for more accurate and robust methods to detect AI-generated student essays in the education domain.","sentences":["Large language models (LLMs) have exhibited remarkable capabilities in text generation tasks.","However, the utilization of these models carries inherent risks, including but not limited to plagiarism, the dissemination of fake news, and issues in educational exercises.","Although several detectors have been proposed to address these concerns, their effectiveness against adversarial perturbations, specifically in the context of student essay writing, remains largely unexplored.","This paper aims to bridge this gap by constructing AIG-ASAP, an AI-generated student essay dataset, employing a range of text perturbation methods that are expected to generate high-quality essays while evading detection.","Through empirical experiments, we assess the performance of current AIGC detectors on the AIG-ASAP dataset.","The results reveal that the existing detectors can be easily circumvented using straightforward automatic adversarial attacks.","Specifically, we explore word substitution and sentence substitution perturbation methods that effectively evade detection while maintaining the quality of the generated essays.","This highlights the urgent need for more accurate and robust methods to detect AI-generated student essays in the education domain."],"url":"http://arxiv.org/abs/2402.00412v1","category":"cs.CL"}
{"created":"2024-02-01 08:10:39","title":"LM-HT SNN: Enhancing the Performance of SNN to ANN Counterpart through Learnable Multi-hierarchical Threshold Model","abstract":"Compared to traditional Artificial Neural Network (ANN), Spiking Neural Network (SNN) has garnered widespread academic interest for its intrinsic ability to transmit information in a more biological-inspired and energy-efficient manner. However, despite previous efforts to optimize the learning gradients and model structure of SNNs through various methods, SNNs still lag behind ANNs in terms of performance to some extent. The recently proposed multi-threshold model provides more possibilities for further enhancing the learning capability of SNNs. In this paper, we rigorously analyze the relationship among the multi-threshold model, vanilla spiking model and quantized ANNs from a mathematical perspective, then propose a novel LM-HT model, which is an equidistant multi-hierarchical model that can dynamically regulate the global input current and membrane potential leakage on the time dimension. In addition, we note that the direct training algorithm based on the LM-HT model can seamlessly integrate with the traditional ANN-SNN Conversion framework. This novel hybrid learning framework can effectively improve the relatively poor performance of converted SNNs under low time latency. Extensive experimental results have demonstrated that our LM-HT model can significantly outperform previous state-of-the-art works on various types of datasets, which promote SNNs to achieve a brand-new level of performance comparable to quantized ANNs.","sentences":["Compared to traditional Artificial Neural Network (ANN), Spiking Neural Network (SNN) has garnered widespread academic interest for its intrinsic ability to transmit information in a more biological-inspired and energy-efficient manner.","However, despite previous efforts to optimize the learning gradients and model structure of SNNs through various methods, SNNs still lag behind ANNs in terms of performance to some extent.","The recently proposed multi-threshold model provides more possibilities for further enhancing the learning capability of SNNs.","In this paper, we rigorously analyze the relationship among the multi-threshold model, vanilla spiking model and quantized ANNs from a mathematical perspective, then propose a novel LM-HT model, which is an equidistant multi-hierarchical model that can dynamically regulate the global input current and membrane potential leakage on the time dimension.","In addition, we note that the direct training algorithm based on the LM-HT model can seamlessly integrate with the traditional ANN-SNN Conversion framework.","This novel hybrid learning framework can effectively improve the relatively poor performance of converted SNNs under low time latency.","Extensive experimental results have demonstrated that our LM-HT model can significantly outperform previous state-of-the-art works on various types of datasets, which promote SNNs to achieve a brand-new level of performance comparable to quantized ANNs."],"url":"http://arxiv.org/abs/2402.00411v1","category":"cs.NE"}
{"created":"2024-02-01 08:02:10","title":"InfMAE: A Foundation Model in Infrared Modality","abstract":"In recent years, the foundation models have swept the computer vision field and facilitated the development of various tasks within different modalities. However, it remains an open question on how to design an infrared foundation model. In this paper, we propose InfMAE, a foundation model in infrared modality. We release an infrared dataset, called Inf30 to address the problem of lacking large-scale data for self-supervised learning in the infrared vision community. Besides, we design an information-aware masking strategy, which is suitable for infrared images. This masking strategy allows for a greater emphasis on the regions with richer information in infrared images during the self-supervised learning process, which is conducive to learning the generalized representation. In addition, we adopt a multi-scale encoder to enhance the performance of the pre-trained encoders in downstream tasks. Finally, based on the fact that infrared images do not have a lot of details and texture information, we design an infrared decoder module, which further improves the performance of downstream tasks. Extensive experiments show that our proposed method InfMAE outperforms other supervised methods and self-supervised learning methods in three downstream tasks. Our code will be made public at https://github.com/liufangcen/InfMAE.","sentences":["In recent years, the foundation models have swept the computer vision field and facilitated the development of various tasks within different modalities.","However, it remains an open question on how to design an infrared foundation model.","In this paper, we propose InfMAE, a foundation model in infrared modality.","We release an infrared dataset, called Inf30 to address the problem of lacking large-scale data for self-supervised learning in the infrared vision community.","Besides, we design an information-aware masking strategy, which is suitable for infrared images.","This masking strategy allows for a greater emphasis on the regions with richer information in infrared images during the self-supervised learning process, which is conducive to learning the generalized representation.","In addition, we adopt a multi-scale encoder to enhance the performance of the pre-trained encoders in downstream tasks.","Finally, based on the fact that infrared images do not have a lot of details and texture information, we design an infrared decoder module, which further improves the performance of downstream tasks.","Extensive experiments show that our proposed method InfMAE outperforms other supervised methods and self-supervised learning methods in three downstream tasks.","Our code will be made public at https://github.com/liufangcen/InfMAE."],"url":"http://arxiv.org/abs/2402.00407v1","category":"cs.CV"}
{"created":"2024-02-01 08:01:35","title":"A rigorous integrator and global existence for higher-dimensional semilinear parabolic PDEs via semigroup theory","abstract":"In this paper, we introduce a general constructive method to compute solutions of initial value problems of semilinear parabolic partial differential equations via semigroup theory and computer-assisted proofs. Once a numerical candidate for the solution is obtained via a finite dimensional projection, Chebyshev series expansions are used to solve the linearized equations about the approximation from which a solution map operator is constructed. Using the solution operator (which exists from semigroup theory), we define an infinite dimensional contraction operator whose unique fixed point together with its rigorous bounds provide the local inclusion of the solution. Applying this technique for multiple time steps leads to constructive proofs of existence of solutions over long time intervals. As applications, we study the 3D/2D Swift-Hohenberg, where we combine our method with explicit constructions of trapping regions to prove global existence of solutions of initial value problems converging asymptotically to nontrivial equilibria. A second application consists of the 2D Ohta-Kawasaki equation, providing a framework for handling derivatives in nonlinear terms.","sentences":["In this paper, we introduce a general constructive method to compute solutions of initial value problems of semilinear parabolic partial differential equations via semigroup theory and computer-assisted proofs.","Once a numerical candidate for the solution is obtained via a finite dimensional projection, Chebyshev series expansions are used to solve the linearized equations about the approximation from which a solution map operator is constructed.","Using the solution operator (which exists from semigroup theory), we define an infinite dimensional contraction operator whose unique fixed point together with its rigorous bounds provide the local inclusion of the solution.","Applying this technique for multiple time steps leads to constructive proofs of existence of solutions over long time intervals.","As applications, we study the 3D/2D Swift-Hohenberg, where we combine our method with explicit constructions of trapping regions to prove global existence of solutions of initial value problems converging asymptotically to nontrivial equilibria.","A second application consists of the 2D Ohta-Kawasaki equation, providing a framework for handling derivatives in nonlinear terms."],"url":"http://arxiv.org/abs/2402.00406v1","category":"math.AP"}
{"created":"2024-02-01 07:48:50","title":"Investigating Bias Representations in Llama 2 Chat via Activation Steering","abstract":"We address the challenge of societal bias in Large Language Models (LLMs), focusing on the Llama 2 7B Chat model. As LLMs are increasingly integrated into decision-making processes with substantial societal impact, it becomes imperative to ensure these models do not reinforce existing biases. Our approach employs activation steering to probe for and mitigate biases related to gender, race, and religion. This method manipulates model activations to direct responses towards or away from biased outputs, utilizing steering vectors derived from the StereoSet dataset and custom GPT4 generated gender bias prompts. Our findings reveal inherent gender bias in Llama 2 7B Chat, persisting even after Reinforcement Learning from Human Feedback (RLHF). We also observe a predictable negative correlation between bias and the model's tendency to refuse responses. Significantly, our study uncovers that RLHF tends to increase the similarity in the model's representation of different forms of societal biases, which raises questions about the model's nuanced understanding of different forms of bias. This work also provides valuable insights into effective red-teaming strategies for LLMs using activation steering, particularly emphasizing the importance of integrating a refusal vector.","sentences":["We address the challenge of societal bias in Large Language Models (LLMs), focusing on the Llama 2 7B Chat model.","As LLMs are increasingly integrated into decision-making processes with substantial societal impact, it becomes imperative to ensure these models do not reinforce existing biases.","Our approach employs activation steering to probe for and mitigate biases related to gender, race, and religion.","This method manipulates model activations to direct responses towards or away from biased outputs, utilizing steering vectors derived from the StereoSet dataset and custom GPT4 generated gender bias prompts.","Our findings reveal inherent gender bias in Llama 2 7B Chat, persisting even after Reinforcement Learning from Human Feedback (RLHF).","We also observe a predictable negative correlation between bias and the model's tendency to refuse responses.","Significantly, our study uncovers that RLHF tends to increase the similarity in the model's representation of different forms of societal biases, which raises questions about the model's nuanced understanding of different forms of bias.","This work also provides valuable insights into effective red-teaming strategies for LLMs using activation steering, particularly emphasizing the importance of integrating a refusal vector."],"url":"http://arxiv.org/abs/2402.00402v1","category":"cs.CL"}
{"created":"2024-02-01 07:41:31","title":"Reconfigurable Intelligent Computational Surfaces for MEC-Assisted Autonomous Driving Networks","abstract":"In this paper, we focus on improving autonomous driving safety via task offloading from cellular vehicles (CVs), using vehicle-to-infrastructure (V2I) links, to an multi-access edge computing (MEC) server. Considering that the frequencies used for V2I links can be reused for vehicle-to-vehicle (V2V) communications to improve spectrum utilization, the receiver of each V2I link may suffer from severe interference, causing outages in the task offloading process. To tackle this issue, we propose the deployment of a reconfigurable intelligent computational surface (RICS) to enable, not only V2I reflective links, but also interference cancellation at the V2V links exploiting the computational capability of its metamaterials. We devise a joint optimization formulation for the task offloading ratio between the CVs and the MEC server, the spectrum sharing strategy between V2V and V2I communications, as well as the RICS reflection and refraction matrices, with the objective to maximize a safety-based autonomous driving task. Due to the non-convexity of the problem and the coupling among its free variables, we transform it into a more tractable equivalent form, which is then decomposed into three sub-problems and solved via an alternate approximation method. Our simulation results demonstrate the effectiveness of the proposed RICS optimization in improving the safety in autonomous driving networks.","sentences":["In this paper, we focus on improving autonomous driving safety via task offloading from cellular vehicles (CVs), using vehicle-to-infrastructure (V2I) links, to an multi-access edge computing (MEC) server.","Considering that the frequencies used for V2I links can be reused for vehicle-to-vehicle (V2V) communications to improve spectrum utilization, the receiver of each V2I link may suffer from severe interference, causing outages in the task offloading process.","To tackle this issue, we propose the deployment of a reconfigurable intelligent computational surface (RICS) to enable, not only V2I reflective links, but also interference cancellation at the V2V links exploiting the computational capability of its metamaterials.","We devise a joint optimization formulation for the task offloading ratio between the CVs and the MEC server, the spectrum sharing strategy between V2V and V2I communications, as well as the RICS reflection and refraction matrices, with the objective to maximize a safety-based autonomous driving task.","Due to the non-convexity of the problem and the coupling among its free variables, we transform it into a more tractable equivalent form, which is then decomposed into three sub-problems and solved via an alternate approximation method.","Our simulation results demonstrate the effectiveness of the proposed RICS optimization in improving the safety in autonomous driving networks."],"url":"http://arxiv.org/abs/2402.00398v1","category":"cs.DC"}
{"created":"2024-02-01 07:33:31","title":"Multi-scale Traffic Pattern Bank for Cross-city Few-shot Traffic Forecasting","abstract":"Traffic forecasting is crucial for intelligent transportation systems (ITS), aiding in efficient resource allocation and effective traffic control. However, its effectiveness often relies heavily on abundant traffic data, while many cities lack sufficient data due to limited device support, posing a significant challenge for traffic forecasting. Recognizing this challenge, we have made a noteworthy observation: traffic patterns exhibit similarities across diverse cities. Building on this key insight, we propose a solution for the cross-city few-shot traffic forecasting problem called Multi-scale Traffic Pattern Bank (MTPB). Primarily, MTPB initiates its learning process by leveraging data-rich source cities, effectively acquiring comprehensive traffic knowledge through a spatial-temporal-aware pre-training process. Subsequently, the framework employs advanced clustering techniques to systematically generate a multi-scale traffic pattern bank derived from the learned knowledge. Next, the traffic data of the data-scarce target city could query the traffic pattern bank, facilitating the aggregation of meta-knowledge. This meta-knowledge, in turn, assumes a pivotal role as a robust guide in subsequent processes involving graph reconstruction and forecasting. Empirical assessments conducted on real-world traffic datasets affirm the superior performance of MTPB, surpassing existing methods across various categories and exhibiting numerous attributes conducive to the advancement of cross-city few-shot forecasting methodologies. The code is available in https://github.com/zhyliu00/MTPB.","sentences":["Traffic forecasting is crucial for intelligent transportation systems (ITS), aiding in efficient resource allocation and effective traffic control.","However, its effectiveness often relies heavily on abundant traffic data, while many cities lack sufficient data due to limited device support, posing a significant challenge for traffic forecasting.","Recognizing this challenge, we have made a noteworthy observation: traffic patterns exhibit similarities across diverse cities.","Building on this key insight, we propose a solution for the cross-city few-shot traffic forecasting problem called Multi-scale Traffic Pattern Bank (MTPB).","Primarily, MTPB initiates its learning process by leveraging data-rich source cities, effectively acquiring comprehensive traffic knowledge through a spatial-temporal-aware pre-training process.","Subsequently, the framework employs advanced clustering techniques to systematically generate a multi-scale traffic pattern bank derived from the learned knowledge.","Next, the traffic data of the data-scarce target city could query the traffic pattern bank, facilitating the aggregation of meta-knowledge.","This meta-knowledge, in turn, assumes a pivotal role as a robust guide in subsequent processes involving graph reconstruction and forecasting.","Empirical assessments conducted on real-world traffic datasets affirm the superior performance of MTPB, surpassing existing methods across various categories and exhibiting numerous attributes conducive to the advancement of cross-city few-shot forecasting methodologies.","The code is available in https://github.com/zhyliu00/MTPB."],"url":"http://arxiv.org/abs/2402.00397v1","category":"cs.LG"}
{"created":"2024-02-01 07:32:24","title":"Efficient Exploration for LLMs","abstract":"We present evidence of substantial benefit from efficient exploration in gathering human feedback to improve large language models. In our experiments, an agent sequentially generates queries while fitting a reward model to the feedback received. Our best-performing agent generates queries using double Thompson sampling, with uncertainty represented by an epistemic neural network. Our results demonstrate that efficient exploration enables high levels of performance with far fewer queries. Further, both uncertainty estimation and the choice of exploration scheme play critical roles.","sentences":["We present evidence of substantial benefit from efficient exploration in gathering human feedback to improve large language models.","In our experiments, an agent sequentially generates queries while fitting a reward model to the feedback received.","Our best-performing agent generates queries using double Thompson sampling, with uncertainty represented by an epistemic neural network.","Our results demonstrate that efficient exploration enables high levels of performance with far fewer queries.","Further, both uncertainty estimation and the choice of exploration scheme play critical roles."],"url":"http://arxiv.org/abs/2402.00396v1","category":"cs.LG"}
{"created":"2024-02-01 07:32:00","title":"ONE-SA: Enabling Nonlinear Operations in Systolic Arrays for Efficient and Flexible Neural Network Inference","abstract":"The computation and memory-intensive nature of DNNs limits their use in many mobile and embedded contexts. Application-specific integrated circuit (ASIC) hardware accelerators employ matrix multiplication units (such as the systolic arrays) and dedicated nonlinear function units to speed up DNN computations. A close examination of these ASIC accelerators reveals that the designs are often specialized and lack versatility across different networks, especially when the networks have different types of computation. In this paper, we introduce a novel systolic array architecture, which is capable of executing nonlinear functions. By encompassing both inherent linear and newly enabled nonlinear functions within the systolic arrays, the proposed architecture facilitates versatile network inferences, substantially enhancing computational power and energy efficiency. Experimental results show that employing this systolic array enables seamless execution of entire DNNs, incurring only a negligible loss in the network inference accuracy. Furthermore, assessment and evaluation with FPGAs reveal that integrating nonlinear computation capacity into a systolic array does not introduce extra notable (less than 1.5%) block memory memories (BRAMs), look-up-tables (LUTs), or digital signal processors (DSPs) but a mere 13.3% - 24.1% more flip flops (FFs). In comparison to existing methodologies, executing the networks with the proposed systolic array, which enables the flexibility of different network models, yields up to 25.73x, 5.21x, and 1.54x computational efficiency when compared to general-purpose CPUs, GPUs, and SoCs respectively, while achieving comparable (83.4% - 135.8%) performance with the conventional accelerators which are designed for specific neural network models.","sentences":["The computation and memory-intensive nature of DNNs limits their use in many mobile and embedded contexts.","Application-specific integrated circuit (ASIC) hardware accelerators employ matrix multiplication units (such as the systolic arrays) and dedicated nonlinear function units to speed up DNN computations.","A close examination of these ASIC accelerators reveals that the designs are often specialized and lack versatility across different networks, especially when the networks have different types of computation.","In this paper, we introduce a novel systolic array architecture, which is capable of executing nonlinear functions.","By encompassing both inherent linear and newly enabled nonlinear functions within the systolic arrays, the proposed architecture facilitates versatile network inferences, substantially enhancing computational power and energy efficiency.","Experimental results show that employing this systolic array enables seamless execution of entire DNNs, incurring only a negligible loss in the network inference accuracy.","Furthermore, assessment and evaluation with FPGAs reveal that integrating nonlinear computation capacity into a systolic array does not introduce extra notable (less than 1.5%) block memory memories (BRAMs), look-up-tables (LUTs), or digital signal processors (DSPs) but a mere 13.3% - 24.1% more flip flops (FFs).","In comparison to existing methodologies, executing the networks with the proposed systolic array, which enables the flexibility of different network models, yields up to 25.73x, 5.21x, and 1.54x computational efficiency when compared to general-purpose CPUs, GPUs, and SoCs respectively, while achieving comparable (83.4% - 135.8%) performance with the conventional accelerators which are designed for specific neural network models."],"url":"http://arxiv.org/abs/2402.00395v1","category":"cs.AR"}
{"created":"2024-02-01 07:31:50","title":"Random partitions, potential of the Shapley value, and games with externalities","abstract":"The Shapley value equals a player's contribution to the potential of a game. The potential is a most natural one-number summary of a game, which can be computed as the expected accumulated worth of a random partition of the players. This computation integrates the coalition formation of all players and readily extends to games with externalities. We investigate those potential functions for games with externalities that can be computed this way. It turns out that the potential that corresponds to the MPW solution introduced by Macho-Stadler et al. (2007, J. Econ. Theory 135, 339-356), is unique in the following sense. It is obtained as a the expected accumulated worth of a random partition, it generalizes the potential for games without externalities, and it induces a solution that satisfies the null player property even in the presence of externalities.","sentences":["The Shapley value equals a player's contribution to the potential of a game.","The potential is a most natural one-number summary of a game, which can be computed as the expected accumulated worth of a random partition of the players.","This computation integrates the coalition formation of all players and readily extends to games with externalities.","We investigate those potential functions for games with externalities that can be computed this way.","It turns out that the potential that corresponds to the MPW solution introduced by Macho-Stadler et al.","(2007, J. Econ.","Theory 135, 339-356), is unique in the following sense.","It is obtained as a the expected accumulated worth of a random partition, it generalizes the potential for games without externalities, and it induces a solution that satisfies the null player property even in the presence of externalities."],"url":"http://arxiv.org/abs/2402.00394v1","category":"econ.TH"}
{"created":"2024-02-01 07:28:55","title":"Loss Function Considering Dead Zone for Neural Networks","abstract":"It is important to reveal the inverse dynamics of manipulators to improve control performance of model-based control. Neural networks (NNs) are promising techniques to represent complicated inverse dynamics while they require a large amount of motion data. However, motion data in dead zones of actuators is not suitable for training models decreasing the number of useful training data. In this study, based on the fact that the manipulator joint does not work irrespective of input torque in dead zones, we propose a new loss function that considers only errors of joints not in dead zones. The proposed method enables to increase in the amount of motion data available for training and the accuracy of the inverse dynamics computation. Experiments on actual equipment using a three-degree-of-freedom (DOF) manipulator showed higher accuracy than conventional methods. We also confirmed and discussed the behavior of the model of the proposed method in dead zones.","sentences":["It is important to reveal the inverse dynamics of manipulators to improve control performance of model-based control.","Neural networks (NNs) are promising techniques to represent complicated inverse dynamics while they require a large amount of motion data.","However, motion data in dead zones of actuators is not suitable for training models decreasing the number of useful training data.","In this study, based on the fact that the manipulator joint does not work irrespective of input torque in dead zones, we propose a new loss function that considers only errors of joints not in dead zones.","The proposed method enables to increase in the amount of motion data available for training and the accuracy of the inverse dynamics computation.","Experiments on actual equipment using a three-degree-of-freedom (DOF) manipulator showed higher accuracy than conventional methods.","We also confirmed and discussed the behavior of the model of the proposed method in dead zones."],"url":"http://arxiv.org/abs/2402.00393v1","category":"cs.RO"}
{"created":"2024-02-01 07:22:52","title":"EASRec: Elastic Architecture Search for Efficient Long-term Sequential Recommender Systems","abstract":"In this age where data is abundant, the ability to distill meaningful insights from the sea of information is essential. Our research addresses the computational and resource inefficiencies that current Sequential Recommender Systems (SRSs) suffer from. especially those employing attention-based models like SASRec, These systems are designed for next-item recommendations in various applications, from e-commerce to social networks. However, such systems suffer from substantial computational costs and resource consumption during the inference stage. To tackle these issues, our research proposes a novel method that combines automatic pruning techniques with advanced model architectures. We also explore the potential of resource-constrained Neural Architecture Search (NAS), a technique prevalent in the realm of recommendation systems, to fine-tune models for reduced FLOPs, latency, and energy usage while retaining or even enhancing accuracy. The main contribution of our work is developing the Elastic Architecture Search for Efficient Long-term Sequential Recommender Systems (EASRec). This approach aims to find optimal compact architectures for attention-based SRSs, ensuring accuracy retention. EASRec introduces data-aware gates that leverage historical information from input data batch to improve the performance of the recommendation network. Additionally, it utilizes a dynamic resource constraint approach, which standardizes the search process and results in more appropriate architectures. The effectiveness of our methodology is validated through exhaustive experiments on three benchmark datasets, which demonstrates EASRec's superiority in SRSs. Our research set a new standard for future exploration into efficient and accurate recommender systems, signifying a substantial advancement within this swiftly advancing field.","sentences":["In this age where data is abundant, the ability to distill meaningful insights from the sea of information is essential.","Our research addresses the computational and resource inefficiencies that current Sequential Recommender Systems (SRSs) suffer from.","especially those employing attention-based models like SASRec, These systems are designed for next-item recommendations in various applications, from e-commerce to social networks.","However, such systems suffer from substantial computational costs and resource consumption during the inference stage.","To tackle these issues, our research proposes a novel method that combines automatic pruning techniques with advanced model architectures.","We also explore the potential of resource-constrained Neural Architecture Search (NAS), a technique prevalent in the realm of recommendation systems, to fine-tune models for reduced FLOPs, latency, and energy usage while retaining or even enhancing accuracy.","The main contribution of our work is developing the Elastic Architecture Search for Efficient Long-term Sequential Recommender Systems (EASRec).","This approach aims to find optimal compact architectures for attention-based SRSs, ensuring accuracy retention.","EASRec introduces data-aware gates that leverage historical information from input data batch to improve the performance of the recommendation network.","Additionally, it utilizes a dynamic resource constraint approach, which standardizes the search process and results in more appropriate architectures.","The effectiveness of our methodology is validated through exhaustive experiments on three benchmark datasets, which demonstrates EASRec's superiority in SRSs.","Our research set a new standard for future exploration into efficient and accurate recommender systems, signifying a substantial advancement within this swiftly advancing field."],"url":"http://arxiv.org/abs/2402.00390v1","category":"cs.IR"}
{"created":"2024-02-01 07:21:32","title":"On the $O(\\frac{\\sqrt{d}}{T^{1/4}})$ Convergence Rate of RMSProp and Its Momentum Extension Measured by $\\ell_1$ Norm: Better Dependence on the Dimension","abstract":"Although adaptive gradient methods have been extensively used in deep learning, their convergence rates have not been thoroughly studied, particularly with respect to their dependence on the dimension. This paper considers the classical RMSProp and its momentum extension and establishes the convergence rate of $\\frac{1}{T}\\sum_{k=1}^TE\\left[\\|\\nabla f(x^k)\\|_1\\right]\\leq O(\\frac{\\sqrt{d}}{T^{1/4}})$ measured by $\\ell_1$ norm without the bounded gradient assumption, where $d$ is the dimension of the optimization variable and $T$ is the iteration number. Since $\\|x\\|_2\\ll\\|x\\|_1\\leq\\sqrt{d}\\|x\\|_2$ for problems with extremely large $d$, our convergence rate can be considered to be analogous to the $\\frac{1}{T}\\sum_{k=1}^TE\\left[\\|\\nabla f(x^k)\\|_2\\right]\\leq O(\\frac{1}{T^{1/4}})$ one of SGD measured by $\\ell_1$ norm.","sentences":["Although adaptive gradient methods have been extensively used in deep learning, their convergence rates have not been thoroughly studied, particularly with respect to their dependence on the dimension.","This paper considers the classical RMSProp and its momentum extension and establishes the convergence rate of $\\frac{1}{T}\\sum_{k=1}^TE\\left[\\|\\nabla f(x^k)\\|_1\\right]\\leq O(\\frac{\\sqrt{d}}{T^{1/4}})$ measured by $\\ell_1$ norm without the bounded gradient assumption, where $d$ is the dimension of the optimization variable and $T$ is the iteration number.","Since $\\|x\\|_2\\ll\\|x\\|_1\\leq\\sqrt{d}\\|x\\|_2$ for problems with extremely large $d$, our convergence rate can be considered to be analogous to the $\\frac{1}{T}\\sum_{k=1}^TE\\left[\\|\\nabla f(x^k)\\|_2\\right]\\leq O(\\frac{1}{T^{1/4}})$ one of SGD measured by $\\ell_1$ norm."],"url":"http://arxiv.org/abs/2402.00389v1","category":"math.OC"}
{"created":"2024-02-01 17:08:10","title":"Transitive $(q-1)$-fold packings of $\\rm{PG}_n(q)$","abstract":"A $t$-fold packing of a projective space $\\rm{PG}_n(q)$ is a collection $\\mathcal{P}$ of line-spreads such that each line of $\\rm{PG}_n(q)$ occurs in precisely $t$ spreads in $\\mathcal{P}$. A $t$-fold packing $\\mathcal{P}$ is transitive if a subgroup of $\\rm{P\\Gamma L}_{n+1}(q)$ preserves and acts transitively on $\\mathcal{P}$. We give a construction for a transitive $(q-1)$-fold packing of $\\rm{PG}_n(q)$, where $q=2^k$, for any odd positive integers $n$ and $k$, such that $n\\geq 3$. This generalises a construction of Baker from 1976 for the case $q=2$.","sentences":["A $t$-fold packing of a projective space $\\rm{PG}_n(q)$ is a collection $\\mathcal{P}$ of line-spreads such that each line of $\\rm{PG}_n(q)$ occurs in precisely $t$ spreads in $\\mathcal{P}$. A $t$-fold packing $\\mathcal{P}$ is transitive if a subgroup of $\\rm{P\\Gamma L}_{n+1}(q)$ preserves and acts transitively on $\\mathcal{P}$. We give a construction for a transitive $(q-1)$-fold packing of $\\rm{PG}_n(q)$, where $q=2^k$, for any odd positive integers $n$ and $k$, such that $n\\geq 3$.","This generalises a construction of Baker from 1976 for the case $q=2$."],"url":"http://arxiv.org/abs/2402.00780v1","category":"math.CO"}
{"created":"2024-02-01 16:05:22","title":"CP Violation in B Decays: Recent Developments and Future Perspectives","abstract":"CP violation in B decays provides a powerful tool to probe physics from beyond the Standard Model. A theoretical overview of recent developments of benchmark channels is given, ranging from non-leptonic to rare leptonic and semileptonic modes, opening up exciting perspectives for the future high-precision era of flavour physics and the pursuit of New Physics.","sentences":["CP violation in B decays provides a powerful tool to probe physics from beyond the Standard Model.","A theoretical overview of recent developments of benchmark channels is given, ranging from non-leptonic to rare leptonic and semileptonic modes, opening up exciting perspectives for the future high-precision era of flavour physics and the pursuit of New Physics."],"url":"http://arxiv.org/abs/2402.00710v1","category":"hep-ph"}
{"created":"2024-02-01 16:05:15","title":"Towards an autonomous industry 4.0 warehouse: A UAV and blockchain-based system for inventory and traceability applications in big data-driven supply chain management","abstract":"In this paper we present the design and evaluation of a UAV-based system aimed at automating inventory tasks and keeping the traceability of industrial items attached to Radio-Frequency IDentification (RFID) tags. To confront current shortcomings, such a system is developed under a versatile, modular and scalable architecture aimed to reinforce cyber security and decentralization while fostering external audits and big data analytics. Therefore, the system uses a blockchain and a distributed ledger to store certain inventory data collected by UAVs, validate them, ensure their trustworthiness and make them available to the interested parties. In order to show the performance of the proposed system, different tests were performed in a real industrial warehouse, concluding that the system is able to obtain the inventory data really fast in comparison to traditional manual tasks, while being also able to estimate the position of the items when hovering over them thanks to their tag's signal strength. In addition, the performance of the proposed blockchain-based architecture was evaluated in different scenarios.","sentences":["In this paper we present the design and evaluation of a UAV-based system aimed at automating inventory tasks and keeping the traceability of industrial items attached to Radio-Frequency IDentification (RFID) tags.","To confront current shortcomings, such a system is developed under a versatile, modular and scalable architecture aimed to reinforce cyber security and decentralization while fostering external audits and big data analytics.","Therefore, the system uses a blockchain and a distributed ledger to store certain inventory data collected by UAVs, validate them, ensure their trustworthiness and make them available to the interested parties.","In order to show the performance of the proposed system, different tests were performed in a real industrial warehouse, concluding that the system is able to obtain the inventory data really fast in comparison to traditional manual tasks, while being also able to estimate the position of the items when hovering over them thanks to their tag's signal strength.","In addition, the performance of the proposed blockchain-based architecture was evaluated in different scenarios."],"url":"http://arxiv.org/abs/2402.00709v1","category":"cs.CR"}
{"created":"2024-02-01 15:55:25","title":"Time-Series Analysis Approach for Improving Energy Efficiency of a Fixed-Route Vessel in Short-Sea Shipping","abstract":"Several approaches have been developed for improving the ship energy efficiency, thereby reducing operating costs and ensuring compliance with climate change mitigation regulations. Many of these approaches will heavily depend on measured data from onboard IoT devices, including operational and environmental information, as well as external data sources for additional navigational data. In this paper, we develop a framework that implements time-series analysis techniques to optimize the vessel's speed profile for improving the vessel's energy efficiency. We present a case study involving a real-world data from a passenger vessel that was collected over a span of 15 months in the south of Sweden. The results indicate that the implemented models exhibit a range of outcomes and adaptability across different scenarios. The findings highlight the effectiveness of time-series analysis approach for optimizing vessel voyages within the context of constrained landscapes, as often seen in short-sea shipping.","sentences":["Several approaches have been developed for improving the ship energy efficiency, thereby reducing operating costs and ensuring compliance with climate change mitigation regulations.","Many of these approaches will heavily depend on measured data from onboard IoT devices, including operational and environmental information, as well as external data sources for additional navigational data.","In this paper, we develop a framework that implements time-series analysis techniques to optimize the vessel's speed profile for improving the vessel's energy efficiency.","We present a case study involving a real-world data from a passenger vessel that was collected over a span of 15 months in the south of Sweden.","The results indicate that the implemented models exhibit a range of outcomes and adaptability across different scenarios.","The findings highlight the effectiveness of time-series analysis approach for optimizing vessel voyages within the context of constrained landscapes, as often seen in short-sea shipping."],"url":"http://arxiv.org/abs/2402.00698v1","category":"cs.CE"}
{"created":"2024-02-01 15:31:01","title":"ECALL: Expectation-calibrated learning for unsupervised blind deconvolution","abstract":"Blind deconvolution aims to recover an original image from a blurred version in the case where the blurring kernel is unknown. It has wide applications in diverse fields such as astronomy, microscopy, and medical imaging. Blind deconvolution is a challenging ill-posed problem that suffers from significant non-uniqueness. Solution methods therefore require the integration of appropriate prior information. Early approaches rely on hand-crafted priors for the original image and the kernel. Recently, deep learning methods have shown excellent performance in addressing this challenge. However, most existing learning methods for blind deconvolution require a paired dataset of original and blurred images, which is often difficult to obtain. In this paper, we present a novel unsupervised learning approach named ECALL (Expectation-CALibrated Learning) that uses separate unpaired collections of original and blurred images. Key features of the proposed loss function are cycle consistency involving the kernel and associated reconstruction operator, and terms that use expectation values of data distributions to obtain information about the kernel. Numerical results are used to support ECALL.","sentences":["Blind deconvolution aims to recover an original image from a blurred version in the case where the blurring kernel is unknown.","It has wide applications in diverse fields such as astronomy, microscopy, and medical imaging.","Blind deconvolution is a challenging ill-posed problem that suffers from significant non-uniqueness.","Solution methods therefore require the integration of appropriate prior information.","Early approaches rely on hand-crafted priors for the original image and the kernel.","Recently, deep learning methods have shown excellent performance in addressing this challenge.","However, most existing learning methods for blind deconvolution require a paired dataset of original and blurred images, which is often difficult to obtain.","In this paper, we present a novel unsupervised learning approach named ECALL (Expectation-CALibrated Learning) that uses separate unpaired collections of original and blurred images.","Key features of the proposed loss function are cycle consistency involving the kernel and associated reconstruction operator, and terms that use expectation values of data distributions to obtain information about the kernel.","Numerical results are used to support ECALL."],"url":"http://arxiv.org/abs/2402.00670v1","category":"math.NA"}
{"created":"2024-02-01 15:14:16","title":"Improving the accuracy of freight mode choice models: A case study using the 2017 CFS PUF data set and ensemble learning techniques","abstract":"The US Census Bureau has collected two rounds of experimental data from the Commodity Flow Survey, providing shipment-level characteristics of nationwide commodity movements, published in 2012 (i.e., Public Use Microdata) and in 2017 (i.e., Public Use File). With this information, data-driven methods have become increasingly valuable for understanding detailed patterns in freight logistics. In this study, we used the 2017 Commodity Flow Survey Public Use File data set to explore building a high-performance freight mode choice model, considering three main improvements: (1) constructing local models for each separate commodity/industry category; (2) extracting useful geographical features, particularly the derived distance of each freight mode between origin/destination zones; and (3) applying additional ensemble learning methods such as stacking or voting to combine results from local and unified models for improved performance. The proposed method achieved over 92% accuracy without incorporating external information, an over 19% increase compared to directly fitting Random Forests models over 10,000 samples. Furthermore, SHAP (Shapely Additive Explanations) values were computed to explain the outputs and major patterns obtained from the proposed model. The model framework could enhance the performance and interpretability of existing freight mode choice models.","sentences":["The US Census Bureau has collected two rounds of experimental data from the Commodity Flow Survey, providing shipment-level characteristics of nationwide commodity movements, published in 2012 (i.e., Public Use Microdata) and in 2017 (i.e., Public Use File).","With this information, data-driven methods have become increasingly valuable for understanding detailed patterns in freight logistics.","In this study, we used the 2017 Commodity Flow Survey Public Use File data set to explore building a high-performance freight mode choice model, considering three main improvements: (1) constructing local models for each separate commodity/industry category; (2) extracting useful geographical features, particularly the derived distance of each freight mode between origin/destination zones; and (3) applying additional ensemble learning methods such as stacking or voting to combine results from local and unified models for improved performance.","The proposed method achieved over 92% accuracy without incorporating external information, an over 19% increase compared to directly fitting Random Forests models over 10,000 samples.","Furthermore, SHAP (Shapely Additive Explanations) values were computed to explain the outputs and major patterns obtained from the proposed model.","The model framework could enhance the performance and interpretability of existing freight mode choice models."],"url":"http://arxiv.org/abs/2402.00654v1","category":"cs.LG"}
{"created":"2024-02-01 14:02:45","title":"Multivariate ordinal regression for multiple repeated measurements","abstract":"In this paper we propose a multivariate ordinal regression model which allows the joint modeling of three-dimensional panel data containing both repeated and multiple measurements for a collection of subjects. This is achieved by a multivariate autoregressive structure on the errors of the latent variables underlying the ordinal responses, where we distinguish between the correlations at a single point in time and the persistence over time. The error distribution is assumed to be normal or Student t distributed. The estimation is performed using composite likelihood methods. We perform several simulation exercises to investigate the quality of the estimates in different settings as well as in comparison with a Bayesian approach. The simulation study confirms that the estimation procedure is able to recover the model parameters well and is competitive in terms of computation time. We also introduce R package mvordflex and illustrate how this implementation can be used to estimate the proposed model in a user-friendly, convenient way. Finally, we illustrate the framework on a data set containing firm failure and credit ratings information from the rating agencies S&P and Moody's for US listed companies.","sentences":["In this paper we propose a multivariate ordinal regression model which allows the joint modeling of three-dimensional panel data containing both repeated and multiple measurements for a collection of subjects.","This is achieved by a multivariate autoregressive structure on the errors of the latent variables underlying the ordinal responses, where we distinguish between the correlations at a single point in time and the persistence over time.","The error distribution is assumed to be normal or Student t distributed.","The estimation is performed using composite likelihood methods.","We perform several simulation exercises to investigate the quality of the estimates in different settings as well as in comparison with a Bayesian approach.","The simulation study confirms that the estimation procedure is able to recover the model parameters well and is competitive in terms of computation time.","We also introduce R package mvordflex and illustrate how this implementation can be used to estimate the proposed model in a user-friendly, convenient way.","Finally, we illustrate the framework on a data set containing firm failure and credit ratings information from the rating agencies S&P and Moody's for US listed companies."],"url":"http://arxiv.org/abs/2402.00610v1","category":"stat.ME"}
{"created":"2024-02-01 10:07:12","title":"Data Management Challenges in Agile Software Projects: A Systematic Literature Review","abstract":"Agile software development follows an adaptive and iterative approach. However, the management of data (e.g., development data or product data) can pose significant challenges for projects and agile teams. We aim to identify and characterize key challenges faced in data management within agile projects and to examine potential solutions proposed in the literature. We used a Systematic Literature Review (SLR) to collect and analyse relevant studies. We identified 45 studies related to data management in agile software development. We then manually analysed and mapped data from these studies to categorise different data management aspects and identify challenges and solutions as identified in those studies. Our findings reveal major challenges such as data integration and quality assurance. We found implications of challenges on team members and the product delivery process. We found that teams frequently struggle to integrate heterogeneous data sources, ensuring data reliability and real-time analytics. Additionally, fragmented data collection and a lack of standardized practices can impede team collaboration and project transparency. The studies have also proposed various solutions to address those challenges, including the use of ontologies, diverse data management strategies, automated tools, and the adoption of quality-focused development methods. Solutions also include training to enhance data quality and analysis. This SLR provides in-depth insights and recommendations for practitioners, emphasizing the importance of robust data management strategies. It suggests integrating advanced data management techniques into agile frameworks to enhance decision-making and improve software project outcomes. The study highlights the need for a more focused approach to data management in agile environments, advocating tailored solutions to meet the unique demands of agile software development.","sentences":["Agile software development follows an adaptive and iterative approach.","However, the management of data (e.g., development data or product data) can pose significant challenges for projects and agile teams.","We aim to identify and characterize key challenges faced in data management within agile projects and to examine potential solutions proposed in the literature.","We used a Systematic Literature Review (SLR) to collect and analyse relevant studies.","We identified 45 studies related to data management in agile software development.","We then manually analysed and mapped data from these studies to categorise different data management aspects and identify challenges and solutions as identified in those studies.","Our findings reveal major challenges such as data integration and quality assurance.","We found implications of challenges on team members and the product delivery process.","We found that teams frequently struggle to integrate heterogeneous data sources, ensuring data reliability and real-time analytics.","Additionally, fragmented data collection and a lack of standardized practices can impede team collaboration and project transparency.","The studies have also proposed various solutions to address those challenges, including the use of ontologies, diverse data management strategies, automated tools, and the adoption of quality-focused development methods.","Solutions also include training to enhance data quality and analysis.","This SLR provides in-depth insights and recommendations for practitioners, emphasizing the importance of robust data management strategies.","It suggests integrating advanced data management techniques into agile frameworks to enhance decision-making and improve software project outcomes.","The study highlights the need for a more focused approach to data management in agile environments, advocating tailored solutions to meet the unique demands of agile software development."],"url":"http://arxiv.org/abs/2402.00462v1","category":"cs.SE"}
{"created":"2024-02-01 07:46:44","title":"A lower bound for the volumes of modular link complements","abstract":"Every finite collection of oriented closed geodesics in the modular surface has a canonically associated link in its unit tangent bundle coming from the periodic orbits of the geodesic flow. We study the volume of the associated link complement with respect to its unique complete hyperbolic metric. We provide the first lower volume bound that is linear in terms of the number of distinct exponents in the code words corresponding to the collection of closed geodesics.","sentences":["Every finite collection of oriented closed geodesics in the modular surface has a canonically associated link in its unit tangent bundle coming from the periodic orbits of the geodesic flow.","We study the volume of the associated link complement with respect to its unique complete hyperbolic metric.","We provide the first lower volume bound that is linear in terms of the number of distinct exponents in the code words corresponding to the collection of closed geodesics."],"url":"http://arxiv.org/abs/2402.00400v1","category":"math.GT"}
{"created":"2024-02-01 07:21:30","title":"Cumulative Distribution Function based General Temporal Point Processes","abstract":"Temporal Point Processes (TPPs) hold a pivotal role in modeling event sequences across diverse domains, including social networking and e-commerce, and have significantly contributed to the advancement of recommendation systems and information retrieval strategies. Through the analysis of events such as user interactions and transactions, TPPs offer valuable insights into behavioral patterns, facilitating the prediction of future trends. However, accurately forecasting future events remains a formidable challenge due to the intricate nature of these patterns. The integration of Neural Networks with TPPs has ushered in the development of advanced deep TPP models. While these models excel at processing complex and nonlinear temporal data, they encounter limitations in modeling intensity functions, grapple with computational complexities in integral computations, and struggle to capture long-range temporal dependencies effectively. In this study, we introduce the CuFun model, representing a novel approach to TPPs that revolves around the Cumulative Distribution Function (CDF). CuFun stands out by uniquely employing a monotonic neural network for CDF representation, utilizing past events as a scaling factor. This innovation significantly bolsters the model's adaptability and precision across a wide range of data scenarios. Our approach addresses several critical issues inherent in traditional TPP modeling: it simplifies log-likelihood calculations, extends applicability beyond predefined density function forms, and adeptly captures long-range temporal patterns. Our contributions encompass the introduction of a pioneering CDF-based TPP model, the development of a methodology for incorporating past event information into future event prediction, and empirical validation of CuFun's effectiveness through extensive experimentation on synthetic and real-world datasets.","sentences":["Temporal Point Processes (TPPs) hold a pivotal role in modeling event sequences across diverse domains, including social networking and e-commerce, and have significantly contributed to the advancement of recommendation systems and information retrieval strategies.","Through the analysis of events such as user interactions and transactions, TPPs offer valuable insights into behavioral patterns, facilitating the prediction of future trends.","However, accurately forecasting future events remains a formidable challenge due to the intricate nature of these patterns.","The integration of Neural Networks with TPPs has ushered in the development of advanced deep TPP models.","While these models excel at processing complex and nonlinear temporal data, they encounter limitations in modeling intensity functions, grapple with computational complexities in integral computations, and struggle to capture long-range temporal dependencies effectively.","In this study, we introduce the CuFun model, representing a novel approach to TPPs that revolves around the Cumulative Distribution Function (CDF).","CuFun stands out by uniquely employing a monotonic neural network for CDF representation, utilizing past events as a scaling factor.","This innovation significantly bolsters the model's adaptability and precision across a wide range of data scenarios.","Our approach addresses several critical issues inherent in traditional TPP modeling: it simplifies log-likelihood calculations, extends applicability beyond predefined density function forms, and adeptly captures long-range temporal patterns.","Our contributions encompass the introduction of a pioneering CDF-based TPP model, the development of a methodology for incorporating past event information into future event prediction, and empirical validation of CuFun's effectiveness through extensive experimentation on synthetic and real-world datasets."],"url":"http://arxiv.org/abs/2402.00388v1","category":"cs.LG"}
{"created":"2024-02-01 06:06:59","title":"Legged Robot State Estimation With Invariant Extended Kalman Filter Using Neural Measurement Network","abstract":"This paper introduces a novel proprioceptive state estimator for legged robots that combines model-based filters and deep neural networks. Recent studies have shown that neural networks such as multi-layer perceptron or recurrent neural networks can estimate the robot states, including contact probability and linear velocity. Inspired by this, we develop a state estimation framework that integrates a neural measurement network (NMN) with an invariant extended Kalman filter. We show that our framework improves estimation performance in various terrains. Existing studies that combine model-based filters and learning-based approaches typically use real-world data. However, our approach relies solely on simulation data, as it allows us to easily obtain extensive data. This difference leads to a gap between the learning and the inference domain, commonly referred to as a sim-to-real gap. We address this challenge by adapting existing learning techniques and regularization. To validate our proposed method, we conduct experiments using a quadruped robot on four types of terrain: \\textit{flat}, \\textit{debris}, \\textit{soft}, and \\textit{slippery}. We observe that our approach significantly reduces position drift compared to the existing model-based state estimator.","sentences":["This paper introduces a novel proprioceptive state estimator for legged robots that combines model-based filters and deep neural networks.","Recent studies have shown that neural networks such as multi-layer perceptron or recurrent neural networks can estimate the robot states, including contact probability and linear velocity.","Inspired by this, we develop a state estimation framework that integrates a neural measurement network (NMN) with an invariant extended Kalman filter.","We show that our framework improves estimation performance in various terrains.","Existing studies that combine model-based filters and learning-based approaches typically use real-world data.","However, our approach relies solely on simulation data, as it allows us to easily obtain extensive data.","This difference leads to a gap between the learning and the inference domain, commonly referred to as a sim-to-real gap.","We address this challenge by adapting existing learning techniques and regularization.","To validate our proposed method, we conduct experiments using a quadruped robot on four types of terrain: \\textit{flat}, \\textit{debris}, \\textit{soft}, and \\textit{slippery}.","We observe that our approach significantly reduces position drift compared to the existing model-based state estimator."],"url":"http://arxiv.org/abs/2402.00366v1","category":"cs.RO"}
{"created":"2024-02-01 06:02:51","title":"Limitations in design and applications of ultra-small mode volume photonic crystals","abstract":"Ultra-small mode volume nanophotonic crystal cavities have been proposed as powerful tools for increasing coupling rates in cavity quantum electrodynamics systems. However, their adoption in quantum information applications remains elusive. In this work, we investigate possible reasons why, and analyze the impact of different low mode volume resonator design choices on their utility in quantum optics experiments. We analyze band structure features and loss rates of low mode volume bowtie cavities in diamond and demonstrate independent design control over cavity-emitter coupling strength and loss rates. Further, using silicon vacancy centers in diamond as exemplary emitters, we investigate the influence of placement imprecision. We find that the benefit on photon collection efficiency and indistinguishability is limited, while the fabrication complexity of ultra-small cavity designs increases substantially compared to conventional photonic crystals. We conclude that ultra-small mode volume designs are primarily of interest for dispersive spin-photon interactions, which are of great interest for future quantum networks.","sentences":["Ultra-small mode volume nanophotonic crystal cavities have been proposed as powerful tools for increasing coupling rates in cavity quantum electrodynamics systems.","However, their adoption in quantum information applications remains elusive.","In this work, we investigate possible reasons why, and analyze the impact of different low mode volume resonator design choices on their utility in quantum optics experiments.","We analyze band structure features and loss rates of low mode volume bowtie cavities in diamond and demonstrate independent design control over cavity-emitter coupling strength and loss rates.","Further, using silicon vacancy centers in diamond as exemplary emitters, we investigate the influence of placement imprecision.","We find that the benefit on photon collection efficiency and indistinguishability is limited, while the fabrication complexity of ultra-small cavity designs increases substantially compared to conventional photonic crystals.","We conclude that ultra-small mode volume designs are primarily of interest for dispersive spin-photon interactions, which are of great interest for future quantum networks."],"url":"http://arxiv.org/abs/2402.00363v1","category":"quant-ph"}
{"created":"2024-02-01 06:02:29","title":"Climate Trends of Tropical Cyclone Intensity and Energy Extremes Revealed by Deep Learning","abstract":"Anthropogenic influences have been linked to tropical cyclone (TC) poleward migration, TC extreme precipitation, and an increased proportion of major hurricanes [1, 2, 3, 4]. Understanding past TC trends and variability is critical for projecting future TC impacts on human society considering the changing climate [5]. However, past trends of TC structure/energy remain uncertain due to limited observations; subjective-analyzed and spatiotemporal-heterogeneous \"best-track\" datasets lead to reduced confidence in the assessed TC repose to climate change [6, 7]. Here, we use deep learning to reconstruct past \"observations\" and yield an objective global TC wind profile dataset during 1981 to 2020, facilitating a comprehensive examination of TC structure/energy. By training with uniquely labeled data integrating best tracks and numerical model analysis of 2004 to 2018 TCs, our model converts multichannel satellite imagery to a 0-750-km wind profile of axisymmetric surface winds. The model performance is verified to be sufficient for climate studies by comparing it to independent satellite-radar surface winds. Based on the new homogenized dataset, the major TC proportion has increased by ~13% in the past four decades. Moreover, the proportion of extremely high-energy TCs has increased by ~25%, along with an increasing trend (> one standard deviation of the 40-y variability) of the mean total energy of high-energy TCs. Although the warming ocean favors TC intensification, the TC track migration to higher latitudes and altered environments further affect TC structure/energy. This new deep learning method/dataset reveals novel trends regarding TC structure extremes and may help verify simulations/studies regarding TCs in the changing climate.","sentences":["Anthropogenic influences have been linked to tropical cyclone (TC) poleward migration, TC extreme precipitation, and an increased proportion of major hurricanes [1, 2, 3, 4].","Understanding past TC trends and variability is critical for projecting future TC impacts on human society considering the changing climate [5].","However, past trends of TC structure/energy remain uncertain due to limited observations; subjective-analyzed and spatiotemporal-heterogeneous \"best-track\" datasets lead to reduced confidence in the assessed TC repose to climate change [6, 7].","Here, we use deep learning to reconstruct past \"observations\" and yield an objective global TC wind profile dataset during 1981 to 2020, facilitating a comprehensive examination of TC structure/energy.","By training with uniquely labeled data integrating best tracks and numerical model analysis of 2004 to 2018 TCs, our model converts multichannel satellite imagery to a 0-750-km wind profile of axisymmetric surface winds.","The model performance is verified to be sufficient for climate studies by comparing it to independent satellite-radar surface winds.","Based on the new homogenized dataset, the major TC proportion has increased by ~13% in the past four decades.","Moreover, the proportion of extremely high-energy TCs has increased by ~25%, along with an increasing trend (> one standard deviation of the 40-y variability) of the mean total energy of high-energy TCs.","Although the warming ocean favors TC intensification, the TC track migration to higher latitudes and altered environments further affect TC structure/energy.","This new deep learning method/dataset reveals novel trends regarding TC structure extremes and may help verify simulations/studies regarding TCs in the changing climate."],"url":"http://arxiv.org/abs/2402.00362v1","category":"physics.ao-ph"}
{"created":"2024-02-01 05:53:44","title":"Adaptive Primal-Dual Method for Safe Reinforcement Learning","abstract":"Primal-dual methods have a natural application in Safe Reinforcement Learning (SRL), posed as a constrained policy optimization problem. In practice however, applying primal-dual methods to SRL is challenging, due to the inter-dependency of the learning rate (LR) and Lagrangian multipliers (dual variables) each time an embedded unconstrained RL problem is solved. In this paper, we propose, analyze and evaluate adaptive primal-dual (APD) methods for SRL, where two adaptive LRs are adjusted to the Lagrangian multipliers so as to optimize the policy in each iteration. We theoretically establish the convergence, optimality and feasibility of the APD algorithm. Finally, we conduct numerical evaluation of the practical APD algorithm with four well-known environments in Bullet-Safey-Gym employing two state-of-the-art SRL algorithms: PPO-Lagrangian and DDPG-Lagrangian. All experiments show that the practical APD algorithm outperforms (or achieves comparable performance) and attains more stable training than the constant LR cases. Additionally, we substantiate the robustness of selecting the two adaptive LRs by empirical evidence.","sentences":["Primal-dual methods have a natural application in Safe Reinforcement Learning (SRL), posed as a constrained policy optimization problem.","In practice however, applying primal-dual methods to SRL is challenging, due to the inter-dependency of the learning rate (LR) and Lagrangian multipliers (dual variables) each time an embedded unconstrained RL problem is solved.","In this paper, we propose, analyze and evaluate adaptive primal-dual (APD) methods for SRL, where two adaptive LRs are adjusted to the Lagrangian multipliers so as to optimize the policy in each iteration.","We theoretically establish the convergence, optimality and feasibility of the APD algorithm.","Finally, we conduct numerical evaluation of the practical APD algorithm with four well-known environments in Bullet-Safey-Gym employing two state-of-the-art SRL algorithms: PPO-Lagrangian and DDPG-Lagrangian.","All experiments show that the practical APD algorithm outperforms (or achieves comparable performance) and attains more stable training than the constant LR cases.","Additionally, we substantiate the robustness of selecting the two adaptive LRs by empirical evidence."],"url":"http://arxiv.org/abs/2402.00355v1","category":"cs.LG"}
{"created":"2024-02-01 05:51:03","title":"High-Quality Medical Image Generation from Free-hand Sketch","abstract":"Generating medical images from human-drawn free-hand sketches holds promise for various important medical imaging applications. Due to the extreme difficulty in collecting free-hand sketch data in the medical domain, most deep learning-based methods have been proposed to generate medical images from the synthesized sketches (e.g., edge maps or contours of segmentation masks from real images). However, these models often fail to generalize on the free-hand sketches, leading to unsatisfactory results. In this paper, we propose a practical free-hand sketch-to-image generation model called Sketch2MedI that learns to represent sketches in StyleGAN's latent space and generate medical images from it. Thanks to the ability to encode sketches into this meaningful representation space, Sketch2MedI only requires synthesized sketches for training, enabling a cost-effective learning process. Our Sketch2MedI demonstrates a robust generalization to free-hand sketches, resulting in high-quality and realistic medical image generations. Comparative evaluations of Sketch2MedI against the pix2pix, CycleGAN, UNIT, and U-GAT-IT models show superior performance in generating pharyngeal images, both quantitative and qualitative across various metrics.","sentences":["Generating medical images from human-drawn free-hand sketches holds promise for various important medical imaging applications.","Due to the extreme difficulty in collecting free-hand sketch data in the medical domain, most deep learning-based methods have been proposed to generate medical images from the synthesized sketches (e.g., edge maps or contours of segmentation masks from real images).","However, these models often fail to generalize on the free-hand sketches, leading to unsatisfactory results.","In this paper, we propose a practical free-hand sketch-to-image generation model called Sketch2MedI that learns to represent sketches in StyleGAN's latent space and generate medical images from it.","Thanks to the ability to encode sketches into this meaningful representation space, Sketch2MedI only requires synthesized sketches for training, enabling a cost-effective learning process.","Our Sketch2MedI demonstrates a robust generalization to free-hand sketches, resulting in high-quality and realistic medical image generations.","Comparative evaluations of Sketch2MedI against the pix2pix, CycleGAN, UNIT, and U-GAT-IT models show superior performance in generating pharyngeal images, both quantitative and qualitative across various metrics."],"url":"http://arxiv.org/abs/2402.00353v1","category":"cs.CV"}
{"created":"2024-02-01 05:47:50","title":"A Data-Driven Autopilot for Fixed-Wing Aircraft Based on Model Predictive Control","abstract":"Autopilots for fixed-wing aircraft are typically designed based on linearized aerodynamic models consisting of stability and control derivatives obtained from wind-tunnel testing. The resulting local controllers are then pieced together using gain scheduling. For applications in which the aerodynamics are unmodeled, the present paper proposes an autopilot based on predictive cost adaptive control (PCAC). As an indirect adaptive control extension of model predictive control, PCAC uses recursive least squares (RLS) with variable-rate forgetting for online, closed-loop system identification. At each time step, RLS-based system identification updates the coefficients of an input-output model whose order is a hyperparameter specified by the user. For MPC, the receding-horizon optimization can be performed by either the backward-propagating Riccati equation or quadratic programming. The present paper investigates the performance of PCAC for fixed-wing aircraft without the use of any aerodynamic modeling or offline/prior data collection.","sentences":["Autopilots for fixed-wing aircraft are typically designed based on linearized aerodynamic models consisting of stability and control derivatives obtained from wind-tunnel testing.","The resulting local controllers are then pieced together using gain scheduling.","For applications in which the aerodynamics are unmodeled, the present paper proposes an autopilot based on predictive cost adaptive control (PCAC).","As an indirect adaptive control extension of model predictive control, PCAC uses recursive least squares (RLS) with variable-rate forgetting for online, closed-loop system identification.","At each time step, RLS-based system identification updates the coefficients of an input-output model whose order is a hyperparameter specified by the user.","For MPC, the receding-horizon optimization can be performed by either the backward-propagating Riccati equation or quadratic programming.","The present paper investigates the performance of PCAC for fixed-wing aircraft without the use of any aerodynamic modeling or offline/prior data collection."],"url":"http://arxiv.org/abs/2402.00352v1","category":"eess.SY"}
{"created":"2024-02-01 05:34:03","title":"Large Language Models Based Fuzzing Techniques: A Survey","abstract":"In the modern era where software plays a pivotal role, software security and vulnerability analysis have become essential for software development. Fuzzing test, as an efficient software testing method, are widely used in various domains. Moreover, the rapid development of Large Language Models (LLMs) has facilitated their application in the field of software testing, demonstrating remarkable performance. Considering that existing fuzzing test techniques are not entirely automated and software vulnerabilities continue to evolve, there is a growing trend towards employing fuzzing test generated based on large language models. This survey provides a systematic overview of the approaches that fuse LLMs and fuzzing tests for software testing. In this paper, a statistical analysis and discussion of the literature in three areas, namely LLMs, fuzzing test, and fuzzing test generated based on LLMs, are conducted by summarising the state-of-the-art methods up until 2024. Our survey also investigates the potential for widespread deployment and application of fuzzing test techniques generated by LLMs in the future.","sentences":["In the modern era where software plays a pivotal role, software security and vulnerability analysis have become essential for software development.","Fuzzing test, as an efficient software testing method, are widely used in various domains.","Moreover, the rapid development of Large Language Models (LLMs) has facilitated their application in the field of software testing, demonstrating remarkable performance.","Considering that existing fuzzing test techniques are not entirely automated and software vulnerabilities continue to evolve, there is a growing trend towards employing fuzzing test generated based on large language models.","This survey provides a systematic overview of the approaches that fuse LLMs and fuzzing tests for software testing.","In this paper, a statistical analysis and discussion of the literature in three areas, namely LLMs, fuzzing test, and fuzzing test generated based on LLMs, are conducted by summarising the state-of-the-art methods up until 2024.","Our survey also investigates the potential for widespread deployment and application of fuzzing test techniques generated by LLMs in the future."],"url":"http://arxiv.org/abs/2402.00350v1","category":"cs.SE"}
{"created":"2024-02-01 05:30:51","title":"ODICE: Revealing the Mystery of Distribution Correction Estimation via Orthogonal-gradient Update","abstract":"In this study, we investigate the DIstribution Correction Estimation (DICE) methods, an important line of work in offline reinforcement learning (RL) and imitation learning (IL). DICE-based methods impose state-action-level behavior constraint, which is an ideal choice for offline learning. However, they typically perform much worse than current state-of-the-art (SOTA) methods that solely use action-level behavior constraint. After revisiting DICE-based methods, we find there exist two gradient terms when learning the value function using true-gradient update: forward gradient (taken on the current state) and backward gradient (taken on the next state). Using forward gradient bears a large similarity to many offline RL methods, and thus can be regarded as applying action-level constraint. However, directly adding the backward gradient may degenerate or cancel out its effect if these two gradients have conflicting directions. To resolve this issue, we propose a simple yet effective modification that projects the backward gradient onto the normal plane of the forward gradient, resulting in an orthogonal-gradient update, a new learning rule for DICE-based methods. We conduct thorough theoretical analyses and find that the projected backward gradient brings state-level behavior regularization, which reveals the mystery of DICE-based methods: the value learning objective does try to impose state-action-level constraint, but needs to be used in a corrected way. Through toy examples and extensive experiments on complex offline RL and IL tasks, we demonstrate that DICE-based methods using orthogonal-gradient updates (O-DICE) achieve SOTA performance and great robustness.","sentences":["In this study, we investigate the DIstribution Correction Estimation (DICE) methods, an important line of work in offline reinforcement learning (RL) and imitation learning (IL).","DICE-based methods impose state-action-level behavior constraint, which is an ideal choice for offline learning.","However, they typically perform much worse than current state-of-the-art (SOTA) methods that solely use action-level behavior constraint.","After revisiting DICE-based methods, we find there exist two gradient terms when learning the value function using true-gradient update: forward gradient (taken on the current state) and backward gradient (taken on the next state).","Using forward gradient bears a large similarity to many offline RL methods, and thus can be regarded as applying action-level constraint.","However, directly adding the backward gradient may degenerate or cancel out its effect if these two gradients have conflicting directions.","To resolve this issue, we propose a simple yet effective modification that projects the backward gradient onto the normal plane of the forward gradient, resulting in an orthogonal-gradient update, a new learning rule for DICE-based methods.","We conduct thorough theoretical analyses and find that the projected backward gradient brings state-level behavior regularization, which reveals the mystery of DICE-based methods: the value learning objective does try to impose state-action-level constraint, but needs to be used in a corrected way.","Through toy examples and extensive experiments on complex offline RL and IL tasks, we demonstrate that DICE-based methods using orthogonal-gradient updates (O-DICE) achieve SOTA performance and great robustness."],"url":"http://arxiv.org/abs/2402.00348v1","category":"cs.LG"}
{"created":"2024-02-01 05:28:28","title":"Diverse Explanations from Data-driven and Domain-driven Perspectives for Machine Learning Models","abstract":"Explanations of machine learning models are important, especially in scientific areas such as chemistry, biology, and physics, where they guide future laboratory experiments and resource requirements. These explanations can be derived from well-trained machine learning models (data-driven perspective) or specific domain knowledge (domain-driven perspective). However, there exist inconsistencies between these perspectives due to accurate yet misleading machine learning models and various stakeholders with specific needs, wants, or aims. This paper calls attention to these inconsistencies and suggests a way to find an accurate model with expected explanations that reinforce physical laws and meet stakeholders' requirements from a set of equally-good models, also known as Rashomon sets. Our goal is to foster a comprehensive understanding of these inconsistencies and ultimately contribute to the integration of eXplainable Artificial Intelligence (XAI) into scientific domains.","sentences":["Explanations of machine learning models are important, especially in scientific areas such as chemistry, biology, and physics, where they guide future laboratory experiments and resource requirements.","These explanations can be derived from well-trained machine learning models (data-driven perspective) or specific domain knowledge (domain-driven perspective).","However, there exist inconsistencies between these perspectives due to accurate yet misleading machine learning models and various stakeholders with specific needs, wants, or aims.","This paper calls attention to these inconsistencies and suggests a way to find an accurate model with expected explanations that reinforce physical laws and meet stakeholders' requirements from a set of equally-good models, also known as Rashomon sets.","Our goal is to foster a comprehensive understanding of these inconsistencies and ultimately contribute to the integration of eXplainable Artificial Intelligence (XAI) into scientific domains."],"url":"http://arxiv.org/abs/2402.00347v1","category":"cs.LG"}
{"created":"2024-02-01 04:39:15","title":"Multi-agent Path Finding for Cooperative Autonomous Driving","abstract":"Anticipating possible future deployment of connected and automated vehicles (CAVs), cooperative autonomous driving at intersections has been studied by many works in control theory and intelligent transportation across decades. Simultaneously, recent parallel works in robotics have devised efficient algorithms for multi-agent path finding (MAPF), though often in environments with simplified kinematics. In this work, we hybridize insights and algorithms from MAPF with the structure and heuristics of optimizing the crossing order of CAVs at signal-free intersections. We devise an optimal and complete algorithm, Order-based Search with Kinematics Arrival Time Scheduling (OBS-KATS), which significantly outperforms existing algorithms, fixed heuristics, and prioritized planning with KATS. The performance is maintained under different vehicle arrival rates, lane lengths, crossing speeds, and control horizon. Through ablations and dissections, we offer insight on the contributing factors to OBS-KATS's performance. Our work is directly applicable to many similarly scaled traffic and multi-robot scenarios with directed lanes.","sentences":["Anticipating possible future deployment of connected and automated vehicles (CAVs), cooperative autonomous driving at intersections has been studied by many works in control theory and intelligent transportation across decades.","Simultaneously, recent parallel works in robotics have devised efficient algorithms for multi-agent path finding (MAPF), though often in environments with simplified kinematics.","In this work, we hybridize insights and algorithms from MAPF with the structure and heuristics of optimizing the crossing order of CAVs at signal-free intersections.","We devise an optimal and complete algorithm, Order-based Search with Kinematics Arrival Time Scheduling (OBS-KATS), which significantly outperforms existing algorithms, fixed heuristics, and prioritized planning with KATS.","The performance is maintained under different vehicle arrival rates, lane lengths, crossing speeds, and control horizon.","Through ablations and dissections, we offer insight on the contributing factors to OBS-KATS's performance.","Our work is directly applicable to many similarly scaled traffic and multi-robot scenarios with directed lanes."],"url":"http://arxiv.org/abs/2402.00334v1","category":"cs.MA"}
{"created":"2024-02-01 04:09:17","title":"SCO-VIST: Social Interaction Commonsense Knowledge-based Visual Storytelling","abstract":"Visual storytelling aims to automatically generate a coherent story based on a given image sequence. Unlike tasks like image captioning, visual stories should contain factual descriptions, worldviews, and human social commonsense to put disjointed elements together to form a coherent and engaging human-writeable story. However, most models mainly focus on applying factual information and using taxonomic/lexical external knowledge when attempting to create stories. This paper introduces SCO-VIST, a framework representing the image sequence as a graph with objects and relations that includes human action motivation and its social interaction commonsense knowledge. SCO-VIST then takes this graph representing plot points and creates bridges between plot points with semantic and occurrence-based edge weights. This weighted story graph produces the storyline in a sequence of events using Floyd-Warshall's algorithm. Our proposed framework produces stories superior across multiple metrics in terms of visual grounding, coherence, diversity, and humanness, per both automatic and human evaluations.","sentences":["Visual storytelling aims to automatically generate a coherent story based on a given image sequence.","Unlike tasks like image captioning, visual stories should contain factual descriptions, worldviews, and human social commonsense to put disjointed elements together to form a coherent and engaging human-writeable story.","However, most models mainly focus on applying factual information and using taxonomic/lexical external knowledge when attempting to create stories.","This paper introduces SCO-VIST, a framework representing the image sequence as a graph with objects and relations that includes human action motivation and its social interaction commonsense knowledge.","SCO-VIST then takes this graph representing plot points and creates bridges between plot points with semantic and occurrence-based edge weights.","This weighted story graph produces the storyline in a sequence of events using Floyd-Warshall's algorithm.","Our proposed framework produces stories superior across multiple metrics in terms of visual grounding, coherence, diversity, and humanness, per both automatic and human evaluations."],"url":"http://arxiv.org/abs/2402.00319v1","category":"cs.CV"}
{"created":"2024-02-01 03:53:13","title":"The whack-a-mole governance challenge for AI-enabled synthetic biology: literature review and emerging frameworks","abstract":"AI-enabled synthetic biology has tremendous potential but also significantly increases biorisks and brings about a new set of dual use concerns. The picture is complicated given the vast innovations envisioned to emerge by combining emerging technologies, as AI-enabled synthetic biology potentially scales up bioengineering into industrial biomanufacturing. However, the literature review indicates that goals such as maintaining a reasonable scope for innovation, or more ambitiously to foster a huge bioeconomy don't necessarily contrast with biosafety, but need to go hand in hand. This paper presents a literature review of the issues and describes emerging frameworks for policy and practice that transverse the options of command-and control, stewardship, bottom-up, and laissez-faire governance. How to achieve early warning systems that enable prevention and mitigation of future AI-enabled biohazards from the lab, from deliberate misuse, or from the public realm, will constantly need to evolve, and adaptive, interactive approaches should emerge. Although biorisk is subject to an established governance regime, and scientists generally adhere to biosafety protocols, even experimental, but legitimate use by scientists could lead to unexpected developments. Recent advances in chatbots enabled by generative AI have revived fears that advanced biological insight can more easily get into the hands of malignant individuals or organizations. Given these sets of issues, society needs to rethink how AI-enabled synthetic biology should be governed. The suggested way to visualize the challenge at hand is whack-a-mole governance, although the emerging solutions are perhaps not so different either.","sentences":["AI-enabled synthetic biology has tremendous potential but also significantly increases biorisks and brings about a new set of dual use concerns.","The picture is complicated given the vast innovations envisioned to emerge by combining emerging technologies, as AI-enabled synthetic biology potentially scales up bioengineering into industrial biomanufacturing.","However, the literature review indicates that goals such as maintaining a reasonable scope for innovation, or more ambitiously to foster a huge bioeconomy don't necessarily contrast with biosafety, but need to go hand in hand.","This paper presents a literature review of the issues and describes emerging frameworks for policy and practice that transverse the options of command-and control, stewardship, bottom-up, and laissez-faire governance.","How to achieve early warning systems that enable prevention and mitigation of future AI-enabled biohazards from the lab, from deliberate misuse, or from the public realm, will constantly need to evolve, and adaptive, interactive approaches should emerge.","Although biorisk is subject to an established governance regime, and scientists generally adhere to biosafety protocols, even experimental, but legitimate use by scientists could lead to unexpected developments.","Recent advances in chatbots enabled by generative AI have revived fears that advanced biological insight can more easily get into the hands of malignant individuals or organizations.","Given these sets of issues, society needs to rethink how AI-enabled synthetic biology should be governed.","The suggested way to visualize the challenge at hand is whack-a-mole governance, although the emerging solutions are perhaps not so different either."],"url":"http://arxiv.org/abs/2402.00312v1","category":"q-bio.OT"}
{"created":"2024-02-01 03:39:15","title":"An Accurate and Low-Parameter Machine Learning Architecture for Next Location Prediction","abstract":"Next location prediction is a discipline that involves predicting a users next location. Its applications include resource allocation, quality of service, energy efficiency, and traffic management. This paper proposes an energy-efficient, small, and low parameter machine learning (ML) architecture for accurate next location prediction, deployable on modest base stations and edge devices. To accomplish this we ran a hundred hyperparameter experiments on the full human mobility patterns of an entire city, to determine an exact ML architecture that reached a plateau of accuracy with the least amount of model parameters. We successfully achieved a reduction in the number of model parameters within published ML architectures from 202 million down to 2 million. This reduced the total size of the model parameters from 791 MB down to 8 MB. Additionally, this decreased the training time by a factor of four, the amount of graphics processing unit (GPU) memory needed for training by a factor of twenty, and the overall accuracy was increased from 80.16% to 82.54%. This improvement allows for modest base stations and edge devices which do not have a large amount of memory or storage, to deploy and utilize the proposed ML architecture for next location prediction.","sentences":["Next location prediction is a discipline that involves predicting a users next location.","Its applications include resource allocation, quality of service, energy efficiency, and traffic management.","This paper proposes an energy-efficient, small, and low parameter machine learning (ML) architecture for accurate next location prediction, deployable on modest base stations and edge devices.","To accomplish this we ran a hundred hyperparameter experiments on the full human mobility patterns of an entire city, to determine an exact ML architecture that reached a plateau of accuracy with the least amount of model parameters.","We successfully achieved a reduction in the number of model parameters within published ML architectures from 202 million down to 2 million.","This reduced the total size of the model parameters from 791 MB down to 8 MB.","Additionally, this decreased the training time by a factor of four, the amount of graphics processing unit (GPU) memory needed for training by a factor of twenty, and the overall accuracy was increased from 80.16% to 82.54%.","This improvement allows for modest base stations and edge devices which do not have a large amount of memory or storage, to deploy and utilize the proposed ML architecture for next location prediction."],"url":"http://arxiv.org/abs/2402.00306v1","category":"cs.LG"}
{"created":"2024-02-01 03:27:26","title":"Self-supervised learning of video representations from a child's perspective","abstract":"Children learn powerful internal models of the world around them from a few years of egocentric visual experience. Can such internal models be learned from a child's visual experience with highly generic learning algorithms or do they require strong inductive biases? Recent advances in collecting large-scale, longitudinal, developmentally realistic video datasets and generic self-supervised learning (SSL) algorithms are allowing us to begin to tackle this nature vs. nurture question. However, existing work typically focuses on image-based SSL algorithms and visual capabilities that can be learned from static images (e.g. object recognition), thus ignoring temporal aspects of the world. To close this gap, here we train self-supervised video models on longitudinal, egocentric headcam recordings collected from a child over a two year period in their early development (6-31 months). The resulting models are highly effective at facilitating the learning of action concepts from a small number of labeled examples; they have favorable data size scaling properties; and they display emergent video interpolation capabilities. Video models also learn more robust object representations than image-based models trained with the exact same data. These results suggest that important temporal aspects of a child's internal model of the world may be learnable from their visual experience using highly generic learning algorithms and without strong inductive biases.","sentences":["Children learn powerful internal models of the world around them from a few years of egocentric visual experience.","Can such internal models be learned from a child's visual experience with highly generic learning algorithms or do they require strong inductive biases?","Recent advances in collecting large-scale, longitudinal, developmentally realistic video datasets and generic self-supervised learning (SSL) algorithms are allowing us to begin to tackle this nature vs. nurture question.","However, existing work typically focuses on image-based SSL algorithms and visual capabilities that can be learned from static images (e.g. object recognition), thus ignoring temporal aspects of the world.","To close this gap, here we train self-supervised video models on longitudinal, egocentric headcam recordings collected from a child over a two year period in their early development (6-31 months).","The resulting models are highly effective at facilitating the learning of action concepts from a small number of labeled examples; they have favorable data size scaling properties; and they display emergent video interpolation capabilities.","Video models also learn more robust object representations than image-based models trained with the exact same data.","These results suggest that important temporal aspects of a child's internal model of the world may be learnable from their visual experience using highly generic learning algorithms and without strong inductive biases."],"url":"http://arxiv.org/abs/2402.00300v1","category":"cs.CV"}
{"created":"2024-02-01 02:43:20","title":"Multimodal Embodied Interactive Agent for Cafe Scene","abstract":"With the surge in the development of large language models, embodied intelligence has attracted increasing attention. Nevertheless, prior works on embodied intelligence typically encode scene or historical memory in an unimodal manner, either visual or linguistic, which complicates the alignment of the model's action planning with embodied control. To overcome this limitation, we introduce the Multimodal Embodied Interactive Agent (MEIA), capable of translating high-level tasks expressed in natural language into a sequence of executable actions. Specifically, we propose a novel Multimodal Environment Memory (MEM) module, facilitating the integration of embodied control with large models through the visual-language memory of scenes. This capability enables MEIA to generate executable action plans based on diverse requirements and the robot's capabilities. We conduct experiments in a dynamic virtual cafe environment, utilizing multiple large models through zero-shot learning, and carefully design scenarios for various situations. The experimental results showcase the promising performance of our MEIA in various embodied interactive tasks.","sentences":["With the surge in the development of large language models, embodied intelligence has attracted increasing attention.","Nevertheless, prior works on embodied intelligence typically encode scene or historical memory in an unimodal manner, either visual or linguistic, which complicates the alignment of the model's action planning with embodied control.","To overcome this limitation, we introduce the Multimodal Embodied Interactive Agent (MEIA), capable of translating high-level tasks expressed in natural language into a sequence of executable actions.","Specifically, we propose a novel Multimodal Environment Memory (MEM) module, facilitating the integration of embodied control with large models through the visual-language memory of scenes.","This capability enables MEIA to generate executable action plans based on diverse requirements and the robot's capabilities.","We conduct experiments in a dynamic virtual cafe environment, utilizing multiple large models through zero-shot learning, and carefully design scenarios for various situations.","The experimental results showcase the promising performance of our MEIA in various embodied interactive tasks."],"url":"http://arxiv.org/abs/2402.00290v1","category":"cs.CV"}
{"created":"2024-02-01 02:29:16","title":"PAP-REC: Personalized Automatic Prompt for Recommendation Language Model","abstract":"Recently emerged prompt-based Recommendation Language Models (RLM) can solve multiple recommendation tasks uniformly. The RLMs make full use of the inherited knowledge learned from the abundant pre-training data to solve the downstream recommendation tasks by prompts, without introducing additional parameters or network training. However, handcrafted prompts require significant expertise and human effort since slightly rewriting prompts may cause massive performance changes. In this paper, we propose PAP-REC, a framework to generate the Personalized Automatic Prompt for RECommendation language models to mitigate the inefficiency and ineffectiveness problems derived from manually designed prompts. Specifically, personalized automatic prompts allow different users to have different prompt tokens for the same task, automatically generated using a gradient-based method. One challenge for personalized automatic prompt generation for recommendation language models is the extremely large search space, leading to a long convergence time. To effectively and efficiently address the problem, we develop surrogate metrics and leverage an alternative updating schedule for prompting recommendation language models. Experimental results show that our PAP-REC framework manages to generate personalized prompts, and the automatically generated prompts outperform manually constructed prompts and also outperform various baseline recommendation models. The source code of the work is available at https://github.com/rutgerswiselab/PAP-REC.","sentences":["Recently emerged prompt-based Recommendation Language Models (RLM) can solve multiple recommendation tasks uniformly.","The RLMs make full use of the inherited knowledge learned from the abundant pre-training data to solve the downstream recommendation tasks by prompts, without introducing additional parameters or network training.","However, handcrafted prompts require significant expertise and human effort since slightly rewriting prompts may cause massive performance changes.","In this paper, we propose PAP-REC, a framework to generate the Personalized Automatic Prompt for RECommendation language models to mitigate the inefficiency and ineffectiveness problems derived from manually designed prompts.","Specifically, personalized automatic prompts allow different users to have different prompt tokens for the same task, automatically generated using a gradient-based method.","One challenge for personalized automatic prompt generation for recommendation language models is the extremely large search space, leading to a long convergence time.","To effectively and efficiently address the problem, we develop surrogate metrics and leverage an alternative updating schedule for prompting recommendation language models.","Experimental results show that our PAP-REC framework manages to generate personalized prompts, and the automatically generated prompts outperform manually constructed prompts and also outperform various baseline recommendation models.","The source code of the work is available at https://github.com/rutgerswiselab/PAP-REC."],"url":"http://arxiv.org/abs/2402.00284v1","category":"cs.IR"}
{"created":"2024-02-01 01:17:46","title":"Computational Experiments Meet Large Language Model Based Agents: A Survey and Perspective","abstract":"Computational experiments have emerged as a valuable method for studying complex systems, involving the algorithmization of counterfactuals. However, accurately representing real social systems in Agent-based Modeling (ABM) is challenging due to the diverse and intricate characteristics of humans, including bounded rationality and heterogeneity. To address this limitation, the integration of Large Language Models (LLMs) has been proposed, enabling agents to possess anthropomorphic abilities such as complex reasoning and autonomous learning. These agents, known as LLM-based Agent, offer the potential to enhance the anthropomorphism lacking in ABM. Nonetheless, the absence of explicit explainability in LLMs significantly hinders their application in the social sciences. Conversely, computational experiments excel in providing causal analysis of individual behaviors and complex phenomena. Thus, combining computational experiments with LLM-based Agent holds substantial research potential. This paper aims to present a comprehensive exploration of this fusion. Primarily, it outlines the historical development of agent structures and their evolution into artificial societies, emphasizing their importance in computational experiments. Then it elucidates the advantages that computational experiments and LLM-based Agents offer each other, considering the perspectives of LLM-based Agent for computational experiments and vice versa. Finally, this paper addresses the challenges and future trends in this research domain, offering guidance for subsequent related studies.","sentences":["Computational experiments have emerged as a valuable method for studying complex systems, involving the algorithmization of counterfactuals.","However, accurately representing real social systems in Agent-based Modeling (ABM) is challenging due to the diverse and intricate characteristics of humans, including bounded rationality and heterogeneity.","To address this limitation, the integration of Large Language Models (LLMs) has been proposed, enabling agents to possess anthropomorphic abilities such as complex reasoning and autonomous learning.","These agents, known as LLM-based Agent, offer the potential to enhance the anthropomorphism lacking in ABM.","Nonetheless, the absence of explicit explainability in LLMs significantly hinders their application in the social sciences.","Conversely, computational experiments excel in providing causal analysis of individual behaviors and complex phenomena.","Thus, combining computational experiments with LLM-based Agent holds substantial research potential.","This paper aims to present a comprehensive exploration of this fusion.","Primarily, it outlines the historical development of agent structures and their evolution into artificial societies, emphasizing their importance in computational experiments.","Then it elucidates the advantages that computational experiments and LLM-based Agents offer each other, considering the perspectives of LLM-based Agent for computational experiments and vice versa.","Finally, this paper addresses the challenges and future trends in this research domain, offering guidance for subsequent related studies."],"url":"http://arxiv.org/abs/2402.00262v1","category":"cs.AI"}
{"created":"2024-02-01 01:09:00","title":"Towards scalable robotic intervention of children with Autism Spectrum Disorder using LLMs","abstract":"In this paper, we propose a social robot capable of verbally interacting with children with Autism Spectrum Disorder (ASD). This communication is meant to teach perspective-taking using text generated using a Large Language Model (LLM) pipeline. The social robot NAO acts as a stimulator (verbally describes a social situation and asks a question), prompter (presents three options to choose from), and reinforcer (praises when the answer is correct). For the role of the stimulator, the social situation, questions, and options are generated using our LLM pipeline. We compare two approaches: GPT-2 + BART and GPT-2 + GPT-2, where the first GPT-2 common between the pipelines is used for unsupervised social situation generation. We use the SOCIALIQA dataset to fine-tune all of our LLM pipelines. We found that the GPT-2 + BART pipeline had a better BERTscore for generating the questions and the options by combining their individual loss functions. This observation was also consistent with the human evaluations. Lastly, the unsupervised generation of social situations was visualized using T-SNE plots, and the entire pipeline was evaluated for appropriateness for children with ASD by human experts.","sentences":["In this paper, we propose a social robot capable of verbally interacting with children with Autism Spectrum Disorder (ASD).","This communication is meant to teach perspective-taking using text generated using a Large Language Model (LLM) pipeline.","The social robot NAO acts as a stimulator (verbally describes a social situation and asks a question), prompter (presents three options to choose from), and reinforcer (praises when the answer is correct).","For the role of the stimulator, the social situation, questions, and options are generated using our LLM pipeline.","We compare two approaches: GPT-2 + BART and GPT-2 + GPT-2, where the first GPT-2 common between the pipelines is used for unsupervised social situation generation.","We use the SOCIALIQA dataset to fine-tune all of our LLM pipelines.","We found that the GPT-2 + BART pipeline had a better BERTscore for generating the questions and the options by combining their individual loss functions.","This observation was also consistent with the human evaluations.","Lastly, the unsupervised generation of social situations was visualized using T-SNE plots, and the entire pipeline was evaluated for appropriateness for children with ASD by human experts."],"url":"http://arxiv.org/abs/2402.00260v1","category":"cs.RO"}
{"created":"2024-02-01 00:54:48","title":"Vertical Symbolic Regression via Deep Policy Gradient","abstract":"Vertical Symbolic Regression (VSR) recently has been proposed to expedite the discovery of symbolic equations with many independent variables from experimental data. VSR reduces the search spaces following the vertical discovery path by building from reduced-form equations involving a subset of independent variables to full-fledged ones. Proved successful by many symbolic regressors, deep neural networks are expected to further scale up VSR. Nevertheless, directly combining VSR with deep neural networks will result in difficulty in passing gradients and other engineering issues. We propose Vertical Symbolic Regression using Deep Policy Gradient (VSR-DPG) and demonstrate that VSR-DPG can recover ground-truth equations involving multiple input variables, significantly beyond both deep reinforcement learning-based approaches and previous VSR variants. Our VSR-DPG models symbolic regression as a sequential decision-making process, in which equations are built from repeated applications of grammar rules. The integrated deep model is trained to maximize a policy gradient objective. Experimental results demonstrate that our VSR-DPG significantly outperforms popular baselines in identifying both algebraic equations and ordinary differential equations on a series of benchmarks.","sentences":["Vertical Symbolic Regression (VSR) recently has been proposed to expedite the discovery of symbolic equations with many independent variables from experimental data.","VSR reduces the search spaces following the vertical discovery path by building from reduced-form equations involving a subset of independent variables to full-fledged ones.","Proved successful by many symbolic regressors, deep neural networks are expected to further scale up VSR.","Nevertheless, directly combining VSR with deep neural networks will result in difficulty in passing gradients and other engineering issues.","We propose Vertical Symbolic Regression using Deep Policy Gradient (VSR-DPG) and demonstrate that VSR-DPG can recover ground-truth equations involving multiple input variables, significantly beyond both deep reinforcement learning-based approaches and previous VSR variants.","Our VSR-DPG models symbolic regression as a sequential decision-making process, in which equations are built from repeated applications of grammar rules.","The integrated deep model is trained to maximize a policy gradient objective.","Experimental results demonstrate that our VSR-DPG significantly outperforms popular baselines in identifying both algebraic equations and ordinary differential equations on a series of benchmarks."],"url":"http://arxiv.org/abs/2402.00254v1","category":"cs.LG"}
{"created":"2024-02-01 00:23:31","title":"Efficient Non-Parametric Uncertainty Quantification for Black-Box Large Language Models and Decision Planning","abstract":"Step-by-step decision planning with large language models (LLMs) is gaining attention in AI agent development. This paper focuses on decision planning with uncertainty estimation to address the hallucination problem in language models. Existing approaches are either white-box or computationally demanding, limiting use of black-box proprietary LLMs within budgets. The paper's first contribution is a non-parametric uncertainty quantification method for LLMs, efficiently estimating point-wise dependencies between input-decision on the fly with a single inference, without access to token logits. This estimator informs the statistical interpretation of decision trustworthiness. The second contribution outlines a systematic design for a decision-making agent, generating actions like ``turn on the bathroom light'' based on user prompts such as ``take a bath''. Users will be asked to provide preferences when more than one action has high estimated point-wise dependencies. In conclusion, our uncertainty estimation and decision-making agent design offer a cost-efficient approach for AI agent development.","sentences":["Step-by-step decision planning with large language models (LLMs) is gaining attention in AI agent development.","This paper focuses on decision planning with uncertainty estimation to address the hallucination problem in language models.","Existing approaches are either white-box or computationally demanding, limiting use of black-box proprietary LLMs within budgets.","The paper's first contribution is a non-parametric uncertainty quantification method for LLMs, efficiently estimating point-wise dependencies between input-decision on the fly with a single inference, without access to token logits.","This estimator informs the statistical interpretation of decision trustworthiness.","The second contribution outlines a systematic design for a decision-making agent, generating actions like ``turn on the bathroom light'' based on user prompts such as ``take a bath''.","Users will be asked to provide preferences when more than one action has high estimated point-wise dependencies.","In conclusion, our uncertainty estimation and decision-making agent design offer a cost-efficient approach for AI agent development."],"url":"http://arxiv.org/abs/2402.00251v1","category":"cs.LG"}
{"created":"2024-02-01 00:07:23","title":"Towards AI-Assisted Synthesis of Verified Dafny Methods","abstract":"Large stochastic language models show great promise in many domains, including programming. A promise is easy to make but hard to keep, and language models often fail to keep their promises when applied to programming, generating erroneous code. One promising avenue to keep models honest is to have them generate code in a language that supports formal verification: if and when that is adopted, the model would provide proof along with the code, and that proof would be automatically verified. Unfortunately, existing large language models show a severe lack of proficiency in verified programming languages. In this paper we demonstrate how to improve two pretrained models' proficiency in the Dafny verified programming language. Using 178 programming problems from the MBPP dataset, we prompt two contemporary models (GPT-4 and PaLM-2) to generate methods in Dafny. We use three different types of prompts: a direct contextless prompt, a second one that includes a signature of the method and test cases, and a third one that decomposes the problem into steps and includes dynamically chosen similar examples. Our results show that GPT-4 is better than PaLM-2, but that, in both models, the third prompt greatly improves the success of the generation task for the direct prompt. With the third prompt, GPT-4 was able to generate verified (and human-evaluated) Dafny methods in 58% of the cases, while the first prompt generated verified (and human-evaluated) methods in only 19% of the cases. Surprisingly, the second prompt had the worst performance, with only 10%. One tangible contribution of our work is a collection of 153 MBPP problems that are implemented and formally verified in Dafny, 50 of which were written by us and 103 were automatically synthesized by GPT-4. Additionally, our results demonstrate that the benefits of formal program verification (proof of correctness) are now within reach...","sentences":["Large stochastic language models show great promise in many domains, including programming.","A promise is easy to make but hard to keep, and language models often fail to keep their promises when applied to programming, generating erroneous code.","One promising avenue to keep models honest is to have them generate code in a language that supports formal verification: if and when that is adopted, the model would provide proof along with the code, and that proof would be automatically verified.","Unfortunately, existing large language models show a severe lack of proficiency in verified programming languages.","In this paper we demonstrate how to improve two pretrained models' proficiency in the Dafny verified programming language.","Using 178 programming problems from the MBPP dataset, we prompt two contemporary models (GPT-4 and PaLM-2) to generate methods in Dafny.","We use three different types of prompts: a direct contextless prompt, a second one that includes a signature of the method and test cases, and a third one that decomposes the problem into steps and includes dynamically chosen similar examples.","Our results show that GPT-4 is better than PaLM-2, but that, in both models, the third prompt greatly improves the success of the generation task for the direct prompt.","With the third prompt, GPT-4 was able to generate verified (and human-evaluated)","Dafny methods in 58% of the cases, while the first prompt generated verified (and human-evaluated) methods in only 19% of the cases.","Surprisingly, the second prompt had the worst performance, with only 10%.","One tangible contribution of our work is a collection of 153 MBPP problems that are implemented and formally verified in Dafny, 50 of which were written by us and 103 were automatically synthesized by GPT-4.","Additionally, our results demonstrate that the benefits of formal program verification (proof of correctness) are now within reach..."],"url":"http://arxiv.org/abs/2402.00247v1","category":"cs.SE"}
{"created":"2024-02-01 18:51:54","title":"Towards Efficient and Exact Optimization of Language Model Alignment","abstract":"The alignment of language models with human preferences is vital for their application in real-world tasks. The problem is formulated as optimizing the model's policy to maximize the expected reward that reflects human preferences with minimal deviation from the initial policy. While considered as a straightforward solution, reinforcement learning (RL) suffers from high variance in policy updates, which impedes efficient policy improvement. Recently, direct preference optimization (DPO) was proposed to directly optimize the policy from preference data. Though simple to implement, DPO is derived based on the optimal policy that is not assured to be achieved in practice, which undermines its convergence to the intended solution.   In this paper, we propose efficient exact optimization (EXO) of the alignment objective. We prove that EXO is guaranteed to optimize in the same direction as the RL algorithms asymptotically for arbitary parametrization of the policy, while enables efficient optimization by circumventing the complexities associated with RL algorithms. We compare our method to DPO with both theoretical and empirical analyses, and further demonstrate the advantages of our method over existing approaches on realistic human preference data.","sentences":["The alignment of language models with human preferences is vital for their application in real-world tasks.","The problem is formulated as optimizing the model's policy to maximize the expected reward that reflects human preferences with minimal deviation from the initial policy.","While considered as a straightforward solution, reinforcement learning (RL) suffers from high variance in policy updates, which impedes efficient policy improvement.","Recently, direct preference optimization (DPO) was proposed to directly optimize the policy from preference data.","Though simple to implement, DPO is derived based on the optimal policy that is not assured to be achieved in practice, which undermines its convergence to the intended solution.   ","In this paper, we propose efficient exact optimization (EXO) of the alignment objective.","We prove that EXO is guaranteed to optimize in the same direction as the RL algorithms asymptotically for arbitary parametrization of the policy, while enables efficient optimization by circumventing the complexities associated with RL algorithms.","We compare our method to DPO with both theoretical and empirical analyses, and further demonstrate the advantages of our method over existing approaches on realistic human preference data."],"url":"http://arxiv.org/abs/2402.00856v1","category":"cs.CL"}
{"created":"2024-02-01 18:50:35","title":"Two-stroke thermal machine using spin squeezing operation","abstract":"Quantum thermal machines are powerful platforms to investigate how quantum effects impact the energy flow between different systems. We here investigate a two-stroke cycle in which spin squeezing effects are intrinsically switched on during all the operation time. By using the Kitagawa and Ueda's parameter and the l1-norm to compute the degree of spin squeezing and the quantum coherence, we firstly show that the more the spin squeezing effect the more the amount of coherence in the energy basis. Then we employ the characteristic function approach to investigate the engine performance in view of the amount of spin squeezing into the system. Our results show that even assuming an always-on spin squeezing, which is directly associated with the amount of entropy production in the cycle, it is possible to find a better set of efficiency and extracted power for the engine provided a high control over the relevant parameters, i.e., the operation time and the squeezing intensity.","sentences":["Quantum thermal machines are powerful platforms to investigate how quantum effects impact the energy flow between different systems.","We here investigate a two-stroke cycle in which spin squeezing effects are intrinsically switched on during all the operation time.","By using the Kitagawa and Ueda's parameter and the l1-norm to compute the degree of spin squeezing and the quantum coherence, we firstly show that the more the spin squeezing effect the more the amount of coherence in the energy basis.","Then we employ the characteristic function approach to investigate the engine performance in view of the amount of spin squeezing into the system.","Our results show that even assuming an always-on spin squeezing, which is directly associated with the amount of entropy production in the cycle, it is possible to find a better set of efficiency and extracted power for the engine provided a high control over the relevant parameters, i.e., the operation time and the squeezing intensity."],"url":"http://arxiv.org/abs/2402.00852v1","category":"quant-ph"}
{"created":"2024-02-01 18:38:10","title":"Computing scattering resonances of rough obstacles","abstract":"This paper is concerned with the numerical computation of scattering resonances of the Laplacian for Dirichlet obstacles with rough boundary. We prove that under mild geometric assumptions on the obstacle there exists an algorithm whose output is guaranteed to converge to the set of resonances of the problem. The result is formulated using the framework of Solvability Complexity Indices. The proof is constructive and provides an efficient numerical method. The algorithm is based on a combination of a Glazman decomposition, a polygonal approximation of the obstacle and a finite element method. Our result applies in particular to obstacles with fractal boundary, such as the Koch Snowflake and certain filled Julia sets. Finally, we provide numerical experiments in MATLAB for a range of interesting obstacle domains.","sentences":["This paper is concerned with the numerical computation of scattering resonances of the Laplacian for Dirichlet obstacles with rough boundary.","We prove that under mild geometric assumptions on the obstacle there exists an algorithm whose output is guaranteed to converge to the set of resonances of the problem.","The result is formulated using the framework of Solvability Complexity Indices.","The proof is constructive and provides an efficient numerical method.","The algorithm is based on a combination of a Glazman decomposition, a polygonal approximation of the obstacle and a finite element method.","Our result applies in particular to obstacles with fractal boundary, such as the Koch Snowflake and certain filled Julia sets.","Finally, we provide numerical experiments in MATLAB for a range of interesting obstacle domains."],"url":"http://arxiv.org/abs/2402.00846v1","category":"math.NA"}
{"created":"2024-02-01 18:35:50","title":"When to Preempt in a Status Update System?","abstract":"We consider a time slotted status update system with an error-free preemptive queue. The goal of the sampler-scheduler pair is to minimize the age of information at the monitor by sampling and transmitting the freshly sampled update packets to the monitor. The sampler-scheduler pair also has a choice to preempt an old update packet from the server and transmit a new update packet to the server. We formulate this problem as a Markov decision process and find the optimal sampling policy. We show that it is optimal for the sampler-scheduler pair to sample a new packet immediately upon the reception of an update packet at the monitor. We also show that the optimal choice for the scheduler is to preempt an update packet in the server, if the age of that packet crosses a fixed threshold. Finally, we find the optimal preemption threshold when the range of the service time of the server is finite, otherwise we find the $\\epsilon$-optimal preemption threshold.","sentences":["We consider a time slotted status update system with an error-free preemptive queue.","The goal of the sampler-scheduler pair is to minimize the age of information at the monitor by sampling and transmitting the freshly sampled update packets to the monitor.","The sampler-scheduler pair also has a choice to preempt an old update packet from the server and transmit a new update packet to the server.","We formulate this problem as a Markov decision process and find the optimal sampling policy.","We show that it is optimal for the sampler-scheduler pair to sample a new packet immediately upon the reception of an update packet at the monitor.","We also show that the optimal choice for the scheduler is to preempt an update packet in the server, if the age of that packet crosses a fixed threshold.","Finally, we find the optimal preemption threshold when the range of the service time of the server is finite, otherwise we find the $\\epsilon$-optimal preemption threshold."],"url":"http://arxiv.org/abs/2402.00845v1","category":"cs.IT"}
{"created":"2024-02-01 18:26:49","title":"Unveiling nonequilibrium from multifilar events","abstract":"Closely related to the laws of thermodynamics, the detection and quantification of disequilibria are crucial in unraveling the complexities of nature, particularly those beneath observable layers. Theoretical developments in nonequilibrium thermodynamics employ coarse-graining methods to consider a diversity of partial information scenarios that mimic experimental limitations, allowing the inference of properties such as the entropy production rate. A ubiquitous but rather unexplored scenario involves observing events that can possibly arise from many transitions in the underlying Markov process--which we dub $\\textit{multifilar events}$--as in the cases of exchanges measured at particle reservoirs, hidden Markov models, mixed chemical and mechanical transformations in biological function, composite systems, and more. We relax one of the main assumptions in a previously developed framework, based on first-passage problems, to assess the non-Markovian statistics of mutifilar events. By using the asymmetry of event distributions and their waiting-times, we put forward model-free tools to detect nonequilibrium behavior and estimate entropy production, while discussing their suitability for different classes of systems and regimes where they provide no new information, evidence of nonequilibrium, a lower bound for entropy production, or even its exact value. The results are illustrated in reference models through analytics and numerics.","sentences":["Closely related to the laws of thermodynamics, the detection and quantification of disequilibria are crucial in unraveling the complexities of nature, particularly those beneath observable layers.","Theoretical developments in nonequilibrium thermodynamics employ coarse-graining methods to consider a diversity of partial information scenarios that mimic experimental limitations, allowing the inference of properties such as the entropy production rate.","A ubiquitous but rather unexplored scenario involves observing events that can possibly arise from many transitions in the underlying Markov process--which we dub $\\textit{multifilar events}$--as in the cases of exchanges measured at particle reservoirs, hidden Markov models, mixed chemical and mechanical transformations in biological function, composite systems, and more.","We relax one of the main assumptions in a previously developed framework, based on first-passage problems, to assess the non-Markovian statistics of mutifilar events.","By using the asymmetry of event distributions and their waiting-times, we put forward model-free tools to detect nonequilibrium behavior and estimate entropy production, while discussing their suitability for different classes of systems and regimes where they provide no new information, evidence of nonequilibrium, a lower bound for entropy production, or even its exact value.","The results are illustrated in reference models through analytics and numerics."],"url":"http://arxiv.org/abs/2402.00837v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-01 18:12:53","title":"Connected power operations and simplicial Poincar\u00e9 duality","abstract":"We introduce a structure termed ``connected cyclic diagonal'' on a chain complex, which induces stable power operations in its cohomology with the property that negative power operations consistently vanish. This chain level structure is useful to represent power operations for spectra, whose cohomology lacks a cup product. Using a Poincar\\'e duality algebra structure on the integral chains of the standard augmented simplex, we provide an effective method for the construction of cyclic diagonals on a broad class of augmented simplicial objects.","sentences":["We introduce a structure termed ``connected cyclic diagonal'' on a chain complex, which induces stable power operations in its cohomology with the property that negative power operations consistently vanish.","This chain level structure is useful to represent power operations for spectra, whose cohomology lacks a cup product.","Using a Poincar\\'e duality algebra structure on the integral chains of the standard augmented simplex, we provide an effective method for the construction of cyclic diagonals on a broad class of augmented simplicial objects."],"url":"http://arxiv.org/abs/2402.00826v1","category":"math.AT"}
{"created":"2024-02-01 18:11:22","title":"Resolution invariant deep operator network for PDEs with complex geometries","abstract":"Neural operators (NO) are discretization invariant deep learning methods with functional output and can approximate any continuous operator. NO have demonstrated the superiority of solving partial differential equations (PDEs) over other deep learning methods. However, the spatial domain of its input function needs to be identical to its output, which limits its applicability. For instance, the widely used Fourier neural operator (FNO) fails to approximate the operator that maps the boundary condition to the PDE solution. To address this issue, we propose a novel framework called resolution-invariant deep operator (RDO) that decouples the spatial domain of the input and output. RDO is motivated by the Deep operator network (DeepONet) and it does not require retraining the network when the input/output is changed compared with DeepONet. RDO takes functional input and its output is also functional so that it keeps the resolution invariant property of NO. It can also resolve PDEs with complex geometries whereas NO fail. Various numerical experiments demonstrate the advantage of our method over DeepONet and FNO.","sentences":["Neural operators (NO) are discretization invariant deep learning methods with functional output and can approximate any continuous operator.","NO have demonstrated the superiority of solving partial differential equations (PDEs) over other deep learning methods.","However, the spatial domain of its input function needs to be identical to its output, which limits its applicability.","For instance, the widely used Fourier neural operator (FNO) fails to approximate the operator that maps the boundary condition to the PDE solution.","To address this issue, we propose a novel framework called resolution-invariant deep operator (RDO) that decouples the spatial domain of the input and output.","RDO is motivated by the Deep operator network (DeepONet) and it does not require retraining the network when the input/output is changed compared with DeepONet.","RDO takes functional input and its output is also functional so that it keeps the resolution invariant property of NO.","It can also resolve PDEs with complex geometries whereas NO fail.","Various numerical experiments demonstrate the advantage of our method over DeepONet and FNO."],"url":"http://arxiv.org/abs/2402.00825v1","category":"math.NA"}
{"created":"2024-02-01 17:28:56","title":"Analytical methods in Celestial Mechanics: satellites' stability and galactic billiards","abstract":"In this paper, two models of interest for Celestial Mechanics are presented and analysed, using both analytic and numerical techniques, from the point of view of the possible presence of regular and/or chaotic motion, as well as the stability of the considered orbits. The first model, presented in a Hamiltonian formalism, can be used to describe the motion of a satellite around the Earth, taking into account both the non-spherical shape of our planet and the third-body gravitational influence of Sun and Moon. Using semi-analytical techniques coming from Normal Form and Nekhoroshev theories it is possible to provide stability estimates for the orbital elements of its geocentric motion. The second dynamical system presented can be used as a simplified model to describe the motion of a particle in an elliptic galaxy having a central massive core, and is constructed as a refraction billiard where an inner dynamics, induced by a Keplerian potential, is coupled with an external one, where a harmonic oscillator-type potential is considered. The investigation of the dynamics is carried on by using tools of ODEs' theory and is focused on studying the trajectories' properties in terms of periodicity, stability and, possibly, chaoticity.","sentences":["In this paper, two models of interest for Celestial Mechanics are presented and analysed, using both analytic and numerical techniques, from the point of view of the possible presence of regular and/or chaotic motion, as well as the stability of the considered orbits.","The first model, presented in a Hamiltonian formalism, can be used to describe the motion of a satellite around the Earth, taking into account both the non-spherical shape of our planet and the third-body gravitational influence of Sun and Moon.","Using semi-analytical techniques coming from Normal Form and Nekhoroshev theories it is possible to provide stability estimates for the orbital elements of its geocentric motion.","The second dynamical system presented can be used as a simplified model to describe the motion of a particle in an elliptic galaxy having a central massive core, and is constructed as a refraction billiard where an inner dynamics, induced by a Keplerian potential, is coupled with an external one, where a harmonic oscillator-type potential is considered.","The investigation of the dynamics is carried on by using tools of ODEs' theory and is focused on studying the trajectories' properties in terms of periodicity, stability and, possibly, chaoticity."],"url":"http://arxiv.org/abs/2402.00796v1","category":"astro-ph.EP"}
{"created":"2024-02-01 17:22:48","title":"From Pre-Quantum to Post-Quantum IoT Security: A Survey on Quantum-Resistant Cryptosystems for the Internet of Things","abstract":"This article provides a survey on what can be called post-quantum IoT systems (IoT systems protected from the currently known quantum computing attacks): the main post-quantum cryptosystems and initiatives are reviewed, the most relevant IoT architectures and challenges are analyzed, and the expected future trends are indicated. Thus, this paper is aimed at providing a wide view of post-quantum IoT security and give useful guidelines to the future post-quantum IoT developers.","sentences":["This article provides a survey on what can be called post-quantum IoT systems (IoT systems protected from the currently known quantum computing attacks): the main post-quantum cryptosystems and initiatives are reviewed, the most relevant IoT architectures and challenges are analyzed, and the expected future trends are indicated.","Thus, this paper is aimed at providing a wide view of post-quantum IoT security and give useful guidelines to the future post-quantum IoT developers."],"url":"http://arxiv.org/abs/2402.00790v1","category":"cs.CR"}
{"created":"2024-02-01 17:07:09","title":"Effect of radius ratio on the sheared annular centrifugal turbulent convection","abstract":"We perform the linear stability analysis and direct numerical simulations to study the effect of radius ratio on the instability and flow characteristics of the sheared annular centrifugal Rayleigh-B\\'enard convection (ACRBC), where the cold inner cylinder and the hot outer cylinder rotate with a small angular velocity difference. With the shear enhancement, the thermal convection is suppressed and finally gets stable for different radius ratios $\\eta\\in[0.2, 0.95]$. Considering the inhomogeneous distribution of shear stresses in the base flow, a new global Richardson number $Ri_g$ is defined and the marginal-state curves for different radius ratios are successfully unified in the parameter domain of $Ri_g$ and the Rayleigh number $Ra$. The results are consistent with the marginal-state curve of the wall-sheared classical RBC in the streamwise direction, demonstrating that the basic stabilization mechanisms are identical. Moreover, systems with small radius ratios exhibit greater geometric asymmetry. On the one hand, this results in a smaller equivalent aspect ratio for the system, accommodating fewer convection roll pairs. Fewer roll pairs are more likely to cause a transition in the flow structure during shear enhancement. On the other hand, the shear distribution is more inhomogeneous, allowing for an outward shift of the convection region and the elevation of bulk temperature under strong shear.","sentences":["We perform the linear stability analysis and direct numerical simulations to study the effect of radius ratio on the instability and flow characteristics of the sheared annular centrifugal Rayleigh-B\\'enard convection (ACRBC), where the cold inner cylinder and the hot outer cylinder rotate with a small angular velocity difference.","With the shear enhancement, the thermal convection is suppressed and finally gets stable for different radius ratios $\\eta\\in[0.2, 0.95]$.","Considering the inhomogeneous distribution of shear stresses in the base flow, a new global Richardson number $Ri_g$ is defined and the marginal-state curves for different radius ratios are successfully unified in the parameter domain of $Ri_g$ and the Rayleigh number $Ra$.","The results are consistent with the marginal-state curve of the wall-sheared classical RBC in the streamwise direction, demonstrating that the basic stabilization mechanisms are identical.","Moreover, systems with small radius ratios exhibit greater geometric asymmetry.","On the one hand, this results in a smaller equivalent aspect ratio for the system, accommodating fewer convection roll pairs.","Fewer roll pairs are more likely to cause a transition in the flow structure during shear enhancement.","On the other hand, the shear distribution is more inhomogeneous, allowing for an outward shift of the convection region and the elevation of bulk temperature under strong shear."],"url":"http://arxiv.org/abs/2402.00779v1","category":"physics.flu-dyn"}
{"created":"2024-02-01 17:05:37","title":"Hybrid Quantum Vision Transformers for Event Classification in High Energy Physics","abstract":"Models based on vision transformer architectures are considered state-of-the-art when it comes to image classification tasks. However, they require extensive computational resources both for training and deployment. The problem is exacerbated as the amount and complexity of the data increases. Quantum-based vision transformer models could potentially alleviate this issue by reducing the training and operating time while maintaining the same predictive power. Although current quantum computers are not yet able to perform high-dimensional tasks yet, they do offer one of the most efficient solutions for the future. In this work, we construct several variations of a quantum hybrid vision transformer for a classification problem in high energy physics (distinguishing photons and electrons in the electromagnetic calorimeter). We test them against classical vision transformer architectures. Our findings indicate that the hybrid models can achieve comparable performance to their classical analogues with a similar number of parameters.","sentences":["Models based on vision transformer architectures are considered state-of-the-art when it comes to image classification tasks.","However, they require extensive computational resources both for training and deployment.","The problem is exacerbated as the amount and complexity of the data increases.","Quantum-based vision transformer models could potentially alleviate this issue by reducing the training and operating time while maintaining the same predictive power.","Although current quantum computers are not yet able to perform high-dimensional tasks yet, they do offer one of the most efficient solutions for the future.","In this work, we construct several variations of a quantum hybrid vision transformer for a classification problem in high energy physics (distinguishing photons and electrons in the electromagnetic calorimeter).","We test them against classical vision transformer architectures.","Our findings indicate that the hybrid models can achieve comparable performance to their classical analogues with a similar number of parameters."],"url":"http://arxiv.org/abs/2402.00776v1","category":"quant-ph"}
{"created":"2024-02-01 17:04:41","title":"Adaptive Control for Triadic Human-Robot-FES Collaboration in Gait Rehabilitation: A Pilot Study","abstract":"The hybridisation of robot-assisted gait training and functional electrical stimulation (FES) can provide numerous physiological benefits to neurological patients. However, the design of an effective hybrid controller poses significant challenges. In this over-actuated system, it is extremely difficult to find the right balance between robotic assistance and FES that will provide personalised assistance, prevent muscle fatigue and encourage the patient's active participation in order to accelerate recovery. In this paper, we present an adaptive hybrid robot-FES controller to do this and enable the triadic collaboration between the patient, the robot and FES. A patient-driven controller is designed where the voluntary movement of the patient is prioritised and assistance is provided using FES and the robot in a hierarchical order depending on the patient's performance and their muscles' fitness. The performance of this hybrid adaptive controller is tested in simulation and on one healthy subject. Our results indicate an increase in tracking performance with lower overall assistance, and less muscle fatigue when the hybrid adaptive controller is used, compared to its non adaptive equivalent. This suggests that our hybrid adaptive controller may be able to adapt to the behaviour of the user to provide assistance as needed and prevent the early termination of physical therapy due to muscle fatigue.","sentences":["The hybridisation of robot-assisted gait training and functional electrical stimulation (FES) can provide numerous physiological benefits to neurological patients.","However, the design of an effective hybrid controller poses significant challenges.","In this over-actuated system, it is extremely difficult to find the right balance between robotic assistance and FES that will provide personalised assistance, prevent muscle fatigue and encourage the patient's active participation in order to accelerate recovery.","In this paper, we present an adaptive hybrid robot-FES controller to do this and enable the triadic collaboration between the patient, the robot and FES.","A patient-driven controller is designed where the voluntary movement of the patient is prioritised and assistance is provided using FES and the robot in a hierarchical order depending on the patient's performance and their muscles' fitness.","The performance of this hybrid adaptive controller is tested in simulation and on one healthy subject.","Our results indicate an increase in tracking performance with lower overall assistance, and less muscle fatigue when the hybrid adaptive controller is used, compared to its non adaptive equivalent.","This suggests that our hybrid adaptive controller may be able to adapt to the behaviour of the user to provide assistance as needed and prevent the early termination of physical therapy due to muscle fatigue."],"url":"http://arxiv.org/abs/2402.00775v1","category":"cs.RO"}
{"created":"2024-02-01 17:02:20","title":"On the Choice of Loss Function in Learning-based Optimal Power Flow","abstract":"We analyze and contrast two ways to train machine learning models for solving AC optimal power flow (OPF) problems, distinguished with the loss functions used. The first trains a mapping from the loads to the optimal dispatch decisions, utilizing mean square error (MSE) between predicted and optimal dispatch decisions as the loss function. The other intends to learn the same mapping, but directly uses the OPF cost of the predicted decisions, referred to as decision loss, as the loss function. In addition to better aligning with the OPF cost which results in reduced suboptimality, the use of decision loss can circumvent feasibility issues that arise with MSE when the underlying mapping from loads to optimal dispatch is discontinuous. Since decision loss does not capture the OPF constraints, we further develop a neural network with a specific structure and introduce a modified training algorithm incorporating Lagrangian duality to improve feasibility.} This result in an improved performance measured by feasibility and suboptimality as demonstrated with an IEEE 39-bus case study.","sentences":["We analyze and contrast two ways to train machine learning models for solving AC optimal power flow (OPF) problems, distinguished with the loss functions used.","The first trains a mapping from the loads to the optimal dispatch decisions, utilizing mean square error (MSE) between predicted and optimal dispatch decisions as the loss function.","The other intends to learn the same mapping, but directly uses the OPF cost of the predicted decisions, referred to as decision loss, as the loss function.","In addition to better aligning with the OPF cost which results in reduced suboptimality, the use of decision loss can circumvent feasibility issues that arise with MSE when the underlying mapping from loads to optimal dispatch is discontinuous.","Since decision loss does not capture the OPF constraints, we further develop a neural network with a specific structure and introduce a modified training algorithm incorporating Lagrangian duality to improve feasibility.}","This result in an improved performance measured by feasibility and suboptimality as demonstrated with an IEEE 39-bus case study."],"url":"http://arxiv.org/abs/2402.00773v1","category":"eess.SY"}
{"created":"2024-02-01 16:45:31","title":"To tweak or not to tweak. How exploiting flexibilities in gene set analysis leads to over-optimism","abstract":"Gene set analysis, a popular approach for analysing high-throughput gene expression data, aims to identify sets of genes that show enriched expression patterns between two conditions. In addition to the multitude of methods available for this task, users are typically left with many options when creating the required input and specifying the internal parameters of the chosen method. This flexibility can lead to uncertainty about the 'right' choice, further reinforced by a lack of evidence-based guidance. Especially when their statistical experience is scarce, this uncertainty might entice users to produce preferable results using a 'trial-and-error' approach. While it may seem unproblematic at first glance, this practice can be viewed as a form of 'cherry-picking' and cause an optimistic bias, rendering the results non-replicable on independent data. After this problem has attracted a lot of attention in the context of classical hypothesis testing, we now aim to raise awareness of such over-optimism in the different and more complex context of gene set analyses. We mimic a hypothetical researcher who systematically selects the analysis variants yielding their preferred results, thereby considering three distinct goals they might pursue. Using a selection of popular gene set analysis methods, we tweak the results in this way for two frequently used benchmark gene expression data sets. Our study indicates that the potential for over-optimism is particularly high for a group of methods frequently used despite being commonly criticised. We conclude by providing practical recommendations to counter over-optimism in research findings in gene set analysis and beyond.","sentences":["Gene set analysis, a popular approach for analysing high-throughput gene expression data, aims to identify sets of genes that show enriched expression patterns between two conditions.","In addition to the multitude of methods available for this task, users are typically left with many options when creating the required input and specifying the internal parameters of the chosen method.","This flexibility can lead to uncertainty about the 'right' choice, further reinforced by a lack of evidence-based guidance.","Especially when their statistical experience is scarce, this uncertainty might entice users to produce preferable results using a 'trial-and-error' approach.","While it may seem unproblematic at first glance, this practice can be viewed as a form of 'cherry-picking' and cause an optimistic bias, rendering the results non-replicable on independent data.","After this problem has attracted a lot of attention in the context of classical hypothesis testing, we now aim to raise awareness of such over-optimism in the different and more complex context of gene set analyses.","We mimic a hypothetical researcher who systematically selects the analysis variants yielding their preferred results, thereby considering three distinct goals they might pursue.","Using a selection of popular gene set analysis methods, we tweak the results in this way for two frequently used benchmark gene expression data sets.","Our study indicates that the potential for over-optimism is particularly high for a group of methods frequently used despite being commonly criticised.","We conclude by providing practical recommendations to counter over-optimism in research findings in gene set analysis and beyond."],"url":"http://arxiv.org/abs/2402.00754v1","category":"stat.AP"}
{"created":"2024-02-01 16:42:44","title":"A System Level Analysis for Integrated Sensing and Communication","abstract":"In this work, we provide a system level analysis of integrated sensing and communication (ISAC) systems, where a setup with a mono-static dual-functional radar communication base station is assumed. We derive the ISAC signal-to-noise ratio (SNR) equation that relates communication and radar SNR for different distances. We also derive the ISAC range equation, which can be used for sensing-assisted beamforming applications. Specifically, we show that increasing the frequency and bandwidth is more favorable to the radar application in terms of relative SNR and range while increasing the transmit power is more favorable to communications. Numerical examples reveal that if the range for communication and radar is desired to be in the same order, the ISAC system should operate in mmWave or sub-THz bands, whereas sub-6GHz allows scenarios where the communication range is of orders of magnitude higher than that of radar.","sentences":["In this work, we provide a system level analysis of integrated sensing and communication (ISAC) systems, where a setup with a mono-static dual-functional radar communication base station is assumed.","We derive the ISAC signal-to-noise ratio (SNR) equation that relates communication and radar SNR for different distances.","We also derive the ISAC range equation, which can be used for sensing-assisted beamforming applications.","Specifically, we show that increasing the frequency and bandwidth is more favorable to the radar application in terms of relative SNR and range while increasing the transmit power is more favorable to communications.","Numerical examples reveal that if the range for communication and radar is desired to be in the same order, the ISAC system should operate in mmWave or sub-THz bands, whereas sub-6GHz allows scenarios where the communication range is of orders of magnitude higher than that of radar."],"url":"http://arxiv.org/abs/2402.00750v1","category":"eess.SP"}
{"created":"2024-02-01 16:33:06","title":"An Analytical Approach for Intermodal Urban Transportation Network Equilibrium including Shared Mobility Services","abstract":"Shared Mobility Services (SMSs) are reshaping urban transportation systems by providing flexible mobility options. With their capacity to decrease the number of cars on the roads, these services can potentially improve the transportation system's performance in terms of travel times and emissions. This emphasizes the importance of analyzing and understanding their impacts on the system and users' choices, especially when integrated into a complex multi-modal system, including public transport (PT). Many studies overlook the synergies between SMSs and PT, leading to inaccurate traffic estimations and planning. This research offers an exhaustive review of multi-modal transportation system models involving SMSs. We then introduce a traffic assignment analytical model framed as a Mixed-Integer Quadratic Problem (MIQP). This model comprises diverse travel possibilities, including SMSs, and handles intermodality by allowing commuters to combine modes to optimize time and monetary expense. An in-depth examination of commuters' behavior on two test cases and an analysis of the price of anarchy highlights the disparities between user equilibrium and system optimum in such intricate systems.","sentences":["Shared Mobility Services (SMSs) are reshaping urban transportation systems by providing flexible mobility options.","With their capacity to decrease the number of cars on the roads, these services can potentially improve the transportation system's performance in terms of travel times and emissions.","This emphasizes the importance of analyzing and understanding their impacts on the system and users' choices, especially when integrated into a complex multi-modal system, including public transport (PT).","Many studies overlook the synergies between SMSs and PT, leading to inaccurate traffic estimations and planning.","This research offers an exhaustive review of multi-modal transportation system models involving SMSs.","We then introduce a traffic assignment analytical model framed as a Mixed-Integer Quadratic Problem (MIQP).","This model comprises diverse travel possibilities, including SMSs, and handles intermodality by allowing commuters to combine modes to optimize time and monetary expense.","An in-depth examination of commuters' behavior on two test cases and an analysis of the price of anarchy highlights the disparities between user equilibrium and system optimum in such intricate systems."],"url":"http://arxiv.org/abs/2402.00735v1","category":"math.OC"}
{"created":"2024-02-01 16:30:00","title":"MobilityDL: A Review of Deep Learning From Trajectory Data","abstract":"Trajectory data combines the complexities of time series, spatial data, and (sometimes irrational) movement behavior. As data availability and computing power have increased, so has the popularity of deep learning from trajectory data. This review paper provides the first comprehensive overview of deep learning approaches for trajectory data. We have identified eight specific mobility use cases which we analyze with regards to the deep learning models and the training data used. Besides a comprehensive quantitative review of the literature since 2018, the main contribution of our work is the data-centric analysis of recent work in this field, placing it along the mobility data continuum which ranges from detailed dense trajectories of individual movers (quasi-continuous tracking data), to sparse trajectories (such as check-in data), and aggregated trajectories (crowd information).","sentences":["Trajectory data combines the complexities of time series, spatial data, and (sometimes irrational) movement behavior.","As data availability and computing power have increased, so has the popularity of deep learning from trajectory data.","This review paper provides the first comprehensive overview of deep learning approaches for trajectory data.","We have identified eight specific mobility use cases which we analyze with regards to the deep learning models and the training data used.","Besides a comprehensive quantitative review of the literature since 2018, the main contribution of our work is the data-centric analysis of recent work in this field, placing it along the mobility data continuum which ranges from detailed dense trajectories of individual movers (quasi-continuous tracking data), to sparse trajectories (such as check-in data), and aggregated trajectories (crowd information)."],"url":"http://arxiv.org/abs/2402.00732v1","category":"cs.LG"}
{"created":"2024-02-01 16:12:15","title":"Orientation-aware Incremental Potential Contact","abstract":"The Incremental Potential Contact (IPC) method enables robust complex simulations of deformable objects with contact and friction. The key to IPC's robustness is its strict adherence to geometric constraints, avoiding intersections, which are a common cause of robustness issues in contact mechanics. A key element of the IPC approach to contact is a geometric barrier function, which is defined directly in the discrete setting. While IPC achieves its main goal of providing guarantees for contact constraints, its parameters need to be chosen carefully to avoid significant simulation artifacts and inaccuracies. We present a systematic derivation of an IPC-like continuum potential defined for smooth and piecewise smooth surfaces, starting from identifying a set of natural requirements for contact potentials, including the barrier property, locality, differentiable dependence of shape, and absence of forces in rest configurations, based on the idea of candidate sets. Our potential is formulated in a way independent of surface discretization.   This new potential is suitable for piecewise-linear surfaces and its efficiency is similar to standard IPC. We demonstrate its behavior and compare it to IPC on a range of challenging contact examples.","sentences":["The Incremental Potential Contact (IPC) method enables robust complex simulations of deformable objects with contact and friction.","The key to IPC's robustness is its strict adherence to geometric constraints, avoiding intersections, which are a common cause of robustness issues in contact mechanics.","A key element of the IPC approach to contact is a geometric barrier function, which is defined directly in the discrete setting.","While IPC achieves its main goal of providing guarantees for contact constraints, its parameters need to be chosen carefully to avoid significant simulation artifacts and inaccuracies.","We present a systematic derivation of an IPC-like continuum potential defined for smooth and piecewise smooth surfaces, starting from identifying a set of natural requirements for contact potentials, including the barrier property, locality, differentiable dependence of shape, and absence of forces in rest configurations, based on the idea of candidate sets.","Our potential is formulated in a way independent of surface discretization.   ","This new potential is suitable for piecewise-linear surfaces and its efficiency is similar to standard IPC.","We demonstrate its behavior and compare it to IPC on a range of challenging contact examples."],"url":"http://arxiv.org/abs/2402.00719v1","category":"cs.GR"}
{"created":"2024-02-01 16:09:39","title":"Investigation of fluorescence versus transmission readout for three-photon Rydberg excitation used in electrometry","abstract":"We present a three-photon based fluorescence readout method where the strength of the fluorescence scales with the strength of the radio-frequency (RF) field being applied. We compare this method to conventional three-photon electromagnetically-induced transparency (EIT) and electromagnetically-induced absorption (EIA). Our demonstrated EIA/EIT sensitivity in the collinear three-photon Cesium system is the best reported to date at roughly 30 uVm^{-1}Hz^{-1/2}. The fluorescence is nearly 4 fold better in sensitivity compared to EIA/EIT readout.","sentences":["We present a three-photon based fluorescence readout method where the strength of the fluorescence scales with the strength of the radio-frequency (RF) field being applied.","We compare this method to conventional three-photon electromagnetically-induced transparency (EIT) and electromagnetically-induced absorption (EIA).","Our demonstrated EIA/EIT sensitivity in the collinear three-photon Cesium system is the best reported to date at roughly 30 uVm^{-1}Hz^{-1/2}.","The fluorescence is nearly 4 fold better in sensitivity compared to EIA/EIT readout."],"url":"http://arxiv.org/abs/2402.00718v1","category":"quant-ph"}
{"created":"2024-02-01 16:08:25","title":"Neutral carbon in diffuse interstellar medium: abundance matching with H2 for DLAs at high redshifts","abstract":"We present the study of CI/H$_2$ relative abundance in the diffuse cold neutral medium. Using the chemical and thermal balance model we calculated the dependence of CI/H$_2$ on the main parameters of the medium: hydrogen number density, metallicity, strength of the UV field, and cosmic ray ionization rate (CRIR). We show that observed relative CI and H$_2$ column densities in damped Lyman alpha systems (DLAs) at high redshifts can be reproduced within our model assuming the typically expected conditions in the diffuse cold neutral medium (CNM). Using the additional observed information the on metallicity, HI column density, and excitation of CI fine-structure levels, as well as temperature we estimated for a wide range metallicities in the CNM at high redshifts that CRIRs to be in the range from $\\sim10^{-16}$ to $\\rm few \\times 10^{-15}\\rm s^{-1}$, hydrogen number densities to be in range $\\sim10 - 10^3$cm$^{-3}$, and UV field in range from $10^{-2}$ to $\\rm few \\times 10^2$ of Mathis field. We argue, that since the observed quantities used in this work are quite homogeneous and much less affected by the radiative transfer effects (in comparison with e.g. dissociation of HD and UV pumping of H$_2$ rotational levels) our estimates are quite robust against the assumption of the exact geometrical model of the cloud and local sources of the UV field.","sentences":["We present the study of CI/H$_2$ relative abundance in the diffuse cold neutral medium.","Using the chemical and thermal balance model we calculated the dependence of CI/H$_2$ on the main parameters of the medium: hydrogen number density, metallicity, strength of the UV field, and cosmic ray ionization rate (CRIR).","We show that observed relative CI and H$_2$ column densities in damped Lyman alpha systems (DLAs) at high redshifts can be reproduced within our model assuming the typically expected conditions in the diffuse cold neutral medium (CNM).","Using the additional observed information the on metallicity, HI column density, and excitation of CI fine-structure levels, as well as temperature we estimated for a wide range metallicities in the CNM at high redshifts that CRIRs to be in the range from $\\sim10^{-16}$ to $\\rm few \\times 10^{-15}\\rm s^{-1}$, hydrogen number densities to be in range $\\sim10 - 10^3$cm$^{-3}$, and UV field in range from $10^{-2}$ to $\\rm few \\times 10^2$ of Mathis field.","We argue, that since the observed quantities used in this work are quite homogeneous and much less affected by the radiative transfer effects (in comparison with e.g. dissociation of HD and UV pumping of H$_2$ rotational levels)","our estimates are quite robust against the assumption of the exact geometrical model of the cloud and local sources of the UV field."],"url":"http://arxiv.org/abs/2402.00714v1","category":"astro-ph.GA"}
{"created":"2024-02-01 16:07:12","title":"ChaosBench: A Multi-Channel, Physics-Based Benchmark for Subseasonal-to-Seasonal Climate Prediction","abstract":"Accurate prediction of climate in the subseasonal-to-seasonal scale is crucial for disaster readiness, reduced economic risk, and improved policy-making amidst climate change. Yet, S2S prediction remains challenging due to the chaotic nature of the system. At present, existing benchmarks for weather and climate applications, tend to (1) have shorter forecasting range of up-to 14 days, (2) do not include a wide range of operational baseline forecasts, and (3) lack physics-based constraints for explainability. Thus, we propose ChaosBench, a large-scale, multi-channel, physics-based benchmark for S2S prediction. ChaosBench has over 460K frames of real-world observations and simulations, each with 60 variable-channels and spanning for up-to 45 years. We also propose several physics-based, in addition to vision-based metrics, that enables for a more physically-consistent model. Furthermore, we include a diverse set of physics-based forecasts from 4 national weather agencies as baselines to our data-driven counterpart. We establish two tasks that vary in complexity: full and sparse dynamics prediction. Our benchmark is one of the first to perform large-scale evaluation on existing models including PanguWeather, FourCastNetV2, GraphCast, and ClimaX, and finds methods originally developed for weather-scale applications fails on S2S task. We release our benchmark code and datasets at https://leap-stc.github.io/ChaosBench.","sentences":["Accurate prediction of climate in the subseasonal-to-seasonal scale is crucial for disaster readiness, reduced economic risk, and improved policy-making amidst climate change.","Yet, S2S prediction remains challenging due to the chaotic nature of the system.","At present, existing benchmarks for weather and climate applications, tend to (1) have shorter forecasting range of up-to 14 days, (2) do not include a wide range of operational baseline forecasts, and (3) lack physics-based constraints for explainability.","Thus, we propose ChaosBench, a large-scale, multi-channel, physics-based benchmark for S2S prediction.","ChaosBench has over 460K frames of real-world observations and simulations, each with 60 variable-channels and spanning for up-to 45 years.","We also propose several physics-based, in addition to vision-based metrics, that enables for a more physically-consistent model.","Furthermore, we include a diverse set of physics-based forecasts from 4 national weather agencies as baselines to our data-driven counterpart.","We establish two tasks that vary in complexity: full and sparse dynamics prediction.","Our benchmark is one of the first to perform large-scale evaluation on existing models including PanguWeather, FourCastNetV2, GraphCast, and ClimaX, and finds methods originally developed for weather-scale applications fails on S2S task.","We release our benchmark code and datasets at https://leap-stc.github.io/ChaosBench."],"url":"http://arxiv.org/abs/2402.00712v1","category":"cs.CV"}
{"created":"2024-02-01 16:04:36","title":"Benchmarking human-robot collaborative assembly tasks","abstract":"Manufacturing assembly tasks can vary in complexity and level of automation. Yet, achieving full automation can be challenging and inefficient, particularly due to the complexity of certain assembly operations. Human-robot collaborative work, leveraging the strengths of human labor alongside the capabilities of robots, can be a solution for enhancing efficiency. This paper introduces the CT benchmark, a benchmark and model set designed to facilitate the testing and evaluation of human-robot collaborative assembly scenarios. It was designed to compare manual and automatic processes using metrics such as the assembly time and human workload. The components of the model set can be assembled through the most common assembly tasks, each with varying levels of difficulty. The CT benchmark was designed with a focus on its applicability in human-robot collaborative environments, with the aim of ensuring the reproducibility and replicability of experiments. Experiments were carried out to assess assembly performance in three different setups (manual, automatic and collaborative), measuring metrics related to the assembly time and the workload on human operators. The results suggest that the collaborative approach takes longer than the fully manual assembly, with an increase of 70.8%. However, users reported a lower overall workload, as well as reduced mental demand, physical demand, and effort according to the NASA-TLX questionnaire.","sentences":["Manufacturing assembly tasks can vary in complexity and level of automation.","Yet, achieving full automation can be challenging and inefficient, particularly due to the complexity of certain assembly operations.","Human-robot collaborative work, leveraging the strengths of human labor alongside the capabilities of robots, can be a solution for enhancing efficiency.","This paper introduces the CT benchmark, a benchmark and model set designed to facilitate the testing and evaluation of human-robot collaborative assembly scenarios.","It was designed to compare manual and automatic processes using metrics such as the assembly time and human workload.","The components of the model set can be assembled through the most common assembly tasks, each with varying levels of difficulty.","The CT benchmark was designed with a focus on its applicability in human-robot collaborative environments, with the aim of ensuring the reproducibility and replicability of experiments.","Experiments were carried out to assess assembly performance in three different setups (manual, automatic and collaborative), measuring metrics related to the assembly time and the workload on human operators.","The results suggest that the collaborative approach takes longer than the fully manual assembly, with an increase of 70.8%.","However, users reported a lower overall workload, as well as reduced mental demand, physical demand, and effort according to the NASA-TLX questionnaire."],"url":"http://arxiv.org/abs/2402.00708v1","category":"cs.RO"}
{"created":"2024-02-01 15:54:47","title":"Combining Belief Function Theory and Stochastic Model Predictive Control for Multi-Modal Uncertainty in Autonomous Driving","abstract":"In automated driving, predicting and accommodating the uncertain future motion of other traffic participants is challenging, especially in unstructured environments in which the high-level intention of traffic participants is difficult to predict. Several possible uncertain future behaviors of traffic participants must be considered, resulting in multi-modal uncertainty. We propose a novel combination of Belief Function Theory and Stochastic Model Predictive Control for trajectory planning of the autonomous vehicle in presence of significant uncertainty about the intention estimation of traffic participants. A misjudgment of the intention of traffic participants may result in dangerous situations. At the same time, excessive conservatism must be avoided. Therefore, the measure of reliability of the estimation provided by Belief Function Theory is used in the design of collision-avoidance safety constraints, in particular to increase safety when the intention of traffic participants is not clear. We discuss two methods to leverage on Belief Function Theory: we introduce a novel belief-to-probability transformation designed not to underestimate unlikely events if the information is uncertain, and a constraint tightening mechanism using the reliability of the estimation. We evaluate our proposal through simulations comparing to state-of-the-art approaches.","sentences":["In automated driving, predicting and accommodating the uncertain future motion of other traffic participants is challenging, especially in unstructured environments in which the high-level intention of traffic participants is difficult to predict.","Several possible uncertain future behaviors of traffic participants must be considered, resulting in multi-modal uncertainty.","We propose a novel combination of Belief Function Theory and Stochastic Model Predictive Control for trajectory planning of the autonomous vehicle in presence of significant uncertainty about the intention estimation of traffic participants.","A misjudgment of the intention of traffic participants may result in dangerous situations.","At the same time, excessive conservatism must be avoided.","Therefore, the measure of reliability of the estimation provided by Belief Function Theory is used in the design of collision-avoidance safety constraints, in particular to increase safety when the intention of traffic participants is not clear.","We discuss two methods to leverage on Belief Function Theory: we introduce a novel belief-to-probability transformation designed not to underestimate unlikely events if the information is uncertain, and a constraint tightening mechanism using the reliability of the estimation.","We evaluate our proposal through simulations comparing to state-of-the-art approaches."],"url":"http://arxiv.org/abs/2402.00697v1","category":"eess.SY"}
{"created":"2024-02-01 15:50:46","title":"In-plane anisotropic magnetoresistance in detwinned $BaFe_{2-x}Ni_{x}As_{2}$ ($x$ = 0, 0.6)","abstract":"Understanding the magnetoresistance (MR) of a magnetic material forms the basis for uncovering the orbital mechanisms and charge-spin interactions in the system. Although the parent state of iron-based high-temperature superconductors, including $BaFe_2As_2$, exhibits unusual electron transport properties resulting from spin and charge correlations, there is still valuable insight to be gained by understanding the in-plane MR effect due to twin domains in the orthorhombic antiferromagnetic (AF) ordered state. Here, we study the in-plane magnetoresistance anisotropy in detwinned $BaFe_2As_2$ and compare the results to the non-magnetic Ni-doped sample. We find that in the antiferromagnetically ordered state, $BaFe_2As_2$ exhibits anisotropic MR that becomes large at low temperatures and high fields. Both transverse and longitudinal MRs are highly anisotropic and dependent on the field and current orientations. These results cannot be fully explained by calculations considering only the anisotropic Fermi surface. Instead, the spin orientation of the ordered moment also affects the MR effect, suggesting the presence of a large charge-spin interaction in $BaFe_2As_2$ that is not present in the Ni-doped material.","sentences":["Understanding the magnetoresistance (MR) of a magnetic material forms the basis for uncovering the orbital mechanisms and charge-spin interactions in the system.","Although the parent state of iron-based high-temperature superconductors, including $BaFe_2As_2$, exhibits unusual electron transport properties resulting from spin and charge correlations, there is still valuable insight to be gained by understanding the in-plane MR effect due to twin domains in the orthorhombic antiferromagnetic (AF) ordered state.","Here, we study the in-plane magnetoresistance anisotropy in detwinned $BaFe_2As_2$ and compare the results to the non-magnetic Ni-doped sample.","We find that in the antiferromagnetically ordered state, $BaFe_2As_2$ exhibits anisotropic MR that becomes large at low temperatures and high fields.","Both transverse and longitudinal MRs are highly anisotropic and dependent on the field and current orientations.","These results cannot be fully explained by calculations considering only the anisotropic Fermi surface.","Instead, the spin orientation of the ordered moment also affects the MR effect, suggesting the presence of a large charge-spin interaction in $BaFe_2As_2$ that is not present in the Ni-doped material."],"url":"http://arxiv.org/abs/2402.00693v1","category":"cond-mat.str-el"}
{"created":"2024-02-01 15:50:07","title":"On uniform recurrence for hyperbolic automorphisms of the $2$-dimensional torus","abstract":"We are interested in studying sets of the form \\[ \\mathcal{U}(\\alpha) :=   \\left\\{ x\\in X: \\ \\exists M=M(x) \\geq 1 \\text{ such that } \\forall N\\geq M, \\ \\exists n\\leq N \\text{ such that } d(T^nx, x) \\leq |\\lambda|^{-\\alpha N} \\right\\} \\] where $(X,T,d)$ is our metric dynamical system and $|\\lambda|>1$. Although a lot of results exist for the one dimensional case, not as many are known for systems in higher dimensions and especially in the hyperbolic case. We consider $X=\\mathbb{T}^2$, $T(x) = Ax \\pmod{1}$, where $A$ is a hyperbolic, area preserving, $2\\times 2$ matrix with integer entries and $\\lambda$ is the eigenvalue of $A$ of modulus larger than $1$ and we explicitly calculate the Hausdorff dimension of this set.","sentences":["We are interested in studying sets of the form \\[ \\mathcal{U}(\\alpha) :=   \\left\\{ x\\in X: \\ \\exists M=M(x) \\geq 1","\\text{ such that } \\forall N\\geq M, \\ \\exists n\\leq N \\text{ such that } d(T^nx, x) \\leq |\\lambda|^{-\\alpha N} \\right\\} \\] where $(X,T,d)$ is our metric dynamical system and $|\\lambda|>1$. Although a lot of results exist for the one dimensional case, not as many are known for systems in higher dimensions and especially in the hyperbolic case.","We consider $X=\\mathbb{T}^2$, $T(x) = Ax \\pmod{1}$, where $A$ is a hyperbolic, area preserving, $2\\times 2$ matrix with integer entries and $\\lambda$ is the eigenvalue of $A$ of modulus larger than $1$ and we explicitly calculate the Hausdorff dimension of this set."],"url":"http://arxiv.org/abs/2402.00690v1","category":"math.DS"}
{"created":"2024-02-01 15:47:01","title":"An Investigation of Hardware Security Bug Characteristics in Open-Source Projects","abstract":"Hardware security is an important concern of system security as vulnerabilities can arise from design errors introduced throughout the development lifecycle. Recent works have proposed techniques to detect hardware security bugs, such as static analysis, fuzzing, and symbolic execution. However, the fundamental properties of hardware security bugs remain relatively unexplored. To gain a better understanding of hardware security bugs, we perform a deep dive into the popular OpenTitan project, including its bug reports and bug fixes. We manually classify the bugs as relevant to functionality or security and analyze characteristics, such as the impact and location of security bugs, and the size of their bug fixes. We also investigate relationships between security impact and bug management during development. Finally, we propose an abstract syntax tree-based analysis to identify the syntactic characteristics of bug fixes. Our results show that 53% of the bugs in OpenTitan have potential security implications and that 55% of all bug fixes modify only one file. Our findings underscore the importance of security-aware development practices and tools and motivate the development of techniques that leverage the highly localized nature of hardware bugs.","sentences":["Hardware security is an important concern of system security as vulnerabilities can arise from design errors introduced throughout the development lifecycle.","Recent works have proposed techniques to detect hardware security bugs, such as static analysis, fuzzing, and symbolic execution.","However, the fundamental properties of hardware security bugs remain relatively unexplored.","To gain a better understanding of hardware security bugs, we perform a deep dive into the popular OpenTitan project, including its bug reports and bug fixes.","We manually classify the bugs as relevant to functionality or security and analyze characteristics, such as the impact and location of security bugs, and the size of their bug fixes.","We also investigate relationships between security impact and bug management during development.","Finally, we propose an abstract syntax tree-based analysis to identify the syntactic characteristics of bug fixes.","Our results show that 53% of the bugs in OpenTitan have potential security implications and that 55% of all bug fixes modify only one file.","Our findings underscore the importance of security-aware development practices and tools and motivate the development of techniques that leverage the highly localized nature of hardware bugs."],"url":"http://arxiv.org/abs/2402.00684v1","category":"cs.CR"}
{"created":"2024-02-01 15:46:04","title":"WayFASTER: a Self-Supervised Traversability Prediction for Increased Navigation Awareness","abstract":"Accurate and robust navigation in unstructured environments requires fusing data from multiple sensors. Such fusion ensures that the robot is better aware of its surroundings, including areas of the environment that are not immediately visible, but were visible at a different time. To solve this problem, we propose a method for traversability prediction in challenging outdoor environments using a sequence of RGB and depth images fused with pose estimations. Our method, termed WayFASTER (Waypoints-Free Autonomous System for Traversability with Enhanced Robustness), uses experience data recorded from a receding horizon estimator to train a self-supervised neural network for traversability prediction, eliminating the need for heuristics. Our experiments demonstrate that our method excels at avoiding geometric obstacles, and correctly detects that traversable terrains, such as tall grass, can be navigable. By using a sequence of images, WayFASTER significantly enhances the robot's awareness of its surroundings, enabling it to predict the traversability of terrains that are not immediately visible. This enhanced awareness contributes to better navigation performance in environments where such predictive capabilities are essential.","sentences":["Accurate and robust navigation in unstructured environments requires fusing data from multiple sensors.","Such fusion ensures that the robot is better aware of its surroundings, including areas of the environment that are not immediately visible, but were visible at a different time.","To solve this problem, we propose a method for traversability prediction in challenging outdoor environments using a sequence of RGB and depth images fused with pose estimations.","Our method, termed WayFASTER (Waypoints-Free Autonomous System for Traversability with Enhanced Robustness), uses experience data recorded from a receding horizon estimator to train a self-supervised neural network for traversability prediction, eliminating the need for heuristics.","Our experiments demonstrate that our method excels at avoiding geometric obstacles, and correctly detects that traversable terrains, such as tall grass, can be navigable.","By using a sequence of images, WayFASTER significantly enhances the robot's awareness of its surroundings, enabling it to predict the traversability of terrains that are not immediately visible.","This enhanced awareness contributes to better navigation performance in environments where such predictive capabilities are essential."],"url":"http://arxiv.org/abs/2402.00683v1","category":"cs.RO"}
{"created":"2024-02-01 15:45:29","title":"Generalised triple homomorphisms and Derivations","abstract":"We introduce generalised triple homomorphism between Jordan Banach triple systems as a concept which extends the notion of generalised homomorphism between Banach algebras given by Jarosz and Johnson in 1985 and 1987, respectively. We prove that every generalised triple homomorphism between JB$^*$-triples is automatically continuous. When particularised to C$^*$-algebras, we rediscover one of the main theorems established by Johnson. We shall also consider generalised triple derivations from a Jordan Banach triple $E$ into a Jordan Banach triple $E$-module, proving that every generalised triple derivation from a JB$^*$-triple $E$ into $E^*$ is automatically continuous.","sentences":["We introduce generalised triple homomorphism between Jordan Banach triple systems as a concept which extends the notion of generalised homomorphism between Banach algebras given by Jarosz and Johnson in 1985 and 1987, respectively.","We prove that every generalised triple homomorphism between JB$^*$-triples is automatically continuous.","When particularised to C$^*$-algebras, we rediscover one of the main theorems established by Johnson.","We shall also consider generalised triple derivations from a Jordan Banach triple $E$ into a Jordan Banach triple $E$-module, proving that every generalised triple derivation from a JB$^*$-triple $E$ into $E^*$ is automatically continuous."],"url":"http://arxiv.org/abs/2402.00682v1","category":"math.OA"}
{"created":"2024-02-01 15:44:17","title":"Sampling-based Stochastic Data-driven Predictive Control under Data Uncertainty","abstract":"We present a stochastic output-feedback data-driven predictive control scheme for linear time-invariant systems subject to bounded additive disturbances and probabilistic chance constraints. The approach uses data-driven predictors based on an extension of Willems' fundamental lemma from behavioral systems theory and a single persistently exciting input-output data trajectory. Compared to current state-of-the-art approaches that rely on availability of exact disturbance data, we deterministically approximate the chance constraints in a sampling-based fashion by leveraging a novel parameterization of the unknown disturbance data trajectory, considering consistency with the measured data and the system class. A robust constraint on the first predicted step guarantees recursive feasibility of the proposed controller as well as constraint satisfaction in closed-loop. We show robust asymptotic stability in expectation under further standard assumptions. A numerical example demonstrates the efficiency of the proposed control scheme.","sentences":["We present a stochastic output-feedback data-driven predictive control scheme for linear time-invariant systems subject to bounded additive disturbances and probabilistic chance constraints.","The approach uses data-driven predictors based on an extension of Willems' fundamental lemma from behavioral systems theory and a single persistently exciting input-output data trajectory.","Compared to current state-of-the-art approaches that rely on availability of exact disturbance data, we deterministically approximate the chance constraints in a sampling-based fashion by leveraging a novel parameterization of the unknown disturbance data trajectory, considering consistency with the measured data and the system class.","A robust constraint on the first predicted step guarantees recursive feasibility of the proposed controller as well as constraint satisfaction in closed-loop.","We show robust asymptotic stability in expectation under further standard assumptions.","A numerical example demonstrates the efficiency of the proposed control scheme."],"url":"http://arxiv.org/abs/2402.00681v1","category":"eess.SY"}
{"created":"2024-02-01 15:43:43","title":"LVC-LGMC: Joint Local and Global Motion Compensation for Learned Video Compression","abstract":"Existing learned video compression models employ flow net or deformable convolutional networks (DCN) to estimate motion information. However, the limited receptive fields of flow net and DCN inherently direct their attentiveness towards the local contexts. Global contexts, such as large-scale motions and global correlations among frames are ignored, presenting a significant bottleneck for capturing accurate motions. To address this issue, we propose a joint local and global motion compensation module (LGMC) for leaned video coding. More specifically, we adopt flow net for local motion compensation. To capture global context, we employ the cross attention in feature domain for motion compensation. In addition, to avoid the quadratic complexity of vanilla cross attention, we divide the softmax operations in attention into two independent softmax operations, leading to linear complexity. To validate the effectiveness of our proposed LGMC, we integrate it with DCVC-TCM and obtain learned video compression with joint local and global motion compensation (LVC-LGMC). Extensive experiments demonstrate that our LVC-LGMC has significant rate-distortion performance improvements over baseline DCVC-TCM.","sentences":["Existing learned video compression models employ flow net or deformable convolutional networks (DCN) to estimate motion information.","However, the limited receptive fields of flow net and DCN inherently direct their attentiveness towards the local contexts.","Global contexts, such as large-scale motions and global correlations among frames are ignored, presenting a significant bottleneck for capturing accurate motions.","To address this issue, we propose a joint local and global motion compensation module (LGMC) for leaned video coding.","More specifically, we adopt flow net for local motion compensation.","To capture global context, we employ the cross attention in feature domain for motion compensation.","In addition, to avoid the quadratic complexity of vanilla cross attention, we divide the softmax operations in attention into two independent softmax operations, leading to linear complexity.","To validate the effectiveness of our proposed LGMC, we integrate it with DCVC-TCM and obtain learned video compression with joint local and global motion compensation (LVC-LGMC).","Extensive experiments demonstrate that our LVC-LGMC has significant rate-distortion performance improvements over baseline DCVC-TCM."],"url":"http://arxiv.org/abs/2402.00680v1","category":"eess.IV"}
{"created":"2024-02-01 15:34:49","title":"The global Cauchy problem for the Euler-Riesz equations","abstract":"We completely resolve the global Cauchy problem for the multi-dimensional Euler-Riesz equations, where the interaction forcing is given by $\\nabla (-\\Delta)^{-\\sigma/2}\\rho$ for some $\\sigma \\in (0,2)$. We construct the global-in-time unique solution to the Euler-Riesz system in a $H^s$ Sobolev space under a smallness assumption on the initial density and a dispersive spectral condition on the initial velocity. Moreover, we investigate the algebraic time decay of convergences for the constructed solutions. Our results cover the both attractive and repulsive cases as well as the whole regime $\\sigma \\in (0,2)$.","sentences":["We completely resolve the global Cauchy problem for the multi-dimensional Euler-Riesz equations, where the interaction forcing is given by $\\nabla (-\\Delta)^{-\\sigma/2}\\rho$ for some $\\sigma \\in (0,2)$. We construct the global-in-time unique solution to the Euler-Riesz system in a $H^s$ Sobolev space under a smallness assumption on the initial density and a dispersive spectral condition on the initial velocity.","Moreover, we investigate the algebraic time decay of convergences for the constructed solutions.","Our results cover the both attractive and repulsive cases as well as the whole regime $\\sigma \\in (0,2)$."],"url":"http://arxiv.org/abs/2402.00674v1","category":"math.AP"}
{"created":"2024-02-01 15:34:25","title":"On singular pencils with commuting coefficients","abstract":"We investigate the relation between the spectrum of a linear pencil and the Taylor spectrum of its coefficients. We prove that the linear pencil of commuting matrices is singular, i.e. its spectrum is the whole complex plane, if and only if (0, 0) belongs to the Taylor spectrum of its coefficients. This result implies two descriptions of Taylor spectrum of a pair of matrices. On the other hand we prove that this equivalence is not longer true if we consider the operators on infinite dimensional Hilbert space. Additionally, we pointed out the Kronecker forms of the pencils with commuting coefficients","sentences":["We investigate the relation between the spectrum of a linear pencil and the Taylor spectrum of its coefficients.","We prove that the linear pencil of commuting matrices is singular, i.e. its spectrum is the whole complex plane, if and only if (0, 0) belongs to the Taylor spectrum of its coefficients.","This result implies two descriptions of Taylor spectrum of a pair of matrices.","On the other hand we prove that this equivalence is not longer true if we consider the operators on infinite dimensional Hilbert space.","Additionally, we pointed out the Kronecker forms of the pencils with commuting coefficients"],"url":"http://arxiv.org/abs/2402.00673v1","category":"math.SP"}
{"created":"2024-02-01 15:32:07","title":"Uncertainty-Aware Guidance for Target Tracking subject to Intermittent Measurements using Motion Model Learning","abstract":"This letter presents a novel guidance law for target tracking applications where the target motion model is unknown and sensor measurements are intermittent due to unknown environmental conditions and low measurement update rate. In this work, the target motion model is represented by a transformer-based neural network and trained by previous target position measurements. This neural network (NN)-based motion model serves as the prediction step in a particle filter for target state estimation and uncertainty quantification. Then this estimation uncertainty is utilized in the information-driven guidance law to compute a path for the mobile agent to travel to a position with maximum expected entropy reduction (EER). The computation of EER is performed in real-time by approximating the probability distribution of the state using the particle representation from particle filter. Simulation and hardware experiments are performed with a quadcopter agent and TurtleBot target to demonstrate that the presented guidance law outperforms two other baseline guidance methods.","sentences":["This letter presents a novel guidance law for target tracking applications where the target motion model is unknown and sensor measurements are intermittent due to unknown environmental conditions and low measurement update rate.","In this work, the target motion model is represented by a transformer-based neural network and trained by previous target position measurements.","This neural network (NN)-based motion model serves as the prediction step in a particle filter for target state estimation and uncertainty quantification.","Then this estimation uncertainty is utilized in the information-driven guidance law to compute a path for the mobile agent to travel to a position with maximum expected entropy reduction (EER).","The computation of EER is performed in real-time by approximating the probability distribution of the state using the particle representation from particle filter.","Simulation and hardware experiments are performed with a quadcopter agent and TurtleBot target to demonstrate that the presented guidance law outperforms two other baseline guidance methods."],"url":"http://arxiv.org/abs/2402.00671v1","category":"eess.SY"}
{"created":"2024-02-01 15:30:57","title":"Global solutions of Euler-Maxwell equations with dissipation","abstract":"We consider the Cauchy problem for a damped Euler-Maxwell system with no ionic background. For smooth enough data satisfying suitable so-called dispersive conditions, we establish the global in time existence and uniqueness of a strong solution that decays uniformly in time. Our method is inspired by the works of D. Serre and M. Grassin dedicated to the compressible Euler system.","sentences":["We consider the Cauchy problem for a damped Euler-Maxwell system with no ionic background.","For smooth enough data satisfying suitable so-called dispersive conditions, we establish the global in time existence and uniqueness of a strong solution that decays uniformly in time.","Our method is inspired by the works of D. Serre and M. Grassin dedicated to the compressible Euler system."],"url":"http://arxiv.org/abs/2402.00669v1","category":"math.AP"}
{"created":"2024-02-01 15:12:00","title":"Improving the Representativeness of Simulation Intervals for the Cache Memory System","abstract":"Accurate simulation techniques are indispensable to efficiently propose new memory or architectural organizations. As implementing new hardware concepts in real systems is often not feasible, cycle-accurate simulators employed together with certain benchmarks are commonly used. However, detailed simulators may take too much time to execute these programs until completion. Therefore, several techniques aimed at reducing this time are usually employed. These schemes select fragments of the source code considered as representative of the entire application's behaviour -- mainly in terms of performance, but not plenty considering the behaviour of cache memory levels -- and only these intervals are simulated. Our hypothesis is that the different simulation windows currently employed when evaluating microarchitectural proposals, especially those involving the last level cache (LLC), do not reproduce the overall cache behaviour during the entire execution, potentially leading to wrong conclusions on the real performance of the proposals assessed. In this work, we first demonstrate this hypothesis by evaluating different cache replacement policies using various typical simulation approaches. Consequently, we also propose a simulation strategy, based on the applications' LLC activity, which mimics the overall behaviour of the cache much closer than conventional simulation intervals. Our proposal allows a fairer comparison between cache-related approaches as it reports, on average, a number of changes in the relative order among the policies assessed -- with respect to the full simulation -- more than 30\\% lower than that of conventional strategies, maintaining the simulation time largely unchanged and without losing accuracy on performance terms, especially for memory-intensive applications.","sentences":["Accurate simulation techniques are indispensable to efficiently propose new memory or architectural organizations.","As implementing new hardware concepts in real systems is often not feasible, cycle-accurate simulators employed together with certain benchmarks are commonly used.","However, detailed simulators may take too much time to execute these programs until completion.","Therefore, several techniques aimed at reducing this time are usually employed.","These schemes select fragments of the source code considered as representative of the entire application's behaviour -- mainly in terms of performance, but not plenty considering the behaviour of cache memory levels -- and only these intervals are simulated.","Our hypothesis is that the different simulation windows currently employed when evaluating microarchitectural proposals, especially those involving the last level cache (LLC), do not reproduce the overall cache behaviour during the entire execution, potentially leading to wrong conclusions on the real performance of the proposals assessed.","In this work, we first demonstrate this hypothesis by evaluating different cache replacement policies using various typical simulation approaches.","Consequently, we also propose a simulation strategy, based on the applications' LLC activity, which mimics the overall behaviour of the cache much closer than conventional simulation intervals.","Our proposal allows a fairer comparison between cache-related approaches as it reports, on average, a number of changes in the relative order among the policies assessed -- with respect to the full simulation -- more than 30\\% lower than that of conventional strategies, maintaining the simulation time largely unchanged and without losing accuracy on performance terms, especially for memory-intensive applications."],"url":"http://arxiv.org/abs/2402.00649v1","category":"cs.AR"}
{"created":"2024-02-01 14:56:34","title":"An electrically pumped topological polariton laser","abstract":"With a seminal work of Raghu and Haldane in 2008, concepts of topology have been successfully introduced in a wide range of optical systems, emulating specific lattice Hamiltonians. Certainly, one of the most promising routes to an application of topological photonics in an actual device are topological lasers, where efficient and highly coherent lasing from a topologically non-trivial mode is achieved. While some attempts have been made to excite such structures electrically, the majority of published fundamental experiments use a form of laser excitation. In this paper, we use a lattice of vertical resonator polariton micropillars to form an exponentially localized topological Su-Schrieffer-Heeger defect. Upon electrical excitation of the p-i-n doped structure, the system unequivocally shows polariton lasing from the topological defect using a carefully placed gold contact. Despite the presence of doping and electrical contacts, the polariton band structure clearly preserves its topological properties. At high excitation power the Mott density is exceeded leading to highly efficient lasing in the weak coupling regime. This work is an important step towards applied topological lasers using vertical resonator microcavity structures.","sentences":["With a seminal work of Raghu and Haldane in 2008, concepts of topology have been successfully introduced in a wide range of optical systems, emulating specific lattice Hamiltonians.","Certainly, one of the most promising routes to an application of topological photonics in an actual device are topological lasers, where efficient and highly coherent lasing from a topologically non-trivial mode is achieved.","While some attempts have been made to excite such structures electrically, the majority of published fundamental experiments use a form of laser excitation.","In this paper, we use a lattice of vertical resonator polariton micropillars to form an exponentially localized topological Su-Schrieffer-Heeger defect.","Upon electrical excitation of the p-i-n doped structure, the system unequivocally shows polariton lasing from the topological defect using a carefully placed gold contact.","Despite the presence of doping and electrical contacts, the polariton band structure clearly preserves its topological properties.","At high excitation power the Mott density is exceeded leading to highly efficient lasing in the weak coupling regime.","This work is an important step towards applied topological lasers using vertical resonator microcavity structures."],"url":"http://arxiv.org/abs/2402.00639v1","category":"physics.optics"}
{"created":"2024-02-01 14:50:29","title":"Detection of an intranight optical hard-lag with colour variability in blazar PKS 0735+178","abstract":"Blazars are a highly variable subclass of active galactic nuclei that have been observed to vary significantly during a single night. This intranight variability remains a debated phenomenon, with various mechanisms proposed to explain the behaviour including jet energy density evolution or system geometric changes. We present the results of an intranight optical monitoring campaign of four blazars: TXS 0506+056, OJ287, PKS 0735+178, and OJ248 using the Carlos S\\'anchez Telescope. We detect significant but colourless behaviour in OJ287 and both bluer- and redder-when-brighter colour trends in PKS 0735+178. Additionally, the g band shows a lag of ~10 min with respect to the r,i,z_s bands for PKS 0735+178 on 2023 January 17. This unexpected hard-lag in PKS 0735+178 is not in accordance with the standard synchrotron shock cooling model (which would predict a soft lag) and instead suggests the variability may be a result of changes in the jet's electron energy density distribution, with energy injection from Fermi acceleration processes into a post-shocked medium.","sentences":["Blazars are a highly variable subclass of active galactic nuclei that have been observed to vary significantly during a single night.","This intranight variability remains a debated phenomenon, with various mechanisms proposed to explain the behaviour including jet energy density evolution or system geometric changes.","We present the results of an intranight optical monitoring campaign of four blazars: TXS 0506+056, OJ287, PKS 0735+178, and OJ248 using the Carlos S\\'anchez Telescope.","We detect significant but colourless behaviour in OJ287 and both bluer- and redder-when-brighter colour trends in PKS 0735+178.","Additionally, the g band shows a lag of ~10 min with respect to the r,i,z_s bands for PKS 0735+178 on 2023 January 17.","This unexpected hard-lag in PKS 0735+178 is not in accordance with the standard synchrotron shock cooling model (which would predict a soft lag) and instead suggests the variability may be a result of changes in the jet's electron energy density distribution, with energy injection from Fermi acceleration processes into a post-shocked medium."],"url":"http://arxiv.org/abs/2402.00633v1","category":"astro-ph.HE"}
{"created":"2024-02-01 14:31:23","title":"Estimate for the bulk viscosity of strongly coupled quark matter","abstract":"Modern hydrodynamic simulations of core-collapse supernovae and neutron-star mergers require knowledge not only of the equilibrium properties of strongly interacting matter, but also of the system's response to perturbations, encoded in various transport coefficients. Using perturbative and holographic tools, we derive here an improved weak-coupling and a new strong-coupling result for the most important transport coefficient of unpaired quark matter, its bulk viscosity. These results are combined in a simple analytic pocket formula for the quantity that is rooted in perturbative Quantum Chromodynamics at high densities but takes into account nonperturbative holographic input at neutron-star densities, where the system is strongly coupled. This expression can be used in the modeling of unpaired quark matter at astrophysically relevant temperatures and densities.","sentences":["Modern hydrodynamic simulations of core-collapse supernovae and neutron-star mergers require knowledge not only of the equilibrium properties of strongly interacting matter, but also of the system's response to perturbations, encoded in various transport coefficients.","Using perturbative and holographic tools, we derive here an improved weak-coupling and a new strong-coupling result for the most important transport coefficient of unpaired quark matter, its bulk viscosity.","These results are combined in a simple analytic pocket formula for the quantity that is rooted in perturbative Quantum Chromodynamics at high densities but takes into account nonperturbative holographic input at neutron-star densities, where the system is strongly coupled.","This expression can be used in the modeling of unpaired quark matter at astrophysically relevant temperatures and densities."],"url":"http://arxiv.org/abs/2402.00621v1","category":"hep-ph"}
{"created":"2024-02-01 13:51:35","title":"A Promise Theory Perspective on the Role of Intent in Group Dynamics","abstract":"We present a simple argument using Promise Theory and dimensional analysis for the Dunbar scaling hierarchy, supported by recent data from group formation in Wikipedia editing. We show how the assumption of a common priority seeds group alignment until the costs associated with attending to the group outweigh the benefits in a detailed balance scenario. Subject to partial efficiency of implementing promised intentions, we can reproduce a series of compatible rates that balance growth with entropy.","sentences":["We present a simple argument using Promise Theory and dimensional analysis for the Dunbar scaling hierarchy, supported by recent data from group formation in Wikipedia editing.","We show how the assumption of a common priority seeds group alignment until the costs associated with attending to the group outweigh the benefits in a detailed balance scenario.","Subject to partial efficiency of implementing promised intentions, we can reproduce a series of compatible rates that balance growth with entropy."],"url":"http://arxiv.org/abs/2402.00598v1","category":"cs.SI"}
{"created":"2024-02-01 13:45:12","title":"Group Related Phenomena in Wikipedia Edits","abstract":"Human communities have self-organizing properties that give rise to very specific natural grouping patterns, reflected in the Dunbar Number and its layered structure (a Dunbar Graph). Since work-groups are necessarily also social groups, we might expect the same principles to apply here as well. One factor likely to be important in limiting the size of groups is that conflicts typically escalate with the number of people involved. Here we analyse Wikipedia editing histories across a wide range of topics to show that there is an emergent coherence in the size of groups formed transiently to edit the content of subject texts, with two peaks averaging at around $N=8$ for the size corresponding to maximal contention, and at around $N=4$ as a regular team. These values are consistent with the observed sizes of conversational groups, as well as the hierarchical structuring of Dunbar graphs. We use the Promise Theory of trust to suggest a scaling law that may apply to all group distributions based on seeded attraction. In addition to providing further evidence that even natural communities of strangers are self-organising, the results have important implications for the governance of the Wikipedia commons and for the security of all online social platforms and associations.","sentences":["Human communities have self-organizing properties that give rise to very specific natural grouping patterns, reflected in the Dunbar Number and its layered structure (a Dunbar Graph).","Since work-groups are necessarily also social groups, we might expect the same principles to apply here as well.","One factor likely to be important in limiting the size of groups is that conflicts typically escalate with the number of people involved.","Here we analyse Wikipedia editing histories across a wide range of topics to show that there is an emergent coherence in the size of groups formed transiently to edit the content of subject texts, with two peaks averaging at around $N=8$ for the size corresponding to maximal contention, and at around $N=4$ as a regular team.","These values are consistent with the observed sizes of conversational groups, as well as the hierarchical structuring of Dunbar graphs.","We use the Promise Theory of trust to suggest a scaling law that may apply to all group distributions based on seeded attraction.","In addition to providing further evidence that even natural communities of strangers are self-organising, the results have important implications for the governance of the Wikipedia commons and for the security of all online social platforms and associations."],"url":"http://arxiv.org/abs/2402.00595v1","category":"cs.SI"}
{"created":"2024-02-01 13:43:33","title":"Coronary Artery Disease Classification with Different Lesion Degree Ranges based on Deep Learning","abstract":"Invasive Coronary Angiography (ICA) images are considered the gold standard for assessing the state of the coronary arteries. Deep learning classification methods are widely used and well-developed in different areas where medical imaging evaluation has an essential impact due to the development of computer-aided diagnosis systems that can support physicians in their clinical procedures. In this paper, a new performance analysis of deep learning methods for binary ICA classification with different lesion degrees is reported. To reach this goal, an annotated dataset of ICA images that contains the ground truth, the location of lesions and seven possible severity degrees ranging between 0% and 100% was employed. The ICA images were divided into 'lesion' or 'non-lesion' patches. We aim to study how binary classification performance is affected by the different lesion degrees considered in the positive class. Therefore, five known convolutional neural network architectures were trained with different input images where different lesion degree ranges were gradually incorporated until considering the seven lesion degrees. Besides, four types of experiments with and without data augmentation were designed, whose F-measure and Area Under Curve (AUC) were computed. Reported results achieved an F-measure and AUC of 92.7% and 98.1%, respectively. However, lesion classification is highly affected by the degree of the lesion intended to classify, with 15% less accuracy when <99% lesion patches are present.","sentences":["Invasive Coronary Angiography (ICA) images are considered the gold standard for assessing the state of the coronary arteries.","Deep learning classification methods are widely used and well-developed in different areas where medical imaging evaluation has an essential impact due to the development of computer-aided diagnosis systems that can support physicians in their clinical procedures.","In this paper, a new performance analysis of deep learning methods for binary ICA classification with different lesion degrees is reported.","To reach this goal, an annotated dataset of ICA images that contains the ground truth, the location of lesions and seven possible severity degrees ranging between 0% and 100% was employed.","The ICA images were divided into 'lesion' or 'non-lesion' patches.","We aim to study how binary classification performance is affected by the different lesion degrees considered in the positive class.","Therefore, five known convolutional neural network architectures were trained with different input images where different lesion degree ranges were gradually incorporated until considering the seven lesion degrees.","Besides, four types of experiments with and without data augmentation were designed, whose F-measure and Area Under Curve (AUC) were computed.","Reported results achieved an F-measure and AUC of 92.7% and 98.1%, respectively.","However, lesion classification is highly affected by the degree of the lesion intended to classify, with 15% less accuracy when <99% lesion patches are present."],"url":"http://arxiv.org/abs/2402.00593v1","category":"eess.IV"}
{"created":"2024-02-01 13:03:13","title":"CADICA: a new dataset for coronary artery disease detection by using invasive coronary angiography","abstract":"Coronary artery disease (CAD) remains the leading cause of death globally and invasive coronary angiography (ICA) is considered the gold standard of anatomical imaging evaluation when CAD is suspected. However, risk evaluation based on ICA has several limitations, such as visual assessment of stenosis severity, which has significant interobserver variability. This motivates to development of a lesion classification system that can support specialists in their clinical procedures. Although deep learning classification methods are well-developed in other areas of medical imaging, ICA image classification is still at an early stage. One of the most important reasons is the lack of available and high-quality open-access datasets. In this paper, we reported a new annotated ICA images dataset, CADICA, to provide the research community with a comprehensive and rigorous dataset of coronary angiography consisting of a set of acquired patient videos and associated disease-related metadata. This dataset can be used by clinicians to train their skills in angiographic assessment of CAD severity and by computer scientists to create computer-aided diagnostic systems to help in such assessment. In addition, baseline classification methods are proposed and analyzed, validating the functionality of CADICA and giving the scientific community a starting point to improve CAD detection.","sentences":["Coronary artery disease (CAD) remains the leading cause of death globally and invasive coronary angiography (ICA) is considered the gold standard of anatomical imaging evaluation when CAD is suspected.","However, risk evaluation based on ICA has several limitations, such as visual assessment of stenosis severity, which has significant interobserver variability.","This motivates to development of a lesion classification system that can support specialists in their clinical procedures.","Although deep learning classification methods are well-developed in other areas of medical imaging, ICA image classification is still at an early stage.","One of the most important reasons is the lack of available and high-quality open-access datasets.","In this paper, we reported a new annotated ICA images dataset, CADICA, to provide the research community with a comprehensive and rigorous dataset of coronary angiography consisting of a set of acquired patient videos and associated disease-related metadata.","This dataset can be used by clinicians to train their skills in angiographic assessment of CAD severity and by computer scientists to create computer-aided diagnostic systems to help in such assessment.","In addition, baseline classification methods are proposed and analyzed, validating the functionality of CADICA and giving the scientific community a starting point to improve CAD detection."],"url":"http://arxiv.org/abs/2402.00570v1","category":"eess.IV"}
{"created":"2024-02-01 13:02:07","title":"Radio Map Assisted Approach for Interference-Aware Predictive UAV Communications","abstract":"Herein, an interference-aware predictive aerial-and-terrestrial communication problem is studied, where an unmanned aerial vehicle (UAV) delivers some data payload to a few nodes within a communication deadline. The first challenge is the possible interference to the ground base stations (BSs) and users possibly at unknown locations. This paper develops a radio-map-based approach to predict the channel to the receivers and the unintended nodes. Therefore, a predictive communication strategy can be optimized ahead of time to reduce the interference power and duration for the ground nodes. Such predictive optimization raises the second challenge of developing a low-complexity solution for a batch of transmission strategies over T time slots for N receivers before the flight. Mathematically, while the proposed interference-aware predictive communication problem is non-convex, it is converted into a relaxed convex problem, and solved by a novel dual-based algorithm, which is shown to achieve global optimality at asymptotically small slot duration. The proposed algorithm demonstrates orders of magnitude saving of the computational time for moderate T and N compared to several existing solvers. Simulations show that the radio-map-assisted scheme can prevent all unintended receivers with known positions from experiencing interference and significantly reduce the interference to the users at unknown locations.","sentences":["Herein, an interference-aware predictive aerial-and-terrestrial communication problem is studied, where an unmanned aerial vehicle (UAV) delivers some data payload to a few nodes within a communication deadline.","The first challenge is the possible interference to the ground base stations (BSs) and users possibly at unknown locations.","This paper develops a radio-map-based approach to predict the channel to the receivers and the unintended nodes.","Therefore, a predictive communication strategy can be optimized ahead of time to reduce the interference power and duration for the ground nodes.","Such predictive optimization raises the second challenge of developing a low-complexity solution for a batch of transmission strategies over T time slots for N receivers before the flight.","Mathematically, while the proposed interference-aware predictive communication problem is non-convex, it is converted into a relaxed convex problem, and solved by a novel dual-based algorithm, which is shown to achieve global optimality at asymptotically small slot duration.","The proposed algorithm demonstrates orders of magnitude saving of the computational time for moderate T and N compared to several existing solvers.","Simulations show that the radio-map-assisted scheme can prevent all unintended receivers with known positions from experiencing interference and significantly reduce the interference to the users at unknown locations."],"url":"http://arxiv.org/abs/2402.00569v1","category":"eess.SP"}
{"created":"2024-02-01 12:46:45","title":"A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains","abstract":"Prompting language models to provide step-by-step answers (e.g., \"Chain-of-Thought\") is the prominent approach for complex reasoning tasks, where more accurate reasoning chains typically improve downstream task performance. Recent literature discusses automatic methods to verify reasoning steps to evaluate and improve their correctness. However, no fine-grained step-level datasets are available to enable thorough evaluation of such verification methods, hindering progress in this direction. We introduce Reveal: Reasoning Verification Evaluation, a new dataset to benchmark automatic verifiers of complex Chain-of-Thought reasoning in open-domain question answering settings. Reveal includes comprehensive labels for the relevance, attribution to evidence passages, and logical correctness of each reasoning step in a language model's answer, across a wide variety of datasets and state-of-the-art language models.","sentences":["Prompting language models to provide step-by-step answers (e.g., \"Chain-of-Thought\") is the prominent approach for complex reasoning tasks, where more accurate reasoning chains typically improve downstream task performance.","Recent literature discusses automatic methods to verify reasoning steps to evaluate and improve their correctness.","However, no fine-grained step-level datasets are available to enable thorough evaluation of such verification methods, hindering progress in this direction.","We introduce Reveal: Reasoning Verification Evaluation, a new dataset to benchmark automatic verifiers of complex Chain-of-Thought reasoning in open-domain question answering settings.","Reveal includes comprehensive labels for the relevance, attribution to evidence passages, and logical correctness of each reasoning step in a language model's answer, across a wide variety of datasets and state-of-the-art language models."],"url":"http://arxiv.org/abs/2402.00559v1","category":"cs.CL"}
{"created":"2024-02-01 12:46:44","title":"Neutron-scattering signature of the Dzyaloshinskii-Moriya interaction in nanoparticles","abstract":"The antisymmetric Dzyaloshinkii-Moriya interaction (DMI) arises in systems with broken inversion symmetry and strong spin-orbit coupling. In conjunction with the isotropic and symmetric exchange interaction, magnetic anisotropy, the dipolar interaction, and an externally applied magnetic field, the DMI supports and stabilizes the formation of various kinds of complex mesoscale magnetization configurations, such as helices, spin spirals, skyrmions, or hopfions. A question of importance in this context addresses the neutron-scattering signature of the DMI, in particular in nanoparticle assemblies, where the related magnetic scattering signal is diffuse in character and not of the single-crystal diffraction-peak-type, as it is e.g.\\ seen in the B20 compounds. Using micromagnetic simulations we study the effect of the DMI in spherical FeGe nanoparticles on the randomly-averaged magnetic neutron scattering observables, more specifically on the spin-flip small-angle neutron scattering cross section, the related chiral function, and the pair-distance distribution function. Within the studied parameter space for the particle size ($60 \\, \\mathrm{nm} \\leq L \\leq 200 \\, \\mathrm{nm}$) and the applied magnetic field ($-1 \\, \\mathrm{T} \\leq \\mu_0 H_0 \\leq 1 \\, \\mathrm{T}$), we find that the chiral function is only nonzero when the DMI is taken into account in the simulations. This result is discussed within the context of the symmetry properties of the magnetization Fourier components and of the involved energies under space inversion. Finally, for small applied magnetic fields, we provide an easy-to-implement analytical correlation function for the DMI-induced spin modulations (with wave vector $k_{\\mathrm{d}}$). The corresponding randomly-averaged spin-flip SANS cross section reproduces the main features found in the numerical simulations.","sentences":["The antisymmetric Dzyaloshinkii-Moriya interaction (DMI) arises in systems with broken inversion symmetry and strong spin-orbit coupling.","In conjunction with the isotropic and symmetric exchange interaction, magnetic anisotropy, the dipolar interaction, and an externally applied magnetic field, the DMI supports and stabilizes the formation of various kinds of complex mesoscale magnetization configurations, such as helices, spin spirals, skyrmions, or hopfions.","A question of importance in this context addresses the neutron-scattering signature of the DMI, in particular in nanoparticle assemblies, where the related magnetic scattering signal is diffuse in character and not of the single-crystal diffraction-peak-type, as it is e.g.\\ seen in the B20 compounds.","Using micromagnetic simulations we study the effect of the DMI in spherical FeGe nanoparticles on the randomly-averaged magnetic neutron scattering observables, more specifically on the spin-flip small-angle neutron scattering cross section, the related chiral function, and the pair-distance distribution function.","Within the studied parameter space for the particle size ($60 \\, \\mathrm{nm} \\leq L \\leq 200 \\, \\mathrm{nm}$) and the applied magnetic field ($-1 \\, \\mathrm{T} \\leq \\mu_0 H_0 \\leq 1 \\, \\mathrm{T}$), we find that the chiral function is only nonzero when the DMI is taken into account in the simulations.","This result is discussed within the context of the symmetry properties of the magnetization Fourier components and of the involved energies under space inversion.","Finally, for small applied magnetic fields, we provide an easy-to-implement analytical correlation function for the DMI-induced spin modulations (with wave vector $k_{\\mathrm{d}}$).","The corresponding randomly-averaged spin-flip SANS cross section reproduces the main features found in the numerical simulations."],"url":"http://arxiv.org/abs/2402.00558v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-01 12:42:35","title":"Polydifferential Lie bialgebras and graph complexes","abstract":"We study the deformation complex of a canonical morphism $i$ from the properad of (degree shifted) Lie bialgebras $\\mathbf{Lieb}_{c,d}$ to its polydifferential version $\\mathcal{D}(\\mathbf{Lieb}_{c,d})$ and show that it is quasi-isomorphic to the oriented graph complex $\\mathbf{GC}^{\\text{or}}_{c+d+1}$, up to one rescaling class. As the latter complex is quasi-isomorphic to the original graph complex $\\mathbf{GC}_{c+d}$, we conclude that the space of homotopy non-trivial infinitesimal deformations of the canonical map $i$ can be identified with the Grothendieck-Teichm\\\"uller Lie algebra $\\mathfrak{grt}$; moreover, every such an infinitesimal deformation extends to a genuine deformation of the canonical morphism $i$ from $\\mathbf{Lieb}_{c,d}$ to $\\mathcal{D}(\\mathbf{Lieb}_{c,d})$. The full deformation complex is described with the help of a new graph complex of so called entangled graphs, whose suitable quotient complex is shown to contain the tensor product $H(\\mathbf{GC}_c) \\otimes H(\\mathbf{GC}_d)$ of cohomologies of Kontsevich graph complexes $\\mathbf{GC}_c \\otimes \\mathbf{GC}_d$.","sentences":["We study the deformation complex of a canonical morphism $i$ from the properad of (degree shifted)","Lie bialgebras $\\mathbf{Lieb}_{c,d}$ to its polydifferential version $\\mathcal{D}(\\mathbf{Lieb}_{c,d})$ and show that it is quasi-isomorphic to the oriented graph complex $\\mathbf{GC}^{\\text{or}}_{c+d+1}$, up to one rescaling class.","As the latter complex is quasi-isomorphic to the original graph complex $\\mathbf{GC}_{c+d}$, we conclude that the space of homotopy non-trivial infinitesimal deformations of the canonical map $i$ can be identified with the Grothendieck-Teichm\\\"uller Lie algebra $\\mathfrak{grt}$; moreover, every such an infinitesimal deformation extends to a genuine deformation of the canonical morphism $i$ from $\\mathbf{Lieb}_{c,d}$ to $\\mathcal{D}(\\mathbf{Lieb}_{c,d})$.","The full deformation complex is described with the help of a new graph complex of so called entangled graphs, whose suitable quotient complex is shown to contain the tensor product $H(\\mathbf{GC}_c)","\\otimes H(\\mathbf{GC}_d)$ of cohomologies of Kontsevich graph complexes $\\mathbf{GC}_c \\otimes \\mathbf{GC}_d$."],"url":"http://arxiv.org/abs/2402.00554v1","category":"math.QA"}
{"created":"2024-02-01 12:40:02","title":"Classifying optical (out)bursts in cataclysmic variables: the distinct observational characteristics of dwarf novae, micronovae, stellar flares and magnetic gating","abstract":"Cataclysmic variables can experience short optical brightenings, which are commonly attributed to phenomena such as dwarf novae outbursts, micronovae, donor flares or magnetic gating bursts. Since these events exhibit similar observational characteristics, their identification has often been ambiguous. In particular, magnetic gating bursts and micronovae have been suggested as alternative interpretations of the same phenomena. Here we show that the timescales and energies separate the optical brightenings into separate clusters consistent with their different classifications. This suggest that micronovae and magnetic gating bursts are in fact separate phenomena. Based on our finding we develop diagnostic diagrams that can distinguish between these bursts/flares based on their properties. We demonstrate the effectiveness of this approach on observations of a newly identified intermediate polar, CTCV J0333-4451, which we classify as a magnetic gating system. CTCV J0333-4451 is the third high spin-to-orbital period ratio intermediate polar with magnetic gating, suggesting that these bursts are common among these rare systems.","sentences":["Cataclysmic variables can experience short optical brightenings, which are commonly attributed to phenomena such as dwarf novae outbursts, micronovae, donor flares or magnetic gating bursts.","Since these events exhibit similar observational characteristics, their identification has often been ambiguous.","In particular, magnetic gating bursts and micronovae have been suggested as alternative interpretations of the same phenomena.","Here we show that the timescales and energies separate the optical brightenings into separate clusters consistent with their different classifications.","This suggest that micronovae and magnetic gating bursts are in fact separate phenomena.","Based on our finding we develop diagnostic diagrams that can distinguish between these bursts/flares based on their properties.","We demonstrate the effectiveness of this approach on observations of a newly identified intermediate polar, CTCV J0333-4451, which we classify as a magnetic gating system.","CTCV J0333-4451 is the third high spin-to-orbital period ratio intermediate polar with magnetic gating, suggesting that these bursts are common among these rare systems."],"url":"http://arxiv.org/abs/2402.00553v1","category":"astro-ph.SR"}
{"created":"2024-02-01 12:34:08","title":"Numerical simulation of endovascular treatment options for cerebral aneurysms","abstract":"Predicting the long-term success of endovascular interventions in the clinical management of cerebral aneurysms requires detailed insight into the patient-specific physiological conditions. In this work, we not only propose numerical representations of endovascular medical devices such as coils, flow diverters or Woven EndoBridge but also outline numerical models for the prediction of blood flow patterns in the aneurysm cavity right after a surgical intervention. Detailed knowledge about the post-surgical state then lays the basis to assess the chances of a stable occlusion of the aneurysm required for a long-term treatment success. To this end, we propose mathematical and mechanical models of endovascular medical devices made out of thin metal wires. These can then be used for fully resolved flow simulations of the post-surgical blood flow, which in this work will be performed by means of a Lattice Boltzmann method applied to the incompressible Navier-Stokes equations and patient-specific geometries. To probe the suitability of homogenized models, we also investigate poro-elastic models to represent such medical devices. In particular, we examine the validity of this modeling approach for flow diverter placement across the opening of the aneurysm cavity. For both approaches, physiologically meaningful boundary conditions are provided from reduced-order models of the vascular system. The present study demonstrates our capabilities to predict the post-surgical state and lays a solid foundation to tackle the prediction of thrombus formation and, thus, the aneurysm occlusion in a next step.","sentences":["Predicting the long-term success of endovascular interventions in the clinical management of cerebral aneurysms requires detailed insight into the patient-specific physiological conditions.","In this work, we not only propose numerical representations of endovascular medical devices such as coils, flow diverters or Woven EndoBridge but also outline numerical models for the prediction of blood flow patterns in the aneurysm cavity right after a surgical intervention.","Detailed knowledge about the post-surgical state then lays the basis to assess the chances of a stable occlusion of the aneurysm required for a long-term treatment success.","To this end, we propose mathematical and mechanical models of endovascular medical devices made out of thin metal wires.","These can then be used for fully resolved flow simulations of the post-surgical blood flow, which in this work will be performed by means of a Lattice Boltzmann method applied to the incompressible Navier-Stokes equations and patient-specific geometries.","To probe the suitability of homogenized models, we also investigate poro-elastic models to represent such medical devices.","In particular, we examine the validity of this modeling approach for flow diverter placement across the opening of the aneurysm cavity.","For both approaches, physiologically meaningful boundary conditions are provided from reduced-order models of the vascular system.","The present study demonstrates our capabilities to predict the post-surgical state and lays a solid foundation to tackle the prediction of thrombus formation and, thus, the aneurysm occlusion in a next step."],"url":"http://arxiv.org/abs/2402.00550v1","category":"math.NA"}
{"created":"2024-02-01 12:23:54","title":"Effect of the spin crossover of local copper-oxygen states on the electronic structure of HTSC cuprates","abstract":"In this work, the effect of uniaxial pressure along the c axis on the electronic structure of the HTSC cuprate La2-xSrxCuO4 is investigated at the doping levels x = 0.1, 0.15, 0.25. The GTB method within the five-band p-d model framework is used to describe the electron system. The uniaxial compression leads to a significant reconstruction of the electronic structure and a change in the character of low-energy quasiparticle excitations: a large contribution of a1g symmetry orbitals appears at the top of the valence band. The crossover between the local Zhang-Rice singlet and the Emery-Reiter triplet was found at the pressure Pc = 15.1 GPa. The characteristic changes in the electronic structure under pressure occur abruptly as a result of the crossover. In particular, the top of the valence band displaces to the region around the k-point (pi,0), the Fermi contour transforms to the four pockets around (0,0),(2pi,0),(0,2pi),(2pi,2pi) and the one contour around (pi,pi).","sentences":["In this work, the effect of uniaxial pressure along the c axis on the electronic structure of the HTSC cuprate La2-xSrxCuO4 is investigated at the doping levels x = 0.1, 0.15, 0.25.","The GTB method within the five-band p-d model framework is used to describe the electron system.","The uniaxial compression leads to a significant reconstruction of the electronic structure and a change in the character of low-energy quasiparticle excitations: a large contribution of a1g symmetry orbitals appears at the top of the valence band.","The crossover between the local Zhang-Rice singlet and the Emery-Reiter triplet was found at the pressure Pc = 15.1 GPa.","The characteristic changes in the electronic structure under pressure occur abruptly as a result of the crossover.","In particular, the top of the valence band displaces to the region around the k-point (pi,0), the Fermi contour transforms to the four pockets around (0,0),(2pi,0),(0,2pi),(2pi,2pi) and the one contour around (pi,pi)."],"url":"http://arxiv.org/abs/2402.00547v1","category":"cond-mat.str-el"}
{"created":"2024-02-01 12:13:35","title":"Quantum-Assisted Hilbert-Space Gaussian Process Regression","abstract":"Gaussian processes are probabilistic models that are commonly used as functional priors in machine learning. Due to their probabilistic nature, they can be used to capture the prior information on the statistics of noise, smoothness of the functions, and training data uncertainty. However, their computational complexity quickly becomes intractable as the size of the data set grows. We propose a Hilbert space approximation-based quantum algorithm for Gaussian process regression to overcome this limitation. Our method consists of a combination of classical basis function expansion with quantum computing techniques of quantum principal component analysis, conditional rotations, and Hadamard and Swap tests. The quantum principal component analysis is used to estimate the eigenvalues while the conditional rotations and the Hadamard and Swap tests are employed to evaluate the posterior mean and variance of the Gaussian process. Our method provides polynomial computational complexity reduction over the classical method.","sentences":["Gaussian processes are probabilistic models that are commonly used as functional priors in machine learning.","Due to their probabilistic nature, they can be used to capture the prior information on the statistics of noise, smoothness of the functions, and training data uncertainty.","However, their computational complexity quickly becomes intractable as the size of the data set grows.","We propose a Hilbert space approximation-based quantum algorithm for Gaussian process regression to overcome this limitation.","Our method consists of a combination of classical basis function expansion with quantum computing techniques of quantum principal component analysis, conditional rotations, and Hadamard and Swap tests.","The quantum principal component analysis is used to estimate the eigenvalues while the conditional rotations and the Hadamard and Swap tests are employed to evaluate the posterior mean and variance of the Gaussian process.","Our method provides polynomial computational complexity reduction over the classical method."],"url":"http://arxiv.org/abs/2402.00544v1","category":"stat.CO"}
{"created":"2024-02-01 12:06:26","title":"A Kaplansky Theorem for JB*-triples","abstract":"Let $T:E\\rightarrow F$ be a non-necessarily continuous triple homomorphism from a (complex) JB$^*$-triple (respectively, a (real) J$^*$B-triple) to a normed Jordan triple. The following statements hold:   (1) $T$ has closed range whenever $T$ is continuous   (2) $T$ has closed range whenever $T$ is continuous   This result generalises classical theorems of I. Kaplansky and S.B. Cleveland in the setting of C$^*$-algebras and of A. Bensebah and J.P\\'erez, L. Rico and A. Rodr'\\iguez Palacios in the setting of JB$^*$-algebras.","sentences":["Let $T:E\\rightarrow F$ be a non-necessarily continuous triple homomorphism from a (complex) JB$^*$-triple (respectively, a (real) J$^*$B-triple) to a normed Jordan triple.","The following statements hold:   (1) $T$ has closed range whenever $T$ is continuous   (2) $T$ has closed range whenever $T$ is continuous   ","This result generalises classical theorems of I. Kaplansky and S.B. Cleveland in the setting of C$^*$-algebras and of A. Bensebah and J.P\\'erez, L. Rico and A. Rodr'\\iguez Palacios in the setting of JB$^*$-algebras."],"url":"http://arxiv.org/abs/2402.00538v1","category":"math.OA"}
{"created":"2024-02-01 12:04:04","title":"Continuous field tracking with machine learning and steady state spin squeezing","abstract":"Entanglement plays a crucial role in proposals for quantum metrology, yet demonstrating quantum enhancement in sensing with sustained spin entanglement remains a challenging endeavor. Here, we combine optical pumping and continuous quantum nondemolition measurements to achieve a sustained spin squeezed state with $\\bm{4 \\times 10^{10}}$ hot atoms. A metrologically relevant steady state squeezing of $\\bm{-3.23 \\pm 0.24}$ dB using prediction and retrodiction is maintained for about one day. We employ the system to track different types of continuous time-fluctuating magnetic fields, where we construct deep learning models to decode the measurement records from the optical signals. Quantum enhancement due to the steady spin squeezing is verified in our atomic magnetometer. These results represent important progress towards applying long-lived quantum entanglement resources in realistic settings.","sentences":["Entanglement plays a crucial role in proposals for quantum metrology, yet demonstrating quantum enhancement in sensing with sustained spin entanglement remains a challenging endeavor.","Here, we combine optical pumping and continuous quantum nondemolition measurements to achieve a sustained spin squeezed state with $\\bm{4 \\times 10^{10}}$ hot atoms.","A metrologically relevant steady state squeezing of $\\bm{-3.23 \\pm 0.24}$ dB using prediction and retrodiction is maintained for about one day.","We employ the system to track different types of continuous time-fluctuating magnetic fields, where we construct deep learning models to decode the measurement records from the optical signals.","Quantum enhancement due to the steady spin squeezing is verified in our atomic magnetometer.","These results represent important progress towards applying long-lived quantum entanglement resources in realistic settings."],"url":"http://arxiv.org/abs/2402.00536v1","category":"quant-ph"}
{"created":"2024-02-01 11:48:53","title":"A calculus for modal compact Hausdorff spaces","abstract":"The symmetric strict implication calculus $\\mathsf{S^2IC}$, introduced in [5], is a modal calculus for compact Hausdorff spaces. This is established through de Vries duality, linking compact Hausdorff spaces with de Vries algebras-complete Boolean algebras equipped with a special relation. Modal compact Hausdorff spaces are compact Hausdorff spaces enriched with a continuous relation. These spaces correspond, via modalized de Vries duality of [3], to upper continuous modal de Vries algebras.   In this paper we introduce the modal symmetric strict implication calculus $\\mathsf{MS^2IC}$, which extends $\\mathsf{S^2IC}$. We prove that $\\mathsf{MS^2IC}$ is strongly sound and complete with respect to upper continuous modal de Vries algebras, thereby providing a logical calculus for modal compact Hausdorff spaces. We also develop a relational semantics for $\\mathsf{MS^2IC}$ that we employ to show admissibility of various $\\Pi_2$-rules in this system.","sentences":["The symmetric strict implication calculus $\\mathsf{S^2IC}$, introduced in [5], is a modal calculus for compact Hausdorff spaces.","This is established through de Vries duality, linking compact Hausdorff spaces with de Vries algebras-complete Boolean algebras equipped with a special relation.","Modal compact Hausdorff spaces are compact Hausdorff spaces enriched with a continuous relation.","These spaces correspond, via modalized de Vries duality of [3], to upper continuous modal de Vries algebras.   ","In this paper we introduce the modal symmetric strict implication calculus $\\mathsf{MS^2IC}$, which extends $\\mathsf{S^2IC}$. We prove that $\\mathsf{MS^2IC}$ is strongly sound and complete with respect to upper continuous modal de Vries algebras, thereby providing a logical calculus for modal compact Hausdorff spaces.","We also develop a relational semantics for $\\mathsf{MS^2IC}$ that we employ to show admissibility of various $\\Pi_2$-rules in this system."],"url":"http://arxiv.org/abs/2402.00528v1","category":"math.LO"}
{"created":"2024-02-01 11:47:41","title":"Tracking optimal feedback control under uncertain parameters","abstract":"Optimal control problems of tracking type for a class of linear systems with uncertain parameters in the dynamics are investigated. An affine tracking feedback control input is obtained by considering the minimization of an energy-like functional depending on a finite ensemble of training/sample parameters. It is computed from the nonnegative definite solution of an associated differential Riccati equation. Simulations are presented showing the tracking performance of the computed input for trained as well as untrained parameters.","sentences":["Optimal control problems of tracking type for a class of linear systems with uncertain parameters in the dynamics are investigated.","An affine tracking feedback control input is obtained by considering the minimization of an energy-like functional depending on a finite ensemble of training/sample parameters.","It is computed from the nonnegative definite solution of an associated differential Riccati equation.","Simulations are presented showing the tracking performance of the computed input for trained as well as untrained parameters."],"url":"http://arxiv.org/abs/2402.00526v1","category":"math.OC"}
{"created":"2024-02-01 11:31:26","title":"Developing A Multi-Agent and Self-Adaptive Framework with Deep Reinforcement Learning for Dynamic Portfolio Risk Management","abstract":"Deep or reinforcement learning (RL) approaches have been adapted as reactive agents to quickly learn and respond with new investment strategies for portfolio management under the highly turbulent financial market environments in recent years. In many cases, due to the very complex correlations among various financial sectors, and the fluctuating trends in different financial markets, a deep or reinforcement learning based agent can be biased in maximising the total returns of the newly formulated investment portfolio while neglecting its potential risks under the turmoil of various market conditions in the global or regional sectors. Accordingly, a multi-agent and self-adaptive framework namely the MASA is proposed in which a sophisticated multi-agent reinforcement learning (RL) approach is adopted through two cooperating and reactive agents to carefully and dynamically balance the trade-off between the overall portfolio returns and their potential risks. Besides, a very flexible and proactive agent as the market observer is integrated into the MASA framework to provide some additional information on the estimated market trends as valuable feedbacks for multi-agent RL approach to quickly adapt to the ever-changing market conditions. The obtained empirical results clearly reveal the potential strengths of our proposed MASA framework based on the multi-agent RL approach against many well-known RL-based approaches on the challenging data sets of the CSI 300, Dow Jones Industrial Average and S&P 500 indexes over the past 10 years. More importantly, our proposed MASA framework shed lights on many possible directions for future investigation.","sentences":["Deep or reinforcement learning (RL) approaches have been adapted as reactive agents to quickly learn and respond with new investment strategies for portfolio management under the highly turbulent financial market environments in recent years.","In many cases, due to the very complex correlations among various financial sectors, and the fluctuating trends in different financial markets, a deep or reinforcement learning based agent can be biased in maximising the total returns of the newly formulated investment portfolio while neglecting its potential risks under the turmoil of various market conditions in the global or regional sectors.","Accordingly, a multi-agent and self-adaptive framework namely the MASA is proposed in which a sophisticated multi-agent reinforcement learning (RL) approach is adopted through two cooperating and reactive agents to carefully and dynamically balance the trade-off between the overall portfolio returns and their potential risks.","Besides, a very flexible and proactive agent as the market observer is integrated into the MASA framework to provide some additional information on the estimated market trends as valuable feedbacks for multi-agent RL approach to quickly adapt to the ever-changing market conditions.","The obtained empirical results clearly reveal the potential strengths of our proposed MASA framework based on the multi-agent RL approach against many well-known RL-based approaches on the challenging data sets of the CSI 300, Dow Jones Industrial Average and S&P 500 indexes over the past 10 years.","More importantly, our proposed MASA framework shed lights on many possible directions for future investigation."],"url":"http://arxiv.org/abs/2402.00515v1","category":"q-fin.PM"}
{"created":"2024-02-01 11:25:37","title":"Quaternionic resolvent equation and series expansion of the $\\mathcal{S}$-resolvent operator","abstract":"In the present paper, we prove a resolvent equation for the $\\mathcal{S}$-resolvent operator in the quaternionic framework. Exploiting this resolvent equation, we find a series expansion for the $\\mathcal{S}$-resolvent operator in an open neighborhood of any given quaternion belonging to the $\\mathcal{S}$-resolvent set. Some consequences of the series expansion are deduced. In particular, we describe a property of the geometry of the $\\mathcal{S}$-resolvent set in terms of the Cassini pseudo-metric on quaternions. The concept of vector-valued real analytic function of several variables plays a crucial role in the proof of the mentioned series expansion for the $\\mathcal{S}$-resolvent operator.","sentences":["In the present paper, we prove a resolvent equation for the $\\mathcal{S}$-resolvent operator in the quaternionic framework.","Exploiting this resolvent equation, we find a series expansion for the $\\mathcal{S}$-resolvent operator in an open neighborhood of any given quaternion belonging to the $\\mathcal{S}$-resolvent set.","Some consequences of the series expansion are deduced.","In particular, we describe a property of the geometry of the $\\mathcal{S}$-resolvent set in terms of the Cassini pseudo-metric on quaternions.","The concept of vector-valued real analytic function of several variables plays a crucial role in the proof of the mentioned series expansion for the $\\mathcal{S}$-resolvent operator."],"url":"http://arxiv.org/abs/2402.00511v1","category":"math.SP"}
{"created":"2024-02-01 11:18:50","title":"Real-Time Coordination of Integrated Transmission and Distribution Systems: Flexibility Modeling and Distributed NMPC Scheduling","abstract":"This paper proposes a real-time distributed operational architecture to efficiently coordinate intergrated transmission and distribution systems (ITD). At the distribution system level, the distribution system operator (DSO) computes the aggregated flexibility of all controllable devices by power-energy envelopes and provides them to the transmission system operators. At the transmission system level, a distributed nonlinear MPC approach is proposed to coordinate the economic dispatch of multiple TSOs, considering the aggregated flexibility of all distribution systems. The subproblems of the proposed approach are associated with different TSOs and individual time periods. In addition, the aggregated flexibility of controllable devices in distribution networks is encapsulated, re-calculated, and communicated through the power-energy envelopes, significantly reducing computational costs and eliminating redundant information exchanges between TSOs and DSOs for privacy and security preservation. The extensive numerical experiment demonstrates the scalability and the computational efficiency of the proposed distributed operational architecture.","sentences":["This paper proposes a real-time distributed operational architecture to efficiently coordinate intergrated transmission and distribution systems (ITD).","At the distribution system level, the distribution system operator (DSO) computes the aggregated flexibility of all controllable devices by power-energy envelopes and provides them to the transmission system operators.","At the transmission system level, a distributed nonlinear MPC approach is proposed to coordinate the economic dispatch of multiple TSOs, considering the aggregated flexibility of all distribution systems.","The subproblems of the proposed approach are associated with different TSOs and individual time periods.","In addition, the aggregated flexibility of controllable devices in distribution networks is encapsulated, re-calculated, and communicated through the power-energy envelopes, significantly reducing computational costs and eliminating redundant information exchanges between TSOs and DSOs for privacy and security preservation.","The extensive numerical experiment demonstrates the scalability and the computational efficiency of the proposed distributed operational architecture."],"url":"http://arxiv.org/abs/2402.00508v1","category":"math.OC"}
{"created":"2024-02-01 11:10:33","title":"Early Stages of Drop Coalescence","abstract":"Despite the large body of research on coalescence, firm agreement between experiment, theory, and computation has not been established for the very earliest times following the initial contact of two liquid volumes. Combining a range of experimental and computational modelling approaches, we have been able to elucidate the influence of the intervening gas, of the van der Waals forces and of thermal fluctuations on coalescence. For simple liquids, the gas influences both pre- and post-contact regimes, with a jump-to-contact the primary mode of merging. Subsequently, wave-like air pockets are observed. For a system with ultralow interfacial tension, that mimics nanodrop behaviour, the very first moments are governed by thermal fluctuations at the interfaces, with a nontrivial opening speed given by stochastic thermally driven motion.","sentences":["Despite the large body of research on coalescence, firm agreement between experiment, theory, and computation has not been established for the very earliest times following the initial contact of two liquid volumes.","Combining a range of experimental and computational modelling approaches, we have been able to elucidate the influence of the intervening gas, of the van der Waals forces and of thermal fluctuations on coalescence.","For simple liquids, the gas influences both pre- and post-contact regimes, with a jump-to-contact the primary mode of merging.","Subsequently, wave-like air pockets are observed.","For a system with ultralow interfacial tension, that mimics nanodrop behaviour, the very first moments are governed by thermal fluctuations at the interfaces, with a nontrivial opening speed given by stochastic thermally driven motion."],"url":"http://arxiv.org/abs/2402.00500v1","category":"physics.flu-dyn"}
{"created":"2024-02-01 11:08:12","title":"Fermion-like behavior of elements in their spatial distribution around points of interest","abstract":"In this paper we analyze the spatial distribution of elements around points of interest. Based on a spatial exclusion principle we model the system by means of a Fermi-Dirac distribution defined by two easily interpretable parameters. By means of image analysis, two real cases are studied and compared to the theory: people in an open-air concert and cars in a mall parking-lot. We show that they closely obey the proposed fermion-like statistics.","sentences":["In this paper we analyze the spatial distribution of elements around points of interest.","Based on a spatial exclusion principle we model the system by means of a Fermi-Dirac distribution defined by two easily interpretable parameters.","By means of image analysis, two real cases are studied and compared to the theory: people in an open-air concert and cars in a mall parking-lot.","We show that they closely obey the proposed fermion-like statistics."],"url":"http://arxiv.org/abs/2402.00499v1","category":"physics.soc-ph"}
{"created":"2024-02-01 10:40:42","title":"Extreme value statistics of nerve transmission delay","abstract":"Nerve transmission delay is an important topic in neuroscience. Spike signals fired or received at the dendrites of a neuron travel from the axon to the presynaptic cell. The spike signal triggers a chemical reaction at the synapse, wherein a presynaptic cell transfers neurotransmitters to the postsynaptic cell, and regenerates electrical signals by a chemical reaction process through ion channels and transmits it to neighboring neurons. In the context of describing the complex physiological reaction process as a stochastic process, this study aimed to show that the distribution of the maximum time interval of spike signals follows extreme order statistics. By considering the statistical variance in the time constant of the Leaky Integrate-and-Fire model, which is a deterministic time evolution model of spike signals, we enabled randomness in the time interval of spike signals. When the time constant follows an exponential distribution function, the time interval of the spike signal also follows an exponential distribution. In this case, our theory and simulations confirmed that the histogram of the maximum time interval follows the Gumbel distribution, which is one of the three types of extreme value statistics. We also confirmed that the histogram of the maximum time interval follows a Fr\\'{e}chet distribution when the time interval of the spike signal follows a Pareto distribution. These findings confirm that nerve transmission delay can be described using extreme value statistics and could, therefore, be used as a new indicator for transmission delay.","sentences":["Nerve transmission delay is an important topic in neuroscience.","Spike signals fired or received at the dendrites of a neuron travel from the axon to the presynaptic cell.","The spike signal triggers a chemical reaction at the synapse, wherein a presynaptic cell transfers neurotransmitters to the postsynaptic cell, and regenerates electrical signals by a chemical reaction process through ion channels and transmits it to neighboring neurons.","In the context of describing the complex physiological reaction process as a stochastic process, this study aimed to show that the distribution of the maximum time interval of spike signals follows extreme order statistics.","By considering the statistical variance in the time constant of the Leaky Integrate-and-Fire model, which is a deterministic time evolution model of spike signals, we enabled randomness in the time interval of spike signals.","When the time constant follows an exponential distribution function, the time interval of the spike signal also follows an exponential distribution.","In this case, our theory and simulations confirmed that the histogram of the maximum time interval follows the Gumbel distribution, which is one of the three types of extreme value statistics.","We also confirmed that the histogram of the maximum time interval follows a Fr\\'{e}chet distribution when the time interval of the spike signal follows a Pareto distribution.","These findings confirm that nerve transmission delay can be described using extreme value statistics and could, therefore, be used as a new indicator for transmission delay."],"url":"http://arxiv.org/abs/2402.00484v1","category":"q-bio.NC"}
{"created":"2024-02-01 10:32:05","title":"Resource Bounds for Quantum Circuit Mapping via Quantum Circuit Complexity","abstract":"Efficiently mapping quantum circuits onto hardware is an integral part of the quantum compilation process, wherein a quantum circuit is modified in accordance with the stringent architectural demands of a quantum processor. Many techniques exist for solving the quantum circuit mapping problem, many of which relate quantum circuit mapping to classical computer science. This work considers a novel perspective on quantum circuit mapping, in which the routing process of a simplified circuit is viewed as a composition of quantum operations acting on density matrices representing the quantum circuit and processor. Drawing on insight from recent advances in quantum information theory and information geometry, we show that a minimal SWAP gate count for executing a quantum circuit on a device emerges via the minimization of the distance between quantum states using the quantum Jensen-Shannon divergence. Additionally, we develop a novel initial placement algorithm based on a graph similarity search that selects the partition nearest to a graph isomorphism between interaction and coupling graphs. From these two ingredients, we then construct a polynomial-time algorithm for calculating the SWAP gate lower bound, which is directly compared alongside the IBM Qiskit compiler for over 600 realistic benchmark experiments, as well as against a brute-force method for smaller benchmarks. In our simulations, we unambiguously find that neither the brute-force method nor the Qiskit compiler surpass our bound, implying utility as a precise estimation of minimal overhead when realizing quantum algorithms on constrained quantum hardware. This work constitutes the first use of quantum circuit uncomplexity to practically-relevant quantum computing. We anticipate that this method may have diverse applicability outside of the scope of quantum information science, and we discuss several of these possibilities.","sentences":["Efficiently mapping quantum circuits onto hardware is an integral part of the quantum compilation process, wherein a quantum circuit is modified in accordance with the stringent architectural demands of a quantum processor.","Many techniques exist for solving the quantum circuit mapping problem, many of which relate quantum circuit mapping to classical computer science.","This work considers a novel perspective on quantum circuit mapping, in which the routing process of a simplified circuit is viewed as a composition of quantum operations acting on density matrices representing the quantum circuit and processor.","Drawing on insight from recent advances in quantum information theory and information geometry, we show that a minimal SWAP gate count for executing a quantum circuit on a device emerges via the minimization of the distance between quantum states using the quantum Jensen-Shannon divergence.","Additionally, we develop a novel initial placement algorithm based on a graph similarity search that selects the partition nearest to a graph isomorphism between interaction and coupling graphs.","From these two ingredients, we then construct a polynomial-time algorithm for calculating the SWAP gate lower bound, which is directly compared alongside the IBM Qiskit compiler for over 600 realistic benchmark experiments, as well as against a brute-force method for smaller benchmarks.","In our simulations, we unambiguously find that neither the brute-force method nor the Qiskit compiler surpass our bound, implying utility as a precise estimation of minimal overhead when realizing quantum algorithms on constrained quantum hardware.","This work constitutes the first use of quantum circuit uncomplexity to practically-relevant quantum computing.","We anticipate that this method may have diverse applicability outside of the scope of quantum information science, and we discuss several of these possibilities."],"url":"http://arxiv.org/abs/2402.00478v1","category":"quant-ph"}
{"created":"2024-02-01 10:31:30","title":"HERITRACE: Tracing Evolution and Bridging Data for Streamlined Curatorial Work in the GLAM Domain","abstract":"HERITRACE is a semantic data management system tailored for the GLAM sector. It is engineered to streamline data curation for non-technical users while also offering an efficient administrative interface for technical staff. The paper compares HERITRACE with other established platforms such as OmekaS, Semantic MediaWiki, Research Space, and CLEF, emphasizing its advantages in user friendliness, provenance management, change tracking, customization capabilities, and data integration. The system leverages SHACL for data modeling and employs the OpenCitations Data Model (OCDM) for provenance and change tracking, ensuring a harmonious blend of advanced technical features and user accessibility. Future developments include the integration of a robust authentication system and the expansion of data compatibility via the RDF Mapping Language (RML), enhancing HERITRACE's utility in digital heritage management.","sentences":["HERITRACE is a semantic data management system tailored for the GLAM sector.","It is engineered to streamline data curation for non-technical users while also offering an efficient administrative interface for technical staff.","The paper compares HERITRACE with other established platforms such as OmekaS, Semantic MediaWiki, Research Space, and CLEF, emphasizing its advantages in user friendliness, provenance management, change tracking, customization capabilities, and data integration.","The system leverages SHACL for data modeling and employs the OpenCitations Data Model (OCDM) for provenance and change tracking, ensuring a harmonious blend of advanced technical features and user accessibility.","Future developments include the integration of a robust authentication system and the expansion of data compatibility via the RDF Mapping Language (RML), enhancing HERITRACE's utility in digital heritage management."],"url":"http://arxiv.org/abs/2402.00477v1","category":"cs.DL"}
{"created":"2024-02-01 10:28:12","title":"Reflections on coproducts for non-unital algebras","abstract":"A coproduct on a vector space $A$ is defined as a linear map $\\Delta:A\\to A\\otimes A$ satisfying coassociativity $(\\Delta\\otimes\\iota)\\Delta=(\\iota\\otimes\\Delta)\\Delta$. We use $\\iota$ for the identity map. If $G$ is a finite group and if $A$ is the space of all complex functions on $G$, a coproduct on $A$ is defined by $\\Delta(f)(p,q)=f(pq)$ where $p,q\\in G$. We identify $A\\otimes A$ with complex functions on the Cartesian product $G\\times G$. Coassociativity follows from the associativity of the product in $G$. Unfortunately, sometimes this notion of a coproduct is not the appropriate one. In this note, we consider the case of an algebra $A$, not necessarily unital but with a non-degenerate product. Now a coproduct is a linear map from $A$ to $M(A\\otimes A)$, the multiplier algebra of $A\\otimes A$. Unfortunately, it is no longer possible to express coassociativity in its usual form as the maps $\\Delta\\otimes\\iota$ and $\\iota\\otimes \\Delta$, defined on $A\\otimes A$, may no longer be defined on the multiplier algebra $M(A\\otimes A)$. Similar problems occurs when we want to define a useful notion of a coaction in the case of non-unital algebras. We discuss this in another paper. Not all the results we present in this paper are new. We provide a number of references to the original papers where some of this material is treated. However, in the original papers, results are not always found in an organized form and we hope to improve that here. Further a few solutions to some open questions are included as well as some more peculiar examples. Finally, we discuss some open problems and possible further research.","sentences":["A coproduct on a vector space $A$ is defined as a linear map $\\Delta:A\\to A\\otimes A$ satisfying coassociativity $(\\Delta\\otimes\\iota)\\Delta=(\\iota\\otimes\\Delta)\\Delta$.","We use $\\iota$ for the identity map.","If $G$ is a finite group and if $A$ is the space of all complex functions on $G$, a coproduct on $A$ is defined by $\\Delta(f)(p,q)=f(pq)$ where $p,q\\in G$. We identify $A\\otimes A$ with complex functions on the Cartesian product $G\\times G$. Coassociativity follows from the associativity of the product in $G$. Unfortunately, sometimes this notion of a coproduct is not the appropriate one.","In this note, we consider the case of an algebra $A$, not necessarily unital","but with a non-degenerate product.","Now a coproduct is a linear map from $A$ to $M(A\\otimes A)$, the multiplier algebra of $A\\otimes A$.","Unfortunately, it is no longer possible to express coassociativity in its usual form as the maps $\\Delta\\otimes\\iota$ and $\\iota\\otimes \\Delta$, defined on $A\\otimes A$, may no longer be defined on the multiplier algebra $M(A\\otimes A)$.","Similar problems occurs when we want to define a useful notion of a coaction in the case of non-unital algebras.","We discuss this in another paper.","Not all the results we present in this paper are new.","We provide a number of references to the original papers where some of this material is treated.","However, in the original papers, results are not always found in an organized form and we hope to improve that here.","Further a few solutions to some open questions are included as well as some more peculiar examples.","Finally, we discuss some open problems and possible further research."],"url":"http://arxiv.org/abs/2402.00476v1","category":"math.RA"}
{"created":"2024-02-01 10:16:45","title":"Modeling of heat conduction through rate equations","abstract":"Starting from a classical thermodynamic approach, we derive rate-type equations to describe the behavior of heat flow in deformable media. Constitutive equations are defined in the material (Lagrangian) description where the standard time derivative satisfies the principle of objectivity. The statement of the Second Law is formulated in the classical form and the thermodynamic restrictions are then developed following the Coleman-Noll procedure. However, instead of the Clausius Duhem inequality we consider the corresponding equality where the entropy production rate is prescribed by a non-negative constitutive function. Both the free energy and the entropy production are assumed to depend on a common set of independent variables involving, in addition to temperature, both temperature gradient and heat-flux vector together with their time derivatives. This approach results in rate-type constitutive equations for the heat-flux vector that are intrinsically consistent with the Second Law and easily amenable to analysis. In addition to obtaining already known models (e.g. Cattaneo-Maxwell's, Jeffreys-like and Green-Naghdi's heat conductors), this scheme allows us to build new and more complex models of heat transport that may have applications in describing the thermal behavior in nanosystems. Indeed, when higher order time derivatives of the heat flux vector are involved, many different relaxation times occur within the rate equation.","sentences":["Starting from a classical thermodynamic approach, we derive rate-type equations to describe the behavior of heat flow in deformable media.","Constitutive equations are defined in the material (Lagrangian) description where the standard time derivative satisfies the principle of objectivity.","The statement of the Second Law is formulated in the classical form and the thermodynamic restrictions are then developed following the Coleman-Noll procedure.","However, instead of the Clausius Duhem inequality we consider the corresponding equality where the entropy production rate is prescribed by a non-negative constitutive function.","Both the free energy and the entropy production are assumed to depend on a common set of independent variables involving, in addition to temperature, both temperature gradient and heat-flux vector together with their time derivatives.","This approach results in rate-type constitutive equations for the heat-flux vector that are intrinsically consistent with the Second Law and easily amenable to analysis.","In addition to obtaining already known models (e.g. Cattaneo-Maxwell's, Jeffreys-like and Green-Naghdi's heat conductors), this scheme allows us to build new and more complex models of heat transport that may have applications in describing the thermal behavior in nanosystems.","Indeed, when higher order time derivatives of the heat flux vector are involved, many different relaxation times occur within the rate equation."],"url":"http://arxiv.org/abs/2402.00470v1","category":"math-ph"}
{"created":"2024-02-01 10:11:33","title":"Normalized solutions for a fractional Schr\u00f6dinger-Poisson system with critical growth","abstract":"In this paper, we study the fractional critical Schr\\\"{o}dinger-Poisson system \\[\\begin{cases} (-\\Delta)^su +\\lambda\\phi u= \\alpha u+\\mu|u|^{q-2}u+|u|^{2^*_s-2}u,&~~ \\mbox{in}~{\\mathbb R}^3,\\\\ (-\\Delta)^t\\phi=u^2,&~~ \\mbox{in}~{\\mathbb R}^3,\\end{cases} \\] having prescribed mass \\[\\int_{\\mathbb R^3} |u|^2dx=a^2,\\] where $ s, t \\in (0, 1)$ satisfies $2s+2t > 3, q\\in(2,2^*_s), a>0$ and $\\lambda,\\mu>0$ parameters and $\\alpha\\in{\\mathbb R}$ is an undetermined parameter. Under the $L^2$-subcritical perturbation $q\\in (2, 2+\\frac{4s}{3})$, we derive the existence of multiple normalized solutions by means of the truncation technique, concentration-compactness principle and the genus theory. For the $L^2$-supercritical perturbation $q\\in (2+\\frac{4s}{3}, 2^*_s)$, by applying the constrain variational methods and the mountain pass theorem, we show the existence of positive normalized ground state solutions.","sentences":["In this paper, we study the fractional critical Schr\\\"{o}dinger-Poisson system \\[\\begin{cases} (-\\Delta)^su +\\lambda\\phi u= \\alpha u+\\mu|u|^{q-2}u+|u|^{2^*_s-2}u,&~~ \\mbox{in}~{\\mathbb","R}^3,\\\\ (-\\Delta)^t\\phi=u^2,&~~ \\mbox{in}~{\\mathbb R}^3,\\end{cases} \\] having prescribed mass \\[\\int_{\\mathbb R^3} |u|^2dx=a^2,\\] where $ s, t \\in (0, 1)$ satisfies $2s+2t > 3, q\\in(2,2^*_s), a>0$ and $\\lambda,\\mu>0$ parameters and $\\alpha\\in{\\mathbb R}$ is an undetermined parameter.","Under the $L^2$-subcritical perturbation $q\\in (2, 2+\\frac{4s}{3})$, we derive the existence of multiple normalized solutions by means of the truncation technique, concentration-compactness principle and the genus theory.","For the $L^2$-supercritical perturbation $q\\in (2+\\frac{4s}{3}, 2^*_s)$, by applying the constrain variational methods and the mountain pass theorem, we show the existence of positive normalized ground state solutions."],"url":"http://arxiv.org/abs/2402.00464v1","category":"math.AP"}
{"created":"2024-02-01 09:58:44","title":"Octet baryon and heavy meson interactions in chiral effective field theory","abstract":"We calculate the effective potentials of the octet baryon and heavy meson systems using the chiral effective field theory up to the next-to-leading order. We consider the contact terms, one-pseudoscalar-meson and two-pseudoscalar-meson exchange contributions, facilitating a comprehensive analysis of the short-, mid-, and long-range interactions in these systems. The low energy constants (LECs) are correlated with those of the $\\bar{N}N$ interaction using a quark-level Lagrangian approach. We also incorporate the decuplet baryon contributions in the loop diagrams. Our research provides new insights into several near-threshold charmed baryons [e.g., $\\Lambda_{c}(2940)$, $\\Xi_{c}(3055)$, and $\\Omega_{c}(3188)$, etc.] around $3$ GeV from the hadronic molecular perspective. We also identify several molecular states, designated as $\\Xi_{c}$, within the mass range of $3100-3500$ MeV. Further measurements of their resonance parameters and decay patterns in experiments will help to discriminate the conventional baryon and hadronic molecule explanations for these near-threshold states.","sentences":["We calculate the effective potentials of the octet baryon and heavy meson systems using the chiral effective field theory up to the next-to-leading order.","We consider the contact terms, one-pseudoscalar-meson and two-pseudoscalar-meson exchange contributions, facilitating a comprehensive analysis of the short-, mid-, and long-range interactions in these systems.","The low energy constants (LECs) are correlated with those of the $\\bar{N}N$ interaction using a quark-level Lagrangian approach.","We also incorporate the decuplet baryon contributions in the loop diagrams.","Our research provides new insights into several near-threshold charmed baryons [e.g., $\\Lambda_{c}(2940)$, $\\Xi_{c}(3055)$, and $\\Omega_{c}(3188)$, etc.] around $3$ GeV from the hadronic molecular perspective.","We also identify several molecular states, designated as $\\Xi_{c}$, within the mass range of $3100-3500$ MeV. Further measurements of their resonance parameters and decay patterns in experiments will help to discriminate the conventional baryon and hadronic molecule explanations for these near-threshold states."],"url":"http://arxiv.org/abs/2402.00460v1","category":"hep-ph"}
{"created":"2024-02-01 09:45:11","title":"New Lower Bounds on Aperiodic Ambiguity Function of Unimodular Sequences","abstract":"This paper presents new aperiodic ambiguity function (AF) lower bounds of unimodular sequences under certain low ambiguity zone. Our key idea, motivated by the Levenshtein correlation bound, is to introduce two weight vectors associated to the delay and Doppler shifts, respectively, and then exploit the upper and lower bounds on the Frobenius norm of the weighted auto- and cross-AF matrices to derive these bounds. Furthermore, the inherent structure properties of aperiodic AF are also utilized in our derivation. The derived bounds are useful design guidelines for optimal AF shaping in modern communication and radar systems.","sentences":["This paper presents new aperiodic ambiguity function (AF) lower bounds of unimodular sequences under certain low ambiguity zone.","Our key idea, motivated by the Levenshtein correlation bound, is to introduce two weight vectors associated to the delay and Doppler shifts, respectively, and then exploit the upper and lower bounds on the Frobenius norm of the weighted auto- and cross-AF matrices to derive these bounds.","Furthermore, the inherent structure properties of aperiodic AF are also utilized in our derivation.","The derived bounds are useful design guidelines for optimal AF shaping in modern communication and radar systems."],"url":"http://arxiv.org/abs/2402.00455v1","category":"cs.IT"}
{"created":"2024-02-01 09:36:56","title":"CPT: Competence-progressive Training Strategy for Few-shot Node Classification","abstract":"Graph Neural Networks (GNNs) have made significant advancements in node classification, but their success relies on sufficient labeled nodes per class in the training data. Real-world graph data often exhibits a long-tail distribution with sparse labels, emphasizing the importance of GNNs' ability in few-shot node classification, which entails categorizing nodes with limited data. Traditional episodic meta-learning approaches have shown promise in this domain, but they face an inherent limitation: it might lead the model to converge to suboptimal solutions because of random and uniform task assignment, ignoring task difficulty levels. This could lead the meta-learner to face complex tasks too soon, hindering proper learning. Ideally, the meta-learner should start with simple concepts and advance to more complex ones, like human learning. So, we introduce CPT, a novel two-stage curriculum learning method that aligns task difficulty with the meta-learner's progressive competence, enhancing overall performance. Specifically, in CPT's initial stage, the focus is on simpler tasks, fostering foundational skills for engaging with complex tasks later. Importantly, the second stage dynamically adjusts task difficulty based on the meta-learner's growing competence, aiming for optimal knowledge acquisition. Extensive experiments on popular node classification datasets demonstrate significant improvements of our strategy over existing methods.","sentences":["Graph Neural Networks (GNNs) have made significant advancements in node classification, but their success relies on sufficient labeled nodes per class in the training data.","Real-world graph data often exhibits a long-tail distribution with sparse labels, emphasizing the importance of GNNs' ability in few-shot node classification, which entails categorizing nodes with limited data.","Traditional episodic meta-learning approaches have shown promise in this domain, but they face an inherent limitation: it might lead the model to converge to suboptimal solutions because of random and uniform task assignment, ignoring task difficulty levels.","This could lead the meta-learner to face complex tasks too soon, hindering proper learning.","Ideally, the meta-learner should start with simple concepts and advance to more complex ones, like human learning.","So, we introduce CPT, a novel two-stage curriculum learning method that aligns task difficulty with the meta-learner's progressive competence, enhancing overall performance.","Specifically, in CPT's initial stage, the focus is on simpler tasks, fostering foundational skills for engaging with complex tasks later.","Importantly, the second stage dynamically adjusts task difficulty based on the meta-learner's growing competence, aiming for optimal knowledge acquisition.","Extensive experiments on popular node classification datasets demonstrate significant improvements of our strategy over existing methods."],"url":"http://arxiv.org/abs/2402.00450v1","category":"cs.LG"}
{"created":"2024-02-01 09:18:03","title":"Hadron Spectroscopy with lattice QCD: challenges and opportunities","abstract":"Ongoing challenges in computing the spectrum of hadronic resonances and shallow bound-states from lattice QCD are reviewed. Since such states are identified as poles in the scattering matrix, nearby non-analyticities must be treated to analytically continue to complex center-of-mass energies. Significant lattice spacing effects have also been observed in some channels, necessitating a continuum limit. Recent achievements are also highlighted, including lattice investigations of states in the charm region, baryon-baryon scattering, and the first coupled channel meson-baryon amplitude in the $\\Lambda(1405)$ channel.","sentences":["Ongoing challenges in computing the spectrum of hadronic resonances and shallow bound-states from lattice QCD are reviewed.","Since such states are identified as poles in the scattering matrix, nearby non-analyticities must be treated to analytically continue to complex center-of-mass energies.","Significant lattice spacing effects have also been observed in some channels, necessitating a continuum limit.","Recent achievements are also highlighted, including lattice investigations of states in the charm region, baryon-baryon scattering, and the first coupled channel meson-baryon amplitude in the $\\Lambda(1405)$ channel."],"url":"http://arxiv.org/abs/2402.00443v1","category":"hep-lat"}
{"created":"2024-02-01 09:13:55","title":"Collisional effects in modeling solar polarized lines","abstract":"Rigorous implementation of the effects of collisions in modeling the formation of the polarized solar lines is of utmost importance in order to realistically analyze the available, highly sensitive solar spectropolarimetric observations. Indeed, even when an observation seems to fit well with theory, one can misinterpret results if important effects due to collisions are not correctly implemented in the modeling process. We point out inconsistencies in the models adopted to implement the Paschen Back effect together with collisional effects on the solar linear polarization formed by scattering of anisotropic radiation. Because the significance of these inconsistencies increases as polarization becomes increasingly responsive to collisions, we investigate the range of hydrogen densities $n_\\text{H}$ to which the polarization is sensitive. We used the density matrix formalism in the tensorial irreducible basis, which was developed within the theory of atom-radiation interaction and of atomic collisions. We solved the statistical equilibrium equations for multi-level atoms with hyperfine structure (HFS) in order to evaluate the collisional depolarization of levels of the D1-D2 lines of the K I atom. We find that collisions play a prominent role, particularly at hydrogen densities of between 10$^{13}$ and 10$^{16}$ cm$^{-3}$. So far, analyses of polarized lines formed in the presence of solar magnetic field have incorporated, if at all, collisional rates calculated assuming zero magnetic field. This could be a good approximation in the Hanle regime but not in the Paschen Back regime. For typical quiet Sun magnetic fields, the latter regime could be reached, and level-crossing takes place in several atomic systems. Therefore, one must be careful when using collisional rates calculated in the zero-field case to interpret linear polarization formed in magnetized media.","sentences":["Rigorous implementation of the effects of collisions in modeling the formation of the polarized solar lines is of utmost importance in order to realistically analyze the available, highly sensitive solar spectropolarimetric observations.","Indeed, even when an observation seems to fit well with theory, one can misinterpret results if important effects due to collisions are not correctly implemented in the modeling process.","We point out inconsistencies in the models adopted to implement the Paschen Back effect together with collisional effects on the solar linear polarization formed by scattering of anisotropic radiation.","Because the significance of these inconsistencies increases as polarization becomes increasingly responsive to collisions, we investigate the range of hydrogen densities $n_\\text{H}$ to which the polarization is sensitive.","We used the density matrix formalism in the tensorial irreducible basis, which was developed within the theory of atom-radiation interaction and of atomic collisions.","We solved the statistical equilibrium equations for multi-level atoms with hyperfine structure (HFS) in order to evaluate the collisional depolarization of levels of the D1-D2 lines of the K I atom.","We find that collisions play a prominent role, particularly at hydrogen densities of between 10$^{13}$ and 10$^{16}$ cm$^{-3}$. So far, analyses of polarized lines formed in the presence of solar magnetic field have incorporated, if at all, collisional rates calculated assuming zero magnetic field.","This could be a good approximation in the Hanle regime but not in the Paschen Back regime.","For typical quiet Sun magnetic fields, the latter regime could be reached, and level-crossing takes place in several atomic systems.","Therefore, one must be careful when using collisional rates calculated in the zero-field case to interpret linear polarization formed in magnetized media."],"url":"http://arxiv.org/abs/2402.00441v1","category":"astro-ph.SR"}
{"created":"2024-02-01 08:54:30","title":"Electromagnetic and gravitational form factors of the nucleon","abstract":"Form factors are Lorentz invariant functions describing the internal structure of a system. In particular, they encode how physical properties like, e.g., charge, energy, momentum, and pressure are spatially distributed. While nucleon electromagnetic form factors have been studied for a long time, the first extraction of nucleon gravitational form factors from experimental data was reported in 2018, triggering a lot of enthusiasm and attention in the hadronic community. In this contribution we review some theoretical bases, discuss recent developments regarding the physical interpretation of these form factors, and give a glimpse of what can be learned about the mass and spin structure of the nucleon.","sentences":["Form factors are Lorentz invariant functions describing the internal structure of a system.","In particular, they encode how physical properties like, e.g., charge, energy, momentum, and pressure are spatially distributed.","While nucleon electromagnetic form factors have been studied for a long time, the first extraction of nucleon gravitational form factors from experimental data was reported in 2018, triggering a lot of enthusiasm and attention in the hadronic community.","In this contribution we review some theoretical bases, discuss recent developments regarding the physical interpretation of these form factors, and give a glimpse of what can be learned about the mass and spin structure of the nucleon."],"url":"http://arxiv.org/abs/2402.00429v1","category":"hep-ph"}
{"created":"2024-02-01 08:50:00","title":"Ralph Kenna's scaling relations in critical phenomena","abstract":"In this note, we revisit the scaling relations among ``hatted critical exponents'' which were first derived by Ralph Kenna, Des Johnston and Wolfhard Janke, and we propose an alternative derivation for some of them. For the scaling relation involving the behavior of the correlation function, we will propose an alternative form since we believe that the expression is erroneous in the work of Ralph and his collaborators.","sentences":["In this note, we revisit the scaling relations among ``hatted critical exponents'' which were first derived by Ralph Kenna, Des Johnston and Wolfhard Janke, and we propose an alternative derivation for some of them.","For the scaling relation involving the behavior of the correlation function, we will propose an alternative form since we believe that the expression is erroneous in the work of Ralph and his collaborators."],"url":"http://arxiv.org/abs/2402.00427v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-01 08:44:16","title":"Hierarchical Integral Probability Metrics: A distance on random probability measures with low sample complexity","abstract":"Random probabilities are a key component to many nonparametric methods in Statistics and Machine Learning. To quantify comparisons between different laws of random probabilities several works are starting to use the elegant Wasserstein over Wasserstein distance. In this paper we prove that the infinite-dimensionality of the space of probabilities drastically deteriorates its sample complexity, which is slower than any polynomial rate in the sample size. We thus propose a new distance that preserves many desirable properties of the former while achieving a parametric rate of convergence. In particular, our distance 1) metrizes weak convergence; 2) can be estimated numerically through samples with low complexity; 3) can be bounded analytically from above and below. The main ingredient are integral probability metrics, which lead to the name hierarchical IPM.","sentences":["Random probabilities are a key component to many nonparametric methods in Statistics and Machine Learning.","To quantify comparisons between different laws of random probabilities several works are starting to use the elegant Wasserstein over Wasserstein distance.","In this paper we prove that the infinite-dimensionality of the space of probabilities drastically deteriorates its sample complexity, which is slower than any polynomial rate in the sample size.","We thus propose a new distance that preserves many desirable properties of the former while achieving a parametric rate of convergence.","In particular, our distance 1) metrizes weak convergence; 2) can be estimated numerically through samples with low complexity; 3) can be bounded analytically from above and below.","The main ingredient are integral probability metrics, which lead to the name hierarchical IPM."],"url":"http://arxiv.org/abs/2402.00423v1","category":"math.ST"}
{"created":"2024-02-01 08:36:19","title":"The algebraic classification of nilpotent Novikov algebras","abstract":"This paper is devoted to the complete algebraic classification of complex $5$-dimensional nilpotent Novikov algebras.","sentences":["This paper is devoted to the complete algebraic classification of complex $5$-dimensional nilpotent Novikov algebras."],"url":"http://arxiv.org/abs/2402.00419v1","category":"math.RA"}
{"created":"2024-02-01 08:15:16","title":"System Characterization of Dispersive Readout in Superconducting Qubits","abstract":"Designing quantum systems with the measurement speed and accuracy needed for quantum error correction using superconducting qubits requires iterative design and test informed by accurate models and characterization tools. We introduce a single protocol, with few prerequisite calibrations, which measures the dispersive shift, resonator linewidth, and drive power used in the dispersive readout of superconducting qubits. We find that the resonator linewidth is poorly controlled with a factor of 2 between the maximum and minimum measured values, and is likely to require focused attention in future quantum error correction experiments. We also introduce a protocol for measuring the readout system efficiency using the same power levels as are used in typical qubit readout, and without the need to measure the qubit coherence. We routinely run these protocols on chips with tens of qubits, driven by automation software with little human interaction. Using the extracted system parameters, we find that a model based on those parameters predicts the readout signal to noise ratio to within 10% over a device with 54 qubits.","sentences":["Designing quantum systems with the measurement speed and accuracy needed for quantum error correction using superconducting qubits requires iterative design and test informed by accurate models and characterization tools.","We introduce a single protocol, with few prerequisite calibrations, which measures the dispersive shift, resonator linewidth, and drive power used in the dispersive readout of superconducting qubits.","We find that the resonator linewidth is poorly controlled with a factor of 2 between the maximum and minimum measured values, and is likely to require focused attention in future quantum error correction experiments.","We also introduce a protocol for measuring the readout system efficiency using the same power levels as are used in typical qubit readout, and without the need to measure the qubit coherence.","We routinely run these protocols on chips with tens of qubits, driven by automation software with little human interaction.","Using the extracted system parameters, we find that a model based on those parameters predicts the readout signal to noise ratio to within 10% over a device with 54 qubits."],"url":"http://arxiv.org/abs/2402.00413v1","category":"quant-ph"}
{"created":"2024-02-01 08:04:25","title":"On the Complexity of Interpolation by Polynomials with Non-negative Real Coefficients","abstract":"In this paper, we consider interpolation by \\textit{completely monotonous} polynomials (CMPs for short), that is, polynomials with non-negative real coefficients. In particular, given a finite set $S\\subset \\mathbb{R}_{>0} \\times \\mathbb{R}_{\\geq 0}$, we consider \\textit{the minimal polynomial} of $S$, introduced by Berg [1985], which is `minimal,' in the sense that it is eventually majorized by all the other CMPs interpolating $S$. We give an upper bound of the degree of the minimal polynomial of $S$ when it exists. Furthermore, we give another algorithm for computing the minimal polynomial of given $S$ which utilizes an order structure on sign sequences. Applying the upper bound above, we also analyze the computational complexity of algorithms for computing minimal polynomials including ours.","sentences":["In this paper, we consider interpolation by \\textit{completely monotonous} polynomials (CMPs for short), that is, polynomials with non-negative real coefficients.","In particular, given a finite set $S\\subset \\mathbb{R}_{>0} \\times \\mathbb{R}_{\\geq 0}$, we consider \\textit{the minimal polynomial} of $S$, introduced by Berg [1985], which is `minimal,' in the sense that it is eventually majorized by all the other CMPs interpolating $S$. We give an upper bound of the degree of the minimal polynomial of $S$ when it exists.","Furthermore, we give another algorithm for computing the minimal polynomial of given $S$ which utilizes an order structure on sign sequences.","Applying the upper bound above, we also analyze the computational complexity of algorithms for computing minimal polynomials including ours."],"url":"http://arxiv.org/abs/2402.00409v1","category":"math.NA"}
{"created":"2024-02-01 08:03:00","title":"Unraveling the complexity of inverting the Sturm-Liouville boundary value problem to its canonical form","abstract":"The Sturm-Liouville boundary value problem (SLBVP) stands as a fundamental cornerstone in the realm of mathematical analysis and physical modeling. Also known as the Sturm-Liouville problem (SLP), this paper explores the intricacies of this classical problem, particularly the relationship between its canonical and Liouville normal (Schr\\\"odinger) forms. While the conversion from canonical to Schr\\\"odinger form using Liouville's transformation is well-known in the literature, the inverse transformation seems to be neglected. Our study attempts to fill this gap by investigating the inverse of Liouville's transformation, that is, given any SLP in the Schr\\\"odinger form with some invariant function, we seek the SLP in its canonical form. By examining closely the second Paine-de Hoog-Anderson (PdHA) problem, we argue that retrieving the SLP to its canonical form can be notoriously difficult and even impossible to achieve in its exact form. Finding the inverse relationship between the two independent variables seems to be the main obstacle. We confirm this claim by considering four different scenarios depending on the potential and density functions that appear in the corresponding invariant function. In the second PdHA problem, this invariant function takes a reciprocal quadratic binomial form. In some cases, the inverse Liouville's transformation produces an exact expression for the SLP in its canonical form. In other situations, however, while an exact canonical form is not possible to obtain, we have successfully derived the SLP in its canonical form asymptotically.","sentences":["The Sturm-Liouville boundary value problem (SLBVP) stands as a fundamental cornerstone in the realm of mathematical analysis and physical modeling.","Also known as the Sturm-Liouville problem (SLP), this paper explores the intricacies of this classical problem, particularly the relationship between its canonical and Liouville normal (Schr\\\"odinger) forms.","While the conversion from canonical to Schr\\\"odinger form using Liouville's transformation is well-known in the literature, the inverse transformation seems to be neglected.","Our study attempts to fill this gap by investigating the inverse of Liouville's transformation, that is, given any SLP in the Schr\\\"odinger form with some invariant function, we seek the SLP in its canonical form.","By examining closely the second Paine-de Hoog-Anderson (PdHA) problem, we argue that retrieving the SLP to its canonical form can be notoriously difficult and even impossible to achieve in its exact form.","Finding the inverse relationship between the two independent variables seems to be the main obstacle.","We confirm this claim by considering four different scenarios depending on the potential and density functions that appear in the corresponding invariant function.","In the second PdHA problem, this invariant function takes a reciprocal quadratic binomial form.","In some cases, the inverse Liouville's transformation produces an exact expression for the SLP in its canonical form.","In other situations, however, while an exact canonical form is not possible to obtain, we have successfully derived the SLP in its canonical form asymptotically."],"url":"http://arxiv.org/abs/2402.00408v1","category":"math.CA"}
{"created":"2024-02-01 08:00:02","title":"Long-time behavior of the heterogeneous SIRS epidemiological model","abstract":"We study the long-time behavior of solutions of the SIRS model, a reaction-diffusion system that appears in epidemiology to describe the spread of epidemics. We allow the system to be heterogeneous periodic. Under some hypotheses on the coefficients, we prove that the solutions converge to an equilibrium that we identify and establish some estimates on the speed of propagation.","sentences":["We study the long-time behavior of solutions of the SIRS model, a reaction-diffusion system that appears in epidemiology to describe the spread of epidemics.","We allow the system to be heterogeneous periodic.","Under some hypotheses on the coefficients, we prove that the solutions converge to an equilibrium that we identify and establish some estimates on the speed of propagation."],"url":"http://arxiv.org/abs/2402.00405v1","category":"math.AP"}
{"created":"2024-02-01 07:53:25","title":"Improving Critical Node Detection Using Neural Network-based Initialization in a Genetic Algorithm","abstract":"The Critical Node Problem (CNP) is concerned with identifying the critical nodes in a complex network. These nodes play a significant role in maintaining the connectivity of the network, and removing them can negatively impact network performance. CNP has been studied extensively due to its numerous real-world applications. Among the different versions of CNP, CNP-1a has gained the most popularity. The primary objective of CNP-1a is to minimize the pair-wise connectivity in the remaining network after deleting a limited number of nodes from a network. Due to the NP-hard nature of CNP-1a, many heuristic/metaheuristic algorithms have been proposed to solve this problem. However, most existing algorithms start with a random initialization, leading to a high cost of obtaining an optimal solution. To improve the efficiency of solving CNP-1a, a knowledge-guided genetic algorithm named K2GA has been proposed. Unlike the standard genetic algorithm framework, K2GA has two main components: a pretrained neural network to obtain prior knowledge on possible critical nodes, and a hybrid genetic algorithm with local search for finding an optimal set of critical nodes based on the knowledge given by the trained neural network. The local search process utilizes a cut node-based greedy strategy. The effectiveness of the proposed knowledgeguided genetic algorithm is verified by experiments on 26 realworld instances of complex networks. Experimental results show that K2GA outperforms the state-of-the-art algorithms regarding the best, median, and average objective values, and improves the best upper bounds on the best objective values for eight realworld instances.","sentences":["The Critical Node Problem (CNP) is concerned with identifying the critical nodes in a complex network.","These nodes play a significant role in maintaining the connectivity of the network, and removing them can negatively impact network performance.","CNP has been studied extensively due to its numerous real-world applications.","Among the different versions of CNP, CNP-1a has gained the most popularity.","The primary objective of CNP-1a is to minimize the pair-wise connectivity in the remaining network after deleting a limited number of nodes from a network.","Due to the NP-hard nature of CNP-1a, many heuristic/metaheuristic algorithms have been proposed to solve this problem.","However, most existing algorithms start with a random initialization, leading to a high cost of obtaining an optimal solution.","To improve the efficiency of solving CNP-1a, a knowledge-guided genetic algorithm named K2GA has been proposed.","Unlike the standard genetic algorithm framework, K2GA has two main components: a pretrained neural network to obtain prior knowledge on possible critical nodes, and a hybrid genetic algorithm with local search for finding an optimal set of critical nodes based on the knowledge given by the trained neural network.","The local search process utilizes a cut node-based greedy strategy.","The effectiveness of the proposed knowledgeguided genetic algorithm is verified by experiments on 26 realworld instances of complex networks.","Experimental results show that K2GA outperforms the state-of-the-art algorithms regarding the best, median, and average objective values, and improves the best upper bounds on the best objective values for eight realworld instances."],"url":"http://arxiv.org/abs/2402.00404v1","category":"cs.NE"}
{"created":"2024-02-01 07:43:51","title":"Continuous-time Trajectory Estimation: A Comparative Study Between Gaussian Process and Spline-based Approaches","abstract":"Continuous-time trajectory estimation is an attractive alternative to discrete-time batch estimation due to the ability to incorporate high-frequency measurements from asynchronous sensors while keeping the number of optimization parameters bounded. Two types of continuous-time estimation have become prevalent in the literature: Gaussian process regression and spline-based estimation. In this paper, we present a direct comparison between these two methods. We first compare them using a simple linear system, and then compare them in a camera and IMU sensor fusion scenario on SE(3) in both simulation and hardware. Our results show that if the same measurements and motion model are used, the two methods achieve similar trajectory accuracy. In addition, if the spline order is chosen so that the degree-of-differentiability of the two trajectory representations match, then they achieve similar solve times as well.","sentences":["Continuous-time trajectory estimation is an attractive alternative to discrete-time batch estimation due to the ability to incorporate high-frequency measurements from asynchronous sensors while keeping the number of optimization parameters bounded.","Two types of continuous-time estimation have become prevalent in the literature: Gaussian process regression and spline-based estimation.","In this paper, we present a direct comparison between these two methods.","We first compare them using a simple linear system, and then compare them in a camera and IMU sensor fusion scenario on SE(3) in both simulation and hardware.","Our results show that if the same measurements and motion model are used, the two methods achieve similar trajectory accuracy.","In addition, if the spline order is chosen so that the degree-of-differentiability of the two trajectory representations match, then they achieve similar solve times as well."],"url":"http://arxiv.org/abs/2402.00399v1","category":"cs.RO"}
{"created":"2024-02-01 07:23:15","title":"Quantum liquid states of spin solitons in a ferroelectric spin-Peierls state","abstract":"In this study, we performed high-magnetic-field magnetization, dielectric, and ultrasound measurements on an organic salt showing a ferroelectric spin-Peierls (FSP) state, which is in close proximity to a quantum critical point. In contrast to the sparsely distributed gas-like spin solitons typically observed in conventional spin-Peierls (SP) states, the FSP state exhibits dense liquid-like spin solitons resulting from strong quantum fluctuations, even at low fields. Nevertheless, akin to conventional SP systems, a magnetic-field-induced transition is observed in the FSP state. In conventional high-field SP states, an emergent wave vector results in the formation of a spin-soliton lattice. However, in the present high-field FSP state, the strong quantum fluctuations preclude the formation of such a soliton lattice, causing the dense solitons to remain in a quantum-mechanically melted state. This observation implies the realization of a quantum liquid--liquid transition of topological particles carrying spin and charge in a ferroelectric insulator.","sentences":["In this study, we performed high-magnetic-field magnetization, dielectric, and ultrasound measurements on an organic salt showing a ferroelectric spin-Peierls (FSP) state, which is in close proximity to a quantum critical point.","In contrast to the sparsely distributed gas-like spin solitons typically observed in conventional spin-Peierls (SP) states, the FSP state exhibits dense liquid-like spin solitons resulting from strong quantum fluctuations, even at low fields.","Nevertheless, akin to conventional SP systems, a magnetic-field-induced transition is observed in the FSP state.","In conventional high-field SP states, an emergent wave vector results in the formation of a spin-soliton lattice.","However, in the present high-field FSP state, the strong quantum fluctuations preclude the formation of such a soliton lattice, causing the dense solitons to remain in a quantum-mechanically melted state.","This observation implies the realization of a quantum liquid--liquid transition of topological particles carrying spin and charge in a ferroelectric insulator."],"url":"http://arxiv.org/abs/2402.00391v1","category":"cond-mat.str-el"}
{"created":"2024-02-01 07:15:03","title":"AssertLLM: Generating and Evaluating Hardware Verification Assertions from Design Specifications via Multi-LLMs","abstract":"Assertion-based verification (ABV) is a critical method for ensuring design circuits comply with their architectural specifications, which are typically described in natural language. This process often requires significant interpretation by engineers to convert these specifications into functional verification assertions. Existing methods for generating assertions from natural language specifications are limited to sentences extracted by engineers, discouraging the practical application. In this work, we present AssertLLM, an automatic assertion generation framework for complete specification files. AssertLLM breaks down the complex task into three phases, incorporating three customized Large Language Models (LLMs) for extracting structural specifications, mapping signal definitions, and generating assertions. Additionally, we provide an open-source benchmark for assessing assertion generation capabilities. Our evaluation of AssertLLM on a full design, encompassing 23 signals, demonstrates that 89% of the generated assertions are both syntactically and functionally accurate.","sentences":["Assertion-based verification (ABV) is a critical method for ensuring design circuits comply with their architectural specifications, which are typically described in natural language.","This process often requires significant interpretation by engineers to convert these specifications into functional verification assertions.","Existing methods for generating assertions from natural language specifications are limited to sentences extracted by engineers, discouraging the practical application.","In this work, we present AssertLLM, an automatic assertion generation framework for complete specification files.","AssertLLM breaks down the complex task into three phases, incorporating three customized Large Language Models (LLMs) for extracting structural specifications, mapping signal definitions, and generating assertions.","Additionally, we provide an open-source benchmark for assessing assertion generation capabilities.","Our evaluation of AssertLLM on a full design, encompassing 23 signals, demonstrates that 89% of the generated assertions are both syntactically and functionally accurate."],"url":"http://arxiv.org/abs/2402.00386v1","category":"cs.AR"}
{"created":"2024-02-01 07:05:10","title":"Adaptive FRIT-based Recursive Robust Controller Design Using Forgetting Factors","abstract":"Adaptive FRIT (A-FRIT) with exponential forgetting (EF) has been proposed for time-varying systems to improve the data dependence of FRIT, which is a direct data-driven tuning method. However, the EF-based method is not a reliable controller because it can cause significant degradation of the control performance and instability unless the persistent excitation (PE) condition is satisfied. To solve this problem, we propose a new A-FRIT method based on directional forgetting (DF) and exponential resetting that can forget old data without instability regardless of the PE condition. To confirm the effectiveness of the proposed method, we applied it to artificial muscle control with strong asymmetric hysteresis characteristics and evaluated its robust performance against load changes during the experiment. The experimental results show that the proposed method based on DF achieves high control performance and is robust against changes in the characteristics and/or target trajectory. The proposed method is also practical because it does not require system identification, model structure, or prior experimentation.","sentences":["Adaptive FRIT (A-FRIT) with exponential forgetting (EF) has been proposed for time-varying systems to improve the data dependence of FRIT, which is a direct data-driven tuning method.","However, the EF-based method is not a reliable controller because it can cause significant degradation of the control performance and instability unless the persistent excitation (PE) condition is satisfied.","To solve this problem, we propose a new A-FRIT method based on directional forgetting (DF) and exponential resetting that can forget old data without instability regardless of the PE condition.","To confirm the effectiveness of the proposed method, we applied it to artificial muscle control with strong asymmetric hysteresis characteristics and evaluated its robust performance against load changes during the experiment.","The experimental results show that the proposed method based on DF achieves high control performance and is robust against changes in the characteristics and/or target trajectory.","The proposed method is also practical because it does not require system identification, model structure, or prior experimentation."],"url":"http://arxiv.org/abs/2402.00384v1","category":"eess.SY"}
{"created":"2024-02-01 06:59:00","title":"A Joint Communication and Computation Framework for Digital Twin over Wireless Networks","abstract":"In this paper, the problem of low-latency communication and computation resource allocation for digital twin (DT) over wireless networks is investigated. In the considered model, multiple physical devices in the physical network (PN) needs to frequently offload the computation task related data to the digital network twin (DNT), which is generated and controlled by the central server. Due to limited energy budget of the physical devices, both computation accuracy and wireless transmission power must be considered during the DT procedure. This joint communication and computation problem is formulated as an optimization problem whose goal is to minimize the overall transmission delay of the system under total PN energy and DNT model accuracy constraints. To solve this problem, an alternating algorithm with iteratively solving device scheduling, power control, and data offloading subproblems. For the device scheduling subproblem, the optimal solution is obtained in closed form through the dual method. For the special case with one physical device, the optimal number of transmission times is reveled. Based on the theoretical findings, the original problem is transformed into a simplified problem and the optimal device scheduling can be found. Numerical results verify that the proposed algorithm can reduce the transmission delay of the system by up to 51.2\\% compared to the conventional schemes.","sentences":["In this paper, the problem of low-latency communication and computation resource allocation for digital twin (DT) over wireless networks is investigated.","In the considered model, multiple physical devices in the physical network (PN) needs to frequently offload the computation task related data to the digital network twin (DNT), which is generated and controlled by the central server.","Due to limited energy budget of the physical devices, both computation accuracy and wireless transmission power must be considered during the DT procedure.","This joint communication and computation problem is formulated as an optimization problem whose goal is to minimize the overall transmission delay of the system under total PN energy and DNT model accuracy constraints.","To solve this problem, an alternating algorithm with iteratively solving device scheduling, power control, and data offloading subproblems.","For the device scheduling subproblem, the optimal solution is obtained in closed form through the dual method.","For the special case with one physical device, the optimal number of transmission times is reveled.","Based on the theoretical findings, the original problem is transformed into a simplified problem and the optimal device scheduling can be found.","Numerical results verify that the proposed algorithm can reduce the transmission delay of the system by up to 51.2\\% compared to the conventional schemes."],"url":"http://arxiv.org/abs/2402.00381v1","category":"cs.IT"}
{"created":"2024-02-01 06:54:53","title":"On the Minimum Depth of Circuits with Linear Number of Wires Encoding Good Codes","abstract":"Let $S_d(n)$ denote the minimum number of wires of a depth-$d$ (unbounded fan-in) circuit encoding an error-correcting code $C:\\{0, 1\\}^n \\to \\{0, 1\\}^{32n}$ with distance at least $4n$. G\\'{a}l, Hansen, Kouck\\'{y}, Pudl\\'{a}k, and Viola [IEEE Trans. Inform. Theory 59(10), 2013] proved that $S_d(n) = \\Theta_d(\\lambda_d(n)\\cdot n)$ for any fixed $d \\ge 3$. By improving their construction and analysis, we prove $S_d(n)= O(\\lambda_d(n)\\cdot n)$. Letting $d = \\alpha(n)$, a version of the inverse Ackermann function, we obtain circuits of linear size. This depth $\\alpha(n)$ is the minimum possible to within an additive constant 2; we credit the nearly-matching depth lower bound to G\\'{a}l et al., since it directly follows their method (although not explicitly claimed or fully verified in that work), and is obtained by making some constants explicit in a graph-theoretic lemma of Pudl\\'{a}k [Combinatorica, 14(2), 1994], extending it to super-constant depths.   We also study a subclass of MDS codes $C: \\mathbb{F}^n \\to \\mathbb{F}^m$ characterized by the Hamming-distance relation $\\mathrm{dist}(C(x), C(y)) \\ge m - \\mathrm{dist}(x, y) + 1$ for any distinct $x, y \\in \\mathbb{F}^n$. (For linear codes this is equivalent to the generator matrix being totally invertible.) We call these superconcentrator-induced codes, and we show their tight connection with superconcentrators. Specifically, we observe that any linear or nonlinear circuit encoding a superconcentrator-induced code must be a superconcentrator graph, and any superconcentrator graph can be converted to a linear circuit, over a sufficiently large field (exponential in the size of the graph), encoding a superconcentrator-induced code.","sentences":["Let $S_d(n)$ denote the minimum number of wires of a depth-$d$ (unbounded fan-in) circuit encoding an error-correcting code $C:\\{0, 1\\}^n \\to \\{0, 1\\}^{32n}$ with distance at least $4n$. G\\'{a}l, Hansen, Kouck\\'{y}, Pudl\\'{a}k, and","Viola [IEEE Trans.","Inform.","Theory 59(10), 2013] proved that $S_d(n) = \\Theta_d(\\lambda_d(n)\\cdot n)$ for any fixed $d \\ge 3$. By improving their construction and analysis, we prove $S_d(n)= O(\\lambda_d(n)\\cdot n)$. Letting $d = \\alpha(n)$, a version of the inverse Ackermann function, we obtain circuits of linear size.","This depth $\\alpha(n)$ is the minimum possible to within an additive constant 2; we credit the nearly-matching depth lower bound to G\\'{a}l et al., since it directly follows their method (although not explicitly claimed or fully verified in that work), and is obtained by making some constants explicit in a graph-theoretic lemma of Pudl\\'{a}k [Combinatorica, 14(2), 1994], extending it to super-constant depths.   ","We also study a subclass of MDS codes $C: \\mathbb{F}^n \\to \\mathbb{F}^m$ characterized by the Hamming-distance relation $\\mathrm{dist}(C(x), C(y))","\\ge m - \\mathrm{dist}(x, y)","+ 1$ for any distinct $x, y \\in \\mathbb{F}^n$. (For linear codes this is equivalent to the generator matrix being totally invertible.)","We call these superconcentrator-induced codes, and we show their tight connection with superconcentrators.","Specifically, we observe that any linear or nonlinear circuit encoding a superconcentrator-induced code must be a superconcentrator graph, and any superconcentrator graph can be converted to a linear circuit, over a sufficiently large field (exponential in the size of the graph), encoding a superconcentrator-induced code."],"url":"http://arxiv.org/abs/2402.00378v1","category":"cs.CC"}
{"created":"2024-02-01 06:28:21","title":"Quantum Information Geometry with Non-Hermitian Systems","abstract":"Information geometry is the application of differential geometry in statistics, where the Fisher-Rao metric serves as the Riemannian metric on the statistical manifold, providing an intrinsic property for parameter sensitivity. In this paper, we explore the Fisher-Rao metric with the non-Hermitian systems. By approximating the Lindblad master equation in the non-Hermitian Hamiltonian, we calculate the time evolution of the quantum geometric metric. Finally, we give an example of the quantum spin Ising model of the imaginary magnetic field, explore the energy spectrum of $\\mathcal{PT}$-symmetric Hamiltonian and the evolution of geometric metric, and discuss that the dissipative effect of the imaginary magnetic field can be eliminated under the condition of adding the control Hamiltonian, so as to improve the accuracy of parameter estimation.","sentences":["Information geometry is the application of differential geometry in statistics, where the Fisher-Rao metric serves as the Riemannian metric on the statistical manifold, providing an intrinsic property for parameter sensitivity.","In this paper, we explore the Fisher-Rao metric with the non-Hermitian systems.","By approximating the Lindblad master equation in the non-Hermitian Hamiltonian, we calculate the time evolution of the quantum geometric metric.","Finally, we give an example of the quantum spin Ising model of the imaginary magnetic field, explore the energy spectrum of $\\mathcal{PT}$-symmetric Hamiltonian and the evolution of geometric metric, and discuss that the dissipative effect of the imaginary magnetic field can be eliminated under the condition of adding the control Hamiltonian, so as to improve the accuracy of parameter estimation."],"url":"http://arxiv.org/abs/2402.00374v1","category":"quant-ph"}
{"created":"2024-02-01 06:24:31","title":"Solutions of the loop equations of a class of generalized Frobenius manifolds","abstract":"We prove the existence and uniqueness of solution of the loop equation associated with a semisimple generalized Frobenius manifold with non-flat unity, and show, for a particular example of one dimensional generalized Frobenius manifold, that the deformation of the Principal Hierarchy induced by the solution of the loop equation is the extended q-deformed KdV hierarchy.","sentences":["We prove the existence and uniqueness of solution of the loop equation associated with a semisimple generalized Frobenius manifold with non-flat unity, and show, for a particular example of one dimensional generalized Frobenius manifold, that the deformation of the Principal Hierarchy induced by the solution of the loop equation is the extended q-deformed KdV hierarchy."],"url":"http://arxiv.org/abs/2402.00373v1","category":"math-ph"}
{"created":"2024-02-01 06:21:46","title":"Quantum information theoretic measures to distinguish fermionized bosons from non-interacting fermions","abstract":"We study the dynamical fermionization of strongly interacting one-dimensional bosons in Tonks-Girardeau limit by solving the time dependent many-boson Schr\\\"odinger equation numerically exactly. We establish that the one-body momentum distribution approaches the ideal Fermi gas distribution at the time of dynamical fermionization. The analysis is further complemented by the measures on two-body level. Investigation on two-body momentum distribution, two-body local and non-local correlation clearly distinguish the fermionized bosons from non-interacting fermions. The magnitude of distinguishablity between the two systems is further discussed employing suitable measures of information theory, i.e., the well known Kullback-Leibler relative entropy and the Jensen-Shannon divergence entropy. We also observe very rich structure in the higher-body density for strongly correlated bosons whereas non-interacting fermions do not possess any higher order correlation beyond two-body.","sentences":["We study the dynamical fermionization of strongly interacting one-dimensional bosons in Tonks-Girardeau limit by solving the time dependent many-boson Schr\\\"odinger equation numerically exactly.","We establish that the one-body momentum distribution approaches the ideal Fermi gas distribution at the time of dynamical fermionization.","The analysis is further complemented by the measures on two-body level.","Investigation on two-body momentum distribution, two-body local and non-local correlation clearly distinguish the fermionized bosons from non-interacting fermions.","The magnitude of distinguishablity between the two systems is further discussed employing suitable measures of information theory, i.e., the well known Kullback-Leibler relative entropy and the Jensen-Shannon divergence entropy.","We also observe very rich structure in the higher-body density for strongly correlated bosons whereas non-interacting fermions do not possess any higher order correlation beyond two-body."],"url":"http://arxiv.org/abs/2402.00372v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-01 06:21:19","title":"What Does the Bot Say? Opportunities and Risks of Large Language Models in Social Media Bot Detection","abstract":"Social media bot detection has always been an arms race between advancements in machine learning bot detectors and adversarial bot strategies to evade detection. In this work, we bring the arms race to the next level by investigating the opportunities and risks of state-of-the-art large language models (LLMs) in social bot detection. To investigate the opportunities, we design novel LLM-based bot detectors by proposing a mixture-of-heterogeneous-experts framework to divide and conquer diverse user information modalities. To illuminate the risks, we explore the possibility of LLM-guided manipulation of user textual and structured information to evade detection. Extensive experiments with three LLMs on two datasets demonstrate that instruction tuning on merely 1,000 annotated examples produces specialized LLMs that outperform state-of-the-art baselines by up to 9.1% on both datasets, while LLM-guided manipulation strategies could significantly bring down the performance of existing bot detectors by up to 29.6% and harm the calibration and reliability of bot detection systems.","sentences":["Social media bot detection has always been an arms race between advancements in machine learning bot detectors and adversarial bot strategies to evade detection.","In this work, we bring the arms race to the next level by investigating the opportunities and risks of state-of-the-art large language models (LLMs) in social bot detection.","To investigate the opportunities, we design novel LLM-based bot detectors by proposing a mixture-of-heterogeneous-experts framework to divide and conquer diverse user information modalities.","To illuminate the risks, we explore the possibility of LLM-guided manipulation of user textual and structured information to evade detection.","Extensive experiments with three LLMs on two datasets demonstrate that instruction tuning on merely 1,000 annotated examples produces specialized LLMs that outperform state-of-the-art baselines by up to 9.1% on both datasets, while LLM-guided manipulation strategies could significantly bring down the performance of existing bot detectors by up to 29.6% and harm the calibration and reliability of bot detection systems."],"url":"http://arxiv.org/abs/2402.00371v1","category":"cs.CL"}
{"created":"2024-02-01 06:05:53","title":"bypass4netns: Accelerating TCP/IP Communications in Rootless Containers","abstract":"\"Rootless containers\" is a concept to run the entire container runtimes and containers without the root privileges. It protects the host environment from attackers exploiting container runtime vulnerabilities. However, when rootless containers communicate with external endpoints, the network performance is low compared to rootful containers because of the overhead of rootless networking components. In this paper, we propose bypass4netns that accelerates TCP/IP communications in rootless containers by bypassing slow networking components. bypass4netns uses sockets allocated on the host. It switches sockets in containers to the host's sockets by intercepting syscalls and injecting the file descriptors using Seccomp. Our method with Seccomp can handle statically linked applications that previous works could not handle. Also, we propose high-performance rootless multi-node communication. We confirmed that rootless containers with bypass4netns achieve more than 30x faster throughput than rootless containers without it. In addition, we evaluated performance with applications and it showed large improvements on some applications.","sentences":["\"Rootless containers\" is a concept to run the entire container runtimes and containers without the root privileges.","It protects the host environment from attackers exploiting container runtime vulnerabilities.","However, when rootless containers communicate with external endpoints, the network performance is low compared to rootful containers because of the overhead of rootless networking components.","In this paper, we propose bypass4netns that accelerates TCP/IP communications in rootless containers by bypassing slow networking components.","bypass4netns uses sockets allocated on the host.","It switches sockets in containers to the host's sockets by intercepting syscalls and injecting the file descriptors using Seccomp.","Our method with Seccomp can handle statically linked applications that previous works could not handle.","Also, we propose high-performance rootless multi-node communication.","We confirmed that rootless containers with bypass4netns achieve more than 30x faster throughput than rootless containers without it.","In addition, we evaluated performance with applications and it showed large improvements on some applications."],"url":"http://arxiv.org/abs/2402.00365v1","category":"cs.NI"}
{"created":"2024-02-01 05:55:43","title":"Securing Cloud-Based Internet of Things: Challenges and Mitigations","abstract":"The Internet of Things (IoT) has seen remarkable advancements in recent years, leading to a paradigm shift in the digital landscape. However, these technological strides have also brought new challenges, particularly in terms of cybersecurity. IoT devices are inherently connected to the internet, which makes them more vulnerable to attack. In addition, IoT services often handle sensitive user data, which could be misused by malicious actors or unauthorized service providers. As more mainstream service providers emerge without uniform regulations, these security risks are expected to escalate exponentially. The task of maintaining the security of IoT devices while they interact with cloud services is also challenging. Newer IoT services, especially those developed and deployed via Platform-as-a-Service (PaaS) and Infrastructure-as-a-Service (IaaS) models, pose additional security threats. Although IoT devices are becoming more affordable and ubiquitous, their growing complexity could expose users to heightened security and privacy risks. This paper highlights these pressing security concerns associated with the widespread adoption of IoT devices and services. We propose potential solutions to bridge the existing security gaps and expect future challenges. Our approach entails a comprehensive exploration of the key security challenges that IoT services are currently facing. We also suggest proactive strategies to mitigate these risks, strengthening the overall security of IoT devices and services.","sentences":["The Internet of Things (IoT) has seen remarkable advancements in recent years, leading to a paradigm shift in the digital landscape.","However, these technological strides have also brought new challenges, particularly in terms of cybersecurity.","IoT devices are inherently connected to the internet, which makes them more vulnerable to attack.","In addition, IoT services often handle sensitive user data, which could be misused by malicious actors or unauthorized service providers.","As more mainstream service providers emerge without uniform regulations, these security risks are expected to escalate exponentially.","The task of maintaining the security of IoT devices while they interact with cloud services is also challenging.","Newer IoT services, especially those developed and deployed via Platform-as-a-Service (PaaS) and Infrastructure-as-a-Service (IaaS) models, pose additional security threats.","Although IoT devices are becoming more affordable and ubiquitous, their growing complexity could expose users to heightened security and privacy risks.","This paper highlights these pressing security concerns associated with the widespread adoption of IoT devices and services.","We propose potential solutions to bridge the existing security gaps and expect future challenges.","Our approach entails a comprehensive exploration of the key security challenges that IoT services are currently facing.","We also suggest proactive strategies to mitigate these risks, strengthening the overall security of IoT devices and services."],"url":"http://arxiv.org/abs/2402.00356v1","category":"cs.CR"}
{"created":"2024-02-01 05:35:25","title":"Machine Unlearning for Image-to-Image Generative Models","abstract":"Machine unlearning has emerged as a new paradigm to deliberately forget data samples from a given model in order to adhere to stringent regulations. However, existing machine unlearning methods have been primarily focused on classification models, leaving the landscape of unlearning for generative models relatively unexplored. This paper serves as a bridge, addressing the gap by providing a unifying framework of machine unlearning for image-to-image generative models. Within this framework, we propose a computationally-efficient algorithm, underpinned by rigorous theoretical analysis, that demonstrates negligible performance degradation on the retain samples, while effectively removing the information from the forget samples. Empirical studies on two large-scale datasets, ImageNet-1K and Places-365, further show that our algorithm does not rely on the availability of the retain samples, which further complies with data retention policy. To our best knowledge, this work is the first that represents systemic, theoretical, empirical explorations of machine unlearning specifically tailored for image-to-image generative models. Our code is available at https://github.com/jpmorganchase/l2l-generator-unlearning.","sentences":["Machine unlearning has emerged as a new paradigm to deliberately forget data samples from a given model in order to adhere to stringent regulations.","However, existing machine unlearning methods have been primarily focused on classification models, leaving the landscape of unlearning for generative models relatively unexplored.","This paper serves as a bridge, addressing the gap by providing a unifying framework of machine unlearning for image-to-image generative models.","Within this framework, we propose a computationally-efficient algorithm, underpinned by rigorous theoretical analysis, that demonstrates negligible performance degradation on the retain samples, while effectively removing the information from the forget samples.","Empirical studies on two large-scale datasets, ImageNet-1K and Places-365, further show that our algorithm does not rely on the availability of the retain samples, which further complies with data retention policy.","To our best knowledge, this work is the first that represents systemic, theoretical, empirical explorations of machine unlearning specifically tailored for image-to-image generative models.","Our code is available at https://github.com/jpmorganchase/l2l-generator-unlearning."],"url":"http://arxiv.org/abs/2402.00351v1","category":"cs.LG"}
{"created":"2024-02-01 05:31:17","title":"Control of strongly interacting quantum many-body states with an effective mean-field approach","abstract":"Shortcuts to adiabaticity (STA) are powerful tools that can be used to control quantum systems with high fidelity. They work particularly well for single particle and non-interacting systems which can be described exactly and which possess invariant or self-similar dynamics. However, finding an exact STA for strongly correlated many-body systems is difficult, as their complex dynamics cannot be easily described, especially for larger systems. Here, we outline a method to design STAs for these systems using a mean-field approach that succinctly captures the strong interaction effects through a quartic nonlinear term. We show that for the case of the harmonic oscillator with a time-dependent trap frequency the mean-field approach works exactly and recovers the well-known STA from literature. To highlight the robustness of our approach we also show that it works effectively for anharmonic potentials, achieving higher fidelities than other typical control techniques.","sentences":["Shortcuts to adiabaticity (STA) are powerful tools that can be used to control quantum systems with high fidelity.","They work particularly well for single particle and non-interacting systems which can be described exactly and which possess invariant or self-similar dynamics.","However, finding an exact STA for strongly correlated many-body systems is difficult, as their complex dynamics cannot be easily described, especially for larger systems.","Here, we outline a method to design STAs for these systems using a mean-field approach that succinctly captures the strong interaction effects through a quartic nonlinear term.","We show that for the case of the harmonic oscillator with a time-dependent trap frequency the mean-field approach works exactly and recovers the well-known STA from literature.","To highlight the robustness of our approach we also show that it works effectively for anharmonic potentials, achieving higher fidelities than other typical control techniques."],"url":"http://arxiv.org/abs/2402.00349v1","category":"quant-ph"}
{"created":"2024-02-01 05:25:22","title":"Experimental Application of Predictive Cost Adaptive Control to Thermoacoustic Oscillations in a Rijke Tube","abstract":"Model predictive control (MPC) has been used successfully in diverse applications. As its name suggests, MPC requires a model for predictive optimization. The present paper focuses on the application of MPC to a Rijke tube, in which a heating source and acoustic dynamics interact to produce self-excited oscillations. Since the dynamics of a Rijke tube are difficult to model to a high level of accuracy, the implementation of MPC requires leveraging data from the physical setup as well as knowledge about thermoacoustics, which is labor intensive and requires domain expertise. With this motivation, the present paper uses predictive cost adaptive control (PCAC) for sampled-data control of an experimental Rijke-tube setup. PCAC performs online closed-loop linear model identification for receding-horizon optimization based on the backward propagating Riccati equation. In place of analytical modeling, open-loop experiments are used to create a simple emulation model, which is used for choosing PCAC hyperparameters. PCAC is applied to the Rijke-tube setup under various experimental scenarios.","sentences":["Model predictive control (MPC) has been used successfully in diverse applications.","As its name suggests, MPC requires a model for predictive optimization.","The present paper focuses on the application of MPC to a Rijke tube, in which a heating source and acoustic dynamics interact to produce self-excited oscillations.","Since the dynamics of a Rijke tube are difficult to model to a high level of accuracy, the implementation of MPC requires leveraging data from the physical setup as well as knowledge about thermoacoustics, which is labor intensive and requires domain expertise.","With this motivation, the present paper uses predictive cost adaptive control (PCAC) for sampled-data control of an experimental Rijke-tube setup.","PCAC performs online closed-loop linear model identification for receding-horizon optimization based on the backward propagating Riccati equation.","In place of analytical modeling, open-loop experiments are used to create a simple emulation model, which is used for choosing PCAC hyperparameters.","PCAC is applied to the Rijke-tube setup under various experimental scenarios."],"url":"http://arxiv.org/abs/2402.00346v1","category":"eess.SY"}
{"created":"2024-02-01 05:19:15","title":"Reimagining TaxiVis through an Immersive Space-Time Cube metaphor and reflecting on potential benefits of Immersive Analytics for urban data exploration","abstract":"Current visualization research has identified the potential of more immersive settings for data exploration, leveraging VR and AR technologies. To explore how a traditional visualization system could be adapted into an immersive framework, and how it could benefit from this, we decided to revisit a landmark paper presented ten years ago at IEEE VIS. TaxiVis, by Ferreira et al., enabled interactive spatio-temporal querying of a large dataset of taxi trips in New York City. Here, we reimagine how TaxiVis' functionalities could be implemented and extended in a 3D immersive environment. Among the unique features we identify as being enabled by the Immersive TaxiVis prototype are alternative uses of the additional visual dimension, a fully visual 3D spatio-temporal query framework, and the opportunity to explore the data at different scales and frames of reference. By revisiting the case studies from the original paper, we demonstrate workflows that can benefit from this immersive perspective. Through reporting on our experience, and on the vision and reasoning behind our design decisions, we hope to contribute to the debate on how conventional and immersive visualization paradigms can complement each other and on how the exploration of urban datasets can be facilitated in the coming years.","sentences":["Current visualization research has identified the potential of more immersive settings for data exploration, leveraging VR and AR technologies.","To explore how a traditional visualization system could be adapted into an immersive framework, and how it could benefit from this, we decided to revisit a landmark paper presented ten years ago at IEEE VIS.","TaxiVis, by Ferreira et al., enabled interactive spatio-temporal querying of a large dataset of taxi trips in New York City.","Here, we reimagine how TaxiVis' functionalities could be implemented and extended in a 3D immersive environment.","Among the unique features we identify as being enabled by the Immersive TaxiVis prototype are alternative uses of the additional visual dimension, a fully visual 3D spatio-temporal query framework, and the opportunity to explore the data at different scales and frames of reference.","By revisiting the case studies from the original paper, we demonstrate workflows that can benefit from this immersive perspective.","Through reporting on our experience, and on the vision and reasoning behind our design decisions, we hope to contribute to the debate on how conventional and immersive visualization paradigms can complement each other and on how the exploration of urban datasets can be facilitated in the coming years."],"url":"http://arxiv.org/abs/2402.00344v1","category":"cs.HC"}
{"created":"2024-02-01 05:19:12","title":"Enhancement of the catalytic activity upon the surface of strongly disordered hollow Pt nanoparticles: a First Principles investigation","abstract":"A series of First Principles calculations is undertaken to characterize and explain the enhancement of the catalytic activity of oxygen on top of very disordered nanomaterials of Pt. As the adsorption of OH fragment on top of the surfaces is known as the limiting factor in the Oxygen Reduction Reactions (ORR) process in these systems, our calculations propose to determine the influence of the local geometry of the various sites on the adsorption energy of OH in order to discover a simple descriptor allowing to predict the reactivity at these surfaces as a function of their morphology and strain. For this purpose, the geometry of Pt slabs with various thickness (3, 5 and 7 atomic layers) including a large number of point defects are optimized in order to generate a very rich catalog of inequivalent sites of reactivity on both surfaces of the slabs. Given the very large distortion of the geometry of the sites, these latter had to be categorized into several classes for which the behavior with respect to catalytic activity is determined. A new descriptor taking into account the distortion of the geometry of the sites is introduced, allowing to recover the linear dependence of the adsorption energy of OH with respect to the effective coordination number of the sites, as observed in highly symmetric and planar surfaces of Pt.","sentences":["A series of First Principles calculations is undertaken to characterize and explain the enhancement of the catalytic activity of oxygen on top of very disordered nanomaterials of Pt.","As the adsorption of OH fragment on top of the surfaces is known as the limiting factor in the Oxygen Reduction Reactions (ORR) process in these systems, our calculations propose to determine the influence of the local geometry of the various sites on the adsorption energy of OH in order to discover a simple descriptor allowing to predict the reactivity at these surfaces as a function of their morphology and strain.","For this purpose, the geometry of Pt slabs with various thickness (3, 5 and 7 atomic layers) including a large number of point defects are optimized in order to generate a very rich catalog of inequivalent sites of reactivity on both surfaces of the slabs.","Given the very large distortion of the geometry of the sites, these latter had to be categorized into several classes for which the behavior with respect to catalytic activity is determined.","A new descriptor taking into account the distortion of the geometry of the sites is introduced, allowing to recover the linear dependence of the adsorption energy of OH with respect to the effective coordination number of the sites, as observed in highly symmetric and planar surfaces of Pt."],"url":"http://arxiv.org/abs/2402.00343v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-01 04:39:25","title":"Regression-Based Proximal Causal Inference","abstract":"In observational studies, identification of causal effects is threatened by the potential for unmeasured confounding. Negative controls have become widely used to evaluate the presence of potential unmeasured confounding thus enhancing credibility of reported causal effect estimates. Going beyond simply testing for residual confounding, proximal causal inference (PCI) was recently developed to debias causal effect estimates subject to confounding by hidden factors, by leveraging a pair of negative control variables, also known as treatment and outcome confounding proxies. While formal statistical inference has been developed for PCI, these methods can be challenging to implement in practice as they involve solving complex integral equations that are typically ill-posed. In this paper, we develop a regression-based PCI approach, employing a two-stage regression via familiar generalized linear models to implement the PCI framework, which completely obviates the need to solve difficult integral equations. In the first stage, one fits a generalized linear model (GLM) for the outcome confounding proxy in terms of the treatment confounding proxy and the primary treatment. In the second stage, one fits a GLM for the primary outcome in terms of the primary treatment, using the predicted value of the first-stage regression model as a regressor which as we establish accounts for any residual confounding for which the proxies are relevant. The proposed approach has merit in that (i) it is applicable to continuous, count, and binary outcomes cases, making it relevant to a wide range of real-world applications, and (ii) it is easy to implement using off-the-shelf software for GLMs. We establish the statistical properties of regression-based PCI and illustrate their performance in both synthetic and real-world empirical applications.","sentences":["In observational studies, identification of causal effects is threatened by the potential for unmeasured confounding.","Negative controls have become widely used to evaluate the presence of potential unmeasured confounding thus enhancing credibility of reported causal effect estimates.","Going beyond simply testing for residual confounding, proximal causal inference (PCI) was recently developed to debias causal effect estimates subject to confounding by hidden factors, by leveraging a pair of negative control variables, also known as treatment and outcome confounding proxies.","While formal statistical inference has been developed for PCI, these methods can be challenging to implement in practice as they involve solving complex integral equations that are typically ill-posed.","In this paper, we develop a regression-based PCI approach, employing a two-stage regression via familiar generalized linear models to implement the PCI framework, which completely obviates the need to solve difficult integral equations.","In the first stage, one fits a generalized linear model (GLM) for the outcome confounding proxy in terms of the treatment confounding proxy and the primary treatment.","In the second stage, one fits a GLM for the primary outcome in terms of the primary treatment, using the predicted value of the first-stage regression model as a regressor which as we establish accounts for any residual confounding for which the proxies are relevant.","The proposed approach has merit in that (i) it is applicable to continuous, count, and binary outcomes cases, making it relevant to a wide range of real-world applications, and (ii) it is easy to implement using off-the-shelf software for GLMs.","We establish the statistical properties of regression-based PCI and illustrate their performance in both synthetic and real-world empirical applications."],"url":"http://arxiv.org/abs/2402.00335v1","category":"stat.ME"}
{"created":"2024-02-01 04:23:32","title":"Night-Rider: Nocturnal Vision-aided Localization in Streetlight Maps Using Invariant Extended Kalman Filtering","abstract":"Vision-aided localization for low-cost mobile robots in diverse environments has attracted widespread attention recently. Although many current systems are applicable in daytime environments, nocturnal visual localization is still an open problem owing to the lack of stable visual information. An insight from most nocturnal scenes is that the static and bright streetlights are reliable visual information for localization. Hence we propose a nocturnal vision-aided localization system in streetlight maps with a novel data association and matching scheme using object detection methods. We leverage the Invariant Extended Kalman Filter (InEKF) to fuse IMU, odometer, and camera measurements for consistent state estimation at night. Furthermore, a tracking recovery module is also designed for tracking failures. Experiments on multiple real nighttime scenes validate that the system can achieve remarkably accurate and robust localization in nocturnal environments.","sentences":["Vision-aided localization for low-cost mobile robots in diverse environments has attracted widespread attention recently.","Although many current systems are applicable in daytime environments, nocturnal visual localization is still an open problem owing to the lack of stable visual information.","An insight from most nocturnal scenes is that the static and bright streetlights are reliable visual information for localization.","Hence we propose a nocturnal vision-aided localization system in streetlight maps with a novel data association and matching scheme using object detection methods.","We leverage the Invariant Extended Kalman Filter (InEKF) to fuse IMU, odometer, and camera measurements for consistent state estimation at night.","Furthermore, a tracking recovery module is also designed for tracking failures.","Experiments on multiple real nighttime scenes validate that the system can achieve remarkably accurate and robust localization in nocturnal environments."],"url":"http://arxiv.org/abs/2402.00330v1","category":"cs.RO"}
{"created":"2024-02-01 04:17:56","title":"PirateNets: Physics-informed Deep Learning with Residual Adaptive Networks","abstract":"While physics-informed neural networks (PINNs) have become a popular deep learning framework for tackling forward and inverse problems governed by partial differential equations (PDEs), their performance is known to degrade when larger and deeper neural network architectures are employed. Our study identifies that the root of this counter-intuitive behavior lies in the use of multi-layer perceptron (MLP) architectures with non-suitable initialization schemes, which result in poor trainablity for the network derivatives, and ultimately lead to an unstable minimization of the PDE residual loss. To address this, we introduce Physics-informed Residual Adaptive Networks (PirateNets), a novel architecture that is designed to facilitate stable and efficient training of deep PINN models. PirateNets leverage a novel adaptive residual connection, which allows the networks to be initialized as shallow networks that progressively deepen during training. We also show that the proposed initialization scheme allows us to encode appropriate inductive biases corresponding to a given PDE system into the network architecture. We provide comprehensive empirical evidence showing that PirateNets are easier to optimize and can gain accuracy from considerably increased depth, ultimately achieving state-of-the-art results across various benchmarks. All code and data accompanying this manuscript will be made publicly available at \\url{https://github.com/PredictiveIntelligenceLab/jaxpi}.","sentences":["While physics-informed neural networks (PINNs) have become a popular deep learning framework for tackling forward and inverse problems governed by partial differential equations (PDEs), their performance is known to degrade when larger and deeper neural network architectures are employed.","Our study identifies that the root of this counter-intuitive behavior lies in the use of multi-layer perceptron (MLP) architectures with non-suitable initialization schemes, which result in poor trainablity for the network derivatives, and ultimately lead to an unstable minimization of the PDE residual loss.","To address this, we introduce Physics-informed Residual Adaptive Networks (PirateNets), a novel architecture that is designed to facilitate stable and efficient training of deep PINN models.","PirateNets leverage a novel adaptive residual connection, which allows the networks to be initialized as shallow networks that progressively deepen during training.","We also show that the proposed initialization scheme allows us to encode appropriate inductive biases corresponding to a given PDE system into the network architecture.","We provide comprehensive empirical evidence showing that PirateNets are easier to optimize and can gain accuracy from considerably increased depth, ultimately achieving state-of-the-art results across various benchmarks.","All code and data accompanying this manuscript will be made publicly available at \\url{https://github.com/PredictiveIntelligenceLab/jaxpi}."],"url":"http://arxiv.org/abs/2402.00326v1","category":"cs.LG"}
{"created":"2024-02-01 04:17:31","title":"How digital twins provide new opportunities for managing change in complex projects","abstract":"Digital twins provide opportunities for the development of new techniques for managing change in complex projects, such as infrastructure, new energy and resource projects. Managing change is an important element of a systems approach to infrastructure, and for ensuring the systems integration and handover to asset owners, yet it is often done poorly, becoming a source of errors that lead projects to be over budget and schedule. We set out to articulate the opportunities provided by the digital twin for advanced methodologies for managing change in complex projects, through an under-pinning state of the art review of the existing technical literature and a small pilot to identify the characteristics of future data-driven solutions. This identifies the need to integrate work on identifying systems interfaces, change propagation and visualisation, and the potential to significantly extend the limitations of existing solutions by using developments in the digital twin, such as linked data, semantic enrichment, network analyses and machine learning.","sentences":["Digital twins provide opportunities for the development of new techniques for managing change in complex projects, such as infrastructure, new energy and resource projects.","Managing change is an important element of a systems approach to infrastructure, and for ensuring the systems integration and handover to asset owners, yet it is often done poorly, becoming a source of errors that lead projects to be over budget and schedule.","We set out to articulate the opportunities provided by the digital twin for advanced methodologies for managing change in complex projects, through an under-pinning state of the art review of the existing technical literature and a small pilot to identify the characteristics of future data-driven solutions.","This identifies the need to integrate work on identifying systems interfaces, change propagation and visualisation, and the potential to significantly extend the limitations of existing solutions by using developments in the digital twin, such as linked data, semantic enrichment, network analyses and machine learning."],"url":"http://arxiv.org/abs/2402.00325v1","category":"eess.SY"}
{"created":"2024-02-01 03:53:56","title":"Control in Stochastic Environment with Delays: A Model-based Reinforcement Learning Approach","abstract":"In this paper we are introducing a new reinforcement learning method for control problems in environments with delayed feedback. Specifically, our method employs stochastic planning, versus previous methods that used deterministic planning. This allows us to embed risk preference in the policy optimization problem. We show that this formulation can recover the optimal policy for problems with deterministic transitions. We contrast our policy with two prior methods from literature. We apply the methodology to simple tasks to understand its features. Then, we compare the performance of the methods in controlling multiple Atari games.","sentences":["In this paper we are introducing a new reinforcement learning method for control problems in environments with delayed feedback.","Specifically, our method employs stochastic planning, versus previous methods that used deterministic planning.","This allows us to embed risk preference in the policy optimization problem.","We show that this formulation can recover the optimal policy for problems with deterministic transitions.","We contrast our policy with two prior methods from literature.","We apply the methodology to simple tasks to understand its features.","Then, we compare the performance of the methods in controlling multiple Atari games."],"url":"http://arxiv.org/abs/2402.00313v1","category":"cs.LG"}
{"created":"2024-02-01 03:46:11","title":"An Exam-based Evaluation Approach Beyond Traditional Relevance Judgments","abstract":"Current IR evaluation is based on relevance judgments, created either manually or automatically, with decisions outsourced to Large Language Models (LLMs). We offer an alternative paradigm, that never relies on relevance judgments in any form. Instead, a text is defined as relevant if it contains information that enables the answering of key questions. We use this idea to design the EXAM Answerability Metric to evaluate information retrieval/generation systems for their ability to provide topically relevant information.   We envision the role of a human judge to edit and define an exam question bank that will test for the presence of relevant information in text. We support this step by generating an initial set of exam questions. In the next phase, an LLM-based question answering system will automatically grade system responses by tracking which exam questions are answerable with which system responses. We propose two evaluation measures, the recall-oriented EXAM Cover metric, and the precision-oriented EXAM Qrels metric, the latter which can be implemented with trec_eval. This paradigm not only allows for the expansion of the exam question set post-hoc but also facilitates the ongoing evaluation of future information systems, whether they focus on retrieval, generation, or both.","sentences":["Current IR evaluation is based on relevance judgments, created either manually or automatically, with decisions outsourced to Large Language Models (LLMs).","We offer an alternative paradigm, that never relies on relevance judgments in any form.","Instead, a text is defined as relevant if it contains information that enables the answering of key questions.","We use this idea to design the EXAM Answerability Metric to evaluate information retrieval/generation systems for their ability to provide topically relevant information.   ","We envision the role of a human judge to edit and define an exam question bank that will test for the presence of relevant information in text.","We support this step by generating an initial set of exam questions.","In the next phase, an LLM-based question answering system will automatically grade system responses by tracking which exam questions are answerable with which system responses.","We propose two evaluation measures, the recall-oriented EXAM Cover metric, and the precision-oriented EXAM Qrels metric, the latter which can be implemented with trec_eval.","This paradigm not only allows for the expansion of the exam question set post-hoc but also facilitates the ongoing evaluation of future information systems, whether they focus on retrieval, generation, or both."],"url":"http://arxiv.org/abs/2402.00309v1","category":"cs.IR"}
{"created":"2024-02-01 03:34:41","title":"Ab-Initio Calculations of Nonlinear Susceptibility and Multi-Phonon Mixing Processes in a 2DEG-Piezoelectric Heterostructure","abstract":"Solid-state elastic-wave phonons are a promising platform for a wide range of quantum information applications. An outstanding challenge and enabling capability in harnessing phonons for quantum information processing is achieving strong nonlinear interactions between them. To this end, we propose a general architecture using piezoelectric-semiconductor heterostructures consisting of a piezoelectric acoustic material hosting phonon modes in direct proximity to a two-dimensional electron gas (2DEG). Each phonon in the piezoelectric material carries an electric field, which extends into the 2DEG. The fields induce polarization of 2DEG electrons, which in turn interact with other piezoelectric phononic electric fields. The net result is coupling between the various phonon modes. We derive, from first principles, the nonlinear phononic susceptibility of the system. We show that many nonlinear processes are strongly favored at high electron mobility, motivating the use of the 2DEG to mediate the nonlinearities. We derive in detail the first, second, and third-order susceptibilities and calculate them for the case of a lithium niobate surface acoustic wave interacting with a GaAs-AlGaAs heterostructure 2DEG. We show that, for this system, the strong third-order nonlinearity could enable single-phonon Kerr shift in an acoustic cavity that exceeds realistic cavity linewidths, potentially leading to a new class of acoustic qubit. We further show that the strong second-order nonlinearity could be used to produce a high-gain, traveling-wave parametric amplifier to amplify--and ultimately detect--the outputs of the acoustic cavity qubits. Assuming favorable losses in such a system, these capabilities, combined with the ability to efficiently transduce phonons from microwave electromagnetic fields in transmission lines, thus hold promise for creating all-acoustic quantum information processors.","sentences":["Solid-state elastic-wave phonons are a promising platform for a wide range of quantum information applications.","An outstanding challenge and enabling capability in harnessing phonons for quantum information processing is achieving strong nonlinear interactions between them.","To this end, we propose a general architecture using piezoelectric-semiconductor heterostructures consisting of a piezoelectric acoustic material hosting phonon modes in direct proximity to a two-dimensional electron gas (2DEG).","Each phonon in the piezoelectric material carries an electric field, which extends into the 2DEG.","The fields induce polarization of 2DEG electrons, which in turn interact with other piezoelectric phononic electric fields.","The net result is coupling between the various phonon modes.","We derive, from first principles, the nonlinear phononic susceptibility of the system.","We show that many nonlinear processes are strongly favored at high electron mobility, motivating the use of the 2DEG to mediate the nonlinearities.","We derive in detail the first, second, and third-order susceptibilities and calculate them for the case of a lithium niobate surface acoustic wave interacting with a GaAs-AlGaAs heterostructure 2DEG.","We show that, for this system, the strong third-order nonlinearity could enable single-phonon Kerr shift in an acoustic cavity that exceeds realistic cavity linewidths, potentially leading to a new class of acoustic qubit.","We further show that the strong second-order nonlinearity could be used to produce a high-gain, traveling-wave parametric amplifier to amplify--and ultimately detect--the outputs of the acoustic cavity qubits.","Assuming favorable losses in such a system, these capabilities, combined with the ability to efficiently transduce phonons from microwave electromagnetic fields in transmission lines, thus hold promise for creating all-acoustic quantum information processors."],"url":"http://arxiv.org/abs/2402.00303v1","category":"physics.app-ph"}
{"created":"2024-02-01 03:33:19","title":"Inertia and slip effects on the instability of a liquid film coated on a fibre","abstract":"To investigate the influence of inertia and slip on the instability of a liquid film on a fibre, a theoretical framework based on the axisymmetric Navier-Stokes equations is proposed via linear instability analysis. The model reveals that slip significantly enhances perturbation growth in viscous film flows, whereas it exerts minimal influence on flows dominated by inertia. Moreover, under no-slip boundary conditions, the dominant instability mode of thin films remains unaltered by inertia, closely aligning with predictions from a no-slip lubrication model. Conversely, when slip is introduced, the dominant wavenumber experiences a noticeable reduction as inertia decreases. This trend is captured by an introduced lubrication model with giant slip. Direct numerical simulations of the Navier-Stokes equations are then performed to further confirm the theoretical findings at the linear stage. For the nonlinear dynamics, no-slip simulations show complex vortical structures within films, driven by fluid inertia near surfaces. Additionally, in scenarios with weak inertia, a reduction in the volume of satellite droplets is observed due to slip, following a power-law relationship.","sentences":["To investigate the influence of inertia and slip on the instability of a liquid film on a fibre, a theoretical framework based on the axisymmetric Navier-Stokes equations is proposed via linear instability analysis.","The model reveals that slip significantly enhances perturbation growth in viscous film flows, whereas it exerts minimal influence on flows dominated by inertia.","Moreover, under no-slip boundary conditions, the dominant instability mode of thin films remains unaltered by inertia, closely aligning with predictions from a no-slip lubrication model.","Conversely, when slip is introduced, the dominant wavenumber experiences a noticeable reduction as inertia decreases.","This trend is captured by an introduced lubrication model with giant slip.","Direct numerical simulations of the Navier-Stokes equations are then performed to further confirm the theoretical findings at the linear stage.","For the nonlinear dynamics, no-slip simulations show complex vortical structures within films, driven by fluid inertia near surfaces.","Additionally, in scenarios with weak inertia, a reduction in the volume of satellite droplets is observed due to slip, following a power-law relationship."],"url":"http://arxiv.org/abs/2402.00302v1","category":"physics.flu-dyn"}
{"created":"2024-02-01 03:18:45","title":"Oracle separation of QMA and QCMA with bounded adaptivity","abstract":"We give an oracle separation between QMA and QCMA for quantum algorithms that have bounded adaptivity in their oracle queries; that is, the number of rounds of oracle calls is small, though each round may involve polynomially many queries in parallel. Our oracle construction is a simplified version of the construction used recently by Li, Liu, Pelecanos, and Yamakawa (2023), who showed an oracle separation between QMA and QCMA when the quantum algorithms are only allowed to access the oracle classically. To prove our results, we introduce a property of relations called \\emph{slipperiness}, which may be useful for getting a fully general classical oracle separation between QMA and QCMA.","sentences":["We give an oracle separation between QMA and QCMA for quantum algorithms that have bounded adaptivity in their oracle queries; that is, the number of rounds of oracle calls is small, though each round may involve polynomially many queries in parallel.","Our oracle construction is a simplified version of the construction used recently by Li, Liu, Pelecanos, and Yamakawa (2023), who showed an oracle separation between QMA and QCMA when the quantum algorithms are only allowed to access the oracle classically.","To prove our results, we introduce a property of relations called \\emph{slipperiness}, which may be useful for getting a fully general classical oracle separation between QMA and QCMA."],"url":"http://arxiv.org/abs/2402.00298v1","category":"quant-ph"}
{"created":"2024-02-01 03:02:12","title":"Exploring inverse orbital Hall and orbital Rashba effects: unveiling the oxidation states of the Cu surface","abstract":"In this work, employing spin-pumping techniques driven by both ferromagnetic resonance (SP-FMR) and longitudinal spin Seebeck effect (LSSE) to manipulate and direct observe orbital currents, we investigated the volume conversion of spin-orbital currents into charge-current in YIG(100nm)/Pt(2nm)/NM2 structures, where NM2 represents Ti or Ru. While the YIG/Ti bilayer displayed a negligible SP-FMR signal, the YIG/Pt/Ti structure exhibited a significantly stronger signal attributed to the orbital Hall effect of Ti. Substituting the Ti layer with Ru revealed a similar phenomenon, wherein the effect is ascribed to the combined action of both spin and orbital Hall effects. Furthermore, we measured the SP-FMR signal in the YIG/Pt(2)/Ru(6)/Ti(6) and YIG/Pt(2)/Ti(6)/Ru(6) heterostructures by just altering the stack order of Ti and Ru layers, where the peak value of the spin pumping signal is larger for the first sample. To verify the influence on the oxidation of Ti and Ru films, we studied a series of thin films subjected to controlled and natural oxidation. As Cu and CuOx is a system that is already known to be highly influenced by oxidation, this metal was chosen to carry out this study. We investigated these samples using SP-FMR in YIG/Pt(2)/CuOx(tCu) and X-ray absorption spectroscopy and concluded that samples with natural oxidation of Cu exhibit more significant results than those when the CuOx is obtained by reactive sputtering. In particular, samples where the Cu layer is naturally oxidized exhibit a Cu2O-rich phase. Our findings help to elucidate the mechanisms underlying the inverse orbital Hall and inverse orbital Rashba-Edelstein-like effects. These insights indeed contribute to the advancement of devices that rely on orbital-charge conversion.","sentences":["In this work, employing spin-pumping techniques driven by both ferromagnetic resonance (SP-FMR) and longitudinal spin Seebeck effect (LSSE) to manipulate and direct observe orbital currents, we investigated the volume conversion of spin-orbital currents into charge-current in YIG(100nm)/Pt(2nm)/NM2 structures, where NM2 represents Ti or Ru.","While the YIG/Ti bilayer displayed a negligible SP-FMR signal, the YIG/Pt/Ti structure exhibited a significantly stronger signal attributed to the orbital Hall effect of Ti.","Substituting the Ti layer with Ru revealed a similar phenomenon, wherein the effect is ascribed to the combined action of both spin and orbital Hall effects.","Furthermore, we measured the SP-FMR signal in the YIG/Pt(2)/Ru(6)/Ti(6) and YIG/Pt(2)/Ti(6)/Ru(6) heterostructures by just altering the stack order of Ti and Ru layers, where the peak value of the spin pumping signal is larger for the first sample.","To verify the influence on the oxidation of Ti and Ru films, we studied a series of thin films subjected to controlled and natural oxidation.","As Cu and CuOx is a system that is already known to be highly influenced by oxidation, this metal was chosen to carry out this study.","We investigated these samples using SP-FMR in YIG/Pt(2)/CuOx(tCu) and X-ray absorption spectroscopy and concluded that samples with natural oxidation of Cu exhibit more significant results than those when the CuOx is obtained by reactive sputtering.","In particular, samples where the Cu layer is naturally oxidized exhibit a Cu2O-rich phase.","Our findings help to elucidate the mechanisms underlying the inverse orbital Hall and inverse orbital Rashba-Edelstein-like effects.","These insights indeed contribute to the advancement of devices that rely on orbital-charge conversion."],"url":"http://arxiv.org/abs/2402.00297v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-01 03:00:13","title":"High-Level, Collaborative Task Planning Grammar and Execution for Heterogeneous Agents","abstract":"We propose a new multi-agent task grammar to encode collaborative tasks for a team of heterogeneous agents that can have overlapping capabilities. The grammar allows users to specify the relationship between agents and parts of the task without providing explicit assignments or constraints on the number of agents required. We develop a method to automatically find a team of agents and synthesize correct-by-construction control with synchronization policies to satisfy the task. We demonstrate the scalability of our approach through simulation and compare our method to existing task grammars that encode multi-agent tasks.","sentences":["We propose a new multi-agent task grammar to encode collaborative tasks for a team of heterogeneous agents that can have overlapping capabilities.","The grammar allows users to specify the relationship between agents and parts of the task without providing explicit assignments or constraints on the number of agents required.","We develop a method to automatically find a team of agents and synthesize correct-by-construction control with synchronization policies to satisfy the task.","We demonstrate the scalability of our approach through simulation and compare our method to existing task grammars that encode multi-agent tasks."],"url":"http://arxiv.org/abs/2402.00296v1","category":"cs.RO"}
{"created":"2024-02-01 02:54:49","title":"Comparative Evaluation of Traditional and Deep Learning-Based Segmentation Methods for Spoil Pile Delineation Using UAV Images","abstract":"The stability of mine dumps is contingent upon the precise arrangement of spoil piles, taking into account their geological and geotechnical attributes. Yet, on-site characterisation of individual piles poses a formidable challenge. The utilisation of image-based techniques for spoil pile characterisation, employing remotely acquired data through unmanned aerial systems, is a promising complementary solution. Image processing, such as object-based classification and feature extraction, are dependent upon effective segmentation. This study refines and juxtaposes various segmentation approaches, specifically colour-based and morphology-based techniques. The objective is to enhance and evaluate avenues for object-based analysis for spoil characterisation within the context of mining environments. Furthermore, a comparative analysis is conducted between conventional segmentation approaches and those rooted in deep learning methodologies. Among the diverse segmentation approaches evaluated, the morphology-based deep learning segmentation approach, Segment Anything Model (SAM), exhibited superior performance in comparison to other approaches. This outcome underscores the efficacy of incorporating advanced morphological and deep learning techniques for accurate and efficient spoil pile characterisation. The findings of this study contribute valuable insights to the optimisation of segmentation strategies, thereby advancing the application of image-based techniques for the characterisation of spoil piles in mining environments.","sentences":["The stability of mine dumps is contingent upon the precise arrangement of spoil piles, taking into account their geological and geotechnical attributes.","Yet, on-site characterisation of individual piles poses a formidable challenge.","The utilisation of image-based techniques for spoil pile characterisation, employing remotely acquired data through unmanned aerial systems, is a promising complementary solution.","Image processing, such as object-based classification and feature extraction, are dependent upon effective segmentation.","This study refines and juxtaposes various segmentation approaches, specifically colour-based and morphology-based techniques.","The objective is to enhance and evaluate avenues for object-based analysis for spoil characterisation within the context of mining environments.","Furthermore, a comparative analysis is conducted between conventional segmentation approaches and those rooted in deep learning methodologies.","Among the diverse segmentation approaches evaluated, the morphology-based deep learning segmentation approach, Segment Anything Model (SAM), exhibited superior performance in comparison to other approaches.","This outcome underscores the efficacy of incorporating advanced morphological and deep learning techniques for accurate and efficient spoil pile characterisation.","The findings of this study contribute valuable insights to the optimisation of segmentation strategies, thereby advancing the application of image-based techniques for the characterisation of spoil piles in mining environments."],"url":"http://arxiv.org/abs/2402.00295v1","category":"cs.CV"}
{"created":"2024-02-01 02:50:41","title":"Explicit arithmetic Eisenstein cocycles: the toric case","abstract":"We construct cocycles for $GL_n(\\mathbb{Q})$ valued in cup products of units on the $n$-dimensional algebraic torus $\\mathbb{G}_m^n$ over an arbitrary DVR, viewed in Milnor $K$-theory or motivic cohomology. We show how various combinatorially-defined complexes encoding linear algebraic data capture the structure of these cup products inside motivic complexes, and examine some properties of the resulting classes. This generalizes work of Sharifi and Venkatesh in the case $n=2$, in which case pullbacks of the resulting cocycle to an arithmetic base are of central importance in the Sharifi conjectures.","sentences":["We construct cocycles for $GL_n(\\mathbb{Q})$ valued in cup products of units on the $n$-dimensional algebraic torus $\\mathbb{G}_m^n$ over an arbitrary DVR, viewed in Milnor $K$-theory or motivic cohomology.","We show how various combinatorially-defined complexes encoding linear algebraic data capture the structure of these cup products inside motivic complexes, and examine some properties of the resulting classes.","This generalizes work of Sharifi and Venkatesh in the case $n=2$, in which case pullbacks of the resulting cocycle to an arithmetic base are of central importance in the Sharifi conjectures."],"url":"http://arxiv.org/abs/2402.00294v1","category":"math.NT"}
{"created":"2024-02-01 02:46:24","title":"Effective Bug Detection in Graph Database Engines: An LLM-based Approach","abstract":"Graph database engines play a pivotal role in efficiently storing and managing graph data across various domains, including bioinformatics, knowledge graphs, and recommender systems. Ensuring data accuracy within graph database engines is paramount, as inaccuracies can yield unreliable analytical outcomes. Current bug-detection approaches are confined to specific graph query languages, limiting their applicabilities when handling graph database engines that use various graph query languages across various domains. Moreover, they require extensive prior knowledge to generate queries for detecting bugs. To address these challenges, we introduces DGDB, a novel paradigm harnessing large language models(LLM), such as ChatGPT, for comprehensive bug detection in graph database engines. DGDB leverages ChatGPT to generate high-quality queries for different graph query languages. It subsequently employs differential testing to identify bugs in graph database engines. We applied this paradigm to graph database engines using the Gremlin query language and those using the Cypher query language, generating approximately 4,000 queries each. In the latest versions of Neo4j, Agensgraph, and JanusGraph databases, we detected 2, 5, and 3 wrong-result bugs, respectively.","sentences":["Graph database engines play a pivotal role in efficiently storing and managing graph data across various domains, including bioinformatics, knowledge graphs, and recommender systems.","Ensuring data accuracy within graph database engines is paramount, as inaccuracies can yield unreliable analytical outcomes.","Current bug-detection approaches are confined to specific graph query languages, limiting their applicabilities when handling graph database engines that use various graph query languages across various domains.","Moreover, they require extensive prior knowledge to generate queries for detecting bugs.","To address these challenges, we introduces DGDB, a novel paradigm harnessing large language models(LLM), such as ChatGPT, for comprehensive bug detection in graph database engines.","DGDB leverages ChatGPT to generate high-quality queries for different graph query languages.","It subsequently employs differential testing to identify bugs in graph database engines.","We applied this paradigm to graph database engines using the Gremlin query language and those using the Cypher query language, generating approximately 4,000 queries each.","In the latest versions of Neo4j, Agensgraph, and JanusGraph databases, we detected 2, 5, and 3 wrong-result bugs, respectively."],"url":"http://arxiv.org/abs/2402.00292v1","category":"cs.DB"}
{"created":"2024-02-01 02:45:17","title":"A link between anomalous viscous loss and boson peak in soft jammed solids","abstract":"Soft jammed solids exhibit intriguing mechanical properties, while their linear response is elusive. In particular, foams and emulsions generally reveal anomalous viscous loss with the loss and storage modulus following $G^{\\prime \\prime} \\propto \\sqrt{\\omega}$ and $G^{\\prime} \\propto \\omega^0$. In this study, we offer a comprehensive microscopic understanding of this behavior. Using microrheology experiment, we measured $G^* = G^{\\prime} + i G^{\\prime \\prime}$ of concentrated emulsions in a wide range of frequencies. In theory, we applied a linear response formalism for microrheology to a soft sphere model that undergoes the jamming transition. We find that the theory quantitatively explains the experiments without the need for parameter adjustments. Our analysis reveals that the anomalous viscous loss results from the boson peak, which is a universal vibrational property of amorphous solids and reflects the marginal stability in soft jammed solids. We discuss that the anomalous viscous loss is universal in systems with various interparticle interactions as it stems from the universal boson peak, and it even survives below the jamming density where thermal fluctuation is pronounced and the dynamics becomes inherently nonlinear.","sentences":["Soft jammed solids exhibit intriguing mechanical properties, while their linear response is elusive.","In particular, foams and emulsions generally reveal anomalous viscous loss with the loss and storage modulus following $G^{\\prime \\prime} \\propto \\sqrt{\\omega}$ and $G^{\\prime} \\propto \\omega^0$. In this study, we offer a comprehensive microscopic understanding of this behavior.","Using microrheology experiment, we measured $G^* = G^{\\prime} + i G^{\\prime \\prime}$ of concentrated emulsions in a wide range of frequencies.","In theory, we applied a linear response formalism for microrheology to a soft sphere model that undergoes the jamming transition.","We find that the theory quantitatively explains the experiments without the need for parameter adjustments.","Our analysis reveals that the anomalous viscous loss results from the boson peak, which is a universal vibrational property of amorphous solids and reflects the marginal stability in soft jammed solids.","We discuss that the anomalous viscous loss is universal in systems with various interparticle interactions as it stems from the universal boson peak, and it even survives below the jamming density where thermal fluctuation is pronounced and the dynamics becomes inherently nonlinear."],"url":"http://arxiv.org/abs/2402.00291v1","category":"cond-mat.soft"}
{"created":"2024-02-01 02:42:16","title":"Subgradient evolution of value functions in discrete-time optimal control","abstract":"In this paper we investigate how the subgradients of the value function of a discrete-time convex Bolza problem evolve over time. In particular, we develop a discrete-time version of the characteristic method introduced by Rockafellar and Wolenski in the 2000s, by showing that the time-evolution of the subgradients of the value functions can be associated with trajectories of a discrete-time Hamiltonian system. To do so, we first prove that the value function has a dual counterpart, which corresponds to the conjugate of the value function of a suitable dual problem. We finally make a discussion about the qualification conditions we require for our results, showing in particular that classical problems, such as the Liner-Quadratic regulator, satisfy these hypotheses.","sentences":["In this paper we investigate how the subgradients of the value function of a discrete-time convex Bolza problem evolve over time.","In particular, we develop a discrete-time version of the characteristic method introduced by Rockafellar and Wolenski in the 2000s, by showing that the time-evolution of the subgradients of the value functions can be associated with trajectories of a discrete-time Hamiltonian system.","To do so, we first prove that the value function has a dual counterpart, which corresponds to the conjugate of the value function of a suitable dual problem.","We finally make a discussion about the qualification conditions we require for our results, showing in particular that classical problems, such as the Liner-Quadratic regulator, satisfy these hypotheses."],"url":"http://arxiv.org/abs/2402.00289v1","category":"math.OC"}
{"created":"2024-02-01 02:39:19","title":"Frame-Wise Breath Detection with Self-Training: An Exploration of Enhancing Breath Naturalness in Text-to-Speech","abstract":"Developing Text-to-Speech (TTS) systems that can synthesize natural breath is essential for human-like voice agents but requires extensive manual annotation of breath positions in training data. To this end, we propose a self-training method for training a breath detection model that can automatically detect breath positions in speech. Our method trains the model using a large speech corpus and involves: 1) annotation of limited breath sounds utilizing a rule-based approach, and 2) iterative augmentation of these annotations through pseudo-labeling based on the model's predictions. Our detection model employs Conformer blocks with down-/up-sampling layers, enabling accurate frame-wise breath detection. We investigate its effectiveness in multi-speaker TTS using text transcripts with detected breath marks. The results indicate that using our proposed model for breath detection and breath mark insertion synthesizes breath-contained speech more naturally than a baseline model.","sentences":["Developing Text-to-Speech (TTS) systems that can synthesize natural breath is essential for human-like voice agents but requires extensive manual annotation of breath positions in training data.","To this end, we propose a self-training method for training a breath detection model that can automatically detect breath positions in speech.","Our method trains the model using a large speech corpus and involves: 1) annotation of limited breath sounds utilizing a rule-based approach, and 2) iterative augmentation of these annotations through pseudo-labeling based on the model's predictions.","Our detection model employs Conformer blocks with down-/up-sampling layers, enabling accurate frame-wise breath detection.","We investigate its effectiveness in multi-speaker TTS using text transcripts with detected breath marks.","The results indicate that using our proposed model for breath detection and breath mark insertion synthesizes breath-contained speech more naturally than a baseline model."],"url":"http://arxiv.org/abs/2402.00288v1","category":"eess.AS"}
{"created":"2024-02-01 02:35:01","title":"A study of chaos and randomness in quantum systems","abstract":"How classical chaos emerges from the underlying quantum world is a fundamental problem in physics. The origin of this question is in the correspondence principle. Classical chaos arises due to non-linear dynamics, whereas quantum mechanics, driven by unitary evolution, is linear. The question that still remains is - what are the footprints of classical chaos in the quantum world? One can understand the quantum signatures of classical chaos by studying a quantum system whose classical analogue is chaotic. In this thesis, we use the quantum kicked top model of few qubits in the deep quantum regime to investigate signatures that can be considered as a precursor to chaos in the classical limit. In particular, we study out-of-time-ordered correlators (OTOCs) and Loschmidt echo, the two well-known dynamical diagnostics of chaos. We find vestiges of classical chaos even in such a deep quantum regime.   Another arena where one can study the effects of chaos and randomness is quantum state tomography. We study quantum tomography from a continuous measurement record obtained by measuring expectation values of a set of Hermitian operators generated by a unitary evolution of an initial observable. The rate of information gain and reconstruction fidelity shows vestiges of chaos. As another contribution of this thesis, we have harnessed the power of randomness inherent in the maximally mixed state to give an efficient quantum algorithm to measure OTOCs. The protocol achieves an exponential speedup over the best known classical algorithm, provided the OTOC operator to be estimated admits an efficient gate decomposition. This protocol also helps benchmark unitary gates, which is important from the quantum computation and control perspective.","sentences":["How classical chaos emerges from the underlying quantum world is a fundamental problem in physics.","The origin of this question is in the correspondence principle.","Classical chaos arises due to non-linear dynamics, whereas quantum mechanics, driven by unitary evolution, is linear.","The question that still remains is - what are the footprints of classical chaos in the quantum world?","One can understand the quantum signatures of classical chaos by studying a quantum system whose classical analogue is chaotic.","In this thesis, we use the quantum kicked top model of few qubits in the deep quantum regime to investigate signatures that can be considered as a precursor to chaos in the classical limit.","In particular, we study out-of-time-ordered correlators (OTOCs) and Loschmidt echo, the two well-known dynamical diagnostics of chaos.","We find vestiges of classical chaos even in such a deep quantum regime.   ","Another arena where one can study the effects of chaos and randomness is quantum state tomography.","We study quantum tomography from a continuous measurement record obtained by measuring expectation values of a set of Hermitian operators generated by a unitary evolution of an initial observable.","The rate of information gain and reconstruction fidelity shows vestiges of chaos.","As another contribution of this thesis, we have harnessed the power of randomness inherent in the maximally mixed state to give an efficient quantum algorithm to measure OTOCs.","The protocol achieves an exponential speedup over the best known classical algorithm, provided the OTOC operator to be estimated admits an efficient gate decomposition.","This protocol also helps benchmark unitary gates, which is important from the quantum computation and control perspective."],"url":"http://arxiv.org/abs/2402.00287v1","category":"quant-ph"}
{"created":"2024-02-01 02:33:38","title":"Dirac series of $E_{8(-24)}$","abstract":"This paper classifies the Dirac series of $E_{8(-24)}$, the linear quaternionic real form of complex $E_8$. One tool for us is a further sharpening of the Helgason-Johnson bound in 1969. Our calculation continues to support Vogan's fundamental parallelepiped conjecture.","sentences":["This paper classifies the Dirac series of $E_{8(-24)}$, the linear quaternionic real form of complex $E_8$. One tool for us is a further sharpening of the Helgason-Johnson bound in 1969.","Our calculation continues to support Vogan's fundamental parallelepiped conjecture."],"url":"http://arxiv.org/abs/2402.00286v1","category":"math.RT"}
{"created":"2024-02-01 02:13:47","title":"A quantization of interacting particle systems","abstract":"Interacting particle systems studied in this paper are probabilistic cellular automata with nearest-neighbor interaction including the Domany-Kinzel model. A special case of the Domany-Kinzel model is directed percolation. We regard the interacting particle system as a Markov chain on a graph. Then we present a new quantization of the interacting particle system. After that, we introduce a zeta function of the quantized model and give its determinant expression. Moreover, we calculate the absolute zeta function of the quantized model for the Domany-Kinzel model.","sentences":["Interacting particle systems studied in this paper are probabilistic cellular automata with nearest-neighbor interaction including the Domany-Kinzel model.","A special case of the Domany-Kinzel model is directed percolation.","We regard the interacting particle system as a Markov chain on a graph.","Then we present a new quantization of the interacting particle system.","After that, we introduce a zeta function of the quantized model and give its determinant expression.","Moreover, we calculate the absolute zeta function of the quantized model for the Domany-Kinzel model."],"url":"http://arxiv.org/abs/2402.00280v1","category":"quant-ph"}
{"created":"2024-02-01 02:12:32","title":"Rapid Integrator for a Class of Multi-Contact Systems","abstract":"Many problems in robotics involve creating or breaking multiple contacts nearly simultaneously or in an indeterminate order. We present a novel general purpose numerical integrator based on the theory of Event Selected Systems (ESS). Many multicontact models are ESS, which has recently been shown to imply that despite a discontinuous vector field, the flow of these systems is continuous, piecewise smooth, and has a well defined orbital derivative for all trajectories, which can be rapidly computed. We provide an elementary proof that our integrator is first-order accurate and verify numerically that it is in fact second-order accurate as its construction anticipated. We also compare our integrator, implemented in NumPy, to a MuJoCo simulation on models with 2 to 100 contacts, and confirm that the increase in simulation time per contact is nearly identical. The results suggest that this novel integrator can be invaluable for modelling and control in many robotics applications.","sentences":["Many problems in robotics involve creating or breaking multiple contacts nearly simultaneously or in an indeterminate order.","We present a novel general purpose numerical integrator based on the theory of Event Selected Systems (ESS).","Many multicontact models are ESS, which has recently been shown to imply that despite a discontinuous vector field, the flow of these systems is continuous, piecewise smooth, and has a well defined orbital derivative for all trajectories, which can be rapidly computed.","We provide an elementary proof that our integrator is first-order accurate and verify numerically that it is in fact second-order accurate as its construction anticipated.","We also compare our integrator, implemented in NumPy, to a MuJoCo simulation on models with 2 to 100 contacts, and confirm that the increase in simulation time per contact is nearly identical.","The results suggest that this novel integrator can be invaluable for modelling and control in many robotics applications."],"url":"http://arxiv.org/abs/2402.00279v1","category":"cs.RO"}
{"created":"2024-02-01 02:02:56","title":"Practical No-Switching Continuous-Variable Quantum Key Distribution with Biased Quadrature Detection","abstract":"Continuous-variable quantum key distribution protocol using coherent states and heterodyne detection, called No-Switching protocol, is widely used in practical systems due to the simple experimental setup without basis switching and easy assessment to phase information. The security of an ideal No-Switching protocol has been proved against general attacks in finite-size regime and composable security framework, whose heterodyne detector consists of a beam splitter with transmittance of $50\\%$ and two ideal homodyne detectors. However, the transmittance of a beam splitter is inaccurate and the two detectors always have different quantum efficiency and electronic noise, which introduce asymmetry into the heterodyne detection, and further lead to the mismatch between the ideal protocol and practical systems, thereby overestimating the secret key rate and resulting in a practical security loophole. In this paper, we close this loophole by proposing a modified No-Switching protocol with biased quadrature detection, where the asymmetry of the heterodyne detection is modeled to match the practical systems, and the security of the protocol is analyzed in asymptotic and finite-size regimes. Further, an optimization strategy is proposed to achieve the optimal secret key rate by adjusting the transmittance of the beam splitter. Simulation results show the necessity of considering the asymmetry in heterodyne detection and the effectiveness of the optimization, which provides a promising way to realize a practical secure and high-performance No-Switching system.","sentences":["Continuous-variable quantum key distribution protocol using coherent states and heterodyne detection, called No-Switching protocol, is widely used in practical systems due to the simple experimental setup without basis switching and easy assessment to phase information.","The security of an ideal No-Switching protocol has been proved against general attacks in finite-size regime and composable security framework, whose heterodyne detector consists of a beam splitter with transmittance of $50\\%$ and two ideal homodyne detectors.","However, the transmittance of a beam splitter is inaccurate and the two detectors always have different quantum efficiency and electronic noise, which introduce asymmetry into the heterodyne detection, and further lead to the mismatch between the ideal protocol and practical systems, thereby overestimating the secret key rate and resulting in a practical security loophole.","In this paper, we close this loophole by proposing a modified No-Switching protocol with biased quadrature detection, where the asymmetry of the heterodyne detection is modeled to match the practical systems, and the security of the protocol is analyzed in asymptotic and finite-size regimes.","Further, an optimization strategy is proposed to achieve the optimal secret key rate by adjusting the transmittance of the beam splitter.","Simulation results show the necessity of considering the asymmetry in heterodyne detection and the effectiveness of the optimization, which provides a promising way to realize a practical secure and high-performance No-Switching system."],"url":"http://arxiv.org/abs/2402.00277v1","category":"quant-ph"}
{"created":"2024-02-01 01:58:34","title":"The Maude strategy language","abstract":"Rewriting logic is a natural and expressive framework for the specification of concurrent systems and logics. The Maude specification language provides an implementation of this formalism that allows executing, verifying, and analyzing the represented systems. These specifications declare their objects by means of terms and equations, and provide rewriting rules to represent potentially non-deterministic local transformations on the state. Sometimes a controlled application of these rules is required to reduce non-determinism, to capture global, goal-oriented or efficiency concerns, or to select specific executions for their analysis. That is what we call a strategy. In order to express them, respecting the separation of concerns principle, a Maude strategy language was proposed and developed. The first implementation of the strategy language was done in Maude itself using its reflective features. After ample experimentation, some more features have been added and, for greater efficiency, the strategy language has been implemented in C++ as an integral part of the Maude system.   This paper describes the Maude strategy language along with its semantics, its implementation decisions, and several application examples from various fields.","sentences":["Rewriting logic is a natural and expressive framework for the specification of concurrent systems and logics.","The Maude specification language provides an implementation of this formalism that allows executing, verifying, and analyzing the represented systems.","These specifications declare their objects by means of terms and equations, and provide rewriting rules to represent potentially non-deterministic local transformations on the state.","Sometimes a controlled application of these rules is required to reduce non-determinism, to capture global, goal-oriented or efficiency concerns, or to select specific executions for their analysis.","That is what we call a strategy.","In order to express them, respecting the separation of concerns principle, a Maude strategy language was proposed and developed.","The first implementation of the strategy language was done in Maude itself using its reflective features.","After ample experimentation, some more features have been added and, for greater efficiency, the strategy language has been implemented in C++ as an integral part of the Maude system.   ","This paper describes the Maude strategy language along with its semantics, its implementation decisions, and several application examples from various fields."],"url":"http://arxiv.org/abs/2402.00275v1","category":"cs.LO"}
{"created":"2024-02-01 01:49:05","title":"Nonlinearity-Enhanced Coutinues Microwave Detection Based on Stochastic Resonance","abstract":"In practical sensing tasks, noise is usually regarded as an obstruction to better performance and will degrade the sensitivity. Fortunately, \\textit{stochastic resonance} (SR), a counterintuitive concept, can utilize noise to greatly enhance the detected signal amplitude. Although fundamentally important as a mechanism of weak signal amplification, and has been continually explored in geological, biological, and physical science, both theoretically and experimentally, SR has yet to be demonstrated in realistic sensing tasks. Here we develop a novel SR-based nonlinear sensor using a thermal ensemble of interacting Rydberg atoms. With the assistance of stochastic noise (either inherently in the system or added externally) and strong nonlinearity in the Rydberg ensembles, the signal encoded in a weak MW field is greatly enhanced (over 25 dB). Moreover, we show that the SR-based atomic sensor can achieve a better sensitivity in our system, which is over 6.6 dB compared to a heterodye atomic sensor. Our results show potential advantage of SR-based MW sensors in commercial or defense-related applications.","sentences":["In practical sensing tasks, noise is usually regarded as an obstruction to better performance and will degrade the sensitivity.","Fortunately, \\textit{stochastic resonance} (SR), a counterintuitive concept, can utilize noise to greatly enhance the detected signal amplitude.","Although fundamentally important as a mechanism of weak signal amplification, and has been continually explored in geological, biological, and physical science, both theoretically and experimentally, SR has yet to be demonstrated in realistic sensing tasks.","Here we develop a novel SR-based nonlinear sensor using a thermal ensemble of interacting Rydberg atoms.","With the assistance of stochastic noise (either inherently in the system or added externally) and strong nonlinearity in the Rydberg ensembles, the signal encoded in a weak MW field is greatly enhanced (over 25 dB).","Moreover, we show that the SR-based atomic sensor can achieve a better sensitivity in our system, which is over 6.6 dB compared to a heterodye atomic sensor.","Our results show potential advantage of SR-based MW sensors in commercial or defense-related applications."],"url":"http://arxiv.org/abs/2402.00273v1","category":"physics.atom-ph"}
{"created":"2024-02-01 18:59:57","title":"Molecular Pairing in Twisted Bilayer Graphene Superconductivity","abstract":"We propose a theory for how the weak phonon-mediated interaction ($J_{\\rm A}\\!=\\!1\\!\\sim\\!4$meV) wins over the prohibitive Coulomb repulsion ($U\\!=\\!30\\!\\sim\\!60$meV) and leads to a nematic superconductor in magic-angle twisted bilayer graphene (MATBG). We find the pairing mechanism akin to that in the A$_3$C$_{60}$ family of molecular superconductors: Each AA stacking region of MATBG resembles a C$_{60}$ molecule, in that optical phonons can dynamically lift the degeneracy of the moir\\'e orbitals, in analogy to the dynamical Jahn-Teller effect. Such induced $J_{\\rm A}$ has the form of an inter-valley anti-Hund's coupling and is less suppressed than $U$ by the Kondo screening near a Mott insulator. Additionally, we also considered an intra-orbital Hund's coupling $J_{\\rm H}$ that originates from the on-site repulsion of a carbon atom. Under a reasonable approximation of the realistic model, we prove that the renormalized local interaction between quasi-particles must have a pairing (negative) channel in a doped correlated insulator at $\\nu=\\pm(2+\\delta\\nu)$, albeit the bare interaction is positive definite. The proof is non-perturbative and based on $exact$ asymptotic behaviors of the vertex function imposed by Ward identities. Existence of an optimal $U$ for superconductivity is predicted. We also analyzed the pairing symmetry. In a large area of the parameter space of $J_{\\rm A}$, $J_{\\rm H}$, the ground state has a nematic $d$-wave singlet pairing, which, however, can lead to a $p$-wave-like nodal structure due to the Berry's phase on Fermi surfaces (or Euler obstruction). A fully gapped $s$-wave pairing is also possible but spans a smaller phase space in the phase diagram.","sentences":["We propose a theory for how the weak phonon-mediated interaction ($J_{\\rm A}\\!=\\!1\\!\\sim\\!4$meV) wins over the prohibitive Coulomb repulsion ($U\\!=\\!30\\!\\sim\\!60$meV) and leads to a nematic superconductor in magic-angle twisted bilayer graphene (MATBG).","We find the pairing mechanism akin to that in the A$_3$C$_{60}$ family of molecular superconductors: Each AA stacking region of MATBG resembles a C$_{60}$ molecule, in that optical phonons can dynamically lift the degeneracy of the moir\\'e orbitals, in analogy to the dynamical Jahn-Teller effect.","Such induced $J_{\\rm A}$ has the form of an inter-valley anti-Hund's coupling and is less suppressed than $U$ by the Kondo screening near a Mott insulator.","Additionally, we also considered an intra-orbital Hund's coupling $J_{\\rm H}$ that originates from the on-site repulsion of a carbon atom.","Under a reasonable approximation of the realistic model, we prove that the renormalized local interaction between quasi-particles must have a pairing (negative) channel in a doped correlated insulator at $\\nu=\\pm(2+\\delta\\nu)$, albeit the bare interaction is positive definite.","The proof is non-perturbative and based on $exact$ asymptotic behaviors of the vertex function imposed by Ward identities.","Existence of an optimal $U$ for superconductivity is predicted.","We also analyzed the pairing symmetry.","In a large area of the parameter space of $J_{\\rm A}$, $J_{\\rm H}$, the ground state has a nematic $d$-wave singlet pairing, which, however, can lead to a $p$-wave-like nodal structure due to the Berry's phase on Fermi surfaces (or Euler obstruction).","A fully gapped $s$-wave pairing is also possible but spans a smaller phase space in the phase diagram."],"url":"http://arxiv.org/abs/2402.00869v1","category":"cond-mat.supr-con"}
{"created":"2024-02-01 18:34:29","title":"Helmholtz quasi-resonances are unstable under most single-signed perturbations of the wave speed","abstract":"We consider Helmholtz problems with a perturbed wave speed, where the single-signed perturbation is governed by a parameter $z$. Both the wave speed and the perturbation are allowed to be discontinuous (modelling a penetrable obstacle). We show that, for any frequency, for most values of $z$, the solution operator is polynomially bounded in the frequency.   This solution-operator bound is most interesting for Helmholtz problems with strong trapping; recall that here there exist a sequence of real frequencies, tending to infinity, through which the solution operator grows superalgebraically, with these frequencies often called $\\textit{quasi-resonances}$. The result of this paper then shows that, at every quasi-resonance, the superalgebraic growth of the solution operator does not occur for most single-signed perturbations of the wave speed, i.e., quasi-resonances are unstable under most such perturbations.","sentences":["We consider Helmholtz problems with a perturbed wave speed, where the single-signed perturbation is governed by a parameter $z$. Both the wave speed and the perturbation are allowed to be discontinuous (modelling a penetrable obstacle).","We show that, for any frequency, for most values of $z$, the solution operator is polynomially bounded in the frequency.   ","This solution-operator bound is most interesting for Helmholtz problems with strong trapping; recall that here there exist a sequence of real frequencies, tending to infinity, through which the solution operator grows superalgebraically, with these frequencies often called $\\textit{quasi-resonances}$. The result of this paper then shows that, at every quasi-resonance, the superalgebraic growth of the solution operator does not occur for most single-signed perturbations of the wave speed, i.e., quasi-resonances are unstable under most such perturbations."],"url":"http://arxiv.org/abs/2402.00843v1","category":"math.AP"}
{"created":"2024-02-01 18:31:34","title":"Tiny Titans: Can Smaller Large Language Models Punch Above Their Weight in the Real World for Meeting Summarization?","abstract":"Large Language Models (LLMs) have demonstrated impressive capabilities to solve a wide range of tasks without being explicitly fine-tuned on task-specific datasets. However, deploying LLMs in the real world is not trivial, as it requires substantial computing resources. In this paper, we investigate whether smaller, compact LLMs are a good alternative to the comparatively Larger LLMs2 to address significant costs associated with utilizing LLMs in the real world. In this regard, we study the meeting summarization task in a real-world industrial environment and conduct extensive experiments by comparing the performance of fine-tuned compact LLMs (e.g., FLAN-T5, TinyLLaMA, LiteLLaMA) with zero-shot larger LLMs (e.g., LLaMA-2, GPT-3.5, PaLM-2). We observe that most smaller LLMs, even after fine-tuning, fail to outperform larger zero-shot LLMs in meeting summarization datasets. However, a notable exception is FLAN-T5 (780M parameters), which performs on par or even better than many zero-shot Larger LLMs (from 7B to above 70B parameters), while being significantly smaller. This makes compact LLMs like FLAN-T5 a suitable cost-efficient solution for real-world industrial deployment.","sentences":["Large Language Models (LLMs) have demonstrated impressive capabilities to solve a wide range of tasks without being explicitly fine-tuned on task-specific datasets.","However, deploying LLMs in the real world is not trivial, as it requires substantial computing resources.","In this paper, we investigate whether smaller, compact LLMs are a good alternative to the comparatively Larger LLMs2 to address significant costs associated with utilizing LLMs in the real world.","In this regard, we study the meeting summarization task in a real-world industrial environment and conduct extensive experiments by comparing the performance of fine-tuned compact LLMs (e.g., FLAN-T5, TinyLLaMA, LiteLLaMA) with zero-shot larger LLMs (e.g., LLaMA-2, GPT-3.5, PaLM-2).","We observe that most smaller LLMs, even after fine-tuning, fail to outperform larger zero-shot LLMs in meeting summarization datasets.","However, a notable exception is FLAN-T5 (780M parameters), which performs on par or even better than many zero-shot Larger LLMs (from 7B to above 70B parameters), while being significantly smaller.","This makes compact LLMs like FLAN-T5 a suitable cost-efficient solution for real-world industrial deployment."],"url":"http://arxiv.org/abs/2402.00841v1","category":"cs.CL"}
{"created":"2024-02-01 17:45:49","title":"Anomalous entropy-driven kinetics of dislocation nucleation","abstract":"The kinetics of dislocation reactions, such as dislocation multiplication, controls the plastic deformation in crystals beyond their elastic limit, therefore critical mechanisms in a number of applications in materials science. We present a series of large-scale molecular dynamics simulations that shows that one such type of reactions, the nucleation of dislocation at free surfaces, exhibit extremely unconventional kinetics, including unexpectedly large nucleation rates under compression, very strong entropic stabilization under tension, as well as strong non-Arrhenius behavior. These unusual kinetics are quantitatively rationalized using a variational transition state theory approach coupled with an efficient numerical scheme for the estimation of vibrational entropy changes. These results highlight the need for a variational treatment of the kinetics to quantitatively capture dislocation reaction kinetics, especially at low-to-moderate strains where large deformations are required to activate reactions. These observations suggest possible explanations to previously observed unconventional deformation kinetics in both molecular dynamics simulations and experiments.","sentences":["The kinetics of dislocation reactions, such as dislocation multiplication, controls the plastic deformation in crystals beyond their elastic limit, therefore critical mechanisms in a number of applications in materials science.","We present a series of large-scale molecular dynamics simulations that shows that one such type of reactions, the nucleation of dislocation at free surfaces, exhibit extremely unconventional kinetics, including unexpectedly large nucleation rates under compression, very strong entropic stabilization under tension, as well as strong non-Arrhenius behavior.","These unusual kinetics are quantitatively rationalized using a variational transition state theory approach coupled with an efficient numerical scheme for the estimation of vibrational entropy changes.","These results highlight the need for a variational treatment of the kinetics to quantitatively capture dislocation reaction kinetics, especially at low-to-moderate strains where large deformations are required to activate reactions.","These observations suggest possible explanations to previously observed unconventional deformation kinetics in both molecular dynamics simulations and experiments."],"url":"http://arxiv.org/abs/2402.00810v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-01 17:45:26","title":"Position Paper: Bayesian Deep Learning in the Age of Large-Scale AI","abstract":"In the current landscape of deep learning research, there is a predominant emphasis on achieving high predictive accuracy in supervised tasks involving large image and language datasets. However, a broader perspective reveals a multitude of overlooked metrics, tasks, and data types, such as uncertainty, active and continual learning, and scientific data, that demand attention. Bayesian deep learning (BDL) constitutes a promising avenue, offering advantages across these diverse settings. This paper posits that BDL can elevate the capabilities of deep learning. It revisits the strengths of BDL, acknowledges existing challenges, and highlights some exciting research avenues aimed at addressing these obstacles. Looking ahead, the discussion focuses on possible ways to combine large-scale foundation models with BDL to unlock their full potential.","sentences":["In the current landscape of deep learning research, there is a predominant emphasis on achieving high predictive accuracy in supervised tasks involving large image and language datasets.","However, a broader perspective reveals a multitude of overlooked metrics, tasks, and data types, such as uncertainty, active and continual learning, and scientific data, that demand attention.","Bayesian deep learning (BDL) constitutes a promising avenue, offering advantages across these diverse settings.","This paper posits that BDL can elevate the capabilities of deep learning.","It revisits the strengths of BDL, acknowledges existing challenges, and highlights some exciting research avenues aimed at addressing these obstacles.","Looking ahead, the discussion focuses on possible ways to combine large-scale foundation models with BDL to unlock their full potential."],"url":"http://arxiv.org/abs/2402.00809v1","category":"cs.LG"}
{"created":"2024-02-01 17:31:13","title":"Investigating the magnetism of Ni from a momentum space perspective","abstract":"For more than three decades, clear discrepancies have existed between spin densities in momentum space revealed by Magnetic Compton scattering experiments and theoretical calculations based on density functional theory (DFT). Here by making a wide comparison between different theoretical methods, including DFT, DFT combined with dynamical mean field theory, and Hedin's $GW$ approximation, we discover how the magnetic Compton profiles of Ni can be predicted remarkably well. We find that the essential ingredients missing in DFT are (i) local spin fluctuations and (ii) a non-local treatment of electron correlations.","sentences":["For more than three decades, clear discrepancies have existed between spin densities in momentum space revealed by Magnetic Compton scattering experiments and theoretical calculations based on density functional theory (DFT).","Here by making a wide comparison between different theoretical methods, including DFT, DFT combined with dynamical mean field theory, and Hedin's $GW$ approximation, we discover how the magnetic Compton profiles of Ni can be predicted remarkably well.","We find that the essential ingredients missing in DFT are (i) local spin fluctuations and (ii) a non-local treatment of electron correlations."],"url":"http://arxiv.org/abs/2402.00799v1","category":"cond-mat.str-el"}
{"created":"2024-02-01 17:06:51","title":"Robust Sufficient Dimension Reduction via $\u03b1$-Distance Covariance","abstract":"We introduce a novel sufficient dimension-reduction (SDR) method which is robust against outliers using $\\alpha$-distance covariance (dCov) in dimension-reduction problems. Under very mild conditions on the predictors, the central subspace is effectively estimated and model-free advantage without estimating link function based on the projection on the Stiefel manifold. We establish the convergence property of the proposed estimation under some regularity conditions. We compare the performance of our method with existing SDR methods by simulation and real data analysis and show that our algorithm improves the computational efficiency and effectiveness.","sentences":["We introduce a novel sufficient dimension-reduction (SDR) method which is robust against outliers using $\\alpha$-distance covariance (dCov) in dimension-reduction problems.","Under very mild conditions on the predictors, the central subspace is effectively estimated and model-free advantage without estimating link function based on the projection on the Stiefel manifold.","We establish the convergence property of the proposed estimation under some regularity conditions.","We compare the performance of our method with existing SDR methods by simulation and real data analysis and show that our algorithm improves the computational efficiency and effectiveness."],"url":"http://arxiv.org/abs/2402.00778v1","category":"stat.ME"}
{"created":"2024-02-01 16:52:21","title":"360-GS: Layout-guided Panoramic Gaussian Splatting For Indoor Roaming","abstract":"3D Gaussian Splatting (3D-GS) has recently attracted great attention with real-time and photo-realistic renderings. This technique typically takes perspective images as input and optimizes a set of 3D elliptical Gaussians by splatting them onto the image planes, resulting in 2D Gaussians. However, applying 3D-GS to panoramic inputs presents challenges in effectively modeling the projection onto the spherical surface of ${360^\\circ}$ images using 2D Gaussians. In practical applications, input panoramas are often sparse, leading to unreliable initialization of 3D Gaussians and subsequent degradation of 3D-GS quality. In addition, due to the under-constrained geometry of texture-less planes (e.g., walls and floors), 3D-GS struggles to model these flat regions with elliptical Gaussians, resulting in significant floaters in novel views. To address these issues, we propose 360-GS, a novel $360^{\\circ}$ Gaussian splatting for a limited set of panoramic inputs. Instead of splatting 3D Gaussians directly onto the spherical surface, 360-GS projects them onto the tangent plane of the unit sphere and then maps them to the spherical projections. This adaptation enables the representation of the projection using Gaussians. We guide the optimization of 360-GS by exploiting layout priors within panoramas, which are simple to obtain and contain strong structural information about the indoor scene. Our experimental results demonstrate that 360-GS allows panoramic rendering and outperforms state-of-the-art methods with fewer artifacts in novel view synthesis, thus providing immersive roaming in indoor scenarios.","sentences":["3D Gaussian Splatting (3D-GS) has recently attracted great attention with real-time and photo-realistic renderings.","This technique typically takes perspective images as input and optimizes a set of 3D elliptical Gaussians by splatting them onto the image planes, resulting in 2D Gaussians.","However, applying 3D-GS to panoramic inputs presents challenges in effectively modeling the projection onto the spherical surface of ${360^\\circ}$ images using 2D Gaussians.","In practical applications, input panoramas are often sparse, leading to unreliable initialization of 3D Gaussians and subsequent degradation of 3D-GS quality.","In addition, due to the under-constrained geometry of texture-less planes (e.g., walls and floors), 3D-GS struggles to model these flat regions with elliptical Gaussians, resulting in significant floaters in novel views.","To address these issues, we propose 360-GS, a novel $360^{\\circ}$ Gaussian splatting for a limited set of panoramic inputs.","Instead of splatting 3D Gaussians directly onto the spherical surface, 360-GS projects them onto the tangent plane of the unit sphere and then maps them to the spherical projections.","This adaptation enables the representation of the projection using Gaussians.","We guide the optimization of 360-GS by exploiting layout priors within panoramas, which are simple to obtain and contain strong structural information about the indoor scene.","Our experimental results demonstrate that 360-GS allows panoramic rendering and outperforms state-of-the-art methods with fewer artifacts in novel view synthesis, thus providing immersive roaming in indoor scenarios."],"url":"http://arxiv.org/abs/2402.00763v1","category":"cs.CV"}
{"created":"2024-02-01 16:41:47","title":"Mott resistive switching initiated by topological defects","abstract":"Resistive switching is the fundamental process that triggers the sudden change of the electrical properties in solid-state devices under the action of intense electric fields. Despite its relevance for information processing, ultrafast electronics, neuromorphic devices, resistive memories and brain-inspired computation, the nature of the local stochastic fluctuations that drive the formation of metallic nuclei out of the insulating state has remained hidden. Here, using operando X-ray nano-imaging, we have captured the early-stages of resistive switching in a V2O3-based device under working conditions. V2O3 is a paradigmatic Mott material, which undergoes a first-order metal-to-insulator transition coupled to a lattice transformation that breaks the threefold rotational symmetry of the rhombohedral metal phase. We reveal a new class of volatile electronic switching triggered by nanoscale topological defects of the lattice order parameter of the insulating phase. Our results pave the way to the use of strain engineering approaches to manipulate topological defects and achieve the full control of the electronic Mott switching. The concept of topology-driven reversible electronic transition is of interest for a broad class of quantum materials, comprising transition metal oxides, chalcogenides and kagome metals, that exhibit first-order electronic transitions coupled to a symmetry-breaking order.","sentences":["Resistive switching is the fundamental process that triggers the sudden change of the electrical properties in solid-state devices under the action of intense electric fields.","Despite its relevance for information processing, ultrafast electronics, neuromorphic devices, resistive memories and brain-inspired computation, the nature of the local stochastic fluctuations that drive the formation of metallic nuclei out of the insulating state has remained hidden.","Here, using operando X-ray nano-imaging, we have captured the early-stages of resistive switching in a V2O3-based device under working conditions.","V2O3 is a paradigmatic Mott material, which undergoes a first-order metal-to-insulator transition coupled to a lattice transformation that breaks the threefold rotational symmetry of the rhombohedral metal phase.","We reveal a new class of volatile electronic switching triggered by nanoscale topological defects of the lattice order parameter of the insulating phase.","Our results pave the way to the use of strain engineering approaches to manipulate topological defects and achieve the full control of the electronic Mott switching.","The concept of topology-driven reversible electronic transition is of interest for a broad class of quantum materials, comprising transition metal oxides, chalcogenides and kagome metals, that exhibit first-order electronic transitions coupled to a symmetry-breaking order."],"url":"http://arxiv.org/abs/2402.00747v1","category":"cond-mat.str-el"}
{"created":"2024-02-01 16:09:36","title":"Sensitivity Analysis of Separation Time Along Weak Stability Boundary Transfers","abstract":"This study analyzes the sensitivity of the dynamics around Weak Stability Boundary Transfers (WSBT) in the elliptical restricted three-body problem. With WSBTs increasing popularity for cislunar transfers, understanding its inherently chaotic dynamics becomes pivotal for guiding and navigating cooperative spacecrafts as well as detecting non-cooperative objects. We introduce the notion of separation time to gauge the deviation of a point near a nominal WSBT from the trajectory's vicinity. Employing the Cauchy-Green tensor to identify stretching directions in position and velocity, the separation time, along with the Finite-Time Lyapunov Exponent are studied within a ball of state uncertainty scaled to typical orbit determination performances.","sentences":["This study analyzes the sensitivity of the dynamics around Weak Stability Boundary Transfers (WSBT) in the elliptical restricted three-body problem.","With WSBTs increasing popularity for cislunar transfers, understanding its inherently chaotic dynamics becomes pivotal for guiding and navigating cooperative spacecrafts as well as detecting non-cooperative objects.","We introduce the notion of separation time to gauge the deviation of a point near a nominal WSBT from the trajectory's vicinity.","Employing the Cauchy-Green tensor to identify stretching directions in position and velocity, the separation time, along with the Finite-Time Lyapunov Exponent are studied within a ball of state uncertainty scaled to typical orbit determination performances."],"url":"http://arxiv.org/abs/2402.00717v1","category":"nlin.CD"}
{"created":"2024-02-01 15:59:16","title":"Vehicle Perception from Satellite","abstract":"Satellites are capable of capturing high-resolution videos. It makes vehicle perception from satellite become possible. Compared to street surveillance, drive recorder or other equipments, satellite videos provide a much broader city-scale view, so that the global dynamic scene of the traffic are captured and displayed. Traffic monitoring from satellite is a new task with great potential applications, including traffic jams prediction, path planning, vehicle dispatching, \\emph{etc.}. Practically, limited by the resolution and view, the captured vehicles are very tiny (a few pixels) and move slowly. Worse still, these satellites are in Low Earth Orbit (LEO) to capture such high-resolution videos, so the background is also moving. Under this circumstance, traffic monitoring from the satellite view is an extremely challenging task. To attract more researchers into this field, we build a large-scale benchmark for traffic monitoring from satellite. It supports several tasks, including tiny object detection, counting and density estimation. The dataset is constructed based on 12 satellite videos and 14 synthetic videos recorded from GTA-V. They are separated into 408 video clips, which contain 7,336 real satellite images and 1,960 synthetic images. 128,801 vehicles are annotated totally, and the number of vehicles in each image varies from 0 to 101. Several classic and state-of-the-art approaches in traditional computer vision are evaluated on the datasets, so as to compare the performance of different approaches, analyze the challenges in this task, and discuss the future prospects. The dataset is available at: https://github.com/Chenxi1510/Vehicle-Perception-from-Satellite-Videos.","sentences":["Satellites are capable of capturing high-resolution videos.","It makes vehicle perception from satellite become possible.","Compared to street surveillance, drive recorder or other equipments, satellite videos provide a much broader city-scale view, so that the global dynamic scene of the traffic are captured and displayed.","Traffic monitoring from satellite is a new task with great potential applications, including traffic jams prediction, path planning, vehicle dispatching, \\emph{etc.}.","Practically, limited by the resolution and view, the captured vehicles are very tiny (a few pixels) and move slowly.","Worse still, these satellites are in Low Earth Orbit (LEO) to capture such high-resolution videos, so the background is also moving.","Under this circumstance, traffic monitoring from the satellite view is an extremely challenging task.","To attract more researchers into this field, we build a large-scale benchmark for traffic monitoring from satellite.","It supports several tasks, including tiny object detection, counting and density estimation.","The dataset is constructed based on 12 satellite videos and 14 synthetic videos recorded from GTA-V.","They are separated into 408 video clips, which contain 7,336 real satellite images and 1,960 synthetic images.","128,801 vehicles are annotated totally, and the number of vehicles in each image varies from 0 to 101.","Several classic and state-of-the-art approaches in traditional computer vision are evaluated on the datasets, so as to compare the performance of different approaches, analyze the challenges in this task, and discuss the future prospects.","The dataset is available at: https://github.com/Chenxi1510/Vehicle-Perception-from-Satellite-Videos."],"url":"http://arxiv.org/abs/2402.00703v1","category":"cs.CV"}
{"created":"2024-02-01 15:48:40","title":"Maximum a posteriori testing in statistical inverse problems","abstract":"This paper is concerned with a Bayesian approach to testing hypotheses in statistical inverse problems. Based on the posterior distribution $\\Pi \\left(\\cdot |Y = y\\right)$, we want to infer whether a feature $\\left\\langle\\varphi, u^\\dagger\\right\\rangle$ of the unknown quantity of interest $u^\\dagger$ is positive. This can be done by the so-called maximum a posteriori test. We provide a frequentistic analysis of this test's properties such as level and power, and prove that it is a regularized test in the sense of Kretschmann et al. (2024). Furthermore we provide lower bounds for its power under classical spectral source conditions in case of Gaussian priors. Numerical simulations illustrate its superior performance both in moderately and severely ill-posed situations.","sentences":["This paper is concerned with a Bayesian approach to testing hypotheses in statistical inverse problems.","Based on the posterior distribution $\\Pi \\left(\\cdot |Y = y\\right)$, we want to infer whether a feature $\\left\\langle\\varphi, u^\\dagger\\right\\rangle$ of the unknown quantity of interest $u^\\dagger$ is positive.","This can be done by the so-called maximum a posteriori test.","We provide a frequentistic analysis of this test's properties such as level and power, and prove that it is a regularized test in the sense of Kretschmann et al. (2024).","Furthermore we provide lower bounds for its power under classical spectral source conditions in case of Gaussian priors.","Numerical simulations illustrate its superior performance both in moderately and severely ill-posed situations."],"url":"http://arxiv.org/abs/2402.00686v1","category":"math.ST"}
{"created":"2024-02-01 15:47:34","title":"Near and full quasi-optimality of finite element approximations of stationary second-order mean field games","abstract":"We establish a priori error bounds for monotone stabilized finite element discretizations of stationary second-order mean field games (MFG) on Lipschitz polytopal domains. Under suitable hypotheses, we prove that the approximation is asymptotically nearly quasi-optimal in the $H^1$-norm in the sense that, on sufficiently fine meshes, the error between exact and computed solutions is bounded by the best approximation error of the corresponding finite element space, plus possibly an additional term, due to the stabilization, that is of optimal order with respect to the mesh size. We thereby deduce optimal rates of convergence of the error with respect to the mesh-size for solutions with sufficient regularity. We further show full asymptotic quasi-optimality of the approximation error in the more restricted case of sequences of strictly acute meshes. Our third main contribution is to further show, in the case where the domain is convex, that the convergence rate for the $H^1$-norm error of the value function approximation remains optimal even if the density function only has minimal regularity in $H^1$.","sentences":["We establish a priori error bounds for monotone stabilized finite element discretizations of stationary second-order mean field games (MFG) on Lipschitz polytopal domains.","Under suitable hypotheses, we prove that the approximation is asymptotically nearly quasi-optimal in the $H^1$-norm in the sense that, on sufficiently fine meshes, the error between exact and computed solutions is bounded by the best approximation error of the corresponding finite element space, plus possibly an additional term, due to the stabilization, that is of optimal order with respect to the mesh size.","We thereby deduce optimal rates of convergence of the error with respect to the mesh-size for solutions with sufficient regularity.","We further show full asymptotic quasi-optimality of the approximation error in the more restricted case of sequences of strictly acute meshes.","Our third main contribution is to further show, in the case where the domain is convex, that the convergence rate for the $H^1$-norm error of the value function approximation remains optimal even if the density function only has minimal regularity in $H^1$."],"url":"http://arxiv.org/abs/2402.00685v1","category":"math.NA"}
{"created":"2024-02-01 15:12:35","title":"Calder\u00f3n problem for nonlocal viscous wave equations: Unique determination of linear and nonlinear perturbations","abstract":"The main goal of this article is the study of a Calder\\'on type inverse problem for a viscous wave equation. We show that the partial Dirichlet to Neumann map uniquely determines on the one hand linear perturbations and on the other hand homogeneous nonlinearities $f(u)$ whenever the latter satisfy a certain growth assumption. As a preliminary step we discuss the well-posedness in each case, where for the nonlinear setting we invoke the implicit function theorem after establishing the differentiability of the associated Nemytskii operator $f(u)$. In the linear case we establish a Runge approximation theorem in $L^2(0,T;\\widetilde{H}^{s}(\\Omega))$, which allows us to uniquely determine potentials that belong only to $L^{\\infty}(0,T;L^p(\\Omega))$ for some $1<p\\leq \\infty$ satisfying suitable restrictions. In the nonlinear case, we first derive an appropriate integral identity and combine this with the differentiability of the solution map around zero to show that the nonlinearity is uniquely determined by the Dirichlet to Neumann map. To make this linearization technique work, it is essential that we have a Runge approximation in $L^2(0,T;\\widetilde{H}^s(\\Omega))$ instead of $L^2(\\Omega_T)$ at our disposal.","sentences":["The main goal of this article is the study of a Calder\\'on type inverse problem for a viscous wave equation.","We show that the partial Dirichlet to Neumann map uniquely determines on the one hand linear perturbations and on the other hand homogeneous nonlinearities $f(u)$ whenever the latter satisfy a certain growth assumption.","As a preliminary step we discuss the well-posedness in each case, where for the nonlinear setting we invoke the implicit function theorem after establishing the differentiability of the associated Nemytskii operator $f(u)$. In the linear case we establish a Runge approximation theorem in $L^2(0,T;\\widetilde{H}^{s}(\\Omega))$, which allows us to uniquely determine potentials that belong only to $L^{\\infty}(0,T;L^p(\\Omega))$ for some $1<p\\leq \\infty$ satisfying suitable restrictions.","In the nonlinear case, we first derive an appropriate integral identity and combine this with the differentiability of the solution map around zero to show that the nonlinearity is uniquely determined by the Dirichlet to Neumann map.","To make this linearization technique work, it is essential that we have a Runge approximation in $L^2(0,T;\\widetilde{H}^s(\\Omega))$ instead of $L^2(\\Omega_T)$ at our disposal."],"url":"http://arxiv.org/abs/2402.00650v1","category":"math.AP"}
{"created":"2024-02-01 14:56:54","title":"Testing side-channel security of cryptographic implementations against future microarchitectures","abstract":"How will future microarchitectures impact the security of existing cryptographic implementations? As we cannot keep reducing the size of transistors, chip vendors have started developing new microarchitectural optimizations to speed up computation. A recent study (Sanchez Vicarte et al., ISCA 2021) suggests that these optimizations might open the Pandora's box of microarchitectural attacks. However, there is little guidance on how to evaluate the security impact of future optimization proposals.   To help chip vendors explore the impact of microarchitectural optimizations on cryptographic implementations, we develop (i) an expressive domain-specific language, called LmSpec, that allows them to specify the leakage model for the given optimization and (ii) a testing framework, called LmTest, to automatically detect leaks under the specified leakage model within the given implementation. Using this framework, we conduct an empirical study of 18 proposed microarchitectural optimizations on 25 implementations of eight cryptographic primitives in five popular libraries. We find that every implementation would contain secret-dependent leaks, sometimes sufficient to recover a victim's secret key, if these optimizations were realized. Ironically, some leaks are possible only because of coding idioms used to prevent leaks under the standard constant-time model.","sentences":["How will future microarchitectures impact the security of existing cryptographic implementations?","As we cannot keep reducing the size of transistors, chip vendors have started developing new microarchitectural optimizations to speed up computation.","A recent study (Sanchez Vicarte et al., ISCA 2021) suggests that these optimizations might open the Pandora's box of microarchitectural attacks.","However, there is little guidance on how to evaluate the security impact of future optimization proposals.   ","To help chip vendors explore the impact of microarchitectural optimizations on cryptographic implementations, we develop (i) an expressive domain-specific language, called LmSpec, that allows them to specify the leakage model for the given optimization and (ii) a testing framework, called LmTest, to automatically detect leaks under the specified leakage model within the given implementation.","Using this framework, we conduct an empirical study of 18 proposed microarchitectural optimizations on 25 implementations of eight cryptographic primitives in five popular libraries.","We find that every implementation would contain secret-dependent leaks, sometimes sufficient to recover a victim's secret key, if these optimizations were realized.","Ironically, some leaks are possible only because of coding idioms used to prevent leaks under the standard constant-time model."],"url":"http://arxiv.org/abs/2402.00641v1","category":"cs.CR"}
{"created":"2024-02-01 14:45:00","title":"Tensile and compressive strain tuning of a Kondo lattice","abstract":"We present electrical resistivity measurements on the prototypical heavy-fermion metal YbRh$_{2}$Si$_{2}$ (YRS) under $a$-axis tensile and compressive strain and focus on the evolution of the resistivity maximum near 136~K that arises from the interplay of the Kondo effect and the crystal electric field (CEF) splitting. While compressive strain reduces $T_{\\rm max}$, similar as previously reported for hydrostatic pressure, $T_{\\rm max}$ is enhanced up to 145~K for 0.13\\% tensile strain. Model calculations for the strain effect on CEF splitting in YRS reveal a negligible shift of the levels. Instead, the enhancement of the resistivity maximum indicates a 20\\% increase of the Kondo temperature. This opens the perspective to access the hidden zero-field QCP in pure YRS.","sentences":["We present electrical resistivity measurements on the prototypical heavy-fermion metal YbRh$_{2}$Si$_{2}$ (YRS) under $a$-axis tensile and compressive strain and focus on the evolution of the resistivity maximum near 136~K that arises from the interplay of the Kondo effect and the crystal electric field (CEF) splitting.","While compressive strain reduces $T_{\\rm max}$, similar as previously reported for hydrostatic pressure, $T_{\\rm max}$ is enhanced up to 145~K for 0.13\\% tensile strain.","Model calculations for the strain effect on CEF splitting in YRS reveal a negligible shift of the levels.","Instead, the enhancement of the resistivity maximum indicates a 20\\% increase of the Kondo temperature.","This opens the perspective to access the hidden zero-field QCP in pure YRS."],"url":"http://arxiv.org/abs/2402.00630v1","category":"cond-mat.str-el"}
{"created":"2024-02-01 14:39:59","title":"Bayesian Causal Inference with Gaussian Process Networks","abstract":"Causal discovery and inference from observational data is an essential problem in statistics posing both modeling and computational challenges. These are typically addressed by imposing strict assumptions on the joint distribution such as linearity. We consider the problem of the Bayesian estimation of the effects of hypothetical interventions in the Gaussian Process Network (GPN) model, a flexible causal framework which allows describing the causal relationships nonparametrically. We detail how to perform causal inference on GPNs by simulating the effect of an intervention across the whole network and propagating the effect of the intervention on downstream variables. We further derive a simpler computational approximation by estimating the intervention distribution as a function of local variables only, modeling the conditional distributions via additive Gaussian processes. We extend both frameworks beyond the case of a known causal graph, incorporating uncertainty about the causal structure via Markov chain Monte Carlo methods. Simulation studies show that our approach is able to identify the effects of hypothetical interventions with non-Gaussian, non-linear observational data and accurately reflect the posterior uncertainty of the causal estimates. Finally we compare the results of our GPN-based causal inference approach to existing methods on a dataset of $A.~thaliana$ gene expressions.","sentences":["Causal discovery and inference from observational data is an essential problem in statistics posing both modeling and computational challenges.","These are typically addressed by imposing strict assumptions on the joint distribution such as linearity.","We consider the problem of the Bayesian estimation of the effects of hypothetical interventions in the Gaussian Process Network (GPN) model, a flexible causal framework which allows describing the causal relationships nonparametrically.","We detail how to perform causal inference on GPNs by simulating the effect of an intervention across the whole network and propagating the effect of the intervention on downstream variables.","We further derive a simpler computational approximation by estimating the intervention distribution as a function of local variables only, modeling the conditional distributions via additive Gaussian processes.","We extend both frameworks beyond the case of a known causal graph, incorporating uncertainty about the causal structure via Markov chain Monte Carlo methods.","Simulation studies show that our approach is able to identify the effects of hypothetical interventions with non-Gaussian, non-linear observational data and accurately reflect the posterior uncertainty of the causal estimates.","Finally we compare the results of our GPN-based causal inference approach to existing methods on a dataset of $A.~thaliana$ gene expressions."],"url":"http://arxiv.org/abs/2402.00623v1","category":"stat.ML"}
{"created":"2024-02-01 13:33:41","title":"Structure determination of alkali trimers on helium nanodroplets through laser-induced Coulomb explosion","abstract":"Alkali trimers, Ak$_3$, located on the surface of He nanodroplets are triply ionized following multiphoton absorption from an intense femtosecond laser pulse leading to fragmentation into three correlated Ak$^+$ ions. Combining the information from three-fold covariance analysis of the emission direction of the fragment ions and from their kinetic energy distributions $P(E_{\\text{kin}})$, we find that Na$_3$, K$_3$, and Rb$_3$ have an equilateral triangular structure, corresponding to that of the lowest-lying quartet state $^{4}\\mathrm{A}_{2}'$, and determine the equilibrium bond distance $R_\\text{eq}$(Na$_3$) = 4.65 $\\pm$ 0.15 {\\AA}, $R_\\text{eq}$(K$_3$) = 5.03 $\\pm$ 0.18 {\\AA}, and $R_\\text{eq}$(Rb$_3$) = 5.45 $\\pm$ 0.22 {\\AA}. For K$_3$ and Rb$_3$ these values agree well with existing theoretical calculations, while for Na$_3$ the value is 0.2-0.3 {\\AA} larger than the existing theoretical results. The discrepancy is ascribed to a minor internuclear motion of Na$_3$ during the ionization process. Also, we determine the distribution of internuclear distances $P(R)$ under the assumption of fixed bond angles. The results are compared to the square of the internuclear wave function $|\\Psi(R)|^2$.","sentences":["Alkali trimers, Ak$_3$, located on the surface of He nanodroplets are triply ionized following multiphoton absorption from an intense femtosecond laser pulse leading to fragmentation into three correlated Ak$^+$ ions.","Combining the information from three-fold covariance analysis of the emission direction of the fragment ions and from their kinetic energy distributions $P(E_{\\text{kin}})$, we find that Na$_3$, K$_3$, and Rb$_3$ have an equilateral triangular structure, corresponding to that of the lowest-lying quartet state $^{4}\\mathrm{A}_{2}'$, and determine the equilibrium bond distance $R_\\text{eq}$(Na$_3$) = 4.65 $\\pm$ 0.15 {\\AA}, $R_\\text{eq}$(K$_3$) = 5.03 $\\pm$ 0.18 {\\AA}, and $R_\\text{eq}$(Rb$_3$) = 5.45 $\\pm$ 0.22 {\\AA}.","For K$_3$ and Rb$_3$ these values agree well with existing theoretical calculations, while for Na$_3$ the value is 0.2-0.3 {\\AA} larger than the existing theoretical results.","The discrepancy is ascribed to a minor internuclear motion of Na$_3$ during the ionization process.","Also, we determine the distribution of internuclear distances $P(R)$ under the assumption of fixed bond angles.","The results are compared to the square of the internuclear wave function $|\\Psi(R)|^2$."],"url":"http://arxiv.org/abs/2402.00587v1","category":"physics.atm-clus"}
{"created":"2024-02-01 13:31:54","title":"Arellano-Bond LASSO Estimator for Dynamic Linear Panel Models","abstract":"The Arellano-Bond estimator can be severely biased when the time series dimension of the data, $T$, is long. The source of the bias is the large degree of overidentification. We propose a simple two-step approach to deal with this problem. The first step applies LASSO to the cross-section data at each time period to select the most informative moment conditions. The second step applies a linear instrumental variable estimator using the instruments constructed from the moment conditions selected in the first step. The two stages are combined using sample-splitting and cross-fitting to avoid overfitting bias. Using asymptotic sequences where the two dimensions of the panel grow with the sample size, we show that the new estimator is consistent and asymptotically normal under much weaker conditions on $T$ than the Arellano-Bond estimator. Our theory covers models with high dimensional covariates including multiple lags of the dependent variable, which are common in modern applications. We illustrate our approach with an application to the short and long-term effects of the opening of K-12 schools and other policies on the spread of COVID-19 using weekly county-level panel data from the United States.","sentences":["The Arellano-Bond estimator can be severely biased when the time series dimension of the data, $T$, is long.","The source of the bias is the large degree of overidentification.","We propose a simple two-step approach to deal with this problem.","The first step applies LASSO to the cross-section data at each time period to select the most informative moment conditions.","The second step applies a linear instrumental variable estimator using the instruments constructed from the moment conditions selected in the first step.","The two stages are combined using sample-splitting and cross-fitting to avoid overfitting bias.","Using asymptotic sequences where the two dimensions of the panel grow with the sample size, we show that the new estimator is consistent and asymptotically normal under much weaker conditions on $T$ than the Arellano-Bond estimator.","Our theory covers models with high dimensional covariates including multiple lags of the dependent variable, which are common in modern applications.","We illustrate our approach with an application to the short and long-term effects of the opening of K-12 schools and other policies on the spread of COVID-19 using weekly county-level panel data from the United States."],"url":"http://arxiv.org/abs/2402.00584v1","category":"econ.EM"}
{"created":"2024-02-01 13:14:38","title":"Tropical Decision Boundaries for Neural Networks Are Robust Against Adversarial Attacks","abstract":"We introduce a simple, easy to implement, and computationally efficient tropical convolutional neural network architecture that is robust against adversarial attacks. We exploit the tropical nature of piece-wise linear neural networks by embedding the data in the tropical projective torus in a single hidden layer which can be added to any model. We study the geometry of its decision boundary theoretically and show its robustness against adversarial attacks on image datasets using computational experiments.","sentences":["We introduce a simple, easy to implement, and computationally efficient tropical convolutional neural network architecture that is robust against adversarial attacks.","We exploit the tropical nature of piece-wise linear neural networks by embedding the data in the tropical projective torus in a single hidden layer which can be added to any model.","We study the geometry of its decision boundary theoretically and show its robustness against adversarial attacks on image datasets using computational experiments."],"url":"http://arxiv.org/abs/2402.00576v1","category":"cs.LG"}
{"created":"2024-02-01 12:54:13","title":"Superconducting Noncontact Device for Precision Positioning in Cryogenic Environments","abstract":"In this paper, a noncontact linear positioner based on superconducting magnetic levitation for high-precision positioning has been tested under cryogenic conditions","sentences":["In this paper, a noncontact linear positioner based on superconducting magnetic levitation for high-precision positioning has been tested under cryogenic conditions"],"url":"http://arxiv.org/abs/2402.00566v1","category":"physics.app-ph"}
{"created":"2024-02-01 12:48:18","title":"Integrated photogrammetric-acoustic technique for qualitative analysis of the performance of acoustic screens in sandy soils","abstract":"In this work we present an integrated photogrammetric-acoustic technique that, together with the construction of a scaled wind tunnel, allows us to experimentally analyze the permeability behavior of a new type of acoustic screens based on a material called sonic crystal. Acoustic screens are devices used to reduce the noise, mostly due to the communication infrastructures, in its transmission phase from the source to the receiver. The main constructive difference between these new screens and the classic ones is that the first ones are formed by arrays of acoustic scatterers while the second ones are formed by continuous walls. This implies that, due to their geometry, screens based on sonic crystals are permeable to wind and water, unlike the classic ones. This fact may allow the use of these new screens in sandy soils, where sand would pass through the screen avoiding the formation of sand dunes that are formed in classic screens and drastically reduce their acoustic performance. In this work, the movement of the sand and the resulting acoustic attenuation in these new screens are analyzed qualitatively comparing the results with those obtained with the classic ones, obtaining interesting results under the acoustic point of view","sentences":["In this work we present an integrated photogrammetric-acoustic technique that, together with the construction of a scaled wind tunnel, allows us to experimentally analyze the permeability behavior of a new type of acoustic screens based on a material called sonic crystal.","Acoustic screens are devices used to reduce the noise, mostly due to the communication infrastructures, in its transmission phase from the source to the receiver.","The main constructive difference between these new screens and the classic ones is that the first ones are formed by arrays of acoustic scatterers while the second ones are formed by continuous walls.","This implies that, due to their geometry, screens based on sonic crystals are permeable to wind and water, unlike the classic ones.","This fact may allow the use of these new screens in sandy soils, where sand would pass through the screen avoiding the formation of sand dunes that are formed in classic screens and drastically reduce their acoustic performance.","In this work, the movement of the sand and the resulting acoustic attenuation in these new screens are analyzed qualitatively comparing the results with those obtained with the classic ones, obtaining interesting results under the acoustic point of view"],"url":"http://arxiv.org/abs/2402.00560v1","category":"physics.app-ph"}
{"created":"2024-02-01 12:29:15","title":"On ordered groups of regular growth rates","abstract":"We introduce an elementary class of linearly ordered groups, called growth order groups, encompassing certain groups under composition of formal series (e.g. transseries) as well as certain groups $\\mathcal{G}_{\\mathcal{M}}$ of infinitely large germs at infinity of unary functions definable in an o-minimal structure $\\mathcal{M}$. We study the algebraic structure of growth order groups and give methods for constructing examples. We show that if $\\mathcal{M}$ expands the real ordered field and germs in $\\mathcal{G}_{\\mathcal{M}}$ are levelled in the sense of Marker & Miller, then $\\mathcal{G}_{\\mathcal{M}}$ is a growth order group.","sentences":["We introduce an elementary class of linearly ordered groups, called growth order groups, encompassing certain groups under composition of formal series (e.g. transseries) as well as certain groups $\\mathcal{G}_{\\mathcal{M}}$ of infinitely large germs at infinity of unary functions definable in an o-minimal structure $\\mathcal{M}$. We study the algebraic structure of growth order groups and give methods for constructing examples.","We show that if $\\mathcal{M}$ expands the real ordered field and germs in $\\mathcal{G}_{\\mathcal{M}}$ are levelled in the sense of Marker & Miller, then $\\mathcal{G}_{\\mathcal{M}}$ is a growth order group."],"url":"http://arxiv.org/abs/2402.00549v1","category":"math.LO"}
{"created":"2024-02-01 12:28:45","title":"Plant sesquiterpene lactones","abstract":"Sesquiterpene lactones (STLs) are a prominent group of plant secondary metabolites predominantly found in the Asteraceae family and have multiple ecological roles and medicinal applications. This review describes the ecological significance of STLs, highlighting their roles in plant defense mechanisms against herbivory and as phytotoxins, alongside their function as environmental signaling molecules. We also cover the substantial role of STLs in medicine and their mode of action in health and disease. We discuss the biosynthetic pathways and the various modifications that make STLs one of the most diverse groups of metabolites. Finally, we discuss methods in identifying and predicting STL biosynthesis pathways.","sentences":["Sesquiterpene lactones (STLs) are a prominent group of plant secondary metabolites predominantly found in the Asteraceae family and have multiple ecological roles and medicinal applications.","This review describes the ecological significance of STLs, highlighting their roles in plant defense mechanisms against herbivory and as phytotoxins, alongside their function as environmental signaling molecules.","We also cover the substantial role of STLs in medicine and their mode of action in health and disease.","We discuss the biosynthetic pathways and the various modifications that make STLs one of the most diverse groups of metabolites.","Finally, we discuss methods in identifying and predicting STL biosynthesis pathways."],"url":"http://arxiv.org/abs/2402.00548v1","category":"q-bio.QM"}
{"created":"2024-02-01 12:12:54","title":"The extension of Pearson correlation coefficient, measuring noise, and selecting features","abstract":"Not a matter of serious contention, Pearson's correlation coefficient is still the most important statistical association measure. Restricted to just two variables, this measure sometimes doesn't live up to users' needs and expectations. Specifically, a multivariable version of the correlation coefficient can greatly contribute to better assessment of the risk in a multi-asset investment portfolio. Needless to say, the correlation coefficient is derived from another concept: covariance. Even though covariance can be extended naturally by its mathematical formula, such an extension is to no use. Making matters worse, the correlation coefficient can never be extended based on its mathematical definition. In this article, we briefly explore random matrix theory to extend the notion of Pearson's correlation coefficient to an arbitrary number of variables. Then, we show that how useful this measure is at gauging noise, thereby selecting features particularly in classification.","sentences":["Not a matter of serious contention, Pearson's correlation coefficient is still the most important statistical association measure.","Restricted to just two variables, this measure sometimes doesn't live up to users' needs and expectations.","Specifically, a multivariable version of the correlation coefficient can greatly contribute to better assessment of the risk in a multi-asset investment portfolio.","Needless to say, the correlation coefficient is derived from another concept: covariance.","Even though covariance can be extended naturally by its mathematical formula, such an extension is to no use.","Making matters worse, the correlation coefficient can never be extended based on its mathematical definition.","In this article, we briefly explore random matrix theory to extend the notion of Pearson's correlation coefficient to an arbitrary number of variables.","Then, we show that how useful this measure is at gauging noise, thereby selecting features particularly in classification."],"url":"http://arxiv.org/abs/2402.00543v1","category":"q-fin.MF"}
{"created":"2024-02-01 11:46:44","title":"StopThePop: Sorted Gaussian Splatting for View-Consistent Real-time Rendering","abstract":"Gaussian Splatting has emerged as a prominent model for constructing 3D representations from images across diverse domains. However, the efficiency of the 3D Gaussian Splatting rendering pipeline relies on several simplifications. Notably, reducing Gaussian to 2D splats with a single view-space depth introduces popping and blending artifacts during view rotation. Addressing this issue requires accurate per-pixel depth computation, yet a full per-pixel sort proves excessively costly compared to a global sort operation. In this paper, we present a novel hierarchical rasterization approach that systematically resorts and culls splats with minimal processing overhead. Our software rasterizer effectively eliminates popping artifacts and view inconsistencies, as demonstrated through both quantitative and qualitative measurements. Simultaneously, our method mitigates the potential for cheating view-dependent effects with popping, ensuring a more authentic representation. Despite the elimination of cheating, our approach achieves comparable quantitative results for test images, while increasing the consistency for novel view synthesis in motion. Due to its design, our hierarchical approach is only 4% slower on average than the original Gaussian Splatting. Notably, enforcing consistency enables a reduction in the number of Gaussians by approximately half with nearly identical quality and view-consistency. Consequently, rendering performance is nearly doubled, making our approach 1.6x faster than the original Gaussian Splatting, with a 50% reduction in memory requirements.","sentences":["Gaussian Splatting has emerged as a prominent model for constructing 3D representations from images across diverse domains.","However, the efficiency of the 3D Gaussian Splatting rendering pipeline relies on several simplifications.","Notably, reducing Gaussian to 2D splats with a single view-space depth introduces popping and blending artifacts during view rotation.","Addressing this issue requires accurate per-pixel depth computation, yet a full per-pixel sort proves excessively costly compared to a global sort operation.","In this paper, we present a novel hierarchical rasterization approach that systematically resorts and culls splats with minimal processing overhead.","Our software rasterizer effectively eliminates popping artifacts and view inconsistencies, as demonstrated through both quantitative and qualitative measurements.","Simultaneously, our method mitigates the potential for cheating view-dependent effects with popping, ensuring a more authentic representation.","Despite the elimination of cheating, our approach achieves comparable quantitative results for test images, while increasing the consistency for novel view synthesis in motion.","Due to its design, our hierarchical approach is only 4% slower on average than the original Gaussian Splatting.","Notably, enforcing consistency enables a reduction in the number of Gaussians by approximately half with nearly identical quality and view-consistency.","Consequently, rendering performance is nearly doubled, making our approach 1.6x faster than the original Gaussian Splatting, with a 50% reduction in memory requirements."],"url":"http://arxiv.org/abs/2402.00525v1","category":"cs.GR"}
{"created":"2024-02-01 11:46:41","title":"Witnessing non-Markovianity with Gaussian quantum steering in collision model","abstract":"The nonincreasing feature of temporal quantum steering under a completely positive trace-preserving (CPTP) map, as proposed by Chen, et al. in Phys. Rev. Lett. 116, 020503 (2016), has been considered as a practical measure of non-Markovianity. In this paper, we utilize an all-optical scheme to simulate a non-Markovian collision model and to examine how Gaussian steering can be used as a tool for quantifying the non-Markovianity of a structured continuous variable (CV) Gaussian channel. By modifying the reflectivity of the beam splitters (BSs), we are able to tune the degree of non-Markovianity of the channel. After analyzing the non-Markovian degree of the dissipative channel within two steering scenarios, we discovered that the Gaussian steering-based non-Markovian measure depends the specific scenario because of the asymmetry of Gaussian steering. We also compared the Gaussian steering based non-Markovianity to the one based on the violation of the divisibility of CPTP map.","sentences":["The nonincreasing feature of temporal quantum steering under a completely positive trace-preserving (CPTP) map, as proposed by Chen, et al. in Phys.","Rev. Lett.","116, 020503 (2016), has been considered as a practical measure of non-Markovianity.","In this paper, we utilize an all-optical scheme to simulate a non-Markovian collision model and to examine how Gaussian steering can be used as a tool for quantifying the non-Markovianity of a structured continuous variable (CV) Gaussian channel.","By modifying the reflectivity of the beam splitters (BSs), we are able to tune the degree of non-Markovianity of the channel.","After analyzing the non-Markovian degree of the dissipative channel within two steering scenarios, we discovered that the Gaussian steering-based non-Markovian measure depends the specific scenario because of the asymmetry of Gaussian steering.","We also compared the Gaussian steering based non-Markovianity to the one based on the violation of the divisibility of CPTP map."],"url":"http://arxiv.org/abs/2402.00524v1","category":"quant-ph"}
{"created":"2024-02-01 11:26:02","title":"A goodness-of-fit test for regression models with spatially correlated errors","abstract":"The problem of assessing a parametric regression model in the presence of spatial correlation is addressed in this work. For that purpose, a goodness-of-fit test based on a $L_2$-distance comparing a parametric and a nonparametric regression estimators is proposed. Asymptotic properties of the test statistic, both under the null hypothesis and under local alternatives, are derived. Additionally, a bootstrap procedure is designed to calibrate the test in practice. Finite sample performance of the test is analyzed through a simulation study, and its applicability is illustrated using a real data example.","sentences":["The problem of assessing a parametric regression model in the presence of spatial correlation is addressed in this work.","For that purpose, a goodness-of-fit test based on a $L_2$-distance comparing a parametric and a nonparametric regression estimators is proposed.","Asymptotic properties of the test statistic, both under the null hypothesis and under local alternatives, are derived.","Additionally, a bootstrap procedure is designed to calibrate the test in practice.","Finite sample performance of the test is analyzed through a simulation study, and its applicability is illustrated using a real data example."],"url":"http://arxiv.org/abs/2402.00512v1","category":"stat.ME"}
{"created":"2024-02-01 11:22:14","title":"Precise SMEFT predictions for di-Higgs production","abstract":"We present results of precision calculations for di-Higgs production that combine NLO QCD corrections with operators at canonical dimension six within Standard Model Effective Field Theory (SMEFT). We discuss possible options for operator contributions within a given EFT framework and sources of theory uncertainties.","sentences":["We present results of precision calculations for di-Higgs production that combine NLO QCD corrections with operators at canonical dimension six within Standard Model Effective Field Theory (SMEFT).","We discuss possible options for operator contributions within a given EFT framework and sources of theory uncertainties."],"url":"http://arxiv.org/abs/2402.00509v1","category":"hep-ph"}
{"created":"2024-02-01 11:17:15","title":"FLIMFLAM DR1: The First Constraints on the Cosmic Baryon Distribution from 8 FRB sightlines","abstract":"The dispersion measure of fast radio bursts (FRBs), arising from the interactions of the pulses with free electrons along the propagation path, constitutes a unique probe of the cosmic baryon distribution. Their constraining power is further enhanced in combination with observations of the foreground large-scale structure and intervening galaxies. In this work, we present the first constraints on the partition of the cosmic baryons between the intergalactic medium (IGM) and circumgalactic medium (CGM), inferred from the FLIMFLAM spectroscopic survey. In its first data release, the FLIMFLAM survey targeted galaxies in the foreground of 8 localized FRBs. Using Bayesian techniques, we reconstruct the underlying ~Mpc-scale matter density field that is traced by the IGM gas. Simultaneously, deeper spectroscopy of intervening foreground galaxies (at impact parameters $b_\\perp \\lesssim r_{200}$) and the FRB host galaxies constrains the contribution from the CGM. Applying Bayesian parameter inference to our data and assuming a fiducial set of priors, we infer the IGM cosmic baryon fraction to be $f_{\\rm igm}=0.59^{+0.11}_{-0.10}$, and a CGM gas fraction of $f_{\\rm gas} = 0.55^{+0.26}_{-0.29}$ for $10^{10}\\,M_\\odot \\lesssim M_{\\rm halo}\\lesssim 10^{13}\\,M_\\odot$ halos. The mean FRB host dispersion measure (rest-frame) in our sample is $\\langle \\rm{DM_{host}}\\rangle = 90^{+29}_{-19}\\rm{pc~cm^{-3}}$, of which $\\langle{\\rm DM_{host}^{unk}}\\rangle =69^{+28}_{-19}~\\rm{pc~cm^{-3}}$ arises from the host galaxy ISM and/or the FRB progenitor environment. While our current $f_{\\rm igm}$ and $f_{\\rm gas}$ uncertainties are too broad to constrain most galactic feedback models, this result marks the first measurement of the IGM and CGM baryon fractions, as well as the first systematic separation of the FRB host dispersion measure into two components: arising from the halo and from the inner ISM/FRB engine.","sentences":["The dispersion measure of fast radio bursts (FRBs), arising from the interactions of the pulses with free electrons along the propagation path, constitutes a unique probe of the cosmic baryon distribution.","Their constraining power is further enhanced in combination with observations of the foreground large-scale structure and intervening galaxies.","In this work, we present the first constraints on the partition of the cosmic baryons between the intergalactic medium (IGM) and circumgalactic medium (CGM), inferred from the FLIMFLAM spectroscopic survey.","In its first data release, the FLIMFLAM survey targeted galaxies in the foreground of 8 localized FRBs.","Using Bayesian techniques, we reconstruct the underlying ~Mpc-scale matter density field that is traced by the IGM gas.","Simultaneously, deeper spectroscopy of intervening foreground galaxies (at impact parameters $b_\\perp \\lesssim r_{200}$) and the FRB host galaxies constrains the contribution from the CGM.","Applying Bayesian parameter inference to our data and assuming a fiducial set of priors, we infer the IGM cosmic baryon fraction to be $f_{\\rm igm}=0.59^{+0.11}_{-0.10}$, and a CGM gas fraction of $f_{\\rm gas} = 0.55^{+0.26}_{-0.29}$ for $10^{10}\\,M_\\odot \\lesssim M_{\\rm halo}\\lesssim 10^{13}\\,M_\\odot$ halos.","The mean FRB host dispersion measure (rest-frame) in our sample is $\\langle \\rm{DM_{host}}\\rangle = 90^{+29}_{-19}\\rm{pc~cm^{-3}}$, of which $\\langle{\\rm DM_{host}^{unk}}\\rangle =69^{+28}_{-19}~\\rm{pc~cm^{-3}}$ arises from the host galaxy ISM and/or the FRB progenitor environment.","While our current $f_{\\rm igm}$ and $f_{\\rm gas}$ uncertainties are too broad to constrain most galactic feedback models, this result marks the first measurement of the IGM and CGM baryon fractions, as well as the first systematic separation of the FRB host dispersion measure into two components: arising from the halo and from the inner ISM/FRB engine."],"url":"http://arxiv.org/abs/2402.00505v1","category":"astro-ph.GA"}
{"created":"2024-02-01 11:12:00","title":"Equivalence of the Empirical Risk Minimization to Regularization on the Family of f-Divergences","abstract":"The solution to empirical risk minimization with $f$-divergence regularization (ERM-$f$DR) is presented under mild conditions on $f$. Under such conditions, the optimal measure is shown to be unique. Examples of the solution for particular choices of the function $f$ are presented. Previously known solutions to common regularization choices are obtained by leveraging the flexibility of the family of $f$-divergences. These include the unique solutions to empirical risk minimization with relative entropy regularization (Type-I and Type-II). The analysis of the solution unveils the following properties of $f$-divergences when used in the ERM-$f$DR problem: $i\\bigl)$ $f$-divergence regularization forces the support of the solution to coincide with the support of the reference measure, which introduces a strong inductive bias that dominates the evidence provided by the training data; and $ii\\bigl)$ any $f$-divergence regularization is equivalent to a different $f$-divergence regularization with an appropriate transformation of the empirical risk function.","sentences":["The solution to empirical risk minimization with $f$-divergence regularization (ERM-$f$DR) is presented under mild conditions on $f$. Under such conditions, the optimal measure is shown to be unique.","Examples of the solution for particular choices of the function $f$ are presented.","Previously known solutions to common regularization choices are obtained by leveraging the flexibility of the family of $f$-divergences.","These include the unique solutions to empirical risk minimization with relative entropy regularization (Type-I and Type-II).","The analysis of the solution unveils the following properties of $f$-divergences when used in the ERM-$f$DR problem: $i\\bigl)$ $f$-divergence regularization forces the support of the solution to coincide with the support of the reference measure, which introduces a strong inductive bias that dominates the evidence provided by the training data; and $ii\\bigl)$ any $f$-divergence regularization is equivalent to a different $f$-divergence regularization with an appropriate transformation of the empirical risk function."],"url":"http://arxiv.org/abs/2402.00501v1","category":"stat.ML"}
{"created":"2024-02-01 11:06:49","title":"On the nature of apparent transient sources on the National Geographic Society-Palomar Observatory Sky Survey glass copy plates","abstract":"We examine critically recent claims for the presence of above-atmosphere optical transients in publicly-available digitised scans of Schmidt telescope photographic plate material derived from the National Geographic Society-Palomar Observatory Sky Survey. We employ the publicly available SuperCOSMOS Sky Survey catalogues to examine statistically the morphology of the sources. We develop a simple, objective and automated image classification scheme based on a random forest decision tree classifier. We find that the putative transients are likely to be spurious artefacts of the photographic emulsion. We suggest a possible cause of the appearance of these images as resulting from the copying procedure employed to disseminate glass copy survey atlas sets in the era before large-scale digitisation programmes.","sentences":["We examine critically recent claims for the presence of above-atmosphere optical transients in publicly-available digitised scans of Schmidt telescope photographic plate material derived from the National Geographic Society-Palomar Observatory Sky Survey.","We employ the publicly available SuperCOSMOS Sky Survey catalogues to examine statistically the morphology of the sources.","We develop a simple, objective and automated image classification scheme based on a random forest decision tree classifier.","We find that the putative transients are likely to be spurious artefacts of the photographic emulsion.","We suggest a possible cause of the appearance of these images as resulting from the copying procedure employed to disseminate glass copy survey atlas sets in the era before large-scale digitisation programmes."],"url":"http://arxiv.org/abs/2402.00497v1","category":"astro-ph.IM"}
{"created":"2024-02-01 10:49:31","title":"Proton Pencil-Beam Scanning Stereotactic Body Radiation Therapy and Hypofractionated Radiation Therapy for Thoracic Malignancies: Patterns of Practice Survey and Recommendations for Future Development from NRG Oncology and PTCOG","abstract":"Stereotactic body radiation therapy (SBRT) and hypofractionation using pencil-beam scanning (PBS) proton therapy (PBSPT) is an attractive option for thoracic malignancies. Combining the advantages of target coverage conformity and critical organ sparing from both PBSPT and SBRT, this new delivery technique has great potential to improve the therapeutic ratio, particularly for tumors near critical organs. Safe and effective implementation of PBSPT SBRT/hypofractionation to treat thoracic malignancies is more challenging than the conventionally-fractionated PBSPT due to concerns of amplified uncertainties at the larger dose per fraction. NRG Oncology and Particle Therapy Cooperative Group (PTCOG) Thoracic Subcommittee surveyed US proton centers to identify practice patterns of thoracic PBSPT SBRT/hypofractionation. From these patterns, we present recommendations for future technical development of proton SBRT/hypofractionation for thoracic treatment. Amongst other points, the recommendations highlight the need for volumetric image guidance and multiple CT-based robust optimization and robustness tools to minimize further the impact of uncertainties associated with respiratory motion. Advances in direct motion analysis techniques are urgently needed to supplement current motion management techniques.","sentences":["Stereotactic body radiation therapy (SBRT) and hypofractionation using pencil-beam scanning (PBS) proton therapy (PBSPT) is an attractive option for thoracic malignancies.","Combining the advantages of target coverage conformity and critical organ sparing from both PBSPT and SBRT, this new delivery technique has great potential to improve the therapeutic ratio, particularly for tumors near critical organs.","Safe and effective implementation of PBSPT SBRT/hypofractionation to treat thoracic malignancies is more challenging than the conventionally-fractionated PBSPT due to concerns of amplified uncertainties at the larger dose per fraction.","NRG Oncology and Particle Therapy Cooperative Group (PTCOG) Thoracic Subcommittee surveyed US proton centers to identify practice patterns of thoracic PBSPT SBRT/hypofractionation.","From these patterns, we present recommendations for future technical development of proton SBRT/hypofractionation for thoracic treatment.","Amongst other points, the recommendations highlight the need for volumetric image guidance and multiple CT-based robust optimization and robustness tools to minimize further the impact of uncertainties associated with respiratory motion.","Advances in direct motion analysis techniques are urgently needed to supplement current motion management techniques."],"url":"http://arxiv.org/abs/2402.00489v1","category":"physics.med-ph"}
{"created":"2024-02-01 10:37:18","title":"Symmetric unisolvent equations for linear elasticity in pure stresses","abstract":"In this work we introduce novel stress-only formulations of linear elasticity with special attention to their approximate solution using weighted residual methods. We present four sets of boundary value problems for a pure stress formulation of three-dimensional solids, and in two dimensions for plane stress and plane strain. The associated governing equations are derived by modifications and combinations of the Beltrami-Michell equations and the Navier-Cauchy equations. The corresponding variational forms of dimension $d \\in \\{2,3\\}$ allow to directly approximate the stress tensor without any presupposed potential stress functions, and are shown to be well-posed in $\\mathit{H}^1 \\otimes \\mathrm{Sym}(d)$ in the framework of functional analysis via the Lax-Milgram theorem, making their finite element implementation using $\\mathit{C}^0$-continuous elements straightforward. Further, in the finite element setting we provide a treatment for constant and piece-wise constant body forces via distributions. The operators and differential identities in this work are provided in modern tensor notation and rely on exact sequences, making the resulting equations and differential relations directly comprehensible. Finally, numerical benchmarks for convergence as well as spectral analysis are used to test the limits and identify viable use-cases of the equations.","sentences":["In this work we introduce novel stress-only formulations of linear elasticity with special attention to their approximate solution using weighted residual methods.","We present four sets of boundary value problems for a pure stress formulation of three-dimensional solids, and in two dimensions for plane stress and plane strain.","The associated governing equations are derived by modifications and combinations of the Beltrami-Michell equations and the Navier-Cauchy equations.","The corresponding variational forms of dimension $d \\in \\{2,3\\}$ allow to directly approximate the stress tensor without any presupposed potential stress functions, and are shown to be well-posed in $\\mathit{H}^1 \\otimes \\mathrm{Sym}(d)$ in the framework of functional analysis via the Lax-Milgram theorem, making their finite element implementation using $\\mathit{C}^0$-continuous elements straightforward.","Further, in the finite element setting we provide a treatment for constant and piece-wise constant body forces via distributions.","The operators and differential identities in this work are provided in modern tensor notation and rely on exact sequences, making the resulting equations and differential relations directly comprehensible.","Finally, numerical benchmarks for convergence as well as spectral analysis are used to test the limits and identify viable use-cases of the equations."],"url":"http://arxiv.org/abs/2402.00480v1","category":"math.NA"}
{"created":"2024-02-01 10:04:25","title":"Electric field tunable edge transport in Bernal stacked trilayer graphene","abstract":"This letter presents a non-local study on the electric field tunable edge transport in an hBN-encapsulated dual-gated Bernal stacked (ABA) trilayer graphene across various displacement fields ($D$) and temperatures ($T$). Our measurements revealed that the non-local resistance ($R_{NL}$) surpassed the expected classical ohmic contribution by a factor of at least two orders of magnitude. Through scaling analysis, we found that the non-local resistance scales linearly with the local resistance ($R_{L}$) only when the $D$ exceeds a critical value of $\\sim0.2$ V/nm. Additionally, we observed that the scaling exponent remains constant at unity for temperatures below the bulk-band gap energy threshold ($T<25$ K). Further, the value of $R_{NL}$ decreases in a linear fashion as the channel length ($L$) increases. These experimental findings provide evidence for edge-mediated charge transport in ABA trilayer graphene under the influence of a finite displacement field. Furthermore, our theoretical calculations support these results by demonstrating the emergence of dispersive edge modes within the bulk-band gap energy range when a sufficient displacement field is applied.","sentences":["This letter presents a non-local study on the electric field tunable edge transport in an hBN-encapsulated dual-gated Bernal stacked (ABA) trilayer graphene across various displacement fields ($D$) and temperatures ($T$).","Our measurements revealed that the non-local resistance ($R_{NL}$) surpassed the expected classical ohmic contribution by a factor of at least two orders of magnitude.","Through scaling analysis, we found that the non-local resistance scales linearly with the local resistance ($R_{L}$) only when the $D$ exceeds a critical value of $\\sim0.2$ V/nm.","Additionally, we observed that the scaling exponent remains constant at unity for temperatures below the bulk-band gap energy threshold ($T<25$ K).","Further, the value of $R_{NL}$ decreases in a linear fashion as the channel length ($L$) increases.","These experimental findings provide evidence for edge-mediated charge transport in ABA trilayer graphene under the influence of a finite displacement field.","Furthermore, our theoretical calculations support these results by demonstrating the emergence of dispersive edge modes within the bulk-band gap energy range when a sufficient displacement field is applied."],"url":"http://arxiv.org/abs/2402.00461v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-01 09:44:44","title":"Analyzing Crowdfunding of Public Projects Under Dynamic Beliefs","abstract":"In the last decade, social planners have used crowdfunding to raise funds for public projects. As these public projects are non-excludable, the beneficiaries may free-ride. Thus, there is a need to design incentive mechanisms for such strategic agents to contribute to the project. The existing mechanisms, like PPR or PPRx, assume that the agent's beliefs about the project getting funded do not change over time, i.e., their beliefs are static. Researchers highlight that unless appropriately incentivized, the agents defer their contributions in static settings, leading to a ``race'' to contribute at the deadline. In this work, we model the evolution of agents' beliefs as a random walk. We study PPRx -- an existing mechanism for the static belief setting -- in this dynamic belief setting and refer to it as PPRx-DB for readability. We prove that in PPRx-DB, the project is funded at equilibrium. More significantly, we prove that under certain conditions on agent's belief evolution, agents will contribute as soon as they arrive at the mechanism. Thus, we believe that by incorporating dynamic belief evolution in analysis, the social planner can mitigate the concern of race conditions in many mechanisms.","sentences":["In the last decade, social planners have used crowdfunding to raise funds for public projects.","As these public projects are non-excludable, the beneficiaries may free-ride.","Thus, there is a need to design incentive mechanisms for such strategic agents to contribute to the project.","The existing mechanisms, like PPR or PPRx, assume that the agent's beliefs about the project getting funded do not change over time, i.e., their beliefs are static.","Researchers highlight that unless appropriately incentivized, the agents defer their contributions in static settings, leading to a ``race'' to contribute at the deadline.","In this work, we model the evolution of agents' beliefs as a random walk.","We study PPRx -- an existing mechanism for the static belief setting -- in this dynamic belief setting and refer to it as PPRx-DB for readability.","We prove that in PPRx-DB, the project is funded at equilibrium.","More significantly, we prove that under certain conditions on agent's belief evolution, agents will contribute as soon as they arrive at the mechanism.","Thus, we believe that by incorporating dynamic belief evolution in analysis, the social planner can mitigate the concern of race conditions in many mechanisms."],"url":"http://arxiv.org/abs/2402.00454v1","category":"cs.GT"}
{"created":"2024-02-01 09:43:30","title":"Instruction Makes a Difference","abstract":"We introduce Instruction Document Visual Question Answering (iDocVQA) dataset and Large Language Document (LLaDoc) model, for training Language-Vision (LV) models for document analysis and predictions on document images, respectively. Usually, deep neural networks for the DocVQA task are trained on datasets lacking instructions. We show that using instruction-following datasets improves performance. We compare performance across document-related datasets using the recent state-of-the-art (SotA) Large Language and Vision Assistant (LLaVA)1.5 as the base model. We also evaluate the performance of the derived models for object hallucination using the Polling-based Object Probing Evaluation (POPE) dataset. The results show that instruction-tuning performance ranges from 11X to 32X of zero-shot performance and from 0.1% to 4.2% over non-instruction (traditional task) finetuning. Despite the gains, these still fall short of human performance (94.36%), implying there's much room for improvement.","sentences":["We introduce Instruction Document Visual Question Answering (iDocVQA) dataset and Large Language Document (LLaDoc) model, for training Language-Vision (LV) models for document analysis and predictions on document images, respectively.","Usually, deep neural networks for the DocVQA task are trained on datasets lacking instructions.","We show that using instruction-following datasets improves performance.","We compare performance across document-related datasets using the recent state-of-the-art (SotA) Large Language and Vision Assistant (LLaVA)1.5 as the base model.","We also evaluate the performance of the derived models for object hallucination using the Polling-based Object Probing Evaluation (POPE) dataset.","The results show that instruction-tuning performance ranges from 11X to 32X of zero-shot performance and from 0.1% to 4.2% over non-instruction (traditional task) finetuning.","Despite the gains, these still fall short of human performance (94.36%), implying there's much room for improvement."],"url":"http://arxiv.org/abs/2402.00453v1","category":"cs.CV"}
{"created":"2024-02-01 09:18:34","title":"Evaluating Genetic Algorithms through the Approximability Hierarchy","abstract":"Optimization problems frequently appear in any scientific domain. Most of the times, the corresponding decision problem turns out to be NP-hard, and in these cases genetic algorithms are often used to obtain approximated solutions. However, the difficulty to approximate different NP-hard problems can vary a lot. In this paper, we analyze the usefulness of using genetic algorithms depending on the approximation class the problem belongs to. In particular, we use the standard approximability hierarchy, showing that genetic algorithms are especially useful for the most pessimistic classes of the hierarchy","sentences":["Optimization problems frequently appear in any scientific domain.","Most of the times, the corresponding decision problem turns out to be NP-hard, and in these cases genetic algorithms are often used to obtain approximated solutions.","However, the difficulty to approximate different NP-hard problems can vary a lot.","In this paper, we analyze the usefulness of using genetic algorithms depending on the approximation class the problem belongs to.","In particular, we use the standard approximability hierarchy, showing that genetic algorithms are especially useful for the most pessimistic classes of the hierarchy"],"url":"http://arxiv.org/abs/2402.00444v1","category":"cs.NE"}
{"created":"2024-02-01 09:12:45","title":"Optimal investment, consumption and life insurance decisions for households with consumption habits under the health shock risk","abstract":"This paper investigates the optimal investment, consumption, and life insurance strategies for households under the impact of health shock risk. Considering the uncertainty of the future health status of family members, a non-homogeneous Markov process is used to model the health status of the breadwinner. Drawing upon the theory of habit formation, we investigate the influence of different consumption habits on households' investment, consumption, and life insurance strategies. Based on whether the breadwinner is alive or not, we formulate and solve the corresponding Hamilton-Jacobi-Bellman (HJB) equations for the two scenarios of breadwinner survival and breadwinner's demise, respectively, and obtain explicit expressions for the optimal investment, consumption, and life insurance strategies. Through sensitivity analysis, it has been shown that the presence of health shocks within households has a negative impact on investment and consumption decisions, while the formation of consumption habits increases household propensity for precautionary savings.","sentences":["This paper investigates the optimal investment, consumption, and life insurance strategies for households under the impact of health shock risk.","Considering the uncertainty of the future health status of family members, a non-homogeneous Markov process is used to model the health status of the breadwinner.","Drawing upon the theory of habit formation, we investigate the influence of different consumption habits on households' investment, consumption, and life insurance strategies.","Based on whether the breadwinner is alive or not, we formulate and solve the corresponding Hamilton-Jacobi-Bellman (HJB) equations for the two scenarios of breadwinner survival and breadwinner's demise, respectively, and obtain explicit expressions for the optimal investment, consumption, and life insurance strategies.","Through sensitivity analysis, it has been shown that the presence of health shocks within households has a negative impact on investment and consumption decisions, while the formation of consumption habits increases household propensity for precautionary savings."],"url":"http://arxiv.org/abs/2402.00440v1","category":"stat.AP"}
{"created":"2024-02-01 08:45:28","title":"Reproducibility of Build Environments through Space and Time","abstract":"Modern software engineering builds up on the composability of software components, that rely on more and more direct and transitive dependencies to build their functionalities. This principle of reusability however makes it harder to reproduce projects' build environments, even though reproducibility of build environments is essential for collaboration, maintenance and component lifetime. In this work, we argue that functional package managers provide the tooling to make build environments reproducible in space and time, and we produce a preliminary evaluation to justify this claim. Using historical data, we show that we are able to reproduce build environments of about 7 million Nix packages, and to rebuild 99.94% of the 14 thousand packages from a 6-year-old Nixpkgs revision.","sentences":["Modern software engineering builds up on the composability of software components, that rely on more and more direct and transitive dependencies to build their functionalities.","This principle of reusability however makes it harder to reproduce projects' build environments, even though reproducibility of build environments is essential for collaboration, maintenance and component lifetime.","In this work, we argue that functional package managers provide the tooling to make build environments reproducible in space and time, and we produce a preliminary evaluation to justify this claim.","Using historical data, we show that we are able to reproduce build environments of about 7 million Nix packages, and to rebuild 99.94% of the 14 thousand packages from a 6-year-old Nixpkgs revision."],"url":"http://arxiv.org/abs/2402.00424v1","category":"cs.SE"}
{"created":"2024-02-01 07:48:26","title":"Higgs boson pair production and decay at NLO in QCD: the $b\\bar{b}\u03b3\u03b3$ final state","abstract":"The Higgs boson pair production at the LHC provides a probe to the Higgs boson self-coupling. The higher-order QCD corrections in this process are sizable and must be taken into account in comparison with data. Due to the small cross section, it is necessary to consider at least one of the Higgs bosons decaying to bottom quarks. The QCD corrections to the decay processes would also be important in such cases. We present a full calculation of the total and differential cross sections for the $b\\bar{b}\\gamma\\gamma$ final state with next-to-leading order (NLO) QCD corrections. After applying typical kinematic cuts in the final state, we find that QCD NLO corrections in the decay decrease the LO result by $19\\%$ and reduce the scale uncertainties by a factor of two. The QCD corrections to the invariant mass $m_{jj\\gamma\\gamma}$ distribution, the transverse momentum spectra of the leading bottom quark jet and photon are significant and can not be approximated by a constant factor.","sentences":["The Higgs boson pair production at the LHC provides a probe to the Higgs boson self-coupling.","The higher-order QCD corrections in this process are sizable and must be taken into account in comparison with data.","Due to the small cross section, it is necessary to consider at least one of the Higgs bosons decaying to bottom quarks.","The QCD corrections to the decay processes would also be important in such cases.","We present a full calculation of the total and differential cross sections for the $b\\bar{b}\\gamma\\gamma$ final state with next-to-leading order (NLO) QCD corrections.","After applying typical kinematic cuts in the final state, we find that QCD NLO corrections in the decay decrease the LO result by $19\\%$ and reduce the scale uncertainties by a factor of two.","The QCD corrections to the invariant mass $m_{jj\\gamma\\gamma}$ distribution, the transverse momentum spectra of the leading bottom quark jet and photon are significant and can not be approximated by a constant factor."],"url":"http://arxiv.org/abs/2402.00401v1","category":"hep-ph"}
{"created":"2024-02-01 07:25:44","title":"Dirac or Majorana: Higgs-$Z^\\prime$ probe to investigate neutrino nature","abstract":"We propose a novel approach to determine one of the most important properties of neutrinos, namely, a Dirac or Majorana particle in a scenario with a new $U(1)_X$ gauge symmetry. In this scenario, three right-handed neutrinos are inevitably introduced due to the gauge anomaly cancellations. We find that the decay branching ratio of the discovered Higgs boson into a pair of new massive gauge bosons ($Z'$) can significantly be enhanced in the Dirac neutrino case as compared with the Majorana case for a fixed value of the new gauge coupling and the mass of $Z'$ under constraints from current experimental data. Because of such an enhancement, the Dirac case can indirectly be tested via the Higgs decay in addition to the absence of neutrinoless double-beta decays.","sentences":["We propose a novel approach to determine one of the most important properties of neutrinos, namely, a Dirac or Majorana particle in a scenario with a new $U(1)_X$ gauge symmetry.","In this scenario, three right-handed neutrinos are inevitably introduced due to the gauge anomaly cancellations.","We find that the decay branching ratio of the discovered Higgs boson into a pair of new massive gauge bosons ($Z'$) can significantly be enhanced in the Dirac neutrino case as compared with the Majorana case for a fixed value of the new gauge coupling and the mass of $Z'$ under constraints from current experimental data.","Because of such an enhancement, the Dirac case can indirectly be tested via the Higgs decay in addition to the absence of neutrinoless double-beta decays."],"url":"http://arxiv.org/abs/2402.00392v1","category":"hep-ph"}
{"created":"2024-02-01 07:15:16","title":"Superconductivity in $\\textit{a}$-MoGe thin films: effect of phase fluctuations with decreasing thickness and study of vortex dynamics in presence of low-frequency ac excitation","abstract":"In this thesis, we have studied the evolution of superconductivity in amorphous Molybdenum Germanium ($\\textit{a}$-MoGe) thin films. The work can be broken down into two parts. In the first part, we investigate the effect of decreasing thickness on the suppression of superconductivity in $\\textit{a}$-MoGe thin films. Thick $\\textit{a}$-MoGe thin film is a typical type-II superconductor and follows the conventional Bardeen-Cooper-Schrieffer (BCS) equation. Conventionally, it is believed that decreasing thickness will decrease the effective attractive pairing interaction because of the gradual loss of screening which holds true for large thicknesses in $\\textit{a}$-MoGe. But for lower thicknesses, a new mechanism comes into the picture where superfluid density is suppressed making the superconductor vulnerable to phase fluctuations. This is known as the Bosonic mechanism, where superconductivity is destroyed due to the loss of phase coherence of the superconducting state even though the attractive pairing amplitude remains finite above the transition temperature. In the second part, we explore the electromagnetic response of vortices in $\\textit{a}$-MoGe thin film by low-frequency two-coil mutual inductance technique. Penetration depth measured from the two-coil technique was earlier used to determine superfluid density. However in the present work, by analyzing the in-field penetration depth data with the help of a mean-field model proposed by Coffey and Clem, we have demonstrated a procedure of extraction of vortex parameters such as pinning restoring force constant or Labusch parameter, vortex lattice drag coefficient and pinning potential barrier for the thermally activated motion of vortices. The temperature variation of vortex parameters suggests the dominant effect of thermal fluctuations.","sentences":["In this thesis, we have studied the evolution of superconductivity in amorphous Molybdenum Germanium ($\\textit{a}$-MoGe) thin films.","The work can be broken down into two parts.","In the first part, we investigate the effect of decreasing thickness on the suppression of superconductivity in $\\textit{a}$-MoGe thin films.","Thick $\\textit{a}$-MoGe thin film is a typical type-II superconductor and follows the conventional Bardeen-Cooper-Schrieffer (BCS) equation.","Conventionally, it is believed that decreasing thickness will decrease the effective attractive pairing interaction because of the gradual loss of screening which holds true for large thicknesses in $\\textit{a}$-MoGe.","But for lower thicknesses, a new mechanism comes into the picture where superfluid density is suppressed making the superconductor vulnerable to phase fluctuations.","This is known as the Bosonic mechanism, where superconductivity is destroyed due to the loss of phase coherence of the superconducting state even though the attractive pairing amplitude remains finite above the transition temperature.","In the second part, we explore the electromagnetic response of vortices in $\\textit{a}$-MoGe thin film by low-frequency two-coil mutual inductance technique.","Penetration depth measured from the two-coil technique was earlier used to determine superfluid density.","However in the present work, by analyzing the in-field penetration depth data with the help of a mean-field model proposed by Coffey and Clem, we have demonstrated a procedure of extraction of vortex parameters such as pinning restoring force constant or Labusch parameter, vortex lattice drag coefficient and pinning potential barrier for the thermally activated motion of vortices.","The temperature variation of vortex parameters suggests the dominant effect of thermal fluctuations."],"url":"http://arxiv.org/abs/2402.00387v1","category":"cond-mat.supr-con"}
{"created":"2024-02-01 07:05:45","title":"Computational Morphology and Lexicography Modeling of Modern Standard Arabic Nominals","abstract":"Modern Standard Arabic (MSA) nominals present many morphological and lexical modeling challenges that have not been consistently addressed previously. This paper attempts to define the space of such challenges, and leverage a recently proposed morphological framework to build a comprehensive and extensible model for MSA nominals. Our model design addresses the nominals' intricate morphotactics, as well as their paradigmatic irregularities. Our implementation showcases enhanced accuracy and consistency compared to a commonly used MSA morphological analyzer and generator. We make our models publicly available.","sentences":["Modern Standard Arabic (MSA) nominals present many morphological and lexical modeling challenges that have not been consistently addressed previously.","This paper attempts to define the space of such challenges, and leverage a recently proposed morphological framework to build a comprehensive and extensible model for MSA nominals.","Our model design addresses the nominals' intricate morphotactics, as well as their paradigmatic irregularities.","Our implementation showcases enhanced accuracy and consistency compared to a commonly used MSA morphological analyzer and generator.","We make our models publicly available."],"url":"http://arxiv.org/abs/2402.00385v1","category":"cs.CL"}
{"created":"2024-02-01 07:04:48","title":"Schur rings over Free Abelian Group of Rank Two","abstract":"Schur rings are a type of subrings of group rings afforded by a partition of the underlined group. In this paper, Schur rings over free abelian group of rank two are classified under the assumption that one of the direct factor is a union of some basic sets. There are eight different types, and all but one type of which are traditional.","sentences":["Schur rings are a type of subrings of group rings afforded by a partition of the underlined group.","In this paper, Schur rings over free abelian group of rank two are classified under the assumption that one of the direct factor is a union of some basic sets.","There are eight different types, and all but one type of which are traditional."],"url":"http://arxiv.org/abs/2402.00383v1","category":"math.GR"}
{"created":"2024-02-01 06:52:33","title":"Kurdyka-\u0141ojasiewicz exponent via Hadamard parametrization","abstract":"We consider a class of $\\ell_1$-regularized optimization problems and the associated smooth ``over-parameterized\" optimization problems built upon the Hadamard parametrization, or equivalently, the Hadamard difference parametrization (HDP). We characterize the set of second-order stationary points of the HDP-based model and show that they correspond to some stationary points of the corresponding $\\ell_1$-regularized model. More importantly, we show that the Kurdyka-Lojasiewicz (KL) exponent of the HDP-based model at a second-order stationary point can be inferred from that of the corresponding $\\ell_1$-regularized model under suitable assumptions. Our assumptions are general enough to cover a wide variety of loss functions commonly used in $\\ell_1$-regularized models, such as the least squares loss function and the logistic loss function. Since the KL exponents of many $\\ell_1$-regularized models are explicitly known in the literature, our results allow us to leverage these known exponents to deduce the KL exponents at second-order stationary points of the corresponding HDP-based models, which were previously unknown. Finally, we demonstrate how these explicit KL exponents at second-order stationary points can be applied to deducing the explicit local convergence rate of a standard gradient descent method for solving the HDP-based model.","sentences":["We consider a class of $\\ell_1$-regularized optimization problems and the associated smooth ``over-parameterized\" optimization problems built upon the Hadamard parametrization, or equivalently, the Hadamard difference parametrization (HDP).","We characterize the set of second-order stationary points of the HDP-based model and show that they correspond to some stationary points of the corresponding $\\ell_1$-regularized model.","More importantly, we show that the Kurdyka-Lojasiewicz (KL) exponent of the HDP-based model at a second-order stationary point can be inferred from that of the corresponding $\\ell_1$-regularized model under suitable assumptions.","Our assumptions are general enough to cover a wide variety of loss functions commonly used in $\\ell_1$-regularized models, such as the least squares loss function and the logistic loss function.","Since the KL exponents of many $\\ell_1$-regularized models are explicitly known in the literature, our results allow us to leverage these known exponents to deduce the KL exponents at second-order stationary points of the corresponding HDP-based models, which were previously unknown.","Finally, we demonstrate how these explicit KL exponents at second-order stationary points can be applied to deducing the explicit local convergence rate of a standard gradient descent method for solving the HDP-based model."],"url":"http://arxiv.org/abs/2402.00377v1","category":"math.OC"}
{"created":"2024-02-01 06:13:13","title":"Emergence of tension-compression asymmetry from a complete phase-field approach to brittle fracture","abstract":"The classical variational approach to brittle fracture propagation does not distinguish between strain energy accumulation in tension versus compression and consequently results in physically unrealistic cracking under compression. A variety of energy splits have been proposed as a possible remedy. However, a unique energy split that can describe this asymmetry for general loading conditions has not been found. The main objective of this paper is to show that a complete phase-field theory of brittle fracture nucleation and propagation, one that accounts for the material strength at large, can naturally capture the tension-compression asymmetry without an energy split. One such theory has been recently proposed by Kumar et al. (2018). Over the past few years, several studies have shown that this theory is capable of accurately describing fracture nucleation and propagation for materials soft and hard under arbitrary monotonic loading conditions. However, a systematic study of the tension-compression asymmetry that emerges from this theory has not yet been reported. This paper does precisely that. In particular, this paper reports a comprehensive study of crack propagation in two problems, one involving a symmetric tension-compression state and the other involving larger compressive stresses at the crack tip. The results are compared with popular energy splits used in literature. The results show that, remarkably, for the second problem, only the complete theory is able to produce experimentally consistent results.","sentences":["The classical variational approach to brittle fracture propagation does not distinguish between strain energy accumulation in tension versus compression and consequently results in physically unrealistic cracking under compression.","A variety of energy splits have been proposed as a possible remedy.","However, a unique energy split that can describe this asymmetry for general loading conditions has not been found.","The main objective of this paper is to show that a complete phase-field theory of brittle fracture nucleation and propagation, one that accounts for the material strength at large, can naturally capture the tension-compression asymmetry without an energy split.","One such theory has been recently proposed by Kumar et al.","(2018).","Over the past few years, several studies have shown that this theory is capable of accurately describing fracture nucleation and propagation for materials soft and hard under arbitrary monotonic loading conditions.","However, a systematic study of the tension-compression asymmetry that emerges from this theory has not yet been reported.","This paper does precisely that.","In particular, this paper reports a comprehensive study of crack propagation in two problems, one involving a symmetric tension-compression state and the other involving larger compressive stresses at the crack tip.","The results are compared with popular energy splits used in literature.","The results show that, remarkably, for the second problem, only the complete theory is able to produce experimentally consistent results."],"url":"http://arxiv.org/abs/2402.00368v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-01 04:50:30","title":"Chiral $QED_2$ with Faddeevian anomaly in the context of the augmented superfield approach","abstract":"We consider the bosonized version of the Chiral Schwinger model in $(1+1)$ dimension with the generalized Faddeevian anomaly, which does not have the Lorentz covariance structure and does not have gauge invariance either. BRST embedding is made possible after making it gauge invariant by the incorporation of Wess-Zumino field. For this $(1+1)$ dimensional anomalous model, we use the Bonora-Tonin superfield formalism to construct the nilpotent and absolutely anti-commuting anti-BRST as well as anti-co-BRST symmetry transformations. We use the gauge-invariant constraints on the superfields defined onto the (2, 2)-Dimensional supermanifold along with the dual horizontality criteria. We provide the conserved charges linked to the aforementioned nilpotent symmetries as well as their geometric interpretation. The anti-BRST and anti-co-BRST charges' nilpotency and total anticommutativity. It has also been confirmed that, in the context of the augmented superfield formalism, the anti-BRST and anti-co-BRST charges are nilpotent and absolutely anti-commutative. One notable aspect of the current study is the application of the dual-horizontality requirement to obtain appropriate anti-co-BRST symmetry","sentences":["We consider the bosonized version of the Chiral Schwinger model in $(1+1)$ dimension with the generalized Faddeevian anomaly, which does not have the Lorentz covariance structure and does not have gauge invariance either.","BRST embedding is made possible after making it gauge invariant by the incorporation of Wess-Zumino field.","For this $(1+1)$ dimensional anomalous model, we use the Bonora-Tonin superfield formalism to construct the nilpotent and absolutely anti-commuting anti-BRST as well as anti-co-BRST symmetry transformations.","We use the gauge-invariant constraints on the superfields defined onto the (2, 2)-Dimensional supermanifold along with the dual horizontality criteria.","We provide the conserved charges linked to the aforementioned nilpotent symmetries as well as their geometric interpretation.","The anti-BRST and anti-co-BRST charges' nilpotency and total anticommutativity",".","It has also been confirmed that, in the context of the augmented superfield formalism, the anti-BRST and anti-co-BRST charges are nilpotent and absolutely anti-commutative.","One notable aspect of the current study is the application of the dual-horizontality requirement to obtain appropriate anti-co-BRST symmetry"],"url":"http://arxiv.org/abs/2402.00338v1","category":"hep-th"}
{"created":"2024-02-01 04:17:15","title":"A Consistent Lebesgue Measure for Multi-label Learning","abstract":"Multi-label loss functions are usually non-differentiable, requiring surrogate loss functions for gradient-based optimisation. The consistency of surrogate loss functions is not proven and is exacerbated by the conflicting nature of multi-label loss functions. To directly learn from multiple related, yet potentially conflicting multi-label loss functions, we propose a Consistent Lebesgue Measure-based Multi-label Learner (CLML) and prove that CLML can achieve theoretical consistency under a Bayes risk framework. Empirical evidence supports our theory by demonstrating that: (1) CLML can consistently achieve state-of-the-art results; (2) the primary performance factor is the Lebesgue measure design, as CLML optimises a simpler feedforward model without additional label graph, perturbation-based conditioning, or semantic embeddings; and (3) an analysis of the results not only distinguishes CLML's effectiveness but also highlights inconsistencies between the surrogate and the desired loss functions.","sentences":["Multi-label loss functions are usually non-differentiable, requiring surrogate loss functions for gradient-based optimisation.","The consistency of surrogate loss functions is not proven and is exacerbated by the conflicting nature of multi-label loss functions.","To directly learn from multiple related, yet potentially conflicting multi-label loss functions, we propose a Consistent Lebesgue Measure-based Multi-label Learner (CLML) and prove that CLML can achieve theoretical consistency under a Bayes risk framework.","Empirical evidence supports our theory by demonstrating that: (1) CLML can consistently achieve state-of-the-art results; (2) the primary performance factor is the Lebesgue measure design, as CLML optimises a simpler feedforward model without additional label graph, perturbation-based conditioning, or semantic embeddings; and (3) an analysis of the results not only distinguishes CLML's effectiveness but also highlights inconsistencies between the surrogate and the desired loss functions."],"url":"http://arxiv.org/abs/2402.00324v1","category":"cs.LG"}
{"created":"2024-02-01 04:02:06","title":"Chiral superconducting diode effect by Dzyaloshinsky-Moriya interaction","abstract":"A two-component quasi-two-dimensional superconductor with Dzyaloshinsky-Moriya interaction is studied based on the Ginzburg-Landau and Bogoliubov-de Gennes theories. Under external in-plane magnetic fields, the order parameter of the superconducting state is a type of the Fulde-Ferrell state with a finite momentum of Cooper pairs due to the Dzyaloshinsky-Moriya interaction. It is shown that the superconducting diode effect can emerge when a supercurrent flows parallel to the external magnetic field, characteristic of chiral crystals. In the Bogoliubov-de Gennes theory, phase diagrams associated with the transition of the Cooper-pair momentum and the Josephson phase between spin-singlet and spin-triplet Cooper pairs are derived, and a close relationship with the diode quality factor is demonstrated. Implications of critical currents in the aspect of thermodynamics are also discussed. Based on such an argument, it is argued that the first-order phase transition in terms of Cooper-pair momentum and the coexistence of phases with different Cooper-pair momentum and Josephson phase can occur. The argument also implies the issue with the definition of critical currents calculated from the extremes of the supercurrent when metastable states exist. Comments on purely two-dimensional superconductors are also given.","sentences":["A two-component quasi-two-dimensional superconductor with Dzyaloshinsky-Moriya interaction is studied based on the Ginzburg-Landau and Bogoliubov-de Gennes theories.","Under external in-plane magnetic fields, the order parameter of the superconducting state is a type of the Fulde-Ferrell state with a finite momentum of Cooper pairs due to the Dzyaloshinsky-Moriya interaction.","It is shown that the superconducting diode effect can emerge when a supercurrent flows parallel to the external magnetic field, characteristic of chiral crystals.","In the Bogoliubov-de Gennes theory, phase diagrams associated with the transition of the Cooper-pair momentum and the Josephson phase between spin-singlet and spin-triplet Cooper pairs are derived, and a close relationship with the diode quality factor is demonstrated.","Implications of critical currents in the aspect of thermodynamics are also discussed.","Based on such an argument, it is argued that the first-order phase transition in terms of Cooper-pair momentum and the coexistence of phases with different Cooper-pair momentum and Josephson phase can occur.","The argument also implies the issue with the definition of critical currents calculated from the extremes of the supercurrent when metastable states exist.","Comments on purely two-dimensional superconductors are also given."],"url":"http://arxiv.org/abs/2402.00317v1","category":"cond-mat.supr-con"}
{"created":"2024-02-01 03:56:48","title":"Online Distribution Learning with Local Private Constraints","abstract":"We study the problem of online conditional distribution estimation with \\emph{unbounded} label sets under local differential privacy. Let $\\mathcal{F}$ be a distribution-valued function class with unbounded label set. We aim at estimating an \\emph{unknown} function $f\\in \\mathcal{F}$ in an online fashion so that at time $t$ when the context $\\boldsymbol{x}_t$ is provided we can generate an estimate of $f(\\boldsymbol{x}_t)$ under KL-divergence knowing only a privatized version of the true labels sampling from $f(\\boldsymbol{x}_t)$. The ultimate objective is to minimize the cumulative KL-risk of a finite horizon $T$. We show that under $(\\epsilon,0)$-local differential privacy of the privatized labels, the KL-risk grows as $\\tilde{\\Theta}(\\frac{1}{\\epsilon}\\sqrt{KT})$ upto poly-logarithmic factors where $K=|\\mathcal{F}|$. This is in stark contrast to the $\\tilde{\\Theta}(\\sqrt{T\\log K})$ bound demonstrated by Wu et al. (2023a) for bounded label sets. As a byproduct, our results recover a nearly tight upper bound for the hypothesis selection problem of gopi et al. (2020) established only for the batch setting.","sentences":["We study the problem of online conditional distribution estimation with \\emph{unbounded} label sets under local differential privacy.","Let $\\mathcal{F}$ be a distribution-valued function class with unbounded label set.","We aim at estimating an \\emph{unknown} function $f\\in \\mathcal{F}$ in an online fashion so that at time $t$ when the context $\\boldsymbol{x}_t$ is provided we can generate an estimate of $f(\\boldsymbol{x}_t)$ under KL-divergence knowing only a privatized version of the true labels sampling from $f(\\boldsymbol{x}_t)$. The ultimate objective is to minimize the cumulative KL-risk of a finite horizon $T$. We show that under $(\\epsilon,0)$-local differential privacy of the privatized labels, the KL-risk grows as $\\tilde{\\Theta}(\\frac{1}{\\epsilon}\\sqrt{KT})$ upto poly-logarithmic factors where $K=|\\mathcal{F}|$. This is in stark contrast to the $\\tilde{\\Theta}(\\sqrt{T\\log K})$ bound demonstrated by Wu et al. (2023a) for bounded label sets.","As a byproduct, our results recover a nearly tight upper bound for the hypothesis selection problem of gopi et al.","(2020) established only for the batch setting."],"url":"http://arxiv.org/abs/2402.00315v1","category":"cs.LG"}
{"created":"2024-02-01 02:13:49","title":"Guided Interpretable Facial Expression Recognition via Spatial Action Unit Cues","abstract":"While state-of-the-art facial expression recognition (FER) classifiers achieve a high level of accuracy, they lack interpretability, an important aspect for end-users. To recognize basic facial expressions, experts resort to a codebook associating a set of spatial action units to a facial expression. In this paper, we follow the same expert footsteps, and propose a learning strategy that allows us to explicitly incorporate spatial action units (aus) cues into the classifier's training to build a deep interpretable model. In particular, using this aus codebook, input image expression label, and facial landmarks, a single action units heatmap is built to indicate the most discriminative regions of interest in the image w.r.t the facial expression. We leverage this valuable spatial cue to train a deep interpretable classifier for FER. This is achieved by constraining the spatial layer features of a classifier to be correlated with \\aus map. Using a composite loss, the classifier is trained to correctly classify an image while yielding interpretable visual layer-wise attention correlated with aus maps, simulating the experts' decision process. This is achieved using only the image class expression as supervision and without any extra manual annotations. Moreover, our method is generic. It can be applied to any CNN- or transformer-based deep classifier without the need for architectural change or adding significant training time. Our extensive evaluation on two public benchmarks RAFDB, and AFFECTNET datasets shows that our proposed strategy can improve layer-wise interpretability without degrading classification performance. In addition, we explore a common type of interpretable classifiers that rely on Class-Activation Mapping methods (CAMs), and we show that our training technique improves the CAM interpretability.","sentences":["While state-of-the-art facial expression recognition (FER) classifiers achieve a high level of accuracy, they lack interpretability, an important aspect for end-users.","To recognize basic facial expressions, experts resort to a codebook associating a set of spatial action units to a facial expression.","In this paper, we follow the same expert footsteps, and propose a learning strategy that allows us to explicitly incorporate spatial action units (aus) cues into the classifier's training to build a deep interpretable model.","In particular, using this aus codebook, input image expression label, and facial landmarks, a single action units heatmap is built to indicate the most discriminative regions of interest in the image w.r.t the facial expression.","We leverage this valuable spatial cue to train a deep interpretable classifier for FER.","This is achieved by constraining the spatial layer features of a classifier to be correlated with \\aus map.","Using a composite loss, the classifier is trained to correctly classify an image while yielding interpretable visual layer-wise attention correlated with aus maps, simulating the experts' decision process.","This is achieved using only the image class expression as supervision and without any extra manual annotations.","Moreover, our method is generic.","It can be applied to any CNN- or transformer-based deep classifier without the need for architectural change or adding significant training time.","Our extensive evaluation on two public benchmarks RAFDB, and AFFECTNET datasets shows that our proposed strategy can improve layer-wise interpretability without degrading classification performance.","In addition, we explore a common type of interpretable classifiers that rely on Class-Activation Mapping methods (CAMs), and we show that our training technique improves the CAM interpretability."],"url":"http://arxiv.org/abs/2402.00281v1","category":"cs.CV"}
{"created":"2024-02-01 01:39:09","title":"Not All Learnable Distribution Classes are Privately Learnable","abstract":"We give an example of a class of distributions that is learnable in total variation distance with a finite number of samples, but not learnable under $(\\varepsilon, \\delta)$-differential privacy. This refutes a conjecture of Ashtiani.","sentences":["We give an example of a class of distributions that is learnable in total variation distance with a finite number of samples, but not learnable under $(\\varepsilon, \\delta)$-differential privacy.","This refutes a conjecture of Ashtiani."],"url":"http://arxiv.org/abs/2402.00267v1","category":"cs.DS"}
{"created":"2024-02-01 01:06:32","title":"Multi-group Learning for Hierarchical Groups","abstract":"The multi-group learning model formalizes the learning scenario in which a single predictor must generalize well on multiple, possibly overlapping subgroups of interest. We extend the study of multi-group learning to the natural case where the groups are hierarchically structured. We design an algorithm for this setting that outputs an interpretable and deterministic decision tree predictor with near-optimal sample complexity. We then conduct an empirical evaluation of our algorithm and find that it achieves attractive generalization properties on real datasets with hierarchical group structure.","sentences":["The multi-group learning model formalizes the learning scenario in which a single predictor must generalize well on multiple, possibly overlapping subgroups of interest.","We extend the study of multi-group learning to the natural case where the groups are hierarchically structured.","We design an algorithm for this setting that outputs an interpretable and deterministic decision tree predictor with near-optimal sample complexity.","We then conduct an empirical evaluation of our algorithm and find that it achieves attractive generalization properties on real datasets with hierarchical group structure."],"url":"http://arxiv.org/abs/2402.00258v1","category":"cs.LG"}
{"created":"2024-02-01 01:00:50","title":"The parity-odd structure function of nucleon from the Compton amplitude","abstract":"The dominant contribution to the theoretical uncertainty in the extracted weak parameters of the Standard Model comes from the hadronic uncertainties in the electroweak boxes, i.e. $\\gamma-W^\\pm/Z$ exchange diagrams. A dispersive analysis relates the box diagrams to the parity-odd structure function, $F_3$, for which the experimental data either do not exist or belong to a separate isospin channel. Therefore a first-principles calculation of $F_3$ is highly desirable. In this contribution, we report on the QCDSF/UKQCD Collaboration's progress in calculating the moments of the $F_3^{\\gamma Z}$ structure function from the forward Compton amplitude at the SU(3) symmetric point.","sentences":["The dominant contribution to the theoretical uncertainty in the extracted weak parameters of the Standard Model comes from the hadronic uncertainties in the electroweak boxes, i.e. $\\gamma-W^\\pm/Z$ exchange diagrams.","A dispersive analysis relates the box diagrams to the parity-odd structure function, $F_3$, for which the experimental data either do not exist or belong to a separate isospin channel.","Therefore a first-principles calculation of $F_3$ is highly desirable.","In this contribution, we report on the QCDSF/UKQCD Collaboration's progress in calculating the moments of the $F_3^{\\gamma Z}$ structure function from the forward Compton amplitude at the SU(3) symmetric point."],"url":"http://arxiv.org/abs/2402.00255v1","category":"hep-lat"}
{"created":"2024-02-01 00:19:57","title":"LRDif: Diffusion Models for Under-Display Camera Emotion Recognition","abstract":"This study introduces LRDif, a novel diffusion-based framework designed specifically for facial expression recognition (FER) within the context of under-display cameras (UDC). To address the inherent challenges posed by UDC's image degradation, such as reduced sharpness and increased noise, LRDif employs a two-stage training strategy that integrates a condensed preliminary extraction network (FPEN) and an agile transformer network (UDCformer) to effectively identify emotion labels from UDC images. By harnessing the robust distribution mapping capabilities of Diffusion Models (DMs) and the spatial dependency modeling strength of transformers, LRDif effectively overcomes the obstacles of noise and distortion inherent in UDC environments. Comprehensive experiments on standard FER datasets including RAF-DB, KDEF, and FERPlus, LRDif demonstrate state-of-the-art performance, underscoring its potential in advancing FER applications. This work not only addresses a significant gap in the literature by tackling the UDC challenge in FER but also sets a new benchmark for future research in the field.","sentences":["This study introduces LRDif, a novel diffusion-based framework designed specifically for facial expression recognition (FER) within the context of under-display cameras (UDC).","To address the inherent challenges posed by UDC's image degradation, such as reduced sharpness and increased noise, LRDif employs a two-stage training strategy that integrates a condensed preliminary extraction network (FPEN) and an agile transformer network (UDCformer) to effectively identify emotion labels from UDC images.","By harnessing the robust distribution mapping capabilities of Diffusion Models (DMs) and the spatial dependency modeling strength of transformers, LRDif effectively overcomes the obstacles of noise and distortion inherent in UDC environments.","Comprehensive experiments on standard FER datasets including RAF-DB, KDEF, and FERPlus, LRDif demonstrate state-of-the-art performance, underscoring its potential in advancing FER applications.","This work not only addresses a significant gap in the literature by tackling the UDC challenge in FER but also sets a new benchmark for future research in the field."],"url":"http://arxiv.org/abs/2402.00250v1","category":"cs.CV"}
{"created":"2024-02-01 00:06:59","title":"A Diffusion-Based Approach for Simulating Forward-in-Time State-Dependent Speciation and Extinction Dynamics","abstract":"We establish a general framework using a diffusion approximation to simulate forward-in-time state counts or frequencies for cladogenetic state-dependent speciation-extinction (ClaSSE) models. We apply the framework to various two- and three-region geographic-state speciation-extinction (GeoSSE) models. We show that the species range state dynamics simulated under tree-based and diffusion-based processes are comparable. We derive a method to infer rate parameters that are compatible with given observed stationary state frequencies and obtain an analytical result to compute stationary state frequencies for a given set of rate parameters. We also describe a procedure to find the time to reach the stationary frequencies of a ClaSSE model using our diffusion-based approach, which we demonstrate using a worked example for a two-region GeoSSE model. Finally, we discuss how the diffusion framework can be applied to formalize relationships between evolutionary patterns and processes under state-dependent diversification scenarios.","sentences":["We establish a general framework using a diffusion approximation to simulate forward-in-time state counts or frequencies for cladogenetic state-dependent speciation-extinction (ClaSSE) models.","We apply the framework to various two- and three-region geographic-state speciation-extinction (GeoSSE) models.","We show that the species range state dynamics simulated under tree-based and diffusion-based processes are comparable.","We derive a method to infer rate parameters that are compatible with given observed stationary state frequencies and obtain an analytical result to compute stationary state frequencies for a given set of rate parameters.","We also describe a procedure to find the time to reach the stationary frequencies of a ClaSSE model using our diffusion-based approach, which we demonstrate using a worked example for a two-region GeoSSE model.","Finally, we discuss how the diffusion framework can be applied to formalize relationships between evolutionary patterns and processes under state-dependent diversification scenarios."],"url":"http://arxiv.org/abs/2402.00246v1","category":"q-bio.PE"}
{"created":"2024-01-31 23:47:04","title":"Publication bias adjustment in network meta-analysis: an inverse probability weighting approach using clinical trial registries","abstract":"Network meta-analysis (NMA) is a useful tool to compare multiple interventions simultaneously in a single meta-analysis, it can be very helpful for medical decision making when the study aims to find the best therapy among several active candidates. However, the validity of its results is threatened by the publication bias issue. Existing methods to handle the publication bias issue in the standard pairwise meta-analysis are hard to extend to this area with the complicated data structure and the underlying assumptions for pooling the data. In this paper, we aimed to provide a flexible inverse probability weighting (IPW) framework along with several t-type selection functions to deal with the publication bias problem in the NMA context. To solve these proposed selection functions, we recommend making use of the additional information from the unpublished studies from multiple clinical trial registries. A comprehensive numerical study and a real example showed that our methodology can help obtain more accurate estimates and higher coverage probabilities, and improve other properties of an NMA (e.g., ranking the interventions).","sentences":["Network meta-analysis (NMA) is a useful tool to compare multiple interventions simultaneously in a single meta-analysis, it can be very helpful for medical decision making when the study aims to find the best therapy among several active candidates.","However, the validity of its results is threatened by the publication bias issue.","Existing methods to handle the publication bias issue in the standard pairwise meta-analysis are hard to extend to this area with the complicated data structure and the underlying assumptions for pooling the data.","In this paper, we aimed to provide a flexible inverse probability weighting (IPW) framework along with several t-type selection functions to deal with the publication bias problem in the NMA context.","To solve these proposed selection functions, we recommend making use of the additional information from the unpublished studies from multiple clinical trial registries.","A comprehensive numerical study and a real example showed that our methodology can help obtain more accurate estimates and higher coverage probabilities, and improve other properties of an NMA (e.g., ranking the interventions)."],"url":"http://arxiv.org/abs/2402.00239v1","category":"stat.ME"}
{"created":"2024-02-01 18:59:56","title":"We're Not Using Videos Effectively: An Updated Domain Adaptive Video Segmentation Baseline","abstract":"There has been abundant work in unsupervised domain adaptation for semantic segmentation (DAS) seeking to adapt a model trained on images from a labeled source domain to an unlabeled target domain. While the vast majority of prior work has studied this as a frame-level Image-DAS problem, a few Video-DAS works have sought to additionally leverage the temporal signal present in adjacent frames. However, Video-DAS works have historically studied a distinct set of benchmarks from Image-DAS, with minimal cross-benchmarking. In this work, we address this gap. Surprisingly, we find that (1) even after carefully controlling for data and model architecture, state-of-the-art Image-DAS methods (HRDA and HRDA+MIC)} outperform Video-DAS methods on established Video-DAS benchmarks (+14.5 mIoU on Viper$\\rightarrow$CityscapesSeq, +19.0 mIoU on Synthia$\\rightarrow$CityscapesSeq), and (2) naive combinations of Image-DAS and Video-DAS techniques only lead to marginal improvements across datasets. To avoid siloed progress between Image-DAS and Video-DAS, we open-source our codebase with support for a comprehensive set of Video-DAS and Image-DAS methods on a common benchmark. Code available at https://github.com/SimarKareer/UnifiedVideoDA","sentences":["There has been abundant work in unsupervised domain adaptation for semantic segmentation (DAS) seeking to adapt a model trained on images from a labeled source domain to an unlabeled target domain.","While the vast majority of prior work has studied this as a frame-level Image-DAS problem, a few Video-DAS works have sought to additionally leverage the temporal signal present in adjacent frames.","However, Video-DAS works have historically studied a distinct set of benchmarks from Image-DAS, with minimal cross-benchmarking.","In this work, we address this gap.","Surprisingly, we find that (1) even after carefully controlling for data and model architecture, state-of-the-art Image-DAS methods (HRDA and HRDA+MIC)} outperform Video-DAS methods on established Video-DAS benchmarks (+14.5 mIoU on Viper$\\rightarrow$CityscapesSeq, +19.0 mIoU on Synthia$\\rightarrow$CityscapesSeq), and (2) naive combinations of Image-DAS and Video-DAS techniques only lead to marginal improvements across datasets.","To avoid siloed progress between Image-DAS and Video-DAS, we open-source our codebase with support for a comprehensive set of Video-DAS and Image-DAS methods on a common benchmark.","Code available at https://github.com/SimarKareer/UnifiedVideoDA"],"url":"http://arxiv.org/abs/2402.00868v1","category":"cs.CV"}
{"created":"2024-02-01 15:13:26","title":"Coherent Feed Forward Quantum Neural Network","abstract":"Quantum machine learning, focusing on quantum neural networks (QNNs), remains a vastly uncharted field of study. Current QNN models primarily employ variational circuits on an ansatz or a quantum feature map, often requiring multiple entanglement layers. This methodology not only increases the computational cost of the circuit beyond what is practical on near-term quantum devices but also misleadingly labels these models as neural networks, given their divergence from the structure of a typical feed-forward neural network (FFNN). Moreover, the circuit depth and qubit needs of these models scale poorly with the number of data features, resulting in an efficiency challenge for real-world machine-learning tasks. We introduce a bona fide QNN model, which seamlessly aligns with the versatility of a traditional FFNN in terms of its adaptable intermediate layers and nodes, absent from intermediate measurements such that our entire model is coherent. This model stands out with its reduced circuit depth and number of requisite C-NOT gates to outperform prevailing QNN models. Furthermore, the qubit count in our model remains unaffected by the data's feature quantity. We test our proposed model on various benchmarking datasets such as the diagnostic breast cancer (Wisconsin) and credit card fraud detection datasets. We compare the outcomes of our model with the existing QNN methods to showcase the advantageous efficacy of our approach, even with a reduced requirement on quantum resources. Our model paves the way for application of quantum neural networks to real relevant machine learning problems.","sentences":["Quantum machine learning, focusing on quantum neural networks (QNNs), remains a vastly uncharted field of study.","Current QNN models primarily employ variational circuits on an ansatz or a quantum feature map, often requiring multiple entanglement layers.","This methodology not only increases the computational cost of the circuit beyond what is practical on near-term quantum devices but also misleadingly labels these models as neural networks, given their divergence from the structure of a typical feed-forward neural network (FFNN).","Moreover, the circuit depth and qubit needs of these models scale poorly with the number of data features, resulting in an efficiency challenge for real-world machine-learning tasks.","We introduce a bona fide QNN model, which seamlessly aligns with the versatility of a traditional FFNN in terms of its adaptable intermediate layers and nodes, absent from intermediate measurements such that our entire model is coherent.","This model stands out with its reduced circuit depth and number of requisite C-NOT gates to outperform prevailing QNN models.","Furthermore, the qubit count in our model remains unaffected by the data's feature quantity.","We test our proposed model on various benchmarking datasets such as the diagnostic breast cancer (Wisconsin) and credit card fraud detection datasets.","We compare the outcomes of our model with the existing QNN methods to showcase the advantageous efficacy of our approach, even with a reduced requirement on quantum resources.","Our model paves the way for application of quantum neural networks to real relevant machine learning problems."],"url":"http://arxiv.org/abs/2402.00653v1","category":"quant-ph"}
{"created":"2024-02-01 14:44:35","title":"Cylindrically symmetric diffusion model for relativistic heavy-ion collisions","abstract":"A relativistic diffusion model with cylindrical symmetry, which propagates an initial state based on quantum chromodynamics in time towards a thermal equilibrium limit, is derived from nonequilibrium-statistical considerations: Adapting an existing framework for Markovian stochastic processes representing relativistic phase-space trajectories, a Fokker-Planck equation is obtained for the time evolution of particle-number distribution functions with respect to transverse and longitudinal rapidity. The resulting partially-evolved distribution functions are transformed to transverse-momentum and pseudorapidity space, and compared with charged-hadron data from the CERN Large Hadron Collider (LHC).","sentences":["A relativistic diffusion model with cylindrical symmetry, which propagates an initial state based on quantum chromodynamics in time towards a thermal equilibrium limit, is derived from nonequilibrium-statistical considerations: Adapting an existing framework for Markovian stochastic processes representing relativistic phase-space trajectories, a Fokker-Planck equation is obtained for the time evolution of particle-number distribution functions with respect to transverse and longitudinal rapidity.","The resulting partially-evolved distribution functions are transformed to transverse-momentum and pseudorapidity space, and compared with charged-hadron data from the CERN Large Hadron Collider (LHC)."],"url":"http://arxiv.org/abs/2402.00628v1","category":"hep-ph"}
{"created":"2024-02-01 14:39:39","title":"Gain of Grain: A Film Grain Handling Toolchain for VVC-based Open Implementations","abstract":"Film grain is a distinctive visual characteristic cherished by filmmakers and cinephiles for its ability to evoke nostalgia and artistic aesthetics. However, faithful preservation of film grain during encoding poses unique challenges. Film grain introduces random noise, complicating traditional compression techniques. Consequently, specialized algorithms and encoding strategies have emerged, aiming to strike a harmonious equilibrium. This paper delves into the nuanced realm of film grain handling in Versatile Video Coding (VVC) encoding. We explore the delicate balance between retaining the cinematic charm of film grain and achieving efficient compression. Moreover, we discuss the importance of perceptual quality assessment and adaptive encoding techniques in preserving film grain fidelity. Additionally, we delve into the impact of film grain handling on bitrate control and compression efficiency using VVenC, an open and optimized VVC encoder. Understanding the role of film grain and its nuanced treatment within encoders becomes increasingly pivotal for delivering high-quality, grain-inclusive content in the digital age.","sentences":["Film grain is a distinctive visual characteristic cherished by filmmakers and cinephiles for its ability to evoke nostalgia and artistic aesthetics.","However, faithful preservation of film grain during encoding poses unique challenges.","Film grain introduces random noise, complicating traditional compression techniques.","Consequently, specialized algorithms and encoding strategies have emerged, aiming to strike a harmonious equilibrium.","This paper delves into the nuanced realm of film grain handling in Versatile Video Coding (VVC) encoding.","We explore the delicate balance between retaining the cinematic charm of film grain and achieving efficient compression.","Moreover, we discuss the importance of perceptual quality assessment and adaptive encoding techniques in preserving film grain fidelity.","Additionally, we delve into the impact of film grain handling on bitrate control and compression efficiency using VVenC, an open and optimized VVC encoder.","Understanding the role of film grain and its nuanced treatment within encoders becomes increasingly pivotal for delivering high-quality, grain-inclusive content in the digital age."],"url":"http://arxiv.org/abs/2402.00622v1","category":"cs.MM"}
{"created":"2024-02-01 06:11:49","title":"Don't Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration","abstract":"Despite efforts to expand the knowledge of large language models (LLMs), knowledge gaps -- missing or outdated information in LLMs -- might always persist given the evolving nature of knowledge. In this work, we study approaches to identify LLM knowledge gaps and abstain from answering questions when knowledge gaps are present. We first adapt existing approaches to model calibration or adaptation through fine-tuning/prompting and analyze their ability to abstain from generating low-confidence outputs. Motivated by their failures in self-reflection and over-reliance on held-out sets, we propose two novel approaches that are based on model collaboration, i.e., LLMs probing other LLMs for knowledge gaps, either cooperatively or competitively. Extensive experiments with three LLMs on four QA tasks featuring diverse knowledge domains demonstrate that both cooperative and competitive approaches to unveiling LLM knowledge gaps achieve up to 19.3% improvements on abstain accuracy against the strongest baseline. Further analysis reveals that our proposed mechanisms could help identify failure cases in retrieval augmentation and pinpoint knowledge gaps in multi-hop reasoning.","sentences":["Despite efforts to expand the knowledge of large language models (LLMs), knowledge gaps -- missing or outdated information in LLMs -- might always persist given the evolving nature of knowledge.","In this work, we study approaches to identify LLM knowledge gaps and abstain from answering questions when knowledge gaps are present.","We first adapt existing approaches to model calibration or adaptation through fine-tuning/prompting and analyze their ability to abstain from generating low-confidence outputs.","Motivated by their failures in self-reflection and over-reliance on held-out sets, we propose two novel approaches that are based on model collaboration, i.e., LLMs probing other LLMs for knowledge gaps, either cooperatively or competitively.","Extensive experiments with three LLMs on four QA tasks featuring diverse knowledge domains demonstrate that both cooperative and competitive approaches to unveiling LLM knowledge gaps achieve up to 19.3% improvements on abstain accuracy against the strongest baseline.","Further analysis reveals that our proposed mechanisms could help identify failure cases in retrieval augmentation and pinpoint knowledge gaps in multi-hop reasoning."],"url":"http://arxiv.org/abs/2402.00367v1","category":"cs.CL"}
{"created":"2024-02-01 05:20:07","title":"IndiVec: An Exploration of Leveraging Large Language Models for Media Bias Detection with Fine-Grained Bias Indicators","abstract":"This study focuses on media bias detection, crucial in today's era of influential social media platforms shaping individual attitudes and opinions. In contrast to prior work that primarily relies on training specific models tailored to particular datasets, resulting in limited adaptability and subpar performance on out-of-domain data, we introduce a general bias detection framework, IndiVec, built upon large language models. IndiVec begins by constructing a fine-grained media bias database, leveraging the robust instruction-following capabilities of large language models and vector database techniques. When confronted with new input for bias detection, our framework automatically selects the most relevant indicator from the vector database and employs majority voting to determine the input's bias label. IndiVec excels compared to previous methods due to its adaptability (demonstrating consistent performance across diverse datasets from various sources) and explainability (providing explicit top-k indicators to interpret bias predictions). Experimental results on four political bias datasets highlight IndiVec's significant superiority over baselines. Furthermore, additional experiments and analysis provide profound insights into the framework's effectiveness.","sentences":["This study focuses on media bias detection, crucial in today's era of influential social media platforms shaping individual attitudes and opinions.","In contrast to prior work that primarily relies on training specific models tailored to particular datasets, resulting in limited adaptability and subpar performance on out-of-domain data, we introduce a general bias detection framework, IndiVec, built upon large language models.","IndiVec begins by constructing a fine-grained media bias database, leveraging the robust instruction-following capabilities of large language models and vector database techniques.","When confronted with new input for bias detection, our framework automatically selects the most relevant indicator from the vector database and employs majority voting to determine the input's bias label.","IndiVec excels compared to previous methods due to its adaptability (demonstrating consistent performance across diverse datasets from various sources) and explainability (providing explicit top-k indicators to interpret bias predictions).","Experimental results on four political bias datasets highlight IndiVec's significant superiority over baselines.","Furthermore, additional experiments and analysis provide profound insights into the framework's effectiveness."],"url":"http://arxiv.org/abs/2402.00345v1","category":"cs.CL"}
{"created":"2024-02-01 04:47:12","title":"Real-time Stereo Speech Enhancement with Spatial-Cue Preservation based on Dual-Path Structure","abstract":"We introduce a real-time, multichannel speech enhancement algorithm which maintains the spatial cues of stereo recordings including two speech sources. Recognizing that each source has unique spatial information, our method utilizes a dual-path structure, ensuring the spatial cues remain unaffected during enhancement by applying source-specific common-band gain. This method also seamlessly integrates pretrained monaural speech enhancement, eliminating the need for retraining on stereo inputs. Source separation from stereo mixtures is achieved via spatial beamforming, with the steering vector for each source being adaptively updated using post-enhancement output signal. This ensures accurate tracking of the spatial information. The final stereo output is derived by merging the spatial images of the enhanced sources, with its efficacy not heavily reliant on the separation performance of the beamforming. The algorithm runs in real-time on 10-ms frames with a 40 ms of look-ahead. Evaluations reveal its effectiveness in enhancing speech and preserving spatial cues in both fully and sparsely overlapped mixtures.","sentences":["We introduce a real-time, multichannel speech enhancement algorithm which maintains the spatial cues of stereo recordings including two speech sources.","Recognizing that each source has unique spatial information, our method utilizes a dual-path structure, ensuring the spatial cues remain unaffected during enhancement by applying source-specific common-band gain.","This method also seamlessly integrates pretrained monaural speech enhancement, eliminating the need for retraining on stereo inputs.","Source separation from stereo mixtures is achieved via spatial beamforming, with the steering vector for each source being adaptively updated using post-enhancement output signal.","This ensures accurate tracking of the spatial information.","The final stereo output is derived by merging the spatial images of the enhanced sources, with its efficacy not heavily reliant on the separation performance of the beamforming.","The algorithm runs in real-time on 10-ms frames with a 40 ms of look-ahead.","Evaluations reveal its effectiveness in enhancing speech and preserving spatial cues in both fully and sparsely overlapped mixtures."],"url":"http://arxiv.org/abs/2402.00337v1","category":"eess.AS"}
{"created":"2024-02-01 04:35:37","title":"Comparing Spectral Bias and Robustness For Two-Layer Neural Networks: SGD vs Adaptive Random Fourier Features","abstract":"We present experimental results highlighting two key differences resulting from the choice of training algorithm for two-layer neural networks. The spectral bias of neural networks is well known, while the spectral bias dependence on the choice of training algorithm is less studied. Our experiments demonstrate that an adaptive random Fourier features algorithm (ARFF) can yield a spectral bias closer to zero compared to the stochastic gradient descent optimizer (SGD). Additionally, we train two identically structured classifiers, employing SGD and ARFF, to the same accuracy levels and empirically assess their robustness against adversarial noise attacks.","sentences":["We present experimental results highlighting two key differences resulting from the choice of training algorithm for two-layer neural networks.","The spectral bias of neural networks is well known, while the spectral bias dependence on the choice of training algorithm is less studied.","Our experiments demonstrate that an adaptive random Fourier features algorithm (ARFF) can yield a spectral bias closer to zero compared to the stochastic gradient descent optimizer (SGD).","Additionally, we train two identically structured classifiers, employing SGD and ARFF, to the same accuracy levels and empirically assess their robustness against adversarial noise attacks."],"url":"http://arxiv.org/abs/2402.00332v1","category":"cs.LG"}
{"created":"2024-02-01 04:15:59","title":"Bias in Opinion Summarisation from Pre-training to Adaptation: A Case Study in Political Bias","abstract":"Opinion summarisation aims to summarise the salient information and opinions presented in documents such as product reviews, discussion forums, and social media texts into short summaries that enable users to effectively understand the opinions therein. Generating biased summaries has the risk of potentially swaying public opinion. Previous studies focused on studying bias in opinion summarisation using extractive models, but limited research has paid attention to abstractive summarisation models. In this study, using political bias as a case study, we first establish a methodology to quantify bias in abstractive models, then trace it from the pre-trained models to the task of summarising social media opinions using different models and adaptation methods. We find that most models exhibit intrinsic bias. Using a social media text summarisation dataset and contrasting various adaptation methods, we find that tuning a smaller number of parameters is less biased compared to standard fine-tuning; however, the diversity of topics in training data used for fine-tuning is critical.","sentences":["Opinion summarisation aims to summarise the salient information and opinions presented in documents such as product reviews, discussion forums, and social media texts into short summaries that enable users to effectively understand the opinions therein.","Generating biased summaries has the risk of potentially swaying public opinion.","Previous studies focused on studying bias in opinion summarisation using extractive models, but limited research has paid attention to abstractive summarisation models.","In this study, using political bias as a case study, we first establish a methodology to quantify bias in abstractive models, then trace it from the pre-trained models to the task of summarising social media opinions using different models and adaptation methods.","We find that most models exhibit intrinsic bias.","Using a social media text summarisation dataset and contrasting various adaptation methods, we find that tuning a smaller number of parameters is less biased compared to standard fine-tuning; however, the diversity of topics in training data used for fine-tuning is critical."],"url":"http://arxiv.org/abs/2402.00322v1","category":"cs.CL"}
{"created":"2024-02-01 04:15:39","title":"SmartCooper: Vehicular Collaborative Perception with Adaptive Fusion and Judger Mechanism","abstract":"In recent years, autonomous driving has garnered significant attention due to its potential for improving road safety through collaborative perception among connected and autonomous vehicles (CAVs). However, time-varying channel variations in vehicular transmission environments demand dynamic allocation of communication resources. Moreover, in the context of collaborative perception, it is important to recognize that not all CAVs contribute valuable data, and some CAV data even have detrimental effects on collaborative perception. In this paper, we introduce SmartCooper, an adaptive collaborative perception framework that incorporates communication optimization and a judger mechanism to facilitate CAV data fusion. Our approach begins with optimizing the connectivity of vehicles while considering communication constraints. We then train a learnable encoder to dynamically adjust the compression ratio based on the channel state information (CSI). Subsequently, we devise a judger mechanism to filter the detrimental image data reconstructed by adaptive decoders. We evaluate the effectiveness of our proposed algorithm on the OpenCOOD platform. Our results demonstrate a substantial reduction in communication costs by 23.10\\% compared to the non-judger scheme. Additionally, we achieve a significant improvement on the average precision of Intersection over Union (AP@IoU) by 7.15\\% compared with state-of-the-art schemes.","sentences":["In recent years, autonomous driving has garnered significant attention due to its potential for improving road safety through collaborative perception among connected and autonomous vehicles (CAVs).","However, time-varying channel variations in vehicular transmission environments demand dynamic allocation of communication resources.","Moreover, in the context of collaborative perception, it is important to recognize that not all CAVs contribute valuable data, and some CAV data even have detrimental effects on collaborative perception.","In this paper, we introduce SmartCooper, an adaptive collaborative perception framework that incorporates communication optimization and a judger mechanism to facilitate CAV data fusion.","Our approach begins with optimizing the connectivity of vehicles while considering communication constraints.","We then train a learnable encoder to dynamically adjust the compression ratio based on the channel state information (CSI).","Subsequently, we devise a judger mechanism to filter the detrimental image data reconstructed by adaptive decoders.","We evaluate the effectiveness of our proposed algorithm on the OpenCOOD platform.","Our results demonstrate a substantial reduction in communication costs by 23.10\\% compared to the non-judger scheme.","Additionally, we achieve a significant improvement on the average precision of Intersection over Union (AP@IoU) by 7.15\\% compared with state-of-the-art schemes."],"url":"http://arxiv.org/abs/2402.00321v1","category":"cs.CV"}
{"created":"2024-02-01 03:50:12","title":"A Tuning-Free Primal-Dual Splitting Algorithm for Large-Scale Semidefinite Programming","abstract":"This paper proposes and analyzes a tuning-free variant of Primal-Dual Hybrid Gradient (PDHG), and investigates its effectiveness for solving large-scale semidefinite programming (SDP). The core idea is based on the combination of two seemingly unrelated results: (1) the equivalence of PDHG and Douglas-Rachford splitting (DRS); (2) the asymptotic convergence of non-stationary DRS. This combination provides a unified approach to analyze the convergence of generic adaptive PDHG, including the proposed tuning-free algorithm and various existing ones. Numerical experiments are conducted to show the performance of our algorithm, highlighting its superior convergence speed and robustness in the context of SDP.","sentences":["This paper proposes and analyzes a tuning-free variant of Primal-Dual Hybrid Gradient (PDHG), and investigates its effectiveness for solving large-scale semidefinite programming (SDP).","The core idea is based on the combination of two seemingly unrelated results: (1) the equivalence of PDHG and Douglas-Rachford splitting (DRS); (2) the asymptotic convergence of non-stationary DRS.","This combination provides a unified approach to analyze the convergence of generic adaptive PDHG, including the proposed tuning-free algorithm and various existing ones.","Numerical experiments are conducted to show the performance of our algorithm, highlighting its superior convergence speed and robustness in the context of SDP."],"url":"http://arxiv.org/abs/2402.00311v1","category":"math.OC"}
{"created":"2024-01-31 23:40:44","title":"CNN-FL for Biotechnology Industry Empowered by Internet-of-BioNano Things and Digital Twins","abstract":"Digital twins (DTs) are revolutionizing the biotechnology industry by enabling sophisticated digital representations of biological assets, microorganisms, drug development processes, and digital health applications. However, digital twinning at micro and nano scales, particularly in modeling complex entities like bacteria, presents significant challenges in terms of requiring advanced Internet of Things (IoT) infrastructure and computing approaches to achieve enhanced accuracy and scalability. In this work, we propose a novel framework that integrates the Internet of Bio-Nano Things (IoBNT) with advanced machine learning techniques, specifically convolutional neural networks (CNN) and federated learning (FL), to effectively tackle the identified challenges. Within our framework, IoBNT devices are deployed to gather image-based biological data across various physical environments, leveraging the strong capabilities of CNNs for robust machine vision and pattern recognition. Subsequently, FL is utilized to aggregate insights from these disparate data sources, creating a refined global model that continually enhances accuracy and predictive reliability, which is crucial for the effective deployment of DTs in biotechnology. The primary contribution is the development of a novel framework that synergistically combines CNN and FL, augmented by the capabilities of the IoBNT. This novel approach is specifically tailored to enhancing DTs in the biotechnology industry. The results showcase enhancements in the reliability and safety of microorganism DTs, while preserving their accuracy. Furthermore, the proposed framework excels in energy efficiency and security, offering a user-friendly and adaptable solution. This broadens its applicability across diverse sectors, including biotechnology and pharmaceutical industries, as well as clinical and hospital settings.","sentences":["Digital twins (DTs) are revolutionizing the biotechnology industry by enabling sophisticated digital representations of biological assets, microorganisms, drug development processes, and digital health applications.","However, digital twinning at micro and nano scales, particularly in modeling complex entities like bacteria, presents significant challenges in terms of requiring advanced Internet of Things (IoT) infrastructure and computing approaches to achieve enhanced accuracy and scalability.","In this work, we propose a novel framework that integrates the Internet of Bio-Nano Things (IoBNT) with advanced machine learning techniques, specifically convolutional neural networks (CNN) and federated learning (FL), to effectively tackle the identified challenges.","Within our framework, IoBNT devices are deployed to gather image-based biological data across various physical environments, leveraging the strong capabilities of CNNs for robust machine vision and pattern recognition.","Subsequently, FL is utilized to aggregate insights from these disparate data sources, creating a refined global model that continually enhances accuracy and predictive reliability, which is crucial for the effective deployment of DTs in biotechnology.","The primary contribution is the development of a novel framework that synergistically combines CNN and FL, augmented by the capabilities of the IoBNT.","This novel approach is specifically tailored to enhancing DTs in the biotechnology industry.","The results showcase enhancements in the reliability and safety of microorganism DTs, while preserving their accuracy.","Furthermore, the proposed framework excels in energy efficiency and security, offering a user-friendly and adaptable solution.","This broadens its applicability across diverse sectors, including biotechnology and pharmaceutical industries, as well as clinical and hospital settings."],"url":"http://arxiv.org/abs/2402.00238v1","category":"cs.LG"}
{"created":"2024-02-01 18:59:09","title":"ViCA-NeRF: View-Consistency-Aware 3D Editing of Neural Radiance Fields","abstract":"We introduce ViCA-NeRF, the first view-consistency-aware method for 3D editing with text instructions. In addition to the implicit neural radiance field (NeRF) modeling, our key insight is to exploit two sources of regularization that explicitly propagate the editing information across different views, thus ensuring multi-view consistency. For geometric regularization, we leverage the depth information derived from NeRF to establish image correspondences between different views. For learned regularization, we align the latent codes in the 2D diffusion model between edited and unedited images, enabling us to edit key views and propagate the update throughout the entire scene. Incorporating these two strategies, our ViCA-NeRF operates in two stages. In the initial stage, we blend edits from different views to create a preliminary 3D edit. This is followed by a second stage of NeRF training, dedicated to further refining the scene's appearance. Experimental results demonstrate that ViCA-NeRF provides more flexible, efficient (3 times faster) editing with higher levels of consistency and details, compared with the state of the art. Our code is publicly available.","sentences":["We introduce ViCA-NeRF, the first view-consistency-aware method for 3D editing with text instructions.","In addition to the implicit neural radiance field (NeRF) modeling, our key insight is to exploit two sources of regularization that explicitly propagate the editing information across different views, thus ensuring multi-view consistency.","For geometric regularization, we leverage the depth information derived from NeRF to establish image correspondences between different views.","For learned regularization, we align the latent codes in the 2D diffusion model between edited and unedited images, enabling us to edit key views and propagate the update throughout the entire scene.","Incorporating these two strategies, our ViCA-NeRF operates in two stages.","In the initial stage, we blend edits from different views to create a preliminary 3D edit.","This is followed by a second stage of NeRF training, dedicated to further refining the scene's appearance.","Experimental results demonstrate that ViCA-NeRF provides more flexible, efficient (3 times faster) editing with higher levels of consistency and details, compared with the state of the art.","Our code is publicly available."],"url":"http://arxiv.org/abs/2402.00864v1","category":"cs.CV"}
{"created":"2024-02-01 18:02:29","title":"USDnet: Unsupervised Speech Dereverberation via Neural Forward Filtering","abstract":"In reverberant conditions with a single speaker, each far-field microphone records a reverberant version of the same speaker signal at a different location. In over-determined conditions, where there are more microphones than speakers, each recorded mixture signal can be leveraged as a constraint to narrow down the solutions to target anechoic speech and thereby reduce reverberation. Equipped with this insight, we propose USDnet, a novel deep neural network (DNN) approach for unsupervised speech dereverberation (USD). At each training step, we first feed an input mixture to USDnet to produce an estimate for target speech, and then linearly filter the DNN estimate to approximate the multi-microphone mixture so that the constraint can be satisfied at each microphone, thereby regularizing the DNN estimate to approximate target anechoic speech. The linear filter can be estimated based on the mixture and DNN estimate via neural forward filtering algorithms such as forward convolutive prediction. We show that this novel methodology can promote unsupervised dereverberation of single-source reverberant speech.","sentences":["In reverberant conditions with a single speaker, each far-field microphone records a reverberant version of the same speaker signal at a different location.","In over-determined conditions, where there are more microphones than speakers, each recorded mixture signal can be leveraged as a constraint to narrow down the solutions to target anechoic speech and thereby reduce reverberation.","Equipped with this insight, we propose USDnet, a novel deep neural network (DNN) approach for unsupervised speech dereverberation (USD).","At each training step, we first feed an input mixture to USDnet to produce an estimate for target speech, and then linearly filter the DNN estimate to approximate the multi-microphone mixture so that the constraint can be satisfied at each microphone, thereby regularizing the DNN estimate to approximate target anechoic speech.","The linear filter can be estimated based on the mixture and DNN estimate via neural forward filtering algorithms such as forward convolutive prediction.","We show that this novel methodology can promote unsupervised dereverberation of single-source reverberant speech."],"url":"http://arxiv.org/abs/2402.00820v1","category":"eess.AS"}
{"created":"2024-02-01 17:54:16","title":"On the stability of the Yamabe invariant of $S^3$","abstract":"Let $g$ be a complete, asymptotically flat metric on $\\mathbb{R}^3$ with vanishing scalar curvature. Moreover, assume that $(\\mathbb{R}^3,g)$ supports a nearly Euclidean $L^2$ Sobolev inequality. We prove that $(\\mathbb{R}^3,g)$ must be close to Euclidean space with respect to the $d_p$-distance defined by Lee-Naber-Neumayer. We then discuss some consequences for the stability of the Yamabe invariant of $S^3$. More precisely, we show that if such a manifold $(\\mathbb{R}^3,g)$ carries a suitably normalized, positive solution to $\\Delta_g w + \\lambda w^5 = 0$ then $w$ must be close, in a certain sense, to a conformal factor that transforms Euclidean space into a round sphere.","sentences":["Let $g$ be a complete, asymptotically flat metric on $\\mathbb{R}^3$ with vanishing scalar curvature.","Moreover, assume that $(\\mathbb{R}^3,g)$ supports a nearly Euclidean $L^2$ Sobolev inequality.","We prove that $(\\mathbb{R}^3,g)$ must be close to Euclidean space with respect to the $d_p$-distance defined by Lee-Naber-Neumayer.","We then discuss some consequences for the stability of the Yamabe invariant of $S^3$. More precisely, we show that if such a manifold $(\\mathbb{R}^3,g)$ carries a suitably normalized, positive solution to $\\Delta_g w + \\lambda w^5 = 0$ then $w$ must be close, in a certain sense, to a conformal factor that transforms Euclidean space into a round sphere."],"url":"http://arxiv.org/abs/2402.00815v1","category":"math.DG"}
{"created":"2024-02-01 17:09:30","title":"Reply to: Assessing the precision of morphogen gradients in neural tube development","abstract":"In a recent article [Vetter and Iber, Nat. Commun. 13, 1145 (2022)], we demonstrated that single morphogen gradients in the developing mouse neural tube can carry sufficient positional accuracy to explain the patterning precision of progenitor domain boundaries. Zagorski et al. had previously concluded otherwise [Zagorski et al., Science 356, 1379-1383 (2017)], based on methodological inconsistencies that we have revealed. The authors now comment on our work with a Matters Arising letter. We rebut their criticism point by point in the Supplement, and summarize the main aspects here.","sentences":["In a recent article [Vetter and Iber, Nat.","Commun.","13, 1145 (2022)], we demonstrated that single morphogen gradients in the developing mouse neural tube can carry sufficient positional accuracy to explain the patterning precision of progenitor domain boundaries.","Zagorski et al. had previously concluded otherwise","[Zagorski et al., Science 356, 1379-1383 (2017)], based on methodological inconsistencies that we have revealed.","The authors now comment on our work with a Matters Arising letter.","We rebut their criticism point by point in the Supplement, and summarize the main aspects here."],"url":"http://arxiv.org/abs/2402.00781v1","category":"physics.bio-ph"}
{"created":"2024-02-01 16:57:46","title":"Multiple q-Kravchuk polynomials","abstract":"We study a family of type II multiple orthogonal polynomials. We consider orthogonality conditions with respect to a vector measure, in which each component is a q-analogue of the binomial distribution. The lowering and raising operators as well as the Rodrigues formula for these polynomials are obtained. The difference equation of order r+1 is studied. The connection via limit relation between four types of Kravchuk polynomials is discussed.","sentences":["We study a family of type II multiple orthogonal polynomials.","We consider orthogonality conditions with respect to a vector measure, in which each component is a q-analogue of the binomial distribution.","The lowering and raising operators as well as the Rodrigues formula for these polynomials are obtained.","The difference equation of order r+1 is studied.","The connection via limit relation between four types of Kravchuk polynomials is discussed."],"url":"http://arxiv.org/abs/2402.00768v1","category":"math.CA"}
{"created":"2024-02-01 16:55:17","title":"Loop soup representation of zeta-regularised determinants and equivariant Symanzik identities","abstract":"We derive a stochastic representation for determinants of Laplace-type operators on vectors bundles over manifolds. Namely, inverse powers of those determinants are written as the expectation of a product of holonomies defined over Brownian loop soups. Our results hold over compact manifolds of dimension 2 or 3, in the presence of mass or a boundary. We derive a few consequences, including some regularity as a function of the operator and the conformal invariance of the zeta function on surfaces.   Our second main result is the rigorous construction of a stochastic gauge theory minimally coupling a scalar field to a prescribed random smooth gauge field, which we prove obeys the so-called Symanzik identities. Some of these results are continuous analogues of the work of A. Kassel and T. L\\'evy in the discrete.","sentences":["We derive a stochastic representation for determinants of Laplace-type operators on vectors bundles over manifolds.","Namely, inverse powers of those determinants are written as the expectation of a product of holonomies defined over Brownian loop soups.","Our results hold over compact manifolds of dimension 2 or 3, in the presence of mass or a boundary.","We derive a few consequences, including some regularity as a function of the operator and the conformal invariance of the zeta function on surfaces.   ","Our second main result is the rigorous construction of a stochastic gauge theory minimally coupling a scalar field to a prescribed random smooth gauge field, which we prove obeys the so-called Symanzik identities.","Some of these results are continuous analogues of the work of A. Kassel and T. L\\'evy in the discrete."],"url":"http://arxiv.org/abs/2402.00767v1","category":"math.PR"}
{"created":"2024-02-01 16:54:54","title":"On the global in time existence and uniqueness of solutions to the Boltzmann hierarchy","abstract":"In this paper we establish the global in time existence and uniqueness of solutions to the Boltzmann hierarchy, a hierarchy of equations instrumental for the rigorous derivation of the Boltzmann equation from many particles. Inspired by available $L^{\\infty}$-based a-priori estimate for solutions to the Boltzmann equation, we develop the polynomially weighted $L^\\infty$ a-priori bounds for solutions to the Boltzmann hierarchy and handle the factorial growth of the number of terms in the Dyson's series by reorganizing the sum through a combinatorial technique known as the Klainerman-Machedon board game argument. This paper is the first work that exploits such a combinatorial technique in conjunction with an $L^{\\infty}$-based estimate to prove uniqueness of the mild solutions to the Boltzmann hierarchy. Our proof of existence of global in time mild solutions to the Boltzmann hierarchy for admissible initial data is constructive and it employs known global in time solutions to the Boltzmann equation via a Hewitt-Savage type theorem.","sentences":["In this paper we establish the global in time existence and uniqueness of solutions to the Boltzmann hierarchy, a hierarchy of equations instrumental for the rigorous derivation of the Boltzmann equation from many particles.","Inspired by available $L^{\\infty}$-based a-priori estimate for solutions to the Boltzmann equation, we develop the polynomially weighted $L^\\infty$ a-priori bounds for solutions to the Boltzmann hierarchy and handle the factorial growth of the number of terms in the Dyson's series by reorganizing the sum through a combinatorial technique known as the Klainerman-Machedon board game argument.","This paper is the first work that exploits such a combinatorial technique in conjunction with an $L^{\\infty}$-based estimate to prove uniqueness of the mild solutions to the Boltzmann hierarchy.","Our proof of existence of global in time mild solutions to the Boltzmann hierarchy for admissible initial data is constructive and it employs known global in time solutions to the Boltzmann equation via a Hewitt-Savage type theorem."],"url":"http://arxiv.org/abs/2402.00765v1","category":"math.AP"}
{"created":"2024-02-01 16:48:35","title":"Relative Lie algebra cohomology of SU(2,1) and Eisenstein classes on Picard surfaces","abstract":"We consider Picard surfaces, locally symmetric varieties $S_{\\Gamma}$ attached to the Lie group SU(2,1), and we construct explicit differential forms on $S_{\\Gamma}$ representing Eisenstein classes, i.e. cohomology classes restricting non-trivially to the boundary of the Borel-Serre compactification. This is needed for the computation of the class of the extensions of the Hodge structure that we have constructed in [2] according to the predictions of the Bloch-Beilinson conjectures. The tool for the construction of the differential forms is an analysis of relative Lie algebra cohomology of the principal series of SU(2,1) using recent methods of Buttcane and Miller.","sentences":["We consider Picard surfaces, locally symmetric varieties $S_{\\Gamma}$ attached to the Lie group SU(2,1), and we construct explicit differential forms on $S_{\\Gamma}$ representing Eisenstein classes, i.e. cohomology classes restricting non-trivially to the boundary of the Borel-Serre compactification.","This is needed for the computation of the class of the extensions of the Hodge structure that we have constructed in [2] according to the predictions of the Bloch-Beilinson conjectures.","The tool for the construction of the differential forms is an analysis of relative Lie algebra cohomology of the principal series of SU(2,1) using recent methods of Buttcane and Miller."],"url":"http://arxiv.org/abs/2402.00757v1","category":"math.NT"}
{"created":"2024-02-01 16:43:58","title":"GS++: Error Analyzing and Optimal Gaussian Splatting","abstract":"3D Gaussian Splatting has garnered extensive attention and application in real-time neural rendering. Concurrently, concerns have been raised about the limitations of this technology in aspects such as point cloud storage, performance , and robustness in sparse viewpoints , leading to various improvements. However, there has been a notable lack of attention to the projection errors introduced by the local affine approximation inherent in the splatting itself, and the consequential impact of these errors on the quality of photo-realistic rendering. This paper addresses the projection error function of 3D Gaussian Splatting, commencing with the residual error from the first-order Taylor expansion of the projection function $\\phi$. The analysis establishes a correlation between the error and the Gaussian mean position. Subsequently, leveraging function optimization theory, this paper analyzes the function's minima to provide an optimal projection strategy for Gaussian Splatting referred to Optimal Gaussian Splatting. Experimental validation further confirms that this projection methodology reduces artifacts, resulting in a more convincingly realistic rendering.","sentences":["3D Gaussian Splatting has garnered extensive attention and application in real-time neural rendering.","Concurrently, concerns have been raised about the limitations of this technology in aspects such as point cloud storage, performance , and robustness in sparse viewpoints , leading to various improvements.","However, there has been a notable lack of attention to the projection errors introduced by the local affine approximation inherent in the splatting itself, and the consequential impact of these errors on the quality of photo-realistic rendering.","This paper addresses the projection error function of 3D Gaussian Splatting, commencing with the residual error from the first-order Taylor expansion of the projection function $\\phi$. The analysis establishes a correlation between the error and the Gaussian mean position.","Subsequently, leveraging function optimization theory, this paper analyzes the function's minima to provide an optimal projection strategy for Gaussian Splatting referred to Optimal Gaussian Splatting.","Experimental validation further confirms that this projection methodology reduces artifacts, resulting in a more convincingly realistic rendering."],"url":"http://arxiv.org/abs/2402.00752v1","category":"cs.CV"}
{"created":"2024-02-01 16:37:34","title":"p-adically convergent loci in varieties arising from periodic continued fractions","abstract":"Inspired by several alternative definitions of continued fraction expansions for elements in $\\mathbb Q_p$, we study $p$-adically convergent periodic continued fractions with partial quotients in $\\mathbb Z[1/p]$. To this end, following a previous work by Brock, Elkies, and Jordan, we consider certain algebraic varieties whose points represent formal periodic continued fractions with period and preperiod of fixed lengths, satisfying a given quadratic equation. We then focus on the $p$-adically convergent loci of these varieties, characterizing the zero and one-dimensional cases.","sentences":["Inspired by several alternative definitions of continued fraction expansions for elements in $\\mathbb Q_p$, we study $p$-adically convergent periodic continued fractions with partial quotients in $\\mathbb Z[1/p]$. To this end, following a previous work by Brock, Elkies, and Jordan, we consider certain algebraic varieties whose points represent formal periodic continued fractions with period and preperiod of fixed lengths, satisfying a given quadratic equation.","We then focus on the $p$-adically convergent loci of these varieties, characterizing the zero and one-dimensional cases."],"url":"http://arxiv.org/abs/2402.00739v1","category":"math.NT"}
{"created":"2024-02-01 16:14:54","title":"Automatic Segmentation of the Spinal Cord Nerve Rootlets","abstract":"Precise identification of spinal nerve rootlets is relevant to delineate spinal levels for the study of functional activity in the spinal cord. The goal of this study was to develop an automatic method for the semantic segmentation of spinal nerve rootlets from T2-weighted magnetic resonance imaging (MRI) scans. Images from two open-access MRI datasets were used to train a 3D multi-class convolutional neural network using an active learning approach to segment C2-C8 dorsal nerve rootlets. Each output class corresponds to a spinal level. The method was tested on 3T T2-weighted images from datasets unseen during training to assess inter-site, inter-session, and inter-resolution variability. The test Dice score was 0.67 +- 0.16 (mean +- standard deviation across rootlets levels), suggesting a good performance. The method also demonstrated low inter-vendor and inter-site variability (coefficient of variation <= 1.41 %), as well as low inter-session variability (coefficient of variation <= 1.30 %) indicating stable predictions across different MRI vendors, sites, and sessions. The proposed methodology is open-source and readily available in the Spinal Cord Toolbox (SCT) v6.2 and higher.","sentences":["Precise identification of spinal nerve rootlets is relevant to delineate spinal levels for the study of functional activity in the spinal cord.","The goal of this study was to develop an automatic method for the semantic segmentation of spinal nerve rootlets from T2-weighted magnetic resonance imaging (MRI) scans.","Images from two open-access MRI datasets were used to train a 3D multi-class convolutional neural network using an active learning approach to segment C2-C8 dorsal nerve rootlets.","Each output class corresponds to a spinal level.","The method was tested on 3T T2-weighted images from datasets unseen during training to assess inter-site, inter-session, and inter-resolution variability.","The test Dice score was 0.67 +- 0.16 (mean +- standard deviation across rootlets levels), suggesting a good performance.","The method also demonstrated low inter-vendor and inter-site variability (coefficient of variation <= 1.41 %), as well as low inter-session variability (coefficient of variation <= 1.30 %) indicating stable predictions across different MRI vendors, sites, and sessions.","The proposed methodology is open-source and readily available in the Spinal Cord Toolbox (SCT) v6.2 and higher."],"url":"http://arxiv.org/abs/2402.00724v1","category":"cs.CV"}
{"created":"2024-02-01 15:11:10","title":"Metric Multiview Geometry -- a Catalogue in Low Dimensions","abstract":"We systematically compile an exhaustive catalogue of multiview varieties and anchored multiview varieties arising from projections of points and lines in 1, 2, and 3-dimensional projective space. We say that two such varieties are ED-equivalent if there is a linear isomorphism between that that preserve ED-critical points. This gives rise to fourteen equivalence classes, and we determine various properties - dimension, set-theoretic equations, and multidegrees - for all varieties featured in our catalogue. In the case of points, we also present a complementary study of resectioning varieties and their singular loci. Finally, we propose conjectures for the Euclidean distance degrees of all varieties appearing in our comprehensive compilation.","sentences":["We systematically compile an exhaustive catalogue of multiview varieties and anchored multiview varieties arising from projections of points and lines in 1, 2, and 3-dimensional projective space.","We say that two such varieties are ED-equivalent if there is a linear isomorphism between that that preserve ED-critical points.","This gives rise to fourteen equivalence classes, and we determine various properties - dimension, set-theoretic equations, and multidegrees - for all varieties featured in our catalogue.","In the case of points, we also present a complementary study of resectioning varieties and their singular loci.","Finally, we propose conjectures for the Euclidean distance degrees of all varieties appearing in our comprehensive compilation."],"url":"http://arxiv.org/abs/2402.00648v1","category":"math.AG"}
{"created":"2024-02-01 15:05:31","title":"Two molecular devices for superconducting spintronics","abstract":"We create two molecular devices with superconducting junctions, using nickelocene molecules, single Fe atoms, and Pb electrodes at low temperature. We find contrasting behavior based on the coordination of the Fe atom: one device shows low-bias features in its differential conductance due to the superposition of multiple Andreev reflections (MAR) and Fe-induced in-gap states. The other reveals interference between MAR and in-gap states, showcasing the diversity achievable in atomically engineered devices with identical components.","sentences":["We create two molecular devices with superconducting junctions, using nickelocene molecules, single Fe atoms, and Pb electrodes at low temperature.","We find contrasting behavior based on the coordination of the Fe atom: one device shows low-bias features in its differential conductance due to the superposition of multiple Andreev reflections (MAR) and Fe-induced in-gap states.","The other reveals interference between MAR and in-gap states, showcasing the diversity achievable in atomically engineered devices with identical components."],"url":"http://arxiv.org/abs/2402.00644v1","category":"cond-mat.supr-con"}
{"created":"2024-02-01 14:40:12","title":"Targeting of chondrocyte plasticity via connexin43 modulation attenuates cellular senescence and fosters a pro-regenerative environment in osteoarthritis","abstract":"Osteoarthritis (OA), a chronic disease characterized by articular cartilage degeneration, is a leading cause of disability and pain worldwide. In OA, chondrocytes in cartilage undergo phenotypic changes and senescence, restricting cartilage regeneration and favouring disease progression. Similar to other wound-healing disorders, chondrocytes from OA patients show a chronic increase in the gap junction channel protein connexin43 (Cx43), which regulates signal transduction through the exchange of elements or recruitment/release of signalling factors. Although immature or stem-like cells are present in cartilage from OA patients, their origin and role in disease progression are unknown. In this study, we found that Cx43 acts as a positive regulator of chondrocyte-mesenchymal transition. Downregulation of either Cx43 by CRISPR/Cas9 or Cx43-mediated gap junctional intercellular communication (GJIC) by carbenoxolone treatment triggered rediferentiation of osteoarthritic chondrocytes into a more differentiated state, associated with decreased synthesis of MMPs and proinflammatory factors, and reduced senescence. We have identified causal Cx43-sensitive circuit in chondrocytes that regulates dedifferentiation, redifferentiation and senescence. We propose that chondrocytes undergo chondrocyte-mesenchymal transition where increased Cx43-mediated GJIC during OA facilitates Twist-1 nuclear translocation as a novel mechanism involved in OA progression. These findings support the use of Cx43 as an appropriate therapeutic target to halt OA progression and to promote cartilage regeneration.","sentences":["Osteoarthritis (OA), a chronic disease characterized by articular cartilage degeneration, is a leading cause of disability and pain worldwide.","In OA, chondrocytes in cartilage undergo phenotypic changes and senescence, restricting cartilage regeneration and favouring disease progression.","Similar to other wound-healing disorders, chondrocytes from OA patients show a chronic increase in the gap junction channel protein connexin43 (Cx43), which regulates signal transduction through the exchange of elements or recruitment/release of signalling factors.","Although immature or stem-like cells are present in cartilage from OA patients, their origin and role in disease progression are unknown.","In this study, we found that Cx43 acts as a positive regulator of chondrocyte-mesenchymal transition.","Downregulation of either Cx43 by CRISPR/Cas9 or Cx43-mediated gap junctional intercellular communication (GJIC) by carbenoxolone treatment triggered rediferentiation of osteoarthritic chondrocytes into a more differentiated state, associated with decreased synthesis of MMPs and proinflammatory factors, and reduced senescence.","We have identified causal Cx43-sensitive circuit in chondrocytes that regulates dedifferentiation, redifferentiation and senescence.","We propose that chondrocytes undergo chondrocyte-mesenchymal transition where increased Cx43-mediated GJIC during OA facilitates Twist-1 nuclear translocation as a novel mechanism involved in OA progression.","These findings support the use of Cx43 as an appropriate therapeutic target to halt OA progression and to promote cartilage regeneration."],"url":"http://arxiv.org/abs/2402.00624v1","category":"q-bio.TO"}
{"created":"2024-02-01 14:02:06","title":"Deep Clustering Using the Soft Silhouette Score: Towards Compact and Well-Separated Clusters","abstract":"Unsupervised learning has gained prominence in the big data era, offering a means to extract valuable insights from unlabeled datasets. Deep clustering has emerged as an important unsupervised category, aiming to exploit the non-linear mapping capabilities of neural networks in order to enhance clustering performance. The majority of deep clustering literature focuses on minimizing the inner-cluster variability in some embedded space while keeping the learned representation consistent with the original high-dimensional dataset. In this work, we propose soft silhoutte, a probabilistic formulation of the silhouette coefficient. Soft silhouette rewards compact and distinctly separated clustering solutions like the conventional silhouette coefficient. When optimized within a deep clustering framework, soft silhouette guides the learned representations towards forming compact and well-separated clusters. In addition, we introduce an autoencoder-based deep learning architecture that is suitable for optimizing the soft silhouette objective function. The proposed deep clustering method has been tested and compared with several well-studied deep clustering methods on various benchmark datasets, yielding very satisfactory clustering results.","sentences":["Unsupervised learning has gained prominence in the big data era, offering a means to extract valuable insights from unlabeled datasets.","Deep clustering has emerged as an important unsupervised category, aiming to exploit the non-linear mapping capabilities of neural networks in order to enhance clustering performance.","The majority of deep clustering literature focuses on minimizing the inner-cluster variability in some embedded space while keeping the learned representation consistent with the original high-dimensional dataset.","In this work, we propose soft silhoutte, a probabilistic formulation of the silhouette coefficient.","Soft silhouette rewards compact and distinctly separated clustering solutions like the conventional silhouette coefficient.","When optimized within a deep clustering framework, soft silhouette guides the learned representations towards forming compact and well-separated clusters.","In addition, we introduce an autoencoder-based deep learning architecture that is suitable for optimizing the soft silhouette objective function.","The proposed deep clustering method has been tested and compared with several well-studied deep clustering methods on various benchmark datasets, yielding very satisfactory clustering results."],"url":"http://arxiv.org/abs/2402.00608v1","category":"cs.LG"}
{"created":"2024-02-01 13:32:22","title":"Spin wave-driven variable-phase mutual synchronization in spin Hall nano-oscillators","abstract":"Spin-orbit torque can drive auto-oscillations of propagating spin wave (PSW) modes in nano-constriction spin Hall nano-oscillators (SHNOs). These modes allow both long-range coupling and the potential of controlling its phase -- critical aspect for nano-magnonics, spin wave logic, and Ising machines. Here, we demonstrate PSW-driven variable-phase coupling between two nano-constriction SHNOs and study how their separation and the PSW wave vector impact their mutual synchronization. In addition to ordinary in-phase mutual synchronization, we observe, using both electrical measurements and phase-resolved $\\mu-$Brillouin Light Scattering microscopy, mutual synchronization with a phase that can be tuned from 0 to $\\pi$ using the drive current or the applied field. Micromagnetic simulations corroborate the experiments and visualize how the PSW patterns in the bridge connecting the two nano-constrictions govern the coupling. These results advance the capabilities of mutually synchronized SHNOs and open up new possibilities for applications in spin wave logic, unconventional computing, and Ising Machines.","sentences":["Spin-orbit torque can drive auto-oscillations of propagating spin wave (PSW) modes in nano-constriction spin Hall nano-oscillators (SHNOs).","These modes allow both long-range coupling and the potential of controlling its phase -- critical aspect for nano-magnonics, spin wave logic, and Ising machines.","Here, we demonstrate PSW-driven variable-phase coupling between two nano-constriction SHNOs and study how their separation and the PSW wave vector impact their mutual synchronization.","In addition to ordinary in-phase mutual synchronization, we observe, using both electrical measurements and phase-resolved $\\mu-$Brillouin Light Scattering microscopy, mutual synchronization with a phase that can be tuned from 0 to $\\pi$ using the drive current or the applied field.","Micromagnetic simulations corroborate the experiments and visualize how the PSW patterns in the bridge connecting the two nano-constrictions govern the coupling.","These results advance the capabilities of mutually synchronized SHNOs and open up new possibilities for applications in spin wave logic, unconventional computing, and Ising Machines."],"url":"http://arxiv.org/abs/2402.00586v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-01 12:50:48","title":"A Single Graph Convolution Is All You Need: Efficient Grayscale Image Classification","abstract":"Image classifiers often rely on convolutional neural networks (CNN) for their tasks, which are inherently more heavyweight than multilayer perceptrons (MLPs), which can be problematic in real-time applications. Additionally, many image classification models work on both RGB and grayscale datasets. Classifiers that operate solely on grayscale images are much less common. Grayscale image classification has diverse applications, including but not limited to medical image classification and synthetic aperture radar (SAR) automatic target recognition (ATR). Thus, we present a novel grayscale (single channel) image classification approach using a vectorized view of images. We exploit the lightweightness of MLPs by viewing images as a vector and reducing our problem setting to the grayscale image classification setting. We find that using a single graph convolutional layer batch-wise increases accuracy and reduces variance in the performance of our model. Moreover, we develop a customized accelerator on FPGA for the proposed model with several optimizations to improve its performance. Our experimental results on benchmark grayscale image datasets demonstrate the effectiveness of the proposed model, achieving vastly lower latency (up to 16$\\times$ less) and competitive or leading performance compared to other state-of-the-art image classification models on various domain-specific grayscale image classification datasets.","sentences":["Image classifiers often rely on convolutional neural networks (CNN) for their tasks, which are inherently more heavyweight than multilayer perceptrons (MLPs), which can be problematic in real-time applications.","Additionally, many image classification models work on both RGB and grayscale datasets.","Classifiers that operate solely on grayscale images are much less common.","Grayscale image classification has diverse applications, including but not limited to medical image classification and synthetic aperture radar (SAR) automatic target recognition (ATR).","Thus, we present a novel grayscale (single channel) image classification approach using a vectorized view of images.","We exploit the lightweightness of MLPs by viewing images as a vector and reducing our problem setting to the grayscale image classification setting.","We find that using a single graph convolutional layer batch-wise increases accuracy and reduces variance in the performance of our model.","Moreover, we develop a customized accelerator on FPGA for the proposed model with several optimizations to improve its performance.","Our experimental results on benchmark grayscale image datasets demonstrate the effectiveness of the proposed model, achieving vastly lower latency (up to 16$\\times$ less) and competitive or leading performance compared to other state-of-the-art image classification models on various domain-specific grayscale image classification datasets."],"url":"http://arxiv.org/abs/2402.00564v1","category":"cs.CV"}
{"created":"2024-02-01 11:58:28","title":"Preconditioning for Physics-Informed Neural Networks","abstract":"Physics-informed neural networks (PINNs) have shown promise in solving various partial differential equations (PDEs). However, training pathologies have negatively affected the convergence and prediction accuracy of PINNs, which further limits their practical applications. In this paper, we propose to use condition number as a metric to diagnose and mitigate the pathologies in PINNs. Inspired by classical numerical analysis, where the condition number measures sensitivity and stability, we highlight its pivotal role in the training dynamics of PINNs. We prove theorems to reveal how condition number is related to both the error control and convergence of PINNs. Subsequently, we present an algorithm that leverages preconditioning to improve the condition number. Evaluations of 18 PDE problems showcase the superior performance of our method. Significantly, in 7 of these problems, our method reduces errors by an order of magnitude. These empirical findings verify the critical role of the condition number in PINNs' training.","sentences":["Physics-informed neural networks (PINNs) have shown promise in solving various partial differential equations (PDEs).","However, training pathologies have negatively affected the convergence and prediction accuracy of PINNs, which further limits their practical applications.","In this paper, we propose to use condition number as a metric to diagnose and mitigate the pathologies in PINNs.","Inspired by classical numerical analysis, where the condition number measures sensitivity and stability, we highlight its pivotal role in the training dynamics of PINNs.","We prove theorems to reveal how condition number is related to both the error control and convergence of PINNs.","Subsequently, we present an algorithm that leverages preconditioning to improve the condition number.","Evaluations of 18 PDE problems showcase the superior performance of our method.","Significantly, in 7 of these problems, our method reduces errors by an order of magnitude.","These empirical findings verify the critical role of the condition number in PINNs' training."],"url":"http://arxiv.org/abs/2402.00531v1","category":"cs.LG"}
{"created":"2024-02-01 11:42:35","title":"Almost global existence for some Hamiltonian PDEs on manifolds with globally integrable geodesic flow","abstract":"In this paper we prove an abstract result of almost global existence for small and smooth solutions of some semilinear PDEs on Riemannian manifolds with globally integrable geodesic flow. Some examples of such manifolds are Lie groups (including flat tori), homogeneous spaces and rotational invariant surfaces. As applications of the abstract result we prove almost global existence for a nonlinear Schr\\\"odinger equation with a convolution potential and for a nonlinear beam equation. We also prove $H^s$ stability of the ground state in NLS equation. The proof is based on a normal form procedure.","sentences":["In this paper we prove an abstract result of almost global existence for small and smooth solutions of some semilinear PDEs on Riemannian manifolds with globally integrable geodesic flow.","Some examples of such manifolds are Lie groups (including flat tori), homogeneous spaces and rotational invariant surfaces.","As applications of the abstract result we prove almost global existence for a nonlinear Schr\\\"odinger equation with a convolution potential and for a nonlinear beam equation.","We also prove $H^s$ stability of the ground state in NLS equation.","The proof is based on a normal form procedure."],"url":"http://arxiv.org/abs/2402.00521v1","category":"math.AP"}
{"created":"2024-02-01 11:17:09","title":"Correlated Optical Convolutional Neural Network with Quantum Speedup","abstract":"Compared with electrical neural networks, optical neural networks (ONNs) have the potentials to break the limit of the bandwidth and reduce the consumption of energy, and therefore draw much attention in recent years. By far, several types of ONNs have been implemented. However, the current ONNs cannot realize the acceleration as powerful as that indicated by the models like quantum neural networks. How to construct and realize an ONN with the quantum speedup is a huge challenge. Here, we propose theoretically and demonstrate experimentally a new type of optical convolutional neural network by introducing the optical correlation. It is called the correlated optical convolutional neural network (COCNN). We show that the COCNN can exhibit quantum speedup in the training process. The character is verified from the two aspects. One is the direct illustration of the faster convergence by comparing the loss function curves of the COCNN with that of the traditional convolutional neural network (CNN). Such a result is compatible with the training performance of the recently proposed quantum convolutional neural network (QCNN). The other is the demonstration of the COCNNs capability to perform the QCNN phase recognition circuit, validating the connection between the COCNN and the QCNN. Furthermore, we take the COCNN analog to the 3-qubit QCNN phase recognition circuit as an example and perform an experiment to show the soundness and the feasibility of it. The results perfectly match the theoretical calculations. Our proposal opens up a new avenue for realizing the ONNs with the quantum speedup, which will benefit the information processing in the era of big data.","sentences":["Compared with electrical neural networks, optical neural networks (ONNs) have the potentials to break the limit of the bandwidth and reduce the consumption of energy, and therefore draw much attention in recent years.","By far, several types of ONNs have been implemented.","However, the current ONNs cannot realize the acceleration as powerful as that indicated by the models like quantum neural networks.","How to construct and realize an ONN with the quantum speedup is a huge challenge.","Here, we propose theoretically and demonstrate experimentally a new type of optical convolutional neural network by introducing the optical correlation.","It is called the correlated optical convolutional neural network (COCNN).","We show that the COCNN can exhibit quantum speedup in the training process.","The character is verified from the two aspects.","One is the direct illustration of the faster convergence by comparing the loss function curves of the COCNN with that of the traditional convolutional neural network (CNN).","Such a result is compatible with the training performance of the recently proposed quantum convolutional neural network (QCNN).","The other is the demonstration of the COCNNs capability to perform the QCNN phase recognition circuit, validating the connection between the COCNN and the QCNN.","Furthermore, we take the COCNN analog to the 3-qubit QCNN phase recognition circuit as an example and perform an experiment to show the soundness and the feasibility of it.","The results perfectly match the theoretical calculations.","Our proposal opens up a new avenue for realizing the ONNs with the quantum speedup, which will benefit the information processing in the era of big data."],"url":"http://arxiv.org/abs/2402.00504v1","category":"physics.optics"}
{"created":"2024-02-01 09:57:30","title":"Simulations of poloidal flow stabilization of ballooning modes in a classical l=2 stellarator using JOREK","abstract":"Magnetohydrodynamics (MHD), combining fluid dynamics and Maxwell's equations, provides a useful means of analysing the dynamic evolution of plasmas and plasma instabilities. JOREK is a non-linear MHD code which solves these equations in the context of magnetic confinement fusion. Originally developed for tokamaks, JOREK has been extended to model stellarators. In this project, ExB poloidal flows are implemented in a classical l=2 stellarator configuration, by imposing a simple radial electric potential profile via initial conditions. The influence of this sheared background flow velocity on pressure-driven modes is interrogated, demonstrating a stabilizing effect when the shearing rate is comparable to the growth rate. This effect is observed for multiple toroidal modes and at different viscosities, demonstrating that the stabilization occurs as a result of shear decorrelation. Oscillations of the linear growth rate are observed in cases with higher flow speeds; this phenomenon is hypothesized to be due to phase misalignment between the poloidally coupled modes contributing to the ballooning mode. Some indicators are provided to support this, however analysis of this phenomenon is ongoing.","sentences":["Magnetohydrodynamics (MHD), combining fluid dynamics and Maxwell's equations, provides a useful means of analysing the dynamic evolution of plasmas and plasma instabilities.","JOREK is a non-linear MHD code which solves these equations in the context of magnetic confinement fusion.","Originally developed for tokamaks, JOREK has been extended to model stellarators.","In this project, ExB poloidal flows are implemented in a classical l=2 stellarator configuration, by imposing a simple radial electric potential profile via initial conditions.","The influence of this sheared background flow velocity on pressure-driven modes is interrogated, demonstrating a stabilizing effect when the shearing rate is comparable to the growth rate.","This effect is observed for multiple toroidal modes and at different viscosities, demonstrating that the stabilization occurs as a result of shear decorrelation.","Oscillations of the linear growth rate are observed in cases with higher flow speeds; this phenomenon is hypothesized to be due to phase misalignment between the poloidally coupled modes contributing to the ballooning mode.","Some indicators are provided to support this, however analysis of this phenomenon is ongoing."],"url":"http://arxiv.org/abs/2402.00458v1","category":"physics.plasm-ph"}
{"created":"2024-02-01 09:46:18","title":"The failure of H\u00f6lder regularity of solutions for the Euler-Poincar\u00e9 equations in Besov spaces","abstract":"In this paper, we investigate the continuity of solution to the Euler-Poincar\\'{e} equations. We show that the continuity of the solution cannot be improved to the H\\\"{o}lder continuity. That is, the solution of the Euler-Poincar\\'{e} equations with initial data $u_0\\in B^s_{p,r}$ belongs to $\\mathcal{C}([0,T];B^s_{p,r}(\\mathbb R^d))$ but not to $\\mathcal{C}^\\alpha([0,T];B^s_{p,r}(\\mathbb R^d))$ with any $\\alpha\\in(0,1)$.","sentences":["In this paper, we investigate the continuity of solution to the Euler-Poincar\\'{e} equations.","We show that the continuity of the solution cannot be improved to the H\\\"{o}lder continuity.","That is, the solution of the Euler-Poincar\\'{e} equations with initial data $u_0\\in B^s_{p,r}$ belongs to $\\mathcal{C}([0,T];B^s_{p,r}(\\mathbb R^d))$ but not to $\\mathcal{C}^\\alpha([0,T];B^s_{p,r}(\\mathbb R^d))$ with any $\\alpha\\in(0,1)$."],"url":"http://arxiv.org/abs/2402.00456v1","category":"math.AP"}
{"created":"2024-02-01 08:57:26","title":"Two-loop QCD amplitudes for $t\\bar{t}H$ production from boosted limit","abstract":"The production of a Higgs boson in association with a top-antitop quark pair ($t\\bar{t}H$) holds significant importance in directly probing the top-quark Yukawa coupling, which is related to various fundamental questions in high energy physics. This paper focuses on the calculation of two-loop amplitudes for $t\\bar t H$ production at hadron colliders in the high-energy boosted limit. The calculation employs our recently developed mass-factorization formula. To validate the accuracy of our approximate methods, we compare our results for the one-loop amplitudes and the two-loop infrared poles with the exact calculations. We then provide predictions for the finite parts of the two-loop amplitudes. By combining the contributions from real emissions, our results can be utilized to compute the next-to-next-to-leading order differential cross sections for $t\\bar t H$ production in the high-energy boosted limit.","sentences":["The production of a Higgs boson in association with a top-antitop quark pair ($t\\bar{t}H$) holds significant importance in directly probing the top-quark Yukawa coupling, which is related to various fundamental questions in high energy physics.","This paper focuses on the calculation of two-loop amplitudes for $t\\bar t H$ production at hadron colliders in the high-energy boosted limit.","The calculation employs our recently developed mass-factorization formula.","To validate the accuracy of our approximate methods, we compare our results for the one-loop amplitudes and the two-loop infrared poles with the exact calculations.","We then provide predictions for the finite parts of the two-loop amplitudes.","By combining the contributions from real emissions, our results can be utilized to compute the next-to-next-to-leading order differential cross sections for $t\\bar t H$ production in the high-energy boosted limit."],"url":"http://arxiv.org/abs/2402.00431v1","category":"hep-ph"}
{"created":"2024-02-01 08:48:12","title":"Genetic Programming Theory and Practice: A Fifteen-Year Trajectory","abstract":"The GPTP workshop series, which began in 2003, has served over the years as a focal meeting for genetic programming (GP) researchers. As such, we think it provides an excellent source for studying the development of GP over the past fifteen years. We thus present herein a trajectory of the thematic developments in the field of GP.","sentences":["The GPTP workshop series, which began in 2003, has served over the years as a focal meeting for genetic programming (GP) researchers.","As such, we think it provides an excellent source for studying the development of GP over the past fifteen years.","We thus present herein a trajectory of the thematic developments in the field of GP."],"url":"http://arxiv.org/abs/2402.00425v1","category":"cs.NE"}
{"created":"2024-02-01 08:39:31","title":"Lightweight Pixel Difference Networks for Efficient Visual Representation Learning","abstract":"Recently, there have been tremendous efforts in developing lightweight Deep Neural Networks (DNNs) with satisfactory accuracy, which can enable the ubiquitous deployment of DNNs in edge devices. The core challenge of developing compact and efficient DNNs lies in how to balance the competing goals of achieving high accuracy and high efficiency. In this paper we propose two novel types of convolutions, dubbed \\emph{Pixel Difference Convolution (PDC) and Binary PDC (Bi-PDC)} which enjoy the following benefits: capturing higher-order local differential information, computationally efficient, and able to be integrated with existing DNNs. With PDC and Bi-PDC, we further present two lightweight deep networks named \\emph{Pixel Difference Networks (PiDiNet)} and \\emph{Binary PiDiNet (Bi-PiDiNet)} respectively to learn highly efficient yet more accurate representations for visual tasks including edge detection and object recognition. Extensive experiments on popular datasets (BSDS500, ImageNet, LFW, YTF, \\emph{etc.}) show that PiDiNet and Bi-PiDiNet achieve the best accuracy-efficiency trade-off. For edge detection, PiDiNet is the first network that can be trained without ImageNet, and can achieve the human-level performance on BSDS500 at 100 FPS and with $<$1M parameters. For object recognition, among existing Binary DNNs, Bi-PiDiNet achieves the best accuracy and a nearly $2\\times$ reduction of computational cost on ResNet18. Code available at \\href{https://github.com/hellozhuo/pidinet}{https://github.com/hellozhuo/pidinet}.","sentences":["Recently, there have been tremendous efforts in developing lightweight Deep Neural Networks (DNNs) with satisfactory accuracy, which can enable the ubiquitous deployment of DNNs in edge devices.","The core challenge of developing compact and efficient DNNs lies in how to balance the competing goals of achieving high accuracy and high efficiency.","In this paper we propose two novel types of convolutions, dubbed \\emph{Pixel Difference Convolution (PDC) and Binary PDC (Bi-PDC)} which enjoy the following benefits: capturing higher-order local differential information, computationally efficient, and able to be integrated with existing DNNs.","With PDC and Bi-PDC, we further present two lightweight deep networks named \\emph{Pixel Difference Networks (PiDiNet)} and \\emph{Binary PiDiNet (Bi-PiDiNet)} respectively to learn highly efficient yet more accurate representations for visual tasks including edge detection and object recognition.","Extensive experiments on popular datasets (BSDS500, ImageNet, LFW, YTF, \\emph{etc.}) show that PiDiNet and Bi-PiDiNet achieve the best accuracy-efficiency trade-off.","For edge detection, PiDiNet is the first network that can be trained without ImageNet, and can achieve the human-level performance on BSDS500 at 100 FPS and with $<$1M parameters.","For object recognition, among existing Binary DNNs, Bi-PiDiNet achieves the best accuracy and a nearly $2\\times$ reduction of computational cost on ResNet18.","Code available at \\href{https://github.com/hellozhuo/pidinet}{https://github.com/hellozhuo/pidinet}."],"url":"http://arxiv.org/abs/2402.00422v1","category":"cs.CV"}
{"created":"2024-02-01 08:16:59","title":"A two-phase Volume of Fluid approach to model rigid-perfectly plastic granular materials","abstract":"Granular flow problems characterized by large deformations are widespread in various applications, including coastal and geotechnical engineering. The paper deals with the application of a rigid-perfectly plastic two-phase model extended by the Drucker-Prager yield criterion to simulate granular media with a finite volume flow solver (FV). The model refers to the combination of a Bingham fluid and an Eulerian strain measure to assess the failure region of granular dam slides. A monolithic volume-of-fluid (VoF) method is used to distinguish between the air and granular phases, both governed by the incompressible Navier-Stokes equations. The numerical framework enables modeling of large displacements and arbitrary shapes for large-scale applications. The displayed validation and verification focuses on the rigid-perfectly plastic material model for non-cohesive and cohesive materials with varying angles of repose. Results indicate a good agreement of the predicted soil surface and strain results with experimental and numerical data.","sentences":["Granular flow problems characterized by large deformations are widespread in various applications, including coastal and geotechnical engineering.","The paper deals with the application of a rigid-perfectly plastic two-phase model extended by the Drucker-Prager yield criterion to simulate granular media with a finite volume flow solver (FV).","The model refers to the combination of a Bingham fluid and an Eulerian strain measure to assess the failure region of granular dam slides.","A monolithic volume-of-fluid (VoF) method is used to distinguish between the air and granular phases, both governed by the incompressible Navier-Stokes equations.","The numerical framework enables modeling of large displacements and arbitrary shapes for large-scale applications.","The displayed validation and verification focuses on the rigid-perfectly plastic material model for non-cohesive and cohesive materials with varying angles of repose.","Results indicate a good agreement of the predicted soil surface and strain results with experimental and numerical data."],"url":"http://arxiv.org/abs/2402.00415v1","category":"physics.flu-dyn"}
{"created":"2024-02-01 07:01:54","title":"On the design-dependent suboptimality of the Lasso","abstract":"This paper investigates the effect of the design matrix on the ability (or inability) to estimate a sparse parameter in linear regression. More specifically, we characterize the optimal rate of estimation when the smallest singular value of the design matrix is bounded away from zero. In addition to this information-theoretic result, we provide and analyze a procedure which is simultaneously statistically optimal and computationally efficient, based on soft thresholding the ordinary least squares estimator. Most surprisingly, we show that the Lasso estimator -- despite its widespread adoption for sparse linear regression -- is provably minimax rate-suboptimal when the minimum singular value is small. We present a family of design matrices and sparse parameters for which we can guarantee that the Lasso with any choice of regularization parameter -- including those which are data-dependent and randomized -- would fail in the sense that its estimation rate is suboptimal by polynomial factors in the sample size. Our lower bound is strong enough to preclude the statistical optimality of all forms of the Lasso, including its highly popular penalized, norm-constrained, and cross-validated variants.","sentences":["This paper investigates the effect of the design matrix on the ability (or inability) to estimate a sparse parameter in linear regression.","More specifically, we characterize the optimal rate of estimation when the smallest singular value of the design matrix is bounded away from zero.","In addition to this information-theoretic result, we provide and analyze a procedure which is simultaneously statistically optimal and computationally efficient, based on soft thresholding the ordinary least squares estimator.","Most surprisingly, we show that the Lasso estimator -- despite its widespread adoption for sparse linear regression -- is provably minimax rate-suboptimal when the minimum singular value is small.","We present a family of design matrices and sparse parameters for which we can guarantee that the Lasso with any choice of regularization parameter -- including those which are data-dependent and randomized -- would fail in the sense that its estimation rate is suboptimal by polynomial factors in the sample size.","Our lower bound is strong enough to preclude the statistical optimality of all forms of the Lasso, including its highly popular penalized, norm-constrained, and cross-validated variants."],"url":"http://arxiv.org/abs/2402.00382v1","category":"math.ST"}
{"created":"2024-02-01 06:34:35","title":"Disentangled Multimodal Brain MR Image Translation via Transformer-based Modality Infuser","abstract":"Multimodal Magnetic Resonance (MR) Imaging plays a crucial role in disease diagnosis due to its ability to provide complementary information by analyzing a relationship between multimodal images on the same subject. Acquiring all MR modalities, however, can be expensive, and, during a scanning session, certain MR images may be missed depending on the study protocol. The typical solution would be to synthesize the missing modalities from the acquired images such as using generative adversarial networks (GANs). Yet, GANs constructed with convolutional neural networks (CNNs) are likely to suffer from a lack of global relationships and mechanisms to condition the desired modality. To address this, in this work, we propose a transformer-based modality infuser designed to synthesize multimodal brain MR images. In our method, we extract modality-agnostic features from the encoder and then transform them into modality-specific features using the modality infuser. Furthermore, the modality infuser captures long-range relationships among all brain structures, leading to the generation of more realistic images. We carried out experiments on the BraTS 2018 dataset, translating between four MR modalities, and our experimental results demonstrate the superiority of our proposed method in terms of synthesis quality. In addition, we conducted experiments on a brain tumor segmentation task and different conditioning methods.","sentences":["Multimodal Magnetic Resonance (MR) Imaging plays a crucial role in disease diagnosis due to its ability to provide complementary information by analyzing a relationship between multimodal images on the same subject.","Acquiring all MR modalities, however, can be expensive, and, during a scanning session, certain MR images may be missed depending on the study protocol.","The typical solution would be to synthesize the missing modalities from the acquired images such as using generative adversarial networks (GANs).","Yet, GANs constructed with convolutional neural networks (CNNs) are likely to suffer from a lack of global relationships and mechanisms to condition the desired modality.","To address this, in this work, we propose a transformer-based modality infuser designed to synthesize multimodal brain MR images.","In our method, we extract modality-agnostic features from the encoder and then transform them into modality-specific features using the modality infuser.","Furthermore, the modality infuser captures long-range relationships among all brain structures, leading to the generation of more realistic images.","We carried out experiments on the BraTS 2018 dataset, translating between four MR modalities, and our experimental results demonstrate the superiority of our proposed method in terms of synthesis quality.","In addition, we conducted experiments on a brain tumor segmentation task and different conditioning methods."],"url":"http://arxiv.org/abs/2402.00375v1","category":"eess.IV"}
{"created":"2024-02-01 06:02:54","title":"A parallel domain decomposition method for solving elliptic equations on manifolds","abstract":"We propose a new numerical domain decomposition method for solving elliptic equations on compact Riemannian manifolds. One advantage of this method is its ability to bypass the need for global triangulations or grids on the manifolds. Additionally, it features a highly parallel iterative scheme. To verify its efficacy, we conduct numerical experiments on some $4$-dimensional manifolds without and with boundary.","sentences":["We propose a new numerical domain decomposition method for solving elliptic equations on compact Riemannian manifolds.","One advantage of this method is its ability to bypass the need for global triangulations or grids on the manifolds.","Additionally, it features a highly parallel iterative scheme.","To verify its efficacy, we conduct numerical experiments on some $4$-dimensional manifolds without and with boundary."],"url":"http://arxiv.org/abs/2402.00364v1","category":"math.NA"}
{"created":"2024-02-01 04:38:36","title":"Constraining the Yukawa Gravity with Post Newtonian Approximation using S-star Orbits around the Supermassive Black Hole in our Galactic Center","abstract":"A number of modified gravity theories (e.g., $f(R)$-gravity) lead to a Yukawa-like metric in the weak field limit which can be described by two Yukawa parameters, i.e., the strength $\\kappa$ and the length scale $\\lambda$. The S-stars, orbiting around the supermassive black hole in the Galactic Center, are unique probes to test these gravity theories in relatively strong gravitational field. The Newtonian Yukawa gravity potential or a simple approximation to the Yukawa metric was usually adopted in previous studies when using the orbital motion of S-stars to constrain such modified gravity theories, which may be not sufficiently accurate considering recent and future high resolution observations. In this paper, we first derive the Post-Newtonian (PN) Yukawa motion equation at the 2PN order, and then investigate the high order effects on the orbital motions by comparison with those from the Newtonian Yukawa gravity potential. We further obtain constraints on $\\kappa$ by using the observations on the orbital motions of several S-stars (i.e., S2, S38, and S55). Our results show that the current observations of these stars are compatible with the General Relativity and $\\kappa$ can be constrained to $|\\kappa|<0.01$ with $95\\%$ confidence if $\\lambda\\in(100,250)$\\,AU. We also estimate the possible improvements (about an order of magnitude or more) to the constraints by future higher resolution observations and the inclusion of closer S-stars, such as S4716.","sentences":["A number of modified gravity theories (e.g., $f(R)$-gravity) lead to a Yukawa-like metric in the weak field limit which can be described by two Yukawa parameters, i.e., the strength $\\kappa$ and the length scale $\\lambda$. The S-stars, orbiting around the supermassive black hole in the Galactic Center, are unique probes to test these gravity theories in relatively strong gravitational field.","The Newtonian Yukawa gravity potential or a simple approximation to the Yukawa metric was usually adopted in previous studies when using the orbital motion of S-stars to constrain such modified gravity theories, which may be not sufficiently accurate considering recent and future high resolution observations.","In this paper, we first derive the Post-Newtonian (PN) Yukawa motion equation at the 2PN order, and then investigate the high order effects on the orbital motions by comparison with those from the Newtonian Yukawa gravity potential.","We further obtain constraints on $\\kappa$ by using the observations on the orbital motions of several S-stars (i.e., S2, S38, and S55).","Our results show that the current observations of these stars are compatible with the General Relativity and $\\kappa$ can be constrained to $|\\kappa|<0.01$ with $95\\%$ confidence if $\\lambda\\in(100,250)$\\,AU.","We also estimate the possible improvements (about an order of magnitude or more) to the constraints by future higher resolution observations and the inclusion of closer S-stars, such as S4716."],"url":"http://arxiv.org/abs/2402.00333v1","category":"gr-qc"}
{"created":"2024-02-01 04:16:31","title":"HAYATE: Photometric redshift estimation by hybridising machine learning with template fitting","abstract":"Machine learning photo-z methods, trained directly on spectroscopic redshifts, provide a viable alternative to traditional template fitting methods but may not generalise well on new data that deviates from that in the training set. In this work, we present a Hybrid Algorithm for WI(Y)de-range photo-z estimation with Artificial neural networks and TEmplate fitting (HAYATE), a novel photo-z method that combines template fitting and data-driven approaches and whose training loss is optimised in terms of both redshift point estimates and probability distributions. We produce artificial training data from low-redshift galaxy SEDs at z<1.3, artificially redshifted up to z=5. We test the model on data from the ZFOURGE surveys, demonstrating that HAYATE can function as a reliable emulator of EAZY for the broad redshift range beyond the region of sufficient spectroscopic completeness. The network achieves precise photo-z estimations with smaller errors ($\\sigma_{NMAD}$) than EAZY in the initial low-z region (z<1.3), while being comparable even in the high-z extrapolated regime (1.3<z<5). Meanwhile, it provides more robust photo-z estimations than EAZY with the lower outlier rate ($\\eta_{0.2}\\lesssim 1\\%$) but runs $\\sim100$ times faster than the original template fitting method. We also demonstrate HAYATE offers more reliable redshift PDFs, showing a flatter distribution of Probability Integral Transform scores than EAZY. The performance is further improved using transfer learning with spec-z samples. We expect that future large surveys will benefit from our novel methodology applicable to observations over a wide redshift range.","sentences":["Machine learning photo-z methods, trained directly on spectroscopic redshifts, provide a viable alternative to traditional template fitting methods but may not generalise well on new data that deviates from that in the training set.","In this work, we present a Hybrid Algorithm for WI(Y)de-range photo-z estimation with Artificial neural networks and TEmplate fitting (HAYATE), a novel photo-z method that combines template fitting and data-driven approaches and whose training loss is optimised in terms of both redshift point estimates and probability distributions.","We produce artificial training data from low-redshift galaxy SEDs at z<1.3, artificially redshifted up to z=5.","We test the model on data from the ZFOURGE surveys, demonstrating that HAYATE can function as a reliable emulator of EAZY for the broad redshift range beyond the region of sufficient spectroscopic completeness.","The network achieves precise photo-z estimations with smaller errors ($\\sigma_{NMAD}$) than EAZY in the initial low-z region (z<1.3), while being comparable even in the high-z extrapolated regime (1.3<z<5).","Meanwhile, it provides more robust photo-z estimations than EAZY with the lower outlier rate ($\\eta_{0.2}\\lesssim 1\\%$) but runs $\\sim100$ times faster than the original template fitting method.","We also demonstrate HAYATE offers more reliable redshift PDFs, showing a flatter distribution of Probability Integral Transform scores than EAZY.","The performance is further improved using transfer learning with spec-z samples.","We expect that future large surveys will benefit from our novel methodology applicable to observations over a wide redshift range."],"url":"http://arxiv.org/abs/2402.00323v1","category":"astro-ph.IM"}
{"created":"2024-02-01 03:57:31","title":"Steady gradient Ricci solitons with nonnegative curvature operator away from a compact set","abstract":"Let $(M^n,g)$ $(n\\ge 4)$ be a complete noncompact $\\kappa$-noncollapsed steady Ricci soliton with $\\rm{Rm}\\geq 0$ and $\\rm{Ric}> 0$ away from a compact set $K$ of $M$.   We prove that there is no any $(n-1)$-dimensional compact split limit Ricci flow of type I arising from the blow-down of $(M, g)$, if there is an $(n-1)$-dimensional noncompact split limit Ricci flow.   Consequently, the compact split limit ancient flows of type I and type II cannot occur simultaneously from the blow-down.   As an application, we prove that $(M^n,g)$ with $\\rm{Rm}\\geq 0$ must be isometric the Bryant Ricci soliton up to scaling, if there exists a sequence of rescaled Ricci flows $(M,g_{p_i}(t); p_i)$ of $(M,g)$ converges subsequently to a family of shrinking quotient cylinders.","sentences":["Let $(M^n,g)$ $(n\\ge 4)$ be a complete noncompact $\\kappa$-noncollapsed steady Ricci soliton with $\\rm{Rm}\\geq 0$ and $\\rm{Ric}> 0","$ away from a compact set $K$ of $M$.   We prove that there is no any $(n-1)$-dimensional compact split limit Ricci flow of type I arising from the blow-down of $(M, g)$, if there is an $(n-1)$-dimensional noncompact split limit Ricci flow.   ","Consequently, the compact split limit ancient flows of type I and type II cannot occur simultaneously from the blow-down.   ","As an application, we prove that $(M^n,g)$ with $\\rm{Rm}\\geq 0$ must be isometric the Bryant Ricci soliton up to scaling, if there exists a sequence of rescaled Ricci flows $(M,g_{p_i}(t); p_i)$ of $(M,g)$ converges subsequently to a family of shrinking quotient cylinders."],"url":"http://arxiv.org/abs/2402.00316v1","category":"math.DG"}
{"created":"2024-02-01 03:49:10","title":"Seismic Traveltime Tomography with Label-free Learning","abstract":"Deep learning techniques have been used to build velocity models (VMs) for seismic traveltime tomography and have shown encouraging performance in recent years. However, they need to generate labeled samples (i.e., pairs of input and label) to train the deep neural network (NN) with end-to-end learning, and the real labels for field data inversion are usually missing or very expensive. Some traditional tomographic methods can be implemented quickly, but their effectiveness is often limited by prior assumptions. To avoid generating labeled samples, we propose a novel method by integrating deep learning and dictionary learning to enhance the VMs with low resolution by using the traditional tomography-least square method (LSQR). We first design a type of shallow and simple NN to reduce computational cost followed by proposing a two-step strategy to enhance the VMs with low resolution: (1) Warming up. An initial dictionary is trained from the estimation by LSQR through dictionary learning method; (2) Dictionary optimization. The initial dictionary obtained in the warming-up step will be optimized by the NN, and then it will be used to reconstruct high-resolution VMs with the reference slowness and the estimation by LSQR. Furthermore, we design a loss function to minimize traveltime misfit to ensure that NN training is label-free, and the optimized dictionary can be obtained after each epoch of NN training. We demonstrate the effectiveness of the proposed method through numerical tests.","sentences":["Deep learning techniques have been used to build velocity models (VMs) for seismic traveltime tomography and have shown encouraging performance in recent years.","However, they need to generate labeled samples (i.e., pairs of input and label) to train the deep neural network (NN) with end-to-end learning, and the real labels for field data inversion are usually missing or very expensive.","Some traditional tomographic methods can be implemented quickly, but their effectiveness is often limited by prior assumptions.","To avoid generating labeled samples, we propose a novel method by integrating deep learning and dictionary learning to enhance the VMs with low resolution by using the traditional tomography-least square method (LSQR).","We first design a type of shallow and simple NN to reduce computational cost followed by proposing a two-step strategy to enhance the VMs with low resolution: (1) Warming up.","An initial dictionary is trained from the estimation by LSQR through dictionary learning method; (2) Dictionary optimization.","The initial dictionary obtained in the warming-up step will be optimized by the NN, and then it will be used to reconstruct high-resolution VMs with the reference slowness and the estimation by LSQR.","Furthermore, we design a loss function to minimize traveltime misfit to ensure that NN training is label-free, and the optimized dictionary can be obtained after each epoch of NN training.","We demonstrate the effectiveness of the proposed method through numerical tests."],"url":"http://arxiv.org/abs/2402.00310v1","category":"physics.geo-ph"}
{"created":"2024-02-01 03:34:48","title":"Invariance-powered Trustworthy Defense via Remove Then Restore","abstract":"Adversarial attacks pose a challenge to the deployment of deep neural networks (DNNs), while previous defense models overlook the generalization to various attacks. Inspired by targeted therapies for cancer, we view adversarial samples as local lesions of natural benign samples, because a key finding is that salient attack in an adversarial sample dominates the attacking process, while trivial attack unexpectedly provides trustworthy evidence for obtaining generalizable robustness. Based on this finding, a Pixel Surgery and Semantic Regeneration (PSSR) model following the targeted therapy mechanism is developed, which has three merits: 1) To remove the salient attack, a score-based Pixel Surgery module is proposed, which retains the trivial attack as a kind of invariance information. 2) To restore the discriminative content, a Semantic Regeneration module based on a conditional alignment extrapolator is proposed, which achieves pixel and semantic consistency. 3) To further harmonize robustness and accuracy, an intractable problem, a self-augmentation regularizer with adversarial R-drop is designed. Experiments on numerous benchmarks show the superiority of PSSR.","sentences":["Adversarial attacks pose a challenge to the deployment of deep neural networks (DNNs), while previous defense models overlook the generalization to various attacks.","Inspired by targeted therapies for cancer, we view adversarial samples as local lesions of natural benign samples, because a key finding is that salient attack in an adversarial sample dominates the attacking process, while trivial attack unexpectedly provides trustworthy evidence for obtaining generalizable robustness.","Based on this finding, a Pixel Surgery and Semantic Regeneration (PSSR) model following the targeted therapy mechanism is developed, which has three merits: 1) To remove the salient attack, a score-based Pixel Surgery module is proposed, which retains the trivial attack as a kind of invariance information.","2) To restore the discriminative content, a Semantic Regeneration module based on a conditional alignment extrapolator is proposed, which achieves pixel and semantic consistency.","3) To further harmonize robustness and accuracy, an intractable problem, a self-augmentation regularizer with adversarial R-drop is designed.","Experiments on numerous benchmarks show the superiority of PSSR."],"url":"http://arxiv.org/abs/2402.00304v1","category":"cs.CV"}
{"created":"2024-02-01 03:20:53","title":"Attention-based Dynamic Multilayer Graph Neural Networks for Loan Default Prediction","abstract":"Whereas traditional credit scoring tends to employ only individual borrower- or loan-level predictors, it has been acknowledged for some time that connections between borrowers may result in default risk propagating over a network. In this paper, we present a model for credit risk assessment leveraging a dynamic multilayer network built from a Graph Neural Network and a Recurrent Neural Network, each layer reflecting a different source of network connection. We test our methodology in a behavioural credit scoring context using a dataset provided by U.S. mortgage financier Freddie Mac, in which different types of connections arise from the geographical location of the borrower and their choice of mortgage provider. The proposed model considers both types of connections and the evolution of these connections over time. We enhance the model by using a custom attention mechanism that weights the different time snapshots according to their importance. After testing multiple configurations, a model with GAT, LSTM, and the attention mechanism provides the best results. Empirical results demonstrate that, when it comes to predicting probability of default for the borrowers, our proposed model brings both better results and novel insights for the analysis of the importance of connections and timestamps, compared to traditional methods.","sentences":["Whereas traditional credit scoring tends to employ only individual borrower- or loan-level predictors, it has been acknowledged for some time that connections between borrowers may result in default risk propagating over a network.","In this paper, we present a model for credit risk assessment leveraging a dynamic multilayer network built from a Graph Neural Network and a Recurrent Neural Network, each layer reflecting a different source of network connection.","We test our methodology in a behavioural credit scoring context using a dataset provided by U.S. mortgage financier Freddie Mac, in which different types of connections arise from the geographical location of the borrower and their choice of mortgage provider.","The proposed model considers both types of connections and the evolution of these connections over time.","We enhance the model by using a custom attention mechanism that weights the different time snapshots according to their importance.","After testing multiple configurations, a model with GAT, LSTM, and the attention mechanism provides the best results.","Empirical results demonstrate that, when it comes to predicting probability of default for the borrowers, our proposed model brings both better results and novel insights for the analysis of the importance of connections and timestamps, compared to traditional methods."],"url":"http://arxiv.org/abs/2402.00299v1","category":"q-fin.GN"}
{"created":"2024-02-01 02:29:40","title":"Pion Electromagnetic Form Factor from Bethe-Salpeter Amplitudes with Appropriate Kinematics","abstract":"Within the framework provided by quantum chromodynamics's Schwinger-Dyson equations (SDEs), the pion's electromagnetic form factor is computed using solutions of the Bethe-Salpeter equation (BSE) that have finite spacial momentum, and therefore allow the appropriate kinematics for a given momentum transfer. This removes, for the first time, a limiting approximation in previous SDE calculations where rest-frame solutions to the BSE are used for both the initial and final pion states. In performing these calculations, the rainbow-ladder truncation to the SDEs in the Landau gauge is used, with the quark-gluon interaction given by the Maris-Tandy model. Using Bethe-Salpeter amplitudes (BSAs) that have the correct spacial momentum for a given momentum transfer, $Q^2$, has a dramatic impact on results for the pion's electromagnetic form factor. The difference between results that use rest-frame BSAs and those with correct kinematics is less that 10\\% for $0 \\leqslant Q^2 \\lesssim 1\\,$GeV$^2$, however, these corrections grow with increasing momentum transfer and approach 100\\% for $Q^2 \\simeq 3\\,$GeV$^2$.","sentences":["Within the framework provided by quantum chromodynamics's Schwinger-Dyson equations (SDEs), the pion's electromagnetic form factor is computed using solutions of the Bethe-Salpeter equation (BSE) that have finite spacial momentum, and therefore allow the appropriate kinematics for a given momentum transfer.","This removes, for the first time, a limiting approximation in previous SDE calculations where rest-frame solutions to the BSE are used for both the initial and final pion states.","In performing these calculations, the rainbow-ladder truncation to the SDEs in the Landau gauge is used, with the quark-gluon interaction given by the Maris-Tandy model.","Using Bethe-Salpeter amplitudes (BSAs) that have the correct spacial momentum for a given momentum transfer, $Q^2$, has a dramatic impact on results for the pion's electromagnetic form factor.","The difference between results that use rest-frame BSAs and those with correct kinematics is less that 10\\% for $0 \\leqslant Q^2 \\lesssim 1\\,$GeV$^2$, however, these corrections grow with increasing momentum transfer and approach 100\\% for $Q^2 \\simeq 3\\,$GeV$^2$."],"url":"http://arxiv.org/abs/2402.00285v1","category":"hep-ph"}
{"created":"2024-02-01 01:41:25","title":"Valuative invariants for linearized line bundles on a spherical variety","abstract":"We give formulas for various valuative invariants of linearized ample line bundles on a projective spherical variety. Then we show how to use these formulas to study $g$-solitons on a Fano spherical variety. As a corollary, we show that for a Fano horospherical manifold $X$, the corresponding cone $(K_{X})^{\\times}$ always admits a Calabi-Yau cone metric.","sentences":["We give formulas for various valuative invariants of linearized ample line bundles on a projective spherical variety.","Then we show how to use these formulas to study $g$-solitons on a Fano spherical variety.","As a corollary, we show that for a Fano horospherical manifold $X$, the corresponding cone $(K_{X})^{\\times}$ always admits a Calabi-Yau cone metric."],"url":"http://arxiv.org/abs/2402.00269v1","category":"math.AG"}
{"created":"2024-02-01 01:32:13","title":"Limits of Random Motzkin paths with KPZ related asymptotics","abstract":"We study Motzkin paths of length $L$ with general weights on the edges and end points. We investigate the limit behavior of the initial and final segments of the random Motzkin path viewed as a pair of processes starting from each of the two end points as $L$ becomes large. We then study macroscopic limits of the resulting processes, where in two different regimes we obtain Markov processes that appeared in the description of the stationary measure for the KPZ equation on the half line and of conjectural stationary measure of the hypothetical KPZ fixed point on the half line. Our results rely on the behavior of the Al-Salam--Chihara polynomials in the neighbourhood of the upper end of their orthogonality interval and on the limiting properties of the $q$-Pochhammer and $q$-Gamma functions as $q\\nearrow 1$.","sentences":["We study Motzkin paths of length $L$ with general weights on the edges and end points.","We investigate the limit behavior of the initial and final segments of the random Motzkin path viewed as a pair of processes starting from each of the two end points as $L$ becomes large.","We then study macroscopic limits of the resulting processes, where in two different regimes we obtain Markov processes that appeared in the description of the stationary measure for the KPZ equation on the half line and of conjectural stationary measure of the hypothetical KPZ fixed point on the half line.","Our results rely on the behavior of the Al-Salam--Chihara polynomials in the neighbourhood of the upper end of their orthogonality interval and on the limiting properties of the $q$-Pochhammer and $q$-Gamma functions as $q\\nearrow 1$."],"url":"http://arxiv.org/abs/2402.00265v1","category":"math.PR"}
{"created":"2024-02-01 01:11:15","title":"Understanding Neural Network Systems for Image Analysis using Vector Spaces and Inverse Maps","abstract":"There is strong interest in developing mathematical methods that can be used to understand complex neural networks used in image analysis. In this paper, we introduce techniques from Linear Algebra to model neural network layers as maps between signal spaces. First, we demonstrate how signal spaces can be used to visualize weight spaces and convolutional layer kernels. We also demonstrate how residual vector spaces can be used to further visualize information lost at each layer. Second, we introduce the concept of invertible networks and an algorithm for computing input images that yield specific outputs. We demonstrate our approach on two invertible networks and ResNet18.","sentences":["There is strong interest in developing mathematical methods that can be used to understand complex neural networks used in image analysis.","In this paper, we introduce techniques from Linear Algebra to model neural network layers as maps between signal spaces.","First, we demonstrate how signal spaces can be used to visualize weight spaces and convolutional layer kernels.","We also demonstrate how residual vector spaces can be used to further visualize information lost at each layer.","Second, we introduce the concept of invertible networks and an algorithm for computing input images that yield specific outputs.","We demonstrate our approach on two invertible networks and ResNet18."],"url":"http://arxiv.org/abs/2402.00261v1","category":"cs.CV"}
{"created":"2024-02-01 01:01:06","title":"Genus one WDVV solutions induced by the holomorphic differential and applications","abstract":"Consider the genus one Hurwitz space $\\mathcal{H}_1(n_0,\\dots,n_m)$ of ramified covering of fixed degree with $m+1$ prescribed poles of order $n_0+1,\\dots,n_m+1$, respectively. Based on a recent formula proved in \\cite{Rejeb23}, we derive an explicit solution to the WDVV equations associated with the genus one Dubrovin-Hurwitz-Frobenius manifold structure induced by the normalized holomorphic differential. The obtained solution is written in terms of Bell polynomials, Eisenstein series as well as the Weierstrass functions.","sentences":["Consider the genus one Hurwitz space $\\mathcal{H}_1(n_0,\\dots,n_m)$ of ramified covering of fixed degree with $m+1$ prescribed poles of order $n_0+1,\\dots,n_m+1$, respectively.","Based on a recent formula proved in \\cite{Rejeb23}, we derive an explicit solution to the WDVV equations associated with the genus one Dubrovin-Hurwitz-Frobenius manifold structure induced by the normalized holomorphic differential.","The obtained solution is written in terms of Bell polynomials, Eisenstein series as well as the Weierstrass functions."],"url":"http://arxiv.org/abs/2402.00256v1","category":"math-ph"}
{"created":"2024-02-01 00:29:54","title":"Reformulating polarized radiative transfer. (I) A consistent formalism allowing non-local Magnus solutions","abstract":"The physical diagnosis of the solar atmosphere is achieved by solving the polarized radiative transfer problem for plasmas in Non-Local Thermodynamical Equilibrium (NLTE). This scenario poses theoretical challenges for integrating the radiative transfer equation (RTE) efficiently. Namely, current methods are limited to constant propagation matrices, thus imposing local solutions. To spark significant advances, this paper lays the foundations of a formalism that achieves an efficient non-local integration of the RTE based on the Magnus expansion. First, we revisit the problem and its solutions in Jones and Stokes formalisms. Looking at them as equivalent representations of the Lorentz/Poincar\\'e group of rotations, we interpret the RTE in terms of Lie group theory to show the suitability of the Magnus expansion for obtaining non-local solutions. We then present a detailed algebraic characterization of the propagation matrix and combine it with the Magnus expansion to reformulate the homogenous solution to the RTE in Stokes formalism. Thus, we obtain a compact evolution operator supporting arbitrary variations of the propagation matrix to first order in the Magnus expansion. Finally, we reformulate the corresponding inhomogeneous solution as an equivalent homogeneous system, which is then solved with the Magnus expansion again. This gives the first efficient and consistent formal solution of the RTE that furthermore is non-local, natively accurate, and that separates the integration from the formal solution. Such disruptive formulation leads to a new whole family of numerical radiative transfer methods and suggests accelerating NLTE syntheses and inversions with non-local radiative transfer. With minor cosmetic changes, our results are valid for other universal physical problems sharing the Lorentz/Poincar\\'e algebra of the RTE and special relativity","sentences":["The physical diagnosis of the solar atmosphere is achieved by solving the polarized radiative transfer problem for plasmas in Non-Local Thermodynamical Equilibrium (NLTE).","This scenario poses theoretical challenges for integrating the radiative transfer equation (RTE) efficiently.","Namely, current methods are limited to constant propagation matrices, thus imposing local solutions.","To spark significant advances, this paper lays the foundations of a formalism that achieves an efficient non-local integration of the RTE based on the Magnus expansion.","First, we revisit the problem and its solutions in Jones and Stokes formalisms.","Looking at them as equivalent representations of the Lorentz/Poincar\\'e group of rotations, we interpret the RTE in terms of Lie group theory to show the suitability of the Magnus expansion for obtaining non-local solutions.","We then present a detailed algebraic characterization of the propagation matrix and combine it with the Magnus expansion to reformulate the homogenous solution to the RTE in Stokes formalism.","Thus, we obtain a compact evolution operator supporting arbitrary variations of the propagation matrix to first order in the Magnus expansion.","Finally, we reformulate the corresponding inhomogeneous solution as an equivalent homogeneous system, which is then solved with the Magnus expansion again.","This gives the first efficient and consistent formal solution of the RTE that furthermore is non-local, natively accurate, and that separates the integration from the formal solution.","Such disruptive formulation leads to a new whole family of numerical radiative transfer methods and suggests accelerating NLTE syntheses and inversions with non-local radiative transfer.","With minor cosmetic changes, our results are valid for other universal physical problems sharing the Lorentz/Poincar\\'e algebra of the RTE and special relativity"],"url":"http://arxiv.org/abs/2402.00252v1","category":"astro-ph.SR"}
{"created":"2024-01-31 23:52:14","title":"Capacity Constraint Analysis Using Object Detection for Smart Manufacturing","abstract":"The increasing popularity of Deep Learning (DL) based Object Detection (OD) methods and their real-world applications have opened new venues in smart manufacturing. Traditional industries struck by capacity constraints after Coronavirus Disease (COVID-19) require non-invasive methods for in-depth operations' analysis to optimize and increase their revenue. In this study, we have initially developed a Convolutional Neural Network (CNN) based OD model to tackle this issue. This model is trained to accurately identify the presence of chairs and individuals on the production floor. The identified objects are then passed to the CNN based tracker, which tracks them throughout their life cycle in the workstation. The extracted meta-data is further processed through a novel framework for the capacity constraint analysis. We identified that the Station C is only 70.6% productive through 6 months. Additionally, the time spent at each station is recorded and aggregated for each object. This data proves helpful in conducting annual audits and effectively managing labor and material over time.","sentences":["The increasing popularity of Deep Learning (DL) based Object Detection (OD) methods and their real-world applications have opened new venues in smart manufacturing.","Traditional industries struck by capacity constraints after Coronavirus Disease (COVID-19) require non-invasive methods for in-depth operations' analysis to optimize and increase their revenue.","In this study, we have initially developed a Convolutional Neural Network (CNN) based OD model to tackle this issue.","This model is trained to accurately identify the presence of chairs and individuals on the production floor.","The identified objects are then passed to the CNN based tracker, which tracks them throughout their life cycle in the workstation.","The extracted meta-data is further processed through a novel framework for the capacity constraint analysis.","We identified that the Station C is only 70.6% productive through 6 months.","Additionally, the time spent at each station is recorded and aggregated for each object.","This data proves helpful in conducting annual audits and effectively managing labor and material over time."],"url":"http://arxiv.org/abs/2402.00243v1","category":"cs.CV"}
{"created":"2024-01-31 23:48:48","title":"Spectral Norm of Convolutional Layers with Circular and Zero Paddings","abstract":"This paper leverages the use of \\emph{Gram iteration} an efficient, deterministic, and differentiable method for computing spectral norm with an upper bound guarantee. Designed for circular convolutional layers, we generalize the use of the Gram iteration to zero padding convolutional layers and prove its quadratic convergence. We also provide theorems for bridging the gap between circular and zero padding convolution's spectral norm. We design a \\emph{spectral rescaling} that can be used as a competitive $1$-Lipschitz layer that enhances network robustness. Demonstrated through experiments, our method outperforms state-of-the-art techniques in precision, computational cost, and scalability. The code of experiments is available at https://github.com/blaisedelattre/lip4conv.","sentences":["This paper leverages the use of \\emph{Gram iteration} an efficient, deterministic, and differentiable method for computing spectral norm with an upper bound guarantee.","Designed for circular convolutional layers, we generalize the use of the Gram iteration to zero padding convolutional layers and prove its quadratic convergence.","We also provide theorems for bridging the gap between circular and zero padding convolution's spectral norm.","We design a \\emph{spectral rescaling} that can be used as a competitive $1$-Lipschitz layer that enhances network robustness.","Demonstrated through experiments, our method outperforms state-of-the-art techniques in precision, computational cost, and scalability.","The code of experiments is available at https://github.com/blaisedelattre/lip4conv."],"url":"http://arxiv.org/abs/2402.00240v1","category":"cs.LG"}
{"created":"2024-02-01 18:54:34","title":"Early Time Classification with Accumulated Accuracy Gap Control","abstract":"Early time classification algorithms aim to label a stream of features without processing the full input stream, while maintaining accuracy comparable to that achieved by applying the classifier to the entire input. In this paper, we introduce a statistical framework that can be applied to any sequential classifier, formulating a calibrated stopping rule. This data-driven rule attains finite-sample, distribution-free control of the accuracy gap between full and early-time classification. We start by presenting a novel method that builds on the Learn-then-Test calibration framework to control this gap marginally, on average over i.i.d. instances. As this algorithm tends to yield an excessively high accuracy gap for early halt times, our main contribution is the proposal of a framework that controls a stronger notion of error, where the accuracy gap is controlled conditionally on the accumulated halt times. Numerical experiments demonstrate the effectiveness, applicability, and usefulness of our method. We show that our proposed early stopping mechanism reduces up to 94% of timesteps used for classification while achieving rigorous accuracy gap control.","sentences":["Early time classification algorithms aim to label a stream of features without processing the full input stream, while maintaining accuracy comparable to that achieved by applying the classifier to the entire input.","In this paper, we introduce a statistical framework that can be applied to any sequential classifier, formulating a calibrated stopping rule.","This data-driven rule attains finite-sample, distribution-free control of the accuracy gap between full and early-time classification.","We start by presenting a novel method that builds on the Learn-then-Test calibration framework to control this gap marginally, on average over i.i.d. instances.","As this algorithm tends to yield an excessively high accuracy gap for early halt times, our main contribution is the proposal of a framework that controls a stronger notion of error, where the accuracy gap is controlled conditionally on the accumulated halt times.","Numerical experiments demonstrate the effectiveness, applicability, and usefulness of our method.","We show that our proposed early stopping mechanism reduces up to 94% of timesteps used for classification while achieving rigorous accuracy gap control."],"url":"http://arxiv.org/abs/2402.00857v1","category":"cs.LG"}
{"created":"2024-02-01 18:38:55","title":"BootsTAP: Bootstrapped Training for Tracking-Any-Point","abstract":"To endow models with greater understanding of physics and motion, it is useful to enable them to perceive how solid surfaces move and deform in real scenes. This can be formalized as Tracking-Any-Point (TAP), which requires the algorithm to be able to track any point corresponding to a solid surface in a video, potentially densely in space and time. Large-scale ground-truth training data for TAP is only available in simulation, which currently has limited variety of objects and motion. In this work, we demonstrate how large-scale, unlabeled, uncurated real-world data can improve a TAP model with minimal architectural changes, using a self-supervised student-teacher setup. We demonstrate state-of-the-art performance on the TAP-Vid benchmark surpassing previous results by a wide margin: for example, TAP-Vid-DAVIS performance improves from 61.3% to 66.4%, and TAP-Vid-Kinetics from 57.2% to 61.5%.","sentences":["To endow models with greater understanding of physics and motion, it is useful to enable them to perceive how solid surfaces move and deform in real scenes.","This can be formalized as Tracking-Any-Point (TAP), which requires the algorithm to be able to track any point corresponding to a solid surface in a video, potentially densely in space and time.","Large-scale ground-truth training data for TAP is only available in simulation, which currently has limited variety of objects and motion.","In this work, we demonstrate how large-scale, unlabeled, uncurated real-world data can improve a TAP model with minimal architectural changes, using a self-supervised student-teacher setup.","We demonstrate state-of-the-art performance on the TAP-Vid benchmark surpassing previous results by a wide margin: for example, TAP-Vid-DAVIS performance improves from 61.3% to 66.4%, and TAP-Vid-Kinetics from 57.2% to 61.5%."],"url":"http://arxiv.org/abs/2402.00847v1","category":"cs.CV"}
{"created":"2024-02-01 17:04:04","title":"Mesh motion in fluid-structure interaction with deep operator networks","abstract":"A mesh motion model based on deep operator networks is presented. The model is trained on and evaluated against a biharmonic mesh motion model on a fluid-structure interaction benchmark problem and further evaluated in a setting where biharmonic mesh motion fails. The performance of the proposed mesh motion model is comparable to the biharmonic mesh motion on the test problems.","sentences":["A mesh motion model based on deep operator networks is presented.","The model is trained on and evaluated against a biharmonic mesh motion model on a fluid-structure interaction benchmark problem and further evaluated in a setting where biharmonic mesh motion fails.","The performance of the proposed mesh motion model is comparable to the biharmonic mesh motion on the test problems."],"url":"http://arxiv.org/abs/2402.00774v1","category":"math.NA"}
{"created":"2024-02-01 16:39:45","title":"Benefits of Transformer: In-Context Learning in Linear Regression Tasks with Unstructured Data","abstract":"In practice, it is observed that transformer-based models can learn concepts in context in the inference stage. While existing literature, e.g., \\citet{zhang2023trained,huang2023context}, provide theoretical explanations on this in-context learning ability, they assume the input $x_i$ and the output $y_i$ for each sample are embedded in the same token (i.e., structured data). However, in reality, they are presented in two tokens (i.e., unstructured data \\cite{wibisono2023role}). In this case, this paper conducts experiments in linear regression tasks to study the benefits of the architecture of transformers and provides some corresponding theoretical intuitions to explain why the transformer can learn from unstructured data. We study the exact components in a transformer that facilitate the in-context learning. In particular, we observe that (1) a transformer with two layers of softmax (self-)attentions with look-ahead attention mask can learn from the prompt if $y_i$ is in the token next to $x_i$ for each example; (2) positional encoding can further improve the performance; and (3) multi-head attention with a high input embedding dimension has a better prediction performance than single-head attention.","sentences":["In practice, it is observed that transformer-based models can learn concepts in context in the inference stage.","While existing literature, e.g., \\citet{zhang2023trained,huang2023context}, provide theoretical explanations on this in-context learning ability, they assume the input $x_i$ and the output $y_i$ for each sample are embedded in the same token (i.e., structured data).","However, in reality, they are presented in two tokens (i.e., unstructured data \\cite{wibisono2023role}).","In this case, this paper conducts experiments in linear regression tasks to study the benefits of the architecture of transformers and provides some corresponding theoretical intuitions to explain why the transformer can learn from unstructured data.","We study the exact components in a transformer that facilitate the in-context learning.","In particular, we observe that (1) a transformer with two layers of softmax (self-)attentions with look-ahead attention mask can learn from the prompt if $y_i$ is in the token next to $x_i$ for each example; (2) positional encoding can further improve the performance; and (3) multi-head attention with a high input embedding dimension has a better prediction performance than single-head attention."],"url":"http://arxiv.org/abs/2402.00743v1","category":"cs.LG"}
{"created":"2024-02-01 16:00:21","title":"Combining the Strengths of Dutch Survey and Register Data in a Data Challenge to Predict Fertility (PreFer)","abstract":"The social sciences have produced an impressive body of research on determinants of fertility outcomes, or whether and when people have children. However, the strength of these determinants and underlying theories are rarely evaluated on their predictive ability on new data. This prevents us from systematically comparing studies, hindering the evaluation and accumulation of knowledge. In this paper, we present two datasets which can be used to study the predictability of fertility outcomes in the Netherlands. One dataset is based on the LISS panel, a longitudinal survey which includes thousands of variables on a wide range of topics, including individual preferences and values. The other is based on the Dutch register data which lacks attitudinal data but includes detailed information about the life courses of millions of Dutch residents. We provide information about the datasets and the samples, and describe the fertility outcome of interest. We also introduce the fertility prediction data challenge PreFer which is based on these datasets and will start in Spring 2024. We outline the ways in which measuring the predictability of fertility outcomes using these datasets and combining their strengths in the data challenge can advance our understanding of fertility behaviour and computational social science. We further provide details for participants on how to take part in the data challenge.","sentences":["The social sciences have produced an impressive body of research on determinants of fertility outcomes, or whether and when people have children.","However, the strength of these determinants and underlying theories are rarely evaluated on their predictive ability on new data.","This prevents us from systematically comparing studies, hindering the evaluation and accumulation of knowledge.","In this paper, we present two datasets which can be used to study the predictability of fertility outcomes in the Netherlands.","One dataset is based on the LISS panel, a longitudinal survey which includes thousands of variables on a wide range of topics, including individual preferences and values.","The other is based on the Dutch register data which lacks attitudinal data but includes detailed information about the life courses of millions of Dutch residents.","We provide information about the datasets and the samples, and describe the fertility outcome of interest.","We also introduce the fertility prediction data challenge PreFer which is based on these datasets and will start in Spring 2024.","We outline the ways in which measuring the predictability of fertility outcomes using these datasets and combining their strengths in the data challenge can advance our understanding of fertility behaviour and computational social science.","We further provide details for participants on how to take part in the data challenge."],"url":"http://arxiv.org/abs/2402.00705v1","category":"cs.LG"}
{"created":"2024-02-01 15:18:19","title":"Pre-training by Predicting Program Dependencies for Vulnerability Analysis Tasks","abstract":"Vulnerability analysis is crucial for software security. This work focuses on using pre-training techniques to enhance the understanding of vulnerable code and boost vulnerability analysis. The code understanding ability of a pre-trained model is highly related to its pre-training objectives. The semantic structure, e.g., control and data dependencies, of code is important for vulnerability analysis. However, existing pre-training objectives either ignore such structure or focus on learning to use it. The feasibility and benefits of learning the knowledge of analyzing semantic structure have not been investigated. To this end, this work proposes two novel pre-training objectives, namely Control Dependency Prediction (CDP) and Data Dependency Prediction (DDP), which aim to predict the statement-level control dependencies and token-level data dependencies, respectively, in a code snippet only based on its source code. During pre-training, CDP and DDP can guide the model to learn the knowledge required for analyzing fine-grained dependencies in code. After pre-training, the pre-trained model can boost the understanding of vulnerable code during fine-tuning and can directly be used to perform dependence analysis for both partial and complete functions. To demonstrate the benefits of our pre-training objectives, we pre-train a Transformer model named PDBERT with CDP and DDP, fine-tune it on three vulnerability analysis tasks, i.e., vulnerability detection, vulnerability classification, and vulnerability assessment, and also evaluate it on program dependence analysis. Experimental results show that PDBERT benefits from CDP and DDP, leading to state-of-the-art performance on the three downstream tasks. Also, PDBERT achieves F1-scores of over 99% and 94% for predicting control and data dependencies, respectively, in partial and complete functions.","sentences":["Vulnerability analysis is crucial for software security.","This work focuses on using pre-training techniques to enhance the understanding of vulnerable code and boost vulnerability analysis.","The code understanding ability of a pre-trained model is highly related to its pre-training objectives.","The semantic structure, e.g., control and data dependencies, of code is important for vulnerability analysis.","However, existing pre-training objectives either ignore such structure or focus on learning to use it.","The feasibility and benefits of learning the knowledge of analyzing semantic structure have not been investigated.","To this end, this work proposes two novel pre-training objectives, namely Control Dependency Prediction (CDP) and Data Dependency Prediction (DDP), which aim to predict the statement-level control dependencies and token-level data dependencies, respectively, in a code snippet only based on its source code.","During pre-training, CDP and DDP can guide the model to learn the knowledge required for analyzing fine-grained dependencies in code.","After pre-training, the pre-trained model can boost the understanding of vulnerable code during fine-tuning and can directly be used to perform dependence analysis for both partial and complete functions.","To demonstrate the benefits of our pre-training objectives, we pre-train a Transformer model named PDBERT with CDP and DDP, fine-tune it on three vulnerability analysis tasks, i.e., vulnerability detection, vulnerability classification, and vulnerability assessment, and also evaluate it on program dependence analysis.","Experimental results show that PDBERT benefits from CDP and DDP, leading to state-of-the-art performance on the three downstream tasks.","Also, PDBERT achieves F1-scores of over 99% and 94% for predicting control and data dependencies, respectively, in partial and complete functions."],"url":"http://arxiv.org/abs/2402.00657v1","category":"cs.SE"}
{"created":"2024-02-01 13:01:47","title":"Secure Supervised Learning-Based Smart Home Authentication Framework","abstract":"The Smart home possesses the capability of facilitating home services to their users with the systematic advance in The Internet of Things (IoT) and information and communication technologies (ICT) in recent decades. The home service offered by the smart devices helps the users in utilize maximized level of comfort for the objective of improving life quality. As the user and smart devices communicate through an insecure channel, the smart home environment is prone to security and privacy problems. A secure authentication protocol needs to be established between the smart devices and the user, such that a situation for device authentication can be made feasible in smart home environments. Most of the existing smart home authentication protocols were identified to fail in facilitating a secure mutual authentication and increases the possibility of lunching the attacks of session key disclosure, impersonation and stolen smart device. In this paper, Secure Supervised Learning-based Smart Home Authentication Framework (SSL-SHAF) is proposed as are liable mutual authentication that can be contextually imposed for better security. The formal analysis of the proposed SSL-SHAF confirmed better resistance against session key disclosure, impersonation and stolen smart device attacks. The results of SSL-SHAF confirmed minimized computational costs and security compared to the baseline protocols considered for investigation.","sentences":["The Smart home possesses the capability of facilitating home services to their users with the systematic advance in The Internet of Things (IoT) and information and communication technologies (ICT) in recent decades.","The home service offered by the smart devices helps the users in utilize maximized level of comfort for the objective of improving life quality.","As the user and smart devices communicate through an insecure channel, the smart home environment is prone to security and privacy problems.","A secure authentication protocol needs to be established between the smart devices and the user, such that a situation for device authentication can be made feasible in smart home environments.","Most of the existing smart home authentication protocols were identified to fail in facilitating a secure mutual authentication and increases the possibility of lunching the attacks of session key disclosure, impersonation and stolen smart device.","In this paper, Secure Supervised Learning-based Smart Home Authentication Framework (SSL-SHAF) is proposed as are liable mutual authentication that can be contextually imposed for better security.","The formal analysis of the proposed SSL-SHAF confirmed better resistance against session key disclosure, impersonation and stolen smart device attacks.","The results of SSL-SHAF confirmed minimized computational costs and security compared to the baseline protocols considered for investigation."],"url":"http://arxiv.org/abs/2402.00568v1","category":"cs.CR"}
{"created":"2024-02-01 11:43:13","title":"Understanding the Expressive Power and Mechanisms of Transformer for Sequence Modeling","abstract":"We conduct a systematic study of the approximation properties of Transformer for sequence modeling with long, sparse and complicated memory. We investigate the mechanisms through which different components of Transformer, such as the dot-product self-attention, positional encoding and feed-forward layer, affect its expressive power, and we study their combined effects through establishing explicit approximation rates. Our study reveals the roles of critical parameters in the Transformer, such as the number of layers and the number of attention heads, and these insights also provide natural suggestions for alternative architectures.","sentences":["We conduct a systematic study of the approximation properties of Transformer for sequence modeling with long, sparse and complicated memory.","We investigate the mechanisms through which different components of Transformer, such as the dot-product self-attention, positional encoding and feed-forward layer, affect its expressive power, and we study their combined effects through establishing explicit approximation rates.","Our study reveals the roles of critical parameters in the Transformer, such as the number of layers and the number of attention heads, and these insights also provide natural suggestions for alternative architectures."],"url":"http://arxiv.org/abs/2402.00522v1","category":"cs.LG"}
{"created":"2024-02-01 10:37:41","title":"Bias Mitigating Few-Shot Class-Incremental Learning","abstract":"Few-shot class-incremental learning (FSCIL) aims at recognizing novel classes continually with limited novel class samples. A mainstream baseline for FSCIL is first to train the whole model in the base session, then freeze the feature extractor in the incremental sessions. Despite achieving high overall accuracy, most methods exhibit notably low accuracy for incremental classes. Some recent methods somewhat alleviate the accuracy imbalance between base and incremental classes by fine-tuning the feature extractor in the incremental sessions, but they further cause the accuracy imbalance between past and current incremental classes. In this paper, we study the causes of such classification accuracy imbalance for FSCIL, and abstract them into a unified model bias problem. Based on the analyses, we propose a novel method to mitigate model bias of the FSCIL problem during training and inference processes, which includes mapping ability stimulation, separately dual-feature classification, and self-optimizing classifiers. Extensive experiments on three widely-used FSCIL benchmark datasets show that our method significantly mitigates the model bias problem and achieves state-of-the-art performance.","sentences":["Few-shot class-incremental learning (FSCIL) aims at recognizing novel classes continually with limited novel class samples.","A mainstream baseline for FSCIL is first to train the whole model in the base session, then freeze the feature extractor in the incremental sessions.","Despite achieving high overall accuracy, most methods exhibit notably low accuracy for incremental classes.","Some recent methods somewhat alleviate the accuracy imbalance between base and incremental classes by fine-tuning the feature extractor in the incremental sessions, but they further cause the accuracy imbalance between past and current incremental classes.","In this paper, we study the causes of such classification accuracy imbalance for FSCIL, and abstract them into a unified model bias problem.","Based on the analyses, we propose a novel method to mitigate model bias of the FSCIL problem during training and inference processes, which includes mapping ability stimulation, separately dual-feature classification, and self-optimizing classifiers.","Extensive experiments on three widely-used FSCIL benchmark datasets show that our method significantly mitigates the model bias problem and achieves state-of-the-art performance."],"url":"http://arxiv.org/abs/2402.00481v1","category":"cs.CV"}
{"created":"2024-02-01 06:47:56","title":"Image2Points:A 3D Point-based Context Clusters GAN for High-Quality PET Image Reconstruction","abstract":"To obtain high-quality Positron emission tomography (PET) images while minimizing radiation exposure, numerous methods have been proposed to reconstruct standard-dose PET (SPET) images from the corresponding low-dose PET (LPET) images. However, these methods heavily rely on voxel-based representations, which fall short of adequately accounting for the precise structure and fine-grained context, leading to compromised reconstruction. In this paper, we propose a 3D point-based context clusters GAN, namely PCC-GAN, to reconstruct high-quality SPET images from LPET. Specifically, inspired by the geometric representation power of points, we resort to a point-based representation to enhance the explicit expression of the image structure, thus facilitating the reconstruction with finer details. Moreover, a context clustering strategy is applied to explore the contextual relationships among points, which mitigates the ambiguities of small structures in the reconstructed images. Experiments on both clinical and phantom datasets demonstrate that our PCC-GAN outperforms the state-of-the-art reconstruction methods qualitatively and quantitatively. Code is available at https://github.com/gluucose/PCCGAN.","sentences":["To obtain high-quality Positron emission tomography (PET) images while minimizing radiation exposure, numerous methods have been proposed to reconstruct standard-dose PET (SPET) images from the corresponding low-dose PET (LPET) images.","However, these methods heavily rely on voxel-based representations, which fall short of adequately accounting for the precise structure and fine-grained context, leading to compromised reconstruction.","In this paper, we propose a 3D point-based context clusters GAN, namely PCC-GAN, to reconstruct high-quality SPET images from LPET.","Specifically, inspired by the geometric representation power of points, we resort to a point-based representation to enhance the explicit expression of the image structure, thus facilitating the reconstruction with finer details.","Moreover, a context clustering strategy is applied to explore the contextual relationships among points, which mitigates the ambiguities of small structures in the reconstructed images.","Experiments on both clinical and phantom datasets demonstrate that our PCC-GAN outperforms the state-of-the-art reconstruction methods qualitatively and quantitatively.","Code is available at https://github.com/gluucose/PCCGAN."],"url":"http://arxiv.org/abs/2402.00376v1","category":"eess.IV"}
{"created":"2024-02-01 05:58:21","title":"Defect Cluster Morphology in W from Collision Cascades: Results Comparing Five Inter-atomic potentials","abstract":"The size and morphology of defect clusters formed during primary damage play an important part in the subsequent evolution of the micro-structure of irradiated materials. Molecular dynamics (MD) simulations of collision cascades in W have been carried out using five interatomic potentials (IAP) including the quantum accurate machine learned (ML) spectral neighbor analysis potential (WSNAP), the ML based tabGAP potential and three embedded atom method (EAM) based potentials. A total of 3500 MD simulations with the primary knock-on atoms (PKA) having energys 5, 10, 20, 50, 75, 100 and 150 keV were carried out. The PKA are launched in hundred random directions at each of the PKA energies to obtain statistically valid results. Analysis using CSaransh, a web based tool to analyze a large collision cascade database, was carried out to obtain the number of defects (individual and in clusters), the defect cluster morphologies, the defect cluster size distributions and the number of sub-cascades formed. It is seen that <1 1 1> clusters dominate across all the inter-atomic potentials (IAP), except for the WSNAP potential. The WSNAP potential shows significantly higher number of C-15 like rings across all PKA energies. The WSNAP potential also shows a higher fraction of in-cluster vacancies. It is also seen that the stiffness and range of IAP do not affect the number of defects, defect clustering or vacancy clusters, but shows a tendency to form more sub-cascades for PKA energies less than 60 keV.","sentences":["The size and morphology of defect clusters formed during primary damage play an important part in the subsequent evolution of the micro-structure of irradiated materials.","Molecular dynamics (MD) simulations of collision cascades in W have been carried out using five interatomic potentials (IAP) including the quantum accurate machine learned (ML) spectral neighbor analysis potential (WSNAP), the ML based tabGAP potential and three embedded atom method (EAM) based potentials.","A total of 3500 MD simulations with the primary knock-on atoms (PKA) having energys 5, 10, 20, 50, 75, 100 and 150 keV were carried out.","The PKA are launched in hundred random directions at each of the PKA energies to obtain statistically valid results.","Analysis using CSaransh, a web based tool to analyze a large collision cascade database, was carried out to obtain the number of defects (individual and in clusters), the defect cluster morphologies, the defect cluster size distributions and the number of sub-cascades formed.","It is seen that <1 1 1> clusters dominate across all the inter-atomic potentials (IAP), except for the WSNAP potential.","The WSNAP potential shows significantly higher number of C-15 like rings across all PKA energies.","The WSNAP potential also shows a higher fraction of in-cluster vacancies.","It is also seen that the stiffness and range of IAP do not affect the number of defects, defect clustering or vacancy clusters, but shows a tendency to form more sub-cascades for PKA energies less than 60 keV."],"url":"http://arxiv.org/abs/2402.00359v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-01 05:13:14","title":"Survey of Privacy Threats and Countermeasures in Federated Learning","abstract":"Federated learning is widely considered to be as a privacy-aware learning method because no training data is exchanged directly between clients. Nevertheless, there are threats to privacy in federated learning, and privacy countermeasures have been studied. However, we note that common and unique privacy threats among typical types of federated learning have not been categorized and described in a comprehensive and specific way. In this paper, we describe privacy threats and countermeasures for the typical types of federated learning; horizontal federated learning, vertical federated learning, and transfer federated learning.","sentences":["Federated learning is widely considered to be as a privacy-aware learning method because no training data is exchanged directly between clients.","Nevertheless, there are threats to privacy in federated learning, and privacy countermeasures have been studied.","However, we note that common and unique privacy threats among typical types of federated learning have not been categorized and described in a comprehensive and specific way.","In this paper, we describe privacy threats and countermeasures for the typical types of federated learning; horizontal federated learning, vertical federated learning, and transfer federated learning."],"url":"http://arxiv.org/abs/2402.00342v1","category":"cs.LG"}
{"created":"2024-02-01 05:08:39","title":"Recasting Regional Lighting for Shadow Removal","abstract":"Removing shadows requires an understanding of both lighting conditions and object textures in a scene. Existing methods typically learn pixel-level color mappings between shadow and non-shadow images, in which the joint modeling of lighting and object textures is implicit and inadequate. We observe that in a shadow region, the degradation degree of object textures depends on the local illumination, while simply enhancing the local illumination cannot fully recover the attenuated textures. Based on this observation, we propose to condition the restoration of attenuated textures on the corrected local lighting in the shadow region. Specifically, We first design a shadow-aware decomposition network to estimate the illumination and reflectance layers of shadow regions explicitly. We then propose a novel bilateral correction network to recast the lighting of shadow regions in the illumination layer via a novel local lighting correction module, and to restore the textures conditioned on the corrected illumination layer via a novel illumination-guided texture restoration module. We further annotate pixel-wise shadow masks for the public SRD dataset, which originally contains only image pairs. Experiments on three benchmarks show that our method outperforms existing state-of-the-art shadow removal methods.","sentences":["Removing shadows requires an understanding of both lighting conditions and object textures in a scene.","Existing methods typically learn pixel-level color mappings between shadow and non-shadow images, in which the joint modeling of lighting and object textures is implicit and inadequate.","We observe that in a shadow region, the degradation degree of object textures depends on the local illumination, while simply enhancing the local illumination cannot fully recover the attenuated textures.","Based on this observation, we propose to condition the restoration of attenuated textures on the corrected local lighting in the shadow region.","Specifically, We first design a shadow-aware decomposition network to estimate the illumination and reflectance layers of shadow regions explicitly.","We then propose a novel bilateral correction network to recast the lighting of shadow regions in the illumination layer via a novel local lighting correction module, and to restore the textures conditioned on the corrected illumination layer via a novel illumination-guided texture restoration module.","We further annotate pixel-wise shadow masks for the public SRD dataset, which originally contains only image pairs.","Experiments on three benchmarks show that our method outperforms existing state-of-the-art shadow removal methods."],"url":"http://arxiv.org/abs/2402.00341v1","category":"cs.CV"}
{"created":"2024-02-01 05:03:05","title":"Can you Remove the Downstream Model for Speaker Recognition with Self-Supervised Speech Features?","abstract":"Self-supervised features are typically used in place of filter-banks in speaker verification models. However, these models were originally designed to ingest filter-banks as inputs, and thus, training them on top of self-supervised features assumes that both feature types require the same amount of learning for the task. In this work, we observe that pre-trained self-supervised speech features inherently include information required for downstream speaker verification task, and therefore, we can simplify the downstream model without sacrificing performance. To this end, we revisit the design of the downstream model for speaker verification using self-supervised features. We show that we can simplify the model to use 97.51% fewer parameters while achieving a 29.93% average improvement in performance on SUPERB. Consequently, we show that the simplified downstream model is more data efficient compared to baseline--it achieves better performance with only 60% of the training data.","sentences":["Self-supervised features are typically used in place of filter-banks in speaker verification models.","However, these models were originally designed to ingest filter-banks as inputs, and thus, training them on top of self-supervised features assumes that both feature types require the same amount of learning for the task.","In this work, we observe that pre-trained self-supervised speech features inherently include information required for downstream speaker verification task, and therefore, we can simplify the downstream model without sacrificing performance.","To this end, we revisit the design of the downstream model for speaker verification using self-supervised features.","We show that we can simplify the model to use 97.51% fewer parameters while achieving a 29.93% average improvement in performance on SUPERB.","Consequently, we show that the simplified downstream model is more data efficient compared to baseline--it achieves better performance with only 60% of the training data."],"url":"http://arxiv.org/abs/2402.00340v1","category":"cs.SD"}
{"created":"2024-02-01 04:11:03","title":"DARCS: Memory-Efficient Deep Compressed Sensing Reconstruction for Acceleration of 3D Whole-Heart Coronary MR Angiography","abstract":"Three-dimensional coronary magnetic resonance angiography (CMRA) demands reconstruction algorithms that can significantly suppress the artifacts from a heavily undersampled acquisition. While unrolling-based deep reconstruction methods have achieved state-of-the-art performance on 2D image reconstruction, their application to 3D reconstruction is hindered by the large amount of memory needed to train an unrolled network. In this study, we propose a memory-efficient deep compressed sensing method by employing a sparsifying transform based on a pre-trained artifact estimation network. The motivation is that the artifact image estimated by a well-trained network is sparse when the input image is artifact-free, and less sparse when the input image is artifact-affected. Thus, the artifact-estimation network can be used as an inherent sparsifying transform. The proposed method, named De-Aliasing Regularization based Compressed Sensing (DARCS), was compared with a traditional compressed sensing method, de-aliasing generative adversarial network (DAGAN), model-based deep learning (MoDL), and plug-and-play for accelerations of 3D CMRA. The results demonstrate that the proposed method improved the reconstruction quality relative to the compared methods by a large margin. Furthermore, the proposed method well generalized for different undersampling rates and noise levels. The memory usage of the proposed method was only 63% of that needed by MoDL. In conclusion, the proposed method achieves improved reconstruction quality for 3D CMRA with reduced memory burden.","sentences":["Three-dimensional coronary magnetic resonance angiography (CMRA) demands reconstruction algorithms that can significantly suppress the artifacts from a heavily undersampled acquisition.","While unrolling-based deep reconstruction methods have achieved state-of-the-art performance on 2D image reconstruction, their application to 3D reconstruction is hindered by the large amount of memory needed to train an unrolled network.","In this study, we propose a memory-efficient deep compressed sensing method by employing a sparsifying transform based on a pre-trained artifact estimation network.","The motivation is that the artifact image estimated by a well-trained network is sparse when the input image is artifact-free, and less sparse when the input image is artifact-affected.","Thus, the artifact-estimation network can be used as an inherent sparsifying transform.","The proposed method, named De-Aliasing Regularization based Compressed Sensing (DARCS), was compared with a traditional compressed sensing method, de-aliasing generative adversarial network (DAGAN), model-based deep learning (MoDL), and plug-and-play for accelerations of 3D CMRA.","The results demonstrate that the proposed method improved the reconstruction quality relative to the compared methods by a large margin.","Furthermore, the proposed method well generalized for different undersampling rates and noise levels.","The memory usage of the proposed method was only 63% of that needed by MoDL.","In conclusion, the proposed method achieves improved reconstruction quality for 3D CMRA with reduced memory burden."],"url":"http://arxiv.org/abs/2402.00320v1","category":"eess.IV"}
{"created":"2024-02-01 04:05:24","title":"Analog-digital Scheduling for Federated Learning: A Communication-Efficient Approach","abstract":"Over-the-air (OTA) computation has recently emerged as a communication-efficient Federated Learning (FL) paradigm to train machine learning models over wireless networks. However, its performance is limited by the device with the worst SNR, resulting in fast yet noisy updates. On the other hand, allocating orthogonal resource blocks (RB) to individual devices via digital channels mitigates the noise problem, at the cost of increased communication latency. In this paper, we address this discrepancy and present ADFL, a novel Analog-Digital FL scheme: in each round, the parameter server (PS) schedules each device to either upload its gradient via the analog OTA scheme or transmit its quantized gradient over an orthogonal RB using the ``digital\" scheme. Focusing on a single FL round, we cast the optimal scheduling problem as the minimization of the mean squared error (MSE) on the estimated global gradient at the PS, subject to a delay constraint, yielding the optimal device scheduling configuration and quantization bits for the digital devices. Our simulation results show that ADFL, by scheduling most of the devices in the OTA scheme while also occasionally employing the digital scheme for a few devices, consistently outperforms OTA-only and digital-only schemes, in both i.i.d. and non-i.i.d. settings.","sentences":["Over-the-air (OTA) computation has recently emerged as a communication-efficient Federated Learning (FL) paradigm to train machine learning models over wireless networks.","However, its performance is limited by the device with the worst SNR, resulting in fast yet noisy updates.","On the other hand, allocating orthogonal resource blocks (RB) to individual devices via digital channels mitigates the noise problem, at the cost of increased communication latency.","In this paper, we address this discrepancy and present ADFL, a novel Analog-Digital FL scheme: in each round, the parameter server (PS) schedules each device to either upload its gradient via the analog OTA scheme or transmit its quantized gradient over an orthogonal RB using the ``digital\" scheme.","Focusing on a single FL round, we cast the optimal scheduling problem as the minimization of the mean squared error (MSE) on the estimated global gradient at the PS, subject to a delay constraint, yielding the optimal device scheduling configuration and quantization bits for the digital devices.","Our simulation results show that ADFL, by scheduling most of the devices in the OTA scheme while also occasionally employing the digital scheme for a few devices, consistently outperforms OTA-only and digital-only schemes, in both i.i.d. and non-i.i.d. settings."],"url":"http://arxiv.org/abs/2402.00318v1","category":"cs.LG"}
{"created":"2024-02-01 03:39:01","title":"Information-Theoretic Thresholds for Planted Dense Cycles","abstract":"We study a random graph model for small-world networks which are ubiquitous in social and biological sciences. In this model, a dense cycle of expected bandwidth $n \\tau$, representing the hidden one-dimensional geometry of vertices, is planted in an ambient random graph on $n$ vertices. For both detection and recovery of the planted dense cycle, we characterize the information-theoretic thresholds in terms of $n$, $\\tau$, and an edge-wise signal-to-noise ratio $\\lambda$. In particular, the information-theoretic thresholds differ from the computational thresholds established in a recent work for low-degree polynomial algorithms, thereby justifying the existence of statistical-to-computational gaps for this problem.","sentences":["We study a random graph model for small-world networks which are ubiquitous in social and biological sciences.","In this model, a dense cycle of expected bandwidth $n \\tau$, representing the hidden one-dimensional geometry of vertices, is planted in an ambient random graph on $n$ vertices.","For both detection and recovery of the planted dense cycle, we characterize the information-theoretic thresholds in terms of $n$, $\\tau$, and an edge-wise signal-to-noise ratio $\\lambda$. In particular, the information-theoretic thresholds differ from the computational thresholds established in a recent work for low-degree polynomial algorithms, thereby justifying the existence of statistical-to-computational gaps for this problem."],"url":"http://arxiv.org/abs/2402.00305v1","category":"math.ST"}
{"created":"2024-02-01 01:23:07","title":"Does \\textsc{DetectGPT} Fully Utilize Perturbation? Selective Perturbation on Model-Based Contrastive Learning Detector would be Better","abstract":"The burgeoning capabilities of large language models (LLMs) have raised growing concerns about abuse. DetectGPT, a zero-shot metric-based unsupervised machine-generated text detector, first introduces perturbation and shows great performance improvement. However, DetectGPT's random perturbation strategy might introduce noise, limiting the distinguishability and further performance improvements. Moreover, its logit regression module relies on setting the threshold, which harms the generalizability and applicability of individual or small-batch inputs. Hence, we propose a novel detector, \\modelname{}, which uses selective strategy perturbation to relieve the important information loss caused by random masking, and multi-pair contrastive learning to capture the implicit pattern information during perturbation, facilitating few-shot performance. The experiments show that \\modelname{} outperforms the SOTA method by 1.20\\% in accuracy on average on four public datasets. We further analyze the effectiveness, robustness, and generalization of our perturbation method.","sentences":["The burgeoning capabilities of large language models (LLMs) have raised growing concerns about abuse.","DetectGPT, a zero-shot metric-based unsupervised machine-generated text detector, first introduces perturbation and shows great performance improvement.","However, DetectGPT's random perturbation strategy might introduce noise, limiting the distinguishability and further performance improvements.","Moreover, its logit regression module relies on setting the threshold, which harms the generalizability and applicability of individual or small-batch inputs.","Hence, we propose a novel detector, \\modelname{}, which uses selective strategy perturbation to relieve the important information loss caused by random masking, and multi-pair contrastive learning to capture the implicit pattern information during perturbation, facilitating few-shot performance.","The experiments show that \\modelname{} outperforms the SOTA method by 1.20\\% in accuracy on average on four public datasets.","We further analyze the effectiveness, robustness, and generalization of our perturbation method."],"url":"http://arxiv.org/abs/2402.00263v1","category":"cs.CL"}
{"created":"2024-02-01 00:33:21","title":"A Survey on Hallucination in Large Vision-Language Models","abstract":"Recent development of Large Vision-Language Models (LVLMs) has attracted growing attention within the AI landscape for its practical implementation potential. However, ``hallucination'', or more specifically, the misalignment between factual visual content and corresponding textual generation, poses a significant challenge of utilizing LVLMs. In this comprehensive survey, we dissect LVLM-related hallucinations in an attempt to establish an overview and facilitate future mitigation. Our scrutiny starts with a clarification of the concept of hallucinations in LVLMs, presenting a variety of hallucination symptoms and highlighting the unique challenges inherent in LVLM hallucinations. Subsequently, we outline the benchmarks and methodologies tailored specifically for evaluating hallucinations unique to LVLMs. Additionally, we delve into an investigation of the root causes of these hallucinations, encompassing insights from the training data and model components. We also critically review existing methods for mitigating hallucinations. The open questions and future directions pertaining to hallucinations within LVLMs are discussed to conclude this survey.","sentences":["Recent development of Large Vision-Language Models (LVLMs) has attracted growing attention within the AI landscape for its practical implementation potential.","However, ``hallucination'', or more specifically, the misalignment between factual visual content and corresponding textual generation, poses a significant challenge of utilizing LVLMs.","In this comprehensive survey, we dissect LVLM-related hallucinations in an attempt to establish an overview and facilitate future mitigation.","Our scrutiny starts with a clarification of the concept of hallucinations in LVLMs, presenting a variety of hallucination symptoms and highlighting the unique challenges inherent in LVLM hallucinations.","Subsequently, we outline the benchmarks and methodologies tailored specifically for evaluating hallucinations unique to LVLMs.","Additionally, we delve into an investigation of the root causes of these hallucinations, encompassing insights from the training data and model components.","We also critically review existing methods for mitigating hallucinations.","The open questions and future directions pertaining to hallucinations within LVLMs are discussed to conclude this survey."],"url":"http://arxiv.org/abs/2402.00253v1","category":"cs.CV"}
{"created":"2024-02-01 18:58:44","title":"Geometry Transfer for Stylizing Radiance Fields","abstract":"Shape and geometric patterns are essential in defining stylistic identity. However, current 3D style transfer methods predominantly focus on transferring colors and textures, often overlooking geometric aspects. In this paper, we introduce Geometry Transfer, a novel method that leverages geometric deformation for 3D style transfer. This technique employs depth maps to extract a style guide, subsequently applied to stylize the geometry of radiance fields. Moreover, we propose new techniques that utilize geometric cues from the 3D scene, thereby enhancing aesthetic expressiveness and more accurately reflecting intended styles. Our extensive experiments show that Geometry Transfer enables a broader and more expressive range of stylizations, thereby significantly expanding the scope of 3D style transfer.","sentences":["Shape and geometric patterns are essential in defining stylistic identity.","However, current 3D style transfer methods predominantly focus on transferring colors and textures, often overlooking geometric aspects.","In this paper, we introduce Geometry Transfer, a novel method that leverages geometric deformation for 3D style transfer.","This technique employs depth maps to extract a style guide, subsequently applied to stylize the geometry of radiance fields.","Moreover, we propose new techniques that utilize geometric cues from the 3D scene, thereby enhancing aesthetic expressiveness and more accurately reflecting intended styles.","Our extensive experiments show that Geometry Transfer enables a broader and more expressive range of stylizations, thereby significantly expanding the scope of 3D style transfer."],"url":"http://arxiv.org/abs/2402.00863v1","category":"cs.CV"}
{"created":"2024-02-01 18:30:10","title":"Scattering wave packets of hadrons in gauge theories: Preparation on a quantum computer","abstract":"Quantum simulation holds promise of enabling a complete description of high-energy scattering processes rooted in gauge theories of the Standard Model. A first step in such simulations is preparation of interacting hadronic wave packets. To create the wave packets, one typically resorts to adiabatic evolution to bridge between wave packets in the free theory and those in the interacting theory, rendering the simulation resource intensive. In this work, we construct a wave-packet creation operator directly in the interacting theory to circumvent adiabatic evolution, taking advantage of resource-efficient schemes for ground-state preparation, such as variational quantum eigensolvers. By means of an ansatz for bound mesonic excitations in confining gauge theories, which is subsequently optimized using classical or quantum methods, we show that interacting mesonic wave packets can be created efficiently and accurately using digital quantum algorithms that we develop. Specifically, we obtain high-fidelity mesonic wave packets in the $Z_2$ and $U(1)$ lattice gauge theories coupled to fermionic matter in 1+1 dimensions. Our method is applicable to both perturbative and non-perturbative regimes of couplings. The wave-packet creation circuit for the case of the $Z_2$ lattice gauge theory is built and implemented on the Quantinuum H1-1 trapped-ion quantum computer using 13 qubits and up to 308 entangling gates. The fidelities agree well with classical benchmark calculations after employing a simple symmetry-based noise-mitigation technique. This work serves as a step toward quantum computing scattering processes in quantum chromodynamics.","sentences":["Quantum simulation holds promise of enabling a complete description of high-energy scattering processes rooted in gauge theories of the Standard Model.","A first step in such simulations is preparation of interacting hadronic wave packets.","To create the wave packets, one typically resorts to adiabatic evolution to bridge between wave packets in the free theory and those in the interacting theory, rendering the simulation resource intensive.","In this work, we construct a wave-packet creation operator directly in the interacting theory to circumvent adiabatic evolution, taking advantage of resource-efficient schemes for ground-state preparation, such as variational quantum eigensolvers.","By means of an ansatz for bound mesonic excitations in confining gauge theories, which is subsequently optimized using classical or quantum methods, we show that interacting mesonic wave packets can be created efficiently and accurately using digital quantum algorithms that we develop.","Specifically, we obtain high-fidelity mesonic wave packets in the $Z_2$ and $U(1)$ lattice gauge theories coupled to fermionic matter in 1+1 dimensions.","Our method is applicable to both perturbative and non-perturbative regimes of couplings.","The wave-packet creation circuit for the case of the $Z_2$ lattice gauge theory is built and implemented on the Quantinuum H1-1 trapped-ion quantum computer using 13 qubits and up to 308 entangling gates.","The fidelities agree well with classical benchmark calculations after employing a simple symmetry-based noise-mitigation technique.","This work serves as a step toward quantum computing scattering processes in quantum chromodynamics."],"url":"http://arxiv.org/abs/2402.00840v1","category":"quant-ph"}
{"created":"2024-02-01 18:28:55","title":"OLMo: Accelerating the Science of Language Models","abstract":"Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, this technical report details the first release of OLMo, a state-of-the-art, truly Open Language Model and its framework to build and study the science of language modeling. Unlike most prior efforts that have only released model weights and inference code, we release OLMo and the whole framework, including training data and training and evaluation code. We hope this release will empower and strengthen the open research community and inspire a new wave of innovation.","sentences":["Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings.","As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed.","Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs.","To this end, this technical report details the first release of OLMo, a state-of-the-art, truly Open Language Model and its framework to build and study the science of language modeling.","Unlike most prior efforts that have only released model weights and inference code, we release OLMo and the whole framework, including training data and training and evaluation code.","We hope this release will empower and strengthen the open research community and inspire a new wave of innovation."],"url":"http://arxiv.org/abs/2402.00838v1","category":"cs.CL"}
{"created":"2024-02-01 18:08:28","title":"Role of filler lanthanide ions on lattice dynamics of phosphide skutterudites RFe$_4$P$_{12}$ (R = La, Ce, and Pr) from first principles","abstract":"Phosphide skutterudites primarily show promise for thermoelectric applications due to their chemical stability at high temperatures and relatively low cost. Ion doping and band gap engineering have been used to enhance their typically poor thermoelectric performance, opening avenues for practical applications. Herein, we report a comparative lattice dynamics study on the impact of filler and temperature on the structural and vibrational properties of RFe$_4$P$_{12}$ (R = La, Ce, and Pr) skutterudites. Calculations are performed within the quasi-harmonic approximation, and the results are critically compared against experimental data and other \\textit{ab initio} calculations. We found gaps between the heat-carrying acoustic and optical modes, a-o gaps, of approximately 4, -2, and 0.01\\,cm$^{-1}$ for La, Ce, and Pr compounds, respectively. These results suggest a filler-induced reduction in the a-o gap is attributed to the softening of the optical modes instead of the conventionally considered upward shift of acoustic modes proposed in the \\textit{rattling} scenario. The distinct softening of the optical modes is rationalized by the stiffening of chemical bonds between the filler and host lattice.","sentences":["Phosphide skutterudites primarily show promise for thermoelectric applications due to their chemical stability at high temperatures and relatively low cost.","Ion doping and band gap engineering have been used to enhance their typically poor thermoelectric performance, opening avenues for practical applications.","Herein, we report a comparative lattice dynamics study on the impact of filler and temperature on the structural and vibrational properties of RFe$_4$P$_{12}$ (R = La, Ce, and Pr) skutterudites.","Calculations are performed within the quasi-harmonic approximation, and the results are critically compared against experimental data and other \\textit{ab initio} calculations.","We found gaps between the heat-carrying acoustic and optical modes, a-o gaps, of approximately 4, -2, and 0.01\\,cm$^{-1}$ for La, Ce, and Pr compounds, respectively.","These results suggest a filler-induced reduction in the a-o gap is attributed to the softening of the optical modes instead of the conventionally considered upward shift of acoustic modes proposed in the \\textit{rattling} scenario.","The distinct softening of the optical modes is rationalized by the stiffening of chemical bonds between the filler and host lattice."],"url":"http://arxiv.org/abs/2402.00824v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-01 16:33:06","title":"BIOMERO: BioImage analysis in OMERO","abstract":"In the rapidly evolving field of bioimaging, the integration and orchestration of Findable, Accessible, Interoperable, and Reusable (FAIR) image analysis workflows remains a challenge. We introduce BIOMERO, a bridge connecting OMERO, a renowned bioimaging data management platform, FAIR workflows and high-performance computing (HPC) environments. BIOMERO, featuring our opensource Python library \"OMERO Slurm Client\", facilitates seamless execution of FAIR workflows, particularly for large datasets from High Content or High Throughput Screening. BIOMERO empowers researchers by eliminating the need for specialized knowledge, enabling scalable image processing directly from OMERO. BIOMERO notably supports the sharing and utilization of FAIR workflows between OMERO, Cytomine/BIAFLOWS, and other bioimaging communities. BIOMERO will promote the widespread adoption of FAIR workflows, emphasizing reusability, across the realm of bioimaging research. Its user-friendly interface will empower users, including those without technical expertise, to seamlessly apply these workflows to their datasets, democratizing the utilization of AI by the broader research community.","sentences":["In the rapidly evolving field of bioimaging, the integration and orchestration of Findable, Accessible, Interoperable, and Reusable (FAIR) image analysis workflows remains a challenge.","We introduce BIOMERO, a bridge connecting OMERO, a renowned bioimaging data management platform, FAIR workflows and high-performance computing (HPC) environments.","BIOMERO, featuring our opensource Python library \"OMERO Slurm Client\", facilitates seamless execution of FAIR workflows, particularly for large datasets from High Content or High Throughput Screening.","BIOMERO empowers researchers by eliminating the need for specialized knowledge, enabling scalable image processing directly from OMERO.","BIOMERO notably supports the sharing and utilization of FAIR workflows between OMERO, Cytomine/BIAFLOWS, and other bioimaging communities.","BIOMERO will promote the widespread adoption of FAIR workflows, emphasizing reusability, across the realm of bioimaging research.","Its user-friendly interface will empower users, including those without technical expertise, to seamlessly apply these workflows to their datasets, democratizing the utilization of AI by the broader research community."],"url":"http://arxiv.org/abs/2402.00734v1","category":"cs.SE"}
{"created":"2024-02-01 16:03:49","title":"Examples of solvable and nilpotent finite quantum groups","abstract":"We prove the solvability and nilpotency of Kac--Paljutkin's finite quantum group and Sekine quantum groups and we classify the solvable series of Kac--Paljutkin's finite quantum group via Cohen--Westreich's Burnside theorem. Some semisimple quasitriangular Hopf algebras of dimensions $2pq$ are also studied. In Appendix A, we give a direct computation of the universal $R$-matrices for Kac--Paljutkin's $8$-dimensional finite quantum group.","sentences":["We prove the solvability and nilpotency of Kac--Paljutkin's finite quantum group and Sekine quantum groups and we classify the solvable series of Kac--Paljutkin's finite quantum group via Cohen--Westreich's Burnside theorem.","Some semisimple quasitriangular Hopf algebras of dimensions $2pq$ are also studied.","In Appendix A, we give a direct computation of the universal $R$-matrices for Kac--Paljutkin's $8$-dimensional finite quantum group."],"url":"http://arxiv.org/abs/2402.00706v1","category":"math.QA"}
{"created":"2024-02-01 14:51:51","title":"Double-scaled SYK, Chords and de Sitter Gravity","abstract":"We study the partition function of 3D de Sitter gravity defined as the trace over the Hilbert space obtained by quantizing the phase space of non-rotating Schwarzschild-de Sitter spacetime. Motivated by the correspondence with double scaled SYK, we identify the Hamiltonian with the gravitational Wilson-line that measures the conical deficit angle. We express the Hamiltonian in terms of canonical variables and find that it leads to the exact same chord rules and energy spectrum as the double scaled SYK model. We use the obtained match to compute the partition function and scalar two-point function in 3D de Sitter gravity.","sentences":["We study the partition function of 3D de Sitter gravity defined as the trace over the Hilbert space obtained by quantizing the phase space of non-rotating Schwarzschild-de Sitter spacetime.","Motivated by the correspondence with double scaled SYK, we identify the Hamiltonian with the gravitational Wilson-line that measures the conical deficit angle.","We express the Hamiltonian in terms of canonical variables and find that it leads to the exact same chord rules and energy spectrum as the double scaled SYK model.","We use the obtained match to compute the partition function and scalar two-point function in 3D de Sitter gravity."],"url":"http://arxiv.org/abs/2402.00635v1","category":"hep-th"}
{"created":"2024-02-01 13:58:32","title":"Dynamic Texture Transfer using PatchMatch and Transformers","abstract":"How to automatically transfer the dynamic texture of a given video to the target still image is a challenging and ongoing problem. In this paper, we propose to handle this task via a simple yet effective model that utilizes both PatchMatch and Transformers. The key idea is to decompose the task of dynamic texture transfer into two stages, where the start frame of the target video with the desired dynamic texture is synthesized in the first stage via a distance map guided texture transfer module based on the PatchMatch algorithm. Then, in the second stage, the synthesized image is decomposed into structure-agnostic patches, according to which their corresponding subsequent patches can be predicted by exploiting the powerful capability of Transformers equipped with VQ-VAE for processing long discrete sequences. After getting all those patches, we apply a Gaussian weighted average merging strategy to smoothly assemble them into each frame of the target stylized video. Experimental results demonstrate the effectiveness and superiority of the proposed method in dynamic texture transfer compared to the state of the art.","sentences":["How to automatically transfer the dynamic texture of a given video to the target still image is a challenging and ongoing problem.","In this paper, we propose to handle this task via a simple yet effective model that utilizes both PatchMatch and Transformers.","The key idea is to decompose the task of dynamic texture transfer into two stages, where the start frame of the target video with the desired dynamic texture is synthesized in the first stage via a distance map guided texture transfer module based on the PatchMatch algorithm.","Then, in the second stage, the synthesized image is decomposed into structure-agnostic patches, according to which their corresponding subsequent patches can be predicted by exploiting the powerful capability of Transformers equipped with VQ-VAE for processing long discrete sequences.","After getting all those patches, we apply a Gaussian weighted average merging strategy to smoothly assemble them into each frame of the target stylized video.","Experimental results demonstrate the effectiveness and superiority of the proposed method in dynamic texture transfer compared to the state of the art."],"url":"http://arxiv.org/abs/2402.00606v1","category":"cs.CV"}
{"created":"2024-02-01 13:50:29","title":"An efficient multivariate volatility model for many assets","abstract":"This paper develops a flexible and computationally efficient multivariate volatility model, which allows for dynamic conditional correlations and volatility spillover effects among financial assets. The new model has desirable properties such as identifiability and computational tractability for many assets. A sufficient condition of the strict stationarity is derived for the new process. Two quasi-maximum likelihood estimation methods are proposed for the new model with and without low-rank constraints on the coefficient matrices respectively, and the asymptotic properties for both estimators are established. Moreover, a Bayesian information criterion with selection consistency is developed for order selection, and the testing for volatility spillover effects is carefully discussed. The finite sample performance of the proposed methods is evaluated in simulation studies for small and moderate dimensions. The usefulness of the new model and its inference tools is illustrated by two empirical examples for 5 stock markets and 17 industry portfolios, respectively.","sentences":["This paper develops a flexible and computationally efficient multivariate volatility model, which allows for dynamic conditional correlations and volatility spillover effects among financial assets.","The new model has desirable properties such as identifiability and computational tractability for many assets.","A sufficient condition of the strict stationarity is derived for the new process.","Two quasi-maximum likelihood estimation methods are proposed for the new model with and without low-rank constraints on the coefficient matrices respectively, and the asymptotic properties for both estimators are established.","Moreover, a Bayesian information criterion with selection consistency is developed for order selection, and the testing for volatility spillover effects is carefully discussed.","The finite sample performance of the proposed methods is evaluated in simulation studies for small and moderate dimensions.","The usefulness of the new model and its inference tools is illustrated by two empirical examples for 5 stock markets and 17 industry portfolios, respectively."],"url":"http://arxiv.org/abs/2402.00597v1","category":"stat.ME"}
{"created":"2024-02-01 12:48:33","title":"Discovery of the local counterpart of disc galaxies at z > 4: The oldest thin disc of Milky Way using Gaia-RVS","abstract":"JWST has recently detected numerous disc galaxies at high-redshifts and there have been observations of cold disc galaxies at z > 4 with ALMA. In the Milky Way, recent studies find metal-poor stars in cold disc orbits, suggesting an ancient disc. We investigated a sample of 565,606 stars from the hybrid-CNN analysis of the Gaia-DR3 RVS stars. The sample contains 8,500 stars with [Fe/H]<-1. For a subset of ~200,000 main sequence turnoff and subgiant stars we computed distances and ages using the StarHorse code with a mean precision of 1% and 12%, respectively. First, we confirm the existence of metal-poor stars in thin disc orbits - over 50% are older than 13 Gyr. Second, we report the discovery of the oldest thin disc of the Milky Way extending across a wide range of metallicities from metal-poor to super-solar. The metal-poor stars in disc orbits manifest as a readily visible tail of the metallicity distribution. The high-[{\\alpha}/Fe] thick disc exhibits a vertical velocity dispersion of 35 km/s, while the thin disc shows 10 to 15 km/s lower at similar ages. Our old thin disc $\\sigma_{V_z}$ appears similar to those estimated for the high-z disc galaxies. Third, we extend the [Y/Mg] chemical clock to the oldest ages and estimate a slope of -0.038 dex/Gyr. Finally, we show that the Splash includes both old (> 9 Gyr) high- and low-[{\\alpha}/Fe] populations and extends to super-solar [Fe/H]. We find about 6 to 10% of the old thin disc was heated to thick disc orbits with the youngest splashed stars being 9 to 10 Gyrs. We conclude the Milky Way thin disc forms <1 billion years from Big Bang, building up inside-out, preceding earlier estimates by about 4-5 billion years. Considering a massive merger event such as the GSE, a Splash is expected - we find a portion of the old thin disc is heated to thick disc velocities and the Splash extends to super-solar [Fe/H] regimes.","sentences":["JWST has recently detected numerous disc galaxies at high-redshifts and there have been observations of cold disc galaxies at z > 4 with ALMA.","In the Milky Way, recent studies find metal-poor stars in cold disc orbits, suggesting an ancient disc.","We investigated a sample of 565,606 stars from the hybrid-CNN analysis of the Gaia-DR3 RVS stars.","The sample contains 8,500 stars with [Fe/H]<-1.","For a subset of ~200,000 main sequence turnoff and subgiant stars we computed distances and ages using the StarHorse code with a mean precision of 1% and 12%, respectively.","First, we confirm the existence of metal-poor stars in thin disc orbits - over 50% are older than 13 Gyr.","Second, we report the discovery of the oldest thin disc of the Milky Way extending across a wide range of metallicities from metal-poor to super-solar.","The metal-poor stars in disc orbits manifest as a readily visible tail of the metallicity distribution.","The high-[{\\alpha}/Fe] thick disc exhibits a vertical velocity dispersion of 35 km/s, while the thin disc shows 10 to 15 km/s lower at similar ages.","Our old thin disc $\\sigma_{V_z}$ appears similar to those estimated for the high-z disc galaxies.","Third, we extend the [Y/Mg] chemical clock to the oldest ages and estimate a slope of -0.038 dex/Gyr.","Finally, we show that the Splash includes both old (> 9 Gyr) high- and low-[{\\alpha}/Fe] populations and extends to super-solar [Fe/H].","We find about 6 to 10% of the old thin disc was heated to thick disc orbits with the youngest splashed stars being 9 to 10 Gyrs.","We conclude the Milky Way thin disc forms <1 billion years from Big Bang, building up inside-out, preceding earlier estimates by about 4-5 billion years.","Considering a massive merger event such as the GSE, a Splash is expected - we find a portion of the old thin disc is heated to thick disc velocities and the Splash extends to super-solar [Fe/H] regimes."],"url":"http://arxiv.org/abs/2402.00561v1","category":"astro-ph.GA"}
{"created":"2024-02-01 12:01:37","title":"Reuse Detector: Improving the Management of STT-RAM SLLCs","abstract":"Various constraints of Static Random Access Memory (SRAM) are leading to consider new memory technologies as candidates for building on-chip shared last-level caches (SLLCs). Spin-Transfer Torque RAM (STT-RAM) is currently postulated as the prime contender due to its better energy efficiency, smaller die footprint and higher scalability. However, STT-RAM also exhibits some drawbacks, like slow and energy-hungry write operations, that need to be mitigated. In this work we address these shortcomings by leveraging a new management mechanism for STT-RAM SLLCs. This approach is based on the previous observation that the stream of references arriving at the SLLC of a Chip MultiProcessor (CMP) exhibits reuse locality, i.e., those blocks referenced several times manifest high probability of forthcoming reuse. In this paper, we employ a cache management mechanism that selects the contents of the SLLC aimed to exploit reuse locality instead of temporal locality. Specifically, our proposal consists in the inclusion of a Reuse Detector between private cache levels and the STT-RAM SLLC to detect blocks that do not exhibit reuse, in order to avoid their insertion in the SLLC, hence reducing the number of write operations and the energy consumption in the STT-RAM. Our evaluation reveals that our scheme reports on average, energy reductions in the SLLC in the range of 37-30\\%, additional energy savings in the main memory in the range of 6-8\\% and performance improvements of 3\\% up to 14\\% (16-core) compared to an STT-RAM SLLC baseline where no reuse detector is employed. More importantly, our approach outperforms DASCA, the state-of-the-art STT-RAM SLLC management, reporting SLLC energy savings in the range of 4-11\\% higher than those of DASCA, delivering higher performance in the range of 1.5-14\\%, and additional improvements in DRAM energy consumption in the range of 2-9\\% higher than DASCA.","sentences":["Various constraints of Static Random Access Memory (SRAM) are leading to consider new memory technologies as candidates for building on-chip shared last-level caches (SLLCs).","Spin-Transfer Torque RAM (STT-RAM) is currently postulated as the prime contender due to its better energy efficiency, smaller die footprint and higher scalability.","However, STT-RAM also exhibits some drawbacks, like slow and energy-hungry write operations, that need to be mitigated.","In this work we address these shortcomings by leveraging a new management mechanism for STT-RAM SLLCs.","This approach is based on the previous observation that the stream of references arriving at the SLLC of a Chip MultiProcessor (CMP) exhibits reuse locality, i.e., those blocks referenced several times manifest high probability of forthcoming reuse.","In this paper, we employ a cache management mechanism that selects the contents of the SLLC aimed to exploit reuse locality instead of temporal locality.","Specifically, our proposal consists in the inclusion of a Reuse Detector between private cache levels and the STT-RAM SLLC to detect blocks that do not exhibit reuse, in order to avoid their insertion in the SLLC, hence reducing the number of write operations and the energy consumption in the STT-RAM.","Our evaluation reveals that our scheme reports on average, energy reductions in the SLLC in the range of 37-30\\%, additional energy savings in the main memory in the range of 6-8\\% and performance improvements of 3\\% up to 14\\% (16-core) compared to an STT-RAM SLLC baseline where no reuse detector is employed.","More importantly, our approach outperforms DASCA, the state-of-the-art STT-RAM SLLC management, reporting SLLC energy savings in the range of 4-11\\% higher than those of DASCA, delivering higher performance in the range of 1.5-14\\%, and additional improvements in DRAM energy consumption in the range of 2-9\\% higher than DASCA."],"url":"http://arxiv.org/abs/2402.00533v1","category":"cs.AR"}
{"created":"2024-02-01 11:57:53","title":"Superfiltering: Weak-to-Strong Data Filtering for Fast Instruction-Tuning","abstract":"Instruction tuning is critical to improve LLMs but usually suffers from low-quality and redundant data. Data filtering for instruction tuning has proved important in improving both the efficiency and performance of the tuning process. But it also leads to extra cost and computation due to the involvement of LLMs in this process. To reduce the filtering cost, we study Superfiltering: Can we use a smaller and weaker model to select data for finetuning a larger and stronger model? Despite the performance gap between weak and strong language models, we find their highly consistent capability to perceive instruction difficulty and data selection results. This enables us to use a much smaller and more efficient model to filter the instruction data used to train a larger language model. Not only does it largely speed up the data filtering, but the filtered-data-finetuned LLM achieves even better performance on standard benchmarks. Extensive experiments validate the efficacy and efficiency of our approach.","sentences":["Instruction tuning is critical to improve LLMs but usually suffers from low-quality and redundant data.","Data filtering for instruction tuning has proved important in improving both the efficiency and performance of the tuning process.","But it also leads to extra cost and computation due to the involvement of LLMs in this process.","To reduce the filtering cost, we study Superfiltering:","Can we use a smaller and weaker model to select data for finetuning a larger and stronger model?","Despite the performance gap between weak and strong language models, we find their highly consistent capability to perceive instruction difficulty and data selection results.","This enables us to use a much smaller and more efficient model to filter the instruction data used to train a larger language model.","Not only does it largely speed up the data filtering, but the filtered-data-finetuned LLM achieves even better performance on standard benchmarks.","Extensive experiments validate the efficacy and efficiency of our approach."],"url":"http://arxiv.org/abs/2402.00530v1","category":"cs.CL"}
{"created":"2024-02-01 10:48:27","title":"Investigating the multidimensional separation behavior of particles in a cyclosizer setting -- A case study on calcite, fluorite and magnesite","abstract":"Particle separation is typically investigated regarding one particulate property only. Virtually all separation processes, however, act on various particle properties in different ways. Modern particle analytical modalities enable a statistically meaningful multidimensional particle characterization. Within this study, individual particle fractions of magnesite, calcite and fluorite (-71 $\\mu m$) are processed via the turbulent cross-flow separator cascade Cyclosizer (M16, MARC Technologies Pty Ltd), consisting of 5 hydrocyclones, thus producing 5 different product streams. Particle characterization is achieved via dynamic image analysis from which information on the particle shape and size is obtained. Using this data, bivariate Tromp functions are computed, which show the combined effect of the particle descriptors of roundness and area-equivalent diameter on the separation behavior. While the first cyclones recover predominantly coarse particles with high roundness values, fine particles with varying roundness are recovered in the latter cyclones.","sentences":["Particle separation is typically investigated regarding one particulate property only.","Virtually all separation processes, however, act on various particle properties in different ways.","Modern particle analytical modalities enable a statistically meaningful multidimensional particle characterization.","Within this study, individual particle fractions of magnesite, calcite and fluorite (-71 $\\mu m$) are processed via the turbulent cross-flow separator cascade Cyclosizer (M16, MARC Technologies Pty Ltd), consisting of 5 hydrocyclones, thus producing 5 different product streams.","Particle characterization is achieved via dynamic image analysis from which information on the particle shape and size is obtained.","Using this data, bivariate Tromp functions are computed, which show the combined effect of the particle descriptors of roundness and area-equivalent diameter on the separation behavior.","While the first cyclones recover predominantly coarse particles with high roundness values, fine particles with varying roundness are recovered in the latter cyclones."],"url":"http://arxiv.org/abs/2402.00488v1","category":"cond-mat.soft"}
{"created":"2024-02-01 10:26:37","title":"Caustics by Refraction of Circles and Lines","abstract":"This short note revisits a classical result that the complete caustic by refraction of a circle is the evolute of a Cartesian oval. We provide additional details to the statement and geometric proof of this fact, as presented in G. Salmon's 1879 book `Higher Plane Curves'. We observe that as the circle tends to a line, this Cartesian oval collapses into an ellipse. Finally, we discuss a computational method to find the complete caustics by refraction, independent of Salmon's proof.","sentences":["This short note revisits a classical result that the complete caustic by refraction of a circle is the evolute of a Cartesian oval.","We provide additional details to the statement and geometric proof of this fact, as presented in G. Salmon's 1879 book `Higher Plane Curves'.","We observe that as the circle tends to a line, this Cartesian oval collapses into an ellipse.","Finally, we discuss a computational method to find the complete caustics by refraction, independent of Salmon's proof."],"url":"http://arxiv.org/abs/2402.00475v1","category":"math.AG"}
{"created":"2024-02-01 10:16:04","title":"Spatio-Temporal Characterization of Qubit Routing in Connectivity-Constrained Quantum Processors","abstract":"Designing efficient quantum processor topologies is pivotal for advancing scalable quantum computing architectures. The communication overhead, a critical factor affecting the execution fidelity of quantum circuits, arises from inevitable qubit routing that brings interacting qubits into physical proximity by the means of serial SWAP gates to enable the direct two-qubit gate application. Characterizing the qubit movement across the processor is crucial for tailoring techniques for minimizing the SWAP gates. This work presents a comparative analysis of the resulting communication overhead among three processor topologies: star, heavy-hexagon lattice, and square lattice topologies, according to performance metrics of communication-to-computation ratio, mean qubit hotspotness, and temporal burstiness, showcasing that the square lattice layout is favourable for quantum computer architectures at a scale.","sentences":["Designing efficient quantum processor topologies is pivotal for advancing scalable quantum computing architectures.","The communication overhead, a critical factor affecting the execution fidelity of quantum circuits, arises from inevitable qubit routing that brings interacting qubits into physical proximity by the means of serial SWAP gates to enable the direct two-qubit gate application.","Characterizing the qubit movement across the processor is crucial for tailoring techniques for minimizing the SWAP gates.","This work presents a comparative analysis of the resulting communication overhead among three processor topologies: star, heavy-hexagon lattice, and square lattice topologies, according to performance metrics of communication-to-computation ratio, mean qubit hotspotness, and temporal burstiness, showcasing that the square lattice layout is favourable for quantum computer architectures at a scale."],"url":"http://arxiv.org/abs/2402.00469v1","category":"quant-ph"}
{"created":"2024-02-01 10:14:53","title":"Can you see me now? Blind spot estimation for autonomous vehicles using scenario-based simulation with random reference sensors","abstract":"In this paper, we introduce a method for estimating blind spots for sensor setups of autonomous or automated vehicles and/or robotics applications. In comparison to previous methods that rely on geometric approximations, our presented approach provides more realistic coverage estimates by utilizing accurate and detailed 3D simulation environments. Our method leverages point clouds from LiDAR sensors or camera depth images from high-fidelity simulations of target scenarios to provide accurate and actionable visibility estimates. A Monte Carlo-based reference sensor simulation enables us to accurately estimate blind spot size as a metric of coverage, as well as detection probabilities of objects at arbitrary positions.","sentences":["In this paper, we introduce a method for estimating blind spots for sensor setups of autonomous or automated vehicles and/or robotics applications.","In comparison to previous methods that rely on geometric approximations, our presented approach provides more realistic coverage estimates by utilizing accurate and detailed 3D simulation environments.","Our method leverages point clouds from LiDAR sensors or camera depth images from high-fidelity simulations of target scenarios to provide accurate and actionable visibility estimates.","A Monte Carlo-based reference sensor simulation enables us to accurately estimate blind spot size as a metric of coverage, as well as detection probabilities of objects at arbitrary positions."],"url":"http://arxiv.org/abs/2402.00467v1","category":"cs.RO"}
{"created":"2024-02-01 10:12:51","title":"Coded Multi-User Information Retrieval with a Multi-Antenna Helper Node","abstract":"A novel coding design is proposed to enhance information retrieval in a wireless network of users with partial access to the data, in the sense of observation, measurement, computation, or storage. Information exchange in the network is assisted by a multi-antenna base station (BS), with no direct access to the data. Accordingly, the missing parts of data are exchanged among users through an uplink (UL) step followed by a downlink (DL) step. In this paper, new coding strategies, inspired by coded caching (CC) techniques, are devised to enhance both UL and DL steps. In the UL step, users transmit encoded and properly combined parts of their accessible data to the BS. Then, during the DL step, the BS carries out the required processing on its received signals and forwards a proper combination of the resulting signal terms back to the users, enabling each user to retrieve the desired information. Using the devised coded data retrieval strategy, the data exchange in both UL and DL steps requires the same communication delay, measured by normalized delivery time (NDT). Furthermore, the NDT of the UL/DL step is shown to coincide with the optimal NDT of the original DL multi-input single-output CC scheme, in which the BS is connected to a centralized data library.","sentences":["A novel coding design is proposed to enhance information retrieval in a wireless network of users with partial access to the data, in the sense of observation, measurement, computation, or storage.","Information exchange in the network is assisted by a multi-antenna base station (BS), with no direct access to the data.","Accordingly, the missing parts of data are exchanged among users through an uplink (UL) step followed by a downlink (DL) step.","In this paper, new coding strategies, inspired by coded caching (CC) techniques, are devised to enhance both UL and DL steps.","In the UL step, users transmit encoded and properly combined parts of their accessible data to the BS.","Then, during the DL step, the BS carries out the required processing on its received signals and forwards a proper combination of the resulting signal terms back to the users, enabling each user to retrieve the desired information.","Using the devised coded data retrieval strategy, the data exchange in both UL and DL steps requires the same communication delay, measured by normalized delivery time (NDT).","Furthermore, the NDT of the UL/DL step is shown to coincide with the optimal NDT of the original DL multi-input single-output CC scheme, in which the BS is connected to a centralized data library."],"url":"http://arxiv.org/abs/2402.00465v1","category":"cs.IT"}
{"created":"2024-02-01 10:10:52","title":"Understanding gender differences in experiences and concerns surrounding online harms: A short report on a nationally representative survey of UK adults","abstract":"Online harms, such as hate speech, misinformation, harassment and self-harm promotion, continue to be widespread. While some work suggests that women are disproportionately affected by such harms, other studies find little evidence for gender differences in overall exposure. Here, we present preliminary results from a large, nationally representative survey of UK adults (N = 2000). We asked about exposure to 15 specific harms, along with fears surrounding exposure and comfort engaging in certain online behaviours. While men and women report seeing online harms to a roughly equal extent overall, we find that women are significantly more fearful of experiencing every type of harm that we asked about, and are significantly less comfortable partaking in several online behaviours. Strikingly, just 24% of women report being comfortable expressing political opinions online compared with almost 40% of men, with similar overall proportions for challenging certain content. Our work suggests that women may suffer an additional psychological burden in response to the proliferation of harmful online content, doing more 'safety work' to protect themselves. With much public discourse happening online, gender inequality in public voice is likely to be perpetuated if women feel too fearful to participate. Our results are important because to establish greater equality in society, we must take measures to ensure all members feel safe and able to participate in the online space.","sentences":["Online harms, such as hate speech, misinformation, harassment and self-harm promotion, continue to be widespread.","While some work suggests that women are disproportionately affected by such harms, other studies find little evidence for gender differences in overall exposure.","Here, we present preliminary results from a large, nationally representative survey of UK adults (N = 2000).","We asked about exposure to 15 specific harms, along with fears surrounding exposure and comfort engaging in certain online behaviours.","While men and women report seeing online harms to a roughly equal extent overall, we find that women are significantly more fearful of experiencing every type of harm that we asked about, and are significantly less comfortable partaking in several online behaviours.","Strikingly, just 24% of women report being comfortable expressing political opinions online compared with almost 40% of men, with similar overall proportions for challenging certain content.","Our work suggests that women may suffer an additional psychological burden in response to the proliferation of harmful online content, doing more 'safety work' to protect themselves.","With much public discourse happening online, gender inequality in public voice is likely to be perpetuated if women feel too fearful to participate.","Our results are important because to establish greater equality in society, we must take measures to ensure all members feel safe and able to participate in the online space."],"url":"http://arxiv.org/abs/2402.00463v1","category":"cs.CY"}
{"created":"2024-02-01 09:43:11","title":"A Hoare Logic for Domain Specification (Full Version)","abstract":"Programs must be correct with respect to their application domain. Yet, the program specification and verification approaches so far only consider correctness in terms of computations. In this work, we present a two-tier Hoare Logic that integrates assertions for both implementation and domain. For domain specification, we use description logics and semantic lifting, a recently proposed approach to interpret a program as a knowledge graph. We present a calculus that uses translations between both kinds of assertions, thus separating the concerns in specification, but enabling the use of description logic in verification.","sentences":["Programs must be correct with respect to their application domain.","Yet, the program specification and verification approaches so far only consider correctness in terms of computations.","In this work, we present a two-tier Hoare Logic that integrates assertions for both implementation and domain.","For domain specification, we use description logics and semantic lifting, a recently proposed approach to interpret a program as a knowledge graph.","We present a calculus that uses translations between both kinds of assertions, thus separating the concerns in specification, but enabling the use of description logic in verification."],"url":"http://arxiv.org/abs/2402.00452v1","category":"cs.LO"}
{"created":"2024-02-01 09:14:19","title":"Responsible developments and networking research: a reflection beyond a paper ethical statement","abstract":"Several recent initiatives have proposed new directions for research practices and their operations in the computer science community, from updated codes of conduct that clarify the use of AI-assisted tools to the inclusion of ethical statements and the organization of working groups on the environmental footprint of digitalization. In this position paper, we focus on the specific case of networking research. We reflect on the technical realization of the community and its incidence beyond techno-centric contributions. In particular, we structure the discussion around two frameworks that were recently developed in different contexts to describe the sense of engagement and responsibilities to which the practitioner of a computing-related area may be confronted.","sentences":["Several recent initiatives have proposed new directions for research practices and their operations in the computer science community, from updated codes of conduct that clarify the use of AI-assisted tools to the inclusion of ethical statements and the organization of working groups on the environmental footprint of digitalization.","In this position paper, we focus on the specific case of networking research.","We reflect on the technical realization of the community and its incidence beyond techno-centric contributions.","In particular, we structure the discussion around two frameworks that were recently developed in different contexts to describe the sense of engagement and responsibilities to which the practitioner of a computing-related area may be confronted."],"url":"http://arxiv.org/abs/2402.00442v1","category":"cs.CY"}
{"created":"2024-02-01 08:55:42","title":"Sound velocity peak induced by the chiral partner in dense two-color QCD","abstract":"Recently, the peak structure of the sound velocity was observed in the lattice simulation of two-color and two-flavor QCD at the finite quark chemical potential. The comparison with the chiral perturbation theory (ChPT) result was undertaken, however, the ChPT failed in reproducing the peak structure. In this study, to extend the ChPT framework, we incorporate contributions of the $\\sigma$ meson, that is identified as the chiral partner of pions, on top of the low-energy pion dynamics by using the linear sigma model (LSM). Based on the LSM we derive analytic expressions of the thermodynamic quantities as well as the sound velocity within a mean-field approximation. As a result, we find that those quantities are provided by sums of the ChPT results and corrections, where the latter is characterized by a mass difference between the chiral partners, the $\\sigma$ meson and pion. The chiral partner contributions are found to yield a peak in the sound velocity successfully. We furthermore show that the sound velocity peak emerges only when $m_\\sigma >\\sqrt{3}m_\\pi$ and $\\mu_q > m_\\pi$, with $m_{\\sigma(\\pi)}$ and $\\mu_q$ being the $\\sigma$ meson (pion) mass and the quark chemical potential, respectively. The correlation between the sound velocity peak and the sign of the trace anomaly is also addressed.","sentences":["Recently, the peak structure of the sound velocity was observed in the lattice simulation of two-color and two-flavor QCD at the finite quark chemical potential.","The comparison with the chiral perturbation theory (ChPT) result was undertaken, however, the ChPT failed in reproducing the peak structure.","In this study, to extend the ChPT framework, we incorporate contributions of the $\\sigma$ meson, that is identified as the chiral partner of pions, on top of the low-energy pion dynamics by using the linear sigma model (LSM).","Based on the LSM we derive analytic expressions of the thermodynamic quantities as well as the sound velocity within a mean-field approximation.","As a result, we find that those quantities are provided by sums of the ChPT results and corrections, where the latter is characterized by a mass difference between the chiral partners, the $\\sigma$ meson and pion.","The chiral partner contributions are found to yield a peak in the sound velocity successfully.","We furthermore show that the sound velocity peak emerges only when","$m_\\sigma >\\sqrt{3}m_\\pi$ and $\\mu_q > m_\\pi$, with $m_{\\sigma(\\pi)}$ and $\\mu_q$ being the $\\sigma$ meson (pion) mass and the quark chemical potential, respectively.","The correlation between the sound velocity peak and the sign of the trace anomaly is also addressed."],"url":"http://arxiv.org/abs/2402.00430v1","category":"hep-ph"}
{"created":"2024-02-01 08:08:01","title":"The enhancement of pair production in oscillated overlapped fields","abstract":"The influence of potential well width on electron-positron pair production has been examined through theoretical and numerical approaches by employing the computational quantum field theory. Quantum interference effects in pair production is investigated in the two overlapped potential wells with varied widths and frequencies. Several dominant processes, involving the absorption of an integer number of photons, significantly impact on pair production. Notably, specific multiphoton absorption processes exhibit distinct changes as the potential well width expands, with the absorption of four photons process displaying noteworthy effects. Additionally, the influence of the smaller frequency to the yield of the pair production can not be ignored and the most optimized frequencies in our overlapped fields has been studied and exhibited.","sentences":["The influence of potential well width on electron-positron pair production has been examined through theoretical and numerical approaches by employing the computational quantum field theory.","Quantum interference effects in pair production is investigated in the two overlapped potential wells with varied widths and frequencies.","Several dominant processes, involving the absorption of an integer number of photons, significantly impact on pair production.","Notably, specific multiphoton absorption processes exhibit distinct changes as the potential well width expands, with the absorption of four photons process displaying noteworthy effects.","Additionally, the influence of the smaller frequency to the yield of the pair production can not be ignored and the most optimized frequencies in our overlapped fields has been studied and exhibited."],"url":"http://arxiv.org/abs/2402.00410v1","category":"hep-ph"}
{"created":"2024-02-01 06:55:59","title":"Error-Tolerant Amplification and Simulation of the Ultrastrong-Coupling Quantum Rabi Model","abstract":"Cat-state qubits formed by photonic cat states show great promise for hardware-efficient universal quantum computing. We demonstrate that cat-state qubits are also promising for error-tolerant simulations of the quantum Rabi model (and its varieties) to enhance the coupling strength between the cat-state qubit and a cavity, to reach the ultrastrong-coupling regime. This allows us to explore several fascinating quantum phenomena relying on the counter-rotating interaction. A benefit from biased-noise cat qubits is that the two main error channels (frequency and amplitude mismatches) are both exponentially suppressed. Therefore, the simulation protocols are robust against parameter errors of the parametric drive which determines the projection subspace. We analyze three examples: (i) collapse and revivals of quantum states; (ii) hidden symmetry and tunneling dynamics; and (iii) pair-cat-code computation.","sentences":["Cat-state qubits formed by photonic cat states show great promise for hardware-efficient universal quantum computing.","We demonstrate that cat-state qubits are also promising for error-tolerant simulations of the quantum Rabi model (and its varieties) to enhance the coupling strength between the cat-state qubit and a cavity, to reach the ultrastrong-coupling regime.","This allows us to explore several fascinating quantum phenomena relying on the counter-rotating interaction.","A benefit from biased-noise cat qubits is that the two main error channels (frequency and amplitude mismatches) are both exponentially suppressed.","Therefore, the simulation protocols are robust against parameter errors of the parametric drive which determines the projection subspace.","We analyze three examples: (i) collapse and revivals of quantum states; (ii) hidden symmetry and tunneling dynamics; and (iii) pair-cat-code computation."],"url":"http://arxiv.org/abs/2402.00379v1","category":"quant-ph"}
{"created":"2024-02-01 05:57:13","title":"nhppp: Simulating Nonhomogeneous Poisson Point Processes in R","abstract":"We introduce the `nhppp' package for simulating events from one dimensional non-homogeneous Poisson point processes (NHPPPs) in R. Its functions are based on three algorithms that provably sample from a target NHPPP: the time-transformation of a homogeneous Poisson process (of intensity one) via the inverse of the integrated intensity function; the generation of a Poisson number of order statistics from a fixed density function; and the thinning of a majorizing NHPPP via an acceptance-rejection scheme. We present a study of numerical accuracy and time performance of the algorithms and advice on which algorithm to prefer in each situation. Functions available in the package are illustrated with simple reproducible examples.","sentences":["We introduce the `nhppp' package for simulating events from one dimensional non-homogeneous Poisson point processes (NHPPPs) in R. Its functions are based on three algorithms that provably sample from a target NHPPP: the time-transformation of a homogeneous Poisson process (of intensity one) via the inverse of the integrated intensity function; the generation of a Poisson number of order statistics from a fixed density function; and the thinning of a majorizing NHPPP via an acceptance-rejection scheme.","We present a study of numerical accuracy and time performance of the algorithms and advice on which algorithm to prefer in each situation.","Functions available in the package are illustrated with simple reproducible examples."],"url":"http://arxiv.org/abs/2402.00358v1","category":"stat.CO"}
{"created":"2024-02-01 05:57:10","title":"Safety of Multimodal Large Language Models on Images and Text","abstract":"Attracted by the impressive power of Multimodal Large Language Models (MLLMs), the public is increasingly utilizing them to improve the efficiency of daily work. Nonetheless, the vulnerabilities of MLLMs to unsafe instructions bring huge safety risks when these models are deployed in real-world scenarios. In this paper, we systematically survey current efforts on the evaluation, attack, and defense of MLLMs' safety on images and text. We begin with introducing the overview of MLLMs on images and text and understanding of safety, which helps researchers know the detailed scope of our survey. Then, we review the evaluation datasets and metrics for measuring the safety of MLLMs. Next, we comprehensively present attack and defense techniques related to MLLMs' safety. Finally, we analyze several unsolved issues and discuss promising research directions.","sentences":["Attracted by the impressive power of Multimodal Large Language Models (MLLMs), the public is increasingly utilizing them to improve the efficiency of daily work.","Nonetheless, the vulnerabilities of MLLMs to unsafe instructions bring huge safety risks when these models are deployed in real-world scenarios.","In this paper, we systematically survey current efforts on the evaluation, attack, and defense of MLLMs' safety on images and text.","We begin with introducing the overview of MLLMs on images and text and understanding of safety, which helps researchers know the detailed scope of our survey.","Then, we review the evaluation datasets and metrics for measuring the safety of MLLMs.","Next, we comprehensively present attack and defense techniques related to MLLMs' safety.","Finally, we analyze several unsolved issues and discuss promising research directions."],"url":"http://arxiv.org/abs/2402.00357v1","category":"cs.CV"}
{"created":"2024-02-01 04:42:54","title":"Approximating the Smallest $k$-Enclosing Geodesic Disc in a Simple Polygon","abstract":"We consider the problem of finding a geodesic disc of smallest radius containing at least $k$ points from a set of $n$ points in a simple polygon that has $m$ vertices, $r$ of which are reflex vertices. We refer to such a disc as a SKEG disc. We present an algorithm to compute a SKEG disc using higher-order geodesic Voronoi diagrams with worst-case time $O(k^{2} n + k^{2} r + \\min(kr, r(n-k)) + m)$ ignoring polylogarithmic factors.   We then present two $2$-approximation algorithms that find a geodesic disc containing at least $k$ points whose radius is at most twice that of a SKEG disc. The first algorithm computes a $2$-approximation with high probability in $O((n^{2} / k) \\log n \\log r + m)$ worst-case time with $O(n + m)$ space. The second algorithm runs in $O(n \\log^{2} n \\log r + m)$ expected time using $O(n + m)$ expected space, independent of $k$. Note that the first algorithm is faster when $k \\in \\omega(n / \\log n)$.","sentences":["We consider the problem of finding a geodesic disc of smallest radius containing at least $k$ points from a set of $n$ points in a simple polygon that has $m$ vertices, $r$ of which are reflex vertices.","We refer to such a disc as a SKEG disc.","We present an algorithm to compute a SKEG disc using higher-order geodesic Voronoi diagrams with worst-case time $O(k^{2} n + k^{2} r + \\min(kr, r(n-k))","+ m)$ ignoring polylogarithmic factors.   ","We then present two $2$-approximation algorithms that find a geodesic disc containing at least $k$ points whose radius is at most twice that of a SKEG disc.","The first algorithm computes a $2$-approximation with high probability in $O((n^{2} / k) \\log n \\log r + m)$ worst-case time with $O(n + m)$ space.","The second algorithm runs in $O(n \\log^{2} n \\log r + m)$ expected time using $O(n + m)$ expected space, independent of $k$. Note that the first algorithm is faster when $k \\in \\omega(n / \\log n)$."],"url":"http://arxiv.org/abs/2402.00336v1","category":"cs.CG"}
{"created":"2024-02-01 04:35:31","title":"Smooth and Proper Maps","abstract":"This is an expository note explaining how the geometric notions of local connectedness and properness are related to the $\\Sigma$-type and $\\Pi$-type constructors of dependent type theory.","sentences":["This is an expository note explaining how the geometric notions of local connectedness and properness are related to the $\\Sigma$-type and $\\Pi$-type constructors of dependent type theory."],"url":"http://arxiv.org/abs/2402.00331v1","category":"math.CT"}
{"created":"2024-02-01 18:20:55","title":"Approximating maximum-size properly colored forests","abstract":"In the Properly Colored Spanning Tree problem, we are given an edge-colored undirected graph and the goal is to find a properly colored spanning tree, i.e., a spanning tree in which any two adjacent edges have distinct colors. The problem is interesting not only from a graph coloring point of view, but is also closely related to the Degree Bounded Spanning Tree and (1,2)-Traveling Salesman problems, two classical questions that have attracted considerable interest in combinatorial optimization and approximation theory. Previous work on properly colored spanning trees has mainly focused on determining the existence of such a tree and hence has not considered the question from an algorithmic perspective. We propose an optimization version called Maximum-size Properly Colored Forest problem, which aims to find a properly colored forest with as many edges as possible. We consider the problem in different graph classes and for different numbers of colors, and present polynomial-time approximation algorithms as well as inapproximability results for these settings. Our proof technique relies on the sum of matching matroids defined by the color classes, a connection that might be of independent combinatorial interest.   We also consider the Maximum-size Properly Colored Tree problem, which asks for the maximum size of a properly colored tree not necessarily spanning all the vertices. We show that the optimum is significantly more difficult to approximate than in the forest case, and provide an approximation algorithm for complete multigraphs.","sentences":["In the Properly Colored Spanning Tree problem, we are given an edge-colored undirected graph and the goal is to find a properly colored spanning tree, i.e., a spanning tree in which any two adjacent edges have distinct colors.","The problem is interesting not only from a graph coloring point of view, but is also closely related to the Degree Bounded Spanning Tree and (1,2)-Traveling Salesman problems, two classical questions that have attracted considerable interest in combinatorial optimization and approximation theory.","Previous work on properly colored spanning trees has mainly focused on determining the existence of such a tree and hence has not considered the question from an algorithmic perspective.","We propose an optimization version called Maximum-size Properly Colored Forest problem, which aims to find a properly colored forest with as many edges as possible.","We consider the problem in different graph classes and for different numbers of colors, and present polynomial-time approximation algorithms as well as inapproximability results for these settings.","Our proof technique relies on the sum of matching matroids defined by the color classes, a connection that might be of independent combinatorial interest.   ","We also consider the Maximum-size Properly Colored Tree problem, which asks for the maximum size of a properly colored tree not necessarily spanning all the vertices.","We show that the optimum is significantly more difficult to approximate than in the forest case, and provide an approximation algorithm for complete multigraphs."],"url":"http://arxiv.org/abs/2402.00834v1","category":"cs.DS"}
{"created":"2024-02-01 18:16:53","title":"The En Route Truck-Drone Delivery Problem","abstract":"We study the truck-drone cooperative delivery problem in a setting where a single truck carrying a drone travels at constant speed on a straight-line trajectory/street. Delivery to clients located in the plane and not on the truck's trajectory is performed by the drone, which has limited carrying capacity and flying range, and whose battery can be recharged when on the truck. We show that the problem of maximizing the number of deliveries is strongly NP-hard even in this simple setting. We present a 2-approximation algorithm for the problem, and an optimal algorithm for a non-trivial family of instances.","sentences":["We study the truck-drone cooperative delivery problem in a setting where a single truck carrying a drone travels at constant speed on a straight-line trajectory/street.","Delivery to clients located in the plane and not on the truck's trajectory is performed by the drone, which has limited carrying capacity and flying range, and whose battery can be recharged when on the truck.","We show that the problem of maximizing the number of deliveries is strongly NP-hard even in this simple setting.","We present a 2-approximation algorithm for the problem, and an optimal algorithm for a non-trivial family of instances."],"url":"http://arxiv.org/abs/2402.00829v1","category":"cs.DS"}
{"created":"2024-02-01 16:37:15","title":"Localization of point scatterers via sparse optimization on measures","abstract":"We consider the inverse scattering problem for time-harmonic acoustic waves in a medium with pointwise inhomogeneities. In the Foldy-Lax model, the estimation of the scatterers' locations and intensities from far field measurements can be recast as the recovery of a discrete measure from nonlinear observations. We propose a \"linearize and locally optimize\" approach to perform this reconstruction. We first solve a convex program in the space of measures (known as the Beurling LASSO), which involves a linearization of the forward operator (the far field pattern in the Born approximation). Then, we locally minimize a second functional involving the nonlinear forward map, using the output of the first step as initialization. We provide guarantees that the output of the first step is close to the sought-after measure when the scatterers have small intensities and are sufficiently separated. We also provide numerical evidence that the second step still allows for accurate recovery in settings that are more involved.","sentences":["We consider the inverse scattering problem for time-harmonic acoustic waves in a medium with pointwise inhomogeneities.","In the Foldy-Lax model, the estimation of the scatterers' locations and intensities from far field measurements can be recast as the recovery of a discrete measure from nonlinear observations.","We propose a \"linearize and locally optimize\" approach to perform this reconstruction.","We first solve a convex program in the space of measures (known as the Beurling LASSO), which involves a linearization of the forward operator (the far field pattern in the Born approximation).","Then, we locally minimize a second functional involving the nonlinear forward map, using the output of the first step as initialization.","We provide guarantees that the output of the first step is close to the sought-after measure when the scatterers have small intensities and are sufficiently separated.","We also provide numerical evidence that the second step still allows for accurate recovery in settings that are more involved."],"url":"http://arxiv.org/abs/2402.00737v1","category":"math.NA"}
{"created":"2024-02-01 15:26:48","title":"Nanoscale Control over Magnetic Light-Matter Interactions","abstract":"Light-matter interactions are frequently perceived as predominantly influenced by the electric optical field, with the magnetic component of light often overlooked. Nonetheless, the magnetic aspect plays a pivotal role in various optical processes, including chiral light-matter interactions, photon-avalanching, and forbidden photochemistry, underscoring the significance of manipulating magnetic processes in optical phenomena. Here, we explore the ability to control the magnetic light and matter interactions at the nanoscale. In particular, we demonstrate experimentally, using a plasmonic nanostructure, the transfer of energy from the optical magnetic field to a nanoparticle, thanks to the deep subwavelength magnetic confinement allowed by our nano-antenna. This control is made possible by the particular design of our plasmonic nanostructure, which has been optimized to spatially separate the electric and magnetic fields of the localized plasmon. Furthermore, by studying the spontaneous emission from the Lanthanide-ions doped nanoparticle, we observe that the optical field distributions are not spatially correlated with the electric and magnetic near-field quantum environments of this antenna, which seemingly contradicts the reciprocity theorem. We demonstrate that this counter-intuitive observation is in fact, the result of the different optical paths followed by the excitation and emission of the ions, which forbids a direct application of that theorem.","sentences":["Light-matter interactions are frequently perceived as predominantly influenced by the electric optical field, with the magnetic component of light often overlooked.","Nonetheless, the magnetic aspect plays a pivotal role in various optical processes, including chiral light-matter interactions, photon-avalanching, and forbidden photochemistry, underscoring the significance of manipulating magnetic processes in optical phenomena.","Here, we explore the ability to control the magnetic light and matter interactions at the nanoscale.","In particular, we demonstrate experimentally, using a plasmonic nanostructure, the transfer of energy from the optical magnetic field to a nanoparticle, thanks to the deep subwavelength magnetic confinement allowed by our nano-antenna.","This control is made possible by the particular design of our plasmonic nanostructure, which has been optimized to spatially separate the electric and magnetic fields of the localized plasmon.","Furthermore, by studying the spontaneous emission from the Lanthanide-ions doped nanoparticle, we observe that the optical field distributions are not spatially correlated with the electric and magnetic near-field quantum environments of this antenna, which seemingly contradicts the reciprocity theorem.","We demonstrate that this counter-intuitive observation is in fact, the result of the different optical paths followed by the excitation and emission of the ions, which forbids a direct application of that theorem."],"url":"http://arxiv.org/abs/2402.00666v1","category":"physics.optics"}
{"created":"2024-02-01 10:57:51","title":"Optimized sample addressing in prism-coupled surface plasmon resonance experiments","abstract":"In this work we study the walk-off of the beam from the interrogation spot during rotation in surface plasmon resonance experiments using prism-based coupling such as the widespread Kretschmann configuration. The impossibility of maintaining a stationary footprint on the sensing surface with a fixed rotation axis can be of high importance. This would be specially so if samples are not homogeneous such as in arrays for multiplexing. By theoretically analyzing the behavior of the walk-off during rotation around an arbitrary fixed axis, we find an optimal and simple configuration to minimize this effect. The proposed setup is experimentally tested to verify the results and to show its ease of implementation. Interestingly, the conclusions reached may also be applied to other techniques employing reflection prisms.","sentences":["In this work we study the walk-off of the beam from the interrogation spot during rotation in surface plasmon resonance experiments using prism-based coupling such as the widespread Kretschmann configuration.","The impossibility of maintaining a stationary footprint on the sensing surface with a fixed rotation axis can be of high importance.","This would be specially so if samples are not homogeneous such as in arrays for multiplexing.","By theoretically analyzing the behavior of the walk-off during rotation around an arbitrary fixed axis, we find an optimal and simple configuration to minimize this effect.","The proposed setup is experimentally tested to verify the results and to show its ease of implementation.","Interestingly, the conclusions reached may also be applied to other techniques employing reflection prisms."],"url":"http://arxiv.org/abs/2402.00492v1","category":"physics.optics"}
{"created":"2024-02-01 06:01:00","title":"Autometrized Lattice Ordered Monoids","abstract":"In this paper, we introduce the notion of Autometrized lattice ordered monoids (for short,AL-monoids) as a generalization to DRl-semi groups. We obtain the basic properties of AL-monoids. Also, we prove that Autometrized lattice ordered monoids are equationally definable. Furthermore, we show that AL-monoids are an optimal common abstraction of Boolean algebras and l-groups.","sentences":["In this paper, we introduce the notion of Autometrized lattice ordered monoids (for short,AL-monoids) as a generalization to DRl-semi groups.","We obtain the basic properties of AL-monoids.","Also, we prove that Autometrized lattice ordered monoids are equationally definable.","Furthermore, we show that AL-monoids are an optimal common abstraction of Boolean algebras and l-groups."],"url":"http://arxiv.org/abs/2402.00361v1","category":"math.LO"}
{"created":"2024-02-01 05:02:44","title":"A Physics-Informed Indirect Method for Trajectory Optimization","abstract":"This work presents a Physics-Informed Indirect Method (PIIM) that propagates the dynamics of both states and co-states backward in time for trajectory optimization problems. In the case of a Time-Optimal Soft Landing Problem (TOSLP), based on the initial co-state vector normalization technique, we show that the initial guess of the mass co-state and the numerical factor can be eliminated from the shooting procedure. As a result, the initial guess of the unknown co-states can be constrained to lie on a unit 3-D hypersphere. Then, using the PIIM allows one to exploit the physical significance of the optimal control law, which further narrows down the solution space to a unit 3-D octant sphere. Meanwhile, the analytical estimations of the fuel consumption and final time are provided. Additionally, a usually overlooked issue that results in an infeasible solution with a negative final time, is fixed by a simple remedy strategy. Consequently, the reduced solution space becomes sufficiently small to ensure fast, robust, and guaranteed convergence for the TOSLP. Then, we extend the PIIM to solve the Fuel-Optimal Soft Landing Problem (FOSLP) with a homotopy approach. The numerical simulations show that compared with the conventional indirect method with a success rate of 89.35%, it takes a shorter time for the proposed method to find the feasible solution to the FOSLP with a success rate of 100%.","sentences":["This work presents a Physics-Informed Indirect Method (PIIM) that propagates the dynamics of both states and co-states backward in time for trajectory optimization problems.","In the case of a Time-Optimal Soft Landing Problem (TOSLP), based on the initial co-state vector normalization technique, we show that the initial guess of the mass co-state and the numerical factor can be eliminated from the shooting procedure.","As a result, the initial guess of the unknown co-states can be constrained to lie on a unit 3-D hypersphere.","Then, using the PIIM allows one to exploit the physical significance of the optimal control law, which further narrows down the solution space to a unit 3-D octant sphere.","Meanwhile, the analytical estimations of the fuel consumption and final time are provided.","Additionally, a usually overlooked issue that results in an infeasible solution with a negative final time, is fixed by a simple remedy strategy.","Consequently, the reduced solution space becomes sufficiently small to ensure fast, robust, and guaranteed convergence for the TOSLP.","Then, we extend the PIIM to solve the Fuel-Optimal Soft Landing Problem (FOSLP) with a homotopy approach.","The numerical simulations show that compared with the conventional indirect method with a success rate of 89.35%, it takes a shorter time for the proposed method to find the feasible solution to the FOSLP with a success rate of 100%."],"url":"http://arxiv.org/abs/2402.00339v1","category":"math.OC"}
{"created":"2024-02-01 04:22:20","title":"Optimized Parameter Design for Channel State Information-Free Location Spoofing","abstract":"In this paper, an augmented analysis of a delay-angle information spoofing (DAIS) is provided for location-privacy preservation, where the location-relevant delays and angles are artificially shifted to obfuscate the eavesdropper with an incorrect physical location. A simplified mismatched Cramer-Rao bound (MCRB) is derived, which clearly manifests that not only estimation error, but also the geometric mismatch introduced by DAIS can lead to a significant increase in localization error for an eavesdropper. Given an assumption of the orthogonality among wireless paths, the simplified MCRB can be further expressed as a function of delay-angle shifts in a closed-form, which enables the more straightforward optimization of these design parameters for location-privacy enhancement. Numerical results are provided, validating the theoretical analysis and showing that the root-mean-square error for eavesdropper's localization can be more than 150 m with the optimized delay-angle shifts for DAIS.","sentences":["In this paper, an augmented analysis of a delay-angle information spoofing (DAIS) is provided for location-privacy preservation, where the location-relevant delays and angles are artificially shifted to obfuscate the eavesdropper with an incorrect physical location.","A simplified mismatched Cramer-Rao bound (MCRB) is derived, which clearly manifests that not only estimation error, but also the geometric mismatch introduced by DAIS can lead to a significant increase in localization error for an eavesdropper.","Given an assumption of the orthogonality among wireless paths, the simplified MCRB can be further expressed as a function of delay-angle shifts in a closed-form, which enables the more straightforward optimization of these design parameters for location-privacy enhancement.","Numerical results are provided, validating the theoretical analysis and showing that the root-mean-square error for eavesdropper's localization can be more than 150 m with the optimized delay-angle shifts for DAIS."],"url":"http://arxiv.org/abs/2402.00329v1","category":"eess.SP"}
{"created":"2024-02-01 01:45:46","title":"A Crucial Parameter for Rank-Frequency Relation in Natural Languages","abstract":"$f \\propto r^{-\\alpha} \\cdot (r+\\gamma)^{-\\beta}$ has been empirically shown more precise than a na\\\"ive power law $f\\propto r^{-\\alpha}$ to model the rank-frequency ($r$-$f$) relation of words in natural languages. This work shows that the only crucial parameter in the formulation is $\\gamma$, which depicts the resistance to vocabulary growth on a corpus. A method of parameter estimation by searching an optimal $\\gamma$ is proposed, where a ``zeroth word'' is introduced technically for the calculation. The formulation and parameters are further discussed with several case studies.","sentences":["$f \\propto r^{-\\alpha} \\cdot (r+\\gamma)^{-\\beta}$ has been empirically shown more precise than a na\\\"ive power law $f\\propto r^{-\\alpha}$ to model the rank-frequency ($r$-$f$) relation of words in natural languages.","This work shows that the only crucial parameter in the formulation is $\\gamma$, which depicts the resistance to vocabulary growth on a corpus.","A method of parameter estimation by searching an optimal $\\gamma$ is proposed, where a ``zeroth word'' is introduced technically for the calculation.","The formulation and parameters are further discussed with several case studies."],"url":"http://arxiv.org/abs/2402.00271v1","category":"cs.CL"}
{"created":"2024-02-01 01:08:22","title":"Optimum classical beam position sensing","abstract":"Beam displacement measurements are widely used in optical sensing and communications; however, their performance is affected by numerous intrinsic and extrinsic factors including beam profile, propagation loss, and receiver architecture. Here we present a framework for designing a classically optimal beam displacement transceiver, using quantum estimation theory. We consider the canonical task of estimating the position of a diffraction-limited laser beam after passing through an apertured volume characterized by Fresnel-number product DF. As a rule of thumb, higher-order Gaussian modes provide more information about beam displacement, but are more sensitive to loss. Applying quantum Fisher information, we design mode combinations that optimally leverage this trade-off, and show that a greater than 10-fold improvement in precision is possible, relative to the fundamental mode, for a practically relevant DF = 100. We also show that this improvement is realizable with a variety of practical receiver architectures. Our findings extend previous works on lossless transceivers, may have immediate impact on applications such as atomic force microscopy and near-field optical communication, and pave the way towards globally optimal transceivers using non-classical laser fields.","sentences":["Beam displacement measurements are widely used in optical sensing and communications; however, their performance is affected by numerous intrinsic and extrinsic factors including beam profile, propagation loss, and receiver architecture.","Here we present a framework for designing a classically optimal beam displacement transceiver, using quantum estimation theory.","We consider the canonical task of estimating the position of a diffraction-limited laser beam after passing through an apertured volume characterized by Fresnel-number product DF.","As a rule of thumb, higher-order Gaussian modes provide more information about beam displacement, but are more sensitive to loss.","Applying quantum Fisher information, we design mode combinations that optimally leverage this trade-off, and show that a greater than 10-fold improvement in precision is possible, relative to the fundamental mode, for a practically relevant DF = 100.","We also show that this improvement is realizable with a variety of practical receiver architectures.","Our findings extend previous works on lossless transceivers, may have immediate impact on applications such as atomic force microscopy and near-field optical communication, and pave the way towards globally optimal transceivers using non-classical laser fields."],"url":"http://arxiv.org/abs/2402.00259v1","category":"physics.optics"}
