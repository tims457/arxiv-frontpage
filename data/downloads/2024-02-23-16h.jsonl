{"created":"2024-02-21 18:54:37","title":"Corrective Machine Unlearning","abstract":"Machine Learning models increasingly face data integrity challenges due to the use of large-scale training datasets drawn from the internet. We study what model developers can do if they detect that some data was manipulated or incorrect. Such manipulated data can cause adverse effects like vulnerability to backdoored samples, systematic biases, and in general, reduced accuracy on certain input domains. Often, all manipulated training samples are not known, and only a small, representative subset of the affected data is flagged.   We formalize \"Corrective Machine Unlearning\" as the problem of mitigating the impact of data affected by unknown manipulations on a trained model, possibly knowing only a subset of impacted samples. We demonstrate that the problem of corrective unlearning has significantly different requirements from traditional privacy-oriented unlearning. We find most existing unlearning methods, including the gold-standard retraining-from-scratch, require most of the manipulated data to be identified for effective corrective unlearning. However, one approach, SSD, achieves limited success in unlearning adverse effects with just a small portion of the manipulated samples, showing the tractability of this setting. We hope our work spurs research towards developing better methods for corrective unlearning and offers practitioners a new strategy to handle data integrity challenges arising from web-scale training.","sentences":["Machine Learning models increasingly face data integrity challenges due to the use of large-scale training datasets drawn from the internet.","We study what model developers can do if they detect that some data was manipulated or incorrect.","Such manipulated data can cause adverse effects like vulnerability to backdoored samples, systematic biases, and in general, reduced accuracy on certain input domains.","Often, all manipulated training samples are not known, and only a small, representative subset of the affected data is flagged.   ","We formalize \"Corrective Machine Unlearning\" as the problem of mitigating the impact of data affected by unknown manipulations on a trained model, possibly knowing only a subset of impacted samples.","We demonstrate that the problem of corrective unlearning has significantly different requirements from traditional privacy-oriented unlearning.","We find most existing unlearning methods, including the gold-standard retraining-from-scratch, require most of the manipulated data to be identified for effective corrective unlearning.","However, one approach, SSD, achieves limited success in unlearning adverse effects with just a small portion of the manipulated samples, showing the tractability of this setting.","We hope our work spurs research towards developing better methods for corrective unlearning and offers practitioners a new strategy to handle data integrity challenges arising from web-scale training."],"url":"http://arxiv.org/abs/2402.14015v1","category":"cs.LG"}
{"created":"2024-02-21 18:48:38","title":"Can Watermarks Survive Translation? On the Cross-lingual Consistency of Text Watermark for Large Language Models","abstract":"Text watermarking technology aims to tag and identify content produced by large language models (LLMs) to prevent misuse. In this study, we introduce the concept of ''cross-lingual consistency'' in text watermarking, which assesses the ability of text watermarks to maintain their effectiveness after being translated into other languages. Preliminary empirical results from two LLMs and three watermarking methods reveal that current text watermarking technologies lack consistency when texts are translated into various languages. Based on this observation, we propose a Cross-lingual Watermark Removal Attack (CWRA) to bypass watermarking by first obtaining a response from an LLM in a pivot language, which is then translated into the target language. CWRA can effectively remove watermarks by reducing the Area Under the Curve (AUC) from 0.95 to 0.67 without performance loss. Furthermore, we analyze two key factors that contribute to the cross-lingual consistency in text watermarking and propose a defense method that increases the AUC from 0.67 to 0.88 under CWRA.","sentences":["Text watermarking technology aims to tag and identify content produced by large language models (LLMs) to prevent misuse.","In this study, we introduce the concept of ''cross-lingual consistency'' in text watermarking, which assesses the ability of text watermarks to maintain their effectiveness after being translated into other languages.","Preliminary empirical results from two LLMs and three watermarking methods reveal that current text watermarking technologies lack consistency when texts are translated into various languages.","Based on this observation, we propose a Cross-lingual Watermark Removal Attack (CWRA) to bypass watermarking by first obtaining a response from an LLM in a pivot language, which is then translated into the target language.","CWRA can effectively remove watermarks by reducing the Area Under the Curve (AUC) from 0.95 to 0.67 without performance loss.","Furthermore, we analyze two key factors that contribute to the cross-lingual consistency in text watermarking and propose a defense method that increases the AUC from 0.67 to 0.88 under CWRA."],"url":"http://arxiv.org/abs/2402.14007v1","category":"cs.CL"}
{"created":"2024-02-21 18:44:38","title":"Information Elicitation in Agency Games","abstract":"Rapid progress in scalable, commoditized tools for data collection and data processing has made it possible for firms and policymakers to employ ever more complex metrics as guides for decision-making. These developments have highlighted a prevailing challenge -- deciding *which* metrics to compute. In particular, a firm's ability to compute a wider range of existing metrics does not address the problem of *unknown unknowns*, which reflects informational limitations on the part of the firm. To guide the choice of metrics in the face of this informational problem, we turn to the evaluated agents themselves, who may have more information than a principal about how to measure outcomes effectively. We model this interaction as a simple agency game, where we ask: *When does an agent have an incentive to reveal the observability of a cost-correlated variable to the principal?* There are two effects: better information reduces the agent's information rents but also makes some projects go forward that otherwise would fail. We show that the agent prefers to reveal information that exposes a strong enough differentiation between high and low costs. Expanding the agent's action space to include the ability to *garble* their information, we show that the agent often prefers to garble over full revelation. Still, giving the agent the ability to garble can lead to higher total welfare. Our model has analogies with price discrimination, and we leverage some of these synergies to analyze total welfare.","sentences":["Rapid progress in scalable, commoditized tools for data collection and data processing has made it possible for firms and policymakers to employ ever more complex metrics as guides for decision-making.","These developments have highlighted a prevailing challenge -- deciding *which* metrics to compute.","In particular, a firm's ability to compute a wider range of existing metrics does not address the problem of *unknown unknowns*, which reflects informational limitations on the part of the firm.","To guide the choice of metrics in the face of this informational problem, we turn to the evaluated agents themselves, who may have more information than a principal about how to measure outcomes effectively.","We model this interaction as a simple agency game, where we ask: *When does an agent have an incentive to reveal the observability of a cost-correlated variable to the principal?","*","There are two effects: better information reduces the agent's information rents but also makes some projects go forward that otherwise would fail.","We show that the agent prefers to reveal information that exposes a strong enough differentiation between high and low costs.","Expanding the agent's action space to include the ability to *garble* their information, we show that the agent often prefers to garble over full revelation.","Still, giving the agent the ability to garble can lead to higher total welfare.","Our model has analogies with price discrimination, and we leverage some of these synergies to analyze total welfare."],"url":"http://arxiv.org/abs/2402.14005v1","category":"cs.GT"}
{"created":"2024-02-21 18:40:24","title":"Hallucinations or Attention Misdirection? The Path to Strategic Value Extraction in Business Using Large Language Models","abstract":"Large Language Models with transformer architecture have revolutionized the domain of text generation, setting unprecedented benchmarks. Despite their impressive capabilities, LLMs have been criticized for generating outcomes that deviate from factual accuracy or display logical inconsistencies, phenomena commonly referred to as hallucinations. This term, however, has often been misapplied to any results deviating from the instructor's expectations, which this paper defines as attention misdirection rather than true hallucinations. Understanding the distinction between hallucinations and attention misdirection becomes increasingly relevant in business contexts, where the ramifications of such errors can significantly impact the value extraction from these inherently pre-trained models. This paper highlights the best practices of the PGI, Persona, Grouping, and Intelligence, method, a strategic framework that achieved a remarkable error rate of only 3,15 percent across 4,000 responses generated by GPT in response to a real business challenge. It emphasizes that by equipping experimentation with knowledge, businesses can unlock opportunities for innovation through the use of these natively pre-trained models. This reinforces the notion that strategic application grounded in a skilled team can maximize the benefits of emergent technologies such as the LLMs.","sentences":["Large Language Models with transformer architecture have revolutionized the domain of text generation, setting unprecedented benchmarks.","Despite their impressive capabilities, LLMs have been criticized for generating outcomes that deviate from factual accuracy or display logical inconsistencies, phenomena commonly referred to as hallucinations.","This term, however, has often been misapplied to any results deviating from the instructor's expectations, which this paper defines as attention misdirection rather than true hallucinations.","Understanding the distinction between hallucinations and attention misdirection becomes increasingly relevant in business contexts, where the ramifications of such errors can significantly impact the value extraction from these inherently pre-trained models.","This paper highlights the best practices of the PGI, Persona, Grouping, and Intelligence, method, a strategic framework that achieved a remarkable error rate of only 3,15 percent across 4,000 responses generated by GPT in response to a real business challenge.","It emphasizes that by equipping experimentation with knowledge, businesses can unlock opportunities for innovation through the use of these natively pre-trained models.","This reinforces the notion that strategic application grounded in a skilled team can maximize the benefits of emergent technologies such as the LLMs."],"url":"http://arxiv.org/abs/2402.14002v1","category":"cs.CL"}
{"created":"2024-02-21 18:09:04","title":"The Importance of Architecture Choice in Deep Learning for Climate Applications","abstract":"Machine Learning has become a pervasive tool in climate science applications. However, current models fail to address nonstationarity induced by anthropogenic alterations in greenhouse emissions and do not routinely quantify the uncertainty of proposed projections. In this paper, we model the Atlantic Meridional Overturning Circulation (AMOC) which is of major importance to climate in Europe and the US East Coast by transporting warm water to these regions, and has the potential for abrupt collapse. We can generate arbitrarily extreme climate scenarios through arbitrary time scales which we then predict using neural networks. Our analysis shows that the AMOC is predictable using neural networks under a diverse set of climate scenarios. Further experiments reveal that MLPs and Deep Ensembles can learn the physics of the AMOC instead of imitating its progression through autocorrelation. With quantified uncertainty, an intriguing pattern of \"spikes\" before critical points of collapse in the AMOC casts doubt on previous analyses that predicted an AMOC collapse within this century. Our results show that Bayesian Neural Networks perform poorly compared to more dense architectures and care should be taken when applying neural networks to nonstationary scenarios such as climate projections. Further, our results highlight that big NN models might have difficulty in modeling global Earth System dynamics accurately and be successfully applied in nonstationary climate scenarios due to the physics being challenging for neural networks to capture.","sentences":["Machine Learning has become a pervasive tool in climate science applications.","However, current models fail to address nonstationarity induced by anthropogenic alterations in greenhouse emissions and do not routinely quantify the uncertainty of proposed projections.","In this paper, we model the Atlantic Meridional Overturning Circulation (AMOC) which is of major importance to climate in Europe and the US East Coast by transporting warm water to these regions, and has the potential for abrupt collapse.","We can generate arbitrarily extreme climate scenarios through arbitrary time scales which we then predict using neural networks.","Our analysis shows that the AMOC is predictable using neural networks under a diverse set of climate scenarios.","Further experiments reveal that MLPs and Deep Ensembles can learn the physics of the AMOC instead of imitating its progression through autocorrelation.","With quantified uncertainty, an intriguing pattern of \"spikes\" before critical points of collapse in the AMOC casts doubt on previous analyses that predicted an AMOC collapse within this century.","Our results show that Bayesian Neural Networks perform poorly compared to more dense architectures and care should be taken when applying neural networks to nonstationary scenarios such as climate projections.","Further, our results highlight that big NN models might have difficulty in modeling global Earth System dynamics accurately and be successfully applied in nonstationary climate scenarios due to the physics being challenging for neural networks to capture."],"url":"http://arxiv.org/abs/2402.13979v1","category":"cs.LG"}
{"created":"2024-02-21 17:15:47","title":"Probabilistic Neural Networks (PNNs) for Modeling Aleatoric Uncertainty in Scientific Machine Learning","abstract":"This paper investigates the use of probabilistic neural networks (PNNs) to model aleatoric uncertainty, which refers to the inherent variability in the input-output relationships of a system, often characterized by unequal variance or heteroscedasticity. Unlike traditional neural networks that produce deterministic outputs, PNNs generate probability distributions for the target variable, allowing the determination of both predicted means and intervals in regression scenarios. Contributions of this paper include the development of a probabilistic distance metric to optimize PNN architecture, and the deployment of PNNs in controlled data sets as well as a practical material science case involving fiber-reinforced composites. The findings confirm that PNNs effectively model aleatoric uncertainty, proving to be more appropriate than the commonly employed Gaussian process regression for this purpose. Specifically, in a real-world scientific machine learning context, PNNs yield remarkably accurate output mean estimates with R-squared scores approaching 0.97, and their predicted intervals exhibit a high correlation coefficient of nearly 0.80, closely matching observed data intervals. Hence, this research contributes to the ongoing exploration of leveraging the sophisticated representational capacity of neural networks to delineate complex input-output relationships in scientific problems.","sentences":["This paper investigates the use of probabilistic neural networks (PNNs) to model aleatoric uncertainty, which refers to the inherent variability in the input-output relationships of a system, often characterized by unequal variance or heteroscedasticity.","Unlike traditional neural networks that produce deterministic outputs, PNNs generate probability distributions for the target variable, allowing the determination of both predicted means and intervals in regression scenarios.","Contributions of this paper include the development of a probabilistic distance metric to optimize PNN architecture, and the deployment of PNNs in controlled data sets as well as a practical material science case involving fiber-reinforced composites.","The findings confirm that PNNs effectively model aleatoric uncertainty, proving to be more appropriate than the commonly employed Gaussian process regression for this purpose.","Specifically, in a real-world scientific machine learning context, PNNs yield remarkably accurate output mean estimates with R-squared scores approaching 0.97, and their predicted intervals exhibit a high correlation coefficient of nearly 0.80, closely matching observed data intervals.","Hence, this research contributes to the ongoing exploration of leveraging the sophisticated representational capacity of neural networks to delineate complex input-output relationships in scientific problems."],"url":"http://arxiv.org/abs/2402.13945v1","category":"stat.ML"}
{"created":"2024-02-21 17:07:09","title":"What is the focus of XAI in UI design? Prioritizing UI design principles for enhancing XAI user experience","abstract":"With the widespread application of artificial intelligence(AI), the explainable AI (XAI) field has undergone a notable resurgence. In this background, the importance of user experience in XAI has become increasingly prominent. Simultaneously, the user interface (UI) serves as a crucial link between XAI and users. However, despite the existence of UI design principles for XAI, there is a lack of prioritization based on their significance. This will lead practitioners to have a vague understanding of different design principles, making it difficult to allocate design space reasonably and emphasize design focal points. This paper aims to prioritize four design principles, providing clear guidance for UI design in XAI. Initially, we conducted a lightweight summary to derive five user experience standards for non-expert users in XAI. Subsequently, we developed four corresponding webpage prototypes for the four design principles. Nineteen participants then interacted with these prototypes, providing ratings based on five user experience standards, and We calculated the weights of the design principles. Our findings indicate that, for non-expert users, \"sensitivity\" is the optimal UI design principle (weight = 0.3296), followed by \"flexibility\" (weight = 0.3014). Finally, we engage in further discussion and summarization of our research results, and present future works and limitations.","sentences":["With the widespread application of artificial intelligence(AI), the explainable AI (XAI) field has undergone a notable resurgence.","In this background, the importance of user experience in XAI has become increasingly prominent.","Simultaneously, the user interface (UI) serves as a crucial link between XAI and users.","However, despite the existence of UI design principles for XAI, there is a lack of prioritization based on their significance.","This will lead practitioners to have a vague understanding of different design principles, making it difficult to allocate design space reasonably and emphasize design focal points.","This paper aims to prioritize four design principles, providing clear guidance for UI design in XAI.","Initially, we conducted a lightweight summary to derive five user experience standards for non-expert users in XAI.","Subsequently, we developed four corresponding webpage prototypes for the four design principles.","Nineteen participants then interacted with these prototypes, providing ratings based on five user experience standards, and We calculated the weights of the design principles.","Our findings indicate that, for non-expert users, \"sensitivity\" is the optimal UI design principle (weight = 0.3296), followed by \"flexibility\" (weight = 0.3014).","Finally, we engage in further discussion and summarization of our research results, and present future works and limitations."],"url":"http://arxiv.org/abs/2402.13939v1","category":"cs.HC"}
{"created":"2024-02-21 17:00:56","title":"Do Efficient Transformers Really Save Computation?","abstract":"As transformer-based language models are trained on increasingly large datasets and with vast numbers of parameters, finding more efficient alternatives to the standard Transformer has become very valuable. While many efficient Transformers and Transformer alternatives have been proposed, none provide theoretical guarantees that they are a suitable replacement for the standard Transformer. This makes it challenging to identify when to use a specific model and what directions to prioritize for further investigation. In this paper, we aim to understand the capabilities and limitations of efficient Transformers, specifically the Sparse Transformer and the Linear Transformer. We focus on their reasoning capability as exhibited by Chain-of-Thought (CoT) prompts and follow previous works to model them as Dynamic Programming (DP) problems. Our results show that while these models are expressive enough to solve general DP tasks, contrary to expectations, they require a model size that scales with the problem size. Nonetheless, we identify a class of DP problems for which these models can be more efficient than the standard Transformer. We confirm our theoretical results through experiments on representative DP tasks, adding to the understanding of efficient Transformers' practical strengths and weaknesses.","sentences":["As transformer-based language models are trained on increasingly large datasets and with vast numbers of parameters, finding more efficient alternatives to the standard Transformer has become very valuable.","While many efficient Transformers and Transformer alternatives have been proposed, none provide theoretical guarantees that they are a suitable replacement for the standard Transformer.","This makes it challenging to identify when to use a specific model and what directions to prioritize for further investigation.","In this paper, we aim to understand the capabilities and limitations of efficient Transformers, specifically the Sparse Transformer and the Linear Transformer.","We focus on their reasoning capability as exhibited by Chain-of-Thought (CoT) prompts and follow previous works to model them as Dynamic Programming (DP) problems.","Our results show that while these models are expressive enough to solve general DP tasks, contrary to expectations, they require a model size that scales with the problem size.","Nonetheless, we identify a class of DP problems for which these models can be more efficient than the standard Transformer.","We confirm our theoretical results through experiments on representative DP tasks, adding to the understanding of efficient Transformers' practical strengths and weaknesses."],"url":"http://arxiv.org/abs/2402.13934v1","category":"cs.LG"}
{"created":"2024-02-21 16:51:05","title":"SDXL-Lightning: Progressive Adversarial Diffusion Distillation","abstract":"We propose a diffusion distillation method that achieves new state-of-the-art in one-step/few-step 1024px text-to-image generation based on SDXL. Our method combines progressive and adversarial distillation to achieve a balance between quality and mode coverage. In this paper, we discuss the theoretical analysis, discriminator design, model formulation, and training techniques. We open-source our distilled SDXL-Lightning models both as LoRA and full UNet weights.","sentences":["We propose a diffusion distillation method that achieves new state-of-the-art in one-step/few-step 1024px text-to-image generation based on SDXL.","Our method combines progressive and adversarial distillation to achieve a balance between quality and mode coverage.","In this paper, we discuss the theoretical analysis, discriminator design, model formulation, and training techniques.","We open-source our distilled SDXL-Lightning models both as LoRA and full UNet weights."],"url":"http://arxiv.org/abs/2402.13929v1","category":"cs.CV"}
{"created":"2024-02-21 16:48:07","title":"The Delusional Hedge Algorithm as a Model of Human Learning from Diverse Opinions","abstract":"Whereas cognitive models of learning often assume direct experience with both the features of an event and with a true label or outcome, much of everyday learning arises from hearing the opinions of others, without direct access to either the experience or the ground truth outcome. We consider how people can learn which opinions to trust in such scenarios by extending the hedge algorithm: a classic solution for learning from diverse information sources. We first introduce a semi-supervised variant we call the delusional hedge capable of learning from both supervised and unsupervised experiences. In two experiments, we examine the alignment between human judgments and predictions from the standard hedge, the delusional hedge, and a heuristic baseline model. Results indicate that humans effectively incorporate both labeled and unlabeled information in a manner consistent with the delusional hedge algorithm -- suggesting that human learners not only gauge the accuracy of information sources but also their consistency with other reliable sources. The findings advance our understanding of human learning from diverse opinions, with implications for the development of algorithms that better capture how people learn to weigh conflicting information sources.","sentences":["Whereas cognitive models of learning often assume direct experience with both the features of an event and with a true label or outcome, much of everyday learning arises from hearing the opinions of others, without direct access to either the experience or the ground truth outcome.","We consider how people can learn which opinions to trust in such scenarios by extending the hedge algorithm: a classic solution for learning from diverse information sources.","We first introduce a semi-supervised variant we call the delusional hedge capable of learning from both supervised and unsupervised experiences.","In two experiments, we examine the alignment between human judgments and predictions from the standard hedge, the delusional hedge, and a heuristic baseline model.","Results indicate that humans effectively incorporate both labeled and unlabeled information in a manner consistent with the delusional hedge algorithm -- suggesting that human learners not only gauge the accuracy of information sources but also their consistency with other reliable sources.","The findings advance our understanding of human learning from diverse opinions, with implications for the development of algorithms that better capture how people learn to weigh conflicting information sources."],"url":"http://arxiv.org/abs/2402.13927v1","category":"cs.AI"}
{"created":"2024-02-21 16:46:36","title":"Large Language Models are Vulnerable to Bait-and-Switch Attacks for Generating Harmful Content","abstract":"The risks derived from large language models (LLMs) generating deceptive and damaging content have been the subject of considerable research, but even safe generations can lead to problematic downstream impacts. In our study, we shift the focus to how even safe text coming from LLMs can be easily turned into potentially dangerous content through Bait-and-Switch attacks. In such attacks, the user first prompts LLMs with safe questions and then employs a simple find-and-replace post-hoc technique to manipulate the outputs into harmful narratives. The alarming efficacy of this approach in generating toxic content highlights a significant challenge in developing reliable safety guardrails for LLMs. In particular, we stress that focusing on the safety of the verbatim LLM outputs is insufficient and that we also need to consider post-hoc transformations.","sentences":["The risks derived from large language models (LLMs) generating deceptive and damaging content have been the subject of considerable research, but even safe generations can lead to problematic downstream impacts.","In our study, we shift the focus to how even safe text coming from LLMs can be easily turned into potentially dangerous content through Bait-and-Switch attacks.","In such attacks, the user first prompts LLMs with safe questions and then employs a simple find-and-replace post-hoc technique to manipulate the outputs into harmful narratives.","The alarming efficacy of this approach in generating toxic content highlights a significant challenge in developing reliable safety guardrails for LLMs.","In particular, we stress that focusing on the safety of the verbatim LLM outputs is insufficient and that we also need to consider post-hoc transformations."],"url":"http://arxiv.org/abs/2402.13926v1","category":"cs.CL"}
{"created":"2024-02-21 16:33:22","title":"SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization","abstract":"Large Language Models (LLMs) such as GPT and Llama have demonstrated significant achievements in summarization tasks but struggle with factual inaccuracies, a critical issue in clinical NLP applications where errors could lead to serious consequences. To counter the high costs and limited availability of expert-annotated data for factual alignment, this study introduces an innovative pipeline that utilizes GPT-3.5 and GPT-4 to generate high-quality feedback aimed at enhancing factual consistency in clinical note summarization. Our research primarily focuses on edit feedback, mirroring the practical scenario in which medical professionals refine AI system outputs without the need for additional annotations. Despite GPT's proven expertise in various clinical NLP tasks, such as the Medical Licensing Examination, there is scant research on its capacity to deliver expert-level edit feedback for improving weaker LMs or LLMs generation quality. This work leverages GPT's advanced capabilities in clinical NLP to offer expert-level edit feedback. Through the use of two distinct alignment algorithms (DPO and SALT) based on GPT edit feedback, our goal is to reduce hallucinations and align closely with medical facts, endeavoring to narrow the divide between AI-generated content and factual accuracy. This highlights the substantial potential of GPT edits in enhancing the alignment of clinical factuality.","sentences":["Large Language Models (LLMs) such as GPT and Llama have demonstrated significant achievements in summarization tasks but struggle with factual inaccuracies, a critical issue in clinical NLP applications where errors could lead to serious consequences.","To counter the high costs and limited availability of expert-annotated data for factual alignment, this study introduces an innovative pipeline that utilizes GPT-3.5 and GPT-4 to generate high-quality feedback aimed at enhancing factual consistency in clinical note summarization.","Our research primarily focuses on edit feedback, mirroring the practical scenario in which medical professionals refine AI system outputs without the need for additional annotations.","Despite GPT's proven expertise in various clinical NLP tasks, such as the Medical Licensing Examination, there is scant research on its capacity to deliver expert-level edit feedback for improving weaker LMs or LLMs generation quality.","This work leverages GPT's advanced capabilities in clinical NLP to offer expert-level edit feedback.","Through the use of two distinct alignment algorithms (DPO and SALT) based on GPT edit feedback, our goal is to reduce hallucinations and align closely with medical facts, endeavoring to narrow the divide between AI-generated content and factual accuracy.","This highlights the substantial potential of GPT edits in enhancing the alignment of clinical factuality."],"url":"http://arxiv.org/abs/2402.13919v1","category":"cs.CL"}
{"created":"2024-02-21 16:32:38","title":"What Linguistic Features and Languages are Important in LLM Translation?","abstract":"Large Language Models (LLMs) demonstrate strong capability across multiple tasks, including machine translation. Our study focuses on evaluating Llama2's machine translation capabilities and exploring how translation depends on languages in its training data. Our experiments show that the 7B Llama2 model yields above 10 BLEU score for all languages it has seen, but not always for languages it has not seen. Most gains for those unseen languages are observed the most with the model scale compared to using chat versions or adding shot count. Furthermore, our linguistic distance analysis reveals that syntactic similarity is not always the primary linguistic factor in determining translation quality. Interestingly, we discovered that under specific circumstances, some languages, despite having significantly less training data than English, exhibit strong correlations comparable to English. Our discoveries here give new perspectives for the current landscape of LLMs, raising the possibility that LLMs centered around languages other than English may offer a more effective foundation for a multilingual model.","sentences":["Large Language Models (LLMs) demonstrate strong capability across multiple tasks, including machine translation.","Our study focuses on evaluating Llama2's machine translation capabilities and exploring how translation depends on languages in its training data.","Our experiments show that the 7B Llama2 model yields above 10 BLEU score for all languages it has seen, but not always for languages it has not seen.","Most gains for those unseen languages are observed the most with the model scale compared to using chat versions or adding shot count.","Furthermore, our linguistic distance analysis reveals that syntactic similarity is not always the primary linguistic factor in determining translation quality.","Interestingly, we discovered that under specific circumstances, some languages, despite having significantly less training data than English, exhibit strong correlations comparable to English.","Our discoveries here give new perspectives for the current landscape of LLMs, raising the possibility that LLMs centered around languages other than English may offer a more effective foundation for a multilingual model."],"url":"http://arxiv.org/abs/2402.13917v1","category":"cs.CL"}
{"created":"2024-02-21 16:31:07","title":"A Combined Learning and Optimization Framework to Transfer Human Whole-body Loco-manipulation Skills to Mobile Manipulators","abstract":"Humans' ability to smoothly switch between locomotion and manipulation is a remarkable feature of sensorimotor coordination. Leaning and replication of such human-like strategies can lead to the development of more sophisticated robots capable of performing complex whole-body tasks in real-world environments. To this end, this paper proposes a combined learning and optimization framework for transferring human's loco-manipulation soft-switching skills to mobile manipulators. The methodology departs from data collection of human demonstrations for a locomotion-integrated manipulation task through a vision system. Next, the wrist and pelvis motions are mapped to mobile manipulators' End-Effector (EE) and mobile base. A kernelized movement primitive algorithm learns the wrist and pelvis trajectories and generalizes to new desired points according to task requirements. Next, the reference trajectories are sent to a hierarchical quadratic programming controller, where the EE and the mobile base reference trajectories are provided as the first and second priority tasks, generating the feasible and optimal joint level commands. A locomotion-integrated pick-and-place task is executed to validate the proposed approach. After a human demonstrates the task, a mobile manipulator executes the task with the same and new settings, grasping a bottle at non-zero velocity. The results showed that the proposed approach successfully transfers the human loco-manipulation skills to mobile manipulators, even with different geometry.","sentences":["Humans' ability to smoothly switch between locomotion and manipulation is a remarkable feature of sensorimotor coordination.","Leaning and replication of such human-like strategies can lead to the development of more sophisticated robots capable of performing complex whole-body tasks in real-world environments.","To this end, this paper proposes a combined learning and optimization framework for transferring human's loco-manipulation soft-switching skills to mobile manipulators.","The methodology departs from data collection of human demonstrations for a locomotion-integrated manipulation task through a vision system.","Next, the wrist and pelvis motions are mapped to mobile manipulators' End-Effector (EE) and mobile base.","A kernelized movement primitive algorithm learns the wrist and pelvis trajectories and generalizes to new desired points according to task requirements.","Next, the reference trajectories are sent to a hierarchical quadratic programming controller, where the EE and the mobile base reference trajectories are provided as the first and second priority tasks, generating the feasible and optimal joint level commands.","A locomotion-integrated pick-and-place task is executed to validate the proposed approach.","After a human demonstrates the task, a mobile manipulator executes the task with the same and new settings, grasping a bottle at non-zero velocity.","The results showed that the proposed approach successfully transfers the human loco-manipulation skills to mobile manipulators, even with different geometry."],"url":"http://arxiv.org/abs/2402.13915v1","category":"cs.RO"}
{"created":"2024-02-21 16:30:24","title":"Explain to Question not to Justify","abstract":"Explainable Artificial Intelligence (XAI) is a young but very promising field of research. Unfortunately, the progress in this field is currently slowed down by divergent and incompatible goals. In this paper, we separate various threads tangled within the area of XAI into two complementary cultures of human/value-oriented explanations (BLUE XAI) and model/validation-oriented explanations (RED XAI). We also argue that the area of RED XAI is currently under-explored and hides great opportunities and potential for important research necessary to ensure the safety of AI systems. We conclude this paper by presenting promising challenges in this area.","sentences":["Explainable Artificial Intelligence (XAI) is a young but very promising field of research.","Unfortunately, the progress in this field is currently slowed down by divergent and incompatible goals.","In this paper, we separate various threads tangled within the area of XAI into two complementary cultures of human/value-oriented explanations (BLUE XAI) and model/validation-oriented explanations (RED XAI).","We also argue that the area of RED XAI is currently under-explored and hides great opportunities and potential for important research necessary to ensure the safety of AI systems.","We conclude this paper by presenting promising challenges in this area."],"url":"http://arxiv.org/abs/2402.13914v1","category":"cs.AI"}
{"created":"2024-02-21 16:22:21","title":"Leveraging Collection-Wide Similarities for Unsupervised Document Structure Extraction","abstract":"Document collections of various domains, e.g., legal, medical, or financial, often share some underlying collection-wide structure, which captures information that can aid both human users and structure-aware models. We propose to identify the typical structure of document within a collection, which requires to capture recurring topics across the collection, while abstracting over arbitrary header paraphrases, and ground each topic to respective document locations. These requirements pose several challenges: headers that mark recurring topics frequently differ in phrasing, certain section headers are unique to individual documents and do not reflect the typical structure, and the order of topics can vary between documents. Subsequently, we develop an unsupervised graph-based method which leverages both inter- and intra-document similarities, to extract the underlying collection-wide structure. Our evaluations on three diverse domains in both English and Hebrew indicate that our method extracts meaningful collection-wide structure, and we hope that future work will leverage our method for multi-document applications and structure-aware models.","sentences":["Document collections of various domains, e.g., legal, medical, or financial, often share some underlying collection-wide structure, which captures information that can aid both human users and structure-aware models.","We propose to identify the typical structure of document within a collection, which requires to capture recurring topics across the collection, while abstracting over arbitrary header paraphrases, and ground each topic to respective document locations.","These requirements pose several challenges: headers that mark recurring topics frequently differ in phrasing, certain section headers are unique to individual documents and do not reflect the typical structure, and the order of topics can vary between documents.","Subsequently, we develop an unsupervised graph-based method which leverages both inter- and intra-document similarities, to extract the underlying collection-wide structure.","Our evaluations on three diverse domains in both English and Hebrew indicate that our method extracts meaningful collection-wide structure, and we hope that future work will leverage our method for multi-document applications and structure-aware models."],"url":"http://arxiv.org/abs/2402.13906v1","category":"cs.CL"}
{"created":"2024-02-21 16:09:25","title":"Science Checker Reloaded: A Bidirectional Paradigm for Transparency and Logical Reasoning","abstract":"Information retrieval is a rapidly evolving field. However it still faces significant limitations in the scientific and industrial vast amounts of information, such as semantic divergence and vocabulary gaps in sparse retrieval, low precision and lack of interpretability in semantic search, or hallucination and outdated information in generative models. In this paper, we introduce a two-block approach to tackle these hurdles for long documents. The first block enhances language understanding in sparse retrieval by query expansion to retrieve relevant documents. The second block deepens the result by providing comprehensive and informative answers to the complex question using only the information spread in the long document, enabling bidirectional engagement. At various stages of the pipeline, intermediate results are presented to users to facilitate understanding of the system's reasoning. We believe this bidirectional approach brings significant advancements in terms of transparency, logical thinking, and comprehensive understanding in the field of scientific information retrieval.","sentences":["Information retrieval is a rapidly evolving field.","However it still faces significant limitations in the scientific and industrial vast amounts of information, such as semantic divergence and vocabulary gaps in sparse retrieval, low precision and lack of interpretability in semantic search, or hallucination and outdated information in generative models.","In this paper, we introduce a two-block approach to tackle these hurdles for long documents.","The first block enhances language understanding in sparse retrieval by query expansion to retrieve relevant documents.","The second block deepens the result by providing comprehensive and informative answers to the complex question using only the information spread in the long document, enabling bidirectional engagement.","At various stages of the pipeline, intermediate results are presented to users to facilitate understanding of the system's reasoning.","We believe this bidirectional approach brings significant advancements in terms of transparency, logical thinking, and comprehensive understanding in the field of scientific information retrieval."],"url":"http://arxiv.org/abs/2402.13897v1","category":"cs.IR"}
{"created":"2024-02-21 15:23:21","title":"An Explainable Transformer-based Model for Phishing Email Detection: A Large Language Model Approach","abstract":"Phishing email is a serious cyber threat that tries to deceive users by sending false emails with the intention of stealing confidential information or causing financial harm. Attackers, often posing as trustworthy entities, exploit technological advancements and sophistication to make detection and prevention of phishing more challenging. Despite extensive academic research, phishing detection remains an ongoing and formidable challenge in the cybersecurity landscape. Large Language Models (LLMs) and Masked Language Models (MLMs) possess immense potential to offer innovative solutions to address long-standing challenges. In this research paper, we present an optimized, fine-tuned transformer-based DistilBERT model designed for the detection of phishing emails. In the detection process, we work with a phishing email dataset and utilize the preprocessing techniques to clean and solve the imbalance class issues. Through our experiments, we found that our model effectively achieves high accuracy, demonstrating its capability to perform well. Finally, we demonstrate our fine-tuned model using Explainable-AI (XAI) techniques such as Local Interpretable Model-Agnostic Explanations (LIME) and Transformer Interpret to explain how our model makes predictions in the context of text classification for phishing emails.","sentences":["Phishing email is a serious cyber threat that tries to deceive users by sending false emails with the intention of stealing confidential information or causing financial harm.","Attackers, often posing as trustworthy entities, exploit technological advancements and sophistication to make detection and prevention of phishing more challenging.","Despite extensive academic research, phishing detection remains an ongoing and formidable challenge in the cybersecurity landscape.","Large Language Models (LLMs) and Masked Language Models (MLMs) possess immense potential to offer innovative solutions to address long-standing challenges.","In this research paper, we present an optimized, fine-tuned transformer-based DistilBERT model designed for the detection of phishing emails.","In the detection process, we work with a phishing email dataset and utilize the preprocessing techniques to clean and solve the imbalance class issues.","Through our experiments, we found that our model effectively achieves high accuracy, demonstrating its capability to perform well.","Finally, we demonstrate our fine-tuned model using Explainable-AI (XAI) techniques such as Local Interpretable Model-Agnostic Explanations (LIME) and Transformer Interpret to explain how our model makes predictions in the context of text classification for phishing emails."],"url":"http://arxiv.org/abs/2402.13871v1","category":"cs.LG"}
{"created":"2024-02-21 15:14:20","title":"Kuaiji: the First Chinese Accounting Large Language Model","abstract":"Large Language Models (LLMs) like ChatGPT and GPT-4 have demonstrated impressive proficiency in comprehending and generating natural language. However, they encounter difficulties when tasked with adapting to specialized domains such as accounting. To address this challenge, we introduce Kuaiji, a tailored Accounting Large Language Model. Kuaiji is meticulously fine-tuned using the Baichuan framework, which encompasses continuous pre-training and supervised fine-tuning processes. Supported by CAtAcctQA, a dataset containing large genuine accountant-client dialogues, Kuaiji exhibits exceptional accuracy and response speed. Our contributions encompass the creation of the first Chinese accounting dataset, the establishment of Kuaiji as a leading open-source Chinese accounting LLM, and the validation of its efficacy through real-world accounting scenarios.","sentences":["Large Language Models (LLMs) like ChatGPT and GPT-4 have demonstrated impressive proficiency in comprehending and generating natural language.","However, they encounter difficulties when tasked with adapting to specialized domains such as accounting.","To address this challenge, we introduce Kuaiji, a tailored Accounting Large Language Model.","Kuaiji is meticulously fine-tuned using the Baichuan framework, which encompasses continuous pre-training and supervised fine-tuning processes.","Supported by CAtAcctQA, a dataset containing large genuine accountant-client dialogues, Kuaiji exhibits exceptional accuracy and response speed.","Our contributions encompass the creation of the first Chinese accounting dataset, the establishment of Kuaiji as a leading open-source Chinese accounting LLM, and the validation of its efficacy through real-world accounting scenarios."],"url":"http://arxiv.org/abs/2402.13866v1","category":"cs.CL"}
{"created":"2024-02-21 15:13:00","title":"Measurement of energy correlators inside jets and determination of the strong coupling $\u03b1_\\mathrm{S}(m_\\mathrm{Z})$","abstract":"Energy correlators that describe energy-weighted distances between two or three particles in a jet are measured using an event sample of $\\sqrt{s}$ = 13 TeV proton-proton collisions collected by the CMS experiment and corresponding to an integrated luminosity of 36.3 fb$^{-1}$. The measured distributions reveal two key features of the strong interaction: confinement and asymptotic freedom. By comparing the ratio of the two measured distributions with theoretical calculations that resum collinear emissions at approximate next-to-next-to-leading logarithmic accuracy matched to a next-to-leading order calculation, the strong coupling is determined at the Z boson mass: $\\alpha_\\mathrm{S}(m_\\mathrm{Z})$ = 0.1229$^{+0.0040}_{-0.0050}$, the most precise $\\alpha_\\mathrm{S}(m_\\mathrm{Z})$ value obtained using jet substructure observables.","sentences":["Energy correlators that describe energy-weighted distances between two or three particles in a jet are measured using an event sample of $\\sqrt{s}$ = 13 TeV proton-proton collisions collected by the CMS experiment and corresponding to an integrated luminosity of 36.3 fb$^{-1}$. The measured distributions reveal two key features of the strong interaction: confinement and asymptotic freedom.","By comparing the ratio of the two measured distributions with theoretical calculations that resum collinear emissions at approximate next-to-next-to-leading logarithmic accuracy matched to a next-to-leading order calculation, the strong coupling is determined at the Z boson mass: $\\alpha_\\mathrm{S}(m_\\mathrm{Z})$ = 0.1229$^{+0.0040}_{-0.0050}$, the most precise $\\alpha_\\mathrm{S}(m_\\mathrm{Z})$ value obtained using jet substructure observables."],"url":"http://arxiv.org/abs/2402.13864v1","category":"hep-ex"}
{"created":"2024-02-21 14:59:49","title":"What we can learn from TikTok through its Research API","abstract":"TikTok is a social media platform that has gained immense popularity over the last few years, particularly among younger demographics, due to the viral trends and challenges shared worldwide. The recent release of a free Research API opens doors to collect data on posted videos, associated comments, and user activities. Our study focuses on evaluating the reliability of results returned by the Research API, by collecting and analyzing a random sample of TikTok videos posted in a span of 6 years. Our preliminary results are instrumental for future research that aims to study the platform, highlighting caveats on the geographical distribution of videos and on the global prevalence of viral hashtags.","sentences":["TikTok is a social media platform that has gained immense popularity over the last few years, particularly among younger demographics, due to the viral trends and challenges shared worldwide.","The recent release of a free Research API opens doors to collect data on posted videos, associated comments, and user activities.","Our study focuses on evaluating the reliability of results returned by the Research API, by collecting and analyzing a random sample of TikTok videos posted in a span of 6 years.","Our preliminary results are instrumental for future research that aims to study the platform, highlighting caveats on the geographical distribution of videos and on the global prevalence of viral hashtags."],"url":"http://arxiv.org/abs/2402.13855v1","category":"cs.CY"}
{"created":"2024-02-21 14:59:46","title":"RealDex: Towards Human-like Grasping for Robotic Dexterous Hand","abstract":"In this paper, we introduce RealDex, a pioneering dataset capturing authentic dexterous hand grasping motions infused with human behavioral patterns, enriched by multi-view and multimodal visual data. Utilizing a teleoperation system, we seamlessly synchronize human-robot hand poses in real time. This collection of human-like motions is crucial for training dexterous hands to mimic human movements more naturally and precisely. RealDex holds immense promise in advancing humanoid robot for automated perception, cognition, and manipulation in real-world scenarios. Moreover, we introduce a cutting-edge dexterous grasping motion generation framework, which aligns with human experience and enhances real-world applicability through effectively utilizing Multimodal Large Language Models. Extensive experiments have demonstrated the superior performance of our method on RealDex and other open datasets. The complete dataset and code will be made available upon the publication of this work.","sentences":["In this paper, we introduce RealDex, a pioneering dataset capturing authentic dexterous hand grasping motions infused with human behavioral patterns, enriched by multi-view and multimodal visual data.","Utilizing a teleoperation system, we seamlessly synchronize human-robot hand poses in real time.","This collection of human-like motions is crucial for training dexterous hands to mimic human movements more naturally and precisely.","RealDex holds immense promise in advancing humanoid robot for automated perception, cognition, and manipulation in real-world scenarios.","Moreover, we introduce a cutting-edge dexterous grasping motion generation framework, which aligns with human experience and enhances real-world applicability through effectively utilizing Multimodal Large Language Models.","Extensive experiments have demonstrated the superior performance of our method on RealDex and other open datasets.","The complete dataset and code will be made available upon the publication of this work."],"url":"http://arxiv.org/abs/2402.13853v1","category":"cs.RO"}
{"created":"2024-02-21 14:56:36","title":"Neural Control System for Continuous Glucose Monitoring and Maintenance","abstract":"Precise glucose level management is pivotal for individuals with diabetes, averting severe complications. In this work, we introduce a novel neural control system for continuous glucose monitoring and maintenance, utilizing differential predictive control. Our system, guided by a sophisticated neural policy and differentiable modeling, dynamically adjusts insulin delivery in real-time, enhancing glucose optimization. This end-to-end approach maximizes efficiency, ensuring personalized care and improved health outcomes, as affirmed by empirical findings.","sentences":["Precise glucose level management is pivotal for individuals with diabetes, averting severe complications.","In this work, we introduce a novel neural control system for continuous glucose monitoring and maintenance, utilizing differential predictive control.","Our system, guided by a sophisticated neural policy and differentiable modeling, dynamically adjusts insulin delivery in real-time, enhancing glucose optimization.","This end-to-end approach maximizes efficiency, ensuring personalized care and improved health outcomes, as affirmed by empirical findings."],"url":"http://arxiv.org/abs/2402.13852v1","category":"cs.LG"}
{"created":"2024-02-21 14:44:00","title":"Large Language Models are Advanced Anonymizers","abstract":"Recent work in privacy research on large language models has shown that they achieve near human-level performance at inferring personal data from real-world online texts. With consistently increasing model capabilities, existing text anonymization methods are currently lacking behind regulatory requirements and adversarial threats. This raises the question of how individuals can effectively protect their personal data in sharing online texts. In this work, we take two steps to answer this question: We first present a new setting for evaluating anonymizations in the face of adversarial LLMs inferences, allowing for a natural measurement of anonymization performance while remedying some of the shortcomings of previous metrics. We then present our LLM-based adversarial anonymization framework leveraging the strong inferential capabilities of LLMs to inform our anonymization procedure. In our experimental evaluation, we show on real-world and synthetic online texts how adversarial anonymization outperforms current industry-grade anonymizers both in terms of the resulting utility and privacy.","sentences":["Recent work in privacy research on large language models has shown that they achieve near human-level performance at inferring personal data from real-world online texts.","With consistently increasing model capabilities, existing text anonymization methods are currently lacking behind regulatory requirements and adversarial threats.","This raises the question of how individuals can effectively protect their personal data in sharing online texts.","In this work, we take two steps to answer this question: We first present a new setting for evaluating anonymizations in the face of adversarial LLMs inferences, allowing for a natural measurement of anonymization performance while remedying some of the shortcomings of previous metrics.","We then present our LLM-based adversarial anonymization framework leveraging the strong inferential capabilities of LLMs to inform our anonymization procedure.","In our experimental evaluation, we show on real-world and synthetic online texts how adversarial anonymization outperforms current industry-grade anonymizers both in terms of the resulting utility and privacy."],"url":"http://arxiv.org/abs/2402.13846v1","category":"cs.CR"}
{"created":"2024-02-21 14:38:02","title":"LLM4SBR: A Lightweight and Effective Framework for Integrating Large Language Models in Session-based Recommendation","abstract":"Traditional session-based recommendation (SBR) utilizes session behavior sequences from anonymous users for recommendation. Although this strategy is highly efficient, it sacrifices the inherent semantic information of the items, making it difficult for the model to understand the true intent of the session and resulting in a lack of interpretability in the recommended results. Recently, large language models (LLMs) have flourished across various domains, offering a glimpse of hope in addressing the aforementioned challenges. Inspired by the impact of LLMs, research exploring the integration of LLMs with the Recommender system (RS) has surged like mushrooms after rain. However, constrained by high time and space costs, as well as the brief and anonymous nature of session data, the first LLM recommendation framework suitable for industrial deployment has yet to emerge in the field of SBR. To address the aforementioned challenges, we have proposed the LLM Integration Framework for SBR (LLM4SBR). Serving as a lightweight and plug-and-play framework, LLM4SBR adopts a two-step strategy. Firstly, we transform session data into a bimodal form of text and behavior. In the first step, leveraging the inferential capabilities of LLMs, we conduct inference on session text data from different perspectives and design the component for auxiliary enhancement. In the second step, the SBR model is trained on behavior data, aligning and averaging two modal session representations from different perspectives. Finally, we fuse session representations from different perspectives and modalities as the ultimate session representation for recommendation. We conducted experiments on two real-world datasets, and the results demonstrate that LLM4SBR significantly improves the performance of traditional SBR models and is highly lightweight and efficient, making it suitable for industrial deployment.","sentences":["Traditional session-based recommendation (SBR) utilizes session behavior sequences from anonymous users for recommendation.","Although this strategy is highly efficient, it sacrifices the inherent semantic information of the items, making it difficult for the model to understand the true intent of the session and resulting in a lack of interpretability in the recommended results.","Recently, large language models (LLMs) have flourished across various domains, offering a glimpse of hope in addressing the aforementioned challenges.","Inspired by the impact of LLMs, research exploring the integration of LLMs with the Recommender system (RS) has surged like mushrooms after rain.","However, constrained by high time and space costs, as well as the brief and anonymous nature of session data, the first LLM recommendation framework suitable for industrial deployment has yet to emerge in the field of SBR.","To address the aforementioned challenges, we have proposed the LLM Integration Framework for SBR (LLM4SBR).","Serving as a lightweight and plug-and-play framework, LLM4SBR adopts a two-step strategy.","Firstly, we transform session data into a bimodal form of text and behavior.","In the first step, leveraging the inferential capabilities of LLMs, we conduct inference on session text data from different perspectives and design the component for auxiliary enhancement.","In the second step, the SBR model is trained on behavior data, aligning and averaging two modal session representations from different perspectives.","Finally, we fuse session representations from different perspectives and modalities as the ultimate session representation for recommendation.","We conducted experiments on two real-world datasets, and the results demonstrate that LLM4SBR significantly improves the performance of traditional SBR models and is highly lightweight and efficient, making it suitable for industrial deployment."],"url":"http://arxiv.org/abs/2402.13840v1","category":"cs.IR"}
{"created":"2024-02-21 14:29:27","title":"Design of a Miniature Underwater Vehicle and Data Collection System for Indoor Experimentation","abstract":"This paper describes the design of a miniature uncrewed underwater vehicle (MiniUUV) and related instrumentation for indoor experimentation. The MiniUUV was developed using 3D printed components and low-cost, off-the-shelf electronics. The vehicle uses a propeller differential propulsion drive and a peristaltic pump with a syringe for buoyancy control. A water tank with an overhead camera system was constructed to allow for convenient indoor data collection in a controlled environment. Several tests were conducted to demonstrate the capabilities of the MiniUUV and data collection system, including buoyancy pump actuation tests and straight line, circular, and zig-zag motion tests on the surface. During each planar motion test an AprilTag was attached to the MiniUUV and an overhead camera system obtained video recordings that were processed offline to estimate vehicle position, surge velocity, sway velocity, yaw angle, and yaw rate.","sentences":["This paper describes the design of a miniature uncrewed underwater vehicle (MiniUUV) and related instrumentation for indoor experimentation.","The MiniUUV was developed using 3D printed components and low-cost, off-the-shelf electronics.","The vehicle uses a propeller differential propulsion drive and a peristaltic pump with a syringe for buoyancy control.","A water tank with an overhead camera system was constructed to allow for convenient indoor data collection in a controlled environment.","Several tests were conducted to demonstrate the capabilities of the MiniUUV and data collection system, including buoyancy pump actuation tests and straight line, circular, and zig-zag motion tests on the surface.","During each planar motion test an AprilTag was attached to the MiniUUV and an overhead camera system obtained video recordings that were processed offline to estimate vehicle position, surge velocity, sway velocity, yaw angle, and yaw rate."],"url":"http://arxiv.org/abs/2402.13837v1","category":"cs.RO"}
{"created":"2024-02-21 13:59:21","title":"FLD: Fourier Latent Dynamics for Structured Motion Representation and Learning","abstract":"Motion trajectories offer reliable references for physics-based motion learning but suffer from sparsity, particularly in regions that lack sufficient data coverage. To address this challenge, we introduce a self-supervised, structured representation and generation method that extracts spatial-temporal relationships in periodic or quasi-periodic motions. The motion dynamics in a continuously parameterized latent space enable our method to enhance the interpolation and generalization capabilities of motion learning algorithms. The motion learning controller, informed by the motion parameterization, operates online tracking of a wide range of motions, including targets unseen during training. With a fallback mechanism, the controller dynamically adapts its tracking strategy and automatically resorts to safe action execution when a potentially risky target is proposed. By leveraging the identified spatial-temporal structure, our work opens new possibilities for future advancements in general motion representation and learning algorithms.","sentences":["Motion trajectories offer reliable references for physics-based motion learning but suffer from sparsity, particularly in regions that lack sufficient data coverage.","To address this challenge, we introduce a self-supervised, structured representation and generation method that extracts spatial-temporal relationships in periodic or quasi-periodic motions.","The motion dynamics in a continuously parameterized latent space enable our method to enhance the interpolation and generalization capabilities of motion learning algorithms.","The motion learning controller, informed by the motion parameterization, operates online tracking of a wide range of motions, including targets unseen during training.","With a fallback mechanism, the controller dynamically adapts its tracking strategy and automatically resorts to safe action execution when a potentially risky target is proposed.","By leveraging the identified spatial-temporal structure, our work opens new possibilities for future advancements in general motion representation and learning algorithms."],"url":"http://arxiv.org/abs/2402.13820v1","category":"cs.LG"}
{"created":"2024-02-21 13:53:25","title":"An Empirical Study on Oculus Virtual Reality Applications: Security and Privacy Perspectives","abstract":"Although Virtual Reality (VR) has accelerated its prevalent adoption in emerging metaverse applications, it is not a fundamentally new technology. On one hand, most VR operating systems (OS) are based on off-the-shelf mobile OS. As a result, VR apps also inherit privacy and security deficiencies from conventional mobile apps. On the other hand, in contrast to conventional mobile apps, VR apps can achieve immersive experience via diverse VR devices, such as head-mounted displays, body sensors, and controllers though achieving this requires the extensive collection of privacy-sensitive human biometrics. Moreover, VR apps have been typically implemented by 3D gaming engines (e.g., Unity), which also contain intrinsic security vulnerabilities. Inappropriate use of these technologies may incur privacy leaks and security vulnerabilities although these issues have not received significant attention compared to the proliferation of diverse VR apps. In this paper, we develop a security and privacy assessment tool, namely the VR-SP detector for VR apps. The VR-SP detector has integrated program static analysis tools and privacy-policy analysis methods. Using the VR-SP detector, we conduct a comprehensive empirical study on 500 popular VR apps. We obtain the original apps from the popular Oculus and SideQuest app stores and extract APK files via the Meta Oculus Quest 2 device. We evaluate security vulnerabilities and privacy data leaks of these VR apps by VR app analysis, taint analysis, and privacy-policy analysis. We find that a number of security vulnerabilities and privacy leaks widely exist in VR apps. Moreover, our results also reveal conflicting representations in the privacy policies of these apps and inconsistencies of the actual data collection with the privacy-policy statements of the apps. Based on these findings, we make suggestions for the future development of VR apps.","sentences":["Although Virtual Reality (VR) has accelerated its prevalent adoption in emerging metaverse applications, it is not a fundamentally new technology.","On one hand, most VR operating systems (OS) are based on off-the-shelf mobile OS.","As a result, VR apps also inherit privacy and security deficiencies from conventional mobile apps.","On the other hand, in contrast to conventional mobile apps, VR apps can achieve immersive experience via diverse VR devices, such as head-mounted displays, body sensors, and controllers though achieving this requires the extensive collection of privacy-sensitive human biometrics.","Moreover, VR apps have been typically implemented by 3D gaming engines (e.g., Unity), which also contain intrinsic security vulnerabilities.","Inappropriate use of these technologies may incur privacy leaks and security vulnerabilities although these issues have not received significant attention compared to the proliferation of diverse VR apps.","In this paper, we develop a security and privacy assessment tool, namely the VR-SP detector for VR apps.","The VR-SP detector has integrated program static analysis tools and privacy-policy analysis methods.","Using the VR-SP detector, we conduct a comprehensive empirical study on 500 popular VR apps.","We obtain the original apps from the popular Oculus and SideQuest app stores and extract APK files via the Meta Oculus Quest 2 device.","We evaluate security vulnerabilities and privacy data leaks of these VR apps by VR app analysis, taint analysis, and privacy-policy analysis.","We find that a number of security vulnerabilities and privacy leaks widely exist in VR apps.","Moreover, our results also reveal conflicting representations in the privacy policies of these apps and inconsistencies of the actual data collection with the privacy-policy statements of the apps.","Based on these findings, we make suggestions for the future development of VR apps."],"url":"http://arxiv.org/abs/2402.13815v1","category":"cs.SE"}
{"created":"2024-02-21 13:46:25","title":"NeuralDiffuser: Controllable fMRI Reconstruction with Primary Visual Feature Guided Diffusion","abstract":"Reconstructing visual stimuli from functional Magnetic Resonance Imaging (fMRI) based on Latent Diffusion Models (LDM) provides a fine-grained retrieval of the brain. A challenge persists in reconstructing a cohesive alignment of details (such as structure, background, texture, color, etc.). Moreover, LDMs would generate different image results even under the same conditions. For these, we first uncover the neuroscientific perspective of LDM-based methods that is top-down creation based on pre-trained knowledge from massive images but lack of detail-driven bottom-up perception resulting in unfaithful details. We propose NeuralDiffuser which introduces primary visual feature guidance to provide detail cues in the form of gradients, extending the bottom-up process for LDM-based methods to achieve faithful semantics and details. We also developed a novel guidance strategy to ensure the consistency of repeated reconstructions rather than a variety of results. We obtain the state-of-the-art performance of NeuralDiffuser on the Natural Senses Dataset (NSD), which offers more faithful details and consistent results.","sentences":["Reconstructing visual stimuli from functional Magnetic Resonance Imaging (fMRI) based on Latent Diffusion Models (LDM) provides a fine-grained retrieval of the brain.","A challenge persists in reconstructing a cohesive alignment of details (such as structure, background, texture, color, etc.).","Moreover, LDMs would generate different image results even under the same conditions.","For these, we first uncover the neuroscientific perspective of LDM-based methods that is top-down creation based on pre-trained knowledge from massive images but lack of detail-driven bottom-up perception resulting in unfaithful details.","We propose NeuralDiffuser which introduces primary visual feature guidance to provide detail cues in the form of gradients, extending the bottom-up process for LDM-based methods to achieve faithful semantics and details.","We also developed a novel guidance strategy to ensure the consistency of repeated reconstructions rather than a variety of results.","We obtain the state-of-the-art performance of NeuralDiffuser on the Natural Senses Dataset (NSD), which offers more faithful details and consistent results."],"url":"http://arxiv.org/abs/2402.13809v1","category":"cs.NE"}
{"created":"2024-02-21 13:45:07","title":"A search for bottom-type vector-like quark pair production in dileptonic and fully hadronic final states in proton-proton collisions at $\\sqrt{s}$ = 13 TeV","abstract":"A search is described for the production of a pair of bottom-type vector-like quarks (B VLQs) with mass greater than 1000 GeV. Each B VLQ decays into a b quark and a Higgs boson, a b quark and a Z boson, or a t quark and a W boson. This analysis considers both fully hadronic final states and those containing a charged lepton pair from a Z boson decay. The products of the H $to$ bb boson decay and of the hadronic Z or W boson decays can be resolved as two distinct jets or merged into a single jet, so the final states are classified by the number of reconstructed jets. The analysis uses data corresponding to an integrated luminosity of 138 fb$^{-1}$ collected in proton-proton collisions at $\\sqrt{s}$ = 13 TeV with the CMS detector at the LHC from 2016 to 2018. No excess over the expected background is observed. Lower limits are set on the B VLQ mass at 95% confidence level. These depend on the B VLQ branching fractions and are 1570 and 1540 GeV for 100% B $\\to$ bH and 100% B $\\to$ bZ, respectively. In most cases, the mass limits obtained exceed previous limits by at least 100 GeV.","sentences":["A search is described for the production of a pair of bottom-type vector-like quarks (B VLQs) with mass greater than 1000 GeV.","Each B VLQ decays into a b quark and a Higgs boson, a b quark and a Z boson, or a t quark and a W boson.","This analysis considers both fully hadronic final states and those containing a charged lepton pair from a Z boson decay.","The products of the H $to$ bb boson decay and of the hadronic Z or W boson decays can be resolved as two distinct jets or merged into a single jet, so the final states are classified by the number of reconstructed jets.","The analysis uses data corresponding to an integrated luminosity of 138 fb$^{-1}$ collected in proton-proton collisions at $\\sqrt{s}$ = 13 TeV with the CMS detector at the LHC from 2016 to 2018.","No excess over the expected background is observed.","Lower limits are set on the B VLQ mass at 95% confidence level.","These depend on the B VLQ branching fractions and are 1570 and 1540 GeV for 100% B $\\to$ bH and 100% B $\\to$ bZ, respectively.","In most cases, the mass limits obtained exceed previous limits by at least 100 GeV."],"url":"http://arxiv.org/abs/2402.13808v1","category":"hep-ex"}
{"created":"2024-02-21 13:37:43","title":"Reconfigurable Intelligent Surfaces for THz: Hardware Impairments and Switching Technologies","abstract":"The demand for unprecedented performance in the upcoming 6G wireless networks is fomenting the research on THz communications empowered by Reconfigurable Inteligent Surfaces (RISs). A wide range of use cases have been proposed, most of them, assuming high-level RIS models that overlook some of the hardware impairments that this technology faces. The expectation is that the emergent reconfigurable THz technologies will eventually overcome its current limitations. This disassociation from the hardware may mask nonphysical assumptions, perceived as hardware limitations. In this paper, a top-down approach bounded by physical constraints is presented, distilling from system-level specifications, hardware requirements, and upper bounds for the RIS-aided system performance. We consider D-band indoor and outdoor scenarios where a more realistic assessment of the state-of-the-art solution can be made. The goal is to highlight the intricacies of the design procedure based on sound assumptions for the RIS performance. For a given signal range and angular coverage, we quantify the required RIS size, number of switching elements, and maximum achievable bandwidth and capacity.","sentences":["The demand for unprecedented performance in the upcoming 6G wireless networks is fomenting the research on THz communications empowered by Reconfigurable Inteligent Surfaces (RISs).","A wide range of use cases have been proposed, most of them, assuming high-level RIS models that overlook some of the hardware impairments that this technology faces.","The expectation is that the emergent reconfigurable THz technologies will eventually overcome its current limitations.","This disassociation from the hardware may mask nonphysical assumptions, perceived as hardware limitations.","In this paper, a top-down approach bounded by physical constraints is presented, distilling from system-level specifications, hardware requirements, and upper bounds for the RIS-aided system performance.","We consider D-band indoor and outdoor scenarios where a more realistic assessment of the state-of-the-art solution can be made.","The goal is to highlight the intricacies of the design procedure based on sound assumptions for the RIS performance.","For a given signal range and angular coverage, we quantify the required RIS size, number of switching elements, and maximum achievable bandwidth and capacity."],"url":"http://arxiv.org/abs/2402.13804v1","category":"cs.IT"}
{"created":"2024-02-21 13:10:58","title":"Synthesis of Hierarchical Controllers Based on Deep Reinforcement Learning Policies","abstract":"We propose a novel approach to the problem of controller design for environments modeled as Markov decision processes (MDPs). Specifically, we consider a hierarchical MDP a graph with each vertex populated by an MDP called a \"room\". We first apply deep reinforcement learning (DRL) to obtain low-level policies for each room, scaling to large rooms of unknown structure. We then apply reactive synthesis to obtain a high-level planner that chooses which low-level policy to execute in each room. The central challenge in synthesizing the planner is the need for modeling rooms. We address this challenge by developing a DRL procedure to train concise \"latent\" policies together with PAC guarantees on their performance. Unlike previous approaches, ours circumvents a model distillation step. Our approach combats sparse rewards in DRL and enables reusability of low-level policies. We demonstrate feasibility in a case study involving agent navigation amid moving obstacles.","sentences":["We propose a novel approach to the problem of controller design for environments modeled as Markov decision processes (MDPs).","Specifically, we consider a hierarchical MDP a graph with each vertex populated by an MDP called a \"room\".","We first apply deep reinforcement learning (DRL) to obtain low-level policies for each room, scaling to large rooms of unknown structure.","We then apply reactive synthesis to obtain a high-level planner that chooses which low-level policy to execute in each room.","The central challenge in synthesizing the planner is the need for modeling rooms.","We address this challenge by developing a DRL procedure to train concise \"latent\" policies together with PAC guarantees on their performance.","Unlike previous approaches, ours circumvents a model distillation step.","Our approach combats sparse rewards in DRL and enables reusability of low-level policies.","We demonstrate feasibility in a case study involving agent navigation amid moving obstacles."],"url":"http://arxiv.org/abs/2402.13785v1","category":"cs.AI"}
{"created":"2024-02-21 13:06:52","title":"Semirings for Probabilistic and Neuro-Symbolic Logic Programming","abstract":"The field of probabilistic logic programming (PLP) focuses on integrating probabilistic models into programming languages based on logic. Over the past 30 years, numerous languages and frameworks have been developed for modeling, inference and learning in probabilistic logic programs. While originally PLP focused on discrete probability, more recent approaches have incorporated continuous distributions as well as neural networks, effectively yielding neural-symbolic methods. We provide a unified algebraic perspective on PLP, showing that many if not most of the extensions of PLP can be cast within a common algebraic logic programming framework, in which facts are labeled with elements of a semiring and disjunction and conjunction are replaced by addition and multiplication. This does not only hold for the PLP variations itself but also for the underlying execution mechanism that is based on (algebraic) model counting.","sentences":["The field of probabilistic logic programming (PLP) focuses on integrating probabilistic models into programming languages based on logic.","Over the past 30 years, numerous languages and frameworks have been developed for modeling, inference and learning in probabilistic logic programs.","While originally PLP focused on discrete probability, more recent approaches have incorporated continuous distributions as well as neural networks, effectively yielding neural-symbolic methods.","We provide a unified algebraic perspective on PLP, showing that many if not most of the extensions of PLP can be cast within a common algebraic logic programming framework, in which facts are labeled with elements of a semiring and disjunction and conjunction are replaced by addition and multiplication.","This does not only hold for the PLP variations itself but also for the underlying execution mechanism that is based on (algebraic) model counting."],"url":"http://arxiv.org/abs/2402.13782v1","category":"cs.AI"}
{"created":"2024-02-21 12:58:40","title":"Contextual Molecule Representation Learning from Chemical Reaction Knowledge","abstract":"In recent years, self-supervised learning has emerged as a powerful tool to harness abundant unlabelled data for representation learning and has been broadly adopted in diverse areas. However, when applied to molecular representation learning (MRL), prevailing techniques such as masked sub-unit reconstruction often fall short, due to the high degree of freedom in the possible combinations of atoms within molecules, which brings insurmountable complexity to the masking-reconstruction paradigm. To tackle this challenge, we introduce REMO, a self-supervised learning framework that takes advantage of well-defined atom-combination rules in common chemistry. Specifically, REMO pre-trains graph/Transformer encoders on 1.7 million known chemical reactions in the literature. We propose two pre-training objectives: Masked Reaction Centre Reconstruction (MRCR) and Reaction Centre Identification (RCI). REMO offers a novel solution to MRL by exploiting the underlying shared patterns in chemical reactions as \\textit{context} for pre-training, which effectively infers meaningful representations of common chemistry knowledge. Such contextual representations can then be utilized to support diverse downstream molecular tasks with minimum finetuning, such as affinity prediction and drug-drug interaction prediction. Extensive experimental results on MoleculeACE, ACNet, drug-drug interaction (DDI), and reaction type classification show that across all tested downstream tasks, REMO outperforms the standard baseline of single-molecule masked modeling used in current MRL. Remarkably, REMO is the pioneering deep learning model surpassing fingerprint-based methods in activity cliff benchmarks.","sentences":["In recent years, self-supervised learning has emerged as a powerful tool to harness abundant unlabelled data for representation learning and has been broadly adopted in diverse areas.","However, when applied to molecular representation learning (MRL), prevailing techniques such as masked sub-unit reconstruction often fall short, due to the high degree of freedom in the possible combinations of atoms within molecules, which brings insurmountable complexity to the masking-reconstruction paradigm.","To tackle this challenge, we introduce REMO, a self-supervised learning framework that takes advantage of well-defined atom-combination rules in common chemistry.","Specifically, REMO pre-trains graph/Transformer encoders on 1.7 million known chemical reactions in the literature.","We propose two pre-training objectives: Masked Reaction Centre Reconstruction (MRCR) and Reaction Centre Identification (RCI).","REMO offers a novel solution to MRL by exploiting the underlying shared patterns in chemical reactions as \\textit{context} for pre-training, which effectively infers meaningful representations of common chemistry knowledge.","Such contextual representations can then be utilized to support diverse downstream molecular tasks with minimum finetuning, such as affinity prediction and drug-drug interaction prediction.","Extensive experimental results on MoleculeACE, ACNet, drug-drug interaction (DDI), and reaction type classification show that across all tested downstream tasks, REMO outperforms the standard baseline of single-molecule masked modeling used in current MRL.","Remarkably, REMO is the pioneering deep learning model surpassing fingerprint-based methods in activity cliff benchmarks."],"url":"http://arxiv.org/abs/2402.13779v1","category":"cs.LG"}
{"created":"2024-02-21 12:54:48","title":"Deep Generative Models for Offline Policy Learning: Tutorial, Survey, and Perspectives on Future Directions","abstract":"Deep generative models (DGMs) have demonstrated great success across various domains, particularly in generating texts, images, and videos using models trained from offline data. Similarly, data-driven decision-making and robotic control also necessitate learning a generator function from the offline data to serve as the strategy or policy. In this case, applying deep generative models in offline policy learning exhibits great potential, and numerous studies have explored in this direction. However, this field still lacks a comprehensive review and so developments of different branches are relatively independent. Thus, we provide the first systematic review on the applications of deep generative models for offline policy learning. In particular, we cover five mainstream deep generative models, including Variational Auto-Encoders, Generative Adversarial Networks, Normalizing Flows, Transformers, and Diffusion Models, and their applications in both offline reinforcement learning (offline RL) and imitation learning (IL). Offline RL and IL are two main branches of offline policy learning and are widely-adopted techniques for sequential decision-making. Specifically, for each type of DGM-based offline policy learning, we distill its fundamental scheme, categorize related works based on the usage of the DGM, and sort out the development process of algorithms in that field. Subsequent to the main content, we provide in-depth discussions on deep generative models and offline policy learning as a summary, based on which we present our perspectives on future research directions. This work offers a hands-on reference for the research progress in deep generative models for offline policy learning, and aims to inspire improved DGM-based offline RL or IL algorithms.","sentences":["Deep generative models (DGMs) have demonstrated great success across various domains, particularly in generating texts, images, and videos using models trained from offline data.","Similarly, data-driven decision-making and robotic control also necessitate learning a generator function from the offline data to serve as the strategy or policy.","In this case, applying deep generative models in offline policy learning exhibits great potential, and numerous studies have explored in this direction.","However, this field still lacks a comprehensive review and so developments of different branches are relatively independent.","Thus, we provide the first systematic review on the applications of deep generative models for offline policy learning.","In particular, we cover five mainstream deep generative models, including Variational Auto-Encoders, Generative Adversarial Networks, Normalizing Flows, Transformers, and Diffusion Models, and their applications in both offline reinforcement learning (offline RL) and imitation learning (IL).","Offline RL and IL are two main branches of offline policy learning and are widely-adopted techniques for sequential decision-making.","Specifically, for each type of DGM-based offline policy learning, we distill its fundamental scheme, categorize related works based on the usage of the DGM, and sort out the development process of algorithms in that field.","Subsequent to the main content, we provide in-depth discussions on deep generative models and offline policy learning as a summary, based on which we present our perspectives on future research directions.","This work offers a hands-on reference for the research progress in deep generative models for offline policy learning, and aims to inspire improved DGM-based offline RL or IL algorithms."],"url":"http://arxiv.org/abs/2402.13777v2","category":"cs.LG"}
{"created":"2024-02-21 12:50:44","title":"Spatial-Domain Wireless Jamming with Reconfigurable Intelligent Surfaces","abstract":"Today, we rely heavily on the constant availability of wireless communication systems. As a result, wireless jamming continues to prevail as an imminent threat: Attackers can create deliberate radio interference to overshadow desired signals, leading to denial of service. Although the broadcast nature of radio signal propagation makes such an attack possible in the first place, it likewise poses a challenge for the attacker, preventing precise targeting of single devices. In particular, the jamming signal will likely not only reach the victim receiver but also other neighboring devices. In this work, we introduce spatial control of wireless jamming signals, granting a new degree of freedom to leverage for jamming attacks. Our novel strategy employs an environment-adaptive reconfigurable intelligent surface (RIS), exploiting multipath signal propagation to spatially focus jamming signals on particular victim devices. We investigate this effect through extensive experimentation and show that our approach can disable the wireless communication of a victim device while leaving neighbouring devices unaffected. In particular, we demonstrate complete denial-of-service of a Wi-Fi device while a second device located at a distance as close as 5 mm remains unaffected, sustaining wireless communication at a data rate of 60 Mbit/s. We also show that the attacker can change the attack target on-the-fly, dynamically selecting the device to be jammed.","sentences":["Today, we rely heavily on the constant availability of wireless communication systems.","As a result, wireless jamming continues to prevail as an imminent threat: Attackers can create deliberate radio interference to overshadow desired signals, leading to denial of service.","Although the broadcast nature of radio signal propagation makes such an attack possible in the first place, it likewise poses a challenge for the attacker, preventing precise targeting of single devices.","In particular, the jamming signal will likely not only reach the victim receiver but also other neighboring devices.","In this work, we introduce spatial control of wireless jamming signals, granting a new degree of freedom to leverage for jamming attacks.","Our novel strategy employs an environment-adaptive reconfigurable intelligent surface (RIS), exploiting multipath signal propagation to spatially focus jamming signals on particular victim devices.","We investigate this effect through extensive experimentation and show that our approach can disable the wireless communication of a victim device while leaving neighbouring devices unaffected.","In particular, we demonstrate complete denial-of-service of a Wi-Fi device while a second device located at a distance as close as 5 mm remains unaffected, sustaining wireless communication at a data rate of 60 Mbit/s. We also show that the attacker can change the attack target on-the-fly, dynamically selecting the device to be jammed."],"url":"http://arxiv.org/abs/2402.13773v1","category":"cs.CR"}
{"created":"2024-02-21 12:48:45","title":"Mask-up: Investigating Biases in Face Re-identification for Masked Faces","abstract":"AI based Face Recognition Systems (FRSs) are now widely distributed and deployed as MLaaS solutions all over the world, moreso since the COVID-19 pandemic for tasks ranging from validating individuals' faces while buying SIM cards to surveillance of citizens. Extensive biases have been reported against marginalized groups in these systems and have led to highly discriminatory outcomes. The post-pandemic world has normalized wearing face masks but FRSs have not kept up with the changing times. As a result, these systems are susceptible to mask based face occlusion. In this study, we audit four commercial and nine open-source FRSs for the task of face re-identification between different varieties of masked and unmasked images across five benchmark datasets (total 14,722 images). These simulate a realistic validation/surveillance task as deployed in all major countries around the world. Three of the commercial and five of the open-source FRSs are highly inaccurate; they further perpetuate biases against non-White individuals, with the lowest accuracy being 0%. A survey for the same task with 85 human participants also results in a low accuracy of 40%. Thus a human-in-the-loop moderation in the pipeline does not alleviate the concerns, as has been frequently hypothesized in literature. Our large-scale study shows that developers, lawmakers and users of such services need to rethink the design principles behind FRSs, especially for the task of face re-identification, taking cognizance of observed biases.","sentences":["AI based Face Recognition Systems (FRSs) are now widely distributed and deployed as MLaaS solutions all over the world, moreso since the COVID-19 pandemic for tasks ranging from validating individuals' faces while buying SIM cards to surveillance of citizens.","Extensive biases have been reported against marginalized groups in these systems and have led to highly discriminatory outcomes.","The post-pandemic world has normalized wearing face masks but FRSs have not kept up with the changing times.","As a result, these systems are susceptible to mask based face occlusion.","In this study, we audit four commercial and nine open-source FRSs for the task of face re-identification between different varieties of masked and unmasked images across five benchmark datasets (total 14,722 images).","These simulate a realistic validation/surveillance task as deployed in all major countries around the world.","Three of the commercial and five of the open-source FRSs are highly inaccurate; they further perpetuate biases against non-White individuals, with the lowest accuracy being 0%.","A survey for the same task with 85 human participants also results in a low accuracy of 40%.","Thus a human-in-the-loop moderation in the pipeline does not alleviate the concerns, as has been frequently hypothesized in literature.","Our large-scale study shows that developers, lawmakers and users of such services need to rethink the design principles behind FRSs, especially for the task of face re-identification, taking cognizance of observed biases."],"url":"http://arxiv.org/abs/2402.13771v1","category":"cs.CV"}
{"created":"2024-02-21 12:39:20","title":"Accuracy-Preserving Calibration via Statistical Modeling on Probability Simplex","abstract":"Classification models based on deep neural networks (DNNs) must be calibrated to measure the reliability of predictions. Some recent calibration methods have employed a probabilistic model on the probability simplex. However, these calibration methods cannot preserve the accuracy of pre-trained models, even those with a high classification accuracy. We propose an accuracy-preserving calibration method using the Concrete distribution as the probabilistic model on the probability simplex. We theoretically prove that a DNN model trained on cross-entropy loss has optimality as the parameter of the Concrete distribution. We also propose an efficient method that synthetically generates samples for training probabilistic models on the probability simplex. We demonstrate that the proposed method can outperform previous methods in accuracy-preserving calibration tasks using benchmarks.","sentences":["Classification models based on deep neural networks (DNNs) must be calibrated to measure the reliability of predictions.","Some recent calibration methods have employed a probabilistic model on the probability simplex.","However, these calibration methods cannot preserve the accuracy of pre-trained models, even those with a high classification accuracy.","We propose an accuracy-preserving calibration method using the Concrete distribution as the probabilistic model on the probability simplex.","We theoretically prove that a DNN model trained on cross-entropy loss has optimality as the parameter of the Concrete distribution.","We also propose an efficient method that synthetically generates samples for training probabilistic models on the probability simplex.","We demonstrate that the proposed method can outperform previous methods in accuracy-preserving calibration tasks using benchmarks."],"url":"http://arxiv.org/abs/2402.13765v1","category":"cs.LG"}
{"created":"2024-02-21 12:38:59","title":"CriticBench: Evaluating Large Language Models as Critic","abstract":"Critique ability are crucial in the scalable oversight and self-improvement of Large Language Models (LLMs). While many recent studies explore the critique ability of LLMs to judge and refine flaws in generations, how to comprehensively and reliably measure the critique abilities of LLMs is under-explored. This paper introduces \\shortname, a novel benchmark designed to comprehensively and reliably evaluate four key critique ability dimensions of LLMs: feedback, comparison, refinement and meta-feedback. CriticBench encompasses nine diverse tasks, each assessing the LLMs' ability to critique responses at varying levels of quality granularity. Our extensive evaluations of open-source and closed-source LLMs reveal intriguing relationships between the critique ability and tasks, response qualities, and model scales. Datasets, resources and evaluation toolkit for CriticBench will be publicly released at \\url{https://github.com/open-compass/CriticBench}.","sentences":["Critique ability are crucial in the scalable oversight and self-improvement of Large Language Models (LLMs).","While many recent studies explore the critique ability of LLMs to judge and refine flaws in generations, how to comprehensively and reliably measure the critique abilities of LLMs is under-explored.","This paper introduces \\shortname, a novel benchmark designed to comprehensively and reliably evaluate four key critique ability dimensions of LLMs: feedback, comparison, refinement and meta-feedback.","CriticBench encompasses nine diverse tasks, each assessing the LLMs' ability to critique responses at varying levels of quality granularity.","Our extensive evaluations of open-source and closed-source LLMs reveal intriguing relationships between the critique ability and tasks, response qualities, and model scales.","Datasets, resources and evaluation toolkit for CriticBench will be publicly released at \\url{https://github.com/open-compass/CriticBench}."],"url":"http://arxiv.org/abs/2402.13764v2","category":"cs.CL"}
{"created":"2024-02-21 12:35:12","title":"Critical Behavior and Collective Modes at the Superfluid Transition in Amorphous Systems","abstract":"We investigate the critical behavior and the dynamics of the amplitude (Higgs) mode close to the superfluid-insulator quantum phase transition in an amorphous system (i.e., a system subject to topological randomness). In particular, we map the two-dimensional Bose-Hubbard Hamiltonian defined on a random Voronoi-Delaunay lattice onto a (2+1)-dimensional layered classical XY model with correlated topological disorder. We study the resulting model by laying recourse to classical Monte Carlo simulations. We specifically focus on the scalar susceptibility of the order parameter to study the dynamics of the amplitude mode. To do so, we harness the maximum entropy method to perform the analytic continuation of the scalar susceptibility to real frequencies. Our analysis shows that the amplitude mode remains delocalized in the presence of such topological disorder, quite at odds with its behavior in generic disordered systems, where the randomness localizes the Higgs mode. Furthermore, we show that the critical behavior of the topologically disordered system is identical to that of its translationally invariant counterpart, consistent with a modified Harris criterion. This suggests that the localization of the collective excitations in the presence of disorder is tied to the critical behavior of the quantum phase transition rather than a simple Anderson-localization-type interference mechanism.","sentences":["We investigate the critical behavior and the dynamics of the amplitude (Higgs) mode close to the superfluid-insulator quantum phase transition in an amorphous system (i.e., a system subject to topological randomness).","In particular, we map the two-dimensional Bose-Hubbard Hamiltonian defined on a random Voronoi-Delaunay lattice onto a (2+1)-dimensional layered classical XY model with correlated topological disorder.","We study the resulting model by laying recourse to classical Monte Carlo simulations.","We specifically focus on the scalar susceptibility of the order parameter to study the dynamics of the amplitude mode.","To do so, we harness the maximum entropy method to perform the analytic continuation of the scalar susceptibility to real frequencies.","Our analysis shows that the amplitude mode remains delocalized in the presence of such topological disorder, quite at odds with its behavior in generic disordered systems, where the randomness localizes the Higgs mode.","Furthermore, we show that the critical behavior of the topologically disordered system is identical to that of its translationally invariant counterpart, consistent with a modified Harris criterion.","This suggests that the localization of the collective excitations in the presence of disorder is tied to the critical behavior of the quantum phase transition rather than a simple Anderson-localization-type interference mechanism."],"url":"http://arxiv.org/abs/2402.13757v1","category":"cond-mat.dis-nn"}
{"created":"2024-02-21 12:30:39","title":"Reinforcement learning-assisted quantum architecture search for variational quantum algorithms","abstract":"A significant hurdle in the noisy intermediate-scale quantum (NISQ) era is identifying functional quantum circuits. These circuits must also adhere to the constraints imposed by current quantum hardware limitations. Variational quantum algorithms (VQAs), a class of quantum-classical optimization algorithms, were developed to address these challenges in the currently available quantum devices. However, the overall performance of VQAs depends on the initialization strategy of the variational circuit, the structure of the circuit (also known as ansatz), and the configuration of the cost function. Focusing on the structure of the circuit, in this thesis, we improve the performance of VQAs by automating the search for an optimal structure for the variational circuits using reinforcement learning (RL). Within the thesis, the optimality of a circuit is determined by evaluating its depth, the overall count of gates and parameters, and its accuracy in solving the given problem. The task of automating the search for optimal quantum circuits is known as quantum architecture search (QAS). The majority of research in QAS is primarily focused on a noiseless scenario. Yet, the impact of noise on the QAS remains inadequately explored. In this thesis, we tackle the issue by introducing a tensor-based quantum circuit encoding, restrictions on environment dynamics to explore the search space of possible circuits efficiently, an episode halting scheme to steer the agent to find shorter circuits, a double deep Q-network (DDQN) with an $\\epsilon$-greedy policy for better stability. The numerical experiments on noiseless and noisy quantum hardware show that in dealing with various VQAs, our RL-based QAS outperforms existing QAS. Meanwhile, the methods we propose in the thesis can be readily adapted to address a wide range of other VQAs.","sentences":["A significant hurdle in the noisy intermediate-scale quantum (NISQ) era is identifying functional quantum circuits.","These circuits must also adhere to the constraints imposed by current quantum hardware limitations.","Variational quantum algorithms (VQAs), a class of quantum-classical optimization algorithms, were developed to address these challenges in the currently available quantum devices.","However, the overall performance of VQAs depends on the initialization strategy of the variational circuit, the structure of the circuit (also known as ansatz), and the configuration of the cost function.","Focusing on the structure of the circuit, in this thesis, we improve the performance of VQAs by automating the search for an optimal structure for the variational circuits using reinforcement learning (RL).","Within the thesis, the optimality of a circuit is determined by evaluating its depth, the overall count of gates and parameters, and its accuracy in solving the given problem.","The task of automating the search for optimal quantum circuits is known as quantum architecture search (QAS).","The majority of research in QAS is primarily focused on a noiseless scenario.","Yet, the impact of noise on the QAS remains inadequately explored.","In this thesis, we tackle the issue by introducing a tensor-based quantum circuit encoding, restrictions on environment dynamics to explore the search space of possible circuits efficiently, an episode halting scheme to steer the agent to find shorter circuits, a double deep Q-network (DDQN) with an $\\epsilon$-greedy policy for better stability.","The numerical experiments on noiseless and noisy quantum hardware show that in dealing with various VQAs, our RL-based QAS outperforms existing QAS.","Meanwhile, the methods we propose in the thesis can be readily adapted to address a wide range of other VQAs."],"url":"http://arxiv.org/abs/2402.13754v1","category":"quant-ph"}
{"created":"2024-02-21 12:23:09","title":"AI-Powered Predictions for Electricity Load in Prosumer Communities","abstract":"The flexibility in electricity consumption and production in communities of residential buildings, including those with renewable energy sources and energy storage (a.k.a., prosumers), can effectively be utilized through the advancement of short-term demand response mechanisms. It is known that flexibility can further be increased if demand response is performed at the level of communities of prosumers, since aggregated groups can better coordinate electricity consumption. However, the effectiveness of such short-term optimization is highly dependent on the accuracy of electricity load forecasts both for each building as well as for the whole community. Structural variations in the electricity load profile can be associated with different exogenous factors, such as weather conditions, calendar information and day of the week, as well as user behavior. In this paper, we review a wide range of electricity load forecasting techniques, that can provide significant assistance in optimizing load consumption in prosumer communities. We present and test artificial intelligence (AI) powered short-term load forecasting methodologies that operate with black-box time series models, such as Facebook's Prophet and Long Short-term Memory (LSTM) models; season-based SARIMA and smoothing Holt-Winters models; and empirical regression-based models that utilize domain knowledge. The integration of weather forecasts into data-driven time series forecasts is also tested. Results show that the combination of persistent and regression terms (adapted to the load forecasting task) achieves the best forecast accuracy.","sentences":["The flexibility in electricity consumption and production in communities of residential buildings, including those with renewable energy sources and energy storage (a.k.a., prosumers), can effectively be utilized through the advancement of short-term demand response mechanisms.","It is known that flexibility can further be increased if demand response is performed at the level of communities of prosumers, since aggregated groups can better coordinate electricity consumption.","However, the effectiveness of such short-term optimization is highly dependent on the accuracy of electricity load forecasts both for each building as well as for the whole community.","Structural variations in the electricity load profile can be associated with different exogenous factors, such as weather conditions, calendar information and day of the week, as well as user behavior.","In this paper, we review a wide range of electricity load forecasting techniques, that can provide significant assistance in optimizing load consumption in prosumer communities.","We present and test artificial intelligence (AI) powered short-term load forecasting methodologies that operate with black-box time series models, such as Facebook's Prophet and Long Short-term Memory (LSTM) models; season-based SARIMA and smoothing Holt-Winters models; and empirical regression-based models that utilize domain knowledge.","The integration of weather forecasts into data-driven time series forecasts is also tested.","Results show that the combination of persistent and regression terms (adapted to the load forecasting task) achieves the best forecast accuracy."],"url":"http://arxiv.org/abs/2402.13752v1","category":"cs.LG"}
{"created":"2024-02-21 12:22:01","title":"Breaking the Barrier: Utilizing Large Language Models for Industrial Recommendation Systems through an Inferential Knowledge Graph","abstract":"Recommendation systems are widely used in e-commerce websites and online platforms to address information overload. However, existing systems primarily rely on historical data and user feedback, making it difficult to capture user intent transitions. Recently, Knowledge Base (KB)-based models are proposed to incorporate expert knowledge, but it struggle to adapt to new items and the evolving e-commerce environment. To address these challenges, we propose a novel Large Language Model based Complementary Knowledge Enhanced Recommendation System (LLM-KERec). It introduces an entity extractor that extracts unified concept terms from item and user information. To provide cost-effective and reliable prior knowledge, entity pairs are generated based on entity popularity and specific strategies. The large language model determines complementary relationships in each entity pair, constructing a complementary knowledge graph. Furthermore, a new complementary recall module and an Entity-Entity-Item (E-E-I) weight decision model refine the scoring of the ranking model using real complementary exposure-click samples. Extensive experiments conducted on three industry datasets demonstrate the significant performance improvement of our model compared to existing approaches. Additionally, detailed analysis shows that LLM-KERec enhances users' enthusiasm for consumption by recommending complementary items. In summary, LLM-KERec addresses the limitations of traditional recommendation systems by incorporating complementary knowledge and utilizing a large language model to capture user intent transitions, adapt to new items, and enhance recommendation efficiency in the evolving e-commerce landscape.","sentences":["Recommendation systems are widely used in e-commerce websites and online platforms to address information overload.","However, existing systems primarily rely on historical data and user feedback, making it difficult to capture user intent transitions.","Recently, Knowledge Base (KB)-based models are proposed to incorporate expert knowledge, but it struggle to adapt to new items and the evolving e-commerce environment.","To address these challenges, we propose a novel Large Language Model based Complementary Knowledge Enhanced Recommendation System (LLM-KERec).","It introduces an entity extractor that extracts unified concept terms from item and user information.","To provide cost-effective and reliable prior knowledge, entity pairs are generated based on entity popularity and specific strategies.","The large language model determines complementary relationships in each entity pair, constructing a complementary knowledge graph.","Furthermore, a new complementary recall module and an Entity-Entity-Item (E-E-I) weight decision model refine the scoring of the ranking model using real complementary exposure-click samples.","Extensive experiments conducted on three industry datasets demonstrate the significant performance improvement of our model compared to existing approaches.","Additionally, detailed analysis shows that LLM-KERec enhances users' enthusiasm for consumption by recommending complementary items.","In summary, LLM-KERec addresses the limitations of traditional recommendation systems by incorporating complementary knowledge and utilizing a large language model to capture user intent transitions, adapt to new items, and enhance recommendation efficiency in the evolving e-commerce landscape."],"url":"http://arxiv.org/abs/2402.13750v1","category":"cs.IR"}
{"created":"2024-02-21 12:16:51","title":"Reasoning Algorithmically in Graph Neural Networks","abstract":"The development of artificial intelligence systems with advanced reasoning capabilities represents a persistent and long-standing research question. Traditionally, the primary strategy to address this challenge involved the adoption of symbolic approaches, where knowledge was explicitly represented by means of symbols and explicitly programmed rules. However, with the advent of machine learning, there has been a paradigm shift towards systems that can autonomously learn from data, requiring minimal human guidance. In light of this shift, in latest years, there has been increasing interest and efforts at endowing neural networks with the ability to reason, bridging the gap between data-driven learning and logical reasoning. Within this context, Neural Algorithmic Reasoning (NAR) stands out as a promising research field, aiming to integrate the structured and rule-based reasoning of algorithms with the adaptive learning capabilities of neural networks, typically by tasking neural models to mimic classical algorithms. In this dissertation, we provide theoretical and practical contributions to this area of research. We explore the connections between neural networks and tropical algebra, deriving powerful architectures that are aligned with algorithm execution. Furthermore, we discuss and show the ability of such neural reasoners to learn and manipulate complex algorithmic and combinatorial optimization concepts, such as the principle of strong duality. Finally, in our empirical efforts, we validate the real-world utility of NAR networks across different practical scenarios. This includes tasks as diverse as planning problems, large-scale edge classification tasks and the learning of polynomial-time approximate algorithms for NP-hard combinatorial problems. Through this exploration, we aim to showcase the potential integrating algorithmic reasoning in machine learning models.","sentences":["The development of artificial intelligence systems with advanced reasoning capabilities represents a persistent and long-standing research question.","Traditionally, the primary strategy to address this challenge involved the adoption of symbolic approaches, where knowledge was explicitly represented by means of symbols and explicitly programmed rules.","However, with the advent of machine learning, there has been a paradigm shift towards systems that can autonomously learn from data, requiring minimal human guidance.","In light of this shift, in latest years, there has been increasing interest and efforts at endowing neural networks with the ability to reason, bridging the gap between data-driven learning and logical reasoning.","Within this context, Neural Algorithmic Reasoning (NAR) stands out as a promising research field, aiming to integrate the structured and rule-based reasoning of algorithms with the adaptive learning capabilities of neural networks, typically by tasking neural models to mimic classical algorithms.","In this dissertation, we provide theoretical and practical contributions to this area of research.","We explore the connections between neural networks and tropical algebra, deriving powerful architectures that are aligned with algorithm execution.","Furthermore, we discuss and show the ability of such neural reasoners to learn and manipulate complex algorithmic and combinatorial optimization concepts, such as the principle of strong duality.","Finally, in our empirical efforts, we validate the real-world utility of NAR networks across different practical scenarios.","This includes tasks as diverse as planning problems, large-scale edge classification tasks and the learning of polynomial-time approximate algorithms for NP-hard combinatorial problems.","Through this exploration, we aim to showcase the potential integrating algorithmic reasoning in machine learning models."],"url":"http://arxiv.org/abs/2402.13744v1","category":"cs.LG"}
{"created":"2024-02-21 12:12:16","title":"Unlocking Instructive In-Context Learning with Tabular Prompting for Relational Triple Extraction","abstract":"The in-context learning (ICL) for relational triple extraction (RTE) has achieved promising performance, but still encounters two key challenges: (1) how to design effective prompts and (2) how to select proper demonstrations. Existing methods, however, fail to address these challenges appropriately. On the one hand, they usually recast RTE task to text-to-text prompting formats, which is unnatural and results in a mismatch between the output format at the pre-training time and the inference time for large language models (LLMs). On the other hand, they only utilize surface natural language features and lack consideration of triple semantics in sample selection. These issues are blocking improved performance in ICL for RTE, thus we aim to tackle prompt designing and sample selection challenges simultaneously. To this end, we devise a tabular prompting for RTE (\\textsc{TableIE}) which frames RTE task into a table generation task to incorporate explicit structured information into ICL, facilitating conversion of outputs to RTE structures. Then we propose instructive in-context learning (I$^2$CL) which only selects and annotates a few samples considering internal triple semantics in massive unlabeled samples.","sentences":["The in-context learning (ICL) for relational triple extraction (RTE) has achieved promising performance, but still encounters two key challenges: (1) how to design effective prompts and (2) how to select proper demonstrations.","Existing methods, however, fail to address these challenges appropriately.","On the one hand, they usually recast RTE task to text-to-text prompting formats, which is unnatural and results in a mismatch between the output format at the pre-training time and the inference time for large language models (LLMs).","On the other hand, they only utilize surface natural language features and lack consideration of triple semantics in sample selection.","These issues are blocking improved performance in ICL for RTE, thus we aim to tackle prompt designing and sample selection challenges simultaneously.","To this end, we devise a tabular prompting for RTE (\\textsc{TableIE}) which frames RTE task into a table generation task to incorporate explicit structured information into ICL, facilitating conversion of outputs to RTE structures.","Then we propose instructive in-context learning (I$^2$CL) which only selects and annotates a few samples considering internal triple semantics in massive unlabeled samples."],"url":"http://arxiv.org/abs/2402.13741v1","category":"cs.CL"}
{"created":"2024-02-21 11:50:32","title":"The Da Vinci Code of Large Pre-trained Language Models: Deciphering Degenerate Knowledge Neurons","abstract":"This study explores the mechanism of factual knowledge storage in pre-trained language models (PLMs). Previous research suggests that factual knowledge is stored within multi-layer perceptron weights, and some storage units exhibit degeneracy, referred to as Degenerate Knowledge Neurons (DKNs). This paper provides a comprehensive definition of DKNs that covers both structural and functional aspects, pioneering the study of structures in PLMs' factual knowledge storage units. Based on this, we introduce the Neurological Topology Clustering method, which allows the formation of DKNs in any numbers and structures, leading to a more accurate DKN acquisition. Furthermore, we introduce the Neuro-Degeneracy Analytic Analysis Framework, which uniquely integrates model robustness, evolvability, and complexity for a holistic assessment of PLMs. Within this framework, our execution of 34 experiments across 2 PLMs, 4 datasets, and 6 settings highlights the critical role of DKNs. The code will be available soon.","sentences":["This study explores the mechanism of factual knowledge storage in pre-trained language models (PLMs).","Previous research suggests that factual knowledge is stored within multi-layer perceptron weights, and some storage units exhibit degeneracy, referred to as Degenerate Knowledge Neurons (DKNs).","This paper provides a comprehensive definition of DKNs that covers both structural and functional aspects, pioneering the study of structures in PLMs' factual knowledge storage units.","Based on this, we introduce the Neurological Topology Clustering method, which allows the formation of DKNs in any numbers and structures, leading to a more accurate DKN acquisition.","Furthermore, we introduce the Neuro-Degeneracy Analytic Analysis Framework, which uniquely integrates model robustness, evolvability, and complexity for a holistic assessment of PLMs.","Within this framework, our execution of 34 experiments across 2 PLMs, 4 datasets, and 6 settings highlights the critical role of DKNs.","The code will be available soon."],"url":"http://arxiv.org/abs/2402.13731v1","category":"cs.CL"}
{"created":"2024-02-21 11:27:31","title":"An Evaluation of Large Language Models in Bioinformatics Research","abstract":"Large language models (LLMs) such as ChatGPT have gained considerable interest across diverse research communities. Their notable ability for text completion and generation has inaugurated a novel paradigm for language-interfaced problem solving. However, the potential and efficacy of these models in bioinformatics remain incompletely explored. In this work, we study the performance LLMs on a wide spectrum of crucial bioinformatics tasks. These tasks include the identification of potential coding regions, extraction of named entities for genes and proteins, detection of antimicrobial and anti-cancer peptides, molecular optimization, and resolution of educational bioinformatics problems. Our findings indicate that, given appropriate prompts, LLMs like GPT variants can successfully handle most of these tasks. In addition, we provide a thorough analysis of their limitations in the context of complicated bioinformatics tasks. In conclusion, we believe that this work can provide new perspectives and motivate future research in the field of LLMs applications, AI for Science and bioinformatics.","sentences":["Large language models (LLMs) such as ChatGPT have gained considerable interest across diverse research communities.","Their notable ability for text completion and generation has inaugurated a novel paradigm for language-interfaced problem solving.","However, the potential and efficacy of these models in bioinformatics remain incompletely explored.","In this work, we study the performance LLMs on a wide spectrum of crucial bioinformatics tasks.","These tasks include the identification of potential coding regions, extraction of named entities for genes and proteins, detection of antimicrobial and anti-cancer peptides, molecular optimization, and resolution of educational bioinformatics problems.","Our findings indicate that, given appropriate prompts, LLMs like GPT variants can successfully handle most of these tasks.","In addition, we provide a thorough analysis of their limitations in the context of complicated bioinformatics tasks.","In conclusion, we believe that this work can provide new perspectives and motivate future research in the field of LLMs applications, AI for Science and bioinformatics."],"url":"http://arxiv.org/abs/2402.13714v1","category":"q-bio.QM"}
{"created":"2024-02-21 11:25:54","title":"DSLR: Diversity Enhancement and Structure Learning for Rehearsal-based Graph Continual Learning","abstract":"We investigate the replay buffer in rehearsal-based approaches for graph continual learning (GCL) methods. Existing rehearsal-based GCL methods select the most representative nodes for each class and store them in a replay buffer for later use in training subsequent tasks. However, we discovered that considering only the class representativeness of each replayed node makes the replayed nodes to be concentrated around the center of each class, incurring a potential risk of overfitting to nodes residing in those regions, which aggravates catastrophic forgetting. Moreover, as the rehearsal-based approach heavily relies on a few replayed nodes to retain knowledge obtained from previous tasks, involving the replayed nodes that have irrelevant neighbors in the model training may have a significant detrimental impact on model performance. In this paper, we propose a GCL model named DSLR, specifically, we devise a coverage-based diversity (CD) approach to consider both the class representativeness and the diversity within each class of the replayed nodes. Moreover, we adopt graph structure learning (GSL) to ensure that the replayed nodes are connected to truly informative neighbors. Extensive experimental results demonstrate the effectiveness and efficiency of DSLR. Our source code is available at https://github.com/seungyoon-Choi/DSLR_official.","sentences":["We investigate the replay buffer in rehearsal-based approaches for graph continual learning (GCL) methods.","Existing rehearsal-based GCL methods select the most representative nodes for each class and store them in a replay buffer for later use in training subsequent tasks.","However, we discovered that considering only the class representativeness of each replayed node makes the replayed nodes to be concentrated around the center of each class, incurring a potential risk of overfitting to nodes residing in those regions, which aggravates catastrophic forgetting.","Moreover, as the rehearsal-based approach heavily relies on a few replayed nodes to retain knowledge obtained from previous tasks, involving the replayed nodes that have irrelevant neighbors in the model training may have a significant detrimental impact on model performance.","In this paper, we propose a GCL model named DSLR, specifically, we devise a coverage-based diversity (CD) approach to consider both the class representativeness and the diversity within each class of the replayed nodes.","Moreover, we adopt graph structure learning (GSL) to ensure that the replayed nodes are connected to truly informative neighbors.","Extensive experimental results demonstrate the effectiveness and efficiency of DSLR.","Our source code is available at https://github.com/seungyoon-Choi/DSLR_official."],"url":"http://arxiv.org/abs/2402.13711v2","category":"cs.LG"}
{"created":"2024-02-21 11:23:21","title":"SaGE: Evaluating Moral Consistency in Large Language Models","abstract":"Despite recent advancements showcasing the impressive capabilities of Large Language Models (LLMs) in conversational systems, we show that even state-of-the-art LLMs are morally inconsistent in their generations, questioning their reliability (and trustworthiness in general). Prior works in LLM evaluation focus on developing ground-truth data to measure accuracy on specific tasks. However, for moral scenarios that often lack universally agreed-upon answers, consistency in model responses becomes crucial for their reliability. To address this issue, we propose an information-theoretic measure called Semantic Graph Entropy (SaGE), grounded in the concept of \"Rules of Thumb\" (RoTs) to measure a model's moral consistency. RoTs are abstract principles learned by a model and can help explain their decision-making strategies effectively. To this extent, we construct the Moral Consistency Corpus (MCC), containing 50K moral questions, responses to them by LLMs, and the RoTs that these models followed. Furthermore, to illustrate the generalizability of SaGE, we use it to investigate LLM consistency on two popular datasets -- TruthfulQA and HellaSwag. Our results reveal that task-accuracy and consistency are independent problems, and there is a dire need to investigate these issues further.","sentences":["Despite recent advancements showcasing the impressive capabilities of Large Language Models (LLMs) in conversational systems, we show that even state-of-the-art LLMs are morally inconsistent in their generations, questioning their reliability (and trustworthiness in general).","Prior works in LLM evaluation focus on developing ground-truth data to measure accuracy on specific tasks.","However, for moral scenarios that often lack universally agreed-upon answers, consistency in model responses becomes crucial for their reliability.","To address this issue, we propose an information-theoretic measure called Semantic Graph Entropy (SaGE), grounded in the concept of \"Rules of Thumb\" (RoTs) to measure a model's moral consistency.","RoTs are abstract principles learned by a model and can help explain their decision-making strategies effectively.","To this extent, we construct the Moral Consistency Corpus (MCC), containing 50K moral questions, responses to them by LLMs, and the RoTs that these models followed.","Furthermore, to illustrate the generalizability of SaGE, we use it to investigate LLM consistency on two popular datasets -- TruthfulQA and HellaSwag.","Our results reveal that task-accuracy and consistency are independent problems, and there is a dire need to investigate these issues further."],"url":"http://arxiv.org/abs/2402.13709v1","category":"cs.CL"}
{"created":"2024-02-21 10:54:47","title":"How Do Microservice API Patterns Impact Understandability? A Controlled Experiment","abstract":"Microservices expose their functionality via remote Application Programming Interfaces (APIs), e.g., based on HTTP or asynchronous messaging technology. To solve recurring problems in this design space, Microservice API Patterns (MAPs) have emerged to capture the collective experience of the API design community. At present, there is a lack of empirical evidence for the effectiveness of these patterns, e.g., how they impact understandability and API usability. We therefore conducted a controlled experiment with 6 microservice patterns to evaluate their impact on understandability with 65 diverse participants. Additionally, we wanted to study how demographics like years of professional experience or experience with MAPs influence the effects of the patterns. Per pattern, we constructed two API examples, each in a pattern version \"P\" and a functionally equivalent non-pattern version \"N\" (24 in total). Based on a crossover design, participants had to answer comprehension questions, while we measured the time. For five of the six patterns, we identified a significant positive impact on understandability, i.e., participants answered faster and / or more correctly for \"P\". However, effect sizes were mostly small, with one pattern showing a medium effect. The correlations between performance and demographics seem to suggest that certain patterns may introduce additional complexity; people experienced with MAPs will profit more from their effects. This has important implications for training and education around MAPs and other patterns.","sentences":["Microservices expose their functionality via remote Application Programming Interfaces (APIs), e.g., based on HTTP or asynchronous messaging technology.","To solve recurring problems in this design space, Microservice API Patterns (MAPs) have emerged to capture the collective experience of the API design community.","At present, there is a lack of empirical evidence for the effectiveness of these patterns, e.g., how they impact understandability and API usability.","We therefore conducted a controlled experiment with 6 microservice patterns to evaluate their impact on understandability with 65 diverse participants.","Additionally, we wanted to study how demographics like years of professional experience or experience with MAPs influence the effects of the patterns.","Per pattern, we constructed two API examples, each in a pattern version \"P\" and a functionally equivalent non-pattern version \"N\" (24 in total).","Based on a crossover design, participants had to answer comprehension questions, while we measured the time.","For five of the six patterns, we identified a significant positive impact on understandability, i.e., participants answered faster and / or more correctly for \"P\".","However, effect sizes were mostly small, with one pattern showing a medium effect.","The correlations between performance and demographics seem to suggest that certain patterns may introduce additional complexity; people experienced with MAPs will profit more from their effects.","This has important implications for training and education around MAPs and other patterns."],"url":"http://arxiv.org/abs/2402.13696v1","category":"cs.SE"}
{"created":"2024-02-21 10:53:23","title":"Reconfigurable Intelligent Surface assisted Integrated Sensing, Communication and Computation Systems","abstract":"This paper investigates a reconfigurable intelligent surface (RIS)-assisted integrated sensing, communication, and computation (ISCC) system. In this paradigm, the integrated sensing and communication (ISAC)-enabled user equipments (UEs) simultaneously detect the target and offload the computational tasks of radar sensing to the edge computing server (ECS) through their communication functionality. To enhance the efficiency of computation offloading, we deploy an RIS to mitigate the high attenuation between UEs and the ECS. A latency minimization problem is investigated with constraints on UE's transmit power, radar signal-to-interference-plus-noise ratio (SINR), RIS phase shift, and computation capability. We propose an algorithm based on the block coordinate descent (BCD) method to decouple the original problem into two subproblems, and then the computational and beamforming variables are optimized alternately utilizing efficient iterative algorithms. Simulation results demonstrate the effectiveness of our proposed algorithm.","sentences":["This paper investigates a reconfigurable intelligent surface (RIS)-assisted integrated sensing, communication, and computation (ISCC) system.","In this paradigm, the integrated sensing and communication (ISAC)-enabled user equipments (UEs) simultaneously detect the target and offload the computational tasks of radar sensing to the edge computing server (ECS) through their communication functionality.","To enhance the efficiency of computation offloading, we deploy an RIS to mitigate the high attenuation between UEs and the ECS.","A latency minimization problem is investigated with constraints on UE's transmit power, radar signal-to-interference-plus-noise ratio (SINR), RIS phase shift, and computation capability.","We propose an algorithm based on the block coordinate descent (BCD) method to decouple the original problem into two subproblems, and then the computational and beamforming variables are optimized alternately utilizing efficient iterative algorithms.","Simulation results demonstrate the effectiveness of our proposed algorithm."],"url":"http://arxiv.org/abs/2402.13692v1","category":"eess.SP"}
{"created":"2024-02-21 10:09:56","title":"KInIT at SemEval-2024 Task 8: Fine-tuned LLMs for Multilingual Machine-Generated Text Detection","abstract":"SemEval-2024 Task 8 is focused on multigenerator, multidomain, and multilingual black-box machine-generated text detection. Such a detection is important for preventing a potential misuse of large language models (LLMs), the newest of which are very capable in generating multilingual human-like texts. We have coped with this task in multiple ways, utilizing language identification and parameter-efficient fine-tuning of smaller LLMs for text classification. We have further used the per-language classification-threshold calibration to uniquely combine fine-tuned models predictions with statistical detection metrics to improve generalization of the system detection performance. Our submitted method achieved competitive results, ranking at the fourth place, just under 1 percentage point behind the winner.","sentences":["SemEval-2024 Task 8 is focused on multigenerator, multidomain, and multilingual black-box machine-generated text detection.","Such a detection is important for preventing a potential misuse of large language models (LLMs), the newest of which are very capable in generating multilingual human-like texts.","We have coped with this task in multiple ways, utilizing language identification and parameter-efficient fine-tuning of smaller LLMs for text classification.","We have further used the per-language classification-threshold calibration to uniquely combine fine-tuned models predictions with statistical detection metrics to improve generalization of the system detection performance.","Our submitted method achieved competitive results, ranking at the fourth place, just under 1 percentage point behind the winner."],"url":"http://arxiv.org/abs/2402.13671v1","category":"cs.CL"}
{"created":"2024-02-21 10:08:13","title":"The Riemannian Convex Bundle Method","abstract":"We introduce the convex bundle method to solve convex, non-smooth optimization problems on Riemannian manifolds. Each step of our method is based on a model that involves the convex hull of previously collected subgradients, parallely transported into the current serious iterate. This approach generalizes the dual form of classical bundle subproblems in Euclidean space. We prove that, under mild conditions, the convex bundle method converges to a minimizer. Several numerical examples implemented using the Julia package Manopt.jl illustrate the performance of the proposed method and compare it to the subgradient method, the cyclic proximal point, as well as the proximal bundle algorithm from Hoseini Monjezi, Nobakhtian, Pouryayevali, 2021.","sentences":["We introduce the convex bundle method to solve convex, non-smooth optimization problems on Riemannian manifolds.","Each step of our method is based on a model that involves the convex hull of previously collected subgradients, parallely transported into the current serious iterate.","This approach generalizes the dual form of classical bundle subproblems in Euclidean space.","We prove that, under mild conditions, the convex bundle method converges to a minimizer.","Several numerical examples implemented using the Julia package Manopt.jl illustrate the performance of the proposed method and compare it to the subgradient method, the cyclic proximal point, as well as the proximal bundle algorithm from Hoseini Monjezi, Nobakhtian, Pouryayevali, 2021."],"url":"http://arxiv.org/abs/2402.13670v1","category":"math.OC"}
{"created":"2024-02-21 09:45:08","title":"Privacy-Preserving Instructions for Aligning Large Language Models","abstract":"Service providers of large language model (LLM) applications collect user instructions in the wild and use them in further aligning LLMs with users' intentions. These instructions, which potentially contain sensitive information, are annotated by human workers in the process. This poses a new privacy risk not addressed by the typical private optimization. To this end, we propose using synthetic instructions to replace real instructions in data annotation and model fine-tuning. Formal differential privacy is guaranteed by generating those synthetic instructions using privately fine-tuned generators. Crucial in achieving the desired utility is our novel filtering algorithm that matches the distribution of the synthetic instructions to that of the real ones. In both supervised fine-tuning and reinforcement learning from human feedback, our extensive experiments demonstrate the high utility of the final set of synthetic instructions by showing comparable results to real instructions. In supervised fine-tuning, models trained with private synthetic instructions outperform leading open-source models such as Vicuna.","sentences":["Service providers of large language model (LLM) applications collect user instructions in the wild and use them in further aligning LLMs with users' intentions.","These instructions, which potentially contain sensitive information, are annotated by human workers in the process.","This poses a new privacy risk not addressed by the typical private optimization.","To this end, we propose using synthetic instructions to replace real instructions in data annotation and model fine-tuning.","Formal differential privacy is guaranteed by generating those synthetic instructions using privately fine-tuned generators.","Crucial in achieving the desired utility is our novel filtering algorithm that matches the distribution of the synthetic instructions to that of the real ones.","In both supervised fine-tuning and reinforcement learning from human feedback, our extensive experiments demonstrate the high utility of the final set of synthetic instructions by showing comparable results to real instructions.","In supervised fine-tuning, models trained with private synthetic instructions outperform leading open-source models such as Vicuna."],"url":"http://arxiv.org/abs/2402.13659v1","category":"cs.CR"}
{"created":"2024-02-21 09:28:02","title":"Unsupervised Text Style Transfer via LLMs and Attention Masking with Multi-way Interactions","abstract":"Unsupervised Text Style Transfer (UTST) has emerged as a critical task within the domain of Natural Language Processing (NLP), aiming to transfer one stylistic aspect of a sentence into another style without changing its semantics, syntax, or other attributes. This task is especially challenging given the intrinsic lack of parallel text pairings. Among existing methods for UTST tasks, attention masking approach and Large Language Models (LLMs) are deemed as two pioneering methods. However, they have shortcomings in generating unsmooth sentences and changing the original contents, respectively. In this paper, we investigate if we can combine these two methods effectively. We propose four ways of interactions, that are pipeline framework with tuned orders; knowledge distillation from LLMs to attention masking model; in-context learning with constructed parallel examples. We empirically show these multi-way interactions can improve the baselines in certain perspective of style strength, content preservation and text fluency. Experiments also demonstrate that simply conducting prompting followed by attention masking-based revision can consistently surpass the other systems, including supervised text style transfer systems. On Yelp-clean and Amazon-clean datasets, it improves the previously best mean metric by 0.5 and 3.0 absolute percentages respectively, and achieves new SOTA results.","sentences":["Unsupervised Text Style Transfer (UTST) has emerged as a critical task within the domain of Natural Language Processing (NLP), aiming to transfer one stylistic aspect of a sentence into another style without changing its semantics, syntax, or other attributes.","This task is especially challenging given the intrinsic lack of parallel text pairings.","Among existing methods for UTST tasks, attention masking approach and Large Language Models (LLMs) are deemed as two pioneering methods.","However, they have shortcomings in generating unsmooth sentences and changing the original contents, respectively.","In this paper, we investigate if we can combine these two methods effectively.","We propose four ways of interactions, that are pipeline framework with tuned orders; knowledge distillation from LLMs to attention masking model; in-context learning with constructed parallel examples.","We empirically show these multi-way interactions can improve the baselines in certain perspective of style strength, content preservation and text fluency.","Experiments also demonstrate that simply conducting prompting followed by attention masking-based revision can consistently surpass the other systems, including supervised text style transfer systems.","On Yelp-clean and Amazon-clean datasets, it improves the previously best mean metric by 0.5 and 3.0 absolute percentages respectively, and achieves new SOTA results."],"url":"http://arxiv.org/abs/2402.13647v1","category":"cs.CL"}
{"created":"2024-02-21 09:15:46","title":"The METRIC-framework for assessing data quality for trustworthy AI in medicine: a systematic review","abstract":"The adoption of machine learning (ML) and, more specifically, deep learning (DL) applications into all major areas of our lives is underway. The development of trustworthy AI is especially important in medicine due to the large implications for patients' lives. While trustworthiness concerns various aspects including ethical, technical and privacy requirements, we focus on the importance of data quality (training/test) in DL. Since data quality dictates the behaviour of ML products, evaluating data quality will play a key part in the regulatory approval of medical AI products. We perform a systematic review following PRISMA guidelines using the databases PubMed and ACM Digital Library. We identify 2362 studies, out of which 62 records fulfil our eligibility criteria. From this literature, we synthesise the existing knowledge on data quality frameworks and combine it with the perspective of ML applications in medicine. As a result, we propose the METRIC-framework, a specialised data quality framework for medical training data comprising 15 awareness dimensions, along which developers of medical ML applications should investigate a dataset. This knowledge helps to reduce biases as a major source of unfairness, increase robustness, facilitate interpretability and thus lays the foundation for trustworthy AI in medicine. Incorporating such systematic assessment of medical datasets into regulatory approval processes has the potential to accelerate the approval of ML products and builds the basis for new standards.","sentences":["The adoption of machine learning (ML) and, more specifically, deep learning (DL) applications into all major areas of our lives is underway.","The development of trustworthy AI is especially important in medicine due to the large implications for patients' lives.","While trustworthiness concerns various aspects including ethical, technical and privacy requirements, we focus on the importance of data quality (training/test) in DL.","Since data quality dictates the behaviour of ML products, evaluating data quality will play a key part in the regulatory approval of medical AI products.","We perform a systematic review following PRISMA guidelines using the databases PubMed and ACM Digital Library.","We identify 2362 studies, out of which 62 records fulfil our eligibility criteria.","From this literature, we synthesise the existing knowledge on data quality frameworks and combine it with the perspective of ML applications in medicine.","As a result, we propose the METRIC-framework, a specialised data quality framework for medical training data comprising 15 awareness dimensions, along which developers of medical ML applications should investigate a dataset.","This knowledge helps to reduce biases as a major source of unfairness, increase robustness, facilitate interpretability and thus lays the foundation for trustworthy AI in medicine.","Incorporating such systematic assessment of medical datasets into regulatory approval processes has the potential to accelerate the approval of ML products and builds the basis for new standards."],"url":"http://arxiv.org/abs/2402.13635v1","category":"cs.LG"}
{"created":"2024-02-21 09:06:31","title":"UniGraph: Learning a Cross-Domain Graph Foundation Model From Natural Language","abstract":"Foundation models like ChatGPT and GPT-4 have revolutionized artificial intelligence, exhibiting remarkable abilities to generalize across a wide array of tasks and applications beyond their initial training objectives. However, when this concept is applied to graph learning, a stark contrast emerges. Graph learning has predominantly focused on single-graph models, tailored to specific tasks or datasets, lacking the ability to transfer learned knowledge to different domains. This limitation stems from the inherent complexity and diversity of graph structures, along with the different feature and label spaces specific to graph data. In this paper, we present our UniGraph framework, designed to train a graph foundation model capable of generalizing to unseen graphs and tasks across diverse domains. Unlike single-graph models that use pre-computed node features of varying dimensions as input, our approach leverages Text-Attributed Graphs (TAGs) for unifying node representations. We propose a cascaded architecture of Language Models (LMs) and Graph Neural Networks (GNNs) as backbone networks with a self-supervised training objective based on Masked Graph Modeling (MGM). We introduce graph instruction tuning using Large Language Models (LLMs) to enable zero-shot prediction ability. Our comprehensive experiments across various graph learning tasks and domains demonstrate the model's effectiveness in self-supervised representation learning on unseen graphs, few-shot in-context transfer, and zero-shot transfer, even surpassing or matching the performance of GNNs that have undergone supervised training on target datasets.","sentences":["Foundation models like ChatGPT and GPT-4 have revolutionized artificial intelligence, exhibiting remarkable abilities to generalize across a wide array of tasks and applications beyond their initial training objectives.","However, when this concept is applied to graph learning, a stark contrast emerges.","Graph learning has predominantly focused on single-graph models, tailored to specific tasks or datasets, lacking the ability to transfer learned knowledge to different domains.","This limitation stems from the inherent complexity and diversity of graph structures, along with the different feature and label spaces specific to graph data.","In this paper, we present our UniGraph framework, designed to train a graph foundation model capable of generalizing to unseen graphs and tasks across diverse domains.","Unlike single-graph models that use pre-computed node features of varying dimensions as input, our approach leverages Text-Attributed Graphs (TAGs) for unifying node representations.","We propose a cascaded architecture of Language Models (LMs) and Graph Neural Networks (GNNs) as backbone networks with a self-supervised training objective based on Masked Graph Modeling (MGM).","We introduce graph instruction tuning using Large Language Models (LLMs) to enable zero-shot prediction ability.","Our comprehensive experiments across various graph learning tasks and domains demonstrate the model's effectiveness in self-supervised representation learning on unseen graphs, few-shot in-context transfer, and zero-shot transfer, even surpassing or matching the performance of GNNs that have undergone supervised training on target datasets."],"url":"http://arxiv.org/abs/2402.13630v1","category":"cs.LG"}
{"created":"2024-02-21 18:59:13","title":"Coercing LLMs to do and reveal (almost) anything","abstract":"It has recently been shown that adversarial attacks on large language models (LLMs) can \"jailbreak\" the model into making harmful statements. In this work, we argue that the spectrum of adversarial attacks on LLMs is much larger than merely jailbreaking. We provide a broad overview of possible attack surfaces and attack goals. Based on a series of concrete examples, we discuss, categorize and systematize attacks that coerce varied unintended behaviors, such as misdirection, model control, denial-of-service, or data extraction.   We analyze these attacks in controlled experiments, and find that many of them stem from the practice of pre-training LLMs with coding capabilities, as well as the continued existence of strange \"glitch\" tokens in common LLM vocabularies that should be removed for security reasons.","sentences":["It has recently been shown that adversarial attacks on large language models (LLMs) can \"jailbreak\" the model into making harmful statements.","In this work, we argue that the spectrum of adversarial attacks on LLMs is much larger than merely jailbreaking.","We provide a broad overview of possible attack surfaces and attack goals.","Based on a series of concrete examples, we discuss, categorize and systematize attacks that coerce varied unintended behaviors, such as misdirection, model control, denial-of-service, or data extraction.   ","We analyze these attacks in controlled experiments, and find that many of them stem from the practice of pre-training LLMs with coding capabilities, as well as the continued existence of strange \"glitch\" tokens in common LLM vocabularies that should be removed for security reasons."],"url":"http://arxiv.org/abs/2402.14020v1","category":"cs.LG"}
{"created":"2024-02-21 18:36:26","title":"Real-time 3D-aware Portrait Editing from a Single Image","abstract":"This work presents 3DPE, a practical tool that can efficiently edit a face image following given prompts, like reference images or text descriptions, in the 3D-aware manner. To this end, a lightweight module is distilled from a 3D portrait generator and a text-to-image model, which provide prior knowledge of face geometry and open-vocabulary editing capability, respectively. Such a design brings two compelling advantages over existing approaches. First, our system achieves real-time editing with a feedforward network (i.e., ~0.04s per image), over 100x faster than the second competitor. Second, thanks to the powerful priors, our module could focus on the learning of editing-related variations, such that it manages to handle various types of editing simultaneously in the training phase and further supports fast adaptation to user-specified novel types of editing during inference (e.g., with ~5min fine-tuning per case). The code, the model, and the interface will be made publicly available to facilitate future research.","sentences":["This work presents 3DPE, a practical tool that can efficiently edit a face image following given prompts, like reference images or text descriptions, in the 3D-aware manner.","To this end, a lightweight module is distilled from a 3D portrait generator and a text-to-image model, which provide prior knowledge of face geometry and open-vocabulary editing capability, respectively.","Such a design brings two compelling advantages over existing approaches.","First, our system achieves real-time editing with a feedforward network (i.e., ~0.04s per image), over 100x faster than the second competitor.","Second, thanks to the powerful priors, our module could focus on the learning of editing-related variations, such that it manages to handle various types of editing simultaneously in the training phase and further supports fast adaptation to user-specified novel types of editing during inference (e.g., with ~5min fine-tuning per case).","The code, the model, and the interface will be made publicly available to facilitate future research."],"url":"http://arxiv.org/abs/2402.14000v1","category":"cs.CV"}
{"created":"2024-02-21 18:21:21","title":"A General Theory of Static Response for Markov Jump Processes","abstract":"We consider Markov jump processes on a graph described by a rate matrix that depends on various control parameters. We derive explicit expressions for the static responses of edge currents and steady-state probabilities. We show that they are constrained by the graph topology (i.e. the incidence matrix) by deriving response relations (i.e. linear constraints linking the different responses) and topology-dependent bounds. For unicyclic networks, all scaled current sensitivities are between zero and one and must sum to one. Applying these results to stochastic thermodynamics, we derive explicit expressions for the static response of fundamental currents (which carry the full dissipation) to fundamental thermodynamic forces (which drive the system away from equilibrium).","sentences":["We consider Markov jump processes on a graph described by a rate matrix that depends on various control parameters.","We derive explicit expressions for the static responses of edge currents and steady-state probabilities.","We show that they are constrained by the graph topology (i.e. the incidence matrix) by deriving response relations (i.e. linear constraints linking the different responses) and topology-dependent bounds.","For unicyclic networks, all scaled current sensitivities are between zero and one and must sum to one.","Applying these results to stochastic thermodynamics, we derive explicit expressions for the static response of fundamental currents (which carry the full dissipation) to fundamental thermodynamic forces (which drive the system away from equilibrium)."],"url":"http://arxiv.org/abs/2402.13990v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-21 18:19:20","title":"FedADMM-InSa: An Inexact and Self-Adaptive ADMM for Federated Learning","abstract":"Federated learning (FL) is a promising framework for learning from distributed data while maintaining privacy. The development of efficient FL algorithms encounters various challenges, including heterogeneous data and systems, limited communication capacities, and constrained local computational resources. Recently developed FedADMM methods show great resilience to both data and system heterogeneity. However, they still suffer from performance deterioration if the hyperparameters are not carefully tuned. To address this issue, we propose an inexact and self-adaptive FedADMM algorithm, termed FedADMM-InSa. First, we design an inexactness criterion for the clients' local updates to eliminate the need for empirically setting the local training accuracy. This inexactness criterion can be assessed by each client independently based on its unique condition, thereby reducing the local computational cost and mitigating the undesirable straggle effect. The convergence of the resulting inexact ADMM is proved under the assumption of strongly convex loss functions. Additionally, we present a self-adaptive scheme that dynamically adjusts each client's penalty parameter, enhancing algorithm robustness by mitigating the need for empirical penalty parameter choices for each client. Extensive numerical experiments on both synthetic and real-world datasets are conducted. As validated by some numerical tests, our proposed algorithm can reduce the clients' local computational load significantly and also accelerate the learning process compared to the vanilla FedADMM.","sentences":["Federated learning (FL) is a promising framework for learning from distributed data while maintaining privacy.","The development of efficient FL algorithms encounters various challenges, including heterogeneous data and systems, limited communication capacities, and constrained local computational resources.","Recently developed FedADMM methods show great resilience to both data and system heterogeneity.","However, they still suffer from performance deterioration if the hyperparameters are not carefully tuned.","To address this issue, we propose an inexact and self-adaptive FedADMM algorithm, termed FedADMM-InSa.","First, we design an inexactness criterion for the clients' local updates to eliminate the need for empirically setting the local training accuracy.","This inexactness criterion can be assessed by each client independently based on its unique condition, thereby reducing the local computational cost and mitigating the undesirable straggle effect.","The convergence of the resulting inexact ADMM is proved under the assumption of strongly convex loss functions.","Additionally, we present a self-adaptive scheme that dynamically adjusts each client's penalty parameter, enhancing algorithm robustness by mitigating the need for empirical penalty parameter choices for each client.","Extensive numerical experiments on both synthetic and real-world datasets are conducted.","As validated by some numerical tests, our proposed algorithm can reduce the clients' local computational load significantly and also accelerate the learning process compared to the vanilla FedADMM."],"url":"http://arxiv.org/abs/2402.13989v1","category":"cs.LG"}
{"created":"2024-02-21 18:03:38","title":"Non-Markovian maximal couplings and a vertical reflection principle on a class of sub-Riemannian manifolds","abstract":"We develop non-Markovian, non-co-adapted couplings for sub-Riemannian Brownian motions in various sub-Riemannian manifolds, namely the three-dimensional Heisenberg group, higher-dimensional non-isotropic Heisenberg groups, SL(2,R) and its universal cover, and SU(2,C). Our primary focus is on the situation when the processes start from two points on the same vertical fiber, since in general co-adapted couplings cannot give the sharp rate for the coupling time in this case. Then for general points, we use this vertical coupling as the second stage of a two-stage coupling. Non-Markovian couplings in this context were first introduced by Banerjee-Gordina-Mariano, for the three-dimensional Heisenberg group, and were more recently extended by B\\'en\\'efice to SL(2,R) and SU(2,C), using a detailed consideration of the Brownian bridge. In contrast, our couplings are based on global isometries of the space, giving couplings that are maximal, as well as making the construction relatively simple and uniform across different manifolds. Moreover, this construction gives a coupling time that equals a hitting time for the vertical component of one of the Brownian motions, and this vertical component satisfies a reflection principle, which is useful in explicitly bounding the tail probability of the coupling time. We estimate the coupling time in these various situations and give applications to inequalities for the heat semigroup.","sentences":["We develop non-Markovian, non-co-adapted couplings for sub-Riemannian Brownian motions in various sub-Riemannian manifolds, namely the three-dimensional Heisenberg group, higher-dimensional non-isotropic Heisenberg groups, SL(2,R) and its universal cover, and SU(2,C).","Our primary focus is on the situation when the processes start from two points on the same vertical fiber, since in general co-adapted couplings cannot give the sharp rate for the coupling time in this case.","Then for general points, we use this vertical coupling as the second stage of a two-stage coupling.","Non-Markovian couplings in this context were first introduced by Banerjee-Gordina-Mariano, for the three-dimensional Heisenberg group, and were more recently extended by B\\'en\\'efice to SL(2,R) and SU(2,C), using a detailed consideration of the Brownian bridge.","In contrast, our couplings are based on global isometries of the space, giving couplings that are maximal, as well as making the construction relatively simple and uniform across different manifolds.","Moreover, this construction gives a coupling time that equals a hitting time for the vertical component of one of the Brownian motions, and this vertical component satisfies a reflection principle, which is useful in explicitly bounding the tail probability of the coupling time.","We estimate the coupling time in these various situations and give applications to inequalities for the heat semigroup."],"url":"http://arxiv.org/abs/2402.13976v1","category":"math.PR"}
{"created":"2024-02-21 17:47:20","title":"Towards Building Multilingual Language Model for Medicine","abstract":"In this paper, we aim to develop an open-source, multilingual language model for medicine, that the benefits a wider, linguistically diverse audience from different regions. In general, we present the contribution from the following aspects: first, for multilingual medical-specific adaptation, we construct a new multilingual medical corpus, that contains approximately 25.5B tokens encompassing 6 main languages, termed as MMedC, that enables auto-regressive training for existing general LLMs. second, to monitor the development of multilingual LLMs in medicine, we propose a new multilingual medical multi-choice question-answering benchmark with rationale, termed as MMedBench; third, we have assessed a number of popular, opensource large language models (LLMs) on our benchmark, along with those further auto-regressive trained on MMedC, as a result, our final model, termed as MMedLM 2, with only 7B parameters, achieves superior performance compared to all other open-source models, even rivaling GPT-4 on MMedBench. We will make the resources publicly available, including code, model weights, and datasets.","sentences":["In this paper, we aim to develop an open-source, multilingual language model for medicine, that the benefits a wider, linguistically diverse audience from different regions.","In general, we present the contribution from the following aspects: first, for multilingual medical-specific adaptation, we construct a new multilingual medical corpus, that contains approximately 25.5B tokens encompassing 6 main languages, termed as MMedC, that enables auto-regressive training for existing general LLMs.","second, to monitor the development of multilingual LLMs in medicine, we propose a new multilingual medical multi-choice question-answering benchmark with rationale, termed as MMedBench; third, we have assessed a number of popular, opensource large language models (LLMs) on our benchmark, along with those further auto-regressive trained on MMedC, as a result, our final model, termed as MMedLM 2, with only 7B parameters, achieves superior performance compared to all other open-source models, even rivaling GPT-4 on MMedBench.","We will make the resources publicly available, including code, model weights, and datasets."],"url":"http://arxiv.org/abs/2402.13963v1","category":"cs.CL"}
{"created":"2024-02-21 17:37:30","title":"Advancing Audio Fingerprinting Accuracy Addressing Background Noise and Distortion Challenges","abstract":"Audio fingerprinting, exemplified by pioneers like Shazam, has transformed digital audio recognition. However, existing systems struggle with accuracy in challenging conditions, limiting broad applicability. This research proposes an AI and ML integrated audio fingerprinting algorithm to enhance accuracy. Built on the Dejavu Project's foundations, the study emphasizes real-world scenario simulations with diverse background noises and distortions. Signal processing, central to Dejavu's model, includes the Fast Fourier Transform, spectrograms, and peak extraction. The \"constellation\" concept and fingerprint hashing enable unique song identification. Performance evaluation attests to 100% accuracy within a 5-second audio input, with a system showcasing predictable matching speed for efficiency. Storage analysis highlights the critical space-speed trade-off for practical implementation. This research advances audio fingerprinting's adaptability, addressing challenges in varied environments and applications.","sentences":["Audio fingerprinting, exemplified by pioneers like Shazam, has transformed digital audio recognition.","However, existing systems struggle with accuracy in challenging conditions, limiting broad applicability.","This research proposes an AI and ML integrated audio fingerprinting algorithm to enhance accuracy.","Built on the Dejavu Project's foundations, the study emphasizes real-world scenario simulations with diverse background noises and distortions.","Signal processing, central to Dejavu's model, includes the Fast Fourier Transform, spectrograms, and peak extraction.","The \"constellation\" concept and fingerprint hashing enable unique song identification.","Performance evaluation attests to 100% accuracy within a 5-second audio input, with a system showcasing predictable matching speed for efficiency.","Storage analysis highlights the critical space-speed trade-off for practical implementation.","This research advances audio fingerprinting's adaptability, addressing challenges in varied environments and applications."],"url":"http://arxiv.org/abs/2402.13957v1","category":"cs.SD"}
{"created":"2024-02-21 16:59:53","title":"Tumor segmentation on whole slide images: training or prompting?","abstract":"Tumor segmentation stands as a pivotal task in cancer diagnosis. Given the immense dimensions of whole slide images (WSI) in histology, deep learning approaches for WSI classification mainly operate at patch-wise or superpixel-wise level. However, these solutions often struggle to capture global WSI information and cannot directly generate the binary mask. Downsampling the WSI and performing semantic segmentation is another possible approach. While this method offers computational efficiency, it necessitates a large amount of annotated data since resolution reduction may lead to information loss. Visual prompting is a novel paradigm that allows the model to perform new tasks by making subtle modifications to the input space, rather than adapting the model itself. Such approach has demonstrated promising results on many computer vision tasks. In this paper, we show the efficacy of visual prompting in the context of tumor segmentation for three distinct organs. In comparison to classical methods trained for this specific task, our findings reveal that, with appropriate prompt examples, visual prompting can achieve comparable or better performance without extensive fine-tuning.","sentences":["Tumor segmentation stands as a pivotal task in cancer diagnosis.","Given the immense dimensions of whole slide images (WSI) in histology, deep learning approaches for WSI classification mainly operate at patch-wise or superpixel-wise level.","However, these solutions often struggle to capture global WSI information and cannot directly generate the binary mask.","Downsampling the WSI and performing semantic segmentation is another possible approach.","While this method offers computational efficiency, it necessitates a large amount of annotated data since resolution reduction may lead to information loss.","Visual prompting is a novel paradigm that allows the model to perform new tasks by making subtle modifications to the input space, rather than adapting the model itself.","Such approach has demonstrated promising results on many computer vision tasks.","In this paper, we show the efficacy of visual prompting in the context of tumor segmentation for three distinct organs.","In comparison to classical methods trained for this specific task, our findings reveal that, with appropriate prompt examples, visual prompting can achieve comparable or better performance without extensive fine-tuning."],"url":"http://arxiv.org/abs/2402.13932v1","category":"cs.CV"}
{"created":"2024-02-21 16:52:26","title":"Enhancing Reinforcement Learning Agents with Local Guides","abstract":"This paper addresses the problem of integrating local guide policies into a Reinforcement Learning agent. For this, we show how to adapt existing algorithms to this setting before introducing a novel algorithm based on a noisy policy-switching procedure. This approach builds on a proper Approximate Policy Evaluation (APE) scheme to provide a perturbation that carefully leads the local guides towards better actions. We evaluated our method on a set of classical Reinforcement Learning problems, including safety-critical systems where the agent cannot enter some areas at the risk of triggering catastrophic consequences. In all the proposed environments, our agent proved to be efficient at leveraging those policies to improve the performance of any APE-based Reinforcement Learning algorithm, especially in its first learning stages.","sentences":["This paper addresses the problem of integrating local guide policies into a Reinforcement Learning agent.","For this, we show how to adapt existing algorithms to this setting before introducing a novel algorithm based on a noisy policy-switching procedure.","This approach builds on a proper Approximate Policy Evaluation (APE) scheme to provide a perturbation that carefully leads the local guides towards better actions.","We evaluated our method on a set of classical Reinforcement Learning problems, including safety-critical systems where the agent cannot enter some areas at the risk of triggering catastrophic consequences.","In all the proposed environments, our agent proved to be efficient at leveraging those policies to improve the performance of any APE-based Reinforcement Learning algorithm, especially in its first learning stages."],"url":"http://arxiv.org/abs/2402.13930v1","category":"cs.LG"}
{"created":"2024-02-21 16:48:38","title":"Supporting the next generation lithography roadmap using partial state-feedback reduced-order switching predictive models","abstract":"To support the ever-increasing performance requirements of lithography systems in terms of throughput and accuracy, in this paper, we introduce a design framework for partial state-feedback reduced-order switching predictive models. By combining measurements and predictions, this method aims to: 1) improve overall system performance by reducing the placement errors in a die within and across the full-wafer and 2) eliminate redundant measurements by using predictions to improve system throughput. We primarily focus on well-known trade-off introduced by measurement time, which can correct errors at a cost of throughput, noise and not being robust to nonlinearities. The proposed predictive model consists of a reduced-order model with a switching logic that acts a scheduler to deal with uncertain operating conditions. The utilization of linear predictive models as a basis for the control design appeals to the ease and cost of implementation therewith enhancing the applicability. For the add-on part, the scheduler logic is adapted based on expected operating conditions of the system while guaranteeing global uniform ultimate bounded asymptotic stability. Lastly, to deal with measurement layouts, the predictor combines the measurement into model using partial state-feedback. Effectiveness of the proposed strategy is demonstrated in practice on a high-precision industrial scanner.","sentences":["To support the ever-increasing performance requirements of lithography systems in terms of throughput and accuracy, in this paper, we introduce a design framework for partial state-feedback reduced-order switching predictive models.","By combining measurements and predictions, this method aims to: 1) improve overall system performance by reducing the placement errors in a die within and across the full-wafer and 2) eliminate redundant measurements by using predictions to improve system throughput.","We primarily focus on well-known trade-off introduced by measurement time, which can correct errors at a cost of throughput, noise and not being robust to nonlinearities.","The proposed predictive model consists of a reduced-order model with a switching logic that acts a scheduler to deal with uncertain operating conditions.","The utilization of linear predictive models as a basis for the control design appeals to the ease and cost of implementation therewith enhancing the applicability.","For the add-on part, the scheduler logic is adapted based on expected operating conditions of the system while guaranteeing global uniform ultimate bounded asymptotic stability.","Lastly, to deal with measurement layouts, the predictor combines the measurement into model using partial state-feedback.","Effectiveness of the proposed strategy is demonstrated in practice on a high-precision industrial scanner."],"url":"http://arxiv.org/abs/2402.13928v1","category":"math.OC"}
{"created":"2024-02-21 16:32:43","title":"BenchCloudVision: A Benchmark Analysis of Deep Learning Approaches for Cloud Detection and Segmentation in Remote Sensing Imagery","abstract":"Satellites equipped with optical sensors capture high-resolution imagery, providing valuable insights into various environmental phenomena. In recent years, there has been a surge of research focused on addressing some challenges in remote sensing, ranging from water detection in diverse landscapes to the segmentation of mountainous and terrains. Ongoing investigations goals to enhance the precision and efficiency of satellite imagery analysis. Especially, there is a growing emphasis on developing methodologies for accurate water body detection, snow and clouds, important for environmental monitoring, resource management, and disaster response. Within this context, this paper focus on the cloud segmentation from remote sensing imagery. Accurate remote sensing data analysis can be challenging due to the presence of clouds in optical sensor-based applications. The quality of resulting products such as applications and research is directly impacted by cloud detection, which plays a key role in the remote sensing data processing pipeline. This paper examines seven cutting-edge semantic segmentation and detection algorithms applied to clouds identification, conducting a benchmark analysis to evaluate their architectural approaches and identify the most performing ones. To increase the model's adaptability, critical elements including the type of imagery and the amount of spectral bands used during training are analyzed. Additionally, this research tries to produce machine learning algorithms that can perform cloud segmentation using only a few spectral bands, including RGB and RGBN-IR combinations. The model's flexibility for a variety of applications and user scenarios is assessed by using imagery from Sentinel-2 and Landsat-8 as datasets. This benchmark can be reproduced using the material from this github link: \\url{https://github.com/toelt-llc/cloud\\_segmentation\\_comparative}.","sentences":["Satellites equipped with optical sensors capture high-resolution imagery, providing valuable insights into various environmental phenomena.","In recent years, there has been a surge of research focused on addressing some challenges in remote sensing, ranging from water detection in diverse landscapes to the segmentation of mountainous and terrains.","Ongoing investigations goals to enhance the precision and efficiency of satellite imagery analysis.","Especially, there is a growing emphasis on developing methodologies for accurate water body detection, snow and clouds, important for environmental monitoring, resource management, and disaster response.","Within this context, this paper focus on the cloud segmentation from remote sensing imagery.","Accurate remote sensing data analysis can be challenging due to the presence of clouds in optical sensor-based applications.","The quality of resulting products such as applications and research is directly impacted by cloud detection, which plays a key role in the remote sensing data processing pipeline.","This paper examines seven cutting-edge semantic segmentation and detection algorithms applied to clouds identification, conducting a benchmark analysis to evaluate their architectural approaches and identify the most performing ones.","To increase the model's adaptability, critical elements including the type of imagery and the amount of spectral bands used during training are analyzed.","Additionally, this research tries to produce machine learning algorithms that can perform cloud segmentation using only a few spectral bands, including RGB and RGBN-IR combinations.","The model's flexibility for a variety of applications and user scenarios is assessed by using imagery from Sentinel-2 and Landsat-8 as datasets.","This benchmark can be reproduced using the material from this github link: \\url{https://github.com/toelt-llc/cloud\\_segmentation\\_comparative}."],"url":"http://arxiv.org/abs/2402.13918v1","category":"cs.CV"}
{"created":"2024-02-21 16:02:14","title":"Overcoming Saturation in Density Ratio Estimation by Iterated Regularization","abstract":"Estimating the ratio of two probability densities from finitely many samples, is a central task in machine learning and statistics. In this work, we show that a large class of kernel methods for density ratio estimation suffers from error saturation, which prevents algorithms from achieving fast error convergence rates on highly regular learning problems. To resolve saturation, we introduce iterated regularization in density ratio estimation to achieve fast error rates. Our methods outperform its non-iteratively regularized versions on benchmarks for density ratio estimation as well as on large-scale evaluations for importance-weighted ensembling of deep unsupervised domain adaptation models.","sentences":["Estimating the ratio of two probability densities from finitely many samples, is a central task in machine learning and statistics.","In this work, we show that a large class of kernel methods for density ratio estimation suffers from error saturation, which prevents algorithms from achieving fast error convergence rates on highly regular learning problems.","To resolve saturation, we introduce iterated regularization in density ratio estimation to achieve fast error rates.","Our methods outperform its non-iteratively regularized versions on benchmarks for density ratio estimation as well as on large-scale evaluations for importance-weighted ensembling of deep unsupervised domain adaptation models."],"url":"http://arxiv.org/abs/2402.13891v1","category":"cs.LG"}
{"created":"2024-02-21 16:01:06","title":"A unified Bayesian framework for interval hypothesis testing in clinical trials","abstract":"The American Statistical Association (ASA) statement on statistical significance and P-values \\cite{wasserstein2016asa} cautioned statisticians against making scientific decisions solely on the basis of traditional P-values. The statement delineated key issues with P-values, including a lack of transparency, an inability to quantify evidence in support of the null hypothesis, and an inability to measure the size of an effect or the importance of a result. In this article, we demonstrate that the interval null hypothesis framework (instead of the point null hypothesis framework), when used in tandem with Bayes factor-based tests, is instrumental in circumnavigating the key issues of P-values. Further, we note that specifying prior densities for Bayes factors is challenging and has been a reason for criticism of Bayesian hypothesis testing in existing literature. We address this by adapting Bayes factors directly based on common test statistics. We demonstrate, through numerical experiments and real data examples, that the proposed Bayesian interval hypothesis testing procedures can be calibrated to ensure frequentist error control while retaining their inherent interpretability. Finally, we illustrate the improved flexibility and applicability of the proposed methods by providing coherent frameworks for competitive landscape analysis and end-to-end Bayesian hypothesis tests in the context of reporting clinical trial outcomes.","sentences":["The American Statistical Association (ASA) statement on statistical significance and P-values \\cite{wasserstein2016asa} cautioned statisticians against making scientific decisions solely on the basis of traditional P-values.","The statement delineated key issues with P-values, including a lack of transparency, an inability to quantify evidence in support of the null hypothesis, and an inability to measure the size of an effect or the importance of a result.","In this article, we demonstrate that the interval null hypothesis framework (instead of the point null hypothesis framework), when used in tandem with Bayes factor-based tests, is instrumental in circumnavigating the key issues of P-values.","Further, we note that specifying prior densities for Bayes factors is challenging and has been a reason for criticism of Bayesian hypothesis testing in existing literature.","We address this by adapting Bayes factors directly based on common test statistics.","We demonstrate, through numerical experiments and real data examples, that the proposed Bayesian interval hypothesis testing procedures can be calibrated to ensure frequentist error control while retaining their inherent interpretability.","Finally, we illustrate the improved flexibility and applicability of the proposed methods by providing coherent frameworks for competitive landscape analysis and end-to-end Bayesian hypothesis tests in the context of reporting clinical trial outcomes."],"url":"http://arxiv.org/abs/2402.13890v1","category":"stat.ME"}
{"created":"2024-02-21 15:35:04","title":"$\\texttt{Se}^2$: $\\textit{Se}$quential Example $\\textit{Se}$lection for In-Context Learning","abstract":"The remarkable capability of large language models (LLMs) for in-context learning (ICL) needs to be activated by demonstration examples. Prior work has extensively explored the selection of examples for ICL, predominantly following the \"select then organize\" paradigm, such approaches often neglect the internal relationships between examples and exist an inconsistency between the training and inference. In this paper, we formulate the problem as a $\\textit{se}$quential $\\textit{se}$lection problem and introduce $\\texttt{Se}^2$, a sequential-aware method that leverages the LLM's feedback on varying context, aiding in capturing inter-relationships and sequential information among examples, significantly enriching the contextuality and relevance of ICL prompts. Meanwhile, we utilize beam search to seek and construct example sequences, enhancing both quality and diversity. Extensive experiments across 23 NLP tasks from 8 distinct categories illustrate that $\\texttt{Se}^2$ markedly surpasses competitive baselines and achieves 42% relative improvement over random selection. Further in-depth analysis show the effectiveness of proposed strategies, highlighting $\\texttt{Se}^2$'s exceptional stability and adaptability across various scenarios. Our code will be released to facilitate future research.","sentences":["The remarkable capability of large language models (LLMs) for in-context learning (ICL) needs to be activated by demonstration examples.","Prior work has extensively explored the selection of examples for ICL, predominantly following the \"select then organize\" paradigm, such approaches often neglect the internal relationships between examples and exist an inconsistency between the training and inference.","In this paper, we formulate the problem as a $\\textit{se}$quential $\\textit{se}$lection problem and introduce $\\texttt{Se}^2$, a sequential-aware method that leverages the LLM's feedback on varying context, aiding in capturing inter-relationships and sequential information among examples, significantly enriching the contextuality and relevance of ICL prompts.","Meanwhile, we utilize beam search to seek and construct example sequences, enhancing both quality and diversity.","Extensive experiments across 23 NLP tasks from 8 distinct categories illustrate that $\\texttt{Se}^2$ markedly surpasses competitive baselines and achieves 42% relative improvement over random selection.","Further in-depth analysis show the effectiveness of proposed strategies, highlighting $\\texttt{Se}^2$'s exceptional stability and adaptability across various scenarios.","Our code will be released to facilitate future research."],"url":"http://arxiv.org/abs/2402.13874v1","category":"cs.CL"}
{"created":"2024-02-21 13:59:55","title":"MSTAR: Multi-Scale Backbone Architecture Search for Timeseries Classification","abstract":"Most of the previous approaches to Time Series Classification (TSC) highlight the significance of receptive fields and frequencies while overlooking the time resolution. Hence, unavoidably suffered from scalability issues as they integrated an extensive range of receptive fields into classification models. Other methods, while having a better adaptation for large datasets, require manual design and yet not being able to reach the optimal architecture due to the uniqueness of each dataset. We overcome these challenges by proposing a novel multi-scale search space and a framework for Neural architecture search (NAS), which addresses both the problem of frequency and time resolution, discovering the suitable scale for a specific dataset. We further show that our model can serve as a backbone to employ a powerful Transformer module with both untrained and pre-trained weights. Our search space reaches the state-of-the-art performance on four datasets on four different domains while introducing more than ten highly fine-tuned models for each data.","sentences":["Most of the previous approaches to Time Series Classification (TSC) highlight the significance of receptive fields and frequencies while overlooking the time resolution.","Hence, unavoidably suffered from scalability issues as they integrated an extensive range of receptive fields into classification models.","Other methods, while having a better adaptation for large datasets, require manual design and yet not being able to reach the optimal architecture due to the uniqueness of each dataset.","We overcome these challenges by proposing a novel multi-scale search space and a framework for Neural architecture search (NAS), which addresses both the problem of frequency and time resolution, discovering the suitable scale for a specific dataset.","We further show that our model can serve as a backbone to employ a powerful Transformer module with both untrained and pre-trained weights.","Our search space reaches the state-of-the-art performance on four datasets on four different domains while introducing more than ten highly fine-tuned models for each data."],"url":"http://arxiv.org/abs/2402.13822v1","category":"cs.CV"}
{"created":"2024-02-21 13:55:48","title":"A unified framework of non-local parametric methods for image denoising","abstract":"We propose a unified view of non-local methods for single-image denoising, for which BM3D is the most popular representative, that operate by gathering noisy patches together according to their similarities in order to process them collaboratively. Our general estimation framework is based on the minimization of the quadratic risk, which is approximated in two steps, and adapts to photon and electronic noises. Relying on unbiased risk estimation (URE) for the first step and on ``internal adaptation'', a concept borrowed from deep learning theory, for the second, we show that our approach enables to reinterpret and reconcile previous state-of-the-art non-local methods. Within this framework, we propose a novel denoiser called NL-Ridge that exploits linear combinations of patches. While conceptually simpler, we show that NL-Ridge can outperform well-established state-of-the-art single-image denoisers.","sentences":["We propose a unified view of non-local methods for single-image denoising, for which BM3D is the most popular representative, that operate by gathering noisy patches together according to their similarities in order to process them collaboratively.","Our general estimation framework is based on the minimization of the quadratic risk, which is approximated in two steps, and adapts to photon and electronic noises.","Relying on unbiased risk estimation (URE) for the first step and on ``internal adaptation'', a concept borrowed from deep learning theory, for the second, we show that our approach enables to reinterpret and reconcile previous state-of-the-art non-local methods.","Within this framework, we propose a novel denoiser called NL-Ridge that exploits linear combinations of patches.","While conceptually simpler, we show that NL-Ridge can outperform well-established state-of-the-art single-image denoisers."],"url":"http://arxiv.org/abs/2402.13816v1","category":"cs.CV"}
{"created":"2024-02-21 13:27:14","title":"AFPR-CIM: An Analog-Domain Floating-Point RRAM-based Compute-In-Memory Architecture with Dynamic Range Adaptive FP-ADC","abstract":"Power consumption has become the major concern in neural network accelerators for edge devices. The novel non-volatile-memory (NVM) based computing-in-memory (CIM) architecture has shown great potential for better energy efficiency. However, most of the recent NVM-CIM solutions mainly focus on fixed-point calculation and are not applicable to floating-point (FP) processing. In this paper, we propose an analog-domain floating-point CIM architecture (AFPR-CIM) based on resistive random-access memory (RRAM). A novel adaptive dynamic-range FP-ADC is designed to convert the analog computation results into FP codes. Output current with high dynamic range is converted to a normalized voltage range for readout, to prevent precision loss at low power consumption. Moreover, a novel FP-DAC is also implemented which reconstructs FP digital codes into analog values to perform analog computation. The proposed AFPR-CIM architecture enables neural network acceleration with FP8 (E2M5) activation for better accuracy and energy efficiency. Evaluation results show that AFPR-CIM can achieve 19.89 TFLOPS/W energy efficiency and 1474.56 GOPS throughput. Compared to traditional FP8 accelerator, digital FP-CIM, and analog INT8-CIM, this work achieves 4.135x, 5.376x, and 2.841x energy efficiency enhancement, respectively.","sentences":["Power consumption has become the major concern in neural network accelerators for edge devices.","The novel non-volatile-memory (NVM) based computing-in-memory (CIM) architecture has shown great potential for better energy efficiency.","However, most of the recent NVM-CIM solutions mainly focus on fixed-point calculation and are not applicable to floating-point (FP) processing.","In this paper, we propose an analog-domain floating-point CIM architecture (AFPR-CIM) based on resistive random-access memory (RRAM).","A novel adaptive dynamic-range FP-ADC is designed to convert the analog computation results into FP codes.","Output current with high dynamic range is converted to a normalized voltage range for readout, to prevent precision loss at low power consumption.","Moreover, a novel FP-DAC is also implemented which reconstructs FP digital codes into analog values to perform analog computation.","The proposed AFPR-CIM architecture enables neural network acceleration with FP8 (E2M5) activation for better accuracy and energy efficiency.","Evaluation results show that AFPR-CIM can achieve 19.89 TFLOPS/W energy efficiency and 1474.56 GOPS throughput.","Compared to traditional FP8 accelerator, digital FP-CIM, and analog INT8-CIM, this work achieves 4.135x, 5.376x, and 2.841x energy efficiency enhancement, respectively."],"url":"http://arxiv.org/abs/2402.13798v1","category":"eess.SY"}
{"created":"2024-02-21 13:16:11","title":"A Unifying Theory for Runge--Kutta-like Time Integrators: Convergence and Stability","abstract":"The work deals with two major topics concerning the numerical analysis of Runge-Kutta-like (RK-like) methods, namely their stability and order of convergence. RK-like methods differ from additive RK methods in that their coefficients are allowed to depend on the solution and the step size. As a result of this, we also refer to them as non-standard additive RK (NSARK) methods. The first major part of this thesis is dedicated to providing a tool for deriving order conditions for NSARK methods. The proposed approach may yield implicit order conditions, which can be rewritten in explicit form using the NB-series of the stages. The obtained explicit order conditions can be further reduced using Gr\\\"obner bases computations. With the presented approach, it was possible for the first time to obtain conditions for the construction of 3rd and 4th order GeCo as well as 4th order MPRK schemes. Moreover, a new fourth order MPRK method is constructed using our theory and the order of convergence is validated numerically. The second major part is concerned with the stability of nonlinear time integrators preserving at least one linear invariant. We discuss how the given approach generalizes the notion of A-stability. We can prove that investigating the Jacobian of the generating map is sufficient to understand the stability of the nonlinear method in a neighborhood of the steady state. This approach allows for the first time the investigation of several modified Patankar. In the case of MPRK schemes, we compute a general stability function in a way that can be easily adapted to the case of PDRS. Finally, the approach from the theory of dynamical systems is used to derive a necessary condition for avoiding unrealistic oscillations of the numerical approximation.","sentences":["The work deals with two major topics concerning the numerical analysis of Runge-Kutta-like (RK-like) methods, namely their stability and order of convergence.","RK-like methods differ from additive RK methods in that their coefficients are allowed to depend on the solution and the step size.","As a result of this, we also refer to them as non-standard additive RK (NSARK) methods.","The first major part of this thesis is dedicated to providing a tool for deriving order conditions for NSARK methods.","The proposed approach may yield implicit order conditions, which can be rewritten in explicit form using the NB-series of the stages.","The obtained explicit order conditions can be further reduced using Gr\\\"obner bases computations.","With the presented approach, it was possible for the first time to obtain conditions for the construction of 3rd and 4th order GeCo as well as 4th order MPRK schemes.","Moreover, a new fourth order MPRK method is constructed using our theory and the order of convergence is validated numerically.","The second major part is concerned with the stability of nonlinear time integrators preserving at least one linear invariant.","We discuss how the given approach generalizes the notion of A-stability.","We can prove that investigating the Jacobian of the generating map is sufficient to understand the stability of the nonlinear method in a neighborhood of the steady state.","This approach allows for the first time the investigation of several modified Patankar.","In the case of MPRK schemes, we compute a general stability function in a way that can be easily adapted to the case of PDRS.","Finally, the approach from the theory of dynamical systems is used to derive a necessary condition for avoiding unrealistic oscillations of the numerical approximation."],"url":"http://arxiv.org/abs/2402.13788v1","category":"math.NA"}
{"created":"2024-02-21 12:50:15","title":"Parameter identification algorithm for a LTV system with partially unknown state matrix","abstract":"In this paper an adaptive state observer and parameter identification algorithm for a linear time-varying system are developed under condition that the state matrix of the system contains unknown time-varying parameters of a known form. The state vector is observed using only output and input measurements without identification of the unknown parameters. When the state vector estimate is obtained, the identification algorithm is applied to find unknown parameters of the system.","sentences":["In this paper an adaptive state observer and parameter identification algorithm for a linear time-varying system are developed under condition that the state matrix of the system contains unknown time-varying parameters of a known form.","The state vector is observed using only output and input measurements without identification of the unknown parameters.","When the state vector estimate is obtained, the identification algorithm is applied to find unknown parameters of the system."],"url":"http://arxiv.org/abs/2402.13772v1","category":"eess.SY"}
{"created":"2024-02-21 12:33:34","title":"Adaptive Massively Parallel Coloring in Sparse Graphs","abstract":"Classic symmetry-breaking problems on graphs have gained a lot of attention in models of modern parallel computation. The Adaptive Massively Parallel Computation (AMPC) is a model that captures central challenges in data center computations. Chang et al. [PODC'2019] gave an extremely fast, constant time, algorithm for the $(\\Delta + 1)$-coloring problem, where $\\Delta$ is the maximum degree of an input graph of $n$ nodes. The algorithm works in the most restrictive low-space setting, where each machine has $n^{\\delta}$ local space for a constant $0 < \\delta < 1$.   In this work, we study the vertex-coloring problem in sparse graphs parameterized by their arboricity $\\alpha$, a standard measure for sparsity. We give deterministic algorithms that in constant, or almost constant, time give $\\text{poly}(\\alpha)$ and $O(\\alpha)$-colorings, where $\\alpha$ can be arbitrarily smaller than $\\Delta$. A strong and standard approach to compute arboricity-dependent colorings is through the Nash-Williams forest decomposition, which gives rise to an (acyclic) orientation of the edges such that each node has a small outdegree.   Our main technical contribution is giving efficient deterministic algorithms to compute these orientations and showing how to leverage them to find colorings in low-space AMPC. A key technical challenge is that the color of a node may depend on almost all of the other nodes in the graph and these dependencies cannot be stored on a single machine. Nevertheless, our novel and careful exploration technique yields the orientation, and the arboricity-dependent coloring, with a sublinear number of adaptive queries per node.","sentences":["Classic symmetry-breaking problems on graphs have gained a lot of attention in models of modern parallel computation.","The Adaptive Massively Parallel Computation (AMPC) is a model that captures central challenges in data center computations.","Chang et al.","[PODC'2019] gave an extremely fast, constant time, algorithm for the $(\\Delta + 1)$-coloring problem, where $\\Delta$ is the maximum degree of an input graph of $n$ nodes.","The algorithm works in the most restrictive low-space setting, where each machine has $n^{\\delta}$ local space for a constant $0 <","\\delta < 1$.   ","In this work, we study the vertex-coloring problem in sparse graphs parameterized by their arboricity $\\alpha$, a standard measure for sparsity.","We give deterministic algorithms that in constant, or almost constant, time give $\\text{poly}(\\alpha)$ and $O(\\alpha)$-colorings, where $\\alpha$ can be arbitrarily smaller than $\\Delta$. A strong and standard approach to compute arboricity-dependent colorings is through the Nash-Williams forest decomposition, which gives rise to an (acyclic) orientation of the edges such that each node has a small outdegree.   ","Our main technical contribution is giving efficient deterministic algorithms to compute these orientations and showing how to leverage them to find colorings in low-space AMPC.","A key technical challenge is that the color of a node may depend on almost all of the other nodes in the graph and these dependencies cannot be stored on a single machine.","Nevertheless, our novel and careful exploration technique yields the orientation, and the arboricity-dependent coloring, with a sublinear number of adaptive queries per node."],"url":"http://arxiv.org/abs/2402.13755v1","category":"cs.DC"}
{"created":"2024-02-21 12:10:57","title":"Hidden Gems in the Rough: Computational Notebooks as an Uncharted Oasis for IDEs","abstract":"In this paper, we outline potential ways for the further development of computational notebooks in Integrated Development Environments (IDEs). We discuss notebooks integration with IDEs, focusing on three main areas: facilitating experimentation, adding collaborative features, and improving code comprehension. We propose that better support of notebooks will not only benefit the notebooks, but also enhance IDEs by supporting new development processes native to notebooks. In conclusion, we suggest that adapting IDEs for more experimentation-oriented notebook processes will prepare them for the future of AI-powered programming.","sentences":["In this paper, we outline potential ways for the further development of computational notebooks in Integrated Development Environments (IDEs).","We discuss notebooks integration with IDEs, focusing on three main areas: facilitating experimentation, adding collaborative features, and improving code comprehension.","We propose that better support of notebooks will not only benefit the notebooks, but also enhance IDEs by supporting new development processes native to notebooks.","In conclusion, we suggest that adapting IDEs for more experimentation-oriented notebook processes will prepare them for the future of AI-powered programming."],"url":"http://arxiv.org/abs/2402.13739v1","category":"cs.SE"}
{"created":"2024-02-21 11:33:09","title":"Exploiting Adaptive Contextual Masking for Aspect-Based Sentiment Analysis","abstract":"Aspect-Based Sentiment Analysis (ABSA) is a fine-grained linguistics problem that entails the extraction of multifaceted aspects, opinions, and sentiments from the given text. Both standalone and compound ABSA tasks have been extensively used in the literature to examine the nuanced information present in online reviews and social media posts. Current ABSA methods often rely on static hyperparameters for attention-masking mechanisms, which can struggle with context adaptation and may overlook the unique relevance of words in varied situations. This leads to challenges in accurately analyzing complex sentences containing multiple aspects with differing sentiments. In this work, we present adaptive masking methods that remove irrelevant tokens based on context to assist in Aspect Term Extraction and Aspect Sentiment Classification subtasks of ABSA. We show with our experiments that the proposed methods outperform the baseline methods in terms of accuracy and F1 scores on four benchmark online review datasets. Further, we show that the proposed methods can be extended with multiple adaptations and demonstrate a qualitative analysis of the proposed approach using sample text for aspect term extraction.","sentences":["Aspect-Based Sentiment Analysis (ABSA) is a fine-grained linguistics problem that entails the extraction of multifaceted aspects, opinions, and sentiments from the given text.","Both standalone and compound ABSA tasks have been extensively used in the literature to examine the nuanced information present in online reviews and social media posts.","Current ABSA methods often rely on static hyperparameters for attention-masking mechanisms, which can struggle with context adaptation and may overlook the unique relevance of words in varied situations.","This leads to challenges in accurately analyzing complex sentences containing multiple aspects with differing sentiments.","In this work, we present adaptive masking methods that remove irrelevant tokens based on context to assist in Aspect Term Extraction and Aspect Sentiment Classification subtasks of ABSA.","We show with our experiments that the proposed methods outperform the baseline methods in terms of accuracy and F1 scores on four benchmark online review datasets.","Further, we show that the proposed methods can be extended with multiple adaptations and demonstrate a qualitative analysis of the proposed approach using sample text for aspect term extraction."],"url":"http://arxiv.org/abs/2402.13722v1","category":"cs.CL"}
{"created":"2024-02-21 11:30:20","title":"Neeko: Leveraging Dynamic LoRA for Efficient Multi-Character Role-Playing Agent","abstract":"Large Language Models (LLMs) have revolutionized open-domain dialogue agents but encounter challenges in multi-character role-playing (MCRP) scenarios. To address the issue, we present Neeko, an innovative framework designed for efficient multiple characters imitation. Unlike existing methods, Neeko employs a dynamic low-rank adapter (LoRA) strategy, enabling it to adapt seamlessly to diverse characters. Our framework breaks down the role-playing process into agent pre-training, multiple characters playing, and character incremental learning, effectively handling both seen and unseen roles. This dynamic approach, coupled with distinct LoRA blocks for each character, enhances Neeko's adaptability to unique attributes, personalities, and speaking patterns. As a result, Neeko demonstrates superior performance in MCRP over most existing methods, offering more engaging and versatile user interaction experiences. Code and data are available at https://github.com/weiyifan1023/Neeko.","sentences":["Large Language Models (LLMs) have revolutionized open-domain dialogue agents but encounter challenges in multi-character role-playing (MCRP) scenarios.","To address the issue, we present Neeko, an innovative framework designed for efficient multiple characters imitation.","Unlike existing methods, Neeko employs a dynamic low-rank adapter (LoRA) strategy, enabling it to adapt seamlessly to diverse characters.","Our framework breaks down the role-playing process into agent pre-training, multiple characters playing, and character incremental learning, effectively handling both seen and unseen roles.","This dynamic approach, coupled with distinct LoRA blocks for each character, enhances Neeko's adaptability to unique attributes, personalities, and speaking patterns.","As a result, Neeko demonstrates superior performance in MCRP over most existing methods, offering more engaging and versatile user interaction experiences.","Code and data are available at https://github.com/weiyifan1023/Neeko."],"url":"http://arxiv.org/abs/2402.13717v1","category":"cs.CL"}
{"created":"2024-02-21 11:29:03","title":"Probabilistic Constellation Shaping for Enhancing Spectral Efficiency in NOMA VLC Systems","abstract":"The limited modulation bandwidth of the light emitting diodes (LEDs) presents a challenge in the development of practical high-data-rate visible light communication (VLC) systems. In this paper, a novel adaptive coded probabilistic shaping (PS)-based nonorthogonal multiple access (NOMA) scheme is proposed to improve spectral efficiency (SE) of VLC systems in multiuser uplink communication scenarios. The proposed scheme adapts its rate to the optical signal-to-noise ratio (OSNR) by utilizing non-uniformly distributed discrete constellation symbols and low complexity channel encoder. Furthermore, an alternate optimization algorithm is proposed to determine the optimal channel coding rate, constellation spacing, and probability mass function (PMF) of each user. The extensive numerical results show that the proposed PS-based NOMA scheme closely approaches the capacity of NOMA with fine granularity. Presented results demonstrate the effectiveness of our scheme in improving the SE of VLC systems in multiuser scenarios. For instance, our scheme exhibits substantial SE gains over existing schemes, namely, the pairwise coded modulation (PCM), geometric shaping (GS), and uniform-distribution schemes. These findings highlight the potential of our approach to significantly enhance VLC systems.","sentences":["The limited modulation bandwidth of the light emitting diodes (LEDs) presents a challenge in the development of practical high-data-rate visible light communication (VLC) systems.","In this paper, a novel adaptive coded probabilistic shaping (PS)-based nonorthogonal multiple access (NOMA) scheme is proposed to improve spectral efficiency (SE) of VLC systems in multiuser uplink communication scenarios.","The proposed scheme adapts its rate to the optical signal-to-noise ratio (OSNR) by utilizing non-uniformly distributed discrete constellation symbols and low complexity channel encoder.","Furthermore, an alternate optimization algorithm is proposed to determine the optimal channel coding rate, constellation spacing, and probability mass function (PMF) of each user.","The extensive numerical results show that the proposed PS-based NOMA scheme closely approaches the capacity of NOMA with fine granularity.","Presented results demonstrate the effectiveness of our scheme in improving the SE of VLC systems in multiuser scenarios.","For instance, our scheme exhibits substantial SE gains over existing schemes, namely, the pairwise coded modulation (PCM), geometric shaping (GS), and uniform-distribution schemes.","These findings highlight the potential of our approach to significantly enhance VLC systems."],"url":"http://arxiv.org/abs/2402.13715v1","category":"cs.IT"}
{"created":"2024-02-21 11:07:07","title":"Investigating Multilingual Instruction-Tuning: Do Polyglot Models Demand for Multilingual Instructions?","abstract":"The adaption of multilingual pre-trained Large Language Models (LLMs) into eloquent and helpful assistants is essential to facilitate their use across different language regions. In that spirit, we are the first to conduct an extensive study of the performance of multilingual models on parallel, multi-turn instruction-tuning benchmarks across a selection of the most-spoken Indo-European languages. We systematically examine the effects of language and instruction dataset size on a mid-sized, multilingual LLM by instruction-tuning it on parallel instruction-tuning datasets. Our results demonstrate that instruction-tuning on parallel instead of monolingual corpora benefits cross-lingual instruction following capabilities by up to 4.6%. Furthermore, we show that the Superficial Alignment Hypothesis does not hold in general, as the investigated multilingual 7B parameter model presents a counter-example requiring large-scale instruction-tuning datasets. Finally, we conduct a human annotation study to understand the alignment between human-based and GPT-4-based evaluation within multilingual chat scenarios.","sentences":["The adaption of multilingual pre-trained Large Language Models (LLMs) into eloquent and helpful assistants is essential to facilitate their use across different language regions.","In that spirit, we are the first to conduct an extensive study of the performance of multilingual models on parallel, multi-turn instruction-tuning benchmarks across a selection of the most-spoken Indo-European languages.","We systematically examine the effects of language and instruction dataset size on a mid-sized, multilingual LLM by instruction-tuning it on parallel instruction-tuning datasets.","Our results demonstrate that instruction-tuning on parallel instead of monolingual corpora benefits cross-lingual instruction following capabilities by up to 4.6%.","Furthermore, we show that the Superficial Alignment Hypothesis does not hold in general, as the investigated multilingual 7B parameter model presents a counter-example requiring large-scale instruction-tuning datasets.","Finally, we conduct a human annotation study to understand the alignment between human-based and GPT-4-based evaluation within multilingual chat scenarios."],"url":"http://arxiv.org/abs/2402.13703v1","category":"cs.CL"}
{"created":"2024-02-21 09:42:13","title":"Numerical methods for closed-loop systems with non-autonomous data","abstract":"By computing a feedback control via the linear quadratic regulator (LQR) approach and simulating a non-linear non-autonomous closed-loop system using this feedback, we combine two numerically challenging tasks. For the first task, the computation of the feedback control, we use the non-autonomous generalized differential Riccati equation (DRE), whose solution determines the time-varying feedback gain matrix. Regarding the second task, we want to be able to simulate non-linear closed-loop systems for which it is known that the regulator is only valid for sufficiently small perturbations. Thus, one easily runs into numerical issues in the integrators when the closed-loop control varies greatly. For these systems, e.g., the A-stable implicit Euler methods fails.\\newline On the one hand, we implement non-autonomous versions of splitting schemes and BDF methods for the solution of our non-autonomous DREs. These are well-established DRE solvers in the autonomous case. On the other hand, to tackle the numerical issues in the simulation of the non-linear closed-loop system, we apply a fractional-step-theta scheme with time-adaptivity tuned specifically to this kind of challenge. That is, we additionally base the time-adaptivity on the activity of the control. We compare this approach to the more classical error-based time-adaptivity.\\newline We describe techniques to make these two tasks computable in a reasonable amount of time and are able to simulate closed-loop systems with strongly varying controls, while avoiding numerical issues. Our time-adaptivity approach requires fewer time steps than the error-based alternative and is more reliable.","sentences":["By computing a feedback control via the linear quadratic regulator (LQR) approach and simulating a non-linear non-autonomous closed-loop system using this feedback, we combine two numerically challenging tasks.","For the first task, the computation of the feedback control, we use the non-autonomous generalized differential Riccati equation (DRE), whose solution determines the time-varying feedback gain matrix.","Regarding the second task, we want to be able to simulate non-linear closed-loop systems for which it is known that the regulator is only valid for sufficiently small perturbations.","Thus, one easily runs into numerical issues in the integrators when the closed-loop control varies greatly.","For these systems, e.g., the A-stable implicit Euler methods fails.\\newline","On the one hand, we implement non-autonomous versions of splitting schemes and BDF methods for the solution of our non-autonomous DREs.","These are well-established DRE solvers in the autonomous case.","On the other hand, to tackle the numerical issues in the simulation of the non-linear closed-loop system, we apply a fractional-step-theta scheme with time-adaptivity tuned specifically to this kind of challenge.","That is, we additionally base the time-adaptivity on the activity of the control.","We compare this approach to the more classical error-based time-adaptivity.\\newline We describe techniques to make these two tasks computable in a reasonable amount of time and are able to simulate closed-loop systems with strongly varying controls, while avoiding numerical issues.","Our time-adaptivity approach requires fewer time steps than the error-based alternative and is more reliable."],"url":"http://arxiv.org/abs/2402.13656v1","category":"math.NA"}
{"created":"2024-02-21 09:23:18","title":"Bifurcation of time crystals in driven and dissipative Rydberg atomic gas","abstract":"A time crystal is an exotic phase of matter where time-translational symmetry is broken; this phase differs from the spatial symmetry breaking induced in crystals in space. Lots of experiments report the transition from a thermal equilibrium phase to time crystal phase. However, there is no experimental method to probe the bifurcation effect of distinct time crystals in quantum many-body systems. Here, in a driven and dissipative many-body Rydberg atom system, we observe multiple continuous dissipative time crystals and emergence of more complex temporal symmetries beyond the single time crystal phase. Bifurcation of time crystals in strongly interacting Rydberg atoms is observed; the process manifests as a transition from a time crystal state of long temporal order to one of short temporal order, or vice versa. By manipulating the driving field parameters, we observe the time crystal's bistability and a hysteresis loop. These investigations indicate new possibilities for control and manipulation of the temporal symmetries of non-equilibrium systems.","sentences":["A time crystal is an exotic phase of matter where time-translational symmetry is broken; this phase differs from the spatial symmetry breaking induced in crystals in space.","Lots of experiments report the transition from a thermal equilibrium phase to time crystal phase.","However, there is no experimental method to probe the bifurcation effect of distinct time crystals in quantum many-body systems.","Here, in a driven and dissipative many-body Rydberg atom system, we observe multiple continuous dissipative time crystals and emergence of more complex temporal symmetries beyond the single time crystal phase.","Bifurcation of time crystals in strongly interacting Rydberg atoms is observed; the process manifests as a transition from a time crystal state of long temporal order to one of short temporal order, or vice versa.","By manipulating the driving field parameters, we observe the time crystal's bistability and a hysteresis loop.","These investigations indicate new possibilities for control and manipulation of the temporal symmetries of non-equilibrium systems."],"url":"http://arxiv.org/abs/2402.13644v2","category":"cond-mat.quant-gas"}
{"created":"2024-02-21 09:20:09","title":"Adaptive Ridge Approach to Heteroscedastic Regression","abstract":"We propose an adaptive ridge (AR) based estimation scheme for a heteroscedastic linear model equipped with log-linear errors. We simultaneously estimate the mean and variance parameters and show new asymptotic distributional and tightness properties in a sparse setting. We also show that estimates for zero parameters shrink with more iterations under suitable assumptions for tuning parameters. We observe possible generalizations of this paper's results through simulations and will apply the estimation method in forecasting electricity consumption.","sentences":["We propose an adaptive ridge (AR) based estimation scheme for a heteroscedastic linear model equipped with log-linear errors.","We simultaneously estimate the mean and variance parameters and show new asymptotic distributional and tightness properties in a sparse setting.","We also show that estimates for zero parameters shrink with more iterations under suitable assumptions for tuning parameters.","We observe possible generalizations of this paper's results through simulations and will apply the estimation method in forecasting electricity consumption."],"url":"http://arxiv.org/abs/2402.13642v1","category":"math.ST"}
{"created":"2024-02-21 09:18:59","title":"FlexHB: a More Efficient and Flexible Framework for Hyperparameter Optimization","abstract":"Given a Hyperparameter Optimization(HPO) problem, how to design an algorithm to find optimal configurations efficiently? Bayesian Optimization(BO) and the multi-fidelity BO methods employ surrogate models to sample configurations based on history evaluations. More recent studies obtain better performance by integrating BO with HyperBand(HB), which accelerates evaluation by early stopping mechanism. However, these methods ignore the advantage of a suitable evaluation scheme over the default HyperBand, and the capability of BO is still constrained by skewed evaluation results. In this paper, we propose FlexHB, a new method pushing multi-fidelity BO to the limit as well as re-designing a framework for early stopping with Successive Halving(SH). Comprehensive study on FlexHB shows that (1) our fine-grained fidelity method considerably enhances the efficiency of searching optimal configurations, (2) our FlexBand framework (self-adaptive allocation of SH brackets, and global ranking of configurations in both current and past SH procedures) grants the algorithm with more flexibility and improves the anytime performance. Our method achieves superior efficiency and outperforms other methods on various HPO tasks. Empirical results demonstrate that FlexHB can achieve up to 6.9X and 11.1X speedups over the state-of-the-art MFES-HB and BOHB respectively.","sentences":["Given a Hyperparameter Optimization(HPO) problem, how to design an algorithm to find optimal configurations efficiently?","Bayesian Optimization(BO) and the multi-fidelity BO methods employ surrogate models to sample configurations based on history evaluations.","More recent studies obtain better performance by integrating BO with HyperBand(HB), which accelerates evaluation by early stopping mechanism.","However, these methods ignore the advantage of a suitable evaluation scheme over the default HyperBand, and the capability of BO is still constrained by skewed evaluation results.","In this paper, we propose FlexHB, a new method pushing multi-fidelity BO to the limit as well as re-designing a framework for early stopping with Successive Halving(SH).","Comprehensive study on FlexHB shows that (1) our fine-grained fidelity method considerably enhances the efficiency of searching optimal configurations, (2) our FlexBand framework (self-adaptive allocation of SH brackets, and global ranking of configurations in both current and past SH procedures) grants the algorithm with more flexibility and improves the anytime performance.","Our method achieves superior efficiency and outperforms other methods on various HPO tasks.","Empirical results demonstrate that FlexHB can achieve up to 6.9X and 11.1X speedups over the state-of-the-art MFES-HB and BOHB respectively."],"url":"http://arxiv.org/abs/2402.13641v1","category":"cs.LG"}
{"created":"2024-02-21 09:13:08","title":"Learning Dual-arm Object Rearrangement for Cartesian Robots","abstract":"This work focuses on the dual-arm object rearrangement problem abstracted from a realistic industrial scenario of Cartesian robots. The goal of this problem is to transfer all the objects from sources to targets with the minimum total completion time. To achieve the goal, the core idea is to develop an effective object-to-arm task assignment strategy for minimizing the cumulative task execution time and maximizing the dual-arm cooperation efficiency. One of the difficulties in the task assignment is the scalability problem. As the number of objects increases, the computation time of traditional offline-search-based methods grows strongly for computational complexity. Encouraged by the adaptability of reinforcement learning (RL) in long-sequence task decisions, we propose an online task assignment decision method based on RL, and the computation time of our method only increases linearly with the number of objects. Further, we design an attention-based network to model the dependencies between the input states during the whole task execution process to help find the most reasonable object-to-arm correspondence in each task assignment round. In the experimental part, we adapt some search-based methods to this specific setting and compare our method with them. Experimental result shows that our approach achieves outperformance over search-based methods in total execution time and computational efficiency, and also verifies the generalization of our method to different numbers of objects. In addition, we show the effectiveness of our method deployed on the real robot in the supplementary video.","sentences":["This work focuses on the dual-arm object rearrangement problem abstracted from a realistic industrial scenario of Cartesian robots.","The goal of this problem is to transfer all the objects from sources to targets with the minimum total completion time.","To achieve the goal, the core idea is to develop an effective object-to-arm task assignment strategy for minimizing the cumulative task execution time and maximizing the dual-arm cooperation efficiency.","One of the difficulties in the task assignment is the scalability problem.","As the number of objects increases, the computation time of traditional offline-search-based methods grows strongly for computational complexity.","Encouraged by the adaptability of reinforcement learning (RL) in long-sequence task decisions, we propose an online task assignment decision method based on RL, and the computation time of our method only increases linearly with the number of objects.","Further, we design an attention-based network to model the dependencies between the input states during the whole task execution process to help find the most reasonable object-to-arm correspondence in each task assignment round.","In the experimental part, we adapt some search-based methods to this specific setting and compare our method with them.","Experimental result shows that our approach achieves outperformance over search-based methods in total execution time and computational efficiency, and also verifies the generalization of our method to different numbers of objects.","In addition, we show the effectiveness of our method deployed on the real robot in the supplementary video."],"url":"http://arxiv.org/abs/2402.13634v1","category":"cs.RO"}
{"created":"2024-02-21 08:53:04","title":"Towards Linear Spanners in All Temporal Cliques","abstract":"Many real-world networks, like transportation networks and social networks, are dynamic in the sense that the edge set may change over time, but these changes are known in advance. This behavior is captured by the temporal graphs model, which has recently become a trending topic in theoretical computer science. A core open problem in the field is to prove the existence of linear-size temporal spanners in temporal cliques, i.e., sparse subgraphs of complete temporal graphs that ensure all-pairs reachability via temporal paths. So far, the best known result is the existence of temporal spanners with $\\mathcal{O}(n\\log n)$ many edges. We present significant progress towards proving that linear-size temporal spanners exist in all temporal cliques.   We adapt techniques used in previous works and heavily expand and generalize them to provide a simpler and more intuitive proof of the $\\mathcal{O}(n\\log n)$ bound. Moreover, we use our novel approach to show that a large class of temporal cliques, called edge-pivot graphs, admit linear-size temporal spanners. To contrast this, we investigate other classes of temporal cliques that do not belong to the class of edge-pivot graphs. We introduce two such graph classes and we develop novel techniques for establishing the existence of linear temporal spanners in these graph classes as well.","sentences":["Many real-world networks, like transportation networks and social networks, are dynamic in the sense that the edge set may change over time, but these changes are known in advance.","This behavior is captured by the temporal graphs model, which has recently become a trending topic in theoretical computer science.","A core open problem in the field is to prove the existence of linear-size temporal spanners in temporal cliques, i.e., sparse subgraphs of complete temporal graphs that ensure all-pairs reachability via temporal paths.","So far, the best known result is the existence of temporal spanners with $\\mathcal{O}(n\\log n)$ many edges.","We present significant progress towards proving that linear-size temporal spanners exist in all temporal cliques.   ","We adapt techniques used in previous works and heavily expand and generalize them to provide a simpler and more intuitive proof of the $\\mathcal{O}(n\\log n)$ bound.","Moreover, we use our novel approach to show that a large class of temporal cliques, called edge-pivot graphs, admit linear-size temporal spanners.","To contrast this, we investigate other classes of temporal cliques that do not belong to the class of edge-pivot graphs.","We introduce two such graph classes and we develop novel techniques for establishing the existence of linear temporal spanners in these graph classes as well."],"url":"http://arxiv.org/abs/2402.13624v1","category":"cs.DM"}
{"created":"2024-02-21 08:44:32","title":"A Monolithic Cybersecurity Architecture for Power Electronic Systems","abstract":"Power electronic systems (PES) face significant threats from various data availability and integrity attacks, significantly affecting the performance of communication networks and power system operation. As a result, several attack detection and reconstruction techniques are deployed, which makes it a costly \\& complex cybersecurity operational platform with significant room for incremental extensions for mitigation against future threats. Unlike the said traditional arrangements, our paper introduces a foundational approach by establishing a monolithic cybersecurity architecture (MCA) via incorporating semantic principles into the sampling process for distributed energy resources (DERs). This unified approach concurrently compensates for the intrusion challenges posed by cyber attacks by reconstructing signals using the dynamics of the inner control layer. This reconstruction considers essential semantic attributes, like Priority, Freshness, and Relevance to ensure resilient dynamic performance. Hence, the proposed scheme promises a generalized route to concurrently tackle a global set of cyber attacks in elevating the resilience of PES. Finally, rigorous validation on a modified IEEE 69-bus distribution system and a real-world South California Edison (SCE) 47-bus network, using OPAL-RT under diverse operating conditions, underscores its robustness, model-free design capability, scalability, and adaptability to dynamic cyber graphs and system reconfiguration.","sentences":["Power electronic systems (PES) face significant threats from various data availability and integrity attacks, significantly affecting the performance of communication networks and power system operation.","As a result, several attack detection and reconstruction techniques are deployed, which makes it a costly \\& complex cybersecurity operational platform with significant room for incremental extensions for mitigation against future threats.","Unlike the said traditional arrangements, our paper introduces a foundational approach by establishing a monolithic cybersecurity architecture (MCA) via incorporating semantic principles into the sampling process for distributed energy resources (DERs).","This unified approach concurrently compensates for the intrusion challenges posed by cyber attacks by reconstructing signals using the dynamics of the inner control layer.","This reconstruction considers essential semantic attributes, like Priority, Freshness, and Relevance to ensure resilient dynamic performance.","Hence, the proposed scheme promises a generalized route to concurrently tackle a global set of cyber attacks in elevating the resilience of PES.","Finally, rigorous validation on a modified IEEE 69-bus distribution system and a real-world South California Edison (SCE) 47-bus network, using OPAL-RT under diverse operating conditions, underscores its robustness, model-free design capability, scalability, and adaptability to dynamic cyber graphs and system reconfiguration."],"url":"http://arxiv.org/abs/2402.13617v2","category":"eess.SY"}
{"created":"2024-02-21 08:40:04","title":"Analyizing the Conjunction Fallacy as a Fact","abstract":"Since the seminal paper by Tversky and Kahneman, the conjunction fallacy has been the subject of multiple debates and become a fundamental challenge for cognitive theories in decision-making. In this article, we take a rather uncommon perspective on this phenomenon. Instead of trying to explain the nature or causes of the conjunction fallacy (intensional definition), we analyze its range of factual possibilities (extensional definition). We show that the majority of research on the conjunction fallacy, according to our sample of experiments reviewed which covers literature between 1983 and 2016, has focused on a narrow part of the a priori factual possibilities, implying that explanations of the conjunction fallacy are fundamentally biased by the short scope of possibilities explored. The latter is a rather curious aspect of the research evolution in the conjunction fallacy considering that the very nature of it is motivated by extensional considerations.","sentences":["Since the seminal paper by Tversky and Kahneman, the conjunction fallacy has been the subject of multiple debates and become a fundamental challenge for cognitive theories in decision-making.","In this article, we take a rather uncommon perspective on this phenomenon.","Instead of trying to explain the nature or causes of the conjunction fallacy (intensional definition), we analyze its range of factual possibilities (extensional definition).","We show that the majority of research on the conjunction fallacy, according to our sample of experiments reviewed which covers literature between 1983 and 2016, has focused on a narrow part of the a priori factual possibilities, implying that explanations of the conjunction fallacy are fundamentally biased by the short scope of possibilities explored.","The latter is a rather curious aspect of the research evolution in the conjunction fallacy considering that the very nature of it is motivated by extensional considerations."],"url":"http://arxiv.org/abs/2402.13615v1","category":"cs.AI"}
{"created":"2024-02-21 08:09:05","title":"Hybrid Reasoning Based on Large Language Models for Autonomous Car Driving","abstract":"Large Language Models (LLMs) have garnered significant attention for their ability to understand text and images, generate human-like text, and perform complex reasoning tasks. However, their ability to generalize this advanced reasoning with a combination of natural language text for decision-making in dynamic situations requires further exploration. In this study, we investigate how well LLMs can adapt and apply a combination of arithmetic and common-sense reasoning, particularly in autonomous driving scenarios. We hypothesize that LLMs hybrid reasoning abilities can improve autonomous driving by enabling them to analyze detected object and sensor data, understand driving regulations and physical laws, and offer additional context. This addresses complex scenarios, like decisions in low visibility (due to weather conditions), where traditional methods might fall short. We evaluated Large Language Models (LLMs) based on accuracy by comparing their answers with human-generated ground truth inside CARLA. The results showed that when a combination of images (detected objects) and sensor data is fed into the LLM, it can offer precise information for brake and throttle control in autonomous vehicles across various weather conditions. This formulation and answers can assist in decision-making for auto-pilot systems.","sentences":["Large Language Models (LLMs) have garnered significant attention for their ability to understand text and images, generate human-like text, and perform complex reasoning tasks.","However, their ability to generalize this advanced reasoning with a combination of natural language text for decision-making in dynamic situations requires further exploration.","In this study, we investigate how well LLMs can adapt and apply a combination of arithmetic and common-sense reasoning, particularly in autonomous driving scenarios.","We hypothesize that LLMs hybrid reasoning abilities can improve autonomous driving by enabling them to analyze detected object and sensor data, understand driving regulations and physical laws, and offer additional context.","This addresses complex scenarios, like decisions in low visibility (due to weather conditions), where traditional methods might fall short.","We evaluated Large Language Models (LLMs) based on accuracy by comparing their answers with human-generated ground truth inside CARLA.","The results showed that when a combination of images (detected objects) and sensor data is fed into the LLM, it can offer precise information for brake and throttle control in autonomous vehicles across various weather conditions.","This formulation and answers can assist in decision-making for auto-pilot systems."],"url":"http://arxiv.org/abs/2402.13602v1","category":"cs.CV"}
{"created":"2024-02-21 08:03:27","title":"User-LLM: Efficient LLM Contextualization with User Embeddings","abstract":"Large language models (LLMs) have revolutionized natural language processing. However, effectively incorporating complex and potentially noisy user interaction data remains a challenge. To address this, we propose User-LLM, a novel framework that leverages user embeddings to contextualize LLMs. These embeddings, distilled from diverse user interactions using self-supervised pretraining, capture latent user preferences and their evolution over time. We integrate these user embeddings with LLMs through cross-attention and soft-prompting, enabling LLMs to dynamically adapt to user context. Our comprehensive experiments on MovieLens, Amazon Review, and Google Local Review datasets demonstrate significant performance gains across various tasks. Notably, our approach outperforms text-prompt-based contextualization on long sequence tasks and tasks that require deep user understanding while being computationally efficient. We further incorporate Perceiver layers to streamline the integration between user encoders and LLMs, reducing computational demands.","sentences":["Large language models (LLMs) have revolutionized natural language processing.","However, effectively incorporating complex and potentially noisy user interaction data remains a challenge.","To address this, we propose User-LLM, a novel framework that leverages user embeddings to contextualize LLMs.","These embeddings, distilled from diverse user interactions using self-supervised pretraining, capture latent user preferences and their evolution over time.","We integrate these user embeddings with LLMs through cross-attention and soft-prompting, enabling LLMs to dynamically adapt to user context.","Our comprehensive experiments on MovieLens, Amazon Review, and Google Local Review datasets demonstrate significant performance gains across various tasks.","Notably, our approach outperforms text-prompt-based contextualization on long sequence tasks and tasks that require deep user understanding while being computationally efficient.","We further incorporate Perceiver layers to streamline the integration between user encoders and LLMs, reducing computational demands."],"url":"http://arxiv.org/abs/2402.13598v1","category":"cs.CL"}
{"created":"2024-02-21 07:36:51","title":"Delay-Aware Semantic Sampling in Power Electronic Systems","abstract":"In power electronic systems (PES), attacks on data availability such as latency attacks, data dropouts, and time-synchronization attacks (TSAs) continue to pose significant threats to both the communication network and the control system performance. As per the conventional norms of communication engineering, PES still rely on time synchronized sampling, which translates every received message with equal importance. In this paper, we go beyond event-triggered sampling/estimation to integrate semantic principles into the sampling process for each distributed energy resource (DER), which not only compensates for delayed communicated signals by reconstruction of a new signal from the inner control layer dynamics, but also evaluates the reconstruction stage using key semantic requirements, namely Freshness, Relevance and Priority for good dynamic performance. As a result, the sparsity provided by event-driven sampling of internal control loop dynamics translates as semantics in PES. The proposed scheme has been extensively tested and validated on a modified IEEE 37-bus AC distribution system, under many operating conditions and noisy environment in OPAL-RT environment to establish its robustness, model-free design ability and adaptive behavior to dynamic cyber graph topologies.","sentences":["In power electronic systems (PES), attacks on data availability such as latency attacks, data dropouts, and time-synchronization attacks (TSAs) continue to pose significant threats to both the communication network and the control system performance.","As per the conventional norms of communication engineering, PES still rely on time synchronized sampling, which translates every received message with equal importance.","In this paper, we go beyond event-triggered sampling/estimation to integrate semantic principles into the sampling process for each distributed energy resource (DER), which not only compensates for delayed communicated signals by reconstruction of a new signal from the inner control layer dynamics, but also evaluates the reconstruction stage using key semantic requirements, namely Freshness, Relevance and Priority for good dynamic performance.","As a result, the sparsity provided by event-driven sampling of internal control loop dynamics translates as semantics in PES.","The proposed scheme has been extensively tested and validated on a modified IEEE 37-bus AC distribution system, under many operating conditions and noisy environment in OPAL-RT environment to establish its robustness, model-free design ability and adaptive behavior to dynamic cyber graph topologies."],"url":"http://arxiv.org/abs/2402.13586v2","category":"eess.SY"}
{"created":"2024-02-21 07:15:16","title":"Flexible Physical Camouflage Generation Based on a Differential Approach","abstract":"This study introduces a novel approach to neural rendering, specifically tailored for adversarial camouflage, within an extensive 3D rendering framework. Our method, named FPA, goes beyond traditional techniques by faithfully simulating lighting conditions and material variations, ensuring a nuanced and realistic representation of textures on a 3D target. To achieve this, we employ a generative approach that learns adversarial patterns from a diffusion model. This involves incorporating a specially designed adversarial loss and covert constraint loss to guarantee the adversarial and covert nature of the camouflage in the physical world. Furthermore, we showcase the effectiveness of the proposed camouflage in sticker mode, demonstrating its ability to cover the target without compromising adversarial information. Through empirical and physical experiments, FPA exhibits strong performance in terms of attack success rate and transferability. Additionally, the designed sticker-mode camouflage, coupled with a concealment constraint, adapts to the environment, yielding diverse styles of texture. Our findings highlight the versatility and efficacy of the FPA approach in adversarial camouflage applications.","sentences":["This study introduces a novel approach to neural rendering, specifically tailored for adversarial camouflage, within an extensive 3D rendering framework.","Our method, named FPA, goes beyond traditional techniques by faithfully simulating lighting conditions and material variations, ensuring a nuanced and realistic representation of textures on a 3D target.","To achieve this, we employ a generative approach that learns adversarial patterns from a diffusion model.","This involves incorporating a specially designed adversarial loss and covert constraint loss to guarantee the adversarial and covert nature of the camouflage in the physical world.","Furthermore, we showcase the effectiveness of the proposed camouflage in sticker mode, demonstrating its ability to cover the target without compromising adversarial information.","Through empirical and physical experiments, FPA exhibits strong performance in terms of attack success rate and transferability.","Additionally, the designed sticker-mode camouflage, coupled with a concealment constraint, adapts to the environment, yielding diverse styles of texture.","Our findings highlight the versatility and efficacy of the FPA approach in adversarial camouflage applications."],"url":"http://arxiv.org/abs/2402.13575v1","category":"cs.CV"}
{"created":"2024-02-21 06:37:07","title":"Analysis of Multi-Source Language Training in Cross-Lingual Transfer","abstract":"The successful adaptation of multilingual language models (LMs) to a specific language-task pair critically depends on the availability of data tailored for that condition. While cross-lingual transfer (XLT) methods have contributed to addressing this data scarcity problem, there still exists ongoing debate about the mechanisms behind their effectiveness. In this work, we focus on one of promising assumptions about inner workings of XLT, that it encourages multilingual LMs to place greater emphasis on language-agnostic or task-specific features. We test this hypothesis by examining how the patterns of XLT change with a varying number of source languages involved in the process. Our experimental findings show that the use of multiple source languages in XLT-a technique we term Multi-Source Language Training (MSLT)-leads to increased mingling of embedding spaces for different languages, supporting the claim that XLT benefits from making use of language-independent information. On the other hand, we discover that using an arbitrary combination of source languages does not always guarantee better performance. We suggest simple heuristics for identifying effective language combinations for MSLT and empirically prove its effectiveness.","sentences":["The successful adaptation of multilingual language models (LMs) to a specific language-task pair critically depends on the availability of data tailored for that condition.","While cross-lingual transfer (XLT) methods have contributed to addressing this data scarcity problem, there still exists ongoing debate about the mechanisms behind their effectiveness.","In this work, we focus on one of promising assumptions about inner workings of XLT, that it encourages multilingual LMs to place greater emphasis on language-agnostic or task-specific features.","We test this hypothesis by examining how the patterns of XLT change with a varying number of source languages involved in the process.","Our experimental findings show that the use of multiple source languages in XLT-a technique we term Multi-Source Language Training (MSLT)-leads to increased mingling of embedding spaces for different languages, supporting the claim that XLT benefits from making use of language-independent information.","On the other hand, we discover that using an arbitrary combination of source languages does not always guarantee better performance.","We suggest simple heuristics for identifying effective language combinations for MSLT and empirically prove its effectiveness."],"url":"http://arxiv.org/abs/2402.13562v1","category":"cs.CL"}
{"created":"2024-02-21 06:34:46","title":"Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension with Enhanced Visual Knowledge Alignment","abstract":"Evaluating and Rethinking the current landscape of Large Multimodal Models (LMMs), we observe that widely-used visual-language projection approaches (e.g., Q-former or MLP) focus on the alignment of image-text descriptions yet ignore the visual knowledge-dimension alignment, i.e., connecting visuals to their relevant knowledge. Visual knowledge plays a significant role in analyzing, inferring, and interpreting information from visuals, helping improve the accuracy of answers to knowledge-based visual questions. In this paper, we mainly explore improving LMMs with visual-language knowledge alignment, especially aimed at challenging knowledge-based visual question answering (VQA). To this end, we present a Cognitive Visual-Language Mapper (CVLM), which contains a pretrained Visual Knowledge Aligner (VKA) and a Fine-grained Knowledge Adapter (FKA) used in the multimodal instruction tuning stage. Specifically, we design the VKA based on the interaction between a small language model and a visual encoder, training it on collected image-knowledge pairs to achieve visual knowledge acquisition and projection. FKA is employed to distill the fine-grained visual knowledge of an image and inject it into Large Language Models (LLMs). We conduct extensive experiments on knowledge-based VQA benchmarks and experimental results show that CVLM significantly improves the performance of LMMs on knowledge-based VQA (average gain by 5.0%). Ablation studies also verify the effectiveness of VKA and FKA, respectively.","sentences":["Evaluating and Rethinking the current landscape of Large Multimodal Models (LMMs), we observe that widely-used visual-language projection approaches (e.g., Q-former or MLP) focus on the alignment of image-text descriptions yet ignore the visual knowledge-dimension alignment, i.e., connecting visuals to their relevant knowledge.","Visual knowledge plays a significant role in analyzing, inferring, and interpreting information from visuals, helping improve the accuracy of answers to knowledge-based visual questions.","In this paper, we mainly explore improving LMMs with visual-language knowledge alignment, especially aimed at challenging knowledge-based visual question answering (VQA).","To this end, we present a Cognitive Visual-Language Mapper (CVLM), which contains a pretrained Visual Knowledge Aligner (VKA) and a Fine-grained Knowledge Adapter (FKA) used in the multimodal instruction tuning stage.","Specifically, we design the VKA based on the interaction between a small language model and a visual encoder, training it on collected image-knowledge pairs to achieve visual knowledge acquisition and projection.","FKA is employed to distill the fine-grained visual knowledge of an image and inject it into Large Language Models (LLMs).","We conduct extensive experiments on knowledge-based VQA benchmarks and experimental results show that CVLM significantly improves the performance of LMMs on knowledge-based VQA (average gain by 5.0%).","Ablation studies also verify the effectiveness of VKA and FKA, respectively."],"url":"http://arxiv.org/abs/2402.13561v1","category":"cs.CL"}
{"created":"2024-02-21 06:22:41","title":"Generative AI for Secure Physical Layer Communications: A Survey","abstract":"Generative Artificial Intelligence (GAI) stands at the forefront of AI innovation, demonstrating rapid advancement and unparalleled proficiency in generating diverse content. Beyond content creation, GAI has significant analytical abilities to learn complex data distribution, offering numerous opportunities to resolve security issues. In the realm of security from physical layer perspectives, traditional AI approaches frequently struggle, primarily due to their limited capacity to dynamically adjust to the evolving physical attributes of transmission channels and the complexity of contemporary cyber threats. This adaptability and analytical depth are precisely where GAI excels. Therefore, in this paper, we offer an extensive survey on the various applications of GAI in enhancing security within the physical layer of communication networks. We first emphasize the importance of advanced GAI models in this area, including Generative Adversarial Networks (GANs), Autoencoders (AEs), Variational Autoencoders (VAEs), and Diffusion Models (DMs). We delve into the roles of GAI in addressing challenges of physical layer security, focusing on communication confidentiality, authentication, availability, resilience, and integrity. Furthermore, we also present future research directions focusing model improvements, multi-scenario deployment, resource-efficient optimization, and secure semantic communication, highlighting the multifaceted potential of GAI to address emerging challenges in secure physical layer communications and sensing.","sentences":["Generative Artificial Intelligence (GAI) stands at the forefront of AI innovation, demonstrating rapid advancement and unparalleled proficiency in generating diverse content.","Beyond content creation, GAI has significant analytical abilities to learn complex data distribution, offering numerous opportunities to resolve security issues.","In the realm of security from physical layer perspectives, traditional AI approaches frequently struggle, primarily due to their limited capacity to dynamically adjust to the evolving physical attributes of transmission channels and the complexity of contemporary cyber threats.","This adaptability and analytical depth are precisely where GAI excels.","Therefore, in this paper, we offer an extensive survey on the various applications of GAI in enhancing security within the physical layer of communication networks.","We first emphasize the importance of advanced GAI models in this area, including Generative Adversarial Networks (GANs), Autoencoders (AEs), Variational Autoencoders (VAEs), and Diffusion Models (DMs).","We delve into the roles of GAI in addressing challenges of physical layer security, focusing on communication confidentiality, authentication, availability, resilience, and integrity.","Furthermore, we also present future research directions focusing model improvements, multi-scenario deployment, resource-efficient optimization, and secure semantic communication, highlighting the multifaceted potential of GAI to address emerging challenges in secure physical layer communications and sensing."],"url":"http://arxiv.org/abs/2402.13553v1","category":"cs.CR"}
{"created":"2024-02-21 06:10:27","title":"Q-learning-based Joint Design of Adaptive Modulation and Precoding for Physical Layer Security in Visible Light Communications","abstract":"There has been an increasing interest in physical layer security (PLS), which, compared with conventional cryptography, offers a unique approach to guaranteeing information confidentiality against eavesdroppers. In this paper, we study a joint design of adaptive $M$-ary pulse amplitude modulation (PAM) and precoding, which aims to optimize wiretap visible-light channels' secrecy capacity and bit error rate (BER) performances. The proposed design is motivated by higher-order modulation, which results in better secrecy capacity at the expense of a higher BER. On the other hand, a proper precoding design, which can manipulate the received signal quality at the legitimate user and the eavesdropper, can also enhance secrecy performance and influence the BER. A reward function that considers the secrecy capacity and the BERs of the legitimate user's (Bob) and the eavesdropper's (Eve) channels is introduced and maximized. Due to the non-linearity and complexity of the reward function, it is challenging to solve the optical design using classical optimization techniques. Therefore, reinforcement learning-based designs using Q-learning and Deep Q-learning are proposed to maximize the reward function. Simulation results verify that compared with the baseline designs, the proposed joint designs achieve better reward values while maintaining the BER of Bob's channel (Eve's channel) well below (above) the pre-FEC (forward error correction) BER threshold.","sentences":["There has been an increasing interest in physical layer security (PLS), which, compared with conventional cryptography, offers a unique approach to guaranteeing information confidentiality against eavesdroppers.","In this paper, we study a joint design of adaptive $M$-ary pulse amplitude modulation (PAM) and precoding, which aims to optimize wiretap visible-light channels' secrecy capacity and bit error rate (BER) performances.","The proposed design is motivated by higher-order modulation, which results in better secrecy capacity at the expense of a higher BER.","On the other hand, a proper precoding design, which can manipulate the received signal quality at the legitimate user and the eavesdropper, can also enhance secrecy performance and influence the BER.","A reward function that considers the secrecy capacity and the BERs of the legitimate user's (Bob) and the eavesdropper's (Eve) channels is introduced and maximized.","Due to the non-linearity and complexity of the reward function, it is challenging to solve the optical design using classical optimization techniques.","Therefore, reinforcement learning-based designs using Q-learning and Deep Q-learning are proposed to maximize the reward function.","Simulation results verify that compared with the baseline designs, the proposed joint designs achieve better reward values while maintaining the BER of Bob's channel (Eve's channel) well below (above) the pre-FEC (forward error correction) BER threshold."],"url":"http://arxiv.org/abs/2402.13549v1","category":"cs.IT"}
{"created":"2024-02-21 06:07:33","title":"DiffPLF: A Conditional Diffusion Model for Probabilistic Forecasting of EV Charging Load","abstract":"Due to the vast electric vehicle (EV) penetration to distribution grid, charging load forecasting is essential to promote charging station operation and demand-side management.However, the stochastic charging behaviors and associated exogenous factors render future charging load patterns quite volatile and hard to predict. Accordingly, we devise a novel Diffusion model termed DiffPLF for Probabilistic Load Forecasting of EV charging, which can explicitly approximate the predictive load distribution conditioned on historical data and related covariates. Specifically, we leverage a denoising diffusion model, which can progressively convert the Gaussian prior to real time-series data by learning a reversal of the diffusion process. Besides, we couple such diffusion model with a cross-attention-based conditioning mechanism to execute conditional generation for possible charging demand profiles. We also propose a task-informed fine-tuning technique to better adapt DiffPLF to the probabilistic time-series forecasting task and acquire more accurate and reliable predicted intervals. Finally, we conduct multiple experiments to validate the superiority of DiffPLF to predict complex temporal patterns of erratic charging load and carry out controllable generation based on certain covariate. Results demonstrate that we can attain a notable rise of 39.58% and 49.87% on MAE and CRPS respectively compared to the conventional method.","sentences":["Due to the vast electric vehicle (EV) penetration to distribution grid, charging load forecasting is essential to promote charging station operation and demand-side management.","However, the stochastic charging behaviors and associated exogenous factors render future charging load patterns quite volatile and hard to predict.","Accordingly, we devise a novel Diffusion model termed DiffPLF for Probabilistic Load Forecasting of EV charging, which can explicitly approximate the predictive load distribution conditioned on historical data and related covariates.","Specifically, we leverage a denoising diffusion model, which can progressively convert the Gaussian prior to real time-series data by learning a reversal of the diffusion process.","Besides, we couple such diffusion model with a cross-attention-based conditioning mechanism to execute conditional generation for possible charging demand profiles.","We also propose a task-informed fine-tuning technique to better adapt DiffPLF to the probabilistic time-series forecasting task and acquire more accurate and reliable predicted intervals.","Finally, we conduct multiple experiments to validate the superiority of DiffPLF to predict complex temporal patterns of erratic charging load and carry out controllable generation based on certain covariate.","Results demonstrate that we can attain a notable rise of 39.58% and 49.87% on MAE and CRPS respectively compared to the conventional method."],"url":"http://arxiv.org/abs/2402.13548v1","category":"cs.LG"}
{"created":"2024-02-21 05:56:52","title":"LLMs Meet Long Video: Advancing Long Video Comprehension with An Interactive Visual Adapter in LLMs","abstract":"Long video understanding is a significant and ongoing challenge in the intersection of multimedia and artificial intelligence. Employing large language models (LLMs) for comprehending video becomes an emerging and promising method. However, this approach incurs high computational costs due to the extensive array of video tokens, experiences reduced visual clarity as a consequence of token aggregation, and confronts challenges arising from irrelevant visual tokens while answering video-related questions. To alleviate these issues, we present an Interactive Visual Adapter (IVA) within LLMs, designed to enhance interaction with fine-grained visual elements. Specifically, we first transform long videos into temporal video tokens via leveraging a visual encoder alongside a pretrained causal transformer, then feed them into LLMs with the video instructions. Subsequently, we integrated IVA, which contains a lightweight temporal frame selector and a spatial feature interactor, within the internal blocks of LLMs to capture instruction-aware and fine-grained visual signals. Consequently, the proposed video-LLM facilitates a comprehensive understanding of long video content through appropriate long video modeling and precise visual interactions. We conducted extensive experiments on nine video understanding benchmarks and experimental results show that our interactive visual adapter significantly improves the performance of video LLMs on long video QA tasks. Ablation studies further verify the effectiveness of IVA in long and short video understandings.","sentences":["Long video understanding is a significant and ongoing challenge in the intersection of multimedia and artificial intelligence.","Employing large language models (LLMs) for comprehending video becomes an emerging and promising method.","However, this approach incurs high computational costs due to the extensive array of video tokens, experiences reduced visual clarity as a consequence of token aggregation, and confronts challenges arising from irrelevant visual tokens while answering video-related questions.","To alleviate these issues, we present an Interactive Visual Adapter (IVA) within LLMs, designed to enhance interaction with fine-grained visual elements.","Specifically, we first transform long videos into temporal video tokens via leveraging a visual encoder alongside a pretrained causal transformer, then feed them into LLMs with the video instructions.","Subsequently, we integrated IVA, which contains a lightweight temporal frame selector and a spatial feature interactor, within the internal blocks of LLMs to capture instruction-aware and fine-grained visual signals.","Consequently, the proposed video-LLM facilitates a comprehensive understanding of long video content through appropriate long video modeling and precise visual interactions.","We conducted extensive experiments on nine video understanding benchmarks and experimental results show that our interactive visual adapter significantly improves the performance of video LLMs on long video QA tasks.","Ablation studies further verify the effectiveness of IVA in long and short video understandings."],"url":"http://arxiv.org/abs/2402.13546v1","category":"cs.CL"}
{"created":"2024-02-21 05:41:34","title":"ARL2: Aligning Retrievers for Black-box Large Language Models via Self-guided Adaptive Relevance Labeling","abstract":"Retrieval-augmented generation enhances large language models (LLMs) by incorporating relevant information from external knowledge sources. This enables LLMs to adapt to specific domains and mitigate hallucinations in knowledge-intensive tasks. However, existing retrievers are often misaligned with LLMs due to their separate training processes and the black-box nature of LLMs. To address this challenge, we propose ARL2, a retriever learning technique that harnesses LLMs as labelers. ARL2 leverages LLMs to annotate and score relevant evidence, enabling learning the retriever from robust LLM supervision. Furthermore, ARL2 uses an adaptive self-training strategy for curating high-quality and diverse relevance data, which can effectively reduce the annotation cost. Extensive experiments demonstrate the effectiveness of ARL2, achieving accuracy improvements of 5.4% on NQ and 4.6% on MMLU compared to the state-of-the-art methods. Additionally, ARL2 exhibits robust transfer learning capabilities and strong zero-shot generalization abilities. Our code will be published at \\url{https://github.com/zhanglingxi-cs/ARL2}.","sentences":["Retrieval-augmented generation enhances large language models (LLMs) by incorporating relevant information from external knowledge sources.","This enables LLMs to adapt to specific domains and mitigate hallucinations in knowledge-intensive tasks.","However, existing retrievers are often misaligned with LLMs due to their separate training processes and the black-box nature of LLMs.","To address this challenge, we propose ARL2, a retriever learning technique that harnesses LLMs as labelers.","ARL2 leverages LLMs to annotate and score relevant evidence, enabling learning the retriever from robust LLM supervision.","Furthermore, ARL2 uses an adaptive self-training strategy for curating high-quality and diverse relevance data, which can effectively reduce the annotation cost.","Extensive experiments demonstrate the effectiveness of ARL2, achieving accuracy improvements of 5.4% on NQ and 4.6% on MMLU compared to the state-of-the-art methods.","Additionally, ARL2 exhibits robust transfer learning capabilities and strong zero-shot generalization abilities.","Our code will be published at \\url{https://github.com/zhanglingxi-cs/ARL2}."],"url":"http://arxiv.org/abs/2402.13542v1","category":"cs.CL"}
{"created":"2024-02-21 04:58:41","title":"Private Gradient Descent for Linear Regression: Tighter Error Bounds and Instance-Specific Uncertainty Estimation","abstract":"We provide an improved analysis of standard differentially private gradient descent for linear regression under the squared error loss. Under modest assumptions on the input, we characterize the distribution of the iterate at each time step.   Our analysis leads to new results on the algorithm's accuracy: for a proper fixed choice of hyperparameters, the sample complexity depends only linearly on the dimension of the data. This matches the dimension-dependence of the (non-private) ordinary least squares estimator as well as that of recent private algorithms that rely on sophisticated adaptive gradient-clipping schemes (Varshney et al., 2022; Liu et al., 2023).   Our analysis of the iterates' distribution also allows us to construct confidence intervals for the empirical optimizer which adapt automatically to the variance of the algorithm on a particular data set. We validate our theorems through experiments on synthetic data.","sentences":["We provide an improved analysis of standard differentially private gradient descent for linear regression under the squared error loss.","Under modest assumptions on the input, we characterize the distribution of the iterate at each time step.   ","Our analysis leads to new results on the algorithm's accuracy: for a proper fixed choice of hyperparameters, the sample complexity depends only linearly on the dimension of the data.","This matches the dimension-dependence of the (non-private) ordinary least squares estimator as well as that of recent private algorithms that rely on sophisticated adaptive gradient-clipping schemes (Varshney et al., 2022; Liu et al., 2023).   ","Our analysis of the iterates' distribution also allows us to construct confidence intervals for the empirical optimizer which adapt automatically to the variance of the algorithm on a particular data set.","We validate our theorems through experiments on synthetic data."],"url":"http://arxiv.org/abs/2402.13531v1","category":"cs.LG"}
{"created":"2024-02-21 04:00:54","title":"RITFIS: Robust input testing framework for LLMs-based intelligent software","abstract":"The dependence of Natural Language Processing (NLP) intelligent software on Large Language Models (LLMs) is increasingly prominent, underscoring the necessity for robustness testing. Current testing methods focus solely on the robustness of LLM-based software to prompts. Given the complexity and diversity of real-world inputs, studying the robustness of LLMbased software in handling comprehensive inputs (including prompts and examples) is crucial for a thorough understanding of its performance.   To this end, this paper introduces RITFIS, a Robust Input Testing Framework for LLM-based Intelligent Software. To our knowledge, RITFIS is the first framework designed to assess the robustness of LLM-based intelligent software against natural language inputs. This framework, based on given threat models and prompts, primarily defines the testing process as a combinatorial optimization problem. Successful test cases are determined by a goal function, creating a transformation space for the original examples through perturbation means, and employing a series of search methods to filter cases that meet both the testing objectives and language constraints. RITFIS, with its modular design, offers a comprehensive method for evaluating the robustness of LLMbased intelligent software.   RITFIS adapts 17 automated testing methods, originally designed for Deep Neural Network (DNN)-based intelligent software, to the LLM-based software testing scenario. It demonstrates the effectiveness of RITFIS in evaluating LLM-based intelligent software through empirical validation. However, existing methods generally have limitations, especially when dealing with lengthy texts and structurally complex threat models. Therefore, we conducted a comprehensive analysis based on five metrics and provided insightful testing method optimization strategies, benefiting both researchers and everyday users.","sentences":["The dependence of Natural Language Processing (NLP) intelligent software on Large Language Models (LLMs) is increasingly prominent, underscoring the necessity for robustness testing.","Current testing methods focus solely on the robustness of LLM-based software to prompts.","Given the complexity and diversity of real-world inputs, studying the robustness of LLMbased software in handling comprehensive inputs (including prompts and examples) is crucial for a thorough understanding of its performance.   ","To this end, this paper introduces RITFIS, a Robust Input Testing Framework for LLM-based Intelligent Software.","To our knowledge, RITFIS is the first framework designed to assess the robustness of LLM-based intelligent software against natural language inputs.","This framework, based on given threat models and prompts, primarily defines the testing process as a combinatorial optimization problem.","Successful test cases are determined by a goal function, creating a transformation space for the original examples through perturbation means, and employing a series of search methods to filter cases that meet both the testing objectives and language constraints.","RITFIS, with its modular design, offers a comprehensive method for evaluating the robustness of LLMbased intelligent software.   ","RITFIS adapts 17 automated testing methods, originally designed for Deep Neural Network (DNN)-based intelligent software, to the LLM-based software testing scenario.","It demonstrates the effectiveness of RITFIS in evaluating LLM-based intelligent software through empirical validation.","However, existing methods generally have limitations, especially when dealing with lengthy texts and structurally complex threat models.","Therefore, we conducted a comprehensive analysis based on five metrics and provided insightful testing method optimization strategies, benefiting both researchers and everyday users."],"url":"http://arxiv.org/abs/2402.13518v1","category":"cs.SE"}
{"created":"2024-02-21 03:55:02","title":"Self-DC: When to retrieve and When to generate? Self Divide-and-Conquer for Compositional Unknown Questions","abstract":"Retrieve-then-read and generate-then-read are two typical solutions to handle unknown and known questions in open-domain question-answering, while the former retrieves necessary external knowledge and the later prompt the large language models to generate internal known knowledge encoded in the parameters. However, few of previous works consider the compositional unknown questions, which consist of several known or unknown sub-questions. Thus, simple binary classification (known or unknown) becomes sub-optimal and inefficient since it will call external retrieval excessively for each compositional unknown question. To this end, we propose the first Compositional unknown Question-Answering dataset (CuQA), and introduce a Self Divide-and-Conquer (Self-DC) framework to empower LLMs to adaptively call different methods on-demand, resulting in better performance and efficiency. Experimental results on two datasets (CuQA and FreshQA) demonstrate that Self-DC can achieve comparable or even better performance with much more less retrieval times compared with several strong baselines.","sentences":["Retrieve-then-read and generate-then-read are two typical solutions to handle unknown and known questions in open-domain question-answering, while the former retrieves necessary external knowledge and the later prompt the large language models to generate internal known knowledge encoded in the parameters.","However, few of previous works consider the compositional unknown questions, which consist of several known or unknown sub-questions.","Thus, simple binary classification (known or unknown) becomes sub-optimal and inefficient since it will call external retrieval excessively for each compositional unknown question.","To this end, we propose the first Compositional unknown Question-Answering dataset (CuQA), and introduce a Self Divide-and-Conquer (Self-DC) framework to empower LLMs to adaptively call different methods on-demand, resulting in better performance and efficiency.","Experimental results on two datasets (CuQA and FreshQA) demonstrate that Self-DC can achieve comparable or even better performance with much more less retrieval times compared with several strong baselines."],"url":"http://arxiv.org/abs/2402.13514v1","category":"cs.CL"}
{"created":"2024-02-21 03:39:04","title":"SimPro: A Simple Probabilistic Framework Towards Realistic Long-Tailed Semi-Supervised Learning","abstract":"Recent advancements in semi-supervised learning have focused on a more realistic yet challenging task: addressing imbalances in labeled data while the class distribution of unlabeled data remains both unknown and potentially mismatched. Current approaches in this sphere often presuppose rigid assumptions regarding the class distribution of unlabeled data, thereby limiting the adaptability of models to only certain distribution ranges. In this study, we propose a novel approach, introducing a highly adaptable framework, designated as SimPro, which does not rely on any predefined assumptions about the distribution of unlabeled data. Our framework, grounded in a probabilistic model, innovatively refines the expectation-maximization (EM) algorithm by explicitly decoupling the modeling of conditional and marginal class distributions. This separation facilitates a closed-form solution for class distribution estimation during the maximization phase, leading to the formulation of a Bayes classifier. The Bayes classifier, in turn, enhances the quality of pseudo-labels in the expectation phase. Remarkably, the SimPro framework not only comes with theoretical guarantees but also is straightforward to implement. Moreover, we introduce two novel class distributions broadening the scope of the evaluation. Our method showcases consistent state-of-the-art performance across diverse benchmarks and data distribution scenarios. Our code is available at https://github.com/LeapLabTHU/SimPro.","sentences":["Recent advancements in semi-supervised learning have focused on a more realistic yet challenging task: addressing imbalances in labeled data while the class distribution of unlabeled data remains both unknown and potentially mismatched.","Current approaches in this sphere often presuppose rigid assumptions regarding the class distribution of unlabeled data, thereby limiting the adaptability of models to only certain distribution ranges.","In this study, we propose a novel approach, introducing a highly adaptable framework, designated as SimPro, which does not rely on any predefined assumptions about the distribution of unlabeled data.","Our framework, grounded in a probabilistic model, innovatively refines the expectation-maximization (EM) algorithm by explicitly decoupling the modeling of conditional and marginal class distributions.","This separation facilitates a closed-form solution for class distribution estimation during the maximization phase, leading to the formulation of a Bayes classifier.","The Bayes classifier, in turn, enhances the quality of pseudo-labels in the expectation phase.","Remarkably, the SimPro framework not only comes with theoretical guarantees but also is straightforward to implement.","Moreover, we introduce two novel class distributions broadening the scope of the evaluation.","Our method showcases consistent state-of-the-art performance across diverse benchmarks and data distribution scenarios.","Our code is available at https://github.com/LeapLabTHU/SimPro."],"url":"http://arxiv.org/abs/2402.13505v1","category":"cs.LG"}
{"created":"2024-02-21 03:09:21","title":"GradSafe: Detecting Unsafe Prompts for LLMs via Safety-Critical Gradient Analysis","abstract":"Large Language Models (LLMs) face threats from unsafe prompts. Existing methods for detecting unsafe prompts are primarily online moderation APIs or finetuned LLMs. These strategies, however, often require extensive and resource-intensive data collection and training processes. In this study, we propose GradSafe, which effectively detects unsafe prompts by scrutinizing the gradients of safety-critical parameters in LLMs. Our methodology is grounded in a pivotal observation: the gradients of an LLM's loss for unsafe prompts paired with compliance response exhibit similar patterns on certain safety-critical parameters. In contrast, safe prompts lead to markedly different gradient patterns. Building on this observation, GradSafe analyzes the gradients from prompts (paired with compliance responses) to accurately detect unsafe prompts. We show that GradSafe, applied to Llama-2 without further training, outperforms Llama Guard, despite its extensive finetuning with a large dataset, in detecting unsafe prompts. This superior performance is consistent across both zero-shot and adaptation scenarios, as evidenced by our evaluations on the ToxicChat and XSTest. The source code is available at https://github.com/xyq7/GradSafe.","sentences":["Large Language Models (LLMs) face threats from unsafe prompts.","Existing methods for detecting unsafe prompts are primarily online moderation APIs or finetuned LLMs.","These strategies, however, often require extensive and resource-intensive data collection and training processes.","In this study, we propose GradSafe, which effectively detects unsafe prompts by scrutinizing the gradients of safety-critical parameters in LLMs.","Our methodology is grounded in a pivotal observation: the gradients of an LLM's loss for unsafe prompts paired with compliance response exhibit similar patterns on certain safety-critical parameters.","In contrast, safe prompts lead to markedly different gradient patterns.","Building on this observation, GradSafe analyzes the gradients from prompts (paired with compliance responses) to accurately detect unsafe prompts.","We show that GradSafe, applied to Llama-2 without further training, outperforms Llama Guard, despite its extensive finetuning with a large dataset, in detecting unsafe prompts.","This superior performance is consistent across both zero-shot and adaptation scenarios, as evidenced by our evaluations on the ToxicChat and XSTest.","The source code is available at https://github.com/xyq7/GradSafe."],"url":"http://arxiv.org/abs/2402.13494v1","category":"cs.CL"}
{"created":"2024-02-21 03:05:50","title":"Retrieval Helps or Hurts? A Deeper Dive into the Efficacy of Retrieval Augmentation to Language Models","abstract":"While large language models (LMs) demonstrate remarkable performance, they encounter challenges in providing accurate responses when queried for information beyond their pre-trained memorization. Although augmenting them with relevant external information can mitigate these issues, failure to consider the necessity of retrieval may adversely affect overall performance. Previous research has primarily focused on examining how entities influence retrieval models and knowledge recall in LMs, leaving other aspects relatively unexplored. In this work, our goal is to offer a more detailed, fact-centric analysis by exploring the effects of combinations of entities and relations. To facilitate this, we construct a new question answering (QA) dataset called WiTQA (Wikipedia Triple Question Answers). This dataset includes questions about entities and relations of various popularity levels, each accompanied by a supporting passage. Our extensive experiments with diverse LMs and retrievers reveal when retrieval does not consistently enhance LMs from the viewpoints of fact-centric popularity.Confirming earlier findings, we observe that larger LMs excel in recalling popular facts. However, they notably encounter difficulty with infrequent entity-relation pairs compared to retrievers. Interestingly, they can effectively retain popular relations of less common entities. We demonstrate the efficacy of our finer-grained metric and insights through an adaptive retrieval system that selectively employs retrieval and recall based on the frequencies of entities and relations in the question.","sentences":["While large language models (LMs) demonstrate remarkable performance, they encounter challenges in providing accurate responses when queried for information beyond their pre-trained memorization.","Although augmenting them with relevant external information can mitigate these issues, failure to consider the necessity of retrieval may adversely affect overall performance.","Previous research has primarily focused on examining how entities influence retrieval models and knowledge recall in LMs, leaving other aspects relatively unexplored.","In this work, our goal is to offer a more detailed, fact-centric analysis by exploring the effects of combinations of entities and relations.","To facilitate this, we construct a new question answering (QA) dataset called WiTQA (Wikipedia Triple Question Answers).","This dataset includes questions about entities and relations of various popularity levels, each accompanied by a supporting passage.","Our extensive experiments with diverse LMs and retrievers reveal when retrieval does not consistently enhance LMs from the viewpoints of fact-centric popularity.","Confirming earlier findings, we observe that larger LMs excel in recalling popular facts.","However, they notably encounter difficulty with infrequent entity-relation pairs compared to retrievers.","Interestingly, they can effectively retain popular relations of less common entities.","We demonstrate the efficacy of our finer-grained metric and insights through an adaptive retrieval system that selectively employs retrieval and recall based on the frequencies of entities and relations in the question."],"url":"http://arxiv.org/abs/2402.13492v1","category":"cs.CL"}
{"created":"2024-02-21 01:33:47","title":"Approximation analysis for the minimization problem of difference-of-convex functions with Moreau envelopes","abstract":"In this work the minimization problem for the difference of convex (DC) functions is studied by using Moreau envelopes and the descent method with Moreau gradient is employed to approximate the numerical solution. The main regularization idea in this work is inspired by Hiriart-Urruty [14], Moudafi[17], regularize the components of the DC problem by adapting the different parameters and strategic matrices flexibly to evaluate the whole DC problem. It is shown that the inertial gradient method as well as the classic gradient descent scheme tend towards an approximation stationary point of the original problem.","sentences":["In this work the minimization problem for the difference of convex (DC) functions is studied by using Moreau envelopes and the descent method with Moreau gradient is employed to approximate the numerical solution.","The main regularization idea in this work is inspired by Hiriart-Urruty","[14], Moudafi[17], regularize the components of the DC problem by adapting the different parameters and strategic matrices flexibly to evaluate the whole DC problem.","It is shown that the inertial gradient method as well as the classic gradient descent scheme tend towards an approximation stationary point of the original problem."],"url":"http://arxiv.org/abs/2402.13461v1","category":"math.OC"}
{"created":"2024-02-20 22:15:13","title":"Layout-to-Image Generation with Localized Descriptions using ControlNet with Cross-Attention Control","abstract":"While text-to-image diffusion models can generate highquality images from textual descriptions, they generally lack fine-grained control over the visual composition of the generated images. Some recent works tackle this problem by training the model to condition the generation process on additional input describing the desired image layout. Arguably the most popular among such methods, ControlNet, enables a high degree of control over the generated image using various types of conditioning inputs (e.g. segmentation maps). However, it still lacks the ability to take into account localized textual descriptions that indicate which image region is described by which phrase in the prompt. In this work, we show the limitations of ControlNet for the layout-to-image task and enable it to use localized descriptions using a training-free approach that modifies the crossattention scores during generation. We adapt and investigate several existing cross-attention control methods in the context of ControlNet and identify shortcomings that cause failure (concept bleeding) or image degradation under specific conditions. To address these shortcomings, we develop a novel cross-attention manipulation method in order to maintain image quality while improving control. Qualitative and quantitative experimental studies focusing on challenging cases are presented, demonstrating the effectiveness of the investigated general approach, and showing the improvements obtained by the proposed cross-attention control method.","sentences":["While text-to-image diffusion models can generate highquality images from textual descriptions, they generally lack fine-grained control over the visual composition of the generated images.","Some recent works tackle this problem by training the model to condition the generation process on additional input describing the desired image layout.","Arguably the most popular among such methods, ControlNet, enables a high degree of control over the generated image using various types of conditioning inputs (e.g. segmentation maps).","However, it still lacks the ability to take into account localized textual descriptions that indicate which image region is described by which phrase in the prompt.","In this work, we show the limitations of ControlNet for the layout-to-image task and enable it to use localized descriptions using a training-free approach that modifies the crossattention scores during generation.","We adapt and investigate several existing cross-attention control methods in the context of ControlNet and identify shortcomings that cause failure (concept bleeding) or image degradation under specific conditions.","To address these shortcomings, we develop a novel cross-attention manipulation method in order to maintain image quality while improving control.","Qualitative and quantitative experimental studies focusing on challenging cases are presented, demonstrating the effectiveness of the investigated general approach, and showing the improvements obtained by the proposed cross-attention control method."],"url":"http://arxiv.org/abs/2402.13404v1","category":"cs.CV"}
{"created":"2024-02-20 22:12:33","title":"Towards accelerating physical discovery via non-interactive and interactive multi-fidelity Bayesian Optimization: Current challenges and future opportunities","abstract":"Both computational and experimental material discovery bring forth the challenge of exploring multidimensional and often non-differentiable parameter spaces, such as phase diagrams of Hamiltonians with multiple interactions, composition spaces of combinatorial libraries, processing spaces, and molecular embedding spaces. Often these systems are expensive or time-consuming to evaluate a single instance, and hence classical approaches based on exhaustive grid or random search are too data intensive. This resulted in strong interest towards active learning methods such as Bayesian optimization (BO) where the adaptive exploration occurs based on human learning (discovery) objective. However, classical BO is based on a predefined optimization target, and policies balancing exploration and exploitation are purely data driven. In practical settings, the domain expert can pose prior knowledge on the system in form of partially known physics laws and often varies exploration policies during the experiment. Here, we explore interactive workflows building on multi-fidelity BO (MFBO), starting with classical (data-driven) MFBO, then structured (physics-driven) sMFBO, and extending it to allow human in the loop interactive iMFBO workflows for adaptive and domain expert aligned exploration. These approaches are demonstrated over highly non-smooth multi-fidelity simulation data generated from an Ising model, considering spin-spin interaction as parameter space, lattice sizes as fidelity spaces, and the objective as maximizing heat capacity. Detailed analysis and comparison show the impact of physics knowledge injection and on-the-fly human decisions for improved exploration, current challenges, and potential opportunities for algorithm development with combining data, physics and real time human decisions.","sentences":["Both computational and experimental material discovery bring forth the challenge of exploring multidimensional and often non-differentiable parameter spaces, such as phase diagrams of Hamiltonians with multiple interactions, composition spaces of combinatorial libraries, processing spaces, and molecular embedding spaces.","Often these systems are expensive or time-consuming to evaluate a single instance, and hence classical approaches based on exhaustive grid or random search are too data intensive.","This resulted in strong interest towards active learning methods such as Bayesian optimization (BO) where the adaptive exploration occurs based on human learning (discovery) objective.","However, classical BO is based on a predefined optimization target, and policies balancing exploration and exploitation are purely data driven.","In practical settings, the domain expert can pose prior knowledge on the system in form of partially known physics laws and often varies exploration policies during the experiment.","Here, we explore interactive workflows building on multi-fidelity BO (MFBO), starting with classical (data-driven) MFBO, then structured (physics-driven) sMFBO, and extending it to allow human in the loop interactive iMFBO workflows for adaptive and domain expert aligned exploration.","These approaches are demonstrated over highly non-smooth multi-fidelity simulation data generated from an Ising model, considering spin-spin interaction as parameter space, lattice sizes as fidelity spaces, and the objective as maximizing heat capacity.","Detailed analysis and comparison show the impact of physics knowledge injection and on-the-fly human decisions for improved exploration, current challenges, and potential opportunities for algorithm development with combining data, physics and real time human decisions."],"url":"http://arxiv.org/abs/2402.13402v1","category":"cs.LG"}
{"created":"2024-02-20 22:06:43","title":"Dissipative solutions to the model of a general compressible viscous fluid with the Coulomb friction law boundary condition","abstract":"We study a model of a general compressible viscous fluid subject to the Coulomb friction law boundary condition. For this model, we introduce a dissipative formulation and prove the existence of dissipative solutions. The proof of this result consists of a three-level approximation method: A Galerkin approximation, the classical parabolic regularization of the continuity equation as well as convex regularizations of the potential generating the viscous stress and the boundary terms incorporating the Coulomb friction law into the dissipative formulation. This approach combines the techniques already known from the proof of the existence of dissipative solutions to a model of general compressible viscous fluids under inflow-outflow boundary conditions as well as the proof of the existence of a weak solution to the incompressible Navier-Stokes equations under the Coulomb friction law boundary condition. It is the first time that this type of boundary condition is considered in the case of compressible flow.","sentences":["We study a model of a general compressible viscous fluid subject to the Coulomb friction law boundary condition.","For this model, we introduce a dissipative formulation and prove the existence of dissipative solutions.","The proof of this result consists of a three-level approximation method: A Galerkin approximation, the classical parabolic regularization of the continuity equation as well as convex regularizations of the potential generating the viscous stress and the boundary terms incorporating the Coulomb friction law into the dissipative formulation.","This approach combines the techniques already known from the proof of the existence of dissipative solutions to a model of general compressible viscous fluids under inflow-outflow boundary conditions as well as the proof of the existence of a weak solution to the incompressible Navier-Stokes equations under the Coulomb friction law boundary condition.","It is the first time that this type of boundary condition is considered in the case of compressible flow."],"url":"http://arxiv.org/abs/2402.13401v1","category":"math.AP"}
{"created":"2024-02-20 21:59:41","title":"The Dimension of Self-Directed Learning","abstract":"Understanding the self-directed learning complexity has been an important problem that has captured the attention of the online learning theory community since the early 1990s. Within this framework, the learner is allowed to adaptively choose its next data point in making predictions unlike the setting in adversarial online learning.   In this paper, we study the self-directed learning complexity in both the binary and multi-class settings, and we develop a dimension, namely $SDdim$, that exactly characterizes the self-directed learning mistake-bound for any concept class. The intuition behind $SDdim$ can be understood as a two-player game called the \"labelling game\". Armed with this two-player game, we calculate $SDdim$ on a whole host of examples with notable results on axis-aligned rectangles, VC dimension $1$ classes, and linear separators. We demonstrate several learnability gaps with a central focus on self-directed learning and offline sequence learning models that include either the best or worst ordering. Finally, we extend our analysis to the self-directed binary agnostic setting where we derive upper and lower bounds.","sentences":["Understanding the self-directed learning complexity has been an important problem that has captured the attention of the online learning theory community since the early 1990s.","Within this framework, the learner is allowed to adaptively choose its next data point in making predictions unlike the setting in adversarial online learning.   ","In this paper, we study the self-directed learning complexity in both the binary and multi-class settings, and we develop a dimension, namely $SDdim$, that exactly characterizes the self-directed learning mistake-bound for any concept class.","The intuition behind $SDdim$ can be understood as a two-player game called the \"labelling game\".","Armed with this two-player game, we calculate $SDdim$ on a whole host of examples with notable results on axis-aligned rectangles, VC dimension $1$ classes, and linear separators.","We demonstrate several learnability gaps with a central focus on self-directed learning and offline sequence learning models that include either the best or worst ordering.","Finally, we extend our analysis to the self-directed binary agnostic setting where we derive upper and lower bounds."],"url":"http://arxiv.org/abs/2402.13400v1","category":"stat.ML"}
{"created":"2024-02-20 21:09:04","title":"Referee-Meta-Learning for Fast Adaptation of Locational Fairness","abstract":"When dealing with data from distinct locations, machine learning algorithms tend to demonstrate an implicit preference of some locations over the others, which constitutes biases that sabotage the spatial fairness of the algorithm. This unfairness can easily introduce biases in subsequent decision-making given broad adoptions of learning-based solutions in practice. However, locational biases in AI are largely understudied. To mitigate biases over locations, we propose a locational meta-referee (Meta-Ref) to oversee the few-shot meta-training and meta-testing of a deep neural network. Meta-Ref dynamically adjusts the learning rates for training samples of given locations to advocate a fair performance across locations, through an explicit consideration of locational biases and the characteristics of input data. We present a three-phase training framework to learn both a meta-learning-based predictor and an integrated Meta-Ref that governs the fairness of the model. Once trained with a distribution of spatial tasks, Meta-Ref is applied to samples from new spatial tasks (i.e., regions outside the training area) to promote fairness during the fine-tune step. We carried out experiments with two case studies on crop monitoring and transportation safety, which show Meta-Ref can improve locational fairness while keeping the overall prediction quality at a similar level.","sentences":["When dealing with data from distinct locations, machine learning algorithms tend to demonstrate an implicit preference of some locations over the others, which constitutes biases that sabotage the spatial fairness of the algorithm.","This unfairness can easily introduce biases in subsequent decision-making given broad adoptions of learning-based solutions in practice.","However, locational biases in AI are largely understudied.","To mitigate biases over locations, we propose a locational meta-referee (Meta-Ref) to oversee the few-shot meta-training and meta-testing of a deep neural network.","Meta-Ref dynamically adjusts the learning rates for training samples of given locations to advocate a fair performance across locations, through an explicit consideration of locational biases and the characteristics of input data.","We present a three-phase training framework to learn both a meta-learning-based predictor and an integrated Meta-Ref that governs the fairness of the model.","Once trained with a distribution of spatial tasks, Meta-Ref is applied to samples from new spatial tasks (i.e., regions outside the training area) to promote fairness during the fine-tune step.","We carried out experiments with two case studies on crop monitoring and transportation safety, which show Meta-Ref can improve locational fairness while keeping the overall prediction quality at a similar level."],"url":"http://arxiv.org/abs/2402.13379v1","category":"cs.LG"}
{"created":"2024-02-20 20:52:21","title":"An adaptive lattice Green's function method for external flows with two unbounded and one homogeneous directions","abstract":"We solve the incompressible Navier-Stokes equations using a lattice Green's function (LGF) approach, including immersed boundaries (IB) and adaptive mesh refinement (AMR), for external flows with one homogeneous direction (e.g. infinite cylinders of arbitrary cross-section). We hybridize a Fourier collocation (pseudo-spectral) method for the homogeneous direction with a specially designed, staggered-grid finite-volume scheme on an AMR grid. The Fourier series is also truncated variably according to the refinement level in the other directions. We derive new algorithms to tabulate the LGF of the screened Poisson operator and viscous integrating factor. After adapting other algorithmic details from the fully inhomogeneous case, we validate and demonstrate the new method with transitional and turbulent flows over a circular cylinder at $Re=300$ and $Re=12,000$, respectively.","sentences":["We solve the incompressible Navier-Stokes equations using a lattice Green's function (LGF) approach, including immersed boundaries (IB) and adaptive mesh refinement (AMR), for external flows with one homogeneous direction (e.g. infinite cylinders of arbitrary cross-section).","We hybridize a Fourier collocation (pseudo-spectral) method for the homogeneous direction with a specially designed, staggered-grid finite-volume scheme on an AMR grid.","The Fourier series is also truncated variably according to the refinement level in the other directions.","We derive new algorithms to tabulate the LGF of the screened Poisson operator and viscous integrating factor.","After adapting other algorithmic details from the fully inhomogeneous case, we validate and demonstrate the new method with transitional and turbulent flows over a circular cylinder at $Re=300$ and $Re=12,000$, respectively."],"url":"http://arxiv.org/abs/2402.13370v1","category":"physics.flu-dyn"}
{"created":"2024-02-20 20:44:40","title":"Statistical curriculum learning: An elimination algorithm achieving an oracle risk","abstract":"We consider a statistical version of curriculum learning (CL) in a parametric prediction setting. The learner is required to estimate a target parameter vector, and can adaptively collect samples from either the target model, or other source models that are similar to the target model, but less noisy. We consider three types of learners, depending on the level of side-information they receive. The first two, referred to as strong/weak-oracle learners, receive high/low degrees of information about the models, and use these to learn. The third, a fully adaptive learner, estimates the target parameter vector without any prior information. In the single source case, we propose an elimination learning method, whose risk matches that of a strong-oracle learner. In the multiple source case, we advocate that the risk of the weak-oracle learner is a realistic benchmark for the risk of adaptive learners. We develop an adaptive multiple elimination-rounds CL algorithm, and characterize instance-dependent conditions for its risk to match that of the weak-oracle learner. We consider instance-dependent minimax lower bounds, and discuss the challenges associated with defining the class of instances for the bound. We derive two minimax lower bounds, and determine the conditions under which the performance weak-oracle learner is minimax optimal.","sentences":["We consider a statistical version of curriculum learning (CL) in a parametric prediction setting.","The learner is required to estimate a target parameter vector, and can adaptively collect samples from either the target model, or other source models that are similar to the target model, but less noisy.","We consider three types of learners, depending on the level of side-information they receive.","The first two, referred to as strong/weak-oracle learners, receive high/low degrees of information about the models, and use these to learn.","The third, a fully adaptive learner, estimates the target parameter vector without any prior information.","In the single source case, we propose an elimination learning method, whose risk matches that of a strong-oracle learner.","In the multiple source case, we advocate that the risk of the weak-oracle learner is a realistic benchmark for the risk of adaptive learners.","We develop an adaptive multiple elimination-rounds CL algorithm, and characterize instance-dependent conditions for its risk to match that of the weak-oracle learner.","We consider instance-dependent minimax lower bounds, and discuss the challenges associated with defining the class of instances for the bound.","We derive two minimax lower bounds, and determine the conditions under which the performance weak-oracle learner is minimax optimal."],"url":"http://arxiv.org/abs/2402.13366v1","category":"cs.LG"}
{"created":"2024-02-20 20:42:02","title":"A Simple but Effective Approach to Improve Structured Language Model Output for Information Extraction","abstract":"Large language models (LLMs) have demonstrated impressive abilities in generating unstructured natural language according to instructions. However, their performance can be inconsistent when tasked with producing text that adheres to specific structured formats, which is crucial in applications like named entity recognition (NER) or relation extraction (RE). To address this issue, this paper introduces an efficient method, G&O, to enhance their structured text generation capabilities. It breaks the generation into a two-step pipeline: initially, LLMs generate answers in natural language as intermediate responses. Subsequently, LLMs are asked to organize the output into the desired structure, using the intermediate responses as context. G&O effectively separates the generation of content from the structuring process, reducing the pressure of completing two orthogonal tasks simultaneously. Tested on zero-shot NER and RE, the results indicate a significant improvement in LLM performance with minimal additional efforts. This straightforward and adaptable prompting technique can also be combined with other strategies, like self-consistency, to further elevate LLM capabilities in various structured text generation tasks.","sentences":["Large language models (LLMs) have demonstrated impressive abilities in generating unstructured natural language according to instructions.","However, their performance can be inconsistent when tasked with producing text that adheres to specific structured formats, which is crucial in applications like named entity recognition (NER) or relation extraction (RE).","To address this issue, this paper introduces an efficient method, G&O, to enhance their structured text generation capabilities.","It breaks the generation into a two-step pipeline: initially, LLMs generate answers in natural language as intermediate responses.","Subsequently, LLMs are asked to organize the output into the desired structure, using the intermediate responses as context.","G&O effectively separates the generation of content from the structuring process, reducing the pressure of completing two orthogonal tasks simultaneously.","Tested on zero-shot NER and RE, the results indicate a significant improvement in LLM performance with minimal additional efforts.","This straightforward and adaptable prompting technique can also be combined with other strategies, like self-consistency, to further elevate LLM capabilities in various structured text generation tasks."],"url":"http://arxiv.org/abs/2402.13364v1","category":"cs.CL"}
{"created":"2024-02-20 20:34:04","title":"Compact stars in $f(R,L_m,T)$ gravity","abstract":"This study explores the behavior of compact stars within the framework of $f(R,L_m,T)$ gravity, focusing on the functional form $f(R,L_m,T) = R + \\alpha TL_m$. The modified Tolman-Oppenheimer-Volkoff (TOV) equations are derived and numerically solved for several values of the free parameter $\\alpha$ by considering both quark and hadronic matter -- described by realistic equations of state (EoSs). Furthermore, the stellar structure equations are adapted for two different choices of the matter Lagrangian density (namely, $L_m= p$ and $L_m= -\\rho$), laying the groundwork for our numerical analysis. As expected, we recover the traditional TOV equations in General Relativity (GR) when $\\alpha \\rightarrow 0$. Remarkably, we found that the two choices for $L_m$ have appreciably different effects on the mass-radius diagrams. Results showcase the impact of $\\alpha$ on compact star properties, while final remarks summarize key findings and discuss implications, including compatibility with observational data from NGC 6397's neutron star. Overall, this research enhances comprehension of $f(R,L_m,T)$ gravity's effects on compact star internal structures, offering insights for future investigations.","sentences":["This study explores the behavior of compact stars within the framework of $f(R,L_m,T)$ gravity, focusing on the functional form $f(R,L_m,T) = R + \\alpha TL_m$. The modified Tolman-Oppenheimer-Volkoff (TOV) equations are derived and numerically solved for several values of the free parameter $\\alpha$ by considering both quark and hadronic matter -- described by realistic equations of state (EoSs).","Furthermore, the stellar structure equations are adapted for two different choices of the matter Lagrangian density (namely, $L_m= p$ and $L_m= -\\rho$), laying the groundwork for our numerical analysis.","As expected, we recover the traditional TOV equations in General Relativity (GR) when $\\alpha \\rightarrow 0$.","Remarkably, we found that the two choices for $L_m$ have appreciably different effects on the mass-radius diagrams.","Results showcase the impact of $\\alpha$ on compact star properties, while final remarks summarize key findings and discuss implications, including compatibility with observational data from NGC 6397's neutron star.","Overall, this research enhances comprehension of $f(R,L_m,T)$ gravity's effects on compact star internal structures, offering insights for future investigations."],"url":"http://arxiv.org/abs/2402.13360v1","category":"gr-qc"}
{"created":"2024-02-20 19:49:44","title":"Dissipative spatiotemporal soliton in a driven waveguide laser","abstract":"A distributed Kerr-lens mode locking regime can be realized in a waveguide laser by spatial profiling of the pump beam, thus creating a spatio-temporal soliton. Additional slow temporal modulation of the pump source stabilizes the spatio-temporal solution in a broad range of parameters, which are defined by the dynamic gain saturation. We choose a Cr:ZnS waveguide laser as a practical example, but such a regime is feasible in various waveguide and fiber oscillators. A far-reaching analogy with Bose-Einstein condensates allows using this approach to stabilization of the weakly dissipative BECs.","sentences":["A distributed Kerr-lens mode locking regime can be realized in a waveguide laser by spatial profiling of the pump beam, thus creating a spatio-temporal soliton.","Additional slow temporal modulation of the pump source stabilizes the spatio-temporal solution in a broad range of parameters, which are defined by the dynamic gain saturation.","We choose a Cr:ZnS waveguide laser as a practical example, but such a regime is feasible in various waveguide and fiber oscillators.","A far-reaching analogy with Bose-Einstein condensates allows using this approach to stabilization of the weakly dissipative BECs."],"url":"http://arxiv.org/abs/2402.13348v1","category":"physics.optics"}
{"created":"2024-02-20 19:08:16","title":"Quantum Control for Zeno effect with noises","abstract":"The quantum Zeno effect is a distinctive phenomenon in quantum mechanics, describing the nontrivial effect of frequent projective measurements on hindering the evolution of a quantum system. However, when subjecting to environmental noises, the quantum system may dissipate and the quantum Zeno effect no longer works. This research studies the physical mechanism for the decay of the quantum Zeno effect in the presence of noises, and investigates the effect of coherent quantum controls on mitigating the decrease of the survival probability that the system stays in the initial state induced by the noises. We derive the decay rate of the survival probability with and without coherent quantum controls in general, and show that when the frequency of the projective measurements is large but finite, proper coherent controls by sufficiently strong Hamiltonians can be designed to decrease the decay rate of the survival probability. A two-level quantum system suffering from typical unitary and non-unitary noises is then considered to demonstrate the effect of the proposed coherent quantum control scheme in protecting the quantum Zeno effect against the noises. The decay rate of the survival probability is obtained in the presence of the noises, and the control Hamiltonian is further optimized analytically to minimize the decay rate by a variational approach. The evolution paths of the quantum system with the optimal coherent controls is illustrated numerically for different scenarios to explicitly show how the coherent control scheme works in lowering the decay of survival probability.","sentences":["The quantum Zeno effect is a distinctive phenomenon in quantum mechanics, describing the nontrivial effect of frequent projective measurements on hindering the evolution of a quantum system.","However, when subjecting to environmental noises, the quantum system may dissipate and the quantum Zeno effect no longer works.","This research studies the physical mechanism for the decay of the quantum Zeno effect in the presence of noises, and investigates the effect of coherent quantum controls on mitigating the decrease of the survival probability that the system stays in the initial state induced by the noises.","We derive the decay rate of the survival probability with and without coherent quantum controls in general, and show that when the frequency of the projective measurements is large but finite, proper coherent controls by sufficiently strong Hamiltonians can be designed to decrease the decay rate of the survival probability.","A two-level quantum system suffering from typical unitary and non-unitary noises is then considered to demonstrate the effect of the proposed coherent quantum control scheme in protecting the quantum Zeno effect against the noises.","The decay rate of the survival probability is obtained in the presence of the noises, and the control Hamiltonian is further optimized analytically to minimize the decay rate by a variational approach.","The evolution paths of the quantum system with the optimal coherent controls is illustrated numerically for different scenarios to explicitly show how the coherent control scheme works in lowering the decay of survival probability."],"url":"http://arxiv.org/abs/2402.13325v2","category":"quant-ph"}
{"created":"2024-02-21 18:56:03","title":"D-Flow: Differentiating through Flows for Controlled Generation","abstract":"Taming the generation outcome of state of the art Diffusion and Flow-Matching (FM) models without having to re-train a task-specific model unlocks a powerful tool for solving inverse problems, conditional generation, and controlled generation in general. In this work we introduce D-Flow, a simple framework for controlling the generation process by differentiating through the flow, optimizing for the source (noise) point. We motivate this framework by our key observation stating that for Diffusion/FM models trained with Gaussian probability paths, differentiating through the generation process projects gradient on the data manifold, implicitly injecting the prior into the optimization process. We validate our framework on linear and non-linear controlled generation problems including: image and audio inverse problems and conditional molecule generation reaching state of the art performance across all.","sentences":["Taming the generation outcome of state of the art Diffusion and Flow-Matching (FM) models without having to re-train a task-specific model unlocks a powerful tool for solving inverse problems, conditional generation, and controlled generation in general.","In this work we introduce D-Flow, a simple framework for controlling the generation process by differentiating through the flow, optimizing for the source (noise) point.","We motivate this framework by our key observation stating that for Diffusion/FM models trained with Gaussian probability paths, differentiating through the generation process projects gradient on the data manifold, implicitly injecting the prior into the optimization process.","We validate our framework on linear and non-linear controlled generation problems including: image and audio inverse problems and conditional molecule generation reaching state of the art performance across all."],"url":"http://arxiv.org/abs/2402.14017v1","category":"cs.LG"}
{"created":"2024-02-21 18:50:12","title":"Geometry-Informed Neural Networks","abstract":"We introduce the concept of geometry-informed neural networks (GINNs), which encompass (i) learning under geometric constraints, (ii) neural fields as a suitable representation, and (iii) generating diverse solutions to under-determined systems often encountered in geometric tasks. Notably, the GINN formulation does not require training data, and as such can be considered generative modeling driven purely by constraints. We add an explicit diversity loss to mitigate mode collapse. We consider several constraints, in particular, the connectedness of components which we convert to a differentiable loss through Morse theory. Experimentally, we demonstrate the efficacy of the GINN learning paradigm across a range of two and three-dimensional scenarios with increasing levels of complexity.","sentences":["We introduce the concept of geometry-informed neural networks (GINNs), which encompass (i) learning under geometric constraints, (ii) neural fields as a suitable representation, and (iii) generating diverse solutions to under-determined systems often encountered in geometric tasks.","Notably, the GINN formulation does not require training data, and as such can be considered generative modeling driven purely by constraints.","We add an explicit diversity loss to mitigate mode collapse.","We consider several constraints, in particular, the connectedness of components which we convert to a differentiable loss through Morse theory.","Experimentally, we demonstrate the efficacy of the GINN learning paradigm across a range of two and three-dimensional scenarios with increasing levels of complexity."],"url":"http://arxiv.org/abs/2402.14009v1","category":"cs.LG"}
{"created":"2024-02-21 18:35:27","title":"Asymptotics of Learning with Deep Structured (Random) Features","abstract":"For a large class of feature maps we provide a tight asymptotic characterisation of the test error associated with learning the readout layer, in the high-dimensional limit where the input dimension, hidden layer widths, and number of training samples are proportionally large. This characterization is formulated in terms of the population covariance of the features. Our work is partially motivated by the problem of learning with Gaussian rainbow neural networks, namely deep non-linear fully-connected networks with random but structured weights, whose row-wise covariances are further allowed to depend on the weights of previous layers. For such networks we also derive a closed-form formula for the feature covariance in terms of the weight matrices. We further find that in some cases our results can capture feature maps learned by deep, finite-width neural networks trained under gradient descent.","sentences":["For a large class of feature maps we provide a tight asymptotic characterisation of the test error associated with learning the readout layer, in the high-dimensional limit where the input dimension, hidden layer widths, and number of training samples are proportionally large.","This characterization is formulated in terms of the population covariance of the features.","Our work is partially motivated by the problem of learning with Gaussian rainbow neural networks, namely deep non-linear fully-connected networks with random but structured weights, whose row-wise covariances are further allowed to depend on the weights of previous layers.","For such networks we also derive a closed-form formula for the feature covariance in terms of the weight matrices.","We further find that in some cases our results can capture feature maps learned by deep, finite-width neural networks trained under gradient descent."],"url":"http://arxiv.org/abs/2402.13999v1","category":"stat.ML"}
{"created":"2024-02-21 18:17:17","title":"Hamiltonian Descent and Coordinate Hamiltonian Descent","abstract":"We propose an optimization algorithm called Hamiltonian Descent, which is a direct counterpart of classical Hamiltonian Monte Carlo in sampling. We show that Hamiltonian Descent has a better convergence rate than the Chebyshev method for solving a linear system of equations, albeit with a high computational cost. To overcome the cost issue, we then propose Coordinate Hamiltonian Descent and its parallelizable variant, which turns out to encapsulate the classical Gauss-Seidel method, Successive Over-relaxation, Jacobi method, and more. The result not only offers a new perspective on these existing algorithms but also leads to a broader class of update schemes that guarantee the convergence.","sentences":["We propose an optimization algorithm called Hamiltonian Descent, which is a direct counterpart of classical Hamiltonian Monte Carlo in sampling.","We show that Hamiltonian Descent has a better convergence rate than the Chebyshev method for solving a linear system of equations, albeit with a high computational cost.","To overcome the cost issue, we then propose Coordinate Hamiltonian Descent and its parallelizable variant, which turns out to encapsulate the classical Gauss-Seidel method, Successive Over-relaxation, Jacobi method, and more.","The result not only offers a new perspective on these existing algorithms but also leads to a broader class of update schemes that guarantee the convergence."],"url":"http://arxiv.org/abs/2402.13988v1","category":"math.OC"}
{"created":"2024-02-21 18:16:48","title":"A Simple and Yet Fairly Effective Defense for Graph Neural Networks","abstract":"Graph Neural Networks (GNNs) have emerged as the dominant approach for machine learning on graph-structured data. However, concerns have arisen regarding the vulnerability of GNNs to small adversarial perturbations. Existing defense methods against such perturbations suffer from high time complexity and can negatively impact the model's performance on clean graphs. To address these challenges, this paper introduces NoisyGNNs, a novel defense method that incorporates noise into the underlying model's architecture. We establish a theoretical connection between noise injection and the enhancement of GNN robustness, highlighting the effectiveness of our approach. We further conduct extensive empirical evaluations on the node classification task to validate our theoretical findings, focusing on two popular GNNs: the GCN and GIN. The results demonstrate that NoisyGNN achieves superior or comparable defense performance to existing methods while minimizing added time complexity. The NoisyGNN approach is model-agnostic, allowing it to be integrated with different GNN architectures. Successful combinations of our NoisyGNN approach with existing defense techniques demonstrate even further improved adversarial defense results. Our code is publicly available at: https://github.com/Sennadir/NoisyGNN.","sentences":["Graph Neural Networks (GNNs) have emerged as the dominant approach for machine learning on graph-structured data.","However, concerns have arisen regarding the vulnerability of GNNs to small adversarial perturbations.","Existing defense methods against such perturbations suffer from high time complexity and can negatively impact the model's performance on clean graphs.","To address these challenges, this paper introduces NoisyGNNs, a novel defense method that incorporates noise into the underlying model's architecture.","We establish a theoretical connection between noise injection and the enhancement of GNN robustness, highlighting the effectiveness of our approach.","We further conduct extensive empirical evaluations on the node classification task to validate our theoretical findings, focusing on two popular GNNs: the GCN and GIN.","The results demonstrate that NoisyGNN achieves superior or comparable defense performance to existing methods while minimizing added time complexity.","The NoisyGNN approach is model-agnostic, allowing it to be integrated with different GNN architectures.","Successful combinations of our NoisyGNN approach with existing defense techniques demonstrate even further improved adversarial defense results.","Our code is publicly available at: https://github.com/Sennadir/NoisyGNN."],"url":"http://arxiv.org/abs/2402.13987v1","category":"cs.LG"}
{"created":"2024-02-21 18:12:07","title":"Stability-Aware Training of Neural Network Interatomic Potentials with Differentiable Boltzmann Estimators","abstract":"Neural network interatomic potentials (NNIPs) are an attractive alternative to ab-initio methods for molecular dynamics (MD) simulations. However, they can produce unstable simulations which sample unphysical states, limiting their usefulness for modeling phenomena occurring over longer timescales. To address these challenges, we present Stability-Aware Boltzmann Estimator (StABlE) Training, a multi-modal training procedure which combines conventional supervised training from quantum-mechanical energies and forces with reference system observables, to produce stable and accurate NNIPs. StABlE Training iteratively runs MD simulations to seek out unstable regions, and corrects the instabilities via supervision with a reference observable. The training procedure is enabled by the Boltzmann Estimator, which allows efficient computation of gradients required to train neural networks to system observables, and can detect both global and local instabilities. We demonstrate our methodology across organic molecules, tetrapeptides, and condensed phase systems, along with using three modern NNIP architectures. In all three cases, StABlE-trained models achieve significant improvements in simulation stability and recovery of structural and dynamic observables. In some cases, StABlE-trained models outperform conventional models trained on datasets 50 times larger. As a general framework applicable across NNIP architectures and systems, StABlE Training is a powerful tool for training stable and accurate NNIPs, particularly in the absence of large reference datasets.","sentences":["Neural network interatomic potentials (NNIPs) are an attractive alternative to ab-initio methods for molecular dynamics (MD) simulations.","However, they can produce unstable simulations which sample unphysical states, limiting their usefulness for modeling phenomena occurring over longer timescales.","To address these challenges, we present Stability-Aware Boltzmann Estimator (StABlE) Training, a multi-modal training procedure which combines conventional supervised training from quantum-mechanical energies and forces with reference system observables, to produce stable and accurate NNIPs.","StABlE Training iteratively runs MD simulations to seek out unstable regions, and corrects the instabilities via supervision with a reference observable.","The training procedure is enabled by the Boltzmann Estimator, which allows efficient computation of gradients required to train neural networks to system observables, and can detect both global and local instabilities.","We demonstrate our methodology across organic molecules, tetrapeptides, and condensed phase systems, along with using three modern NNIP architectures.","In all three cases, StABlE-trained models achieve significant improvements in simulation stability and recovery of structural and dynamic observables.","In some cases, StABlE-trained models outperform conventional models trained on datasets 50 times larger.","As a general framework applicable across NNIP architectures and systems, StABlE Training is a powerful tool for training stable and accurate NNIPs, particularly in the absence of large reference datasets."],"url":"http://arxiv.org/abs/2402.13984v1","category":"cs.LG"}
{"created":"2024-02-21 18:09:12","title":"Geometry-induced wavefunction collapse","abstract":"When a quantum particle moves in a curved space, a geometric potential can arise. In spite of a long history of extensive theoretical studies, to experimentally observe the geometric potential remains to be a challenge. What are the physically observable consequences of such a geometric potential? Solving the Schrodinger equation on a truncated conic surface, we uncover a class of quantum scattering states that bear a strong resemblance with the quasi-resonant states associated with atomic collapse about a Coulomb impurity, a remarkable quantum phenomenon in which an infinite number of quasi-resonant states emerge. A characteristic defining feature of such collapse states is the infinite oscillations of the local density of states (LDOS) about the zero energy point separating the scattering from the bound states. The emergence of such states in the curved (Riemannian) space requires neither a relativistic quantum mechanism nor any Coulomb impurity: they have zero angular momentum and their origin is purely geometrical - henceforth the term geometry-induced wavefunction collapse. We establish the collapsing nature of these states through a detailed comparative analysis of the behavior of the LDOS for both the zero and finite angular-momentum states as well as the corresponding classical picture. Potential experimental schemes to realize the geometry-induced collapse states are articulated. Not only has our study uncovered an intrinsic connection between the geometric potential and atomic collapse, it also provides a method to experimentally observe and characterize geometric potentials arising from different subfields of physics. For example, in nanoscience and nanotechnology, curved geometry has become increasingly common. Our finding suggests that wavefunction collapse should be an important factor of consideration in designing and developing nanodevices.","sentences":["When a quantum particle moves in a curved space, a geometric potential can arise.","In spite of a long history of extensive theoretical studies, to experimentally observe the geometric potential remains to be a challenge.","What are the physically observable consequences of such a geometric potential?","Solving the Schrodinger equation on a truncated conic surface, we uncover a class of quantum scattering states that bear a strong resemblance with the quasi-resonant states associated with atomic collapse about a Coulomb impurity, a remarkable quantum phenomenon in which an infinite number of quasi-resonant states emerge.","A characteristic defining feature of such collapse states is the infinite oscillations of the local density of states (LDOS) about the zero energy point separating the scattering from the bound states.","The emergence of such states in the curved (Riemannian) space requires neither a relativistic quantum mechanism nor any Coulomb impurity: they have zero angular momentum and their origin is purely geometrical - henceforth the term geometry-induced wavefunction collapse.","We establish the collapsing nature of these states through a detailed comparative analysis of the behavior of the LDOS for both the zero and finite angular-momentum states as well as the corresponding classical picture.","Potential experimental schemes to realize the geometry-induced collapse states are articulated.","Not only has our study uncovered an intrinsic connection between the geometric potential and atomic collapse, it also provides a method to experimentally observe and characterize geometric potentials arising from different subfields of physics.","For example, in nanoscience and nanotechnology, curved geometry has become increasingly common.","Our finding suggests that wavefunction collapse should be an important factor of consideration in designing and developing nanodevices."],"url":"http://arxiv.org/abs/2402.13980v1","category":"quant-ph"}
{"created":"2024-02-21 17:58:10","title":"Linear-Time Graph Neural Networks for Scalable Recommendations","abstract":"In an era of information explosion, recommender systems are vital tools to deliver personalized recommendations for users. The key of recommender systems is to forecast users' future behaviors based on previous user-item interactions. Due to their strong expressive power of capturing high-order connectivities in user-item interaction data, recent years have witnessed a rising interest in leveraging Graph Neural Networks (GNNs) to boost the prediction performance of recommender systems. Nonetheless, classic Matrix Factorization (MF) and Deep Neural Network (DNN) approaches still play an important role in real-world large-scale recommender systems due to their scalability advantages. Despite the existence of GNN-acceleration solutions, it remains an open question whether GNN-based recommender systems can scale as efficiently as classic MF and DNN methods. In this paper, we propose a Linear-Time Graph Neural Network (LTGNN) to scale up GNN-based recommender systems to achieve comparable scalability as classic MF approaches while maintaining GNNs' powerful expressiveness for superior prediction accuracy. Extensive experiments and ablation studies are presented to validate the effectiveness and scalability of the proposed algorithm. Our implementation based on PyTorch is available.","sentences":["In an era of information explosion, recommender systems are vital tools to deliver personalized recommendations for users.","The key of recommender systems is to forecast users' future behaviors based on previous user-item interactions.","Due to their strong expressive power of capturing high-order connectivities in user-item interaction data, recent years have witnessed a rising interest in leveraging Graph Neural Networks (GNNs) to boost the prediction performance of recommender systems.","Nonetheless, classic Matrix Factorization (MF) and Deep Neural Network (DNN) approaches still play an important role in real-world large-scale recommender systems due to their scalability advantages.","Despite the existence of GNN-acceleration solutions, it remains an open question whether GNN-based recommender systems can scale as efficiently as classic MF and DNN methods.","In this paper, we propose a Linear-Time Graph Neural Network (LTGNN) to scale up GNN-based recommender systems to achieve comparable scalability as classic MF approaches while maintaining GNNs' powerful expressiveness for superior prediction accuracy.","Extensive experiments and ablation studies are presented to validate the effectiveness and scalability of the proposed algorithm.","Our implementation based on PyTorch is available."],"url":"http://arxiv.org/abs/2402.13973v1","category":"cs.IR"}
{"created":"2024-02-21 17:57:40","title":"Multi-indice B-series","abstract":"We propose a novel way to describe numerical methods for ordinary differential equations via the notion of multi-indice. The main idea is to replace rooted trees in Butcher's B-series by multi-indices. The latter were introduced recently in the context of describing solutions of singular stochastic partial differential equations. The combinatorial shift away from rooted trees allows for a compressed description of numerical schemes. Moreover, these multi-indices B-series characterise uniquely the Taylor development of local and affine equivariant maps.","sentences":["We propose a novel way to describe numerical methods for ordinary differential equations via the notion of multi-indice.","The main idea is to replace rooted trees in Butcher's B-series by multi-indices.","The latter were introduced recently in the context of describing solutions of singular stochastic partial differential equations.","The combinatorial shift away from rooted trees allows for a compressed description of numerical schemes.","Moreover, these multi-indices B-series characterise uniquely the Taylor development of local and affine equivariant maps."],"url":"http://arxiv.org/abs/2402.13971v1","category":"math.NA"}
{"created":"2024-02-21 17:36:07","title":"Can You Learn Semantics Through Next-Word Prediction? The Case of Entailment","abstract":"Do LMs infer the semantics of text from co-occurrence patterns in their training data? Merrill et al. (2022) argue that, in theory, probabilities predicted by an optimal LM encode semantic information about entailment relations, but it is unclear whether neural LMs trained on corpora learn entailment in this way because of strong idealizing assumptions made by Merrill et al. In this work, we investigate whether their theory can be used to decode entailment judgments from neural LMs. We find that a test similar to theirs can decode entailment relations between natural sentences, well above random chance, though not perfectly, across many datasets and LMs. This suggests LMs implicitly model aspects of semantics to predict semantic effects on sentence co-occurrence patterns. However, we find the test that predicts entailment in practice works in the opposite direction to the theoretical test. We thus revisit the assumptions underlying the original test, finding its derivation did not adequately account for redundancy in human-written text. We argue that correctly accounting for redundancy related to explanations might derive the observed flipped test and, more generally, improve linguistic theories of human speakers.","sentences":["Do LMs infer the semantics of text from co-occurrence patterns in their training data?","Merrill et al. (2022) argue that, in theory, probabilities predicted by an optimal LM encode semantic information about entailment relations, but it is unclear whether neural LMs trained on corpora learn entailment in this way because of strong idealizing assumptions made by Merrill et al.","In this work, we investigate whether their theory can be used to decode entailment judgments from neural LMs.","We find that a test similar to theirs can decode entailment relations between natural sentences, well above random chance, though not perfectly, across many datasets and LMs.","This suggests LMs implicitly model aspects of semantics to predict semantic effects on sentence co-occurrence patterns.","However, we find the test that predicts entailment in practice works in the opposite direction to the theoretical test.","We thus revisit the assumptions underlying the original test, finding its derivation did not adequately account for redundancy in human-written text.","We argue that correctly accounting for redundancy related to explanations might derive the observed flipped test and, more generally, improve linguistic theories of human speakers."],"url":"http://arxiv.org/abs/2402.13956v1","category":"cs.CL"}
{"created":"2024-02-21 17:35:51","title":"BEE-NET: A deep neural network to identify in-the-wild Bodily Expression of Emotions","abstract":"In this study, we investigate how environmental factors, specifically the scenes and objects involved, can affect the expression of emotions through body language. To this end, we introduce a novel multi-stream deep convolutional neural network named BEE-NET. We also propose a new late fusion strategy that incorporates meta-information on places and objects as prior knowledge in the learning process. Our proposed probabilistic pooling model leverages this information to generate a joint probability distribution of both available and anticipated non-available contextual information in latent space. Importantly, our fusion strategy is differentiable, allowing for end-to-end training and capturing of hidden associations among data points without requiring further post-processing or regularisation. To evaluate our deep model, we use the Body Language Database (BoLD), which is currently the largest available database for the Automatic Identification of the in-the-wild Bodily Expression of Emotions (AIBEE). Our experimental results demonstrate that our proposed approach surpasses the current state-of-the-art in AIBEE by a margin of 2.07%, achieving an Emotional Recognition Score of 66.33%.","sentences":["In this study, we investigate how environmental factors, specifically the scenes and objects involved, can affect the expression of emotions through body language.","To this end, we introduce a novel multi-stream deep convolutional neural network named BEE-NET.","We also propose a new late fusion strategy that incorporates meta-information on places and objects as prior knowledge in the learning process.","Our proposed probabilistic pooling model leverages this information to generate a joint probability distribution of both available and anticipated non-available contextual information in latent space.","Importantly, our fusion strategy is differentiable, allowing for end-to-end training and capturing of hidden associations among data points without requiring further post-processing or regularisation.","To evaluate our deep model, we use the Body Language Database (BoLD), which is currently the largest available database for the Automatic Identification of the in-the-wild Bodily Expression of Emotions (AIBEE).","Our experimental results demonstrate that our proposed approach surpasses the current state-of-the-art in AIBEE by a margin of 2.07%, achieving an Emotional Recognition Score of 66.33%."],"url":"http://arxiv.org/abs/2402.13955v1","category":"cs.CV"}
{"created":"2024-02-21 17:29:05","title":"A Post-Newtonian Analysis of Regularized 4D-EGB Theory: Complete Set of PPN Parameters and Observational Constraints","abstract":"We performed a post-Newtonian analysis of the regularized four-dimensional Einstein-Gauss-Bonnet gravitational theory (4D-EGB). The resulting metric differs from the classical parametrized post-Newtonian (PPN) formalism in that a new gravitational potential arises from the integration of the approximate field equations. We also investigated the conserved quantities and equations of motion for massive bodies and light rays to a certain degree. By computing the predicted periastron advance rate in a binary system, we obtained an observational constraint that is stronger than those of previous analyses. Although the usual 10 PPN parameters can still be derived within the PPN framework, an extra parameter is needed to account for the full post-Newtonian tests.","sentences":["We performed a post-Newtonian analysis of the regularized four-dimensional Einstein-Gauss-Bonnet gravitational theory (4D-EGB).","The resulting metric differs from the classical parametrized post-Newtonian (PPN) formalism in that a new gravitational potential arises from the integration of the approximate field equations.","We also investigated the conserved quantities and equations of motion for massive bodies and light rays to a certain degree.","By computing the predicted periastron advance rate in a binary system, we obtained an observational constraint that is stronger than those of previous analyses.","Although the usual 10 PPN parameters can still be derived within the PPN framework, an extra parameter is needed to account for the full post-Newtonian tests."],"url":"http://arxiv.org/abs/2402.13951v1","category":"gr-qc"}
{"created":"2024-02-21 17:23:29","title":"Improved Syndrome-based Neural Decoder for Linear Block Codes","abstract":"In this work, we investigate the problem of neural-based error correction decoding, and more specifically, the new so-called syndrome-based decoding technique introduced to tackle scalability in the training phase for larger code sizes. We improve on previous works in terms of allowing full decoding of the message rather than codewords, allowing thus the application to non-systematic codes, and proving that the single-message training property is still viable. The suggested system is implemented and tested on polar codes of sizes (64,32) and (128,64), and a BCH of size (63,51), leading to a significant improvement in both Bit Error Rate (BER) and Frame Error Rate (FER), with gains between 0.3dB and 1dB for the implemented codes in the high Signal-to-Noise Ratio (SNR) regime.","sentences":["In this work, we investigate the problem of neural-based error correction decoding, and more specifically, the new so-called syndrome-based decoding technique introduced to tackle scalability in the training phase for larger code sizes.","We improve on previous works in terms of allowing full decoding of the message rather than codewords, allowing thus the application to non-systematic codes, and proving that the single-message training property is still viable.","The suggested system is implemented and tested on polar codes of sizes (64,32) and (128,64), and a BCH of size (63,51), leading to a significant improvement in both Bit Error Rate (BER) and Frame Error Rate (FER), with gains between 0.3dB and 1dB for the implemented codes in the high Signal-to-Noise Ratio (SNR) regime."],"url":"http://arxiv.org/abs/2402.13948v2","category":"cs.IT"}
{"created":"2024-02-21 17:18:25","title":"AttackGNN: Red-Teaming GNNs in Hardware Security Using Reinforcement Learning","abstract":"Machine learning has shown great promise in addressing several critical hardware security problems. In particular, researchers have developed novel graph neural network (GNN)-based techniques for detecting intellectual property (IP) piracy, detecting hardware Trojans (HTs), and reverse engineering circuits, to name a few. These techniques have demonstrated outstanding accuracy and have received much attention in the community. However, since these techniques are used for security applications, it is imperative to evaluate them thoroughly and ensure they are robust and do not compromise the security of integrated circuits.   In this work, we propose AttackGNN, the first red-team attack on GNN-based techniques in hardware security. To this end, we devise a novel reinforcement learning (RL) agent that generates adversarial examples, i.e., circuits, against the GNN-based techniques. We overcome three challenges related to effectiveness, scalability, and generality to devise a potent RL agent. We target five GNN-based techniques for four crucial classes of problems in hardware security: IP piracy, detecting/localizing HTs, reverse engineering, and hardware obfuscation. Through our approach, we craft circuits that fool all GNNs considered in this work. For instance, to evade IP piracy detection, we generate adversarial pirated circuits that fool the GNN-based defense into classifying our crafted circuits as not pirated. For attacking HT localization GNN, our attack generates HT-infested circuits that fool the defense on all tested circuits. We obtain a similar 100% success rate against GNNs for all classes of problems.","sentences":["Machine learning has shown great promise in addressing several critical hardware security problems.","In particular, researchers have developed novel graph neural network (GNN)-based techniques for detecting intellectual property (IP) piracy, detecting hardware Trojans (HTs), and reverse engineering circuits, to name a few.","These techniques have demonstrated outstanding accuracy and have received much attention in the community.","However, since these techniques are used for security applications, it is imperative to evaluate them thoroughly and ensure they are robust and do not compromise the security of integrated circuits.   ","In this work, we propose AttackGNN, the first red-team attack on GNN-based techniques in hardware security.","To this end, we devise a novel reinforcement learning (RL) agent that generates adversarial examples, i.e., circuits, against the GNN-based techniques.","We overcome three challenges related to effectiveness, scalability, and generality to devise a potent RL agent.","We target five GNN-based techniques for four crucial classes of problems in hardware security: IP piracy, detecting/localizing HTs, reverse engineering, and hardware obfuscation.","Through our approach, we craft circuits that fool all GNNs considered in this work.","For instance, to evade IP piracy detection, we generate adversarial pirated circuits that fool the GNN-based defense into classifying our crafted circuits as not pirated.","For attacking HT localization GNN, our attack generates HT-infested circuits that fool the defense on all tested circuits.","We obtain a similar 100% success rate against GNNs for all classes of problems."],"url":"http://arxiv.org/abs/2402.13946v1","category":"cs.LG"}
{"created":"2024-02-21 17:11:07","title":"The Maintenance of Coherent Vortex Topology by Lagrangian Chaos in Drift-Rossby Wave Turbulence","abstract":"This work introduces the \"potential vorticity (PV) bucket brigade\", a conceptual mechanism for explaining the resilience of coherent vortex structures in drift-Rossby wave turbulence, critical for understanding turbulent transport in magnetically confined fusion plasmas and geophysical flows. Drawing parallels with the PV staircase, we show how spatially inhomogeneous patterns of mixing can reinforce, rather than destroy non-zonal structures in the flow. We accomplish this through an exact stochastic Lagrangian representation of vorticity transport, together with a wave eigenmode near-integrability property which elucidates the relationship between coherent flow topology and fluid relabeling symmetries. For concreteness, we demonstrate these ideas in a transitional regime of gradient-driven drift wave turbulence modeled by the flux-balanced Hasegawa-Wakatani equations. However, the tools we develop here are model-agnostic and have potential relevance to fluid and plasma systems well beyond the model studied here.","sentences":["This work introduces the \"potential vorticity (PV) bucket brigade\", a conceptual mechanism for explaining the resilience of coherent vortex structures in drift-Rossby wave turbulence, critical for understanding turbulent transport in magnetically confined fusion plasmas and geophysical flows.","Drawing parallels with the PV staircase, we show how spatially inhomogeneous patterns of mixing can reinforce, rather than destroy non-zonal structures in the flow.","We accomplish this through an exact stochastic Lagrangian representation of vorticity transport, together with a wave eigenmode near-integrability property which elucidates the relationship between coherent flow topology and fluid relabeling symmetries.","For concreteness, we demonstrate these ideas in a transitional regime of gradient-driven drift wave turbulence modeled by the flux-balanced Hasegawa-Wakatani equations.","However, the tools we develop here are model-agnostic and have potential relevance to fluid and plasma systems well beyond the model studied here."],"url":"http://arxiv.org/abs/2402.13942v1","category":"physics.plasm-ph"}
{"created":"2024-02-21 17:09:29","title":"On the topological classification of complex plane curve singularities","abstract":"This final degree project is devoted to study the topological classification of complex plane curves. These are subsets of $\\mathbb{C}^2$ that can be described by an equation $f(x,y)=0$. Loosely speaking, curves are said to be equivalent in a topological sense whenever they are ambient homeomorphic, i.e., there exists an orientation-preserving homeomorphism of the ambient space carrying one curve to the other. The project's aim is to develop operative and clear conditions to determine whether two curves are equivalent. Curves will be shown to be decomposable into branches: sets that can be explicitly parametrised in the form $x=\\phi(t), y=\\psi(t)$. These parametric expressions will be analysed to extract a complete numerical invariant for the classification of branches: the Puiseux characteristic. The intermediate key result to lend this notion with topological weight is that a branch can be completely described through its associated knot, arising from the intersection of the branch with a small enough 3-sphere. The combination of these above-mentioned facts will then culminate in the project's most powerful result, which assures that two curves are equivalent if and only if their branches share the same Puiseux characteristics and intersection numbers.","sentences":["This final degree project is devoted to study the topological classification of complex plane curves.","These are subsets of $\\mathbb{C}^2$ that can be described by an equation $f(x,y)=0$. Loosely speaking, curves are said to be equivalent in a topological sense whenever they are ambient homeomorphic, i.e., there exists an orientation-preserving homeomorphism of the ambient space carrying one curve to the other.","The project's aim is to develop operative and clear conditions to determine whether two curves are equivalent.","Curves will be shown to be decomposable into branches: sets that can be explicitly parametrised in the form $x=\\phi(t),","y=\\psi(t)$. These parametric expressions will be analysed to extract a complete numerical invariant for the classification of branches: the Puiseux characteristic.","The intermediate key result to lend this notion with topological weight is that a branch can be completely described through its associated knot, arising from the intersection of the branch with a small enough 3-sphere.","The combination of these above-mentioned facts will then culminate in the project's most powerful result, which assures that two curves are equivalent if and only if their branches share the same Puiseux characteristics and intersection numbers."],"url":"http://arxiv.org/abs/2402.13941v1","category":"math.AG"}
{"created":"2024-02-21 17:05:27","title":"Verifying message-passing neural networks via topology-based bounds tightening","abstract":"Since graph neural networks (GNNs) are often vulnerable to attack, we need to know when we can trust them. We develop a computationally effective approach towards providing robust certificates for message-passing neural networks (MPNNs) using a Rectified Linear Unit (ReLU) activation function. Because our work builds on mixed-integer optimization, it encodes a wide variety of subproblems, for example it admits (i) both adding and removing edges, (ii) both global and local budgets, and (iii) both topological perturbations and feature modifications. Our key technology, topology-based bounds tightening, uses graph structure to tighten bounds. We also experiment with aggressive bounds tightening to dynamically change the optimization constraints by tightening variable bounds. To demonstrate the effectiveness of these strategies, we implement an extension to the open-source branch-and-cut solver SCIP. We test on both node and graph classification problems and consider topological attacks that both add and remove edges.","sentences":["Since graph neural networks (GNNs) are often vulnerable to attack, we need to know when we can trust them.","We develop a computationally effective approach towards providing robust certificates for message-passing neural networks (MPNNs) using a Rectified Linear Unit (ReLU) activation function.","Because our work builds on mixed-integer optimization, it encodes a wide variety of subproblems, for example it admits (i) both adding and removing edges, (ii) both global and local budgets, and (iii) both topological perturbations and feature modifications.","Our key technology, topology-based bounds tightening, uses graph structure to tighten bounds.","We also experiment with aggressive bounds tightening to dynamically change the optimization constraints by tightening variable bounds.","To demonstrate the effectiveness of these strategies, we implement an extension to the open-source branch-and-cut solver SCIP.","We test on both node and graph classification problems and consider topological attacks that both add and remove edges."],"url":"http://arxiv.org/abs/2402.13937v1","category":"math.OC"}
{"created":"2024-02-21 17:00:41","title":"Powerful Large-scale Inference in High Dimensional Mediation Analysis","abstract":"In genome-wide epigenetic studies, exposures (e.g., Single Nucleotide Polymorphisms) affect outcomes (e.g., gene expression) through intermediate variables such as DNA methylation. Mediation analysis offers a way to study these intermediate variables and identify the presence or absence of causal mediation effects. Testing for mediation effects lead to a composite null hypothesis. Existing methods like the Sobel's test or the Max-P test are often underpowered because 1) statistical inference is often conducted based on distributions determined under a subset of the null and 2) they are not designed to shoulder the multiple testing burden. To tackle these issues, we introduce a technique called MLFDR (Mediation Analysis using Local False Discovery Rates) for high dimensional mediation analysis, which uses the local False Discovery Rates based on the coefficients of the structural equation model specifying the mediation relationship to construct a rejection region. We have shown theoretically as well as through simulation studies that in the high-dimensional setting, the new method of identifying the mediating variables controls the FDR asymptotically and performs better with respect to power than several existing methods such as DACT (Liu et al.)and JS-mixture (Dai et al).","sentences":["In genome-wide epigenetic studies, exposures (e.g., Single Nucleotide Polymorphisms) affect outcomes (e.g., gene expression) through intermediate variables such as DNA methylation.","Mediation analysis offers a way to study these intermediate variables and identify the presence or absence of causal mediation effects.","Testing for mediation effects lead to a composite null hypothesis.","Existing methods like the Sobel's test or the Max-P test are often underpowered because 1) statistical inference is often conducted based on distributions determined under a subset of the null and 2) they are not designed to shoulder the multiple testing burden.","To tackle these issues, we introduce a technique called MLFDR (Mediation Analysis using Local False Discovery Rates) for high dimensional mediation analysis, which uses the local False Discovery Rates based on the coefficients of the structural equation model specifying the mediation relationship to construct a rejection region.","We have shown theoretically as well as through simulation studies that in the high-dimensional setting, the new method of identifying the mediating variables controls the FDR asymptotically and performs better with respect to power than several existing methods such as DACT (Liu et al.)and JS-mixture (Dai et al)."],"url":"http://arxiv.org/abs/2402.13933v1","category":"stat.ME"}
{"created":"2024-02-21 16:31:45","title":"Bias correction of wind power forecasts with SCADA data and continuous learning","abstract":"Wind energy plays a critical role in the transition towards renewable energy sources. However, the uncertainty and variability of wind can impede its full potential and the necessary growth of wind power capacity. To mitigate these challenges, wind power forecasting methods are employed for applications in power management, energy trading, or maintenance scheduling. In this work, we present, evaluate, and compare four machine learning-based wind power forecasting models. Our models correct and improve 48-hour forecasts extracted from a numerical weather prediction (NWP) model. The models are evaluated on datasets from a wind park comprising 65 wind turbines. The best improvement in forecasting error and mean bias was achieved by a convolutional neural network, reducing the average NRMSE down to 22%, coupled with a significant reduction in mean bias, compared to a NRMSE of 35% from the strongly biased baseline model using uncorrected NWP forecasts. Our findings further indicate that changes to neural network architectures play a minor role in affecting the forecasting performance, and that future research should rather investigate changes in the model pipeline. Moreover, we introduce a continuous learning strategy, which is shown to achieve the highest forecasting performance improvements when new data is made available.","sentences":["Wind energy plays a critical role in the transition towards renewable energy sources.","However, the uncertainty and variability of wind can impede its full potential and the necessary growth of wind power capacity.","To mitigate these challenges, wind power forecasting methods are employed for applications in power management, energy trading, or maintenance scheduling.","In this work, we present, evaluate, and compare four machine learning-based wind power forecasting models.","Our models correct and improve 48-hour forecasts extracted from a numerical weather prediction (NWP) model.","The models are evaluated on datasets from a wind park comprising 65 wind turbines.","The best improvement in forecasting error and mean bias was achieved by a convolutional neural network, reducing the average NRMSE down to 22%, coupled with a significant reduction in mean bias, compared to a NRMSE of 35% from the strongly biased baseline model using uncorrected NWP forecasts.","Our findings further indicate that changes to neural network architectures play a minor role in affecting the forecasting performance, and that future research should rather investigate changes in the model pipeline.","Moreover, we introduce a continuous learning strategy, which is shown to achieve the highest forecasting performance improvements when new data is made available."],"url":"http://arxiv.org/abs/2402.13916v1","category":"cs.LG"}
{"created":"2024-02-21 16:09:49","title":"Room-temperature quantum sensing with photoexcited triplet electrons in organic crystals","abstract":"Quantum sensors have notably advanced high-sensitivity magnetic field detection. Here, we report quantum sensors constructed from polarized spin-triplet electrons in photoexcited organic chromophores, specifically focusing on pentacene-doped para-terphenyl (${\\approx}$0.1%). We demonstrate essential quantum sensing properties at room temperature: electronic optical polarization and state-dependent fluorescence contrast, by leveraging differential pumping and relaxation rates between triplet and ground states. We measure high optically detected magnetic resonance (ODMR) contrast ${\\approx}16.8\\%$ of the triplet states at room temperature, along with long coherence times under spin echo and CPMG sequences, $T_2{=}2.7\\mu$s and $T_2^{DD}{=}18.4\\mu$s respectively, limited only by the triplet lifetimes. The material offers several advantages for quantum sensing, including the ability to grow large ($cm$-scale) crystals at low cost, the absence of paramagnetic impurities, and the diamagnetism of electronic states used for sensing when not optically illuminated. Utilizing pentacene as a representative of a broader class of spin triplet-polarizable organic molecules, this study highlights new potential for quantum sensing in chemical systems.","sentences":["Quantum sensors have notably advanced high-sensitivity magnetic field detection.","Here, we report quantum sensors constructed from polarized spin-triplet electrons in photoexcited organic chromophores, specifically focusing on pentacene-doped para-terphenyl (${\\approx}$0.1%).","We demonstrate essential quantum sensing properties at room temperature: electronic optical polarization and state-dependent fluorescence contrast, by leveraging differential pumping and relaxation rates between triplet and ground states.","We measure high optically detected magnetic resonance (ODMR) contrast ${\\approx}16.8\\%$ of the triplet states at room temperature, along with long coherence times under spin echo and CPMG sequences, $T_2{=}2.7\\mu$s and $T_2^{DD}{=}18.4\\mu$s respectively, limited only by the triplet lifetimes.","The material offers several advantages for quantum sensing, including the ability to grow large ($cm$-scale) crystals at low cost, the absence of paramagnetic impurities, and the diamagnetism of electronic states used for sensing when not optically illuminated.","Utilizing pentacene as a representative of a broader class of spin triplet-polarizable organic molecules, this study highlights new potential for quantum sensing in chemical systems."],"url":"http://arxiv.org/abs/2402.13898v1","category":"quant-ph"}
{"created":"2024-02-21 16:05:26","title":"Partial convex hulls of coadjoint orbits and degrees of invariants","abstract":"We study properties of convex hulls of (co)adjoint orbits of compact groups, with applications to invariant theory and tensor product decompositions. The notion of partial convex hulls is introduced and applied to define two numerical invariants of a coadjoint orbit of a semisimple connected compact Lie group. It is shown that the orbits, where any one of these invariants does not exceed a given number $r$, form, upon intersection with a fixed Weyl chamber, a rational convex polyhedral cone in that chamber, related to the Littlewood-Richardson cone of the $r$-fold diagonal embedding of $K$. The numerical invariants are shown to provide lower bounds for degrees of invariant polynomials on irreducible unitary representations.","sentences":["We study properties of convex hulls of (co)adjoint orbits of compact groups, with applications to invariant theory and tensor product decompositions.","The notion of partial convex hulls is introduced and applied to define two numerical invariants of a coadjoint orbit of a semisimple connected compact Lie group.","It is shown that the orbits, where any one of these invariants does not exceed a given number $r$, form, upon intersection with a fixed Weyl chamber, a rational convex polyhedral cone in that chamber, related to the Littlewood-Richardson cone of the $r$-fold diagonal embedding of $K$. The numerical invariants are shown to provide lower bounds for degrees of invariant polynomials on irreducible unitary representations."],"url":"http://arxiv.org/abs/2402.13893v1","category":"math.RT"}
{"created":"2024-02-21 16:00:42","title":"Bispectral duality and separation of variables from surface defect transition","abstract":"We study two types of surface observables $-$ the $\\mathbf{Q}$-observables and the $\\mathbf{H}$-observables $-$ of the 4d $\\mathcal{N}=2$ $A_1$-quiver $U(N)$ gauge theory obtained by coupling a 2d $\\mathcal{N}=(2,2)$ gauged linear sigma model. We demonstrate that the transition between the two surface defects manifests as a Fourier transformation between the surface observables. Utilizing the results from our previous works, which establish that the $\\mathbf{Q}$-observables and the $\\mathbf{H}$-observables give rise, respectively, to the $Q$-operators on the evaluation module over the Yangian $Y(\\mathfrak{gl}(2))$ and the Hecke operators on the twisted $\\widehat{\\mathfrak{sl}}(N)$-coinvariants, we derive an exact duality between the spectral problems of the $\\mathfrak{gl}(2)$ XXX spin chain with $N$ sites and the $\\mathfrak{sl}(N)$ Gaudin model with 4 sites, both of which are defined on bi-infinite modules. Moreover, we present a dual description of the monodromy surface defect as coupling a 2d $\\mathcal{N}=(2,2)$ gauged linear sigma model. Employing this dual perspective, we demonstrate how the monodromy surface defect undergoes a transition to multiple $\\mathbf{Q}$-observables or $\\mathbf{H}$-observables, implemented through integral transformations between their surface observables. These transformations provide, respectively, $\\hbar$-deformation and a higher-rank generalization of the KZ/BPZ correspondence. In the limit $\\varepsilon_2\\to 0$, they give rise to the quantum separation of variables for the $\\mathfrak{gl}(2)$ XXX spin chain and the $\\mathfrak{sl}(N)$ Gaudin model, respectively.","sentences":["We study two types of surface observables $-$ the $\\mathbf{Q}$-observables and the $\\mathbf{H}$-observables $-$ of the 4d $\\mathcal{N}=2$ $A_1$-quiver $U(N)$ gauge theory obtained by coupling a 2d $\\mathcal{N}=(2,2)$ gauged linear sigma model.","We demonstrate that the transition between the two surface defects manifests as a Fourier transformation between the surface observables.","Utilizing the results from our previous works, which establish that the $\\mathbf{Q}$-observables and the $\\mathbf{H}$-observables give rise, respectively, to the $Q$-operators on the evaluation module over the Yangian $Y(\\mathfrak{gl}(2))$ and the Hecke operators on the twisted $\\widehat{\\mathfrak{sl}}(N)$-coinvariants, we derive an exact duality between the spectral problems of the $\\mathfrak{gl}(2)$ XXX spin chain with $N$ sites and the $\\mathfrak{sl}(N)$ Gaudin model with 4 sites, both of which are defined on bi-infinite modules.","Moreover, we present a dual description of the monodromy surface defect as coupling a 2d $\\mathcal{N}=(2,2)$ gauged linear sigma model.","Employing this dual perspective, we demonstrate how the monodromy surface defect undergoes a transition to multiple $\\mathbf{Q}$-observables or $\\mathbf{H}$-observables, implemented through integral transformations between their surface observables.","These transformations provide, respectively, $\\hbar$-deformation and a higher-rank generalization of the KZ/BPZ correspondence.","In the limit $\\varepsilon_2\\to 0$, they give rise to the quantum separation of variables for the $\\mathfrak{gl}(2)$ XXX spin chain and the $\\mathfrak{sl}(N)$ Gaudin model, respectively."],"url":"http://arxiv.org/abs/2402.13889v1","category":"hep-th"}
{"created":"2024-02-21 16:00:36","title":"di-Langlands correspondence and extended observables","abstract":"We explore the $\\textit{difference Langlands correspondence}$ using the four dimensional ${\\mathcal{N}}=2$ super-QCD. Surface defects and surface observables play the crucial role. As an application, we give the first construction of the full set of quantum integrals, i.e. commuting differential operators, such that the partition function of the so-called regular monodromy surface defect is their joint eigenvectors in an evaluation module over the Yangian $Y(\\mathfrak{gl}(2))$, making it the wavefunction of a $N$-site $\\mathfrak{gl}(2)$ spin chain with bi-infinite spin modules. We construct the $\\mathbf{Q}$- and $\\tilde{\\mathbf{Q}}$-surface observables which are believed to be the $Q$-operators on the bi-infinite module over the Yangian $Y(\\mathfrak{gl}(2))$, and compute their eigenvalues, the $Q$-functions, as vevs of the surface observables.","sentences":["We explore the $\\textit{difference Langlands correspondence}$ using the four dimensional ${\\mathcal{N}}=2$ super-QCD.","Surface defects and surface observables play the crucial role.","As an application, we give the first construction of the full set of quantum integrals, i.e. commuting differential operators, such that the partition function of the so-called regular monodromy surface defect is their joint eigenvectors in an evaluation module over the Yangian $Y(\\mathfrak{gl}(2))$, making it the wavefunction of a $N$-site $\\mathfrak{gl}(2)$ spin chain with bi-infinite spin modules.","We construct the $\\mathbf{Q}$- and $\\tilde{\\mathbf{Q}}$-surface observables which are believed to be the $Q$-operators on the bi-infinite module over the Yangian $Y(\\mathfrak{gl}(2))$, and compute their eigenvalues, the $Q$-functions, as vevs of the surface observables."],"url":"http://arxiv.org/abs/2402.13888v1","category":"hep-th"}
{"created":"2024-02-21 15:32:23","title":"Analytical and numerical studies for integrable and non-integrable fractional discrete modified Korteweg-de Vries hierarchies","abstract":"Under investigation in this paper is the fractional integrable and non-integrable discrete modified Korteweg-de Vries hierarchies. The linear dispersion relations, completeness relations, inverse scattering transform, and fractional soliton solutions of the fractional integrable discrete modified Korteweg-de Vries hierarchy will be explored. The inverse scattering problem will be solved accurately by using Gel'fand-Levitan-Marchenko (GLM) equations and Riemann-Hilbert (RH) problem. The peak velocity of fractional soliton solutions will be analyzed. The numerical solutions of the non-integrable fractional averaged discrete modified Korteweg-de Vries equation which has a simpler form than the integrable one will be obtained by a split-step fourier method.","sentences":["Under investigation in this paper is the fractional integrable and non-integrable discrete modified Korteweg-de Vries hierarchies.","The linear dispersion relations, completeness relations, inverse scattering transform, and fractional soliton solutions of the fractional integrable discrete modified Korteweg-de Vries hierarchy will be explored.","The inverse scattering problem will be solved accurately by using Gel'fand-Levitan-Marchenko (GLM) equations and Riemann-Hilbert (RH) problem.","The peak velocity of fractional soliton solutions will be analyzed.","The numerical solutions of the non-integrable fractional averaged discrete modified Korteweg-de Vries equation which has a simpler form than the integrable one will be obtained by a split-step fourier method."],"url":"http://arxiv.org/abs/2402.13872v1","category":"nlin.SI"}
{"created":"2024-02-21 15:10:20","title":"Improving Efficiency of Iso-Surface Extraction on Implicit Neural Representations Using Uncertainty Propagation","abstract":"Implicit Neural representations (INRs) are widely used for scientific data reduction and visualization by modeling the function that maps a spatial location to a data value. Without any prior knowledge about the spatial distribution of values, we are forced to sample densely from INRs to perform visualization tasks like iso-surface extraction which can be very computationally expensive. Recently, range analysis has shown promising results in improving the efficiency of geometric queries, such as ray casting and hierarchical mesh extraction, on INRs for 3D geometries by using arithmetic rules to bound the output range of the network within a spatial region. However, the analysis bounds are often too conservative for complex scientific data. In this paper, we present an improved technique for range analysis by revisiting the arithmetic rules and analyzing the probability distribution of the network output within a spatial region. We model this distribution efficiently as a Gaussian distribution by applying the central limit theorem. Excluding low probability values, we are able to tighten the output bounds, resulting in a more accurate estimation of the value range, and hence more accurate identification of iso-surface cells and more efficient iso-surface extraction on INRs. Our approach demonstrates superior performance in terms of the iso-surface extraction time on four datasets compared to the original range analysis method and can also be generalized to other geometric query tasks.","sentences":["Implicit Neural representations (INRs) are widely used for scientific data reduction and visualization by modeling the function that maps a spatial location to a data value.","Without any prior knowledge about the spatial distribution of values, we are forced to sample densely from INRs to perform visualization tasks like iso-surface extraction which can be very computationally expensive.","Recently, range analysis has shown promising results in improving the efficiency of geometric queries, such as ray casting and hierarchical mesh extraction, on INRs for 3D geometries by using arithmetic rules to bound the output range of the network within a spatial region.","However, the analysis bounds are often too conservative for complex scientific data.","In this paper, we present an improved technique for range analysis by revisiting the arithmetic rules and analyzing the probability distribution of the network output within a spatial region.","We model this distribution efficiently as a Gaussian distribution by applying the central limit theorem.","Excluding low probability values, we are able to tighten the output bounds, resulting in a more accurate estimation of the value range, and hence more accurate identification of iso-surface cells and more efficient iso-surface extraction on INRs.","Our approach demonstrates superior performance in terms of the iso-surface extraction time on four datasets compared to the original range analysis method and can also be generalized to other geometric query tasks."],"url":"http://arxiv.org/abs/2402.13861v1","category":"cs.GR"}
{"created":"2024-02-21 15:10:03","title":"Friedmann's Universe Controlled by Gauss-Bonnet Modified Gravity","abstract":"The accepted idea that the expansion of the universe is accelerating needs, for compatibility to general relativity, the introduction of some unusual forms of matter. However, several authors have proposed that instead of making weird hypothesis on some yet unobservable species of matter, one should follow the original idea of the first Einstein's paper on cosmology and consider that in the cosmic scene one has to modify the equations that controls the gravitational metric. This possibility led us to re-examine the evolution of the topological invariant containing two duals in a dynamical universe, the so called Gauss-Bonnet topological invariant. The particular interest on this invariant is due to the fact that in a homogeneous and isotropic universe this invariant drives the cosmic acceleration. In a decelerating scenario and as a necessary previous condition of an ulterior acceleration this invariant must have an extremum identified to its maximum value. We will examine the conditions for this to occur, and a description of the universe with epochs of accelerated and decelerated expansion.","sentences":["The accepted idea that the expansion of the universe is accelerating needs, for compatibility to general relativity, the introduction of some unusual forms of matter.","However, several authors have proposed that instead of making weird hypothesis on some yet unobservable species of matter, one should follow the original idea of the first Einstein's paper on cosmology and consider that in the cosmic scene one has to modify the equations that controls the gravitational metric.","This possibility led us to re-examine the evolution of the topological invariant containing two duals in a dynamical universe, the so called Gauss-Bonnet topological invariant.","The particular interest on this invariant is due to the fact that in a homogeneous and isotropic universe this invariant drives the cosmic acceleration.","In a decelerating scenario and as a necessary previous condition of an ulterior acceleration this invariant must have an extremum identified to its maximum value.","We will examine the conditions for this to occur, and a description of the universe with epochs of accelerated and decelerated expansion."],"url":"http://arxiv.org/abs/2402.13860v1","category":"gr-qc"}
{"created":"2024-02-21 14:52:01","title":"Conformal and nonminimal couplings in fractional cosmology","abstract":"Fractional differential calculus is a mathematical tool that has found applications in studying social and physical behaviours considered \"anomalous\". It is often used when traditional integer derivatives models fail to represent cases where the power law is observed accurately. Fractional calculus must reflect non-local, frequency- and history-dependent properties of power-law phenomena. This tool has various important applications, such as fractional mass conservation, electrochemical analysis, groundwater flow problems, and fractional spatiotemporal diffusion equations. It can also be used in cosmology to explain late-time cosmic acceleration without the need for dark energy. We review some models using fractional differential equations. We assume the Einstein-Hilbert action based on a fractional derivative action and add a scalar field $\\phi$ to create a non-minimal interaction theory with the coupling $\\xi R \\phi^2 $ between gravity and the scalar field, where $\\xi$ is the interaction constant. By employing various mathematical approaches, we can offer precise schemes to find analytical and numerical approximations of the solutions. Moreover, we comprehensively study the modified cosmological equations and analyze the solution space using the theory of dynamical systems and asymptotic expansion methods. This enables us to provide a qualitative description of cosmologies with a scalar field based on fractional calculus formalism.","sentences":["Fractional differential calculus is a mathematical tool that has found applications in studying social and physical behaviours considered \"anomalous\".","It is often used when traditional integer derivatives models fail to represent cases where the power law is observed accurately.","Fractional calculus must reflect non-local, frequency- and history-dependent properties of power-law phenomena.","This tool has various important applications, such as fractional mass conservation, electrochemical analysis, groundwater flow problems, and fractional spatiotemporal diffusion equations.","It can also be used in cosmology to explain late-time cosmic acceleration without the need for dark energy.","We review some models using fractional differential equations.","We assume the Einstein-Hilbert action based on a fractional derivative action and add a scalar field $\\phi$ to create a non-minimal interaction theory with the coupling $\\xi R \\phi^2 $ between gravity and the scalar field, where $\\xi$ is the interaction constant.","By employing various mathematical approaches, we can offer precise schemes to find analytical and numerical approximations of the solutions.","Moreover, we comprehensively study the modified cosmological equations and analyze the solution space using the theory of dynamical systems and asymptotic expansion methods.","This enables us to provide a qualitative description of cosmologies with a scalar field based on fractional calculus formalism."],"url":"http://arxiv.org/abs/2402.13850v1","category":"gr-qc"}
{"created":"2024-02-21 14:44:16","title":"Coupled coherent states method for tunneling dynamics: an interpretative study","abstract":"Numerical solutions of the time-dependent Schr\\\"odinger equation based on the variational principle may offer physical insight that cannot be gained by a solution using fixed grids in position and momentum space. Here we focus on the tunneling dynamics in a quartic double-well and the use of classical, trajectory-guided coherent states to gain insight into the workings of the coupled coherent states method developed by Shalashilin and Child [J. Chem. Phys. {\\bf 113}, 10028 (2000)]. It is shown that over-the-barrier classical trajectories, alone, can accurately describe the tunneling effect.","sentences":["Numerical solutions of the time-dependent Schr\\\"odinger equation based on the variational principle may offer physical insight that cannot be gained by a solution using fixed grids in position and momentum space.","Here we focus on the tunneling dynamics in a quartic double-well and the use of classical, trajectory-guided coherent states to gain insight into the workings of the coupled coherent states method developed by Shalashilin and Child [J. Chem.","Phys. {\\bf 113}, 10028 (2000)].","It is shown that over-the-barrier classical trajectories, alone, can accurately describe the tunneling effect."],"url":"http://arxiv.org/abs/2402.13847v1","category":"quant-ph"}
{"created":"2024-02-21 14:42:32","title":"State-dependent stiffness enhances wave propagation along elastic filaments","abstract":"We study an elastic filament beating in a viscous fluid with curvature-dependent bending stiffness. Our numerical and experimental investigations reveal that such differential stiffness can sustain planar bending waves far along flexible filaments. In particular, basal actuation is a viable, parsimonious mechanism for generating high-amplitude planar bending waves, in stark contrast to the uniform-stiffness case. Further, the resulting beat patterns closely resemble the power-and-recovery strokes of propulsive biological filaments, suggesting applications in robotic and engineered systems.","sentences":["We study an elastic filament beating in a viscous fluid with curvature-dependent bending stiffness.","Our numerical and experimental investigations reveal that such differential stiffness can sustain planar bending waves far along flexible filaments.","In particular, basal actuation is a viable, parsimonious mechanism for generating high-amplitude planar bending waves, in stark contrast to the uniform-stiffness case.","Further, the resulting beat patterns closely resemble the power-and-recovery strokes of propulsive biological filaments, suggesting applications in robotic and engineered systems."],"url":"http://arxiv.org/abs/2402.13844v1","category":"cond-mat.soft"}
{"created":"2024-02-21 14:42:23","title":"Investigation of QGP-like properties via identified particles production in oxygen-oxygen collisions at \\sqrt{s_{\\mathrm{NN}}} from EPOS4","abstract":"The Large Hadron Collider (LHC) aims to inject oxygen (${}^{16}O$) ions in next run into its experiments. This include anticipated one-day physics run focusing on \\oo collisions at center-of-mass energy \\sevenn. In this study, we present the production of identified particles ($\\pi^\\pm$, $K^\\pm$ and $p(\\overline{p})$) as well as the bulk-properties of the medium produced in \\oo collisions at \\sqrt{s_{\\mathrm{NN}}} from EPOS4 simulations. We report the transverse momentum ({\\ppt}) spectra, integrated yield (\\dNdy), charged particle multiplicity (\\dndeta), mean transverse momentum (\\mpt), \\ppt differential particle ratios ($K/\\pi$ and $p/\\pi$) for different centrality classes. We found that EPOS4 simulations reasonably reproduce the shapes and distributions of all observables. The freeze-out parameters have been extracted by fitting the \\ppt spectra of identified particles simultaneously by the Boltzmann Gibbs Blast Wave model. We found the presence of significant radial flow (\\mbeta) and comparable kinetic freeze-out temperature (\\tkin) with \\pbpb collisions at LHC energies. Additionally, a strong anti-correlation is found between \\tkin and \\mbeta from central to peripheral collisions. We also compare the freeze-out parameters from EPOS4 with AMPT and it is observed that the flow velocity \\mbeta from EPOS4 is relatively higher compared to that of AMPT. Furthermore, the foreseen data from \\oo collisions at the LHC, when available, will help to better understand the heavy-ion-like behaviour in small systems. This is due to final state multiplicity overlap in \\oo collisions with \\pp, \\ppb and \\pbpb collisions, and it may also help to put possible constraints on the model parameters .","sentences":["The Large Hadron Collider (LHC) aims to inject oxygen (${}^{16}O$) ions in next run into its experiments.","This include anticipated one-day physics run focusing on \\oo collisions at center-of-mass energy \\sevenn.","In this study, we present the production of identified particles ($\\pi^\\pm$, $K^\\pm$ and $p(\\overline{p})$) as well as the bulk-properties of the medium produced in \\oo collisions at \\sqrt{s_{\\mathrm{NN}}} from EPOS4 simulations.","We report the transverse momentum ({\\ppt}) spectra, integrated yield (\\dNdy), charged particle multiplicity (\\dndeta), mean transverse momentum (\\mpt), \\ppt differential particle ratios ($K/\\pi$ and $p/\\pi$) for different centrality classes.","We found that EPOS4 simulations reasonably reproduce the shapes and distributions of all observables.","The freeze-out parameters have been extracted by fitting the \\ppt spectra of identified particles simultaneously by the Boltzmann Gibbs Blast Wave model.","We found the presence of significant radial flow (\\mbeta) and comparable kinetic freeze-out temperature (\\tkin) with \\pbpb collisions at LHC energies.","Additionally, a strong anti-correlation is found between \\tkin and \\mbeta from central to peripheral collisions.","We also compare the freeze-out parameters from EPOS4 with AMPT and it is observed that the flow velocity \\mbeta from EPOS4 is relatively higher compared to that of AMPT.","Furthermore, the foreseen data from \\oo collisions at the LHC, when available, will help to better understand the heavy-ion-like behaviour in small systems.","This is due to final state multiplicity overlap in \\oo collisions with \\pp, \\ppb and \\pbpb collisions, and it may also help to put possible constraints on the model parameters ."],"url":"http://arxiv.org/abs/2402.13843v1","category":"hep-ph"}
{"created":"2024-02-21 14:35:53","title":"Emergent spin and clock variable in Bianchi type-I quantum cosmology","abstract":"We consider the Bianchi type-I model of the universe in the Wheeler-DeWitt quantization scheme with the matter degree of freedom represented by a scalar field. As a consequence, the quantum mechanical equation of the universe is obtained in the minisuperspace consisting of the Misner variables and the scalar field. Employing Dirac factorization, we find that the volume parameter makes a suitable choice for the clock variable whereas the matter clock leads to various inconsistencies. We further find that the minisuperspace orbital angular momentum operator does not commute with the Hamiltonian in the Dirac-type equation. We therefore find the missing part of the angular momentum whereby the total angular momentum commutes with the Hamiltonian. We interpret this missing part as the spin of the quantum universe during its early stage of evolution. The emergence of the three-component spin vector is owing to the presence of anisotropy in the Bianchi type-I model of the universe which is absent in the quantization of isotropic models.","sentences":["We consider the Bianchi type-I model of the universe in the Wheeler-DeWitt quantization scheme with the matter degree of freedom represented by a scalar field.","As a consequence, the quantum mechanical equation of the universe is obtained in the minisuperspace consisting of the Misner variables and the scalar field.","Employing Dirac factorization, we find that the volume parameter makes a suitable choice for the clock variable whereas the matter clock leads to various inconsistencies.","We further find that the minisuperspace orbital angular momentum operator does not commute with the Hamiltonian in the Dirac-type equation.","We therefore find the missing part of the angular momentum whereby the total angular momentum commutes with the Hamiltonian.","We interpret this missing part as the spin of the quantum universe during its early stage of evolution.","The emergence of the three-component spin vector is owing to the presence of anisotropy in the Bianchi type-I model of the universe which is absent in the quantization of isotropic models."],"url":"http://arxiv.org/abs/2402.13839v1","category":"gr-qc"}
{"created":"2024-02-21 14:25:30","title":"Loop equations for generalised eigenvalue models","abstract":"We derive the loop equation for the 1-matrix model with generic difference-type measure for eigenvalues and develop a recursive algebraic framework for solving it to an arbitrary order in the coupling constant in and beyond the planar approximation. The planar limit is solved exactly for a one-parametric family of models and in the general case at strong coupling. The Wilson loop in the N=2* super-Yang-Mills theory and the Hoppe model are used to illustrate our methods.","sentences":["We derive the loop equation for the 1-matrix model with generic difference-type measure for eigenvalues and develop a recursive algebraic framework for solving it to an arbitrary order in the coupling constant in and beyond the planar approximation.","The planar limit is solved exactly for a one-parametric family of models and in the general case at strong coupling.","The Wilson loop in the N=2* super-Yang-Mills theory and the Hoppe model are used to illustrate our methods."],"url":"http://arxiv.org/abs/2402.13835v1","category":"hep-th"}
{"created":"2024-02-21 14:23:41","title":"High-resolution spectroscopy of the intermediate polar EX Hydrae: II. The inner disk radius","abstract":"EX Hya is one of the best studied, but still enigmatic intermediate polars. We present phase-resolved blue VLT/UVES high-resolution ($\\lambda/\\Delta \\lambda\\simeq16.000$) spectra of EX Hya taken in January 2004. Our analysis involves a unique decomposition of the Balmer line profiles into the spin-modulated line wings that represent streaming motions in the magnetosphere and the orbital-phase modulated line core that represents the accretion disk. Spectral analysis and tomography show that the division line between the two is solidly located at $\\mid\\upsilon_{rad}\\mid\\simeq1200$ km s$^{-1}$, defining the inner edge of the accretion disk at $r_{in}\\simeq{7}\\times 10^{9}$ cm or $\\sim10 R_1$ (WD radii). This large central hole allows an unimpeded view of the tall accretion curtain at the lower pole with a shock height up to $h_{sh}\\sim1 R_1$ that is required by X-ray and optical observations. Our results contradict models that advocate a small magnetosphere and a small inner disk hole. Equating $r_{in}$ with the magnetospheric radius in the orbital plane allows us to derive a magnetic moment of the WD of $\\mu_1\\simeq1.3\\times 10^{32}$ G cm$^{3}$ and a surface field strength $B_1\\sim0.35$ MG. Given a polar field strength $B_{p} \\lesssim 1.0$ MG, optical circular polarization is not expected. With an accretion rate $\\dot M = 3.9\\times10^{-11}$ $M_{\\odot}$yr$^{-1}$, the accretion torque is $G_{acc}\\simeq 2.2 \\times 10^{33}$ g cm$^{2}$s$^{-2}$. The magnetostatic torque is of similar magnitude, suggesting that EX Hya is not far from being synchronized. We measured the orbital radial-velocity amplitude of the WD, $K_1=58.7\\pm3.9$ km s$^{-1}$, and found a spin-dependent velocity modulation as well. The former is in perfect agreement with the mean velocity amplitude obtained by other researchers, confirming the published component masses $M_1\\simeq0.79 M_\\odot$ and $M_2\\simeq0.11 M_\\odot$.","sentences":["EX Hya is one of the best studied, but still enigmatic intermediate polars.","We present phase-resolved blue VLT/UVES high-resolution ($\\lambda/\\Delta \\lambda\\simeq16.000$) spectra of EX Hya taken in January 2004.","Our analysis involves a unique decomposition of the Balmer line profiles into the spin-modulated line wings that represent streaming motions in the magnetosphere and the orbital-phase modulated line core that represents the accretion disk.","Spectral analysis and tomography show that the division line between the two is solidly located at $\\mid\\upsilon_{rad}\\mid\\simeq1200$ km s$^{-1}$, defining the inner edge of the accretion disk at $r_{in}\\simeq{7}\\times 10^{9}$ cm or $\\sim10 R_1$ (WD radii).","This large central hole allows an unimpeded view of the tall accretion curtain at the lower pole with a shock height up to $h_{sh}\\sim1 R_1$ that is required by X-ray and optical observations.","Our results contradict models that advocate a small magnetosphere and a small inner disk hole.","Equating $r_{in}$ with the magnetospheric radius in the orbital plane allows us to derive a magnetic moment of the WD of $\\mu_1\\simeq1.3\\times 10^{32}$ G cm$^{3}$ and a surface field strength $B_1\\sim0.35$ MG.","Given a polar field strength $B_{p} \\lesssim 1.0$ MG, optical circular polarization is not expected.","With an accretion rate $\\dot M = 3.9\\times10^{-11}$ $M_{\\odot}$yr$^{-1}$, the accretion torque is $G_{acc}\\simeq 2.2 \\times 10^{33}$ g cm$^{2}$s$^{-2}$.","The magnetostatic torque is of similar magnitude, suggesting that EX Hya is not far from being synchronized.","We measured the orbital radial-velocity amplitude of the WD, $K_1=58.7\\pm3.9$ km s$^{-1}$, and found a spin-dependent velocity modulation as well.","The former is in perfect agreement with the mean velocity amplitude obtained by other researchers, confirming the published component masses $M_1\\simeq0.79 M_\\odot$ and $M_2\\simeq0.11 M_\\odot$."],"url":"http://arxiv.org/abs/2402.13834v1","category":"astro-ph.SR"}
{"created":"2024-02-21 14:17:45","title":"Origami: (un)folding the abstraction of recursion schemes for program synthesis","abstract":"Program synthesis with Genetic Programming searches for a correct program that satisfies the input specification, which is usually provided as input-output examples. One particular challenge is how to effectively handle loops and recursion avoiding programs that never terminate. A helpful abstraction that can alleviate this problem is the employment of Recursion Schemes that generalize the combination of data production and consumption. Recursion Schemes are very powerful as they allow the construction of programs that can summarize data, create sequences, and perform advanced calculations. The main advantage of writing a program using Recursion Schemes is that the programs are composed of well defined templates with only a few parts that need to be synthesized. In this paper we make an initial study of the benefits of using program synthesis with fold and unfold templates, and outline some preliminary experimental results. To highlight the advantages and disadvantages of this approach, we manually solved the entire GPSB benchmark using recursion schemes, highlighting the parts that should be evolved compared to alternative implementations. We noticed that, once the choice of which recursion scheme is made, the synthesis process can be simplified as each of the missing parts of the template are reduced to simpler functions, which are further constrained by their own input and output types.","sentences":["Program synthesis with Genetic Programming searches for a correct program that satisfies the input specification, which is usually provided as input-output examples.","One particular challenge is how to effectively handle loops and recursion avoiding programs that never terminate.","A helpful abstraction that can alleviate this problem is the employment of Recursion Schemes that generalize the combination of data production and consumption.","Recursion Schemes are very powerful as they allow the construction of programs that can summarize data, create sequences, and perform advanced calculations.","The main advantage of writing a program using Recursion Schemes is that the programs are composed of well defined templates with only a few parts that need to be synthesized.","In this paper we make an initial study of the benefits of using program synthesis with fold and unfold templates, and outline some preliminary experimental results.","To highlight the advantages and disadvantages of this approach, we manually solved the entire GPSB benchmark using recursion schemes, highlighting the parts that should be evolved compared to alternative implementations.","We noticed that, once the choice of which recursion scheme is made, the synthesis process can be simplified as each of the missing parts of the template are reduced to simpler functions, which are further constrained by their own input and output types."],"url":"http://arxiv.org/abs/2402.13828v1","category":"cs.NE"}
{"created":"2024-02-21 14:16:49","title":"Identifying Unnecessary 3D Gaussians using Clustering for Fast Rendering of 3D Gaussian Splatting","abstract":"3D Gaussian splatting (3D-GS) is a new rendering approach that outperforms the neural radiance field (NeRF) in terms of both speed and image quality. 3D-GS represents 3D scenes by utilizing millions of 3D Gaussians and projects these Gaussians onto the 2D image plane for rendering. However, during the rendering process, a substantial number of unnecessary 3D Gaussians exist for the current view direction, resulting in significant computation costs associated with their identification. In this paper, we propose a computational reduction technique that quickly identifies unnecessary 3D Gaussians in real-time for rendering the current view without compromising image quality. This is accomplished through the offline clustering of 3D Gaussians that are close in distance, followed by the projection of these clusters onto a 2D image plane during runtime. Additionally, we analyze the bottleneck associated with the proposed technique when executed on GPUs and propose an efficient hardware architecture that seamlessly supports the proposed scheme. For the Mip-NeRF360 dataset, the proposed technique excludes 63% of 3D Gaussians on average before the 2D image projection, which reduces the overall rendering computation by almost 38.3% without sacrificing peak-signal-to-noise-ratio (PSNR). The proposed accelerator also achieves a speedup of 10.7x compared to a GPU.","sentences":["3D Gaussian splatting (3D-GS) is a new rendering approach that outperforms the neural radiance field (NeRF) in terms of both speed and image quality.","3D-GS represents 3D scenes by utilizing millions of 3D Gaussians and projects these Gaussians onto the 2D image plane for rendering.","However, during the rendering process, a substantial number of unnecessary 3D Gaussians exist for the current view direction, resulting in significant computation costs associated with their identification.","In this paper, we propose a computational reduction technique that quickly identifies unnecessary 3D Gaussians in real-time for rendering the current view without compromising image quality.","This is accomplished through the offline clustering of 3D Gaussians that are close in distance, followed by the projection of these clusters onto a 2D image plane during runtime.","Additionally, we analyze the bottleneck associated with the proposed technique when executed on GPUs and propose an efficient hardware architecture that seamlessly supports the proposed scheme.","For the Mip-NeRF360 dataset, the proposed technique excludes 63% of 3D Gaussians on average before the 2D image projection, which reduces the overall rendering computation by almost 38.3% without sacrificing peak-signal-to-noise-ratio (PSNR).","The proposed accelerator also achieves a speedup of 10.7x compared to a GPU."],"url":"http://arxiv.org/abs/2402.13827v1","category":"cs.CV"}
{"created":"2024-02-21 14:13:09","title":"Linearity of the co-moving velocity","abstract":"The co-moving velocity is a new variable in the description of immiscible two-phase flow in porous media. It is the saturation-weighted derivatives of the seepage velocities of the two immiscible fluids with respect to saturation. I show that it is linear in the derivative of the average seepage velocity with respect to the saturation. This is in accordance with recent measurements. Two parameters are needed to describe it. In terms of relative permeability theory, this linear relation leads to a differential equation relating the two relative permeabilities describing the flow. I present this equation together with a solution.","sentences":["The co-moving velocity is a new variable in the description of immiscible two-phase flow in porous media.","It is the saturation-weighted derivatives of the seepage velocities of the two immiscible fluids with respect to saturation.","I show that it is linear in the derivative of the average seepage velocity with respect to the saturation.","This is in accordance with recent measurements.","Two parameters are needed to describe it.","In terms of relative permeability theory, this linear relation leads to a differential equation relating the two relative permeabilities describing the flow.","I present this equation together with a solution."],"url":"http://arxiv.org/abs/2402.13826v1","category":"physics.flu-dyn"}
{"created":"2024-02-21 13:58:27","title":"Dupin cyclides passing through a fixed circle","abstract":"We derive algebraic equations on the coefficients of the implicit equation to characterize all Dupin cyclides passing through a fixed circle. The results are applied to solve the basic problems in CAGD about blending of Dupin cyclides along circles.","sentences":["We derive algebraic equations on the coefficients of the implicit equation to characterize all Dupin cyclides passing through a fixed circle.","The results are applied to solve the basic problems in CAGD about blending of Dupin cyclides along circles."],"url":"http://arxiv.org/abs/2402.13819v1","category":"math.AG"}
{"created":"2024-02-21 13:47:51","title":"The Expected Loss of Preconditioned Langevin Dynamics Reveals the Hessian Rank","abstract":"Langevin dynamics (LD) is widely used for sampling from distributions and for optimization. In this work, we derive a closed-form expression for the expected loss of preconditioned LD near stationary points of the objective function. We use the fact that at the vicinity of such points, LD reduces to an Ornstein-Uhlenbeck process, which is amenable to convenient mathematical treatment. Our analysis reveals that when the preconditioning matrix satisfies a particular relation with respect to the noise covariance, LD's expected loss becomes proportional to the rank of the objective's Hessian. We illustrate the applicability of this result in the context of neural networks, where the Hessian rank has been shown to capture the complexity of the predictor function but is usually computationally hard to probe. Finally, we use our analysis to compare SGD-like and Adam-like preconditioners and identify the regimes under which each of them leads to a lower expected loss.","sentences":["Langevin dynamics (LD) is widely used for sampling from distributions and for optimization.","In this work, we derive a closed-form expression for the expected loss of preconditioned LD near stationary points of the objective function.","We use the fact that at the vicinity of such points, LD reduces to an Ornstein-Uhlenbeck process, which is amenable to convenient mathematical treatment.","Our analysis reveals that when the preconditioning matrix satisfies a particular relation with respect to the noise covariance, LD's expected loss becomes proportional to the rank of the objective's Hessian.","We illustrate the applicability of this result in the context of neural networks, where the Hessian rank has been shown to capture the complexity of the predictor function but is usually computationally hard to probe.","Finally, we use our analysis to compare SGD-like and Adam-like preconditioners and identify the regimes under which each of them leads to a lower expected loss."],"url":"http://arxiv.org/abs/2402.13810v1","category":"cs.LG"}
{"created":"2024-02-21 13:27:05","title":"Radiating particle in the vicinity of the weakly charged Schwarzschild black hole","abstract":"It is well known that supermassive black holes in the centers of galaxies are capable of accelerating charged particles to very high energies. In many cases, the particle acceleration by black holes occurs electromagnetically through an electric field induced by the source. In such scenarios, the accelerated particles radiate electromagnetic waves, leading to the appearance of the backreaction force, which can considerably change the dynamics, especially, if the particles are relativistic. The effect of the radiation reaction force due to accelerating electric field of the central body in curved spacetime has not been considered previously. We study the dynamics of radiating charged particles in the field of the Schwarzschild black hole in the presence of an electric field associated with a small central charge of negligible gravitational influence. We start from the flat spacetime description, solving the Lorentz-Dirac equation reduced to the Landau-Lifshitz form. In curved spacetime, we use the DeWitt-Brehme equation and discuss the effect of the self-force, also known as the tail term, within the given approach. We also study the pure effect of the self-force to calculate the radiative deceleration of radially moving charged particles. In the case of bounded orbits, we find that the radiation reaction force can stabilize and circularize the orbits of oscillating charged particles by suppressing the oscillations or causing the particles to spiral down into the black hole depending on the sign of the electrostatic interaction. In all cases, we calculate the energy losses and exact trajectories of charged particles for different values and signs of electric charge.","sentences":["It is well known that supermassive black holes in the centers of galaxies are capable of accelerating charged particles to very high energies.","In many cases, the particle acceleration by black holes occurs electromagnetically through an electric field induced by the source.","In such scenarios, the accelerated particles radiate electromagnetic waves, leading to the appearance of the backreaction force, which can considerably change the dynamics, especially, if the particles are relativistic.","The effect of the radiation reaction force due to accelerating electric field of the central body in curved spacetime has not been considered previously.","We study the dynamics of radiating charged particles in the field of the Schwarzschild black hole in the presence of an electric field associated with a small central charge of negligible gravitational influence.","We start from the flat spacetime description, solving the Lorentz-Dirac equation reduced to the Landau-Lifshitz form.","In curved spacetime, we use the DeWitt-Brehme equation and discuss the effect of the self-force, also known as the tail term, within the given approach.","We also study the pure effect of the self-force to calculate the radiative deceleration of radially moving charged particles.","In the case of bounded orbits, we find that the radiation reaction force can stabilize and circularize the orbits of oscillating charged particles by suppressing the oscillations or causing the particles to spiral down into the black hole depending on the sign of the electrostatic interaction.","In all cases, we calculate the energy losses and exact trajectories of charged particles for different values and signs of electric charge."],"url":"http://arxiv.org/abs/2402.13797v1","category":"gr-qc"}
{"created":"2024-02-21 13:19:07","title":"Nonlocal-to-Local Convergence for a Cahn-Hilliard Tumor Growth Model","abstract":"We consider a local Cahn-Hilliard-type model for tumor growth as well as a nonlocal model where, compared to the local system, the Laplacian in the equation for the chemical potential is replaced by a nonlocal operator. The latter is defined as a convolution integral with suitable kernels parametrized by a small parameter. For sufficiently smooth bounded domains in three dimensions, we prove convergence of weak solutions of the nonlocal model towards strong solutions of the local model together with convergence rates with respect to the small parameter. The proof is done via a Gronwall-type argument and a convergence result with rates for the nonlocal integral operator towards the Laplacian due to Abels, Hurm arXiv:2307.02264.","sentences":["We consider a local Cahn-Hilliard-type model for tumor growth as well as a nonlocal model where, compared to the local system, the Laplacian in the equation for the chemical potential is replaced by a nonlocal operator.","The latter is defined as a convolution integral with suitable kernels parametrized by a small parameter.","For sufficiently smooth bounded domains in three dimensions, we prove convergence of weak solutions of the nonlocal model towards strong solutions of the local model together with convergence rates with respect to the small parameter.","The proof is done via a Gronwall-type argument and a convergence result with rates for the nonlocal integral operator towards the Laplacian due to Abels, Hurm arXiv:2307.02264."],"url":"http://arxiv.org/abs/2402.13790v1","category":"math.AP"}
{"created":"2024-02-21 12:44:21","title":"General Debiasing for Graph-based Collaborative Filtering via Adversarial Graph Dropout","abstract":"Graph neural networks (GNNs) have shown impressive performance in recommender systems, particularly in collaborative filtering (CF). The key lies in aggregating neighborhood information on a user-item interaction graph to enhance user/item representations. However, we have discovered that this aggregation mechanism comes with a drawback, which amplifies biases present in the interaction graph. For instance, a user's interactions with items can be driven by both unbiased true interest and various biased factors like item popularity or exposure. However, the current aggregation approach combines all information, both biased and unbiased, leading to biased representation learning. Consequently, graph-based recommenders can learn distorted views of users/items, hindering the modeling of their true preferences and generalizations. To address this issue, we introduce a novel framework called Adversarial Graph Dropout (AdvDrop). It differentiates between unbiased and biased interactions, enabling unbiased representation learning. For each user/item, AdvDrop employs adversarial learning to split the neighborhood into two views: one with bias-mitigated interactions and the other with bias-aware interactions. After view-specific aggregation, AdvDrop ensures that the bias-mitigated and bias-aware representations remain invariant, shielding them from the influence of bias. We validate AdvDrop's effectiveness on five public datasets that cover both general and specific biases, demonstrating significant improvements. Furthermore, our method exhibits meaningful separation of subgraphs and achieves unbiased representations for graph-based CF models, as revealed by in-depth analysis. Our code is publicly available at https://github.com/Arthurma71/AdvDrop.","sentences":["Graph neural networks (GNNs) have shown impressive performance in recommender systems, particularly in collaborative filtering (CF).","The key lies in aggregating neighborhood information on a user-item interaction graph to enhance user/item representations.","However, we have discovered that this aggregation mechanism comes with a drawback, which amplifies biases present in the interaction graph.","For instance, a user's interactions with items can be driven by both unbiased true interest and various biased factors like item popularity or exposure.","However, the current aggregation approach combines all information, both biased and unbiased, leading to biased representation learning.","Consequently, graph-based recommenders can learn distorted views of users/items, hindering the modeling of their true preferences and generalizations.","To address this issue, we introduce a novel framework called Adversarial Graph Dropout (AdvDrop).","It differentiates between unbiased and biased interactions, enabling unbiased representation learning.","For each user/item, AdvDrop employs adversarial learning to split the neighborhood into two views: one with bias-mitigated interactions and the other with bias-aware interactions.","After view-specific aggregation, AdvDrop ensures that the bias-mitigated and bias-aware representations remain invariant, shielding them from the influence of bias.","We validate AdvDrop's effectiveness on five public datasets that cover both general and specific biases, demonstrating significant improvements.","Furthermore, our method exhibits meaningful separation of subgraphs and achieves unbiased representations for graph-based CF models, as revealed by in-depth analysis.","Our code is publicly available at https://github.com/Arthurma71/AdvDrop."],"url":"http://arxiv.org/abs/2402.13769v1","category":"cs.IR"}
{"created":"2024-02-21 12:37:41","title":"Leaving No Matter Unturned -- Analysing existing LHC measurements and events with jets and missing transverse energy measured by the ATLAS Experiment insearch of Dark Matter","abstract":"Various astrophysical observations point towards an as-of-yet unexplained, mainly gravitationally interacting type of matter. If this matter, called Dark Matter, is an elementary particle, it could be produced in particle collisions at the Large Hadron Collider. Given its weak interaction with ordinary matter, however, it would not be directly observable with the general-purpose detectors at the Large Hadron Collider. Its production would therefore manifest as events in which detector-visible objects recoil against the detector-invisible Dark Matter, giving rise to missing transverse energy. This thesis focuses on final states in which these visible objects are jets.   A measurement of the final state of large missing transverse energy and at least one jet in 139 fb$^{-1}$ of proton-proton collisions at 13 TeV recorded with the ATLAS detector at the Large Hadron Collider is performed in this thesis. Good agreement between measured data and Standard-Model prediction is found in a statistical fit, corresponding to a reduced chi-square of 1.37. The measurement is corrected for detector effects to facilitate later reinterpretation. Measurements prepared in such a way can, for example, be exploited by the CONTUR toolkit to set constraints on new theories. Both, the results of the measurement and the CONTUR toolkit making use of existing measurements at the Large Hadron Collider, are employed to set exclusion limits on a model able to explain Dark Matter, the two-Higgs-doublet model with a pseudoscalar mediator to Dark Matter. At $\\tan\\beta=1$, masses of the pseudoscalar $A$ up to 425 GeV and larger than 1600 GeV are excluded at 95 % confidence level. At $m_H\\equiv m_A\\equiv m_{H^\\pm}=$ 600 GeV, masses of the pseudoscalar $a$ up to 550 GeV and values of $\\tan\\beta$ up to 1.5 as well as larger than 20 are excluded at 95 % confidence level.","sentences":["Various astrophysical observations point towards an as-of-yet unexplained, mainly gravitationally interacting type of matter.","If this matter, called Dark Matter, is an elementary particle, it could be produced in particle collisions at the Large Hadron Collider.","Given its weak interaction with ordinary matter, however, it would not be directly observable with the general-purpose detectors at the Large Hadron Collider.","Its production would therefore manifest as events in which detector-visible objects recoil against the detector-invisible Dark Matter, giving rise to missing transverse energy.","This thesis focuses on final states in which these visible objects are jets.   ","A measurement of the final state of large missing transverse energy and at least one jet in 139 fb$^{-1}$ of proton-proton collisions at 13 TeV recorded with the ATLAS detector at the Large Hadron Collider is performed in this thesis.","Good agreement between measured data and Standard-Model prediction is found in a statistical fit, corresponding to a reduced chi-square of 1.37.","The measurement is corrected for detector effects to facilitate later reinterpretation.","Measurements prepared in such a way can, for example, be exploited by the CONTUR toolkit to set constraints on new theories.","Both, the results of the measurement and the CONTUR toolkit making use of existing measurements at the Large Hadron Collider, are employed to set exclusion limits on a model able to explain Dark Matter, the two-Higgs-doublet model with a pseudoscalar mediator to Dark Matter.","At $\\tan\\beta=1$, masses of the pseudoscalar $A$ up to 425 GeV and larger than 1600 GeV are excluded at 95 % confidence level.","At $m_H\\equiv m_A\\equiv m_{H^\\pm}=$ 600 GeV, masses of the pseudoscalar $a$ up to 550 GeV and values of $\\tan\\beta$ up to 1.5 as well as larger than 20 are excluded at 95 % confidence level."],"url":"http://arxiv.org/abs/2402.13760v1","category":"hep-ex"}
{"created":"2024-02-21 12:37:41","title":"Optimizing the Cavity-Arm Ratio of V-Shaped Semiconductor Disk Lasers","abstract":"Passively mode-locked semiconductor disk lasers have received tremendous attention from both science and industry. Their relatively inexpensive production combined with excellent pulse performance and great emission wavelength flexibility make them suitable laser candidates for applications ranging from frequency comb tomography to spectroscopy. However, due to the interaction of the active medium dynamics and the device geometry, emission instabilities occur at high pump powers and thereby limit their performance potential. Hence, understanding those instabilities becomes critical for an optimal laser design. Using a delay-differential equation model, we are able to detect, understand, and classify three distinct instabilities that limit the maximum achievable pump power for the fundamental mode-locking state and link them to characteristic positive net-gain windows. We furthermore derive a simple analytic approximation in order to quantitatively describe the stability boundary. Our results enable us to predict the optimal laser cavity configuration with respect to positive net-gain instabilities and are therefore of great relevance for the future development of passively mode-locking semiconductor disk lasers.","sentences":["Passively mode-locked semiconductor disk lasers have received tremendous attention from both science and industry.","Their relatively inexpensive production combined with excellent pulse performance and great emission wavelength flexibility make them suitable laser candidates for applications ranging from frequency comb tomography to spectroscopy.","However, due to the interaction of the active medium dynamics and the device geometry, emission instabilities occur at high pump powers and thereby limit their performance potential.","Hence, understanding those instabilities becomes critical for an optimal laser design.","Using a delay-differential equation model, we are able to detect, understand, and classify three distinct instabilities that limit the maximum achievable pump power for the fundamental mode-locking state and link them to characteristic positive net-gain windows.","We furthermore derive a simple analytic approximation in order to quantitatively describe the stability boundary.","Our results enable us to predict the optimal laser cavity configuration with respect to positive net-gain instabilities and are therefore of great relevance for the future development of passively mode-locking semiconductor disk lasers."],"url":"http://arxiv.org/abs/2402.13761v1","category":"physics.optics"}
{"created":"2024-02-21 12:34:31","title":"High-throughput Visual Nano-drone to Nano-drone Relative Localization using Onboard Fully Convolutional Networks","abstract":"Relative drone-to-drone localization is a fundamental building block for any swarm operations. We address this task in the context of miniaturized nano-drones, i.e., 10cm in diameter, which show an ever-growing interest due to novel use cases enabled by their reduced form factor. The price for their versatility comes with limited onboard resources, i.e., sensors, processing units, and memory, which limits the complexity of the onboard algorithms. A traditional solution to overcome these limitations is represented by lightweight deep learning models directly deployed aboard nano-drones. This work tackles the challenging relative pose estimation between nano-drones using only a gray-scale low-resolution camera and an ultra-low-power System-on-Chip (SoC) hosted onboard. We present a vertically integrated system based on a novel vision-based fully convolutional neural network (FCNN), which runs at 39Hz within 101mW onboard a Crazyflie nano-drone extended with the GWT GAP8 SoC. We compare our FCNN against three State-of-the-Art (SoA) systems. Considering the best-performing SoA approach, our model results in an R-squared improvement from 32 to 47% on the horizontal image coordinate and from 18 to 55% on the vertical image coordinate, on a real-world dataset of 30k images. Finally, our in-field tests show a reduction of the average tracking error of 37% compared to a previous SoA work and an endurance performance up to the entire battery lifetime of 4 minutes.","sentences":["Relative drone-to-drone localization is a fundamental building block for any swarm operations.","We address this task in the context of miniaturized nano-drones, i.e., 10cm in diameter, which show an ever-growing interest due to novel use cases enabled by their reduced form factor.","The price for their versatility comes with limited onboard resources, i.e., sensors, processing units, and memory, which limits the complexity of the onboard algorithms.","A traditional solution to overcome these limitations is represented by lightweight deep learning models directly deployed aboard nano-drones.","This work tackles the challenging relative pose estimation between nano-drones using only a gray-scale low-resolution camera and an ultra-low-power System-on-Chip (SoC) hosted onboard.","We present a vertically integrated system based on a novel vision-based fully convolutional neural network (FCNN), which runs at 39Hz within 101mW onboard a Crazyflie nano-drone extended with the GWT GAP8 SoC. We compare our FCNN against three State-of-the-Art (SoA) systems.","Considering the best-performing SoA approach, our model results in an R-squared improvement from 32 to 47% on the horizontal image coordinate and from 18 to 55% on the vertical image coordinate, on a real-world dataset of 30k images.","Finally, our in-field tests show a reduction of the average tracking error of 37% compared to a previous SoA work and an endurance performance up to the entire battery lifetime of 4 minutes."],"url":"http://arxiv.org/abs/2402.13756v1","category":"cs.CV"}
{"created":"2024-02-21 12:14:39","title":"Non-unique Ergodicity for the 2D Stochastic Navier-Stokes Equations with Derivative of Space-Time White Noise","abstract":"We prove existence of infinitely many stationary solutions as well as ergodic stationary solutions for the stochastic Navier-Stokes equations on $\\mathbb{T}^2$ \\begin{align*} \\dif u+\\div(u\\otimes u)\\dif t+\\nabla p\\dif t&=\\Delta u\\dif t + (-\\Delta)^{\\fa/2}\\dif B_t,\\ \\ \\ \\ \\div u=0,\\notag \\end{align*} driven by derivative of space-time white noise, where $\\fa\\in[0,\\frac13)$. In this setting, the solutions are not function valued and probabilistic renormalization is required to give a meaning to the equations. Finally, we show that the stationary distributions are not Gaussian distribution $N(0,\\frac12(-\\Delta)^{\\fa-1})$. The proof relies on a time-dependent decomposition and a stochastic version of the convex integration method which provides uniform moment bounds in some function spaces.","sentences":["We prove existence of infinitely many stationary solutions as well as ergodic stationary solutions for the stochastic Navier-Stokes equations on $\\mathbb{T}^2$ \\begin{align*} \\dif u+\\div(u\\otimes u)\\dif t+\\nabla p\\dif t&=\\Delta","u\\dif t + (-\\Delta)^{\\fa/2}\\dif B_t,\\ \\ \\ \\ \\div u=0,\\notag \\end{align*} driven by derivative of space-time white noise, where $\\fa\\in[0,\\frac13)$. In this setting, the solutions are not function valued and probabilistic renormalization is required to give a meaning to the equations.","Finally, we show that the stationary distributions are not Gaussian distribution $N(0,\\frac12(-\\Delta)^{\\fa-1})$.","The proof relies on a time-dependent decomposition and a stochastic version of the convex integration method which provides uniform moment bounds in some function spaces."],"url":"http://arxiv.org/abs/2402.13743v1","category":"math.PR"}
{"created":"2024-02-21 11:51:47","title":"On optimal error rates for strong approximation of SDEs with a drift coefficient of fractional Sobolev regularity","abstract":"We study strong approximation of scalar additive noise driven stochastic differential equations (SDEs) at time point $1$ in the case that the drift coefficient is bounded and has Sobolev regularity $s\\in(0,1)$. Recently, it has been shown in [arXiv:2101.12185v2 (2022)] that for such SDEs the equidistant Euler approximation achieves an $L^2$-error rate of at least $(1+s)/2$, up to an arbitrary small $\\varepsilon$, in terms of the number of evaluations of the driving Brownian motion $W$. In the present article we prove a matching lower error bound for $s\\in(1/2,1)$. More precisely we show that, for every $s\\in(1/2,1)$, the $L^2$-error rate $(1+s)/2$ can, up to a logarithmic term, not be improved in general by no numerical method based on finitely many evaluations of $W$ at fixed time points. Up to now, this result was known in the literature only for the cases $s=1/2-$ and $s=1-$.   For the proof we employ the coupling of noise technique recently introduced in [arXiv:2010.00915 (2020)] to bound the $L^2$-error of an arbitrary approximation from below by the $L^2$-distance of two occupation time functionals provided by a specifically chosen drift coefficient with Sobolev regularity $s$ and two solutions of the corresponding SDE with coupled driving Brownian motions. For the analysis of the latter distance we employ a transformation of the original SDE to overcome the problem of correlated increments of the difference of the two coupled solutions, occupation time estimates to cope with the lack of regularity of the chosen drift coefficient around the point $0$ and scaling properties of the drift coefficient.","sentences":["We study strong approximation of scalar additive noise driven stochastic differential equations (SDEs) at time point $1$ in the case that the drift coefficient is bounded and has Sobolev regularity $s\\in(0,1)$. Recently, it has been shown in [arXiv:2101.12185v2 (2022)] that for such SDEs the equidistant Euler approximation achieves an $L^2$-error rate of at least $(1+s)/2$, up to an arbitrary small $\\varepsilon$, in terms of the number of evaluations of the driving Brownian motion $W$. In the present article we prove a matching lower error bound for $s\\in(1/2,1)$. More precisely we show that, for every $s\\in(1/2,1)$, the $L^2$-error rate $(1+s)/2$ can, up to a logarithmic term, not be improved in general by no numerical method based on finitely many evaluations of $W$ at fixed time points.","Up to now, this result was known in the literature only for the cases $s=1/2-$ and $s=1-$.   For the proof we employ the coupling of noise technique recently introduced in [arXiv:2010.00915 (2020)] to bound the $L^2$-error of an arbitrary approximation from below by the $L^2$-distance of two occupation time functionals provided by a specifically chosen drift coefficient with Sobolev regularity $s$ and two solutions of the corresponding SDE with coupled driving Brownian motions.","For the analysis of the latter distance we employ a transformation of the original SDE to overcome the problem of correlated increments of the difference of the two coupled solutions, occupation time estimates to cope with the lack of regularity of the chosen drift coefficient around the point $0$ and scaling properties of the drift coefficient."],"url":"http://arxiv.org/abs/2402.13732v1","category":"math.PR"}
{"created":"2024-02-21 11:40:27","title":"Average gradient outer product as a mechanism for deep neural collapse","abstract":"Deep Neural Collapse (DNC) refers to the surprisingly rigid structure of the data representations in the final layers of Deep Neural Networks (DNNs). Though the phenomenon has been measured in a wide variety of settings, its emergence is only partially understood. In this work, we provide substantial evidence that DNC formation occurs primarily through deep feature learning with the average gradient outer product (AGOP). This takes a step further compared to efforts that explain neural collapse via feature-agnostic approaches, such as the unconstrained features model. We proceed by providing evidence that the right singular vectors and values of the weights are responsible for the majority of within-class variability collapse in DNNs. As shown in recent work, this singular structure is highly correlated with that of the AGOP. We then establish experimentally and theoretically that AGOP induces neural collapse in a randomly initialized neural network. In particular, we demonstrate that Deep Recursive Feature Machines, a method originally introduced as an abstraction for AGOP feature learning in convolutional neural networks, exhibits DNC.","sentences":["Deep Neural Collapse (DNC) refers to the surprisingly rigid structure of the data representations in the final layers of Deep Neural Networks (DNNs).","Though the phenomenon has been measured in a wide variety of settings, its emergence is only partially understood.","In this work, we provide substantial evidence that DNC formation occurs primarily through deep feature learning with the average gradient outer product (AGOP).","This takes a step further compared to efforts that explain neural collapse via feature-agnostic approaches, such as the unconstrained features model.","We proceed by providing evidence that the right singular vectors and values of the weights are responsible for the majority of within-class variability collapse in DNNs.","As shown in recent work, this singular structure is highly correlated with that of the AGOP.","We then establish experimentally and theoretically that AGOP induces neural collapse in a randomly initialized neural network.","In particular, we demonstrate that Deep Recursive Feature Machines, a method originally introduced as an abstraction for AGOP feature learning in convolutional neural networks, exhibits DNC."],"url":"http://arxiv.org/abs/2402.13728v1","category":"cs.LG"}
{"created":"2024-02-21 11:39:53","title":"Variable Mass and the Noisy Feynman Propagator in Scalar Fields","abstract":"We utilize a mass independent Klein-Gordon equation that is first order in a variable that plays the role of time, the approach taken in parametric time formulations. Using concepts from semigroup evolution, we examine the sign of a noisy Feynman propagator in a quantum field theory, namely, scalar electrodynamics.","sentences":["We utilize a mass independent Klein-Gordon equation that is first order in a variable that plays the role of time, the approach taken in parametric time formulations.","Using concepts from semigroup evolution, we examine the sign of a noisy Feynman propagator in a quantum field theory, namely, scalar electrodynamics."],"url":"http://arxiv.org/abs/2402.13727v1","category":"quant-ph"}
{"created":"2024-02-21 11:35:45","title":"Sparse and Structured Hopfield Networks","abstract":"Modern Hopfield networks have enjoyed recent interest due to their connection to attention in transformers. Our paper provides a unified framework for sparse Hopfield networks by establishing a link with Fenchel-Young losses. The result is a new family of Hopfield-Fenchel-Young energies whose update rules are end-to-end differentiable sparse transformations. We reveal a connection between loss margins, sparsity, and exact memory retrieval. We further extend this framework to structured Hopfield networks via the SparseMAP transformation, which can retrieve pattern associations instead of a single pattern. Experiments on multiple instance learning and text rationalization demonstrate the usefulness of our approach.","sentences":["Modern Hopfield networks have enjoyed recent interest due to their connection to attention in transformers.","Our paper provides a unified framework for sparse Hopfield networks by establishing a link with Fenchel-Young losses.","The result is a new family of Hopfield-Fenchel-Young energies whose update rules are end-to-end differentiable sparse transformations.","We reveal a connection between loss margins, sparsity, and exact memory retrieval.","We further extend this framework to structured Hopfield networks via the SparseMAP transformation, which can retrieve pattern associations instead of a single pattern.","Experiments on multiple instance learning and text rationalization demonstrate the usefulness of our approach."],"url":"http://arxiv.org/abs/2402.13725v1","category":"cs.LG"}
{"created":"2024-02-21 11:15:56","title":"Geometric derivation and structure-preserving simulation of quasi-geostrophy on the sphere","abstract":"We present a geometric derivation of the quasi-geostrophic equations on the sphere, starting from the rotating shallow water equations. We utilise perturbation series methods in vorticity and divergence variables. The derivation employs asymptotic analysis techniques, leading to a global quasi-geostrophic potential vorticity model on the sphere without approximation of the Coriolis parameter. The resulting model forms a closed system for the evolution of potential vorticity with a rich mathematical structure, including Lagrangian and Hamiltonian descriptions. Formulated using the Lie-Poisson bracket reveals the geometric invariants of the quasi-geostrophic model. Motivated by these geometric results, simulations of quasi-geostrophic flow on the sphere are presented based on structure-preserving Lie-Poisson time-integration. We explicitly demonstrate the preservation of Casimir invariants and show that the hyperbolic quasi-geostrophic equations can be simulated in a stable manner over long time. We show the emergence of longitudonal jets, wrapped around the circumference of the sphere in a general direction that is perpendicular to the axis of rotation.","sentences":["We present a geometric derivation of the quasi-geostrophic equations on the sphere, starting from the rotating shallow water equations.","We utilise perturbation series methods in vorticity and divergence variables.","The derivation employs asymptotic analysis techniques, leading to a global quasi-geostrophic potential vorticity model on the sphere without approximation of the Coriolis parameter.","The resulting model forms a closed system for the evolution of potential vorticity with a rich mathematical structure, including Lagrangian and Hamiltonian descriptions.","Formulated using the Lie-Poisson bracket reveals the geometric invariants of the quasi-geostrophic model.","Motivated by these geometric results, simulations of quasi-geostrophic flow on the sphere are presented based on structure-preserving Lie-Poisson time-integration.","We explicitly demonstrate the preservation of Casimir invariants and show that the hyperbolic quasi-geostrophic equations can be simulated in a stable manner over long time.","We show the emergence of longitudonal jets, wrapped around the circumference of the sphere in a general direction that is perpendicular to the axis of rotation."],"url":"http://arxiv.org/abs/2402.13707v1","category":"physics.flu-dyn"}
{"created":"2024-02-21 11:04:30","title":"Neural density estimation for Galactic Binaries in LISA data analysis","abstract":"The future space based gravitational wave detector LISA (Laser Interferometer Space Antenna) will observe millions of Galactic binaries constantly present in the data stream. A small fraction of this population (of the order of several thousand) will be individually resolved. One of the challenging tasks from the data analysis point of view will be to estimate the parameters of resolvable galactic binaries while disentangling them from each other and from other gravitational wave sources present in the data. This problem is quite often referred to as a global fit in the field of LISA data analysis. A Bayesian framework is often used to infer the parameters of the sources and their number. The efficiency of the sampling techniques strongly depends on the proposals, especially in the multi-dimensional parameter space. In this paper we demonstrate how we can use neural density estimators, and in particular Normalising flows, in order to build proposals which significantly improve the convergence of sampling. We also demonstrate how these methods could help in building priors based on physical models and provide an alternative way to represent the catalogue of identified gravitational wave sources.","sentences":["The future space based gravitational wave detector LISA (Laser Interferometer Space Antenna) will observe millions of Galactic binaries constantly present in the data stream.","A small fraction of this population (of the order of several thousand) will be individually resolved.","One of the challenging tasks from the data analysis point of view will be to estimate the parameters of resolvable galactic binaries while disentangling them from each other and from other gravitational wave sources present in the data.","This problem is quite often referred to as a global fit in the field of LISA data analysis.","A Bayesian framework is often used to infer the parameters of the sources and their number.","The efficiency of the sampling techniques strongly depends on the proposals, especially in the multi-dimensional parameter space.","In this paper we demonstrate how we can use neural density estimators, and in particular Normalising flows, in order to build proposals which significantly improve the convergence of sampling.","We also demonstrate how these methods could help in building priors based on physical models and provide an alternative way to represent the catalogue of identified gravitational wave sources."],"url":"http://arxiv.org/abs/2402.13701v1","category":"gr-qc"}
{"created":"2024-02-21 11:00:23","title":"Explainable Classification Techniques for Quantum Dot Device Measurements","abstract":"In the physical sciences, there is an increased need for robust feature representations of image data: image acquisition, in the generalized sense of two-dimensional data, is now widespread across a large number of fields, including quantum information science, which we consider here. While traditional image features are widely utilized in such cases, their use is rapidly being supplanted by Neural Network-based techniques that often sacrifice explainability in exchange for high accuracy. To ameliorate this trade-off, we propose a synthetic data-based technique that results in explainable features. We show, using Explainable Boosting Machines (EBMs), that this method offers superior explainability without sacrificing accuracy. Specifically, we show that there is a meaningful benefit to this technique in the context of quantum dot tuning, where human intervention is necessary at the current stage of development.","sentences":["In the physical sciences, there is an increased need for robust feature representations of image data: image acquisition, in the generalized sense of two-dimensional data, is now widespread across a large number of fields, including quantum information science, which we consider here.","While traditional image features are widely utilized in such cases, their use is rapidly being supplanted by Neural Network-based techniques that often sacrifice explainability in exchange for high accuracy.","To ameliorate this trade-off, we propose a synthetic data-based technique that results in explainable features.","We show, using Explainable Boosting Machines (EBMs), that this method offers superior explainability without sacrificing accuracy.","Specifically, we show that there is a meaningful benefit to this technique in the context of quantum dot tuning, where human intervention is necessary at the current stage of development."],"url":"http://arxiv.org/abs/2402.13699v1","category":"cs.CV"}
{"created":"2024-02-21 10:54:18","title":"Computational unique continuation with finite dimensional Neumann trace","abstract":"We consider finite element approximations of unique continuation problems subject to elliptic equations in the case where the normal derivative of the exact solution is known to reside in some finite dimensional space. To give quantitative error estimates we prove Lipschitz stability of the unique continuation problem in the global H1-norm. This stability is then leveraged to derive optimal a posteriori and a priori error estimates for a primal-dual stabilised finite method.","sentences":["We consider finite element approximations of unique continuation problems subject to elliptic equations in the case where the normal derivative of the exact solution is known to reside in some finite dimensional space.","To give quantitative error estimates we prove Lipschitz stability of the unique continuation problem in the global H1-norm.","This stability is then leveraged to derive optimal a posteriori and a priori error estimates for a primal-dual stabilised finite method."],"url":"http://arxiv.org/abs/2402.13695v1","category":"math.NA"}
{"created":"2024-02-21 10:52:27","title":"Higher-order fractional equations and related time-changed pseudo-processes","abstract":"We study Cauchy problems of fractional differential equations in both space and time variables by expressing the solution in terms of ``stochastic composition\" of the solutions to two simpler problems. These Cauchy sub-problems respectively concern the space and the time differential operator involved in the main equation. We provide some probabilistic and pseudo-probabilistic applications, where the solution can be interpreted as the pseudo-transition density of a time-changed pseudo-process. To extend our results to higher order time-fractional problems, we introduce pseudo-subordinators as well as its pseudo-inverse. Finally, we present our results in the case of more general differential operators and we interpret the results by means of a linear combination of pseudo-subordinators and its inverse.","sentences":["We study Cauchy problems of fractional differential equations in both space and time variables by expressing the solution in terms of ``stochastic composition\" of the solutions to two simpler problems.","These Cauchy sub-problems respectively concern the space and the time differential operator involved in the main equation.","We provide some probabilistic and pseudo-probabilistic applications, where the solution can be interpreted as the pseudo-transition density of a time-changed pseudo-process.","To extend our results to higher order time-fractional problems, we introduce pseudo-subordinators as well as its pseudo-inverse.","Finally, we present our results in the case of more general differential operators and we interpret the results by means of a linear combination of pseudo-subordinators and its inverse."],"url":"http://arxiv.org/abs/2402.13691v1","category":"math.PR"}
{"created":"2024-02-21 10:51:57","title":"General Caputo-type fractional discrete diffusion equation for Schr\u00f6dinger operator","abstract":"This article aims to investigate the semi-classical analog of the general Caputo-type diffusion equation with time-dependent diffusion coefficient associated with the discrete Schr\\\"{o}dinger operator, $\\mathcal{H}_{\\hbar,V}:=-\\hbar^{-2}\\mathcal{L}_{\\hbar}+V$ on the lattice $\\hbar\\mathbb{Z}^{n},$ where $V$ is a non-negative multiplication operator and $\\mathcal{L}_{\\hbar}$ is the discrete Laplacian. We establish the well-posedness of the Cauchy problem for the general Caputo-type diffusion equation with a regular coefficient in the associated Sobolev-type spaces. However, it is very weakly well-posed when the diffusion coefficient has a distributional singularity. Finally, we recapture the classical solution (resp. very weak) for the general Caputo-type diffusion equation in the semi-classical limit $\\hbar\\to 0$.","sentences":["This article aims to investigate the semi-classical analog of the general Caputo-type diffusion equation with time-dependent diffusion coefficient associated with the discrete Schr\\\"{o}dinger operator, $\\mathcal{H}_{\\hbar,V}:=-\\hbar^{-2}\\mathcal{L}_{\\hbar}+V$ on the lattice $\\hbar\\mathbb{Z}^{n},$ where $V$ is a non-negative multiplication operator and $\\mathcal{L}_{\\hbar}$ is the discrete Laplacian.","We establish the well-posedness of the Cauchy problem for the general Caputo-type diffusion equation with a regular coefficient in the associated Sobolev-type spaces.","However, it is very weakly well-posed when the diffusion coefficient has a distributional singularity.","Finally, we recapture the classical solution (resp.","very weak) for the general Caputo-type diffusion equation in the semi-classical limit $\\hbar\\to 0$."],"url":"http://arxiv.org/abs/2402.13690v1","category":"math.AP"}
{"created":"2024-02-21 10:41:54","title":"An Augmented Lagrangian Method for Training Recurrent Neural Networks","abstract":"Recurrent Neural Networks (RNNs) are widely used to model sequential data in a wide range of areas, such as natural language processing, speech recognition, machine translation, and time series analysis. In this paper, we model the training process of RNNs with the ReLU activation function as a constrained optimization problem with a smooth nonconvex objective function and piecewise smooth nonconvex constraints. We prove that any feasible point of the optimization problem satisfies the no nonzero abnormal multiplier constraint qualification (NNAMCQ), and any local minimizer is a Karush-Kuhn-Tucker (KKT) point of the problem. Moreover, we propose an augmented Lagrangian method (ALM) and design an efficient block coordinate descent (BCD) method to solve the subproblems of the ALM. The update of each block of the BCD method has a closed-form solution. The stop criterion for the inner loop is easy to check and can be stopped in finite steps. Moreover, we show that the BCD method can generate a directional stationary point of the subproblem. Furthermore, we establish the global convergence of the ALM to a KKT point of the constrained optimization problem. Compared with the state-of-the-art algorithms, numerical results demonstrate the efficiency and effectiveness of the ALM for training RNNs.","sentences":["Recurrent Neural Networks (RNNs) are widely used to model sequential data in a wide range of areas, such as natural language processing, speech recognition, machine translation, and time series analysis.","In this paper, we model the training process of RNNs with the ReLU activation function as a constrained optimization problem with a smooth nonconvex objective function and piecewise smooth nonconvex constraints.","We prove that any feasible point of the optimization problem satisfies the no nonzero abnormal multiplier constraint qualification (NNAMCQ), and any local minimizer is a Karush-Kuhn-Tucker (KKT) point of the problem.","Moreover, we propose an augmented Lagrangian method (ALM) and design an efficient block coordinate descent (BCD) method to solve the subproblems of the ALM.","The update of each block of the BCD method has a closed-form solution.","The stop criterion for the inner loop is easy to check and can be stopped in finite steps.","Moreover, we show that the BCD method can generate a directional stationary point of the subproblem.","Furthermore, we establish the global convergence of the ALM to a KKT point of the constrained optimization problem.","Compared with the state-of-the-art algorithms, numerical results demonstrate the efficiency and effectiveness of the ALM for training RNNs."],"url":"http://arxiv.org/abs/2402.13687v1","category":"math.OC"}
{"created":"2024-02-21 10:36:44","title":"Dynamics of bubble migration in a square channel flow of a viscoelastic fluid","abstract":"Cross-stream migration of a deformable bubble is investigated computationally in a pressure-driven channel flow of a viscoelastic fluid via interface-resolved simulations. The flow equations are solved fully coupled with the Giesekus model equations using the front-tracking method and extensive simulations are performed for a wide range of flow parameters to reveal the effects of bubble deformability, fluid elasticity, shear-thinning, and fluid inertia on the bubble migration dynamics. Migration rate of a bubble is found to be much higher than that of a solid particle under similar flow conditions mainly due to free-slip condition on its surface. It is observed that direction of bubble migration can be altered by varying shear-thinning of the ambient fluid. With a strong shear-thinning, the bubble migrates towards the wall while it migrates towards the center of the channel in a purely elastic fluid without shear-thinning. An onset of elastic flow instability is observed beyond a critical Weissenberg number, which in turn causes a path instability even for a nearly spherical bubble. An inertial path instability is also observed once bubble deformation exceeds a critical value. Shear-thinning is found to be suppressing the path instability in a viscoelastic fluid with a high polymer concentration whereas it reverses its role and promotes path instability in a dilute polymer solution. It is found that bubble migration towards wall induces a secondary flow with a velocity that is about an order of magnitude higher than the one induced by a solid particle under similar flow conditions.","sentences":["Cross-stream migration of a deformable bubble is investigated computationally in a pressure-driven channel flow of a viscoelastic fluid via interface-resolved simulations.","The flow equations are solved fully coupled with the Giesekus model equations using the front-tracking method and extensive simulations are performed for a wide range of flow parameters to reveal the effects of bubble deformability, fluid elasticity, shear-thinning, and fluid inertia on the bubble migration dynamics.","Migration rate of a bubble is found to be much higher than that of a solid particle under similar flow conditions mainly due to free-slip condition on its surface.","It is observed that direction of bubble migration can be altered by varying shear-thinning of the ambient fluid.","With a strong shear-thinning, the bubble migrates towards the wall while it migrates towards the center of the channel in a purely elastic fluid without shear-thinning.","An onset of elastic flow instability is observed beyond a critical Weissenberg number, which in turn causes a path instability even for a nearly spherical bubble.","An inertial path instability is also observed once bubble deformation exceeds a critical value.","Shear-thinning is found to be suppressing the path instability in a viscoelastic fluid with a high polymer concentration whereas it reverses its role and promotes path instability in a dilute polymer solution.","It is found that bubble migration towards wall induces a secondary flow with a velocity that is about an order of magnitude higher than the one induced by a solid particle under similar flow conditions."],"url":"http://arxiv.org/abs/2402.13683v1","category":"physics.flu-dyn"}
{"created":"2024-02-21 10:30:52","title":"A Pontryagin Maximum Principle for agent-based models with convex state space","abstract":"We derive a first order optimality condition for a class of agent-based systems, as well as for their mean-field counterpart. A relevant difficulty of our analysis is that the state equation is formulated on possibly infinite-dimensional convex subsets of Banach spaces, as required by some problems in multi-population dynamics. Due to the lack of a linear structure and of local compactness, the usual tools of needle variations and linearisation procedures used to derive Pontryagin type conditions have to be generalised to the setting at hand. This is done by considering suitable notions of differentials and by a careful inspection of the underlying functional structures.","sentences":["We derive a first order optimality condition for a class of agent-based systems, as well as for their mean-field counterpart.","A relevant difficulty of our analysis is that the state equation is formulated on possibly infinite-dimensional convex subsets of Banach spaces, as required by some problems in multi-population dynamics.","Due to the lack of a linear structure and of local compactness, the usual tools of needle variations and linearisation procedures used to derive Pontryagin type conditions have to be generalised to the setting at hand.","This is done by considering suitable notions of differentials and by a careful inspection of the underlying functional structures."],"url":"http://arxiv.org/abs/2402.13680v1","category":"math.AP"}
{"created":"2024-02-21 10:22:08","title":"An Edge-based Interface Tracking (EBIT) Method for Multiphase Flows with Phase Change","abstract":"In this paper, the Edge-Based Interface Tracking (EBIT) method is extended to simulate multiphase flows with phase change. As a novel Front-Tracking method, the EBIT method binds interfacial markers to the Eulerian grid to achieve automatic parallelization. To include phase change effects, the energy equation for each phase is solved, with the temperature boundary condition at the interface sharply imposed. When using collocated grids, the cell-centered velocity is approximately projected. This will lead to unphysical oscillations in the presence of phase change, as the velocity will be discontinuous across the interface. It is demonstrated that this issue can be addressed by using the ghost fluid method, in which the ghost velocity is set according to the jump condition, thereby removing the discontinuity. A series of benchmark tests are performed to validate the present method. It is shown that the numerical results agree very well with the theoretical solutions and the experimental results.","sentences":["In this paper, the Edge-Based Interface Tracking (EBIT) method is extended to simulate multiphase flows with phase change.","As a novel Front-Tracking method, the EBIT method binds interfacial markers to the Eulerian grid to achieve automatic parallelization.","To include phase change effects, the energy equation for each phase is solved, with the temperature boundary condition at the interface sharply imposed.","When using collocated grids, the cell-centered velocity is approximately projected.","This will lead to unphysical oscillations in the presence of phase change, as the velocity will be discontinuous across the interface.","It is demonstrated that this issue can be addressed by using the ghost fluid method, in which the ghost velocity is set according to the jump condition, thereby removing the discontinuity.","A series of benchmark tests are performed to validate the present method.","It is shown that the numerical results agree very well with the theoretical solutions and the experimental results."],"url":"http://arxiv.org/abs/2402.13677v1","category":"physics.flu-dyn"}
{"created":"2024-02-21 10:17:23","title":"Computing Transiting Exoplanet Parameters with 1D Convolutional Neural Networks","abstract":"The transit method allows the detection and characterization of planetary systems by analyzing stellar light curves. Convolutional neural networks appear to offer a viable solution for automating these analyses. In this research, two 1D convolutional neural network models, which work with simulated light curves in which transit-like signals were injected, are presented. One model operates on complete light curves and estimates the orbital period, and the other one operates on phase-folded light curves and estimates the semimajor axis of the orbit and the square of the planet-to-star radius ratio. Both models were tested on real data from TESS light curves with confirmed planets to ensure that they are able to work with real data. The results obtained show that 1D CNNs are able to characterize transiting exoplanets from their host star's detrended light curve and, furthermore, reducing both the required time and computational costs compared with the current detection and characterization algorithms.","sentences":["The transit method allows the detection and characterization of planetary systems by analyzing stellar light curves.","Convolutional neural networks appear to offer a viable solution for automating these analyses.","In this research, two 1D convolutional neural network models, which work with simulated light curves in which transit-like signals were injected, are presented.","One model operates on complete light curves and estimates the orbital period, and the other one operates on phase-folded light curves and estimates the semimajor axis of the orbit and the square of the planet-to-star radius ratio.","Both models were tested on real data from TESS light curves with confirmed planets to ensure that they are able to work with real data.","The results obtained show that 1D CNNs are able to characterize transiting exoplanets from their host star's detrended light curve and, furthermore, reducing both the required time and computational costs compared with the current detection and characterization algorithms."],"url":"http://arxiv.org/abs/2402.13673v1","category":"astro-ph.EP"}
{"created":"2024-02-21 09:55:17","title":"A five-dimensional Bianchi type V-like extension","abstract":"We uncover the solution space of a five dimensional geometry which we deem it as the direct counterpart of the Bianchi Type V cosmological model. We kinematically reduce the scale factor matrix and then, with an appropriate scaling and choice of time, we cast the spatial equations into a simple ``Kasner'' like form; thus revealing linear integrals of motion. Their number is enough so that, along with the quadratic constraint, it suffices to scan the entire space of solutions. The latter is revealed to be quite rich, containing cosmological solutions, some of which admit dimensional reduction asymptotically to four dimensions, Kundt spacetimes with vanishing type I (polynomial) curvature scalars and solutions describing periodic universes which behave like cosmological time crystals.","sentences":["We uncover the solution space of a five dimensional geometry which we deem it as the direct counterpart of the Bianchi Type V cosmological model.","We kinematically reduce the scale factor matrix and then, with an appropriate scaling and choice of time, we cast the spatial equations into a simple ``Kasner'' like form; thus revealing linear integrals of motion.","Their number is enough so that, along with the quadratic constraint, it suffices to scan the entire space of solutions.","The latter is revealed to be quite rich, containing cosmological solutions, some of which admit dimensional reduction asymptotically to four dimensions, Kundt spacetimes with vanishing type I (polynomial) curvature scalars and solutions describing periodic universes which behave like cosmological time crystals."],"url":"http://arxiv.org/abs/2402.13665v1","category":"gr-qc"}
{"created":"2024-02-21 09:53:45","title":"Quantum Monte Carlo and perturbative study of repulsive two-dimensional Bose-Fermi mixtures","abstract":"We derive analytically the leading beyond-mean field contributions to the zero-temperature equation of state and to the fermionic quasi-particle residue and effective mass of a dilute Bose-Fermi mixture in two dimensions. In the repulsive case, we perform quantum Monte Carlo simulations for two representative bosonic concentrations and equal masses, extending a method for correcting finite-size effects in fermionic gases to Bose-Fermi mixtures. We find good agreement between analytic expressions and numerical results for weak interactions, while significant discrepancies appear in the regime close to mechanical instability, above which we provide evidence of phase separation of the bosonic component.","sentences":["We derive analytically the leading beyond-mean field contributions to the zero-temperature equation of state and to the fermionic quasi-particle residue and effective mass of a dilute Bose-Fermi mixture in two dimensions.","In the repulsive case, we perform quantum Monte Carlo simulations for two representative bosonic concentrations and equal masses, extending a method for correcting finite-size effects in fermionic gases to Bose-Fermi mixtures.","We find good agreement between analytic expressions and numerical results for weak interactions, while significant discrepancies appear in the regime close to mechanical instability, above which we provide evidence of phase separation of the bosonic component."],"url":"http://arxiv.org/abs/2402.13664v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-21 09:52:35","title":"Continuum limit of the discrete nonlinear Klein-Gordon equation","abstract":"We study the convergence of solutions of the discrete nonlinear Klein-Gordon equation on an infinite lattice in the continuum limit, using recent tools developed in the context of nonlinear discrete dispersive equations. Our approach relies in particular on the use of bilinear estimates of the Shannon interpolation alongside controls on the growth of discrete Sobolev norms of the solution. We conclude by giving perspectives on uniform dispersive estimates for nonlinear waves on lattices.","sentences":["We study the convergence of solutions of the discrete nonlinear Klein-Gordon equation on an infinite lattice in the continuum limit, using recent tools developed in the context of nonlinear discrete dispersive equations.","Our approach relies in particular on the use of bilinear estimates of the Shannon interpolation alongside controls on the growth of discrete Sobolev norms of the solution.","We conclude by giving perspectives on uniform dispersive estimates for nonlinear waves on lattices."],"url":"http://arxiv.org/abs/2402.13663v1","category":"math.AP"}
{"created":"2024-02-21 18:52:20","title":"Misalignment, Learning, and Ranking: Harnessing Users Limited Attention","abstract":"In digital health and EdTech, recommendation systems face a significant challenge: users often choose impulsively, in ways that conflict with the platform's long-term payoffs. This misalignment makes it difficult to effectively learn to rank items, as it may hinder exploration of items with greater long-term payoffs. Our paper tackles this issue by utilizing users' limited attention spans. We propose a model where a platform presents items with unknown payoffs to the platform in a ranked list to $T$ users over time. Each user selects an item by first considering a prefix window of these ranked items and then picking the highest preferred item in that window (and the platform observes its payoff for this item). We study the design of online bandit algorithms that obtain vanishing regret against hindsight optimal benchmarks.   We first consider adversarial window sizes and stochastic iid payoffs. We design an active-elimination-based algorithm that achieves an optimal instance-dependent regret bound of $O(\\log(T))$, by showing matching regret upper and lower bounds. The key idea is using the combinatorial structure of the problem to either obtain a large payoff from each item or to explore by getting a sample from that item. This method systematically narrows down the item choices to enhance learning efficiency and payoff.   Second, we consider adversarial payoffs and stochastic iid window sizes. We start from the full-information problem of finding the permutation that maximizes the expected payoff. By a novel combinatorial argument, we characterize the polytope of admissible item selection probabilities by a permutation and show it has a polynomial-size representation. Using this representation, we show how standard algorithms for adversarial online linear optimization in the space of admissible probabilities can be used to obtain a polynomial-time algorithm with $O(\\sqrt{T})$ regret.","sentences":["In digital health and EdTech, recommendation systems face a significant challenge: users often choose impulsively, in ways that conflict with the platform's long-term payoffs.","This misalignment makes it difficult to effectively learn to rank items, as it may hinder exploration of items with greater long-term payoffs.","Our paper tackles this issue by utilizing users' limited attention spans.","We propose a model where a platform presents items with unknown payoffs to the platform in a ranked list to $T$ users over time.","Each user selects an item by first considering a prefix window of these ranked items and then picking the highest preferred item in that window (and the platform observes its payoff for this item).","We study the design of online bandit algorithms that obtain vanishing regret against hindsight optimal benchmarks.   ","We first consider adversarial window sizes and stochastic iid payoffs.","We design an active-elimination-based algorithm that achieves an optimal instance-dependent regret bound of $O(\\log(T))$, by showing matching regret upper and lower bounds.","The key idea is using the combinatorial structure of the problem to either obtain a large payoff from each item or to explore by getting a sample from that item.","This method systematically narrows down the item choices to enhance learning efficiency and payoff.   ","Second, we consider adversarial payoffs and stochastic iid window sizes.","We start from the full-information problem of finding the permutation that maximizes the expected payoff.","By a novel combinatorial argument, we characterize the polytope of admissible item selection probabilities by a permutation and show it has a polynomial-size representation.","Using this representation, we show how standard algorithms for adversarial online linear optimization in the space of admissible probabilities can be used to obtain a polynomial-time algorithm with $O(\\sqrt{T})$ regret."],"url":"http://arxiv.org/abs/2402.14013v1","category":"cs.LG"}
{"created":"2024-02-21 18:51:42","title":"Chasing Convex Functions with Long-term Constraints","abstract":"We introduce and study a family of online metric problems with long-term constraints. In these problems, an online player makes decisions $\\mathbf{x}_t$ in a metric space $(X,d)$ to simultaneously minimize their hitting cost $f_t(\\mathbf{x}_t)$ and switching cost as determined by the metric. Over the time horizon $T$, the player must satisfy a long-term demand constraint $\\sum_{t} c(\\mathbf{x}_t) \\geq 1$, where $c(\\mathbf{x}_t)$ denotes the fraction of demand satisfied at time $t$. Such problems can find a wide array of applications to online resource allocation in sustainable energy and computing systems. We devise optimal competitive and learning-augmented algorithms for specific instantiations of these problems, and further show that our proposed algorithms perform well in numerical experiments.","sentences":["We introduce and study a family of online metric problems with long-term constraints.","In these problems, an online player makes decisions $\\mathbf{x}_t$ in a metric space $(X,d)$ to simultaneously minimize their hitting cost $f_t(\\mathbf{x}_t)$ and switching cost as determined by the metric.","Over the time horizon $T$, the player must satisfy a long-term demand constraint $\\sum_{t} c(\\mathbf{x}_t) \\geq 1$, where $c(\\mathbf{x}_t)$ denotes the fraction of demand satisfied at time $t$. Such problems can find a wide array of applications to online resource allocation in sustainable energy and computing systems.","We devise optimal competitive and learning-augmented algorithms for specific instantiations of these problems, and further show that our proposed algorithms perform well in numerical experiments."],"url":"http://arxiv.org/abs/2402.14012v1","category":"cs.DS"}
{"created":"2024-02-21 18:09:37","title":"Plug-and-play analytical paradigm for the scattering of plane waves by \"layer-cake\" periodic systems","abstract":"We investigate the scattering of scalar plane waves in two dimensions by a heterogeneous layer that is periodic in the direction parallel to its boundary. On describing the layer as a union of periodic laminae, we develop a solution of the scattering problem by merging the concept of propagator matrices and that of Bloch eigenstates featured by the unit cell of each lamina. The featured Bloch eigenstates are obtained by solving the quadratic eigenvalue problem (QEVP) that seeks a complex-valued wavenumber normal to the layer boundary given (i) the excitation frequency, and (ii) real-valued wavenumber parallel to the boundary -- that is preserved throughout the system. Spectral analysis of the QEVP reveals sufficient conditions for discreteness of the eigenvalue spectrum and the fact that all eigenvalues come in complex-conjugate pairs. By deploying the factorization afforded by the propagator matrix approach, we demonstrate that the contribution of individual eigenvalues (and so eigenmodes) to the solution diminishes exponentially with absolute value of their imaginary part, which then forms a rational basis for truncation of the factorized Bloch-wave solution. The proposed methodology caters for the optimal design of rainbow traps, energy harvesters, and metasurfaces, whose potency to manipulate waves is decided not only by the individual dispersion characteristics of the component laminae, but also by ordering and generally fitting of the latter into a composite layer. Using the factorized Bloch approach, evaluation of trial configurations -- as generated by the permutation and window translation/stretching of the component laminae -- can be accelerated by decades.","sentences":["We investigate the scattering of scalar plane waves in two dimensions by a heterogeneous layer that is periodic in the direction parallel to its boundary.","On describing the layer as a union of periodic laminae, we develop a solution of the scattering problem by merging the concept of propagator matrices and that of Bloch eigenstates featured by the unit cell of each lamina.","The featured Bloch eigenstates are obtained by solving the quadratic eigenvalue problem (QEVP) that seeks a complex-valued wavenumber normal to the layer boundary given (i) the excitation frequency, and (ii) real-valued wavenumber parallel to the boundary -- that is preserved throughout the system.","Spectral analysis of the QEVP reveals sufficient conditions for discreteness of the eigenvalue spectrum and the fact that all eigenvalues come in complex-conjugate pairs.","By deploying the factorization afforded by the propagator matrix approach, we demonstrate that the contribution of individual eigenvalues (and so eigenmodes) to the solution diminishes exponentially with absolute value of their imaginary part, which then forms a rational basis for truncation of the factorized Bloch-wave solution.","The proposed methodology caters for the optimal design of rainbow traps, energy harvesters, and metasurfaces, whose potency to manipulate waves is decided not only by the individual dispersion characteristics of the component laminae, but also by ordering and generally fitting of the latter into a composite layer.","Using the factorized Bloch approach, evaluation of trial configurations -- as generated by the permutation and window translation/stretching of the component laminae -- can be accelerated by decades."],"url":"http://arxiv.org/abs/2402.13981v1","category":"math-ph"}
{"created":"2024-02-21 17:45:03","title":"Evaluating Ground State Energies of Chemical Systems with Low-Depth Quantum Circuits and High Accuracy","abstract":"Solving electronic structure problems is considered one of the most promising applications of quantum computing. However, due to limitations imposed by the coherence time of qubits in the Noisy Intermediate Scale Quantum (NISQ) era or the capabilities of early fault-tolerant quantum devices, it is vital to design algorithms with low-depth circuits. In this work, we develop an enhanced Variational Quantum Eigensolver (VQE) ansatz based on the Qubit Coupled Cluster (QCC) approach, which demands optimization over only $n$ parameters rather than the usual $n+2m$ parameters, where $n$ represents the number of Pauli string time evolution gates $e^{-itP}$, and $m$ is the number of qubits involved. We evaluate the ground state energies of $\\mathrm{O_3}$, $\\mathrm{Li_4}$, and $\\mathrm{Cr_2}$, using CAS(2,2), (4,4) and (6,6) respectively in conjunction with our enhanced QCC ansatz, UCCSD (Unitary Coupled Cluster Single Double) ansatz, and canonical CCSD method as the active space solver, and compare with CASCI results. Finally, we assess our enhanced QCC ansatz on two distinct quantum hardware, IBM Kolkata and Quantinuum H1-1.","sentences":["Solving electronic structure problems is considered one of the most promising applications of quantum computing.","However, due to limitations imposed by the coherence time of qubits in the Noisy Intermediate Scale Quantum (NISQ) era or the capabilities of early fault-tolerant quantum devices, it is vital to design algorithms with low-depth circuits.","In this work, we develop an enhanced Variational Quantum Eigensolver (VQE) ansatz based on the Qubit Coupled Cluster (QCC) approach, which demands optimization over only $n$ parameters rather than the usual $n+2m$ parameters, where $n$ represents the number of Pauli string time evolution gates $e^{-itP}$, and $m$ is the number of qubits involved.","We evaluate the ground state energies of $\\mathrm{O_3}$, $\\mathrm{Li_4}$, and $\\mathrm{Cr_2}$, using CAS(2,2), (4,4) and (6,6) respectively in conjunction with our enhanced QCC ansatz, UCCSD (Unitary Coupled Cluster Single Double) ansatz, and canonical CCSD method as the active space solver, and compare with CASCI results.","Finally, we assess our enhanced QCC ansatz on two distinct quantum hardware, IBM Kolkata and Quantinuum H1-1."],"url":"http://arxiv.org/abs/2402.13960v1","category":"quant-ph"}
{"created":"2024-02-21 17:40:51","title":"Improving threshold for fault-tolerant color code quantum computing by flagged weight optimization","abstract":"Color codes are promising quantum error correction (QEC) codes because they have an advantage over surface codes in that all Clifford gates can be implemented transversally. However, thresholds of color codes under circuit-level noise are relatively low mainly because measurements of their high-weight stabilizer generators cause an increase in a circuit depth, and thus, substantial errors are introduced. This makes color codes not the best candidate. Here, we propose a method to suppress the impact of such errors by optimizing weights of decoders using flag qubits and reducing the circuit depth using cat states. We set the weights based on conditional error probabilities conditioned on the measurement outcomes of flag qubits. In numerical simulations, we improve the threshold of the (4.8.8) color code under the circuit-level noise from 0.14% to around 0.27%, which is calculated by using an integer programming decoder. Furthermore, in the (6.6.6) color code, we achieved a circuit-level threshold of around 0.36%, which is almost the same value as the highest value in the previous studies employing the same noise model. In both cases, the achieved logical error rates at low physical error rates are almost one order of magnitude lower than a conventional method that uses a single ancilla qubit for each stabilizer measurement. This method can also be applied to other weight-based decoders, making the color codes more promising for the candidate of experimental implementation of QEC. Furthermore, one can utilize this approach to improve a threshold of wider classes of QEC codes, such as high-rate quantum low-density parity check codes.","sentences":["Color codes are promising quantum error correction (QEC) codes because they have an advantage over surface codes in that all Clifford gates can be implemented transversally.","However, thresholds of color codes under circuit-level noise are relatively low mainly because measurements of their high-weight stabilizer generators cause an increase in a circuit depth, and thus, substantial errors are introduced.","This makes color codes not the best candidate.","Here, we propose a method to suppress the impact of such errors by optimizing weights of decoders using flag qubits and reducing the circuit depth using cat states.","We set the weights based on conditional error probabilities conditioned on the measurement outcomes of flag qubits.","In numerical simulations, we improve the threshold of the (4.8.8) color code under the circuit-level noise from 0.14% to around 0.27%, which is calculated by using an integer programming decoder.","Furthermore, in the (6.6.6) color code, we achieved a circuit-level threshold of around 0.36%, which is almost the same value as the highest value in the previous studies employing the same noise model.","In both cases, the achieved logical error rates at low physical error rates are almost one order of magnitude lower than a conventional method that uses a single ancilla qubit for each stabilizer measurement.","This method can also be applied to other weight-based decoders, making the color codes more promising for the candidate of experimental implementation of QEC.","Furthermore, one can utilize this approach to improve a threshold of wider classes of QEC codes, such as high-rate quantum low-density parity check codes."],"url":"http://arxiv.org/abs/2402.13958v1","category":"quant-ph"}
{"created":"2024-02-21 17:32:00","title":"On Courant and Pleijel theorems for sub-Riemannian Laplacians","abstract":"We are interested in the number of nodal domains of eigenfunctions of sub-Laplacians on sub-Riemannian manifolds. Specifically, we investigate the validity of Pleijel's theorem, which states that, as soon as the dimension is strictly larger than 1, the number of nodal domains of an eigenfunction corresponding to the k-th eigenvalue is strictly (and uniformly, in a certain sense) smaller than k for large k.   In the first part of this paper we reduce this question from the case of general sub-Riemannian manifolds to that of nilpotent groups.   In the second part, we analyze in detail the case where the nilpotent group is a Heisenberg group times a Euclidean space. Along the way we improve known bounds on the optimal constants in the Faber-Krahn and isoperimetric inequalities on these groups.","sentences":["We are interested in the number of nodal domains of eigenfunctions of sub-Laplacians on sub-Riemannian manifolds.","Specifically, we investigate the validity of Pleijel's theorem, which states that, as soon as the dimension is strictly larger than 1, the number of nodal domains of an eigenfunction corresponding to the k-th eigenvalue is strictly (and uniformly, in a certain sense) smaller than k for large k.   ","In the first part of this paper we reduce this question from the case of general sub-Riemannian manifolds to that of nilpotent groups.   ","In the second part, we analyze in detail the case where the nilpotent group is a Heisenberg group times a Euclidean space.","Along the way we improve known bounds on the optimal constants in the Faber-Krahn and isoperimetric inequalities on these groups."],"url":"http://arxiv.org/abs/2402.13953v1","category":"math.SP"}
{"created":"2024-02-21 17:23:43","title":"Generating Realistic Arm Movements in Reinforcement Learning: A Quantitative Comparison of Reward Terms and Task Requirements","abstract":"The mimicking of human-like arm movement characteristics involves the consideration of three factors during control policy synthesis: (a) chosen task requirements, (b) inclusion of noise during movement execution and (c) chosen optimality principles. Previous studies showed that when considering these factors (a-c) individually, it is possible to synthesize arm movements that either kinematically match the experimental data or reproduce the stereotypical triphasic muscle activation pattern. However, to date no quantitative comparison has been made on how realistic the arm movement generated by each factor is; as well as whether a partial or total combination of all factors results in arm movements with human-like kinematic characteristics and a triphasic muscle pattern. To investigate this, we used reinforcement learning to learn a control policy for a musculoskeletal arm model, aiming to discern which combination of factors (a-c) results in realistic arm movements according to four frequently reported stereotypical characteristics. Our findings indicate that incorporating velocity and acceleration requirements into the reaching task, employing reward terms that encourage minimization of mechanical work, hand jerk, and control effort, along with the inclusion of noise during movement, leads to the emergence of realistic human arm movements in reinforcement learning. We expect that the gained insights will help in the future to better predict desired arm movements and corrective forces in wearable assistive devices.","sentences":["The mimicking of human-like arm movement characteristics involves the consideration of three factors during control policy synthesis: (a) chosen task requirements, (b) inclusion of noise during movement execution and (c) chosen optimality principles.","Previous studies showed that when considering these factors (a-c) individually, it is possible to synthesize arm movements that either kinematically match the experimental data or reproduce the stereotypical triphasic muscle activation pattern.","However, to date no quantitative comparison has been made on how realistic the arm movement generated by each factor is; as well as whether a partial or total combination of all factors results in arm movements with human-like kinematic characteristics and a triphasic muscle pattern.","To investigate this, we used reinforcement learning to learn a control policy for a musculoskeletal arm model, aiming to discern which combination of factors (a-c) results in realistic arm movements according to four frequently reported stereotypical characteristics.","Our findings indicate that incorporating velocity and acceleration requirements into the reaching task, employing reward terms that encourage minimization of mechanical work, hand jerk, and control effort, along with the inclusion of noise during movement, leads to the emergence of realistic human arm movements in reinforcement learning.","We expect that the gained insights will help in the future to better predict desired arm movements and corrective forces in wearable assistive devices."],"url":"http://arxiv.org/abs/2402.13949v1","category":"cs.RO"}
{"created":"2024-02-21 17:08:00","title":"Microstructured large-area photoconductive terahertz emitters driven at high average power","abstract":"Emitters based on photoconductive materials excited by ultrafast lasers are well established and popular devices for THz generation. However, so far, these emitters, both photoconductive antennas and large area emitters, were mostly explored using driving lasers with moderate average powers (either fiber lasers with up to hundreds of milliwatts or Ti:Sapphire systems up to few watts). In this paper, we explore the use of high power, MHz repetition rate Ytterbium (Yb) based oscillator for THz emission using a microstructured large area photoconductive emitter, consist of semi insulating GaAs with a 10 by 10 mm2 active area. As a driving source, we use a frequency doubled home built high average power ultrafast Yb oscillator, delivering 22 W of average power, 115 fs pulses with 91 MHz repetition rate at a central wavelength of 516 nm. When applying 9 W of average power (after an optical chopper with a duty cycle of 50 percent) on the structure without optimized heatsinking, we obtain 65 uW THz average power, 4 THz bandwidth; furthermore, we safely apply up to 18 W of power on the structure without observing damage. We investigate the impact of excitation power, bias voltage, optical fluence, and their interplay on the emitter performance and explore in detail the sources of thermal load originating from electrical and optical power. Optical power is found to have a more critical impact on LAE saturation than electrical power, thus optimized heatsinking will allow us to improve the conversion efficiency in the near future towards much higher emitter power. This work paves the way towards achieving hundreds of MHz or even GHz repetition rates, high power THz sources based on photoconductive emitters, that are of great interest for example for future THz imaging applications.","sentences":["Emitters based on photoconductive materials excited by ultrafast lasers are well established and popular devices for THz generation.","However, so far, these emitters, both photoconductive antennas and large area emitters, were mostly explored using driving lasers with moderate average powers (either fiber lasers with up to hundreds of milliwatts or Ti:Sapphire systems up to few watts).","In this paper, we explore the use of high power, MHz repetition rate Ytterbium (Yb) based oscillator for THz emission using a microstructured large area photoconductive emitter, consist of semi insulating GaAs with a 10 by 10 mm2 active area.","As a driving source, we use a frequency doubled home built high average power ultrafast Yb oscillator, delivering 22 W of average power, 115 fs pulses with 91 MHz repetition rate at a central wavelength of 516 nm.","When applying 9 W of average power (after an optical chopper with a duty cycle of 50 percent) on the structure without optimized heatsinking, we obtain 65 uW THz average power, 4 THz bandwidth; furthermore, we safely apply up to 18 W of power on the structure without observing damage.","We investigate the impact of excitation power, bias voltage, optical fluence, and their interplay on the emitter performance and explore in detail the sources of thermal load originating from electrical and optical power.","Optical power is found to have a more critical impact on LAE saturation than electrical power, thus optimized heatsinking will allow us to improve the conversion efficiency in the near future towards much higher emitter power.","This work paves the way towards achieving hundreds of MHz or even GHz repetition rates, high power THz sources based on photoconductive emitters, that are of great interest for example for future THz imaging applications."],"url":"http://arxiv.org/abs/2402.13940v1","category":"physics.optics"}
{"created":"2024-02-21 17:05:06","title":"Distinctive Image Captioning: Leveraging Ground Truth Captions in CLIP Guided Reinforcement Learning","abstract":"Training image captioning models using teacher forcing results in very generic samples, whereas more distinctive captions can be very useful in retrieval applications or to produce alternative texts describing images for accessibility. Reinforcement Learning (RL) allows to use cross-modal retrieval similarity score between the generated caption and the input image as reward to guide the training, leading to more distinctive captions. Recent studies show that pre-trained cross-modal retrieval models can be used to provide this reward, completely eliminating the need for reference captions. However, we argue in this paper that Ground Truth (GT) captions can still be useful in this RL framework. We propose a new image captioning model training strategy that makes use of GT captions in different ways. Firstly, they can be used to train a simple MLP discriminator that serves as a regularization to prevent reward hacking and ensures the fluency of generated captions, resulting in a textual GAN setup extended for multimodal inputs. Secondly, they can serve as additional trajectories in the RL strategy, resulting in a teacher forcing loss weighted by the similarity of the GT to the image. This objective acts as an additional learning signal grounded to the distribution of the GT captions. Thirdly, they can serve as strong baselines when added to the pool of captions used to compute the proposed contrastive reward to reduce the variance of gradient estimate. Experiments on MS-COCO demonstrate the interest of the proposed training strategy to produce highly distinctive captions while maintaining high writing quality.","sentences":["Training image captioning models using teacher forcing results in very generic samples, whereas more distinctive captions can be very useful in retrieval applications or to produce alternative texts describing images for accessibility.","Reinforcement Learning (RL) allows to use cross-modal retrieval similarity score between the generated caption and the input image as reward to guide the training, leading to more distinctive captions.","Recent studies show that pre-trained cross-modal retrieval models can be used to provide this reward, completely eliminating the need for reference captions.","However, we argue in this paper that Ground Truth (GT) captions can still be useful in this RL framework.","We propose a new image captioning model training strategy that makes use of GT captions in different ways.","Firstly, they can be used to train a simple MLP discriminator that serves as a regularization to prevent reward hacking and ensures the fluency of generated captions, resulting in a textual GAN setup extended for multimodal inputs.","Secondly, they can serve as additional trajectories in the RL strategy, resulting in a teacher forcing loss weighted by the similarity of the GT to the image.","This objective acts as an additional learning signal grounded to the distribution of the GT captions.","Thirdly, they can serve as strong baselines when added to the pool of captions used to compute the proposed contrastive reward to reduce the variance of gradient estimate.","Experiments on MS-COCO demonstrate the interest of the proposed training strategy to produce highly distinctive captions while maintaining high writing quality."],"url":"http://arxiv.org/abs/2402.13936v1","category":"cs.CL"}
{"created":"2024-02-21 16:39:28","title":"Practical algorithms for Hierarchical overlap graphs","abstract":"Genome assembly is a prominent problem studied in bioinformatics, which computes the source string using a set of its overlapping substrings. Classically, genome assembly uses assembly graphs built using this set of substrings to compute the source string efficiently, having a tradeoff between scalability and avoiding information loss. The scalable de Bruijn graphs come at the price of losing crucial overlap information. The complete overlap information is stored in overlap graphs using quadratic space. Hierarchical overlap graphs [IPL20] (HOG) overcome these limitations, avoiding information loss despite using linear space. After a series of suboptimal improvements, Khan and Park et al. simultaneously presented two optimal algorithms [CPM2021], where only the former was seemingly practical.   We empirically analyze all the practical algorithms for computing HOG, where the optimal algorithm [CPM2021] outperforms the previous algorithms as expected, though at the expense of extra memory. However, it uses non-intuitive approach and non-trivial data structures. We present arguably the most intuitive algorithm, using only elementary arrays, which is also optimal. Our algorithm empirically proves even better for both time and memory over all the algorithms, highlighting its significance in both theory and practice.   We further explore the applications of hierarchical overlap graphs to solve various forms of suffix-prefix queries on a set of strings. Loukides et al. [CPM2023] recently presented state-of-the-art algorithms for these queries. However, these algorithms require complex black-box data structures and are seemingly impractical. Our algorithms, despite failing to match the state-of-the-art algorithms theoretically, answer different queries ranging from 0.01-100 milliseconds for a data set having around a billion characters.","sentences":["Genome assembly is a prominent problem studied in bioinformatics, which computes the source string using a set of its overlapping substrings.","Classically, genome assembly uses assembly graphs built using this set of substrings to compute the source string efficiently, having a tradeoff between scalability and avoiding information loss.","The scalable de Bruijn graphs come at the price of losing crucial overlap information.","The complete overlap information is stored in overlap graphs using quadratic space.","Hierarchical overlap graphs [IPL20] (HOG) overcome these limitations, avoiding information loss despite using linear space.","After a series of suboptimal improvements, Khan and Park et al. simultaneously presented two optimal algorithms","[CPM2021], where only the former was seemingly practical.   ","We empirically analyze all the practical algorithms for computing HOG, where the optimal algorithm","[CPM2021] outperforms the previous algorithms as expected, though at the expense of extra memory.","However, it uses non-intuitive approach and non-trivial data structures.","We present arguably the most intuitive algorithm, using only elementary arrays, which is also optimal.","Our algorithm empirically proves even better for both time and memory over all the algorithms, highlighting its significance in both theory and practice.   ","We further explore the applications of hierarchical overlap graphs to solve various forms of suffix-prefix queries on a set of strings.","Loukides et al.","[CPM2023] recently presented state-of-the-art algorithms for these queries.","However, these algorithms require complex black-box data structures and are seemingly impractical.","Our algorithms, despite failing to match the state-of-the-art algorithms theoretically, answer different queries ranging from 0.01-100 milliseconds for a data set having around a billion characters."],"url":"http://arxiv.org/abs/2402.13920v1","category":"cs.DS"}
{"created":"2024-02-21 16:23:14","title":"Quadratic inference with dense functional responses","abstract":"We address the challenge of estimation in the context of constant linear effect models with dense functional responses. In this framework, the conditional expectation of the response curve is represented by a linear combination of functional covariates with constant regression parameters. In this paper, we present an alternative solution by employing the quadratic inference approach, a well-established method for analyzing correlated data, to estimate the regression coefficients. Our approach leverages non-parametrically estimated basis functions, eliminating the need for choosing working correlation structures. Furthermore, we demonstrate that our method achieves a parametric $\\sqrt{n}$-convergence rate, contingent on an appropriate choice of bandwidth. This convergence is observed when the number of repeated measurements per trajectory exceeds a certain threshold, specifically, when it surpasses $n^{a_{0}}$, with $n$ representing the number of trajectories. Additionally, we establish the asymptotic normality of the resulting estimator. The performance of the proposed method is compared with that of existing methods through extensive simulation studies, where our proposed method outperforms. Real data analysis is also conducted to demonstrate the proposed method.","sentences":["We address the challenge of estimation in the context of constant linear effect models with dense functional responses.","In this framework, the conditional expectation of the response curve is represented by a linear combination of functional covariates with constant regression parameters.","In this paper, we present an alternative solution by employing the quadratic inference approach, a well-established method for analyzing correlated data, to estimate the regression coefficients.","Our approach leverages non-parametrically estimated basis functions, eliminating the need for choosing working correlation structures.","Furthermore, we demonstrate that our method achieves a parametric $\\sqrt{n}$-convergence rate, contingent on an appropriate choice of bandwidth.","This convergence is observed when the number of repeated measurements per trajectory exceeds a certain threshold, specifically, when it surpasses $n^{a_{0}}$, with $n$ representing the number of trajectories.","Additionally, we establish the asymptotic normality of the resulting estimator.","The performance of the proposed method is compared with that of existing methods through extensive simulation studies, where our proposed method outperforms.","Real data analysis is also conducted to demonstrate the proposed method."],"url":"http://arxiv.org/abs/2402.13907v1","category":"stat.ME"}
{"created":"2024-02-21 16:13:49","title":"Dealing with unbounded gradients in stochastic saddle-point optimization","abstract":"We study the performance of stochastic first-order methods for finding saddle points of convex-concave functions. A notorious challenge faced by such methods is that the gradients can grow arbitrarily large during optimization, which may result in instability and divergence. In this paper, we propose a simple and effective regularization technique that stabilizes the iterates and yields meaningful performance guarantees even if the domain and the gradient noise scales linearly with the size of the iterates (and is thus potentially unbounded). Besides providing a set of general results, we also apply our algorithm to a specific problem in reinforcement learning, where it leads to performance guarantees for finding near-optimal policies in an average-reward MDP without prior knowledge of the bias span.","sentences":["We study the performance of stochastic first-order methods for finding saddle points of convex-concave functions.","A notorious challenge faced by such methods is that the gradients can grow arbitrarily large during optimization, which may result in instability and divergence.","In this paper, we propose a simple and effective regularization technique that stabilizes the iterates and yields meaningful performance guarantees even if the domain and the gradient noise scales linearly with the size of the iterates (and is thus potentially unbounded).","Besides providing a set of general results, we also apply our algorithm to a specific problem in reinforcement learning, where it leads to performance guarantees for finding near-optimal policies in an average-reward MDP without prior knowledge of the bias span."],"url":"http://arxiv.org/abs/2402.13903v1","category":"cs.LG"}
{"created":"2024-02-21 15:49:33","title":"Partial-transpose-guided entanglement classes and minimum noise filtering in many-body Gaussian quantum systems","abstract":"The reduction and distortion of quantum correlations in the presence of classical noise leads to varied levels of inefficiency in the availability of entanglement as a resource for quantum information processing protocols. While generically minimizing required entanglement for mixed quantum states remains challenging, a class of many-body Gaussian quantum states ($\\mathcal{N}$IC) is here identified that exhibits two-mode bipartite entanglement structure, resembling that of pure states, for which the logarithmic negativity entanglement measure remains invariant upon inclusion of the classical correlations and optimal entanglement resources can be clearly quantified. This subclass is found to be embedded within a broader class of many-body Gaussian states ($\\mathcal{N}$-SOL) that retain two-mode entanglement structure for detection processes. These two entanglement classes are relevant in theoretical and experimental applications from the scalar field vacuum to the local axial motional modes of trapped ion chains. Utilizing the subspace that heralds inseparability in response to partial transposition, a minimum noise filtering process is designed to be necessary, sufficient, and computable for determining membership in these classes of entanglement structure. Application of this process to spacelike regions of the free scalar field vacuum is found to improve resource upper bounds, providing new understanding of the entanglement required for the quantum simulation of quantum fields as observed by arrays of local detectors.","sentences":["The reduction and distortion of quantum correlations in the presence of classical noise leads to varied levels of inefficiency in the availability of entanglement as a resource for quantum information processing protocols.","While generically minimizing required entanglement for mixed quantum states remains challenging, a class of many-body Gaussian quantum states ($\\mathcal{N}$IC) is here identified that exhibits two-mode bipartite entanglement structure, resembling that of pure states, for which the logarithmic negativity entanglement measure remains invariant upon inclusion of the classical correlations and optimal entanglement resources can be clearly quantified.","This subclass is found to be embedded within a broader class of many-body Gaussian states ($\\mathcal{N}$-SOL) that retain two-mode entanglement structure for detection processes.","These two entanglement classes are relevant in theoretical and experimental applications from the scalar field vacuum to the local axial motional modes of trapped ion chains.","Utilizing the subspace that heralds inseparability in response to partial transposition, a minimum noise filtering process is designed to be necessary, sufficient, and computable for determining membership in these classes of entanglement structure.","Application of this process to spacelike regions of the free scalar field vacuum is found to improve resource upper bounds, providing new understanding of the entanglement required for the quantum simulation of quantum fields as observed by arrays of local detectors."],"url":"http://arxiv.org/abs/2402.13881v1","category":"quant-ph"}
{"created":"2024-02-21 15:23:21","title":"Generative Probabilistic Time Series Forecasting and Applications in Grid Operations","abstract":"Generative probabilistic forecasting produces future time series samples according to the conditional probability distribution given past time series observations. Such techniques are essential in risk-based decision-making and planning under uncertainty with broad applications in grid operations, including electricity price forecasting, risk-based economic dispatch, and stochastic optimizations. Inspired by Wiener and Kallianpur's innovation representation, we propose a weak innovation autoencoder architecture and a learning algorithm to extract independent and identically distributed innovation sequences from nonparametric stationary time series. We show that the weak innovation sequence is Bayesian sufficient, which makes the proposed weak innovation autoencoder a canonical architecture for generative probabilistic forecasting. The proposed technique is applied to forecasting highly volatile real-time electricity prices, demonstrating superior performance across multiple forecasting measures over leading probabilistic and point forecasting techniques.","sentences":["Generative probabilistic forecasting produces future time series samples according to the conditional probability distribution given past time series observations.","Such techniques are essential in risk-based decision-making and planning under uncertainty with broad applications in grid operations, including electricity price forecasting, risk-based economic dispatch, and stochastic optimizations.","Inspired by Wiener and Kallianpur's innovation representation, we propose a weak innovation autoencoder architecture and a learning algorithm to extract independent and identically distributed innovation sequences from nonparametric stationary time series.","We show that the weak innovation sequence is Bayesian sufficient, which makes the proposed weak innovation autoencoder a canonical architecture for generative probabilistic forecasting.","The proposed technique is applied to forecasting highly volatile real-time electricity prices, demonstrating superior performance across multiple forecasting measures over leading probabilistic and point forecasting techniques."],"url":"http://arxiv.org/abs/2402.13870v1","category":"cs.LG"}
{"created":"2024-02-21 15:13:43","title":"Variable Projection Algorithms: Theoretical Insights and A Novel Approach for Problems with Large Residual","abstract":"This paper delves into an in-depth exploration of the Variable Projection (VP) algorithm, a powerful tool for solving separable nonlinear optimization problems across multiple domains, including system identification, image processing, and machine learning. We first establish a theoretical framework to examine the effect of the approximate treatment of the coupling relationship among parameters on the local convergence of the VP algorithm and theoretically prove that the Kaufman's VP algorithm can achieve a similar convergence rate as the Golub \\& Pereyra's form. These studies fill the gap in the existing convergence theory analysis, and provide a solid foundation for understanding the mechanism of VP algorithm and broadening its application horizons. Furthermore, drawing inspiration from these theoretical revelations, we design a refined VP algorithm for handling separable nonlinear optimization problems characterized by large residual, called VPLR, which boosts the convergence performance by addressing the interdependence of parameters within the separable model and by continually correcting the approximated Hessian matrix to counteract the influence of large residual during the iterative process. The effectiveness of this refined algorithm is corroborated through numerical experimentation.","sentences":["This paper delves into an in-depth exploration of the Variable Projection (VP) algorithm, a powerful tool for solving separable nonlinear optimization problems across multiple domains, including system identification, image processing, and machine learning.","We first establish a theoretical framework to examine the effect of the approximate treatment of the coupling relationship among parameters on the local convergence of the VP algorithm and theoretically prove that the Kaufman's VP algorithm can achieve a similar convergence rate as the Golub \\& Pereyra's form.","These studies fill the gap in the existing convergence theory analysis, and provide a solid foundation for understanding the mechanism of VP algorithm and broadening its application horizons.","Furthermore, drawing inspiration from these theoretical revelations, we design a refined VP algorithm for handling separable nonlinear optimization problems characterized by large residual, called VPLR, which boosts the convergence performance by addressing the interdependence of parameters within the separable model and by continually correcting the approximated Hessian matrix to counteract the influence of large residual during the iterative process.","The effectiveness of this refined algorithm is corroborated through numerical experimentation."],"url":"http://arxiv.org/abs/2402.13865v1","category":"math.OC"}
{"created":"2024-02-21 15:06:51","title":"Replicable Learning of Large-Margin Halfspaces","abstract":"We provide efficient replicable algorithms for the problem of learning large-margin halfspaces. Our results improve upon the algorithms provided by Impagliazzo, Lei, Pitassi, and Sorrell [STOC, 2022]. We design the first dimension-independent replicable algorithms for this task which runs in polynomial time, is proper, and has strictly improved sample complexity compared to the one achieved by Impagliazzo et al. [2022] with respect to all the relevant parameters. Moreover, our first algorithm has sample complexity that is optimal with respect to the accuracy parameter $\\epsilon$. We also design an SGD-based replicable algorithm that, in some parameters' regimes, achieves better sample and time complexity than our first algorithm.   Departing from the requirement of polynomial time algorithms, using the DP-to-Replicability reduction of Bun, Gaboardi, Hopkins, Impagliazzo, Lei, Pitassi, Sorrell, and Sivakumar [STOC, 2023], we show how to obtain a replicable algorithm for large-margin halfspaces with improved sample complexity with respect to the margin parameter $\\tau$, but running time doubly exponential in $1/\\tau^2$ and worse sample complexity dependence on $\\epsilon$ than one of our previous algorithms. We then design an improved algorithm with better sample complexity than all three of our previous algorithms and running time exponential in $1/\\tau^{2}$.","sentences":["We provide efficient replicable algorithms for the problem of learning large-margin halfspaces.","Our results improve upon the algorithms provided by Impagliazzo, Lei, Pitassi, and Sorrell","[STOC, 2022].","We design the first dimension-independent replicable algorithms for this task which runs in polynomial time, is proper, and has strictly improved sample complexity compared to the one achieved by Impagliazzo et al.","[2022] with respect to all the relevant parameters.","Moreover, our first algorithm has sample complexity that is optimal with respect to the accuracy parameter $\\epsilon$. We also design an SGD-based replicable algorithm that, in some parameters' regimes, achieves better sample and time complexity than our first algorithm.   ","Departing from the requirement of polynomial time algorithms, using the DP-to-Replicability reduction of Bun, Gaboardi, Hopkins, Impagliazzo, Lei, Pitassi, Sorrell, and","Sivakumar","[STOC, 2023], we show how to obtain a replicable algorithm for large-margin halfspaces with improved sample complexity with respect to the margin parameter $\\tau$, but running time doubly exponential in $1/\\tau^2$ and worse sample complexity dependence on $\\epsilon$ than one of our previous algorithms.","We then design an improved algorithm with better sample complexity than all three of our previous algorithms and running time exponential in $1/\\tau^{2}$."],"url":"http://arxiv.org/abs/2402.13857v1","category":"cs.LG"}
{"created":"2024-02-21 14:43:34","title":"Multi-Agent Online Graph Exploration on Cycles and Tadpole Graphs","abstract":"We study the problem of multi-agent online graph exploration, in which a team of k agents has to explore a given graph, starting and ending on the same node. The graph is initially unknown. Whenever a node is visited by an agent, its neighborhood and adjacent edges are revealed. The agents share a global view of the explored parts of the graph. The cost of the exploration has to be minimized, where cost either describes the time needed for the entire exploration (time model), or the length of the longest path traversed by any agent (energy model). We investigate graph exploration on cycles and tadpole graphs for 2-4 agents, providing optimal results on the competitive ratio in the energy model (1-competitive with two agents on cycles and three agents on tadpole graphs), and for tadpole graphs in the time model (1.5-competitive with four agents). We also show competitive upper bounds of 2 for the exploration of tadpole graphs with three agents, and 2.5 for the exploration of tadpole graphs with two agents in the time model.","sentences":["We study the problem of multi-agent online graph exploration, in which a team of k agents has to explore a given graph, starting and ending on the same node.","The graph is initially unknown.","Whenever a node is visited by an agent, its neighborhood and adjacent edges are revealed.","The agents share a global view of the explored parts of the graph.","The cost of the exploration has to be minimized, where cost either describes the time needed for the entire exploration (time model), or the length of the longest path traversed by any agent (energy model).","We investigate graph exploration on cycles and tadpole graphs for 2-4 agents, providing optimal results on the competitive ratio in the energy model (1-competitive with two agents on cycles and three agents on tadpole graphs), and for tadpole graphs in the time model (1.5-competitive with four agents).","We also show competitive upper bounds of 2 for the exploration of tadpole graphs with three agents, and 2.5 for the exploration of tadpole graphs with two agents in the time model."],"url":"http://arxiv.org/abs/2402.13845v1","category":"cs.DS"}
{"created":"2024-02-21 13:52:20","title":"A quadratically convergent semismooth Newton method for nonlinear semidefinite programming without the subdifferential regularity","abstract":"We introduce a quadratically convergent semismooth Newton method for nonlinear semidefinite programming that eliminates the need for the subdifferential regularity, a common yet stringent requirement in existing approaches. Our strategy revolves around identifying a single nonsingular element within the B(ouligand)-subdifferential, thus avoiding the standard requirement for nonsingularity across the entire subdifferential set, which is often too restrictive for practical applications. The theoretical framework is supported by the introduction of the weak second order condition (W-SOC) and the weak strict Robinson constraint qualification (W-SRCQ). These conditions not only guarantee the existence of a nonsingular element in the subdifferential but also forge a primal-dual connection in linearly constrained convex quadratic programming. The theoretical advancements further lay the foundation for the algorithmic designing of a novel semismooth Newton method, which integrates a correction step to address degenerate issues. Particularly, this correction step ensures the local convergence as well as a superlinear/quadratic convergence rate of the proposed method. Preliminary numerical experiments corroborate our theoretical findings and underscore the practical effectiveness of our method.","sentences":["We introduce a quadratically convergent semismooth Newton method for nonlinear semidefinite programming that eliminates the need for the subdifferential regularity, a common yet stringent requirement in existing approaches.","Our strategy revolves around identifying a single nonsingular element within the B(ouligand)-subdifferential, thus avoiding the standard requirement for nonsingularity across the entire subdifferential set, which is often too restrictive for practical applications.","The theoretical framework is supported by the introduction of the weak second order condition (W-SOC) and the weak strict Robinson constraint qualification (W-SRCQ).","These conditions not only guarantee the existence of a nonsingular element in the subdifferential but also forge a primal-dual connection in linearly constrained convex quadratic programming.","The theoretical advancements further lay the foundation for the algorithmic designing of a novel semismooth Newton method, which integrates a correction step to address degenerate issues.","Particularly, this correction step ensures the local convergence as well as a superlinear/quadratic convergence rate of the proposed method.","Preliminary numerical experiments corroborate our theoretical findings and underscore the practical effectiveness of our method."],"url":"http://arxiv.org/abs/2402.13814v1","category":"math.OC"}
{"created":"2024-02-21 13:50:46","title":"Voice-Driven Mortality Prediction in Hospitalized Heart Failure Patients: A Machine Learning Approach Enhanced with Diagnostic Biomarkers","abstract":"Addressing heart failure (HF) as a prevalent global health concern poses difficulties in implementing innovative approaches for enhanced patient care. Predicting mortality rates in HF patients, in particular, is difficult yet critical, necessitating individualized care, proactive management, and enabling educated decision-making to enhance outcomes. Recently, the significance of voice biomarkers coupled with Machine Learning (ML) has surged, demonstrating remarkable efficacy, particularly in predicting heart failure. The synergy of voice analysis and ML algorithms provides a non-invasive and easily accessible means to evaluate patients' health. However, there is a lack of voice biomarkers for predicting mortality rates among heart failure patients with standardized speech protocols. Here, we demonstrate a powerful and effective ML model for predicting mortality rates in hospitalized HF patients through the utilization of voice biomarkers. By seamlessly integrating voice biomarkers into routine patient monitoring, this strategy has the potential to improve patient outcomes, optimize resource allocation, and advance patient-centered HF management. In this study, a Machine Learning system, specifically a logistic regression model, is trained to predict patients' 5-year mortality rates using their speech as input. The model performs admirably and consistently, as demonstrated by cross-validation and statistical approaches (p-value < 0.001). Furthermore, integrating NT-proBNP, a diagnostic biomarker in HF, improves the model's predictive accuracy substantially.","sentences":["Addressing heart failure (HF) as a prevalent global health concern poses difficulties in implementing innovative approaches for enhanced patient care.","Predicting mortality rates in HF patients, in particular, is difficult yet critical, necessitating individualized care, proactive management, and enabling educated decision-making to enhance outcomes.","Recently, the significance of voice biomarkers coupled with Machine Learning (ML) has surged, demonstrating remarkable efficacy, particularly in predicting heart failure.","The synergy of voice analysis and ML algorithms provides a non-invasive and easily accessible means to evaluate patients' health.","However, there is a lack of voice biomarkers for predicting mortality rates among heart failure patients with standardized speech protocols.","Here, we demonstrate a powerful and effective ML model for predicting mortality rates in hospitalized HF patients through the utilization of voice biomarkers.","By seamlessly integrating voice biomarkers into routine patient monitoring, this strategy has the potential to improve patient outcomes, optimize resource allocation, and advance patient-centered HF management.","In this study, a Machine Learning system, specifically a logistic regression model, is trained to predict patients' 5-year mortality rates using their speech as input.","The model performs admirably and consistently, as demonstrated by cross-validation and statistical approaches (p-value < 0.001).","Furthermore, integrating NT-proBNP, a diagnostic biomarker in HF, improves the model's predictive accuracy substantially."],"url":"http://arxiv.org/abs/2402.13812v1","category":"cs.LG"}
{"created":"2024-02-21 13:24:14","title":"Revisiting Convergence of AdaGrad with Relaxed Assumptions","abstract":"In this study, we revisit the convergence of AdaGrad with momentum (covering AdaGrad as a special case) on non-convex smooth optimization problems. We consider a general noise model where the noise magnitude is controlled by the function value gap together with the gradient magnitude. This model encompasses a broad range of noises including bounded noise, sub-Gaussian noise, affine variance noise and the expected smoothness, and it has been shown to be more realistic in many practical applications. Our analysis yields a probabilistic convergence rate which, under the general noise, could reach at (\\tilde{\\mathcal{O}}(1/\\sqrt{T})). This rate does not rely on prior knowledge of problem-parameters and could accelerate to (\\tilde{\\mathcal{O}}(1/T)) where (T) denotes the total number iterations, when the noise parameters related to the function value gap and noise level are sufficiently small. The convergence rate thus matches the lower rate for stochastic first-order methods over non-convex smooth landscape up to logarithm terms [Arjevani et al., 2023]. We further derive a convergence bound for AdaGrad with mometum, considering the generalized smoothness where the local smoothness is controlled by a first-order function of the gradient norm.","sentences":["In this study, we revisit the convergence of AdaGrad with momentum (covering AdaGrad as a special case) on non-convex smooth optimization problems.","We consider a general noise model where the noise magnitude is controlled by the function value gap together with the gradient magnitude.","This model encompasses a broad range of noises including bounded noise, sub-Gaussian noise, affine variance noise and the expected smoothness, and it has been shown to be more realistic in many practical applications.","Our analysis yields a probabilistic convergence rate which, under the general noise, could reach at (\\tilde{\\mathcal{O}}(1/\\sqrt{T})).","This rate does not rely on prior knowledge of problem-parameters and could accelerate to (\\tilde{\\mathcal{O}}(1/T)) where (T) denotes the total number iterations, when the noise parameters related to the function value gap and noise level are sufficiently small.","The convergence rate thus matches the lower rate for stochastic first-order methods over non-convex smooth landscape up to logarithm terms [Arjevani et al., 2023].","We further derive a convergence bound for AdaGrad with mometum, considering the generalized smoothness where the local smoothness is controlled by a first-order function of the gradient norm."],"url":"http://arxiv.org/abs/2402.13794v1","category":"math.OC"}
{"created":"2024-02-21 13:00:44","title":"Preserving Near-Optimal Gradient Sparsification Cost for Scalable Distributed Deep Learning","abstract":"Communication overhead is a major obstacle to scaling distributed training systems. Gradient sparsification is a potential optimization approach to reduce the communication volume without significant loss of model fidelity. However, existing gradient sparsification methods have low scalability owing to inefficient design of their algorithms, which raises the communication overhead significantly. In particular, gradient build-up and inadequate sparsity control methods degrade the sparsification performance considerably. Moreover, communication traffic increases drastically owing to workload imbalance of gradient selection between workers.   To address these challenges, we propose a novel gradient sparsification scheme called ExDyna. In ExDyna, the gradient tensor of the model comprises fined-grained blocks, and contiguous blocks are grouped into non-overlapping partitions. Each worker selects gradients in its exclusively allocated partition so that gradient build-up never occurs. To balance the workload of gradient selection between workers, ExDyna adjusts the topology of partitions by comparing the workloads of adjacent partitions. In addition, ExDyna supports online threshold scaling, which estimates the accurate threshold of gradient selection on-the-fly. Accordingly, ExDyna can satisfy the user-required sparsity level during a training period regardless of models and datasets. Therefore, ExDyna can enhance the scalability of distributed training systems by preserving near-optimal gradient sparsification cost. In experiments, ExDyna outperformed state-of-the-art sparsifiers in terms of training speed and sparsification performance while achieving high accuracy.","sentences":["Communication overhead is a major obstacle to scaling distributed training systems.","Gradient sparsification is a potential optimization approach to reduce the communication volume without significant loss of model fidelity.","However, existing gradient sparsification methods have low scalability owing to inefficient design of their algorithms, which raises the communication overhead significantly.","In particular, gradient build-up and inadequate sparsity control methods degrade the sparsification performance considerably.","Moreover, communication traffic increases drastically owing to workload imbalance of gradient selection between workers.   ","To address these challenges, we propose a novel gradient sparsification scheme called ExDyna.","In ExDyna, the gradient tensor of the model comprises fined-grained blocks, and contiguous blocks are grouped into non-overlapping partitions.","Each worker selects gradients in its exclusively allocated partition so that gradient build-up never occurs.","To balance the workload of gradient selection between workers, ExDyna adjusts the topology of partitions by comparing the workloads of adjacent partitions.","In addition, ExDyna supports online threshold scaling, which estimates the accurate threshold of gradient selection on-the-fly.","Accordingly, ExDyna can satisfy the user-required sparsity level during a training period regardless of models and datasets.","Therefore, ExDyna can enhance the scalability of distributed training systems by preserving near-optimal gradient sparsification cost.","In experiments, ExDyna outperformed state-of-the-art sparsifiers in terms of training speed and sparsification performance while achieving high accuracy."],"url":"http://arxiv.org/abs/2402.13781v1","category":"cs.LG"}
{"created":"2024-02-21 12:59:50","title":"Efficient timing jitter simulation for passively mode-locked semiconductor lasers","abstract":"Efficient simulation of the timing jitter in passively mode-locking lasers is key to their numerical investigation and optimization. We introduce a method based on the pulse-period fluctuation auto-correlation function and compare it against established methods with respect to their estimate error. Potential improvements of the computational cost by about two orders of magnitude are reported. This advantage may facilitate larger parameter studies of passively mode-locked laser on small scale clusters or even desktop computers and thereby guide the target-oriented design of future lasers with ultra low timing jitter.","sentences":["Efficient simulation of the timing jitter in passively mode-locking lasers is key to their numerical investigation and optimization.","We introduce a method based on the pulse-period fluctuation auto-correlation function and compare it against established methods with respect to their estimate error.","Potential improvements of the computational cost by about two orders of magnitude are reported.","This advantage may facilitate larger parameter studies of passively mode-locked laser on small scale clusters or even desktop computers and thereby guide the target-oriented design of future lasers with ultra low timing jitter."],"url":"http://arxiv.org/abs/2402.13780v1","category":"physics.optics"}
{"created":"2024-02-21 12:54:40","title":"Cas-DiffCom: Cascaded diffusion model for infant longitudinal super-resolution 3D medical image completion","abstract":"Early infancy is a rapid and dynamic neurodevelopmental period for behavior and neurocognition. Longitudinal magnetic resonance imaging (MRI) is an effective tool to investigate such a crucial stage by capturing the developmental trajectories of the brain structures. However, longitudinal MRI acquisition always meets a serious data-missing problem due to participant dropout and failed scans, making longitudinal infant brain atlas construction and developmental trajectory delineation quite challenging. Thanks to the development of an AI-based generative model, neuroimage completion has become a powerful technique to retain as much available data as possible. However, current image completion methods usually suffer from inconsistency within each individual subject in the time dimension, compromising the overall quality. To solve this problem, our paper proposed a two-stage cascaded diffusion model, Cas-DiffCom, for dense and longitudinal 3D infant brain MRI completion and super-resolution. We applied our proposed method to the Baby Connectome Project (BCP) dataset. The experiment results validate that Cas-DiffCom achieves both individual consistency and high fidelity in longitudinal infant brain image completion. We further applied the generated infant brain images to two downstream tasks, brain tissue segmentation and developmental trajectory delineation, to declare its task-oriented potential in the neuroscience field.","sentences":["Early infancy is a rapid and dynamic neurodevelopmental period for behavior and neurocognition.","Longitudinal magnetic resonance imaging (MRI) is an effective tool to investigate such a crucial stage by capturing the developmental trajectories of the brain structures.","However, longitudinal MRI acquisition always meets a serious data-missing problem due to participant dropout and failed scans, making longitudinal infant brain atlas construction and developmental trajectory delineation quite challenging.","Thanks to the development of an AI-based generative model, neuroimage completion has become a powerful technique to retain as much available data as possible.","However, current image completion methods usually suffer from inconsistency within each individual subject in the time dimension, compromising the overall quality.","To solve this problem, our paper proposed a two-stage cascaded diffusion model, Cas-DiffCom, for dense and longitudinal 3D infant brain MRI completion and super-resolution.","We applied our proposed method to the Baby Connectome Project (BCP) dataset.","The experiment results validate that Cas-DiffCom achieves both individual consistency and high fidelity in longitudinal infant brain image completion.","We further applied the generated infant brain images to two downstream tasks, brain tissue segmentation and developmental trajectory delineation, to declare its task-oriented potential in the neuroscience field."],"url":"http://arxiv.org/abs/2402.13776v1","category":"eess.IV"}
{"created":"2024-02-21 12:30:33","title":"LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens","abstract":"Large context window is a desirable feature in large language models (LLMs). However, due to high fine-tuning costs, scarcity of long texts, and catastrophic values introduced by new token positions, current extended context windows are limited to around 128k tokens. This paper introduces LongRoPE that, for the first time, extends the context window of pre-trained LLMs to an impressive 2048k tokens, with up to only 1k fine-tuning steps at within 256k training lengths, while maintaining performance at the original short context window. This is achieved by three key innovations: (i) we identify and exploit two forms of non-uniformities in positional interpolation through an efficient search, providing a better initialization for fine-tuning and enabling an 8x extension in non-fine-tuning scenarios; (ii) we introduce a progressive extension strategy that first fine-tunes a 256k length LLM and then conducts a second positional interpolation on the fine-tuned extended LLM to achieve a 2048k context window; (iii) we readjust LongRoPE on 8k length to recover the short context window performance. Extensive experiments on LLaMA2 and Mistral across various tasks demonstrate the effectiveness of our method. Models extended via LongRoPE retain the original architecture with minor modifications to the positional embedding, and can reuse most pre-existing optimizations.","sentences":["Large context window is a desirable feature in large language models (LLMs).","However, due to high fine-tuning costs, scarcity of long texts, and catastrophic values introduced by new token positions, current extended context windows are limited to around 128k tokens.","This paper introduces LongRoPE that, for the first time, extends the context window of pre-trained LLMs to an impressive 2048k tokens, with up to only 1k fine-tuning steps at within 256k training lengths, while maintaining performance at the original short context window.","This is achieved by three key innovations: (i) we identify and exploit two forms of non-uniformities in positional interpolation through an efficient search, providing a better initialization for fine-tuning and enabling an 8x extension in non-fine-tuning scenarios; (ii) we introduce a progressive extension strategy that first fine-tunes a 256k length LLM and then conducts a second positional interpolation on the fine-tuned extended LLM to achieve a 2048k context window; (iii) we readjust LongRoPE on 8k length to recover the short context window performance.","Extensive experiments on LLaMA2 and Mistral across various tasks demonstrate the effectiveness of our method.","Models extended via LongRoPE retain the original architecture with minor modifications to the positional embedding, and can reuse most pre-existing optimizations."],"url":"http://arxiv.org/abs/2402.13753v1","category":"cs.CL"}
{"created":"2024-02-21 11:31:28","title":"Ouroboros: Speculative Decoding with Large Model Enhanced Drafting","abstract":"Drafting-then-verifying decoding methods such as speculative decoding are widely adopted training-free methods to accelerate the inference of large language models (LLMs). Instead of employing an autoregressive process to decode tokens sequentially, speculative decoding initially creates drafts with an efficient small model. Then LLMs are required to conduct verification and correction in a non-autoregressive fashion to minimize time overhead. Generating longer drafts can lead to even more significant speedups once verified, but also incurs substantial trial and error costs if it fails. Suffering from the high verification failure probability, existing decoding methods cannot draft too much content for verification at one time, achieving sub-optimal inference acceleration. In this paper, we introduce Ouroboros, which constructs a phrase candidate pool from the verification process of LLMs to provide candidates for draft generation of the small model. Thereby, Ouroboros can further improve the efficiency and effectiveness of the initial drafts. The experimental results on typical text generation tasks show that Ouroboros achieves speedups of up to 1.9x and 2.8x compared to lookahead decoding and speculative decoding, respectively. The source code of Ouroboros is available at https://github.com/thunlp/Ouroboros.","sentences":["Drafting-then-verifying decoding methods such as speculative decoding are widely adopted training-free methods to accelerate the inference of large language models (LLMs).","Instead of employing an autoregressive process to decode tokens sequentially, speculative decoding initially creates drafts with an efficient small model.","Then LLMs are required to conduct verification and correction in a non-autoregressive fashion to minimize time overhead.","Generating longer drafts can lead to even more significant speedups once verified, but also incurs substantial trial and error costs if it fails.","Suffering from the high verification failure probability, existing decoding methods cannot draft too much content for verification at one time, achieving sub-optimal inference acceleration.","In this paper, we introduce Ouroboros, which constructs a phrase candidate pool from the verification process of LLMs to provide candidates for draft generation of the small model.","Thereby, Ouroboros can further improve the efficiency and effectiveness of the initial drafts.","The experimental results on typical text generation tasks show that Ouroboros achieves speedups of up to 1.9x and 2.8x compared to lookahead decoding and speculative decoding, respectively.","The source code of Ouroboros is available at https://github.com/thunlp/Ouroboros."],"url":"http://arxiv.org/abs/2402.13720v1","category":"cs.CL"}
{"created":"2024-02-21 11:29:32","title":"Edge-Disjoint Paths in Eulerian Digraphs","abstract":"Disjoint paths problems are among the most prominent problems in combinatorial optimization. The edge- as well as vertex-disjoint paths problem, are NP-complete on directed and undirected graphs. But on undirected graphs, Robertson and Seymour (Graph Minors XIII) developed an algorithm for the vertex- and the edge-disjoint paths problem that runs in cubic time for every fixed number $p$ of terminal pairs, i.e. they proved that the problem is fixed-parameter tractable on undirected graphs. On directed graphs, Fortune, Hopcroft, and Wyllie proved that both problems are NP-complete already for $p=2$ terminal pairs. In this paper, we study the edge-disjoint paths problem (EDPP) on Eulerian digraphs, a problem that has received significant attention in the literature. Marx (Marx 2004) proved that the Eulerian EDPP is NP-complete even on structurally very simple Eulerian digraphs. On the positive side, polynomial time algorithms are known only for very restricted cases, such as $p\\leq 3$ or where the demand graph is a union of two stars (see e.g. Ibaraki, Poljak 1991; Frank 1988; Frank, Ibaraki, Nagamochi 1995).   The question of which values of $p$ the edge-disjoint paths problem can be solved in polynomial time on Eulerian digraphs has already been raised by Frank, Ibaraki, and Nagamochi (1995) almost 30 years ago. But despite considerable effort, the complexity of the problem is still wide open and is considered to be the main open problem in this area (see Chapter 4 of Bang-Jensen, Gutin 2018 for a recent survey). In this paper, we solve this long-open problem by showing that the Edge-Disjoint Paths Problem is fixed-parameter tractable on Eulerian digraphs in general (parameterized by the number of terminal pairs). The algorithm itself is reasonably simple but the proof of its correctness requires a deep structural analysis of Eulerian digraphs.","sentences":["Disjoint paths problems are among the most prominent problems in combinatorial optimization.","The edge- as well as vertex-disjoint paths problem, are NP-complete on directed and undirected graphs.","But on undirected graphs, Robertson and Seymour (Graph Minors XIII) developed an algorithm for the vertex- and the edge-disjoint paths problem that runs in cubic time for every fixed number $p$ of terminal pairs, i.e. they proved that the problem is fixed-parameter tractable on undirected graphs.","On directed graphs, Fortune, Hopcroft, and Wyllie proved that both problems are NP-complete already for $p=2$ terminal pairs.","In this paper, we study the edge-disjoint paths problem (EDPP) on Eulerian digraphs, a problem that has received significant attention in the literature.","Marx (Marx 2004) proved that the Eulerian EDPP is NP-complete even on structurally very simple Eulerian digraphs.","On the positive side, polynomial time algorithms are known only for very restricted cases, such as $p\\leq 3$ or where the demand graph is a union of two stars (see e.g. Ibaraki, Poljak 1991; Frank 1988; Frank, Ibaraki, Nagamochi 1995).   ","The question of which values of $p$ the edge-disjoint paths problem can be solved in polynomial time on Eulerian digraphs has already been raised by Frank, Ibaraki, and Nagamochi (1995) almost 30 years ago.","But despite considerable effort, the complexity of the problem is still wide open and is considered to be the main open problem in this area (see Chapter 4 of Bang-Jensen, Gutin 2018 for a recent survey).","In this paper, we solve this long-open problem by showing that the Edge-Disjoint Paths Problem is fixed-parameter tractable on Eulerian digraphs in general (parameterized by the number of terminal pairs).","The algorithm itself is reasonably simple but the proof of its correctness requires a deep structural analysis of Eulerian digraphs."],"url":"http://arxiv.org/abs/2402.13716v1","category":"cs.CC"}
{"created":"2024-02-21 11:12:52","title":"Linear-Quadratic optimal control for boundary controlled networks of waves","abstract":"Linear-Quadratic optimal controls are computed for a class of boundary controlled, boundary observed hyperbolic infinite-dimensional systems, which may be viewed as networks of waves. The main results of this manuscript consist in converting the infinite-dimensional continuous-time systems into infinite-dimensional discrete-time systems for which the operators dynamics are matrices, in solving the LQ-optimal control problem in discrete-time and then in interpreting the solution in the continuous-time variables, giving rise to the optimal boundary control input. The results are applied to two examples, a small network of three vibrating strings and a co-current heat-exchanger, for which boundary sensors and actuators are considered.","sentences":["Linear-Quadratic optimal controls are computed for a class of boundary controlled, boundary observed hyperbolic infinite-dimensional systems, which may be viewed as networks of waves.","The main results of this manuscript consist in converting the infinite-dimensional continuous-time systems into infinite-dimensional discrete-time systems for which the operators dynamics are matrices, in solving the LQ-optimal control problem in discrete-time and then in interpreting the solution in the continuous-time variables, giving rise to the optimal boundary control input.","The results are applied to two examples, a small network of three vibrating strings and a co-current heat-exchanger, for which boundary sensors and actuators are considered."],"url":"http://arxiv.org/abs/2402.13706v1","category":"math.OC"}
{"created":"2024-02-21 11:10:45","title":"Hyperuniformity and optimal transport of point processes","abstract":"We examine optimal matchings or transport between two stationary point processes and in particular, from a point process to the (integer) lattice or the Lebesgue measure respectively. The main focus of the article is the implication of hyperuniformity (reduced variance fluctuations in point processes) to optimal transport: in dimension $2$, we show that the typical matching cost has finite second moment under a mild logarithmic integrability condition on the reduced pair correlation measure, showing that most planar hyperuniform point processes are $ L^2$-perturbed lattices. Our method does not formally require assumptions on the correlation measure or the variance behaviour and it retrieves known sharp bounds for neutral integrable systems such as Poisson processes, and also applies to hyperfluctuating systems. The proof relies on the estimation of the optimal transport cost between point processes restricted to large windows for a well-chosen cost through their Fourier-Stieljes transforms, related to their structure factor. The existence of an infinite matching is obtained through a compactness argument on the space of random stationary measures.","sentences":["We examine optimal matchings or transport between two stationary point processes and in particular, from a point process to the (integer) lattice or the Lebesgue measure respectively.","The main focus of the article is the implication of hyperuniformity (reduced variance fluctuations in point processes) to optimal transport: in dimension $2$, we show that the typical matching cost has finite second moment under a mild logarithmic integrability condition on the reduced pair correlation measure, showing that most planar hyperuniform point processes are $ L^2$-perturbed lattices.","Our method does not formally require assumptions on the correlation measure or the variance behaviour and it retrieves known sharp bounds for neutral integrable systems such as Poisson processes, and also applies to hyperfluctuating systems.","The proof relies on the estimation of the optimal transport cost between point processes restricted to large windows for a well-chosen cost through their Fourier-Stieljes transforms, related to their structure factor.","The existence of an infinite matching is obtained through a compactness argument on the space of random stationary measures."],"url":"http://arxiv.org/abs/2402.13705v1","category":"math.PR"}
{"created":"2024-02-21 11:05:14","title":"Robustness analysis and station-keeping control of an interferometer formation flying mission in low Earth orbit","abstract":"The impact of formation flying on interferometry is growing over the years for the potential performance it could offer. However, it is still an open field, and many studies are still required. This article presents the basic principles behind interferometry focusing first on a single array and secondly on a formation of satellites. A sensitivity analysis is carried out to evaluate how the performance of the interferometry is affected by an error in the relative position in the formation geometry. This is estimated by computing the loss of the performance in terms of percentage deviation due to a non-nominal relative trajectory, including two-dimensional errors and defining a payload index. The main goal of this study is to estimate whether some errors in the relative state are more impacting than others. The final objective is to compute the link between a position error and a specific loss of performance, to foresee the origin of the the error. Furthermore, a dynamical model is developed to describe the relative motion in the Low Earth Orbit environment, considering both the unperturbed and the J2 and drag contributions. A Proportional, Integral and Derivative controller is implemented for the position control of a multiple satellite formation flying, considering a low thrust control profile. The Formation Flying L-band Aperture Synthesis study is taken as the case scenario, analysing both nominal and non-nominal configurations. This study serves as a starting point for the development of a combined tool to assess the performance of the interferometry and the control on the relative state for future remote sensing studies involving relative motion.","sentences":["The impact of formation flying on interferometry is growing over the years for the potential performance it could offer.","However, it is still an open field, and many studies are still required.","This article presents the basic principles behind interferometry focusing first on a single array and secondly on a formation of satellites.","A sensitivity analysis is carried out to evaluate how the performance of the interferometry is affected by an error in the relative position in the formation geometry.","This is estimated by computing the loss of the performance in terms of percentage deviation due to a non-nominal relative trajectory, including two-dimensional errors and defining a payload index.","The main goal of this study is to estimate whether some errors in the relative state are more impacting than others.","The final objective is to compute the link between a position error and a specific loss of performance, to foresee the origin of the the error.","Furthermore, a dynamical model is developed to describe the relative motion in the Low Earth Orbit environment, considering both the unperturbed and the J2 and drag contributions.","A Proportional, Integral and Derivative controller is implemented for the position control of a multiple satellite formation flying, considering a low thrust control profile.","The Formation Flying L-band Aperture Synthesis study is taken as the case scenario, analysing both nominal and non-nominal configurations.","This study serves as a starting point for the development of a combined tool to assess the performance of the interferometry and the control on the relative state for future remote sensing studies involving relative motion."],"url":"http://arxiv.org/abs/2402.13702v1","category":"astro-ph.EP"}
{"created":"2024-02-21 09:59:20","title":"GCOF: Self-iterative Text Generation for Copywriting Using Large Language Model","abstract":"Large language models(LLM) such as ChatGPT have substantially simplified the generation of marketing copy, yet producing content satisfying domain specific requirements, such as effectively engaging customers, remains a significant challenge. In this work, we introduce the Genetic Copy Optimization Framework (GCOF) designed to enhance both efficiency and engagememnt of marketing copy creation. We conduct explicit feature engineering within the prompts of LLM. Additionally, we modify the crossover operator in Genetic Algorithm (GA), integrating it into the GCOF to enable automatic feature engineering. This integration facilitates a self-iterative refinement of the marketing copy. Compared to human curated copy, Online results indicate that copy produced by our framework achieves an average increase in click-through rate (CTR) of over $50\\%$.","sentences":["Large language models(LLM) such as ChatGPT have substantially simplified the generation of marketing copy, yet producing content satisfying domain specific requirements, such as effectively engaging customers, remains a significant challenge.","In this work, we introduce the Genetic Copy Optimization Framework (GCOF) designed to enhance both efficiency and engagememnt of marketing copy creation.","We conduct explicit feature engineering within the prompts of LLM.","Additionally, we modify the crossover operator in Genetic Algorithm (GA), integrating it into the GCOF to enable automatic feature engineering.","This integration facilitates a self-iterative refinement of the marketing copy.","Compared to human curated copy, Online results indicate that copy produced by our framework achieves an average increase in click-through rate (CTR) of over $50\\%$."],"url":"http://arxiv.org/abs/2402.13667v1","category":"cs.CL"}
{"created":"2024-02-21 09:40:26","title":"Improving a Proportional Integral Controller with Reinforcement Learning on a Throttle Valve Benchmark","abstract":"This paper presents a learning-based control strategy for non-linear throttle valves with an asymmetric hysteresis, leading to a near-optimal controller without requiring any prior knowledge about the environment. We start with a carefully tuned Proportional Integrator (PI) controller and exploit the recent advances in Reinforcement Learning (RL) with Guides to improve the closed-loop behavior by learning from the additional interactions with the valve. We test the proposed control method in various scenarios on three different valves, all highlighting the benefits of combining both PI and RL frameworks to improve control performance in non-linear stochastic systems. In all the experimental test cases, the resulting agent has a better sample efficiency than traditional RL agents and outperforms the PI controller.","sentences":["This paper presents a learning-based control strategy for non-linear throttle valves with an asymmetric hysteresis, leading to a near-optimal controller without requiring any prior knowledge about the environment.","We start with a carefully tuned Proportional Integrator (PI) controller and exploit the recent advances in Reinforcement Learning (RL) with Guides to improve the closed-loop behavior by learning from the additional interactions with the valve.","We test the proposed control method in various scenarios on three different valves, all highlighting the benefits of combining both PI and RL frameworks to improve control performance in non-linear stochastic systems.","In all the experimental test cases, the resulting agent has a better sample efficiency than traditional RL agents and outperforms the PI controller."],"url":"http://arxiv.org/abs/2402.13654v1","category":"eess.SY"}
{"created":"2024-02-21 09:34:45","title":"Learning control strategy in soft robotics through a set of configuration spaces","abstract":"The ability of a soft robot to perform specific tasks is determined by its contact configuration, and transitioning between configurations is often necessary to reach a desired position or manipulate an object. Based on this observation, we propose a method for controlling soft robots that involves defining a graph of configuration spaces. Different agents, whether learned or not (convex optimization, expert trajectory, and collision detection), use the structure of the graph to solve the desired task. The graph and the agents are part of the prior knowledge that is intuitively integrated into the learning process. They are used to combine different optimization methods, improve sample efficiency, and provide interpretability. We construct the graph based on the contact configurations and demonstrate its effectiveness through two scenarios, a deformable beam in contact with its environment and a soft manipulator, where it outperforms the baseline in terms of stability, learning speed, and interpretability.","sentences":["The ability of a soft robot to perform specific tasks is determined by its contact configuration, and transitioning between configurations is often necessary to reach a desired position or manipulate an object.","Based on this observation, we propose a method for controlling soft robots that involves defining a graph of configuration spaces.","Different agents, whether learned or not (convex optimization, expert trajectory, and collision detection), use the structure of the graph to solve the desired task.","The graph and the agents are part of the prior knowledge that is intuitively integrated into the learning process.","They are used to combine different optimization methods, improve sample efficiency, and provide interpretability.","We construct the graph based on the contact configurations and demonstrate its effectiveness through two scenarios, a deformable beam in contact with its environment and a soft manipulator, where it outperforms the baseline in terms of stability, learning speed, and interpretability."],"url":"http://arxiv.org/abs/2402.13649v1","category":"cs.RO"}
{"created":"2024-02-21 09:02:39","title":"Algorithms for Claims Trading","abstract":"The recent banking crisis has again emphasized the importance of understanding and mitigating systemic risk in financial networks. In this paper, we study a market-driven approach to rescue a bank in distress based on the idea of claims trading, a notion defined in Chapter 11 of the U.S. Bankruptcy Code. We formalize the idea in the context of financial networks by Eisenberg and Noe. For two given banks v and w, we consider the operation that w takes over some claims of v and in return gives liquidity to v to ultimately rescue v. We study the structural properties and computational complexity of decision and optimization problems for several variants of claims trading.   When trading incoming edges of v, we show that there is no trade in which both banks v and w strictly improve their assets. We therefore consider creditor-positive trades, in which v profits strictly and w remains indifferent. For a given set C of incoming edges of v, we provide an efficient algorithm to compute payments by w that result in maximal assets of v. When the set C must also be chosen, the problem becomes weakly NP-hard. Our main result here is a bicriteria FPTAS to compute an approximate trade. The approximate trade results in nearly the optimal amount of assets of v in any exact trade. Our results extend to the case in which banks use general monotone payment functions and the emerging clearing state can be computed efficiently.   In contrast, for trading outgoing edges of v, the goal is to maximize the increase in assets for the creditors of v. Notably, for these results the characteristics of the payment functions of the banks are essential. For payments ranking creditors one by one, we show NP-hardness of approximation within a factor polynomial in the network size, when the set of claims C is part of the input or not. Instead, for proportional payments, our results indicate more favorable conditions.","sentences":["The recent banking crisis has again emphasized the importance of understanding and mitigating systemic risk in financial networks.","In this paper, we study a market-driven approach to rescue a bank in distress based on the idea of claims trading, a notion defined in Chapter 11 of the U.S. Bankruptcy Code.","We formalize the idea in the context of financial networks by Eisenberg and Noe.","For two given banks v and w, we consider the operation that w takes over some claims of v and in return gives liquidity to v to ultimately rescue v. We study the structural properties and computational complexity of decision and optimization problems for several variants of claims trading.   ","When trading incoming edges of v, we show that there is no trade in which both banks v and w strictly improve their assets.","We therefore consider creditor-positive trades, in which v profits strictly and w remains indifferent.","For a given set C of incoming edges of v, we provide an efficient algorithm to compute payments by w that result in maximal assets of v. When the set C must also be chosen, the problem becomes weakly NP-hard.","Our main result here is a bicriteria FPTAS to compute an approximate trade.","The approximate trade results in nearly the optimal amount of assets of v in any exact trade.","Our results extend to the case in which banks use general monotone payment functions and the emerging clearing state can be computed efficiently.   ","In contrast, for trading outgoing edges of v, the goal is to maximize the increase in assets for the creditors of v. Notably, for these results the characteristics of the payment functions of the banks are essential.","For payments ranking creditors one by one, we show NP-hardness of approximation within a factor polynomial in the network size, when the set of claims C is part of the input or not.","Instead, for proportional payments, our results indicate more favorable conditions."],"url":"http://arxiv.org/abs/2402.13627v1","category":"cs.GT"}
{"created":"2024-02-21 08:55:35","title":"Higher-order singular perturbation models for phase transitions","abstract":"Variational models of phase transitions take into account double-well energies singularly perturbed by gradient terms, such as the Cahn-Hilliard free energy. The derivation by $\\Gamma$-convergence of a sharp-interface limit for such energy is a classical result by Modica and Mortola. We consider a singular perturbation of a double-well energy by derivatives of order $k$, and show that we still can describe the limit as in the case $k=1$ with a suitable interfacial energy density, in accord with the case $k=1$ and with the case $k=2$ previously analyzed by Fonseca and Mantegazza. The main isssue is the derivation of an optimal-profile problem on the real line describing the interfacial energy density, which must be conveniently approximated by minimum problems on finite intervals with homogeneous condition on the derivatives at the endpoints up to order $k-1$. To that end a careful study must be carried on of sets where sequences of functions with equibounded energy are ``close to the wells'' and have ``small derivatives'', in terms of interpolation inequalities and energy estimates.","sentences":["Variational models of phase transitions take into account double-well energies singularly perturbed by gradient terms, such as the Cahn-Hilliard free energy.","The derivation by $\\Gamma$-convergence of a sharp-interface limit for such energy is a classical result by Modica and Mortola.","We consider a singular perturbation of a double-well energy by derivatives of order $k$, and show that we still can describe the limit as in the case $k=1$ with a suitable interfacial energy density, in accord with the case $k=1$ and with the case $k=2$ previously analyzed by Fonseca and Mantegazza.","The main isssue is the derivation of an optimal-profile problem on the real line describing the interfacial energy density, which must be conveniently approximated by minimum problems on finite intervals with homogeneous condition on the derivatives at the endpoints up to order $k-1$. To that end a careful study must be carried on of sets where sequences of functions with equibounded energy are ``close to the wells'' and have ``small derivatives'', in terms of interpolation inequalities and energy estimates."],"url":"http://arxiv.org/abs/2402.13626v1","category":"math.AP"}
{"created":"2024-02-21 08:50:33","title":"Analysis of Bootstrap and Subsampling in High-dimensional Regularized Regression","abstract":"We investigate popular resampling methods for estimating the uncertainty of statistical models, such as subsampling, bootstrap and the jackknife, and their performance in high-dimensional supervised regression tasks. We provide a tight asymptotic description of the biases and variances estimated by these methods in the context of generalized linear models, such as ridge and logistic regression, taking the limit where the number of samples $n$ and dimension $d$ of the covariates grow at a comparable fixed rate $\\alpha\\!=\\! n/d$. Our findings are three-fold: i) resampling methods are fraught with problems in high dimensions and exhibit the double-descent-like behavior typical of these situations; ii) only when $\\alpha$ is large enough do they provide consistent and reliable error estimations (we give convergence rates); iii) in the over-parametrized regime $\\alpha\\!<\\!1$ relevant to modern machine learning practice, their predictions are not consistent, even with optimal regularization.","sentences":["We investigate popular resampling methods for estimating the uncertainty of statistical models, such as subsampling, bootstrap and the jackknife, and their performance in high-dimensional supervised regression tasks.","We provide a tight asymptotic description of the biases and variances estimated by these methods in the context of generalized linear models, such as ridge and logistic regression, taking the limit where the number of samples $n$ and dimension $d$ of the covariates grow at a comparable fixed rate $\\alpha\\!=\\! n/d$. Our findings are three-fold: i) resampling methods are fraught with problems in high dimensions and exhibit the double-descent-like behavior typical of these situations; ii) only when $\\alpha$ is large enough do they provide consistent and reliable error estimations (we give convergence rates); iii) in the over-parametrized regime $\\alpha\\!<\\!1$ relevant to modern machine learning practice, their predictions are not consistent, even with optimal regularization."],"url":"http://arxiv.org/abs/2402.13622v1","category":"stat.ML"}
{"created":"2024-02-21 08:22:46","title":"VOOM: Robust Visual Object Odometry and Mapping using Hierarchical Landmarks","abstract":"In recent years, object-oriented simultaneous localization and mapping (SLAM) has attracted increasing attention due to its ability to provide high-level semantic information while maintaining computational efficiency. Some researchers have attempted to enhance localization accuracy by integrating the modeled object residuals into bundle adjustment. However, few have demonstrated better results than feature-based visual SLAM systems, as the generic coarse object models, such as cuboids or ellipsoids, are less accurate than feature points. In this paper, we propose a Visual Object Odometry and Mapping framework VOOM using high-level objects and low-level points as the hierarchical landmarks in a coarse-to-fine manner instead of directly using object residuals in bundle adjustment. Firstly, we introduce an improved observation model and a novel data association method for dual quadrics, employed to represent physical objects. It facilitates the creation of a 3D map that closely reflects reality. Next, we use object information to enhance the data association of feature points and consequently update the map. In the visual object odometry backend, the updated map is employed to further optimize the camera pose and the objects. Meanwhile, local bundle adjustment is performed utilizing the objects and points-based covisibility graphs in our visual object mapping process. Experiments show that VOOM outperforms both object-oriented SLAM and feature points SLAM systems such as ORB-SLAM2 in terms of localization. The implementation of our method is available at https://github.com/yutongwangBIT/VOOM.git.","sentences":["In recent years, object-oriented simultaneous localization and mapping (SLAM) has attracted increasing attention due to its ability to provide high-level semantic information while maintaining computational efficiency.","Some researchers have attempted to enhance localization accuracy by integrating the modeled object residuals into bundle adjustment.","However, few have demonstrated better results than feature-based visual SLAM systems, as the generic coarse object models, such as cuboids or ellipsoids, are less accurate than feature points.","In this paper, we propose a Visual Object Odometry and Mapping framework VOOM using high-level objects and low-level points as the hierarchical landmarks in a coarse-to-fine manner instead of directly using object residuals in bundle adjustment.","Firstly, we introduce an improved observation model and a novel data association method for dual quadrics, employed to represent physical objects.","It facilitates the creation of a 3D map that closely reflects reality.","Next, we use object information to enhance the data association of feature points and consequently update the map.","In the visual object odometry backend, the updated map is employed to further optimize the camera pose and the objects.","Meanwhile, local bundle adjustment is performed utilizing the objects and points-based covisibility graphs in our visual object mapping process.","Experiments show that VOOM outperforms both object-oriented SLAM and feature points SLAM systems such as ORB-SLAM2 in terms of localization.","The implementation of our method is available at https://github.com/yutongwangBIT/VOOM.git."],"url":"http://arxiv.org/abs/2402.13609v1","category":"cs.RO"}
{"created":"2024-02-21 08:21:48","title":"Convergence Acceleration of Markov Chain Monte Carlo-based Gradient Descent by Deep Unfolding","abstract":"This study proposes a trainable sampling-based solver for combinatorial optimization problems (COPs) using a deep-learning technique called deep unfolding. The proposed solver is based on the Ohzeki method that combines Markov-chain Monte-Carlo (MCMC) and gradient descent, and its step sizes are trained by minimizing a loss function. In the training process, we propose a sampling-based gradient estimation that substitutes auto-differentiation with a variance estimation, thereby circumventing the failure of back propagation due to the non-differentiability of MCMC. The numerical results for a few COPs demonstrated that the proposed solver significantly accelerated the convergence speed compared with the original Ohzeki method.","sentences":["This study proposes a trainable sampling-based solver for combinatorial optimization problems (COPs) using a deep-learning technique called deep unfolding.","The proposed solver is based on the Ohzeki method that combines Markov-chain Monte-Carlo (MCMC) and gradient descent, and its step sizes are trained by minimizing a loss function.","In the training process, we propose a sampling-based gradient estimation that substitutes auto-differentiation with a variance estimation, thereby circumventing the failure of back propagation due to the non-differentiability of MCMC.","The numerical results for a few COPs demonstrated that the proposed solver significantly accelerated the convergence speed compared with the original Ohzeki method."],"url":"http://arxiv.org/abs/2402.13608v1","category":"cond-mat.dis-nn"}
{"created":"2024-02-21 07:59:44","title":"Near-Field Multiuser Beam-Training for Extremely Large-Scale MIMO Systems","abstract":"Extremely large-scale multiple-input multiple-output (XL-MIMO) systems are capable of improving spectral efficiency by employing far more antennas than conventional massive MIMO at the base station (BS). However, beam training in multiuser XL-MIMO systems is challenging. To tackle these issues, we conceive a three-phase graph neural network (GNN)-based beam training scheme for multiuser XL-MIMO systems. In the first phase, only far-field wide beams have to be tested for each user and the GNN is utilized to map the beamforming gain information of the far-field wide beams to the optimal near-field beam for each user. In addition, the proposed GNN-based scheme can exploit the position-correlation between adjacent users for further improvement of the accuracy of beam training. In the second phase, a beam allocation scheme based on the probability vectors produced at the outputs of GNNs is proposed to address the above beam-direction conflicts between users. In the third phase, the hybrid TBF is designed for further reducing the inter-user interference. Our simulation results show that the proposed scheme improves the beam training performance of the benchmarks. Moreover, the performance of the proposed beam training scheme approaches that of an exhaustive search, despite requiring only about 7% of the pilot overhead.","sentences":["Extremely large-scale multiple-input multiple-output (XL-MIMO) systems are capable of improving spectral efficiency by employing far more antennas than conventional massive MIMO at the base station (BS).","However, beam training in multiuser XL-MIMO systems is challenging.","To tackle these issues, we conceive a three-phase graph neural network (GNN)-based beam training scheme for multiuser XL-MIMO systems.","In the first phase, only far-field wide beams have to be tested for each user and the GNN is utilized to map the beamforming gain information of the far-field wide beams to the optimal near-field beam for each user.","In addition, the proposed GNN-based scheme can exploit the position-correlation between adjacent users for further improvement of the accuracy of beam training.","In the second phase, a beam allocation scheme based on the probability vectors produced at the outputs of GNNs is proposed to address the above beam-direction conflicts between users.","In the third phase, the hybrid TBF is designed for further reducing the inter-user interference.","Our simulation results show that the proposed scheme improves the beam training performance of the benchmarks.","Moreover, the performance of the proposed beam training scheme approaches that of an exhaustive search, despite requiring only about 7% of the pilot overhead."],"url":"http://arxiv.org/abs/2402.13597v1","category":"cs.IT"}
{"created":"2024-02-21 07:55:33","title":"A cutting plane algorithm for globally solving low dimensional k-means clustering problems","abstract":"Clustering is one of the most fundamental tools in data science and machine learning, and k-means clustering is one of the most common such methods. There is a variety of approximate algorithms for the k-means problem, but computing the globally optimal solution is in general NP-hard. In this paper we consider the k-means problem for instances with low dimensional data and formulate it as a structured concave assignment problem. This allows us to exploit the low dimensional structure and solve the problem to global optimality within reasonable time for large data sets with several clusters. The method builds on iteratively solving a small concave problem and a large linear programming problem. This gives a sequence of feasible solutions along with bounds which we show converges to zero optimality gap. The paper combines methods from global optimization theory to accelerate the procedure, and we provide numerical results on their performance.","sentences":["Clustering is one of the most fundamental tools in data science and machine learning, and k-means clustering is one of the most common such methods.","There is a variety of approximate algorithms for the k-means problem, but computing the globally optimal solution is in general NP-hard.","In this paper we consider the k-means problem for instances with low dimensional data and formulate it as a structured concave assignment problem.","This allows us to exploit the low dimensional structure and solve the problem to global optimality within reasonable time for large data sets with several clusters.","The method builds on iteratively solving a small concave problem and a large linear programming problem.","This gives a sequence of feasible solutions along with bounds which we show converges to zero optimality gap.","The paper combines methods from global optimization theory to accelerate the procedure, and we provide numerical results on their performance."],"url":"http://arxiv.org/abs/2402.13595v1","category":"math.OC"}
{"created":"2024-02-21 07:55:31","title":"Two-dimensional Closed-Form Analytical Model of Laterally-Excited Film Bulk Acoustic Wave Resonator Multiferroic Antennas","abstract":"To overcome the physical limitations of electrically small antennas, strain-mediated magnetoelectric antennas have been studied experimentally and theoretically. However, current closed-form analytical models include solely one-dimensional approaches. This paper proposes a two-dimensional closed-form analytical model of a laterally-excited multiferroic antenna. To model thickness-shear vibrations, we propose an extended Mason model to two dimensions. This theory solves unidirectionally-coupled elastodynamics-magnetization dynamics-electrodynamics. Without using the infinitesimal dipole approximation, this model provides a closed-form solution of the radiated electromagnetic field with an explicit dependency of the antenna response on the physical parameters, enabling their optimization. Furthermore, we compare the efficiency-bandwidth product of different multiferroic antennas with Chu's limit in the Akhiezer regime, which delimits the highest achievable mechanical quality factor for acoustic resonators at GHz frequencies. We show that phonon-phonon coupling of Akhiezer damping is most likely to limit multiferroic antenna radiation, not Chu's limit. Moreover, judicious choice of materials can enhance the maximum efficiency by more than an order of magnitude.","sentences":["To overcome the physical limitations of electrically small antennas, strain-mediated magnetoelectric antennas have been studied experimentally and theoretically.","However, current closed-form analytical models include solely one-dimensional approaches.","This paper proposes a two-dimensional closed-form analytical model of a laterally-excited multiferroic antenna.","To model thickness-shear vibrations, we propose an extended Mason model to two dimensions.","This theory solves unidirectionally-coupled elastodynamics-magnetization dynamics-electrodynamics.","Without using the infinitesimal dipole approximation, this model provides a closed-form solution of the radiated electromagnetic field with an explicit dependency of the antenna response on the physical parameters, enabling their optimization.","Furthermore, we compare the efficiency-bandwidth product of different multiferroic antennas with Chu's limit in the Akhiezer regime, which delimits the highest achievable mechanical quality factor for acoustic resonators at GHz frequencies.","We show that phonon-phonon coupling of Akhiezer damping is most likely to limit multiferroic antenna radiation, not Chu's limit.","Moreover, judicious choice of materials can enhance the maximum efficiency by more than an order of magnitude."],"url":"http://arxiv.org/abs/2402.13594v1","category":"physics.app-ph"}
{"created":"2024-02-21 07:39:04","title":"PI-CoF: A Bilevel Optimization Framework for Solving Active Learning Problems using Physics-Information","abstract":"Physics informed neural networks (PINNs) have recently been proposed as surrogate models for solving process optimization problems. However, in an active learning setting collecting enough data for reliably training PINNs poses a challenge. This study proposes a broadly applicable method for incorporating physics information into existing machine learning (ML) models of any type. The proposed method - referred to as PI-CoF for Physics-Informed Correction Factors - introduces additive or multiplicative correction factors for pointwise inference, which are identified by solving a regularized unconstrained optimization problem for reconciliation of physics information and ML model predictions. When ML models are used in an optimization context, using the proposed approach translates into a bilevel optimization problem, where the reconciliation problem is solved as an inner problem each time before evaluating the objective and constraint functions of the outer problem. The utility of the proposed approach is demonstrated through a numerical example, emphasizing constraint satisfaction in a safe Bayesian optimization (BO) setting. Furthermore, a simulation study is carried out by using PI-CoF for the real-time optimization of a fuel cell system. The results show reduced fuel consumption and better reference tracking performance when using the proposed PI-CoF approach in comparison to a constrained BO algorithm not using physics information.","sentences":["Physics informed neural networks (PINNs) have recently been proposed as surrogate models for solving process optimization problems.","However, in an active learning setting collecting enough data for reliably training PINNs poses a challenge.","This study proposes a broadly applicable method for incorporating physics information into existing machine learning (ML) models of any type.","The proposed method - referred to as PI-CoF for Physics-Informed Correction Factors - introduces additive or multiplicative correction factors for pointwise inference, which are identified by solving a regularized unconstrained optimization problem for reconciliation of physics information and ML model predictions.","When ML models are used in an optimization context, using the proposed approach translates into a bilevel optimization problem, where the reconciliation problem is solved as an inner problem each time before evaluating the objective and constraint functions of the outer problem.","The utility of the proposed approach is demonstrated through a numerical example, emphasizing constraint satisfaction in a safe Bayesian optimization (BO) setting.","Furthermore, a simulation study is carried out by using PI-CoF for the real-time optimization of a fuel cell system.","The results show reduced fuel consumption and better reference tracking performance when using the proposed PI-CoF approach in comparison to a constrained BO algorithm not using physics information."],"url":"http://arxiv.org/abs/2402.13588v1","category":"eess.SY"}
{"created":"2024-02-21 07:38:29","title":"A Multimodal In-Context Tuning Approach for E-Commerce Product Description Generation","abstract":"In this paper, we propose a new setting for generating product descriptions from images, augmented by marketing keywords. It leverages the combined power of visual and textual information to create descriptions that are more tailored to the unique features of products. For this setting, previous methods utilize visual and textual encoders to encode the image and keywords and employ a language model-based decoder to generate the product description. However, the generated description is often inaccurate and generic since same-category products have similar copy-writings, and optimizing the overall framework on large-scale samples makes models concentrate on common words yet ignore the product features. To alleviate the issue, we present a simple and effective Multimodal In-Context Tuning approach, named ModICT, which introduces a similar product sample as the reference and utilizes the in-context learning capability of language models to produce the description. During training, we keep the visual encoder and language model frozen, focusing on optimizing the modules responsible for creating multimodal in-context references and dynamic prompts. This approach preserves the language generation prowess of large language models (LLMs), facilitating a substantial increase in description diversity. To assess the effectiveness of ModICT across various language model scales and types, we collect data from three distinct product categories within the E-commerce domain. Extensive experiments demonstrate that ModICT significantly improves the accuracy (by up to 3.3% on Rouge-L) and diversity (by up to 9.4% on D-5) of generated results compared to conventional methods. Our findings underscore the potential of ModICT as a valuable tool for enhancing automatic generation of product descriptions in a wide range of applications.","sentences":["In this paper, we propose a new setting for generating product descriptions from images, augmented by marketing keywords.","It leverages the combined power of visual and textual information to create descriptions that are more tailored to the unique features of products.","For this setting, previous methods utilize visual and textual encoders to encode the image and keywords and employ a language model-based decoder to generate the product description.","However, the generated description is often inaccurate and generic since same-category products have similar copy-writings, and optimizing the overall framework on large-scale samples makes models concentrate on common words yet ignore the product features.","To alleviate the issue, we present a simple and effective Multimodal In-Context Tuning approach, named ModICT, which introduces a similar product sample as the reference and utilizes the in-context learning capability of language models to produce the description.","During training, we keep the visual encoder and language model frozen, focusing on optimizing the modules responsible for creating multimodal in-context references and dynamic prompts.","This approach preserves the language generation prowess of large language models (LLMs), facilitating a substantial increase in description diversity.","To assess the effectiveness of ModICT across various language model scales and types, we collect data from three distinct product categories within the E-commerce domain.","Extensive experiments demonstrate that ModICT significantly improves the accuracy (by up to 3.3% on Rouge-L) and diversity (by up to 9.4% on D-5) of generated results compared to conventional methods.","Our findings underscore the potential of ModICT as a valuable tool for enhancing automatic generation of product descriptions in a wide range of applications."],"url":"http://arxiv.org/abs/2402.13587v1","category":"cs.CL"}
{"created":"2024-02-21 07:17:10","title":"TransGOP: Transformer-Based Gaze Object Prediction","abstract":"Gaze object prediction aims to predict the location and category of the object that is watched by a human. Previous gaze object prediction works use CNN-based object detectors to predict the object's location. However, we find that Transformer-based object detectors can predict more accurate object location for dense objects in retail scenarios. Moreover, the long-distance modeling capability of the Transformer can help to build relationships between the human head and the gaze object, which is important for the GOP task. To this end, this paper introduces Transformer into the fields of gaze object prediction and proposes an end-to-end Transformer-based gaze object prediction method named TransGOP. Specifically, TransGOP uses an off-the-shelf Transformer-based object detector to detect the location of objects and designs a Transformer-based gaze autoencoder in the gaze regressor to establish long-distance gaze relationships. Moreover, to improve gaze heatmap regression, we propose an object-to-gaze cross-attention mechanism to let the queries of the gaze autoencoder learn the global-memory position knowledge from the object detector. Finally, to make the whole framework end-to-end trained, we propose a Gaze Box loss to jointly optimize the object detector and gaze regressor by enhancing the gaze heatmap energy in the box of the gaze object. Extensive experiments on the GOO-Synth and GOO-Real datasets demonstrate that our TransGOP achieves state-of-the-art performance on all tracks, i.e., object detection, gaze estimation, and gaze object prediction. Our code will be available at https://github.com/chenxi-Guo/TransGOP.git.","sentences":["Gaze object prediction aims to predict the location and category of the object that is watched by a human.","Previous gaze object prediction works use CNN-based object detectors to predict the object's location.","However, we find that Transformer-based object detectors can predict more accurate object location for dense objects in retail scenarios.","Moreover, the long-distance modeling capability of the Transformer can help to build relationships between the human head and the gaze object, which is important for the GOP task.","To this end, this paper introduces Transformer into the fields of gaze object prediction and proposes an end-to-end Transformer-based gaze object prediction method named TransGOP.","Specifically, TransGOP uses an off-the-shelf Transformer-based object detector to detect the location of objects and designs a Transformer-based gaze autoencoder in the gaze regressor to establish long-distance gaze relationships.","Moreover, to improve gaze heatmap regression, we propose an object-to-gaze cross-attention mechanism to let the queries of the gaze autoencoder learn the global-memory position knowledge from the object detector.","Finally, to make the whole framework end-to-end trained, we propose a Gaze Box loss to jointly optimize the object detector and gaze regressor by enhancing the gaze heatmap energy in the box of the gaze object.","Extensive experiments on the GOO-Synth and GOO-Real datasets demonstrate that our TransGOP achieves state-of-the-art performance on all tracks, i.e., object detection, gaze estimation, and gaze object prediction.","Our code will be available at https://github.com/chenxi-Guo/TransGOP.git."],"url":"http://arxiv.org/abs/2402.13578v1","category":"cs.CV"}
