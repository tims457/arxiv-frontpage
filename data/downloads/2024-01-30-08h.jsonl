{"created":"2024-01-29 18:59:56","title":"Computer Vision for Primate Behavior Analysis in the Wild","abstract":"Advances in computer vision as well as increasingly widespread video-based behavioral monitoring have great potential for transforming how we study animal cognition and behavior. However, there is still a fairly large gap between the exciting prospects and what can actually be achieved in practice today, especially in videos from the wild. With this perspective paper, we want to contribute towards closing this gap, by guiding behavioral scientists in what can be expected from current methods and steering computer vision researchers towards problems that are relevant to advance research in animal behavior. We start with a survey of the state-of-the-art methods for computer vision problems that are directly relevant to the video-based study of animal behavior, including object detection, multi-individual tracking, (inter)action recognition and individual identification. We then review methods for effort-efficient learning, which is one of the biggest challenges from a practical perspective. Finally, we close with an outlook into the future of the emerging field of computer vision for animal behavior, where we argue that the field should move fast beyond the common frame-by-frame processing and treat video as a first-class citizen.","sentences":["Advances in computer vision as well as increasingly widespread video-based behavioral monitoring have great potential for transforming how we study animal cognition and behavior.","However, there is still a fairly large gap between the exciting prospects and what can actually be achieved in practice today, especially in videos from the wild.","With this perspective paper, we want to contribute towards closing this gap, by guiding behavioral scientists in what can be expected from current methods and steering computer vision researchers towards problems that are relevant to advance research in animal behavior.","We start with a survey of the state-of-the-art methods for computer vision problems that are directly relevant to the video-based study of animal behavior, including object detection, multi-individual tracking, (inter)action recognition and individual identification.","We then review methods for effort-efficient learning, which is one of the biggest challenges from a practical perspective.","Finally, we close with an outlook into the future of the emerging field of computer vision for animal behavior, where we argue that the field should move fast beyond the common frame-by-frame processing and treat video as a first-class citizen."],"url":"http://arxiv.org/abs/2401.16424v1","category":"cs.CV"}
{"created":"2024-01-29 18:59:55","title":"Synchformer: Efficient Synchronization from Sparse Cues","abstract":"Our objective is audio-visual synchronization with a focus on 'in-the-wild' videos, such as those on YouTube, where synchronization cues can be sparse. Our contributions include a novel audio-visual synchronization model, and training that decouples feature extraction from synchronization modelling through multi-modal segment-level contrastive pre-training. This approach achieves state-of-the-art performance in both dense and sparse settings. We also extend synchronization model training to AudioSet a million-scale 'in-the-wild' dataset, investigate evidence attribution techniques for interpretability, and explore a new capability for synchronization models: audio-visual synchronizability.","sentences":["Our objective is audio-visual synchronization with a focus on 'in-the-wild' videos, such as those on YouTube, where synchronization cues can be sparse.","Our contributions include a novel audio-visual synchronization model, and training that decouples feature extraction from synchronization modelling through multi-modal segment-level contrastive pre-training.","This approach achieves state-of-the-art performance in both dense and sparse settings.","We also extend synchronization model training to AudioSet a million-scale 'in-the-wild' dataset, investigate evidence attribution techniques for interpretability, and explore a new capability for synchronization models: audio-visual synchronizability."],"url":"http://arxiv.org/abs/2401.16423v1","category":"cs.CV"}
{"created":"2024-01-29 18:59:22","title":"Strategic Usage in a Multi-Learner Setting","abstract":"Real-world systems often involve some pool of users choosing between a set of services. With the increase in popularity of online learning algorithms, these services can now self-optimize, leveraging data collected on users to maximize some reward such as service quality. On the flipside, users may strategically choose which services to use in order to pursue their own reward functions, in the process wielding power over which services can see and use their data. Extensive prior research has been conducted on the effects of strategic users in single-service settings, with strategic behavior manifesting in the manipulation of observable features to achieve a desired classification; however, this can often be costly or unattainable for users and fails to capture the full behavior of multi-service dynamic systems. As such, we analyze a setting in which strategic users choose among several available services in order to pursue positive classifications, while services seek to minimize loss functions on their observations. We focus our analysis on realizable settings, and show that naive retraining can still lead to oscillation even if all users are observed at different times; however, if this retraining uses memory of past observations, convergent behavior can be guaranteed for certain loss function classes. We provide results obtained from synthetic and real-world data to empirically validate our theoretical findings.","sentences":["Real-world systems often involve some pool of users choosing between a set of services.","With the increase in popularity of online learning algorithms, these services can now self-optimize, leveraging data collected on users to maximize some reward such as service quality.","On the flipside, users may strategically choose which services to use in order to pursue their own reward functions, in the process wielding power over which services can see and use their data.","Extensive prior research has been conducted on the effects of strategic users in single-service settings, with strategic behavior manifesting in the manipulation of observable features to achieve a desired classification; however, this can often be costly or unattainable for users and fails to capture the full behavior of multi-service dynamic systems.","As such, we analyze a setting in which strategic users choose among several available services in order to pursue positive classifications, while services seek to minimize loss functions on their observations.","We focus our analysis on realizable settings, and show that naive retraining can still lead to oscillation even if all users are observed at different times; however, if this retraining uses memory of past observations, convergent behavior can be guaranteed for certain loss function classes.","We provide results obtained from synthetic and real-world data to empirically validate our theoretical findings."],"url":"http://arxiv.org/abs/2401.16422v1","category":"cs.LG"}
{"created":"2024-01-29 18:59:07","title":"Two Stones Hit One Bird: Bilevel Positional Encoding for Better Length Extrapolation","abstract":"In this work, we leverage the intrinsic segmentation of language sequences and design a new positional encoding method called Bilevel Positional Encoding (BiPE). For each position, our BiPE blends an intra-segment encoding and an inter-segment encoding. The intra-segment encoding identifies the locations within a segment and helps the model capture the semantic information therein via absolute positional encoding. The inter-segment encoding specifies the segment index, models the relationships between segments, and aims to improve extrapolation capabilities via relative positional encoding. Theoretical analysis shows this disentanglement of positional information makes learning more effective. The empirical results also show that our BiPE has superior length extrapolation capabilities across a wide range of tasks in diverse text modalities.","sentences":["In this work, we leverage the intrinsic segmentation of language sequences and design a new positional encoding method called Bilevel Positional Encoding (BiPE).","For each position, our BiPE blends an intra-segment encoding and an inter-segment encoding.","The intra-segment encoding identifies the locations within a segment and helps the model capture the semantic information therein via absolute positional encoding.","The inter-segment encoding specifies the segment index, models the relationships between segments, and aims to improve extrapolation capabilities via relative positional encoding.","Theoretical analysis shows this disentanglement of positional information makes learning more effective.","The empirical results also show that our BiPE has superior length extrapolation capabilities across a wide range of tasks in diverse text modalities."],"url":"http://arxiv.org/abs/2401.16421v1","category":"cs.LG"}
{"created":"2024-01-29 18:59:02","title":"InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model","abstract":"We introduce InternLM-XComposer2, a cutting-edge vision-language model excelling in free-form text-image composition and comprehension. This model goes beyond conventional vision-language understanding, adeptly crafting interleaved text-image content from diverse inputs like outlines, detailed textual specifications, and reference images, enabling highly customizable content creation. InternLM-XComposer2 proposes a Partial LoRA (PLoRA) approach that applies additional LoRA parameters exclusively to image tokens to preserve the integrity of pre-trained language knowledge, striking a balance between precise vision understanding and text composition with literary talent. Experimental results demonstrate the superiority of InternLM-XComposer2 based on InternLM2-7B in producing high-quality long-text multi-modal content and its exceptional vision-language understanding performance across various benchmarks, where it not only significantly outperforms existing multimodal models but also matches or even surpasses GPT-4V and Gemini Pro in certain assessments. This highlights its remarkable proficiency in the realm of multimodal understanding. The InternLM-XComposer2 model series with 7B parameters are publicly available at https://github.com/InternLM/InternLM-XComposer.","sentences":["We introduce InternLM-XComposer2, a cutting-edge vision-language model excelling in free-form text-image composition and comprehension.","This model goes beyond conventional vision-language understanding, adeptly crafting interleaved text-image content from diverse inputs like outlines, detailed textual specifications, and reference images, enabling highly customizable content creation.","InternLM-XComposer2 proposes a Partial LoRA (PLoRA) approach that applies additional LoRA parameters exclusively to image tokens to preserve the integrity of pre-trained language knowledge, striking a balance between precise vision understanding and text composition with literary talent.","Experimental results demonstrate the superiority of InternLM-XComposer2 based on InternLM2-7B in producing high-quality long-text multi-modal content and its exceptional vision-language understanding performance across various benchmarks, where it not only significantly outperforms existing multimodal models but also matches or even surpasses GPT-4V and Gemini Pro in certain assessments.","This highlights its remarkable proficiency in the realm of multimodal understanding.","The InternLM-XComposer2 model series with 7B parameters are publicly available at https://github.com/InternLM/InternLM-XComposer."],"url":"http://arxiv.org/abs/2401.16420v1","category":"cs.CV"}
{"created":"2024-01-29 18:57:45","title":"Semi-parametric Expert Bayesian Network Learning with Gaussian Processes and Horseshoe Priors","abstract":"This paper proposes a model learning Semi-parametric rela- tionships in an Expert Bayesian Network (SEBN) with linear parameter and structure constraints. We use Gaussian Pro- cesses and a Horseshoe prior to introduce minimal nonlin- ear components. To prioritize modifying the expert graph over adding new edges, we optimize differential Horseshoe scales. In real-world datasets with unknown truth, we gen- erate diverse graphs to accommodate user input, addressing identifiability issues and enhancing interpretability. Evalua- tion on synthetic and UCI Liver Disorders datasets, using metrics like structural Hamming Distance and test likelihood, demonstrates our models outperform state-of-the-art semi- parametric Bayesian Network model.","sentences":["This paper proposes a model learning Semi-parametric rela- tionships in an Expert Bayesian Network (SEBN) with linear parameter and structure constraints.","We use Gaussian Pro- cesses and a Horseshoe prior to introduce minimal nonlin- ear components.","To prioritize modifying the expert graph over adding new edges, we optimize differential Horseshoe scales.","In real-world datasets with unknown truth, we gen- erate diverse graphs to accommodate user input, addressing identifiability issues and enhancing interpretability.","Evalua- tion on synthetic and UCI Liver Disorders datasets, using metrics like structural Hamming Distance and test likelihood, demonstrates our models outperform state-of-the-art semi- parametric Bayesian Network model."],"url":"http://arxiv.org/abs/2401.16419v1","category":"cs.LG"}
{"created":"2024-01-29 18:56:21","title":"Boolean Logic as an Error feedback mechanism","abstract":"The notion of Boolean logic backpropagation was introduced to build neural networks with weights and activations being Boolean numbers. Most of computations can be done with Boolean logic instead of real arithmetic, both during training and inference phases. But the underlying discrete optimization problem is NP-hard, and the Boolean logic has no guarantee. In this work we propose the first convergence analysis, under standard non-convex assumptions.","sentences":["The notion of Boolean logic backpropagation was introduced to build neural networks with weights and activations being Boolean numbers.","Most of computations can be done with Boolean logic instead of real arithmetic, both during training and inference phases.","But the underlying discrete optimization problem is NP-hard, and the Boolean logic has no guarantee.","In this work we propose the first convergence analysis, under standard non-convex assumptions."],"url":"http://arxiv.org/abs/2401.16418v1","category":"stat.ML"}
{"created":"2024-01-29 18:56:10","title":"Channel Coding with Mean and Variance Cost Constraints","abstract":"We consider channel coding for discrete memoryless channels (DMCs) with a novel cost constraint that constrains both the mean and the variance of the cost of the codewords. We show that the maximum (asymptotically) achievable rate under the new cost formulation is equal to the capacity-cost function; in particular, the strong converse holds. We further characterize the optimal second-order coding rate of these cost-constrained codes; in particular, the optimal second-order coding rate is finite. We then show that the second-order coding performance is strictly improved with feedback using a new variation of timid/bold coding, significantly broadening the applicability of timid/bold coding schemes from unconstrained compound-dispersion channels to all cost-constrained channels. Equivalent results on the minimum average probability of error are also given.","sentences":["We consider channel coding for discrete memoryless channels (DMCs) with a novel cost constraint that constrains both the mean and the variance of the cost of the codewords.","We show that the maximum (asymptotically) achievable rate under the new cost formulation is equal to the capacity-cost function; in particular, the strong converse holds.","We further characterize the optimal second-order coding rate of these cost-constrained codes; in particular, the optimal second-order coding rate is finite.","We then show that the second-order coding performance is strictly improved with feedback using a new variation of timid/bold coding, significantly broadening the applicability of timid/bold coding schemes from unconstrained compound-dispersion channels to all cost-constrained channels.","Equivalent results on the minimum average probability of error are also given."],"url":"http://arxiv.org/abs/2401.16417v1","category":"cs.IT"}
{"created":"2024-01-29 18:55:29","title":"Endo-4DGS: Distilling Depth Ranking for Endoscopic Monocular Scene Reconstruction with 4D Gaussian Splatting","abstract":"In the realm of robot-assisted minimally invasive surgery, dynamic scene reconstruction can significantly enhance downstream tasks and improve surgical outcomes. Neural Radiance Fields (NeRF)-based methods have recently risen to prominence for their exceptional ability to reconstruct scenes. Nonetheless, these methods are hampered by slow inference, prolonged training, and substantial computational demands. Additionally, some rely on stereo depth estimation, which is often infeasible due to the high costs and logistical challenges associated with stereo cameras. Moreover, the monocular reconstruction quality for deformable scenes is currently inadequate. To overcome these obstacles, we present Endo-4DGS, an innovative, real-time endoscopic dynamic reconstruction approach that utilizes 4D Gaussian Splatting (GS) and requires no ground truth depth data. This method extends 3D GS by incorporating a temporal component and leverages a lightweight MLP to capture temporal Gaussian deformations. This effectively facilitates the reconstruction of dynamic surgical scenes with variable conditions. We also integrate Depth-Anything to generate pseudo-depth maps from monocular views, enhancing the depth-guided reconstruction process. Our approach has been validated on two surgical datasets, where it has proven to render in real-time, compute efficiently, and reconstruct with remarkable accuracy. These results underline the vast potential of Endo-4DGS to improve surgical assistance.","sentences":["In the realm of robot-assisted minimally invasive surgery, dynamic scene reconstruction can significantly enhance downstream tasks and improve surgical outcomes.","Neural Radiance Fields (NeRF)-based methods have recently risen to prominence for their exceptional ability to reconstruct scenes.","Nonetheless, these methods are hampered by slow inference, prolonged training, and substantial computational demands.","Additionally, some rely on stereo depth estimation, which is often infeasible due to the high costs and logistical challenges associated with stereo cameras.","Moreover, the monocular reconstruction quality for deformable scenes is currently inadequate.","To overcome these obstacles, we present Endo-4DGS, an innovative, real-time endoscopic dynamic reconstruction approach that utilizes 4D Gaussian Splatting (GS) and requires no ground truth depth data.","This method extends 3D GS by incorporating a temporal component and leverages a lightweight MLP to capture temporal Gaussian deformations.","This effectively facilitates the reconstruction of dynamic surgical scenes with variable conditions.","We also integrate Depth-Anything to generate pseudo-depth maps from monocular views, enhancing the depth-guided reconstruction process.","Our approach has been validated on two surgical datasets, where it has proven to render in real-time, compute efficiently, and reconstruct with remarkable accuracy.","These results underline the vast potential of Endo-4DGS to improve surgical assistance."],"url":"http://arxiv.org/abs/2401.16416v1","category":"cs.CV"}
{"created":"2024-01-29 18:51:02","title":"A Causal Model for Quantifying Multipartite Classical and Quantum Correlations","abstract":"We give an operational definition of information-theoretic resources within a given multipartite classical or quantum correlation. We present our causal model that serves as the source coding side of this correlation and introduce a novel concept of resource rate. We argue that, beyond classical secrecy, additional resources exist that are useful for the security of distributed computing problems, which can be captured by the resource rate. Furthermore, we establish a relationship between resource rate and an extension of Shannon's logarithmic information measure, namely, total correlation. Subsequently, we present a novel quantum secrecy monotone and investigate a quantum hybrid key distribution system as an extension of our causal model. Finally, we discuss some connections to optimal transport (OT) problem.","sentences":["We give an operational definition of information-theoretic resources within a given multipartite classical or quantum correlation.","We present our causal model that serves as the source coding side of this correlation and introduce a novel concept of resource rate.","We argue that, beyond classical secrecy, additional resources exist that are useful for the security of distributed computing problems, which can be captured by the resource rate.","Furthermore, we establish a relationship between resource rate and an extension of Shannon's logarithmic information measure, namely, total correlation.","Subsequently, we present a novel quantum secrecy monotone and investigate a quantum hybrid key distribution system as an extension of our causal model.","Finally, we discuss some connections to optimal transport (OT) problem."],"url":"http://arxiv.org/abs/2401.16414v1","category":"cs.IT"}
{"created":"2024-01-29 18:49:50","title":"Learning to Manipulate under Limited Information","abstract":"By classic results in social choice theory, any reasonable preferential voting method sometimes gives individuals an incentive to report an insincere preference. The extent to which different voting methods are more or less resistant to such strategic manipulation has become a key consideration for comparing voting methods. Here we measure resistance to manipulation by whether neural networks of varying sizes can learn to profitably manipulate a given voting method in expectation, given different types of limited information about how other voters will vote. We trained nearly 40,000 neural networks of 26 sizes to manipulate against 8 different voting methods, under 6 types of limited information, in committee-sized elections with 5-21 voters and 3-6 candidates. We find that some voting methods, such as Borda, are highly manipulable by networks with limited information, while others, such as Instant Runoff, are not, despite being quite profitably manipulated by an ideal manipulator with full information.","sentences":["By classic results in social choice theory, any reasonable preferential voting method sometimes gives individuals an incentive to report an insincere preference.","The extent to which different voting methods are more or less resistant to such strategic manipulation has become a key consideration for comparing voting methods.","Here we measure resistance to manipulation by whether neural networks of varying sizes can learn to profitably manipulate a given voting method in expectation, given different types of limited information about how other voters will vote.","We trained nearly 40,000 neural networks of 26 sizes to manipulate against 8 different voting methods, under 6 types of limited information, in committee-sized elections with 5-21 voters and 3-6 candidates.","We find that some voting methods, such as Borda, are highly manipulable by networks with limited information, while others, such as Instant Runoff, are not, despite being quite profitably manipulated by an ideal manipulator with full information."],"url":"http://arxiv.org/abs/2401.16412v1","category":"cs.AI"}
{"created":"2024-01-29 18:47:36","title":"ReTaSA: A Nonparametric Functional Estimation Approach for Addressing Continuous Target Shift","abstract":"The presence of distribution shifts poses a significant challenge for deploying modern machine learning models in real-world applications. This work focuses on the target shift problem in a regression setting (Zhang et al., 2013; Nguyen et al., 2016). More specifically, the target variable y (also known as the response variable), which is continuous, has different marginal distributions in the training source and testing domain, while the conditional distribution of features x given y remains the same. While most literature focuses on classification tasks with finite target space, the regression problem has an infinite dimensional target space, which makes many of the existing methods inapplicable. In this work, we show that the continuous target shift problem can be addressed by estimating the importance weight function from an ill-posed integral equation. We propose a nonparametric regularized approach named ReTaSA to solve the ill-posed integral equation and provide theoretical justification for the estimated importance weight function. The effectiveness of the proposed method has been demonstrated with extensive numerical studies on synthetic and real-world datasets.","sentences":["The presence of distribution shifts poses a significant challenge for deploying modern machine learning models in real-world applications.","This work focuses on the target shift problem in a regression setting (Zhang et al., 2013; Nguyen et al., 2016).","More specifically, the target variable y (also known as the response variable), which is continuous, has different marginal distributions in the training source and testing domain, while the conditional distribution of features x given y remains the same.","While most literature focuses on classification tasks with finite target space, the regression problem has an infinite dimensional target space, which makes many of the existing methods inapplicable.","In this work, we show that the continuous target shift problem can be addressed by estimating the importance weight function from an ill-posed integral equation.","We propose a nonparametric regularized approach named ReTaSA to solve the ill-posed integral equation and provide theoretical justification for the estimated importance weight function.","The effectiveness of the proposed method has been demonstrated with extensive numerical studies on synthetic and real-world datasets."],"url":"http://arxiv.org/abs/2401.16410v1","category":"stat.ML"}
{"created":"2024-01-29 18:46:53","title":"Is K-fold cross validation the best model selection method for Machine Learning?","abstract":"As a technique that can compactly represent complex patterns, machine learning has significant potential for predictive inference. K-fold cross-validation (CV) is the most common approach to ascertaining the likelihood that a machine learning outcome is generated by chance and frequently outperforms conventional hypothesis testing. This improvement uses measures directly obtained from machine learning classifications, such as accuracy, that do not have a parametric description. To approach a frequentist analysis within machine learning pipelines, a permutation test or simple statistics from data partitions (i.e. folds) can be added to estimate confidence intervals. Unfortunately, neither parametric nor non-parametric tests solve the inherent problems around partitioning small sample-size datasets and learning from heterogeneous data sources. The fact that machine learning strongly depends on the learning parameters and the distribution of data across folds recapitulates familiar difficulties around excess false positives and replication. The origins of this problem are demonstrated by simulating common experimental circumstances, including small sample sizes, low numbers of predictors, and heterogeneous data sources. A novel statistical test based on K-fold CV and the Upper Bound of the actual error (K-fold CUBV) is composed, where uncertain predictions of machine learning with CV are bounded by the \\emph{worst case} through the evaluation of concentration inequalities. Probably Approximately Correct-Bayesian upper bounds for linear classifiers in combination with K-fold CV is used to estimate the empirical error. The performance with neuroimaging datasets suggests this is a robust criterion for detecting effects, validating accuracy values obtained from machine learning whilst avoiding excess false positives.","sentences":["As a technique that can compactly represent complex patterns, machine learning has significant potential for predictive inference.","K-fold cross-validation (CV) is the most common approach to ascertaining the likelihood that a machine learning outcome is generated by chance and frequently outperforms conventional hypothesis testing.","This improvement uses measures directly obtained from machine learning classifications, such as accuracy, that do not have a parametric description.","To approach a frequentist analysis within machine learning pipelines, a permutation test or simple statistics from data partitions (i.e. folds) can be added to estimate confidence intervals.","Unfortunately, neither parametric nor non-parametric tests solve the inherent problems around partitioning small sample-size datasets and learning from heterogeneous data sources.","The fact that machine learning strongly depends on the learning parameters and the distribution of data across folds recapitulates familiar difficulties around excess false positives and replication.","The origins of this problem are demonstrated by simulating common experimental circumstances, including small sample sizes, low numbers of predictors, and heterogeneous data sources.","A novel statistical test based on K-fold CV and the Upper Bound of the actual error (K-fold CUBV) is composed, where uncertain predictions of machine learning with CV are bounded by the \\emph{worst case} through the evaluation of concentration inequalities.","Probably Approximately Correct-Bayesian upper bounds for linear classifiers in combination with K-fold CV is used to estimate the empirical error.","The performance with neuroimaging datasets suggests this is a robust criterion for detecting effects, validating accuracy values obtained from machine learning whilst avoiding excess false positives."],"url":"http://arxiv.org/abs/2401.16407v1","category":"stat.ML"}
{"created":"2024-01-29 18:46:53","title":"A Cooper-pair beam splitter as a feasible source of entangled electrons","abstract":"We investigate the generation of an entangled electron pair emerging from a system composed of two quantum dots attached to a superconductor Cooper pair beam splitter. We take into account three processes: Crossed Andreev Reflection, cotuneling, and Coulomb interaction. Together, these processes play crucial roles in the formation of entangled electronic states, with electrons being in spatially separated quantum dots. By using perturbation theory, we derive an analytical effective model that allows a simple picture of the intricate process behind the formation of the entangled state. Several entanglement quantifiers, including quantum mutual information, negativity, and concurrence, are employed to validate our findings. Finally, we define and calculate the covariance associated with the detection of two electrons, each originating from one of the quantum dots with a specific spin value. The time evolution of this observable follows the dynamics of all entanglement quantifiers, thus suggesting that it can be a useful tool for mapping the creation of entangled electrons in future applications within quantum information protocols.","sentences":["We investigate the generation of an entangled electron pair emerging from a system composed of two quantum dots attached to a superconductor Cooper pair beam splitter.","We take into account three processes: Crossed Andreev Reflection, cotuneling, and Coulomb interaction.","Together, these processes play crucial roles in the formation of entangled electronic states, with electrons being in spatially separated quantum dots.","By using perturbation theory, we derive an analytical effective model that allows a simple picture of the intricate process behind the formation of the entangled state.","Several entanglement quantifiers, including quantum mutual information, negativity, and concurrence, are employed to validate our findings.","Finally, we define and calculate the covariance associated with the detection of two electrons, each originating from one of the quantum dots with a specific spin value.","The time evolution of this observable follows the dynamics of all entanglement quantifiers, thus suggesting that it can be a useful tool for mapping the creation of entangled electrons in future applications within quantum information protocols."],"url":"http://arxiv.org/abs/2401.16408v1","category":"quant-ph"}
{"created":"2024-01-29 18:43:49","title":"Scaling Sparse Fine-Tuning to Large Language Models","abstract":"Large Language Models (LLMs) are difficult to fully fine-tune (e.g., with instructions or human feedback) due to their sheer number of parameters. A family of parameter-efficient sparse fine-tuning (SFT) methods have proven promising in terms of performance but their memory requirements increase proportionally to the size of the LLMs. In this work, we scale sparse fine-tuning to state-of-the-art LLMs like LLaMA 2 7B and 13B. At any given time, for a desired density level, we maintain an array of parameter indices and the deltas of these parameters relative to their pretrained values. We iterate among: (a) updating the active deltas, (b) pruning indices (based on the change of magnitude of their deltas) and (c) regrowth of indices. For regrowth, we explore two criteria based on either the accumulated gradients of a few candidate parameters or their approximate momenta estimated using the efficient SM3 optimizer. We experiment with instruction-tuning of LLMs on standard dataset mixtures, finding that SFT is often superior to popular parameter-efficient fine-tuning methods like LoRA (low-rank adaptation) in terms of performance and comparable in terms of run time. We additionally show that SFT is compatible with both quantization and efficient optimizers, to facilitate scaling to ever-larger model sizes. We release the code for SFT at https://github.com/AlanAnsell/peft and for the instruction-tuning experiments at https://github.com/ducdauge/sft-llm.","sentences":["Large Language Models (LLMs) are difficult to fully fine-tune (e.g., with instructions or human feedback) due to their sheer number of parameters.","A family of parameter-efficient sparse fine-tuning (SFT) methods have proven promising in terms of performance but their memory requirements increase proportionally to the size of the LLMs.","In this work, we scale sparse fine-tuning to state-of-the-art LLMs like LLaMA 2 7B and 13B. At any given time, for a desired density level, we maintain an array of parameter indices and the deltas of these parameters relative to their pretrained values.","We iterate among: (a) updating the active deltas, (b) pruning indices (based on the change of magnitude of their deltas) and (c) regrowth of indices.","For regrowth, we explore two criteria based on either the accumulated gradients of a few candidate parameters or their approximate momenta estimated using the efficient SM3 optimizer.","We experiment with instruction-tuning of LLMs on standard dataset mixtures, finding that SFT is often superior to popular parameter-efficient fine-tuning methods like LoRA (low-rank adaptation) in terms of performance and comparable in terms of run time.","We additionally show that SFT is compatible with both quantization and efficient optimizers, to facilitate scaling to ever-larger model sizes.","We release the code for SFT at https://github.com/AlanAnsell/peft and for the instruction-tuning experiments at https://github.com/ducdauge/sft-llm."],"url":"http://arxiv.org/abs/2401.16405v1","category":"cs.CL"}
{"created":"2024-01-29 18:41:39","title":"ViLexNorm: A Lexical Normalization Corpus for Vietnamese Social Media Text","abstract":"Lexical normalization, a fundamental task in Natural Language Processing (NLP), involves the transformation of words into their canonical forms. This process has been proven to benefit various downstream NLP tasks greatly. In this work, we introduce Vietnamese Lexical Normalization (ViLexNorm), the first-ever corpus developed for the Vietnamese lexical normalization task. The corpus comprises over 10,000 pairs of sentences meticulously annotated by human annotators, sourced from public comments on Vietnam's most popular social media platforms. Various methods were used to evaluate our corpus, and the best-performing system achieved a result of 57.74% using the Error Reduction Rate (ERR) metric (van der Goot, 2019a) with the Leave-As-Is (LAI) baseline. For extrinsic evaluation, employing the model trained on ViLexNorm demonstrates the positive impact of the Vietnamese lexical normalization task on other NLP tasks. Our corpus is publicly available exclusively for research purposes.","sentences":["Lexical normalization, a fundamental task in Natural Language Processing (NLP), involves the transformation of words into their canonical forms.","This process has been proven to benefit various downstream NLP tasks greatly.","In this work, we introduce Vietnamese Lexical Normalization (ViLexNorm), the first-ever corpus developed for the Vietnamese lexical normalization task.","The corpus comprises over 10,000 pairs of sentences meticulously annotated by human annotators, sourced from public comments on Vietnam's most popular social media platforms.","Various methods were used to evaluate our corpus, and the best-performing system achieved a result of 57.74% using the Error Reduction Rate (ERR) metric (van der Goot, 2019a) with the Leave-As-Is (LAI) baseline.","For extrinsic evaluation, employing the model trained on ViLexNorm demonstrates the positive impact of the Vietnamese lexical normalization task on other NLP tasks.","Our corpus is publicly available exclusively for research purposes."],"url":"http://arxiv.org/abs/2401.16403v1","category":"cs.CL"}
{"created":"2024-01-29 18:41:21","title":"A Survey on Visual Anomaly Detection: Challenge, Approach, and Prospect","abstract":"Visual Anomaly Detection (VAD) endeavors to pinpoint deviations from the concept of normality in visual data, widely applied across diverse domains, e.g., industrial defect inspection, and medical lesion detection. This survey comprehensively examines recent advancements in VAD by identifying three primary challenges: 1) scarcity of training data, 2) diversity of visual modalities, and 3) complexity of hierarchical anomalies. Starting with a brief overview of the VAD background and its generic concept definitions, we progressively categorize, emphasize, and discuss the latest VAD progress from the perspective of sample number, data modality, and anomaly hierarchy. Through an in-depth analysis of the VAD field, we finally summarize future developments for VAD and conclude the key findings and contributions of this survey.","sentences":["Visual Anomaly Detection (VAD) endeavors to pinpoint deviations from the concept of normality in visual data, widely applied across diverse domains, e.g., industrial defect inspection, and medical lesion detection.","This survey comprehensively examines recent advancements in VAD by identifying three primary challenges: 1) scarcity of training data, 2) diversity of visual modalities, and 3) complexity of hierarchical anomalies.","Starting with a brief overview of the VAD background and its generic concept definitions, we progressively categorize, emphasize, and discuss the latest VAD progress from the perspective of sample number, data modality, and anomaly hierarchy.","Through an in-depth analysis of the VAD field, we finally summarize future developments for VAD and conclude the key findings and contributions of this survey."],"url":"http://arxiv.org/abs/2401.16402v1","category":"cs.CV"}
{"created":"2024-01-29 18:41:05","title":"Effective mass and symmetry breaking in the IKKT matrix model from compactification","abstract":"The IKKT model is a promising candidate for a non-perturbative description of Type IIB superstring theory. It is known from analytic approaches and numerical simulations that the IKKT matrix model with a mass term admits interesting cosmological solutions. However, this mass term is often introduced by hand, and serves as a regulator in the theory. In the present paper, we show that an effective mass matrix can arise naturally in the IKKT model by imposing a toroidal compactification where the space-time fermions acquire anti-periodic boundary conditions. When six spatial dimensions are chosen to be compact, the effective mass matrix breaks the SO(1,9) space-time symmetry of the IKKT model to SO(1,3) $\\times$ SO(6). This paves the way for space-time solutions of the IKKT model where SO(1,9) symmetry is naturally broken to SO(1,3) $\\times$ SO(6).","sentences":["The IKKT model is a promising candidate for a non-perturbative description of Type IIB superstring theory.","It is known from analytic approaches and numerical simulations that the IKKT matrix model with a mass term admits interesting cosmological solutions.","However, this mass term is often introduced by hand, and serves as a regulator in the theory.","In the present paper, we show that an effective mass matrix can arise naturally in the IKKT model by imposing a toroidal compactification where the space-time fermions acquire anti-periodic boundary conditions.","When six spatial dimensions are chosen to be compact, the effective mass matrix breaks the SO(1,9) space-time symmetry of the IKKT model to SO(1,3) $\\times$ SO(6).","This paves the way for space-time solutions of the IKKT model where SO(1,9) symmetry is naturally broken to SO(1,3) $\\times$ SO(6)."],"url":"http://arxiv.org/abs/2401.16401v1","category":"hep-th"}
{"created":"2024-01-29 18:39:52","title":"Gravity and $T\\bar{T}$ flows in higher dimensions","abstract":"In this work we propose a generalisation to arbitrary space-time dimension of the well-known equivalence between two-dimensional $T\\bar{T}$ deformations and coupling to Jackiw-Teitelboim gravity. Focusing on the $T\\bar{T}$-type flows in general dimension recently proposed by one of the authors, we introduce an alternative viewpoint, as coupling to the class of Ricci-based massive gravity theories. This alternative definition rests on the existence of a class of dynamical equivalences between modified massive gravity theories coupled with matter sectors which, in turn, engenders a flow controlled by a parameter proportional to the inverse square of the gravitational mass scale. We conjecture this dynamical equivalence to have a wide range of validity, particularly holding for theories in arbitrary space-time dimensions, and provide a detailed analysis for some examples in $d=4$ dimensions.","sentences":["In this work we propose a generalisation to arbitrary space-time dimension of the well-known equivalence between two-dimensional $T\\bar{T}$ deformations and coupling to Jackiw-Teitelboim gravity.","Focusing on the $T\\bar{T}$-type flows in general dimension recently proposed by one of the authors, we introduce an alternative viewpoint, as coupling to the class of Ricci-based massive gravity theories.","This alternative definition rests on the existence of a class of dynamical equivalences between modified massive gravity theories coupled with matter sectors which, in turn, engenders a flow controlled by a parameter proportional to the inverse square of the gravitational mass scale.","We conjecture this dynamical equivalence to have a wide range of validity, particularly holding for theories in arbitrary space-time dimensions, and provide a detailed analysis for some examples in $d=4$ dimensions."],"url":"http://arxiv.org/abs/2401.16400v1","category":"hep-th"}
{"created":"2024-01-29 18:39:08","title":"Single-Winner Voting with Alliances: Avoiding the Spoiler Effect","abstract":"We study the setting of single-winner elections with ordinal preferences where candidates might be members of \\emph{alliances} (which may correspond to e.g., political parties, factions, or coalitions). However, we do not assume that candidates from the same alliance are necessarily adjacent in voters' rankings. In such case, every classical voting rule is vulnerable to the spoiler effect, i.e., the presence of a candidate may harm his or her alliance. We therefore introduce a new idea of \\emph{alliance-aware} voting rules which extend the classical ones. We show that our approach is superior both to using classical cloneproof voting rules and to running primaries within alliances before the election.   We introduce several alliance-aware voting rules and show that they satisfy the most desirable standard properties of their classical counterparts as well as newly introduced axioms for the model with alliances which, e.g., exclude the possibility of the spoiler effect. Our rules have natural definitions and are simple enough to explain to be used in practice.","sentences":["We study the setting of single-winner elections with ordinal preferences where candidates might be members of \\emph{alliances} (which may correspond to e.g., political parties, factions, or coalitions).","However, we do not assume that candidates from the same alliance are necessarily adjacent in voters' rankings.","In such case, every classical voting rule is vulnerable to the spoiler effect, i.e., the presence of a candidate may harm his or her alliance.","We therefore introduce a new idea of \\emph{alliance-aware} voting rules which extend the classical ones.","We show that our approach is superior both to using classical cloneproof voting rules and to running primaries within alliances before the election.   ","We introduce several alliance-aware voting rules and show that they satisfy the most desirable standard properties of their classical counterparts as well as newly introduced axioms for the model with alliances which, e.g., exclude the possibility of the spoiler effect.","Our rules have natural definitions and are simple enough to explain to be used in practice."],"url":"http://arxiv.org/abs/2401.16399v1","category":"cs.GT"}
{"created":"2024-01-29 18:38:29","title":"Zero-shot Imitation Policy via Search in Demonstration Dataset","abstract":"Behavioral cloning uses a dataset of demonstrations to learn a policy. To overcome computationally expensive training procedures and address the policy adaptation problem, we propose to use latent spaces of pre-trained foundation models to index a demonstration dataset, instantly access similar relevant experiences, and copy behavior from these situations. Actions from a selected similar situation can be performed by the agent until representations of the agent's current situation and the selected experience diverge in the latent space. Thus, we formulate our control problem as a dynamic search problem over a dataset of experts' demonstrations. We test our approach on BASALT MineRL-dataset in the latent representation of a Video Pre-Training model. We compare our model to state-of-the-art, Imitation Learning-based Minecraft agents. Our approach can effectively recover meaningful demonstrations and show human-like behavior of an agent in the Minecraft environment in a wide variety of scenarios. Experimental results reveal that performance of our search-based approach clearly wins in terms of accuracy and perceptual evaluation over learning-based models.","sentences":["Behavioral cloning uses a dataset of demonstrations to learn a policy.","To overcome computationally expensive training procedures and address the policy adaptation problem, we propose to use latent spaces of pre-trained foundation models to index a demonstration dataset, instantly access similar relevant experiences, and copy behavior from these situations.","Actions from a selected similar situation can be performed by the agent until representations of the agent's current situation and the selected experience diverge in the latent space.","Thus, we formulate our control problem as a dynamic search problem over a dataset of experts' demonstrations.","We test our approach on BASALT MineRL-dataset in the latent representation of a Video Pre-Training model.","We compare our model to state-of-the-art, Imitation Learning-based Minecraft agents.","Our approach can effectively recover meaningful demonstrations and show human-like behavior of an agent in the Minecraft environment in a wide variety of scenarios.","Experimental results reveal that performance of our search-based approach clearly wins in terms of accuracy and perceptual evaluation over learning-based models."],"url":"http://arxiv.org/abs/2401.16398v1","category":"cs.AI"}
{"created":"2024-01-29 18:36:28","title":"Deciding Subtyping for Asynchronous Multiparty Sessions","abstract":"Multiparty session types (MSTs) are a type-based approach to verifying communication protocols, represented as global types in the framework. We present a precise subtyping relation for asynchronous MSTs with communicating state machines (CSMs) as implementation model. We address two problems: when can a local implementation safely substitute another, and when does an arbitrary CSM implement a global type? We define safety with respect to a given global type, in terms of subprotocol fidelity and deadlock freedom. Our implementation model subsumes existing work which considers local types with restricted choice. We exploit the connection between MST subtyping and refinement to formulate concise conditions that are directly checkable on the candidate implementations, and use them to show that both problems are decidable in polynomial time.","sentences":["Multiparty session types (MSTs) are a type-based approach to verifying communication protocols, represented as global types in the framework.","We present a precise subtyping relation for asynchronous MSTs with communicating state machines (CSMs) as implementation model.","We address two problems: when can a local implementation safely substitute another, and when does an arbitrary CSM implement a global type?","We define safety with respect to a given global type, in terms of subprotocol fidelity and deadlock freedom.","Our implementation model subsumes existing work which considers local types with restricted choice.","We exploit the connection between MST subtyping and refinement to formulate concise conditions that are directly checkable on the candidate implementations, and use them to show that both problems are decidable in polynomial time."],"url":"http://arxiv.org/abs/2401.16395v1","category":"cs.FL"}
{"created":"2024-01-29 18:34:36","title":"Amazon's 2023 Drought: Sentinel-1 Reveals Extreme Rio Negro River Contraction","abstract":"The Amazon, the world's largest rainforest, faces a severe historic drought. The Rio Negro River, one of the major Amazon River tributaries, reaches its lowest level in a century in October 2023. Here, we used a U-net deep learning model to map water surfaces in the Rio Negro River basin every 12 days in 2022 and 2023 using 10 m spatial resolution Sentinel-1 satellite radar images. The accuracy of the water surface model was high with an F1-score of 0.93. The 12 days mosaic time series of water surface was generated from the Sentinel-1 prediction. The water surface mask demonstrated relatively consistent agreement with the Global Surface Water (GSW) product from Joint Research Centre (F1-score: 0.708) and with the Brazilian Mapbiomas Water initiative (F1-score: 0.686). The main errors of the map were omission errors in flooded woodland, in flooded shrub and because of clouds. Rio Negro water surfaces reached their lowest level around the 25th of November 2023 and were reduced to 68.1\\% (9,559.9 km$^2$) of the maximum water surfaces observed in the period 2022-2023 (14,036.3 km$^2$). Synthetic Aperture Radar (SAR) data, in conjunction with deep learning techniques, can significantly improve near real-time mapping of water surface in tropical regions.","sentences":["The Amazon, the world's largest rainforest, faces a severe historic drought.","The Rio Negro River, one of the major Amazon River tributaries, reaches its lowest level in a century in October 2023.","Here, we used a U-net deep learning model to map water surfaces in the Rio Negro River basin every 12 days in 2022 and 2023 using 10 m spatial resolution Sentinel-1 satellite radar images.","The accuracy of the water surface model was high with an F1-score of 0.93.","The 12 days mosaic time series of water surface was generated from the Sentinel-1 prediction.","The water surface mask demonstrated relatively consistent agreement with the Global Surface Water (GSW) product from Joint Research Centre (F1-score: 0.708) and with the Brazilian Mapbiomas Water initiative (F1-score: 0.686).","The main errors of the map were omission errors in flooded woodland, in flooded shrub and because of clouds.","Rio Negro water surfaces reached their lowest level around the 25th of November 2023 and were reduced to 68.1\\% (9,559.9 km$^2$) of the maximum water surfaces observed in the period 2022-2023 (14,036.3 km$^2$).","Synthetic Aperture Radar (SAR) data, in conjunction with deep learning techniques, can significantly improve near real-time mapping of water surface in tropical regions."],"url":"http://arxiv.org/abs/2401.16393v1","category":"cs.CV"}
{"created":"2024-01-29 18:32:19","title":"Quantum Private Membership Aggregation","abstract":"We consider the problem of private set membership aggregation of $N$ parties by using an entangled quantum state. In this setting, the $N$ parties, which share an entangled state, aim to \\emph{privately} know the number of times each element (message) is repeated among the $N$ parties, with respect to a universal set $\\mathcal{K}$. This problem has applications in private comparison, ranking, voting, etc. We propose an encoding algorithm that maps the classical information into distinguishable quantum states, along with a decoding algorithm that exploits the distinguishability of the mapped states. The proposed scheme can also be used to calculate the $N$ party private summation modulo $P$.","sentences":["We consider the problem of private set membership aggregation of $N$ parties by using an entangled quantum state.","In this setting, the $N$ parties, which share an entangled state, aim to \\emph{privately} know the number of times each element (message) is repeated among the $N$ parties, with respect to a universal set $\\mathcal{K}$. This problem has applications in private comparison, ranking, voting, etc.","We propose an encoding algorithm that maps the classical information into distinguishable quantum states, along with a decoding algorithm that exploits the distinguishability of the mapped states.","The proposed scheme can also be used to calculate the $N$ party private summation modulo $P$."],"url":"http://arxiv.org/abs/2401.16390v1","category":"cs.IT"}
{"created":"2024-01-29 18:28:28","title":"Green Adaptation of Real-Time Web Services for Industrial CPS within a Cloud Environment","abstract":"Managing energy efficiency under timing constraints is an interesting and big challenge. This work proposes an accurate power model in data centers for time-constrained servers in Cloud computing. This model, as opposed to previous approaches, does not only consider the workload assigned to the processing element, but also incorporates the need of considering the static power consumption and, even more interestingly, its dependency with temperature. The proposed model has been used in a multi-objective optimization environment in which the Dynamic Voltage and Frequency Scaling (DVFS) and workload assignment have been efficiently optimized.","sentences":["Managing energy efficiency under timing constraints is an interesting and big challenge.","This work proposes an accurate power model in data centers for time-constrained servers in Cloud computing.","This model, as opposed to previous approaches, does not only consider the workload assigned to the processing element, but also incorporates the need of considering the static power consumption and, even more interestingly, its dependency with temperature.","The proposed model has been used in a multi-objective optimization environment in which the Dynamic Voltage and Frequency Scaling (DVFS) and workload assignment have been efficiently optimized."],"url":"http://arxiv.org/abs/2401.16387v1","category":"cs.AR"}
{"created":"2024-01-29 18:27:52","title":"Continual Learning with Pre-Trained Models: A Survey","abstract":"Nowadays, real-world applications often face streaming data, which requires the learning system to absorb new knowledge as data evolves. Continual Learning (CL) aims to achieve this goal and meanwhile overcome the catastrophic forgetting of former knowledge when learning new ones. Typical CL methods build the model from scratch to grow with incoming data. However, the advent of the pre-trained model (PTM) era has sparked immense research interest, particularly in leveraging PTMs' robust representational capabilities. This paper presents a comprehensive survey of the latest advancements in PTM-based CL. We categorize existing methodologies into three distinct groups, providing a comparative analysis of their similarities, differences, and respective advantages and disadvantages. Additionally, we offer an empirical study contrasting various state-of-the-art methods to highlight concerns regarding fairness in comparisons. The source code to reproduce these evaluations is available at: https://github.com/sun-hailong/LAMDA-PILOT","sentences":["Nowadays, real-world applications often face streaming data, which requires the learning system to absorb new knowledge as data evolves.","Continual Learning (CL) aims to achieve this goal and meanwhile overcome the catastrophic forgetting of former knowledge when learning new ones.","Typical CL methods build the model from scratch to grow with incoming data.","However, the advent of the pre-trained model (PTM) era has sparked immense research interest, particularly in leveraging PTMs' robust representational capabilities.","This paper presents a comprehensive survey of the latest advancements in PTM-based CL.","We categorize existing methodologies into three distinct groups, providing a comparative analysis of their similarities, differences, and respective advantages and disadvantages.","Additionally, we offer an empirical study contrasting various state-of-the-art methods to highlight concerns regarding fairness in comparisons.","The source code to reproduce these evaluations is available at: https://github.com/sun-hailong/LAMDA-PILOT"],"url":"http://arxiv.org/abs/2401.16386v1","category":"cs.LG"}
{"created":"2024-01-29 18:25:58","title":"Dipole superfluid hydrodynamics II","abstract":"We present a dissipative hydrodynamic theory of \"s-wave dipole superfluids\" that arise in phases of translation-invariant and dipole-symmetric models in which the U(1) symmetry is spontaneously broken. The hydrodynamic description is subtle on account of an analogue of dangerously irrelevant operators, which requires us to formalize an entirely new derivative counting scheme suitable for these fluids. We use our hydrodynamic model to investigate the linearized response of such a fluid, characterized by sound modes $\\omega \\sim \\pm k - ik^2$, shear modes $\\omega\\sim-ik^2$, and magnon-like propagating modes $\\omega \\sim \\pm k^2 - ik^4$ that are the dipole-invariant version of superfluid \"second sound\" modes. We find that these fluids can also admit equilibrium states with \"dipole superflow\" that resemble a polarized medium. Finally, we couple our theory to slowly varying background fields, which allows us to compute response functions of hydrodynamic operators and Kubo formulas for hydrodynamic transport coefficients.","sentences":["We present a dissipative hydrodynamic theory of \"s-wave dipole superfluids\" that arise in phases of translation-invariant and dipole-symmetric models in which the U(1) symmetry is spontaneously broken.","The hydrodynamic description is subtle on account of an analogue of dangerously irrelevant operators, which requires us to formalize an entirely new derivative counting scheme suitable for these fluids.","We use our hydrodynamic model to investigate the linearized response of such a fluid, characterized by sound modes $\\omega \\sim \\pm k - ik^2$, shear modes $\\omega\\sim-ik^2$, and magnon-like propagating modes $\\omega \\sim \\pm k^2 - ik^4$ that are the dipole-invariant version of superfluid \"second sound\" modes.","We find that these fluids can also admit equilibrium states with \"dipole superflow\" that resemble a polarized medium.","Finally, we couple our theory to slowly varying background fields, which allows us to compute response functions of hydrodynamic operators and Kubo formulas for hydrodynamic transport coefficients."],"url":"http://arxiv.org/abs/2401.16385v1","category":"hep-th"}
{"created":"2024-01-29 18:24:16","title":"Learning logic programs by finding minimal unsatisfiable subprograms","abstract":"The goal of inductive logic programming (ILP) is to search for a logic program that generalises training examples and background knowledge. We introduce an ILP approach that identifies minimal unsatisfiable subprograms (MUSPs). We show that finding MUSPs allows us to efficiently and soundly prune the search space. Our experiments on multiple domains, including program synthesis and game playing, show that our approach can reduce learning times by 99%.","sentences":["The goal of inductive logic programming (ILP) is to search for a logic program that generalises training examples and background knowledge.","We introduce an ILP approach that identifies minimal unsatisfiable subprograms (MUSPs).","We show that finding MUSPs allows us to efficiently and soundly prune the search space.","Our experiments on multiple domains, including program synthesis and game playing, show that our approach can reduce learning times by 99%."],"url":"http://arxiv.org/abs/2401.16383v1","category":"cs.LG"}
{"created":"2024-01-29 18:22:11","title":"A KDM-Based Approach for Architecture Conformance Checking in Adaptive Systems","abstract":"Adaptive Systems (ASs) are capable to monitor their behavior and make adjustments when quality goals are not achieved through the MAPE-K, a widely recognized reference model that offers abstractions for designing ASs. By making these abstractions evident in the system structure, numerous benefits emerge, particularly in terms of enhancing the architecture's maintenance and comprehensibility. However, it is observed that many existing ASs are not designed in accordance with MAPE-K, causing these abstractions to remain hidden in their architecture. To address this issue, Architectural Conformance Checking (ACC) emerges as a valuable technique for verifying whether the current architecture (CA) of a system adheres to the rules prescribed by the planned architecture (PA) or a reference model, such as MAPE-K. In this paper, we present REMEDY, a domain-specific approach that encompasses the specification of the planned adaptive architecture based on the MAPE-K reference model, the recovery of the current adaptive architecture, the conformance checking process, and architecture visualizations. Furthermore, our approach is specifically tailored for ASs, incorporating well-known rules from the MAPE-K model. The evaluation of the REMEDY DSL involves a comparison with a general-purpose DSL, and the results demonstrate improvements in productivity. REMEDY facilitates the identification and correction of architectural non-conformance issues, thereby enhancing the overall quality of adaptive systems.","sentences":["Adaptive Systems (ASs) are capable to monitor their behavior and make adjustments when quality goals are not achieved through the MAPE-K, a widely recognized reference model that offers abstractions for designing ASs.","By making these abstractions evident in the system structure, numerous benefits emerge, particularly in terms of enhancing the architecture's maintenance and comprehensibility.","However, it is observed that many existing ASs are not designed in accordance with MAPE-K, causing these abstractions to remain hidden in their architecture.","To address this issue, Architectural Conformance Checking (ACC) emerges as a valuable technique for verifying whether the current architecture (CA) of a system adheres to the rules prescribed by the planned architecture (PA) or a reference model, such as MAPE-K. In this paper, we present REMEDY, a domain-specific approach that encompasses the specification of the planned adaptive architecture based on the MAPE-K reference model, the recovery of the current adaptive architecture, the conformance checking process, and architecture visualizations.","Furthermore, our approach is specifically tailored for ASs, incorporating well-known rules from the MAPE-K model.","The evaluation of the REMEDY DSL involves a comparison with a general-purpose DSL, and the results demonstrate improvements in productivity.","REMEDY facilitates the identification and correction of architectural non-conformance issues, thereby enhancing the overall quality of adaptive systems."],"url":"http://arxiv.org/abs/2401.16382v1","category":"cs.SE"}
{"created":"2024-01-29 18:21:49","title":"Highly Efficient Encoding for Job-Shop Scheduling Problems and its Application on Quantum Computers","abstract":"Combinatorial optimization problems are considered to be an application, where quantum computing can have transformative impact. In the industrial context, job shop scheduling problems that aim at finding the optimal schedule for a set of jobs to be run on a set of machines are of immense interest. Here we introduce an efficient encoding of job shop scheduling problems, which requires much fewer bit-strings for counting all possible schedules than previously employed encodings. For problems consisting of $N$ jobs with $N$ operations, the number of required bit-strings is at least reduced by a factor $N / \\log_2(N)$ as compared to time indexed encodings. This is particularly beneficial for solving job shop scheduling problems on quantum computers, since much fewer qubits are needed to represent the problem. Our approach applies to the large class of flexible and usual job-shop scheduling problems, where operations can possibly be executed on multiple machines. Using variational quantum algorithms, we show that the encoding we introduce leads to significantly better performance of quantum algorithms than previously considered strategies. Importantly, the encoding we develop also enables significantly more compact classical representations and will therefore be highly useful even beyond applicability on quantum hardware.","sentences":["Combinatorial optimization problems are considered to be an application, where quantum computing can have transformative impact.","In the industrial context, job shop scheduling problems that aim at finding the optimal schedule for a set of jobs to be run on a set of machines are of immense interest.","Here we introduce an efficient encoding of job shop scheduling problems, which requires much fewer bit-strings for counting all possible schedules than previously employed encodings.","For problems consisting of $N$ jobs with $N$ operations, the number of required bit-strings is at least reduced by a factor $N / \\log_2(N)$ as compared to time indexed encodings.","This is particularly beneficial for solving job shop scheduling problems on quantum computers, since much fewer qubits are needed to represent the problem.","Our approach applies to the large class of flexible and usual job-shop scheduling problems, where operations can possibly be executed on multiple machines.","Using variational quantum algorithms, we show that the encoding we introduce leads to significantly better performance of quantum algorithms than previously considered strategies.","Importantly, the encoding we develop also enables significantly more compact classical representations and will therefore be highly useful even beyond applicability on quantum hardware."],"url":"http://arxiv.org/abs/2401.16381v1","category":"quant-ph"}
{"created":"2024-01-29 18:19:08","title":"Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling","abstract":"Large language models are trained on massive scrapes of the web, which are often unstructured, noisy, and poorly phrased. Current scaling laws show that learning from such data requires an abundance of both compute and data, which grows with the size of the model being trained. This is infeasible both because of the large compute costs and duration associated with pre-training, and the impending scarcity of high-quality data on the web. In this work, we propose Web Rephrase Augmented Pre-training ($\\textbf{WRAP}$) that uses an off-the-shelf instruction-tuned model prompted to paraphrase documents on the web in specific styles such as \"like Wikipedia\" or in \"question-answer format\" to jointly pre-train LLMs on real and synthetic rephrases. First, we show that using WRAP on the C4 dataset, which is naturally noisy, speeds up pre-training by $\\sim3x$. At the same pre-training compute budget, it improves perplexity by more than 10% on average across different subsets of the Pile, and improves zero-shot question answer accuracy across 13 tasks by more than 2%. Second, we investigate the impact of the re-phrasing style on the performance of the model, offering insights into how the composition of the training data can impact the performance of LLMs in OOD settings. Our gains are attributed to the fact that re-phrased synthetic data has higher utility than just real data because it (i) incorporates style diversity that closely reflects downstream evaluation style, and (ii) has higher 'quality' than web-scraped data.","sentences":["Large language models are trained on massive scrapes of the web, which are often unstructured, noisy, and poorly phrased.","Current scaling laws show that learning from such data requires an abundance of both compute and data, which grows with the size of the model being trained.","This is infeasible both because of the large compute costs and duration associated with pre-training, and the impending scarcity of high-quality data on the web.","In this work, we propose Web Rephrase Augmented Pre-training ($\\textbf{WRAP}$) that uses an off-the-shelf instruction-tuned model prompted to paraphrase documents on the web in specific styles such as \"like Wikipedia\" or in \"question-answer format\" to jointly pre-train LLMs on real and synthetic rephrases.","First, we show that using WRAP on the C4 dataset, which is naturally noisy, speeds up pre-training by $\\sim3x$.","At the same pre-training compute budget, it improves perplexity by more than 10% on average across different subsets of the Pile, and improves zero-shot question answer accuracy across 13 tasks by more than 2%.","Second, we investigate the impact of the re-phrasing style on the performance of the model, offering insights into how the composition of the training data can impact the performance of LLMs in OOD settings.","Our gains are attributed to the fact that re-phrased synthetic data has higher utility than just real data because it (i) incorporates style diversity that closely reflects downstream evaluation style, and (ii) has higher 'quality' than web-scraped data."],"url":"http://arxiv.org/abs/2401.16380v1","category":"cs.CL"}
{"created":"2024-01-29 18:18:11","title":"Decomposing dense matrices into dense Pauli tensors","abstract":"Decomposing a matrix into a weighted sum of Pauli strings is a common chore of the quantum computer scientist, whom is not easily discouraged by exponential scaling. But beware, a naive decomposition can be cubically more expensive than necessary! In this manuscript, we derive a fixed-memory, branchless algorithm to compute the inner product between a 2^N-by-2^N complex matrix and an N-term Pauli tensor in O(2^N) time, by leveraging the Gray code. Our scheme permits the embarrassingly parallel decomposition of a matrix into a weighted sum of Pauli strings in O(8^N) time. We implement our algorithm in Python, hosted open-source on Github, and benchmark against a recent state-of-the-art method called the \"PauliComposer\" which has an exponentially growing memory overhead, achieving speedups in the range of 1.5x to 5x for N < 8. Note that our scheme does not leverage sparsity, diagonality, Hermitivity or other properties of the input matrix which might otherwise enable optimised treatment in other methods. As such, our algorithm is well-suited to decomposition of dense, arbitrary, complex matrices which are expected dense in the Pauli basis, or for which the decomposed Pauli tensors are a priori unknown.","sentences":["Decomposing a matrix into a weighted sum of Pauli strings is a common chore of the quantum computer scientist, whom is not easily discouraged by exponential scaling.","But beware, a naive decomposition can be cubically more expensive than necessary!","In this manuscript, we derive a fixed-memory, branchless algorithm to compute the inner product between a 2^N-by-2^N complex matrix and an N-term Pauli tensor in O(2^N) time, by leveraging the Gray code.","Our scheme permits the embarrassingly parallel decomposition of a matrix into a weighted sum of Pauli strings in O(8^N) time.","We implement our algorithm in Python, hosted open-source on Github, and benchmark against a recent state-of-the-art method called the \"PauliComposer\" which has an exponentially growing memory overhead, achieving speedups in the range of 1.5x to 5x for N < 8.","Note that our scheme does not leverage sparsity, diagonality, Hermitivity or other properties of the input matrix which might otherwise enable optimised treatment in other methods.","As such, our algorithm is well-suited to decomposition of dense, arbitrary, complex matrices which are expected dense in the Pauli basis, or for which the decomposed Pauli tensors are a priori unknown."],"url":"http://arxiv.org/abs/2401.16378v1","category":"quant-ph"}
{"created":"2024-01-29 18:13:54","title":"Spot the Error: Non-autoregressive Graphic Layout Generation with Wireframe Locator","abstract":"Layout generation is a critical step in graphic design to achieve meaningful compositions of elements. Most previous works view it as a sequence generation problem by concatenating element attribute tokens (i.e., category, size, position). So far the autoregressive approach (AR) has achieved promising results, but is still limited in global context modeling and suffers from error propagation since it can only attend to the previously generated tokens. Recent non-autoregressive attempts (NAR) have shown competitive results, which provides a wider context range and the flexibility to refine with iterative decoding. However, current works only use simple heuristics to recognize erroneous tokens for refinement which is inaccurate. This paper first conducts an in-depth analysis to better understand the difference between the AR and NAR framework. Furthermore, based on our observation that pixel space is more sensitive in capturing spatial patterns of graphic layouts (e.g., overlap, alignment), we propose a learning-based locator to detect erroneous tokens which takes the wireframe image rendered from the generated layout sequence as input. We show that it serves as a complementary modality to the element sequence in object space and contributes greatly to the overall performance. Experiments on two public datasets show that our approach outperforms both AR and NAR baselines. Extensive studies further prove the effectiveness of different modules with interesting findings. Our code will be available at https://github.com/ffffatgoose/SpotError.","sentences":["Layout generation is a critical step in graphic design to achieve meaningful compositions of elements.","Most previous works view it as a sequence generation problem by concatenating element attribute tokens (i.e., category, size, position).","So far the autoregressive approach (AR) has achieved promising results, but is still limited in global context modeling and suffers from error propagation since it can only attend to the previously generated tokens.","Recent non-autoregressive attempts (NAR) have shown competitive results, which provides a wider context range and the flexibility to refine with iterative decoding.","However, current works only use simple heuristics to recognize erroneous tokens for refinement which is inaccurate.","This paper first conducts an in-depth analysis to better understand the difference between the AR and NAR framework.","Furthermore, based on our observation that pixel space is more sensitive in capturing spatial patterns of graphic layouts (e.g., overlap, alignment), we propose a learning-based locator to detect erroneous tokens which takes the wireframe image rendered from the generated layout sequence as input.","We show that it serves as a complementary modality to the element sequence in object space and contributes greatly to the overall performance.","Experiments on two public datasets show that our approach outperforms both AR and NAR baselines.","Extensive studies further prove the effectiveness of different modules with interesting findings.","Our code will be available at https://github.com/ffffatgoose/SpotError."],"url":"http://arxiv.org/abs/2401.16375v1","category":"cs.CV"}
{"created":"2024-01-29 18:13:43","title":"Analytic Model for Molecules Under Collective Vibrational Strong Coupling in Optical Cavities","abstract":"Analytical results are presented for a model system consisting of an ensemble of N molecules under vibrational strong coupling (VSC). The single bare molecular model is composed of one effective electron, which couples harmonically to multiple nuclei. A priori no harmonic approximation is imposed for the inter-nuclear interactions. Within the cavity Born-Oppenheimer partitioning, i.e., when assuming classical nuclei and displacement field coordinates, the dressed N-electron problem can be solved analytically in the dilute limit. In more detail, we present a self-consistent solution of the corresponding cavity-Hartree equations, which illustrates the relevance of the non-perturbative treatment of electronic screening effects under VSC. We exemplify our derivations for an ensemble of harmonic model CO2 molecules, which shows that common simplifications can introduce non-physical effects (e.g., a spurious coupling of the transverse field to the center-of-mass motion for neutral atoms). In addition, our self-consistent solution reveals a simple analytic expression for the cavity-induced red shift and the associated refractive index, which can be interpreted as a polarizability-dependent detuning of the cavity. Finally, we highlight that anharmonic intra-molecular interactions might become essential for the formation of local strong coupling effects within a molecular ensemble under collective VSC.","sentences":["Analytical results are presented for a model system consisting of an ensemble of N molecules under vibrational strong coupling (VSC).","The single bare molecular model is composed of one effective electron, which couples harmonically to multiple nuclei.","A priori no harmonic approximation is imposed for the inter-nuclear interactions.","Within the cavity Born-Oppenheimer partitioning, i.e., when assuming classical nuclei and displacement field coordinates, the dressed N-electron problem can be solved analytically in the dilute limit.","In more detail, we present a self-consistent solution of the corresponding cavity-Hartree equations, which illustrates the relevance of the non-perturbative treatment of electronic screening effects under VSC.","We exemplify our derivations for an ensemble of harmonic model CO2 molecules, which shows that common simplifications can introduce non-physical effects (e.g., a spurious coupling of the transverse field to the center-of-mass motion for neutral atoms).","In addition, our self-consistent solution reveals a simple analytic expression for the cavity-induced red shift and the associated refractive index, which can be interpreted as a polarizability-dependent detuning of the cavity.","Finally, we highlight that anharmonic intra-molecular interactions might become essential for the formation of local strong coupling effects within a molecular ensemble under collective VSC."],"url":"http://arxiv.org/abs/2401.16374v1","category":"quant-ph"}
{"created":"2024-01-29 18:12:32","title":"Bayesian optimization as a flexible and efficient design framework for sustainable process systems","abstract":"Bayesian optimization (BO) is a powerful technology for optimizing noisy expensive-to-evaluate black-box functions, with a broad range of real-world applications in science, engineering, economics, manufacturing, and beyond. In this paper, we provide an overview of recent developments, challenges, and opportunities in BO for design of next-generation process systems. After describing several motivating applications, we discuss how advanced BO methods have been developed to more efficiently tackle important problems in these applications. We conclude the paper with a summary of challenges and opportunities related to improving the quality of the probabilistic model, the choice of internal optimization procedure used to select the next sample point, and the exploitation of problem structure to improve sample efficiency.","sentences":["Bayesian optimization (BO) is a powerful technology for optimizing noisy expensive-to-evaluate black-box functions, with a broad range of real-world applications in science, engineering, economics, manufacturing, and beyond.","In this paper, we provide an overview of recent developments, challenges, and opportunities in BO for design of next-generation process systems.","After describing several motivating applications, we discuss how advanced BO methods have been developed to more efficiently tackle important problems in these applications.","We conclude the paper with a summary of challenges and opportunities related to improving the quality of the probabilistic model, the choice of internal optimization procedure used to select the next sample point, and the exploitation of problem structure to improve sample efficiency."],"url":"http://arxiv.org/abs/2401.16373v1","category":"cs.LG"}
{"created":"2024-01-29 18:11:52","title":"Duality between controllability and observability for target control and estimation in networks","abstract":"Controllability and observability are properties that establish the existence of full-state controllers and observers, respectively. The notions of output controllability and functional observability are generalizations that enable respectively the control and estimation of part of the state vector. These generalizations are of utmost importance in applications to high-dimensional systems, such as large-scale networks, in which only a target subset of variables (nodes) are sought to be controlled or estimated. Although the duality between controllability and observability is well established, the characterization of the duality between their generalized counterparts remains an outstanding problem. Here, we establish both the weak and the strong duality between output controllability and functional observability. Specifically, we show that functional observability of a system implies output controllability of a dual system (weak duality), and that under a certain condition the converse also holds (strong duality). As an application of the strong duality principle, we derive a necessary and sufficient condition for target control via static feedback. This allow us to establish a separation principle between the design of a feedback target controller and the design of a functional observer in closed-loop systems. These results generalize the well-known duality and separation principles in modern control theory.","sentences":["Controllability and observability are properties that establish the existence of full-state controllers and observers, respectively.","The notions of output controllability and functional observability are generalizations that enable respectively the control and estimation of part of the state vector.","These generalizations are of utmost importance in applications to high-dimensional systems, such as large-scale networks, in which only a target subset of variables (nodes) are sought to be controlled or estimated.","Although the duality between controllability and observability is well established, the characterization of the duality between their generalized counterparts remains an outstanding problem.","Here, we establish both the weak and the strong duality between output controllability and functional observability.","Specifically, we show that functional observability of a system implies output controllability of a dual system (weak duality), and that under a certain condition the converse also holds (strong duality).","As an application of the strong duality principle, we derive a necessary and sufficient condition for target control via static feedback.","This allow us to establish a separation principle between the design of a feedback target controller and the design of a functional observer in closed-loop systems.","These results generalize the well-known duality and separation principles in modern control theory."],"url":"http://arxiv.org/abs/2401.16372v1","category":"math.OC"}
{"created":"2024-01-29 18:10:01","title":"Mixed-Order Meshes through rp-adaptivity for Surface Fitting to Implicit Geometries","abstract":"Computational analysis with the finite element method requires geometrically accurate meshes. It is well known that high-order meshes can accurately capture curved surfaces with fewer degrees of freedom in comparison to low-order meshes. Existing techniques for high-order mesh generation typically output meshes with same polynomial order for all elements. However, high order elements away from curvilinear boundaries or interfaces increase the computational cost of the simulation without increasing geometric accuracy. In prior work, we have presented one such approach for generating body-fitted uniform-order meshes that takes a given mesh and morphs it to align with the surface of interest prescribed as the zero isocontour of a level-set function. We extend this method to generate mixed-order meshes such that curved surfaces of the domain are discretized with high-order elements, while low-order elements are used elsewhere. Numerical experiments demonstrate the robustness of the approach and show that it can be used to generate mixed-order meshes that are much more efficient than high uniform-order meshes. The proposed approach is purely algebraic, and extends to different types of elements (quadrilaterals/triangles/tetrahedron/hexahedra) in two- and three-dimensions.","sentences":["Computational analysis with the finite element method requires geometrically accurate meshes.","It is well known that high-order meshes can accurately capture curved surfaces with fewer degrees of freedom in comparison to low-order meshes.","Existing techniques for high-order mesh generation typically output meshes with same polynomial order for all elements.","However, high order elements away from curvilinear boundaries or interfaces increase the computational cost of the simulation without increasing geometric accuracy.","In prior work, we have presented one such approach for generating body-fitted uniform-order meshes that takes a given mesh and morphs it to align with the surface of interest prescribed as the zero isocontour of a level-set function.","We extend this method to generate mixed-order meshes such that curved surfaces of the domain are discretized with high-order elements, while low-order elements are used elsewhere.","Numerical experiments demonstrate the robustness of the approach and show that it can be used to generate mixed-order meshes that are much more efficient than high uniform-order meshes.","The proposed approach is purely algebraic, and extends to different types of elements (quadrilaterals/triangles/tetrahedron/hexahedra) in two- and three-dimensions."],"url":"http://arxiv.org/abs/2401.16369v1","category":"cs.MS"}
{"created":"2024-01-29 18:07:56","title":"TQCompressor: improving tensor decomposition methods in neural networks via permutations","abstract":"We introduce TQCompressor, a novel method for neural network model compression with improved tensor decompositions. We explore the challenges posed by the computational and storage demands of pre-trained language models in NLP tasks and propose a permutation-based enhancement to Kronecker decomposition. This enhancement makes it possible to reduce loss in model expressivity which is usually associated with factorization. We demonstrate this method applied to the GPT-2$_{small}$. The result of the compression is TQCompressedGPT-2 model, featuring 81 mln. parameters compared to 124 mln. in the GPT-2$_{small}$. We make TQCompressedGPT-2 publicly available. We further enhance the performance of the TQCompressedGPT-2 through a training strategy involving multi-step knowledge distillation, using only a 3.1% of the OpenWebText. TQCompressedGPT-2 surpasses DistilGPT-2 and KnGPT-2 in comparative evaluations, marking an advancement in the efficient and effective deployment of models in resource-constrained environments.","sentences":["We introduce TQCompressor, a novel method for neural network model compression with improved tensor decompositions.","We explore the challenges posed by the computational and storage demands of pre-trained language models in NLP tasks and propose a permutation-based enhancement to Kronecker decomposition.","This enhancement makes it possible to reduce loss in model expressivity which is usually associated with factorization.","We demonstrate this method applied to the GPT-2$_{small}$.","The result of the compression is TQCompressedGPT-2 model, featuring 81 mln.","parameters compared to 124 mln.","in the GPT-2$_{small}$. We make TQCompressedGPT-2 publicly available.","We further enhance the performance of the TQCompressedGPT-2 through a training strategy involving multi-step knowledge distillation, using only a 3.1% of the OpenWebText.","TQCompressedGPT-2 surpasses DistilGPT-2 and KnGPT-2 in comparative evaluations, marking an advancement in the efficient and effective deployment of models in resource-constrained environments."],"url":"http://arxiv.org/abs/2401.16367v1","category":"cs.LG"}
{"created":"2024-01-29 18:06:24","title":"Choiceless Polynomial Space","abstract":"Abstract State Machines (ASMs) provide a model of computations on structures rather than strings. Blass, Gurevich and Shelah showed that deterministic PTIME-bounded ASMs define the choiceless fragment of PTIME, but cannot capture PTIME. In this article deterministic PSPACE-bounded ASMs are introduced, and it is proven that they cannot capture PSPACE. The key for the proof is a characterisation by partial fixed-point formulae over the St\\\"ark/Nanchen logic for deterministic ASMs and a construction of transitive structures, in which such formulae must hold. This construction exploits that the decisive support theorem for choiceless polynomial time holds under slightly weaker assumptions.","sentences":["Abstract State Machines (ASMs) provide a model of computations on structures rather than strings.","Blass, Gurevich and Shelah showed that deterministic PTIME-bounded ASMs define the choiceless fragment of PTIME, but cannot capture PTIME.","In this article deterministic PSPACE-bounded ASMs are introduced, and it is proven that they cannot capture PSPACE.","The key for the proof is a characterisation by partial fixed-point formulae over the St\\\"ark/Nanchen logic for deterministic ASMs and a construction of transitive structures, in which such formulae must hold.","This construction exploits that the decisive support theorem for choiceless polynomial time holds under slightly weaker assumptions."],"url":"http://arxiv.org/abs/2401.16366v1","category":"cs.LO"}
{"created":"2024-01-29 18:02:18","title":"Quantum process matrices as images: new tools to design novel denoising methods","abstract":"Inferring a process matrix characterizing a quantum channel from experimental measure- ments is a key issue of quantum information. Sometimes the noise affecting the measured counts brings to matrices very different from the expected ones and the mainly used es- timation procedure, i.e. the maximum likelihood estimation (MLE), is also characterized by several drawbacks. To lower the noise could be necessary to increase the experimental resources, e.g. time for each measurement. In this paper, an alternative procedure, based on suitable Neural Networks, has been implemented and optimized to obtain a denoised process matrix and this approach has been tested with a specific quantum channel, i.e. a Control Phase. This promising method relies on the analogy that can be established between the elements of a process matrix and the pixels of an im","sentences":["Inferring a process matrix characterizing a quantum channel from experimental measure- ments is a key issue of quantum information.","Sometimes the noise affecting the measured counts brings to matrices very different from the expected ones and the mainly used es- timation procedure, i.e. the maximum likelihood estimation (MLE), is also characterized by several drawbacks.","To lower the noise could be necessary to increase the experimental resources, e.g. time for each measurement.","In this paper, an alternative procedure, based on suitable Neural Networks, has been implemented and optimized to obtain a denoised process matrix and this approach has been tested with a specific quantum channel, i.e. a Control Phase.","This promising method relies on the analogy that can be established between the elements of a process matrix and the pixels of an im"],"url":"http://arxiv.org/abs/2401.16362v1","category":"quant-ph"}
{"created":"2024-01-29 18:01:40","title":"$\\ell$-Proca stars","abstract":"Initially applied to the scalar case, we extend the applicability of the multi-field generalization with angular momentum of bosonic stars to the vector case, in order to obtain new configurations that generalize the one-field spherical Proca stars. These new objects, which we call $\\ell$-Proca stars, arise as stationary and spherically symmetric bosonic stars solutions of the Einstein-(multi)Proca system, whose matter content is formed by an arbitrary odd number of $2\\ell+1$ of complex Proca fields with the same mass, time-frequency, radial profile and angular momentum number $\\ell$. We analyze the system of constraint and evolution radial equations for the matter content to show the consistency of our proposal, and obtain numerically the ground states of these new solutions for the first few values of $\\ell$ using spectral methods.","sentences":["Initially applied to the scalar case, we extend the applicability of the multi-field generalization with angular momentum of bosonic stars to the vector case, in order to obtain new configurations that generalize the one-field spherical Proca stars.","These new objects, which we call $\\ell$-Proca stars, arise as stationary and spherically symmetric bosonic stars solutions of the Einstein-(multi)Proca system, whose matter content is formed by an arbitrary odd number of $2\\ell+1$ of complex Proca fields with the same mass, time-frequency, radial profile and angular momentum number $\\ell$. We analyze the system of constraint and evolution radial equations for the matter content to show the consistency of our proposal, and obtain numerically the ground states of these new solutions for the first few values of $\\ell$ using spectral methods."],"url":"http://arxiv.org/abs/2401.16360v1","category":"gr-qc"}
{"created":"2024-01-29 18:00:50","title":"Reference Coverage Analysis of OpenAlex compared to Web of Science and Scopus","abstract":"OpenAlex is a promising open source of scholarly metadata, and competitor to the established proprietary sources, the Web of Science and Scopus. As OpenAlex provides its data freely and openly, it permits researchers to perform bibliometric studies that can be reproduced in the community without licensing barriers. However, as OpenAlex is a rapidly evolving source and the data contained within is expanding and also quickly changing, the question naturally arises as to the trustworthiness of its data. In this empirical paper, we will study the reference and metadata coverage within each database and compare them with each other to help address this open question in bibliometrics. In our large-scale study, we demonstrate that, when restricted to a cleaned dataset of 16,788,282 recent publications shared by all three databases, OpenAlex has average reference numbers comparable to both Web of Science and Scopus. We also demonstrate that the comparison of other core metadata covered by OpenAlex shows mixed results, with OpenAlex capturing more ORCID identifiers, fewer abstracts and a similar number of Open Access information per article when compared to both Web of Science and Scopus.","sentences":["OpenAlex is a promising open source of scholarly metadata, and competitor to the established proprietary sources, the Web of Science and Scopus.","As OpenAlex provides its data freely and openly, it permits researchers to perform bibliometric studies that can be reproduced in the community without licensing barriers.","However, as OpenAlex is a rapidly evolving source and the data contained within is expanding and also quickly changing, the question naturally arises as to the trustworthiness of its data.","In this empirical paper, we will study the reference and metadata coverage within each database and compare them with each other to help address this open question in bibliometrics.","In our large-scale study, we demonstrate that, when restricted to a cleaned dataset of 16,788,282 recent publications shared by all three databases, OpenAlex has average reference numbers comparable to both Web of Science and Scopus.","We also demonstrate that the comparison of other core metadata covered by OpenAlex shows mixed results, with OpenAlex capturing more ORCID identifiers, fewer abstracts and a similar number of Open Access information per article when compared to both Web of Science and Scopus."],"url":"http://arxiv.org/abs/2401.16359v1","category":"cs.DL"}
{"created":"2024-01-29 17:59:26","title":"cDVGAN: One Flexible Model for Multi-class Gravitational Wave Signal and Glitch Generation","abstract":"Simulating realistic time-domain observations of gravitational waves (GWs) and GW detector glitches can help in advancing GW data analysis. Simulated data can be used in downstream tasks by augmenting datasets for signal searches, balancing data sets for machine learning, and validating detection schemes. In this work, we present Conditional Derivative GAN (cDVGAN), a novel conditional model in the Generative Adversarial Network framework for simulating multiple classes of time-domain observations that represent gravitational waves (GWs) and detector glitches. cDVGAN can also generate generalized hybrid samples that span the variation between classes through interpolation in the conditioned class vector. cDVGAN introduces an additional player into the typical 2-player adversarial game of GANs, where an auxiliary discriminator analyzes the first-order derivative time-series. Our results show that this provides synthetic data that better captures the features of the original data. cDVGAN conditions on three classes, two denoised from LIGO blip and tomte glitch events from its 3rd observing run (O3), and the third representing binary black hole (BBH) mergers. Our proposed cDVGAN outperforms 4 different baseline GAN models in replicating the features of the three classes. Specifically, our experiments show that training convolutional neural networks (CNNs) with our cDVGAN-generated data improves the detection of samples embedded in detector noise beyond the synthetic data from other state-of-the-art GAN models. Our best synthetic dataset yields as much as a 4.2% increase in area-under-the-curve (AUC) performance compared to synthetic datasets from baseline GANs. Moreover, training the CNN with hybrid samples from our cDVGAN outperforms CNNs trained only on the standard classes, when identifying real samples embedded in LIGO detector background (4% AUC improvement for cDVGAN).","sentences":["Simulating realistic time-domain observations of gravitational waves (GWs) and GW detector glitches can help in advancing GW data analysis.","Simulated data can be used in downstream tasks by augmenting datasets for signal searches, balancing data sets for machine learning, and validating detection schemes.","In this work, we present Conditional Derivative GAN (cDVGAN), a novel conditional model in the Generative Adversarial Network framework for simulating multiple classes of time-domain observations that represent gravitational waves (GWs) and detector glitches.","cDVGAN can also generate generalized hybrid samples that span the variation between classes through interpolation in the conditioned class vector.","cDVGAN introduces an additional player into the typical 2-player adversarial game of GANs, where an auxiliary discriminator analyzes the first-order derivative time-series.","Our results show that this provides synthetic data that better captures the features of the original data.","cDVGAN conditions on three classes, two denoised from LIGO blip and tomte glitch events from its 3rd observing run (O3), and the third representing binary black hole (BBH) mergers.","Our proposed cDVGAN outperforms 4 different baseline GAN models in replicating the features of the three classes.","Specifically, our experiments show that training convolutional neural networks (CNNs) with our cDVGAN-generated data improves the detection of samples embedded in detector noise beyond the synthetic data from other state-of-the-art GAN models.","Our best synthetic dataset yields as much as a 4.2% increase in area-under-the-curve (AUC) performance compared to synthetic datasets from baseline GANs.","Moreover, training the CNN with hybrid samples from our cDVGAN outperforms CNNs trained only on the standard classes, when identifying real samples embedded in LIGO detector background (4% AUC improvement for cDVGAN)."],"url":"http://arxiv.org/abs/2401.16356v1","category":"physics.ins-det"}
{"created":"2024-01-29 17:59:19","title":"PathMMU: A Massive Multimodal Expert-Level Benchmark for Understanding and Reasoning in Pathology","abstract":"The emergence of large multimodal models has unlocked remarkable potential in AI, particularly in pathology. However, the lack of specialized, high-quality benchmark impeded their development and precise evaluation. To address this, we introduce PathMMU, the largest and highest-quality expert-validated pathology benchmark for LMMs. It comprises 33,573 multimodal multi-choice questions and 21,599 images from various sources, and an explanation for the correct answer accompanies each question. The construction of PathMMU capitalizes on the robust capabilities of GPT-4V, utilizing approximately 30,000 gathered image-caption pairs to generate Q\\&As. Significantly, to maximize PathMMU's authority, we invite six pathologists to scrutinize each question under strict standards in PathMMU's validation and test sets, while simultaneously setting an expert-level performance benchmark for PathMMU. We conduct extensive evaluations, including zero-shot assessments of 14 open-sourced and three closed-sourced LMMs and their robustness to image corruption. We also fine-tune representative LMMs to assess their adaptability to PathMMU. The empirical findings indicate that advanced LMMs struggle with the challenging PathMMU benchmark, with the top-performing LMM, GPT-4V, achieving only a 51.7\\% zero-shot performance, significantly lower than the 71.4\\% demonstrated by human pathologists. After fine-tuning, even open-sourced LMMs can surpass GPT-4V with a performance of over 60\\%, but still fall short of the expertise shown by pathologists. We hope that the PathMMU will offer valuable insights and foster the development of more specialized, next-generation LLMs for pathology.","sentences":["The emergence of large multimodal models has unlocked remarkable potential in AI, particularly in pathology.","However, the lack of specialized, high-quality benchmark impeded their development and precise evaluation.","To address this, we introduce PathMMU, the largest and highest-quality expert-validated pathology benchmark for LMMs.","It comprises 33,573 multimodal multi-choice questions and 21,599 images from various sources, and an explanation for the correct answer accompanies each question.","The construction of PathMMU capitalizes on the robust capabilities of GPT-4V, utilizing approximately 30,000 gathered image-caption pairs to generate Q\\&As.","Significantly, to maximize PathMMU's authority, we invite six pathologists to scrutinize each question under strict standards in PathMMU's validation and test sets, while simultaneously setting an expert-level performance benchmark for PathMMU.","We conduct extensive evaluations, including zero-shot assessments of 14 open-sourced and three closed-sourced LMMs and their robustness to image corruption.","We also fine-tune representative LMMs to assess their adaptability to PathMMU.","The empirical findings indicate that advanced LMMs struggle with the challenging PathMMU benchmark, with the top-performing LMM, GPT-4V, achieving only a 51.7\\% zero-shot performance, significantly lower than the 71.4\\% demonstrated by human pathologists.","After fine-tuning, even open-sourced LMMs can surpass GPT-4V with a performance of over 60\\%, but still fall short of the expertise shown by pathologists.","We hope that the PathMMU will offer valuable insights and foster the development of more specialized, next-generation LLMs for pathology."],"url":"http://arxiv.org/abs/2401.16355v1","category":"cs.CV"}
{"created":"2024-01-29 17:56:45","title":"Empirical and Theoretical Analysis of Liquid Staking Protocols","abstract":"Liquid staking has become the largest category of decentralized finance protocols in terms of total value locked. However, few studies exist on its implementation designs or underlying risks. The liquid staking protocols allow for earning staking rewards without the disadvantage of locking the capital at the validators. Yet, they are seen by some as a threat to the Proof-of-Stake blockchain security.   This paper is the first work that classifies liquid staking implementations. It analyzes the historical performance of major liquid staking tokens in comparison to the traditional staking for the largest Proof-of-Stake blockchains. Furthermore, the research investigates the impact of centralization, maximum extractable value and the migration of Ethereum from Proof-of-Work to Proof-of-Stake on the tokens' performance. Examining the tracking error of the liquid stacking providers to the staking rewards shows that they are persistent and cannot be explained by macro-variables of the currency, such as the variance or return.","sentences":["Liquid staking has become the largest category of decentralized finance protocols in terms of total value locked.","However, few studies exist on its implementation designs or underlying risks.","The liquid staking protocols allow for earning staking rewards without the disadvantage of locking the capital at the validators.","Yet, they are seen by some as a threat to the Proof-of-Stake blockchain security.   ","This paper is the first work that classifies liquid staking implementations.","It analyzes the historical performance of major liquid staking tokens in comparison to the traditional staking for the largest Proof-of-Stake blockchains.","Furthermore, the research investigates the impact of centralization, maximum extractable value and the migration of Ethereum from Proof-of-Work to Proof-of-Stake on the tokens' performance.","Examining the tracking error of the liquid stacking providers to the staking rewards shows that they are persistent and cannot be explained by macro-variables of the currency, such as the variance or return."],"url":"http://arxiv.org/abs/2401.16353v1","category":"cs.CR"}
{"created":"2024-01-29 17:56:42","title":"Adversarial Training on Purification (AToP): Advancing Both Robustness and Generalization","abstract":"The deep neural networks are known to be vulnerable to well-designed adversarial attacks. The most successful defense technique based on adversarial training (AT) can achieve optimal robustness against particular attacks but cannot generalize well to unseen attacks. Another effective defense technique based on adversarial purification (AP) can enhance generalization but cannot achieve optimal robustness. Meanwhile, both methods share one common limitation on the degraded standard accuracy. To mitigate these issues, we propose a novel framework called Adversarial Training on Purification (AToP), which comprises two components: perturbation destruction by random transforms (RT) and purifier model fine-tuned (FT) by adversarial loss. RT is essential to avoid overlearning to known attacks resulting in the robustness generalization to unseen attacks and FT is essential for the improvement of robustness. To evaluate our method in an efficient and scalable way, we conduct extensive experiments on CIFAR-10, CIFAR-100, and ImageNette to demonstrate that our method achieves state-of-the-art results and exhibits generalization ability against unseen attacks.","sentences":["The deep neural networks are known to be vulnerable to well-designed adversarial attacks.","The most successful defense technique based on adversarial training (AT) can achieve optimal robustness against particular attacks but cannot generalize well to unseen attacks.","Another effective defense technique based on adversarial purification (AP) can enhance generalization but cannot achieve optimal robustness.","Meanwhile, both methods share one common limitation on the degraded standard accuracy.","To mitigate these issues, we propose a novel framework called Adversarial Training on Purification (AToP), which comprises two components: perturbation destruction by random transforms (RT) and purifier model fine-tuned (FT) by adversarial loss.","RT is essential to avoid overlearning to known attacks resulting in the robustness generalization to unseen attacks and FT is essential for the improvement of robustness.","To evaluate our method in an efficient and scalable way, we conduct extensive experiments on CIFAR-10, CIFAR-100, and ImageNette to demonstrate that our method achieves state-of-the-art results and exhibits generalization ability against unseen attacks."],"url":"http://arxiv.org/abs/2401.16352v1","category":"cs.CV"}
{"created":"2024-01-29 17:56:15","title":"FedFair^3: Unlocking Threefold Fairness in Federated Learning","abstract":"Federated Learning (FL) is an emerging paradigm in machine learning without exposing clients' raw data. In practical scenarios with numerous clients, encouraging fair and efficient client participation in federated learning is of utmost importance, which is also challenging given the heterogeneity in data distribution and device properties. Existing works have proposed different client-selection methods that consider fairness; however, they fail to select clients with high utilities while simultaneously achieving fair accuracy levels. In this paper, we propose a fair client-selection approach that unlocks threefold fairness in federated learning. In addition to having a fair client-selection strategy, we enforce an equitable number of rounds for client participation and ensure a fair accuracy distribution over the clients. The experimental results demonstrate that FedFair^3, in comparison to the state-of-the-art baselines, achieves 18.15% less accuracy variance on the IID data and 54.78% on the non-IID data, without decreasing the global accuracy. Furthermore, it shows 24.36% less wall-clock training time on average.","sentences":["Federated Learning (FL) is an emerging paradigm in machine learning without exposing clients' raw data.","In practical scenarios with numerous clients, encouraging fair and efficient client participation in federated learning is of utmost importance, which is also challenging given the heterogeneity in data distribution and device properties.","Existing works have proposed different client-selection methods that consider fairness; however, they fail to select clients with high utilities while simultaneously achieving fair accuracy levels.","In this paper, we propose a fair client-selection approach that unlocks threefold fairness in federated learning.","In addition to having a fair client-selection strategy, we enforce an equitable number of rounds for client participation and ensure a fair accuracy distribution over the clients.","The experimental results demonstrate that FedFair^3, in comparison to the state-of-the-art baselines, achieves 18.15% less accuracy variance on the IID data and 54.78% on the non-IID data, without decreasing the global accuracy.","Furthermore, it shows 24.36% less wall-clock training time on average."],"url":"http://arxiv.org/abs/2401.16350v1","category":"cs.LG"}
{"created":"2024-01-29 17:54:04","title":"Beyond Automated Evaluation Metrics: Evaluating Topic Models On Practical Social Science Content Analysis Tasks","abstract":"Topic models are a popular tool for understanding text collections, but their evaluation has been a point of contention. Automated evaluation metrics such as coherence are often used, however, their validity has been questioned for neural topic models (NTMs) and can overlook the benefits of a model in real world applications. To this end, we conduct the first evaluation of neural, supervised and classical topic models in an interactive task based setting. We combine topic models with a classifier and test their ability to help humans conduct content analysis and document annotation. From simulated, real user and expert pilot studies, the Contextual Neural Topic Model does the best on cluster evaluation metrics and human evaluations; however, LDA is competitive with two other NTMs under our simulated experiment and user study results, contrary to what coherence scores suggest. We show that current automated metrics do not provide a complete picture of topic modeling capabilities, but the right choice of NTMs can be better than classical models on practical tasks.","sentences":["Topic models are a popular tool for understanding text collections, but their evaluation has been a point of contention.","Automated evaluation metrics such as coherence are often used, however, their validity has been questioned for neural topic models (NTMs) and can overlook the benefits of a model in real world applications.","To this end, we conduct the first evaluation of neural, supervised and classical topic models in an interactive task based setting.","We combine topic models with a classifier and test their ability to help humans conduct content analysis and document annotation.","From simulated, real user and expert pilot studies, the Contextual Neural Topic Model does the best on cluster evaluation metrics and human evaluations; however, LDA is competitive with two other NTMs under our simulated experiment and user study results, contrary to what coherence scores suggest.","We show that current automated metrics do not provide a complete picture of topic modeling capabilities, but the right choice of NTMs can be better than classical models on practical tasks."],"url":"http://arxiv.org/abs/2401.16348v1","category":"cs.CL"}
{"created":"2024-01-29 17:53:25","title":"Cross-Modal Coordination Across a Diverse Set of Input Modalities","abstract":"Cross-modal retrieval is the task of retrieving samples of a given modality by using queries of a different one. Due to the wide range of practical applications, the problem has been mainly focused on the vision and language case, e.g. text to image retrieval, where models like CLIP have proven effective in solving such tasks. The dominant approach to learning such coordinated representations consists of projecting them onto a common space where matching views stay close and those from non-matching pairs are pushed away from each other. Although this cross-modal coordination has been applied also to other pairwise combinations, extending it to an arbitrary number of diverse modalities is a problem that has not been fully explored in the literature. In this paper, we propose two different approaches to the problem. The first is based on an extension of the CLIP contrastive objective to an arbitrary number of input modalities, while the second departs from the contrastive formulation and tackles the coordination problem by regressing the cross-modal similarities towards a target that reflects two simple and intuitive constraints of the cross-modal retrieval task. We run experiments on two different datasets, over different combinations of input modalities and show that the approach is not only simple and effective but also allows for tackling the retrieval problem in novel ways. Besides capturing a more diverse set of pair-wise interactions, we show that we can use the learned representations to improve retrieval performance by combining the embeddings from two or more such modalities.","sentences":["Cross-modal retrieval is the task of retrieving samples of a given modality by using queries of a different one.","Due to the wide range of practical applications, the problem has been mainly focused on the vision and language case, e.g. text to image retrieval, where models like CLIP have proven effective in solving such tasks.","The dominant approach to learning such coordinated representations consists of projecting them onto a common space where matching views stay close and those from non-matching pairs are pushed away from each other.","Although this cross-modal coordination has been applied also to other pairwise combinations, extending it to an arbitrary number of diverse modalities is a problem that has not been fully explored in the literature.","In this paper, we propose two different approaches to the problem.","The first is based on an extension of the CLIP contrastive objective to an arbitrary number of input modalities, while the second departs from the contrastive formulation and tackles the coordination problem by regressing the cross-modal similarities towards a target that reflects two simple and intuitive constraints of the cross-modal retrieval task.","We run experiments on two different datasets, over different combinations of input modalities and show that the approach is not only simple and effective but also allows for tackling the retrieval problem in novel ways.","Besides capturing a more diverse set of pair-wise interactions, we show that we can use the learned representations to improve retrieval performance by combining the embeddings from two or more such modalities."],"url":"http://arxiv.org/abs/2401.16347v1","category":"cs.CV"}
{"created":"2024-01-29 17:48:51","title":"Quantization of Einstein-Cartan theory in the first order form","abstract":"We consider the Einstein-Cartan theory with the tetrad $e_{\\mu}^{a}$ and spin connection $\\omega_{\\mu ab}$ taken as being independent fields. Diffeomorphism invariance and local Lorentz invariance result in there being two distinct gauge transformations in this approach, and consequently two ghost fields arise when employing the usual Faddeev-Popov quantization procedure. Our choice of gauge fixing retains the gauge invariances of the background field. We show that the gauge algebra is closed even in the presence of torsion, and the resulting BRST invariance can be found for the effective action. We also derive the Slavnov-Taylor identities, which reflect the BRST symmetries of this theory.","sentences":["We consider the Einstein-Cartan theory with the tetrad $e_{\\mu}^{a}$ and spin connection $\\omega_{\\mu ab}$ taken as being independent fields.","Diffeomorphism invariance and local Lorentz invariance result in there being two distinct gauge transformations in this approach, and consequently two ghost fields arise when employing the usual Faddeev-Popov quantization procedure.","Our choice of gauge fixing retains the gauge invariances of the background field.","We show that the gauge algebra is closed even in the presence of torsion, and the resulting BRST invariance can be found for the effective action.","We also derive the Slavnov-Taylor identities, which reflect the BRST symmetries of this theory."],"url":"http://arxiv.org/abs/2401.16343v1","category":"hep-th"}
{"created":"2024-01-29 17:48:12","title":"On Achievable Rates for the Shotgun Sequencing Channel with Erasures","abstract":"In the shotgun sequencing channel, the input sequence (possibly, a long DNA sequence composed of nucleotide bases) is read into multiple fragments (called `reads') of much shorter lengths. In the context of DNA data storage, the capacity of this channel was identified in a recent work, assuming that the reads themselves are noiseless substrings of the original sequence. Modern shotgun sequencers however also output quality scores for each base read, indicating the confidence in its identification. Bases with low quality scores can be considered to be erased. Motivated by this, we consider the shotgun sequencing channel with erasures, where each symbol in any read can be independently erased with some probability $\\delta$. We identify achievable rates for this channel, using a random code construction and a decoder that uses typicality-like arguments to merge the reads.","sentences":["In the shotgun sequencing channel, the input sequence (possibly, a long DNA sequence composed of nucleotide bases) is read into multiple fragments (called `reads') of much shorter lengths.","In the context of DNA data storage, the capacity of this channel was identified in a recent work, assuming that the reads themselves are noiseless substrings of the original sequence.","Modern shotgun sequencers however also output quality scores for each base read, indicating the confidence in its identification.","Bases with low quality scores can be considered to be erased.","Motivated by this, we consider the shotgun sequencing channel with erasures, where each symbol in any read can be independently erased with some probability $\\delta$.","We identify achievable rates for this channel, using a random code construction and a decoder that uses typicality-like arguments to merge the reads."],"url":"http://arxiv.org/abs/2401.16342v1","category":"cs.IT"}
{"created":"2024-01-29 17:47:07","title":"S-HIDRA: A blockchain and SDN domain-based architecture to orchestrate fog computing environments","abstract":"Fog computing arises as a complement to cloud computing where computing and storage are provided in a decentralized way rather than the centralized approach of the cloud paradigm. In addition, blockchain provides a decentralized and immutable ledger which can provide support for running arbitrary logic thanks to smart contracts. These facts can lead to harness smart contracts on blockchain as the basis for a decentralized, autonomous, and resilient orchestrator for the resources in the fog. However, the potentially vast amount of geographically distributed fog nodes may threaten the feasibility of the orchestration. On the other hand, fog nodes can exhibit highly dynamic workloads which may result in the orchestrator redistributing the services among them. Thus, there is also a need to dynamically support the network connections to those services independently of their location. Software Defined Networking (SDN) can be integrated within the orchestrator to carry out a seamless service management. To tackle both aforementioned issues, the S-HIDRA architecture is proposed. It integrates SDN support within a blockchain-based orchestrator of container-based services for fog environments, in order to provide low network latency and high service availability. Also, a domain-based architecture is outlined \\marev{as potential scenario} to address the geographic distributed nature of fog environments. Results obtained from a proof-of-concept implementation assess the required functionality for S-HIDRA.","sentences":["Fog computing arises as a complement to cloud computing where computing and storage are provided in a decentralized way rather than the centralized approach of the cloud paradigm.","In addition, blockchain provides a decentralized and immutable ledger which can provide support for running arbitrary logic thanks to smart contracts.","These facts can lead to harness smart contracts on blockchain as the basis for a decentralized, autonomous, and resilient orchestrator for the resources in the fog.","However, the potentially vast amount of geographically distributed fog nodes may threaten the feasibility of the orchestration.","On the other hand, fog nodes can exhibit highly dynamic workloads which may result in the orchestrator redistributing the services among them.","Thus, there is also a need to dynamically support the network connections to those services independently of their location.","Software Defined Networking (SDN) can be integrated within the orchestrator to carry out a seamless service management.","To tackle both aforementioned issues, the S-HIDRA architecture is proposed.","It integrates SDN support within a blockchain-based orchestrator of container-based services for fog environments, in order to provide low network latency and high service availability.","Also, a domain-based architecture is outlined \\marev{as potential scenario} to address the geographic distributed nature of fog environments.","Results obtained from a proof-of-concept implementation assess the required functionality for S-HIDRA."],"url":"http://arxiv.org/abs/2401.16341v1","category":"cs.NI"}
{"created":"2024-01-29 17:46:18","title":"The role of library versions in Developer-ChatGPT conversations","abstract":"The latest breakthroughs in large language models (LLM) have empowered software development tools, such as ChatGPT, to aid developers in complex tasks. Developers use ChatGPT to write code, review code changes, and even debug their programs. In these interactions, ChatGPT often recommends code snippets that depend on external libraries. However, code from libraries changes over time, invalidating a once-correct code snippet and making it difficult to reuse recommended code.   In this study, we analyze DevGPT, a dataset of more than 4,000 Developer-ChatGPT interactions, to understand the role of library versions in code-related conversations. We quantify how often library version constraints are mentioned in code-related conversations and when ChatGPT recommends the installation of specific libraries. Our findings show that, albeit to constantly recommend and analyze code with external dependencies, library version constraints only appear in 9% of the conversations. In the majority of conversations, the version constraints are prompted by users (as opposed to being specified by ChatGPT) as a method for receiving better quality responses. Moreover, we study how library version constraints are used in the conversation through qualitative methods, identifying several potential problems that warrant further research.","sentences":["The latest breakthroughs in large language models (LLM) have empowered software development tools, such as ChatGPT, to aid developers in complex tasks.","Developers use ChatGPT to write code, review code changes, and even debug their programs.","In these interactions, ChatGPT often recommends code snippets that depend on external libraries.","However, code from libraries changes over time, invalidating a once-correct code snippet and making it difficult to reuse recommended code.   ","In this study, we analyze DevGPT, a dataset of more than 4,000 Developer-ChatGPT interactions, to understand the role of library versions in code-related conversations.","We quantify how often library version constraints are mentioned in code-related conversations and when ChatGPT recommends the installation of specific libraries.","Our findings show that, albeit to constantly recommend and analyze code with external dependencies, library version constraints only appear in 9% of the conversations.","In the majority of conversations, the version constraints are prompted by users (as opposed to being specified by ChatGPT) as a method for receiving better quality responses.","Moreover, we study how library version constraints are used in the conversation through qualitative methods, identifying several potential problems that warrant further research."],"url":"http://arxiv.org/abs/2401.16340v1","category":"cs.SE"}
{"created":"2024-01-29 17:45:23","title":"SAT-CEP-monitor: An air quality monitoring software architecture combining complex event processing with satellite remote sensing","abstract":"Air pollution is a major problem today that causes serious damage to human health. Urban areas are the most affected by the degradation of air quality caused by anthropogenic gas emissions. Although there are multiple proposals for air quality monitoring, in most cases, two limitations are imposed: the impossibility of processing data in Near Real-Time (NRT) for remote sensing approaches and the impossibility of reaching areas of limited accessibility or low network coverage for ground data approaches. We propose a software architecture that efficiently combines complex event processing with remote sensing data from various satellite sensors to monitor air quality in NRT, giving support to decision-makers. We illustrate the proposed solution by calculating the air quality levels for several areas of Morocco and Spain, extracting and processing satellite information in NRT. This study also validates the air quality measured by ground stations and satellite sensor data.","sentences":["Air pollution is a major problem today that causes serious damage to human health.","Urban areas are the most affected by the degradation of air quality caused by anthropogenic gas emissions.","Although there are multiple proposals for air quality monitoring, in most cases, two limitations are imposed: the impossibility of processing data in Near Real-Time (NRT) for remote sensing approaches and the impossibility of reaching areas of limited accessibility or low network coverage for ground data approaches.","We propose a software architecture that efficiently combines complex event processing with remote sensing data from various satellite sensors to monitor air quality in NRT, giving support to decision-makers.","We illustrate the proposed solution by calculating the air quality levels for several areas of Morocco and Spain, extracting and processing satellite information in NRT.","This study also validates the air quality measured by ground stations and satellite sensor data."],"url":"http://arxiv.org/abs/2401.16339v1","category":"cs.DC"}
{"created":"2024-01-29 17:45:02","title":"Curriculum-Based Reinforcement Learning for Quadrupedal Jumping: A Reference-free Design","abstract":"Deep reinforcement learning (DRL) has emerged as a promising solution to mastering explosive and versatile quadrupedal jumping skills. However, current DRL-based frameworks usually rely on well-defined reference trajectories, which are obtained by capturing animal motions or transferring experience from existing controllers. This work explores the possibility of learning dynamic jumping without imitating a reference trajectory. To this end, we incorporate a curriculum design into DRL so as to accomplish challenging tasks progressively. Starting from a vertical in-place jump, we then generalize the learned policy to forward and diagonal jumps and, finally, learn to jump across obstacles. Conditioned on the desired landing location, orientation, and obstacle dimensions, the proposed approach contributes to a wide range of jumping motions, including omnidirectional jumping and robust jumping, alleviating the effort to extract references in advance. Particularly, without constraints from the reference motion, a 90cm forward jump is achieved, exceeding previous records for similar robots reported in the existing literature. Additionally, continuous jumping on the soft grassy floor is accomplished, even when it is not encountered in the training stage. A supplementary video showing our results can be found at https://youtu.be/nRaMCrwU5X8 .","sentences":["Deep reinforcement learning (DRL) has emerged as a promising solution to mastering explosive and versatile quadrupedal jumping skills.","However, current DRL-based frameworks usually rely on well-defined reference trajectories, which are obtained by capturing animal motions or transferring experience from existing controllers.","This work explores the possibility of learning dynamic jumping without imitating a reference trajectory.","To this end, we incorporate a curriculum design into DRL so as to accomplish challenging tasks progressively.","Starting from a vertical in-place jump, we then generalize the learned policy to forward and diagonal jumps and, finally, learn to jump across obstacles.","Conditioned on the desired landing location, orientation, and obstacle dimensions, the proposed approach contributes to a wide range of jumping motions, including omnidirectional jumping and robust jumping, alleviating the effort to extract references in advance.","Particularly, without constraints from the reference motion, a 90cm forward jump is achieved, exceeding previous records for similar robots reported in the existing literature.","Additionally, continuous jumping on the soft grassy floor is accomplished, even when it is not encountered in the training stage.","A supplementary video showing our results can be found at https://youtu.be/nRaMCrwU5X8 ."],"url":"http://arxiv.org/abs/2401.16337v1","category":"cs.RO"}
{"created":"2024-01-29 17:43:42","title":"Iterative Data Smoothing: Mitigating Reward Overfitting and Overoptimization in RLHF","abstract":"Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique that aligns language models closely with human-centric values. The initial phase of RLHF involves learning human values using a reward model from ranking data. It is observed that the performance of the reward model degrades after one epoch of training, and optimizing too much against the learned reward model eventually hinders the true objective. This paper delves into these issues, leveraging the theoretical insights to design improved reward learning algorithm termed 'Iterative Data Smoothing' (IDS). The core idea is that during each training epoch, we not only update the model with the data, but also update the date using the model, replacing hard labels with soft labels. Our empirical findings highlight the superior performance of this approach over the traditional methods.","sentences":["Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique that aligns language models closely with human-centric values.","The initial phase of RLHF involves learning human values using a reward model from ranking data.","It is observed that the performance of the reward model degrades after one epoch of training, and optimizing too much against the learned reward model eventually hinders the true objective.","This paper delves into these issues, leveraging the theoretical insights to design improved reward learning algorithm termed 'Iterative Data Smoothing' (IDS).","The core idea is that during each training epoch, we not only update the model with the data, but also update the date using the model, replacing hard labels with soft labels.","Our empirical findings highlight the superior performance of this approach over the traditional methods."],"url":"http://arxiv.org/abs/2401.16335v1","category":"cs.LG"}
{"created":"2024-01-29 17:38:14","title":"Tradeoffs Between Alignment and Helpfulness in Language Models","abstract":"Language model alignment has become an important component of AI safety, allowing safe interactions between humans and language models, by enhancing desired behaviors and inhibiting undesired ones. It is often done by tuning the model or inserting preset aligning prompts. Recently, representation engineering, a method which alters the model's behavior via changing its representations post-training, was shown to be effective in aligning LLMs (Zou et al., 2023a). Representation engineering yields gains in alignment oriented tasks such as resistance to adversarial attacks and reduction of social biases, but was also shown to cause a decrease in the ability of the model to perform basic tasks. In this paper we study the tradeoff between the increase in alignment and decrease in helpfulness of the model. We propose a theoretical framework which provides bounds for these two quantities, and demonstrate their relevance empirically. Interestingly, we find that while the helpfulness generally decreases, it does so quadratically with the norm of the representation engineering vector, while the alignment increases linearly with it, indicating a regime in which it is efficient to use representation engineering. We validate our findings empirically, and chart the boundaries to the usefulness of representation engineering for alignment.","sentences":["Language model alignment has become an important component of AI safety, allowing safe interactions between humans and language models, by enhancing desired behaviors and inhibiting undesired ones.","It is often done by tuning the model or inserting preset aligning prompts.","Recently, representation engineering, a method which alters the model's behavior via changing its representations post-training, was shown to be effective in aligning LLMs (Zou et al., 2023a).","Representation engineering yields gains in alignment oriented tasks such as resistance to adversarial attacks and reduction of social biases, but was also shown to cause a decrease in the ability of the model to perform basic tasks.","In this paper we study the tradeoff between the increase in alignment and decrease in helpfulness of the model.","We propose a theoretical framework which provides bounds for these two quantities, and demonstrate their relevance empirically.","Interestingly, we find that while the helpfulness generally decreases, it does so quadratically with the norm of the representation engineering vector, while the alignment increases linearly with it, indicating a regime in which it is efficient to use representation engineering.","We validate our findings empirically, and chart the boundaries to the usefulness of representation engineering for alignment."],"url":"http://arxiv.org/abs/2401.16332v1","category":"cs.CL"}
{"created":"2024-01-29 17:35:19","title":"Synthesis of 3D on-air signatures with the Sigma-Lognormal model","abstract":"Signature synthesis is a computation technique that generates artificial specimens which can support decision making in automatic signature verification. A lot of work has been dedicated to this subject, which centres on synthesizing dynamic and static two-dimensional handwriting on canvas. This paper proposes a framework to generate synthetic 3D on-air signatures exploiting the lognormality principle, which mimics the complex neuromotor control processes at play as the fingertip moves. Addressing the usual cases involving the development of artificial individuals and duplicated samples, this paper contributes to the synthesis of: (1) the trajectory and velocity of entirely 3D new signatures; (2) kinematic information when only the 3D trajectory of the signature is known, and (3) duplicate samples of 3D real signatures. Validation was conducted by generating synthetic 3D signature databases mimicking real ones and showing that automatic signature verifications of genuine and skilled forgeries report performances similar to those of real and synthetic databases. We also observed that training 3D automatic signature verifiers with duplicates can reduce errors. We further demonstrated that our proposal is also valid for synthesizing 3D air writing and gestures. Finally, a perception test confirmed the human likeness of the generated specimens. The databases generated are publicly available, only for research purposes, at .","sentences":["Signature synthesis is a computation technique that generates artificial specimens which can support decision making in automatic signature verification.","A lot of work has been dedicated to this subject, which centres on synthesizing dynamic and static two-dimensional handwriting on canvas.","This paper proposes a framework to generate synthetic 3D on-air signatures exploiting the lognormality principle, which mimics the complex neuromotor control processes at play as the fingertip moves.","Addressing the usual cases involving the development of artificial individuals and duplicated samples, this paper contributes to the synthesis of: (1) the trajectory and velocity of entirely 3D new signatures; (2) kinematic information when only the 3D trajectory of the signature is known, and (3) duplicate samples of 3D real signatures.","Validation was conducted by generating synthetic 3D signature databases mimicking real ones and showing that automatic signature verifications of genuine and skilled forgeries report performances similar to those of real and synthetic databases.","We also observed that training 3D automatic signature verifiers with duplicates can reduce errors.","We further demonstrated that our proposal is also valid for synthesizing 3D air writing and gestures.","Finally, a perception test confirmed the human likeness of the generated specimens.","The databases generated are publicly available, only for research purposes, at ."],"url":"http://arxiv.org/abs/2401.16329v1","category":"cs.CV"}
{"created":"2024-01-29 17:32:22","title":"PICL: Physics Informed Contrastive Learning for Partial Differential Equations","abstract":"Neural operators have recently grown in popularity as Partial Differential Equation (PDEs) surrogate models. Learning solution functionals, rather than functions, has proven to be a powerful approach to calculate fast, accurate solutions to complex PDEs. While much work has been done evaluating neural operator performance on a wide variety of surrogate modeling tasks, these works normally evaluate performance on a single equation at a time. In this work, we develop a novel contrastive pretraining framework utilizing Generalized Contrastive Loss that improves neural operator generalization across multiple governing equations simultaneously. Governing equation coefficients are used to measure ground-truth similarity between systems. A combination of physics-informed system evolution and latent-space model output are anchored to input data and used in our distance function. We find that physics-informed contrastive pretraining improves both accuracy and generalization for the Fourier Neural Operator in fixed-future task, with comparable performance on the autoregressive rollout, and superresolution tasks for the 1D Heat, Burgers', and linear advection equations.","sentences":["Neural operators have recently grown in popularity as Partial Differential Equation (PDEs) surrogate models.","Learning solution functionals, rather than functions, has proven to be a powerful approach to calculate fast, accurate solutions to complex PDEs.","While much work has been done evaluating neural operator performance on a wide variety of surrogate modeling tasks, these works normally evaluate performance on a single equation at a time.","In this work, we develop a novel contrastive pretraining framework utilizing Generalized Contrastive Loss that improves neural operator generalization across multiple governing equations simultaneously.","Governing equation coefficients are used to measure ground-truth similarity between systems.","A combination of physics-informed system evolution and latent-space model output are anchored to input data and used in our distance function.","We find that physics-informed contrastive pretraining improves both accuracy and generalization for the Fourier Neural Operator in fixed-future task, with comparable performance on the autoregressive rollout, and superresolution tasks for the 1D Heat, Burgers', and linear advection equations."],"url":"http://arxiv.org/abs/2401.16327v1","category":"cs.LG"}
{"created":"2024-01-29 17:32:07","title":"Simulation of a Rohksar-Kivelson ladder on a NISQ device","abstract":"We present a quantum-classical algorithm to study the dynamics of the Rohksar-Kivelson plaquette ladder on NISQ devices. We show that complexity is largely reduced using gauge invariance, additional symmetries, and a crucial property associated to how plaquettes are blocked against ring-exchange in the ladder geometry. This allows for an efficient simulation of sizable plaquette ladders with a small number of qubits, well suited for the capabilities of present NISQ devices. We illustrate the procedure for ladders with simulation of up to $8$ plaquettes in an IBM-Q machine, employing scaled quantum gates.","sentences":["We present a quantum-classical algorithm to study the dynamics of the Rohksar-Kivelson plaquette ladder on NISQ devices.","We show that complexity is largely reduced using gauge invariance, additional symmetries, and a crucial property associated to how plaquettes are blocked against ring-exchange in the ladder geometry.","This allows for an efficient simulation of sizable plaquette ladders with a small number of qubits, well suited for the capabilities of present NISQ devices.","We illustrate the procedure for ladders with simulation of up to $8$ plaquettes in an IBM-Q machine, employing scaled quantum gates."],"url":"http://arxiv.org/abs/2401.16326v1","category":"quant-ph"}
{"created":"2024-01-29 17:26:48","title":"Robustness and the Event Horizon Telescope: the case of the first image of M87*","abstract":"We examine the justification for taking the Event Horizon Telescope's famous 2019 image to be a reliable representation of the region surrounding a black hole. We argue that it takes the form of a robustness argument, with the resulting image being robust across variation in a range of data-analysis pipelines. We clarify the sense of \"robustness\" operating here and show how it can account for the reliability of astrophysical inferences, even in cases -- like the EHT -- where these inferences are based on experiments that are (for all practical purposes) unique. This has consequences far beyond the 2019 image.","sentences":["We examine the justification for taking the Event Horizon Telescope's famous 2019 image to be a reliable representation of the region surrounding a black hole.","We argue that it takes the form of a robustness argument, with the resulting image being robust across variation in a range of data-analysis pipelines.","We clarify the sense of \"robustness\" operating here and show how it can account for the reliability of astrophysical inferences, even in cases -- like the EHT -- where these inferences are based on experiments that are (for all practical purposes) unique.","This has consequences far beyond the 2019 image."],"url":"http://arxiv.org/abs/2401.16323v1","category":"physics.hist-ph"}
{"created":"2024-01-29 17:23:25","title":"Prepare Non-classical Collective Spin State by Reinforcement Learning","abstract":"We propose a scheme leveraging reinforcement learning to engineer control fields for generating non-classical states. It is exemplified by the application to prepare spin squeezed state for an open collective spin model where a linear control term is designed to govern the dynamics. The reinforcement learning agent determines the temporal sequence of control pulses, commencing from coherent spin state in an environment characterized by dissipation and dephasing. When compared to constant control scenarios, this approach provides various control sequences maintaining collective spin squeezing and entanglement. It is observed that denser application of the control pulses enhances the performance of the outcomes. Furthermore, there is a minor enhancement in the performance by adding control actions. The proposed strategy demonstrates increased effectiveness for larger systems. And thermal excitations of the reservoir are detrimental to the control outcomes. It should be confirmed that this is an open-loop strategy by closed-loop simulation, circumventing collapse of quantum state induced by measurements. Thanks to the flexible replaceability of the optimization modules and the controlled system, this research paves the way for its application in manipulating other quantum systems.","sentences":["We propose a scheme leveraging reinforcement learning to engineer control fields for generating non-classical states.","It is exemplified by the application to prepare spin squeezed state for an open collective spin model where a linear control term is designed to govern the dynamics.","The reinforcement learning agent determines the temporal sequence of control pulses, commencing from coherent spin state in an environment characterized by dissipation and dephasing.","When compared to constant control scenarios, this approach provides various control sequences maintaining collective spin squeezing and entanglement.","It is observed that denser application of the control pulses enhances the performance of the outcomes.","Furthermore, there is a minor enhancement in the performance by adding control actions.","The proposed strategy demonstrates increased effectiveness for larger systems.","And thermal excitations of the reservoir are detrimental to the control outcomes.","It should be confirmed that this is an open-loop strategy by closed-loop simulation, circumventing collapse of quantum state induced by measurements.","Thanks to the flexible replaceability of the optimization modules and the controlled system, this research paves the way for its application in manipulating other quantum systems."],"url":"http://arxiv.org/abs/2401.16320v1","category":"quant-ph"}
{"created":"2024-01-29 17:21:41","title":"Defining and Extracting generalizable interaction primitives from DNNs","abstract":"Faithfully summarizing the knowledge encoded by a deep neural network (DNN) into a few symbolic primitive patterns without losing much information represents a core challenge in explainable AI. To this end, Ren et al. (2023c) have derived a series of theorems to prove that the inference score of a DNN can be explained as a small set of interactions between input variables. However, the lack of generalization power makes it still hard to consider such interactions as faithful primitive patterns encoded by the DNN. Therefore, given different DNNs trained for the same task, we develop a new method to extract interactions that are shared by these DNNs. Experiments show that the extracted interactions can better reflect common knowledge shared by different DNNs.","sentences":["Faithfully summarizing the knowledge encoded by a deep neural network (DNN) into a few symbolic primitive patterns without losing much information represents a core challenge in explainable AI.","To this end, Ren et al.","(2023c) have derived a series of theorems to prove that the inference score of a DNN can be explained as a small set of interactions between input variables.","However, the lack of generalization power makes it still hard to consider such interactions as faithful primitive patterns encoded by the DNN.","Therefore, given different DNNs trained for the same task, we develop a new method to extract interactions that are shared by these DNNs.","Experiments show that the extracted interactions can better reflect common knowledge shared by different DNNs."],"url":"http://arxiv.org/abs/2401.16318v1","category":"cs.LG"}
{"created":"2024-01-29 17:21:31","title":"Assessing the Benefits and Risks of Quantum Computers","abstract":"Quantum computing is an emerging technology with potentially far-reaching implications for national prosperity and security. Understanding the timeframes over which economic benefits and national security risks may manifest themselves is vital for ensuring the prudent development of this technology. To inform security experts and policy decision makers on this matter, we review what is currently known on the potential uses and risks of quantum computers, leveraging current research literature.   The maturity of currently-available quantum computers is not yet at a level such that they can be used in production for large-scale, industrially-relevant problems, but they are not believed to currently pose security risks. We identify 2 large-scale trends -- new approximate methods (variational algorithms, error mitigation, and circuit knitting) and the commercial exploration of business-relevant quantum applications -- which, together, may enable useful and practical quantum computing in the near future.   Crucially, these methods do not appear likely to change the required resources for cryptanalysis on currently-used cryptosystems. From an analysis we perform of the current and known algorithms for cryptanalysis, we find they require circuits of a size exceeding those that can be run by current and near-future quantum computers (and which will require error correction), though we acknowledge improvements in quantum algorithms for these problems are taking place in the literature. In addition, the risk to cybersecurity can be well-managed by the migration to new, quantum-safe cryptographic protocols, which we survey and discuss.   Given the above, we conclude there is a credible expectation that quantum computers will be capable of performing computations which are economically-impactful before they will be capable of performing ones which are cryptographically-relevant.","sentences":["Quantum computing is an emerging technology with potentially far-reaching implications for national prosperity and security.","Understanding the timeframes over which economic benefits and national security risks may manifest themselves is vital for ensuring the prudent development of this technology.","To inform security experts and policy decision makers on this matter, we review what is currently known on the potential uses and risks of quantum computers, leveraging current research literature.   ","The maturity of currently-available quantum computers is not yet at a level such that they can be used in production for large-scale, industrially-relevant problems, but they are not believed to currently pose security risks.","We identify 2 large-scale trends -- new approximate methods (variational algorithms, error mitigation, and circuit knitting) and the commercial exploration of business-relevant quantum applications -- which, together, may enable useful and practical quantum computing in the near future.   ","Crucially, these methods do not appear likely to change the required resources for cryptanalysis on currently-used cryptosystems.","From an analysis we perform of the current and known algorithms for cryptanalysis, we find they require circuits of a size exceeding those that can be run by current and near-future quantum computers (and which will require error correction), though we acknowledge improvements in quantum algorithms for these problems are taking place in the literature.","In addition, the risk to cybersecurity can be well-managed by the migration to new, quantum-safe cryptographic protocols, which we survey and discuss.   ","Given the above, we conclude there is a credible expectation that quantum computers will be capable of performing computations which are economically-impactful before they will be capable of performing ones which are cryptographically-relevant."],"url":"http://arxiv.org/abs/2401.16317v1","category":"quant-ph"}
{"created":"2024-01-29 17:18:47","title":"Creative Telescoping for Hypergeometric Double Sums","abstract":"We present efficient methods for calculating linear recurrences of hypergeometric double sums and, more generally, of multiple sums. In particular, we supplement this approach with the algorithmic theory of contiguous relations, which guarantees the applicability of our method for many input sums. In addition, we elaborate new techniques to optimize the underlying key task of our method to compute rational solutions of parameterized linear recurrences.","sentences":["We present efficient methods for calculating linear recurrences of hypergeometric double sums and, more generally, of multiple sums.","In particular, we supplement this approach with the algorithmic theory of contiguous relations, which guarantees the applicability of our method for many input sums.","In addition, we elaborate new techniques to optimize the underlying key task of our method to compute rational solutions of parameterized linear recurrences."],"url":"http://arxiv.org/abs/2401.16314v1","category":"cs.SC"}
{"created":"2024-01-29 17:17:42","title":"Machine Translation Meta Evaluation through Translation Accuracy Challenge Sets","abstract":"Recent machine translation (MT) metrics calibrate their effectiveness by correlating with human judgement but without any insights about their behaviour across different error types. Challenge sets are used to probe specific dimensions of metric behaviour but there are very few such datasets and they either focus on a limited number of phenomena or a limited number of language pairs. We introduce ACES, a contrastive challenge set spanning 146 language pairs, aimed at discovering whether metrics can identify 68 translation accuracy errors. These phenomena range from simple alterations at the word/character level to more complex errors based on discourse and real-world knowledge. We conduct a large-scale study by benchmarking ACES on 50 metrics submitted to the WMT 2022 and 2023 metrics shared tasks. We benchmark metric performance, assess their incremental performance over successive campaigns, and measure their sensitivity to a range of linguistic phenomena. We also investigate claims that Large Language Models (LLMs) are effective as MT evaluators by evaluating on ACES. Our results demonstrate that different metric families struggle with different phenomena and that LLM-based methods fail to demonstrate reliable performance. Our analyses indicate that most metrics ignore the source sentence, tend to prefer surface-level overlap and end up incorporating properties of base models which are not always beneficial. We expand ACES to include error span annotations, denoted as SPAN-ACES and we use this dataset to evaluate span-based error metrics showing these metrics also need considerable improvement. Finally, we provide a set of recommendations for building better MT metrics, including focusing on error labels instead of scores, ensembling, designing strategies to explicitly focus on the source sentence, focusing on semantic content and choosing the right base model for representations.","sentences":["Recent machine translation (MT) metrics calibrate their effectiveness by correlating with human judgement but without any insights about their behaviour across different error types.","Challenge sets are used to probe specific dimensions of metric behaviour but there are very few such datasets and they either focus on a limited number of phenomena or a limited number of language pairs.","We introduce ACES, a contrastive challenge set spanning 146 language pairs, aimed at discovering whether metrics can identify 68 translation accuracy errors.","These phenomena range from simple alterations at the word/character level to more complex errors based on discourse and real-world knowledge.","We conduct a large-scale study by benchmarking ACES on 50 metrics submitted to the WMT 2022 and 2023 metrics shared tasks.","We benchmark metric performance, assess their incremental performance over successive campaigns, and measure their sensitivity to a range of linguistic phenomena.","We also investigate claims that Large Language Models (LLMs) are effective as MT evaluators by evaluating on ACES.","Our results demonstrate that different metric families struggle with different phenomena and that LLM-based methods fail to demonstrate reliable performance.","Our analyses indicate that most metrics ignore the source sentence, tend to prefer surface-level overlap and end up incorporating properties of base models which are not always beneficial.","We expand ACES to include error span annotations, denoted as SPAN-ACES and we use this dataset to evaluate span-based error metrics showing these metrics also need considerable improvement.","Finally, we provide a set of recommendations for building better MT metrics, including focusing on error labels instead of scores, ensembling, designing strategies to explicitly focus on the source sentence, focusing on semantic content and choosing the right base model for representations."],"url":"http://arxiv.org/abs/2401.16313v1","category":"cs.CL"}
{"created":"2024-01-29 17:17:34","title":"Degradability of Modified Landau-Streater Type Low-Noise Quantum Channels in High Dimensions","abstract":"This paper delves into the degradability of quantum channels, with a specific focus on high-dimensional extensions of qubit depolarizing channels in low-noise regimes. We build upon the foundation of $\\eta$-approximate degradable channels, as established by Sutter et al. and Leditzky et al., to introduce and examine the Modified Landau-Streater (MLS) channels. These channels expand upon the qubit depolarizing and the recently proposed modified Werner-Holevo channels by Roofeh and Karimipour, extending them to higher-dimensional Hilbert spaces (with dimension $d=2j+1$, where $j$ are positive half-integers). Our investigation centers on their conformity to the $O(\\varepsilon^2)$ degradability pattern, aligning with and extending Leditzky et al.'s findings in the $d=2$ case. By replacing the SU($2$) generators with SU($d$) in our treatment, we may explore the potential inclusion of generalized Gell-Mann matrices in future research. Our results enhance the understanding of super-additivity in quantum channels within the low-noise regime and lay the groundwork for future explorations into conditions and structures that could lead to $O(\\varepsilon^2)$ degradability across a broader spectrum of quantum channels.","sentences":["This paper delves into the degradability of quantum channels, with a specific focus on high-dimensional extensions of qubit depolarizing channels in low-noise regimes.","We build upon the foundation of $\\eta$-approximate degradable channels, as established by Sutter et al. and Leditzky et al., to introduce and examine the Modified Landau-Streater (MLS) channels.","These channels expand upon the qubit depolarizing and the recently proposed modified Werner-Holevo channels by Roofeh and Karimipour, extending them to higher-dimensional Hilbert spaces (with dimension $d=2j+1$, where $j$ are positive half-integers).","Our investigation centers on their conformity to the $O(\\varepsilon^2)$ degradability pattern, aligning with and extending Leditzky et al.'s findings in the $d=2$ case.","By replacing the SU($2$) generators with SU($d$) in our treatment, we may explore the potential inclusion of generalized Gell-Mann matrices in future research.","Our results enhance the understanding of super-additivity in quantum channels within the low-noise regime and lay the groundwork for future explorations into conditions and structures that could lead to $O(\\varepsilon^2)$ degradability across a broader spectrum of quantum channels."],"url":"http://arxiv.org/abs/2401.16312v1","category":"cs.IT"}
{"created":"2024-01-29 17:13:44","title":"Security Code Review by LLMs: A Deep Dive into Responses","abstract":"Security code review aims to combine automated tools and manual efforts to detect security defects during development. The rapid development of Large Language Models (LLMs) has shown promising potential in software development, as well as opening up new possibilities in automated security code review. To explore the challenges of applying LLMs in practical code review for security defect detection, this study compared the detection performance of three state-of-the-art LLMs (Gemini Pro, GPT-4, and GPT-3.5) under five prompts on 549 code files that contain security defects from real-world code reviews. Through analyzing 82 responses generated by the best-performing LLM-prompt combination based on 100 randomly selected code files, we extracted and categorized quality problems present in these responses into 5 themes and 16 categories. Our results indicate that the responses produced by LLMs often suffer from verbosity, vagueness, and incompleteness, highlighting the necessity to enhance their conciseness, understandability, and compliance to security defect detection. This work reveals the deficiencies of LLM-generated responses in security code review and paves the way for future optimization of LLMs towards this task.","sentences":["Security code review aims to combine automated tools and manual efforts to detect security defects during development.","The rapid development of Large Language Models (LLMs) has shown promising potential in software development, as well as opening up new possibilities in automated security code review.","To explore the challenges of applying LLMs in practical code review for security defect detection, this study compared the detection performance of three state-of-the-art LLMs (Gemini Pro, GPT-4, and GPT-3.5) under five prompts on 549 code files that contain security defects from real-world code reviews.","Through analyzing 82 responses generated by the best-performing LLM-prompt combination based on 100 randomly selected code files, we extracted and categorized quality problems present in these responses into 5 themes and 16 categories.","Our results indicate that the responses produced by LLMs often suffer from verbosity, vagueness, and incompleteness, highlighting the necessity to enhance their conciseness, understandability, and compliance to security defect detection.","This work reveals the deficiencies of LLM-generated responses in security code review and paves the way for future optimization of LLMs towards this task."],"url":"http://arxiv.org/abs/2401.16310v1","category":"cs.SE"}
{"created":"2024-01-29 17:08:57","title":"Momentary Stressor Logging and Reflective Visualizations: Implications for Stress Management with Wearables","abstract":"Commercial wearables from Fitbit, Garmin, and Whoop have recently introduced real-time notifications based on detecting changes in physiological responses indicating potential stress. In this paper, we investigate how these new capabilities can be leveraged to improve stress management. We developed a smartwatch app, a smartphone app, and a cloud service, and conducted a 100-day field study with 122 participants who received prompts triggered by physiological responses several times a day. They were asked whether they were stressed, and if so, to log the most likely stressor. Each week, participants received new visualizations of their data to self-reflect on patterns and trends. Participants reported better awareness of their stressors, and self-initiating fourteen kinds of behavioral changes to reduce stress in their daily lives. Repeated self-reports over 14 weeks showed reductions in both stress intensity (in 26,521 momentary ratings) and stress frequency (in 1,057 weekly surveys).","sentences":["Commercial wearables from Fitbit, Garmin, and Whoop have recently introduced real-time notifications based on detecting changes in physiological responses indicating potential stress.","In this paper, we investigate how these new capabilities can be leveraged to improve stress management.","We developed a smartwatch app, a smartphone app, and a cloud service, and conducted a 100-day field study with 122 participants who received prompts triggered by physiological responses several times a day.","They were asked whether they were stressed, and if so, to log the most likely stressor.","Each week, participants received new visualizations of their data to self-reflect on patterns and trends.","Participants reported better awareness of their stressors, and self-initiating fourteen kinds of behavioral changes to reduce stress in their daily lives.","Repeated self-reports over 14 weeks showed reductions in both stress intensity (in 26,521 momentary ratings) and stress frequency (in 1,057 weekly surveys)."],"url":"http://arxiv.org/abs/2401.16307v1","category":"cs.HC"}
{"created":"2024-01-29 17:08:55","title":"Boundary terms, branes and AdS/BCFT in first-order gravity","abstract":"We provide an account of the issue of Gibbons-Hawking-York-like boundary terms for a gravity theory defined on a Riemman-Cartan spacetime. We further discuss different criteria for introducing boundary terms in some familiar first-order gravity theories with both on-shell vanishing and non-vanishing torsion, along with considerations regarding the thermodynamics of black holes and profiles of the End-of-the-World branes. Our analysis confirms the expected geodesic profile of the End-of-the-World brane in the BF formulation of Jackiw-Teitelboim gravity. Finally, we present the first realisation of the AdS/BCFT duality for spacetime with torsion.","sentences":["We provide an account of the issue of Gibbons-Hawking-York-like boundary terms for a gravity theory defined on a Riemman-Cartan spacetime.","We further discuss different criteria for introducing boundary terms in some familiar first-order gravity theories with both on-shell vanishing and non-vanishing torsion, along with considerations regarding the thermodynamics of black holes and profiles of the End-of-the-World branes.","Our analysis confirms the expected geodesic profile of the End-of-the-World brane in the BF formulation of Jackiw-Teitelboim gravity.","Finally, we present the first realisation of the AdS/BCFT duality for spacetime with torsion."],"url":"http://arxiv.org/abs/2401.16306v1","category":"hep-th"}
{"created":"2024-01-29 17:05:19","title":"MixSup: Mixed-grained Supervision for Label-efficient LiDAR-based 3D Object Detection","abstract":"Label-efficient LiDAR-based 3D object detection is currently dominated by weakly/semi-supervised methods. Instead of exclusively following one of them, we propose MixSup, a more practical paradigm simultaneously utilizing massive cheap coarse labels and a limited number of accurate labels for Mixed-grained Supervision. We start by observing that point clouds are usually textureless, making it hard to learn semantics. However, point clouds are geometrically rich and scale-invariant to the distances from sensors, making it relatively easy to learn the geometry of objects, such as poses and shapes. Thus, MixSup leverages massive coarse cluster-level labels to learn semantics and a few expensive box-level labels to learn accurate poses and shapes. We redesign the label assignment in mainstream detectors, which allows them seamlessly integrated into MixSup, enabling practicality and universality. We validate its effectiveness in nuScenes, Waymo Open Dataset, and KITTI, employing various detectors. MixSup achieves up to 97.31% of fully supervised performance, using cheap cluster annotations and only 10% box annotations. Furthermore, we propose PointSAM based on the Segment Anything Model for automated coarse labeling, further reducing the annotation burden. The code is available at https://github.com/BraveGroup/PointSAM-for-MixSup.","sentences":["Label-efficient LiDAR-based 3D object detection is currently dominated by weakly/semi-supervised methods.","Instead of exclusively following one of them, we propose MixSup, a more practical paradigm simultaneously utilizing massive cheap coarse labels and a limited number of accurate labels for Mixed-grained Supervision.","We start by observing that point clouds are usually textureless, making it hard to learn semantics.","However, point clouds are geometrically rich and scale-invariant to the distances from sensors, making it relatively easy to learn the geometry of objects, such as poses and shapes.","Thus, MixSup leverages massive coarse cluster-level labels to learn semantics and a few expensive box-level labels to learn accurate poses and shapes.","We redesign the label assignment in mainstream detectors, which allows them seamlessly integrated into MixSup, enabling practicality and universality.","We validate its effectiveness in nuScenes, Waymo Open Dataset, and KITTI, employing various detectors.","MixSup achieves up to 97.31% of fully supervised performance, using cheap cluster annotations and only 10% box annotations.","Furthermore, we propose PointSAM based on the Segment Anything Model for automated coarse labeling, further reducing the annotation burden.","The code is available at https://github.com/BraveGroup/PointSAM-for-MixSup."],"url":"http://arxiv.org/abs/2401.16305v1","category":"cs.CV"}
{"created":"2024-01-29 17:03:28","title":"Quantum-safe Encryption: A New Method to Reduce Complexity and/or Improve Security Level","abstract":"This work presents some novel techniques to enhance an encryption scheme motivated by classical McEliece cryptosystem. Contributions include: (1) using masking matrices to hide sensitive data, (2) allowing both legitimate parties to incorporate randomness in the public key without sharing any additional public information, (3) using concatenation of a repetition code for error correction, permitting key recovery with a negligible decoding complexity, (4) making attacks more difficult by increasing the complexity in verifying a given key candidate has resulted in the actual key, (5) introducing memory in the error sequence such that: (i) error vector is composed of a random number of erroneous bits, (ii) errors can be all corrected when used in conjunction with concatenation of a repetition code of length 3. Proposed techniques allow generating significantly larger keys, at the same time, with a much lower complexity, as compared to known post-quantum key generation techniques relying on randomization.","sentences":["This work presents some novel techniques to enhance an encryption scheme motivated by classical McEliece cryptosystem.","Contributions include: (1) using masking matrices to hide sensitive data, (2) allowing both legitimate parties to incorporate randomness in the public key without sharing any additional public information, (3) using concatenation of a repetition code for error correction, permitting key recovery with a negligible decoding complexity, (4) making attacks more difficult by increasing the complexity in verifying a given key candidate has resulted in the actual key, (5) introducing memory in the error sequence such that: (i) error vector is composed of a random number of erroneous bits, (ii) errors can be all corrected when used in conjunction with concatenation of a repetition code of length 3.","Proposed techniques allow generating significantly larger keys, at the same time, with a much lower complexity, as compared to known post-quantum key generation techniques relying on randomization."],"url":"http://arxiv.org/abs/2401.16302v1","category":"cs.CR"}
{"created":"2024-01-29 17:02:14","title":"Scalable Factor Graph-Based Heterogeneous Bayesian DDF for Dynamic Systems","abstract":"Heterogeneous Bayesian decentralized data fusion captures the set of problems in which two robots must combine two probability density functions over non-equal, but overlapping sets of random variables. In the context of multi-robot dynamic systems, this enables robots to take a \"divide and conquer\" approach to reason and share data over complementary tasks instead of over the full joint state space. For example, in a target tracking application, this allows robots to track different subsets of targets and share data on only common targets. This paper presents a framework by which robots can each use a local factor graph to represent relevant partitions of a complex global joint probability distribution, thus allowing them to avoid reasoning over the entirety of a more complex model and saving communication as well as computation costs. From a theoretical point of view, this paper makes contributions by casting the heterogeneous decentralized fusion problem in terms of a factor graph, analyzing the challenges that arise due to dynamic filtering, and then developing a new conservative filtering algorithm that ensures statistical correctness. From a practical point of view, we show how this framework can be used to represent different multi-robot applications and then test it with simulations and hardware experiments to validate and demonstrate its statistical conservativeness, applicability, and robustness to real-world challenges.","sentences":["Heterogeneous Bayesian decentralized data fusion captures the set of problems in which two robots must combine two probability density functions over non-equal, but overlapping sets of random variables.","In the context of multi-robot dynamic systems, this enables robots to take a \"divide and conquer\" approach to reason and share data over complementary tasks instead of over the full joint state space.","For example, in a target tracking application, this allows robots to track different subsets of targets and share data on only common targets.","This paper presents a framework by which robots can each use a local factor graph to represent relevant partitions of a complex global joint probability distribution, thus allowing them to avoid reasoning over the entirety of a more complex model and saving communication as well as computation costs.","From a theoretical point of view, this paper makes contributions by casting the heterogeneous decentralized fusion problem in terms of a factor graph, analyzing the challenges that arise due to dynamic filtering, and then developing a new conservative filtering algorithm that ensures statistical correctness.","From a practical point of view, we show how this framework can be used to represent different multi-robot applications and then test it with simulations and hardware experiments to validate and demonstrate its statistical conservativeness, applicability, and robustness to real-world challenges."],"url":"http://arxiv.org/abs/2401.16301v1","category":"cs.RO"}
{"created":"2024-01-29 17:00:28","title":"Enhancing Molecular Property Prediction with Auxiliary Learning and Task-Specific Adaptation","abstract":"Pretrained Graph Neural Networks have been widely adopted for various molecular property prediction tasks. Despite their ability to encode structural and relational features of molecules, traditional fine-tuning of such pretrained GNNs on the target task can lead to poor generalization. To address this, we explore the adaptation of pretrained GNNs to the target task by jointly training them with multiple auxiliary tasks. This could enable the GNNs to learn both general and task-specific features, which may benefit the target task. However, a major challenge is to determine the relatedness of auxiliary tasks with the target task. To address this, we investigate multiple strategies to measure the relevance of auxiliary tasks and integrate such tasks by adaptively combining task gradients or by learning task weights via bi-level optimization. Additionally, we propose a novel gradient surgery-based approach, Rotation of Conflicting Gradients ($\\mathtt{RCGrad}$), that learns to align conflicting auxiliary task gradients through rotation. Our experiments with state-of-the-art pretrained GNNs demonstrate the efficacy of our proposed methods, with improvements of up to 7.7% over fine-tuning. This suggests that incorporating auxiliary tasks along with target task fine-tuning can be an effective way to improve the generalizability of pretrained GNNs for molecular property prediction.","sentences":["Pretrained Graph Neural Networks have been widely adopted for various molecular property prediction tasks.","Despite their ability to encode structural and relational features of molecules, traditional fine-tuning of such pretrained GNNs on the target task can lead to poor generalization.","To address this, we explore the adaptation of pretrained GNNs to the target task by jointly training them with multiple auxiliary tasks.","This could enable the GNNs to learn both general and task-specific features, which may benefit the target task.","However, a major challenge is to determine the relatedness of auxiliary tasks with the target task.","To address this, we investigate multiple strategies to measure the relevance of auxiliary tasks and integrate such tasks by adaptively combining task gradients or by learning task weights via bi-level optimization.","Additionally, we propose a novel gradient surgery-based approach, Rotation of Conflicting Gradients ($\\mathtt{RCGrad}$), that learns to align conflicting auxiliary task gradients through rotation.","Our experiments with state-of-the-art pretrained GNNs demonstrate the efficacy of our proposed methods, with improvements of up to 7.7% over fine-tuning.","This suggests that incorporating auxiliary tasks along with target task fine-tuning can be an effective way to improve the generalizability of pretrained GNNs for molecular property prediction."],"url":"http://arxiv.org/abs/2401.16299v1","category":"cs.LG"}
{"created":"2024-01-29 16:59:39","title":"Breaking the Barrier: Selective Uncertainty-based Active Learning for Medical Image Segmentation","abstract":"Active learning (AL) has found wide applications in medical image segmentation, aiming to alleviate the annotation workload and enhance performance. Conventional uncertainty-based AL methods, such as entropy and Bayesian, often rely on an aggregate of all pixel-level metrics. However, in imbalanced settings, these methods tend to neglect the significance of target regions, eg., lesions, and tumors. Moreover, uncertainty-based selection introduces redundancy. These factors lead to unsatisfactory performance, and in many cases, even underperform random sampling. To solve this problem, we introduce a novel approach called the Selective Uncertainty-based AL, avoiding the conventional practice of summing up the metrics of all pixels. Through a filtering process, our strategy prioritizes pixels within target areas and those near decision boundaries. This resolves the aforementioned disregard for target areas and redundancy. Our method showed substantial improvements across five different uncertainty-based methods and two distinct datasets, utilizing fewer labeled data to reach the supervised baseline and consistently achieving the highest overall performance. Our code is available at https://github.com/HelenMa9998/Selective\\_Uncertainty\\_AL.","sentences":["Active learning (AL) has found wide applications in medical image segmentation, aiming to alleviate the annotation workload and enhance performance.","Conventional uncertainty-based AL methods, such as entropy and Bayesian, often rely on an aggregate of all pixel-level metrics.","However, in imbalanced settings, these methods tend to neglect the significance of target regions, eg., lesions, and tumors.","Moreover, uncertainty-based selection introduces redundancy.","These factors lead to unsatisfactory performance, and in many cases, even underperform random sampling.","To solve this problem, we introduce a novel approach called the Selective Uncertainty-based AL, avoiding the conventional practice of summing up the metrics of all pixels.","Through a filtering process, our strategy prioritizes pixels within target areas and those near decision boundaries.","This resolves the aforementioned disregard for target areas and redundancy.","Our method showed substantial improvements across five different uncertainty-based methods and two distinct datasets, utilizing fewer labeled data to reach the supervised baseline and consistently achieving the highest overall performance.","Our code is available at https://github.com/HelenMa9998/Selective\\_Uncertainty\\_AL."],"url":"http://arxiv.org/abs/2401.16298v1","category":"cs.CV"}
{"created":"2024-01-29 16:57:48","title":"Radial Gravitational Collapse Causes Timelike Incompleteness","abstract":"We show that a globally hyperbolic spacetime containing a trapped surface and satisfying the strong energy condition and a condition on certain radial tidal forces must be timelike geodesically incomplete. This constitutes a \"timelike\" version of Penrose's celebrated singularity theorem. Recall that the latter concludes that certain spacetimes are null incomplete, providing the first theoretical evidence that black holes actually exist in our Universe. By concluding timelike instead of null incompleteness, we obtain, at the expense of stronger assumptions, a clearer physical interpretation and the existence of an event horizon.","sentences":["We show that a globally hyperbolic spacetime containing a trapped surface and satisfying the strong energy condition and a condition on certain radial tidal forces must be timelike geodesically incomplete.","This constitutes a \"timelike\" version of Penrose's celebrated singularity theorem.","Recall that the latter concludes that certain spacetimes are null incomplete, providing the first theoretical evidence that black holes actually exist in our Universe.","By concluding timelike instead of null incompleteness, we obtain, at the expense of stronger assumptions, a clearer physical interpretation and the existence of an event horizon."],"url":"http://arxiv.org/abs/2401.16297v1","category":"gr-qc"}
{"created":"2024-01-29 16:56:21","title":"On the Complexity of Establishing Hereditary Graph Properties via Vertex Splitting","abstract":"Vertex splitting is a graph operation that replaces a vertex $v$ with two nonadjacent new vertices and makes each neighbor of $v$ adjacent with one or both of the introduced vertices. Vertex splitting has been used in contexts from circuit design to statistical analysis. In this work, we explore the computational complexity of achieving a given graph property $\\Pi$ by a limited number of vertex splits, formalized as the problem $\\Pi$ Vertex Splitting ($\\Pi$-VS). We focus on hereditary graph properties and contribute four groups of results: First, we classify the classical complexity of $\\Pi$-VS for graph properties characterized by forbidden subgraphs of size at most 3. Second, we provide a framework that allows to show NP-completeness whenever one can construct a combination of a forbidden subgraph and prescribed vertex splits that satisfy certain conditions. Leveraging this framework we show NP-completeness when $\\Pi$ is characterized by forbidden subgraphs that are sufficiently well connected. In particular, we show that $F$-Free-VS is NP-complete for each biconnected graph $F$. Third, we study infinite families of forbidden subgraphs, obtaining NP-hardness for Bipartite-VS and Perfect-VS. Finally, we touch upon the parameterized complexity of $\\Pi$-VS with respect to the number of allowed splits, showing para-NP-hardness for $K_3$-Free-VS and deriving an XP-algorithm when each vertex is only allowed to be split at most once.","sentences":["Vertex splitting is a graph operation that replaces a vertex $v$ with two nonadjacent new vertices and makes each neighbor of $v$ adjacent with one or both of the introduced vertices.","Vertex splitting has been used in contexts from circuit design to statistical analysis.","In this work, we explore the computational complexity of achieving a given graph property $\\Pi$ by a limited number of vertex splits, formalized as the problem $\\Pi$ Vertex Splitting ($\\Pi$-VS).","We focus on hereditary graph properties and contribute four groups of results:","First, we classify the classical complexity of $\\Pi$-VS for graph properties characterized by forbidden subgraphs of size at most 3.","Second, we provide a framework that allows to show NP-completeness whenever one can construct a combination of a forbidden subgraph and prescribed vertex splits that satisfy certain conditions.","Leveraging this framework we show NP-completeness when $\\Pi$ is characterized by forbidden subgraphs that are sufficiently well connected.","In particular, we show that $F$-Free-VS is NP-complete for each biconnected graph $F$. Third, we study infinite families of forbidden subgraphs, obtaining NP-hardness for Bipartite-VS and Perfect-VS.","Finally, we touch upon the parameterized complexity of $\\Pi$-VS with respect to the number of allowed splits, showing para-NP-hardness for $K_3$-Free-VS and deriving an XP-algorithm when each vertex is only allowed to be split at most once."],"url":"http://arxiv.org/abs/2401.16296v1","category":"cs.CC"}
{"created":"2024-01-29 16:53:04","title":"Dual feature-based and example-based explanation methods","abstract":"A new approach to the local and global explanation is proposed. It is based on selecting a convex hull constructed for the finite number of points around an explained instance. The convex hull allows us to consider a dual representation of instances in the form of convex combinations of extreme points of a produced polytope. Instead of perturbing new instances in the Euclidean feature space, vectors of convex combination coefficients are uniformly generated from the unit simplex, and they form a new dual dataset. A dual linear surrogate model is trained on the dual dataset. The explanation feature importance values are computed by means of simple matrix calculations. The approach can be regarded as a modification of the well-known model LIME. The dual representation inherently allows us to get the example-based explanation. The neural additive model is also considered as a tool for implementing the example-based explanation approach. Many numerical experiments with real datasets are performed for studying the approach. The code of proposed algorithms is available.","sentences":["A new approach to the local and global explanation is proposed.","It is based on selecting a convex hull constructed for the finite number of points around an explained instance.","The convex hull allows us to consider a dual representation of instances in the form of convex combinations of extreme points of a produced polytope.","Instead of perturbing new instances in the Euclidean feature space, vectors of convex combination coefficients are uniformly generated from the unit simplex, and they form a new dual dataset.","A dual linear surrogate model is trained on the dual dataset.","The explanation feature importance values are computed by means of simple matrix calculations.","The approach can be regarded as a modification of the well-known model LIME.","The dual representation inherently allows us to get the example-based explanation.","The neural additive model is also considered as a tool for implementing the example-based explanation approach.","Many numerical experiments with real datasets are performed for studying the approach.","The code of proposed algorithms is available."],"url":"http://arxiv.org/abs/2401.16294v1","category":"cs.LG"}
{"created":"2024-01-29 16:50:56","title":"Textual Entailment for Effective Triple Validation in Object Prediction","abstract":"Knowledge base population seeks to expand knowledge graphs with facts that are typically extracted from a text corpus. Recently, language models pretrained on large corpora have been shown to contain factual knowledge that can be retrieved using cloze-style strategies. Such approach enables zero-shot recall of facts, showing competitive results in object prediction compared to supervised baselines. However, prompt-based fact retrieval can be brittle and heavily depend on the prompts and context used, which may produce results that are unintended or hallucinatory.We propose to use textual entailment to validate facts extracted from language models through cloze statements. Our results show that triple validation based on textual entailment improves language model predictions in different training regimes. Furthermore, we show that entailment-based triple validation is also effective to validate candidate facts extracted from other sources including existing knowledge graphs and text passages where named entities are recognized.","sentences":["Knowledge base population seeks to expand knowledge graphs with facts that are typically extracted from a text corpus.","Recently, language models pretrained on large corpora have been shown to contain factual knowledge that can be retrieved using cloze-style strategies.","Such approach enables zero-shot recall of facts, showing competitive results in object prediction compared to supervised baselines.","However, prompt-based fact retrieval can be brittle and heavily depend on the prompts and context used, which may produce results that are unintended or hallucinatory.","We propose to use textual entailment to validate facts extracted from language models through cloze statements.","Our results show that triple validation based on textual entailment improves language model predictions in different training regimes.","Furthermore, we show that entailment-based triple validation is also effective to validate candidate facts extracted from other sources including existing knowledge graphs and text passages where named entities are recognized."],"url":"http://arxiv.org/abs/2401.16293v1","category":"cs.CL"}
{"created":"2024-01-29 16:50:43","title":"Pilotfish: Distributed Transaction Execution for Lazy Blockchains","abstract":"Pilotfish is the first scale-out blockchain execution engine able to harness any degree of parallelizability existing in its workload. Pilotfish allows each validator to employ multiple machines, named ExecutionWorkers, under its control to scale its execution layer. Given a sufficiently parallelizable and compute-intensive load, the number of transactions that the validator can execute increases linearly with the number of ExecutionWorkers at its disposal.   In addition, Pilotfish maintains the consistency of the state, even when many validators experience simultaneous machine failures. This is possible due to the meticulous co-design of our crash-recovery protocol which leverages the existing fault tolerance in the blockchain's consensus mechanism.   Finally, Pilotfish can also be seen as the first distributed deterministic execution engine that provides support for dynamic reads as transactions are not required to provide a fully accurate read and write set. This loosening of requirements would normally reduce the parallelizability available by blocking write-after-write conflicts, but our novel versioned-queues scheduling algorithm circumvents this by exploiting the lazy recovery property of Pilotfish, which only persists consistent state and re-executes any optimistic steps taken before the crash.   In order to prove our claims we implemented the common path of Pilotfish with support for the MoveVM and evaluated it against the parallel execution MoveVM of Sui. Our results show that our simpler scheduling algorithms outperforms Sui even with a single execution worker, but more importantly provides linear scalability up to 4 ExecutionWorkers even for simple asset-transfers and to any number of ExecutionWorkers for more computationally heavy workloads.","sentences":["Pilotfish is the first scale-out blockchain execution engine able to harness any degree of parallelizability existing in its workload.","Pilotfish allows each validator to employ multiple machines, named ExecutionWorkers, under its control to scale its execution layer.","Given a sufficiently parallelizable and compute-intensive load, the number of transactions that the validator can execute increases linearly with the number of ExecutionWorkers at its disposal.   ","In addition, Pilotfish maintains the consistency of the state, even when many validators experience simultaneous machine failures.","This is possible due to the meticulous co-design of our crash-recovery protocol which leverages the existing fault tolerance in the blockchain's consensus mechanism.   ","Finally, Pilotfish can also be seen as the first distributed deterministic execution engine that provides support for dynamic reads as transactions are not required to provide a fully accurate read and write set.","This loosening of requirements would normally reduce the parallelizability available by blocking write-after-write conflicts, but our novel versioned-queues scheduling algorithm circumvents this by exploiting the lazy recovery property of Pilotfish, which only persists consistent state and re-executes any optimistic steps taken before the crash.   ","In order to prove our claims we implemented the common path of Pilotfish with support for the MoveVM and evaluated it against the parallel execution MoveVM of Sui.","Our results show that our simpler scheduling algorithms outperforms Sui even with a single execution worker, but more importantly provides linear scalability up to 4 ExecutionWorkers even for simple asset-transfers and to any number of ExecutionWorkers for more computationally heavy workloads."],"url":"http://arxiv.org/abs/2401.16292v1","category":"cs.DC"}
{"created":"2024-01-29 16:50:32","title":"MachineLearnAthon: An Action-Oriented Machine Learning Didactic Concept","abstract":"Machine Learning (ML) techniques are encountered nowadays across disciplines, from social sciences, through natural sciences to engineering. The broad application of ML and the accelerated pace of its evolution lead to an increasing need for dedicated teaching concepts aimed at making the application of this technology more reliable and responsible. However, teaching ML is a daunting task. Aside from the methodological complexity of ML algorithms, both with respect to theory and implementation, the interdisciplinary and empirical nature of the field need to be taken into consideration. This paper introduces the MachineLearnAthon format, an innovative didactic concept designed to be inclusive for students of different disciplines with heterogeneous levels of mathematics, programming and domain expertise. At the heart of the concept lie ML challenges, which make use of industrial data sets to solve real-world problems. These cover the entire ML pipeline, promoting data literacy and practical skills, from data preparation, through deployment, to evaluation.","sentences":["Machine Learning (ML) techniques are encountered nowadays across disciplines, from social sciences, through natural sciences to engineering.","The broad application of ML and the accelerated pace of its evolution lead to an increasing need for dedicated teaching concepts aimed at making the application of this technology more reliable and responsible.","However, teaching ML is a daunting task.","Aside from the methodological complexity of ML algorithms, both with respect to theory and implementation, the interdisciplinary and empirical nature of the field need to be taken into consideration.","This paper introduces the MachineLearnAthon format, an innovative didactic concept designed to be inclusive for students of different disciplines with heterogeneous levels of mathematics, programming and domain expertise.","At the heart of the concept lie ML challenges, which make use of industrial data sets to solve real-world problems.","These cover the entire ML pipeline, promoting data literacy and practical skills, from data preparation, through deployment, to evaluation."],"url":"http://arxiv.org/abs/2401.16291v1","category":"cs.LG"}
{"created":"2024-01-29 16:48:34","title":"GAPS: Geometry-Aware Problem Solver","abstract":"Geometry problem solving presents a formidable challenge within the NLP community. Existing approaches often rely on models designed for solving math word problems, neglecting the unique characteristics of geometry math problems. Additionally, the current research predominantly focuses on geometry calculation problems, while overlooking other essential aspects like proving. In this study, we address these limitations by proposing the Geometry-Aware Problem Solver (GAPS) model. GAPS is specifically designed to generate solution programs for geometry math problems of various types with the help of its unique problem-type classifier. To achieve this, GAPS treats the solution program as a composition of operators and operands, segregating their generation processes. Furthermore, we introduce the geometry elements enhancement method, which enhances the ability of GAPS to recognize geometry elements accurately. By leveraging these improvements, GAPS showcases remarkable performance in resolving geometry math problems. Our experiments conducted on the UniGeo dataset demonstrate the superiority of GAPS over the state-of-the-art model, Geoformer. Specifically, GAPS achieves an accuracy improvement of more than 5.3% for calculation tasks and an impressive 41.1% for proving tasks. Notably, GAPS achieves an impressive accuracy of 97.5% on proving problems, representing a significant advancement in solving geometry proving tasks.","sentences":["Geometry problem solving presents a formidable challenge within the NLP community.","Existing approaches often rely on models designed for solving math word problems, neglecting the unique characteristics of geometry math problems.","Additionally, the current research predominantly focuses on geometry calculation problems, while overlooking other essential aspects like proving.","In this study, we address these limitations by proposing the Geometry-Aware Problem Solver (GAPS) model.","GAPS is specifically designed to generate solution programs for geometry math problems of various types with the help of its unique problem-type classifier.","To achieve this, GAPS treats the solution program as a composition of operators and operands, segregating their generation processes.","Furthermore, we introduce the geometry elements enhancement method, which enhances the ability of GAPS to recognize geometry elements accurately.","By leveraging these improvements, GAPS showcases remarkable performance in resolving geometry math problems.","Our experiments conducted on the UniGeo dataset demonstrate the superiority of GAPS over the state-of-the-art model, Geoformer.","Specifically, GAPS achieves an accuracy improvement of more than 5.3% for calculation tasks and an impressive 41.1% for proving tasks.","Notably, GAPS achieves an impressive accuracy of 97.5% on proving problems, representing a significant advancement in solving geometry proving tasks."],"url":"http://arxiv.org/abs/2401.16287v1","category":"cs.AI"}
{"created":"2024-01-29 16:48:34","title":"Upper bounds on the rate of linear $q$-ary $k$-hash codes","abstract":"This paper presents new upper bounds on the rate of linear $k$-hash codes in $\\mathbb{F}_q^n$, $q\\geq k$, that is, codes with the property that any $k$ distinct codewords are all simultaneously distinct in at least one coordinate.","sentences":["This paper presents new upper bounds on the rate of linear $k$-hash codes in $\\mathbb{F}_q^n$, $q\\geq k$, that is, codes with the property that any $k$ distinct codewords are all simultaneously distinct in at least one coordinate."],"url":"http://arxiv.org/abs/2401.16288v1","category":"cs.IT"}
{"created":"2024-01-29 16:42:34","title":"Capturing Pertinent Symbolic Features for Enhanced Content-Based Misinformation Detection","abstract":"Preventing the spread of misinformation is challenging. The detection of misleading content presents a significant hurdle due to its extreme linguistic and domain variability. Content-based models have managed to identify deceptive language by learning representations from textual data such as social media posts and web articles. However, aggregating representative samples of this heterogeneous phenomenon and implementing effective real-world applications is still elusive. Based on analytical work on the language of misinformation, this paper analyzes the linguistic attributes that characterize this phenomenon and how representative of such features some of the most popular misinformation datasets are. We demonstrate that the appropriate use of pertinent symbolic knowledge in combination with neural language models is helpful in detecting misleading content. Our results achieve state-of-the-art performance in misinformation datasets across the board, showing that our approach offers a valid and robust alternative to multi-task transfer learning without requiring any additional training data. Furthermore, our results show evidence that structured knowledge can provide the extra boost required to address a complex and unpredictable real-world problem like misinformation detection, not only in terms of accuracy but also time efficiency and resource utilization.","sentences":["Preventing the spread of misinformation is challenging.","The detection of misleading content presents a significant hurdle due to its extreme linguistic and domain variability.","Content-based models have managed to identify deceptive language by learning representations from textual data such as social media posts and web articles.","However, aggregating representative samples of this heterogeneous phenomenon and implementing effective real-world applications is still elusive.","Based on analytical work on the language of misinformation, this paper analyzes the linguistic attributes that characterize this phenomenon and how representative of such features some of the most popular misinformation datasets are.","We demonstrate that the appropriate use of pertinent symbolic knowledge in combination with neural language models is helpful in detecting misleading content.","Our results achieve state-of-the-art performance in misinformation datasets across the board, showing that our approach offers a valid and robust alternative to multi-task transfer learning without requiring any additional training data.","Furthermore, our results show evidence that structured knowledge can provide the extra boost required to address a complex and unpredictable real-world problem like misinformation detection, not only in terms of accuracy but also time efficiency and resource utilization."],"url":"http://arxiv.org/abs/2401.16285v1","category":"cs.CL"}
{"created":"2024-01-29 16:42:15","title":"Leveraging Positional Encoding for Robust Multi-Reference-Based Object 6D Pose Estimation","abstract":"Accurately estimating the pose of an object is a crucial task in computer vision and robotics. There are two main deep learning approaches for this: geometric representation regression and iterative refinement. However, these methods have some limitations that reduce their effectiveness. In this paper, we analyze these limitations and propose new strategies to overcome them. To tackle the issue of blurry geometric representation, we use positional encoding with high-frequency components for the object's 3D coordinates. To address the local minimum problem in refinement methods, we introduce a normalized image plane-based multi-reference refinement strategy that's independent of intrinsic matrix constraints. Lastly, we utilize adaptive instance normalization and a simple occlusion augmentation method to help our model concentrate on the target object. Our experiments on Linemod, Linemod-Occlusion, and YCB-Video datasets demonstrate that our approach outperforms existing methods. We will soon release the code.","sentences":["Accurately estimating the pose of an object is a crucial task in computer vision and robotics.","There are two main deep learning approaches for this: geometric representation regression and iterative refinement.","However, these methods have some limitations that reduce their effectiveness.","In this paper, we analyze these limitations and propose new strategies to overcome them.","To tackle the issue of blurry geometric representation, we use positional encoding with high-frequency components for the object's 3D coordinates.","To address the local minimum problem in refinement methods, we introduce a normalized image plane-based multi-reference refinement strategy that's independent of intrinsic matrix constraints.","Lastly, we utilize adaptive instance normalization and a simple occlusion augmentation method to help our model concentrate on the target object.","Our experiments on Linemod, Linemod-Occlusion, and YCB-Video datasets demonstrate that our approach outperforms existing methods.","We will soon release the code."],"url":"http://arxiv.org/abs/2401.16284v1","category":"cs.CV"}
{"created":"2024-01-29 16:39:43","title":"Gravity versus Noncommutative Gauge Theory: A Double Copy Perspective","abstract":"We discuss how Moyal deformations of gauge theories, which arise naturally from open string theory, fit into the paradigm of colour-kinematics duality and the double copy of gauge theory to gravity. Along the way we encounter novel noncommutative scalar field theories with rigid colour symmetry that have no interacting commutative counterparts. These scalar theories offer new perspectives on old ideas that rank one noncommutative gauge theories are gravitational theories. This is rendered explicit in four dimensions where they and their double copy images yield deformations of integrable theories describing the self-dual sectors of Yang-Mills theory and gravity.","sentences":["We discuss how Moyal deformations of gauge theories, which arise naturally from open string theory, fit into the paradigm of colour-kinematics duality and the double copy of gauge theory to gravity.","Along the way we encounter novel noncommutative scalar field theories with rigid colour symmetry that have no interacting commutative counterparts.","These scalar theories offer new perspectives on old ideas that rank one noncommutative gauge theories are gravitational theories.","This is rendered explicit in four dimensions where they and their double copy images yield deformations of integrable theories describing the self-dual sectors of Yang-Mills theory and gravity."],"url":"http://arxiv.org/abs/2401.16283v1","category":"hep-th"}
{"created":"2024-01-29 16:39:39","title":"MAPLE: Micro Analysis of Pairwise Language Evolution for Few-Shot Claim Verification","abstract":"Claim verification is an essential step in the automated fact-checking pipeline which assesses the veracity of a claim against a piece of evidence. In this work, we explore the potential of few-shot claim verification, where only very limited data is available for supervision. We propose MAPLE (Micro Analysis of Pairwise Language Evolution), a pioneering approach that explores the alignment between a claim and its evidence with a small seq2seq model and a novel semantic measure. Its innovative utilization of micro language evolution path leverages unlabelled pairwise data to facilitate claim verification while imposing low demand on data annotations and computing resources. MAPLE demonstrates significant performance improvements over SOTA baselines SEED, PET and LLaMA 2 across three fact-checking datasets: FEVER, Climate FEVER, and SciFact. Data and code are available here: https://github.com/XiaZeng0223/MAPLE","sentences":["Claim verification is an essential step in the automated fact-checking pipeline which assesses the veracity of a claim against a piece of evidence.","In this work, we explore the potential of few-shot claim verification, where only very limited data is available for supervision.","We propose MAPLE (Micro Analysis of Pairwise Language Evolution), a pioneering approach that explores the alignment between a claim and its evidence with a small seq2seq model and a novel semantic measure.","Its innovative utilization of micro language evolution path leverages unlabelled pairwise data to facilitate claim verification while imposing low demand on data annotations and computing resources.","MAPLE demonstrates significant performance improvements over SOTA baselines SEED, PET and LLaMA 2 across three fact-checking datasets: FEVER, Climate FEVER, and SciFact.","Data and code are available here: https://github.com/XiaZeng0223/MAPLE"],"url":"http://arxiv.org/abs/2401.16282v1","category":"cs.CL"}
{"created":"2024-01-29 16:37:00","title":"Cutup and Detect: Human Fall Detection on Cutup Untrimmed Videos Using a Large Foundational Video Understanding Model","abstract":"This work explores the performance of a large video understanding foundation model on the downstream task of human fall detection on untrimmed video and leverages a pretrained vision transformer for multi-class action detection, with classes: \"Fall\", \"Lying\" and \"Other/Activities of daily living (ADL)\". A method for temporal action localization that relies on a simple cutup of untrimmed videos is demonstrated. The methodology includes a preprocessing pipeline that converts datasets with timestamp action annotations into labeled datasets of short action clips. Simple and effective clip-sampling strategies are introduced. The effectiveness of the proposed method has been empirically evaluated on the publicly available High-Quality Fall Simulation Dataset (HQFSD). The experimental results validate the performance of the proposed pipeline. The results are promising for real-time application, and the falls are detected on video level with a state-of-the-art 0.96 F1 score on the HQFSD dataset under the given experimental settings. The source code will be made available on GitHub.","sentences":["This work explores the performance of a large video understanding foundation model on the downstream task of human fall detection on untrimmed video and leverages a pretrained vision transformer for multi-class action detection, with classes: \"Fall\", \"Lying\" and \"Other/Activities of daily living (ADL)\".","A method for temporal action localization that relies on a simple cutup of untrimmed videos is demonstrated.","The methodology includes a preprocessing pipeline that converts datasets with timestamp action annotations into labeled datasets of short action clips.","Simple and effective clip-sampling strategies are introduced.","The effectiveness of the proposed method has been empirically evaluated on the publicly available High-Quality Fall Simulation Dataset (HQFSD).","The experimental results validate the performance of the proposed pipeline.","The results are promising for real-time application, and the falls are detected on video level with a state-of-the-art 0.96 F1 score on the HQFSD dataset under the given experimental settings.","The source code will be made available on GitHub."],"url":"http://arxiv.org/abs/2401.16280v1","category":"cs.CV"}
{"created":"2024-01-29 16:34:12","title":"Rethinking the Producer-Consumer Relationship in Modern DRAM-Based Systems","abstract":"Generational improvements to commodity DRAM throughout half a century have long solidified its prevalence as main memory across the computing industry. However, overcoming today's DRAM technology scaling challenges requires new solutions driven by both DRAM producers and consumers. In this paper, we observe that the separation of concerns between producers and consumers specified by industry-wide DRAM standards is becoming a liability to progress in addressing scaling-related concerns.   To understand the problem, we study four key directions for overcoming DRAM scaling challenges using system-memory cooperation: (i) improving memory access latencies; (ii) reducing DRAM refresh overheads; (iii) securely defending against the RowHammer vulnerability; and (iv) addressing worsening memory errors. We find that the single most important barrier to advancement in all four cases is the consumer's lack of insight into DRAM reliability. Based on an analysis of DRAM reliability testing, we recommend revising the separation of concerns to incorporate limited information transparency between producers and consumers. Finally, we propose adopting this revision in a two-step plan, starting with immediate information release through crowdsourcing and publication and culminating in widespread modifications to DRAM standards.","sentences":["Generational improvements to commodity DRAM throughout half a century have long solidified its prevalence as main memory across the computing industry.","However, overcoming today's DRAM technology scaling challenges requires new solutions driven by both DRAM producers and consumers.","In this paper, we observe that the separation of concerns between producers and consumers specified by industry-wide DRAM standards is becoming a liability to progress in addressing scaling-related concerns.   ","To understand the problem, we study four key directions for overcoming DRAM scaling challenges using system-memory cooperation: (i) improving memory access latencies; (ii) reducing DRAM refresh overheads; (iii) securely defending against the RowHammer vulnerability; and (iv) addressing worsening memory errors.","We find that the single most important barrier to advancement in all four cases is the consumer's lack of insight into DRAM reliability.","Based on an analysis of DRAM reliability testing, we recommend revising the separation of concerns to incorporate limited information transparency between producers and consumers.","Finally, we propose adopting this revision in a two-step plan, starting with immediate information release through crowdsourcing and publication and culminating in widespread modifications to DRAM standards."],"url":"http://arxiv.org/abs/2401.16279v1","category":"cs.AR"}
{"created":"2024-01-29 16:32:36","title":"SECOMP: Formally Secure Compilation of Compartmentalized C Programs","abstract":"Undefined behavior in C often causes devastating security vulnerabilities. One practical mitigation is compartmentalization, which allows developers to structure large programs into mutually distrustful compartments with clearly specified privileges and interactions. In this paper we introduce SECOMP, a compiler for compartmentalized C code that comes with machine-checked proofs guaranteeing that the scope of undefined behavior is restricted to the compartments that encounter it and become dynamically compromised. These guarantees are formalized as the preservation of safety properties against adversarial contexts, a secure compilation criterion similar to full abstraction, and this is the first time such a strong criterion is proven for a mainstream programming language. To achieve this we extend the languages of the CompCert verified C compiler with isolated compartments that can only interact via procedure calls and returns, as specified by cross-compartment interfaces. We adapt the passes and optimizations of CompCert as well as their correctness proofs to this compartment-aware setting. We then use compiler correctness as an ingredient in a larger secure compilation proof that involves several proof engineering novelties, needed to scale formally secure compilation up to a C compiler.","sentences":["Undefined behavior in C often causes devastating security vulnerabilities.","One practical mitigation is compartmentalization, which allows developers to structure large programs into mutually distrustful compartments with clearly specified privileges and interactions.","In this paper we introduce SECOMP, a compiler for compartmentalized C code that comes with machine-checked proofs guaranteeing that the scope of undefined behavior is restricted to the compartments that encounter it and become dynamically compromised.","These guarantees are formalized as the preservation of safety properties against adversarial contexts, a secure compilation criterion similar to full abstraction, and this is the first time such a strong criterion is proven for a mainstream programming language.","To achieve this we extend the languages of the CompCert verified C compiler with isolated compartments that can only interact via procedure calls and returns, as specified by cross-compartment interfaces.","We adapt the passes and optimizations of CompCert as well as their correctness proofs to this compartment-aware setting.","We then use compiler correctness as an ingredient in a larger secure compilation proof that involves several proof engineering novelties, needed to scale formally secure compilation up to a C compiler."],"url":"http://arxiv.org/abs/2401.16277v1","category":"cs.PL"}
{"created":"2024-01-29 16:25:38","title":"The HSF Conditions Database Reference Implementation","abstract":"Conditions data is the subset of non-event data that is necessary to process event data. It poses a unique set of challenges, namely a heterogeneous structure and high access rates by distributed computing. The HSF Conditions Databases activity is a forum for cross-experiment discussions inviting as broad a participation as possible. It grew out of the HSF Community White Paper work to study conditions data access, where experts from ATLAS, Belle II, and CMS converged on a common language and proposed a schema that represents best practice. Following discussions with a broader community, including NP as well as HEP experiments, a core set of use cases, functionality and behaviour was defined with the aim to describe a core conditions database API. This paper will describe the reference implementation of both the conditions database service and the client which together encapsulate HSF best practice conditions data handling. Django was chosen for the service implementation, which uses an ORM instead of the direct use of SQL for all but one method. The simple relational database schema to organise conditions data is implemented in PostgreSQL. The task of storing conditions data payloads themselves is outsourced to any POSIX- compliant filesystem, allowing for transparent relocation and redundancy. Cru- cially this design provides a clear separation between retrieving the metadata describing which conditions data are needed for a data processing job, and retrieving the actual payloads from storage. The service deployment using Helm on OKD will be described together with scaling tests and operations experience from the sPHENIX experiment running more than 25k cores at BNL.","sentences":["Conditions data is the subset of non-event data that is necessary to process event data.","It poses a unique set of challenges, namely a heterogeneous structure and high access rates by distributed computing.","The HSF Conditions Databases activity is a forum for cross-experiment discussions inviting as broad a participation as possible.","It grew out of the HSF Community White Paper work to study conditions data access, where experts from ATLAS, Belle II, and CMS converged on a common language and proposed a schema that represents best practice.","Following discussions with a broader community, including NP as well as HEP experiments, a core set of use cases, functionality and behaviour was defined with the aim to describe a core conditions database API.","This paper will describe the reference implementation of both the conditions database service and the client which together encapsulate HSF best practice conditions data handling.","Django was chosen for the service implementation, which uses an ORM instead of the direct use of SQL for all but one method.","The simple relational database schema to organise conditions data is implemented in PostgreSQL.","The task of storing conditions data payloads themselves is outsourced to any POSIX- compliant filesystem, allowing for transparent relocation and redundancy.","Cru- cially this design provides a clear separation between retrieving the metadata describing which conditions data are needed for a data processing job, and retrieving the actual payloads from storage.","The service deployment using Helm on OKD will be described together with scaling tests and operations experience from the sPHENIX experiment running more than 25k cores at BNL."],"url":"http://arxiv.org/abs/2401.16274v1","category":"cs.DB"}
{"created":"2024-01-29 16:18:54","title":"Capturing Knowledge Graphs and Rules with Octagon Embeddings","abstract":"Region based knowledge graph embeddings represent relations as geometric regions. This has the advantage that the rules which are captured by the model are made explicit, making it straightforward to incorporate prior knowledge and to inspect learned models. Unfortunately, existing approaches are severely restricted in their ability to model relational composition, and hence also their ability to model rules, thus failing to deliver on the main promise of region based models. With the aim of addressing these limitations, we investigate regions which are composed of axis-aligned octagons. Such octagons are particularly easy to work with, as intersections and compositions can be straightforwardly computed, while they are still sufficiently expressive to model arbitrary knowledge graphs. Among others, we also show that our octagon embeddings can properly capture a non-trivial class of rule bases. Finally, we show that our model achieves competitive experimental results.","sentences":["Region based knowledge graph embeddings represent relations as geometric regions.","This has the advantage that the rules which are captured by the model are made explicit, making it straightforward to incorporate prior knowledge and to inspect learned models.","Unfortunately, existing approaches are severely restricted in their ability to model relational composition, and hence also their ability to model rules, thus failing to deliver on the main promise of region based models.","With the aim of addressing these limitations, we investigate regions which are composed of axis-aligned octagons.","Such octagons are particularly easy to work with, as intersections and compositions can be straightforwardly computed, while they are still sufficiently expressive to model arbitrary knowledge graphs.","Among others, we also show that our octagon embeddings can properly capture a non-trivial class of rule bases.","Finally, we show that our model achieves competitive experimental results."],"url":"http://arxiv.org/abs/2401.16270v1","category":"cs.AI"}
{"created":"2024-01-29 16:14:29","title":"A.I. In All The Wrong Places","abstract":"This text describes experiences gained across a two-year test period during which two generations of Generative Artificial Intelligence (A.I.) systems were incorpo-rated into an interdisciplinary, university level course on A.I. for art and design practices. The text uses the results from the courses to reflect on new opportuni-ties for generative systems in art and design while considering traps and limits.","sentences":["This text describes experiences gained across a two-year test period during which two generations of Generative Artificial Intelligence (A.I.) systems were incorpo-rated into an interdisciplinary, university level course on A.I. for art and design practices.","The text uses the results from the courses to reflect on new opportuni-ties for generative systems in art and design while considering traps and limits."],"url":"http://arxiv.org/abs/2401.16268v1","category":"cs.CY"}
{"created":"2024-01-29 16:12:31","title":"CO2: Efficient Distributed Training with Full Communication-Computation Overlap","abstract":"The fundamental success of large language models hinges upon the efficacious implementation of large-scale distributed training techniques. Nevertheless, building a vast, high-performance cluster featuring high-speed communication interconnectivity is prohibitively costly, and accessible only to prominent entities. In this work, we aim to lower this barrier and democratize large-scale training with limited bandwidth clusters. We propose a new approach called CO2 that introduces local-updating and asynchronous communication to the distributed data-parallel training, thereby facilitating the full overlap of COmunication with COmputation. CO2 is able to attain a high scalability even on extensive multi-node clusters constrained by very limited communication bandwidth. We further propose the staleness gap penalty and outer momentum clipping techniques together with CO2 to bolster its convergence and training stability. Besides, CO2 exhibits seamless integration with well-established ZeRO-series optimizers which mitigate memory consumption of model states with large model training. We also provide a mathematical proof of convergence, accompanied by the establishment of a stringent upper bound. Furthermore, we validate our findings through an extensive set of practical experiments encompassing a wide range of tasks in the fields of computer vision and natural language processing. These experiments serve to demonstrate the capabilities of CO2 in terms of convergence, generalization, and scalability when deployed across configurations comprising up to 128 A100 GPUs. The outcomes emphasize the outstanding capacity of CO2 to hugely improve scalability, no matter on clusters with 800Gbps RDMA or 80Gbps TCP/IP inter-node connections.","sentences":["The fundamental success of large language models hinges upon the efficacious implementation of large-scale distributed training techniques.","Nevertheless, building a vast, high-performance cluster featuring high-speed communication interconnectivity is prohibitively costly, and accessible only to prominent entities.","In this work, we aim to lower this barrier and democratize large-scale training with limited bandwidth clusters.","We propose a new approach called CO2 that introduces local-updating and asynchronous communication to the distributed data-parallel training, thereby facilitating the full overlap of COmunication with COmputation.","CO2 is able to attain a high scalability even on extensive multi-node clusters constrained by very limited communication bandwidth.","We further propose the staleness gap penalty and outer momentum clipping techniques together with CO2 to bolster its convergence and training stability.","Besides, CO2 exhibits seamless integration with well-established ZeRO-series optimizers which mitigate memory consumption of model states with large model training.","We also provide a mathematical proof of convergence, accompanied by the establishment of a stringent upper bound.","Furthermore, we validate our findings through an extensive set of practical experiments encompassing a wide range of tasks in the fields of computer vision and natural language processing.","These experiments serve to demonstrate the capabilities of CO2 in terms of convergence, generalization, and scalability when deployed across configurations comprising up to 128 A100 GPUs.","The outcomes emphasize the outstanding capacity of CO2 to hugely improve scalability, no matter on clusters with 800Gbps RDMA or 80Gbps TCP/IP inter-node connections."],"url":"http://arxiv.org/abs/2401.16265v1","category":"cs.CL"}
{"created":"2024-01-29 16:12:18","title":"Collaboration Petri Nets: Verification, Equivalence, and Discovery (Extended Version)","abstract":"Process modeling and discovery techniques aim to construct sound and valid process models for different types of processes, i.e., process orchestrations and collaboration processes. Orchestrations represent behavior of cases within one process. Collaboration processes represent behavior of collaborating cases within multiple process orchestrations that interact via collaboration concepts such as organizations, agents, objects, and services. The heterogeneity of collaboration concepts and types such as message exchange and resource sharing has led to different representations and discovery techniques for collaboration process models, but a standard model class is lacking. We propose collaboration Petri nets (cPN) to achieve comparability between techniques, to enable approach and property transfer, and to build a standardized collaboration mining pipeline similar to process mining. For cPN, we require desirable modeling power, decision power, modeling convenience, and relations to existing model classes. We show the representation of collaboration types, structural characterization as workflow nets, automatic verification of soundness, bisimulation equivalence to existing model classes, and application in a general discovery framework. As empirical evidence to discover cPN, we conduct a comparative evaluation between three discovery techniques on a set of existing collaboration event logs.","sentences":["Process modeling and discovery techniques aim to construct sound and valid process models for different types of processes, i.e., process orchestrations and collaboration processes.","Orchestrations represent behavior of cases within one process.","Collaboration processes represent behavior of collaborating cases within multiple process orchestrations that interact via collaboration concepts such as organizations, agents, objects, and services.","The heterogeneity of collaboration concepts and types such as message exchange and resource sharing has led to different representations and discovery techniques for collaboration process models, but a standard model class is lacking.","We propose collaboration Petri nets (cPN) to achieve comparability between techniques, to enable approach and property transfer, and to build a standardized collaboration mining pipeline similar to process mining.","For cPN, we require desirable modeling power, decision power, modeling convenience, and relations to existing model classes.","We show the representation of collaboration types, structural characterization as workflow nets, automatic verification of soundness, bisimulation equivalence to existing model classes, and application in a general discovery framework.","As empirical evidence to discover cPN, we conduct a comparative evaluation between three discovery techniques on a set of existing collaboration event logs."],"url":"http://arxiv.org/abs/2401.16263v1","category":"cs.FL"}
{"created":"2024-01-29 16:09:45","title":"Optical properties of black holes in regularized Maxwell theory","abstract":"Regularized Maxwell electrodynamics is a recently discovered theory of non-linear electrodynamics, with a \"minimally regularized\" field strength of a point charge, that is \"very close\" to the Maxwell theory in many aspects. In this paper we investigate some of the optical properties of its black holes. Namely, we study geodesics, gravitational red-shift, black hole shadow, as well as investigate the relationship between the behavior of (null geodesic) Lyapunov exponents and the existence of thermodynamic critical points in both canonical and grand-canonical ensembles.","sentences":["Regularized Maxwell electrodynamics is a recently discovered theory of non-linear electrodynamics, with a \"minimally regularized\" field strength of a point charge, that is \"very close\" to the Maxwell theory in many aspects.","In this paper we investigate some of the optical properties of its black holes.","Namely, we study geodesics, gravitational red-shift, black hole shadow, as well as investigate the relationship between the behavior of (null geodesic)","Lyapunov exponents and the existence of thermodynamic critical points in both canonical and grand-canonical ensembles."],"url":"http://arxiv.org/abs/2401.16259v1","category":"gr-qc"}
{"created":"2024-01-29 16:08:18","title":"MosquIoT: A System Based on IoT and Machine Learning for the Monitoring of Aedes aegypti (Diptera: Culicidae)","abstract":"Millions of people around the world are infected with mosquito-borne diseases each year. One of the most dangerous species is Aedes aegypti, the main vector of viruses such as dengue, yellow fever, chikungunya, and Zika, among others. Mosquito prevention and eradication campaigns are essential to avoid major public health consequences. In this respect, entomological surveillance is an important tool. At present, this traditional monitoring tool is executed manually and requires digital transformation to help authorities make better decisions, improve their planning efforts, speed up execution, and better manage available resources. Therefore, new technological tools based on proven techniques need to be designed and developed. However, such tools should also be cost-effective, autonomous, reliable, and easy to implement, and should be enabled by connectivity and multi-platform software applications. This paper presents the design, development, and testing of an innovative system named MosquIoT. It is based on traditional ovitraps with embedded Internet of Things (IoT) and Tiny Machine Learning (TinyML) technologies, which enable the detection and quantification of Ae. aegypti eggs. This innovative and promising solution may help dynamically understand the behavior of Ae. aegypti populations in cities, shifting from the current reactive entomological monitoring model to a proactive and predictive digital one.","sentences":["Millions of people around the world are infected with mosquito-borne diseases each year.","One of the most dangerous species is Aedes aegypti, the main vector of viruses such as dengue, yellow fever, chikungunya, and Zika, among others.","Mosquito prevention and eradication campaigns are essential to avoid major public health consequences.","In this respect, entomological surveillance is an important tool.","At present, this traditional monitoring tool is executed manually and requires digital transformation to help authorities make better decisions, improve their planning efforts, speed up execution, and better manage available resources.","Therefore, new technological tools based on proven techniques need to be designed and developed.","However, such tools should also be cost-effective, autonomous, reliable, and easy to implement, and should be enabled by connectivity and multi-platform software applications.","This paper presents the design, development, and testing of an innovative system named MosquIoT.","It is based on traditional ovitraps with embedded Internet of Things (IoT) and Tiny Machine Learning (TinyML) technologies, which enable the detection and quantification of Ae. aegypti eggs.","This innovative and promising solution may help dynamically understand the behavior of Ae. aegypti populations in cities, shifting from the current reactive entomological monitoring model to a proactive and predictive digital one."],"url":"http://arxiv.org/abs/2401.16258v1","category":"cs.LG"}
{"created":"2024-01-29 16:01:46","title":"Cross-silo Federated Learning with Record-level Personalized Differential Privacy","abstract":"Federated learning enhanced by differential privacy has emerged as a popular approach to better safeguard the privacy of client-side data by protecting clients' contributions during the training process. Existing solutions typically assume a uniform privacy budget for all records and provide one-size-fits-all solutions that may not be adequate to meet each record's privacy requirement. In this paper, we explore the uncharted territory of cross-silo FL with record-level personalized differential privacy. We devise a novel framework named rPDP-FL, employing a two-stage hybrid sampling scheme with both client-level sampling and non-uniform record-level sampling to accommodate varying privacy requirements. A critical and non-trivial problem is to select the ideal per-record sampling probability q given the personalized privacy budget {\\epsilon}. We introduce a versatile solution named Simulation-CurveFitting, allowing us to uncover a significant insight into the nonlinear correlation between q and {\\epsilon} and derive an elegant mathematical model to tackle the problem. Our evaluation demonstrates that our solution can provide significant performance gains over the baselines that do not consider personalized privacy preservation.","sentences":["Federated learning enhanced by differential privacy has emerged as a popular approach to better safeguard the privacy of client-side data by protecting clients' contributions during the training process.","Existing solutions typically assume a uniform privacy budget for all records and provide one-size-fits-all solutions that may not be adequate to meet each record's privacy requirement.","In this paper, we explore the uncharted territory of cross-silo FL with record-level personalized differential privacy.","We devise a novel framework named rPDP-FL, employing a two-stage hybrid sampling scheme with both client-level sampling and non-uniform record-level sampling to accommodate varying privacy requirements.","A critical and non-trivial problem is to select the ideal per-record sampling probability q given the personalized privacy budget {\\epsilon}.","We introduce a versatile solution named Simulation-CurveFitting, allowing us to uncover a significant insight into the nonlinear correlation between q and {\\epsilon} and derive an elegant mathematical model to tackle the problem.","Our evaluation demonstrates that our solution can provide significant performance gains over the baselines that do not consider personalized privacy preservation."],"url":"http://arxiv.org/abs/2401.16251v1","category":"cs.CR"}
{"created":"2024-01-29 15:52:11","title":"Molecular dynamics simulations of heat transport using machine-learned potentials: A mini review and tutorial on GPUMD with neuroevolution potentials","abstract":"Molecular dynamics (MD) simulations play an important role in understanding and engineering heat transport properties of complex materials. An essential requirement for reliably predicting heat transport properties is the use of accurate and efficient interatomic potentials. Recently, machine-learned potentials (MLPs) have shown great promise in providing the required accuracy for a broad range of materials. In this mini review and tutorial, we delve into the fundamentals of heat transport, explore pertinent MD simulation methods, and survey the applications of MLPs in MD simulations of heat transport. Furthermore, we provide a step-by-step tutorial on developing MLPs for highly efficient and predictive heat transport simulations, utilizing the neuroevolution potentials (NEPs) as implemented in the GPUMD package. Our aim with this mini review and tutorial is to empower researchers with valuable insights into cutting-edge methodologies that can significantly enhance the accuracy and efficiency of MD simulations for heat transport studies.","sentences":["Molecular dynamics (MD) simulations play an important role in understanding and engineering heat transport properties of complex materials.","An essential requirement for reliably predicting heat transport properties is the use of accurate and efficient interatomic potentials.","Recently, machine-learned potentials (MLPs) have shown great promise in providing the required accuracy for a broad range of materials.","In this mini review and tutorial, we delve into the fundamentals of heat transport, explore pertinent MD simulation methods, and survey the applications of MLPs in MD simulations of heat transport.","Furthermore, we provide a step-by-step tutorial on developing MLPs for highly efficient and predictive heat transport simulations, utilizing the neuroevolution potentials (NEPs) as implemented in the GPUMD package.","Our aim with this mini review and tutorial is to empower researchers with valuable insights into cutting-edge methodologies that can significantly enhance the accuracy and efficiency of MD simulations for heat transport studies."],"url":"http://arxiv.org/abs/2401.16249v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-01-29 15:50:54","title":"Random Geometry and Quantum Spacetime: From scale-invariant random geometries and asymptotic safety to random hyperbolic surfaces and JT gravity","abstract":"This thesis is driven by a central question: \"What can we learn from random geometries about the structure of quantum spacetime?\" In Chapter 2, we provide a partial review of the mathematical foundation of this thesis, random geometry. In Chapter 3, we use a construction coming from random geometry called Mating of Trees to build scale-invariant random geometries that appear in Liouville Quantum Gravity and have the potential to implement the UV fixed point predicted by Asymptotic Safety in two and three dimensions. In Chapter 4 we explore the random geometry formulation of JT gravity and how our understanding of random critical maps yields the discovery of a new family of deformations of JT gravity. Furthermore, the connection between JT gravity and matrix models leads us to delve deeper into the link between discrete geometry and hyperbolic surfaces, building upon the geometry of metric maps and irreducible metric maps in Chapter 5.","sentences":["This thesis is driven by a central question: \"What can we learn from random geometries about the structure of quantum spacetime?\"","In Chapter 2, we provide a partial review of the mathematical foundation of this thesis, random geometry.","In Chapter 3, we use a construction coming from random geometry called Mating of Trees to build scale-invariant random geometries that appear in Liouville Quantum Gravity and have the potential to implement the UV fixed point predicted by Asymptotic Safety in two and three dimensions.","In Chapter 4 we explore the random geometry formulation of JT gravity and how our understanding of random critical maps yields the discovery of a new family of deformations of JT gravity.","Furthermore, the connection between JT gravity and matrix models leads us to delve deeper into the link between discrete geometry and hyperbolic surfaces, building upon the geometry of metric maps and irreducible metric maps in Chapter 5."],"url":"http://arxiv.org/abs/2401.16248v1","category":"hep-th"}
{"created":"2024-01-29 15:49:40","title":"Towards Red Teaming in Multimodal and Multilingual Translation","abstract":"Assessing performance in Natural Language Processing is becoming increasingly complex. One particular challenge is the potential for evaluation datasets to overlap with training data, either directly or indirectly, which can lead to skewed results and overestimation of model performance. As a consequence, human evaluation is gaining increasing interest as a means to assess the performance and reliability of models. One such method is the red teaming approach, which aims to generate edge cases where a model will produce critical errors. While this methodology is becoming standard practice for generative AI, its application to the realm of conditional AI remains largely unexplored. This paper presents the first study on human-based red teaming for Machine Translation (MT), marking a significant step towards understanding and improving the performance of translation models. We delve into both human-based red teaming and a study on automation, reporting lessons learned and providing recommendations for both translation models and red teaming drills. This pioneering work opens up new avenues for research and development in the field of MT.","sentences":["Assessing performance in Natural Language Processing is becoming increasingly complex.","One particular challenge is the potential for evaluation datasets to overlap with training data, either directly or indirectly, which can lead to skewed results and overestimation of model performance.","As a consequence, human evaluation is gaining increasing interest as a means to assess the performance and reliability of models.","One such method is the red teaming approach, which aims to generate edge cases where a model will produce critical errors.","While this methodology is becoming standard practice for generative AI, its application to the realm of conditional AI remains largely unexplored.","This paper presents the first study on human-based red teaming for Machine Translation (MT), marking a significant step towards understanding and improving the performance of translation models.","We delve into both human-based red teaming and a study on automation, reporting lessons learned and providing recommendations for both translation models and red teaming drills.","This pioneering work opens up new avenues for research and development in the field of MT."],"url":"http://arxiv.org/abs/2401.16247v1","category":"cs.CL"}
{"created":"2024-01-29 15:49:28","title":"Solving intractable chemical problems by tensor decomposition","abstract":"Many complex chemical problems encoded in terms of physics-based models become computationally intractable for traditional numerical approaches due to their unfavourable scaling with increasing molecular size. Tensor decomposition techniques can overcome such challenges by decomposing unattainably large numerical representations of chemical problems into smaller, tractable ones. In the first two decades of this century, algorithms based on such tensor factorizations have become state-of-the-art methods in various branches of computational chemistry, ranging from molecular quantum dynamics to electronic structure theory and machine learning. Here, we consider the role that tensor decomposition schemes have played in expanding the scope of computational chemistry. We relate some of the most prominent methods to their common underlying tensor network formalism, providing a unified perspective on leading tensor-based approaches in chemistry and materials science.","sentences":["Many complex chemical problems encoded in terms of physics-based models become computationally intractable for traditional numerical approaches due to their unfavourable scaling with increasing molecular size.","Tensor decomposition techniques can overcome such challenges by decomposing unattainably large numerical representations of chemical problems into smaller, tractable ones.","In the first two decades of this century, algorithms based on such tensor factorizations have become state-of-the-art methods in various branches of computational chemistry, ranging from molecular quantum dynamics to electronic structure theory and machine learning.","Here, we consider the role that tensor decomposition schemes have played in expanding the scope of computational chemistry.","We relate some of the most prominent methods to their common underlying tensor network formalism, providing a unified perspective on leading tensor-based approaches in chemistry and materials science."],"url":"http://arxiv.org/abs/2401.16246v1","category":"physics.chem-ph"}
{"created":"2024-01-29 15:45:06","title":"Competition between phase ordering and phase segregation in the Ti$_x$NbMoTaW and Ti$_x$VNbMoTaW refractory high-entropy alloys","abstract":"Refractory high-entropy alloys are under consideration for applications where materials are subjected to high temperatures and levels of radiation, such as in the fusion power sector. However, at present, their scope is limited because they are highly brittle at room temperature. One suggested route to mitigate this issue is by alloying with Ti. In this theoretical study, using a computationally efficient linear-response theory based on density functional theory calculations of the electronic structure of the disordered alloys, we study the nature of atomic short-range order in these multi-component materials, as well as assessing their overall phase stability. Our analysis enables direct inference of phase transitions in addition to the extraction of an atomistic, pairwise model of the internal energy of an alloy suitable for study via, e.g. Monte Carlo simulations. Once Ti is added into either the NbMoTaW or VNbMoTaW system, we find that there is competition between chemical phase ordering and segregation. These results shed light on observed chemical inhomogeneity in experimental samples, as well as providing fundamental insight into the physics of these complex systems.","sentences":["Refractory high-entropy alloys are under consideration for applications where materials are subjected to high temperatures and levels of radiation, such as in the fusion power sector.","However, at present, their scope is limited because they are highly brittle at room temperature.","One suggested route to mitigate this issue is by alloying with Ti.","In this theoretical study, using a computationally efficient linear-response theory based on density functional theory calculations of the electronic structure of the disordered alloys, we study the nature of atomic short-range order in these multi-component materials, as well as assessing their overall phase stability.","Our analysis enables direct inference of phase transitions in addition to the extraction of an atomistic, pairwise model of the internal energy of an alloy suitable for study via, e.g. Monte Carlo simulations.","Once Ti is added into either the NbMoTaW or VNbMoTaW system, we find that there is competition between chemical phase ordering and segregation.","These results shed light on observed chemical inhomogeneity in experimental samples, as well as providing fundamental insight into the physics of these complex systems."],"url":"http://arxiv.org/abs/2401.16243v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-01-29 15:43:31","title":"Channel Estimation and Hybrid Precoding for Frequency Selective Multiuser mmWave MIMO Systems","abstract":"Configuring the hybrid precoders and combiners in a millimeter wave (mmWave) multiuser (MU) multiple-input multiple-output (MIMO) system is challenging in frequency selective channels. In this paper, we develop a system that uses compressive estimation on the uplink to configure precoders and combiners for the downlink (DL). In the first step, the base station (BS) simultaneously estimates the channels from all the mobile stations (MSs) on each subcarrier. To reduce the number of measurements required, compressed sensing techniques are developed that exploit common support on the different subcarriers. In the second step, exploiting reciprocity and the channel estimates, the base station designs hybrid precoders and combiners. Two algorithms are developed for this purpose, with different performance and complexity tradeoffs: 1) a factorization of the purely digital solution, and 2) an iterative hybrid design. Extensive numerical experiments evaluate the proposed solutions comparing to state-of-the-art strategies, and illustrating design tradeoffs in overhead, complexity, and performance.","sentences":["Configuring the hybrid precoders and combiners in a millimeter wave (mmWave) multiuser (MU) multiple-input multiple-output (MIMO) system is challenging in frequency selective channels.","In this paper, we develop a system that uses compressive estimation on the uplink to configure precoders and combiners for the downlink (DL).","In the first step, the base station (BS) simultaneously estimates the channels from all the mobile stations (MSs) on each subcarrier.","To reduce the number of measurements required, compressed sensing techniques are developed that exploit common support on the different subcarriers.","In the second step, exploiting reciprocity and the channel estimates, the base station designs hybrid precoders and combiners.","Two algorithms are developed for this purpose, with different performance and complexity tradeoffs: 1) a factorization of the purely digital solution, and 2) an iterative hybrid design.","Extensive numerical experiments evaluate the proposed solutions comparing to state-of-the-art strategies, and illustrating design tradeoffs in overhead, complexity, and performance."],"url":"http://arxiv.org/abs/2401.16241v1","category":"cs.IT"}
{"created":"2024-01-29 15:42:57","title":"Clinically meaningful timeline summarisation in social media for mental health monitoring","abstract":"We introduce the new task of clinically meaningful summarisation of social media user timelines, appropriate for mental health monitoring. We develop a novel approach for unsupervised abstractive summarisation that produces a two-layer summary consisting of both high-level information, covering aspects useful to clinical experts, as well as accompanying time sensitive evidence from a user's social media timeline. A key methodological novelty comes from the timeline summarisation component based on a version of hierarchical variational autoencoder (VAE) adapted to represent long texts and guided by LLM-annotated key phrases. The resulting timeline summary is input into a LLM (LLaMA-2) to produce the final summary containing both the high level information, obtained through instruction prompting, as well as corresponding evidence from the user's timeline. We assess the summaries generated by our novel architecture via automatic evaluation against expert written summaries and via human evaluation with clinical experts, showing that timeline summarisation by TH-VAE results in logically coherent summaries rich in clinical utility and superior to LLM-only approaches in capturing changes over time.","sentences":["We introduce the new task of clinically meaningful summarisation of social media user timelines, appropriate for mental health monitoring.","We develop a novel approach for unsupervised abstractive summarisation that produces a two-layer summary consisting of both high-level information, covering aspects useful to clinical experts, as well as accompanying time sensitive evidence from a user's social media timeline.","A key methodological novelty comes from the timeline summarisation component based on a version of hierarchical variational autoencoder (VAE) adapted to represent long texts and guided by LLM-annotated key phrases.","The resulting timeline summary is input into a LLM (LLaMA-2) to produce the final summary containing both the high level information, obtained through instruction prompting, as well as corresponding evidence from the user's timeline.","We assess the summaries generated by our novel architecture via automatic evaluation against expert written summaries and via human evaluation with clinical experts, showing that timeline summarisation by TH-VAE results in logically coherent summaries rich in clinical utility and superior to LLM-only approaches in capturing changes over time."],"url":"http://arxiv.org/abs/2401.16240v1","category":"cs.CL"}
{"created":"2024-01-29 15:42:22","title":"Alternating Minimization for Wideband Multiuser IRS-aided MIMO Systems under Imperfect CSI","abstract":"This work focuses on wideband intelligent reflecting surface (IRS)-aided multiuser MIMO systems. One of the major challenges of this scenario is the joint design of the frequency-dependent base station (BS) precoder and user filters, and the IRS phase-shift matrix which is frequency flat and common to all the users. In addition, we consider that the channel state information (CSI) is imperfect at both the transmitter and the receivers. A statistical model for the imperfect CSI is developed and exploited for the system design. A minimum mean square error (MMSE) approach is followed to determine the IRS phase-shift matrix, the transmit precoders, and the receiving filters. The broadcast (BC)- multiple access channel (MAC) duality is used to solve the optimization problem following an alternating minimization approach. Numerical results show that the proposed approach leads to substantial performance gains with respect to baseline strategies that neglect the inter-user interference and do not optimize the IRS phase-shift matrix. Further performance gains are obtained when incorporating into the system design the statistical information of the channel estimation errors.","sentences":["This work focuses on wideband intelligent reflecting surface (IRS)-aided multiuser MIMO systems.","One of the major challenges of this scenario is the joint design of the frequency-dependent base station (BS) precoder and user filters, and the IRS phase-shift matrix which is frequency flat and common to all the users.","In addition, we consider that the channel state information (CSI) is imperfect at both the transmitter and the receivers.","A statistical model for the imperfect CSI is developed and exploited for the system design.","A minimum mean square error (MMSE) approach is followed to determine the IRS phase-shift matrix, the transmit precoders, and the receiving filters.","The broadcast (BC)- multiple access channel (MAC) duality is used to solve the optimization problem following an alternating minimization approach.","Numerical results show that the proposed approach leads to substantial performance gains with respect to baseline strategies that neglect the inter-user interference and do not optimize the IRS phase-shift matrix.","Further performance gains are obtained when incorporating into the system design the statistical information of the channel estimation errors."],"url":"http://arxiv.org/abs/2401.16238v1","category":"cs.IT"}
{"created":"2024-01-29 15:35:05","title":"Effective Communication with Dynamic Feature Compression","abstract":"The remote wireless control of industrial systems is one of the major use cases for 5G and beyond systems: in these cases, the massive amounts of sensory information that need to be shared over the wireless medium may overload even high-capacity connections. Consequently, solving the effective communication problem by optimizing the transmission strategy to discard irrelevant information can provide a significant advantage, but is often a very complex task. In this work, we consider a prototypal system in which an observer must communicate its sensory data to a robot controlling a task (e.g., a mobile robot in a factory). We then model it as a remote Partially Observable Markov Decision Process (POMDP), considering the effect of adopting semantic and effective communication-oriented solutions on the overall system performance. We split the communication problem by considering an ensemble Vector Quantized Variational Autoencoder (VQ-VAE) encoding, and train a Deep Reinforcement Learning (DRL) agent to dynamically adapt the quantization level, considering both the current state of the environment and the memory of past messages. We tested the proposed approach on the well-known CartPole reference control problem, obtaining a significant performance increase over traditional approaches.","sentences":["The remote wireless control of industrial systems is one of the major use cases for 5G and beyond systems: in these cases, the massive amounts of sensory information that need to be shared over the wireless medium may overload even high-capacity connections.","Consequently, solving the effective communication problem by optimizing the transmission strategy to discard irrelevant information can provide a significant advantage, but is often a very complex task.","In this work, we consider a prototypal system in which an observer must communicate its sensory data to a robot controlling a task (e.g., a mobile robot in a factory).","We then model it as a remote Partially Observable Markov Decision Process (POMDP), considering the effect of adopting semantic and effective communication-oriented solutions on the overall system performance.","We split the communication problem by considering an ensemble Vector Quantized Variational Autoencoder (VQ-VAE) encoding, and train a Deep Reinforcement Learning (DRL) agent to dynamically adapt the quantization level, considering both the current state of the environment and the memory of past messages.","We tested the proposed approach on the well-known CartPole reference control problem, obtaining a significant performance increase over traditional approaches."],"url":"http://arxiv.org/abs/2401.16236v1","category":"cs.LG"}
{"created":"2024-01-29 15:34:49","title":"Player Pressure Map - A Novel Representation of Pressure in Soccer for Evaluating Player Performance in Different Game Contexts","abstract":"In soccer, contextual player performance metrics are invaluable to coaches. For example, the ability to perform under pressure during matches distinguishes the elite from the average. Appropriate pressure metric enables teams to assess players' performance accurately under pressure and design targeted training scenarios to address their weaknesses. The primary objective of this paper is to leverage both tracking and event data and game footage to capture the pressure experienced by the possession team in a soccer game scene. We propose a player pressure map to represent a given game scene, which lowers the dimension of raw data and still contains rich contextual information. Not only does it serve as an effective tool for visualizing and evaluating the pressure on the team and each individual, but it can also be utilized as a backbone for accessing players' performance. Overall, our model provides coaches and analysts with a deeper understanding of players' performance under pressure so that they make data-oriented tactical decisions.","sentences":["In soccer, contextual player performance metrics are invaluable to coaches.","For example, the ability to perform under pressure during matches distinguishes the elite from the average.","Appropriate pressure metric enables teams to assess players' performance accurately under pressure and design targeted training scenarios to address their weaknesses.","The primary objective of this paper is to leverage both tracking and event data and game footage to capture the pressure experienced by the possession team in a soccer game scene.","We propose a player pressure map to represent a given game scene, which lowers the dimension of raw data and still contains rich contextual information.","Not only does it serve as an effective tool for visualizing and evaluating the pressure on the team and each individual, but it can also be utilized as a backbone for accessing players' performance.","Overall, our model provides coaches and analysts with a deeper understanding of players' performance under pressure so that they make data-oriented tactical decisions."],"url":"http://arxiv.org/abs/2401.16235v1","category":"cs.LG"}
{"created":"2024-01-29 15:34:41","title":"DAEDALUS: Defense Against Firmware ROP Exploits Using Stochastic Software Diversity","abstract":"This paper presents DAEDALUS, a software diversity-based framework designed to resist ROP attacks on Linux-based IoT devices. DAEDALUS generates unique, semantically equivalent but syntactically different rewrites of IoT firmware, disrupting large-scale replication of ROP attacks. DAEDALUS employs STOKE, a stochastic optimizer for x86 binaries, as its core diversity engine but introduces significant extensions to address unique IoT firmware challenges. DAEDALUS's effectiveness is evaluated using DDoSim, a published botnet DDoS attack simulation testbed. Results demonstrate that DAEDALUS successfully neutralizes ROP payloads by diversifying critical basic blocks in the firmware, preventing attackers from compromising multiple devices for DDoS attacks via memory error vulnerabilities. The findings indicate that DAEDALUS not only mitigates the impact of ROP attacks on individual IoT devices through probabilistic protection but also thwarts large-scale ROP attacks across multiple devices.","sentences":["This paper presents DAEDALUS, a software diversity-based framework designed to resist ROP attacks on Linux-based IoT devices.","DAEDALUS generates unique, semantically equivalent but syntactically different rewrites of IoT firmware, disrupting large-scale replication of ROP attacks.","DAEDALUS employs STOKE, a stochastic optimizer for x86 binaries, as its core diversity engine","but introduces significant extensions to address unique IoT firmware challenges.","DAEDALUS's effectiveness is evaluated using DDoSim, a published botnet DDoS attack simulation testbed.","Results demonstrate that DAEDALUS successfully neutralizes ROP payloads by diversifying critical basic blocks in the firmware, preventing attackers from compromising multiple devices for DDoS attacks via memory error vulnerabilities.","The findings indicate that DAEDALUS not only mitigates the impact of ROP attacks on individual IoT devices through probabilistic protection but also thwarts large-scale ROP attacks across multiple devices."],"url":"http://arxiv.org/abs/2401.16234v1","category":"cs.CR"}
{"created":"2024-01-29 15:32:18","title":"Cross-Database Liveness Detection: Insights from Comparative Biometric Analysis","abstract":"In an era where biometric security serves as a keystone of modern identity verification systems, ensuring the authenticity of these biometric samples is paramount. Liveness detection, the capability to differentiate between genuine and spoofed biometric samples, stands at the forefront of this challenge. This research presents a comprehensive evaluation of liveness detection models, with a particular focus on their performance in cross-database scenarios, a test paradigm notorious for its complexity and real-world relevance. Our study commenced by meticulously assessing models on individual datasets, revealing the nuances in their performance metrics. Delving into metrics such as the Half Total Error Rate, False Acceptance Rate, and False Rejection Rate, we unearthed invaluable insights into the models' strengths and weaknesses. Crucially, our exploration of cross-database testing provided a unique perspective, highlighting the chasm between training on one dataset and deploying on another. Comparative analysis with extant methodologies, ranging from convolutional networks to more intricate strategies, enriched our understanding of the current landscape. The variance in performance, even among state-of-the-art models, underscored the inherent challenges in this domain. In essence, this paper serves as both a repository of findings and a clarion call for more nuanced, data-diverse, and adaptable approaches in biometric liveness detection. In the dynamic dance between authenticity and deception, our work offers a blueprint for navigating the evolving rhythms of biometric security.","sentences":["In an era where biometric security serves as a keystone of modern identity verification systems, ensuring the authenticity of these biometric samples is paramount.","Liveness detection, the capability to differentiate between genuine and spoofed biometric samples, stands at the forefront of this challenge.","This research presents a comprehensive evaluation of liveness detection models, with a particular focus on their performance in cross-database scenarios, a test paradigm notorious for its complexity and real-world relevance.","Our study commenced by meticulously assessing models on individual datasets, revealing the nuances in their performance metrics.","Delving into metrics such as the Half Total Error Rate, False Acceptance Rate, and False Rejection Rate, we unearthed invaluable insights into the models' strengths and weaknesses.","Crucially, our exploration of cross-database testing provided a unique perspective, highlighting the chasm between training on one dataset and deploying on another.","Comparative analysis with extant methodologies, ranging from convolutional networks to more intricate strategies, enriched our understanding of the current landscape.","The variance in performance, even among state-of-the-art models, underscored the inherent challenges in this domain.","In essence, this paper serves as both a repository of findings and a clarion call for more nuanced, data-diverse, and adaptable approaches in biometric liveness detection.","In the dynamic dance between authenticity and deception, our work offers a blueprint for navigating the evolving rhythms of biometric security."],"url":"http://arxiv.org/abs/2401.16232v1","category":"cs.CV"}
{"created":"2024-01-29 15:30:47","title":"Error Mitigation for Thermodynamic Computing","abstract":"While physics-based computing can offer speed and energy efficiency compared to digital computing, it also is subject to errors that must be mitigated. For example, many error mitigation methods have been proposed for quantum computing. However this error mitigation framework has yet to be applied to other physics-based computing paradigms. In this work, we consider thermodynamic computing, which has recently captured attention due to its relevance to artificial intelligence (AI) applications, such as probabilistic AI and generative AI. A key source of errors in this paradigm is the imprecision of the analog hardware components. Here, we introduce a method that reduces the overall error from a linear to a quadratic dependence (from $\\epsilon$ to $\\epsilon^2$) on the imprecision $\\epsilon$, for Gaussian sampling and linear algebra applications. The method involves sampling from an ensemble of imprecise distributions associated with various rounding events and then merging these samples. We numerically demonstrate the scalability of this method for dimensions greater than 1000. Finally, we implement this method on an actual thermodynamic computer and show $20\\%$ error reduction for matrix inversion; the first thermodynamic error mitigation experiment.","sentences":["While physics-based computing can offer speed and energy efficiency compared to digital computing, it also is subject to errors that must be mitigated.","For example, many error mitigation methods have been proposed for quantum computing.","However this error mitigation framework has yet to be applied to other physics-based computing paradigms.","In this work, we consider thermodynamic computing, which has recently captured attention due to its relevance to artificial intelligence (AI) applications, such as probabilistic AI and generative AI.","A key source of errors in this paradigm is the imprecision of the analog hardware components.","Here, we introduce a method that reduces the overall error from a linear to a quadratic dependence (from $\\epsilon$ to $\\epsilon^2$) on the imprecision $\\epsilon$, for Gaussian sampling and linear algebra applications.","The method involves sampling from an ensemble of imprecise distributions associated with various rounding events and then merging these samples.","We numerically demonstrate the scalability of this method for dimensions greater than 1000.","Finally, we implement this method on an actual thermodynamic computer and show $20\\%$ error reduction for matrix inversion; the first thermodynamic error mitigation experiment."],"url":"http://arxiv.org/abs/2401.16231v1","category":"cs.ET"}
{"created":"2024-01-29 15:30:29","title":"Elementary first-order model checking for sparse graphs","abstract":"It is known that for subgraph-closed graph classes the first-order model checking problem is fixed-parameter tractable if and only if the class is nowhere dense [Grohe, Kreutzer, Siebertz, STOC 2014]. However, the dependency on the formula size is non-elementary, and in fact, this is unavoidable even for the class of all trees [Frick and Grohe, LICS 2002]. On the other hand, it is known that the dependency is elementary for classes of bounded degree [Frick and Grohe, LICS 2002] as well as for classes of bounded pathwidth [Lampis, ICALP 2023]. In this paper we generalise these results and almost completely characterise subgraph-closed graph classes for which the model checking problem is fixed-parameter tractable with an elementary dependency on the formula size. Those are the graph classes for which there exists a number $d$ such that for every $r$, some tree of depth $d$ and size bounded by an elementary function of $r$ is avoided as an $({\\leq} r)$-subdivision in all graphs in the class. In particular, this implies that if the class in question excludes a fixed tree as a topological minor, then first-order model checking for graphs in the class is fixed-parameter tractable with an elementary dependency on the formula size.","sentences":["It is known that for subgraph-closed graph classes the first-order model checking problem is fixed-parameter tractable if and only if the class is nowhere dense [Grohe, Kreutzer, Siebertz, STOC 2014].","However, the dependency on the formula size is non-elementary, and in fact, this is unavoidable even for the class of all trees [Frick and Grohe, LICS 2002].","On the other hand, it is known that the dependency is elementary for classes of bounded degree","[Frick and Grohe, LICS 2002] as well as for classes of bounded pathwidth [Lampis, ICALP 2023].","In this paper we generalise these results and almost completely characterise subgraph-closed graph classes for which the model checking problem is fixed-parameter tractable with an elementary dependency on the formula size.","Those are the graph classes for which there exists a number $d$ such that for every $r$, some tree of depth $d$ and size bounded by an elementary function of $r$ is avoided as an $({\\leq} r)$-subdivision in all graphs in the class.","In particular, this implies that if the class in question excludes a fixed tree as a topological minor, then first-order model checking for graphs in the class is fixed-parameter tractable with an elementary dependency on the formula size."],"url":"http://arxiv.org/abs/2401.16230v1","category":"cs.LO"}
{"created":"2024-01-29 15:29:17","title":"On the Anatomy of Real-World R Code for Static Analysis","abstract":"CONTEXT The R programming language has a huge and active community, especially in the area of statistical computing. Its interpreted nature allows for several interesting constructs, like the manipulation of functions at run-time, that hinder the static analysis of R programs. At the same time, there is a lack of existing research regarding how these features, or even the R language as a whole are used in practice. OBJECTIVE In this paper, we conduct a large-scale, static analysis of more than 50 million lines of real-world R programs and packages to identify their characteristics and the features that are actually used. Moreover, we compare the similarities and differences between the scripts of R users and the implementations of package authors. We provide insights for static analysis tools like the lintr package as well as potential interpreter optimizations and uncover areas for future research. METHOD We analyze 4230 R scripts submitted alongside publications and the sources of 19450 CRAN packages for over 350000 R files, collecting and summarizing quantitative information for features of interest. RESULTS We find a high frequency of name-based indexing operations, assignments, and loops, but a low frequency for most of R's reflective functions. Furthermore, we find neither testing functions nor many calls to R's foreign function interface (FFI) in the publication submissions. CONCLUSION R scripts and package sources differ, for example, in their size, the way they include other packages, and their usage of R's reflective capabilities. We provide features that are used frequently and should be prioritized by static analysis tools, like operator assignments, function calls, and certain reflective functions like load.","sentences":["CONTEXT The R programming language has a huge and active community, especially in the area of statistical computing.","Its interpreted nature allows for several interesting constructs, like the manipulation of functions at run-time, that hinder the static analysis of R programs.","At the same time, there is a lack of existing research regarding how these features, or even the R language as a whole are used in practice.","OBJECTIVE","In this paper, we conduct a large-scale, static analysis of more than 50 million lines of real-world R programs and packages to identify their characteristics and the features that are actually used.","Moreover, we compare the similarities and differences between the scripts of R users and the implementations of package authors.","We provide insights for static analysis tools like the lintr package as well as potential interpreter optimizations and uncover areas for future research.","METHOD We analyze 4230 R scripts submitted alongside publications and the sources of 19450 CRAN packages for over 350000 R files, collecting and summarizing quantitative information for features of interest.","RESULTS We find a high frequency of name-based indexing operations, assignments, and loops, but a low frequency for most of R's reflective functions.","Furthermore, we find neither testing functions nor many calls to R's foreign function interface (FFI) in the publication submissions.","CONCLUSION R scripts and package sources differ, for example, in their size, the way they include other packages, and their usage of R's reflective capabilities.","We provide features that are used frequently and should be prioritized by static analysis tools, like operator assignments, function calls, and certain reflective functions like load."],"url":"http://arxiv.org/abs/2401.16228v1","category":"cs.PL"}
{"created":"2024-01-29 15:24:57","title":"Minimality in Finite-Dimensional ZW-Calculi","abstract":"The ZW-calculus is a graphical language capable of representing 2-dimensional quantum systems (qubit) through its diagrams, and manipulating them through its equational theory. We extend the formalism to accommodate finite dimensional Hilbert spaces beyond qubit systems. First we define a qudit version of the language, where all systems have the same arbitrary finite dimension d, and show that the provided equational theory is both complete -- i.e. semantical equivalence is entirely captured by the equations -- and minimal -- i.e. none of the equations are consequences of the others. We then extend the graphical language further to accommodate all finite dimensional Hilbert spaces at the same time. We again show the completeness of the provided equational theory.","sentences":["The ZW-calculus is a graphical language capable of representing 2-dimensional quantum systems (qubit) through its diagrams, and manipulating them through its equational theory.","We extend the formalism to accommodate finite dimensional Hilbert spaces beyond qubit systems.","First we define a qudit version of the language, where all systems have the same arbitrary finite dimension d, and show that the provided equational theory is both complete -- i.e. semantical equivalence is entirely captured by the equations -- and minimal -- i.e. none of the equations are consequences of the others.","We then extend the graphical language further to accommodate all finite dimensional Hilbert spaces at the same time.","We again show the completeness of the provided equational theory."],"url":"http://arxiv.org/abs/2401.16225v1","category":"quant-ph"}
{"created":"2024-01-29 15:21:37","title":"Diffutoon: High-Resolution Editable Toon Shading via Diffusion Models","abstract":"Toon shading is a type of non-photorealistic rendering task of animation. Its primary purpose is to render objects with a flat and stylized appearance. As diffusion models have ascended to the forefront of image synthesis methodologies, this paper delves into an innovative form of toon shading based on diffusion models, aiming to directly render photorealistic videos into anime styles. In video stylization, extant methods encounter persistent challenges, notably in maintaining consistency and achieving high visual quality. In this paper, we model the toon shading problem as four subproblems: stylization, consistency enhancement, structure guidance, and colorization. To address the challenges in video stylization, we propose an effective toon shading approach called \\textit{Diffutoon}. Diffutoon is capable of rendering remarkably detailed, high-resolution, and extended-duration videos in anime style. It can also edit the content according to prompts via an additional branch. The efficacy of Diffutoon is evaluated through quantitive metrics and human evaluation. Notably, Diffutoon surpasses both open-source and closed-source baseline approaches in our experiments. Our work is accompanied by the release of both the source code and example videos on Github (Project page: https://ecnu-cilab.github.io/DiffutoonProjectPage/).","sentences":["Toon shading is a type of non-photorealistic rendering task of animation.","Its primary purpose is to render objects with a flat and stylized appearance.","As diffusion models have ascended to the forefront of image synthesis methodologies, this paper delves into an innovative form of toon shading based on diffusion models, aiming to directly render photorealistic videos into anime styles.","In video stylization, extant methods encounter persistent challenges, notably in maintaining consistency and achieving high visual quality.","In this paper, we model the toon shading problem as four subproblems: stylization, consistency enhancement, structure guidance, and colorization.","To address the challenges in video stylization, we propose an effective toon shading approach called \\textit{Diffutoon}.","Diffutoon is capable of rendering remarkably detailed, high-resolution, and extended-duration videos in anime style.","It can also edit the content according to prompts via an additional branch.","The efficacy of Diffutoon is evaluated through quantitive metrics and human evaluation.","Notably, Diffutoon surpasses both open-source and closed-source baseline approaches in our experiments.","Our work is accompanied by the release of both the source code and example videos on Github (Project page: https://ecnu-cilab.github.io/DiffutoonProjectPage/)."],"url":"http://arxiv.org/abs/2401.16224v1","category":"cs.CV"}
{"created":"2024-01-29 15:10:09","title":"A mechanism for discovering semantic relationships among agent communication protocols","abstract":"One relevant aspect in the development of the Semantic Web framework is the achievement of a real inter-agents communication capability at the semantic level. Agents should be able to communicate with each other freely using different communication protocols, constituted by communication acts. For that scenario, we introduce in this paper an efficient mechanism presenting the following main features: - It promotes the description of the communication acts of protocols as classes that belong to a communication acts ontology, and associates to those acts a social commitment semantics formalized through predicates in the Event Calculus. - It is sustained on the idea that different protocols can be compared semantically by looking to the set of fluents associated to each branch of the protocols. Those sets are generated using Semantic Web technology rules. - It discovers the following types of protocol relationships: equivalence, specialization, restriction, prefix, suffix, infix and complement_to_infix.","sentences":["One relevant aspect in the development of the Semantic Web framework is the achievement of a real inter-agents communication capability at the semantic level.","Agents should be able to communicate with each other freely using different communication protocols, constituted by communication acts.","For that scenario, we introduce in this paper an efficient mechanism presenting the following main features: - It promotes the description of the communication acts of protocols as classes that belong to a communication acts ontology, and associates to those acts a social commitment semantics formalized through predicates in the Event Calculus.","- It is sustained on the idea that different protocols can be compared semantically by looking to the set of fluents associated to each branch of the protocols.","Those sets are generated using Semantic Web technology rules.","- It discovers the following types of protocol relationships: equivalence, specialization, restriction, prefix, suffix, infix and complement_to_infix."],"url":"http://arxiv.org/abs/2401.16216v1","category":"cs.MA"}
{"created":"2024-01-29 15:08:43","title":"A Unified Study on Sequentiality in Universal Classification with Empirically Observed Statistics","abstract":"In hypothesis testing problems, taking samples sequentially and stopping opportunistically to make the inference greatly enhances the reliability. The design of the stopping and inference policy, however, critically relies on the knowledge of the underlying distribution of each hypothesis. When the knowledge of distributions, say, $P_0$ and $P_1$ in the binary-hypothesis case, is replaced by empirically observed statistics from the respective distributions, the gain of sequentiality is less understood when subject to universality constraints. In this work, the gap is mended by a unified study on sequentiality in the universal binary classification problem. We propose a unified framework where the universality constraints are set on the expected stopping time as well as the type-I error exponent. The type-I error exponent is required to achieve a pre-set distribution-dependent constraint $\\lambda(P_0,P_1)$ for all $P_0,P_1$. The framework is employed to investigate a semi-sequential and a fully-sequential setup, so that fair comparison can be made with the fixed-length setup. The optimal type-II error exponents in different setups are characterized when the function $\\lambda$ satisfies mild continuity conditions. The benefit of sequentiality is shown by comparing the semi-sequential, the fully-sequential, and the fixed-length cases in representative examples of $\\lambda$. Conditions under which sequentiality eradicates the trade-off between error exponents are also derived.","sentences":["In hypothesis testing problems, taking samples sequentially and stopping opportunistically to make the inference greatly enhances the reliability.","The design of the stopping and inference policy, however, critically relies on the knowledge of the underlying distribution of each hypothesis.","When the knowledge of distributions, say, $P_0$ and $P_1$ in the binary-hypothesis case, is replaced by empirically observed statistics from the respective distributions, the gain of sequentiality is less understood when subject to universality constraints.","In this work, the gap is mended by a unified study on sequentiality in the universal binary classification problem.","We propose a unified framework where the universality constraints are set on the expected stopping time as well as the type-I error exponent.","The type-I error exponent is required to achieve a pre-set distribution-dependent constraint $\\lambda(P_0,P_1)$ for all $P_0,P_1$. The framework is employed to investigate a semi-sequential and a fully-sequential setup, so that fair comparison can be made with the fixed-length setup.","The optimal type-II error exponents in different setups are characterized when the function $\\lambda$ satisfies mild continuity conditions.","The benefit of sequentiality is shown by comparing the semi-sequential, the fully-sequential, and the fixed-length cases in representative examples of $\\lambda$. Conditions under which sequentiality eradicates the trade-off between error exponents are also derived."],"url":"http://arxiv.org/abs/2401.16213v1","category":"cs.IT"}
{"created":"2024-01-29 15:02:24","title":"MultiMUC: Multilingual Template Filling on MUC-4","abstract":"We introduce MultiMUC, the first multilingual parallel corpus for template filling, comprising translations of the classic MUC-4 template filling benchmark into five languages: Arabic, Chinese, Farsi, Korean, and Russian. We obtain automatic translations from a strong multilingual machine translation system and manually project the original English annotations into each target language. For all languages, we also provide human translations for sentences in the dev and test splits that contain annotated template arguments. Finally, we present baselines on MultiMUC both with state-of-the-art template filling models and with ChatGPT.","sentences":["We introduce MultiMUC, the first multilingual parallel corpus for template filling, comprising translations of the classic MUC-4 template filling benchmark into five languages: Arabic, Chinese, Farsi, Korean, and Russian.","We obtain automatic translations from a strong multilingual machine translation system and manually project the original English annotations into each target language.","For all languages, we also provide human translations for sentences in the dev and test splits that contain annotated template arguments.","Finally, we present baselines on MultiMUC both with state-of-the-art template filling models and with ChatGPT."],"url":"http://arxiv.org/abs/2401.16209v1","category":"cs.CL"}
{"created":"2024-01-29 15:01:57","title":"Quantum algorithms in particle physics","abstract":"We motivate the use of quantum algorithms in particle physics and provide a brief overview of the most recent applications at high-energy colliders. In particular, we discuss in detail how a quantum approach reduces the complexity of jet clustering algorithms, such as anti-kT , and show how quantum algorithms efficiently identify causal configurations of multiloop Feynman diagrams. We also present a quantum integration algorithm, called QFIAE, which is successfully applied to the evaluation of one-loop Feynman integrals in a quantum simulator or in a real quantum device.","sentences":["We motivate the use of quantum algorithms in particle physics and provide a brief overview of the most recent applications at high-energy colliders.","In particular, we discuss in detail how a quantum approach reduces the complexity of jet clustering algorithms, such as anti-kT , and show how quantum algorithms efficiently identify causal configurations of multiloop Feynman diagrams.","We also present a quantum integration algorithm, called QFIAE, which is successfully applied to the evaluation of one-loop Feynman integrals in a quantum simulator or in a real quantum device."],"url":"http://arxiv.org/abs/2401.16208v1","category":"hep-ph"}
{"created":"2024-01-29 15:00:27","title":"CognitiveOS: Large Multimodal Model based System to Endow Any Type of Robot with Generative AI","abstract":"This paper introduces CognitiveOS, a disruptive system based on multiple transformer-based models, endowing robots of various types with cognitive abilities not only for communication with humans but also for task resolution through physical interaction with the environment. The system operates smoothly on different robotic platforms without extra tuning. It autonomously makes decisions for task execution by analyzing the environment and using information from its long-term memory. The system underwent testing on various platforms, including quadruped robots and manipulator robots, showcasing its capability to formulate behavioral plans even for robots whose behavioral examples were absent in the training dataset.   Experimental results revealed the system's high performance in advanced task comprehension and adaptability, emphasizing its potential for real-world applications. The chapters of this paper describe the key components of the system and the dataset structure. The dataset for fine-tuning step generation model is provided at the following link: link coming soon","sentences":["This paper introduces CognitiveOS, a disruptive system based on multiple transformer-based models, endowing robots of various types with cognitive abilities not only for communication with humans but also for task resolution through physical interaction with the environment.","The system operates smoothly on different robotic platforms without extra tuning.","It autonomously makes decisions for task execution by analyzing the environment and using information from its long-term memory.","The system underwent testing on various platforms, including quadruped robots and manipulator robots, showcasing its capability to formulate behavioral plans even for robots whose behavioral examples were absent in the training dataset.   ","Experimental results revealed the system's high performance in advanced task comprehension and adaptability, emphasizing its potential for real-world applications.","The chapters of this paper describe the key components of the system and the dataset structure.","The dataset for fine-tuning step generation model is provided at the following link: link coming soon"],"url":"http://arxiv.org/abs/2401.16205v1","category":"cs.RO"}
{"created":"2024-01-29 14:58:42","title":"Computing High-Degree Polynomial Gradients in Memory","abstract":"Specialized function gradient computing hardware could greatly improve the performance of state-of-the-art optimization algorithms, e.g., based on gradient descent or conjugate gradient methods that are at the core of control, machine learning, and operations research applications. Prior work on such hardware, performed in the context of the Ising Machines and related concepts, is limited to quadratic polynomials and not scalable to commonly used higher-order functions. Here, we propose a novel approach for massively parallel gradient calculations of high-degree polynomials, which is conducive to efficient mixed-signal in-memory computing circuit implementations and whose area complexity scales linearly with the number of variables and terms in the function and, most importantly, independent of its degree. Two flavors of such an approach are proposed. The first is limited to binary-variable polynomials typical in combinatorial optimization problems, while the second type is broader at the cost of a more complex periphery. To validate the former approach, we experimentally demonstrated solving a small-scale third-order Boolean satisfiability problem based on integrated metal-oxide memristor crossbar circuits, one of the most prospective in-memory computing device technologies, with a competitive heuristics algorithm. Simulation results for larger-scale, more practical problems show orders of magnitude improvements in the area, and related advantages in speed and energy efficiency compared to the state-of-the-art. We discuss how our work could enable even higher-performance systems after co-designing algorithms to exploit massively parallel gradient computation.","sentences":["Specialized function gradient computing hardware could greatly improve the performance of state-of-the-art optimization algorithms, e.g., based on gradient descent or conjugate gradient methods that are at the core of control, machine learning, and operations research applications.","Prior work on such hardware, performed in the context of the Ising Machines and related concepts, is limited to quadratic polynomials and not scalable to commonly used higher-order functions.","Here, we propose a novel approach for massively parallel gradient calculations of high-degree polynomials, which is conducive to efficient mixed-signal in-memory computing circuit implementations and whose area complexity scales linearly with the number of variables and terms in the function and, most importantly, independent of its degree.","Two flavors of such an approach are proposed.","The first is limited to binary-variable polynomials typical in combinatorial optimization problems, while the second type is broader at the cost of a more complex periphery.","To validate the former approach, we experimentally demonstrated solving a small-scale third-order Boolean satisfiability problem based on integrated metal-oxide memristor crossbar circuits, one of the most prospective in-memory computing device technologies, with a competitive heuristics algorithm.","Simulation results for larger-scale, more practical problems show orders of magnitude improvements in the area, and related advantages in speed and energy efficiency compared to the state-of-the-art.","We discuss how our work could enable even higher-performance systems after co-designing algorithms to exploit massively parallel gradient computation."],"url":"http://arxiv.org/abs/2401.16204v1","category":"cs.ET"}
{"created":"2024-01-29 14:58:32","title":"FPIA: Field-Programmable Ising Arrays with In-Memory Computing","abstract":"Ising Machine is a promising computing approach for solving combinatorial optimization problems. It is naturally suited for energy-saving and compact in-memory computing implementations with emerging memories. A na\\\"ive in-memory computing implementation of a quadratic Ising Machine requires an array of coupling weights that grows quadratically with problem size. However, the resources in such an approach are used inefficiently due to sparsity in practical optimization problems. We first show that this issue can be addressed by partitioning a coupling array into smaller sub-arrays. This technique, however, requires interconnecting subarrays; hence, we developed in-memory computing architecture for quadratic Ising Machines inspired by island-type field programmable gate arrays, which is the main contribution of our paper. We adapt open-source tools to optimize problem embedding and model routing overhead. Modeling results of benchmark problems for the developed architecture show up to 60x area improvement and faster operation than the baseline approach. Finally, we discuss algorithm/circuit co-design techniques for further improvements.","sentences":["Ising Machine is a promising computing approach for solving combinatorial optimization problems.","It is naturally suited for energy-saving and compact in-memory computing implementations with emerging memories.","A na\\\"ive in-memory computing implementation of a quadratic Ising Machine requires an array of coupling weights that grows quadratically with problem size.","However, the resources in such an approach are used inefficiently due to sparsity in practical optimization problems.","We first show that this issue can be addressed by partitioning a coupling array into smaller sub-arrays.","This technique, however, requires interconnecting subarrays; hence, we developed in-memory computing architecture for quadratic Ising Machines inspired by island-type field programmable gate arrays, which is the main contribution of our paper.","We adapt open-source tools to optimize problem embedding and model routing overhead.","Modeling results of benchmark problems for the developed architecture show up to 60x area improvement and faster operation than the baseline approach.","Finally, we discuss algorithm/circuit co-design techniques for further improvements."],"url":"http://arxiv.org/abs/2401.16202v1","category":"cs.AR"}
{"created":"2024-01-29 14:53:22","title":"Contracting with a Learning Agent","abstract":"Many real-life contractual relations differ completely from the clean, static model at the heart of principal-agent theory. Typically, they involve repeated strategic interactions of the principal and agent, taking place under uncertainty and over time. While appealing in theory, players seldom use complex dynamic strategies in practice, often preferring to circumvent complexity and approach uncertainty through learning. We initiate the study of repeated contracts with a learning agent, focusing on agents who achieve no-regret outcomes.   Optimizing against a no-regret agent is a known open problem in general games; we achieve an optimal solution to this problem for a canonical contract setting, in which the agent's choice among multiple actions leads to success/failure. The solution has a surprisingly simple structure: for some $\\alpha > 0$, initially offer the agent a linear contract with scalar $\\alpha$, then switch to offering a linear contract with scalar $0$. This switch causes the agent to ``free-fall'' through their action space and during this time provides the principal with non-zero reward at zero cost. Despite apparent exploitation of the agent, this dynamic contract can leave \\emph{both} players better off compared to the best static contract. Our results generalize beyond success/failure, to arbitrary non-linear contracts which the principal rescales dynamically.   Finally, we quantify the dependence of our results on knowledge of the time horizon, and are the first to address this consideration in the study of strategizing against learning agents.","sentences":["Many real-life contractual relations differ completely from the clean, static model at the heart of principal-agent theory.","Typically, they involve repeated strategic interactions of the principal and agent, taking place under uncertainty and over time.","While appealing in theory, players seldom use complex dynamic strategies in practice, often preferring to circumvent complexity and approach uncertainty through learning.","We initiate the study of repeated contracts with a learning agent, focusing on agents who achieve no-regret outcomes.   ","Optimizing against a no-regret agent is a known open problem in general games; we achieve an optimal solution to this problem for a canonical contract setting, in which the agent's choice among multiple actions leads to success/failure.","The solution has a surprisingly simple structure: for some $\\alpha > 0$, initially offer the agent a linear contract with scalar $\\alpha$, then switch to offering a linear contract with scalar $0$. This switch causes the agent to ``free-fall'' through their action space and during this time provides the principal with non-zero reward at zero cost.","Despite apparent exploitation of the agent, this dynamic contract can leave \\emph{both} players better off compared to the best static contract.","Our results generalize beyond success/failure, to arbitrary non-linear contracts which the principal rescales dynamically.   ","Finally, we quantify the dependence of our results on knowledge of the time horizon, and are the first to address this consideration in the study of strategizing against learning agents."],"url":"http://arxiv.org/abs/2401.16198v1","category":"cs.GT"}
{"created":"2024-01-29 14:53:14","title":"Geospatial Disparities: A Case Study on Real Estate Prices in Paris","abstract":"Driven by an increasing prevalence of trackers, ever more IoT sensors, and the declining cost of computing power, geospatial information has come to play a pivotal role in contemporary predictive models. While enhancing prognostic performance, geospatial data also has the potential to perpetuate many historical socio-economic patterns, raising concerns about a resurgence of biases and exclusionary practices, with their disproportionate impacts on society. Addressing this, our paper emphasizes the crucial need to identify and rectify such biases and calibration errors in predictive models, particularly as algorithms become more intricate and less interpretable. The increasing granularity of geospatial information further introduces ethical concerns, as choosing different geographical scales may exacerbate disparities akin to redlining and exclusionary zoning. To address these issues, we propose a toolkit for identifying and mitigating biases arising from geospatial data. Extending classical fairness definitions, we incorporate an ordinal regression case with spatial attributes, deviating from the binary classification focus. This extension allows us to gauge disparities stemming from data aggregation levels and advocates for a less interfering correction approach. Illustrating our methodology using a Parisian real estate dataset, we showcase practical applications and scrutinize the implications of choosing geographical aggregation levels for fairness and calibration measures.","sentences":["Driven by an increasing prevalence of trackers, ever more IoT sensors, and the declining cost of computing power, geospatial information has come to play a pivotal role in contemporary predictive models.","While enhancing prognostic performance, geospatial data also has the potential to perpetuate many historical socio-economic patterns, raising concerns about a resurgence of biases and exclusionary practices, with their disproportionate impacts on society.","Addressing this, our paper emphasizes the crucial need to identify and rectify such biases and calibration errors in predictive models, particularly as algorithms become more intricate and less interpretable.","The increasing granularity of geospatial information further introduces ethical concerns, as choosing different geographical scales may exacerbate disparities akin to redlining and exclusionary zoning.","To address these issues, we propose a toolkit for identifying and mitigating biases arising from geospatial data.","Extending classical fairness definitions, we incorporate an ordinal regression case with spatial attributes, deviating from the binary classification focus.","This extension allows us to gauge disparities stemming from data aggregation levels and advocates for a less interfering correction approach.","Illustrating our methodology using a Parisian real estate dataset, we showcase practical applications and scrutinize the implications of choosing geographical aggregation levels for fairness and calibration measures."],"url":"http://arxiv.org/abs/2401.16197v1","category":"cs.LG"}
{"created":"2024-01-29 14:51:12","title":"Dot-depth three, return of the J-class","abstract":"We look at concatenation hierarchies of classes of regular languages. Each such hierarchy is determined by a single class, its basis: level $n$ is built by applying the Boolean polynomial closure operator (BPol), $n$ times to the basis. A prominent and difficult open question in automata theory is to decide membership of a regular language in a given level. For instance, for the historical dot-depth hierarchy, the decidability of membership is only known at levels one and two.   We give a generic algebraic characterization of the operator BPol. This characterization implies that for any concatenation hierarchy, if $n$ is at least two, membership at level $n$ reduces to a more complex problem, called covering, for the previous level, $n-1$. Combined with earlier results on covering, this implies that membership is decidable for dot-depth three and for level two in most of the prominent hierarchies in the literature. For instance, we obtain that the levels two in both the modulo hierarchy and the group hierarchy have decidable membership.","sentences":["We look at concatenation hierarchies of classes of regular languages.","Each such hierarchy is determined by a single class, its basis: level $n$ is built by applying the Boolean polynomial closure operator (BPol), $n$ times to the basis.","A prominent and difficult open question in automata theory is to decide membership of a regular language in a given level.","For instance, for the historical dot-depth hierarchy, the decidability of membership is only known at levels one and two.   ","We give a generic algebraic characterization of the operator BPol.","This characterization implies that for any concatenation hierarchy, if $n$ is at least two, membership at level $n$ reduces to a more complex problem, called covering, for the previous level, $n-1$. Combined with earlier results on covering, this implies that membership is decidable for dot-depth three and for level two in most of the prominent hierarchies in the literature.","For instance, we obtain that the levels two in both the modulo hierarchy and the group hierarchy have decidable membership."],"url":"http://arxiv.org/abs/2401.16195v1","category":"cs.FL"}
{"created":"2024-01-29 14:47:26","title":"Contributing Dimension Structure of Deep Feature for Coreset Selection","abstract":"Coreset selection seeks to choose a subset of crucial training samples for efficient learning. It has gained traction in deep learning, particularly with the surge in training dataset sizes. Sample selection hinges on two main aspects: a sample's representation in enhancing performance and the role of sample diversity in averting overfitting. Existing methods typically measure both the representation and diversity of data based on similarity metrics, such as L2-norm. They have capably tackled representation via distribution matching guided by the similarities of features, gradients, or other information between data. However, the results of effectively diverse sample selection are mired in sub-optimality. This is because the similarity metrics usually simply aggregate dimension similarities without acknowledging disparities among the dimensions that significantly contribute to the final similarity. As a result, they fall short of adequately capturing diversity. To address this, we propose a feature-based diversity constraint, compelling the chosen subset to exhibit maximum diversity. Our key lies in the introduction of a novel Contributing Dimension Structure (CDS) metric. Different from similarity metrics that measure the overall similarity of high-dimensional features, our CDS metric considers not only the reduction of redundancy in feature dimensions, but also the difference between dimensions that contribute significantly to the final similarity. We reveal that existing methods tend to favor samples with similar CDS, leading to a reduced variety of CDS types within the coreset and subsequently hindering model performance. In response, we enhance the performance of five classical selection methods by integrating the CDS constraint. Our experiments on three datasets demonstrate the general effectiveness of the proposed method in boosting existing methods.","sentences":["Coreset selection seeks to choose a subset of crucial training samples for efficient learning.","It has gained traction in deep learning, particularly with the surge in training dataset sizes.","Sample selection hinges on two main aspects: a sample's representation in enhancing performance and the role of sample diversity in averting overfitting.","Existing methods typically measure both the representation and diversity of data based on similarity metrics, such as L2-norm.","They have capably tackled representation via distribution matching guided by the similarities of features, gradients, or other information between data.","However, the results of effectively diverse sample selection are mired in sub-optimality.","This is because the similarity metrics usually simply aggregate dimension similarities without acknowledging disparities among the dimensions that significantly contribute to the final similarity.","As a result, they fall short of adequately capturing diversity.","To address this, we propose a feature-based diversity constraint, compelling the chosen subset to exhibit maximum diversity.","Our key lies in the introduction of a novel Contributing Dimension Structure (CDS) metric.","Different from similarity metrics that measure the overall similarity of high-dimensional features, our CDS metric considers not only the reduction of redundancy in feature dimensions, but also the difference between dimensions that contribute significantly to the final similarity.","We reveal that existing methods tend to favor samples with similar CDS, leading to a reduced variety of CDS types within the coreset and subsequently hindering model performance.","In response, we enhance the performance of five classical selection methods by integrating the CDS constraint.","Our experiments on three datasets demonstrate the general effectiveness of the proposed method in boosting existing methods."],"url":"http://arxiv.org/abs/2401.16193v1","category":"cs.LG"}
{"created":"2024-01-29 14:47:07","title":"B-twisted Gaiotto-Witten theory and topological quantum field theory","abstract":"We develop representation theoretic techniques to construct three dimensional non-semisimple topological quantum field theories which model homologically truncated topological B-twists of abelian Gaiotto-Witten theory with linear matter. Our constructions are based on relative modular structures on the category of weight modules over an unrolled quantization of a Lie superalgebra. The Lie superalgebra, originally defined by Gaiotto and Witten, is associated to a complex symplectic representation of a metric abelian Lie algebra. The physical theories we model admit alternative realizations as Chern-Simons-Rozansky-Witten theories and supergroup Chern-Simons theories and include as particular examples global forms of $\\mathfrak{gl}(1 \\vert 1)$-Chern-Simons theory and toral Chern-Simons theory. Fundamental to our approach is the systematic incorporation of non-genuine line operators which source flat connections for the topological flavour symmetry of the theory.","sentences":["We develop representation theoretic techniques to construct three dimensional non-semisimple topological quantum field theories which model homologically truncated topological B-twists of abelian Gaiotto-Witten theory with linear matter.","Our constructions are based on relative modular structures on the category of weight modules over an unrolled quantization of a Lie superalgebra.","The Lie superalgebra, originally defined by Gaiotto and Witten, is associated to a complex symplectic representation of a metric abelian Lie algebra.","The physical theories we model admit alternative realizations as Chern-Simons-Rozansky-Witten theories and supergroup Chern-Simons theories and include as particular examples global forms of $\\mathfrak{gl}(1 \\vert 1)$-Chern-Simons theory and toral Chern-Simons theory.","Fundamental to our approach is the systematic incorporation of non-genuine line operators which source flat connections for the topological flavour symmetry of the theory."],"url":"http://arxiv.org/abs/2401.16192v1","category":"math.RT"}
{"created":"2024-01-29 14:45:08","title":"From Tripods to Bipods: Reducing the Queue Number of Planar Graphs Costs Just One Leg","abstract":"As an alternative to previously existing planar graph product structure theorems, we prove that every planar graph $G$ is a subgraph of the strong product of $K_2$, a path and a planar subgraph of a $4$-tree. As an application, we show that the queue number of planar graphs is at most $38$ whereas the queue number of planar bipartite graphs is at most $25$.","sentences":["As an alternative to previously existing planar graph product structure theorems, we prove that every planar graph $G$ is a subgraph of the strong product of $K_2$, a path and a planar subgraph of a $4$-tree.","As an application, we show that the queue number of planar graphs is at most $38$ whereas the queue number of planar bipartite graphs is at most $25$."],"url":"http://arxiv.org/abs/2401.16191v1","category":"cs.DM"}
{"created":"2024-01-29 14:42:06","title":"AI prediction of cardiovascular events using opportunistic epicardial adipose tissue assessments from CT calcium score","abstract":"Background: Recent studies have used basic epicardial adipose tissue (EAT) assessments (e.g., volume and mean HU) to predict risk of atherosclerosis-related, major adverse cardiovascular events (MACE). Objectives: Create novel, hand-crafted EAT features, 'fat-omics', to capture the pathophysiology of EAT and improve MACE prediction. Methods: We segmented EAT using a previously-validated deep learning method with optional manual correction. We extracted 148 radiomic features (morphological, spatial, and intensity) and used Cox elastic-net for feature reduction and prediction of MACE. Results: Traditional fat features gave marginal prediction (EAT-volume/EAT-mean-HU/ BMI gave C-index 0.53/0.55/0.57, respectively). Significant improvement was obtained with 15 fat-omics features (C-index=0.69, test set). High-risk features included volume-of-voxels-having-elevated-HU-[-50, -30-HU] and HU-negative-skewness, both of which assess high HU, which as been implicated in fat inflammation. Other high-risk features include kurtosis-of-EAT-thickness, reflecting the heterogeneity of thicknesses, and EAT-volume-in-the-top-25%-of-the-heart, emphasizing adipose near the proximal coronary arteries. Kaplan-Meyer plots of Cox-identified, high- and low-risk patients were well separated with the median of the fat-omics risk, while high-risk group having HR 2.4 times that of the low-risk group (P<0.001). Conclusion: Preliminary findings indicate an opportunity to use more finely tuned, explainable assessments on EAT for improved cardiovascular risk prediction.","sentences":["Background:","Recent studies have used basic epicardial adipose tissue (EAT) assessments (e.g., volume and mean HU) to predict risk of atherosclerosis-related, major adverse cardiovascular events (MACE).","Objectives: Create novel, hand-crafted EAT features, 'fat-omics', to capture the pathophysiology of EAT and improve MACE prediction.","Methods: We segmented EAT using a previously-validated deep learning method with optional manual correction.","We extracted 148 radiomic features (morphological, spatial, and intensity) and used Cox elastic-net for feature reduction and prediction of MACE.","Results: Traditional fat features gave marginal prediction (EAT-volume/EAT-mean-HU/ BMI gave C-index 0.53/0.55/0.57, respectively).","Significant improvement was obtained with 15 fat-omics features (C-index=0.69, test set).","High-risk features included volume-of-voxels-having-elevated-HU-[-50, -30-HU] and HU-negative-skewness, both of which assess high HU, which as been implicated in fat inflammation.","Other high-risk features include kurtosis-of-EAT-thickness, reflecting the heterogeneity of thicknesses, and EAT-volume-in-the-top-25%-of-the-heart, emphasizing adipose near the proximal coronary arteries.","Kaplan-Meyer plots of Cox-identified, high- and low-risk patients were well separated with the median of the fat-omics risk, while high-risk group having HR 2.4 times that of the low-risk group (P<0.001).","Conclusion: Preliminary findings indicate an opportunity to use more finely tuned, explainable assessments on EAT for improved cardiovascular risk prediction."],"url":"http://arxiv.org/abs/2401.16190v1","category":"q-bio.QM"}
{"created":"2024-01-29 14:41:55","title":"FIMP: Future Interaction Modeling for Multi-Agent Motion Prediction","abstract":"Multi-agent motion prediction is a crucial concern in autonomous driving, yet it remains a challenge owing to the ambiguous intentions of dynamic agents and their intricate interactions. Existing studies have attempted to capture interactions between road entities by using the definite data in history timesteps, as future information is not available and involves high uncertainty. However, without sufficient guidance for capturing future states of interacting agents, they frequently produce unrealistic trajectory overlaps. In this work, we propose Future Interaction modeling for Motion Prediction (FIMP), which captures potential future interactions in an end-to-end manner. FIMP adopts a future decoder that implicitly extracts the potential future information in an intermediate feature-level, and identifies the interacting entity pairs through future affinity learning and top-k filtering strategy. Experiments show that our future interaction modeling improves the performance remarkably, leading to superior performance on the Argoverse motion forecasting benchmark.","sentences":["Multi-agent motion prediction is a crucial concern in autonomous driving, yet it remains a challenge owing to the ambiguous intentions of dynamic agents and their intricate interactions.","Existing studies have attempted to capture interactions between road entities by using the definite data in history timesteps, as future information is not available and involves high uncertainty.","However, without sufficient guidance for capturing future states of interacting agents, they frequently produce unrealistic trajectory overlaps.","In this work, we propose Future Interaction modeling for Motion Prediction (FIMP), which captures potential future interactions in an end-to-end manner.","FIMP adopts a future decoder that implicitly extracts the potential future information in an intermediate feature-level, and identifies the interacting entity pairs through future affinity learning and top-k filtering strategy.","Experiments show that our future interaction modeling improves the performance remarkably, leading to superior performance on the Argoverse motion forecasting benchmark."],"url":"http://arxiv.org/abs/2401.16189v1","category":"cs.CV"}
{"created":"2024-01-29 14:36:40","title":"Graph Neural Network-based Joint Equalization and Decoding","abstract":"This paper proposes to use graph neural networks (GNNs) for equalization, that can also be used to perform joint equalization and decoding (JED). For equalization, the GNN is build upon the factor graph representations of the channel, while for JED, the factor graph is expanded by the Tanner graph of the parity-check matrix (PCM) of the channel code, sharing the variable nodes (VNs). A particularly advantageous property of the GNN is the robustness against cycles in the factor graphs which is the main problem for belief propagation (BP)-based equalization. As a result of having a fully deep learning-based receiver, joint optimization instead of individual optimization of the components is enabled, so-called end-to-end learning. Furthermore, we propose a parallel flooding schedule that further reduces the latency, which turns out to improve also the error correcting performance. The proposed approach is analyzed and compared to state-of-the-art baselines in terms of error correcting capability and latency. At a fixed low latency, the flooding GNN for JED demonstrates a gain of 2.25 dB in bit error rate (BER) compared to an iterative Bahl--Cock--Jelinek--Raviv (BCJR)-BP baseline.","sentences":["This paper proposes to use graph neural networks (GNNs) for equalization, that can also be used to perform joint equalization and decoding (JED).","For equalization, the GNN is build upon the factor graph representations of the channel, while for JED, the factor graph is expanded by the Tanner graph of the parity-check matrix (PCM) of the channel code, sharing the variable nodes (VNs).","A particularly advantageous property of the GNN is the robustness against cycles in the factor graphs which is the main problem for belief propagation (BP)-based equalization.","As a result of having a fully deep learning-based receiver, joint optimization instead of individual optimization of the components is enabled, so-called end-to-end learning.","Furthermore, we propose a parallel flooding schedule that further reduces the latency, which turns out to improve also the error correcting performance.","The proposed approach is analyzed and compared to state-of-the-art baselines in terms of error correcting capability and latency.","At a fixed low latency, the flooding GNN for JED demonstrates a gain of 2.25 dB in bit error rate (BER) compared to an iterative Bahl--Cock--Jelinek--Raviv (BCJR)-BP baseline."],"url":"http://arxiv.org/abs/2401.16187v1","category":"cs.IT"}
{"created":"2024-01-29 14:32:32","title":"An Empirical Study on Usage and Perceptions of LLMs in a Software Engineering Project","abstract":"Large Language Models (LLMs) represent a leap in artificial intelligence, excelling in tasks using human language(s). Although the main focus of general-purpose LLMs is not code generation, they have shown promising results in the domain. However, the usefulness of LLMs in an academic software engineering project has not been fully explored yet. In this study, we explore the usefulness of LLMs for 214 students working in teams consisting of up to six members. Notably, in the academic course through which this study is conducted, students were encouraged to integrate LLMs into their development tool-chain, in contrast to most other academic courses that explicitly prohibit the use of LLMs.   In this paper, we analyze the AI-generated code, prompts used for code generation, and the human intervention levels to integrate the code into the code base. We also conduct a perception study to gain insights into the perceived usefulness, influencing factors, and future outlook of LLM from a computer science student's perspective. Our findings suggest that LLMs can play a crucial role in the early stages of software development, especially in generating foundational code structures, and helping with syntax and error debugging. These insights provide us with a framework on how to effectively utilize LLMs as a tool to enhance the productivity of software engineering students, and highlight the necessity of shifting the educational focus toward preparing students for successful human-AI collaboration.","sentences":["Large Language Models (LLMs) represent a leap in artificial intelligence, excelling in tasks using human language(s).","Although the main focus of general-purpose LLMs is not code generation, they have shown promising results in the domain.","However, the usefulness of LLMs in an academic software engineering project has not been fully explored yet.","In this study, we explore the usefulness of LLMs for 214 students working in teams consisting of up to six members.","Notably, in the academic course through which this study is conducted, students were encouraged to integrate LLMs into their development tool-chain, in contrast to most other academic courses that explicitly prohibit the use of LLMs.   ","In this paper, we analyze the AI-generated code, prompts used for code generation, and the human intervention levels to integrate the code into the code base.","We also conduct a perception study to gain insights into the perceived usefulness, influencing factors, and future outlook of LLM from a computer science student's perspective.","Our findings suggest that LLMs can play a crucial role in the early stages of software development, especially in generating foundational code structures, and helping with syntax and error debugging.","These insights provide us with a framework on how to effectively utilize LLMs as a tool to enhance the productivity of software engineering students, and highlight the necessity of shifting the educational focus toward preparing students for successful human-AI collaboration."],"url":"http://arxiv.org/abs/2401.16186v1","category":"cs.SE"}
{"created":"2024-01-29 14:32:27","title":"LLM4Vuln: A Unified Evaluation Framework for Decoupling and Enhancing LLMs' Vulnerability Reasoning","abstract":"Large language models (LLMs) have demonstrated significant poten- tial for many downstream tasks, including those requiring human- level intelligence, such as vulnerability detection. However, recent attempts to use LLMs for vulnerability detection are still prelim- inary, as they lack an in-depth understanding of a subject LLM's vulnerability reasoning capability - whether it originates from the model itself or from external assistance, such as invoking tool sup- port and retrieving vulnerability knowledge. In this paper, we aim to decouple LLMs' vulnerability reason- ing capability from their other capabilities, including the ability to actively seek additional information (e.g., via function calling in SOTA models), adopt relevant vulnerability knowledge (e.g., via vector-based matching and retrieval), and follow instructions to out- put structured results. To this end, we propose a unified evaluation framework named LLM4Vuln, which separates LLMs' vulnerability reasoning from their other capabilities and evaluates how LLMs' vulnerability reasoning could be enhanced when combined with the enhancement of other capabilities. To demonstrate the effectiveness of LLM4Vuln, we have designed controlled experiments using 75 ground-truth smart contract vulnerabilities, which were extensively audited as high-risk on Code4rena from August to November 2023, and tested them in 4,950 different scenarios across three represen- tative LLMs (GPT-4, Mixtral, and Code Llama). Our results not only reveal ten findings regarding the varying effects of knowledge en- hancement, context supplementation, prompt schemes, and models but also enable us to identify 9 zero-day vulnerabilities in two pilot bug bounty programs with over 1,000 USD being awarded.","sentences":["Large language models (LLMs) have demonstrated significant poten- tial for many downstream tasks, including those requiring human- level intelligence, such as vulnerability detection.","However, recent attempts to use LLMs for vulnerability detection are still prelim- inary, as they lack an in-depth understanding of a subject LLM's vulnerability reasoning capability - whether it originates from the model itself or from external assistance, such as invoking tool sup- port and retrieving vulnerability knowledge.","In this paper, we aim to decouple LLMs' vulnerability reason-","ing capability from their other capabilities, including the ability to actively seek additional information (e.g., via function calling in SOTA models), adopt relevant vulnerability knowledge (e.g., via vector-based matching and retrieval), and follow instructions to out- put structured results.","To this end, we propose a unified evaluation framework named LLM4Vuln, which separates LLMs' vulnerability reasoning from their other capabilities and evaluates how LLMs' vulnerability reasoning could be enhanced when combined with the enhancement of other capabilities.","To demonstrate the effectiveness of LLM4Vuln, we have designed controlled experiments using 75 ground-truth smart contract vulnerabilities, which were extensively audited as high-risk on Code4rena from August to November 2023, and tested them in 4,950 different scenarios across three represen- tative LLMs (GPT-4, Mixtral, and Code Llama).","Our results not only reveal ten findings regarding the varying effects of knowledge en- hancement, context supplementation, prompt schemes, and models but also enable us to identify 9 zero-day vulnerabilities in two pilot bug bounty programs with over 1,000 USD being awarded."],"url":"http://arxiv.org/abs/2401.16185v1","category":"cs.CR"}
{"created":"2024-01-29 14:29:48","title":"On the Semantics of LM Latent Space: A Vocabulary-defined Approach","abstract":"In the realm of deep learning, understanding the latent space of language models (LMs) like transformers is crucial for refining their performance and interpretability. However, existing analyses often fall short in providing absolute and model-centric insights into LM semantics, and neglect essential aspects of LM adaption. In response, we introduce a pioneering method called vocabulary-defined semantics, which establishes a fixed reference frame within the LM latent space, ensuring absolute semantic analysis grounded in LM vocabulary. Our approach transcends prior relative analyses, leveraging LM vocabulary for model-centric insights. Furthermore, we propose a novel technique to compute logits, emphasizing differentiability and local isotropy, and introduce a neural clustering module for semantically calibrating data representations during LM adaptation. Through extensive experiments across diverse text understanding datasets, our approach surpasses state-of-the-art methods of retrieval-augmented generation and parameters-efficient finetuning, showcasing its efficacy and broad applicability. Our findings not only shed light on LM mechanics but also offer practical solutions for enhancing LM performance and interpretability.","sentences":["In the realm of deep learning, understanding the latent space of language models (LMs) like transformers is crucial for refining their performance and interpretability.","However, existing analyses often fall short in providing absolute and model-centric insights into LM semantics, and neglect essential aspects of LM adaption.","In response, we introduce a pioneering method called vocabulary-defined semantics, which establishes a fixed reference frame within the LM latent space, ensuring absolute semantic analysis grounded in LM vocabulary.","Our approach transcends prior relative analyses, leveraging LM vocabulary for model-centric insights.","Furthermore, we propose a novel technique to compute logits, emphasizing differentiability and local isotropy, and introduce a neural clustering module for semantically calibrating data representations during LM adaptation.","Through extensive experiments across diverse text understanding datasets, our approach surpasses state-of-the-art methods of retrieval-augmented generation and parameters-efficient finetuning, showcasing its efficacy and broad applicability.","Our findings not only shed light on LM mechanics but also offer practical solutions for enhancing LM performance and interpretability."],"url":"http://arxiv.org/abs/2401.16184v1","category":"cs.CL"}
{"created":"2024-01-29 14:23:51","title":"LLaMandement: Large Language Models for Summarization of French Legislative Proposals","abstract":"This report introduces LLaMandement, a state-of-the-art Large Language Model, fine-tuned by the French government and designed to enhance the efficiency and efficacy of processing parliamentary sessions (including the production of bench memoranda and documents required for interministerial meetings) by generating neutral summaries of legislative proposals. Addressing the administrative challenges of manually processing a growing volume of legislative amendments, LLaMandement stands as a significant legal technological milestone, providing a solution that exceeds the scalability of traditional human efforts while matching the robustness of a specialized legal drafter. We release all our fine-tuned models and training data to the community.","sentences":["This report introduces LLaMandement, a state-of-the-art Large Language Model, fine-tuned by the French government and designed to enhance the efficiency and efficacy of processing parliamentary sessions (including the production of bench memoranda and documents required for interministerial meetings) by generating neutral summaries of legislative proposals.","Addressing the administrative challenges of manually processing a growing volume of legislative amendments, LLaMandement stands as a significant legal technological milestone, providing a solution that exceeds the scalability of traditional human efforts while matching the robustness of a specialized legal drafter.","We release all our fine-tuned models and training data to the community."],"url":"http://arxiv.org/abs/2401.16182v1","category":"cs.CL"}
{"created":"2024-01-29 14:23:38","title":"On Decentralized Linearly Separable Computation With the Minimum Computation Cost","abstract":"The distributed linearly separable computation problem finds extensive applications across domains such as distributed gradient coding, distributed linear transform, real-time rendering, etc. In this paper, we investigate this problem in a fully decentralized scenario, where $\\mathsf{N}$ workers collaboratively perform the computation task without a central master. Each worker aims to compute a linearly separable computation that can be manifested as $\\mathsf{K}_{\\mathrm{c}}$ linear combinations of $\\mathsf{K}$ messages, where each message is a function of a distinct dataset. We require that each worker successfully fulfill the task based on the transmissions from any $\\mathsf{N}_{\\mathrm{r}}$ workers, such that the system can tolerate any $\\mathsf{N}-\\mathsf{N}_{\\mathrm{r}}$ stragglers. We focus on the scenario where the computation cost (the number of uncoded datasets assigned to each worker) is minimum, and aim to minimize the communication cost (the number of symbols the fastest $\\mathsf{N}_{\\mathrm{r}}$ workers transmit). We propose a novel distributed computing scheme that is optimal under the widely used cyclic data assignment. Interestingly, we demonstrate that the side information at each worker is ineffective in reducing the communication cost when $\\mathsf{K}_{\\mathrm{c}}\\leq {\\mathsf{K}}\\mathsf{N}_{\\mathrm{r}}/{\\mathsf{N}}$, while it helps reduce the communication cost as $\\mathsf{K}_{\\mathrm{c}}$ increases.","sentences":["The distributed linearly separable computation problem finds extensive applications across domains such as distributed gradient coding, distributed linear transform, real-time rendering, etc.","In this paper, we investigate this problem in a fully decentralized scenario, where $\\mathsf{N}$ workers collaboratively perform the computation task without a central master.","Each worker aims to compute a linearly separable computation that can be manifested as $\\mathsf{K}_{\\mathrm{c}}$ linear combinations of $\\mathsf{K}$ messages, where each message is a function of a distinct dataset.","We require that each worker successfully fulfill the task based on the transmissions from any $\\mathsf{N}_{\\mathrm{r}}$ workers, such that the system can tolerate any $\\mathsf{N}-\\mathsf{N}_{\\mathrm{r}}$ stragglers.","We focus on the scenario where the computation cost (the number of uncoded datasets assigned to each worker) is minimum, and aim to minimize the communication cost (the number of symbols the fastest $\\mathsf{N}_{\\mathrm{r}}$ workers transmit).","We propose a novel distributed computing scheme that is optimal under the widely used cyclic data assignment.","Interestingly, we demonstrate that the side information at each worker is ineffective in reducing the communication cost when $\\mathsf{K}_{\\mathrm{c}}\\leq {\\mathsf{K}}\\mathsf{N}_{\\mathrm{r}}/{\\mathsf{N}}$, while it helps reduce the communication cost as $\\mathsf{K}_{\\mathrm{c}}$ increases."],"url":"http://arxiv.org/abs/2401.16181v1","category":"cs.IT"}
{"created":"2024-01-29 14:21:18","title":"Numerical Methods for Scalar Field Dark Energy in Table-top Experiments and Lunar Laser Ranging","abstract":"Numerous tabletop experiments have been dedicated to exploring the manifestations of screened scalar field dark energy, such as symmetron or chameleon fields. Precise theoretical predictions require simulating field configurations within the respective experiments. This paper focuses onto the less-explored environment-dependent dilaton field, which emerges in the strong coupling limit of string theory. Due to its exponential self-coupling, this field can exhibit significantly steeper slopes compared to symmetron and chameleon fields, and the equations of motion can be challenging to solve with standard machine precision. We present the first exact solution for the geometry of a vacuum region between two infinitely extended parallel plates. This solution serves as a benchmark for testing the accuracy of numerical solvers. By reparametrizing the model and transforming the equations of motion, we show how to make the model computable across the entire experimentally accessible parameter space. To simulate the dilaton field in one- and two-mirror geometries, as well as spherical configurations, we introduce a non-uniform finite difference method. Additionally, we provide an algorithm for solving the stationary Schr\\\"odinger equation for a fermion in one dimension in the presence of a dilaton field. The algorithms developed here are not limited to the dilaton field, but can be applied to similar scalar-tensor theories as well. We demonstrate such applications at hand of the chameleon and symmetron field. Our computational tools have practical applications in a variety of experimental contexts, including gravity resonance spectroscopy (qBounce), Lunar Laser Ranging (LLR), and the upcoming Casimir and Non-Newtonian Force Experiment (CANNEX). A Mathematica implementation of all algorithms is provided.","sentences":["Numerous tabletop experiments have been dedicated to exploring the manifestations of screened scalar field dark energy, such as symmetron or chameleon fields.","Precise theoretical predictions require simulating field configurations within the respective experiments.","This paper focuses onto the less-explored environment-dependent dilaton field, which emerges in the strong coupling limit of string theory.","Due to its exponential self-coupling, this field can exhibit significantly steeper slopes compared to symmetron and chameleon fields, and the equations of motion can be challenging to solve with standard machine precision.","We present the first exact solution for the geometry of a vacuum region between two infinitely extended parallel plates.","This solution serves as a benchmark for testing the accuracy of numerical solvers.","By reparametrizing the model and transforming the equations of motion, we show how to make the model computable across the entire experimentally accessible parameter space.","To simulate the dilaton field in one- and two-mirror geometries, as well as spherical configurations, we introduce a non-uniform finite difference method.","Additionally, we provide an algorithm for solving the stationary Schr\\\"odinger equation for a fermion in one dimension in the presence of a dilaton field.","The algorithms developed here are not limited to the dilaton field, but can be applied to similar scalar-tensor theories as well.","We demonstrate such applications at hand of the chameleon and symmetron field.","Our computational tools have practical applications in a variety of experimental contexts, including gravity resonance spectroscopy (qBounce), Lunar Laser Ranging (LLR), and the upcoming Casimir and Non-Newtonian Force Experiment (CANNEX).","A Mathematica implementation of all algorithms is provided."],"url":"http://arxiv.org/abs/2401.16179v1","category":"hep-ph"}
{"created":"2024-01-29 14:20:28","title":"Iterative assembly of $^{171}$Yb atom arrays in cavity-enhanced optical lattices","abstract":"Assembling and maintaining large arrays of individually addressable atoms is a key requirement for continued scaling of neutral-atom-based quantum computers and simulators. In this work, we demonstrate a new paradigm for assembly of atomic arrays, based on a synergistic combination of optical tweezers and cavity-enhanced optical lattices, and the incremental filling of a target array from a repetitively filled reservoir. In this protocol, the tweezers provide microscopic rearrangement of atoms, while the cavity-enhanced lattices enable the creation of large numbers of deep optical potentials that allow for rapid low-loss imaging of atoms. We apply this protocol to demonstrate deterministic filling (99% per-site occupancy) of 1225-site arrays. Because the reservoir is repeatedly filled with fresh atoms, the array can be maintained in a filled state indefinitely. We anticipate that this protocol will be compatible with mid-circuit reloading, which will be a key capability for running large-scale error-corrected quantum computations whose durations exceed the lifetime of a single atom in the system.","sentences":["Assembling and maintaining large arrays of individually addressable atoms is a key requirement for continued scaling of neutral-atom-based quantum computers and simulators.","In this work, we demonstrate a new paradigm for assembly of atomic arrays, based on a synergistic combination of optical tweezers and cavity-enhanced optical lattices, and the incremental filling of a target array from a repetitively filled reservoir.","In this protocol, the tweezers provide microscopic rearrangement of atoms, while the cavity-enhanced lattices enable the creation of large numbers of deep optical potentials that allow for rapid low-loss imaging of atoms.","We apply this protocol to demonstrate deterministic filling (99% per-site occupancy) of 1225-site arrays.","Because the reservoir is repeatedly filled with fresh atoms, the array can be maintained in a filled state indefinitely.","We anticipate that this protocol will be compatible with mid-circuit reloading, which will be a key capability for running large-scale error-corrected quantum computations whose durations exceed the lifetime of a single atom in the system."],"url":"http://arxiv.org/abs/2401.16177v1","category":"quant-ph"}
{"created":"2024-01-29 14:18:09","title":"A Survey on Structure-Preserving Graph Transformers","abstract":"The transformer architecture has shown remarkable success in various domains, such as natural language processing and computer vision. When it comes to graph learning, transformers are required not only to capture the interactions between pairs of nodes but also to preserve graph structures connoting the underlying relations and proximity between them, showing the expressive power to capture different graph structures. Accordingly, various structure-preserving graph transformers have been proposed and widely used for various tasks, such as graph-level tasks in bioinformatics and chemoinformatics. However, strategies related to graph structure preservation have not been well organized and systematized in the literature. In this paper, we provide a comprehensive overview of structure-preserving graph transformers and generalize these methods from the perspective of their design objective. First, we divide strategies into four main groups: node feature modulation, context node sampling, graph rewriting, and transformer architecture improvements. We then further divide the strategies according to the coverage and goals of graph structure preservation. Furthermore, we also discuss challenges and future directions for graph transformer models to preserve the graph structure and understand the nature of graphs.","sentences":["The transformer architecture has shown remarkable success in various domains, such as natural language processing and computer vision.","When it comes to graph learning, transformers are required not only to capture the interactions between pairs of nodes but also to preserve graph structures connoting the underlying relations and proximity between them, showing the expressive power to capture different graph structures.","Accordingly, various structure-preserving graph transformers have been proposed and widely used for various tasks, such as graph-level tasks in bioinformatics and chemoinformatics.","However, strategies related to graph structure preservation have not been well organized and systematized in the literature.","In this paper, we provide a comprehensive overview of structure-preserving graph transformers and generalize these methods from the perspective of their design objective.","First, we divide strategies into four main groups: node feature modulation, context node sampling, graph rewriting, and transformer architecture improvements.","We then further divide the strategies according to the coverage and goals of graph structure preservation.","Furthermore, we also discuss challenges and future directions for graph transformer models to preserve the graph structure and understand the nature of graphs."],"url":"http://arxiv.org/abs/2401.16176v1","category":"cs.LG"}
{"created":"2024-01-29 14:08:02","title":"Reconstructing Close Human Interactions from Multiple Views","abstract":"This paper addresses the challenging task of reconstructing the poses of multiple individuals engaged in close interactions, captured by multiple calibrated cameras. The difficulty arises from the noisy or false 2D keypoint detections due to inter-person occlusion, the heavy ambiguity in associating keypoints to individuals due to the close interactions, and the scarcity of training data as collecting and annotating motion data in crowded scenes is resource-intensive. We introduce a novel system to address these challenges. Our system integrates a learning-based pose estimation component and its corresponding training and inference strategies. The pose estimation component takes multi-view 2D keypoint heatmaps as input and reconstructs the pose of each individual using a 3D conditional volumetric network. As the network doesn't need images as input, we can leverage known camera parameters from test scenes and a large quantity of existing motion capture data to synthesize massive training data that mimics the real data distribution in test scenes. Extensive experiments demonstrate that our approach significantly surpasses previous approaches in terms of pose accuracy and is generalizable across various camera setups and population sizes. The code is available on our project page: https://github.com/zju3dv/CloseMoCap.","sentences":["This paper addresses the challenging task of reconstructing the poses of multiple individuals engaged in close interactions, captured by multiple calibrated cameras.","The difficulty arises from the noisy or false 2D keypoint detections due to inter-person occlusion, the heavy ambiguity in associating keypoints to individuals due to the close interactions, and the scarcity of training data as collecting and annotating motion data in crowded scenes is resource-intensive.","We introduce a novel system to address these challenges.","Our system integrates a learning-based pose estimation component and its corresponding training and inference strategies.","The pose estimation component takes multi-view 2D keypoint heatmaps as input and reconstructs the pose of each individual using a 3D conditional volumetric network.","As the network doesn't need images as input, we can leverage known camera parameters from test scenes and a large quantity of existing motion capture data to synthesize massive training data that mimics the real data distribution in test scenes.","Extensive experiments demonstrate that our approach significantly surpasses previous approaches in terms of pose accuracy and is generalizable across various camera setups and population sizes.","The code is available on our project page: https://github.com/zju3dv/CloseMoCap."],"url":"http://arxiv.org/abs/2401.16173v1","category":"cs.CV"}
{"created":"2024-01-29 14:00:37","title":"A Privacy-preserving key transmission protocol to distribute QRNG keys using zk-SNARKs","abstract":"High-entropy random numbers are an essential part of cryptography, and Quantum Random Number Generators (QRNG) are an emergent technology that can provide high-quality keys for cryptographic algorithms but unfortunately are currently difficult to access. Existing Entropy-as-a-Service solutions require users to trust the central authority distributing the key material, which is not desirable in a high-privacy environment. In this paper, we present a novel key transmission protocol that allows users to obtain cryptographic material generated by a QRNG in such a way that the server is unable to identify which user is receiving each key. This is achieved with the inclusion of Zero Knowledge Succinct Non-interactive Arguments of Knowledge (zk-SNARK), a cryptographic primitive that allow users to prove knowledge of some value without needing to reveal it. The security analysis of the protocol proves that it satisfies the properties of Anonymity, Unforgeability and Confidentiality, as defined in this document. We also provide an implementation of the protocol demonstrating its functionality and performance, using NFC as the transmission channel for the QRNG key.","sentences":["High-entropy random numbers are an essential part of cryptography, and Quantum Random Number Generators (QRNG) are an emergent technology that can provide high-quality keys for cryptographic algorithms but unfortunately are currently difficult to access.","Existing Entropy-as-a-Service solutions require users to trust the central authority distributing the key material, which is not desirable in a high-privacy environment.","In this paper, we present a novel key transmission protocol that allows users to obtain cryptographic material generated by a QRNG in such a way that the server is unable to identify which user is receiving each key.","This is achieved with the inclusion of Zero Knowledge Succinct Non-interactive Arguments of Knowledge (zk-SNARK), a cryptographic primitive that allow users to prove knowledge of some value without needing to reveal it.","The security analysis of the protocol proves that it satisfies the properties of Anonymity, Unforgeability and Confidentiality, as defined in this document.","We also provide an implementation of the protocol demonstrating its functionality and performance, using NFC as the transmission channel for the QRNG key."],"url":"http://arxiv.org/abs/2401.16170v1","category":"cs.CR"}
{"created":"2024-01-29 13:57:53","title":"Extended Spin-Coherence Time in Strongly-Coupled Spin Baths in Quasi Two-Dimensional Layers","abstract":"We investigate the spin-coherence decay of NV$^-$-spins interacting with the strongly-coupled bath of nitrogen defects in diamond layers. For thin diamond layers, we demonstrate that the spin-coherence times exceed those of bulk diamond, thus allowing to surpass the limit imposed by high defect concentrations in bulk. We show that the stretched-exponential parameter for the short-time spin-coherence decay is governed by the hyperfine interaction in the bath, thereby constraining random-noise models. We introduce a novel method based on the cluster-correlation expansion applied to strongly-interacting bath partitions. Our results facilitate material development for quantum-technology devices.","sentences":["We investigate the spin-coherence decay of NV$^-$-spins interacting with the strongly-coupled bath of nitrogen defects in diamond layers.","For thin diamond layers, we demonstrate that the spin-coherence times exceed those of bulk diamond, thus allowing to surpass the limit imposed by high defect concentrations in bulk.","We show that the stretched-exponential parameter for the short-time spin-coherence decay is governed by the hyperfine interaction in the bath, thereby constraining random-noise models.","We introduce a novel method based on the cluster-correlation expansion applied to strongly-interacting bath partitions.","Our results facilitate material development for quantum-technology devices."],"url":"http://arxiv.org/abs/2401.16169v1","category":"quant-ph"}
{"created":"2024-01-29 13:54:48","title":"\"You tell me\": A Dataset of GPT-4-Based Behaviour Change Support Conversations","abstract":"Conversational agents are increasingly used to address emotional needs on top of information needs. One use case of increasing interest are counselling-style mental health and behaviour change interventions, with large language model (LLM)-based approaches becoming more popular. Research in this context so far has been largely system-focused, foregoing the aspect of user behaviour and the impact this can have on LLM-generated texts. To address this issue, we share a dataset containing text-based user interactions related to behaviour change with two GPT-4-based conversational agents collected in a preregistered user study. This dataset includes conversation data, user language analysis, perception measures, and user feedback for LLM-generated turns, and can offer valuable insights to inform the design of such systems based on real interactions.","sentences":["Conversational agents are increasingly used to address emotional needs on top of information needs.","One use case of increasing interest are counselling-style mental health and behaviour change interventions, with large language model (LLM)-based approaches becoming more popular.","Research in this context so far has been largely system-focused, foregoing the aspect of user behaviour and the impact this can have on LLM-generated texts.","To address this issue, we share a dataset containing text-based user interactions related to behaviour change with two GPT-4-based conversational agents collected in a preregistered user study.","This dataset includes conversation data, user language analysis, perception measures, and user feedback for LLM-generated turns, and can offer valuable insights to inform the design of such systems based on real interactions."],"url":"http://arxiv.org/abs/2401.16167v1","category":"cs.HC"}
{"created":"2024-01-29 13:50:56","title":"Constrained Bi-Level Optimization: Proximal Lagrangian Value function Approach and Hessian-free Algorithm","abstract":"This paper presents a new approach and algorithm for solving a class of constrained Bi-Level Optimization (BLO) problems in which the lower-level problem involves constraints coupling both upper-level and lower-level variables. Such problems have recently gained significant attention due to their broad applicability in machine learning. However, conventional gradient-based methods unavoidably rely on computationally intensive calculations related to the Hessian matrix. To address this challenge, we begin by devising a smooth proximal Lagrangian value function to handle the constrained lower-level problem. Utilizing this construct, we introduce a single-level reformulation for constrained BLOs that transforms the original BLO problem into an equivalent optimization problem with smooth constraints. Enabled by this reformulation, we develop a Hessian-free gradient-based algorithm-termed proximal Lagrangian Value function-based Hessian-free Bi-level Algorithm (LV-HBA)-that is straightforward to implement in a single loop manner. Consequently, LV-HBA is especially well-suited for machine learning applications. Furthermore, we offer non-asymptotic convergence analysis for LV-HBA, eliminating the need for traditional strong convexity assumptions for the lower-level problem while also being capable of accommodating non-singleton scenarios. Empirical results substantiate the algorithm's superior practical performance.","sentences":["This paper presents a new approach and algorithm for solving a class of constrained Bi-Level Optimization (BLO) problems in which the lower-level problem involves constraints coupling both upper-level and lower-level variables.","Such problems have recently gained significant attention due to their broad applicability in machine learning.","However, conventional gradient-based methods unavoidably rely on computationally intensive calculations related to the Hessian matrix.","To address this challenge, we begin by devising a smooth proximal Lagrangian value function to handle the constrained lower-level problem.","Utilizing this construct, we introduce a single-level reformulation for constrained BLOs that transforms the original BLO problem into an equivalent optimization problem with smooth constraints.","Enabled by this reformulation, we develop a Hessian-free gradient-based algorithm-termed proximal Lagrangian Value function-based Hessian-free Bi-level Algorithm (LV-HBA)-that is straightforward to implement in a single loop manner.","Consequently, LV-HBA is especially well-suited for machine learning applications.","Furthermore, we offer non-asymptotic convergence analysis for LV-HBA, eliminating the need for traditional strong convexity assumptions for the lower-level problem while also being capable of accommodating non-singleton scenarios.","Empirical results substantiate the algorithm's superior practical performance."],"url":"http://arxiv.org/abs/2401.16164v1","category":"cs.LG"}
{"created":"2024-01-29 13:48:44","title":"Hartman Effect from a Geometrodynamic Extension of Bohmian Mechanics","abstract":"This paper presents the derivation of a general solution to the scattering problem of particles incident onto a barrier of constant potential. This solution is constructed through a geometrodynamic approach to Bohmian mechanics, assuming that particles undergo quantum tunneling along geodesic trajectories in an Alcubierre-type spacetime. Furthermore, from this solution, mathematical expressions for the quantum potential, momentum, position, and tunneling time are determined in terms of the spacetime geometry for each relevant region. This allows us to explain the Hartman effect as a consequence of spacetime distortion generated by the quantum potential within the barrier.","sentences":["This paper presents the derivation of a general solution to the scattering problem of particles incident onto a barrier of constant potential.","This solution is constructed through a geometrodynamic approach to Bohmian mechanics, assuming that particles undergo quantum tunneling along geodesic trajectories in an Alcubierre-type spacetime.","Furthermore, from this solution, mathematical expressions for the quantum potential, momentum, position, and tunneling time are determined in terms of the spacetime geometry for each relevant region.","This allows us to explain the Hartman effect as a consequence of spacetime distortion generated by the quantum potential within the barrier."],"url":"http://arxiv.org/abs/2401.16162v1","category":"quant-ph"}
{"created":"2024-01-29 13:48:36","title":"LLaVA-MoLE: Sparse Mixture of LoRA Experts for Mitigating Data Conflicts in Instruction Finetuning MLLMs","abstract":"Instruction finetuning on a variety of image-text instruction data is the key to obtaining a versatile Multimodal Large Language Model (MLLM), and different configurations of the instruction data can lead to finetuned models with different capabilities. However, we have discovered that data conflicts are inevitable when mixing instruction data from distinct domains, which can result in performance drops for tasks of a specific domain. To address this issue, we propose to apply a sparse mixture of LoRA experts for instruction finetuning MLLMs. Within the Transformer layers, we extend the popular Low-Rank Adaption (LoRA) method by creating a set of LoRA experts specifically for the MLP layer, and route each token to the top-1 expert based on a routing function, allowing adaptive choices for tokens from different domains. Since the LoRA experts are sparsely activated, the training and inference cost are kept roughly constant compared to the original LoRA method. By replacing the plain-LoRA finetuing of LLaVA-1.5, our final model is named LLaVA-MoLE. Extensive experiments proved that LLaVA-MoLE effectively mitigates the data conflict issue when mixing multiple distinct instruction datasets with various configurations, and achieves consistent performance gains over the strong plain-LoRA baselines. Most importantly, on the mixed datasets, LLaVA-MoLE can even outperform the plain-LoRA baseline trained with twice the samples.","sentences":["Instruction finetuning on a variety of image-text instruction data is the key to obtaining a versatile Multimodal Large Language Model (MLLM), and different configurations of the instruction data can lead to finetuned models with different capabilities.","However, we have discovered that data conflicts are inevitable when mixing instruction data from distinct domains, which can result in performance drops for tasks of a specific domain.","To address this issue, we propose to apply a sparse mixture of LoRA experts for instruction finetuning MLLMs.","Within the Transformer layers, we extend the popular Low-Rank Adaption (LoRA) method by creating a set of LoRA experts specifically for the MLP layer, and route each token to the top-1 expert based on a routing function, allowing adaptive choices for tokens from different domains.","Since the LoRA experts are sparsely activated, the training and inference cost are kept roughly constant compared to the original LoRA method.","By replacing the plain-LoRA finetuing of LLaVA-1.5, our final model is named LLaVA-MoLE.","Extensive experiments proved that LLaVA-MoLE effectively mitigates the data conflict issue when mixing multiple distinct instruction datasets with various configurations, and achieves consistent performance gains over the strong plain-LoRA baselines.","Most importantly, on the mixed datasets, LLaVA-MoLE can even outperform the plain-LoRA baseline trained with twice the samples."],"url":"http://arxiv.org/abs/2401.16160v1","category":"cs.CV"}
{"created":"2024-01-29 13:46:37","title":"Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception","abstract":"Mobile device agent based on Multimodal Large Language Models (MLLM) is becoming a popular application. In this paper, we introduce Mobile-Agent, an autonomous multi-modal mobile device agent. Mobile-Agent first leverages visual perception tools to accurately identify and locate both the visual and textual elements within the app's front-end interface. Based on the perceived vision context, it then autonomously plans and decomposes the complex operation task, and navigates the mobile Apps through operations step by step. Different from previous solutions that rely on XML files of Apps or mobile system metadata, Mobile-Agent allows for greater adaptability across diverse mobile operating environments in a vision-centric way, thereby eliminating the necessity for system-specific customizations. To assess the performance of Mobile-Agent, we introduced Mobile-Eval, a benchmark for evaluating mobile device operations. Based on Mobile-Eval, we conducted a comprehensive evaluation of Mobile-Agent. The experimental results indicate that Mobile-Agent achieved remarkable accuracy and completion rates. Even with challenging instructions, such as multi-app operations, Mobile-Agent can still complete the requirements. Code and model will be open-sourced at https://github.com/X-PLUG/MobileAgent.","sentences":["Mobile device agent based on Multimodal Large Language Models (MLLM) is becoming a popular application.","In this paper, we introduce Mobile-Agent, an autonomous multi-modal mobile device agent.","Mobile-Agent first leverages visual perception tools to accurately identify and locate both the visual and textual elements within the app's front-end interface.","Based on the perceived vision context, it then autonomously plans and decomposes the complex operation task, and navigates the mobile Apps through operations step by step.","Different from previous solutions that rely on XML files of Apps or mobile system metadata, Mobile-Agent allows for greater adaptability across diverse mobile operating environments in a vision-centric way, thereby eliminating the necessity for system-specific customizations.","To assess the performance of Mobile-Agent, we introduced Mobile-Eval, a benchmark for evaluating mobile device operations.","Based on Mobile-Eval, we conducted a comprehensive evaluation of Mobile-Agent.","The experimental results indicate that Mobile-Agent achieved remarkable accuracy and completion rates.","Even with challenging instructions, such as multi-app operations, Mobile-Agent can still complete the requirements.","Code and model will be open-sourced at https://github.com/X-PLUG/MobileAgent."],"url":"http://arxiv.org/abs/2401.16158v1","category":"cs.CL"}
{"created":"2024-01-29 13:42:01","title":"Spatial-Aware Latent Initialization for Controllable Image Generation","abstract":"Recently, text-to-image diffusion models have demonstrated impressive ability to generate high-quality images conditioned on the textual input. However, these models struggle to accurately adhere to textual instructions regarding spatial layout information. While previous research has primarily focused on aligning cross-attention maps with layout conditions, they overlook the impact of the initialization noise on the layout guidance. To achieve better layout control, we propose leveraging a spatial-aware initialization noise during the denoising process. Specifically, we find that the inverted reference image with finite inversion steps contains valuable spatial awareness regarding the object's position, resulting in similar layouts in the generated images. Based on this observation, we develop an open-vocabulary framework to customize a spatial-aware initialization noise for each layout condition. Without modifying other modules except the initialization noise, our approach can be seamlessly integrated as a plug-and-play module within other training-free layout guidance frameworks. We evaluate our approach quantitatively and qualitatively on the available Stable Diffusion model and COCO dataset. Equipped with the spatial-aware latent initialization, our method significantly improves the effectiveness of layout guidance while preserving high-quality content.","sentences":["Recently, text-to-image diffusion models have demonstrated impressive ability to generate high-quality images conditioned on the textual input.","However, these models struggle to accurately adhere to textual instructions regarding spatial layout information.","While previous research has primarily focused on aligning cross-attention maps with layout conditions, they overlook the impact of the initialization noise on the layout guidance.","To achieve better layout control, we propose leveraging a spatial-aware initialization noise during the denoising process.","Specifically, we find that the inverted reference image with finite inversion steps contains valuable spatial awareness regarding the object's position, resulting in similar layouts in the generated images.","Based on this observation, we develop an open-vocabulary framework to customize a spatial-aware initialization noise for each layout condition.","Without modifying other modules except the initialization noise, our approach can be seamlessly integrated as a plug-and-play module within other training-free layout guidance frameworks.","We evaluate our approach quantitatively and qualitatively on the available Stable Diffusion model and COCO dataset.","Equipped with the spatial-aware latent initialization, our method significantly improves the effectiveness of layout guidance while preserving high-quality content."],"url":"http://arxiv.org/abs/2401.16157v1","category":"cs.CV"}
{"created":"2024-01-29 13:34:44","title":"Agile Effort Estimation: Comparing the Accuracy and Efficiency of Planning Poker, Bucket System, and Affinity Estimation methods","abstract":"Published studies on agile effort estimation predominantly focus on comparisons of the accuracy of different estimation methods, while efficiency comparisons, i.e. how much time the estimation methods consume was not in the forefront. However, for practical use in software development, the time required can be a very important cost factor for enterprises, especially when the accuracy of different agile effort estimations is similar. In this study, we thus try to advance the current standard accuracy comparison between methods by introducing efficiency i.e. time it takes to use a method as an additional dimension of comparison. We conduct this comparison between three agile effort estimation methods that were not yet compared in the literature, namely Planning Poker, Bucket System and Affinity Estimation. For the comparison, we used eight student teams with 29 students that had to use all the effort estimation methods during the course where they had to finish a programming project in 3 weeks. The results indicate that after the students get used to using the different methods the accuracy between them is not statistically significantly different, however, the efficiency is. On average Bucket System and Affinity Estimation methods take half as much time as Planning Poker.","sentences":["Published studies on agile effort estimation predominantly focus on comparisons of the accuracy of different estimation methods, while efficiency comparisons, i.e. how much time the estimation methods consume was not in the forefront.","However, for practical use in software development, the time required can be a very important cost factor for enterprises, especially when the accuracy of different agile effort estimations is similar.","In this study, we thus try to advance the current standard accuracy comparison between methods by introducing efficiency i.e. time it takes to use a method as an additional dimension of comparison.","We conduct this comparison between three agile effort estimation methods that were not yet compared in the literature, namely Planning Poker, Bucket System and Affinity Estimation.","For the comparison, we used eight student teams with 29 students that had to use all the effort estimation methods during the course where they had to finish a programming project in 3 weeks.","The results indicate that after the students get used to using the different methods the accuracy between them is not statistically significantly different, however, the efficiency is.","On average Bucket System and Affinity Estimation methods take half as much time as Planning Poker."],"url":"http://arxiv.org/abs/2401.16152v1","category":"cs.SE"}
{"created":"2024-01-29 13:23:55","title":"A theory-independent bound saturated by quantum mechanics","abstract":"Tsirelson's original inequality for the precession protocol, first introduced for the harmonic oscillator but applicable to all uniformly-precessing systems, serves as a monopartite test of quantumness. Given that the system is undergoing a uniform precession, the signs of the positions of a classical oscillator must satisfy that inequality, which is violated by certain quantum states. We consider this inequality for measurements with finitely many outcomes in a theory-independent manner. We derive a general bound which depends only on the minimum positive and negative values of the spectrum of the observable. Given any such two values, we construct a quantum observable that saturates this bound. A notable example is the angular momentum of a spin-$3/2$ particle. We also relate our findings to the recently-introduced notion of constrained conditional probabilities.","sentences":["Tsirelson's original inequality for the precession protocol, first introduced for the harmonic oscillator but applicable to all uniformly-precessing systems, serves as a monopartite test of quantumness.","Given that the system is undergoing a uniform precession, the signs of the positions of a classical oscillator must satisfy that inequality, which is violated by certain quantum states.","We consider this inequality for measurements with finitely many outcomes in a theory-independent manner.","We derive a general bound which depends only on the minimum positive and negative values of the spectrum of the observable.","Given any such two values, we construct a quantum observable that saturates this bound.","A notable example is the angular momentum of a spin-$3/2$ particle.","We also relate our findings to the recently-introduced notion of constrained conditional probabilities."],"url":"http://arxiv.org/abs/2401.16147v1","category":"quant-ph"}
{"created":"2024-01-29 13:23:35","title":"Group theory and irreducible representations of the Poincare group","abstract":"In this review, we have reached from the most basic definitions in the theory of groups, group structures, etc. to representation theory and irreducible representations of the Poincar'e group. Also, we tried to get a more comprehensible understanding of group theory by presenting examples from the nature around us to examples in mathematics and physics and using them to examine more important groups in physics such as the Lorentz group and Poincar'e group and representations It is achieved in the physical fields that are used in the quantum field theory.","sentences":["In this review, we have reached from the most basic definitions in the theory of groups, group structures, etc. to representation theory and irreducible representations of the Poincar'e group.","Also, we tried to get a more comprehensible understanding of group theory by presenting examples from the nature around us to examples in mathematics and physics and using them to examine more important groups in physics such as the Lorentz group and Poincar'e group and representations It is achieved in the physical fields that are used in the quantum field theory."],"url":"http://arxiv.org/abs/2401.16145v1","category":"math.GR"}
{"created":"2024-01-29 13:23:34","title":"Divide and Conquer: Rethinking the Training Paradigm of Neural Radiance Fields","abstract":"Neural radiance fields (NeRFs) have exhibited potential in synthesizing high-fidelity views of 3D scenes but the standard training paradigm of NeRF presupposes an equal importance for each image in the training set. This assumption poses a significant challenge for rendering specific views presenting intricate geometries, thereby resulting in suboptimal performance. In this paper, we take a closer look at the implications of the current training paradigm and redesign this for more superior rendering quality by NeRFs. Dividing input views into multiple groups based on their visual similarities and training individual models on each of these groups enables each model to specialize on specific regions without sacrificing speed or efficiency. Subsequently, the knowledge of these specialized models is aggregated into a single entity via a teacher-student distillation paradigm, enabling spatial efficiency for online render-ing. Empirically, we evaluate our novel training framework on two publicly available datasets, namely NeRF synthetic and Tanks&Temples. Our evaluation demonstrates that our DaC training pipeline enhances the rendering quality of a state-of-the-art baseline model while exhibiting convergence to a superior minimum.","sentences":["Neural radiance fields (NeRFs) have exhibited potential in synthesizing high-fidelity views of 3D scenes but the standard training paradigm of NeRF presupposes an equal importance for each image in the training set.","This assumption poses a significant challenge for rendering specific views presenting intricate geometries, thereby resulting in suboptimal performance.","In this paper, we take a closer look at the implications of the current training paradigm and redesign this for more superior rendering quality by NeRFs.","Dividing input views into multiple groups based on their visual similarities and training individual models on each of these groups enables each model to specialize on specific regions without sacrificing speed or efficiency.","Subsequently, the knowledge of these specialized models is aggregated into a single entity via a teacher-student distillation paradigm, enabling spatial efficiency for online render-ing.","Empirically, we evaluate our novel training framework on two publicly available datasets, namely NeRF synthetic and Tanks&Temples.","Our evaluation demonstrates that our DaC training pipeline enhances the rendering quality of a state-of-the-art baseline model while exhibiting convergence to a superior minimum."],"url":"http://arxiv.org/abs/2401.16144v1","category":"cs.CV"}
{"created":"2024-01-29 13:21:15","title":"Effect of a critical magnetic field on the control of scalar neutral boson pair production in the context of Lorentz-symmetry violation","abstract":"This study investigates the production of neutral scalar boson pairs in static electromagnetic fields resulting from Lorentz-symmetry violation (LSV), with a focus on the parity-even sector of the CPT-even photon sector in the Standard Model Extension (SME). Utilizing a cross-configuration involving inhomogeneous static electric fields and homogeneous static magnetic fields, the analysis of the probability of bosons pair production identifies three different regimes determined by critical magnetic field. Below the critical value, creation is exponentially suppressed; at the critical value, the number density of created bosons remains constant, and above the critical field, there is exponential amplification. This behavior prompts an additional investigation using von Neumann entanglement entropy to analyze fluctuations in the bosonic vacuum.","sentences":["This study investigates the production of neutral scalar boson pairs in static electromagnetic fields resulting from Lorentz-symmetry violation (LSV), with a focus on the parity-even sector of the CPT-even photon sector in the Standard Model Extension (SME).","Utilizing a cross-configuration involving inhomogeneous static electric fields and homogeneous static magnetic fields, the analysis of the probability of bosons pair production identifies three different regimes determined by critical magnetic field.","Below the critical value, creation is exponentially suppressed; at the critical value, the number density of created bosons remains constant, and above the critical field, there is exponential amplification.","This behavior prompts an additional investigation using von Neumann entanglement entropy to analyze fluctuations in the bosonic vacuum."],"url":"http://arxiv.org/abs/2401.16143v1","category":"hep-th"}
{"created":"2024-01-29 13:15:04","title":"Improving device-aware Web services and their mobile clients through an aspect-oriented, model-driven approach","abstract":"Context: Mobile devices have become an essential element in our daily lives, even for connecting to the Internet. Consequently, Web services have become extremely important when offering services through the Internet. However, current Web services are very inflexible as regards their invocation from different types of device, especially if we consider the need for them to be adaptable when being invoked from mobile devices. Objective: In this paper, we provide an approach for the creation of flexible Web services which can be invoked transparently from different device types and which return subsequent responses, as well as providing the client's adaptation as a result of the particular device characteristics and end-user preferences in a completely decoupled way. Method: Aspect-Oriented Programming and model-driven development have been used to reduce both the impact of service and client code adaptation for multiple devices as well as to facilitate the developer's task. Results: A model-driven methodology can be followed from system models to code, providing the Web service developer with the option of marking which services should be adapted to mobile devices in the UML models, and obtaining the decoupled adaptation code automatically from the models. Conclusion: We can conclude that the approach presented in this paper provides us with the possibility of following the development of mobile-aware Web services in an integrated platform, benefiting from the use of aspect-oriented techniques not only for maintaining device-related code completely decoupled from the main functionality one, but also allowing a modularized non-intrusive adaptation of mobile clients to the specific device characteristics as well as to final user preferences.","sentences":["Context: Mobile devices have become an essential element in our daily lives, even for connecting to the Internet.","Consequently, Web services have become extremely important when offering services through the Internet.","However, current Web services are very inflexible as regards their invocation from different types of device, especially if we consider the need for them to be adaptable when being invoked from mobile devices.","Objective:","In this paper, we provide an approach for the creation of flexible Web services which can be invoked transparently from different device types and which return subsequent responses, as well as providing the client's adaptation as a result of the particular device characteristics and end-user preferences in a completely decoupled way.","Method: Aspect-Oriented Programming and model-driven development have been used to reduce both the impact of service and client code adaptation for multiple devices as well as to facilitate the developer's task.","Results: A model-driven methodology can be followed from system models to code, providing the Web service developer with the option of marking which services should be adapted to mobile devices in the UML models, and obtaining the decoupled adaptation code automatically from the models.","Conclusion: We can conclude that the approach presented in this paper provides us with the possibility of following the development of mobile-aware Web services in an integrated platform, benefiting from the use of aspect-oriented techniques not only for maintaining device-related code completely decoupled from the main functionality one, but also allowing a modularized non-intrusive adaptation of mobile clients to the specific device characteristics as well as to final user preferences."],"url":"http://arxiv.org/abs/2401.16139v1","category":"cs.SE"}
{"created":"2024-01-29 13:13:32","title":"X-PEFT: eXtremely Parameter-Efficient Fine-Tuning for Extreme Multi-Profile Scenarios","abstract":"Parameter-efficient fine-tuning (PEFT) techniques, such as adapter tuning, aim to fine-tune a pre-trained language model (PLM) using a minimal number of parameters for a specific task or profile. Although adapter tuning provides increased parameter efficiency compared to full-model fine-tuning, it introduces a small set of additional parameters attached to a PLM for each profile. This can become problematic in practical applications with multiple profiles, particularly when a significant increase in the number of profiles linearly boosts the total number of additional parameters. To mitigate this issue, we introduce X-PEFT, a novel PEFT method that leverages a multitude of given adapters by fine-tuning an extremely small set of compact tensors for a new profile, which serve as binary masks to adaptively select the given adapters. To efficiently validate our proposed method, we implement it using a large number of trained or untrained (random) adapters. We evaluate the performance of X-PEFT through LaMP and GLUE tasks and demonstrate that it either matches or surpasses the effectiveness of conventional adapter tuning, despite reducing the memory requirements per profile by a factor of 10,000 compared to it.","sentences":["Parameter-efficient fine-tuning (PEFT) techniques, such as adapter tuning, aim to fine-tune a pre-trained language model (PLM) using a minimal number of parameters for a specific task or profile.","Although adapter tuning provides increased parameter efficiency compared to full-model fine-tuning, it introduces a small set of additional parameters attached to a PLM for each profile.","This can become problematic in practical applications with multiple profiles, particularly when a significant increase in the number of profiles linearly boosts the total number of additional parameters.","To mitigate this issue, we introduce X-PEFT, a novel PEFT method that leverages a multitude of given adapters by fine-tuning an extremely small set of compact tensors for a new profile, which serve as binary masks to adaptively select the given adapters.","To efficiently validate our proposed method, we implement it using a large number of trained or untrained (random) adapters.","We evaluate the performance of X-PEFT through LaMP and GLUE tasks and demonstrate that it either matches or surpasses the effectiveness of conventional adapter tuning, despite reducing the memory requirements per profile by a factor of 10,000 compared to it."],"url":"http://arxiv.org/abs/2401.16137v1","category":"cs.LG"}
{"created":"2024-01-29 13:07:08","title":"Neural Network Training on Encrypted Data with TFHE","abstract":"We present an approach to outsourcing of training neural networks while preserving data confidentiality from malicious parties. We use fully homomorphic encryption to build a unified training approach that works on encrypted data and learns quantized neural network models. The data can be horizontally or vertically split between multiple parties, enabling collaboration on confidential data. We train logistic regression and multi-layer perceptrons on several datasets.","sentences":["We present an approach to outsourcing of training neural networks while preserving data confidentiality from malicious parties.","We use fully homomorphic encryption to build a unified training approach that works on encrypted data and learns quantized neural network models.","The data can be horizontally or vertically split between multiple parties, enabling collaboration on confidential data.","We train logistic regression and multi-layer perceptrons on several datasets."],"url":"http://arxiv.org/abs/2401.16136v1","category":"cs.CR"}
{"created":"2024-01-29 13:00:31","title":"Minimum Detection Efficiencies for Loophole-free Genuine Nonlocality Tests","abstract":"The certification of quantum nonlocality, which has immense significance in architecting device-independent technologies, confronts severe experimental challenges. Detection loophole, originating from the unavailability of perfect detectors, is one of the major issues amongst them. In the present study we focus on the minimum detection efficiency (MDE) required to detect various forms of genuine nonlocality, originating from the type of causal constraints imposed on the involved parties. In this context, we demonstrate that the MDE needed to manifest the recently suggested $T_2$-type nonlocality deviates significantly from perfection. Additionally, we have computed the MDE necessary to manifest Svetlichny's nonlocality, with state-independent approach markedly reducing the previously established bound. Finally, considering the inevitable existence of noise we demonstrate the robustness of the imperfect detectors to certify $T_2$-type nonlocality.","sentences":["The certification of quantum nonlocality, which has immense significance in architecting device-independent technologies, confronts severe experimental challenges.","Detection loophole, originating from the unavailability of perfect detectors, is one of the major issues amongst them.","In the present study we focus on the minimum detection efficiency (MDE) required to detect various forms of genuine nonlocality, originating from the type of causal constraints imposed on the involved parties.","In this context, we demonstrate that the MDE needed to manifest the recently suggested $T_2$-type nonlocality deviates significantly from perfection.","Additionally, we have computed the MDE necessary to manifest Svetlichny's nonlocality, with state-independent approach markedly reducing the previously established bound.","Finally, considering the inevitable existence of noise we demonstrate the robustness of the imperfect detectors to certify $T_2$-type nonlocality."],"url":"http://arxiv.org/abs/2401.16134v1","category":"quant-ph"}
{"created":"2024-01-29 12:58:44","title":"BooleanOCT: Optimal Classification Trees based on multivariate Boolean Rules","abstract":"The global optimization of classification trees has demonstrated considerable promise, notably in enhancing accuracy, optimizing size, and thereby improving human comprehensibility. While existing optimal classification trees substantially enhance accuracy over greedy-based tree models like CART, they still fall short when compared to the more complex black-box models, such as random forests. To bridge this gap, we introduce a new mixed-integer programming (MIP) formulation, grounded in multivariate Boolean rules, to derive the optimal classification tree. Our methodology integrates both linear metrics, including accuracy, balanced accuracy, and cost-sensitive cost, as well as nonlinear metrics such as the F1-score. The approach is implemented in an open-source Python package named BooleanOCT. We comprehensively benchmark these methods on the 36 datasets from the UCI machine learning repository. The proposed models demonstrate practical solvability on real-world datasets, effectively handling sizes in the tens of thousands. Aiming to maximize accuracy, this model achieves an average absolute improvement of 3.1\\% and 1.5\\% over random forests in small-scale and medium-sized datasets, respectively. Experiments targeting various objectives, including balanced accuracy, cost-sensitive cost, and F1-score, demonstrate the framework's wide applicability and its superiority over contemporary state-of-the-art optimal classification tree methods in small to medium-scale datasets.","sentences":["The global optimization of classification trees has demonstrated considerable promise, notably in enhancing accuracy, optimizing size, and thereby improving human comprehensibility.","While existing optimal classification trees substantially enhance accuracy over greedy-based tree models like CART, they still fall short when compared to the more complex black-box models, such as random forests.","To bridge this gap, we introduce a new mixed-integer programming (MIP) formulation, grounded in multivariate Boolean rules, to derive the optimal classification tree.","Our methodology integrates both linear metrics, including accuracy, balanced accuracy, and cost-sensitive cost, as well as nonlinear metrics such as the F1-score.","The approach is implemented in an open-source Python package named BooleanOCT.","We comprehensively benchmark these methods on the 36 datasets from the UCI machine learning repository.","The proposed models demonstrate practical solvability on real-world datasets, effectively handling sizes in the tens of thousands.","Aiming to maximize accuracy, this model achieves an average absolute improvement of 3.1\\% and 1.5\\% over random forests in small-scale and medium-sized datasets, respectively.","Experiments targeting various objectives, including balanced accuracy, cost-sensitive cost, and F1-score, demonstrate the framework's wide applicability and its superiority over contemporary state-of-the-art optimal classification tree methods in small to medium-scale datasets."],"url":"http://arxiv.org/abs/2401.16133v1","category":"cs.LG"}
{"created":"2024-01-29 12:56:11","title":"CIMIL-CRC: a clinically-informed multiple instance learning framework for patient-level colorectal cancer molecular subtypes classification from H\\&E stained images","abstract":"Treatment approaches for colorectal cancer (CRC) are highly dependent on the molecular subtype, as immunotherapy has shown efficacy in cases with microsatellite instability (MSI) but is ineffective for the microsatellite stable (MSS) subtype. There is promising potential in utilizing deep neural networks (DNNs) to automate the differentiation of CRC subtypes by analyzing Hematoxylin and Eosin (H\\&E) stained whole-slide images (WSIs). Due to the extensive size of WSIs, Multiple Instance Learning (MIL) techniques are typically explored. However, existing MIL methods focus on identifying the most representative image patches for classification, which may result in the loss of critical information. Additionally, these methods often overlook clinically relevant information, like the tendency for MSI class tumors to predominantly occur on the proximal (right side) colon. We introduce `CIMIL-CRC', a DNN framework that: 1) solves the MSI/MSS MIL problem by efficiently combining a pre-trained feature extraction model with principal component analysis (PCA) to aggregate information from all patches, and 2) integrates clinical priors, particularly the tumor location within the colon, into the model to enhance patient-level classification accuracy. We assessed our CIMIL-CRC method using the average area under the curve (AUC) from a 5-fold cross-validation experimental setup for model development on the TCGA-CRC-DX cohort, contrasting it with a baseline patch-level classification, MIL-only approach, and Clinically-informed patch-level classification approach. Our CIMIL-CRC outperformed all methods (AUROC: $0.92\\pm0.002$ (95\\% CI 0.91-0.92), vs. $0.79\\pm0.02$ (95\\% CI 0.76-0.82), $0.86\\pm0.01$ (95\\% CI 0.85-0.88), and $0.87\\pm0.01$ (95\\% CI 0.86-0.88), respectively). The improvement was statistically significant.","sentences":["Treatment approaches for colorectal cancer (CRC) are highly dependent on the molecular subtype, as immunotherapy has shown efficacy in cases with microsatellite instability (MSI) but is ineffective for the microsatellite stable (MSS) subtype.","There is promising potential in utilizing deep neural networks (DNNs) to automate the differentiation of CRC subtypes by analyzing Hematoxylin and Eosin (H\\&E) stained whole-slide images (WSIs).","Due to the extensive size of WSIs, Multiple Instance Learning (MIL) techniques are typically explored.","However, existing MIL methods focus on identifying the most representative image patches for classification, which may result in the loss of critical information.","Additionally, these methods often overlook clinically relevant information, like the tendency for MSI class tumors to predominantly occur on the proximal (right side) colon.","We introduce `CIMIL-CRC', a DNN framework that: 1) solves the MSI/MSS MIL problem by efficiently combining a pre-trained feature extraction model with principal component analysis (PCA) to aggregate information from all patches, and 2) integrates clinical priors, particularly the tumor location within the colon, into the model to enhance patient-level classification accuracy.","We assessed our CIMIL-CRC method using the average area under the curve (AUC) from a 5-fold cross-validation experimental setup for model development on the TCGA-CRC-DX cohort, contrasting it with a baseline patch-level classification, MIL-only approach, and Clinically-informed patch-level classification approach.","Our CIMIL-CRC outperformed all methods (AUROC: $0.92\\pm0.002$ (95\\% CI 0.91-0.92), vs. $0.79\\pm0.02$ (95\\% CI 0.76-0.82), $0.86\\pm0.01$ (95\\% CI 0.85-0.88), and $0.87\\pm0.01$ (95\\% CI 0.86-0.88), respectively).","The improvement was statistically significant."],"url":"http://arxiv.org/abs/2401.16131v1","category":"cs.CV"}
{"created":"2024-01-29 12:51:31","title":"Incorporating the Cosmological Constant in a Modified Uncertainty Principle","abstract":"The existence of a tiny but non-zero cosmological constant seems to be a fundamental challenge for physics. This study examines the cosmological constant problem and modified uncertainty principle within a unified framework inspired by a void-dominated cosmology. We model voids/halos as spherical bubbles/drops for simplification and analysis. Our heuristic calculations show significant variations in surface energy values from the largest to smallest scales, resulting in a substantial disparity (approximately $122$ orders of magnitude) in the values of the cosmological constant. Our method suggests that the difference in the values of the cosmological constant is inherent and should be considered natural. As a main outcome of this research, we propose a new form of extended uncertainty principle that incorporates cosmological constant.","sentences":["The existence of a tiny but non-zero cosmological constant seems to be a fundamental challenge for physics.","This study examines the cosmological constant problem and modified uncertainty principle within a unified framework inspired by a void-dominated cosmology.","We model voids/halos as spherical bubbles/drops for simplification and analysis.","Our heuristic calculations show significant variations in surface energy values from the largest to smallest scales, resulting in a substantial disparity (approximately $122$ orders of magnitude) in the values of the cosmological constant.","Our method suggests that the difference in the values of the cosmological constant is inherent and should be considered natural.","As a main outcome of this research, we propose a new form of extended uncertainty principle that incorporates cosmological constant."],"url":"http://arxiv.org/abs/2401.16126v1","category":"gr-qc"}
{"created":"2024-01-29 12:49:09","title":"On the generalization of learned constraints for ASP solving in temporal domains","abstract":"The representation of a dynamic problem in ASP usually boils down to using copies of variables and constraints, one for each time stamp, no matter whether it is directly encoded or via an action or temporal language. The multiplication of variables and constraints is commonly done during grounding and the solver is completely ignorant about the temporal relationship among the different instances. On the other hand, a key factor in the performance of today's ASP solvers is conflict-driven constraint learning. Our question is now whether a constraint learned for particular time steps can be generalized and reused at other time stamps, and ultimately whether this enhances the overall solver performance on temporal problems. Knowing full well the domain of time, we study conditions under which learned dynamic constraints can be generalized. We propose a simple translation of the original logic program such that, for the translated programs, the learned constraints can be generalized to other time points. Additionally, we identify a property of temporal problems that allows us to generalize all learned constraints to all time steps. It turns out that this property is satisfied by many planning problems. Finally, we empirically evaluate the impact of adding the generalized constraints to an ASP solver","sentences":["The representation of a dynamic problem in ASP usually boils down to using copies of variables and constraints, one for each time stamp, no matter whether it is directly encoded or via an action or temporal language.","The multiplication of variables and constraints is commonly done during grounding and the solver is completely ignorant about the temporal relationship among the different instances.","On the other hand, a key factor in the performance of today's ASP solvers is conflict-driven constraint learning.","Our question is now whether a constraint learned for particular time steps can be generalized and reused at other time stamps, and ultimately whether this enhances the overall solver performance on temporal problems.","Knowing full well the domain of time, we study conditions under which learned dynamic constraints can be generalized.","We propose a simple translation of the original logic program such that, for the translated programs, the learned constraints can be generalized to other time points.","Additionally, we identify a property of temporal problems that allows us to generalize all learned constraints to all time steps.","It turns out that this property is satisfied by many planning problems.","Finally, we empirically evaluate the impact of adding the generalized constraints to an ASP solver"],"url":"http://arxiv.org/abs/2401.16124v1","category":"cs.AI"}
{"created":"2024-01-29 12:48:56","title":"Looking for a better fit? An Incremental Learning Multimodal Object Referencing Framework adapting to Individual Drivers","abstract":"The rapid advancement of the automotive industry towards automated and semi-automated vehicles has rendered traditional methods of vehicle interaction, such as touch-based and voice command systems, inadequate for a widening range of non-driving related tasks, such as referencing objects outside of the vehicle. Consequently, research has shifted toward gestural input (e.g., hand, gaze, and head pose gestures) as a more suitable mode of interaction during driving. However, due to the dynamic nature of driving and individual variation, there are significant differences in drivers' gestural input performance. While, in theory, this inherent variability could be moderated by substantial data-driven machine learning models, prevalent methodologies lean towards constrained, single-instance trained models for object referencing. These models show a limited capacity to continuously adapt to the divergent behaviors of individual drivers and the variety of driving scenarios. To address this, we propose \\textit{IcRegress}, a novel regression-based incremental learning approach that adapts to changing behavior and the unique characteristics of drivers engaged in the dual task of driving and referencing objects. We suggest a more personalized and adaptable solution for multimodal gestural interfaces, employing continuous lifelong learning to enhance driver experience, safety, and convenience. Our approach was evaluated using an outside-the-vehicle object referencing use case, highlighting the superiority of the incremental learning models adapted over a single trained model across various driver traits such as handedness, driving experience, and numerous driving conditions. Finally, to facilitate reproducibility, ease deployment, and promote further research, we offer our approach as an open-source framework at \\url{https://github.com/amrgomaaelhady/IcRegress}.","sentences":["The rapid advancement of the automotive industry towards automated and semi-automated vehicles has rendered traditional methods of vehicle interaction, such as touch-based and voice command systems, inadequate for a widening range of non-driving related tasks, such as referencing objects outside of the vehicle.","Consequently, research has shifted toward gestural input (e.g., hand, gaze, and head pose gestures) as a more suitable mode of interaction during driving.","However, due to the dynamic nature of driving and individual variation, there are significant differences in drivers' gestural input performance.","While, in theory, this inherent variability could be moderated by substantial data-driven machine learning models, prevalent methodologies lean towards constrained, single-instance trained models for object referencing.","These models show a limited capacity to continuously adapt to the divergent behaviors of individual drivers and the variety of driving scenarios.","To address this, we propose \\textit{IcRegress}, a novel regression-based incremental learning approach that adapts to changing behavior and the unique characteristics of drivers engaged in the dual task of driving and referencing objects.","We suggest a more personalized and adaptable solution for multimodal gestural interfaces, employing continuous lifelong learning to enhance driver experience, safety, and convenience.","Our approach was evaluated using an outside-the-vehicle object referencing use case, highlighting the superiority of the incremental learning models adapted over a single trained model across various driver traits such as handedness, driving experience, and numerous driving conditions.","Finally, to facilitate reproducibility, ease deployment, and promote further research, we offer our approach as an open-source framework at \\url{https://github.com/amrgomaaelhady/IcRegress}."],"url":"http://arxiv.org/abs/2401.16123v1","category":"cs.HC"}
{"created":"2024-01-29 12:47:55","title":"DeFlow: Decoder of Scene Flow Network in Autonomous Driving","abstract":"Scene flow estimation determines a scene's 3D motion field, by predicting the motion of points in the scene, especially for aiding tasks in autonomous driving. Many networks with large-scale point clouds as input use voxelization to create a pseudo-image for real-time running. However, the voxelization process often results in the loss of point-specific features. This gives rise to a challenge in recovering those features for scene flow tasks. Our paper introduces DeFlow which enables a transition from voxel-based features to point features using Gated Recurrent Unit (GRU) refinement. To further enhance scene flow estimation performance, we formulate a novel loss function that accounts for the data imbalance between static and dynamic points. Evaluations on the Argoverse 2 scene flow task reveal that DeFlow achieves state-of-the-art results on large-scale point cloud data, demonstrating that our network has better performance and efficiency compared to others. The code is open-sourced at https://github.com/KTH-RPL/deflow.","sentences":["Scene flow estimation determines a scene's 3D motion field, by predicting the motion of points in the scene, especially for aiding tasks in autonomous driving.","Many networks with large-scale point clouds as input use voxelization to create a pseudo-image for real-time running.","However, the voxelization process often results in the loss of point-specific features.","This gives rise to a challenge in recovering those features for scene flow tasks.","Our paper introduces DeFlow which enables a transition from voxel-based features to point features using Gated Recurrent Unit (GRU) refinement.","To further enhance scene flow estimation performance, we formulate a novel loss function that accounts for the data imbalance between static and dynamic points.","Evaluations on the Argoverse 2 scene flow task reveal that DeFlow achieves state-of-the-art results on large-scale point cloud data, demonstrating that our network has better performance and efficiency compared to others.","The code is open-sourced at https://github.com/KTH-RPL/deflow."],"url":"http://arxiv.org/abs/2401.16122v1","category":"cs.CV"}
{"created":"2024-01-29 12:46:24","title":"Arithmeticity and covering rate of the $9$-cyclotomic Clifford+$\\mathcal{D}$ gates in $PU(3)$","abstract":"The Clifford+T gate set is a topological generating set for PU(2), which has been well-studied from the perspective of quantum computation on a single qubit. The discovery that it generates a full S-arithmetic subgroup of PU(2) has led to a fruitful interaction between quantum computation and number theory, leading in particular to a proof that words in these gates cover PU(2) in an almost-optimal manner.   In this paper we study an analogue gate set for PU(3) called Clifford+$\\mathcal{D}$. We show that this set generates a full S-arithmetic subgroup of PU(3), and satisfies a slightly weaker almost-optimal covering property. Our proofs are different from those for PU(2): while both gate sets act naturally on a (Bruhat-Tits) tree, in PU(2) the generated group acts transitively on the vertices of the tree, and this is a main ingredient in proving both arithmeticity and efficiency. In the PU(3) (Clifford+$\\mathcal{D}$) case the action on the tree is far from being transitive. This makes the proof of arithmeticity considerably harder, and the study of covering rate by automorphic representation theory becomes more involved and results in a slower covering rate.","sentences":["The Clifford+T gate set is a topological generating set for PU(2), which has been well-studied from the perspective of quantum computation on a single qubit.","The discovery that it generates a full S-arithmetic subgroup of PU(2) has led to a fruitful interaction between quantum computation and number theory, leading in particular to a proof that words in these gates cover PU(2) in an almost-optimal manner.   ","In this paper we study an analogue gate set for PU(3) called Clifford+$\\mathcal{D}$. We show that this set generates a full S-arithmetic subgroup of PU(3), and satisfies a slightly weaker almost-optimal covering property.","Our proofs are different from those for PU(2): while both gate sets act naturally on a (Bruhat-Tits) tree, in PU(2) the generated group acts transitively on the vertices of the tree, and this is a main ingredient in proving both arithmeticity and efficiency.","In the PU(3) (Clifford+$\\mathcal{D}$) case the action on the tree is far from being transitive.","This makes the proof of arithmeticity considerably harder, and the study of covering rate by automorphic representation theory becomes more involved and results in a slower covering rate."],"url":"http://arxiv.org/abs/2401.16120v1","category":"quant-ph"}
{"created":"2024-01-29 12:45:27","title":"Triple Disentangled Representation Learning for Multimodal Affective Analysis","abstract":"Multimodal learning has exhibited a significant advantage in affective analysis tasks owing to the comprehensive information of various modalities, particularly the complementary information. Thus, many emerging studies focus on disentangling the modality-invariant and modality-specific representations from input data and then fusing them for prediction. However, our study shows that modality-specific representations may contain information that is irrelevant or conflicting with the tasks, which downgrades the effectiveness of learned multimodal representations. We revisit the disentanglement issue, and propose a novel triple disentanglement approach, TriDiRA, which disentangles the modality-invariant, effective modality-specific and ineffective modality-specific representations from input data. By fusing only the modality-invariant and effective modality-specific representations, TriDiRA can significantly alleviate the impact of irrelevant and conflicting information across modalities during model training. Extensive experiments conducted on four benchmark datasets demonstrate the effectiveness and generalization of our triple disentanglement, which outperforms SOTA methods.","sentences":["Multimodal learning has exhibited a significant advantage in affective analysis tasks owing to the comprehensive information of various modalities, particularly the complementary information.","Thus, many emerging studies focus on disentangling the modality-invariant and modality-specific representations from input data and then fusing them for prediction.","However, our study shows that modality-specific representations may contain information that is irrelevant or conflicting with the tasks, which downgrades the effectiveness of learned multimodal representations.","We revisit the disentanglement issue, and propose a novel triple disentanglement approach, TriDiRA, which disentangles the modality-invariant, effective modality-specific and ineffective modality-specific representations from input data.","By fusing only the modality-invariant and effective modality-specific representations, TriDiRA can significantly alleviate the impact of irrelevant and conflicting information across modalities during model training.","Extensive experiments conducted on four benchmark datasets demonstrate the effectiveness and generalization of our triple disentanglement, which outperforms SOTA methods."],"url":"http://arxiv.org/abs/2401.16119v1","category":"cs.AI"}
{"created":"2024-01-29 12:41:42","title":"Quantum Cheques","abstract":"Publicly-verifiable quantum money has been a central focus in quantum cryptography. To date, no constructions for this primitive exist based on standard assumptions. In this study, we propose an alternative notion which we refer to as $\\textit{quantum cheques}$ (QCs). A quantum cheque can be verified using a public-key but only by a single user. Specifically, the payer signs the quantum cheque for a particular recipient using their ID, and the recipient can validate it without the assistance of the bank, ensuring that the payer cannot assign the same cheque to another user with a different ID. Unlike quantum money, QCs only necessitate quantum communication when a cheque is issued by the bank, meaning all payments and deposits are entirely classical!   We demonstrate how to construct QCs based on the well-studied learning-with-errors (LWE) assumption. In the process, we build two novel primitives which are of independent interest. Firstly, we construct $\\textit{signatures with publicly-verifiable deletion}$ under LWE. This primitive enables the signing of a message $m$ such that the recipient can produce a classical string that publicly proves the inability to reproduce a signature of $m$. We then demonstrate how this primitive can be used to construct $\\textit{2-message signature tokens}$. This primitive enables the production of a token that can be used to sign a single bit and then self-destructs. Finally, we show that 2-message signature tokens can be used to construct QCs.","sentences":["Publicly-verifiable quantum money has been a central focus in quantum cryptography.","To date, no constructions for this primitive exist based on standard assumptions.","In this study, we propose an alternative notion which we refer to as $\\textit{quantum cheques}$ (QCs).","A quantum cheque can be verified using a public-key but only by a single user.","Specifically, the payer signs the quantum cheque for a particular recipient using their ID, and the recipient can validate it without the assistance of the bank, ensuring that the payer cannot assign the same cheque to another user with a different ID.","Unlike quantum money, QCs only necessitate quantum communication when a cheque is issued by the bank, meaning all payments and deposits are entirely classical!   ","We demonstrate how to construct QCs based on the well-studied learning-with-errors (LWE) assumption.","In the process, we build two novel primitives which are of independent interest.","Firstly, we construct $\\textit{signatures with publicly-verifiable deletion}$ under LWE.","This primitive enables the signing of a message $m$ such that the recipient can produce a classical string that publicly proves the inability to reproduce a signature of $m$. We then demonstrate how this primitive can be used to construct","$\\textit{2-message signature tokens}$.","This primitive enables the production of a token that can be used to sign a single bit and then self-destructs.","Finally, we show that 2-message signature tokens can be used to construct QCs."],"url":"http://arxiv.org/abs/2401.16116v1","category":"quant-ph"}
{"created":"2024-01-29 12:39:37","title":"A spectral approach to Hebbian-like neural networks","abstract":"We consider the Hopfield neural network as a model of associative memory and we define its neuronal interaction matrix $\\bb J$ as a function of a set of $K \\times M$ binary vectors $\\{\\bb \\xi^{\\mu, A} \\}_{\\mu=1,...,K}^{A=1,...,M}$ representing a sample of the reality that we want to retrieve. In particular, any item $\\bb \\xi^{\\mu, A}$ is meant as a corrupted version of an unknown ground pattern $\\bb \\zeta^{\\mu}$, that is the target of our retrieval process. We consider and compare two definitions for $\\bb J$, referred to as supervised and unsupervised, according to whether the class $\\mu$, each example belongs to, is unveiled or not, also, these definitions recover the paradigmatic Hebb's rule under suitable limits. The spectral properties of the resulting matrices are studied and used to inspect the retrieval capabilities of the related models as a function of their control parameters.","sentences":["We consider the Hopfield neural network as a model of associative memory and we define its neuronal interaction matrix $\\bb J$ as a function of a set of $K \\times M$ binary vectors $\\{\\bb \\xi^{\\mu, A} \\}_{\\mu=1,...,K}^{A=1,...,M}$ representing a sample of the reality that we want to retrieve.","In particular, any item $\\bb \\xi^{\\mu, A}$ is meant as a corrupted version of an unknown ground pattern $\\bb \\zeta^{\\mu}$, that is the target of our retrieval process.","We consider and compare two definitions for $\\bb J$, referred to as supervised and unsupervised, according to whether the class $\\mu$, each example belongs to, is unveiled or not, also, these definitions recover the paradigmatic Hebb's rule under suitable limits.","The spectral properties of the resulting matrices are studied and used to inspect the retrieval capabilities of the related models as a function of their control parameters."],"url":"http://arxiv.org/abs/2401.16114v1","category":"math-ph"}
