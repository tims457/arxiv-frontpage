{"created":"2024-06-13 17:59:59","title":"VideoGPT+: Integrating Image and Video Encoders for Enhanced Video Understanding","abstract":"Building on the advances of language models, Large Multimodal Models (LMMs) have contributed significant improvements in video understanding. While the current video LMMs utilize advanced Large Language Models (LLMs), they rely on either image or video encoders to process visual inputs, each of which has its own limitations. Image encoders excel at capturing rich spatial details from frame sequences but lack explicit temporal context, which can be important in videos with intricate action sequences. On the other hand, video encoders provide temporal context but are often limited by computational constraints that lead to processing only sparse frames at lower resolutions, resulting in reduced contextual and spatial understanding. To this end, we introduce VideoGPT+, which combines the complementary benefits of the image encoder (for detailed spatial understanding) and the video encoder (for global temporal context modeling). The model processes videos by dividing them into smaller segments and applies an adaptive pooling strategy on features extracted by both image and video encoders. Our architecture showcases improved performance across multiple video benchmarks, including VCGBench, MVBench and Zero-shot question-answering. Further, we develop 112K video-instruction set using a novel semi-automatic annotation pipeline which further improves the model performance. Additionally, to comprehensively evaluate video LMMs, we present VCGBench-Diverse, covering 18 broad video categories such as lifestyle, sports, science, gaming, and surveillance videos. This benchmark with 4,354 question-answer pairs evaluates the generalization of existing LMMs on dense video captioning, spatial and temporal understanding, and complex reasoning, ensuring comprehensive assessment across diverse video types and dynamics. Code: https://github.com/mbzuai-oryx/VideoGPT-plus.","sentences":["Building on the advances of language models, Large Multimodal Models (LMMs) have contributed significant improvements in video understanding.","While the current video LMMs utilize advanced Large Language Models (LLMs), they rely on either image or video encoders to process visual inputs, each of which has its own limitations.","Image encoders excel at capturing rich spatial details from frame sequences but lack explicit temporal context, which can be important in videos with intricate action sequences.","On the other hand, video encoders provide temporal context but are often limited by computational constraints that lead to processing only sparse frames at lower resolutions, resulting in reduced contextual and spatial understanding.","To this end, we introduce VideoGPT+, which combines the complementary benefits of the image encoder (for detailed spatial understanding) and the video encoder (for global temporal context modeling).","The model processes videos by dividing them into smaller segments and applies an adaptive pooling strategy on features extracted by both image and video encoders.","Our architecture showcases improved performance across multiple video benchmarks, including VCGBench, MVBench and Zero-shot question-answering.","Further, we develop 112K video-instruction set using a novel semi-automatic annotation pipeline which further improves the model performance.","Additionally, to comprehensively evaluate video LMMs, we present VCGBench-Diverse, covering 18 broad video categories such as lifestyle, sports, science, gaming, and surveillance videos.","This benchmark with 4,354 question-answer pairs evaluates the generalization of existing LMMs on dense video captioning, spatial and temporal understanding, and complex reasoning, ensuring comprehensive assessment across diverse video types and dynamics.","Code: https://github.com/mbzuai-oryx/VideoGPT-plus."],"url":"http://arxiv.org/abs/2406.09418v1","category":"cs.CV"}
{"created":"2024-06-13 17:59:58","title":"An Image is Worth More Than 16x16 Patches: Exploring Transformers on Individual Pixels","abstract":"This work does not introduce a new method. Instead, we present an interesting finding that questions the necessity of the inductive bias -- locality in modern computer vision architectures. Concretely, we find that vanilla Transformers can operate by directly treating each individual pixel as a token and achieve highly performant results. This is substantially different from the popular design in Vision Transformer, which maintains the inductive bias from ConvNets towards local neighborhoods (e.g. by treating each 16x16 patch as a token). We mainly showcase the effectiveness of pixels-as-tokens across three well-studied tasks in computer vision: supervised learning for object classification, self-supervised learning via masked autoencoding, and image generation with diffusion models. Although directly operating on individual pixels is less computationally practical, we believe the community must be aware of this surprising piece of knowledge when devising the next generation of neural architectures for computer vision.","sentences":["This work does not introduce a new method.","Instead, we present an interesting finding that questions the necessity of the inductive bias -- locality in modern computer vision architectures.","Concretely, we find that vanilla Transformers can operate by directly treating each individual pixel as a token and achieve highly performant results.","This is substantially different from the popular design in Vision Transformer, which maintains the inductive bias from ConvNets towards local neighborhoods (e.g. by treating each 16x16 patch as a token).","We mainly showcase the effectiveness of pixels-as-tokens across three well-studied tasks in computer vision: supervised learning for object classification, self-supervised learning via masked autoencoding, and image generation with diffusion models.","Although directly operating on individual pixels is less computationally practical, we believe the community must be aware of this surprising piece of knowledge when devising the next generation of neural architectures for computer vision."],"url":"http://arxiv.org/abs/2406.09415v1","category":"cs.CV"}
{"created":"2024-06-13 17:59:58","title":"Alleviating Distortion in Image Generation via Multi-Resolution Diffusion Models","abstract":"This paper presents innovative enhancements to diffusion models by integrating a novel multi-resolution network and time-dependent layer normalization. Diffusion models have gained prominence for their effectiveness in high-fidelity image generation. While conventional approaches rely on convolutional U-Net architectures, recent Transformer-based designs have demonstrated superior performance and scalability. However, Transformer architectures, which tokenize input data (via \"patchification\"), face a trade-off between visual fidelity and computational complexity due to the quadratic nature of self-attention operations concerning token length. While larger patch sizes enable attention computation efficiency, they struggle to capture fine-grained visual details, leading to image distortions. To address this challenge, we propose augmenting the Diffusion model with the Multi-Resolution network (DiMR), a framework that refines features across multiple resolutions, progressively enhancing detail from low to high resolution. Additionally, we introduce Time-Dependent Layer Normalization (TD-LN), a parameter-efficient approach that incorporates time-dependent parameters into layer normalization to inject time information and achieve superior performance. Our method's efficacy is demonstrated on the class-conditional ImageNet generation benchmark, where DiMR-XL variants outperform prior diffusion models, setting new state-of-the-art FID scores of 1.70 on ImageNet 256 x 256 and 2.89 on ImageNet 512 x 512. Project page: https://qihao067.github.io/projects/DiMR","sentences":["This paper presents innovative enhancements to diffusion models by integrating a novel multi-resolution network and time-dependent layer normalization.","Diffusion models have gained prominence for their effectiveness in high-fidelity image generation.","While conventional approaches rely on convolutional U-Net architectures, recent Transformer-based designs have demonstrated superior performance and scalability.","However, Transformer architectures, which tokenize input data (via \"patchification\"), face a trade-off between visual fidelity and computational complexity due to the quadratic nature of self-attention operations concerning token length.","While larger patch sizes enable attention computation efficiency, they struggle to capture fine-grained visual details, leading to image distortions.","To address this challenge, we propose augmenting the Diffusion model with the Multi-Resolution network (DiMR), a framework that refines features across multiple resolutions, progressively enhancing detail from low to high resolution.","Additionally, we introduce Time-Dependent Layer Normalization (TD-LN), a parameter-efficient approach that incorporates time-dependent parameters into layer normalization to inject time information and achieve superior performance.","Our method's efficacy is demonstrated on the class-conditional ImageNet generation benchmark, where DiMR-XL variants outperform prior diffusion models, setting new state-of-the-art FID scores of 1.70 on ImageNet 256 x 256 and 2.89 on ImageNet 512 x 512.","Project page: https://qihao067.github.io/projects/DiMR"],"url":"http://arxiv.org/abs/2406.09416v1","category":"cs.CV"}
{"created":"2024-06-13 17:59:58","title":"Rethinking Score Distillation as a Bridge Between Image Distributions","abstract":"Score distillation sampling (SDS) has proven to be an important tool, enabling the use of large-scale diffusion priors for tasks operating in data-poor domains. Unfortunately, SDS has a number of characteristic artifacts that limit its usefulness in general-purpose applications. In this paper, we make progress toward understanding the behavior of SDS and its variants by viewing them as solving an optimal-cost transport path from a source distribution to a target distribution. Under this new interpretation, these methods seek to transport corrupted images (source) to the natural image distribution (target). We argue that current methods' characteristic artifacts are caused by (1) linear approximation of the optimal path and (2) poor estimates of the source distribution. We show that calibrating the text conditioning of the source distribution can produce high-quality generation and translation results with little extra overhead. Our method can be easily applied across many domains, matching or beating the performance of specialized methods. We demonstrate its utility in text-to-2D, text-based NeRF optimization, translating paintings to real images, optical illusion generation, and 3D sketch-to-real. We compare our method to existing approaches for score distillation sampling and show that it can produce high-frequency details with realistic colors.","sentences":["Score distillation sampling (SDS) has proven to be an important tool, enabling the use of large-scale diffusion priors for tasks operating in data-poor domains.","Unfortunately, SDS has a number of characteristic artifacts that limit its usefulness in general-purpose applications.","In this paper, we make progress toward understanding the behavior of SDS and its variants by viewing them as solving an optimal-cost transport path from a source distribution to a target distribution.","Under this new interpretation, these methods seek to transport corrupted images (source) to the natural image distribution (target).","We argue that current methods' characteristic artifacts are caused by (1) linear approximation of the optimal path and (2) poor estimates of the source distribution.","We show that calibrating the text conditioning of the source distribution can produce high-quality generation and translation results with little extra overhead.","Our method can be easily applied across many domains, matching or beating the performance of specialized methods.","We demonstrate its utility in text-to-2D, text-based NeRF optimization, translating paintings to real images, optical illusion generation, and 3D sketch-to-real.","We compare our method to existing approaches for score distillation sampling and show that it can produce high-frequency details with realistic colors."],"url":"http://arxiv.org/abs/2406.09417v1","category":"cs.CV"}
{"created":"2024-06-13 17:59:56","title":"Interpreting the Weight Space of Customized Diffusion Models","abstract":"We investigate the space of weights spanned by a large collection of customized diffusion models. We populate this space by creating a dataset of over 60,000 models, each of which is a base model fine-tuned to insert a different person's visual identity. We model the underlying manifold of these weights as a subspace, which we term weights2weights. We demonstrate three immediate applications of this space -- sampling, editing, and inversion. First, as each point in the space corresponds to an identity, sampling a set of weights from it results in a model encoding a novel identity. Next, we find linear directions in this space corresponding to semantic edits of the identity (e.g., adding a beard). These edits persist in appearance across generated samples. Finally, we show that inverting a single image into this space reconstructs a realistic identity, even if the input image is out of distribution (e.g., a painting). Our results indicate that the weight space of fine-tuned diffusion models behaves as an interpretable latent space of identities.","sentences":["We investigate the space of weights spanned by a large collection of customized diffusion models.","We populate this space by creating a dataset of over 60,000 models, each of which is a base model fine-tuned to insert a different person's visual identity.","We model the underlying manifold of these weights as a subspace, which we term weights2weights.","We demonstrate three immediate applications of this space -- sampling, editing, and inversion.","First, as each point in the space corresponds to an identity, sampling a set of weights from it results in a model encoding a novel identity.","Next, we find linear directions in this space corresponding to semantic edits of the identity (e.g., adding a beard).","These edits persist in appearance across generated samples.","Finally, we show that inverting a single image into this space reconstructs a realistic identity, even if the input image is out of distribution (e.g., a painting).","Our results indicate that the weight space of fine-tuned diffusion models behaves as an interpretable latent space of identities."],"url":"http://arxiv.org/abs/2406.09413v1","category":"cs.CV"}
{"created":"2024-06-13 17:59:56","title":"Depth Anything V2","abstract":"This work presents Depth Anything V2. Without pursuing fancy techniques, we aim to reveal crucial findings to pave the way towards building a powerful monocular depth estimation model. Notably, compared with V1, this version produces much finer and more robust depth predictions through three key practices: 1) replacing all labeled real images with synthetic images, 2) scaling up the capacity of our teacher model, and 3) teaching student models via the bridge of large-scale pseudo-labeled real images. Compared with the latest models built on Stable Diffusion, our models are significantly more efficient (more than 10x faster) and more accurate. We offer models of different scales (ranging from 25M to 1.3B params) to support extensive scenarios. Benefiting from their strong generalization capability, we fine-tune them with metric depth labels to obtain our metric depth models. In addition to our models, considering the limited diversity and frequent noise in current test sets, we construct a versatile evaluation benchmark with precise annotations and diverse scenes to facilitate future research.","sentences":["This work presents Depth Anything V2.","Without pursuing fancy techniques, we aim to reveal crucial findings to pave the way towards building a powerful monocular depth estimation model.","Notably, compared with V1, this version produces much finer and more robust depth predictions through three key practices: 1) replacing all labeled real images with synthetic images, 2) scaling up the capacity of our teacher model, and 3) teaching student models via the bridge of large-scale pseudo-labeled real images.","Compared with the latest models built on Stable Diffusion, our models are significantly more efficient (more than 10x faster) and more accurate.","We offer models of different scales (ranging from 25M to 1.3B params) to support extensive scenarios.","Benefiting from their strong generalization capability, we fine-tune them with metric depth labels to obtain our metric depth models.","In addition to our models, considering the limited diversity and frequent noise in current test sets, we construct a versatile evaluation benchmark with precise annotations and diverse scenes to facilitate future research."],"url":"http://arxiv.org/abs/2406.09414v1","category":"cs.CV"}
{"created":"2024-06-13 17:59:53","title":"Explore the Limits of Omni-modal Pretraining at Scale","abstract":"We propose to build omni-modal intelligence, which is capable of understanding any modality and learning universal representations. In specific, we propose a scalable pretraining paradigm, named Multimodal Context (MiCo), which can scale up the numbers of modalities and amount of data, together with the model parameters, in the pretraining process. With MiCo, the pretrained models show significant emergent abilities in multimodal learning, which are evaluated on the following tasks: i) single-modality perception benchmarks of 10 different modalities, ii) 25 cross-modality understanding tasks of retrieval, question-answering, captioning, and iii) 18 multimodal large language model benchmarks. Our models establish 37 new records for state-of-the-art performance. We hope that our research could contribute to the development of omni-modal intelligence. Code and Models are at https://github.com/invictus717/MiCo","sentences":["We propose to build omni-modal intelligence, which is capable of understanding any modality and learning universal representations.","In specific, we propose a scalable pretraining paradigm, named Multimodal Context (MiCo), which can scale up the numbers of modalities and amount of data, together with the model parameters, in the pretraining process.","With MiCo, the pretrained models show significant emergent abilities in multimodal learning, which are evaluated on the following tasks: i) single-modality perception benchmarks of 10 different modalities, ii) 25 cross-modality understanding tasks of retrieval, question-answering, captioning, and iii) 18 multimodal large language model benchmarks.","Our models establish 37 new records for state-of-the-art performance.","We hope that our research could contribute to the development of omni-modal intelligence.","Code and Models are at https://github.com/invictus717/MiCo"],"url":"http://arxiv.org/abs/2406.09412v1","category":"cs.CV"}
{"created":"2024-06-13 17:59:52","title":"MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding","abstract":"We introduce MuirBench, a comprehensive benchmark that focuses on robust multi-image understanding capabilities of multimodal LLMs. MuirBench consists of 12 diverse multi-image tasks (e.g., scene understanding, ordering) that involve 10 categories of multi-image relations (e.g., multiview, temporal relations). Comprising 11,264 images and 2,600 multiple-choice questions, MuirBench is created in a pairwise manner, where each standard instance is paired with an unanswerable variant that has minimal semantic differences, in order for a reliable assessment. Evaluated upon 20 recent multi-modal LLMs, our results reveal that even the best-performing models like GPT-4o and Gemini Pro find it challenging to solve MuirBench, achieving 68.0% and 49.3% in accuracy. Open-source multimodal LLMs trained on single images can hardly generalize to multi-image questions, hovering below 33.3% in accuracy. These results highlight the importance of MuirBench in encouraging the community to develop multimodal LLMs that can look beyond a single image, suggesting potential pathways for future improvements.","sentences":["We introduce MuirBench, a comprehensive benchmark that focuses on robust multi-image understanding capabilities of multimodal LLMs.","MuirBench consists of 12 diverse multi-image tasks (e.g., scene understanding, ordering) that involve 10 categories of multi-image relations (e.g., multiview, temporal relations).","Comprising 11,264 images and 2,600 multiple-choice questions, MuirBench is created in a pairwise manner, where each standard instance is paired with an unanswerable variant that has minimal semantic differences, in order for a reliable assessment.","Evaluated upon 20 recent multi-modal LLMs, our results reveal that even the best-performing models like GPT-4o and Gemini Pro find it challenging to solve MuirBench, achieving 68.0% and 49.3% in accuracy.","Open-source multimodal LLMs trained on single images can hardly generalize to multi-image questions, hovering below 33.3% in accuracy.","These results highlight the importance of MuirBench in encouraging the community to develop multimodal LLMs that can look beyond a single image, suggesting potential pathways for future improvements."],"url":"http://arxiv.org/abs/2406.09411v1","category":"cs.CV"}
{"created":"2024-06-13 17:59:51","title":"Scene Graph Generation in Large-Size VHR Satellite Imagery: A Large-Scale Dataset and A Context-Aware Approach","abstract":"Scene graph generation (SGG) in satellite imagery (SAI) benefits promoting intelligent understanding of geospatial scenarios from perception to cognition. In SAI, objects exhibit great variations in scales and aspect ratios, and there exist rich relationships between objects (even between spatially disjoint objects), which makes it necessary to holistically conduct SGG in large-size very-high-resolution (VHR) SAI. However, the lack of SGG datasets with large-size VHR SAI has constrained the advancement of SGG in SAI. Due to the complexity of large-size VHR SAI, mining triplets <subject, relationship, object> in large-size VHR SAI heavily relies on long-range contextual reasoning. Consequently, SGG models designed for small-size natural imagery are not directly applicable to large-size VHR SAI. To address the scarcity of datasets, this paper constructs a large-scale dataset for SGG in large-size VHR SAI with image sizes ranging from 512 x 768 to 27,860 x 31,096 pixels, named RSG, encompassing over 210,000 objects and more than 400,000 triplets. To realize SGG in large-size VHR SAI, we propose a context-aware cascade cognition (CAC) framework to understand SAI at three levels: object detection (OBD), pair pruning and relationship prediction. As a fundamental prerequisite for SGG in large-size SAI, a holistic multi-class object detection network (HOD-Net) that can flexibly integrate multi-scale contexts is proposed. With the consideration that there exist a huge amount of object pairs in large-size SAI but only a minority of object pairs contain meaningful relationships, we design a pair proposal generation (PPG) network via adversarial reconstruction to select high-value pairs. Furthermore, a relationship prediction network with context-aware messaging (RPCM) is proposed to predict the relationship types of these pairs.","sentences":["Scene graph generation (SGG) in satellite imagery (SAI) benefits promoting intelligent understanding of geospatial scenarios from perception to cognition.","In SAI, objects exhibit great variations in scales and aspect ratios, and there exist rich relationships between objects (even between spatially disjoint objects), which makes it necessary to holistically conduct SGG in large-size very-high-resolution (VHR) SAI.","However, the lack of SGG datasets with large-size VHR SAI has constrained the advancement of SGG in SAI.","Due to the complexity of large-size VHR SAI, mining triplets <subject, relationship, object> in large-size VHR SAI heavily relies on long-range contextual reasoning.","Consequently, SGG models designed for small-size natural imagery are not directly applicable to large-size VHR SAI.","To address the scarcity of datasets, this paper constructs a large-scale dataset for SGG in large-size VHR SAI with image sizes ranging from 512 x 768 to 27,860 x 31,096 pixels, named RSG, encompassing over 210,000 objects and more than 400,000 triplets.","To realize SGG in large-size VHR SAI, we propose a context-aware cascade cognition (CAC) framework to understand SAI at three levels: object detection (OBD), pair pruning and relationship prediction.","As a fundamental prerequisite for SGG in large-size SAI, a holistic multi-class object detection network (HOD-Net) that can flexibly integrate multi-scale contexts is proposed.","With the consideration that there exist a huge amount of object pairs in large-size SAI but only a minority of object pairs contain meaningful relationships, we design a pair proposal generation (PPG) network via adversarial reconstruction to select high-value pairs.","Furthermore, a relationship prediction network with context-aware messaging (RPCM) is proposed to predict the relationship types of these pairs."],"url":"http://arxiv.org/abs/2406.09410v1","category":"cs.CV"}
{"created":"2024-06-13 17:59:44","title":"Data Attribution for Text-to-Image Models by Unlearning Synthesized Images","abstract":"The goal of data attribution for text-to-image models is to identify the training images that most influence the generation of a new image. We can define \"influence\" by saying that, for a given output, if a model is retrained from scratch without that output's most influential images, the model should then fail to generate that output image. Unfortunately, directly searching for these influential images is computationally infeasible, since it would require repeatedly retraining from scratch. We propose a new approach that efficiently identifies highly-influential images. Specifically, we simulate unlearning the synthesized image, proposing a method to increase the training loss on the output image, without catastrophic forgetting of other, unrelated concepts. Then, we find training images that are forgotten by proxy, identifying ones with significant loss deviations after the unlearning process, and label these as influential. We evaluate our method with a computationally intensive but \"gold-standard\" retraining from scratch and demonstrate our method's advantages over previous methods.","sentences":["The goal of data attribution for text-to-image models is to identify the training images that most influence the generation of a new image.","We can define \"influence\" by saying that, for a given output, if a model is retrained from scratch without that output's most influential images, the model should then fail to generate that output image.","Unfortunately, directly searching for these influential images is computationally infeasible, since it would require repeatedly retraining from scratch.","We propose a new approach that efficiently identifies highly-influential images.","Specifically, we simulate unlearning the synthesized image, proposing a method to increase the training loss on the output image, without catastrophic forgetting of other, unrelated concepts.","Then, we find training images that are forgotten by proxy, identifying ones with significant loss deviations after the unlearning process, and label these as influential.","We evaluate our method with a computationally intensive but \"gold-standard\" retraining from scratch and demonstrate our method's advantages over previous methods."],"url":"http://arxiv.org/abs/2406.09408v1","category":"cs.CV"}
{"created":"2024-06-13 17:59:42","title":"4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities","abstract":"Current multimodal and multitask foundation models like 4M or UnifiedIO show promising results, but in practice their out-of-the-box abilities to accept diverse inputs and perform diverse tasks are limited by the (usually rather small) number of modalities and tasks they are trained on. In this paper, we expand upon the capabilities of them by training a single model on tens of highly diverse modalities and by performing co-training on large-scale multimodal datasets and text corpora. This includes training on several semantic and geometric modalities, feature maps from recent state of the art models like DINOv2 and ImageBind, pseudo labels of specialist models like SAM and 4DHumans, and a range of new modalities that allow for novel ways to interact with the model and steer the generation, for example image metadata or color palettes. A crucial step in this process is performing discrete tokenization on various modalities, whether they are image-like, neural network feature maps, vectors, structured data like instance segmentation or human poses, or data that can be represented as text. Through this, we expand on the out-of-the-box capabilities of multimodal models and specifically show the possibility of training one model to solve at least 3x more tasks/modalities than existing ones and doing so without a loss in performance. This enables more fine-grained and controllable multimodal generation capabilities and allows us to study the distillation of models trained on diverse data and objectives into a unified model. We successfully scale the training to a three billion parameter model using tens of modalities and different datasets. The resulting models and training code are open sourced at 4m.epfl.ch.","sentences":["Current multimodal and multitask foundation models like 4M or UnifiedIO show promising results, but in practice their out-of-the-box abilities to accept diverse inputs and perform diverse tasks are limited by the (usually rather small) number of modalities and tasks they are trained on.","In this paper, we expand upon the capabilities of them by training a single model on tens of highly diverse modalities and by performing co-training on large-scale multimodal datasets and text corpora.","This includes training on several semantic and geometric modalities, feature maps from recent state of the art models like DINOv2 and ImageBind, pseudo labels of specialist models like SAM and 4DHumans, and a range of new modalities that allow for novel ways to interact with the model and steer the generation, for example image metadata or color palettes.","A crucial step in this process is performing discrete tokenization on various modalities, whether they are image-like, neural network feature maps, vectors, structured data like instance segmentation or human poses, or data that can be represented as text.","Through this, we expand on the out-of-the-box capabilities of multimodal models and specifically show the possibility of training one model to solve at least 3x more tasks/modalities than existing ones and doing so without a loss in performance.","This enables more fine-grained and controllable multimodal generation capabilities and allows us to study the distillation of models trained on diverse data and objectives into a unified model.","We successfully scale the training to a three billion parameter model using tens of modalities and different datasets.","The resulting models and training code are open sourced at 4m.epfl.ch."],"url":"http://arxiv.org/abs/2406.09406v1","category":"cs.CV"}
{"created":"2024-06-13 17:59:32","title":"ConsistDreamer: 3D-Consistent 2D Diffusion for High-Fidelity Scene Editing","abstract":"This paper proposes ConsistDreamer - a novel framework that lifts 2D diffusion models with 3D awareness and 3D consistency, thus enabling high-fidelity instruction-guided scene editing. To overcome the fundamental limitation of missing 3D consistency in 2D diffusion models, our key insight is to introduce three synergetic strategies that augment the input of the 2D diffusion model to become 3D-aware and to explicitly enforce 3D consistency during the training process. Specifically, we design surrounding views as context-rich input for the 2D diffusion model, and generate 3D-consistent, structured noise instead of image-independent noise. Moreover, we introduce self-supervised consistency-enforcing training within the per-scene editing procedure. Extensive evaluation shows that our ConsistDreamer achieves state-of-the-art performance for instruction-guided scene editing across various scenes and editing instructions, particularly in complicated large-scale indoor scenes from ScanNet++, with significantly improved sharpness and fine-grained textures. Notably, ConsistDreamer stands as the first work capable of successfully editing complex (e.g., plaid/checkered) patterns. Our project page is at immortalco.github.io/ConsistDreamer.","sentences":["This paper proposes ConsistDreamer - a novel framework that lifts 2D diffusion models with 3D awareness and 3D consistency, thus enabling high-fidelity instruction-guided scene editing.","To overcome the fundamental limitation of missing 3D consistency in 2D diffusion models, our key insight is to introduce three synergetic strategies that augment the input of the 2D diffusion model to become 3D-aware and to explicitly enforce 3D consistency during the training process.","Specifically, we design surrounding views as context-rich input for the 2D diffusion model, and generate 3D-consistent, structured noise instead of image-independent noise.","Moreover, we introduce self-supervised consistency-enforcing training within the per-scene editing procedure.","Extensive evaluation shows that our ConsistDreamer achieves state-of-the-art performance for instruction-guided scene editing across various scenes and editing instructions, particularly in complicated large-scale indoor scenes from ScanNet++, with significantly improved sharpness and fine-grained textures.","Notably, ConsistDreamer stands as the first work capable of successfully editing complex (e.g., plaid/checkered) patterns.","Our project page is at immortalco.github.io/ConsistDreamer."],"url":"http://arxiv.org/abs/2406.09404v1","category":"cs.CV"}
{"created":"2024-06-13 17:59:30","title":"MMScan: A Multi-Modal 3D Scene Dataset with Hierarchical Grounded Language Annotations","abstract":"With the emergence of LLMs and their integration with other data modalities, multi-modal 3D perception attracts more attention due to its connectivity to the physical world and makes rapid progress. However, limited by existing datasets, previous works mainly focus on understanding object properties or inter-object spatial relationships in a 3D scene. To tackle this problem, this paper builds the first largest ever multi-modal 3D scene dataset and benchmark with hierarchical grounded language annotations, MMScan. It is constructed based on a top-down logic, from region to object level, from a single target to inter-target relationships, covering holistic aspects of spatial and attribute understanding. The overall pipeline incorporates powerful VLMs via carefully designed prompts to initialize the annotations efficiently and further involve humans' correction in the loop to ensure the annotations are natural, correct, and comprehensive. Built upon existing 3D scanning data, the resulting multi-modal 3D dataset encompasses 1.4M meta-annotated captions on 109k objects and 7.7k regions as well as over 3.04M diverse samples for 3D visual grounding and question-answering benchmarks. We evaluate representative baselines on our benchmarks, analyze their capabilities in different aspects, and showcase the key problems to be addressed in the future. Furthermore, we use this high-quality dataset to train state-of-the-art 3D visual grounding and LLMs and obtain remarkable performance improvement both on existing benchmarks and in-the-wild evaluation. Codes, datasets, and benchmarks will be available at https://github.com/OpenRobotLab/EmbodiedScan.","sentences":["With the emergence of LLMs and their integration with other data modalities, multi-modal 3D perception attracts more attention due to its connectivity to the physical world and makes rapid progress.","However, limited by existing datasets, previous works mainly focus on understanding object properties or inter-object spatial relationships in a 3D scene.","To tackle this problem, this paper builds the first largest ever multi-modal 3D scene dataset and benchmark with hierarchical grounded language annotations, MMScan.","It is constructed based on a top-down logic, from region to object level, from a single target to inter-target relationships, covering holistic aspects of spatial and attribute understanding.","The overall pipeline incorporates powerful VLMs via carefully designed prompts to initialize the annotations efficiently and further involve humans' correction in the loop to ensure the annotations are natural, correct, and comprehensive.","Built upon existing 3D scanning data, the resulting multi-modal 3D dataset encompasses 1.4M meta-annotated captions on 109k objects and 7.7k regions as well as over 3.04M diverse samples for 3D visual grounding and question-answering benchmarks.","We evaluate representative baselines on our benchmarks, analyze their capabilities in different aspects, and showcase the key problems to be addressed in the future.","Furthermore, we use this high-quality dataset to train state-of-the-art 3D visual grounding and LLMs and obtain remarkable performance improvement both on existing benchmarks and in-the-wild evaluation.","Codes, datasets, and benchmarks will be available at https://github.com/OpenRobotLab/EmbodiedScan."],"url":"http://arxiv.org/abs/2406.09401v1","category":"cs.CV"}
{"created":"2024-06-13 17:59:30","title":"Instruct 4D-to-4D: Editing 4D Scenes as Pseudo-3D Scenes Using 2D Diffusion","abstract":"This paper proposes Instruct 4D-to-4D that achieves 4D awareness and spatial-temporal consistency for 2D diffusion models to generate high-quality instruction-guided dynamic scene editing results. Traditional applications of 2D diffusion models in dynamic scene editing often result in inconsistency, primarily due to their inherent frame-by-frame editing methodology. Addressing the complexities of extending instruction-guided editing to 4D, our key insight is to treat a 4D scene as a pseudo-3D scene, decoupled into two sub-problems: achieving temporal consistency in video editing and applying these edits to the pseudo-3D scene. Following this, we first enhance the Instruct-Pix2Pix (IP2P) model with an anchor-aware attention module for batch processing and consistent editing. Additionally, we integrate optical flow-guided appearance propagation in a sliding window fashion for more precise frame-to-frame editing and incorporate depth-based projection to manage the extensive data of pseudo-3D scenes, followed by iterative editing to achieve convergence. We extensively evaluate our approach in various scenes and editing instructions, and demonstrate that it achieves spatially and temporally consistent editing results, with significantly enhanced detail and sharpness over the prior art. Notably, Instruct 4D-to-4D is general and applicable to both monocular and challenging multi-camera scenes. Code and more results are available at immortalco.github.io/Instruct-4D-to-4D.","sentences":["This paper proposes Instruct 4D-to-4D that achieves 4D awareness and spatial-temporal consistency for 2D diffusion models to generate high-quality instruction-guided dynamic scene editing results.","Traditional applications of 2D diffusion models in dynamic scene editing often result in inconsistency, primarily due to their inherent frame-by-frame editing methodology.","Addressing the complexities of extending instruction-guided editing to 4D, our key insight is to treat a 4D scene as a pseudo-3D scene, decoupled into two sub-problems: achieving temporal consistency in video editing and applying these edits to the pseudo-3D scene.","Following this, we first enhance the Instruct-Pix2Pix (IP2P) model with an anchor-aware attention module for batch processing and consistent editing.","Additionally, we integrate optical flow-guided appearance propagation in a sliding window fashion for more precise frame-to-frame editing and incorporate depth-based projection to manage the extensive data of pseudo-3D scenes, followed by iterative editing to achieve convergence.","We extensively evaluate our approach in various scenes and editing instructions, and demonstrate that it achieves spatially and temporally consistent editing results, with significantly enhanced detail and sharpness over the prior art.","Notably, Instruct 4D-to-4D is general and applicable to both monocular and challenging multi-camera scenes.","Code and more results are available at immortalco.github.io/Instruct-4D-to-4D."],"url":"http://arxiv.org/abs/2406.09402v1","category":"cs.CV"}
{"created":"2024-06-13 17:59:29","title":"Yo'LLaVA: Your Personalized Language and Vision Assistant","abstract":"Large Multimodal Models (LMMs) have shown remarkable capabilities across a variety of tasks (e.g., image captioning, visual question answering). While broad, their knowledge remains generic (e.g., recognizing a dog), and they are unable to handle personalized subjects (e.g., recognizing a user's pet dog). Human reasoning, in contrast, typically operates within the context of specific subjects in our surroundings. For example, one might ask, \"What should I buy for my dog's birthday?\"; as opposed to a generic inquiry about \"What should I buy for a dog's birthday?\". Similarly, when looking at a friend's image, the interest lies in seeing their activities (e.g., \"my friend is holding a cat\"), rather than merely observing generic human actions (e.g., \"a man is holding a cat\"). In this paper, we introduce the novel task of personalizing LMMs, so that they can have conversations about a specific subject. We propose Yo'LLaVA, which learns to embed a personalized subject into a set of latent tokens given a handful of example images of the subject. Our qualitative and quantitative analyses reveal that Yo'LLaVA can learn the concept more efficiently using fewer tokens and more effectively encode the visual attributes compared to strong prompting baselines (e.g., LLaVA).","sentences":["Large Multimodal Models (LMMs) have shown remarkable capabilities across a variety of tasks (e.g., image captioning, visual question answering).","While broad, their knowledge remains generic (e.g., recognizing a dog), and they are unable to handle personalized subjects (e.g., recognizing a user's pet dog).","Human reasoning, in contrast, typically operates within the context of specific subjects in our surroundings.","For example, one might ask, \"What should I buy for my dog's birthday?\"; as opposed to a generic inquiry about \"What should I buy for a dog's birthday?\".","Similarly, when looking at a friend's image, the interest lies in seeing their activities (e.g., \"my friend is holding a cat\"), rather than merely observing generic human actions (e.g., \"a man is holding a cat\").","In this paper, we introduce the novel task of personalizing LMMs, so that they can have conversations about a specific subject.","We propose Yo'LLaVA, which learns to embed a personalized subject into a set of latent tokens given a handful of example images of the subject.","Our qualitative and quantitative analyses reveal that Yo'LLaVA can learn the concept more efficiently using fewer tokens and more effectively encode the visual attributes compared to strong prompting baselines (e.g., LLaVA)."],"url":"http://arxiv.org/abs/2406.09400v1","category":"cs.CV"}
{"created":"2024-06-13 17:59:26","title":"OmniTokenizer: A Joint Image-Video Tokenizer for Visual Generation","abstract":"Tokenizer, serving as a translator to map the intricate visual data into a compact latent space, lies at the core of visual generative models. Based on the finding that existing tokenizers are tailored to image or video inputs, this paper presents OmniTokenizer, a transformer-based tokenizer for joint image and video tokenization. OmniTokenizer is designed with a spatial-temporal decoupled architecture, which integrates window and causal attention for spatial and temporal modeling. To exploit the complementary nature of image and video data, we further propose a progressive training strategy, where OmniTokenizer is first trained on image data on a fixed resolution to develop the spatial encoding capacity and then jointly trained on image and video data on multiple resolutions to learn the temporal dynamics. OmniTokenizer, for the first time, handles both image and video inputs within a unified framework and proves the possibility of realizing their synergy. Extensive experiments demonstrate that OmniTokenizer achieves state-of-the-art (SOTA) reconstruction performance on various image and video datasets, e.g., 1.11 reconstruction FID on ImageNet and 42 reconstruction FVD on UCF-101, beating the previous SOTA methods by 13% and 26%, respectively. Additionally, we also show that when integrated with OmniTokenizer, both language model-based approaches and diffusion models can realize advanced visual synthesis performance, underscoring the superiority and versatility of our method. Code is available at https://github.com/FoundationVision/OmniTokenizer.","sentences":["Tokenizer, serving as a translator to map the intricate visual data into a compact latent space, lies at the core of visual generative models.","Based on the finding that existing tokenizers are tailored to image or video inputs, this paper presents OmniTokenizer, a transformer-based tokenizer for joint image and video tokenization.","OmniTokenizer is designed with a spatial-temporal decoupled architecture, which integrates window and causal attention for spatial and temporal modeling.","To exploit the complementary nature of image and video data, we further propose a progressive training strategy, where OmniTokenizer is first trained on image data on a fixed resolution to develop the spatial encoding capacity and then jointly trained on image and video data on multiple resolutions to learn the temporal dynamics.","OmniTokenizer, for the first time, handles both image and video inputs within a unified framework and proves the possibility of realizing their synergy.","Extensive experiments demonstrate that OmniTokenizer achieves state-of-the-art (SOTA) reconstruction performance on various image and video datasets, e.g., 1.11 reconstruction FID on ImageNet and 42 reconstruction FVD on UCF-101, beating the previous SOTA methods by 13% and 26%, respectively.","Additionally, we also show that when integrated with OmniTokenizer, both language model-based approaches and diffusion models can realize advanced visual synthesis performance, underscoring the superiority and versatility of our method.","Code is available at https://github.com/FoundationVision/OmniTokenizer."],"url":"http://arxiv.org/abs/2406.09399v1","category":"cs.CV"}
{"created":"2024-06-13 17:59:23","title":"Real-Time Deepfake Detection in the Real-World","abstract":"Recent improvements in generative AI made synthesizing fake images easy; as they can be used to cause harm, it is crucial to develop accurate techniques to identify them. This paper introduces \"Locally Aware Deepfake Detection Algorithm\" (LaDeDa), that accepts a single 9x9 image patch and outputs its deepfake score. The image deepfake score is the pooled score of its patches. With merely patch-level information, LaDeDa significantly improves over the state-of-the-art, achieving around 99% mAP on current benchmarks. Owing to the patch-level structure of LaDeDa, we hypothesize that the generation artifacts can be detected by a simple model. We therefore distill LaDeDa into Tiny-LaDeDa, a highly efficient model consisting of only 4 convolutional layers. Remarkably, Tiny-LaDeDa has 375x fewer FLOPs and is 10,000x more parameter-efficient than LaDeDa, allowing it to run efficiently on edge devices with a minor decrease in accuracy. These almost-perfect scores raise the question: is the task of deepfake detection close to being solved? Perhaps surprisingly, our investigation reveals that current training protocols prevent methods from generalizing to real-world deepfakes extracted from social media. To address this issue, we introduce WildRF, a new deepfake detection dataset curated from several popular social networks. Our method achieves the top performance of 93.7% mAP on WildRF, however the large gap from perfect accuracy shows that reliable real-world deepfake detection is still unsolved.","sentences":["Recent improvements in generative AI made synthesizing fake images easy; as they can be used to cause harm, it is crucial to develop accurate techniques to identify them.","This paper introduces \"Locally Aware Deepfake Detection Algorithm\" (LaDeDa), that accepts a single 9x9 image patch and outputs its deepfake score.","The image deepfake score is the pooled score of its patches.","With merely patch-level information, LaDeDa significantly improves over the state-of-the-art, achieving around 99% mAP on current benchmarks.","Owing to the patch-level structure of LaDeDa, we hypothesize that the generation artifacts can be detected by a simple model.","We therefore distill LaDeDa into Tiny-LaDeDa, a highly efficient model consisting of only 4 convolutional layers.","Remarkably, Tiny-LaDeDa has 375x fewer FLOPs and is 10,000x more parameter-efficient than LaDeDa, allowing it to run efficiently on edge devices with a minor decrease in accuracy.","These almost-perfect scores raise the question: is the task of deepfake detection close to being solved?","Perhaps surprisingly, our investigation reveals that current training protocols prevent methods from generalizing to real-world deepfakes extracted from social media.","To address this issue, we introduce WildRF, a new deepfake detection dataset curated from several popular social networks.","Our method achieves the top performance of 93.7% mAP on WildRF, however the large gap from perfect accuracy shows that reliable real-world deepfake detection is still unsolved."],"url":"http://arxiv.org/abs/2406.09398v1","category":"cs.CV"}
{"created":"2024-06-13 17:59:20","title":"Aligning Vision Models with Human Aesthetics in Retrieval: Benchmarks and Algorithms","abstract":"Modern vision models are trained on very large noisy datasets. While these models acquire strong capabilities, they may not follow the user's intent to output the desired results in certain aspects, e.g., visual aesthetic, preferred style, and responsibility. In this paper, we target the realm of visual aesthetics and aim to align vision models with human aesthetic standards in a retrieval system. Advanced retrieval systems usually adopt a cascade of aesthetic models as re-rankers or filters, which are limited to low-level features like saturation and perform poorly when stylistic, cultural or knowledge contexts are involved. We find that utilizing the reasoning ability of large language models (LLMs) to rephrase the search query and extend the aesthetic expectations can make up for this shortcoming. Based on the above findings, we propose a preference-based reinforcement learning method that fine-tunes the vision models to distill the knowledge from both LLMs reasoning and the aesthetic models to better align the vision models with human aesthetics. Meanwhile, with rare benchmarks designed for evaluating retrieval systems, we leverage large multi-modality model (LMM) to evaluate the aesthetic performance with their strong abilities. As aesthetic assessment is one of the most subjective tasks, to validate the robustness of LMM, we further propose a novel dataset named HPIR to benchmark the alignment with human aesthetics. Experiments demonstrate that our method significantly enhances the aesthetic behaviors of the vision models, under several metrics. We believe the proposed algorithm can be a general practice for aligning vision models with human values.","sentences":["Modern vision models are trained on very large noisy datasets.","While these models acquire strong capabilities, they may not follow the user's intent to output the desired results in certain aspects, e.g., visual aesthetic, preferred style, and responsibility.","In this paper, we target the realm of visual aesthetics and aim to align vision models with human aesthetic standards in a retrieval system.","Advanced retrieval systems usually adopt a cascade of aesthetic models as re-rankers or filters, which are limited to low-level features like saturation and perform poorly when stylistic, cultural or knowledge contexts are involved.","We find that utilizing the reasoning ability of large language models (LLMs) to rephrase the search query and extend the aesthetic expectations can make up for this shortcoming.","Based on the above findings, we propose a preference-based reinforcement learning method that fine-tunes the vision models to distill the knowledge from both LLMs reasoning and the aesthetic models to better align the vision models with human aesthetics.","Meanwhile, with rare benchmarks designed for evaluating retrieval systems, we leverage large multi-modality model (LMM) to evaluate the aesthetic performance with their strong abilities.","As aesthetic assessment is one of the most subjective tasks, to validate the robustness of LMM, we further propose a novel dataset named HPIR to benchmark the alignment with human aesthetics.","Experiments demonstrate that our method significantly enhances the aesthetic behaviors of the vision models, under several metrics.","We believe the proposed algorithm can be a general practice for aligning vision models with human values."],"url":"http://arxiv.org/abs/2406.09397v1","category":"cs.CV"}
{"created":"2024-06-13 17:59:16","title":"Too Many Frames, not all Useful:Efficient Strategies for Long-Form Video QA","abstract":"Long-form videos that span across wide temporal intervals are highly information redundant and contain multiple distinct events or entities that are often loosely-related. Therefore, when performing long-form video question answering (LVQA),all information necessary to generate a correct response can often be contained within a small subset of frames. Recent literature explore the use of large language models (LLMs) in LVQA benchmarks, achieving exceptional performance, while relying on vision language models (VLMs) to convert all visual content within videos into natural language. Such VLMs often independently caption a large number of frames uniformly sampled from long videos, which is not efficient and can mostly be redundant. Questioning these decision choices, we explore optimal strategies for key-frame selection and sequence-aware captioning, that can significantly reduce these redundancies. We propose two novel approaches that improve each of aspects, namely Hierarchical Keyframe Selector and Sequential Visual LLM. Our resulting framework termed LVNet achieves state-of-the-art performance across three benchmark LVQA datasets. Our code will be released publicly.","sentences":["Long-form videos that span across wide temporal intervals are highly information redundant and contain multiple distinct events or entities that are often loosely-related.","Therefore, when performing long-form video question answering (LVQA),all information necessary to generate a correct response can often be contained within a small subset of frames.","Recent literature explore the use of large language models (LLMs) in LVQA benchmarks, achieving exceptional performance, while relying on vision language models (VLMs) to convert all visual content within videos into natural language.","Such VLMs often independently caption a large number of frames uniformly sampled from long videos, which is not efficient and can mostly be redundant.","Questioning these decision choices, we explore optimal strategies for key-frame selection and sequence-aware captioning, that can significantly reduce these redundancies.","We propose two novel approaches that improve each of aspects, namely Hierarchical Keyframe Selector and Sequential Visual LLM.","Our resulting framework termed LVNet achieves state-of-the-art performance across three benchmark LVQA datasets.","Our code will be released publicly."],"url":"http://arxiv.org/abs/2406.09396v1","category":"cs.CV"}
{"created":"2024-06-13 17:59:11","title":"Modeling Ambient Scene Dynamics for Free-view Synthesis","abstract":"We introduce a novel method for dynamic free-view synthesis of an ambient scenes from a monocular capture bringing a immersive quality to the viewing experience. Our method builds upon the recent advancements in 3D Gaussian Splatting (3DGS) that can faithfully reconstruct complex static scenes. Previous attempts to extend 3DGS to represent dynamics have been confined to bounded scenes or require multi-camera captures, and often fail to generalize to unseen motions, limiting their practical application. Our approach overcomes these constraints by leveraging the periodicity of ambient motions to learn the motion trajectory model, coupled with careful regularization. We also propose important practical strategies to improve the visual quality of the baseline 3DGS static reconstructions and to improve memory efficiency critical for GPU-memory intensive learning. We demonstrate high-quality photorealistic novel view synthesis of several ambient natural scenes with intricate textures and fine structural elements.","sentences":["We introduce a novel method for dynamic free-view synthesis of an ambient scenes from a monocular capture bringing a immersive quality to the viewing experience.","Our method builds upon the recent advancements in 3D Gaussian Splatting (3DGS) that can faithfully reconstruct complex static scenes.","Previous attempts to extend 3DGS to represent dynamics have been confined to bounded scenes or require multi-camera captures, and often fail to generalize to unseen motions, limiting their practical application.","Our approach overcomes these constraints by leveraging the periodicity of ambient motions to learn the motion trajectory model, coupled with careful regularization.","We also propose important practical strategies to improve the visual quality of the baseline 3DGS static reconstructions and to improve memory efficiency critical for GPU-memory intensive learning.","We demonstrate high-quality photorealistic novel view synthesis of several ambient natural scenes with intricate textures and fine structural elements."],"url":"http://arxiv.org/abs/2406.09395v1","category":"cs.CV"}
{"created":"2024-06-13 17:59:10","title":"WonderWorld: Interactive 3D Scene Generation from a Single Image","abstract":"We present WonderWorld, a novel framework for \\emph{interactive} 3D scene extrapolation that enables users to explore and shape virtual environments based on a single input image and user-specified text. While significant improvements have been made to the visual quality of scene generation, existing methods are run offline, taking tens of minutes to hours to generate a scene. By leveraging Fast Gaussian Surfels and a guided diffusion-based depth estimation method, WonderWorld generates geometrically consistent extrapolation while significantly reducing computational time. Our framework generates connected and diverse 3D scenes in less than 10 seconds on a single A6000 GPU, enabling real-time user interaction and exploration. We demonstrate the potential of WonderWorld for applications in virtual reality, gaming, and creative design, where users can quickly generate and navigate immersive, potentially infinite virtual worlds from a single image. Our approach represents a significant advancement in interactive 3D scene generation, opening up new possibilities for user-driven content creation and exploration in virtual environments. We will release full code and software for reproducibility. Project website: https://WonderWorld-2024.github.io/","sentences":["We present WonderWorld, a novel framework for \\emph{interactive} 3D scene extrapolation that enables users to explore and shape virtual environments based on a single input image and user-specified text.","While significant improvements have been made to the visual quality of scene generation, existing methods are run offline, taking tens of minutes to hours to generate a scene.","By leveraging Fast Gaussian Surfels and a guided diffusion-based depth estimation method, WonderWorld generates geometrically consistent extrapolation while significantly reducing computational time.","Our framework generates connected and diverse 3D scenes in less than 10 seconds on a single A6000 GPU, enabling real-time user interaction and exploration.","We demonstrate the potential of WonderWorld for applications in virtual reality, gaming, and creative design, where users can quickly generate and navigate immersive, potentially infinite virtual worlds from a single image.","Our approach represents a significant advancement in interactive 3D scene generation, opening up new possibilities for user-driven content creation and exploration in virtual environments.","We will release full code and software for reproducibility.","Project website: https://WonderWorld-2024.github.io/"],"url":"http://arxiv.org/abs/2406.09394v1","category":"cs.CV"}
{"created":"2024-06-13 17:59:09","title":"Improving Autoregressive Training with Dynamic Oracles","abstract":"Many tasks within NLP can be framed as sequential decision problems, ranging from sequence tagging to text generation. However, for many tasks, the standard training methods, including maximum likelihood (teacher forcing) and scheduled sampling, suffer from exposure bias and a mismatch between metrics employed during training and inference. DAgger provides a solution to mitigate these problems, yet it requires a metric-specific dynamic oracle algorithm, which does not exist for many common metrics like span-based F1, ROUGE, and BLEU. In this paper, we develop these novel dynamic oracles and show they maintain DAgger's no-regret guarantee for decomposable metrics like span-based F1. We evaluate the algorithm's performance on named entity recognition (NER), text summarization, and machine translation (MT). While DAgger with dynamic oracle yields less favorable results in our MT experiments, it outperforms the baseline techniques in NER and text summarization.","sentences":["Many tasks within NLP can be framed as sequential decision problems, ranging from sequence tagging to text generation.","However, for many tasks, the standard training methods, including maximum likelihood (teacher forcing) and scheduled sampling, suffer from exposure bias and a mismatch between metrics employed during training and inference.","DAgger provides a solution to mitigate these problems, yet it requires a metric-specific dynamic oracle algorithm, which does not exist for many common metrics like span-based F1, ROUGE, and BLEU.","In this paper, we develop these novel dynamic oracles and show they maintain DAgger's no-regret guarantee for decomposable metrics like span-based F1.","We evaluate the algorithm's performance on named entity recognition (NER), text summarization, and machine translation (MT).","While DAgger with dynamic oracle yields less favorable results in our MT experiments, it outperforms the baseline techniques in NER and text summarization."],"url":"http://arxiv.org/abs/2406.09393v1","category":"cs.CL"}
{"created":"2024-06-13 17:59:06","title":"A More Practical Approach to Machine Unlearning","abstract":"Machine learning models often incorporate vast amounts of data, raising significant privacy concerns. Machine unlearning, the ability to remove the influence of specific data points from a trained model, addresses these concerns. This paper explores practical methods for implementing machine unlearning, focusing on a first-epoch gradient-ascent approach.   Key findings include: 1. Single vs. Multi-Epoch Unlearning: First-epoch gradient unlearning is more effective than multi-epoch gradients. 2. Layer-Based Unlearning: The embedding layer in GPT-2 is crucial for effective unlearning. Gradients from the output layers (11 and 12) have no impact. Efficient unlearning can be achieved using only the embedding layer, halving space complexity. 3. Influence Functions & Scoring: Techniques like Hessian Vector Product and the dot product of activations and tensors are used for quantifying unlearning. 4. Gradient Ascent Considerations: Calibration is necessary to avoid overexposing the model to specific data points during unlearning, which could prematurely terminate the process. 5. Fuzzy Matching vs. Iterative Unlearning: Fuzzy matching techniques shift the model to a new optimum, while iterative unlearning provides a more complete modality.   Our empirical evaluation confirms that first-epoch gradient ascent for machine unlearning is more effective than whole-model gradient ascent. These results highlight the potential of machine unlearning for enhancing data privacy and compliance with regulations such as GDPR and CCPA. The study underscores the importance of formal methods to comprehensively evaluate the unlearning process.","sentences":["Machine learning models often incorporate vast amounts of data, raising significant privacy concerns.","Machine unlearning, the ability to remove the influence of specific data points from a trained model, addresses these concerns.","This paper explores practical methods for implementing machine unlearning, focusing on a first-epoch gradient-ascent approach.   ","Key findings include: 1.","Single vs. Multi-Epoch Unlearning: First-epoch gradient unlearning is more effective than multi-epoch gradients.","2. Layer-Based Unlearning: The embedding layer in GPT-2 is crucial for effective unlearning.","Gradients from the output layers (11 and 12) have no impact.","Efficient unlearning can be achieved using only the embedding layer, halving space complexity.","3. Influence Functions & Scoring: Techniques like Hessian Vector Product and the dot product of activations and tensors are used for quantifying unlearning.","4. Gradient Ascent Considerations: Calibration is necessary to avoid overexposing the model to specific data points during unlearning, which could prematurely terminate the process.","5.","Fuzzy Matching vs. Iterative","Unlearning:","Fuzzy matching techniques shift the model to a new optimum, while iterative unlearning provides a more complete modality.   ","Our empirical evaluation confirms that first-epoch gradient ascent for machine unlearning is more effective than whole-model gradient ascent.","These results highlight the potential of machine unlearning for enhancing data privacy and compliance with regulations such as GDPR and CCPA.","The study underscores the importance of formal methods to comprehensively evaluate the unlearning process."],"url":"http://arxiv.org/abs/2406.09391v1","category":"cs.LG"}
{"created":"2024-06-13 17:58:40","title":"Sagiri: Low Dynamic Range Image Enhancement with Generative Diffusion Prior","abstract":"Capturing High Dynamic Range (HDR) scenery using 8-bit cameras often suffers from over-/underexposure, loss of fine details due to low bit-depth compression, skewed color distributions, and strong noise in dark areas. Traditional LDR image enhancement methods primarily focus on color mapping, which enhances the visual representation by expanding the image's color range and adjusting the brightness. However, these approaches fail to effectively restore content in dynamic range extremes, which are regions with pixel values close to 0 or 255. To address the full scope of challenges in HDR imaging and surpass the limitations of current models, we propose a novel two-stage approach. The first stage maps the color and brightness to an appropriate range while keeping the existing details, and the second stage utilizes a diffusion prior to generate content in dynamic range extremes lost during capture. This generative refinement module can also be used as a plug-and-play module to enhance and complement existing LDR enhancement models. The proposed method markedly improves the quality and details of LDR images, demonstrating superior performance through rigorous experimental validation. The project page is at https://sagiri0208.github.io","sentences":["Capturing High Dynamic Range (HDR) scenery using 8-bit cameras often suffers from over-/underexposure, loss of fine details due to low bit-depth compression, skewed color distributions, and strong noise in dark areas.","Traditional LDR image enhancement methods primarily focus on color mapping, which enhances the visual representation by expanding the image's color range and adjusting the brightness.","However, these approaches fail to effectively restore content in dynamic range extremes, which are regions with pixel values close to 0 or 255.","To address the full scope of challenges in HDR imaging and surpass the limitations of current models, we propose a novel two-stage approach.","The first stage maps the color and brightness to an appropriate range while keeping the existing details, and the second stage utilizes a diffusion prior to generate content in dynamic range extremes lost during capture.","This generative refinement module can also be used as a plug-and-play module to enhance and complement existing LDR enhancement models.","The proposed method markedly improves the quality and details of LDR images, demonstrating superior performance through rigorous experimental validation.","The project page is at https://sagiri0208.github.io"],"url":"http://arxiv.org/abs/2406.09389v1","category":"eess.IV"}
{"created":"2024-06-13 17:58:39","title":"Exploring the Spectrum of Visio-Linguistic Compositionality and Recognition","abstract":"Vision and language models (VLMs) such as CLIP have showcased remarkable zero-shot recognition abilities yet face challenges in visio-linguistic compositionality, particularly in linguistic comprehension and fine-grained image-text alignment. This paper explores the intricate relationship between compositionality and recognition -- two pivotal aspects of VLM capability. We conduct a comprehensive evaluation of existing VLMs, covering both pre-training approaches aimed at recognition and the fine-tuning methods designed to improve compositionality. Our evaluation employs 12 benchmarks for compositionality, along with 21 zero-shot classification and two retrieval benchmarks for recognition. In our analysis from 274 CLIP model checkpoints, we reveal patterns and trade-offs that emerge between compositional understanding and recognition accuracy. Ultimately, this necessitates strategic efforts towards developing models that improve both capabilities, as well as the meticulous formulation of benchmarks for compositionality. We open our evaluation framework at https://github.com/ytaek-oh/vl_compo.","sentences":["Vision and language models (VLMs) such as CLIP have showcased remarkable zero-shot recognition abilities yet face challenges in visio-linguistic compositionality, particularly in linguistic comprehension and fine-grained image-text alignment.","This paper explores the intricate relationship between compositionality and recognition -- two pivotal aspects of VLM capability.","We conduct a comprehensive evaluation of existing VLMs, covering both pre-training approaches aimed at recognition and the fine-tuning methods designed to improve compositionality.","Our evaluation employs 12 benchmarks for compositionality, along with 21 zero-shot classification and two retrieval benchmarks for recognition.","In our analysis from 274 CLIP model checkpoints, we reveal patterns and trade-offs that emerge between compositional understanding and recognition accuracy.","Ultimately, this necessitates strategic efforts towards developing models that improve both capabilities, as well as the meticulous formulation of benchmarks for compositionality.","We open our evaluation framework at https://github.com/ytaek-oh/vl_compo."],"url":"http://arxiv.org/abs/2406.09388v1","category":"cs.CV"}
{"created":"2024-06-13 17:58:32","title":"SimGen: Simulator-conditioned Driving Scene Generation","abstract":"Controllable synthetic data generation can substantially lower the annotation cost of training data in autonomous driving research and development. Prior works use diffusion models to generate driving images conditioned on the 3D object layout. However, those models are trained on small-scale datasets like nuScenes, which lack appearance and layout diversity. Moreover, the trained models can only generate images based on the real-world layout data from the validation set of the same dataset, where overfitting might happen. In this work, we introduce a simulator-conditioned scene generation framework called SimGen that can learn to generate diverse driving scenes by mixing data from the simulator and the real world. It uses a novel cascade diffusion pipeline to address challenging sim-to-real gaps and multi-condition conflicts. A driving video dataset DIVA is collected to enhance the generative diversity of SimGen, which contains over 147.5 hours of real-world driving videos from 73 locations worldwide and simulated driving data from the MetaDrive simulator. SimGen achieves superior generation quality and diversity while preserving controllability based on the text prompt and the layout pulled from a simulator. We further demonstrate the improvements brought by SimGen for synthetic data augmentation on the BEV detection and segmentation task and showcase its capability in safety-critical data generation. Code, data, and models will be made available.","sentences":["Controllable synthetic data generation can substantially lower the annotation cost of training data in autonomous driving research and development.","Prior works use diffusion models to generate driving images conditioned on the 3D object layout.","However, those models are trained on small-scale datasets like nuScenes, which lack appearance and layout diversity.","Moreover, the trained models can only generate images based on the real-world layout data from the validation set of the same dataset, where overfitting might happen.","In this work, we introduce a simulator-conditioned scene generation framework called SimGen that can learn to generate diverse driving scenes by mixing data from the simulator and the real world.","It uses a novel cascade diffusion pipeline to address challenging sim-to-real gaps and multi-condition conflicts.","A driving video dataset DIVA is collected to enhance the generative diversity of SimGen, which contains over 147.5 hours of real-world driving videos from 73 locations worldwide and simulated driving data from the MetaDrive simulator.","SimGen achieves superior generation quality and diversity while preserving controllability based on the text prompt and the layout pulled from a simulator.","We further demonstrate the improvements brought by SimGen for synthetic data augmentation on the BEV detection and segmentation task and showcase its capability in safety-critical data generation.","Code, data, and models will be made available."],"url":"http://arxiv.org/abs/2406.09386v1","category":"cs.CV"}
{"created":"2024-06-13 17:58:32","title":"Oblivious subspace embeddings for compressed Tucker decompositions","abstract":"Emphasis in the tensor literature on random embeddings (tools for low-distortion dimension reduction) for the canonical polyadic (CP) tensor decomposition has left analogous results for the more expressive Tucker decomposition comparatively lacking. This work establishes general Johnson-Lindenstrauss (JL) type guarantees for the estimation of Tucker decompositions when an oblivious random embedding is applied along each mode. When these embeddings are drawn from a JL-optimal family, the decomposition can be estimated within $\\varepsilon$ relative error under restrictions on the embedding dimension that are in line with recent CP results. We implement a higher-order orthogonal iteration (HOOI) decomposition algorithm with random embeddings to demonstrate the practical benefits of this approach and its potential to improve the accessibility of otherwise prohibitive tensor analyses. On moderately large face image and fMRI neuroimaging datasets, empirical results show that substantial dimension reduction is possible with minimal increase in reconstruction error relative to traditional HOOI ($\\leq$5% larger error, 50%-60% lower computation time for large models with 50% dimension reduction along each mode). Especially for large tensors, our method outperforms traditional higher-order singular value decomposition (HOSVD) and recently proposed TensorSketch methods.","sentences":["Emphasis in the tensor literature on random embeddings (tools for low-distortion dimension reduction) for the canonical polyadic (CP) tensor decomposition has left analogous results for the more expressive Tucker decomposition comparatively lacking.","This work establishes general Johnson-Lindenstrauss (JL) type guarantees for the estimation of Tucker decompositions when an oblivious random embedding is applied along each mode.","When these embeddings are drawn from a JL-optimal family, the decomposition can be estimated within $\\varepsilon$ relative error under restrictions on the embedding dimension that are in line with recent CP results.","We implement a higher-order orthogonal iteration (HOOI) decomposition algorithm with random embeddings to demonstrate the practical benefits of this approach and its potential to improve the accessibility of otherwise prohibitive tensor analyses.","On moderately large face image and fMRI neuroimaging datasets, empirical results show that substantial dimension reduction is possible with minimal increase in reconstruction error relative to traditional HOOI ($\\leq$5% larger error, 50%-60% lower computation time for large models with 50% dimension reduction along each mode).","Especially for large tensors, our method outperforms traditional higher-order singular value decomposition (HOSVD) and recently proposed TensorSketch methods."],"url":"http://arxiv.org/abs/2406.09387v1","category":"stat.ML"}
{"created":"2024-06-13 17:57:30","title":"Towards Vision-Language Geo-Foundation Model: A Survey","abstract":"Vision-Language Foundation Models (VLFMs) have made remarkable progress on various multimodal tasks, such as image captioning, image-text retrieval, visual question answering, and visual grounding. However, most methods rely on training with general image datasets, and the lack of geospatial data leads to poor performance on earth observation. Numerous geospatial image-text pair datasets and VLFMs fine-tuned on them have been proposed recently. These new approaches aim to leverage large-scale, multimodal geospatial data to build versatile intelligent models with diverse geo-perceptive capabilities, which we refer to as Vision-Language Geo-Foundation Models (VLGFMs). This paper thoroughly reviews VLGFMs, summarizing and analyzing recent developments in the field. In particular, we introduce the background and motivation behind the rise of VLGFMs, highlighting their unique research significance. Then, we systematically summarize the core technologies employed in VLGFMs, including data construction, model architectures, and applications of various multimodal geospatial tasks. Finally, we conclude with insights, issues, and discussions regarding future research directions. To the best of our knowledge, this is the first comprehensive literature review of VLGFMs. We keep tracing related works at https://github.com/zytx121/Awesome-VLGFM.","sentences":["Vision-Language Foundation Models (VLFMs) have made remarkable progress on various multimodal tasks, such as image captioning, image-text retrieval, visual question answering, and visual grounding.","However, most methods rely on training with general image datasets, and the lack of geospatial data leads to poor performance on earth observation.","Numerous geospatial image-text pair datasets and VLFMs fine-tuned on them have been proposed recently.","These new approaches aim to leverage large-scale, multimodal geospatial data to build versatile intelligent models with diverse geo-perceptive capabilities, which we refer to as Vision-Language Geo-Foundation Models (VLGFMs).","This paper thoroughly reviews VLGFMs, summarizing and analyzing recent developments in the field.","In particular, we introduce the background and motivation behind the rise of VLGFMs, highlighting their unique research significance.","Then, we systematically summarize the core technologies employed in VLGFMs, including data construction, model architectures, and applications of various multimodal geospatial tasks.","Finally, we conclude with insights, issues, and discussions regarding future research directions.","To the best of our knowledge, this is the first comprehensive literature review of VLGFMs.","We keep tracing related works at https://github.com/zytx121/Awesome-VLGFM."],"url":"http://arxiv.org/abs/2406.09385v1","category":"cs.CV"}
{"created":"2024-06-13 17:56:47","title":"Electronic Excitations in the Bulk of Fractional Quantum Hall States","abstract":"We analyze electronic excitations (excitations generated by adding or removing one electron) in the bulk of fractional quantum Hall states in Jain sequence states, using composite fermion Chern-Simons field theory. Starting from meanfield approximation in which gauge field fluctuations are neglected, we use symmetry to constrain the possible composite fermion states contributing to electronic Green's function and expect discrete infinitely-sharp peaks in the electronic spectral function. We further consider the electronic excitations in particle-hole conjugate fractional quantum hall states. Gauge field fluctuations play an increasingly important role in the electron spectral function as the filling factor approaches 1/2, and evolve the discrete coherent peaks into a broad continuum even in the absence of impurities. At that limit, we switch to the electron perspective and calculate the electron spectral function via linked cluster approximation from the low to intermediate energy range. Finally, we compare our results with recent experiments.","sentences":["We analyze electronic excitations (excitations generated by adding or removing one electron) in the bulk of fractional quantum Hall states in Jain sequence states, using composite fermion Chern-Simons field theory.","Starting from meanfield approximation in which gauge field fluctuations are neglected, we use symmetry to constrain the possible composite fermion states contributing to electronic Green's function and expect discrete infinitely-sharp peaks in the electronic spectral function.","We further consider the electronic excitations in particle-hole conjugate fractional quantum hall states.","Gauge field fluctuations play an increasingly important role in the electron spectral function as the filling factor approaches 1/2, and evolve the discrete coherent peaks into a broad continuum even in the absence of impurities.","At that limit, we switch to the electron perspective and calculate the electron spectral function via linked cluster approximation from the low to intermediate energy range.","Finally, we compare our results with recent experiments."],"url":"http://arxiv.org/abs/2406.09382v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-13 17:55:05","title":"The Stability of the BAO Linear Point under Modified Gravity","abstract":"Baryon Acoustic Oscillations (BAOs) are crucial in cosmological analysis, providing a standard ruler, as well as constraints on dark energy. In General Relativity models, the BAO Linear Point - the midpoint between the dip and the peak in the correlation function - has been shown to be rather robust to evolution and redshift space distortions. We show that this remains true even when the gravity model is not General Relativity, at least for $f(R)$ and DGP gravity models which have the same expansion history as the standard $\\Lambda$CDM. For the Linear Point to be able to distinguish between modified gravity (MG) and $\\Lambda$CDM, survey volumes of order tens of cubic Gpc are required.","sentences":["Baryon Acoustic Oscillations (BAOs) are crucial in cosmological analysis, providing a standard ruler, as well as constraints on dark energy.","In General Relativity models, the BAO Linear Point - the midpoint between the dip and the peak in the correlation function - has been shown to be rather robust to evolution and redshift space distortions.","We show that this remains true even when the gravity model is not General Relativity, at least for $f(R)$ and DGP gravity models which have the same expansion history as the standard $\\Lambda$CDM.","For the Linear Point to be able to distinguish between modified gravity (MG) and $\\Lambda$CDM, survey volumes of order tens of cubic Gpc are required."],"url":"http://arxiv.org/abs/2406.09379v1","category":"astro-ph.CO"}
{"created":"2024-06-13 17:54:38","title":"GGHead: Fast and Generalizable 3D Gaussian Heads","abstract":"Learning 3D head priors from large 2D image collections is an important step towards high-quality 3D-aware human modeling. A core requirement is an efficient architecture that scales well to large-scale datasets and large image resolutions. Unfortunately, existing 3D GANs struggle to scale to generate samples at high resolutions due to their relatively slow train and render speeds, and typically have to rely on 2D superresolution networks at the expense of global 3D consistency. To address these challenges, we propose Generative Gaussian Heads (GGHead), which adopts the recent 3D Gaussian Splatting representation within a 3D GAN framework. To generate a 3D representation, we employ a powerful 2D CNN generator to predict Gaussian attributes in the UV space of a template head mesh. This way, GGHead exploits the regularity of the template's UV layout, substantially facilitating the challenging task of predicting an unstructured set of 3D Gaussians. We further improve the geometric fidelity of the generated 3D representations with a novel total variation loss on rendered UV coordinates. Intuitively, this regularization encourages that neighboring rendered pixels should stem from neighboring Gaussians in the template's UV space. Taken together, our pipeline can efficiently generate 3D heads trained only from single-view 2D image observations. Our proposed framework matches the quality of existing 3D head GANs on FFHQ while being both substantially faster and fully 3D consistent. As a result, we demonstrate real-time generation and rendering of high-quality 3D-consistent heads at $1024^2$ resolution for the first time.","sentences":["Learning 3D head priors from large 2D image collections is an important step towards high-quality 3D-aware human modeling.","A core requirement is an efficient architecture that scales well to large-scale datasets and large image resolutions.","Unfortunately, existing 3D GANs struggle to scale to generate samples at high resolutions due to their relatively slow train and render speeds, and typically have to rely on 2D superresolution networks at the expense of global 3D consistency.","To address these challenges, we propose Generative Gaussian Heads (GGHead), which adopts the recent 3D Gaussian Splatting representation within a 3D GAN framework.","To generate a 3D representation, we employ a powerful 2D CNN generator to predict Gaussian attributes in the UV space of a template head mesh.","This way, GGHead exploits the regularity of the template's UV layout, substantially facilitating the challenging task of predicting an unstructured set of 3D Gaussians.","We further improve the geometric fidelity of the generated 3D representations with a novel total variation loss on rendered UV coordinates.","Intuitively, this regularization encourages that neighboring rendered pixels should stem from neighboring Gaussians in the template's UV space.","Taken together, our pipeline can efficiently generate 3D heads trained only from single-view 2D image observations.","Our proposed framework matches the quality of existing 3D head GANs on FFHQ while being both substantially faster and fully 3D consistent.","As a result, we demonstrate real-time generation and rendering of high-quality 3D-consistent heads at $1024^2$ resolution for the first time."],"url":"http://arxiv.org/abs/2406.09377v1","category":"cs.CV"}
{"created":"2024-06-13 17:54:08","title":"Visibility domains relative to the Kobayashi distance in complex manifolds","abstract":"In this paper, we extend the notion of visibility relative to the Kobayashi distance to domains in arbitrary complex manifolds. Visibility here refers to a property resembling visibility in the sense of Eberlein--O'Neill for Riemannian manifolds. Since it is difficult, in general, to determine whether domains are Cauchy-complete with respect to the Kobayashi distance, we do not assume so here. We provide many sufficient conditions for visibility. We establish a Wolff--Denjoy-type theorem in a very general setting as an application. We also explore some connections between visibility and Gromov hyperbolicity for Kobayashi hyperbolic domains in the above setting.","sentences":["In this paper, we extend the notion of visibility relative to the Kobayashi distance to domains in arbitrary complex manifolds.","Visibility here refers to a property resembling visibility in the sense of Eberlein--O'Neill for Riemannian manifolds.","Since it is difficult, in general, to determine whether domains are Cauchy-complete with respect to the Kobayashi distance, we do not assume so here.","We provide many sufficient conditions for visibility.","We establish a Wolff--Denjoy-type theorem in a very general setting as an application.","We also explore some connections between visibility and Gromov hyperbolicity for Kobayashi hyperbolic domains in the above setting."],"url":"http://arxiv.org/abs/2406.09376v1","category":"math.CV"}
{"created":"2024-06-13 17:52:47","title":"Scale-Invariant Monocular Depth Estimation via SSI Depth","abstract":"Existing methods for scale-invariant monocular depth estimation (SI MDE) often struggle due to the complexity of the task, and limited and non-diverse datasets, hindering generalizability in real-world scenarios. This is while shift-and-scale-invariant (SSI) depth estimation, simplifying the task and enabling training with abundant stereo datasets achieves high performance. We present a novel approach that leverages SSI inputs to enhance SI depth estimation, streamlining the network's role and facilitating in-the-wild generalization for SI depth estimation while only using a synthetic dataset for training. Emphasizing the generation of high-resolution details, we introduce a novel sparse ordinal loss that substantially improves detail generation in SSI MDE, addressing critical limitations in existing approaches. Through in-the-wild qualitative examples and zero-shot evaluation we substantiate the practical utility of our approach in computational photography applications, showcasing its ability to generate highly detailed SI depth maps and achieve generalization in diverse scenarios.","sentences":["Existing methods for scale-invariant monocular depth estimation (SI MDE) often struggle due to the complexity of the task, and limited and non-diverse datasets, hindering generalizability in real-world scenarios.","This is while shift-and-scale-invariant (SSI) depth estimation, simplifying the task and enabling training with abundant stereo datasets achieves high performance.","We present a novel approach that leverages SSI inputs to enhance SI depth estimation, streamlining the network's role and facilitating in-the-wild generalization for SI depth estimation while only using a synthetic dataset for training.","Emphasizing the generation of high-resolution details, we introduce a novel sparse ordinal loss that substantially improves detail generation in SSI MDE, addressing critical limitations in existing approaches.","Through in-the-wild qualitative examples and zero-shot evaluation we substantiate the practical utility of our approach in computational photography applications, showcasing its ability to generate highly detailed SI depth maps and achieve generalization in diverse scenarios."],"url":"http://arxiv.org/abs/2406.09374v1","category":"cs.CV"}
{"created":"2024-06-13 17:51:10","title":"Efficient Discrepancy Testing for Learning with Distribution Shift","abstract":"A fundamental notion of distance between train and test distributions from the field of domain adaptation is discrepancy distance. While in general hard to compute, here we provide the first set of provably efficient algorithms for testing localized discrepancy distance, where discrepancy is computed with respect to a fixed output classifier. These results imply a broad set of new, efficient learning algorithms in the recently introduced model of Testable Learning with Distribution Shift (TDS learning) due to Klivans et al. (2023).   Our approach generalizes and improves all prior work on TDS learning: (1) we obtain universal learners that succeed simultaneously for large classes of test distributions, (2) achieve near-optimal error rates, and (3) give exponential improvements for constant depth circuits. Our methods further extend to semi-parametric settings and imply the first positive results for low-dimensional convex sets. Additionally, we separate learning and testing phases and obtain algorithms that run in fully polynomial time at test time.","sentences":["A fundamental notion of distance between train and test distributions from the field of domain adaptation is discrepancy distance.","While in general hard to compute, here we provide the first set of provably efficient algorithms for testing localized discrepancy distance, where discrepancy is computed with respect to a fixed output classifier.","These results imply a broad set of new, efficient learning algorithms in the recently introduced model of Testable Learning with Distribution Shift (TDS learning) due to Klivans et al. (2023).   ","Our approach generalizes and improves all prior work on TDS learning: (1) we obtain universal learners that succeed simultaneously for large classes of test distributions, (2) achieve near-optimal error rates, and (3) give exponential improvements for constant depth circuits.","Our methods further extend to semi-parametric settings and imply the first positive results for low-dimensional convex sets.","Additionally, we separate learning and testing phases and obtain algorithms that run in fully polynomial time at test time."],"url":"http://arxiv.org/abs/2406.09373v1","category":"cs.DS"}
{"created":"2024-06-13 17:50:51","title":"Data-dependent and Oracle Bounds on Forgetting in Continual Learning","abstract":"In continual learning, knowledge must be preserved and re-used between tasks, maintaining good transfer to future tasks and minimizing forgetting of previously learned ones. While several practical algorithms have been devised for this setting, there have been few theoretical works aiming to quantify and bound the degree of Forgetting in general settings. We provide both data-dependent and oracle upper bounds that apply regardless of model and algorithm choice, as well as bounds for Gibbs posteriors. We derive an algorithm inspired by our bounds and demonstrate empirically that our approach yields improved forward and backward transfer.","sentences":["In continual learning, knowledge must be preserved and re-used between tasks, maintaining good transfer to future tasks and minimizing forgetting of previously learned ones.","While several practical algorithms have been devised for this setting, there have been few theoretical works aiming to quantify and bound the degree of Forgetting in general settings.","We provide both data-dependent and oracle upper bounds that apply regardless of model and algorithm choice, as well as bounds for Gibbs posteriors.","We derive an algorithm inspired by our bounds and demonstrate empirically that our approach yields improved forward and backward transfer."],"url":"http://arxiv.org/abs/2406.09370v1","category":"cs.LG"}
{"created":"2024-06-13 17:50:45","title":"Descents in powers of permutations","abstract":"We consider a few special cases of the more general question: How many permutations $\\pi\\in\\mathcal{S}_n$ have the property that $\\pi^2$ has $j$ descents for some $j$? In this paper, we first enumerate Grassmannian permutations $\\pi$ by the number of descents in $\\pi^2$. We then consider all permutations whose square has exactly one descent, fully enumerating when the descent is \"small\" and providing a lower bound in the general case. Finally, we enumerate permutations whose square or cube has the maximum number of descents, and finish the paper with a few future directions for study.","sentences":["We consider a few special cases of the more general question: How many permutations $\\pi\\in\\mathcal{S}_n$ have the property that $\\pi^2$ has $j$ descents for some $j$?","In this paper, we first enumerate Grassmannian permutations $\\pi$ by the number of descents in $\\pi^2$. We then consider all permutations whose square has exactly one descent, fully enumerating when the descent is \"small\" and providing a lower bound in the general case.","Finally, we enumerate permutations whose square or cube has the maximum number of descents, and finish the paper with a few future directions for study."],"url":"http://arxiv.org/abs/2406.09369v1","category":"math.CO"}
{"created":"2024-06-13 17:50:28","title":"CLIPAway: Harmonizing Focused Embeddings for Removing Objects via Diffusion Models","abstract":"Advanced image editing techniques, particularly inpainting, are essential for seamlessly removing unwanted elements while preserving visual integrity. Traditional GAN-based methods have achieved notable success, but recent advancements in diffusion models have produced superior results due to their training on large-scale datasets, enabling the generation of remarkably realistic inpainted images. Despite their strengths, diffusion models often struggle with object removal tasks without explicit guidance, leading to unintended hallucinations of the removed object. To address this issue, we introduce CLIPAway, a novel approach leveraging CLIP embeddings to focus on background regions while excluding foreground elements. CLIPAway enhances inpainting accuracy and quality by identifying embeddings that prioritize the background, thus achieving seamless object removal. Unlike other methods that rely on specialized training datasets or costly manual annotations, CLIPAway provides a flexible, plug-and-play solution compatible with various diffusion-based inpainting techniques.","sentences":["Advanced image editing techniques, particularly inpainting, are essential for seamlessly removing unwanted elements while preserving visual integrity.","Traditional GAN-based methods have achieved notable success, but recent advancements in diffusion models have produced superior results due to their training on large-scale datasets, enabling the generation of remarkably realistic inpainted images.","Despite their strengths, diffusion models often struggle with object removal tasks without explicit guidance, leading to unintended hallucinations of the removed object.","To address this issue, we introduce CLIPAway, a novel approach leveraging CLIP embeddings to focus on background regions while excluding foreground elements.","CLIPAway enhances inpainting accuracy and quality by identifying embeddings that prioritize the background, thus achieving seamless object removal.","Unlike other methods that rely on specialized training datasets or costly manual annotations, CLIPAway provides a flexible, plug-and-play solution compatible with various diffusion-based inpainting techniques."],"url":"http://arxiv.org/abs/2406.09368v1","category":"cs.CV"}
{"created":"2024-06-13 17:50:05","title":"Needle In A Video Haystack: A Scalable Synthetic Framework for Benchmarking Video MLLMs","abstract":"Video understanding is a crucial next step for multimodal large language models (MLLMs). To probe specific aspects of video understanding ability, existing video benchmarks typically require careful video selection based on the target capability, along with laborious annotation of query-response pairs to match the specific video content. This process is both challenging and resource-intensive. In this paper, we propose VideoNIAH (Video Needle In A Haystack), a benchmark construction framework through synthetic video generation. VideoNIAH decouples test video content from their query-responses by inserting unrelated image/text 'needles' into original videos. It generates annotations solely from these needles, ensuring diversity in video sources and a variety of query-responses. Additionally, by inserting multiple needles, VideoNIAH rigorously evaluates the temporal understanding capabilities of models. We utilized VideoNIAH to compile a video benchmark VNBench, including tasks such as retrieval, ordering, and counting. VNBench can efficiently evaluate the fine-grained understanding ability and spatio-temporal modeling ability of a video model, while also supporting the long-context evaluation. Additionally, we evaluated recent video-centric multimodal large language models (MLLMs), both open-source and proprietary, providing a comprehensive analysis. We found that although proprietary models have significant advantages over open-source models, all existing video models still perform poorly on long-distance dependency tasks. VideoNIAH is a simple yet highly scalable benchmark construction framework, and we believe it will inspire future video benchmark works. The code and data are available at https://github.com/joez17/VideoNIAH.","sentences":["Video understanding is a crucial next step for multimodal large language models (MLLMs).","To probe specific aspects of video understanding ability, existing video benchmarks typically require careful video selection based on the target capability, along with laborious annotation of query-response pairs to match the specific video content.","This process is both challenging and resource-intensive.","In this paper, we propose VideoNIAH (Video Needle In A Haystack), a benchmark construction framework through synthetic video generation.","VideoNIAH decouples test video content from their query-responses by inserting unrelated image/text 'needles' into original videos.","It generates annotations solely from these needles, ensuring diversity in video sources and a variety of query-responses.","Additionally, by inserting multiple needles, VideoNIAH rigorously evaluates the temporal understanding capabilities of models.","We utilized VideoNIAH to compile a video benchmark VNBench, including tasks such as retrieval, ordering, and counting.","VNBench can efficiently evaluate the fine-grained understanding ability and spatio-temporal modeling ability of a video model, while also supporting the long-context evaluation.","Additionally, we evaluated recent video-centric multimodal large language models (MLLMs), both open-source and proprietary, providing a comprehensive analysis.","We found that although proprietary models have significant advantages over open-source models, all existing video models still perform poorly on long-distance dependency tasks.","VideoNIAH is a simple yet highly scalable benchmark construction framework, and we believe it will inspire future video benchmark works.","The code and data are available at https://github.com/joez17/VideoNIAH."],"url":"http://arxiv.org/abs/2406.09367v1","category":"cs.CV"}
{"created":"2024-06-13 17:49:10","title":"ElicitationGPT: Text Elicitation Mechanisms via Language Models","abstract":"Scoring rules evaluate probabilistic forecasts of an unknown state against the realized state and are a fundamental building block in the incentivized elicitation of information and the training of machine learning models. This paper develops mechanisms for scoring elicited text against ground truth text using domain-knowledge-free queries to a large language model (specifically ChatGPT) and empirically evaluates their alignment with human preferences. The empirical evaluation is conducted on peer reviews from a peer-grading dataset and in comparison to manual instructor scores for the peer reviews.","sentences":["Scoring rules evaluate probabilistic forecasts of an unknown state against the realized state and are a fundamental building block in the incentivized elicitation of information and the training of machine learning models.","This paper develops mechanisms for scoring elicited text against ground truth text using domain-knowledge-free queries to a large language model (specifically ChatGPT) and empirically evaluates their alignment with human preferences.","The empirical evaluation is conducted on peer reviews from a peer-grading dataset and in comparison to manual instructor scores for the peer reviews."],"url":"http://arxiv.org/abs/2406.09363v1","category":"cs.AI"}
{"created":"2024-06-13 17:46:29","title":"On the independence number of sparser random Cayley graphs","abstract":"The Cayley sum graph $\\Gamma_A$ of a set $A \\subseteq \\mathbb{Z}_n$ is defined to have vertex set $\\mathbb{Z}_n$ and an edge between two distinct vertices $x, y \\in \\mathbb{Z}_n$ if $x + y \\in A$. Green and Morris proved that if the set $A$ is a $p$-random subset of $\\mathbb{Z}_n$ with $p = 1/2$, then the independence number of $\\Gamma_A$ is asymptotically equal to $\\alpha(G(n, 1/2))$ with high probability. Our main theorem is the first extension of their result to $p = o(1)$: we show that, with high probability, $$\\alpha(\\Gamma_A) = (1 + o(1)) \\alpha(G(n, p))$$ as long as $p \\ge (\\log n)^{-1/80}$.   One of the tools in our proof is a geometric-flavoured theorem that generalizes Fre\\u{i}man's lemma, the classical lower bound on the size of high dimensional sumsets. We also give a short proof of this result up to a constant factor; this version yields a much simpler proof of our main theorem at the expense of a worse constant.","sentences":["The Cayley sum graph $\\Gamma_A$ of a set $A \\subseteq \\mathbb{Z}_n$ is defined to have vertex set $\\mathbb{Z}_n$ and an edge between two distinct vertices $x, y \\in \\mathbb{Z}_n$ if $x + y \\in A$.","Green and Morris proved that if the set $A$ is a $p$-random subset of $\\mathbb{Z}_n$ with $p = 1/2$, then the independence number of $\\Gamma_A$ is asymptotically equal to $\\alpha(G(n, 1/2))$ with high probability.","Our main theorem is the first extension of their result to $p = o(1)$: we show that, with high probability, $$\\alpha(\\Gamma_A) = (1 + o(1))","\\alpha(G(n, p))$$ as long as $p \\ge (\\log n)^{-1/80}$.   One of the tools in our proof is a geometric-flavoured theorem that generalizes Fre\\u{i}man's lemma, the classical lower bound on the size of high dimensional sumsets.","We also give a short proof of this result up to a constant factor; this version yields a much simpler proof of our main theorem at the expense of a worse constant."],"url":"http://arxiv.org/abs/2406.09361v1","category":"math.CO"}
{"created":"2024-06-13 17:43:41","title":"Understanding Hallucinations in Diffusion Models through Mode Interpolation","abstract":"Colloquially speaking, image generation models based upon diffusion processes are frequently said to exhibit \"hallucinations,\" samples that could never occur in the training data. But where do such hallucinations come from? In this paper, we study a particular failure mode in diffusion models, which we term mode interpolation. Specifically, we find that diffusion models smoothly \"interpolate\" between nearby data modes in the training set, to generate samples that are completely outside the support of the original training distribution; this phenomenon leads diffusion models to generate artifacts that never existed in real data (i.e., hallucinations). We systematically study the reasons for, and the manifestation of this phenomenon. Through experiments on 1D and 2D Gaussians, we show how a discontinuous loss landscape in the diffusion model's decoder leads to a region where any smooth approximation will cause such hallucinations. Through experiments on artificial datasets with various shapes, we show how hallucination leads to the generation of combinations of shapes that never existed. Finally, we show that diffusion models in fact know when they go out of support and hallucinate. This is captured by the high variance in the trajectory of the generated sample towards the final few backward sampling process. Using a simple metric to capture this variance, we can remove over 95% of hallucinations at generation time while retaining 96% of in-support samples. We conclude our exploration by showing the implications of such hallucination (and its removal) on the collapse (and stabilization) of recursive training on synthetic data with experiments on MNIST and 2D Gaussians dataset. We release our code at https://github.com/locuslab/diffusion-model-hallucination.","sentences":["Colloquially speaking, image generation models based upon diffusion processes are frequently said to exhibit \"hallucinations,\" samples that could never occur in the training data.","But where do such hallucinations come from?","In this paper, we study a particular failure mode in diffusion models, which we term mode interpolation.","Specifically, we find that diffusion models smoothly \"interpolate\" between nearby data modes in the training set, to generate samples that are completely outside the support of the original training distribution; this phenomenon leads diffusion models to generate artifacts that never existed in real data (i.e., hallucinations).","We systematically study the reasons for, and the manifestation of this phenomenon.","Through experiments on 1D and 2D Gaussians, we show how a discontinuous loss landscape in the diffusion model's decoder leads to a region where any smooth approximation will cause such hallucinations.","Through experiments on artificial datasets with various shapes, we show how hallucination leads to the generation of combinations of shapes that never existed.","Finally, we show that diffusion models in fact know when they go out of support and hallucinate.","This is captured by the high variance in the trajectory of the generated sample towards the final few backward sampling process.","Using a simple metric to capture this variance, we can remove over 95% of hallucinations at generation time while retaining 96% of in-support samples.","We conclude our exploration by showing the implications of such hallucination (and its removal) on the collapse (and stabilization) of recursive training on synthetic data with experiments on MNIST and 2D Gaussians dataset.","We release our code at https://github.com/locuslab/diffusion-model-hallucination."],"url":"http://arxiv.org/abs/2406.09358v1","category":"cs.LG"}
{"created":"2024-06-13 17:42:57","title":"Advancing Graph Generation through Beta Diffusion","abstract":"Diffusion models have demonstrated effectiveness in generating natural images and have been extended to generate diverse data types, including graphs. This new generation of diffusion-based graph generative models has demonstrated significant performance improvements over methods that rely on variational autoencoders or generative adversarial networks. It's important to recognize, however, that most of these models employ Gaussian or categorical diffusion processes, which can struggle with sparse and long-tailed data distributions. In our work, we introduce Graph Beta Diffusion (GBD), a diffusion-based generative model particularly adept at capturing diverse graph structures. GBD utilizes a beta diffusion process, tailored for the sparse and range-bounded characteristics of graph adjacency matrices. Furthermore, we have developed a modulation technique that enhances the realism of the generated graphs by stabilizing the generation of critical graph structures, while preserving flexibility elsewhere. The outstanding performance of GBD across three general graph benchmarks and two biochemical graph benchmarks highlights its capability to effectively capture the complexities of real-world graph data. The code will be made available at https://github.com/YH-UtMSB/Graph_Beta_Diffusion","sentences":["Diffusion models have demonstrated effectiveness in generating natural images and have been extended to generate diverse data types, including graphs.","This new generation of diffusion-based graph generative models has demonstrated significant performance improvements over methods that rely on variational autoencoders or generative adversarial networks.","It's important to recognize, however, that most of these models employ Gaussian or categorical diffusion processes, which can struggle with sparse and long-tailed data distributions.","In our work, we introduce Graph Beta Diffusion (GBD), a diffusion-based generative model particularly adept at capturing diverse graph structures.","GBD utilizes a beta diffusion process, tailored for the sparse and range-bounded characteristics of graph adjacency matrices.","Furthermore, we have developed a modulation technique that enhances the realism of the generated graphs by stabilizing the generation of critical graph structures, while preserving flexibility elsewhere.","The outstanding performance of GBD across three general graph benchmarks and two biochemical graph benchmarks highlights its capability to effectively capture the complexities of real-world graph data.","The code will be made available at https://github.com/YH-UtMSB/Graph_Beta_Diffusion"],"url":"http://arxiv.org/abs/2406.09357v1","category":"cs.LG"}
{"created":"2024-06-13 17:40:56","title":"Can't Hide Behind the API: Stealing Black-Box Commercial Embedding Models","abstract":"Embedding models that generate representation vectors from natural language text are widely used, reflect substantial investments, and carry significant commercial value. Companies such as OpenAI and Cohere have developed competing embedding models accessed through APIs that require users to pay for usage. In this architecture, the models are \"hidden\" behind APIs, but this does not mean that they are \"well guarded\". We present, to our knowledge, the first effort to \"steal\" these models for retrieval by training local models on text-embedding pairs obtained from the commercial APIs. Our experiments show using standard benchmarks that it is possible to efficiently replicate the retrieval effectiveness of the commercial embedding models using an attack that costs only around $200 to train (presumably) smaller models with fewer dimensions. Our findings raise important considerations for deploying commercial embedding models and suggest measures to mitigate the risk of model theft.","sentences":["Embedding models that generate representation vectors from natural language text are widely used, reflect substantial investments, and carry significant commercial value.","Companies such as OpenAI and Cohere have developed competing embedding models accessed through APIs that require users to pay for usage.","In this architecture, the models are \"hidden\" behind APIs, but this does not mean that they are \"well guarded\".","We present, to our knowledge, the first effort to \"steal\" these models for retrieval by training local models on text-embedding pairs obtained from the commercial APIs.","Our experiments show using standard benchmarks that it is possible to efficiently replicate the retrieval effectiveness of the commercial embedding models using an attack that costs only around $200 to train (presumably) smaller models with fewer dimensions.","Our findings raise important considerations for deploying commercial embedding models and suggest measures to mitigate the risk of model theft."],"url":"http://arxiv.org/abs/2406.09355v1","category":"cs.IR"}
{"created":"2024-06-13 17:31:02","title":"Scoreformer: A Surrogate Model For Large-Scale Prediction of Docking Scores","abstract":"In this study, we present ScoreFormer, a novel graph transformer model designed to accurately predict molecular docking scores, thereby optimizing high-throughput virtual screening (HTVS) in drug discovery. The architecture integrates Principal Neighborhood Aggregation (PNA) and Learnable Random Walk Positional Encodings (LRWPE), enhancing the model's ability to understand complex molecular structures and their relationship with their respective docking scores. This approach significantly surpasses traditional HTVS methods and recent Graph Neural Network (GNN) models in both recovery and efficiency due to a wider coverage of the chemical space and enhanced performance. Our results demonstrate that ScoreFormer achieves competitive performance in docking score prediction and offers a substantial 1.65-fold reduction in inference time compared to existing models. We evaluated ScoreFormer across multiple datasets under various conditions, confirming its robustness and reliability in identifying potential drug candidates rapidly.","sentences":["In this study, we present ScoreFormer, a novel graph transformer model designed to accurately predict molecular docking scores, thereby optimizing high-throughput virtual screening (HTVS) in drug discovery.","The architecture integrates Principal Neighborhood Aggregation (PNA) and Learnable Random Walk Positional Encodings (LRWPE), enhancing the model's ability to understand complex molecular structures and their relationship with their respective docking scores.","This approach significantly surpasses traditional HTVS methods and recent Graph Neural Network (GNN) models in both recovery and efficiency due to a wider coverage of the chemical space and enhanced performance.","Our results demonstrate that ScoreFormer achieves competitive performance in docking score prediction and offers a substantial 1.65-fold reduction in inference time compared to existing models.","We evaluated ScoreFormer across multiple datasets under various conditions, confirming its robustness and reliability in identifying potential drug candidates rapidly."],"url":"http://arxiv.org/abs/2406.09346v1","category":"cs.LG"}
{"created":"2024-06-13 17:28:13","title":"DiscreteSLU: A Large Language Model with Self-Supervised Discrete Speech Units for Spoken Language Understanding","abstract":"The integration of pre-trained text-based large language models (LLM) with speech input has enabled instruction-following capabilities for diverse speech tasks. This integration requires the use of a speech encoder, a speech adapter, and an LLM, trained on diverse tasks. We propose the use of discrete speech units (DSU), rather than continuous-valued speech encoder outputs, that are converted to the LLM token embedding space using the speech adapter. We generate DSU using a self-supervised speech encoder followed by k-means clustering. The proposed model shows robust performance on speech inputs from seen/unseen domains and instruction-following capability in spoken question answering. We also explore various types of DSU extracted from different layers of the self-supervised speech encoder, as well as Mel frequency Cepstral Coefficients (MFCC). Our findings suggest that the ASR task and datasets are not crucial in instruction-tuning for spoken question answering tasks.","sentences":["The integration of pre-trained text-based large language models (LLM) with speech input has enabled instruction-following capabilities for diverse speech tasks.","This integration requires the use of a speech encoder, a speech adapter, and an LLM, trained on diverse tasks.","We propose the use of discrete speech units (DSU), rather than continuous-valued speech encoder outputs, that are converted to the LLM token embedding space using the speech adapter.","We generate DSU using a self-supervised speech encoder followed by k-means clustering.","The proposed model shows robust performance on speech inputs from seen/unseen domains and instruction-following capability in spoken question answering.","We also explore various types of DSU extracted from different layers of the self-supervised speech encoder, as well as Mel frequency Cepstral Coefficients (MFCC).","Our findings suggest that the ASR task and datasets are not crucial in instruction-tuning for spoken question answering tasks."],"url":"http://arxiv.org/abs/2406.09345v1","category":"cs.CL"}
{"created":"2024-06-13 17:21:43","title":"Investigate the Performance of Distribution Loading with Conditional Quantum Generative Adversarial Network Algorithm on Quantum Hardware with Error Suppression","abstract":"The study examines the efficacy of the Fire Opal error suppression and AI circuit optimization system integrated with IBM's quantum computing platform for a multi-modal distribution loading algorithm. Using Kullback-Leibler (KL) divergence as a quantitative error analysis, the results indicate that Fire Opal can improve on the time-dependent distributions generated by our Conditional Quantum Generative Adversarial algorithm by 30-40\\% in comparison with the results on the simulator. In addition, Fire Opal's performance remains consistent for complex circuits despite the needs to run more trials. The research concludes that Fire Opal's error suppression and circuit optimization significantly enhanced quantum computing processes, highlighting its potential for practical applications. In addition, the study also reviews leading error mitigation strategies, including zero noise extrapolation (ZNE), probabilistic error cancellation (PEC), Pauli twirling, measurement error mitigation, and machine learning methods, assessing their advantages and disadvantages in terms of technical implementation, quantum resources, and scalability.","sentences":["The study examines the efficacy of the Fire Opal error suppression and AI circuit optimization system integrated with IBM's quantum computing platform for a multi-modal distribution loading algorithm.","Using Kullback-Leibler (KL) divergence as a quantitative error analysis, the results indicate that Fire Opal can improve on the time-dependent distributions generated by our Conditional Quantum Generative Adversarial algorithm by 30-40\\% in comparison with the results on the simulator.","In addition, Fire Opal's performance remains consistent for complex circuits despite the needs to run more trials.","The research concludes that Fire Opal's error suppression and circuit optimization significantly enhanced quantum computing processes, highlighting its potential for practical applications.","In addition, the study also reviews leading error mitigation strategies, including zero noise extrapolation (ZNE), probabilistic error cancellation (PEC), Pauli twirling, measurement error mitigation, and machine learning methods, assessing their advantages and disadvantages in terms of technical implementation, quantum resources, and scalability."],"url":"http://arxiv.org/abs/2406.09341v1","category":"quant-ph"}
{"created":"2024-06-13 17:20:24","title":"Wigner function method for the Gibbons-Hawking and the Unruh effect","abstract":"An observer at rest with the expanding universe experiences some extra noise in the quantum vacuum, and so does an accelerated observer in a vacuum at rest (in Minkowski space). The literature mainly focuses on the ideal cases of exponential expansion (de-Sitter space) or uniform acceleration (Rindler trajectories) or both, but the real cosmic expansion is non-exponential and real accelerations are non-uniform. Here we use the frequency-time Wigner function of vacuum correlations to define time-dependent spectra. We found excellent Planck spectra for a class of realistic cosmological models, but also strongly non-Planckian, negative Wigner functions for a standard scenario testable with laboratory analogues.","sentences":["An observer at rest with the expanding universe experiences some extra noise in the quantum vacuum, and so does an accelerated observer in a vacuum at rest (in Minkowski space).","The literature mainly focuses on the ideal cases of exponential expansion (de-Sitter space) or uniform acceleration (Rindler trajectories) or both, but the real cosmic expansion is non-exponential and real accelerations are non-uniform.","Here we use the frequency-time Wigner function of vacuum correlations to define time-dependent spectra.","We found excellent Planck spectra for a class of realistic cosmological models, but also strongly non-Planckian, negative Wigner functions for a standard scenario testable with laboratory analogues."],"url":"http://arxiv.org/abs/2406.09339v1","category":"gr-qc"}
{"created":"2024-06-13 17:17:31","title":"Instance-level quantitative saliency in multiple sclerosis lesion segmentation","abstract":"In recent years, explainable methods for artificial intelligence (XAI) have tried to reveal and describe models' decision mechanisms in the case of classification tasks. However, XAI for semantic segmentation and in particular for single instances has been little studied to date. Understanding the process underlying automatic segmentation of single instances is crucial to reveal what information was used to detect and segment a given object of interest. In this study, we proposed two instance-level explanation maps for semantic segmentation based on SmoothGrad and Grad-CAM++ methods. Then, we investigated their relevance for the detection and segmentation of white matter lesions (WML), a magnetic resonance imaging (MRI) biomarker in multiple sclerosis (MS). 687 patients diagnosed with MS for a total of 4043 FLAIR and MPRAGE MRI scans were collected at the University Hospital of Basel, Switzerland. Data were randomly split into training, validation and test sets to train a 3D U-Net for MS lesion segmentation. We observed 3050 true positive (TP), 1818 false positive (FP), and 789 false negative (FN) cases. We generated instance-level explanation maps for semantic segmentation, by developing two XAI methods based on SmoothGrad and Grad-CAM++. We investigated: 1) the distribution of gradients in saliency maps with respect to both input MRI sequences; 2) the model's response in the case of synthetic lesions; 3) the amount of perilesional tissue needed by the model to segment a lesion. Saliency maps (based on SmoothGrad) in FLAIR showed positive values inside a lesion and negative in its neighborhood. Peak values of saliency maps generated for these four groups of volumes presented distributions that differ significantly from one another, suggesting a quantitative nature of the proposed saliency. Contextual information of 7mm around the lesion border was required for their segmentation.","sentences":["In recent years, explainable methods for artificial intelligence (XAI) have tried to reveal and describe models' decision mechanisms in the case of classification tasks.","However, XAI for semantic segmentation and in particular for single instances has been little studied to date.","Understanding the process underlying automatic segmentation of single instances is crucial to reveal what information was used to detect and segment a given object of interest.","In this study, we proposed two instance-level explanation maps for semantic segmentation based on SmoothGrad and Grad-CAM++ methods.","Then, we investigated their relevance for the detection and segmentation of white matter lesions (WML), a magnetic resonance imaging (MRI) biomarker in multiple sclerosis (MS).","687 patients diagnosed with MS for a total of 4043 FLAIR and MPRAGE MRI scans were collected at the University Hospital of Basel, Switzerland.","Data were randomly split into training, validation and test sets to train a 3D U-Net for MS lesion segmentation.","We observed 3050 true positive (TP), 1818 false positive (FP), and 789 false negative (FN) cases.","We generated instance-level explanation maps for semantic segmentation, by developing two XAI methods based on SmoothGrad and Grad-CAM++.","We investigated: 1) the distribution of gradients in saliency maps with respect to both input MRI sequences; 2) the model's response in the case of synthetic lesions; 3) the amount of perilesional tissue needed by the model to segment a lesion.","Saliency maps (based on SmoothGrad) in FLAIR showed positive values inside a lesion and negative in its neighborhood.","Peak values of saliency maps generated for these four groups of volumes presented distributions that differ significantly from one another, suggesting a quantitative nature of the proposed saliency.","Contextual information of 7mm around the lesion border was required for their segmentation."],"url":"http://arxiv.org/abs/2406.09335v1","category":"eess.IV"}
{"created":"2024-06-13 17:09:07","title":"Topological isotopy and finite type invariants","abstract":"In 1974, D. Rolfsen asked: If two PL links in $S^3$ are isotopic (=homotopic through embeddings), then are they PL isotopic? We prove that they are PL isotopic to another pair of links which are indistinguishable from each other by finite type invariants. Thus if finite type invariants separate PL links in $S^3$, then Rolfsen's problem has an affirmative solution. In fact, we show that finite type invariants separate PL links in $S^3$ if and only if certain 6 rather diverse conjectures hold simultaneously.   We also show that if $v$ is a finite type invariant (or more generally a colored finite type invariant) of PL links, and $v$ is invariant under PL isotopy, then $v$ assumes the same value on all sufficiently close $C^0$-approximations of any given topological link; moreover, the extension of $v$ by continuity to topological links is an invariant of isotopy. Some specific invariants of this kind are discussed.","sentences":["In 1974, D. Rolfsen asked: If two PL links in $S^3$ are isotopic (=homotopic through embeddings), then are they PL isotopic?","We prove that they are PL isotopic to another pair of links which are indistinguishable from each other by finite type invariants.","Thus if finite type invariants separate PL links in $S^3$, then Rolfsen's problem has an affirmative solution.","In fact, we show that finite type invariants separate PL links in $S^3$ if and only if certain 6 rather diverse conjectures hold simultaneously.   ","We also show that if $v$ is a finite type invariant (or more generally a colored finite type invariant) of PL links, and $v$ is invariant under PL isotopy, then $v$ assumes the same value on all sufficiently close $C^0$-approximations of any given topological link; moreover, the extension of $v$ by continuity to topological links is an invariant of isotopy.","Some specific invariants of this kind are discussed."],"url":"http://arxiv.org/abs/2406.09331v1","category":"math.GT"}
{"created":"2024-06-13 17:08:58","title":"Learning from Natural Language Explanations for Generalizable Entity Matching","abstract":"Entity matching is the task of linking records from different sources that refer to the same real-world entity. Past work has primarily treated entity linking as a standard supervised learning problem. However, supervised entity matching models often do not generalize well to new data, and collecting exhaustive labeled training data is often cost prohibitive. Further, recent efforts have adopted LLMs for this task in few/zero-shot settings, exploiting their general knowledge. But LLMs are prohibitively expensive for performing inference at scale for real-world entity matching tasks.   As an efficient alternative, we re-cast entity matching as a conditional generation task as opposed to binary classification. This enables us to \"distill\" LLM reasoning into smaller entity matching models via natural language explanations. This approach achieves strong performance, especially on out-of-domain generalization tests (10.85% F-1) where standalone generative methods struggle. We perform ablations that highlight the importance of explanations, both for performance and model robustness.","sentences":["Entity matching is the task of linking records from different sources that refer to the same real-world entity.","Past work has primarily treated entity linking as a standard supervised learning problem.","However, supervised entity matching models often do not generalize well to new data, and collecting exhaustive labeled training data is often cost prohibitive.","Further, recent efforts have adopted LLMs for this task in few/zero-shot settings, exploiting their general knowledge.","But LLMs are prohibitively expensive for performing inference at scale for real-world entity matching tasks.   ","As an efficient alternative, we re-cast entity matching as a conditional generation task as opposed to binary classification.","This enables us to \"distill\" LLM reasoning into smaller entity matching models via natural language explanations.","This approach achieves strong performance, especially on out-of-domain generalization tests (10.85% F-1) where standalone generative methods struggle.","We perform ablations that highlight the importance of explanations, both for performance and model robustness."],"url":"http://arxiv.org/abs/2406.09330v1","category":"cs.CL"}
{"created":"2024-06-13 17:07:49","title":"Is Value Learning Really the Main Bottleneck in Offline RL?","abstract":"While imitation learning requires access to high-quality data, offline reinforcement learning (RL) should, in principle, perform similarly or better with substantially lower data quality by using a value function. However, current results indicate that offline RL often performs worse than imitation learning, and it is often unclear what holds back the performance of offline RL. Motivated by this observation, we aim to understand the bottlenecks in current offline RL algorithms. While poor performance of offline RL is typically attributed to an imperfect value function, we ask: is the main bottleneck of offline RL indeed in learning the value function, or something else? To answer this question, we perform a systematic empirical study of (1) value learning, (2) policy extraction, and (3) policy generalization in offline RL problems, analyzing how these components affect performance. We make two surprising observations. First, we find that the choice of a policy extraction algorithm significantly affects the performance and scalability of offline RL, often more so than the value learning objective. For instance, we show that common value-weighted behavioral cloning objectives (e.g., AWR) do not fully leverage the learned value function, and switching to behavior-constrained policy gradient objectives (e.g., DDPG+BC) often leads to substantial improvements in performance and scalability. Second, we find that a big barrier to improving offline RL performance is often imperfect policy generalization on test-time states out of the support of the training data, rather than policy learning on in-distribution states. We then show that the use of suboptimal but high-coverage data or test-time policy training techniques can address this generalization issue in practice. Specifically, we propose two simple test-time policy improvement methods and show that these methods lead to better performance.","sentences":["While imitation learning requires access to high-quality data, offline reinforcement learning (RL) should, in principle, perform similarly or better with substantially lower data quality by using a value function.","However, current results indicate that offline RL often performs worse than imitation learning, and it is often unclear what holds back the performance of offline RL.","Motivated by this observation, we aim to understand the bottlenecks in current offline RL algorithms.","While poor performance of offline RL is typically attributed to an imperfect value function, we ask: is the main bottleneck of offline RL indeed in learning the value function, or something else?","To answer this question, we perform a systematic empirical study of (1) value learning, (2) policy extraction, and (3) policy generalization in offline RL problems, analyzing how these components affect performance.","We make two surprising observations.","First, we find that the choice of a policy extraction algorithm significantly affects the performance and scalability of offline RL, often more so than the value learning objective.","For instance, we show that common value-weighted behavioral cloning objectives (e.g., AWR) do not fully leverage the learned value function, and switching to behavior-constrained policy gradient objectives (e.g., DDPG+BC) often leads to substantial improvements in performance and scalability.","Second, we find that a big barrier to improving offline RL performance is often imperfect policy generalization on test-time states out of the support of the training data, rather than policy learning on in-distribution states.","We then show that the use of suboptimal but high-coverage data or test-time policy training techniques can address this generalization issue in practice.","Specifically, we propose two simple test-time policy improvement methods and show that these methods lead to better performance."],"url":"http://arxiv.org/abs/2406.09329v1","category":"cs.LG"}
{"created":"2024-06-13 17:07:05","title":"Learnable Fractal Flames","abstract":"This work presents a differentiable rendering approach that allows latent fractal flame parameters to be learned from image supervision. The approach extends the state-of-the-art in differentiable fractal rendering through support for color images, non-linear generator functions, and multi-fractal compositions. With these additions, differentiable rendering is now a viable tool for the generation of fractal artwork.","sentences":["This work presents a differentiable rendering approach that allows latent fractal flame parameters to be learned from image supervision.","The approach extends the state-of-the-art in differentiable fractal rendering through support for color images, non-linear generator functions, and multi-fractal compositions.","With these additions, differentiable rendering is now a viable tool for the generation of fractal artwork."],"url":"http://arxiv.org/abs/2406.09328v1","category":"cs.GR"}
{"created":"2024-06-13 17:05:23","title":"PianoMotion10M: Dataset and Benchmark for Hand Motion Generation in Piano Performance","abstract":"Recently, artificial intelligence techniques for education have been received increasing attentions, while it still remains an open problem to design the effective music instrument instructing systems. Although key presses can be directly derived from sheet music, the transitional movements among key presses require more extensive guidance in piano performance. In this work, we construct a piano-hand motion generation benchmark to guide hand movements and fingerings for piano playing. To this end, we collect an annotated dataset, PianoMotion10M, consisting of 116 hours of piano playing videos from a bird's-eye view with 10 million annotated hand poses. We also introduce a powerful baseline model that generates hand motions from piano audios through a position predictor and a position-guided gesture generator. Furthermore, a series of evaluation metrics are designed to assess the performance of the baseline model, including motion similarity, smoothness, positional accuracy of left and right hands, and overall fidelity of movement distribution. Despite that piano key presses with respect to music scores or audios are already accessible, PianoMotion10M aims to provide guidance on piano fingering for instruction purposes. The dataset and source code can be accessed at https://agnjason.github.io/PianoMotion-page.","sentences":["Recently, artificial intelligence techniques for education have been received increasing attentions, while it still remains an open problem to design the effective music instrument instructing systems.","Although key presses can be directly derived from sheet music, the transitional movements among key presses require more extensive guidance in piano performance.","In this work, we construct a piano-hand motion generation benchmark to guide hand movements and fingerings for piano playing.","To this end, we collect an annotated dataset, PianoMotion10M, consisting of 116 hours of piano playing videos from a bird's-eye view with 10 million annotated hand poses.","We also introduce a powerful baseline model that generates hand motions from piano audios through a position predictor and a position-guided gesture generator.","Furthermore, a series of evaluation metrics are designed to assess the performance of the baseline model, including motion similarity, smoothness, positional accuracy of left and right hands, and overall fidelity of movement distribution.","Despite that piano key presses with respect to music scores or audios are already accessible, PianoMotion10M aims to provide guidance on piano fingering for instruction purposes.","The dataset and source code can be accessed at https://agnjason.github.io/PianoMotion-page."],"url":"http://arxiv.org/abs/2406.09326v1","category":"cs.SD"}
{"created":"2024-06-13 17:02:32","title":"REVS: Unlearning Sensitive Information in Language Models via Rank Editing in the Vocabulary Space","abstract":"Large language models (LLMs) risk inadvertently memorizing and divulging sensitive or personally identifiable information (PII) seen in training data, causing privacy concerns. Current approaches to address this issue involve costly dataset scrubbing, or model filtering through unlearning and model editing, which can be bypassed through extraction attacks. We propose REVS, a novel model editing method for unlearning sensitive information from LLMs. REVS identifies and modifies a small subset of neurons relevant for each piece of sensitive information. By projecting these neurons to the vocabulary space (unembedding), we pinpoint the components driving its generation. We then compute a model edit based on the pseudo-inverse of the unembedding matrix, and apply it to de-promote generation of the targeted sensitive data. To adequately evaluate our method on truly sensitive information, we curate two datasets: an email dataset inherently memorized by GPT-J, and a synthetic social security number dataset that we tune the model to memorize. Compared to other state-of-the-art model editing methods, REVS demonstrates superior performance in both eliminating sensitive information and robustness to extraction attacks, while retaining integrity of the underlying model. The code and a demo notebook are available at https://technion-cs-nlp.github.io/REVS.","sentences":["Large language models (LLMs) risk inadvertently memorizing and divulging sensitive or personally identifiable information (PII) seen in training data, causing privacy concerns.","Current approaches to address this issue involve costly dataset scrubbing, or model filtering through unlearning and model editing, which can be bypassed through extraction attacks.","We propose REVS, a novel model editing method for unlearning sensitive information from LLMs.","REVS identifies and modifies a small subset of neurons relevant for each piece of sensitive information.","By projecting these neurons to the vocabulary space (unembedding), we pinpoint the components driving its generation.","We then compute a model edit based on the pseudo-inverse of the unembedding matrix, and apply it to de-promote generation of the targeted sensitive data.","To adequately evaluate our method on truly sensitive information, we curate two datasets: an email dataset inherently memorized by GPT-J, and a synthetic social security number dataset that we tune the model to memorize.","Compared to other state-of-the-art model editing methods, REVS demonstrates superior performance in both eliminating sensitive information and robustness to extraction attacks, while retaining integrity of the underlying model.","The code and a demo notebook are available at https://technion-cs-nlp.github.io/REVS."],"url":"http://arxiv.org/abs/2406.09325v1","category":"cs.CL"}
{"created":"2024-06-13 17:01:40","title":"Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs","abstract":"Although Large Language Models (LLMs) have demonstrated significant capabilities in executing complex tasks in a zero-shot manner, they are susceptible to jailbreak attacks and can be manipulated to produce harmful outputs. Recently, a growing body of research has categorized jailbreak attacks into token-level and prompt-level attacks. However, previous work primarily overlooks the diverse key factors of jailbreak attacks, with most studies concentrating on LLM vulnerabilities and lacking exploration of defense-enhanced LLMs. To address these issues, we evaluate the impact of various attack settings on LLM performance and provide a baseline benchmark for jailbreak attacks, encouraging the adoption of a standardized evaluation framework. Specifically, we evaluate the eight key factors of implementing jailbreak attacks on LLMs from both target-level and attack-level perspectives. We further conduct seven representative jailbreak attacks on six defense methods across two widely used datasets, encompassing approximately 320 experiments with about 50,000 GPU hours on A800-80G. Our experimental results highlight the need for standardized benchmarking to evaluate these attacks on defense-enhanced LLMs. Our code is available at https://github.com/usail-hkust/Bag_of_Tricks_for_LLM_Jailbreaking.","sentences":["Although Large Language Models (LLMs) have demonstrated significant capabilities in executing complex tasks in a zero-shot manner, they are susceptible to jailbreak attacks and can be manipulated to produce harmful outputs.","Recently, a growing body of research has categorized jailbreak attacks into token-level and prompt-level attacks.","However, previous work primarily overlooks the diverse key factors of jailbreak attacks, with most studies concentrating on LLM vulnerabilities and lacking exploration of defense-enhanced LLMs.","To address these issues, we evaluate the impact of various attack settings on LLM performance and provide a baseline benchmark for jailbreak attacks, encouraging the adoption of a standardized evaluation framework.","Specifically, we evaluate the eight key factors of implementing jailbreak attacks on LLMs from both target-level and attack-level perspectives.","We further conduct seven representative jailbreak attacks on six defense methods across two widely used datasets, encompassing approximately 320 experiments with about 50,000 GPU hours on A800-80G. Our experimental results highlight the need for standardized benchmarking to evaluate these attacks on defense-enhanced LLMs.","Our code is available at https://github.com/usail-hkust/Bag_of_Tricks_for_LLM_Jailbreaking."],"url":"http://arxiv.org/abs/2406.09324v1","category":"cs.CR"}
{"created":"2024-06-13 17:00:30","title":"Active Inference Meeting Energy-Efficient Control of Parallel and Identical Machines","abstract":"We investigate the application of active inference in developing energy-efficient control agents for manufacturing systems. Active inference, rooted in neuroscience, provides a unified probabilistic framework integrating perception, learning, and action, with inherent uncertainty quantification elements. Our study explores deep active inference, an emerging field that combines deep learning with the active inference decision-making framework. Leveraging a deep active inference agent, we focus on controlling parallel and identical machine workstations to enhance energy efficiency. We address challenges posed by the problem's stochastic nature and delayed policy response by introducing tailored enhancements to existing agent architectures. Specifically, we introduce multi-step transition and hybrid horizon methods to mitigate the need for complex planning. Our experimental results demonstrate the effectiveness of these enhancements and highlight the potential of the active inference-based approach.","sentences":["We investigate the application of active inference in developing energy-efficient control agents for manufacturing systems.","Active inference, rooted in neuroscience, provides a unified probabilistic framework integrating perception, learning, and action, with inherent uncertainty quantification elements.","Our study explores deep active inference, an emerging field that combines deep learning with the active inference decision-making framework.","Leveraging a deep active inference agent, we focus on controlling parallel and identical machine workstations to enhance energy efficiency.","We address challenges posed by the problem's stochastic nature and delayed policy response by introducing tailored enhancements to existing agent architectures.","Specifically, we introduce multi-step transition and hybrid horizon methods to mitigate the need for complex planning.","Our experimental results demonstrate the effectiveness of these enhancements and highlight the potential of the active inference-based approach."],"url":"http://arxiv.org/abs/2406.09322v1","category":"cs.LG"}
{"created":"2024-06-13 16:59:43","title":"JailbreakEval: An Integrated Toolkit for Evaluating Jailbreak Attempts Against Large Language Models","abstract":"Jailbreak attacks aim to induce Large Language Models (LLMs) to generate harmful responses for forbidden instructions, presenting severe misuse threats to LLMs. Up to now, research into jailbreak attacks and defenses is emerging, however, there is (surprisingly) no consensus on how to evaluate whether a jailbreak attempt is successful. In other words, the methods to assess the harmfulness of an LLM's response are varied, such as manual annotation or prompting GPT-4 in specific ways. Each approach has its own set of strengths and weaknesses, impacting their alignment with human values, as well as the time and financial cost. This diversity in evaluation presents challenges for researchers in choosing suitable evaluation methods and conducting fair comparisons across different jailbreak attacks and defenses. In this paper, we conduct a comprehensive analysis of jailbreak evaluation methodologies, drawing from nearly ninety jailbreak research released between May 2023 and April 2024. Our study introduces a systematic taxonomy of jailbreak evaluators, offering in-depth insights into their strengths and weaknesses, along with the current status of their adaptation. Moreover, to facilitate subsequent research, we propose JailbreakEval, a user-friendly toolkit focusing on the evaluation of jailbreak attempts. It includes various well-known evaluators out-of-the-box, so that users can obtain evaluation results with only a single command. JailbreakEval also allows users to customize their own evaluation workflow in a unified framework with the ease of development and comparison. In summary, we regard JailbreakEval to be a catalyst that simplifies the evaluation process in jailbreak research and fosters an inclusive standard for jailbreak evaluation within the community.","sentences":["Jailbreak attacks aim to induce Large Language Models (LLMs) to generate harmful responses for forbidden instructions, presenting severe misuse threats to LLMs.","Up to now, research into jailbreak attacks and defenses is emerging, however, there is (surprisingly) no consensus on how to evaluate whether a jailbreak attempt is successful.","In other words, the methods to assess the harmfulness of an LLM's response are varied, such as manual annotation or prompting GPT-4 in specific ways.","Each approach has its own set of strengths and weaknesses, impacting their alignment with human values, as well as the time and financial cost.","This diversity in evaluation presents challenges for researchers in choosing suitable evaluation methods and conducting fair comparisons across different jailbreak attacks and defenses.","In this paper, we conduct a comprehensive analysis of jailbreak evaluation methodologies, drawing from nearly ninety jailbreak research released between May 2023 and April 2024.","Our study introduces a systematic taxonomy of jailbreak evaluators, offering in-depth insights into their strengths and weaknesses, along with the current status of their adaptation.","Moreover, to facilitate subsequent research, we propose JailbreakEval, a user-friendly toolkit focusing on the evaluation of jailbreak attempts.","It includes various well-known evaluators out-of-the-box, so that users can obtain evaluation results with only a single command.","JailbreakEval also allows users to customize their own evaluation workflow in a unified framework with the ease of development and comparison.","In summary, we regard JailbreakEval to be a catalyst that simplifies the evaluation process in jailbreak research and fosters an inclusive standard for jailbreak evaluation within the community."],"url":"http://arxiv.org/abs/2406.09321v1","category":"cs.CR"}
{"created":"2024-06-13 16:58:02","title":"Khmer Semantic Search Engine: Digital Information Access and Document Retrieval","abstract":"The search engine process is crucial for document content retrieval. For Khmer documents, a tool is needed to extract essential keywords. Despite the daily generation of significant Khmer content, Cambodians struggle to find necessary documents due to the lack of an effective semantic searching tool. Even Google does not deliver high accuracy for Khmer content. Semantic search engines improve search results by employing advanced algorithms to understand various content types. With the rise in Khmer digital content such as reports, articles, and social media feedback enhanced search capabilities are essential. This research proposes the first Khmer Semantic Search Engine (KSE), designed to improve traditional Khmer search methods. Utilizing semantic matching techniques and formally annotated semantic content, our tool extracts meaningful keywords from user queries performs precise matching, and provides the best matching offline documents and online URL documents. We propose two semantic search frameworks based on keyword extraction and semantic search matching. Additionally, we developed tools for data preparation, including document addition and manual keyword extraction. To evaluate performance, we created a ground truth dataset and discussed issues related to searching and semantic search. Our findings show how understanding search term semantics can lead to more accurate results.","sentences":["The search engine process is crucial for document content retrieval.","For Khmer documents, a tool is needed to extract essential keywords.","Despite the daily generation of significant Khmer content, Cambodians struggle to find necessary documents due to the lack of an effective semantic searching tool.","Even Google does not deliver high accuracy for Khmer content.","Semantic search engines improve search results by employing advanced algorithms to understand various content types.","With the rise in Khmer digital content such as reports, articles, and social media feedback enhanced search capabilities are essential.","This research proposes the first Khmer Semantic Search Engine (KSE), designed to improve traditional Khmer search methods.","Utilizing semantic matching techniques and formally annotated semantic content, our tool extracts meaningful keywords from user queries performs precise matching, and provides the best matching offline documents and online URL documents.","We propose two semantic search frameworks based on keyword extraction and semantic search matching.","Additionally, we developed tools for data preparation, including document addition and manual keyword extraction.","To evaluate performance, we created a ground truth dataset and discussed issues related to searching and semantic search.","Our findings show how understanding search term semantics can lead to more accurate results."],"url":"http://arxiv.org/abs/2406.09320v1","category":"cs.IR"}
{"created":"2024-06-13 16:56:07","title":"Autohomeomorphisms of pre-images of $\\mathbb N^*$","abstract":"In the study of the Stone-\\u{C}ech remainder of the real line a detailed study of the Stone-\\u{C}ech remainder of the space $\\mathbb N\\times [0,1]$, which we denote as $\\mathbb M$, has often been utilized. Of course the real line can be covered by two closed sets that are each homeomorphic to $\\mathbb M$. It is known that an autohomeomorphism of $\\mathbb M^*$ induces an autohomeomorphism of $\\mathbb N^*$. We prove that it is consistent with there being non-trivial autohomeomorphism of $\\mathbb N^*$ that those induced by autohomeomorphisms of $\\mathbb M^*$ are trivial.","sentences":["In the study of the Stone-\\u{C}ech remainder of the real line a detailed study of the Stone-\\u{C}ech remainder of the space $\\mathbb N\\times [0,1]$, which we denote as $\\mathbb M$, has often been utilized.","Of course the real line can be covered by two closed sets that are each homeomorphic to $\\mathbb M$. It is known that an autohomeomorphism of $\\mathbb M^*$ induces an autohomeomorphism of $\\mathbb N^*$.","We prove that it is consistent with there being non-trivial autohomeomorphism of $\\mathbb N^*$ that those induced by autohomeomorphisms of $\\mathbb M^*$ are trivial."],"url":"http://arxiv.org/abs/2406.09319v1","category":"math.GN"}
{"created":"2024-06-13 16:55:07","title":"Characterising Interventions in Causal Games","abstract":"Causal games are probabilistic graphical models that enable causal queries to be answered in multi-agent settings. They extend causal Bayesian networks by specifying decision and utility variables to represent the agents' degrees of freedom and objectives. In multi-agent settings, whether each agent decides on their policy before or after knowing the causal intervention is important as this affects whether they can respond to the intervention by adapting their policy. Consequently, previous work in causal games imposed chronological constraints on permissible interventions. We relax this by outlining a sound and complete set of primitive causal interventions so the effect of any arbitrarily complex interventional query can be studied in multi-agent settings. We also demonstrate applications to the design of safe AI systems by considering causal mechanism design and commitment.","sentences":["Causal games are probabilistic graphical models that enable causal queries to be answered in multi-agent settings.","They extend causal Bayesian networks by specifying decision and utility variables to represent the agents' degrees of freedom and objectives.","In multi-agent settings, whether each agent decides on their policy before or after knowing the causal intervention is important as this affects whether they can respond to the intervention by adapting their policy.","Consequently, previous work in causal games imposed chronological constraints on permissible interventions.","We relax this by outlining a sound and complete set of primitive causal interventions so the effect of any arbitrarily complex interventional query can be studied in multi-agent settings.","We also demonstrate applications to the design of safe AI systems by considering causal mechanism design and commitment."],"url":"http://arxiv.org/abs/2406.09318v1","category":"cs.GT"}
{"created":"2024-06-13 16:53:57","title":"Common and Rare Fundus Diseases Identification Using Vision-Language Foundation Model with Knowledge of Over 400 Diseases","abstract":"The current retinal artificial intelligence models were trained using data with a limited category of diseases and limited knowledge. In this paper, we present a retinal vision-language foundation model (RetiZero) with knowledge of over 400 fundus diseases. Specifically, we collected 341,896 fundus images paired with text descriptions from 29 publicly available datasets, 180 ophthalmic books, and online resources, encompassing over 400 fundus diseases across multiple countries and ethnicities. RetiZero achieved outstanding performance across various downstream tasks, including zero-shot retinal disease recognition, image-to-image retrieval, internal domain and cross-domain retinal disease classification, and few-shot fine-tuning. Specially, in the zero-shot scenario, RetiZero achieved a Top5 score of 0.8430 and 0.7561 on 15 and 52 fundus diseases respectively. In the image-retrieval task, RetiZero achieved a Top5 score of 0.9500 and 0.8860 on 15 and 52 retinal diseases respectively. Furthermore, clinical evaluations by ophthalmology experts from different countries demonstrate that RetiZero can achieve performance comparable to experienced ophthalmologists using zero-shot and image retrieval methods without requiring model retraining. These capabilities of retinal disease identification strengthen our RetiZero foundation model in clinical implementation.","sentences":["The current retinal artificial intelligence models were trained using data with a limited category of diseases and limited knowledge.","In this paper, we present a retinal vision-language foundation model (RetiZero) with knowledge of over 400 fundus diseases.","Specifically, we collected 341,896 fundus images paired with text descriptions from 29 publicly available datasets, 180 ophthalmic books, and online resources, encompassing over 400 fundus diseases across multiple countries and ethnicities.","RetiZero achieved outstanding performance across various downstream tasks, including zero-shot retinal disease recognition, image-to-image retrieval, internal domain and cross-domain retinal disease classification, and few-shot fine-tuning.","Specially, in the zero-shot scenario, RetiZero achieved a Top5 score of 0.8430 and 0.7561 on 15 and 52 fundus diseases respectively.","In the image-retrieval task, RetiZero achieved a Top5 score of 0.9500 and 0.8860 on 15 and 52 retinal diseases respectively.","Furthermore, clinical evaluations by ophthalmology experts from different countries demonstrate that RetiZero can achieve performance comparable to experienced ophthalmologists using zero-shot and image retrieval methods without requiring model retraining.","These capabilities of retinal disease identification strengthen our RetiZero foundation model in clinical implementation."],"url":"http://arxiv.org/abs/2406.09317v1","category":"eess.IV"}
{"created":"2024-06-13 16:52:10","title":"Bose-Hubbard model with a single qubit","abstract":"The use of a single-qubit parametrized circuit as an Ansatz for the variational wave function in the calculation of the ground state energy of a quantum many-body system is demonstrated using the one-dimensional Bose-Hubbard model. Comparison is made to calculations where a classic neural network is used to generate the variational wave function. Computations carried out on IBM Quantum hardware are also presented.","sentences":["The use of a single-qubit parametrized circuit as an Ansatz for the variational wave function in the calculation of the ground state energy of a quantum many-body system is demonstrated using the one-dimensional Bose-Hubbard model.","Comparison is made to calculations where a classic neural network is used to generate the variational wave function.","Computations carried out on IBM Quantum hardware are also presented."],"url":"http://arxiv.org/abs/2406.09316v1","category":"quant-ph"}
{"created":"2024-06-13 16:51:33","title":"Vertical LoRA: Dense Expectation-Maximization Interpretation of Transformers","abstract":"In this paper, we show how Transformers can be interpreted as dense Expectation-Maximization algorithms performed on Bayesian Nets. Based on the above interpretation, we propose a new model design paradigm, namely Vertical LoRA (VLoRA), which reduces the parameter count dramatically while preserving performance. In VLoRA, a model consists of layers, each of which recursively learns an increment based on the previous layer. We then apply LoRA decomposition to the increments. VLoRA works on the base model, which is orthogonal to LoRA, meaning they can be used together. We do experiments on various tasks and models. The results show that 1) with VLoRA, the Transformer model parameter count can be reduced dramatically and 2) the performance of the original model is preserved. The source code is available at \\url{https://github.com/neverUseThisName/vlora}","sentences":["In this paper, we show how Transformers can be interpreted as dense Expectation-Maximization algorithms performed on Bayesian Nets.","Based on the above interpretation, we propose a new model design paradigm, namely Vertical LoRA (VLoRA), which reduces the parameter count dramatically while preserving performance.","In VLoRA, a model consists of layers, each of which recursively learns an increment based on the previous layer.","We then apply LoRA decomposition to the increments.","VLoRA works on the base model, which is orthogonal to LoRA, meaning they can be used together.","We do experiments on various tasks and models.","The results show that 1) with VLoRA, the Transformer model parameter count can be reduced dramatically and 2) the performance of the original model is preserved.","The source code is available at \\url{https://github.com/neverUseThisName/vlora}"],"url":"http://arxiv.org/abs/2406.09315v1","category":"cs.AI"}
{"created":"2024-06-13 16:49:48","title":"Ringdown signatures of Kerr black holes immersed in a magnetic field","abstract":"We analyze the quasinormal mode spectrum for Kerr black holes surrounded by an asymptotically uniform magnetic field, modeled with the Ernst-Wild geometry. A perturbative expansion in both the rotation parameter $a$ and the magnetic field $B$ allows separation of the perturbation equations, and we obtain the spectrum for a variety of scalar quasinormal modes over a range of parameters using the continued fraction method. We then interpolate the low-lying mode spectrum to construct an Ernst-Wild template for the ringdown, and use the LIGO-Virgo-KAGRA analysis tool pyRing to assess the impact of the magnetosphere on the extraction of ringdown signatures from several observed binary black hole mergers.","sentences":["We analyze the quasinormal mode spectrum for Kerr black holes surrounded by an asymptotically uniform magnetic field, modeled with the Ernst-Wild geometry.","A perturbative expansion in both the rotation parameter $a$ and the magnetic field $B$ allows separation of the perturbation equations, and we obtain the spectrum for a variety of scalar quasinormal modes over a range of parameters using the continued fraction method.","We then interpolate the low-lying mode spectrum to construct an Ernst-Wild template for the ringdown, and use the LIGO-Virgo-KAGRA analysis tool pyRing to assess the impact of the magnetosphere on the extraction of ringdown signatures from several observed binary black hole mergers."],"url":"http://arxiv.org/abs/2406.09314v1","category":"gr-qc"}
{"created":"2024-06-13 16:48:48","title":"Less Cybersickness, Please: Demystifying and Detecting Stereoscopic Visual Inconsistencies in VR Apps","abstract":"The quality of Virtual Reality (VR) apps is vital, particularly the rendering quality of the VR Graphical User Interface (GUI). Different from traditional 2D apps, VR apps create a 3D digital scene for users, by rendering two distinct 2D images for the user's left and right eyes, respectively. Stereoscopic visual inconsistency (denoted as \"SVI\") issues, however, undermine the rendering process of the user's brain, leading to user discomfort and even adverse health effects. Such issues commonly exist but remain underexplored. We conduct an empirical analysis on 282 SVI bug reports from 15 VR platforms, summarizing 15 types of manifestations. The empirical analysis reveals that automatically detecting SVI issues is challenging, mainly because: (1) lack of training data; (2) the manifestations of SVI issues are diverse, complicated, and often application-specific; (3) most accessible VR apps are closed-source commercial software. Existing pattern-based supervised classification approaches may be inapplicable or ineffective in detecting the SVI issues. To counter these challenges, we propose an unsupervised black-box testing framework named StereoID to identify the stereoscopic visual inconsistencies, based only on the rendered GUI states. StereoID generates a synthetic right-eye image based on the actual left-eye image and computes distances between the synthetic right-eye image and the actual right-eye image to detect SVI issues. We propose a depth-aware conditional stereo image translator to power the image generation process, which captures the expected perspective shifts between left-eye and right-eye images. We build a large-scale unlabeled VR stereo screenshot dataset with larger than 171K images from 288 real-world VR apps for experiments. After substantial experiments, StereoID demonstrates superior performance for detecting SVI issues in both user reports and wild VR apps.","sentences":["The quality of Virtual Reality (VR) apps is vital, particularly the rendering quality of the VR Graphical User Interface (GUI).","Different from traditional 2D apps, VR apps create a 3D digital scene for users, by rendering two distinct 2D images for the user's left and right eyes, respectively.","Stereoscopic visual inconsistency (denoted as \"SVI\") issues, however, undermine the rendering process of the user's brain, leading to user discomfort and even adverse health effects.","Such issues commonly exist but remain underexplored.","We conduct an empirical analysis on 282 SVI bug reports from 15 VR platforms, summarizing 15 types of manifestations.","The empirical analysis reveals that automatically detecting SVI issues is challenging, mainly because: (1) lack of training data; (2) the manifestations of SVI issues are diverse, complicated, and often application-specific; (3) most accessible VR apps are closed-source commercial software.","Existing pattern-based supervised classification approaches may be inapplicable or ineffective in detecting the SVI issues.","To counter these challenges, we propose an unsupervised black-box testing framework named StereoID to identify the stereoscopic visual inconsistencies, based only on the rendered GUI states.","StereoID generates a synthetic right-eye image based on the actual left-eye image and computes distances between the synthetic right-eye image and the actual right-eye image to detect SVI issues.","We propose a depth-aware conditional stereo image translator to power the image generation process, which captures the expected perspective shifts between left-eye and right-eye images.","We build a large-scale unlabeled VR stereo screenshot dataset with larger than 171K images from 288 real-world VR apps for experiments.","After substantial experiments, StereoID demonstrates superior performance for detecting SVI issues in both user reports and wild VR apps."],"url":"http://arxiv.org/abs/2406.09313v1","category":"cs.SE"}
{"created":"2024-06-13 16:44:58","title":"Neural networks in non-metric spaces","abstract":"Leveraging the infinite dimensional neural network architecture we proposed in arXiv:2109.13512v4 and which can process inputs from Fr\\'echet spaces, and using the universal approximation property shown therein, we now largely extend the scope of this architecture by proving several universal approximation theorems for a vast class of input and output spaces. More precisely, the input space $\\mathfrak X$ is allowed to be a general topological space satisfying only a mild condition (\"quasi-Polish\"), and the output space can be either another quasi-Polish space $\\mathfrak Y$ or a topological vector space $E$. Similarly to arXiv:2109.13512v4, we show furthermore that our neural network architectures can be projected down to \"finite dimensional\" subspaces with any desirable accuracy, thus obtaining approximating networks that are easy to implement and allow for fast computation and fitting. The resulting neural network architecture is therefore applicable for prediction tasks based on functional data. To the best of our knowledge, this is the first result which deals with such a wide class of input/output spaces and simultaneously guarantees the numerical feasibility of the ensuing architectures. Finally, we prove an obstruction result which indicates that the category of quasi-Polish spaces is in a certain sense the correct category to work with if one aims at constructing approximating architectures on infinite-dimensional spaces $\\mathfrak X$ which, at the same time, have sufficient expressive power to approximate continuous functions on $\\mathfrak X$, are specified by a finite number of parameters only and are \"stable\" with respect to these parameters.","sentences":["Leveraging the infinite dimensional neural network architecture we proposed in arXiv:2109.13512v4 and which can process inputs from Fr\\'echet spaces, and using the universal approximation property shown therein, we now largely extend the scope of this architecture by proving several universal approximation theorems for a vast class of input and output spaces.","More precisely, the input space $\\mathfrak X$ is allowed to be a general topological space satisfying only a mild condition (\"quasi-Polish\"), and the output space can be either another quasi-Polish space $\\mathfrak Y$ or a topological vector space $E$. Similarly to arXiv:2109.13512v4, we show furthermore that our neural network architectures can be projected down to \"finite dimensional\" subspaces with any desirable accuracy, thus obtaining approximating networks that are easy to implement and allow for fast computation and fitting.","The resulting neural network architecture is therefore applicable for prediction tasks based on functional data.","To the best of our knowledge, this is the first result which deals with such a wide class of input/output spaces and simultaneously guarantees the numerical feasibility of the ensuing architectures.","Finally, we prove an obstruction result which indicates that the category of quasi-Polish spaces is in a certain sense the correct category to work with if one aims at constructing approximating architectures on infinite-dimensional spaces $\\mathfrak X$ which, at the same time, have sufficient expressive power to approximate continuous functions on $\\mathfrak X$, are specified by a finite number of parameters only and are \"stable\" with respect to these parameters."],"url":"http://arxiv.org/abs/2406.09310v1","category":"math.FA"}
{"created":"2024-06-13 16:42:06","title":"Transformers meet Neural Algorithmic Reasoners","abstract":"Transformers have revolutionized machine learning with their simple yet effective architecture. Pre-training Transformers on massive text datasets from the Internet has led to unmatched generalization for natural language understanding (NLU) tasks. However, such language models remain fragile when tasked with algorithmic forms of reasoning, where computations must be precise and robust. To address this limitation, we propose a novel approach that combines the Transformer's language understanding with the robustness of graph neural network (GNN)-based neural algorithmic reasoners (NARs). Such NARs proved effective as generic solvers for algorithmic tasks, when specified in graph form. To make their embeddings accessible to a Transformer, we propose a hybrid architecture with a two-phase training procedure, allowing the tokens in the language model to cross-attend to the node embeddings from the NAR. We evaluate our resulting TransNAR model on CLRS-Text, the text-based version of the CLRS-30 benchmark, and demonstrate significant gains over Transformer-only models for algorithmic reasoning, both in and out of distribution.","sentences":["Transformers have revolutionized machine learning with their simple yet effective architecture.","Pre-training Transformers on massive text datasets from the Internet has led to unmatched generalization for natural language understanding (NLU) tasks.","However, such language models remain fragile when tasked with algorithmic forms of reasoning, where computations must be precise and robust.","To address this limitation, we propose a novel approach that combines the Transformer's language understanding with the robustness of graph neural network (GNN)-based neural algorithmic reasoners (NARs).","Such NARs proved effective as generic solvers for algorithmic tasks, when specified in graph form.","To make their embeddings accessible to a Transformer, we propose a hybrid architecture with a two-phase training procedure, allowing the tokens in the language model to cross-attend to the node embeddings from the NAR.","We evaluate our resulting TransNAR model on CLRS-Text, the text-based version of the CLRS-30 benchmark, and demonstrate significant gains over Transformer-only models for algorithmic reasoning, both in and out of distribution."],"url":"http://arxiv.org/abs/2406.09308v1","category":"cs.CL"}
{"created":"2024-06-13 16:40:39","title":"Toffee: Efficient Million-Scale Dataset Construction for Subject-Driven Text-to-Image Generation","abstract":"In subject-driven text-to-image generation, recent works have achieved superior performance by training the model on synthetic datasets containing numerous image pairs. Trained on these datasets, generative models can produce text-aligned images for specific subject from arbitrary testing image in a zero-shot manner. They even outperform methods which require additional fine-tuning on testing images. However, the cost of creating such datasets is prohibitive for most researchers. To generate a single training pair, current methods fine-tune a pre-trained text-to-image model on the subject image to capture fine-grained details, then use the fine-tuned model to create images for the same subject based on creative text prompts. Consequently, constructing a large-scale dataset with millions of subjects can require hundreds of thousands of GPU hours. To tackle this problem, we propose Toffee, an efficient method to construct datasets for subject-driven editing and generation. Specifically, our dataset construction does not need any subject-level fine-tuning. After pre-training two generative models, we are able to generate infinite number of high-quality samples. We construct the first large-scale dataset for subject-driven image editing and generation, which contains 5 million image pairs, text prompts, and masks. Our dataset is 5 times the size of previous largest dataset, yet our cost is tens of thousands of GPU hours lower. To test the proposed dataset, we also propose a model which is capable of both subject-driven image editing and generation. By simply training the model on our proposed dataset, it obtains competitive results, illustrating the effectiveness of the proposed dataset construction framework.","sentences":["In subject-driven text-to-image generation, recent works have achieved superior performance by training the model on synthetic datasets containing numerous image pairs.","Trained on these datasets, generative models can produce text-aligned images for specific subject from arbitrary testing image in a zero-shot manner.","They even outperform methods which require additional fine-tuning on testing images.","However, the cost of creating such datasets is prohibitive for most researchers.","To generate a single training pair, current methods fine-tune a pre-trained text-to-image model on the subject image to capture fine-grained details, then use the fine-tuned model to create images for the same subject based on creative text prompts.","Consequently, constructing a large-scale dataset with millions of subjects can require hundreds of thousands of GPU hours.","To tackle this problem, we propose Toffee, an efficient method to construct datasets for subject-driven editing and generation.","Specifically, our dataset construction does not need any subject-level fine-tuning.","After pre-training two generative models, we are able to generate infinite number of high-quality samples.","We construct the first large-scale dataset for subject-driven image editing and generation, which contains 5 million image pairs, text prompts, and masks.","Our dataset is 5 times the size of previous largest dataset, yet our cost is tens of thousands of GPU hours lower.","To test the proposed dataset, we also propose a model which is capable of both subject-driven image editing and generation.","By simply training the model on our proposed dataset, it obtains competitive results, illustrating the effectiveness of the proposed dataset construction framework."],"url":"http://arxiv.org/abs/2406.09305v1","category":"cs.CV"}
{"created":"2024-06-13 16:40:17","title":"Self-reconfigurable Multifunctional Memristive Nociceptor for Intelligent Robotics","abstract":"Artificial nociceptors, mimicking human-like stimuli perception, are of significance for intelligent robotics to work in hazardous and dynamic scenarios. One of the most essential characteristics of the human nociceptor is its self-adjustable attribute, which indicates that the threshold of determination of a potentially hazardous stimulus relies on environmental knowledge. This critical attribute has been currently omitted, but it is highly desired for artificial nociceptors. Inspired by these shortcomings, this article presents, for the first time, a Self-Directed Channel (SDC) memristor-based self-reconfigurable nociceptor, capable of perceiving hazardous pressure stimuli under different temperatures and demonstrates key features of tactile nociceptors, including 'threshold,' 'no-adaptation,' and 'sensitization.' The maximum amplification of hazardous external stimuli is 1000%, and its response characteristics dynamically adapt to current temperature conditions by automatically altering the generated modulation schemes for the memristor. The maximum difference ratio of the response of memristors at different temperatures is 500%, and this adaptability closely mimics the functions of biological tactile nociceptors, resulting in accurate danger perception in various conditions. Beyond temperature adaptation, this memristor-based nociceptor has the potential to integrate different sensory modalities by applying various sensors, thereby achieving human-like perception capabilities in real-world environments.","sentences":["Artificial nociceptors, mimicking human-like stimuli perception, are of significance for intelligent robotics to work in hazardous and dynamic scenarios.","One of the most essential characteristics of the human nociceptor is its self-adjustable attribute, which indicates that the threshold of determination of a potentially hazardous stimulus relies on environmental knowledge.","This critical attribute has been currently omitted, but it is highly desired for artificial nociceptors.","Inspired by these shortcomings, this article presents, for the first time, a Self-Directed Channel (SDC) memristor-based self-reconfigurable nociceptor, capable of perceiving hazardous pressure stimuli under different temperatures and demonstrates key features of tactile nociceptors, including 'threshold,' 'no-adaptation,' and 'sensitization.'","The maximum amplification of hazardous external stimuli is 1000%, and its response characteristics dynamically adapt to current temperature conditions by automatically altering the generated modulation schemes for the memristor.","The maximum difference ratio of the response of memristors at different temperatures is 500%, and this adaptability closely mimics the functions of biological tactile nociceptors, resulting in accurate danger perception in various conditions.","Beyond temperature adaptation, this memristor-based nociceptor has the potential to integrate different sensory modalities by applying various sensors, thereby achieving human-like perception capabilities in real-world environments."],"url":"http://arxiv.org/abs/2406.09304v1","category":"physics.app-ph"}
{"created":"2024-06-13 16:30:32","title":"Parameter-Efficient Active Learning for Foundational models","abstract":"Foundational vision transformer models have shown impressive few shot performance on many vision tasks. This research presents a novel investigation into the application of parameter efficient fine-tuning methods within an active learning (AL) framework, to advance the sampling selection process in extremely budget constrained classification tasks. The focus on image datasets, known for their out-of-distribution characteristics, adds a layer of complexity and relevance to our study. Through a detailed evaluation, we illustrate the improved AL performance on these challenging datasets, highlighting the strategic advantage of merging parameter efficient fine tuning methods with foundation models. This contributes to the broader discourse on optimizing AL strategies, presenting a promising avenue for future exploration in leveraging foundation models for efficient and effective data annotation in specialized domains.","sentences":["Foundational vision transformer models have shown impressive few shot performance on many vision tasks.","This research presents a novel investigation into the application of parameter efficient fine-tuning methods within an active learning (AL) framework, to advance the sampling selection process in extremely budget constrained classification tasks.","The focus on image datasets, known for their out-of-distribution characteristics, adds a layer of complexity and relevance to our study.","Through a detailed evaluation, we illustrate the improved AL performance on these challenging datasets, highlighting the strategic advantage of merging parameter efficient fine tuning methods with foundation models.","This contributes to the broader discourse on optimizing AL strategies, presenting a promising avenue for future exploration in leveraging foundation models for efficient and effective data annotation in specialized domains."],"url":"http://arxiv.org/abs/2406.09296v1","category":"cs.CV"}
{"created":"2024-06-13 16:30:03","title":"You Don't Need Data-Augmentation in Self-Supervised Learning","abstract":"Self-Supervised learning (SSL) with Joint-Embedding Architectures (JEA) has led to outstanding performances. All instantiations of this paradigm were trained using strong and well-established hand-crafted data augmentations, leading to the general belief that they are required for the proper training and performance of such models. On the other hand, generative reconstruction-based models such as BEIT and MAE or Joint-Embedding Predictive Architectures such as I-JEPA have shown strong performance without using data augmentations except masking. In this work, we challenge the importance of invariance and data-augmentation in JEAs at scale. By running a case-study on a recent SSL foundation model - DINOv2 - we show that strong image representations can be obtained with JEAs and only cropping without resizing provided the training data is large enough, reaching state-of-the-art results and using the least amount of augmentation in the literature. Through this study, we also discuss the impact of compute constraints on the outcomes of experimental deep learning research, showing that they can lead to very different conclusions.","sentences":["Self-Supervised learning (SSL) with Joint-Embedding Architectures (JEA) has led to outstanding performances.","All instantiations of this paradigm were trained using strong and well-established hand-crafted data augmentations, leading to the general belief that they are required for the proper training and performance of such models.","On the other hand, generative reconstruction-based models such as BEIT and MAE or Joint-Embedding Predictive Architectures such as I-JEPA have shown strong performance without using data augmentations except masking.","In this work, we challenge the importance of invariance and data-augmentation in JEAs at scale.","By running a case-study on a recent SSL foundation model - DINOv2 - we show that strong image representations can be obtained with JEAs and only cropping without resizing provided the training data is large enough, reaching state-of-the-art results and using the least amount of augmentation in the literature.","Through this study, we also discuss the impact of compute constraints on the outcomes of experimental deep learning research, showing that they can lead to very different conclusions."],"url":"http://arxiv.org/abs/2406.09294v1","category":"cs.LG"}
{"created":"2024-06-13 16:29:46","title":"StableMaterials: Enhancing Diversity in Material Generation via Semi-Supervised Learning","abstract":"We introduce StableMaterials, a novel approach for generating photorealistic physical-based rendering (PBR) materials that integrate semi-supervised learning with Latent Diffusion Models (LDMs). Our method employs adversarial training to distill knowledge from existing large-scale image generation models, minimizing the reliance on annotated data and enhancing the diversity in generation. This distillation approach aligns the distribution of the generated materials with that of image textures from an SDXL model, enabling the generation of novel materials that are not present in the initial training dataset. Furthermore, we employ a diffusion-based refiner model to improve the visual quality of the samples and achieve high-resolution generation. Finally, we distill a latent consistency model for fast generation in just four steps and propose a new tileability technique that removes visual artifacts typically associated with fewer diffusion steps. We detail the architecture and training process of StableMaterials, the integration of semi-supervised training within existing LDM frameworks and show the advantages of our approach. Comparative evaluations with state-of-the-art methods show the effectiveness of StableMaterials, highlighting its potential applications in computer graphics and beyond. StableMaterials is publicly available at https://gvecchio.com/stablematerials.","sentences":["We introduce StableMaterials, a novel approach for generating photorealistic physical-based rendering (PBR) materials that integrate semi-supervised learning with Latent Diffusion Models (LDMs).","Our method employs adversarial training to distill knowledge from existing large-scale image generation models, minimizing the reliance on annotated data and enhancing the diversity in generation.","This distillation approach aligns the distribution of the generated materials with that of image textures from an SDXL model, enabling the generation of novel materials that are not present in the initial training dataset.","Furthermore, we employ a diffusion-based refiner model to improve the visual quality of the samples and achieve high-resolution generation.","Finally, we distill a latent consistency model for fast generation in just four steps and propose a new tileability technique that removes visual artifacts typically associated with fewer diffusion steps.","We detail the architecture and training process of StableMaterials, the integration of semi-supervised training within existing LDM frameworks and show the advantages of our approach.","Comparative evaluations with state-of-the-art methods show the effectiveness of StableMaterials, highlighting its potential applications in computer graphics and beyond.","StableMaterials is publicly available at https://gvecchio.com/stablematerials."],"url":"http://arxiv.org/abs/2406.09293v1","category":"cs.CV"}
{"created":"2024-06-13 16:29:18","title":"Neural Assets: 3D-Aware Multi-Object Scene Synthesis with Image Diffusion Models","abstract":"We address the problem of multi-object 3D pose control in image diffusion models. Instead of conditioning on a sequence of text tokens, we propose to use a set of per-object representations, Neural Assets, to control the 3D pose of individual objects in a scene. Neural Assets are obtained by pooling visual representations of objects from a reference image, such as a frame in a video, and are trained to reconstruct the respective objects in a different image, e.g., a later frame in the video. Importantly, we encode object visuals from the reference image while conditioning on object poses from the target frame. This enables learning disentangled appearance and pose features. Combining visual and 3D pose representations in a sequence-of-tokens format allows us to keep the text-to-image architecture of existing models, with Neural Assets in place of text tokens. By fine-tuning a pre-trained text-to-image diffusion model with this information, our approach enables fine-grained 3D pose and placement control of individual objects in a scene. We further demonstrate that Neural Assets can be transferred and recomposed across different scenes. Our model achieves state-of-the-art multi-object editing results on both synthetic 3D scene datasets, as well as two real-world video datasets (Objectron, Waymo Open).","sentences":["We address the problem of multi-object 3D pose control in image diffusion models.","Instead of conditioning on a sequence of text tokens, we propose to use a set of per-object representations, Neural Assets, to control the 3D pose of individual objects in a scene.","Neural Assets are obtained by pooling visual representations of objects from a reference image, such as a frame in a video, and are trained to reconstruct the respective objects in a different image, e.g., a later frame in the video.","Importantly, we encode object visuals from the reference image while conditioning on object poses from the target frame.","This enables learning disentangled appearance and pose features.","Combining visual and 3D pose representations in a sequence-of-tokens format allows us to keep the text-to-image architecture of existing models, with Neural Assets in place of text tokens.","By fine-tuning a pre-trained text-to-image diffusion model with this information, our approach enables fine-grained 3D pose and placement control of individual objects in a scene.","We further demonstrate that Neural Assets can be transferred and recomposed across different scenes.","Our model achieves state-of-the-art multi-object editing results on both synthetic 3D scene datasets, as well as two real-world video datasets (Objectron, Waymo Open)."],"url":"http://arxiv.org/abs/2406.09292v1","category":"cs.CV"}
{"created":"2024-06-13 16:29:06","title":"A Flexible, Equivariant Framework for Subgraph GNNs via Graph Products and Graph Coarsening","abstract":"Subgraph Graph Neural Networks (Subgraph GNNs) enhance the expressivity of message-passing GNNs by representing graphs as sets of subgraphs. They have shown impressive performance on several tasks, but their complexity limits applications to larger graphs. Previous approaches suggested processing only subsets of subgraphs, selected either randomly or via learnable sampling. However, they make suboptimal subgraph selections or can only cope with very small subset sizes, inevitably incurring performance degradation. This paper introduces a new Subgraph GNNs framework to address these issues. We employ a graph coarsening function to cluster nodes into super-nodes with induced connectivity. The product between the coarsened and the original graph reveals an implicit structure whereby subgraphs are associated with specific sets of nodes. By running generalized message-passing on such graph product, our method effectively implements an efficient, yet powerful Subgraph GNN. Controlling the coarsening function enables meaningful selection of any number of subgraphs while, contrary to previous methods, being fully compatible with standard training techniques. Notably, we discover that the resulting node feature tensor exhibits new, unexplored permutation symmetries. We leverage this structure, characterize the associated linear equivariant layers and incorporate them into the layers of our Subgraph GNN architecture. Extensive experiments on multiple graph learning benchmarks demonstrate that our method is significantly more flexible than previous approaches, as it can seamlessly handle any number of subgraphs, while consistently outperforming baseline approaches.","sentences":["Subgraph Graph Neural Networks (Subgraph GNNs) enhance the expressivity of message-passing GNNs by representing graphs as sets of subgraphs.","They have shown impressive performance on several tasks, but their complexity limits applications to larger graphs.","Previous approaches suggested processing only subsets of subgraphs, selected either randomly or via learnable sampling.","However, they make suboptimal subgraph selections or can only cope with very small subset sizes, inevitably incurring performance degradation.","This paper introduces a new Subgraph GNNs framework to address these issues.","We employ a graph coarsening function to cluster nodes into super-nodes with induced connectivity.","The product between the coarsened and the original graph reveals an implicit structure whereby subgraphs are associated with specific sets of nodes.","By running generalized message-passing on such graph product, our method effectively implements an efficient, yet powerful Subgraph GNN.","Controlling the coarsening function enables meaningful selection of any number of subgraphs while, contrary to previous methods, being fully compatible with standard training techniques.","Notably, we discover that the resulting node feature tensor exhibits new, unexplored permutation symmetries.","We leverage this structure, characterize the associated linear equivariant layers and incorporate them into the layers of our Subgraph GNN architecture.","Extensive experiments on multiple graph learning benchmarks demonstrate that our method is significantly more flexible than previous approaches, as it can seamlessly handle any number of subgraphs, while consistently outperforming baseline approaches."],"url":"http://arxiv.org/abs/2406.09291v1","category":"cs.LG"}
{"created":"2024-06-13 16:26:47","title":"Understanding Jailbreak Success: A Study of Latent Space Dynamics in Large Language Models","abstract":"Conversational Large Language Models are trained to refuse to answer harmful questions. However, emergent jailbreaking techniques can still elicit unsafe outputs, presenting an ongoing challenge for model alignment. To better understand how different jailbreak types circumvent safeguards, this paper analyses model activations on different jailbreak inputs. We find that it is possible to extract a jailbreak vector from a single class of jailbreaks that works to mitigate jailbreak effectiveness from other classes. This may indicate that different kinds of effective jailbreaks operate via similar internal mechanisms. We investigate a potential common mechanism of harmfulness feature suppression, and provide evidence for its existence by looking at the harmfulness vector component. These findings offer actionable insights for developing more robust jailbreak countermeasures and lay the groundwork for a deeper, mechanistic understanding of jailbreak dynamics in language models.","sentences":["Conversational Large Language Models are trained to refuse to answer harmful questions.","However, emergent jailbreaking techniques can still elicit unsafe outputs, presenting an ongoing challenge for model alignment.","To better understand how different jailbreak types circumvent safeguards, this paper analyses model activations on different jailbreak inputs.","We find that it is possible to extract a jailbreak vector from a single class of jailbreaks that works to mitigate jailbreak effectiveness from other classes.","This may indicate that different kinds of effective jailbreaks operate via similar internal mechanisms.","We investigate a potential common mechanism of harmfulness feature suppression, and provide evidence for its existence by looking at the harmfulness vector component.","These findings offer actionable insights for developing more robust jailbreak countermeasures and lay the groundwork for a deeper, mechanistic understanding of jailbreak dynamics in language models."],"url":"http://arxiv.org/abs/2406.09289v1","category":"cs.CL"}
{"created":"2024-06-13 16:26:37","title":"Zero-Shot Learning Over Large Output Spaces : Utilizing Indirect Knowledge Extraction from Large Language Models","abstract":"Extreme Multi-label Learning (XMC) is a task that allocates the most relevant labels for an instance from a predefined label set. Extreme Zero-shot XMC (EZ-XMC) is a special setting of XMC wherein no supervision is provided; only the instances (raw text of the document) and the predetermined label set are given. The scenario is designed to address cold-start problems in categorization and recommendation. Traditional state-of-the-art methods extract pseudo labels from the document title or segments. These labels from the document are used to train a zero-shot bi-encoder model. The main issue with these generated labels is their misalignment with the tagging task. In this work, we propose a framework to train a small bi-encoder model via the feedback from the large language model (LLM), the bi-encoder model encodes the document and labels into embeddings for retrieval. Our approach leverages the zero-shot ability of LLM to assess the correlation between labels and the document instead of using the low-quality labels extracted from the document itself. Our method also guarantees fast inference without the involvement of LLM. The performance of our approach outperforms the SOTA methods on various datasets while retaining a similar training time for large datasets.","sentences":["Extreme Multi-label Learning (XMC) is a task that allocates the most relevant labels for an instance from a predefined label set.","Extreme Zero-shot XMC (EZ-XMC) is a special setting of XMC wherein no supervision is provided; only the instances (raw text of the document) and the predetermined label set are given.","The scenario is designed to address cold-start problems in categorization and recommendation.","Traditional state-of-the-art methods extract pseudo labels from the document title or segments.","These labels from the document are used to train a zero-shot bi-encoder model.","The main issue with these generated labels is their misalignment with the tagging task.","In this work, we propose a framework to train a small bi-encoder model via the feedback from the large language model (LLM), the bi-encoder model encodes the document and labels into embeddings for retrieval.","Our approach leverages the zero-shot ability of LLM to assess the correlation between labels and the document instead of using the low-quality labels extracted from the document itself.","Our method also guarantees fast inference without the involvement of LLM.","The performance of our approach outperforms the SOTA methods on various datasets while retaining a similar training time for large datasets."],"url":"http://arxiv.org/abs/2406.09288v1","category":"cs.LG"}
{"created":"2024-06-13 16:26:33","title":"FlowAVSE: Efficient Audio-Visual Speech Enhancement with Conditional Flow Matching","abstract":"This work proposes an efficient method to enhance the quality of corrupted speech signals by leveraging both acoustic and visual cues. While existing diffusion-based approaches have demonstrated remarkable quality, their applicability is limited by slow inference speeds and computational complexity. To address this issue, we present FlowAVSE which enhances the inference speed and reduces the number of learnable parameters without degrading the output quality. In particular, we employ a conditional flow matching algorithm that enables the generation of high-quality speech in a single sampling step. Moreover, we increase efficiency by optimizing the underlying U-net architecture of diffusion-based systems. Our experiments demonstrate that FlowAVSE achieves 22 times faster inference speed and reduces the model size by half while maintaining the output quality. The demo page is available at: https://cyongong.github.io/FlowAVSE.github.io/","sentences":["This work proposes an efficient method to enhance the quality of corrupted speech signals by leveraging both acoustic and visual cues.","While existing diffusion-based approaches have demonstrated remarkable quality, their applicability is limited by slow inference speeds and computational complexity.","To address this issue, we present FlowAVSE which enhances the inference speed and reduces the number of learnable parameters without degrading the output quality.","In particular, we employ a conditional flow matching algorithm that enables the generation of high-quality speech in a single sampling step.","Moreover, we increase efficiency by optimizing the underlying U-net architecture of diffusion-based systems.","Our experiments demonstrate that FlowAVSE achieves 22 times faster inference speed and reduces the model size by half while maintaining the output quality.","The demo page is available at: https://cyongong.github.io/FlowAVSE.github.io/"],"url":"http://arxiv.org/abs/2406.09286v1","category":"eess.AS"}
{"created":"2024-06-13 16:22:40","title":"Local Langlands in families: The banal case","abstract":"We state a conjecture, local Langlands in families, connecting the centre of the category of smooth representations on $\\mathbb{Z}[\\sqrt{q}^{-1}]$-modules of a quasi-split $p$-adic group $\\mathrm{G}$ (where $q$ is the cardinality of the residue field of the underlying local field), the ring of global functions on the stack of Langlands parameters for $\\mathrm{G}$ over $\\mathbb{Z}[\\sqrt{q}^{-1}]$, and the endomorphisms of a Gelfand-Graev representation for $\\mathrm{G}$. For a class of classical $p$-adic groups (symplectic, unitary, or split odd special orthogonal groups), we prove this conjecture after inverting an integer depending only on $\\mathrm{G}$. Along the way, we show that the local Langlands correspondence for classical $p$-adic groups (1) preserves integrality of $\\ell$-adic representations; (2) satisfies an \"extended\" (generic) packet conjecture; (3) is compatible with parabolic induction up to semisimplification (generalizing a result of Moussaoui), hence induces a semisimple local Langlands correspondence; and (4) the semisimple correspondence is compatible with automorphisms of $\\mathbb{C}$ fixing $\\sqrt{q}$.","sentences":["We state a conjecture, local Langlands in families, connecting the centre of the category of smooth representations on $\\mathbb{Z}[\\sqrt{q}^{-1}]$-modules of a quasi-split $p$-adic group $\\mathrm{G}$ (where $q$ is the cardinality of the residue field of the underlying local field), the ring of global functions on the stack of Langlands parameters for $\\mathrm{G}$ over $\\mathbb{Z}[\\sqrt{q}^{-1}]$, and the endomorphisms of a Gelfand-Graev representation for $\\mathrm{G}$. For a class of classical $p$-adic groups (symplectic, unitary, or split odd special orthogonal groups), we prove this conjecture after inverting an integer depending only on $\\mathrm{G}$. Along the way, we show that the local Langlands correspondence for classical $p$-adic groups (1) preserves integrality of $\\ell$-adic representations; (2) satisfies an \"extended\" (generic) packet conjecture; (3) is compatible with parabolic induction up to semisimplification (generalizing a result of Moussaoui), hence induces a semisimple local Langlands correspondence; and (4) the semisimple correspondence is compatible with automorphisms of $\\mathbb{C}$ fixing $\\sqrt{q}$."],"url":"http://arxiv.org/abs/2406.09283v1","category":"math.RT"}
{"created":"2024-06-13 16:19:38","title":"Computing congruences of finite inverse semigroups","abstract":"In this paper we present an algorithm for computing a congruence on an inverse semigroup from a collection of generating pairs. This algorithm uses a myriad of techniques from computational group theory, automata, and the theory of inverse semigroups. An initial implementation of this algorithm outperforms existing implementations by several orders of magnitude.","sentences":["In this paper we present an algorithm for computing a congruence on an inverse semigroup from a collection of generating pairs.","This algorithm uses a myriad of techniques from computational group theory, automata, and the theory of inverse semigroups.","An initial implementation of this algorithm outperforms existing implementations by several orders of magnitude."],"url":"http://arxiv.org/abs/2406.09281v1","category":"math.GR"}
{"created":"2024-06-13 16:17:21","title":"Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback","abstract":"Learning from preference feedback has emerged as an essential step for improving the generation quality and performance of modern language models (LMs). Despite its widespread use, the way preference-based learning is applied varies wildly, with differing data, learning algorithms, and evaluations used, making disentangling the impact of each aspect difficult. In this work, we identify four core aspects of preference-based learning: preference data, learning algorithm, reward model, and policy training prompts, systematically investigate the impact of these components on downstream model performance, and suggest a recipe for strong learning for preference feedback. Our findings indicate that all aspects are important for performance, with better preference data leading to the largest improvements, followed by the choice of learning algorithm, the use of improved reward models, and finally the use of additional unlabeled prompts for policy training. Notably, PPO outperforms DPO by up to 2.5% in math and 1.2% in general domains. High-quality preference data leads to improvements of up to 8% in instruction following and truthfulness. Despite significant gains of up to 5% in mathematical evaluation when scaling up reward models, we surprisingly observe marginal improvements in other categories.   We publicly release the code used for training (https://github.com/hamishivi/EasyLM) and evaluating (https://github.com/allenai/open-instruct) our models, along with the models and datasets themselves (https://huggingface.co/collections/allenai/tulu-v25-suite-66676520fd578080e126f618).","sentences":["Learning from preference feedback has emerged as an essential step for improving the generation quality and performance of modern language models (LMs).","Despite its widespread use, the way preference-based learning is applied varies wildly, with differing data, learning algorithms, and evaluations used, making disentangling the impact of each aspect difficult.","In this work, we identify four core aspects of preference-based learning: preference data, learning algorithm, reward model, and policy training prompts, systematically investigate the impact of these components on downstream model performance, and suggest a recipe for strong learning for preference feedback.","Our findings indicate that all aspects are important for performance, with better preference data leading to the largest improvements, followed by the choice of learning algorithm, the use of improved reward models, and finally the use of additional unlabeled prompts for policy training.","Notably, PPO outperforms DPO by up to 2.5% in math and 1.2% in general domains.","High-quality preference data leads to improvements of up to 8% in instruction following and truthfulness.","Despite significant gains of up to 5% in mathematical evaluation when scaling up reward models, we surprisingly observe marginal improvements in other categories.   ","We publicly release the code used for training (https://github.com/hamishivi/EasyLM) and evaluating (https://github.com/allenai/open-instruct) our models, along with the models and datasets themselves (https://huggingface.co/collections/allenai/tulu-v25-suite-66676520fd578080e126f618)."],"url":"http://arxiv.org/abs/2406.09279v1","category":"cs.CL"}
{"created":"2024-06-13 16:15:53","title":"End-to-end Streaming model for Low-Latency Speech Anonymization","abstract":"Speaker anonymization aims to conceal cues to speaker identity while preserving linguistic content. Current machine learning based approaches require substantial computational resources, hindering real-time streaming applications. To address these concerns, we propose a streaming model that achieves speaker anonymization with low latency. The system is trained in an end-to-end autoencoder fashion using a lightweight content encoder that extracts HuBERT-like information, a pretrained speaker encoder that extract speaker identity, and a variance encoder that injects pitch and energy information. These three disentangled representations are fed to a decoder that resynthesizes the speech signal. We present evaluation results from two implementations of our system, a full model that achieves a latency of 230ms, and a lite version (0.1x in size) that further reduces latency to 66ms while maintaining state-of-the-art performance in naturalness, intelligibility, and privacy preservation.","sentences":["Speaker anonymization aims to conceal cues to speaker identity while preserving linguistic content.","Current machine learning based approaches require substantial computational resources, hindering real-time streaming applications.","To address these concerns, we propose a streaming model that achieves speaker anonymization with low latency.","The system is trained in an end-to-end autoencoder fashion using a lightweight content encoder that extracts HuBERT-like information, a pretrained speaker encoder that extract speaker identity, and a variance encoder that injects pitch and energy information.","These three disentangled representations are fed to a decoder that resynthesizes the speech signal.","We present evaluation results from two implementations of our system, a full model that achieves a latency of 230ms, and a lite version (0.1x in size) that further reduces latency to 66ms while maintaining state-of-the-art performance in naturalness, intelligibility, and privacy preservation."],"url":"http://arxiv.org/abs/2406.09277v1","category":"eess.AS"}
{"created":"2024-06-13 16:14:22","title":"The CUISINES Framework for Conducting Exoplanet Model Intercomparison Projects, Version 1.0","abstract":"As JWST begins to return observations, it is more important than ever that exoplanet climate models can consistently and correctly predict the observability of exoplanets, retrieval of their data, and interpretation of planetary environments from that data. Model intercomparisons play a crucial role in this context, especially now when few data are available to validate model predictions. The CUISINES Working Group of NASA's Nexus for Exoplanet System Science (NExSS) supports a systematic approach to evaluating the performance of exoplanet models, and provides here a framework for conducting community-organized exoplanet Model Intercomparison Projects (exoMIPs). The CUISINES framework adapts Earth climate community practices specifically for the needs of exoplanet researchers, encompassing a range of model types, planetary targets, and parameter space studies. It is intended to help researchers to work collectively, equitably, and openly toward common goals. The CUISINES framework rests on five principles: 1) Define in advance what research question(s) the exoMIP is intended to address. 2) Create an experimental design that maximizes community participation, and advertise it widely. 3) Plan a project timeline that allows all exoMIP members to participate fully. 4) Generate data products from model output for direct comparison to observations. 5) Create a data management plan that is workable in the present and scalable for the future. Within the first years of its existence, CUISINES is already providing logistical support to 10 exoMIPs, and will continue to host annual workshops for further community feedback and presentation of new exoMIP ideas.","sentences":["As JWST begins to return observations, it is more important than ever that exoplanet climate models can consistently and correctly predict the observability of exoplanets, retrieval of their data, and interpretation of planetary environments from that data.","Model intercomparisons play a crucial role in this context, especially now when few data are available to validate model predictions.","The CUISINES Working Group of NASA's Nexus for Exoplanet System Science (NExSS) supports a systematic approach to evaluating the performance of exoplanet models, and provides here a framework for conducting community-organized exoplanet Model Intercomparison Projects (exoMIPs).","The CUISINES framework adapts Earth climate community practices specifically for the needs of exoplanet researchers, encompassing a range of model types, planetary targets, and parameter space studies.","It is intended to help researchers to work collectively, equitably, and openly toward common goals.","The CUISINES framework rests on five principles: 1) Define in advance what research question(s) the exoMIP is intended to address.","2) Create an experimental design that maximizes community participation, and advertise it widely.","3) Plan a project timeline that allows all exoMIP members to participate fully.","4) Generate data products from model output for direct comparison to observations.","5) Create a data management plan that is workable in the present and scalable for the future.","Within the first years of its existence, CUISINES is already providing logistical support to 10 exoMIPs, and will continue to host annual workshops for further community feedback and presentation of new exoMIP ideas."],"url":"http://arxiv.org/abs/2406.09275v1","category":"astro-ph.EP"}
{"created":"2024-06-13 16:10:19","title":"Action2Sound: Ambient-Aware Generation of Action Sounds from Egocentric Videos","abstract":"Generating realistic audio for human interactions is important for many applications, such as creating sound effects for films or virtual reality games. Existing approaches implicitly assume total correspondence between the video and audio during training, yet many sounds happen off-screen and have weak to no correspondence with the visuals -- resulting in uncontrolled ambient sounds or hallucinations at test time. We propose a novel ambient-aware audio generation model, AV-LDM. We devise a novel audio-conditioning mechanism to learn to disentangle foreground action sounds from the ambient background sounds in in-the-wild training videos. Given a novel silent video, our model uses retrieval-augmented generation to create audio that matches the visual content both semantically and temporally. We train and evaluate our model on two in-the-wild egocentric video datasets Ego4D and EPIC-KITCHENS. Our model outperforms an array of existing methods, allows controllable generation of the ambient sound, and even shows promise for generalizing to computer graphics game clips. Overall, our work is the first to focus video-to-audio generation faithfully on the observed visual content despite training from uncurated clips with natural background sounds.","sentences":["Generating realistic audio for human interactions is important for many applications, such as creating sound effects for films or virtual reality games.","Existing approaches implicitly assume total correspondence between the video and audio during training, yet many sounds happen off-screen and have weak to no correspondence with the visuals -- resulting in uncontrolled ambient sounds or hallucinations at test time.","We propose a novel ambient-aware audio generation model, AV-LDM.","We devise a novel audio-conditioning mechanism to learn to disentangle foreground action sounds from the ambient background sounds in in-the-wild training videos.","Given a novel silent video, our model uses retrieval-augmented generation to create audio that matches the visual content both semantically and temporally.","We train and evaluate our model on two in-the-wild egocentric video datasets Ego4D and EPIC-KITCHENS.","Our model outperforms an array of existing methods, allows controllable generation of the ambient sound, and even shows promise for generalizing to computer graphics game clips.","Overall, our work is the first to focus video-to-audio generation faithfully on the observed visual content despite training from uncurated clips with natural background sounds."],"url":"http://arxiv.org/abs/2406.09272v1","category":"cs.CV"}
{"created":"2024-06-13 16:07:27","title":"Cascade of multi-exciton states generated by singlet fission","abstract":"Identifying multi-exciton states generated from singlet fission is key to understanding the carrier multiplication process, which presents a strategy for improving the efficiency of photovoltaics and bio-imaging. Broadband optically detected magnetic resonance is a sensitive technique to detect multi-exciton states. Here we report a dominant species emerging under intense light excitation corresponding to a weakly exchange coupled triplet pair located on adjacent molecules oriented by nearly 90 degrees, contrasting to the pi-stacked triplet pair under low excitation intensity. The weakly coupled species model precisely reproduces the intricate spin transitions in the Hilbert space of the triplet pair. Combining the magneto photoluminescence and high-magnetic field ODMR, we also identify a strongly exchange-coupled state of three triplet excitons formed by photoexcited V2, which manifests through the magnetic field induced level crossings between its quintet and triplet manifolds. The excellent agreement between the experimental Zeeman fan and the two-triplet spin Hamiltonian highlights the potential of multi-exciton states for quantum information processing.","sentences":["Identifying multi-exciton states generated from singlet fission is key to understanding the carrier multiplication process, which presents a strategy for improving the efficiency of photovoltaics and bio-imaging.","Broadband optically detected magnetic resonance is a sensitive technique to detect multi-exciton states.","Here we report a dominant species emerging under intense light excitation corresponding to a weakly exchange coupled triplet pair located on adjacent molecules oriented by nearly 90 degrees, contrasting to the pi-stacked triplet pair under low excitation intensity.","The weakly coupled species model precisely reproduces the intricate spin transitions in the Hilbert space of the triplet pair.","Combining the magneto photoluminescence and high-magnetic field ODMR, we also identify a strongly exchange-coupled state of three triplet excitons formed by photoexcited V2, which manifests through the magnetic field induced level crossings between its quintet and triplet manifolds.","The excellent agreement between the experimental Zeeman fan and the two-triplet spin Hamiltonian highlights the potential of multi-exciton states for quantum information processing."],"url":"http://arxiv.org/abs/2406.09268v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-13 16:06:29","title":"SySTeC: A Symmetric Sparse Tensor Compiler","abstract":"Symmetric and sparse tensors arise naturally in many domains including linear algebra, statistics, physics, chemistry, and graph theory. Symmetric tensors are equal to their transposes, so in the $n$-dimensional case we can save up to a factor of $n!$ by avoiding redundant operations. Sparse tensors, on the other hand, are mostly zero, and we can save asymptotically by processing only nonzeros. Unfortunately, specializing for both symmetry and sparsity at the same time is uniquely challenging. Optimizing for symmetry requires consideration of $n!$ transpositions of a triangular kernel, which can be complex and error prone. Considering multiple transposed iteration orders and triangular loop bounds also complicates iteration through intricate sparse tensor formats. Additionally, since each combination of symmetry and sparse tensor formats requires a specialized implementation, this leads to a combinatorial number of cases. A compiler is needed, but existing compilers cannot take advantage of both symmetry and sparsity within the same kernel. In this paper, we describe the first compiler which can automatically generate symmetry-aware code for sparse or structured tensor kernels. We introduce a taxonomy for symmetry in tensor kernels, and show how to target each kind of symmetry. Our implementation demonstrates significant speedups ranging from 1.36x for SSYMV to 30.4x for a 5-dimensional MTTKRP over the non-symmetric state of the art.","sentences":["Symmetric and sparse tensors arise naturally in many domains including linear algebra, statistics, physics, chemistry, and graph theory.","Symmetric tensors are equal to their transposes, so in the $n$-dimensional case we can save up to a factor of $n!$ by avoiding redundant operations.","Sparse tensors, on the other hand, are mostly zero, and we can save asymptotically by processing only nonzeros.","Unfortunately, specializing for both symmetry and sparsity at the same time is uniquely challenging.","Optimizing for symmetry requires consideration of $n!$ transpositions of a triangular kernel, which can be complex and error prone.","Considering multiple transposed iteration orders and triangular loop bounds also complicates iteration through intricate sparse tensor formats.","Additionally, since each combination of symmetry and sparse tensor formats requires a specialized implementation, this leads to a combinatorial number of cases.","A compiler is needed, but existing compilers cannot take advantage of both symmetry and sparsity within the same kernel.","In this paper, we describe the first compiler which can automatically generate symmetry-aware code for sparse or structured tensor kernels.","We introduce a taxonomy for symmetry in tensor kernels, and show how to target each kind of symmetry.","Our implementation demonstrates significant speedups ranging from 1.36x for SSYMV to 30.4x for a 5-dimensional MTTKRP over the non-symmetric state of the art."],"url":"http://arxiv.org/abs/2406.09266v1","category":"cs.MS"}
{"created":"2024-06-13 16:04:11","title":"Sharing Matters: Analysing Neurons Across Languages and Tasks in LLMs","abstract":"Multilingual large language models (LLMs) have greatly increased the ceiling of performance on non-English tasks. However the mechanisms behind multilingualism in these LLMs are poorly understood. Of particular interest is the degree to which internal representations are shared between languages. Recent work on neuron analysis of LLMs has focused on the monolingual case, and the limited work on the multilingual case has not considered the interaction between tasks and linguistic representations. In our work, we investigate how neuron activation is shared across languages by categorizing neurons into four distinct groups according to their responses across different languages for a particular input: all-shared, partial-shared, specific, and non-activated. This categorization is combined with a study of neuron attribution, i.e. the importance of a neuron w.r.t an output. Our analysis reveals the following insights: (i) the linguistic sharing patterns are strongly affected by the type of task, but neuron behaviour changes across different inputs even for the same task; (ii) all-shared neurons play a key role in generating correct responses; (iii) boosting multilingual alignment by increasing all-shared neurons can enhance accuracy on multilingual tasks. The code is available at https://github.com/weixuan-wang123/multilingual-neurons.","sentences":["Multilingual large language models (LLMs) have greatly increased the ceiling of performance on non-English tasks.","However the mechanisms behind multilingualism in these LLMs are poorly understood.","Of particular interest is the degree to which internal representations are shared between languages.","Recent work on neuron analysis of LLMs has focused on the monolingual case, and the limited work on the multilingual case has not considered the interaction between tasks and linguistic representations.","In our work, we investigate how neuron activation is shared across languages by categorizing neurons into four distinct groups according to their responses across different languages for a particular input: all-shared, partial-shared, specific, and non-activated.","This categorization is combined with a study of neuron attribution, i.e. the importance of a neuron w.r.t an output.","Our analysis reveals the following insights: (i) the linguistic sharing patterns are strongly affected by the type of task, but neuron behaviour changes across different inputs even for the same task; (ii) all-shared neurons play a key role in generating correct responses; (iii) boosting multilingual alignment by increasing all-shared neurons can enhance accuracy on multilingual tasks.","The code is available at https://github.com/weixuan-wang123/multilingual-neurons."],"url":"http://arxiv.org/abs/2406.09265v1","category":"cs.CL"}
{"created":"2024-06-13 16:03:25","title":"Towards Bidirectional Human-AI Alignment: A Systematic Review for Clarifications, Framework, and Future Directions","abstract":"Recent advancements in general-purpose AI have highlighted the importance of guiding AI systems towards the intended goals, ethical principles, and values of individuals and groups, a concept broadly recognized as alignment. However, the lack of clarified definitions and scopes of human-AI alignment poses a significant obstacle, hampering collaborative efforts across research domains to achieve this alignment. In particular, ML- and philosophy-oriented alignment research often views AI alignment as a static, unidirectional process (i.e., aiming to ensure that AI systems' objectives match humans) rather than an ongoing, mutual alignment problem [429]. This perspective largely neglects the long-term interaction and dynamic changes of alignment. To understand these gaps, we introduce a systematic review of over 400 papers published between 2019 and January 2024, spanning multiple domains such as Human-Computer Interaction (HCI), Natural Language Processing (NLP), Machine Learning (ML), and others. We characterize, define and scope human-AI alignment. From this, we present a conceptual framework of \"Bidirectional Human-AI Alignment\" to organize the literature from a human-centered perspective. This framework encompasses both 1) conventional studies of aligning AI to humans that ensures AI produces the intended outcomes determined by humans, and 2) a proposed concept of aligning humans to AI, which aims to help individuals and society adjust to AI advancements both cognitively and behaviorally. Additionally, we articulate the key findings derived from literature analysis, including discussions about human values, interaction techniques, and evaluations. To pave the way for future studies, we envision three key challenges for future directions and propose examples of potential future solutions.","sentences":["Recent advancements in general-purpose AI have highlighted the importance of guiding AI systems towards the intended goals, ethical principles, and values of individuals and groups, a concept broadly recognized as alignment.","However, the lack of clarified definitions and scopes of human-AI alignment poses a significant obstacle, hampering collaborative efforts across research domains to achieve this alignment.","In particular, ML- and philosophy-oriented alignment research often views AI alignment as a static, unidirectional process (i.e., aiming to ensure that AI systems' objectives match humans) rather than an ongoing, mutual alignment problem [429].","This perspective largely neglects the long-term interaction and dynamic changes of alignment.","To understand these gaps, we introduce a systematic review of over 400 papers published between 2019 and January 2024, spanning multiple domains such as Human-Computer Interaction (HCI), Natural Language Processing (NLP), Machine Learning (ML), and others.","We characterize, define and scope human-AI alignment.","From this, we present a conceptual framework of \"Bidirectional Human-AI Alignment\" to organize the literature from a human-centered perspective.","This framework encompasses both 1) conventional studies of aligning AI to humans that ensures AI produces the intended outcomes determined by humans, and 2) a proposed concept of aligning humans to AI, which aims to help individuals and society adjust to AI advancements both cognitively and behaviorally.","Additionally, we articulate the key findings derived from literature analysis, including discussions about human values, interaction techniques, and evaluations.","To pave the way for future studies, we envision three key challenges for future directions and propose examples of potential future solutions."],"url":"http://arxiv.org/abs/2406.09264v1","category":"cs.HC"}
{"created":"2024-06-13 16:03:15","title":"Generative Inverse Design of Crystal Structures via Diffusion Models with Transformers","abstract":"Recent advances in deep learning have enabled the generation of realistic data by training generative models on large datasets of text, images, and audio. While these models have demonstrated exceptional performance in generating novel and plausible data, it remains an open question whether they can effectively accelerate scientific discovery through the data generation and drive significant advancements across various scientific fields. In particular, the discovery of new inorganic materials with promising properties poses a critical challenge, both scientifically and for industrial applications. However, unlike textual or image data, materials, or more specifically crystal structures, consist of multiple types of variables - including lattice vectors, atom positions, and atomic species. This complexity in data give rise to a variety of approaches for representing and generating such data. Consequently, the design choices of generative models for crystal structures remain an open question. In this study, we explore a new type of diffusion model for the generative inverse design of crystal structures, with a backbone based on a Transformer architecture. We demonstrate our models are superior to previous methods in their versatility for generating crystal structures with desired properties. Furthermore, our empirical results suggest that the optimal conditioning methods vary depending on the dataset.","sentences":["Recent advances in deep learning have enabled the generation of realistic data by training generative models on large datasets of text, images, and audio.","While these models have demonstrated exceptional performance in generating novel and plausible data, it remains an open question whether they can effectively accelerate scientific discovery through the data generation and drive significant advancements across various scientific fields.","In particular, the discovery of new inorganic materials with promising properties poses a critical challenge, both scientifically and for industrial applications.","However, unlike textual or image data, materials, or more specifically crystal structures, consist of multiple types of variables - including lattice vectors, atom positions, and atomic species.","This complexity in data give rise to a variety of approaches for representing and generating such data.","Consequently, the design choices of generative models for crystal structures remain an open question.","In this study, we explore a new type of diffusion model for the generative inverse design of crystal structures, with a backbone based on a Transformer architecture.","We demonstrate our models are superior to previous methods in their versatility for generating crystal structures with desired properties.","Furthermore, our empirical results suggest that the optimal conditioning methods vary depending on the dataset."],"url":"http://arxiv.org/abs/2406.09263v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-13 16:01:42","title":"Non-Invertible Surface Defects in 2+1d QFTs from Half Spacetime Gauging","abstract":"We study duality defects in 2+1d theories with $\\bZ^{(0)}_N\\times\\bZ^{(1)}_N$ global symmetry and trivial mixed 't Hooft anomaly. By gauging these symmetries simultaneously in half of the spacetime, we define duality defects for theories that are self-dual under gauging. We calculate the fusion rules involving duality defects and show that they obey a fusion 2-category. We also construct the corresponding symmetry topological field theory, a four-dimensional BF theory on a slab which realizes the duality defects on the boundary upon shrinking the interval. Furthermore, we provide explicit examples of such duality defects in $U(1)\\times U(1)$ gauge theories and in more general product theories. Finally, we find duality defects in non-Lagrangian theories obtained by compactification of 6d $\\cN=(2,0)$ SCFTs of type $A_{N-1}$ on various three-manifolds.","sentences":["We study duality defects in 2+1d theories with $\\bZ^{(0)}_N\\times\\bZ^{(1)}_N$ global symmetry and trivial mixed 't Hooft anomaly.","By gauging these symmetries simultaneously in half of the spacetime, we define duality defects for theories that are self-dual under gauging.","We calculate the fusion rules involving duality defects and show that they obey a fusion 2-category.","We also construct the corresponding symmetry topological field theory, a four-dimensional BF theory on a slab which realizes the duality defects on the boundary upon shrinking the interval.","Furthermore, we provide explicit examples of such duality defects in $U(1)\\times U(1)$ gauge theories and in more general product theories.","Finally, we find duality defects in non-Lagrangian theories obtained by compactification of 6d $\\cN=(2,0)$ SCFTs of type $A_{N-1}$ on various three-manifolds."],"url":"http://arxiv.org/abs/2406.09261v1","category":"hep-th"}
{"created":"2024-06-13 16:01:22","title":"Deep Transformer Network for Monocular Pose Estimation of Ship-Based UAV","abstract":"This paper introduces a deep transformer network for estimating the relative 6D pose of a Unmanned Aerial Vehicle (UAV) with respect to a ship using monocular images. A synthetic dataset of ship images is created and annotated with 2D keypoints of multiple ship parts. A Transformer Neural Network model is trained to detect these keypoints and estimate the 6D pose of each part. The estimates are integrated using Bayesian fusion. The model is tested on synthetic data and in-situ flight experiments, demonstrating robustness and accuracy in various lighting conditions. The position estimation error is approximately 0.8\\% and 1.0\\% of the distance to the ship for the synthetic data and the flight experiments, respectively. The method has potential applications for ship-based autonomous UAV landing and navigation.","sentences":["This paper introduces a deep transformer network for estimating the relative 6D pose of a Unmanned Aerial Vehicle (UAV) with respect to a ship using monocular images.","A synthetic dataset of ship images is created and annotated with 2D keypoints of multiple ship parts.","A Transformer Neural Network model is trained to detect these keypoints and estimate the 6D pose of each part.","The estimates are integrated using Bayesian fusion.","The model is tested on synthetic data and in-situ flight experiments, demonstrating robustness and accuracy in various lighting conditions.","The position estimation error is approximately 0.8\\% and 1.0\\% of the distance to the ship for the synthetic data and the flight experiments, respectively.","The method has potential applications for ship-based autonomous UAV landing and navigation."],"url":"http://arxiv.org/abs/2406.09260v1","category":"cs.CV"}
{"created":"2024-06-13 16:00:03","title":"Extraction of Information from Polarized Deep Exclusive Scattering with Machine Learning","abstract":"A framework defining benchmarks for the analysis of polarized exclusive scattering cross sections is proposed that uses physics symmetry constraints as well as lattice QCD predictions. These constraints are built into machine learning (ML) algorithms. Both physics driven and ML based benchmarks are applied to a wide range of deeply virtual exclusive processes through explainable ML techniques with controllable uncertainties. The observables, namely the Compton Form Factors (CFFs) which are convolutions of Generalized Parton Distributions (GPDs), are extracted using methods such as the random targets method to evaluate the separate contribution of the aleatoric and epistemic uncertainties in exclusive scattering analyses.","sentences":["A framework defining benchmarks for the analysis of polarized exclusive scattering cross sections is proposed that uses physics symmetry constraints as well as lattice QCD predictions.","These constraints are built into machine learning (ML) algorithms.","Both physics driven and ML based benchmarks are applied to a wide range of deeply virtual exclusive processes through explainable ML techniques with controllable uncertainties.","The observables, namely the Compton Form Factors (CFFs) which are convolutions of Generalized Parton Distributions (GPDs), are extracted using methods such as the random targets method to evaluate the separate contribution of the aleatoric and epistemic uncertainties in exclusive scattering analyses."],"url":"http://arxiv.org/abs/2406.09258v1","category":"hep-ph"}
{"created":"2024-06-13 15:58:37","title":"Assessing Model Generalization in Vicinity","abstract":"This paper evaluates the generalization ability of classification models on out-of-distribution test sets without depending on ground truth labels. Common approaches often calculate an unsupervised metric related to a specific model property, like confidence or invariance, which correlates with out-of-distribution accuracy. However, these metrics are typically computed for each test sample individually, leading to potential issues caused by spurious model responses, such as overly high or low confidence. To tackle this challenge, we propose incorporating responses from neighboring test samples into the correctness assessment of each individual sample. In essence, if a model consistently demonstrates high correctness scores for nearby samples, it increases the likelihood of correctly predicting the target sample, and vice versa. The resulting scores are then averaged across all test samples to provide a holistic indication of model accuracy. Developed under the vicinal risk formulation, this approach, named vicinal risk proxy (VRP), computes accuracy without relying on labels. We show that applying the VRP method to existing generalization indicators, such as average confidence and effective invariance, consistently improves over these baselines both methodologically and experimentally. This yields a stronger correlation with model accuracy, especially on challenging out-of-distribution test sets.","sentences":["This paper evaluates the generalization ability of classification models on out-of-distribution test sets without depending on ground truth labels.","Common approaches often calculate an unsupervised metric related to a specific model property, like confidence or invariance, which correlates with out-of-distribution accuracy.","However, these metrics are typically computed for each test sample individually, leading to potential issues caused by spurious model responses, such as overly high or low confidence.","To tackle this challenge, we propose incorporating responses from neighboring test samples into the correctness assessment of each individual sample.","In essence, if a model consistently demonstrates high correctness scores for nearby samples, it increases the likelihood of correctly predicting the target sample, and vice versa.","The resulting scores are then averaged across all test samples to provide a holistic indication of model accuracy.","Developed under the vicinal risk formulation, this approach, named vicinal risk proxy (VRP), computes accuracy without relying on labels.","We show that applying the VRP method to existing generalization indicators, such as average confidence and effective invariance, consistently improves over these baselines both methodologically and experimentally.","This yields a stronger correlation with model accuracy, especially on challenging out-of-distribution test sets."],"url":"http://arxiv.org/abs/2406.09257v1","category":"cs.LG"}
{"created":"2024-06-13 15:57:16","title":"General Bayesian Predictive Synthesis","abstract":"This study investigates Bayesian ensemble learning for improving the quality of decision-making. We consider a decision-maker who selects an action from a set of candidates based on a policy trained using observations. In our setting, we assume the existence of experts who provide predictive distributions based on their own policies. Our goal is to integrate these predictive distributions within the Bayesian framework. Our proposed method, which we refer to as General Bayesian Predictive Synthesis (GBPS), is characterized by a loss minimization framework and does not rely on parameter estimation, unlike existing studies. Inspired by Bayesian predictive synthesis and general Bayes frameworks, we evaluate the performance of our proposed method through simulation studies.","sentences":["This study investigates Bayesian ensemble learning for improving the quality of decision-making.","We consider a decision-maker who selects an action from a set of candidates based on a policy trained using observations.","In our setting, we assume the existence of experts who provide predictive distributions based on their own policies.","Our goal is to integrate these predictive distributions within the Bayesian framework.","Our proposed method, which we refer to as General Bayesian Predictive Synthesis (GBPS), is characterized by a loss minimization framework and does not rely on parameter estimation, unlike existing studies.","Inspired by Bayesian predictive synthesis and general Bayes frameworks, we evaluate the performance of our proposed method through simulation studies."],"url":"http://arxiv.org/abs/2406.09254v1","category":"stat.ME"}
{"created":"2024-06-13 15:55:43","title":"The Tolman-Ehrenfest criterion of thermal equilibrium in scalar-tensor gravity","abstract":"The Tolman-Ehrenfest criterion for the thermal equilibrium of a fluid at rest in a static general-relativistic geometry is generalized to scalar-tensor gravity. The gravitational scalar field, which fixes the strength of the effective gravitational coupling, plays a role in determining thermal equilibrium. As a result, heat sinks more in a gravitational field where the geometry is more curved, but also where gravity is stronger.","sentences":["The Tolman-Ehrenfest criterion for the thermal equilibrium of a fluid at rest in a static general-relativistic geometry is generalized to scalar-tensor gravity.","The gravitational scalar field, which fixes the strength of the effective gravitational coupling, plays a role in determining thermal equilibrium.","As a result, heat sinks more in a gravitational field where the geometry is more curved, but also where gravity is stronger."],"url":"http://arxiv.org/abs/2406.09251v1","category":"gr-qc"}
{"created":"2024-06-13 15:55:04","title":"MirrorCheck: Efficient Adversarial Defense for Vision-Language Models","abstract":"Vision-Language Models (VLMs) are becoming increasingly vulnerable to adversarial attacks as various novel attack strategies are being proposed against these models. While existing defenses excel in unimodal contexts, they currently fall short in safeguarding VLMs against adversarial threats. To mitigate this vulnerability, we propose a novel, yet elegantly simple approach for detecting adversarial samples in VLMs. Our method leverages Text-to-Image (T2I) models to generate images based on captions produced by target VLMs. Subsequently, we calculate the similarities of the embeddings of both input and generated images in the feature space to identify adversarial samples. Empirical evaluations conducted on different datasets validate the efficacy of our approach, outperforming baseline methods adapted from image classification domains. Furthermore, we extend our methodology to classification tasks, showcasing its adaptability and model-agnostic nature. Theoretical analyses and empirical findings also show the resilience of our approach against adaptive attacks, positioning it as an excellent defense mechanism for real-world deployment against adversarial threats.","sentences":["Vision-Language Models (VLMs) are becoming increasingly vulnerable to adversarial attacks as various novel attack strategies are being proposed against these models.","While existing defenses excel in unimodal contexts, they currently fall short in safeguarding VLMs against adversarial threats.","To mitigate this vulnerability, we propose a novel, yet elegantly simple approach for detecting adversarial samples in VLMs.","Our method leverages Text-to-Image (T2I) models to generate images based on captions produced by target VLMs.","Subsequently, we calculate the similarities of the embeddings of both input and generated images in the feature space to identify adversarial samples.","Empirical evaluations conducted on different datasets validate the efficacy of our approach, outperforming baseline methods adapted from image classification domains.","Furthermore, we extend our methodology to classification tasks, showcasing its adaptability and model-agnostic nature.","Theoretical analyses and empirical findings also show the resilience of our approach against adaptive attacks, positioning it as an excellent defense mechanism for real-world deployment against adversarial threats."],"url":"http://arxiv.org/abs/2406.09250v1","category":"cs.CV"}
{"created":"2024-06-13 15:49:55","title":"Wigner non-negative states that verify the Wigner entropy conjecture","abstract":"We present further progress, in the form of analytical results, on the Wigner entropy conjecture set forth in https://link.aps.org/doi/10.1103/PhysRevA.104.042211 and https://iopscience.iop.org/article/10.1088/1751-8121/aa852f/meta. Said conjecture asserts that the differential entropy defined for non-negative, yet physical, Wigner functions is minimized by pure Gaussian states while the minimum entropy is equal to $1+\\ln\\pi$. We prove this conjecture for the qubits formed by Fock states $|0\\rangle$ and $|1\\rangle$ that correspond to non-negative Wigner functions. In particular, we derive an explicit form of the Wigner entropy for those states lying on the boundary of the set of Wigner non-negative qubits. We then consider general mixed states and derive a sufficient condition for Wigner non-negativity. For states satisfying our condition we verify that the conjecture is true. Lastly, we elaborate on the states of the set which is in accordance with our condition.","sentences":["We present further progress, in the form of analytical results, on the Wigner entropy conjecture set forth in https://link.aps.org/doi/10.1103/PhysRevA.104.042211 and https://iopscience.iop.org/article/10.1088/1751-8121/aa852f/meta.","Said conjecture asserts that the differential entropy defined for non-negative, yet physical, Wigner functions is minimized by pure Gaussian states while the minimum entropy is equal to $1+\\ln\\pi$. We prove this conjecture for the qubits formed by Fock states $|0\\rangle$ and $|1\\rangle$ that correspond to non-negative Wigner functions.","In particular, we derive an explicit form of the Wigner entropy for those states lying on the boundary of the set of Wigner non-negative qubits.","We then consider general mixed states and derive a sufficient condition for Wigner non-negativity.","For states satisfying our condition we verify that the conjecture is true.","Lastly, we elaborate on the states of the set which is in accordance with our condition."],"url":"http://arxiv.org/abs/2406.09248v1","category":"quant-ph"}
{"created":"2024-06-13 15:46:55","title":"OpenVLA: An Open-Source Vision-Language-Action Model","abstract":"Large policies pretrained on a combination of Internet-scale vision-language data and diverse robot demonstrations have the potential to change how we teach robots new skills: rather than training new behaviors from scratch, we can fine-tune such vision-language-action (VLA) models to obtain robust, generalizable policies for visuomotor control. Yet, widespread adoption of VLAs for robotics has been challenging as 1) existing VLAs are largely closed and inaccessible to the public, and 2) prior work fails to explore methods for efficiently fine-tuning VLAs for new tasks, a key component for adoption. Addressing these challenges, we introduce OpenVLA, a 7B-parameter open-source VLA trained on a diverse collection of 970k real-world robot demonstrations. OpenVLA builds on a Llama 2 language model combined with a visual encoder that fuses pretrained features from DINOv2 and SigLIP. As a product of the added data diversity and new model components, OpenVLA demonstrates strong results for generalist manipulation, outperforming closed models such as RT-2-X (55B) by 16.5% in absolute task success rate across 29 tasks and multiple robot embodiments, with 7x fewer parameters. We further show that we can effectively fine-tune OpenVLA for new settings, with especially strong generalization results in multi-task environments involving multiple objects and strong language grounding abilities, and outperform expressive from-scratch imitation learning methods such as Diffusion Policy by 20.4%. We also explore compute efficiency; as a separate contribution, we show that OpenVLA can be fine-tuned on consumer GPUs via modern low-rank adaptation methods and served efficiently via quantization without a hit to downstream success rate. Finally, we release model checkpoints, fine-tuning notebooks, and our PyTorch codebase with built-in support for training VLAs at scale on Open X-Embodiment datasets.","sentences":["Large policies pretrained on a combination of Internet-scale vision-language data and diverse robot demonstrations have the potential to change how we teach robots new skills: rather than training new behaviors from scratch, we can fine-tune such vision-language-action (VLA) models to obtain robust, generalizable policies for visuomotor control.","Yet, widespread adoption of VLAs for robotics has been challenging as 1) existing VLAs are largely closed and inaccessible to the public, and 2) prior work fails to explore methods for efficiently fine-tuning VLAs for new tasks, a key component for adoption.","Addressing these challenges, we introduce OpenVLA, a 7B-parameter open-source VLA trained on a diverse collection of 970k real-world robot demonstrations.","OpenVLA builds on a Llama 2 language model combined with a visual encoder that fuses pretrained features from DINOv2 and SigLIP.","As a product of the added data diversity and new model components, OpenVLA demonstrates strong results for generalist manipulation, outperforming closed models such as RT-2-X (55B) by 16.5% in absolute task success rate across 29 tasks and multiple robot embodiments, with 7x fewer parameters.","We further show that we can effectively fine-tune OpenVLA for new settings, with especially strong generalization results in multi-task environments involving multiple objects and strong language grounding abilities, and outperform expressive from-scratch imitation learning methods such as Diffusion Policy by 20.4%.","We also explore compute efficiency; as a separate contribution, we show that OpenVLA can be fine-tuned on consumer GPUs via modern low-rank adaptation methods and served efficiently via quantization without a hit to downstream success rate.","Finally, we release model checkpoints, fine-tuning notebooks, and our PyTorch codebase with built-in support for training VLAs at scale on Open X-Embodiment datasets."],"url":"http://arxiv.org/abs/2406.09246v1","category":"cs.RO"}
{"created":"2024-06-13 15:46:44","title":"Final-state interactions in neutrino-induced proton knockout from argon in MicroBooNE","abstract":"Neutrino event generators make use of intranuclear cascade models (INCs), to predict the kinematics of hadrons produced in neutrino-nucleus interactions. We perform a consistent comparison of different INCs, by using the same set of events as input to the NEUT, NuWro, Achilles and INCL INCs. The inputs correspond to calculations of the fully differential single-proton knockout cross section, either in the distorted-wave impulse approximation (DWIA) or plane-wave impulse approximation (PWIA), both including realistic nuclear hole spectral functions. We compare the INC results to DWIA calculations with an optical potential, used extensively in the analysis of (e,e'p) experiments. We point out a systematic discrepancy between both approaches. We apply the INC results to recent MicroBooNE data. We assess the influence of the choice of spectral function, finding that large variations in realistic spectral functions are indistinguishable with present data. The data is underpredicted, with strength missing in the region where two-nucleon knockout and resonance production contribute. However, the data is underpredicted also in regions of low transverse missing momentum, where one-nucleon knockout dominates. The inclusion of the interference with two-body currents could lead to additional strength in this region.","sentences":["Neutrino event generators make use of intranuclear cascade models (INCs), to predict the kinematics of hadrons produced in neutrino-nucleus interactions.","We perform a consistent comparison of different INCs, by using the same set of events as input to the NEUT, NuWro, Achilles and INCL INCs.","The inputs correspond to calculations of the fully differential single-proton knockout cross section, either in the distorted-wave impulse approximation (DWIA) or plane-wave impulse approximation (PWIA), both including realistic nuclear hole spectral functions.","We compare the INC results to DWIA calculations with an optical potential, used extensively in the analysis of (e,e'p) experiments.","We point out a systematic discrepancy between both approaches.","We apply the INC results to recent MicroBooNE data.","We assess the influence of the choice of spectral function, finding that large variations in realistic spectral functions are indistinguishable with present data.","The data is underpredicted, with strength missing in the region where two-nucleon knockout and resonance production contribute.","However, the data is underpredicted also in regions of low transverse missing momentum, where one-nucleon knockout dominates.","The inclusion of the interference with two-body currents could lead to additional strength in this region."],"url":"http://arxiv.org/abs/2406.09244v1","category":"nucl-th"}
{"created":"2024-06-13 15:46:27","title":"Towards a Characterisation of Monte-Carlo Tree Search Performance in Different Games","abstract":"Many enhancements to Monte-Carlo Tree Search (MCTS) have been proposed over almost two decades of general game playing and other artificial intelligence research. However, our ability to characterise and understand which variants work well or poorly in which games is still lacking. This paper describes work on an initial dataset that we have built to make progress towards such an understanding: 268,386 plays among 61 different agents across 1494 distinct games. We describe a preliminary analysis and work on training predictive models on this dataset, as well as lessons learned and future plans for a new and improved version of the dataset.","sentences":["Many enhancements to Monte-Carlo Tree Search (MCTS) have been proposed over almost two decades of general game playing and other artificial intelligence research.","However, our ability to characterise and understand which variants work well or poorly in which games is still lacking.","This paper describes work on an initial dataset that we have built to make progress towards such an understanding: 268,386 plays among 61 different agents across 1494 distinct games.","We describe a preliminary analysis and work on training predictive models on this dataset, as well as lessons learned and future plans for a new and improved version of the dataset."],"url":"http://arxiv.org/abs/2406.09242v1","category":"cs.AI"}
{"created":"2024-06-13 15:44:23","title":"What is the long-run distribution of stochastic gradient descent? A large deviations analysis","abstract":"In this paper, we examine the long-run distribution of stochastic gradient descent (SGD) in general, non-convex problems. Specifically, we seek to understand which regions of the problem's state space are more likely to be visited by SGD, and by how much. Using an approach based on the theory of large deviations and randomly perturbed dynamical systems, we show that the long-run distribution of SGD resembles the Boltzmann-Gibbs distribution of equilibrium thermodynamics with temperature equal to the method's step-size and energy levels determined by the problem's objective and the statistics of the noise. In particular, we show that, in the long run, (a) the problem's critical region is visited exponentially more often than any non-critical region; (b) the iterates of SGD are exponentially concentrated around the problem's minimum energy state (which does not always coincide with the global minimum of the objective); (c) all other connected components of critical points are visited with frequency that is exponentially proportional to their energy level; and, finally (d) any component of local maximizers or saddle points is \"dominated\" by a component of local minimizers which is visited exponentially more often.","sentences":["In this paper, we examine the long-run distribution of stochastic gradient descent (SGD) in general, non-convex problems.","Specifically, we seek to understand which regions of the problem's state space are more likely to be visited by SGD, and by how much.","Using an approach based on the theory of large deviations and randomly perturbed dynamical systems, we show that the long-run distribution of SGD resembles the Boltzmann-Gibbs distribution of equilibrium thermodynamics with temperature equal to the method's step-size and energy levels determined by the problem's objective and the statistics of the noise.","In particular, we show that, in the long run, (a) the problem's critical region is visited exponentially more often than any non-critical region; (b) the iterates of SGD are exponentially concentrated around the problem's minimum energy state (which does not always coincide with the global minimum of the objective); (c) all other connected components of critical points are visited with frequency that is exponentially proportional to their energy level; and, finally (d) any component of local maximizers or saddle points is \"dominated\" by a component of local minimizers which is visited exponentially more often."],"url":"http://arxiv.org/abs/2406.09241v1","category":"math.OC"}
{"created":"2024-06-13 15:43:59","title":"Comparison Visual Instruction Tuning","abstract":"Comparing two images in terms of Commonalities and Differences (CaD) is a fundamental human capability that forms the basis of advanced visual reasoning and interpretation. It is essential for the generation of detailed and contextually relevant descriptions, performing comparative analysis, novelty detection, and making informed decisions based on visual data. However, surprisingly, little attention has been given to these fundamental concepts in the best current mimic of human visual intelligence - Large Multimodal Models (LMMs). We develop and contribute a new two-phase approach CaD-VI for collecting synthetic visual instructions, together with an instruction-following dataset CaD-Inst containing 349K image pairs with CaD instructions collected using CaD-VI. Our approach significantly improves the CaD spotting capabilities in LMMs, advancing the SOTA on a diverse set of related tasks by up to 17.5%. It is also complementary to existing difference-only instruction datasets, allowing automatic targeted refinement of those resources increasing their effectiveness for CaD tuning by up to 10%. Additionally, we propose an evaluation benchmark with 7.5K open-ended QAs to assess the CaD understanding abilities of LMMs.","sentences":["Comparing two images in terms of Commonalities and Differences (CaD) is a fundamental human capability that forms the basis of advanced visual reasoning and interpretation.","It is essential for the generation of detailed and contextually relevant descriptions, performing comparative analysis, novelty detection, and making informed decisions based on visual data.","However, surprisingly, little attention has been given to these fundamental concepts in the best current mimic of human visual intelligence - Large Multimodal Models (LMMs).","We develop and contribute a new two-phase approach CaD-VI for collecting synthetic visual instructions, together with an instruction-following dataset CaD-Inst containing 349K image pairs with CaD instructions collected using CaD-VI.","Our approach significantly improves the CaD spotting capabilities in LMMs, advancing the SOTA on a diverse set of related tasks by up to 17.5%.","It is also complementary to existing difference-only instruction datasets, allowing automatic targeted refinement of those resources increasing their effectiveness for CaD tuning by up to 10%.","Additionally, we propose an evaluation benchmark with 7.5K open-ended QAs to assess the CaD understanding abilities of LMMs."],"url":"http://arxiv.org/abs/2406.09240v1","category":"cs.CV"}
{"created":"2024-06-13 15:37:22","title":"Two sides of the same coin: the F-statistic and the 5-vector method","abstract":"This work explores the relationship between two data-analysis methods used in the search for continuous gravitational waves in LIGO-Virgo-KAGRA data: the $\\mathcal{F}$-statistic and the 5-vector method. We show that the 5-vector method can be derived from a maximum likelihood framework similar to the $\\mathcal{F}$-statistic. Our analysis demonstrates that the two methods are statistically equivalent, providing the same detection probability for a given false alarm rate. We extend this comparison to multiple detectors, highlighting differences from the standard approach that simply combines 5-vectors from each detector. In our maximum likelihood approach, each 5-vector is weighted by the observation time and sensitivity of its respective detector, resulting in efficient estimators and analytical distributions for the detection statistic. Additionally, we present the analytical computation of sensitivity for different searches, expressed in terms of the minimum detectable amplitude.","sentences":["This work explores the relationship between two data-analysis methods used in the search for continuous gravitational waves in LIGO-Virgo-KAGRA data: the $\\mathcal{F}$-statistic and the 5-vector method.","We show that the 5-vector method can be derived from a maximum likelihood framework similar to the $\\mathcal{F}$-statistic.","Our analysis demonstrates that the two methods are statistically equivalent, providing the same detection probability for a given false alarm rate.","We extend this comparison to multiple detectors, highlighting differences from the standard approach that simply combines 5-vectors from each detector.","In our maximum likelihood approach, each 5-vector is weighted by the observation time and sensitivity of its respective detector, resulting in efficient estimators and analytical distributions for the detection statistic.","Additionally, we present the analytical computation of sensitivity for different searches, expressed in terms of the minimum detectable amplitude."],"url":"http://arxiv.org/abs/2406.09236v1","category":"gr-qc"}
{"created":"2024-06-13 15:33:52","title":"Sparse reconstruction in spin systems II: Ising and other factor of IID measures","abstract":"For a sequence of Boolean functions $f_n : \\{-1, 1\\}^{V_n} \\longrightarrow \\{-1, 1\\}$, with random input given by some probability measure $\\mathbb{P}_n$, we say that there is sparse reconstruction for $f_n$ if there is a sequence of subsets $U_n \\subseteq V_n$ of coordinates satisfying $|U_n| = o(|V_n|)$ such that knowing the spins in $U_n$ gives us a non-vanishing amount of information about the value of $f_n$.   In the first part of this work, we showed that if the $\\mathbb{P}_n$s are product measures, then no sparse reconstruction is possible for any sequence of transitive functions. In this sequel, we consider spin systems that are relatives of IID measures in one way or another, with our main focus being on the Ising model on finite transitive graphs or exhaustions of lattices. We prove that no sparse reconstruction is possible for the entire high temperature regime on Euclidean boxes and the Curie-Weiss model, while sparse reconstruction for the majority function of the spins is possible in the critical and low temperature regimes. We give quantitative bounds for two-dimensional boxes and the Curie-Weiss model, sharp in the latter case.   The proofs employ several different methods, including factor of IID and FK random cluster representations, strong spatial mixing, a generalization of discrete Fourier analysis to Divide-and-Color models, and entropy inequalities.","sentences":["For a sequence of Boolean functions $f_n : \\{-1, 1\\}^{V_n} \\longrightarrow \\{-1, 1\\}$, with random input given by some probability measure $\\mathbb{P}_n$, we say that there is sparse reconstruction for $f_n$ if there is a sequence of subsets $U_n \\subseteq V_n$ of coordinates satisfying $|U_n| = o(|V_n|)$ such that knowing the spins in $U_n$ gives us a non-vanishing amount of information about the value of $f_n$.   In the first part of this work, we showed that if the $\\mathbb{P}_n$s are product measures, then no sparse reconstruction is possible for any sequence of transitive functions.","In this sequel, we consider spin systems that are relatives of IID measures in one way or another, with our main focus being on the Ising model on finite transitive graphs or exhaustions of lattices.","We prove that no sparse reconstruction is possible for the entire high temperature regime on Euclidean boxes and the Curie-Weiss model, while sparse reconstruction for the majority function of the spins is possible in the critical and low temperature regimes.","We give quantitative bounds for two-dimensional boxes and the Curie-Weiss model, sharp in the latter case.   ","The proofs employ several different methods, including factor of IID and FK random cluster representations, strong spatial mixing, a generalization of discrete Fourier analysis to Divide-and-Color models, and entropy inequalities."],"url":"http://arxiv.org/abs/2406.09232v1","category":"math.PR"}
{"created":"2024-06-13 15:30:16","title":"Correlations and Signaling in the Schr\u00f6dinger-Newton Model","abstract":"The Schr\\\"odinger-Newton model is a semi-classical theory in which, in addition to mutual attraction, massive quantum particles interact with their own gravitational fields. While there are many studies on the phenomenology of single particles, correlation dynamics in multipartite systems is largely unexplored. Here, we show that the Schr\\\"odinger-Newton interactions preserve the product form of initial states, yet on average it agrees with classical mechanics of continuous mass distributions. This leads to a simple test of the model, based on verifying bipartite gravitational evolution towards non-product states. We show using standard quantum mechanics that, with currently accessible single-particle parameters, two masses released from harmonic traps get correlated well before any observable entanglement is accumulated. Therefore, the Schr\\\"odinger-Newton model can be tested with setups aimed at observation of gravitational entanglement with significantly relaxed requirements on coherence time. We also present a mixed-state extension of the model that avoids superluminal signaling.","sentences":["The Schr\\\"odinger-Newton model is a semi-classical theory in which, in addition to mutual attraction, massive quantum particles interact with their own gravitational fields.","While there are many studies on the phenomenology of single particles, correlation dynamics in multipartite systems is largely unexplored.","Here, we show that the Schr\\\"odinger-Newton interactions preserve the product form of initial states, yet on average it agrees with classical mechanics of continuous mass distributions.","This leads to a simple test of the model, based on verifying bipartite gravitational evolution towards non-product states.","We show using standard quantum mechanics that, with currently accessible single-particle parameters, two masses released from harmonic traps get correlated well before any observable entanglement is accumulated.","Therefore, the Schr\\\"odinger-Newton model can be tested with setups aimed at observation of gravitational entanglement with significantly relaxed requirements on coherence time.","We also present a mixed-state extension of the model that avoids superluminal signaling."],"url":"http://arxiv.org/abs/2406.09230v1","category":"quant-ph"}
{"created":"2024-06-13 15:29:37","title":"MGRQ: Post-Training Quantization For Vision Transformer With Mixed Granularity Reconstruction","abstract":"Post-training quantization (PTQ) efficiently compresses vision models, but unfortunately, it accompanies a certain degree of accuracy degradation. Reconstruction methods aim to enhance model performance by narrowing the gap between the quantized model and the full-precision model, often yielding promising results. However, efforts to significantly improve the performance of PTQ through reconstruction in the Vision Transformer (ViT) have shown limited efficacy. In this paper, we conduct a thorough analysis of the reasons for this limited effectiveness and propose MGRQ (Mixed Granularity Reconstruction Quantization) as a solution to address this issue. Unlike previous reconstruction schemes, MGRQ introduces a mixed granularity reconstruction approach. Specifically, MGRQ enhances the performance of PTQ by introducing Extra-Block Global Supervision and Intra-Block Local Supervision, building upon Optimized Block-wise Reconstruction. Extra-Block Global Supervision considers the relationship between block outputs and the model's output, aiding block-wise reconstruction through global supervision. Meanwhile, Intra-Block Local Supervision reduces generalization errors by aligning the distribution of outputs at each layer within a block. Subsequently, MGRQ is further optimized for reconstruction through Mixed Granularity Loss Fusion. Extensive experiments conducted on various ViT models illustrate the effectiveness of MGRQ. Notably, MGRQ demonstrates robust performance in low-bit quantization, thereby enhancing the practicality of the quantized model.","sentences":["Post-training quantization (PTQ) efficiently compresses vision models, but unfortunately, it accompanies a certain degree of accuracy degradation.","Reconstruction methods aim to enhance model performance by narrowing the gap between the quantized model and the full-precision model, often yielding promising results.","However, efforts to significantly improve the performance of PTQ through reconstruction in the Vision Transformer (ViT) have shown limited efficacy.","In this paper, we conduct a thorough analysis of the reasons for this limited effectiveness and propose MGRQ (Mixed Granularity Reconstruction Quantization) as a solution to address this issue.","Unlike previous reconstruction schemes, MGRQ introduces a mixed granularity reconstruction approach.","Specifically, MGRQ enhances the performance of PTQ by introducing Extra-Block Global Supervision and Intra-Block Local Supervision, building upon Optimized Block-wise Reconstruction.","Extra-Block Global Supervision considers the relationship between block outputs and the model's output, aiding block-wise reconstruction through global supervision.","Meanwhile, Intra-Block Local Supervision reduces generalization errors by aligning the distribution of outputs at each layer within a block.","Subsequently, MGRQ is further optimized for reconstruction through Mixed Granularity Loss Fusion.","Extensive experiments conducted on various ViT models illustrate the effectiveness of MGRQ.","Notably, MGRQ demonstrates robust performance in low-bit quantization, thereby enhancing the practicality of the quantized model."],"url":"http://arxiv.org/abs/2406.09229v1","category":"cs.CV"}
{"created":"2024-06-13 15:28:21","title":"Measuring gravitational wave memory with LISA","abstract":"Gravitational wave (GW) astronomy has revolutionized our capacity to explore nature. The next generation of observatories, among which the space-borne detector Laser Interferometer Space Antenna LISA, is expected to yield orders of magnitude of signal-to-noise ratio improvement, and reach fainter and novel features of General Relativity. Among them, an exciting possibility is the detection of GW memory. Interpreted as a permanent deformation of the background spacetime after a GW perturbation has passed through the detector, GW memory offers a novel avenue to proof-test General Relativity, access the non-linear nature of gravity, and provide complementary information for a better characterization of the GW source. Previous studies have shown that GW memory detection from individual mergers of massive black hole binaries is expected with LISA. However, these works have not simulated the proper time domain response of the detector to the GW memory. This work is filling this gap and presents the detection prospects of LISA regarding GW memory and the expected signature of GW memory on the data-streams using the most up-to-date LISA consortium simulations of the response, as well as GW memory time-series computation inherited from numerical relativity. We will confront the LISA observation window to massive black hole binary mergers' population forecasted with the state-of-the-art population models and evaluate the odds and the expected accuracies regarding GW memory observations in the LISA lifetime. We conclude that GW memory will be a key feature of several events detected by LISA, and will help to exploit the scientific potential of the mission fully.","sentences":["Gravitational wave (GW) astronomy has revolutionized our capacity to explore nature.","The next generation of observatories, among which the space-borne detector Laser Interferometer Space Antenna LISA, is expected to yield orders of magnitude of signal-to-noise ratio improvement, and reach fainter and novel features of General Relativity.","Among them, an exciting possibility is the detection of GW memory.","Interpreted as a permanent deformation of the background spacetime after a GW perturbation has passed through the detector, GW memory offers a novel avenue to proof-test General Relativity, access the non-linear nature of gravity, and provide complementary information for a better characterization of the GW source.","Previous studies have shown that GW memory detection from individual mergers of massive black hole binaries is expected with LISA.","However, these works have not simulated the proper time domain response of the detector to the GW memory.","This work is filling this gap and presents the detection prospects of LISA regarding GW memory and the expected signature of GW memory on the data-streams using the most up-to-date LISA consortium simulations of the response, as well as GW memory time-series computation inherited from numerical relativity.","We will confront the LISA observation window to massive black hole binary mergers' population forecasted with the state-of-the-art population models and evaluate the odds and the expected accuracies regarding GW memory observations in the LISA lifetime.","We conclude that GW memory will be a key feature of several events detected by LISA, and will help to exploit the scientific potential of the mission fully."],"url":"http://arxiv.org/abs/2406.09228v1","category":"gr-qc"}
{"created":"2024-06-13 15:27:32","title":"How & Why To Use Audience Segmentation to Maximize (Listener) Demand Across Digital Music Portfolio","abstract":"Digital delivery of songs has radically changed the way people can enjoy music, the sort of music available for listening, and the manner by which rights holders are compensated for their contributions to songs. Listeners enjoy an unlimited potpourri of sounds, uniquely free of incremental acquisition or switching costs which have been replaced by subscription or rentier fees. This regime shift has revealed listening patterns governed by affinity, boredom, attention budget, etc.: instantaneous, dynamic, organic or programmatic song selection. This regime shift in demand availability -- with the commensurate translation of revenue implications -- deprecates current orthodoxy for content curation. The impulse to point-of-sale model is insufficient in a regime where demand revenue is proportional to demand affinity and each are strongly dependent time series processes. We explore strategies & implications -- which are generalizable to any media rights holding firm -- from a prediction & optimization point of view for two straightforward demand models.","sentences":["Digital delivery of songs has radically changed the way people can enjoy music, the sort of music available for listening, and the manner by which rights holders are compensated for their contributions to songs.","Listeners enjoy an unlimited potpourri of sounds, uniquely free of incremental acquisition or switching costs which have been replaced by subscription or rentier fees.","This regime shift has revealed listening patterns governed by affinity, boredom, attention budget, etc.: instantaneous, dynamic, organic or programmatic song selection.","This regime shift in demand availability -- with the commensurate translation of revenue implications -- deprecates current orthodoxy for content curation.","The impulse to point-of-sale model is insufficient in a regime where demand revenue is proportional to demand affinity and each are strongly dependent time series processes.","We explore strategies & implications -- which are generalizable to any media rights holding firm -- from a prediction & optimization point of view for two straightforward demand models."],"url":"http://arxiv.org/abs/2406.09226v1","category":"econ.GN"}
{"created":"2024-06-13 15:26:19","title":"Cascaded injection locking of optomechanical crystal oscillators","abstract":"Optomechanical oscillators stand out as high-performance and versatile candidates for serving as reference clocks in sequential photonic integrated circuits. Indeed, they have the unique capability of simultaneously generating mechanical tones and optical signal modulations at frequencies determined by their geometrical design. In this context, the concept of synchronization introduces a powerful means to precisely coordinate the dynamics of multiple oscillators in a controlled manner, thus increasing efficiency and preventing errors in signal processing photonic systems or communication interfaces. In this work, we demonstrate the cascaded injection locking of a pair of silicon-based optomechanical crystal cavities to an external reference signal that subtly modulates the laser driving one of the oscillators. Both cavities interact solely through a weak mechanical link, making the extension of this synchronization mechanism to an increased number of optomechanical oscillators within a common chip more feasible than relying solely on optical interactions. Thus, the combination of the obtained results, supported by a numerical model, with remote optical injection locking schemes discussed in the literature, lays the groundwork for the distribution of reference signals within large networks of processing elements in future phonon-photon hybrid circuits.","sentences":["Optomechanical oscillators stand out as high-performance and versatile candidates for serving as reference clocks in sequential photonic integrated circuits.","Indeed, they have the unique capability of simultaneously generating mechanical tones and optical signal modulations at frequencies determined by their geometrical design.","In this context, the concept of synchronization introduces a powerful means to precisely coordinate the dynamics of multiple oscillators in a controlled manner, thus increasing efficiency and preventing errors in signal processing photonic systems or communication interfaces.","In this work, we demonstrate the cascaded injection locking of a pair of silicon-based optomechanical crystal cavities to an external reference signal that subtly modulates the laser driving one of the oscillators.","Both cavities interact solely through a weak mechanical link, making the extension of this synchronization mechanism to an increased number of optomechanical oscillators within a common chip more feasible than relying solely on optical interactions.","Thus, the combination of the obtained results, supported by a numerical model, with remote optical injection locking schemes discussed in the literature, lays the groundwork for the distribution of reference signals within large networks of processing elements in future phonon-photon hybrid circuits."],"url":"http://arxiv.org/abs/2406.09224v1","category":"physics.optics"}
{"created":"2024-06-13 15:17:31","title":"Convergence and error control of consistent PINNs for elliptic PDEs","abstract":"We provide an a priori analysis of a certain class of numerical methods, commonly referred to as collocation methods, for solving elliptic boundary value problems. They begin with information in the form of point values of the right side f of such equations and point values of the boundary function g and utilize only this information to numerically approximate the solution u of the Partial Differential Equation (PDE). For such a method to provide an approximation to u with guaranteed error bounds, additional assumptions on f and g, called model class assumptions, are needed. We determine the best error (in the energy norm) of approximating u, in terms of the number of point samples m, under all Besov class model assumptions for the right hand side $f$ and boundary g.   We then turn to the study of numerical procedures and asks whether a proposed numerical procedure (nearly) achieves the optimal recovery error. We analyze numerical methods which generate the numerical approximation to $u$ by minimizing a specified data driven loss function over a set $\\Sigma$ which is either a finite dimensional linear space, or more generally, a finite dimensional manifold. We show that the success of such a procedure depends critically on choosing a correct data driven loss function that is consistent with the PDE and provides sharp error control. Based on this analysis a loss function $L^*$ is proposed.   We also address the recent methods of Physics Informed Neural Networks (PINNs). Minimization of the new loss $L^*$ over neural network spaces $\\Sigma$ is referred to as consistent PINNs (CPINNs). We prove that CPINNs provides an optimal recovery of the solution $u$, provided that the optimization problem can be numerically executed and $\\Sigma$ has sufficient approximation capabilities. Finally, numerical examples illustrating the benefits of the CPINNs are given.","sentences":["We provide an a priori analysis of a certain class of numerical methods, commonly referred to as collocation methods, for solving elliptic boundary value problems.","They begin with information in the form of point values of the right side f of such equations and point values of the boundary function g and utilize only this information to numerically approximate the solution u of the Partial Differential Equation (PDE).","For such a method to provide an approximation to u with guaranteed error bounds, additional assumptions on f and g, called model class assumptions, are needed.","We determine the best error (in the energy norm) of approximating u, in terms of the number of point samples m, under all Besov class model assumptions for the right hand side $f$ and boundary g.   ","We then turn to the study of numerical procedures and asks whether a proposed numerical procedure (nearly) achieves the optimal recovery error.","We analyze numerical methods which generate the numerical approximation to $u$ by minimizing a specified data driven loss function over a set $\\Sigma$ which is either a finite dimensional linear space, or more generally, a finite dimensional manifold.","We show that the success of such a procedure depends critically on choosing a correct data driven loss function that is consistent with the PDE and provides sharp error control.","Based on this analysis a loss function $L^*$ is proposed.   ","We also address the recent methods of Physics Informed Neural Networks (PINNs).","Minimization of the new loss $L^*$ over neural network spaces $\\Sigma$ is referred to as consistent PINNs (CPINNs).","We prove that CPINNs provides an optimal recovery of the solution $u$, provided that the optimization problem can be numerically executed and $\\Sigma$ has sufficient approximation capabilities.","Finally, numerical examples illustrating the benefits of the CPINNs are given."],"url":"http://arxiv.org/abs/2406.09217v1","category":"math.NA"}
{"created":"2024-06-13 15:16:11","title":"On Softmax Direct Preference Optimization for Recommendation","abstract":"Recommender systems aim to predict personalized rankings based on user preference data. With the rise of Language Models (LMs), LM-based recommenders have been widely explored due to their extensive world knowledge and powerful reasoning abilities. Most of the LM-based recommenders convert historical interactions into language prompts, pairing with a positive item as the target response and fine-tuning LM with a language modeling loss. However, the current objective fails to fully leverage preference data and is not optimized for personalized ranking tasks, which hinders the performance of LM-based recommenders. Inspired by the current advancement of Direct Preference Optimization (DPO) in human preference alignment and the success of softmax loss in recommendations, we propose Softmax-DPO (\\textbf{S-DPO}) to instill ranking information into the LM to help LM-based recommenders distinguish preferred items from negatives, rather than solely focusing on positives. Specifically, we incorporate multiple negatives in user preference data and devise an alternative version of DPO loss tailored for LM-based recommenders, connected to softmax sampling strategies. Theoretically, we bridge S-DPO with the softmax loss over negative sampling and find that it has a side effect of mining hard negatives, which assures its exceptional capabilities in recommendation tasks. Empirically, extensive experiments conducted on three real-world datasets demonstrate the superiority of S-DPO to effectively model user preference and further boost recommendation performance while mitigating the data likelihood decline issue of DPO. Our codes are available at https://github.com/chenyuxin1999/S-DPO.","sentences":["Recommender systems aim to predict personalized rankings based on user preference data.","With the rise of Language Models (LMs), LM-based recommenders have been widely explored due to their extensive world knowledge and powerful reasoning abilities.","Most of the LM-based recommenders convert historical interactions into language prompts, pairing with a positive item as the target response and fine-tuning LM with a language modeling loss.","However, the current objective fails to fully leverage preference data and is not optimized for personalized ranking tasks, which hinders the performance of LM-based recommenders.","Inspired by the current advancement of Direct Preference Optimization (DPO) in human preference alignment and the success of softmax loss in recommendations, we propose Softmax-DPO (\\textbf{S-DPO}) to instill ranking information into the LM to help LM-based recommenders distinguish preferred items from negatives, rather than solely focusing on positives.","Specifically, we incorporate multiple negatives in user preference data and devise an alternative version of DPO loss tailored for LM-based recommenders, connected to softmax sampling strategies.","Theoretically, we bridge S-DPO with the softmax loss over negative sampling and find that it has a side effect of mining hard negatives, which assures its exceptional capabilities in recommendation tasks.","Empirically, extensive experiments conducted on three real-world datasets demonstrate the superiority of S-DPO to effectively model user preference and further boost recommendation performance while mitigating the data likelihood decline issue of DPO.","Our codes are available at https://github.com/chenyuxin1999/S-DPO."],"url":"http://arxiv.org/abs/2406.09215v1","category":"cs.IR"}
{"created":"2024-06-13 15:15:34","title":"Applying Multi-Agent Negotiation to Solve the Production Routing Problem With Privacy Preserving","abstract":"This paper presents a novel approach to address the Production Routing Problem with Privacy Preserving (PRPPP) in supply chain optimization. The integrated optimization of production, inventory, distribution, and routing decisions in real-world industry applications poses several challenges, including increased complexity, discrepancies between planning and execution, and constraints on information sharing. To mitigate these challenges, this paper proposes the use of intelligent agent negotiation within a hybrid Multi-Agent System (MAS) integrated with optimization algorithms. The MAS facilitates communication and coordination among entities, encapsulates private information, and enables negotiation. This, along with optimization algorithms, makes it a compelling framework for establishing optimal solutions. The approach is supported by real-world applications and synergies between MAS and optimization methods, demonstrating its effectiveness in addressing complex supply chain optimization problems.","sentences":["This paper presents a novel approach to address the Production Routing Problem with Privacy Preserving (PRPPP) in supply chain optimization.","The integrated optimization of production, inventory, distribution, and routing decisions in real-world industry applications poses several challenges, including increased complexity, discrepancies between planning and execution, and constraints on information sharing.","To mitigate these challenges, this paper proposes the use of intelligent agent negotiation within a hybrid Multi-Agent System (MAS) integrated with optimization algorithms.","The MAS facilitates communication and coordination among entities, encapsulates private information, and enables negotiation.","This, along with optimization algorithms, makes it a compelling framework for establishing optimal solutions.","The approach is supported by real-world applications and synergies between MAS and optimization methods, demonstrating its effectiveness in addressing complex supply chain optimization problems."],"url":"http://arxiv.org/abs/2406.09214v1","category":"cs.AI"}
{"created":"2024-06-13 15:15:13","title":"To curve, or not to curve: Is curvature-assisted quintessence observationally viable?","abstract":"Single-field models of accelerated expansion with nearly flat potentials, despite being able to provide observationally viable explanations for the early-time cosmic inflation and the late-time cosmic acceleration, are in strong tension with string theory evidence and the associated de Sitter swampland constraints. It has recently been argued that in an open universe, where the spatial curvature is negative (i.e., with $\\Omega_k>0$), a new stable fixed point arises, which may lead to viable single-field-based accelerated expansion with an arbitrarily steep potential. Here we show, through a dynamical systems analysis and a Bayesian statistical inference of cosmological parameters, that the additional cosmological solutions based on the new fixed point do not render steep-potential, single-field, accelerated expansion observationally viable. We mainly focus on quintessence models of dark energy, but we also argue that a similar conclusion can be drawn for cosmic inflation.","sentences":["Single-field models of accelerated expansion with nearly flat potentials, despite being able to provide observationally viable explanations for the early-time cosmic inflation and the late-time cosmic acceleration, are in strong tension with string theory evidence and the associated de Sitter swampland constraints.","It has recently been argued that in an open universe, where the spatial curvature is negative (i.e., with $\\Omega_k>0$), a new stable fixed point arises, which may lead to viable single-field-based accelerated expansion with an arbitrarily steep potential.","Here we show, through a dynamical systems analysis and a Bayesian statistical inference of cosmological parameters, that the additional cosmological solutions based on the new fixed point do not render steep-potential, single-field, accelerated expansion observationally viable.","We mainly focus on quintessence models of dark energy, but we also argue that a similar conclusion can be drawn for cosmic inflation."],"url":"http://arxiv.org/abs/2406.09212v1","category":"hep-th"}
{"created":"2024-06-13 15:14:12","title":"Acceleration of the Universe without the Hubble tension with Kaniadakis holographic dark energy using the Hubble horizon as the IR cut-off","abstract":"We introduce a holographic dark energy model that incorporates the first-order approximate Kaniadaski entropy, utilizing the Hubble horizon, $1/H$, as the infrared cutoff. We investigate the cosmological evolution within this framework. The model introduces an extra parameter relative to the $\\Lambda$CDM model. It posits a Universe that is initially dominated by dark matter, which then evolves to a phase where dark energy becomes the predominant component, with this transition occurring at a redshift of approximately $z \\sim 0.419$. The energy density of dark energy is ultimately expected to become constant, thereby circumventing the potential issue of a \"big rip\". Employing the most recent Type Ia supernova and Hubble parameter data, we constrain the model's parameters and find a Hubble constant of $H_0=72.8$ km/s/Mpc, thereby resolving the Hubble tension issue. The estimated age of the Universe, based on the best-fit parameter values, is $14.2$ Gyr. Furthermore, we predict the number of strong gravitational lenses and conduct statefinder and $Om$ diagnostic analyses to validate and characterize the model.","sentences":["We introduce a holographic dark energy model that incorporates the first-order approximate Kaniadaski entropy, utilizing the Hubble horizon, $1/H$, as the infrared cutoff.","We investigate the cosmological evolution within this framework.","The model introduces an extra parameter relative to the $\\Lambda$CDM model.","It posits a Universe that is initially dominated by dark matter, which then evolves to a phase where dark energy becomes the predominant component, with this transition occurring at a redshift of approximately $z \\sim 0.419$. The energy density of dark energy is ultimately expected to become constant, thereby circumventing the potential issue of a \"big rip\".","Employing the most recent Type Ia supernova and Hubble parameter data, we constrain the model's parameters and find a Hubble constant of $H_0=72.8$ km/s/Mpc, thereby resolving the Hubble tension issue.","The estimated age of the Universe, based on the best-fit parameter values, is $14.2$ Gyr.","Furthermore, we predict the number of strong gravitational lenses and conduct statefinder and $Om$ diagnostic analyses to validate and characterize the model."],"url":"http://arxiv.org/abs/2406.09209v1","category":"astro-ph.CO"}
{"created":"2024-06-13 15:12:02","title":"Python-based DSL for generating Verilog model of Synchronous Digital Circuits","abstract":"We have designed a Python-based Domain Specific Language (DSL) for modeling synchronous digital circuits. In this DSL, hardware is modeled as a collection of transactions -- running in series, parallel, and loops. When the model is executed by a Python interpreter, synthesizable and behavioural Verilog is generated as output, which can be integrated with other RTL designs or directly used for FPGA and ASIC flows. In this paper, we describe - 1) the language (DSL), which allows users to express computation in series/parallel/loop constructs, with explicit cycle boundaries, 2) the internals of a simple Python implementation to produce synthesizable Verilog, and 3) several design examples and case studies for applications in post-quantum cryptography, stereo-vision, digital signal processing and optimization techniques. In the end, we list ideas to extend this framework.","sentences":["We have designed a Python-based Domain Specific Language (DSL) for modeling synchronous digital circuits.","In this DSL, hardware is modeled as a collection of transactions -- running in series, parallel, and loops.","When the model is executed by a Python interpreter, synthesizable and behavioural Verilog is generated as output, which can be integrated with other RTL designs or directly used for FPGA and ASIC flows.","In this paper, we describe - 1) the language (DSL), which allows users to express computation in series/parallel/loop constructs, with explicit cycle boundaries, 2) the internals of a simple Python implementation to produce synthesizable Verilog, and 3) several design examples and case studies for applications in post-quantum cryptography, stereo-vision, digital signal processing and optimization techniques.","In the end, we list ideas to extend this framework."],"url":"http://arxiv.org/abs/2406.09208v1","category":"cs.AR"}
{"created":"2024-06-13 15:08:44","title":"Investigating potential causes of Sepsis with Bayesian network structure learning","abstract":"Sepsis is a life-threatening and serious global health issue. This study combines knowledge with available hospital data to investigate the potential causes of Sepsis that can be affected by policy decisions. We investigate the underlying causal structure of this problem by combining clinical expertise with score-based, constraint-based, and hybrid structure learning algorithms. A novel approach to model averaging and knowledge-based constraints was implemented to arrive at a consensus structure for causal inference. The structure learning process highlighted the importance of exploring data-driven approaches alongside clinical expertise. This includes discovering unexpected, although reasonable, relationships from a clinical perspective. Hypothetical interventions on Chronic Obstructive Pulmonary Disease, Alcohol dependence, and Diabetes suggest that the presence of any of these risk factors in patients increases the likelihood of Sepsis. This finding, alongside measuring the effect of these risk factors on Sepsis, has potential policy implications. Recognising the importance of prediction in improving Sepsis related health outcomes, the model built is also assessed in its ability to predict Sepsis. The predictions generated by the consensus model were assessed for their accuracy, sensitivity, and specificity. These three indicators all had results around 70%, and the AUC was 80%, which means the causal structure of the model is reasonably accurate given that the models were trained on data available for commissioning purposes only.","sentences":["Sepsis is a life-threatening and serious global health issue.","This study combines knowledge with available hospital data to investigate the potential causes of Sepsis that can be affected by policy decisions.","We investigate the underlying causal structure of this problem by combining clinical expertise with score-based, constraint-based, and hybrid structure learning algorithms.","A novel approach to model averaging and knowledge-based constraints was implemented to arrive at a consensus structure for causal inference.","The structure learning process highlighted the importance of exploring data-driven approaches alongside clinical expertise.","This includes discovering unexpected, although reasonable, relationships from a clinical perspective.","Hypothetical interventions on Chronic Obstructive Pulmonary Disease, Alcohol dependence, and Diabetes suggest that the presence of any of these risk factors in patients increases the likelihood of Sepsis.","This finding, alongside measuring the effect of these risk factors on Sepsis, has potential policy implications.","Recognising the importance of prediction in improving Sepsis related health outcomes, the model built is also assessed in its ability to predict Sepsis.","The predictions generated by the consensus model were assessed for their accuracy, sensitivity, and specificity.","These three indicators all had results around 70%, and the AUC was 80%, which means the causal structure of the model is reasonably accurate given that the models were trained on data available for commissioning purposes only."],"url":"http://arxiv.org/abs/2406.09207v1","category":"cs.LG"}
{"created":"2024-06-13 15:06:11","title":"Self-Training for Sample-Efficient Active Learning for Text Classification with Pre-Trained Language Models","abstract":"Active learning is an iterative labeling process that is used to obtain a small labeled subset, despite the absence of labeled data, thereby enabling to train a model for supervised tasks such as text classification. While active learning has made considerable progress in recent years due to improvements provided by pre-trained language models, there is untapped potential in the often neglected unlabeled portion of the data, although it is available in considerably larger quantities than the usually small set of labeled data. Here we investigate how self-training, a semi-supervised approach where a model is used to obtain pseudo-labels from the unlabeled data, can be used to improve the efficiency of active learning for text classification. Starting with an extensive reproduction of four previous self-training approaches, some of which are evaluated for the first time in the context of active learning or natural language processing, we devise HAST, a new and effective self-training strategy, which is evaluated on four text classification benchmarks, on which it outperforms the reproduced self-training approaches and reaches classification results comparable to previous experiments for three out of four datasets, using only 25% of the data.","sentences":["Active learning is an iterative labeling process that is used to obtain a small labeled subset, despite the absence of labeled data, thereby enabling to train a model for supervised tasks such as text classification.","While active learning has made considerable progress in recent years due to improvements provided by pre-trained language models, there is untapped potential in the often neglected unlabeled portion of the data, although it is available in considerably larger quantities than the usually small set of labeled data.","Here we investigate how self-training, a semi-supervised approach where a model is used to obtain pseudo-labels from the unlabeled data, can be used to improve the efficiency of active learning for text classification.","Starting with an extensive reproduction of four previous self-training approaches, some of which are evaluated for the first time in the context of active learning or natural language processing, we devise HAST, a new and effective self-training strategy, which is evaluated on four text classification benchmarks, on which it outperforms the reproduced self-training approaches and reaches classification results comparable to previous experiments for three out of four datasets, using only 25% of the data."],"url":"http://arxiv.org/abs/2406.09206v1","category":"cs.CL"}
{"created":"2024-06-13 15:03:46","title":"ReadCtrl: Personalizing text generation with readability-controlled instruction learning","abstract":"Content generation conditioning on users's readability is an important application for personalization. In an era of large language models (LLMs), readability-controlled text generation based on LLMs has become increasingly important. This paper introduces a novel methodology called \"Readability-Controlled Instruction Learning (ReadCtrl),\" which aims to instruction-tune LLMs to tailor users' readability levels. Unlike the traditional methods, which primarily focused on categorical readability adjustments typically classified as high, medium, and low or expert and layperson levels with limited success, ReadCtrl introduces a dynamic framework that enables LLMs to generate content at various (near continuous level) complexity levels, thereby enhancing their versatility across different applications. Our results show that the ReadCtrl-Mistral-7B models significantly outperformed strong baseline models such as GPT-4 and Claude-3, with a win rate of 52.1%:35.7% against GPT-4 in human evaluations. Furthermore, Read-Ctrl has shown significant improvements in automatic evaluations, as evidenced by better readability metrics (e.g., FOG, FKGL) and generation quality metrics (e.g., BLEU, SARI, SummaC-Factuality, UniEval-Consistency and Coherence). These results underscore Read-Ctrl's effectiveness and tenacity in producing high-quality, contextually appropriate outputs that closely align with targeted readability levels, marking a significant advancement in personalized content generation using LLMs.","sentences":["Content generation conditioning on users's readability is an important application for personalization.","In an era of large language models (LLMs), readability-controlled text generation based on LLMs has become increasingly important.","This paper introduces a novel methodology called \"Readability-Controlled Instruction Learning (ReadCtrl),\" which aims to instruction-tune LLMs to tailor users' readability levels.","Unlike the traditional methods, which primarily focused on categorical readability adjustments typically classified as high, medium, and low or expert and layperson levels with limited success, ReadCtrl introduces a dynamic framework that enables LLMs to generate content at various (near continuous level) complexity levels, thereby enhancing their versatility across different applications.","Our results show that the ReadCtrl-Mistral-7B models significantly outperformed strong baseline models such as GPT-4 and Claude-3, with a win rate of 52.1%:35.7% against GPT-4 in human evaluations.","Furthermore, Read-Ctrl has shown significant improvements in automatic evaluations, as evidenced by better readability metrics (e.g., FOG, FKGL) and generation quality metrics (e.g., BLEU, SARI, SummaC-Factuality, UniEval-Consistency and Coherence).","These results underscore Read-Ctrl's effectiveness and tenacity in producing high-quality, contextually appropriate outputs that closely align with targeted readability levels, marking a significant advancement in personalized content generation using LLMs."],"url":"http://arxiv.org/abs/2406.09205v1","category":"cs.CL"}
{"created":"2024-06-13 14:59:45","title":"Language Complexity and Speech Recognition Accuracy: Orthographic Complexity Hurts, Phonological Complexity Doesn't","abstract":"We investigate what linguistic factors affect the performance of Automatic Speech Recognition (ASR) models. We hypothesize that orthographic and phonological complexities both degrade accuracy. To examine this, we fine-tune the multilingual self-supervised pretrained model Wav2Vec2-XLSR-53 on 25 languages with 15 writing systems, and we compare their ASR accuracy, number of graphemes, unigram grapheme entropy, logographicity (how much word/morpheme-level information is encoded in the writing system), and number of phonemes. The results demonstrate that orthographic complexities significantly correlate with low ASR accuracy, while phonological complexity shows no significant correlation.","sentences":["We investigate what linguistic factors affect the performance of Automatic Speech Recognition (ASR) models.","We hypothesize that orthographic and phonological complexities both degrade accuracy.","To examine this, we fine-tune the multilingual self-supervised pretrained model Wav2Vec2-XLSR-53 on 25 languages with 15 writing systems, and we compare their ASR accuracy, number of graphemes, unigram grapheme entropy, logographicity (how much word/morpheme-level information is encoded in the writing system), and number of phonemes.","The results demonstrate that orthographic complexities significantly correlate with low ASR accuracy, while phonological complexity shows no significant correlation."],"url":"http://arxiv.org/abs/2406.09202v1","category":"cs.CL"}
{"created":"2024-06-13 14:56:52","title":"Precise analysis of ridge interpolators under heavy correlations -- a Random Duality Theory view","abstract":"We consider fully row/column-correlated linear regression models and study several classical estimators (including minimum norm interpolators (GLS), ordinary least squares (LS), and ridge regressors). We show that \\emph{Random Duality Theory} (RDT) can be utilized to obtain precise closed form characterizations of all estimators related optimizing quantities of interest, including the \\emph{prediction risk} (testing or generalization error). On a qualitative level out results recover the risk's well known non-monotonic (so-called double-descent) behavior as the number of features/sample size ratio increases. On a quantitative level, our closed form results show how the risk explicitly depends on all key model parameters, including the problem dimensions and covariance matrices. Moreover, a special case of our results, obtained when intra-sample (or time-series) correlations are not present, precisely match the corresponding ones obtained via spectral methods in [6,16,17,24].","sentences":["We consider fully row/column-correlated linear regression models and study several classical estimators (including minimum norm interpolators (GLS), ordinary least squares (LS), and ridge regressors).","We show that \\emph{Random Duality Theory} (RDT) can be utilized to obtain precise closed form characterizations of all estimators related optimizing quantities of interest, including the \\emph{prediction risk} (testing or generalization error).","On a qualitative level out results recover the risk's well known non-monotonic (so-called double-descent) behavior as the number of features/sample size ratio increases.","On a quantitative level, our closed form results show how the risk explicitly depends on all key model parameters, including the problem dimensions and covariance matrices.","Moreover, a special case of our results, obtained when intra-sample (or time-series) correlations are not present, precisely match the corresponding ones obtained via spectral methods in [6,16,17,24]."],"url":"http://arxiv.org/abs/2406.09199v1","category":"stat.ML"}
{"created":"2024-06-13 14:56:07","title":"CLIP-Driven Cloth-Agnostic Feature Learning for Cloth-Changing Person Re-Identification","abstract":"Contrastive Language-Image Pre-Training (CLIP) has shown impressive performance in short-term Person Re-Identification (ReID) due to its ability to extract high-level semantic features of pedestrians, yet its direct application to Cloth-Changing Person Re-Identification (CC-ReID) faces challenges due to CLIP's image encoder overly focusing on clothes clues. To address this, we propose a novel framework called CLIP-Driven Cloth-Agnostic Feature Learning (CCAF) for CC-ReID. Accordingly, two modules were custom-designed: the Invariant Feature Prompting (IFP) and the Clothes Feature Minimization (CFM). These modules guide the model to extract cloth-agnostic features positively and attenuate clothes-related features negatively. Specifically, IFP is designed to extract fine-grained semantic features unrelated to clothes from the raw image, guided by the cloth-agnostic text prompts. This module first covers the clothes in the raw image at the pixel level to obtain the shielding image and then utilizes CLIP's knowledge to generate cloth-agnostic text prompts. Subsequently, it aligns the raw image-text and the raw image-shielding image in the feature space, emphasizing discriminative clues related to identity but unrelated to clothes. Furthermore, CFM is designed to examine and weaken the image encoder's ability to extract clothes features. It first generates text prompts corresponding to clothes pixels. Then, guided by these clothes text prompts, it iteratively examines and disentangles clothes features from pedestrian features, ultimately retaining inherent discriminative features. Extensive experiments have demonstrated the effectiveness of the proposed CCAF, achieving new state-of-the-art performance on several popular CC-ReID benchmarks without any additional inference time.","sentences":["Contrastive Language-Image Pre-Training (CLIP) has shown impressive performance in short-term Person Re-Identification (ReID) due to its ability to extract high-level semantic features of pedestrians, yet its direct application to Cloth-Changing Person Re-Identification (CC-ReID) faces challenges due to CLIP's image encoder overly focusing on clothes clues.","To address this, we propose a novel framework called CLIP-Driven Cloth-Agnostic Feature Learning (CCAF) for CC-ReID.","Accordingly, two modules were custom-designed: the Invariant Feature Prompting (IFP) and the Clothes Feature Minimization (CFM).","These modules guide the model to extract cloth-agnostic features positively and attenuate clothes-related features negatively.","Specifically, IFP is designed to extract fine-grained semantic features unrelated to clothes from the raw image, guided by the cloth-agnostic text prompts.","This module first covers the clothes in the raw image at the pixel level to obtain the shielding image and then utilizes CLIP's knowledge to generate cloth-agnostic text prompts.","Subsequently, it aligns the raw image-text and the raw image-shielding image in the feature space, emphasizing discriminative clues related to identity but unrelated to clothes.","Furthermore, CFM is designed to examine and weaken the image encoder's ability to extract clothes features.","It first generates text prompts corresponding to clothes pixels.","Then, guided by these clothes text prompts, it iteratively examines and disentangles clothes features from pedestrian features, ultimately retaining inherent discriminative features.","Extensive experiments have demonstrated the effectiveness of the proposed CCAF, achieving new state-of-the-art performance on several popular CC-ReID benchmarks without any additional inference time."],"url":"http://arxiv.org/abs/2406.09198v1","category":"cs.CV"}
{"created":"2024-06-13 14:55:20","title":"A Hybrid Modelling of a Water and Air Injector in a Subsonic Icing Wind Tunnel","abstract":"The study of droplet generation in wind tunnels in conducting icing experiments is of great importance in determining ice formation on structures or surfaces, where parameters such as Liquid Water Content (LWC) and Median Volumetric Diameter (MVD) play a relevant role. The measurement of these parameters requires specialised instrumentation. In this paper, several experiments have been carried out in a subsonic wind tunnel facility to study the parameters that are part of the icing process in structures. Furthermore, a mathematical modelling of the constituent subsystems of the plant study that allow us to have a comprehensive understanding of the behaviour of the system is developed using techniques based on first principles and machine learning techniques such as regression trees and neural networks. The simulation results show that the implementation of the model manages to obtain prominent expected values of LWC and MVD within the range of values obtained in the real experimental data.","sentences":["The study of droplet generation in wind tunnels in conducting icing experiments is of great importance in determining ice formation on structures or surfaces, where parameters such as Liquid Water Content (LWC) and Median Volumetric Diameter (MVD) play a relevant role.","The measurement of these parameters requires specialised instrumentation.","In this paper, several experiments have been carried out in a subsonic wind tunnel facility to study the parameters that are part of the icing process in structures.","Furthermore, a mathematical modelling of the constituent subsystems of the plant study that allow us to have a comprehensive understanding of the behaviour of the system is developed using techniques based on first principles and machine learning techniques such as regression trees and neural networks.","The simulation results show that the implementation of the model manages to obtain prominent expected values of LWC and MVD within the range of values obtained in the real experimental data."],"url":"http://arxiv.org/abs/2406.09197v1","category":"eess.SY"}
{"created":"2024-06-13 14:53:30","title":"Joint Power Allocation and Beamforming Design for Active IRS-Aided Directional Modulation Secure Systems","abstract":"Since the secrecy rate (SR) performance improvement obtained by secure directional modulation (DM) network is limited, an active intelligent reflective surface (IRS)-assisted DM network is considered to attain a high SR. To address the SR maximization problem, a novel method based on Lagrangian dual transform and closed-form fractional programming algorithm (LDT-CFFP) is proposed, where the solutions to base station (BS) beamforming vectors and IRS reflection coefficient matrix are achieved. However, the computational complexity of LDT-CFFP method is high . To reduce its complexity, a blocked IRS-assisted DM network is designed. To meet the requirements of the network performance, a power allocation (PA) strategy is proposed and adopted in the system. Specifically, the system power between BS and IRS, as well as the transmission power for confidential messages (CM) and artificial noise (AN) from the BS, are allocated separately. Then we put forward null-space projection (NSP) method, maximum-ratio-reflecting (MRR) algorithm and PA strategy (NSP-MRR-PA) to solve the SR maximization problem. The CF solutions to BS beamforming vectors and IRS reflection coefficient matrix are respectively attained via NSP and MRR algorithms. For the PA factors, we take advantage of exhaustive search (ES) algorithm, particle swarm optimization (PSO) and simulated annealing (SA) algorithm to search for the solutions. From simulation results, it is verified that the LDT-CFFP method derives a higher SR gain over NSP-MRR-PA method. For NSP-MRR-PA method, the number of IRS units in each block possesses a significant SR performance. In addition, the application PA strategies, namely ES, PSO, SA methods outperforms the other PA strategies with fixed PA factors.","sentences":["Since the secrecy rate (SR) performance improvement obtained by secure directional modulation (DM) network is limited, an active intelligent reflective surface (IRS)-assisted DM network is considered to attain a high SR.","To address the SR maximization problem, a novel method based on Lagrangian dual transform and closed-form fractional programming algorithm (LDT-CFFP) is proposed, where the solutions to base station (BS) beamforming vectors and IRS reflection coefficient matrix are achieved.","However, the computational complexity of LDT-CFFP method is high .","To reduce its complexity, a blocked IRS-assisted DM network is designed.","To meet the requirements of the network performance, a power allocation (PA) strategy is proposed and adopted in the system.","Specifically, the system power between BS and IRS, as well as the transmission power for confidential messages (CM) and artificial noise (AN) from the BS, are allocated separately.","Then we put forward null-space projection (NSP) method, maximum-ratio-reflecting (MRR) algorithm and PA strategy (NSP-MRR-PA) to solve the SR maximization problem.","The CF solutions to BS beamforming vectors and IRS reflection coefficient matrix are respectively attained via NSP and MRR algorithms.","For the PA factors, we take advantage of exhaustive search (ES) algorithm, particle swarm optimization (PSO) and simulated annealing (SA) algorithm to search for the solutions.","From simulation results, it is verified that the LDT-CFFP method derives a higher SR gain over NSP-MRR-PA method.","For NSP-MRR-PA method, the number of IRS units in each block possesses a significant SR performance.","In addition, the application PA strategies, namely ES, PSO, SA methods outperforms the other PA strategies with fixed PA factors."],"url":"http://arxiv.org/abs/2406.09192v1","category":"eess.SP"}
{"created":"2024-06-13 14:51:04","title":"Rethinking Waveform for 6G: Harnessing Delay-Doppler Alignment Modulation","abstract":"Waveform design has served as a cornerstone for each generation of mobile communication systems. The future sixth-generation (6G) mobile communication networks are expected to employ larger-scale antenna arrays and exploit higher-frequency bands for further boosting data transmission rate and providing ubiquitous wireless sensing. This brings new opportunities and challenges for 6G waveform design. In this article, by leveraging the super spatial resolution of large antenna arrays and the multi-path spatial sparsity of highfrequency wireless channels, we introduce a new approach for waveform design based on the recently proposed delay-Doppler alignment modulation (DDAM). In particular, DDAM makes a paradigm shift of waveform design from the conventional manner of tolerating channel delay and Doppler spreads to actively manipulating them. First, we review the fundamental constraints and performance limitations of orthogonal frequency division multiplexing (OFDM) and introduce new opportunities for 6G waveform design. Next, the motivations and basic principles of DDAM are presented, followed by its various extensions to different wireless system setups. Finally, the main design considerations for DDAM are discussed and the new opportunities for future research are highlighted.","sentences":["Waveform design has served as a cornerstone for each generation of mobile communication systems.","The future sixth-generation (6G) mobile communication networks are expected to employ larger-scale antenna arrays and exploit higher-frequency bands for further boosting data transmission rate and providing ubiquitous wireless sensing.","This brings new opportunities and challenges for 6G waveform design.","In this article, by leveraging the super spatial resolution of large antenna arrays and the multi-path spatial sparsity of highfrequency wireless channels, we introduce a new approach for waveform design based on the recently proposed delay-Doppler alignment modulation (DDAM).","In particular, DDAM makes a paradigm shift of waveform design from the conventional manner of tolerating channel delay and Doppler spreads to actively manipulating them.","First, we review the fundamental constraints and performance limitations of orthogonal frequency division multiplexing (OFDM) and introduce new opportunities for 6G waveform design.","Next, the motivations and basic principles of DDAM are presented, followed by its various extensions to different wireless system setups.","Finally, the main design considerations for DDAM are discussed and the new opportunities for future research are highlighted."],"url":"http://arxiv.org/abs/2406.09190v1","category":"eess.SP"}
{"created":"2024-06-13 14:49:28","title":"Reducing Task Discrepancy of Text Encoders for Zero-Shot Composed Image Retrieval","abstract":"Composed Image Retrieval (CIR) aims to retrieve a target image based on a reference image and conditioning text, enabling controllable searches. Due to the expensive dataset construction cost for CIR triplets, a zero-shot (ZS) CIR setting has been actively studied to eliminate the need for human-collected triplet datasets. The mainstream of ZS-CIR employs an efficient projection module that projects a CLIP image embedding to the CLIP text token embedding space, while fixing the CLIP encoders. Using the projected image embedding, these methods generate image-text composed features by using the pre-trained text encoder. However, their CLIP image and text encoders suffer from the task discrepancy between the pre-training task (text $\\leftrightarrow$ image) and the target CIR task (image + text $\\leftrightarrow$ image). Conceptually, we need expensive triplet samples to reduce the discrepancy, but we use cheap text triplets instead and update the text encoder. To that end, we introduce the Reducing Task Discrepancy of text encoders for Composed Image Retrieval (RTD), a plug-and-play training scheme for the text encoder that enhances its capability using a novel target-anchored text contrastive learning. We also propose two additional techniques to improve the proposed learning scheme: a hard negatives-based refined batch sampling strategy and a sophisticated concatenation scheme. Integrating RTD into the state-of-the-art projection-based ZS-CIR methods significantly improves performance across various datasets and backbones, demonstrating its efficiency and generalizability.","sentences":["Composed Image Retrieval (CIR) aims to retrieve a target image based on a reference image and conditioning text, enabling controllable searches.","Due to the expensive dataset construction cost for CIR triplets, a zero-shot (ZS) CIR setting has been actively studied to eliminate the need for human-collected triplet datasets.","The mainstream of ZS-CIR employs an efficient projection module that projects a CLIP image embedding to the CLIP text token embedding space, while fixing the CLIP encoders.","Using the projected image embedding, these methods generate image-text composed features by using the pre-trained text encoder.","However, their CLIP image and text encoders suffer from the task discrepancy between the pre-training task (text $\\leftrightarrow$ image) and the target CIR task (image + text $\\leftrightarrow$ image).","Conceptually, we need expensive triplet samples to reduce the discrepancy, but we use cheap text triplets instead and update the text encoder.","To that end, we introduce the Reducing Task Discrepancy of text encoders for Composed Image Retrieval (RTD), a plug-and-play training scheme for the text encoder that enhances its capability using a novel target-anchored text contrastive learning.","We also propose two additional techniques to improve the proposed learning scheme: a hard negatives-based refined batch sampling strategy and a sophisticated concatenation scheme.","Integrating RTD into the state-of-the-art projection-based ZS-CIR methods significantly improves performance across various datasets and backbones, demonstrating its efficiency and generalizability."],"url":"http://arxiv.org/abs/2406.09188v1","category":"cs.CV"}
{"created":"2024-06-13 14:49:26","title":"GuardAgent: Safeguard LLM Agents by a Guard Agent via Knowledge-Enabled Reasoning","abstract":"The rapid advancement of large language models (LLMs) has catalyzed the deployment of LLM-powered agents across numerous applications, raising new concerns regarding their safety and trustworthiness. Existing methods for enhancing the safety of LLMs are not directly transferable to LLM-powered agents due to their diverse objectives and output modalities. In this paper, we propose GuardAgent, the first LLM agent as a guardrail to other LLM agents. Specifically, GuardAgent oversees a target LLM agent by checking whether its inputs/outputs satisfy a set of given guard requests defined by the users. GuardAgent comprises two steps: 1) creating a task plan by analyzing the provided guard requests, and 2) generating guardrail code based on the task plan and executing the code by calling APIs or using external engines. In both steps, an LLM is utilized as the core reasoning component, supplemented by in-context demonstrations retrieved from a memory module. Such knowledge-enabled reasoning allows GuardAgent to understand various textual guard requests and accurately \"translate\" them into executable code that provides reliable guardrails. Furthermore, GuardAgent is equipped with an extendable toolbox containing functions and APIs and requires no additional LLM training, which underscores its generalization capabilities and low operational overhead. Additionally, we propose two novel benchmarks: an EICU-AC benchmark for assessing privacy-related access control for healthcare agents and a Mind2Web-SC benchmark for safety evaluation for web agents. We show the effectiveness of GuardAgent on these two benchmarks with 98.7% and 90.0% accuracy in moderating invalid inputs and outputs for the two types of agents, respectively. We also show that GuardAgent is able to define novel functions in adaption to emergent LLM agents and guard requests, which underscores its strong generalization capabilities.","sentences":["The rapid advancement of large language models (LLMs) has catalyzed the deployment of LLM-powered agents across numerous applications, raising new concerns regarding their safety and trustworthiness.","Existing methods for enhancing the safety of LLMs are not directly transferable to LLM-powered agents due to their diverse objectives and output modalities.","In this paper, we propose GuardAgent, the first LLM agent as a guardrail to other LLM agents.","Specifically, GuardAgent oversees a target LLM agent by checking whether its inputs/outputs satisfy a set of given guard requests defined by the users.","GuardAgent comprises two steps: 1) creating a task plan by analyzing the provided guard requests, and 2) generating guardrail code based on the task plan and executing the code by calling APIs or using external engines.","In both steps, an LLM is utilized as the core reasoning component, supplemented by in-context demonstrations retrieved from a memory module.","Such knowledge-enabled reasoning allows GuardAgent to understand various textual guard requests and accurately \"translate\" them into executable code that provides reliable guardrails.","Furthermore, GuardAgent is equipped with an extendable toolbox containing functions and APIs and requires no additional LLM training, which underscores its generalization capabilities and low operational overhead.","Additionally, we propose two novel benchmarks: an EICU-AC benchmark for assessing privacy-related access control for healthcare agents and a Mind2Web-SC benchmark for safety evaluation for web agents.","We show the effectiveness of GuardAgent on these two benchmarks with 98.7% and 90.0% accuracy in moderating invalid inputs and outputs for the two types of agents, respectively.","We also show that GuardAgent is able to define novel functions in adaption to emergent LLM agents and guard requests, which underscores its strong generalization capabilities."],"url":"http://arxiv.org/abs/2406.09187v1","category":"cs.LG"}
{"created":"2024-06-13 14:47:57","title":"Thoracic Surgery Video Analysis for Surgical Phase Recognition","abstract":"This paper presents an approach for surgical phase recognition using video data, aiming to provide a comprehensive understanding of surgical procedures for automated workflow analysis. The advent of robotic surgery, digitized operating rooms, and the generation of vast amounts of data have opened doors for the application of machine learning and computer vision in the analysis of surgical videos. Among these advancements, Surgical Phase Recognition(SPR) stands out as an emerging technology that has the potential to recognize and assess the ongoing surgical scenario, summarize the surgery, evaluate surgical skills, offer surgical decision support, and facilitate medical training. In this paper, we analyse and evaluate both frame-based and video clipping-based phase recognition on thoracic surgery dataset consisting of 11 classes of phases. Specifically, we utilize ImageNet ViT for image-based classification and VideoMAE as the baseline model for video-based classification. We show that Masked Video Distillation(MVD) exhibits superior performance, achieving a top-1 accuracy of 72.9%, compared to 52.31% achieved by ImageNet ViT. These findings underscore the efficacy of video-based classifiers over their image-based counterparts in surgical phase recognition tasks.","sentences":["This paper presents an approach for surgical phase recognition using video data, aiming to provide a comprehensive understanding of surgical procedures for automated workflow analysis.","The advent of robotic surgery, digitized operating rooms, and the generation of vast amounts of data have opened doors for the application of machine learning and computer vision in the analysis of surgical videos.","Among these advancements, Surgical Phase Recognition(SPR) stands out as an emerging technology that has the potential to recognize and assess the ongoing surgical scenario, summarize the surgery, evaluate surgical skills, offer surgical decision support, and facilitate medical training.","In this paper, we analyse and evaluate both frame-based and video clipping-based phase recognition on thoracic surgery dataset consisting of 11 classes of phases.","Specifically, we utilize ImageNet ViT for image-based classification and VideoMAE as the baseline model for video-based classification.","We show that Masked Video Distillation(MVD) exhibits superior performance, achieving a top-1 accuracy of 72.9%, compared to 52.31% achieved by ImageNet ViT.","These findings underscore the efficacy of video-based classifiers over their image-based counterparts in surgical phase recognition tasks."],"url":"http://arxiv.org/abs/2406.09185v1","category":"cs.CV"}
{"created":"2024-06-13 14:47:01","title":"Existence of solitary waves in particle lattices with power-law forces","abstract":"We prove the existence of small solitary waves for one-dimensional lattices of particles that each repel every other particle with a force that decays as a power of distance. For force exponents $\\alpha+1$ with $\\frac43<\\alpha<3$, we employ fixed-point arguments to find near-sonic solitary waves having scaled velocity profiles close to non-degenerate solitary-wave profiles of fractional KdV or generalized Benjamin-Ono equations. These equations were recently found to approximately govern unidirectional long-wave motions in these lattices.","sentences":["We prove the existence of small solitary waves for one-dimensional lattices of particles that each repel every other particle with a force that decays as a power of distance.","For force exponents $\\alpha+1$ with $\\frac43<\\alpha<3$, we employ fixed-point arguments to find near-sonic solitary waves having scaled velocity profiles close to non-degenerate solitary-wave profiles of fractional KdV or generalized Benjamin-Ono equations.","These equations were recently found to approximately govern unidirectional long-wave motions in these lattices."],"url":"http://arxiv.org/abs/2406.09184v1","category":"nlin.PS"}
{"created":"2024-06-13 14:46:08","title":"Ridge interpolators in correlated factor regression models -- exact risk analysis","abstract":"We consider correlated \\emph{factor} regression models (FRM) and analyze the performance of classical ridge interpolators. Utilizing powerful \\emph{Random Duality Theory} (RDT) mathematical engine, we obtain \\emph{precise} closed form characterizations of the underlying optimization problems and all associated optimizing quantities. In particular, we provide \\emph{excess prediction risk} characterizations that clearly show the dependence on all key model parameters, covariance matrices, loadings, and dimensions. As a function of the over-parametrization ratio, the generalized least squares (GLS) risk also exhibits the well known \\emph{double-descent} (non-monotonic) behavior. Similarly to the classical linear regression models (LRM), we demonstrate that such FRM phenomenon can be smoothened out by the optimally tuned ridge regularization. The theoretical results are supplemented by numerical simulations and an excellent agrement between the two is observed. Moreover, we note that ``ridge smootenhing'' is often of limited effect already for over-parametrization ratios above $5$ and of virtually no effect for those above $10$. This solidifies the notion that one of the recently most popular neural networks paradigms -- \\emph{zero-training (interpolating) generalizes well} -- enjoys wider applicability, including the one within the FRM estimation/prediction context.","sentences":["We consider correlated \\emph{factor} regression models (FRM) and analyze the performance of classical ridge interpolators.","Utilizing powerful \\emph{Random Duality Theory} (RDT) mathematical engine, we obtain \\emph{precise} closed form characterizations of the underlying optimization problems and all associated optimizing quantities.","In particular, we provide \\emph{excess prediction risk} characterizations that clearly show the dependence on all key model parameters, covariance matrices, loadings, and dimensions.","As a function of the over-parametrization ratio, the generalized least squares (GLS) risk also exhibits the well known \\emph{double-descent} (non-monotonic) behavior.","Similarly to the classical linear regression models (LRM), we demonstrate that such FRM phenomenon can be smoothened out by the optimally tuned ridge regularization.","The theoretical results are supplemented by numerical simulations and an excellent agrement between the two is observed.","Moreover, we note that ``ridge smootenhing'' is often of limited effect already for over-parametrization ratios above $5$ and of virtually no effect for those above $10$. This solidifies the notion that one of the recently most popular neural networks paradigms -- \\emph{zero-training (interpolating) generalizes well} -- enjoys wider applicability, including the one within the FRM estimation/prediction context."],"url":"http://arxiv.org/abs/2406.09183v1","category":"stat.ML"}
{"created":"2024-06-13 14:45:35","title":"Federated Contrastive Learning for Personalized Semantic Communication","abstract":"In this letter, we design a federated contrastive learning (FedCL) framework aimed at supporting personalized semantic communication. Our FedCL enables collaborative training of local semantic encoders across multiple clients and a global semantic decoder owned by the base station. This framework supports heterogeneous semantic encoders since it does not require client-side model aggregation. Furthermore, to tackle the semantic imbalance issue arising from heterogeneous datasets across distributed clients, we employ contrastive learning to train a semantic centroid generator (SCG). This generator obtains representative global semantic centroids that exhibit intra-semantic compactness and inter-semantic separability. Consequently, it provides superior supervision for learning discriminative local semantic features. Additionally, we conduct theoretical analysis to quantify the convergence performance of FedCL. Simulation results verify the superiority of the proposed FedCL framework compared to other distributed learning benchmarks in terms of task performance and robustness under different numbers of clients and channel conditions, especially in low signal-to-noise ratio and highly heterogeneous data scenarios.","sentences":["In this letter, we design a federated contrastive learning (FedCL) framework aimed at supporting personalized semantic communication.","Our FedCL enables collaborative training of local semantic encoders across multiple clients and a global semantic decoder owned by the base station.","This framework supports heterogeneous semantic encoders since it does not require client-side model aggregation.","Furthermore, to tackle the semantic imbalance issue arising from heterogeneous datasets across distributed clients, we employ contrastive learning to train a semantic centroid generator (SCG).","This generator obtains representative global semantic centroids that exhibit intra-semantic compactness and inter-semantic separability.","Consequently, it provides superior supervision for learning discriminative local semantic features.","Additionally, we conduct theoretical analysis to quantify the convergence performance of FedCL.","Simulation results verify the superiority of the proposed FedCL framework compared to other distributed learning benchmarks in terms of task performance and robustness under different numbers of clients and channel conditions, especially in low signal-to-noise ratio and highly heterogeneous data scenarios."],"url":"http://arxiv.org/abs/2406.09182v1","category":"eess.SP"}
{"created":"2024-06-13 14:42:59","title":"A Large-scale Universal Evaluation Benchmark For Face Forgery Detection","abstract":"With the rapid development of AI-generated content (AIGC) technology, the production of realistic fake facial images and videos that deceive human visual perception has become possible. Consequently, various face forgery detection techniques have been proposed to identify such fake facial content. However, evaluating the effectiveness and generalizability of these detection techniques remains a significant challenge. To address this, we have constructed a large-scale evaluation benchmark called DeepFaceGen, aimed at quantitatively assessing the effectiveness of face forgery detection and facilitating the iterative development of forgery detection technology. DeepFaceGen consists of 776,990 real face image/video samples and 773,812 face forgery image/video samples, generated using 34 mainstream face generation techniques. During the construction process, we carefully consider important factors such as content diversity, fairness across ethnicities, and availability of comprehensive labels, in order to ensure the versatility and convenience of DeepFaceGen. Subsequently, DeepFaceGen is employed in this study to evaluate and analyze the performance of 13 mainstream face forgery detection techniques from various perspectives. Through extensive experimental analysis, we derive significant findings and propose potential directions for future research. The code and dataset for DeepFaceGen are available at https://anonymous.4open.science/r/DeepFaceGen-47D1.","sentences":["With the rapid development of AI-generated content (AIGC) technology, the production of realistic fake facial images and videos that deceive human visual perception has become possible.","Consequently, various face forgery detection techniques have been proposed to identify such fake facial content.","However, evaluating the effectiveness and generalizability of these detection techniques remains a significant challenge.","To address this, we have constructed a large-scale evaluation benchmark called DeepFaceGen, aimed at quantitatively assessing the effectiveness of face forgery detection and facilitating the iterative development of forgery detection technology.","DeepFaceGen consists of 776,990 real face image/video samples and 773,812 face forgery image/video samples, generated using 34 mainstream face generation techniques.","During the construction process, we carefully consider important factors such as content diversity, fairness across ethnicities, and availability of comprehensive labels, in order to ensure the versatility and convenience of DeepFaceGen.","Subsequently, DeepFaceGen is employed in this study to evaluate and analyze the performance of 13 mainstream face forgery detection techniques from various perspectives.","Through extensive experimental analysis, we derive significant findings and propose potential directions for future research.","The code and dataset for DeepFaceGen are available at https://anonymous.4open.science/r/DeepFaceGen-47D1."],"url":"http://arxiv.org/abs/2406.09181v1","category":"cs.CV"}
{"created":"2024-06-13 14:41:00","title":"Unlearning with Control: Assessing Real-world Utility for Large Language Model Unlearning","abstract":"The compelling goal of eradicating undesirable data behaviors, while preserving usual model functioning, underscores the significance of machine unlearning within the domain of large language models (LLMs). Recent research has begun to approach LLM unlearning via gradient ascent (GA) -- increasing the prediction risk for those training strings targeted to be unlearned, thereby erasing their parameterized responses. Despite their simplicity and efficiency, we suggest that GA-based methods face the propensity towards excessive unlearning, resulting in various undesirable model behaviors, such as catastrophic forgetting, that diminish their practical utility. In this paper, we suggest a set of metrics that can capture multiple facets of real-world utility and propose several controlling methods that can regulate the extent of excessive unlearning. Accordingly, we suggest a general framework to better reflect the practical efficacy of various unlearning methods -- we begin by controlling the unlearning procedures/unlearned models such that no excessive unlearning occurs and follow by the evaluation for unlearning efficacy. Our experimental analysis on established benchmarks revealed that GA-based methods are far from perfect in practice, as strong unlearning is at the high cost of hindering the model utility. We conclude that there is still a long way towards practical and effective LLM unlearning, and more efforts are required in this field.","sentences":["The compelling goal of eradicating undesirable data behaviors, while preserving usual model functioning, underscores the significance of machine unlearning within the domain of large language models (LLMs).","Recent research has begun to approach LLM unlearning via gradient ascent (GA) -- increasing the prediction risk for those training strings targeted to be unlearned, thereby erasing their parameterized responses.","Despite their simplicity and efficiency, we suggest that GA-based methods face the propensity towards excessive unlearning, resulting in various undesirable model behaviors, such as catastrophic forgetting, that diminish their practical utility.","In this paper, we suggest a set of metrics that can capture multiple facets of real-world utility and propose several controlling methods that can regulate the extent of excessive unlearning.","Accordingly, we suggest a general framework to better reflect the practical efficacy of various unlearning methods -- we begin by controlling the unlearning procedures/unlearned models such that no excessive unlearning occurs and follow by the evaluation for unlearning efficacy.","Our experimental analysis on established benchmarks revealed that GA-based methods are far from perfect in practice, as strong unlearning is at the high cost of hindering the model utility.","We conclude that there is still a long way towards practical and effective LLM unlearning, and more efforts are required in this field."],"url":"http://arxiv.org/abs/2406.09179v1","category":"cs.LG"}
{"created":"2024-06-13 14:40:43","title":"AutomaChef: A Physics-informed Demonstration-guided Learning Framework for Granular Material Manipulation","abstract":"Due to the complex physical properties of granular materials, research on robot learning for manipulating such materials predominantly either disregards the consideration of their physical characteristics or uses surrogate models to approximate their physical properties. Learning to manipulate granular materials based on physical information obtained through precise modelling remains an unsolved problem. In this paper, we propose to address this challenge by constructing a differentiable physics simulator for granular materials based on the Taichi programming language and developing a learning framework accelerated by imperfect demonstrations that are generated via gradient-based optimisation on non-granular materials through our simulator. Experimental results show that our method trains three policies that, when chained, are capable of executing the task of transporting granular materials in both simulated and real-world scenarios, which existing popular deep reinforcement learning models fail to accomplish.","sentences":["Due to the complex physical properties of granular materials, research on robot learning for manipulating such materials predominantly either disregards the consideration of their physical characteristics or uses surrogate models to approximate their physical properties.","Learning to manipulate granular materials based on physical information obtained through precise modelling remains an unsolved problem.","In this paper, we propose to address this challenge by constructing a differentiable physics simulator for granular materials based on the Taichi programming language and developing a learning framework accelerated by imperfect demonstrations that are generated via gradient-based optimisation on non-granular materials through our simulator.","Experimental results show that our method trains three policies that, when chained, are capable of executing the task of transporting granular materials in both simulated and real-world scenarios, which existing popular deep reinforcement learning models fail to accomplish."],"url":"http://arxiv.org/abs/2406.09178v1","category":"cs.RO"}
{"created":"2024-06-13 14:37:33","title":"Spherical collapse and black hole evaporation","abstract":"We consider spherically symmetric gravity coupled to a spherically symmetric scalar field with a specific coupling which depends on the Areal Radius. Classical collapse is described by the Vaidya solution. The semiclassical Einstein equations are well defined. Their solution describes an evaporating black hole. A balance law at future null infinity relates the rate of change of a back reaction-corrected Bondi mass to a manifestly positive flux. Its detailed form suggests a quantum extension of future null infinity along which all information including that in the collapsing matter is expected to emerge.","sentences":["We consider spherically symmetric gravity coupled to a spherically symmetric scalar field with a specific coupling which depends on the Areal Radius.","Classical collapse is described by the Vaidya solution.","The semiclassical Einstein equations are well defined.","Their solution describes an evaporating black hole.","A balance law at future null infinity relates the rate of change of a back reaction-corrected Bondi mass to a manifestly positive flux.","Its detailed form suggests a quantum extension of future null infinity along which all information including that in the collapsing matter is expected to emerge."],"url":"http://arxiv.org/abs/2406.09176v1","category":"gr-qc"}
{"created":"2024-06-13 14:32:43","title":"Generative vs. Discriminative modeling under the lens of uncertainty quantification","abstract":"Learning a parametric model from a given dataset indeed enables to capture intrinsic dependencies between random variables via a parametric conditional probability distribution and in turn predict the value of a label variable given observed variables. In this paper, we undertake a comparative analysis of generative and discriminative approaches which differ in their construction and the structure of the underlying inference problem. Our objective is to compare the ability of both approaches to leverage information from various sources in an epistemic uncertainty aware inference via the posterior predictive distribution. We assess the role of a prior distribution, explicit in the generative case and implicit in the discriminative case, leading to a discussion about discriminative models suffering from imbalanced dataset. We next examine the double role played by the observed variables in the generative case, and discuss the compatibility of both approaches with semi-supervised learning. We also provide with practical insights and we examine how the modeling choice impacts the sampling from the posterior predictive distribution. With regard to this, we propose a general sampling scheme enabling supervised learning for both approaches, as well as semi-supervised learning when compatible with the considered modeling approach. Throughout this paper, we illustrate our arguments and conclusions using the example of affine regression, and validate our comparative analysis through classification simulations using neural network based models.","sentences":["Learning a parametric model from a given dataset indeed enables to capture intrinsic dependencies between random variables via a parametric conditional probability distribution and in turn predict the value of a label variable given observed variables.","In this paper, we undertake a comparative analysis of generative and discriminative approaches which differ in their construction and the structure of the underlying inference problem.","Our objective is to compare the ability of both approaches to leverage information from various sources in an epistemic uncertainty aware inference via the posterior predictive distribution.","We assess the role of a prior distribution, explicit in the generative case and implicit in the discriminative case, leading to a discussion about discriminative models suffering from imbalanced dataset.","We next examine the double role played by the observed variables in the generative case, and discuss the compatibility of both approaches with semi-supervised learning.","We also provide with practical insights and we examine how the modeling choice impacts the sampling from the posterior predictive distribution.","With regard to this, we propose a general sampling scheme enabling supervised learning for both approaches, as well as semi-supervised learning when compatible with the considered modeling approach.","Throughout this paper, we illustrate our arguments and conclusions using the example of affine regression, and validate our comparative analysis through classification simulations using neural network based models."],"url":"http://arxiv.org/abs/2406.09172v1","category":"stat.ML"}
{"created":"2024-06-13 14:28:37","title":"Vision Transformer Segmentation for Visual Bird Sound Denoising","abstract":"Audio denoising, especially in the context of bird sounds, remains a challenging task due to persistent residual noise. Traditional and deep learning methods often struggle with artificial or low-frequency noise. In this work, we propose ViTVS, a novel approach that leverages the power of the vision transformer (ViT) architecture. ViTVS adeptly combines segmentation techniques to disentangle clean audio from complex signal mixtures. Our key contributions encompass the development of ViTVS, introducing comprehensive, long-range, and multi-scale representations. These contributions directly tackle the limitations inherent in conventional approaches. Extensive experiments demonstrate that ViTVS outperforms state-of-the-art methods, positioning it as a benchmark solution for real-world bird sound denoising applications. Source code is available at: https://github.com/aiai-4/ViVTS.","sentences":["Audio denoising, especially in the context of bird sounds, remains a challenging task due to persistent residual noise.","Traditional and deep learning methods often struggle with artificial or low-frequency noise.","In this work, we propose ViTVS, a novel approach that leverages the power of the vision transformer (ViT) architecture.","ViTVS adeptly combines segmentation techniques to disentangle clean audio from complex signal mixtures.","Our key contributions encompass the development of ViTVS, introducing comprehensive, long-range, and multi-scale representations.","These contributions directly tackle the limitations inherent in conventional approaches.","Extensive experiments demonstrate that ViTVS outperforms state-of-the-art methods, positioning it as a benchmark solution for real-world bird sound denoising applications.","Source code is available at: https://github.com/aiai-4/ViVTS."],"url":"http://arxiv.org/abs/2406.09167v1","category":"cs.SD"}
{"created":"2024-06-13 14:27:53","title":"Fine-Grained Domain Generalization with Feature Structuralization","abstract":"Fine-grained domain generalization (FGDG) is a more challenging task due to its small inter-class variations and relatively large intra-class disparities. When domain distribution changes, the fragility of subtle features leads to a pronounced deterioration in model performance.Nevertheless, humans inherently demonstrate the capacity for generalizing to out-of-distribution data, leveraging structured multi-granularity knowledge that emerges from discerning both the commonality and specificity within categories.Likewise, we propose a Feature Structuralized Domain Generalization (FSDG) model, wherein features experience structuralization into common, specific, and confounding segments, harmoniously aligned with their relevant semantic concepts, to elevate performance in FGDG. Specifically, feature structuralization (FS) is achieved through a decorrelation function on disentangled segments, constraints on common feature consistency, specific feature distinctiveness, and a prediction calibration operation across granularities. By imposing these stipulations, FSDG is prompted to disentangle and align features based on multi-granularity knowledge, facilitating robust subtle distinctions among categories. Extensive experimentation on three benchmarks consistently validates the superiority of FSDG over state-of-the-art counterparts, with an average improvement of 6.1% in terms of FGDG performance. Beyond that, the explainability analysis and experiments on various mainstream model architectures confirm the validity of FS.","sentences":["Fine-grained domain generalization (FGDG) is a more challenging task due to its small inter-class variations and relatively large intra-class disparities.","When domain distribution changes, the fragility of subtle features leads to a pronounced deterioration in model performance.","Nevertheless, humans inherently demonstrate the capacity for generalizing to out-of-distribution data, leveraging structured multi-granularity knowledge that emerges from discerning both the commonality and specificity within categories.","Likewise, we propose a Feature Structuralized Domain Generalization (FSDG) model, wherein features experience structuralization into common, specific, and confounding segments, harmoniously aligned with their relevant semantic concepts, to elevate performance in FGDG.","Specifically, feature structuralization (FS) is achieved through a decorrelation function on disentangled segments, constraints on common feature consistency, specific feature distinctiveness, and a prediction calibration operation across granularities.","By imposing these stipulations, FSDG is prompted to disentangle and align features based on multi-granularity knowledge, facilitating robust subtle distinctions among categories.","Extensive experimentation on three benchmarks consistently validates the superiority of FSDG over state-of-the-art counterparts, with an average improvement of 6.1% in terms of FGDG performance.","Beyond that, the explainability analysis and experiments on various mainstream model architectures confirm the validity of FS."],"url":"http://arxiv.org/abs/2406.09166v1","category":"cs.CV"}
{"created":"2024-06-13 14:27:15","title":"Generating QES potentials supporting zero energy normalizable states for an extended class of truncated Calogero Sutherland model","abstract":"It is commonly held that quantum states with energy $E = 0$ would belong to the continuum. However, several situations have been reported when a zero-energy state becomes bound subject to certain restrictions on the coupling constants defining the potential. In the present work, we present another evidence of the existence of regular zero-energy normalizable solutions for a system of QES potentials that correspond to rationally extended many-body truncated Calogero-Sutherland model. Our procedure is based upon the potential group approach with an underlying so$(2, 1)$ structure that utilizes the three cases guided by it on profitably carrying out a point canonical transformation. We deal with each case separately by suitably restricting the coupling parameters.","sentences":["It is commonly held that quantum states with energy $E = 0$ would belong to the continuum.","However, several situations have been reported when a zero-energy state becomes bound subject to certain restrictions on the coupling constants defining the potential.","In the present work, we present another evidence of the existence of regular zero-energy normalizable solutions for a system of QES potentials that correspond to rationally extended many-body truncated Calogero-Sutherland model.","Our procedure is based upon the potential group approach with an underlying so$(2, 1)$ structure that utilizes the three cases guided by it on profitably carrying out a point canonical transformation.","We deal with each case separately by suitably restricting the coupling parameters."],"url":"http://arxiv.org/abs/2406.09164v1","category":"quant-ph"}
{"created":"2024-06-13 14:26:43","title":"EMMA: Your Text-to-Image Diffusion Model Can Secretly Accept Multi-Modal Prompts","abstract":"Recent advancements in image generation have enabled the creation of high-quality images from text conditions. However, when facing multi-modal conditions, such as text combined with reference appearances, existing methods struggle to balance multiple conditions effectively, typically showing a preference for one modality over others. To address this challenge, we introduce EMMA, a novel image generation model accepting multi-modal prompts built upon the state-of-the-art text-to-image (T2I) diffusion model, ELLA. EMMA seamlessly incorporates additional modalities alongside text to guide image generation through an innovative Multi-modal Feature Connector design, which effectively integrates textual and supplementary modal information using a special attention mechanism. By freezing all parameters in the original T2I diffusion model and only adjusting some additional layers, we reveal an interesting finding that the pre-trained T2I diffusion model can secretly accept multi-modal prompts. This interesting property facilitates easy adaptation to different existing frameworks, making EMMA a flexible and effective tool for producing personalized and context-aware images and even videos. Additionally, we introduce a strategy to assemble learned EMMA modules to produce images conditioned on multiple modalities simultaneously, eliminating the need for additional training with mixed multi-modal prompts. Extensive experiments demonstrate the effectiveness of EMMA in maintaining high fidelity and detail in generated images, showcasing its potential as a robust solution for advanced multi-modal conditional image generation tasks.","sentences":["Recent advancements in image generation have enabled the creation of high-quality images from text conditions.","However, when facing multi-modal conditions, such as text combined with reference appearances, existing methods struggle to balance multiple conditions effectively, typically showing a preference for one modality over others.","To address this challenge, we introduce EMMA, a novel image generation model accepting multi-modal prompts built upon the state-of-the-art text-to-image (T2I) diffusion model, ELLA.","EMMA seamlessly incorporates additional modalities alongside text to guide image generation through an innovative Multi-modal Feature Connector design, which effectively integrates textual and supplementary modal information using a special attention mechanism.","By freezing all parameters in the original T2I diffusion model and only adjusting some additional layers, we reveal an interesting finding that the pre-trained T2I diffusion model can secretly accept multi-modal prompts.","This interesting property facilitates easy adaptation to different existing frameworks, making EMMA a flexible and effective tool for producing personalized and context-aware images and even videos.","Additionally, we introduce a strategy to assemble learned EMMA modules to produce images conditioned on multiple modalities simultaneously, eliminating the need for additional training with mixed multi-modal prompts.","Extensive experiments demonstrate the effectiveness of EMMA in maintaining high fidelity and detail in generated images, showcasing its potential as a robust solution for advanced multi-modal conditional image generation tasks."],"url":"http://arxiv.org/abs/2406.09162v1","category":"cs.CV"}
{"created":"2024-06-13 14:23:19","title":"Complex Image-Generative Diffusion Transformer for Audio Denoising","abstract":"The audio denoising technique has captured widespread attention in the deep neural network field. Recently, the audio denoising problem has been converted into an image generation task, and deep learning-based approaches have been applied to tackle this problem. However, its performance is still limited, leaving room for further improvement. In order to enhance audio denoising performance, this paper introduces a complex image-generative diffusion transformer that captures more information from the complex Fourier domain. We explore a novel diffusion transformer by integrating the transformer with a diffusion model. Our proposed model demonstrates the scalability of the transformer and expands the receptive field of sparse attention using attention diffusion. Our work is among the first to utilize diffusion transformers to deal with the image generation task for audio denoising. Extensive experiments on two benchmark datasets demonstrate that our proposed model outperforms state-of-the-art methods.","sentences":["The audio denoising technique has captured widespread attention in the deep neural network field.","Recently, the audio denoising problem has been converted into an image generation task, and deep learning-based approaches have been applied to tackle this problem.","However, its performance is still limited, leaving room for further improvement.","In order to enhance audio denoising performance, this paper introduces a complex image-generative diffusion transformer that captures more information from the complex Fourier domain.","We explore a novel diffusion transformer by integrating the transformer with a diffusion model.","Our proposed model demonstrates the scalability of the transformer and expands the receptive field of sparse attention using attention diffusion.","Our work is among the first to utilize diffusion transformers to deal with the image generation task for audio denoising.","Extensive experiments on two benchmark datasets demonstrate that our proposed model outperforms state-of-the-art methods."],"url":"http://arxiv.org/abs/2406.09161v1","category":"cs.SD"}
{"created":"2024-06-13 14:22:59","title":"Beyond the Frontier: Predicting Unseen Walls from Occupancy Grids by Learning from Floor Plans","abstract":"In this paper, we tackle the challenge of predicting the unseen walls of a partially observed environment as a set of 2D line segments, conditioned on occupancy grids integrated along the trajectory of a 360{\\deg} LIDAR sensor. A dataset of such occupancy grids and their corresponding target wall segments is collected by navigating a virtual robot between a set of randomly sampled waypoints in a collection of office-scale floor plans from a university campus. The line segment prediction task is formulated as an autoregressive sequence prediction task, and an attention-based deep network is trained on the dataset. The sequence-based autoregressive formulation is evaluated through predicted information gain, as in frontier-based autonomous exploration, demonstrating significant improvements over both non-predictive estimation and convolution-based image prediction found in the literature. Ablations on key components are evaluated, as well as sensor range and the occupancy grid's metric area. Finally, model generality is validated by predicting walls in a novel floor plan reconstructed on-the-fly in a real-world office environment.","sentences":["In this paper, we tackle the challenge of predicting the unseen walls of a partially observed environment as a set of 2D line segments, conditioned on occupancy grids integrated along the trajectory of a 360{\\deg} LIDAR sensor.","A dataset of such occupancy grids and their corresponding target wall segments is collected by navigating a virtual robot between a set of randomly sampled waypoints in a collection of office-scale floor plans from a university campus.","The line segment prediction task is formulated as an autoregressive sequence prediction task, and an attention-based deep network is trained on the dataset.","The sequence-based autoregressive formulation is evaluated through predicted information gain, as in frontier-based autonomous exploration, demonstrating significant improvements over both non-predictive estimation and convolution-based image prediction found in the literature.","Ablations on key components are evaluated, as well as sensor range and the occupancy grid's metric area.","Finally, model generality is validated by predicting walls in a novel floor plan reconstructed on-the-fly in a real-world office environment."],"url":"http://arxiv.org/abs/2406.09160v1","category":"cs.RO"}
{"created":"2024-06-13 14:22:12","title":"ALPHAGMUT: A Rationale-Guided Alpha Shape Graph Neural Network to Evaluate Mutation Effects","abstract":"In silico methods evaluating the mutation effects of missense mutations are providing an important approach for understanding mutations in personal genomes and identifying disease-relevant biomarkers. However, existing methods, including deep learning methods, heavily rely on sequence-aware information, and do not fully leverage the potential of available 3D structural information. In addition, these methods may exhibit an inability to predict mutations in domains difficult to formulate sequence-based embeddings. In this study, we introduce a novel rationale-guided graph neural network AlphaGMut to evaluate mutation effects and to distinguish pathogenic mutations from neutral mutations. We compute the alpha shapes of protein structures to obtain atomic-resolution edge connectivities and map them to an accurate residue-level graph representation. We then compute structural-, topological-, biophysical-, and sequence properties of the mutation sites, which are assigned as node attributes in the graph. These node attributes could effectively guide the graph neural network to learn the difference between pathogenic and neutral mutations using k-hop message passing with a short training period. We demonstrate that AlphaGMut outperforms state-of-the-art methods, including DeepMind's AlphaMissense, in many performance metrics. In addition, AlphaGMut has the advantage of performing well in alignment-free settings, which provides broader prediction coverage and better generalization compared to current methods requiring deep sequence-aware information.","sentences":["In silico methods evaluating the mutation effects of missense mutations are providing an important approach for understanding mutations in personal genomes and identifying disease-relevant biomarkers.","However, existing methods, including deep learning methods, heavily rely on sequence-aware information, and do not fully leverage the potential of available 3D structural information.","In addition, these methods may exhibit an inability to predict mutations in domains difficult to formulate sequence-based embeddings.","In this study, we introduce a novel rationale-guided graph neural network AlphaGMut to evaluate mutation effects and to distinguish pathogenic mutations from neutral mutations.","We compute the alpha shapes of protein structures to obtain atomic-resolution edge connectivities and map them to an accurate residue-level graph representation.","We then compute structural-, topological-, biophysical-, and sequence properties of the mutation sites, which are assigned as node attributes in the graph.","These node attributes could effectively guide the graph neural network to learn the difference between pathogenic and neutral mutations using k-hop message passing with a short training period.","We demonstrate that AlphaGMut outperforms state-of-the-art methods, including DeepMind's AlphaMissense, in many performance metrics.","In addition, AlphaGMut has the advantage of performing well in alignment-free settings, which provides broader prediction coverage and better generalization compared to current methods requiring deep sequence-aware information."],"url":"http://arxiv.org/abs/2406.09159v1","category":"q-bio.QM"}
{"created":"2024-06-13 14:20:22","title":"Free-space quantum information platform on a chip","abstract":"Emerging technologies that employ quantum physics offer fundamental enhancements in information processing tasks, including sensing, communications, and computing. Here, we introduce the quantum phased array, which generalizes the operating principles of phased arrays and wavefront engineering to quantum fields, and report the first quantum phased array technology demonstration. An integrated photonic-electronic system is used to manipulate free-space quantum information to establish reconfigurable wireless quantum links in a standalone, compact form factor. Such a robust, scalable, and integrated quantum platform can enable broad deployment of quantum technologies with high connectivity, potentially expanding their use cases to real-world applications. We report the first, to our knowledge, free-space-to-chip interface for quantum links, enabled by 32 metamaterial antennas with more than 500,000 sub-wavelength engineered nanophotonic elements over a 550 x 550 $\\mathrm{\\mu m}^2$ physical aperture. We implement a 32-channel array of quantum coherent receivers with 30.3 dB shot noise clearance and 90.2 dB common-mode rejection ratio that downconverts the quantum optical information via homodyne detection and processes it coherently in the radio-frequency domain. With our platform, we demonstrate 32-pixel imaging of squeezed light for quantum sensing, reconfigurable free-space links for quantum communications, and proof-of-concept entanglement generation for measurement-based quantum computing. This approach offers targeted, real-time, dynamically-adjustable free-space capabilities to integrated quantum systems that can enable wireless quantum technologies.","sentences":["Emerging technologies that employ quantum physics offer fundamental enhancements in information processing tasks, including sensing, communications, and computing.","Here, we introduce the quantum phased array, which generalizes the operating principles of phased arrays and wavefront engineering to quantum fields, and report the first quantum phased array technology demonstration.","An integrated photonic-electronic system is used to manipulate free-space quantum information to establish reconfigurable wireless quantum links in a standalone, compact form factor.","Such a robust, scalable, and integrated quantum platform can enable broad deployment of quantum technologies with high connectivity, potentially expanding their use cases to real-world applications.","We report the first, to our knowledge, free-space-to-chip interface for quantum links, enabled by 32 metamaterial antennas with more than 500,000 sub-wavelength engineered nanophotonic elements over a 550 x 550 $\\mathrm{\\mu m}^2$ physical aperture.","We implement a 32-channel array of quantum coherent receivers with 30.3 dB shot noise clearance and 90.2 dB common-mode rejection ratio that downconverts the quantum optical information via homodyne detection and processes it coherently in the radio-frequency domain.","With our platform, we demonstrate 32-pixel imaging of squeezed light for quantum sensing, reconfigurable free-space links for quantum communications, and proof-of-concept entanglement generation for measurement-based quantum computing.","This approach offers targeted, real-time, dynamically-adjustable free-space capabilities to integrated quantum systems that can enable wireless quantum technologies."],"url":"http://arxiv.org/abs/2406.09158v1","category":"quant-ph"}
{"created":"2024-06-13 14:18:59","title":"Uncertainty of quantum channels based on symmetrized \\r{ho}-absolute variance and modified Wigner-Yanase skew information","abstract":"We present the uncertainty relations in terms of the symmetrized \\r{ho}-absolute variance, which generalizes the uncertainty relations for arbitrary operator (not necessarily Hermitian) to quantum channels. By recalling the quantity |U\\r{ho}|({\\Phi}) proposed by Zhang et al. (Quantum Inf. Process. 22 456, 2023), which involves terms of more quantum mechanical nature. We also establish the tighter uncertainty relations for quantum channels by using Cauchy-Schwarz inequality. Detailed examples are provided to illustrate the tightness of our results.","sentences":["We present the uncertainty relations in terms of the symmetrized \\r{ho}-absolute variance, which generalizes the uncertainty relations for arbitrary operator (not necessarily Hermitian) to quantum channels.","By recalling the quantity |U\\r{ho}|({\\Phi}) proposed by Zhang et al.","(Quantum Inf.","Process.","22 456, 2023), which involves terms of more quantum mechanical nature.","We also establish the tighter uncertainty relations for quantum channels by using Cauchy-Schwarz inequality.","Detailed examples are provided to illustrate the tightness of our results."],"url":"http://arxiv.org/abs/2406.09157v1","category":"quant-ph"}
{"created":"2024-06-13 14:18:13","title":"DefAn: Definitive Answer Dataset for LLMs Hallucination Evaluation","abstract":"Large Language Models (LLMs) have demonstrated remarkable capabilities, revolutionizing the integration of AI in daily life applications. However, they are prone to hallucinations, generating claims that contradict established facts, deviating from prompts, and producing inconsistent responses when the same prompt is presented multiple times. Addressing these issues is challenging due to the lack of comprehensive and easily assessable benchmark datasets. Most existing datasets are small and rely on multiple-choice questions, which are inadequate for evaluating the generative prowess of LLMs. To measure hallucination in LLMs, this paper introduces a comprehensive benchmark dataset comprising over 75,000 prompts across eight domains. These prompts are designed to elicit definitive, concise, and informative answers. The dataset is divided into two segments: one publicly available for testing and assessing LLM performance and a hidden segment for benchmarking various LLMs. In our experiments, we tested six LLMs-GPT-3.5, LLama 2, LLama 3, Gemini, Mixtral, and Zephyr-revealing that overall factual hallucination ranges from 59% to 82% on the public dataset and 57% to 76% in the hidden benchmark. Prompt misalignment hallucination ranges from 6% to 95% in the public dataset and 17% to 94% in the hidden counterpart. Average consistency ranges from 21% to 61% and 22% to 63%, respectively. Domain-wise analysis shows that LLM performance significantly deteriorates when asked for specific numeric information while performing moderately with person, location, and date queries. Our dataset demonstrates its efficacy and serves as a comprehensive benchmark for LLM performance evaluation. Our dataset and LLMs responses are available at \\href{https://github.com/ashikiut/DefAn}{https://github.com/ashikiut/DefAn}.","sentences":["Large Language Models (LLMs) have demonstrated remarkable capabilities, revolutionizing the integration of AI in daily life applications.","However, they are prone to hallucinations, generating claims that contradict established facts, deviating from prompts, and producing inconsistent responses when the same prompt is presented multiple times.","Addressing these issues is challenging due to the lack of comprehensive and easily assessable benchmark datasets.","Most existing datasets are small and rely on multiple-choice questions, which are inadequate for evaluating the generative prowess of LLMs.","To measure hallucination in LLMs, this paper introduces a comprehensive benchmark dataset comprising over 75,000 prompts across eight domains.","These prompts are designed to elicit definitive, concise, and informative answers.","The dataset is divided into two segments: one publicly available for testing and assessing LLM performance and a hidden segment for benchmarking various LLMs.","In our experiments, we tested six LLMs-GPT-3.5, LLama 2, LLama 3, Gemini, Mixtral, and Zephyr-revealing that overall factual hallucination ranges from 59% to 82% on the public dataset and 57% to 76% in the hidden benchmark.","Prompt misalignment hallucination ranges from 6% to 95% in the public dataset and 17% to 94% in the hidden counterpart.","Average consistency ranges from 21% to 61% and 22% to 63%, respectively.","Domain-wise analysis shows that LLM performance significantly deteriorates when asked for specific numeric information while performing moderately with person, location, and date queries.","Our dataset demonstrates its efficacy and serves as a comprehensive benchmark for LLM performance evaluation.","Our dataset and LLMs responses are available at \\href{https://github.com/ashikiut/DefAn}{https://github.com/ashikiut/DefAn}."],"url":"http://arxiv.org/abs/2406.09155v1","category":"cs.CL"}
{"created":"2024-06-13 14:11:19","title":"Generative AI-based Prompt Evolution Engineering Design Optimization With Vision-Language Model","abstract":"Engineering design optimization requires an efficient combination of a 3D shape representation, an optimization algorithm, and a design performance evaluation method, which is often computationally expensive. We present a prompt evolution design optimization (PEDO) framework contextualized in a vehicle design scenario that leverages a vision-language model for penalizing impractical car designs synthesized by a generative model. The backbone of our framework is an evolutionary strategy coupled with an optimization objective function that comprises a physics-based solver and a vision-language model for practical or functional guidance in the generated car designs. In the prompt evolutionary search, the optimizer iteratively generates a population of text prompts, which embed user specifications on the aerodynamic performance and visual preferences of the 3D car designs. Then, in addition to the computational fluid dynamics simulations, the pre-trained vision-language model is used to penalize impractical designs and, thus, foster the evolutionary algorithm to seek more viable designs. Our investigations on a car design optimization problem show a wide spread of potential car designs generated at the early phase of the search, which indicates a good diversity of designs in the initial populations, and an increase of over 20\\% in the probability of generating practical designs compared to a baseline framework without using a vision-language model. Visual inspection of the designs against the performance results demonstrates prompt evolution as a very promising paradigm for finding novel designs with good optimization performance while providing ease of use in specifying design specifications and preferences via a natural language interface.","sentences":["Engineering design optimization requires an efficient combination of a 3D shape representation, an optimization algorithm, and a design performance evaluation method, which is often computationally expensive.","We present a prompt evolution design optimization (PEDO) framework contextualized in a vehicle design scenario that leverages a vision-language model for penalizing impractical car designs synthesized by a generative model.","The backbone of our framework is an evolutionary strategy coupled with an optimization objective function that comprises a physics-based solver and a vision-language model for practical or functional guidance in the generated car designs.","In the prompt evolutionary search, the optimizer iteratively generates a population of text prompts, which embed user specifications on the aerodynamic performance and visual preferences of the 3D car designs.","Then, in addition to the computational fluid dynamics simulations, the pre-trained vision-language model is used to penalize impractical designs and, thus, foster the evolutionary algorithm to seek more viable designs.","Our investigations on a car design optimization problem show a wide spread of potential car designs generated at the early phase of the search, which indicates a good diversity of designs in the initial populations, and an increase of over 20\\% in the probability of generating practical designs compared to a baseline framework without using a vision-language model.","Visual inspection of the designs against the performance results demonstrates prompt evolution as a very promising paradigm for finding novel designs with good optimization performance while providing ease of use in specifying design specifications and preferences via a natural language interface."],"url":"http://arxiv.org/abs/2406.09143v1","category":"cs.AI"}
{"created":"2024-06-13 14:08:46","title":"Numerical relativity simulations of compact binaries: comparison of cell- and vertex-centered adaptive meshes","abstract":"Given the compact binary evolution problem of numerical relativity, in the finite-difference, block-based, adaptive mesh refinement context, choices must be made on how evolved fields are to be discretized. In GR-Athena++, the space-time solver was previously fixed to be vertex-centered. Here, our recent extensions to a cell-centered treatment, are described. Simplifications in the handling of variables during the treatment of general relativistic magneto-hydrodynamical (GRMHD) evolution are found. A novelty is that performance comparison for the two choices of grid sampling is made within a single code-base. In the case of a binary black hole inspiral-merger problem, by evolving geometric fields on vertex-centers, an average $\\sim 20\\%$ speed increase is observed, when compared against cell-centered sampling. The opposite occurs in the GRMHD setting. A binary neutron star inspiral-merger-collapse problem, representative of typical production simulations is considered. We find that cell-centered sampling for the space-time solver improves performance, by a similar factor.","sentences":["Given the compact binary evolution problem of numerical relativity, in the finite-difference, block-based, adaptive mesh refinement context, choices must be made on how evolved fields are to be discretized.","In GR-Athena++, the space-time solver was previously fixed to be vertex-centered.","Here, our recent extensions to a cell-centered treatment, are described.","Simplifications in the handling of variables during the treatment of general relativistic magneto-hydrodynamical (GRMHD) evolution are found.","A novelty is that performance comparison for the two choices of grid sampling is made within a single code-base.","In the case of a binary black hole inspiral-merger problem, by evolving geometric fields on vertex-centers, an average $\\sim 20\\%$ speed increase is observed, when compared against cell-centered sampling.","The opposite occurs in the GRMHD setting.","A binary neutron star inspiral-merger-collapse problem, representative of typical production simulations is considered.","We find that cell-centered sampling for the space-time solver improves performance, by a similar factor."],"url":"http://arxiv.org/abs/2406.09139v1","category":"gr-qc"}
{"created":"2024-06-13 14:07:52","title":"Leveraging Explicit Reasoning for Inference Integration in Commonsense-Augmented Dialogue Models","abstract":"Open-domain dialogue systems need to grasp social commonsense to understand and respond effectively to human users. Commonsense-augmented dialogue models have been proposed that aim to infer commonsense knowledge from dialogue contexts in order to improve response quality. However, existing approaches to commonsense-augmented dialogue rely on implicit reasoning to integrate commonsense inferences during response generation. In this study, we explore the impact of explicit reasoning against implicit reasoning over commonsense for dialogue response generation. Our findings demonstrate that separating commonsense reasoning into explicit steps for generating, selecting, and integrating commonsense into responses leads to better dialogue interactions, improving naturalness, engagement, specificity, and overall quality. Subsequent analyses of these findings unveil insights into the effectiveness of various types of commonsense in generating responses and the particular response traits enhanced through explicit reasoning for commonsense integration. Our work advances research in open-domain dialogue by achieving a new state-of-the-art in commonsense-augmented response generation.","sentences":["Open-domain dialogue systems need to grasp social commonsense to understand and respond effectively to human users.","Commonsense-augmented dialogue models have been proposed that aim to infer commonsense knowledge from dialogue contexts in order to improve response quality.","However, existing approaches to commonsense-augmented dialogue rely on implicit reasoning to integrate commonsense inferences during response generation.","In this study, we explore the impact of explicit reasoning against implicit reasoning over commonsense for dialogue response generation.","Our findings demonstrate that separating commonsense reasoning into explicit steps for generating, selecting, and integrating commonsense into responses leads to better dialogue interactions, improving naturalness, engagement, specificity, and overall quality.","Subsequent analyses of these findings unveil insights into the effectiveness of various types of commonsense in generating responses and the particular response traits enhanced through explicit reasoning for commonsense integration.","Our work advances research in open-domain dialogue by achieving a new state-of-the-art in commonsense-augmented response generation."],"url":"http://arxiv.org/abs/2406.09138v1","category":"cs.CL"}
{"created":"2024-06-13 14:07:02","title":"Chain of Preference Optimization: Improving Chain-of-Thought Reasoning in LLMs","abstract":"The recent development of chain-of-thought (CoT) decoding has enabled large language models (LLMs) to generate explicit logical reasoning paths for complex problem-solving. However, research indicates that these paths are not always deliberate and optimal. The tree-of-thought (ToT) method employs tree-searching to extensively explore the reasoning space and find better reasoning paths that CoT decoding might overlook. This deliberation, however, comes at the cost of significantly increased inference complexity. In this work, we demonstrate that fine-tuning LLMs leveraging the search tree constructed by ToT allows CoT to achieve similar or better performance, thereby avoiding the substantial inference burden. This is achieved through Chain of Preference Optimization (CPO), where LLMs are fine-tuned to align each step of the CoT reasoning paths with those of ToT using the inherent preference information in the tree-search process. Extensive experimental results show that CPO significantly improves LLM performance in solving a variety of complex problems, including question answering, fact verification, and arithmetic reasoning, demonstrating its effectiveness. Our code is available at https://github.com/sail-sg/CPO.","sentences":["The recent development of chain-of-thought (CoT) decoding has enabled large language models (LLMs) to generate explicit logical reasoning paths for complex problem-solving.","However, research indicates that these paths are not always deliberate and optimal.","The tree-of-thought (ToT) method employs tree-searching to extensively explore the reasoning space and find better reasoning paths that CoT decoding might overlook.","This deliberation, however, comes at the cost of significantly increased inference complexity.","In this work, we demonstrate that fine-tuning LLMs leveraging the search tree constructed by ToT allows CoT to achieve similar or better performance, thereby avoiding the substantial inference burden.","This is achieved through Chain of Preference Optimization (CPO), where LLMs are fine-tuned to align each step of the CoT reasoning paths with those of ToT using the inherent preference information in the tree-search process.","Extensive experimental results show that CPO significantly improves LLM performance in solving a variety of complex problems, including question answering, fact verification, and arithmetic reasoning, demonstrating its effectiveness.","Our code is available at https://github.com/sail-sg/CPO."],"url":"http://arxiv.org/abs/2406.09136v1","category":"cs.CL"}
{"created":"2024-06-13 14:01:34","title":"Time-Series Forecasting for Out-of-Distribution Generalization Using Invariant Learning","abstract":"Time-series forecasting (TSF) finds broad applications in real-world scenarios. Due to the dynamic nature of time-series data, it is crucial to equip TSF models with out-of-distribution (OOD) generalization abilities, as historical training data and future test data can have different distributions. In this paper, we aim to alleviate the inherent OOD problem in TSF via invariant learning. We identify fundamental challenges of invariant learning for TSF. First, the target variables in TSF may not be sufficiently determined by the input due to unobserved core variables in TSF, breaking the conventional assumption of invariant learning. Second, time-series datasets lack adequate environment labels, while existing environmental inference methods are not suitable for TSF.   To address these challenges, we propose FOIL, a model-agnostic framework that enables timeseries Forecasting for Out-of-distribution generalization via Invariant Learning. FOIL employs a novel surrogate loss to mitigate the impact of unobserved variables. Further, FOIL implements a joint optimization by alternately inferring environments effectively with a multi-head network while preserving the temporal adjacency structure, and learning invariant representations across inferred environments for OOD generalized TSF. We demonstrate that the proposed FOIL significantly improves the performance of various TSF models, achieving gains of up to 85%.","sentences":["Time-series forecasting (TSF) finds broad applications in real-world scenarios.","Due to the dynamic nature of time-series data, it is crucial to equip TSF models with out-of-distribution (OOD) generalization abilities, as historical training data and future test data can have different distributions.","In this paper, we aim to alleviate the inherent OOD problem in TSF via invariant learning.","We identify fundamental challenges of invariant learning for TSF.","First, the target variables in TSF may not be sufficiently determined by the input due to unobserved core variables in TSF, breaking the conventional assumption of invariant learning.","Second, time-series datasets lack adequate environment labels, while existing environmental inference methods are not suitable for TSF.   ","To address these challenges, we propose FOIL, a model-agnostic framework that enables timeseries Forecasting for Out-of-distribution generalization via Invariant Learning.","FOIL employs a novel surrogate loss to mitigate the impact of unobserved variables.","Further, FOIL implements a joint optimization by alternately inferring environments effectively with a multi-head network while preserving the temporal adjacency structure, and learning invariant representations across inferred environments for OOD generalized TSF.","We demonstrate that the proposed FOIL significantly improves the performance of various TSF models, achieving gains of up to 85%."],"url":"http://arxiv.org/abs/2406.09130v1","category":"cs.LG"}
{"created":"2024-06-13 14:01:15","title":"Si1-x-yGeySnx alloy formation by Sn ion implantation and flash lamp annealing","abstract":"For many years, Si1-yGey alloys have been applied in the semiconductor industry due to the ability to adjust the performance of Si-based nanoelectronic devices. Following this alloying approach of group-IV semiconductors, adding tin (Sn) into the alloy appears as the obvious next step, which leads to additional possibilities for tailoring the material properties. Adding Sn enables effective band gap and strain engineering and can improve the carrier mobilities, which makes Si1-x-yGeySnx alloys promising candidates for future opto- and nanoelectronics applications. The bottom-up approach for epitaxial growth of Si1-x-yGeySnx, e.g., by chemical vapor deposition and molecular beam epitaxy, allows tuning the material properties in the growth direction only; the realization of local material modifications to generate lateral heterostructures with such a bottom-up approach is extremely elaborate, since it would require the use of lithography, etching, and either selective epitaxy or epitaxy and chemical-mechanical polishing giving rise to interface issues, non-planar substrates, etc. This article shows the possibility of fabricating Si1-x-yGeySnx alloys by Sn ion beam implantation into Si1-yGey layers followed by millisecond-range flash lamp annealing (FLA). The materials are investigated by Rutherford backscattering spectrometry, micro Raman spectroscopy, X-ray diffraction, and transmission electron microscopy. The fabrication approach was adapted to ultra-thin Si1-yGey layers on silicon-on-insulator substrates. The results show the fabrication of single-crystalline Si1-x-yGeySnx with up to 2.3 at.% incorporated Sn without any indication of Sn segregation after recrystallization via FLA. Finally, we exhibit the possibility of implanting Sn locally in ultra-thin Si1-yGey films by masking unstructured regions on the chip.","sentences":["For many years, Si1-yGey alloys have been applied in the semiconductor industry due to the ability to adjust the performance of Si-based nanoelectronic devices.","Following this alloying approach of group-IV semiconductors, adding tin (Sn) into the alloy appears as the obvious next step, which leads to additional possibilities for tailoring the material properties.","Adding Sn enables effective band gap and strain engineering and can improve the carrier mobilities, which makes Si1-x-yGeySnx alloys promising candidates for future opto- and nanoelectronics applications.","The bottom-up approach for epitaxial growth of Si1-x-yGeySnx, e.g., by chemical vapor deposition and molecular beam epitaxy, allows tuning the material properties in the growth direction only; the realization of local material modifications to generate lateral heterostructures with such a bottom-up approach is extremely elaborate, since it would require the use of lithography, etching, and either selective epitaxy or epitaxy and chemical-mechanical polishing giving rise to interface issues, non-planar substrates, etc.","This article shows the possibility of fabricating Si1-x-yGeySnx alloys by Sn ion beam implantation into Si1-yGey layers followed by millisecond-range flash lamp annealing (FLA).","The materials are investigated by Rutherford backscattering spectrometry, micro Raman spectroscopy, X-ray diffraction, and transmission electron microscopy.","The fabrication approach was adapted to ultra-thin Si1-yGey layers on silicon-on-insulator substrates.","The results show the fabrication of single-crystalline Si1-x-yGeySnx with up to 2.3 at.% incorporated Sn without any indication of Sn segregation after recrystallization via FLA.","Finally, we exhibit the possibility of implanting Sn locally in ultra-thin Si1-yGey films by masking unstructured regions on the chip."],"url":"http://arxiv.org/abs/2406.09129v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-13 13:59:47","title":"Auto-Vocabulary Segmentation for LiDAR Points","abstract":"Existing perception methods for autonomous driving fall short of recognizing unknown entities not covered in the training data. Open-vocabulary methods offer promising capabilities in detecting any object but are limited by user-specified queries representing target classes. We propose AutoVoc3D, a framework for automatic object class recognition and open-ended segmentation. Evaluation on nuScenes showcases AutoVoc3D's ability to generate precise semantic classes and accurate point-wise segmentation. Moreover, we introduce Text-Point Semantic Similarity, a new metric to assess the semantic similarity between text and point cloud without eliminating novel classes.","sentences":["Existing perception methods for autonomous driving fall short of recognizing unknown entities not covered in the training data.","Open-vocabulary methods offer promising capabilities in detecting any object but are limited by user-specified queries representing target classes.","We propose AutoVoc3D, a framework for automatic object class recognition and open-ended segmentation.","Evaluation on nuScenes showcases AutoVoc3D's ability to generate precise semantic classes and accurate point-wise segmentation.","Moreover, we introduce Text-Point Semantic Similarity, a new metric to assess the semantic similarity between text and point cloud without eliminating novel classes."],"url":"http://arxiv.org/abs/2406.09126v1","category":"cs.CV"}
{"created":"2024-06-13 13:59:45","title":"Wavelength tuning of VCSELs via controlled strain","abstract":"Besides major advantages for telecommunication applications, VCSELs have attracted interest for their potential for neuro-inspired computing, frequency comb generation or high-frequency spin oscillations. In the meantime, strain applied to the laser structure has been shown to have a significant impact on the laser emission properties such as the polarization dynamics or birefringence. In this work, we further explore the influence of strain on VCSELs and how this effect could be used to fine tune the laser wavelength. Through a comprehensive investigation, we demonstrate consistent wavelength shift up to 1 nm and report a sensitivity between 0.12 to 0.18 nm / millistrain. We also record birefringence values up to 292 GHz. Our results show that controlled strain level could be considered for fine wavelength tuning and possibly alleviate the selection of VCSEL for precise wavelength requirements.","sentences":["Besides major advantages for telecommunication applications, VCSELs have attracted interest for their potential for neuro-inspired computing, frequency comb generation or high-frequency spin oscillations.","In the meantime, strain applied to the laser structure has been shown to have a significant impact on the laser emission properties such as the polarization dynamics or birefringence.","In this work, we further explore the influence of strain on VCSELs and how this effect could be used to fine tune the laser wavelength.","Through a comprehensive investigation, we demonstrate consistent wavelength shift up to 1 nm and report a sensitivity between 0.12 to 0.18 nm / millistrain.","We also record birefringence values up to 292 GHz.","Our results show that controlled strain level could be considered for fine wavelength tuning and possibly alleviate the selection of VCSEL for precise wavelength requirements."],"url":"http://arxiv.org/abs/2406.09125v1","category":"physics.optics"}
{"created":"2024-06-13 13:58:30","title":"Higher dimensional moduli spaces on Kuznetsov components of Fano threefolds","abstract":"We study moduli spaces of stable objects in the Kuznetsov components of Fano threefolds. We prove a general non-emptiness criterion for moduli spaces, which applies to the cases of prime Fano threefolds of index $1$, degree $10 \\leq d \\leq 18$, and index $2$, degree $d \\leq 4$. In the second part, we focus on cubic threefolds. We show the irreducibility of the moduli spaces, and that the general fibers of the Abel--Jacobi maps from the moduli spaces to the intermediate Jacobian are Fano varieties. When the dimension is sufficiently large, we further show that the general fibers of the Abel--Jacobi maps are stably birational equivalent to each other. As an application of our methods, we prove Conjecture A.1 in [FGLZ24] concerning the existence of Lagrangian subvarieties in moduli spaces of stable objects in the Kuznetsov components of very general cubic fourfolds.","sentences":["We study moduli spaces of stable objects in the Kuznetsov components of Fano threefolds.","We prove a general non-emptiness criterion for moduli spaces, which applies to the cases of prime Fano threefolds of index $1$, degree $10 \\leq d","\\leq 18$, and index $2$, degree $d \\leq","4$.","In the second part, we focus on cubic threefolds.","We show the irreducibility of the moduli spaces, and that the general fibers of the Abel--Jacobi maps from the moduli spaces to the intermediate Jacobian are Fano varieties.","When the dimension is sufficiently large, we further show that the general fibers of the Abel--Jacobi maps are stably birational equivalent to each other.","As an application of our methods, we prove Conjecture A.1 in [FGLZ24] concerning the existence of Lagrangian subvarieties in moduli spaces of stable objects in the Kuznetsov components of very general cubic fourfolds."],"url":"http://arxiv.org/abs/2406.09124v1","category":"math.AG"}
{"created":"2024-06-13 13:56:55","title":"PSN: Persian Social Norms Dataset for Cross-Cultural AI","abstract":"Datasets capturing cultural norms are essential for developing globally aware AI systems. We present Persian Social Norms (PSN) a novel dataset of over 1.7k Persian social norms, including environments, contexts, and cultural labels, alongside English translations. Leveraging large language models and prompt-engineering techniques, we generated potential norms that were reviewed by native speakers for quality and ethical compliance. As the first Persian dataset of its kind, this resource enables computational modeling of norm adaptation, a crucial challenge for cross-cultural AI informed by diverse cultural perspectives.","sentences":["Datasets capturing cultural norms are essential for developing globally aware AI systems.","We present Persian Social Norms (PSN) a novel dataset of over 1.7k Persian social norms, including environments, contexts, and cultural labels, alongside English translations.","Leveraging large language models and prompt-engineering techniques, we generated potential norms that were reviewed by native speakers for quality and ethical compliance.","As the first Persian dataset of its kind, this resource enables computational modeling of norm adaptation, a crucial challenge for cross-cultural AI informed by diverse cultural perspectives."],"url":"http://arxiv.org/abs/2406.09123v1","category":"cs.SI"}
{"created":"2024-06-13 13:55:22","title":"Primordial black hole formation from self-resonant preheating?","abstract":"We revisit the question of how generic is the formation of primordial black holes via self-resonant growth of inflaton fluctuations in the post-inflationary, preheating phase. Using analytical and lattice calculations, we find that primordial black hole production is far from being a generic outcome. Also, in most of the parameter space of viable inflationary models, the metric preheating term is subleading to the anharmonic terms and the approximation of a quadratic potential for describing the resonance dynamics is inadequate. Nonetheless, the anharmonicity of the potential cannot be used to rescue the mechanism: The generic outcome of the non-linear evolution of the scalar field in this case is the formation of metastable transients or oscillons, that do not generically collapse into black holes.","sentences":["We revisit the question of how generic is the formation of primordial black holes via self-resonant growth of inflaton fluctuations in the post-inflationary, preheating phase.","Using analytical and lattice calculations, we find that primordial black hole production is far from being a generic outcome.","Also, in most of the parameter space of viable inflationary models, the metric preheating term is subleading to the anharmonic terms and the approximation of a quadratic potential for describing the resonance dynamics is inadequate.","Nonetheless, the anharmonicity of the potential cannot be used to rescue the mechanism: The generic outcome of the non-linear evolution of the scalar field in this case is the formation of metastable transients or oscillons, that do not generically collapse into black holes."],"url":"http://arxiv.org/abs/2406.09122v1","category":"astro-ph.CO"}
{"created":"2024-06-13 13:44:31","title":"PC-LoRA: Low-Rank Adaptation for Progressive Model Compression with Knowledge Distillation","abstract":"Low-rank adaption (LoRA) is a prominent method that adds a small number of learnable parameters to the frozen pre-trained weights for parameter-efficient fine-tuning. Prompted by the question, ``Can we make its representation enough with LoRA weights solely at the final phase of finetuning without the pre-trained weights?'' In this work, we introduce Progressive Compression LoRA~(PC-LoRA), which utilizes low-rank adaptation (LoRA) to simultaneously perform model compression and fine-tuning. The PC-LoRA method gradually removes the pre-trained weights during the training process, eventually leaving only the low-rank adapters in the end. Thus, these low-rank adapters replace the whole pre-trained weights, achieving the goals of compression and fine-tuning at the same time. Empirical analysis across various models demonstrates that PC-LoRA achieves parameter and FLOPs compression rates of 94.36%/89.1% for vision models, e.g., ViT-B, and 93.42%/84.2% parameters and FLOPs compressions for language models, e.g., BERT.","sentences":["Low-rank adaption (LoRA) is a prominent method that adds a small number of learnable parameters to the frozen pre-trained weights for parameter-efficient fine-tuning.","Prompted by the question, ``Can we make its representation enough with LoRA weights solely at the final phase of finetuning without the pre-trained weights?''","In this work, we introduce Progressive Compression LoRA~(PC-LoRA), which utilizes low-rank adaptation (LoRA) to simultaneously perform model compression and fine-tuning.","The PC-LoRA method gradually removes the pre-trained weights during the training process, eventually leaving only the low-rank adapters in the end.","Thus, these low-rank adapters replace the whole pre-trained weights, achieving the goals of compression and fine-tuning at the same time.","Empirical analysis across various models demonstrates that PC-LoRA achieves parameter and FLOPs compression rates of 94.36%/89.1% for vision models, e.g., ViT-B, and 93.42%/84.2% parameters and FLOPs compressions for language models, e.g., BERT."],"url":"http://arxiv.org/abs/2406.09117v1","category":"cs.CV"}
{"created":"2024-06-13 13:43:59","title":"Injective Flows for parametric hypersurfaces","abstract":"Normalizing Flows (NFs) are powerful and efficient models for density estimation. When modeling densities on manifolds, NFs can be generalized to injective flows but the Jacobian determinant becomes computationally prohibitive. Current approaches either consider bounds on the log-likelihood or rely on some approximations of the Jacobian determinant. In contrast, we propose injective flows for parametric hypersurfaces and show that for such manifolds we can compute the Jacobian determinant exactly and efficiently, with the same cost as NFs. Furthermore, we show that for the subclass of star-like manifolds we can extend the proposed framework to always allow for a Cartesian representation of the density. We showcase the relevance of modeling densities on hypersurfaces in two settings. Firstly, we introduce a novel Objective Bayesian approach to penalized likelihood models by interpreting level-sets of the penalty as star-like manifolds. Secondly, we consider Bayesian mixture models and introduce a general method for variational inference by defining the posterior of mixture weights on the probability simplex.","sentences":["Normalizing Flows (NFs) are powerful and efficient models for density estimation.","When modeling densities on manifolds, NFs can be generalized to injective flows but the Jacobian determinant becomes computationally prohibitive.","Current approaches either consider bounds on the log-likelihood or rely on some approximations of the Jacobian determinant.","In contrast, we propose injective flows for parametric hypersurfaces and show that for such manifolds we can compute the Jacobian determinant exactly and efficiently, with the same cost as NFs.","Furthermore, we show that for the subclass of star-like manifolds we can extend the proposed framework to always allow for a Cartesian representation of the density.","We showcase the relevance of modeling densities on hypersurfaces in two settings.","Firstly, we introduce a novel Objective Bayesian approach to penalized likelihood models by interpreting level-sets of the penalty as star-like manifolds.","Secondly, we consider Bayesian mixture models and introduce a general method for variational inference by defining the posterior of mixture weights on the probability simplex."],"url":"http://arxiv.org/abs/2406.09116v1","category":"cs.LG"}
{"created":"2024-06-13 13:43:33","title":"Polynomial p-adic Low-Discrepancy Sequences","abstract":"The classic example of a low-discrepancy sequence in $\\mathbb{Z}_p$ is $(x_n) = an+b$ with $a \\in \\mathbb{Z}_p^x$ and $b \\in \\mathbb{Z}_p$. Here we address the non-linear case and show that a polynomial $f$ generates a low-discrepancy sequence in $\\mathbb{Z}_p$ if and only if it is a permutation polynomial $\\mod p$ and $\\mod p^2$. By this it is possible to construct non-linear examples of low-discrepancy sequences in $\\mathbb{Z}_p$ for all primes $p$. Moreover, we prove a criterion which decides for any given polynomial in $\\mathbb{Z}_p$ with $p \\in \\left\\{ 3,5, 7\\right\\}$ if it generates a low-discrepancy sequence. We also discuss connections to the theories of Poissonian pair correlations and real discrepancy.","sentences":["The classic example of a low-discrepancy sequence in $\\mathbb{Z}_p$ is $(x_n) = an+b$ with $a \\in \\mathbb{Z}_p^x$ and $b \\in \\mathbb{Z}_p$.","Here we address the non-linear case and show that a polynomial $f$ generates a low-discrepancy sequence in $\\mathbb{Z}_p$","if and only if it is a permutation polynomial $\\mod p$ and $\\mod p^2$. By this it is possible to construct non-linear examples of low-discrepancy sequences in $\\mathbb{Z}_p$ for all primes $p$.","Moreover, we prove a criterion which decides for any given polynomial in $\\mathbb{Z}_p$ with $p \\in \\left\\{ 3,5, 7\\right\\}$ if it generates a low-discrepancy sequence.","We also discuss connections to the theories of Poissonian pair correlations and real discrepancy."],"url":"http://arxiv.org/abs/2406.09114v1","category":"math.NT"}
{"created":"2024-06-13 13:43:01","title":"Large-Scale Evaluation of Open-Set Image Classification Techniques","abstract":"The goal for classification is to correctly assign labels to unseen samples. However, most methods misclassify samples with unseen labels and assign them to one of the known classes. Open-Set Classification (OSC) algorithms aim to maximize both closed and open-set recognition capabilities. Recent studies showed the utility of such algorithms on small-scale data sets, but limited experimentation makes it difficult to assess their performances in real-world problems. Here, we provide a comprehensive comparison of various OSC algorithms, including training-based (SoftMax, Garbage, EOS) and post-processing methods (Maximum SoftMax Scores, Maximum Logit Scores, OpenMax, EVM, PROSER), the latter are applied on features from the former. We perform our evaluation on three large-scale protocols that mimic real-world challenges, where we train on known and negative open-set samples, and test on known and unknown instances. Our results show that EOS helps to improve performance of almost all post-processing algorithms. Particularly, OpenMax and PROSER are able to exploit better-trained networks, demonstrating the utility of hybrid models. However, while most algorithms work well on negative test samples -- samples of open-set classes seen during training -- they tend to perform poorly when tested on samples of previously unseen unknown classes, especially in challenging conditions.","sentences":["The goal for classification is to correctly assign labels to unseen samples.","However, most methods misclassify samples with unseen labels and assign them to one of the known classes.","Open-Set Classification (OSC) algorithms aim to maximize both closed and open-set recognition capabilities.","Recent studies showed the utility of such algorithms on small-scale data sets, but limited experimentation makes it difficult to assess their performances in real-world problems.","Here, we provide a comprehensive comparison of various OSC algorithms, including training-based (SoftMax, Garbage, EOS) and post-processing methods (Maximum SoftMax Scores, Maximum Logit Scores, OpenMax, EVM, PROSER), the latter are applied on features from the former.","We perform our evaluation on three large-scale protocols that mimic real-world challenges, where we train on known and negative open-set samples, and test on known and unknown instances.","Our results show that EOS helps to improve performance of almost all post-processing algorithms.","Particularly, OpenMax and PROSER are able to exploit better-trained networks, demonstrating the utility of hybrid models.","However, while most algorithms work well on negative test samples -- samples of open-set classes seen during training -- they tend to perform poorly when tested on samples of previously unseen unknown classes, especially in challenging conditions."],"url":"http://arxiv.org/abs/2406.09112v1","category":"cs.CV"}
{"created":"2024-06-13 13:40:30","title":"Optimal demonstration of generalized quantum contextuality","abstract":"Finding a set of empirical criteria fulfilled by any theory that satisfies the generalized notion of noncontextuality is a challenging task of both operational and foundational importance. The conventional approach of deriving facet inequalities from the relevant noncontextual polytope is computationally demanding. Specifically, the noncontextual polytope is a product of two polytopes, one for preparations and the other for measurements, and the dimension of the former typically increases polynomially with the number of measurements. This work presents an alternative methodology for constructing a polytope that encompasses the actual noncontextual polytope while ensuring that the dimension of the polytope associated with the preparations remains constant regardless of the number of measurements and their outcome size. In particular, the facet inequalities of this polytope serve as necessary conditions for noncontextuality. To demonstrate the efficacy of our methodology, we apply it to nine distinct contextuality scenarios involving four to nine preparations and two to three measurements to obtain the respective sets of facet inequalities. Additionally, we retrieve the maximum quantum violations of these inequalities. Our investigation uncovers many novel non-trivial noncontextuality inequalities and reveals intriguing aspects and applications of quantum contextual correlations.","sentences":["Finding a set of empirical criteria fulfilled by any theory that satisfies the generalized notion of noncontextuality is a challenging task of both operational and foundational importance.","The conventional approach of deriving facet inequalities from the relevant noncontextual polytope is computationally demanding.","Specifically, the noncontextual polytope is a product of two polytopes, one for preparations and the other for measurements, and the dimension of the former typically increases polynomially with the number of measurements.","This work presents an alternative methodology for constructing a polytope that encompasses the actual noncontextual polytope while ensuring that the dimension of the polytope associated with the preparations remains constant regardless of the number of measurements and their outcome size.","In particular, the facet inequalities of this polytope serve as necessary conditions for noncontextuality.","To demonstrate the efficacy of our methodology, we apply it to nine distinct contextuality scenarios involving four to nine preparations and two to three measurements to obtain the respective sets of facet inequalities.","Additionally, we retrieve the maximum quantum violations of these inequalities.","Our investigation uncovers many novel non-trivial noncontextuality inequalities and reveals intriguing aspects and applications of quantum contextual correlations."],"url":"http://arxiv.org/abs/2406.09111v1","category":"quant-ph"}
{"created":"2024-06-13 13:38:57","title":"Projection algebras and free projection- and idempotent-generated regular $*$-semigroups","abstract":"The purpose of this paper is to introduce a new family of semigroups - the free projection-generated regular $*$-semigroups - and initiate their systematic study. Such a semigroup $PG(P)$ is constructed from a projection algebra $P$, using the recent groupoid approach to regular $*$-semigroups. The assignment $P\\mapsto PG(P)$ is a left adjoint to the forgetful functor that maps a regular $*$-semigroup $S$ to its projection algebra $P(S)$. In fact, the category of projection algebras is coreflective in the category of regular $*$-semigroups. The algebra $P(S)$ uniquely determines the biordered structure of the idempotents $E(S)$, up to isomorphism, and this leads to a category equivalence between projection algebras and regular $*$-biordered sets. As a consequence, $PG(P)$ can be viewed as a quotient of the classical free idempotent-generated (regular) semigroups $IG(E)$ and $RIG(E)$, where $E=E(PG(P))$; this is witnessed by a number of presentations in terms of generators and defining relations. The semigroup $PG(P)$ can also be viewed as the fundamental groupoid of a simplicial complex explicitly constructed from $P$. The theory is then illustrated on a number of examples. In one direction, the free construction applied to the projection algebras of adjacency semigroups yields a new family of graph-based path semigroups. In another, it turns out that, remarkably, the Temperley-Lieb monoid $TL_n$ is the free regular $*$-semigroup over its own projection algebra $P(TL_n)$.","sentences":["The purpose of this paper is to introduce a new family of semigroups - the free projection-generated regular $*$-semigroups - and initiate their systematic study.","Such a semigroup $PG(P)$ is constructed from a projection algebra $P$, using the recent groupoid approach to regular $*$-semigroups.","The assignment $P\\mapsto PG(P)$ is a left adjoint to the forgetful functor that maps a regular $*$-semigroup $S$ to its projection algebra $P(S)$.","In fact, the category of projection algebras is coreflective in the category of regular $*$-semigroups.","The algebra $P(S)$ uniquely determines the biordered structure of the idempotents $E(S)$, up to isomorphism, and this leads to a category equivalence between projection algebras and regular $*$-biordered sets.","As a consequence, $PG(P)$ can be viewed as a quotient of the classical free idempotent-generated (regular) semigroups $IG(E)$ and $RIG(E)$, where $E=E(PG(P))$; this is witnessed by a number of presentations in terms of generators and defining relations.","The semigroup $PG(P)$ can also be viewed as the fundamental groupoid of a simplicial complex explicitly constructed from $P$.","The theory is then illustrated on a number of examples.","In one direction, the free construction applied to the projection algebras of adjacency semigroups yields a new family of graph-based path semigroups.","In another, it turns out that, remarkably, the Temperley-Lieb monoid $TL_n$ is the free regular $*$-semigroup over its own projection algebra $P(TL_n)$."],"url":"http://arxiv.org/abs/2406.09109v1","category":"math.RA"}
{"created":"2024-06-13 13:31:49","title":"INS-MMBench: A Comprehensive Benchmark for Evaluating LVLMs' Performance in Insurance","abstract":"Large Vision-Language Models (LVLMs) have demonstrated outstanding performance in various general multimodal applications such as image recognition and visual reasoning, and have also shown promising potential in specialized domains. However, the application potential of LVLMs in the insurance domain-characterized by rich application scenarios and abundant multimodal data-has not been effectively explored. There is no systematic review of multimodal tasks in the insurance domain, nor a benchmark specifically designed to evaluate the capabilities of LVLMs in insurance. This gap hinders the development of LVLMs within the insurance domain. In this paper, we systematically review and distill multimodal tasks for four representative types of insurance: auto insurance, property insurance, health insurance, and agricultural insurance. We propose INS-MMBench, the first comprehensive LVLMs benchmark tailored for the insurance domain. INS-MMBench comprises a total of 2.2K thoroughly designed multiple-choice questions, covering 12 meta-tasks and 22 fundamental tasks. Furthermore, we evaluate multiple representative LVLMs, including closed-source models such as GPT-4o and open-source models like BLIP-2. This evaluation not only validates the effectiveness of our benchmark but also provides an in-depth performance analysis of current LVLMs on various multimodal tasks in the insurance domain. We hope that INS-MMBench will facilitate the further application of LVLMs in the insurance domain and inspire interdisciplinary development. Our dataset and evaluation code are available at https://github.com/FDU-INS/INS-MMBench.","sentences":["Large Vision-Language Models (LVLMs) have demonstrated outstanding performance in various general multimodal applications such as image recognition and visual reasoning, and have also shown promising potential in specialized domains.","However, the application potential of LVLMs in the insurance domain-characterized by rich application scenarios and abundant multimodal data-has not been effectively explored.","There is no systematic review of multimodal tasks in the insurance domain, nor a benchmark specifically designed to evaluate the capabilities of LVLMs in insurance.","This gap hinders the development of LVLMs within the insurance domain.","In this paper, we systematically review and distill multimodal tasks for four representative types of insurance: auto insurance, property insurance, health insurance, and agricultural insurance.","We propose INS-MMBench, the first comprehensive LVLMs benchmark tailored for the insurance domain.","INS-MMBench comprises a total of 2.2K thoroughly designed multiple-choice questions, covering 12 meta-tasks and 22 fundamental tasks.","Furthermore, we evaluate multiple representative LVLMs, including closed-source models such as GPT-4o and open-source models like BLIP-2.","This evaluation not only validates the effectiveness of our benchmark but also provides an in-depth performance analysis of current LVLMs on various multimodal tasks in the insurance domain.","We hope that INS-MMBench will facilitate the further application of LVLMs in the insurance domain and inspire interdisciplinary development.","Our dataset and evaluation code are available at https://github.com/FDU-INS/INS-MMBench."],"url":"http://arxiv.org/abs/2406.09105v1","category":"cs.CV"}
{"created":"2024-06-13 13:31:04","title":"A nA-Range Area-Efficient Sub-100-ppm/\u00b0C Peaking Current Reference Using Forward Body Biasing in 0.11-$\u03bc$m Bulk and 22-nm FD-SOI","abstract":"In recent years, the development of the Internet of Things (IoT) has prompted the search for nA-range current references that are simultaneously constrained to a small area and robust to process, voltage and temperature variations. Yet, such references have remained elusive, as existing architectures fail to reach a low temperature coefficient (TC) while minimizing silicon area. In this work, we propose a nA-range constant-with-temperature (CWT) peaking current reference, in which a resistor is biased by the threshold voltage difference between two transistors in weak inversion. This bias voltage is lower than in conventional architectures to cut down the silicon area occupied by the resistor and is obtained by forward body biasing one of the two transistors with an ultra-low-power voltage reference so as to reduce its threshold voltage. In addition, the proposed reference includes a circuit to suppress the leakage of parasitic diodes at high temperature, and two simple trimming mechanisms for the reference current and its TC. As the proposed design relies on the body effect, it has been validated in both 0.11-$\\mu$m bulk and 22-nm fully-depleted silicon-on-insulator, to demonstrate feasibility across different technology types. In post-layout simulation, the 0.11-$\\mu$m design generates a 5-nA current with a 65-ppm/{\\deg}C TC and a 2.84-%/V line sensitivity (LS), while in measurement, the 22-nm design achieves a 1.5-nA current with an 89-ppm/{\\deg}C TC and a 0.51-%/V LS. As a result of the low resistor bias voltage, the proposed references occupy a silicon area of 0.00954 mm$^2$ in 0.11 $\\mu$m (resp. 0.00214 mm$^2$ in 22 nm) at least 1.8$\\times$ (resp. 8.2$\\times$) smaller than fabricated nA-range CWT references, but with a TC improved by 6.1$\\times$ (resp. 4.4$\\times$).","sentences":["In recent years, the development of the Internet of Things (IoT) has prompted the search for nA-range current references that are simultaneously constrained to a small area and robust to process, voltage and temperature variations.","Yet, such references have remained elusive, as existing architectures fail to reach a low temperature coefficient (TC) while minimizing silicon area.","In this work, we propose a nA-range constant-with-temperature (CWT) peaking current reference, in which a resistor is biased by the threshold voltage difference between two transistors in weak inversion.","This bias voltage is lower than in conventional architectures to cut down the silicon area occupied by the resistor and is obtained by forward body biasing one of the two transistors with an ultra-low-power voltage reference so as to reduce its threshold voltage.","In addition, the proposed reference includes a circuit to suppress the leakage of parasitic diodes at high temperature, and two simple trimming mechanisms for the reference current and its TC.","As the proposed design relies on the body effect, it has been validated in both 0.11-$\\mu$m bulk and 22-nm fully-depleted silicon-on-insulator, to demonstrate feasibility across different technology types.","In post-layout simulation, the 0.11-$\\mu$m design generates a 5-nA current with a 65-ppm/{\\deg}C TC and a 2.84-%/V line sensitivity (LS), while in measurement, the 22-nm design achieves a 1.5-nA current with an 89-ppm/{\\deg}C TC and a 0.51-%/V LS.","As a result of the low resistor bias voltage, the proposed references occupy a silicon area of 0.00954 mm$^2$ in 0.11 $\\mu$m (resp.","0.00214 mm$^2$ in 22 nm) at least 1.8$\\times$ (resp.","8.2$\\times$) smaller than fabricated nA-range CWT references, but with a TC improved by 6.1$\\times$ (resp.","4.4$\\times$)."],"url":"http://arxiv.org/abs/2406.09104v1","category":"cs.AR"}
{"created":"2024-06-13 13:29:42","title":"V-static metrics and the volume-renormalised mass","abstract":"V-static metrics generalise the notion of static metrics, and stem from the work of Miao and Tam [arXiv:0807.2693], and Corvino, Eichmair and Miao [arXiv:1211.6168] on critical points of the volume functional over the space of compact manifolds with constant scalar curvature. In this article we show that these V-static metrics arise naturally in the context of asymptotically hyperbolic manifolds as critical points of the volume-renormalised mass, recently introduced by Dahl, Kr\\\"oncke and the author [arXiv:2307.06196].   In particular, we show that critical points of the volume-renormalised mass over the space of constant scalar curvature asymptotically hyperbolic manifolds without boundary, or satisfying appropriate boundary conditions, are exactly V-static metrics. This is directly analogous to the relationship between critical points of the ADM mass and static metrics for asymptotically flat manifolds.","sentences":["V-static metrics generalise the notion of static metrics, and stem from the work of Miao and Tam [arXiv:0807.2693], and Corvino, Eichmair and Miao [arXiv:1211.6168] on critical points of the volume functional over the space of compact manifolds with constant scalar curvature.","In this article we show that these V-static metrics arise naturally in the context of asymptotically hyperbolic manifolds as critical points of the volume-renormalised mass, recently introduced by Dahl, Kr\\\"oncke and the author [arXiv:2307.06196].   ","In particular, we show that critical points of the volume-renormalised mass over the space of constant scalar curvature asymptotically hyperbolic manifolds without boundary, or satisfying appropriate boundary conditions, are exactly V-static metrics.","This is directly analogous to the relationship between critical points of the ADM mass and static metrics for asymptotically flat manifolds."],"url":"http://arxiv.org/abs/2406.09101v1","category":"math.DG"}
{"created":"2024-06-13 13:25:50","title":"Modeling Comparative Logical Relation with Contrastive Learning for Text Generation","abstract":"Data-to-Text Generation (D2T), a classic natural language generation problem, aims at producing fluent descriptions for structured input data, such as a table. Existing D2T works mainly focus on describing the superficial associative relations among entities, while ignoring the deep comparative logical relations, such as A is better than B in a certain aspect with a corresponding opinion, which is quite common in our daily life. In this paper, we introduce a new D2T task named comparative logical relation generation (CLRG). Additionally, we propose a Comparative Logic (CoLo) based text generation method, which generates texts following specific comparative logical relations with contrastive learning. Specifically, we first construct various positive and negative samples by fine-grained perturbations in entities, aspects and opinions. Then, we perform contrastive learning in the encoder layer to have a better understanding of the comparative logical relations, and integrate it in the decoder layer to guide the model to correctly generate the relations. Noting the data scarcity problem, we construct a Chinese Comparative Logical Relation Dataset (CLRD), which is a high-quality human-annotated dataset and challenging for text generation with descriptions of multiple entities and annotations on their comparative logical relations. Extensive experiments show that our method achieves impressive performance in both automatic and human evaluations.","sentences":["Data-to-Text Generation (D2T), a classic natural language generation problem, aims at producing fluent descriptions for structured input data, such as a table.","Existing D2T works mainly focus on describing the superficial associative relations among entities, while ignoring the deep comparative logical relations, such as A is better than B in a certain aspect with a corresponding opinion, which is quite common in our daily life.","In this paper, we introduce a new D2T task named comparative logical relation generation (CLRG).","Additionally, we propose a Comparative Logic (CoLo) based text generation method, which generates texts following specific comparative logical relations with contrastive learning.","Specifically, we first construct various positive and negative samples by fine-grained perturbations in entities, aspects and opinions.","Then, we perform contrastive learning in the encoder layer to have a better understanding of the comparative logical relations, and integrate it in the decoder layer to guide the model to correctly generate the relations.","Noting the data scarcity problem, we construct a Chinese Comparative Logical Relation Dataset (CLRD), which is a high-quality human-annotated dataset and challenging for text generation with descriptions of multiple entities and annotations on their comparative logical relations.","Extensive experiments show that our method achieves impressive performance in both automatic and human evaluations."],"url":"http://arxiv.org/abs/2406.09095v1","category":"cs.CL"}
{"created":"2024-06-13 13:22:41","title":"Recombination enables higher numbers of recessive genes, contributing to the emergence of sexual mating in complex organisms","abstract":"The drift-barrier hypothesis states that random genetic drift constrains the refinement of a phenotype under natural selection. The influence of effective population size and the genome-wide deleterious mutation rate were studied theoretically, and an inverse relationship between mutation rate and genome size has been observed for many species. However, the effect of the recessive gene count, an important feature of the genomic architecture, is unknown. In a Wright-Fisher model, we studied the mutation burden for a growing number of N completely recessive and lethal disease genes. Diploid individuals are represented with a binary $2 \\times N$ matrix denoting wild-type and mutated alleles. Analytic results for specific cases were complemented by simulations across a broad parameter regime for gene count, mutation and recombination rates. Simulations revealed transitions to higher mutation burden and prevalence within a few generations that were linked to the extinction of the wild-type haplotype (least-loaded class). This metastability, that is, phases of quasi-equilibrium with intermittent transitions, persists over $100\\,000$ generations. The drift-barrier hypothesis is confirmed by a high mutation burden resulting in population collapse. Simulations showed the emergence of mutually exclusive haplotypes for a mutation rate above 0.02 lethal equivalents per generation for a genomic architecture and population size representing complex multicellular organisms such as humans. In such systems, recombination proves pivotal, preventing population collapse and maintaining a mutation burden below 10. This study advances our understanding of gene pool stability, and particularly the role of the number of recessive disorders. Insights into Muller`s ratchet dynamics are provided, and the essential role of recombination in curbing mutation burden and stabilizing the gene pool is demonstrated.","sentences":["The drift-barrier hypothesis states that random genetic drift constrains the refinement of a phenotype under natural selection.","The influence of effective population size and the genome-wide deleterious mutation rate were studied theoretically, and an inverse relationship between mutation rate and genome size has been observed for many species.","However, the effect of the recessive gene count, an important feature of the genomic architecture, is unknown.","In a Wright-Fisher model, we studied the mutation burden for a growing number of N completely recessive and lethal disease genes.","Diploid individuals are represented with a binary $2 \\times N$ matrix denoting wild-type and mutated alleles.","Analytic results for specific cases were complemented by simulations across a broad parameter regime for gene count, mutation and recombination rates.","Simulations revealed transitions to higher mutation burden and prevalence within a few generations that were linked to the extinction of the wild-type haplotype (least-loaded class).","This metastability, that is, phases of quasi-equilibrium with intermittent transitions, persists over $100\\,000$ generations.","The drift-barrier hypothesis is confirmed by a high mutation burden resulting in population collapse.","Simulations showed the emergence of mutually exclusive haplotypes for a mutation rate above 0.02 lethal equivalents per generation for a genomic architecture and population size representing complex multicellular organisms such as humans.","In such systems, recombination proves pivotal, preventing population collapse and maintaining a mutation burden below 10.","This study advances our understanding of gene pool stability, and particularly the role of the number of recessive disorders.","Insights into Muller`s ratchet dynamics are provided, and the essential role of recombination in curbing mutation burden and stabilizing the gene pool is demonstrated."],"url":"http://arxiv.org/abs/2406.09094v1","category":"q-bio.PE"}
{"created":"2024-06-13 13:20:20","title":"A Functorial Version of Chevalley's Theorem on Constructible Sets","abstract":"To determine whether an $n\\times n$-matrix has rank at most $r$ it suffices to check that the $(r+1)\\times (r+1)$-minors have rank at most $r$. In other words, to describe the set of $n\\times n$-matrices with the property of having rank at most $r$, we only need the description of the corresponding subset of $(r+1)\\times (r+1)$-matrices. We will generalize this observation to a large class of subsets of tensor spaces. A description of certain subsets of a high-dimensional tensor space can always be pulled back from a description of the corresponding subset in a fixed lower-dimensional tensor space.","sentences":["To determine whether an $n\\times n$-matrix has rank at most $r$ it suffices to check that the $(r+1)\\times (r+1)$-minors have rank at most $r$. In other words, to describe the set of $n\\times n$-matrices with the property of having rank at most $r$, we only need the description of the corresponding subset of $(r+1)\\times (r+1)$-matrices.","We will generalize this observation to a large class of subsets of tensor spaces.","A description of certain subsets of a high-dimensional tensor space can always be pulled back from a description of the corresponding subset in a fixed lower-dimensional tensor space."],"url":"http://arxiv.org/abs/2406.09092v1","category":"math.AG"}
{"created":"2024-06-13 13:15:40","title":"DiffPoGAN: Diffusion Policies with Generative Adversarial Networks for Offline Reinforcement Learning","abstract":"Offline reinforcement learning (RL) can learn optimal policies from pre-collected offline datasets without interacting with the environment, but the sampled actions of the agent cannot often cover the action distribution under a given state, resulting in the extrapolation error issue. Recent works address this issue by employing generative adversarial networks (GANs). However, these methods often suffer from insufficient constraints on policy exploration and inaccurate representation of behavior policies. Moreover, the generator in GANs fails in fooling the discriminator while maximizing the expected returns of a policy. Inspired by the diffusion, a generative model with powerful feature expressiveness, we propose a new offline RL method named Diffusion Policies with Generative Adversarial Networks (DiffPoGAN). In this approach, the diffusion serves as the policy generator to generate diverse distributions of actions, and a regularization method based on maximum likelihood estimation (MLE) is developed to generate data that approximate the distribution of behavior policies. Besides, we introduce an additional regularization term based on the discriminator output to effectively constrain policy exploration for policy improvement. Comprehensive experiments are conducted on the datasets for deep data-driven reinforcement learning (D4RL), and experimental results show that DiffPoGAN outperforms state-of-the-art methods in offline RL.","sentences":["Offline reinforcement learning (RL) can learn optimal policies from pre-collected offline datasets without interacting with the environment, but the sampled actions of the agent cannot often cover the action distribution under a given state, resulting in the extrapolation error issue.","Recent works address this issue by employing generative adversarial networks (GANs).","However, these methods often suffer from insufficient constraints on policy exploration and inaccurate representation of behavior policies.","Moreover, the generator in GANs fails in fooling the discriminator while maximizing the expected returns of a policy.","Inspired by the diffusion, a generative model with powerful feature expressiveness, we propose a new offline RL method named Diffusion Policies with Generative Adversarial Networks (DiffPoGAN).","In this approach, the diffusion serves as the policy generator to generate diverse distributions of actions, and a regularization method based on maximum likelihood estimation (MLE) is developed to generate data that approximate the distribution of behavior policies.","Besides, we introduce an additional regularization term based on the discriminator output to effectively constrain policy exploration for policy improvement.","Comprehensive experiments are conducted on the datasets for deep data-driven reinforcement learning (D4RL), and experimental results show that DiffPoGAN outperforms state-of-the-art methods in offline RL."],"url":"http://arxiv.org/abs/2406.09089v1","category":"cs.LG"}
{"created":"2024-06-13 13:13:17","title":"Suitability of KANs for Computer Vision: A preliminary investigation","abstract":"Kolmogorov-Arnold Networks (KANs) introduce a paradigm of neural modeling that implements learnable functions on the edges of the networks, diverging from the traditional node-centric activations in neural networks. This work assesses the applicability and efficacy of KANs in visual modeling, focusing on the image recognition task. We mainly analyze the performance and efficiency of different network architectures built using KAN concepts along with conventional building blocks of convolutional and linear layers, enabling a comparative analysis with the conventional models. Our findings are aimed at contributing to understanding the potential of KANs in computer vision, highlighting both their strengths and areas for further research. Our evaluation shows that whereas KAN-based architectures perform in-line with the original claims of KAN paper for performance and model-complexity in the case of simpler vision datasets like MNIST, the advantages seem to diminish even for slightly more complex datasets like CIFAR-10.","sentences":["Kolmogorov-Arnold Networks (KANs) introduce a paradigm of neural modeling that implements learnable functions on the edges of the networks, diverging from the traditional node-centric activations in neural networks.","This work assesses the applicability and efficacy of KANs in visual modeling, focusing on the image recognition task.","We mainly analyze the performance and efficiency of different network architectures built using KAN concepts along with conventional building blocks of convolutional and linear layers, enabling a comparative analysis with the conventional models.","Our findings are aimed at contributing to understanding the potential of KANs in computer vision, highlighting both their strengths and areas for further research.","Our evaluation shows that whereas KAN-based architectures perform in-line with the original claims of KAN paper for performance and model-complexity in the case of simpler vision datasets like MNIST, the advantages seem to diminish even for slightly more complex datasets like CIFAR-10."],"url":"http://arxiv.org/abs/2406.09087v1","category":"cs.CV"}
{"created":"2024-06-13 13:12:19","title":"Dynamics of Spinning Binary at 2PM","abstract":"We consider the covariant proposal for the gravitational Compton amplitude for a Kerr black hole. Employing the covariant three- and four-point Compton amplitudes, we assemble the classical one-loop integrand on the maximal cut at all orders in spin, utilizing the method of unitarity. Expanding in powers of spin, we evaluate the one-loop amplitude up to $\\mathcal O(G^2 a^8)$. Supplemented with extra contact contributions derived from the far-zone data of the Teukolsky solutions, the one-loop amplitude is in agreement with results available in the literature. We showcase the classical eikonal in the aligned-spin case at $\\mathcal O(G^2 a^7)$.","sentences":["We consider the covariant proposal for the gravitational Compton amplitude for a Kerr black hole.","Employing the covariant three- and four-point Compton amplitudes, we assemble the classical one-loop integrand on the maximal cut at all orders in spin, utilizing the method of unitarity.","Expanding in powers of spin, we evaluate the one-loop amplitude up to $\\mathcal O(G^2 a^8)$. Supplemented with extra contact contributions derived from the far-zone data of the Teukolsky solutions, the one-loop amplitude is in agreement with results available in the literature.","We showcase the classical eikonal in the aligned-spin case at $\\mathcal O(G^2 a^7)$."],"url":"http://arxiv.org/abs/2406.09086v1","category":"hep-th"}
{"created":"2024-06-13 13:10:47","title":"A Symbolic Computing Perspective on Software Systems","abstract":"Symbolic mathematical computing systems have served as a canary in the coal mine of software systems for more than sixty years. They have introduced or have been early adopters of programming language ideas such ideas as dynamic memory management, arbitrary precision arithmetic and dependent types. These systems have the feature of being highly complex while at the same time operating in a domain where results are well-defined and clearly verifiable. These software systems span multiple layers of abstraction with concerns ranging from instruction scheduling and cache pressure up to algorithmic complexity of constructions in algebraic geometry. All of the major symbolic mathematical computing systems include low-level code for arithmetic, memory management and other primitives, a compiler or interpreter for a bespoke programming language, a library of high level mathematical algorithms, and some form of user interface. Each of these parts invokes multiple deep issues.   We present some lessons learned from this environment and free flowing opinions on topics including:   * Portability of software across architectures and decades;   * Infrastructure to embrace and infrastructure to avoid;   * Choosing base abstractions upon which to build;   * How to get the most out of a small code base;   * How developments in compilers both to optimise and to validate code have always been and remain of critical importance, with plenty of remaining challenges;   * The way in which individuals including in particular Alan Mycroft who has been able to span from hand-crafting Z80 machine code up to the most abstruse high level code analysis techniques are needed, and   * Why it is important to teach full-stack thinking to the next generation.","sentences":["Symbolic mathematical computing systems have served as a canary in the coal mine of software systems for more than sixty years.","They have introduced or have been early adopters of programming language ideas such ideas as dynamic memory management, arbitrary precision arithmetic and dependent types.","These systems have the feature of being highly complex while at the same time operating in a domain where results are well-defined and clearly verifiable.","These software systems span multiple layers of abstraction with concerns ranging from instruction scheduling and cache pressure up to algorithmic complexity of constructions in algebraic geometry.","All of the major symbolic mathematical computing systems include low-level code for arithmetic, memory management and other primitives, a compiler or interpreter for a bespoke programming language, a library of high level mathematical algorithms, and some form of user interface.","Each of these parts invokes multiple deep issues.   ","We present some lessons learned from this environment and free flowing opinions on topics including:   * Portability of software across architectures and decades;   *","Infrastructure to embrace and infrastructure to avoid;   ","*","Choosing base abstractions upon which to build;   *","How to get the most out of a small code base;   *","How developments in compilers both to optimise and to validate code have always been and remain of critical importance, with plenty of remaining challenges;   *","The way in which individuals including in particular Alan Mycroft who has been able to span from hand-crafting Z80 machine code up to the most abstruse high level code analysis techniques are needed, and   *","Why it is important to teach full-stack thinking to the next generation."],"url":"http://arxiv.org/abs/2406.09085v1","category":"cs.SC"}
{"created":"2024-06-13 13:06:25","title":"Simple Chopsticks: Playing with any number of hands and fingers","abstract":"Chopsticks is a game played by two players where they start with one finger raised on each hand. On their turn, each player moves by pointing an attacking hand at one of their opponent's hands. The number of fingers on the pointed hand increases by the number of fingers on the attacking hand. If, after a move, a hand contains more than five fingers, it is removed from play. There are also other rules that allow players to move fingers from one hand to another, but we focus on this simple setup.   We introduce a generalization of Chopsticks, called Simple Chopsticks, in which the players may have any number of $n$-fingered hands. We find that having more hands than your opponent is generally good, and use this fact to fully characterize the outcomes of \\octopus/ in the case where the players have 2-fingered hands.","sentences":["Chopsticks is a game played by two players where they start with one finger raised on each hand.","On their turn, each player moves by pointing an attacking hand at one of their opponent's hands.","The number of fingers on the pointed hand increases by the number of fingers on the attacking hand.","If, after a move, a hand contains more than five fingers, it is removed from play.","There are also other rules that allow players to move fingers from one hand to another, but we focus on this simple setup.   ","We introduce a generalization of Chopsticks, called Simple Chopsticks, in which the players may have any number of $n$-fingered hands.","We find that having more hands than your opponent is generally good, and use this fact to fully characterize the outcomes of \\octopus/ in the case where the players have 2-fingered hands."],"url":"http://arxiv.org/abs/2406.09083v1","category":"math.CO"}
{"created":"2024-06-13 13:04:42","title":"Data-driven modeling and supervisory control system optimization for plug-in hybrid electric vehicles","abstract":"Learning-based intelligent energy management systems for plug-in hybrid electric vehicles (PHEVs) are crucial for achieving efficient energy utilization. However, their application faces system reliability challenges in the real world, which prevents widespread acceptance by original equipment manufacturers (OEMs). This paper begins by establishing a PHEV model based on physical and data-driven models, focusing on the high-fidelity training environment. It then proposes a real-vehicle application-oriented control framework, combining horizon-extended reinforcement learning (RL)-based energy management with the equivalent consumption minimization strategy (ECMS) to enhance practical applicability, and improves the flawed method of equivalent factor evaluation based on instantaneous driving cycle and powertrain states found in existing research. Finally, comprehensive simulation and hardware-in-the-loop validation are carried out which demonstrates the advantages of the proposed control framework in fuel economy over adaptive-ECMS and rule-based strategies. Compared to conventional RL architectures that directly control powertrain components, the proposed control method not only achieves similar optimality but also significantly enhances the disturbance resistance of the energy management system, providing an effective control framework for RL-based energy management strategies aimed at real-vehicle applications by OEMs.","sentences":["Learning-based intelligent energy management systems for plug-in hybrid electric vehicles (PHEVs) are crucial for achieving efficient energy utilization.","However, their application faces system reliability challenges in the real world, which prevents widespread acceptance by original equipment manufacturers (OEMs).","This paper begins by establishing a PHEV model based on physical and data-driven models, focusing on the high-fidelity training environment.","It then proposes a real-vehicle application-oriented control framework, combining horizon-extended reinforcement learning (RL)-based energy management with the equivalent consumption minimization strategy (ECMS) to enhance practical applicability, and improves the flawed method of equivalent factor evaluation based on instantaneous driving cycle and powertrain states found in existing research.","Finally, comprehensive simulation and hardware-in-the-loop validation are carried out which demonstrates the advantages of the proposed control framework in fuel economy over adaptive-ECMS and rule-based strategies.","Compared to conventional RL architectures that directly control powertrain components, the proposed control method not only achieves similar optimality but also significantly enhances the disturbance resistance of the energy management system, providing an effective control framework for RL-based energy management strategies aimed at real-vehicle applications by OEMs."],"url":"http://arxiv.org/abs/2406.09082v1","category":"eess.SY"}
{"created":"2024-06-13 13:03:52","title":"Link to Densify: Topological Transition During the Compression of Amorphous Ices","abstract":"In this work we study the structural and topological transition between amorphous ices. We discover that applying external pressure to low-density amorphous ice (LDA) distorts the looped motifs constituting the hydrogen bond network (HBN). The transition to high-density amorphous ice (HDA) is accompanied by an abrupt topological transition of the HBN, which starts to display interpenetrated and Hopf linked looped motifs. This topologically linked HBN is metastable during decompression, and the links disentangle through a first order transition at the critical point. We discover that the rearrangement of the HBN motifs at the transitions induce a loss of mechanical rigidity, dramatically increasing compressiblity. We argue that topological transitions may be a generic mechanism for the densification of network-forming amorphous materials and enriches our understanding of the properties of water.","sentences":["In this work we study the structural and topological transition between amorphous ices.","We discover that applying external pressure to low-density amorphous ice (LDA) distorts the looped motifs constituting the hydrogen bond network (HBN).","The transition to high-density amorphous ice (HDA) is accompanied by an abrupt topological transition of the HBN, which starts to display interpenetrated and Hopf linked looped motifs.","This topologically linked HBN is metastable during decompression, and the links disentangle through a first order transition at the critical point.","We discover that the rearrangement of the HBN motifs at the transitions induce a loss of mechanical rigidity, dramatically increasing compressiblity.","We argue that topological transitions may be a generic mechanism for the densification of network-forming amorphous materials and enriches our understanding of the properties of water."],"url":"http://arxiv.org/abs/2406.09080v1","category":"cond-mat.dis-nn"}
{"created":"2024-06-13 12:55:10","title":"EquiPrompt: Debiasing Diffusion Models via Iterative Bootstrapping in Chain of Thoughts","abstract":"In the domain of text-to-image generative models, the inadvertent propagation of biases inherent in training datasets poses significant ethical challenges, particularly in the generation of socially sensitive content. This paper introduces EquiPrompt, a novel method employing Chain of Thought (CoT) reasoning to reduce biases in text-to-image generative models. EquiPrompt uses iterative bootstrapping and bias-aware exemplar selection to balance creativity and ethical responsibility. It integrates iterative reasoning refinement with controlled evaluation techniques, addressing zero-shot CoT issues in sensitive contexts. Experiments on several generation tasks show EquiPrompt effectively lowers bias while maintaining generative quality, advancing ethical AI and socially responsible creative processes.Code will be publically available.","sentences":["In the domain of text-to-image generative models, the inadvertent propagation of biases inherent in training datasets poses significant ethical challenges, particularly in the generation of socially sensitive content.","This paper introduces EquiPrompt, a novel method employing Chain of Thought (CoT) reasoning to reduce biases in text-to-image generative models.","EquiPrompt uses iterative bootstrapping and bias-aware exemplar selection to balance creativity and ethical responsibility.","It integrates iterative reasoning refinement with controlled evaluation techniques, addressing zero-shot CoT issues in sensitive contexts.","Experiments on several generation tasks show EquiPrompt effectively lowers bias while maintaining generative quality, advancing ethical AI and socially responsible creative processes.","Code will be publically available."],"url":"http://arxiv.org/abs/2406.09070v1","category":"cs.LG"}
{"created":"2024-06-13 12:54:29","title":"Dispelling the Mirage of Progress in Offline MARL through Standardised Baselines and Evaluation","abstract":"Offline multi-agent reinforcement learning (MARL) is an emerging field with great promise for real-world applications. Unfortunately, the current state of research in offline MARL is plagued by inconsistencies in baselines and evaluation protocols, which ultimately makes it difficult to accurately assess progress, trust newly proposed innovations, and allow researchers to easily build upon prior work. In this paper, we firstly identify significant shortcomings in existing methodologies for measuring the performance of novel algorithms through a representative study of published offline MARL work. Secondly, by directly comparing to this prior work, we demonstrate that simple, well-implemented baselines can achieve state-of-the-art (SOTA) results across a wide range of tasks. Specifically, we show that on 35 out of 47 datasets used in prior work (almost 75% of cases), we match or surpass the performance of the current purported SOTA. Strikingly, our baselines often substantially outperform these more sophisticated algorithms. Finally, we correct for the shortcomings highlighted from this prior work by introducing a straightforward standardised methodology for evaluation and by providing our baseline implementations with statistically robust results across several scenarios, useful for comparisons in future work. Our proposal includes simple and sensible steps that are easy to adopt, which in combination with solid baselines and comparative results, could substantially improve the overall rigour of empirical science in offline MARL moving forward.","sentences":["Offline multi-agent reinforcement learning (MARL) is an emerging field with great promise for real-world applications.","Unfortunately, the current state of research in offline MARL is plagued by inconsistencies in baselines and evaluation protocols, which ultimately makes it difficult to accurately assess progress, trust newly proposed innovations, and allow researchers to easily build upon prior work.","In this paper, we firstly identify significant shortcomings in existing methodologies for measuring the performance of novel algorithms through a representative study of published offline MARL work.","Secondly, by directly comparing to this prior work, we demonstrate that simple, well-implemented baselines can achieve state-of-the-art (SOTA) results across a wide range of tasks.","Specifically, we show that on 35 out of 47 datasets used in prior work (almost 75% of cases), we match or surpass the performance of the current purported SOTA.","Strikingly, our baselines often substantially outperform these more sophisticated algorithms.","Finally, we correct for the shortcomings highlighted from this prior work by introducing a straightforward standardised methodology for evaluation and by providing our baseline implementations with statistically robust results across several scenarios, useful for comparisons in future work.","Our proposal includes simple and sensible steps that are easy to adopt, which in combination with solid baselines and comparative results, could substantially improve the overall rigour of empirical science in offline MARL moving forward."],"url":"http://arxiv.org/abs/2406.09068v1","category":"cs.LG"}
{"created":"2024-06-13 12:51:22","title":"State-Space Modeling in Long Sequence Processing: A Survey on Recurrence in the Transformer Era","abstract":"Effectively learning from sequential data is a longstanding goal of Artificial Intelligence, especially in the case of long sequences. From the dawn of Machine Learning, several researchers engaged in the search of algorithms and architectures capable of processing sequences of patterns, retaining information about the past inputs while still leveraging the upcoming data, without losing precious long-term dependencies and correlations. While such an ultimate goal is inspired by the human hallmark of continuous real-time processing of sensory information, several solutions simplified the learning paradigm by artificially limiting the processed context or dealing with sequences of limited length, given in advance. These solutions were further emphasized by the large ubiquity of Transformers, that have initially shaded the role of Recurrent Neural Nets. However, recurrent networks are facing a strong recent revival due to the growing popularity of (deep) State-Space models and novel instances of large-context Transformers, which are both based on recurrent computations to go beyond several limits of currently ubiquitous technologies. In fact, the fast development of Large Language Models enhanced the interest in efficient solutions to process data over time. This survey provides an in-depth summary of the latest approaches that are based on recurrent models for sequential data processing. A complete taxonomy over the latest trends in architectural and algorithmic solutions is reported and discussed, guiding researchers in this appealing research field. The emerging picture suggests that there is room for thinking of novel routes, constituted by learning algorithms which depart from the standard Backpropagation Through Time, towards a more realistic scenario where patterns are effectively processed online, leveraging local-forward computations, opening to further research on this topic.","sentences":["Effectively learning from sequential data is a longstanding goal of Artificial Intelligence, especially in the case of long sequences.","From the dawn of Machine Learning, several researchers engaged in the search of algorithms and architectures capable of processing sequences of patterns, retaining information about the past inputs while still leveraging the upcoming data, without losing precious long-term dependencies and correlations.","While such an ultimate goal is inspired by the human hallmark of continuous real-time processing of sensory information, several solutions simplified the learning paradigm by artificially limiting the processed context or dealing with sequences of limited length, given in advance.","These solutions were further emphasized by the large ubiquity of Transformers, that have initially shaded the role of Recurrent Neural Nets.","However, recurrent networks are facing a strong recent revival due to the growing popularity of (deep) State-Space models and novel instances of large-context Transformers, which are both based on recurrent computations to go beyond several limits of currently ubiquitous technologies.","In fact, the fast development of Large Language Models enhanced the interest in efficient solutions to process data over time.","This survey provides an in-depth summary of the latest approaches that are based on recurrent models for sequential data processing.","A complete taxonomy over the latest trends in architectural and algorithmic solutions is reported and discussed, guiding researchers in this appealing research field.","The emerging picture suggests that there is room for thinking of novel routes, constituted by learning algorithms which depart from the standard Backpropagation Through Time, towards a more realistic scenario where patterns are effectively processed online, leveraging local-forward computations, opening to further research on this topic."],"url":"http://arxiv.org/abs/2406.09062v1","category":"cs.LG"}
{"created":"2024-06-13 12:45:40","title":"Microparticle-assisted 2D super resolution virtual image modeling","abstract":"The approach that makes it possible to explain the phenomenon of super resolution in a virtual image using dielectric microparticles is presented. A resolution of 100 nm in the visible range is demonstrated, resolutions of the order of 50 nm are being discussed. The presented two-dimensional model uses the FTDT code to analyze the generation of radiation and propagation through a system of slits in a metal screen and a dielectric microparticle located above it. The image is constructed using the back propagation method.","sentences":["The approach that makes it possible to explain the phenomenon of super resolution in a virtual image using dielectric microparticles is presented.","A resolution of 100 nm in the visible range is demonstrated, resolutions of the order of 50 nm are being discussed.","The presented two-dimensional model uses the FTDT code to analyze the generation of radiation and propagation through a system of slits in a metal screen and a dielectric microparticle located above it.","The image is constructed using the back propagation method."],"url":"http://arxiv.org/abs/2406.09060v1","category":"physics.optics"}
{"created":"2024-06-13 12:45:07","title":"Environment-Aware Codebook Design for RIS-Assisted MU-MISO Communications: Implementation and Performance Analysis","abstract":"Reconfigurable intelligent surface (RIS) provides a new electromagnetic response control solution, which can proactively reshape the characteristics of wireless channel environments. In RIS-assisted communication systems, the acquisition of channel state information (CSI) and the optimization of reflecting coefficients constitute major design challenges. To address these issues, codebook-based solutions have been developed recently, which, however, are mostly environment-agnostic. In this paper, a novel environment-aware codebook protocol is proposed, which can significantly reduce both pilot overhead and computational complexity, while maintaining expected communication performance. Specifically, first of all, a channel training framework is introduced to divide the training phase into several blocks. In each block, we directly estimate the composite end-to-end channel and focus only on the transmit beamforming. Second, we propose an environment-aware codebook generation scheme, which first generates a group of channels based on statistical CSI, and then obtains their corresponding RIS configuration by utilizing the alternating optimization (AO) method offline. In each online training block, the RIS is configured based on the corresponding codeword in the environment-aware codebook, and the optimal codeword resulting in the highest sum rate is adopted for assisting in the downlink data transmission. Third, we analyze the theoretical performance of the environment-aware codebook-based protocol taking into account the channel estimation errors. Finally, numerical simulations are provided to verify our theoretical analysis and the performance of the proposed scheme. In particular, the simulation results demonstrate that our protocol is more competitive than conventional environment-agnostic codebooks.","sentences":["Reconfigurable intelligent surface (RIS) provides a new electromagnetic response control solution, which can proactively reshape the characteristics of wireless channel environments.","In RIS-assisted communication systems, the acquisition of channel state information (CSI) and the optimization of reflecting coefficients constitute major design challenges.","To address these issues, codebook-based solutions have been developed recently, which, however, are mostly environment-agnostic.","In this paper, a novel environment-aware codebook protocol is proposed, which can significantly reduce both pilot overhead and computational complexity, while maintaining expected communication performance.","Specifically, first of all, a channel training framework is introduced to divide the training phase into several blocks.","In each block, we directly estimate the composite end-to-end channel and focus only on the transmit beamforming.","Second, we propose an environment-aware codebook generation scheme, which first generates a group of channels based on statistical CSI, and then obtains their corresponding RIS configuration by utilizing the alternating optimization (AO) method offline.","In each online training block, the RIS is configured based on the corresponding codeword in the environment-aware codebook, and the optimal codeword resulting in the highest sum rate is adopted for assisting in the downlink data transmission.","Third, we analyze the theoretical performance of the environment-aware codebook-based protocol taking into account the channel estimation errors.","Finally, numerical simulations are provided to verify our theoretical analysis and the performance of the proposed scheme.","In particular, the simulation results demonstrate that our protocol is more competitive than conventional environment-agnostic codebooks."],"url":"http://arxiv.org/abs/2406.09058v1","category":"cs.IT"}
{"created":"2024-06-13 12:43:40","title":"CUDRT: Benchmarking the Detection of Human vs. Large Language Models Generated Texts","abstract":"The proliferation of large language models (LLMs) has significantly enhanced text generation capabilities across various industries. However, these models' ability to generate human-like text poses substantial challenges in discerning between human and AI authorship. Despite the effectiveness of existing AI-generated text detectors, their development is hindered by the lack of comprehensive, publicly available benchmarks. Current benchmarks are limited to specific scenarios, such as question answering and text polishing, and predominantly focus on English texts, failing to capture the diverse applications and linguistic nuances of LLMs. To address these limitations, this paper constructs a comprehensive bilingual benchmark in both Chinese and English to evaluate mainstream AI-generated text detectors. We categorize LLM text generation into five distinct operations: Create, Update, Delete, Rewrite, and Translate (CUDRT), encompassing all current LLMs activities. We also establish a robust benchmark evaluation framework to support scalable and reproducible experiments. For each CUDRT category, we have developed extensive datasets to thoroughly assess detector performance. By employing the latest mainstream LLMs specific to each language, our datasets provide a thorough evaluation environment. Extensive experimental results offer critical insights for optimizing AI-generated text detectors and suggest future research directions to improve detection accuracy and generalizability across various scenarios.","sentences":["The proliferation of large language models (LLMs) has significantly enhanced text generation capabilities across various industries.","However, these models' ability to generate human-like text poses substantial challenges in discerning between human and AI authorship.","Despite the effectiveness of existing AI-generated text detectors, their development is hindered by the lack of comprehensive, publicly available benchmarks.","Current benchmarks are limited to specific scenarios, such as question answering and text polishing, and predominantly focus on English texts, failing to capture the diverse applications and linguistic nuances of LLMs.","To address these limitations, this paper constructs a comprehensive bilingual benchmark in both Chinese and English to evaluate mainstream AI-generated text detectors.","We categorize LLM text generation into five distinct operations: Create, Update, Delete, Rewrite, and Translate (CUDRT), encompassing all current LLMs activities.","We also establish a robust benchmark evaluation framework to support scalable and reproducible experiments.","For each CUDRT category, we have developed extensive datasets to thoroughly assess detector performance.","By employing the latest mainstream LLMs specific to each language, our datasets provide a thorough evaluation environment.","Extensive experimental results offer critical insights for optimizing AI-generated text detectors and suggest future research directions to improve detection accuracy and generalizability across various scenarios."],"url":"http://arxiv.org/abs/2406.09056v1","category":"cs.CL"}
{"created":"2024-06-13 12:37:24","title":"Topological pairing of composite fermions via criticality","abstract":"The fractional quantum Hall effect (FQHE) at the filling factor with an even denominator, 5/2, occurs despite the expectation, due to the electron statistics, that the denominator must be an odd number. It is believed that the Cooper pairing of underlying quasiparticles, composite fermions (CFs), leads to the explanation of this effect. Such a state should have a Pfaffian form of the BCS wave function (due to the absence of spin) and non-Abelian statistics of possible vortex-like excitations (due to the $p$-wave nature of the pairing). Here we expose the origin of pairing by using the effective dipole representation of the problem and show that pairing is encoded in a Hamiltonian that describes the interaction of the charge density with dipoles i.e. the current of CFs. The necessary condition for the paired state to exist is the effective dipole physics at the Fermi level as a consequence of the non-trivial topology of the ideal band in which electrons live - a Landau level (LL); the paired state is a resolution of the unstable, critical behaviour characterized by the distancing of correlation hole with respect to electron (and thus dipole) at the Fermi level due to the topology. We describe analytically this deconfined critical point, at which deconfinement of Majorana neutral fermions takes place. In the presence of large, short-range repulsive interaction inside a LL, the critical behavior may be stabilized into a regularized Fermi-liquid-like (FLL) state, like the one that characterizes the physics in the lowest LL (LLL), but in general, for an interaction with slowly decaying pseudopotentials, the system is prone to pairing.","sentences":["The fractional quantum Hall effect (FQHE) at the filling factor with an even denominator, 5/2, occurs despite the expectation, due to the electron statistics, that the denominator must be an odd number.","It is believed that the Cooper pairing of underlying quasiparticles, composite fermions (CFs), leads to the explanation of this effect.","Such a state should have a Pfaffian form of the BCS wave function (due to the absence of spin) and non-Abelian statistics of possible vortex-like excitations (due to the $p$-wave nature of the pairing).","Here we expose the origin of pairing by using the effective dipole representation of the problem and show that pairing is encoded in a Hamiltonian that describes the interaction of the charge density with dipoles i.e. the current of CFs.","The necessary condition for the paired state to exist is the effective dipole physics at the Fermi level as a consequence of the non-trivial topology of the ideal band in which electrons live - a Landau level (LL); the paired state is a resolution of the unstable, critical behaviour characterized by the distancing of correlation hole with respect to electron (and thus dipole) at the Fermi level due to the topology.","We describe analytically this deconfined critical point, at which deconfinement of Majorana neutral fermions takes place.","In the presence of large, short-range repulsive interaction inside a LL, the critical behavior may be stabilized into a regularized Fermi-liquid-like (FLL) state, like the one that characterizes the physics in the lowest LL (LLL), but in general, for an interaction with slowly decaying pseudopotentials, the system is prone to pairing."],"url":"http://arxiv.org/abs/2406.09050v1","category":"cond-mat.str-el"}
{"created":"2024-06-13 12:31:18","title":"Pseudo-Nambu-Goldstone Boson Production from Inflaton Coupling during Reheating","abstract":"The existence of pseudo-Nambu-Goldstone boson (pNGB) fields is a common feature in many models beyond the Standard Model, characterized by their exclusive derivative couplings. This paper investigates a scenario where a pNGB is coupled to the inflaton field during the reheating phase of the early universe. We calculate the perturbative decay rate of a coherently oscillating inflaton into pNGBs on a general basis, considering both constant and field-dependent couplings with monomial potentials at the minimum. As a concrete application, we explore the production of axions when the radial mode of the Peccei-Quinn (PQ) scalar serves as the inflaton, particularly in the presence of a large gravitational non-minimal coupling. Our findings suggest that the presence of pNGBs during reheating can lead to significant non-thermal relics, offering new constraints on inflationary reheating models and providing potential observational signatures in the form of dark radiation.","sentences":["The existence of pseudo-Nambu-Goldstone boson (pNGB) fields is a common feature in many models beyond the Standard Model, characterized by their exclusive derivative couplings.","This paper investigates a scenario where a pNGB is coupled to the inflaton field during the reheating phase of the early universe.","We calculate the perturbative decay rate of a coherently oscillating inflaton into pNGBs on a general basis, considering both constant and field-dependent couplings with monomial potentials at the minimum.","As a concrete application, we explore the production of axions when the radial mode of the Peccei-Quinn (PQ) scalar serves as the inflaton, particularly in the presence of a large gravitational non-minimal coupling.","Our findings suggest that the presence of pNGBs during reheating can lead to significant non-thermal relics, offering new constraints on inflationary reheating models and providing potential observational signatures in the form of dark radiation."],"url":"http://arxiv.org/abs/2406.09045v1","category":"hep-ph"}
{"created":"2024-06-13 12:29:27","title":"Language Models are Crossword Solvers","abstract":"Crosswords are a form of word puzzle that require a solver to demonstrate a high degree of proficiency in natural language understanding, wordplay, reasoning, and world knowledge, along with adherence to character and length constraints. In this paper we tackle the challenge of solving crosswords with Large Language Models (LLMs). We demonstrate that the current generation of state-of-the art (SoTA) language models show significant competence at deciphering cryptic crossword clues, and outperform previously reported SoTA results by a factor of 2-3 in relevant benchmarks. We also develop a search algorithm that builds off this performance to tackle the problem of solving full crossword grids with LLMs for the very first time, achieving an accuracy of 93\\% on New York Times crossword puzzles. Contrary to previous work in this area which concluded that LLMs lag human expert performance significantly, our research suggests this gap is a lot narrower.","sentences":["Crosswords are a form of word puzzle that require a solver to demonstrate a high degree of proficiency in natural language understanding, wordplay, reasoning, and world knowledge, along with adherence to character and length constraints.","In this paper we tackle the challenge of solving crosswords with Large Language Models (LLMs).","We demonstrate that the current generation of state-of-the art (SoTA) language models show significant competence at deciphering cryptic crossword clues, and outperform previously reported SoTA results by a factor of 2-3 in relevant benchmarks.","We also develop a search algorithm that builds off this performance to tackle the problem of solving full crossword grids with LLMs for the very first time, achieving an accuracy of 93\\% on New York Times crossword puzzles.","Contrary to previous work in this area which concluded that LLMs lag human expert performance significantly, our research suggests this gap is a lot narrower."],"url":"http://arxiv.org/abs/2406.09043v1","category":"cs.CL"}
{"created":"2024-06-13 12:28:21","title":"Knowledge Graphs in the Digital Twin: A Systematic Literature Review About the Combination of Semantic Technologies and Simulation in Industrial Automation","abstract":"The ongoing digitization of the industrial sector has reached a pivotal juncture with the emergence of Digital Twins, offering a digital representation of physical assets and processes. One key aspect of those digital representations are simulation models, enabling a deeper insight in the assets current state and its characteristics. This paper asserts that the next evolutionary step in this digitization journey involves the integration of intelligent linkages between diverse simulation models within the Digital Twin framework. Crucially, for the Digital Twin to be a scalable and cost-effective solution, there is a pressing need for automated adaption, (re-)configuration, and generation of simulation models. Recognizing the inherent challenges in achieving such automation, this paper analyses the utilization of knowledge graphs as a potentially very suitable technological solution. Knowledge graphs, acting as interconnected and interrelated databases, provide a means of seamlessly integrating different data sources, facilitating the efficient integration and automated adaption of data and (simulation) models in the Digital Twin. We conducted a comprehensive literature review to analyze the current landscape of knowledge graphs in the context of Digital Twins with focus on simulation models. By addressing the challenges associated with scalability and maintenance, this research contributes to the effective adaption of Digital Twins in the industrial sector, paving the way for enhanced efficiency, adaptability, and resilience in the face of evolving technological landscapes.","sentences":["The ongoing digitization of the industrial sector has reached a pivotal juncture with the emergence of Digital Twins, offering a digital representation of physical assets and processes.","One key aspect of those digital representations are simulation models, enabling a deeper insight in the assets current state and its characteristics.","This paper asserts that the next evolutionary step in this digitization journey involves the integration of intelligent linkages between diverse simulation models within the Digital Twin framework.","Crucially, for the Digital Twin to be a scalable and cost-effective solution, there is a pressing need for automated adaption, (re-)configuration, and generation of simulation models.","Recognizing the inherent challenges in achieving such automation, this paper analyses the utilization of knowledge graphs as a potentially very suitable technological solution.","Knowledge graphs, acting as interconnected and interrelated databases, provide a means of seamlessly integrating different data sources, facilitating the efficient integration and automated adaption of data and (simulation) models in the Digital Twin.","We conducted a comprehensive literature review to analyze the current landscape of knowledge graphs in the context of Digital Twins with focus on simulation models.","By addressing the challenges associated with scalability and maintenance, this research contributes to the effective adaption of Digital Twins in the industrial sector, paving the way for enhanced efficiency, adaptability, and resilience in the face of evolving technological landscapes."],"url":"http://arxiv.org/abs/2406.09042v1","category":"cs.CE"}
{"created":"2024-06-13 12:27:55","title":"ME-Switch: A Memory-Efficient Expert Switching Framework for Large Language Models","abstract":"The typical process for developing LLMs involves pre-training a general foundation model on massive data, followed by fine-tuning on task-specific data to create specialized experts. Serving these experts poses challenges, as loading all experts onto devices is impractical, and frequent switching between experts in response to user requests incurs substantial I/O costs, increasing latency and expenses. Previous approaches decompose expert weights into pre-trained model weights and residual delta weights, then quantize the delta weights to reduce model size. However, these methods often lead to significant quantization errors at extremely low bitwidths and assume the appropriate model for a user request is known in advance, which is not practical. To address these issues, we introduce ME-Switch, a memory-efficient expert switching framework for LLM serving. ME-Switch uses mixed-precision quantization, selectively quantizing non-salient input channels of delta weights to extremely low bits while keeping salient ones intact, significantly reducing storage demands while maintaining performance. Additionally, we develop a routing method that efficiently directs user queries to the most suitable expert by transforming the model selection problem into a domain classification problem. Extensive experiments show ME-Switch's promising memory efficiency and routing performance. For example, when serving three models from the Mistral-7B family, ME-Switch reduces model size by 1.74x while maintaining nearly lossless performance on instruction, mathematical reasoning, and code generation tasks. Furthermore, ME-Switch can efficiently serve 16 models from the Mistral-7B family on a single NVIDIA A100 GPU.","sentences":["The typical process for developing LLMs involves pre-training a general foundation model on massive data, followed by fine-tuning on task-specific data to create specialized experts.","Serving these experts poses challenges, as loading all experts onto devices is impractical, and frequent switching between experts in response to user requests incurs substantial I/O costs, increasing latency and expenses.","Previous approaches decompose expert weights into pre-trained model weights and residual delta weights, then quantize the delta weights to reduce model size.","However, these methods often lead to significant quantization errors at extremely low bitwidths and assume the appropriate model for a user request is known in advance, which is not practical.","To address these issues, we introduce ME-Switch, a memory-efficient expert switching framework for LLM serving.","ME-Switch uses mixed-precision quantization, selectively quantizing non-salient input channels of delta weights to extremely low bits while keeping salient ones intact, significantly reducing storage demands while maintaining performance.","Additionally, we develop a routing method that efficiently directs user queries to the most suitable expert by transforming the model selection problem into a domain classification problem.","Extensive experiments show ME-Switch's promising memory efficiency and routing performance.","For example, when serving three models from the Mistral-7B family, ME-Switch reduces model size by 1.74x while maintaining nearly lossless performance on instruction, mathematical reasoning, and code generation tasks.","Furthermore, ME-Switch can efficiently serve 16 models from the Mistral-7B family on a single NVIDIA A100 GPU."],"url":"http://arxiv.org/abs/2406.09041v1","category":"cs.CL"}
{"created":"2024-06-13 12:23:35","title":"FacEnhance: Facial Expression Enhancing with Recurrent DDPMs","abstract":"Facial expressions, vital in non-verbal human communication, have found applications in various computer vision fields like virtual reality, gaming, and emotional AI assistants. Despite advancements, many facial expression generation models encounter challenges such as low resolution (e.g., 32x32 or 64x64 pixels), poor quality, and the absence of background details. In this paper, we introduce FacEnhance, a novel diffusion-based approach addressing constraints in existing low-resolution facial expression generation models. FacEnhance enhances low-resolution facial expression videos (64x64 pixels) to higher resolutions (192x192 pixels), incorporating background details and improving overall quality. Leveraging conditional denoising within a diffusion framework, guided by a background-free low-resolution video and a single neutral expression high-resolution image, FacEnhance generates a video incorporating the facial expression from the low-resolution video performed by the individual with background from the neutral image. By complementing lightweight low-resolution models, FacEnhance strikes a balance between computational efficiency and desirable image resolution and quality. Extensive experiments on the MUG facial expression database demonstrate the efficacy of FacEnhance in enhancing low-resolution model outputs to state-of-the-art quality while preserving content and identity consistency. FacEnhance represents significant progress towards resource-efficient, high-fidelity facial expression generation, Renewing outdated low-resolution methods to up-to-date standards.","sentences":["Facial expressions, vital in non-verbal human communication, have found applications in various computer vision fields like virtual reality, gaming, and emotional AI assistants.","Despite advancements, many facial expression generation models encounter challenges such as low resolution (e.g., 32x32 or 64x64 pixels), poor quality, and the absence of background details.","In this paper, we introduce FacEnhance, a novel diffusion-based approach addressing constraints in existing low-resolution facial expression generation models.","FacEnhance enhances low-resolution facial expression videos (64x64 pixels) to higher resolutions (192x192 pixels), incorporating background details and improving overall quality.","Leveraging conditional denoising within a diffusion framework, guided by a background-free low-resolution video and a single neutral expression high-resolution image, FacEnhance generates a video incorporating the facial expression from the low-resolution video performed by the individual with background from the neutral image.","By complementing lightweight low-resolution models, FacEnhance strikes a balance between computational efficiency and desirable image resolution and quality.","Extensive experiments on the MUG facial expression database demonstrate the efficacy of FacEnhance in enhancing low-resolution model outputs to state-of-the-art quality while preserving content and identity consistency.","FacEnhance represents significant progress towards resource-efficient, high-fidelity facial expression generation, Renewing outdated low-resolution methods to up-to-date standards."],"url":"http://arxiv.org/abs/2406.09040v1","category":"cs.CV"}
{"created":"2024-06-13 12:22:08","title":"CGP++ : A Modern C++ Implementation of Cartesian Genetic Programming","abstract":"The reference implementation of Cartesian Genetic Programming (CGP) was written in the C programming language. C inherently follows a procedural programming paradigm, which entails challenges in providing a reusable and scalable implementation model for complex structures and methods. Moreover, due to the limiting factors of C, the reference implementation of CGP does not provide a generic framework and is therefore restricted to a set of predefined evaluation types. Besides the reference implementation, we also observe that other existing implementations are limited with respect to the features provided. In this work, we therefore propose the first version of a modern C++ implementation of CGP that pursues object-oriented design and generic programming paradigm to provide an efficient implementation model that can facilitate the discovery of new problem domains and the implementation of complex advanced methods that have been proposed for CGP over time. With the proposal of our new implementation, we aim to generally promote interpretability, accessibility and reproducibility in the field of CGP.","sentences":["The reference implementation of Cartesian Genetic Programming (CGP) was written in the C programming language.","C inherently follows a procedural programming paradigm, which entails challenges in providing a reusable and scalable implementation model for complex structures and methods.","Moreover, due to the limiting factors of C, the reference implementation of CGP does not provide a generic framework and is therefore restricted to a set of predefined evaluation types.","Besides the reference implementation, we also observe that other existing implementations are limited with respect to the features provided.","In this work, we therefore propose the first version of a modern C++ implementation of CGP that pursues object-oriented design and generic programming paradigm to provide an efficient implementation model that can facilitate the discovery of new problem domains and the implementation of complex advanced methods that have been proposed for CGP over time.","With the proposal of our new implementation, we aim to generally promote interpretability, accessibility and reproducibility in the field of CGP."],"url":"http://arxiv.org/abs/2406.09038v1","category":"cs.NE"}
{"created":"2024-06-13 12:18:08","title":"Thermodynamic of the $f(Q)$ universe","abstract":"We investigate thermodynamics of apparent horizon in the $f(Q)$ universe with trivial and nontrivial connections. We first explore the perspectives of the first law, generalized second law and $P-V$ phase transition with trivial connection. We show that the lowest-order correction of entropy has the same form as that in loop quantum gravity, and the critical exponents of the phase transition caused by the lowest-order correction are consistent with those in mean field theory. We then examine the thermodynamic implication of nontrivial connections. We find that nontrivial connections in the $f(Q)$ universe imply non-equilibrium states from the perspective of thermodynamics.","sentences":["We investigate thermodynamics of apparent horizon in the $f(Q)$ universe with trivial and nontrivial connections.","We first explore the perspectives of the first law, generalized second law and $P-V$ phase transition with trivial connection.","We show that the lowest-order correction of entropy has the same form as that in loop quantum gravity, and the critical exponents of the phase transition caused by the lowest-order correction are consistent with those in mean field theory.","We then examine the thermodynamic implication of nontrivial connections.","We find that nontrivial connections in the $f(Q)$ universe imply non-equilibrium states from the perspective of thermodynamics."],"url":"http://arxiv.org/abs/2406.09036v1","category":"gr-qc"}
{"created":"2024-06-13 12:05:37","title":"Stimulated magnon scattering by non-degenerate parametric excitation","abstract":"Parametric spin wave excitation allows studying a variety of nonlinear phenomena, such as magnon scattering. In patterned micro- and nanostructures the magnon spectra is discrete and translational symmetry is broken, which means allowable scattering channels differ from those in continuous films. An example is non-degenerate scattering by which high-power transverse field pumping creates two magnons with distinct frequencies around half the pumping frequency. Through micromagnetics simulations, we show under certain conditions that combining two pumping frequencies generates new magnon modes through a process of stimulated magnon scattering. Such processes are found to depend on the film geometry and sequence of the pumping fields.","sentences":["Parametric spin wave excitation allows studying a variety of nonlinear phenomena, such as magnon scattering.","In patterned micro- and nanostructures the magnon spectra is discrete and translational symmetry is broken, which means allowable scattering channels differ from those in continuous films.","An example is non-degenerate scattering by which high-power transverse field pumping creates two magnons with distinct frequencies around half the pumping frequency.","Through micromagnetics simulations, we show under certain conditions that combining two pumping frequencies generates new magnon modes through a process of stimulated magnon scattering.","Such processes are found to depend on the film geometry and sequence of the pumping fields."],"url":"http://arxiv.org/abs/2406.09032v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-13 12:04:40","title":"A Comprehensive Graph Pooling Benchmark: Effectiveness, Robustness and Generalizability","abstract":"Graph pooling has gained attention for its ability to obtain effective node and graph representations for various downstream tasks. Despite the recent surge in graph pooling approaches, there is a lack of standardized experimental settings and fair benchmarks to evaluate their performance. To address this issue, we have constructed a comprehensive benchmark that includes 15 graph pooling methods and 21 different graph datasets. This benchmark systematically assesses the performance of graph pooling methods in three dimensions, i.e., effectiveness, robustness, and generalizability. We first evaluate the performance of these graph pooling approaches across different tasks including graph classification, graph regression and node classification. Then, we investigate their performance under potential noise attacks and out-of-distribution shifts in real-world scenarios. We also involve detailed efficiency analysis and parameter analysis. Extensive experiments validate the strong capability and applicability of graph pooling approaches in various scenarios, which can provide valuable insights and guidance for deep geometric learning research. The source code of our benchmark is available at https://github.com/goose315/Graph_Pooling_Benchmark.","sentences":["Graph pooling has gained attention for its ability to obtain effective node and graph representations for various downstream tasks.","Despite the recent surge in graph pooling approaches, there is a lack of standardized experimental settings and fair benchmarks to evaluate their performance.","To address this issue, we have constructed a comprehensive benchmark that includes 15 graph pooling methods and 21 different graph datasets.","This benchmark systematically assesses the performance of graph pooling methods in three dimensions, i.e., effectiveness, robustness, and generalizability.","We first evaluate the performance of these graph pooling approaches across different tasks including graph classification, graph regression and node classification.","Then, we investigate their performance under potential noise attacks and out-of-distribution shifts in real-world scenarios.","We also involve detailed efficiency analysis and parameter analysis.","Extensive experiments validate the strong capability and applicability of graph pooling approaches in various scenarios, which can provide valuable insights and guidance for deep geometric learning research.","The source code of our benchmark is available at https://github.com/goose315/Graph_Pooling_Benchmark."],"url":"http://arxiv.org/abs/2406.09031v1","category":"cs.LG"}
{"created":"2024-06-13 12:03:40","title":"CUER: Corrected Uniform Experience Replay for Off-Policy Continuous Deep Reinforcement Learning Algorithms","abstract":"The utilization of the experience replay mechanism enables agents to effectively leverage their experiences on several occasions. In previous studies, the sampling probability of the transitions was modified based on their relative significance. The process of reassigning sample probabilities for every transition in the replay buffer after each iteration is considered extremely inefficient. Hence, in order to enhance computing efficiency, experience replay prioritization algorithms reassess the importance of a transition as it is sampled. However, the relative importance of the transitions undergoes dynamic adjustments when the agent's policy and value function are iteratively updated. Furthermore, experience replay is a mechanism that retains the transitions generated by the agent's past policies, which could potentially diverge significantly from the agent's most recent policy. An increased deviation from the agent's most recent policy results in a greater frequency of off-policy updates, which has a negative impact on the agent's performance. In this paper, we develop a novel algorithm, Corrected Uniform Experience Replay (CUER), which stochastically samples the stored experience while considering the fairness among all other experiences without ignoring the dynamic nature of the transition importance by making sampled state distribution more on-policy. CUER provides promising improvements for off-policy continuous control algorithms in terms of sample efficiency, final performance, and stability of the policy during the training.","sentences":["The utilization of the experience replay mechanism enables agents to effectively leverage their experiences on several occasions.","In previous studies, the sampling probability of the transitions was modified based on their relative significance.","The process of reassigning sample probabilities for every transition in the replay buffer after each iteration is considered extremely inefficient.","Hence, in order to enhance computing efficiency, experience replay prioritization algorithms reassess the importance of a transition as it is sampled.","However, the relative importance of the transitions undergoes dynamic adjustments when the agent's policy and value function are iteratively updated.","Furthermore, experience replay is a mechanism that retains the transitions generated by the agent's past policies, which could potentially diverge significantly from the agent's most recent policy.","An increased deviation from the agent's most recent policy results in a greater frequency of off-policy updates, which has a negative impact on the agent's performance.","In this paper, we develop a novel algorithm, Corrected Uniform Experience Replay (CUER), which stochastically samples the stored experience while considering the fairness among all other experiences without ignoring the dynamic nature of the transition importance by making sampled state distribution more on-policy.","CUER provides promising improvements for off-policy continuous control algorithms in terms of sample efficiency, final performance, and stability of the policy during the training."],"url":"http://arxiv.org/abs/2406.09030v1","category":"cs.LG"}
{"created":"2024-06-13 12:02:51","title":"From Biased to Unbiased Dynamics: An Infinitesimal Generator Approach","abstract":"We investigate learning the eigenfunctions of evolution operators for time-reversal invariant stochastic processes, a prime example being the Langevin equation used in molecular dynamics. Many physical or chemical processes described by this equation involve transitions between metastable states separated by high potential barriers that can hardly be crossed during a simulation. To overcome this bottleneck, data are collected via biased simulations that explore the state space more rapidly. We propose a framework for learning from biased simulations rooted in the infinitesimal generator of the process and the associated resolvent operator. We contrast our approach to more common ones based on the transfer operator, showing that it can provably learn the spectral properties of the unbiased system from biased data. In experiments, we highlight the advantages of our method over transfer operator approaches and recent developments based on generator learning, demonstrating its effectiveness in estimating eigenfunctions and eigenvalues. Importantly, we show that even with datasets containing only a few relevant transitions due to sub-optimal biasing, our approach recovers relevant information about the transition mechanism.","sentences":["We investigate learning the eigenfunctions of evolution operators for time-reversal invariant stochastic processes, a prime example being the Langevin equation used in molecular dynamics.","Many physical or chemical processes described by this equation involve transitions between metastable states separated by high potential barriers that can hardly be crossed during a simulation.","To overcome this bottleneck, data are collected via biased simulations that explore the state space more rapidly.","We propose a framework for learning from biased simulations rooted in the infinitesimal generator of the process and the associated resolvent operator.","We contrast our approach to more common ones based on the transfer operator, showing that it can provably learn the spectral properties of the unbiased system from biased data.","In experiments, we highlight the advantages of our method over transfer operator approaches and recent developments based on generator learning, demonstrating its effectiveness in estimating eigenfunctions and eigenvalues.","Importantly, we show that even with datasets containing only a few relevant transitions due to sub-optimal biasing, our approach recovers relevant information about the transition mechanism."],"url":"http://arxiv.org/abs/2406.09028v1","category":"cs.LG"}
{"created":"2024-06-13 12:02:37","title":"Revisiting subregion holography using OPE blocks","abstract":"In this short note, we revisit the entanglement wedge representation of AdS$_3$ bulk fields in terms of CFT operator product expansion (OPE) blocks for a general class of blocks. Given a boundary interval and its associated causal diamond, the OPEs involve boundary operators with or without spin, and located either at spacelike or timelike edges of the diamond. Only for a subset of these cases, can the OPE block be dual to a geodesic bulk field. We show that when applied to de Sitter, a suitable combination of Euclidean OPE blocks can represent a dS scalar integrated over the timelike extremal surfaces, which play an important role in defining pseudo-entropy. We also work out some simple higher dimensional examples.","sentences":["In this short note, we revisit the entanglement wedge representation of AdS$_3$ bulk fields in terms of CFT operator product expansion (OPE) blocks for a general class of blocks.","Given a boundary interval and its associated causal diamond, the OPEs involve boundary operators with or without spin, and located either at spacelike or timelike edges of the diamond.","Only for a subset of these cases, can the OPE block be dual to a geodesic bulk field.","We show that when applied to de Sitter, a suitable combination of Euclidean OPE blocks can represent a dS scalar integrated over the timelike extremal surfaces, which play an important role in defining pseudo-entropy.","We also work out some simple higher dimensional examples."],"url":"http://arxiv.org/abs/2406.09027v1","category":"hep-th"}
{"created":"2024-06-13 12:01:28","title":"Steganalysis on Digital Watermarking: Is Your Defense Truly Impervious?","abstract":"Digital watermarking techniques are crucial for copyright protection and source identification of images, especially in the era of generative AI models. However, many existing watermarking methods, particularly content-agnostic approaches that embed fixed patterns regardless of image content, are vulnerable to steganalysis attacks that can extract and remove the watermark with minimal perceptual distortion. In this work, we categorize watermarking algorithms into content-adaptive and content-agnostic ones, and demonstrate how averaging a collection of watermarked images could reveal the underlying watermark pattern. We then leverage this extracted pattern for effective watermark removal under both graybox and blackbox settings, even when the collection contains multiple watermark patterns. For some algorithms like Tree-Ring watermarks, the extracted pattern can also forge convincing watermarks on clean images. Our quantitative and qualitative evaluations across twelve watermarking methods highlight the threat posed by steganalysis to content-agnostic watermarks and the importance of designing watermarking techniques resilient to such analytical attacks. We propose security guidelines calling for using content-adaptive watermarking strategies and performing security evaluation against steganalysis. We also suggest multi-key assignments as potential mitigations against steganalysis vulnerabilities.","sentences":["Digital watermarking techniques are crucial for copyright protection and source identification of images, especially in the era of generative AI models.","However, many existing watermarking methods, particularly content-agnostic approaches that embed fixed patterns regardless of image content, are vulnerable to steganalysis attacks that can extract and remove the watermark with minimal perceptual distortion.","In this work, we categorize watermarking algorithms into content-adaptive and content-agnostic ones, and demonstrate how averaging a collection of watermarked images could reveal the underlying watermark pattern.","We then leverage this extracted pattern for effective watermark removal under both graybox and blackbox settings, even when the collection contains multiple watermark patterns.","For some algorithms like Tree-Ring watermarks, the extracted pattern can also forge convincing watermarks on clean images.","Our quantitative and qualitative evaluations across twelve watermarking methods highlight the threat posed by steganalysis to content-agnostic watermarks and the importance of designing watermarking techniques resilient to such analytical attacks.","We propose security guidelines calling for using content-adaptive watermarking strategies and performing security evaluation against steganalysis.","We also suggest multi-key assignments as potential mitigations against steganalysis vulnerabilities."],"url":"http://arxiv.org/abs/2406.09026v1","category":"cs.CV"}
{"created":"2024-06-13 11:58:42","title":"Site-Specific Radio Channel Representation -- Current State and Future Applications","abstract":"A site-specific radio channel representation considers the surroundings of the communication system through the environment geometry, such as buildings, vegetation, and mobile objects including their material and surface properties. In this article, we focus on communication technologies for 5G and beyond that are increasingly able to exploit the specific environment geometry for both communication and sensing. We present methods for a site-specific radio channel representation that is spatially consistent, such that mobile transmitters and receivers cause a correlated time-varying channel impulse response. When modelled as random, this channel impulse response has non-stationary statistical properties, i.e., a time-variant Doppler spectrum, power delay profile, K-factor and spatial correlation. A site-specific radio channel representation will enable research into emerging 5G and beyond technologies such as distributed multiple-input multiple-output systems, reconfigurable intelligent surfaces, multi-band communication, and joint communication and sensing. These 5G and beyond technologies will be deployed for a wide range of environments, from dense urban areas to railways, road transportation, industrial automation, and unmanned aerial vehicles.","sentences":["A site-specific radio channel representation considers the surroundings of the communication system through the environment geometry, such as buildings, vegetation, and mobile objects including their material and surface properties.","In this article, we focus on communication technologies for 5G and beyond that are increasingly able to exploit the specific environment geometry for both communication and sensing.","We present methods for a site-specific radio channel representation that is spatially consistent, such that mobile transmitters and receivers cause a correlated time-varying channel impulse response.","When modelled as random, this channel impulse response has non-stationary statistical properties, i.e., a time-variant Doppler spectrum, power delay profile, K-factor and spatial correlation.","A site-specific radio channel representation will enable research into emerging 5G and beyond technologies such as distributed multiple-input multiple-output systems, reconfigurable intelligent surfaces, multi-band communication, and joint communication and sensing.","These 5G and beyond technologies will be deployed for a wide range of environments, from dense urban areas to railways, road transportation, industrial automation, and unmanned aerial vehicles."],"url":"http://arxiv.org/abs/2406.09025v1","category":"eess.SP"}
{"created":"2024-06-13 11:56:20","title":"Schur's Positive-Definite Network: Deep Learning in the SPD cone with structure","abstract":"Estimating matrices in the symmetric positive-definite (SPD) cone is of interest for many applications ranging from computer vision to graph learning. While there exist various convex optimization-based estimators, they remain limited in expressivity due to their model-based approach. The success of deep learning has thus led many to use neural networks to learn to estimate SPD matrices in a data-driven fashion. For learning structured outputs, one promising strategy involves architectures designed by unrolling iterative algorithms, which potentially benefit from inductive bias properties. However, designing correct unrolled architectures for SPD learning is difficult: they either do not guarantee that their output has all the desired properties, rely on heavy computations, or are overly restrained to specific matrices which hinders their expressivity. In this paper, we propose a novel and generic learning module with guaranteed SPD outputs called SpodNet, that also enables learning a larger class of functions than existing approaches. Notably, it solves the challenging task of learning jointly SPD and sparse matrices. Our experiments demonstrate the versatility of SpodNet layers.","sentences":["Estimating matrices in the symmetric positive-definite (SPD) cone is of interest for many applications ranging from computer vision to graph learning.","While there exist various convex optimization-based estimators, they remain limited in expressivity due to their model-based approach.","The success of deep learning has thus led many to use neural networks to learn to estimate SPD matrices in a data-driven fashion.","For learning structured outputs, one promising strategy involves architectures designed by unrolling iterative algorithms, which potentially benefit from inductive bias properties.","However, designing correct unrolled architectures for SPD learning is difficult: they either do not guarantee that their output has all the desired properties, rely on heavy computations, or are overly restrained to specific matrices which hinders their expressivity.","In this paper, we propose a novel and generic learning module with guaranteed SPD outputs called SpodNet, that also enables learning a larger class of functions than existing approaches.","Notably, it solves the challenging task of learning jointly SPD and sparse matrices.","Our experiments demonstrate the versatility of SpodNet layers."],"url":"http://arxiv.org/abs/2406.09023v1","category":"cs.LG"}
{"created":"2024-06-13 11:56:10","title":"Towards Unified AI Models for MU-MIMO Communications: A Tensor Equivariance Framework","abstract":"In this paper, we propose a unified framework based on equivariance for the design of artificial intelligence (AI)-assisted technologies in multi-user multiple-input-multiple-output (MU-MIMO) systems. We first provide definitions of multidimensional equivariance, high-order equivariance, and multidimensional invariance (referred to collectively as tensor equivariance). On this basis, by investigating the design of precoding and user scheduling, which are key techniques in MU-MIMO systems, we delve deeper into revealing tensor equivariance of the mappings from channel information to optimal precoding tensors, precoding auxiliary tensors, and scheduling indicators, respectively. To model mappings with tensor equivariance, we propose a series of plug-and-play tensor equivariant neural network (TENN) modules, where the computation involving intricate parameter sharing patterns is transformed into concise tensor operations. Building upon TENN modules, we propose the unified tensor equivariance framework that can be applicable to various communication tasks, based on which we easily accomplish the design of corresponding AI-assisted precoding and user scheduling schemes. Simulation results demonstrate that the constructed precoding and user scheduling methods achieve near-optimal performance while exhibiting significantly lower computational complexity and generalization to inputs with varying sizes across multiple dimensions. This validates the superiority of TENN modules and the unified framework.","sentences":["In this paper, we propose a unified framework based on equivariance for the design of artificial intelligence (AI)-assisted technologies in multi-user multiple-input-multiple-output (MU-MIMO) systems.","We first provide definitions of multidimensional equivariance, high-order equivariance, and multidimensional invariance (referred to collectively as tensor equivariance).","On this basis, by investigating the design of precoding and user scheduling, which are key techniques in MU-MIMO systems, we delve deeper into revealing tensor equivariance of the mappings from channel information to optimal precoding tensors, precoding auxiliary tensors, and scheduling indicators, respectively.","To model mappings with tensor equivariance, we propose a series of plug-and-play tensor equivariant neural network (TENN) modules, where the computation involving intricate parameter sharing patterns is transformed into concise tensor operations.","Building upon TENN modules, we propose the unified tensor equivariance framework that can be applicable to various communication tasks, based on which we easily accomplish the design of corresponding AI-assisted precoding and user scheduling schemes.","Simulation results demonstrate that the constructed precoding and user scheduling methods achieve near-optimal performance while exhibiting significantly lower computational complexity and generalization to inputs with varying sizes across multiple dimensions.","This validates the superiority of TENN modules and the unified framework."],"url":"http://arxiv.org/abs/2406.09022v1","category":"eess.SP"}
{"created":"2024-06-13 11:52:06","title":"Meta-Learning an Evolvable Developmental Encoding","abstract":"Representations for black-box optimisation methods (such as evolutionary algorithms) are traditionally constructed using a delicate manual process. This is in contrast to the representation that maps DNAs to phenotypes in biological organisms, which is at the hear of biological complexity and evolvability. Additionally, the core of this process is fundamentally the same across nearly all forms of life, reflecting their shared evolutionary origin. Generative models have shown promise in being learnable representations for black-box optimisation but they are not per se designed to be easily searchable. Here we present a system that can meta-learn such representation by directly optimising for a representation's ability to generate quality-diversity. In more detail, we show our meta-learning approach can find one Neural Cellular Automata, in which cells can attend to different parts of a \"DNA\" string genome during development, enabling it to grow different solvable 2D maze structures. We show that the evolved genotype-to-phenotype mappings become more and more evolvable, not only resulting in a faster search but also increasing the quality and diversity of grown artefacts.","sentences":["Representations for black-box optimisation methods (such as evolutionary algorithms) are traditionally constructed using a delicate manual process.","This is in contrast to the representation that maps DNAs to phenotypes in biological organisms, which is at the hear of biological complexity and evolvability.","Additionally, the core of this process is fundamentally the same across nearly all forms of life, reflecting their shared evolutionary origin.","Generative models have shown promise in being learnable representations for black-box optimisation but they are not per se designed to be easily searchable.","Here we present a system that can meta-learn such representation by directly optimising for a representation's ability to generate quality-diversity.","In more detail, we show our meta-learning approach can find one Neural Cellular Automata, in which cells can attend to different parts of a \"DNA\" string genome during development, enabling it to grow different solvable 2D maze structures.","We show that the evolved genotype-to-phenotype mappings become more and more evolvable, not only resulting in a faster search but also increasing the quality and diversity of grown artefacts."],"url":"http://arxiv.org/abs/2406.09020v1","category":"cs.NE"}
{"created":"2024-06-13 11:43:45","title":"Continuous time crystals as a PT symmetric state and the emergence of critical exceptional points","abstract":"Continuous time-translation symmetry is often spontaneously broken in open quantum systems, and the condition for their emergence has been actively investigated. However, there are only a few cases in which its condition for appearance has been fully elucidated. In this Letter, we show that a Lindladian parity-time (PT) symmetry can generically produce persistent periodic oscillations, including dissipative continuous time crystals, in one-collective spin models. By making an analogy to non-reciprocal phase transitions, we demonstrate that a transition point from the dynamical phase is associated with spontaneous PT symmetry breaking and typically corresponds to a \\textit{critical exceptional point}. Interestingly, the periodic orbits in the PT-symmetric phase are found to be center-type, implying an initial-state-dependent amplitude. These results are established by proving that the Lindbladian PT symmetry at the microscopic level implies a non-linear PT symmetry, and by performing a linear stability analysis near the transition point. This research will further our understanding of novel non-equilibrium phases of matter and phase transitions with spontaneous anti-unitary symmetry breaking.","sentences":["Continuous time-translation symmetry is often spontaneously broken in open quantum systems, and the condition for their emergence has been actively investigated.","However, there are only a few cases in which its condition for appearance has been fully elucidated.","In this Letter, we show that a Lindladian parity-time (PT) symmetry can generically produce persistent periodic oscillations, including dissipative continuous time crystals, in one-collective spin models.","By making an analogy to non-reciprocal phase transitions, we demonstrate that a transition point from the dynamical phase is associated with spontaneous PT symmetry breaking and typically corresponds to a \\textit{critical exceptional point}.","Interestingly, the periodic orbits in the PT-symmetric phase are found to be center-type, implying an initial-state-dependent amplitude.","These results are established by proving that the Lindbladian PT symmetry at the microscopic level implies a non-linear PT symmetry, and by performing a linear stability analysis near the transition point.","This research will further our understanding of novel non-equilibrium phases of matter and phase transitions with spontaneous anti-unitary symmetry breaking."],"url":"http://arxiv.org/abs/2406.09018v1","category":"quant-ph"}
{"created":"2024-06-13 11:40:26","title":"A PCA based Keypoint Tracking Approach to Automated Facial Expressions Encoding","abstract":"The Facial Action Coding System (FACS) for studying facial expressions is manual and requires significant effort and expertise. This paper explores the use of automated techniques to generate Action Units (AUs) for studying facial expressions. We propose an unsupervised approach based on Principal Component Analysis (PCA) and facial keypoint tracking to generate data-driven AUs called PCA AUs using the publicly available DISFA dataset. The PCA AUs comply with the direction of facial muscle movements and are capable of explaining over 92.83 percent of the variance in other public test datasets (BP4D-Spontaneous and CK+), indicating their capability to generalize facial expressions. The PCA AUs are also comparable to a keypoint-based equivalence of FACS AUs in terms of variance explained on the test datasets. In conclusion, our research demonstrates the potential of automated techniques to be an alternative to manual FACS labeling which could lead to efficient real-time analysis of facial expressions in psychology and related fields. To promote further research, we have made code repository publicly available.","sentences":["The Facial Action Coding System (FACS) for studying facial expressions is manual and requires significant effort and expertise.","This paper explores the use of automated techniques to generate Action Units (AUs) for studying facial expressions.","We propose an unsupervised approach based on Principal Component Analysis (PCA) and facial keypoint tracking to generate data-driven AUs called PCA AUs using the publicly available DISFA dataset.","The PCA AUs comply with the direction of facial muscle movements and are capable of explaining over 92.83 percent of the variance in other public test datasets (BP4D-Spontaneous and CK+), indicating their capability to generalize facial expressions.","The PCA AUs are also comparable to a keypoint-based equivalence of FACS AUs in terms of variance explained on the test datasets.","In conclusion, our research demonstrates the potential of automated techniques to be an alternative to manual FACS labeling which could lead to efficient real-time analysis of facial expressions in psychology and related fields.","To promote further research, we have made code repository publicly available."],"url":"http://arxiv.org/abs/2406.09017v1","category":"cs.CV"}
{"created":"2024-06-13 17:56:56","title":"Multiagent Multitraversal Multimodal Self-Driving: Open MARS Dataset","abstract":"Large-scale datasets have fueled recent advancements in AI-based autonomous vehicle research. However, these datasets are usually collected from a single vehicle's one-time pass of a certain location, lacking multiagent interactions or repeated traversals of the same place. Such information could lead to transformative enhancements in autonomous vehicles' perception, prediction, and planning capabilities. To bridge this gap, in collaboration with the self-driving company May Mobility, we present the MARS dataset which unifies scenarios that enable MultiAgent, multitraveRSal, and multimodal autonomous vehicle research. More specifically, MARS is collected with a fleet of autonomous vehicles driving within a certain geographical area. Each vehicle has its own route and different vehicles may appear at nearby locations. Each vehicle is equipped with a LiDAR and surround-view RGB cameras. We curate two subsets in MARS: one facilitates collaborative driving with multiple vehicles simultaneously present at the same location, and the other enables memory retrospection through asynchronous traversals of the same location by multiple vehicles. We conduct experiments in place recognition and neural reconstruction. More importantly, MARS introduces new research opportunities and challenges such as multitraversal 3D reconstruction, multiagent perception, and unsupervised object discovery. Our data and codes can be found at https://ai4ce.github.io/MARS/.","sentences":["Large-scale datasets have fueled recent advancements in AI-based autonomous vehicle research.","However, these datasets are usually collected from a single vehicle's one-time pass of a certain location, lacking multiagent interactions or repeated traversals of the same place.","Such information could lead to transformative enhancements in autonomous vehicles' perception, prediction, and planning capabilities.","To bridge this gap, in collaboration with the self-driving company May Mobility, we present the MARS dataset which unifies scenarios that enable MultiAgent, multitraveRSal, and multimodal autonomous vehicle research.","More specifically, MARS is collected with a fleet of autonomous vehicles driving within a certain geographical area.","Each vehicle has its own route and different vehicles may appear at nearby locations.","Each vehicle is equipped with a LiDAR and surround-view RGB cameras.","We curate two subsets in MARS: one facilitates collaborative driving with multiple vehicles simultaneously present at the same location, and the other enables memory retrospection through asynchronous traversals of the same location by multiple vehicles.","We conduct experiments in place recognition and neural reconstruction.","More importantly, MARS introduces new research opportunities and challenges such as multitraversal 3D reconstruction, multiagent perception, and unsupervised object discovery.","Our data and codes can be found at https://ai4ce.github.io/MARS/."],"url":"http://arxiv.org/abs/2406.09383v1","category":"cs.CV"}
{"created":"2024-06-13 17:40:44","title":"Foundations for reconstructing early microbial life","abstract":"For more than 3.5 billion years, life experienced dramatic environmental extremes on Earth. These include shifts from oxygen-less to over-oxygenated atmospheres and cycling between hothouse conditions and global glaciations. Meanwhile, an ecological revolution took place. The planet evolved from one dominated by microbial life to one containing the plants and animals that are most familiar today. The activities of many key cellular inventions evolved early in the history of life, collectively defining the nature of our biosphere and underpinning human survival. There is a critical need for a new disciplinary synthesis to reveal how microbes and their molecular systems survived ever changing global conditions over deep time. This review critically examines our current understanding of early microbial life and describes the foundations of an emerging area in microbiology and evolutionary synthetic biology to reconstruct the earliest microbial innovations.","sentences":["For more than 3.5 billion years, life experienced dramatic environmental extremes on Earth.","These include shifts from oxygen-less to over-oxygenated atmospheres and cycling between hothouse conditions and global glaciations.","Meanwhile, an ecological revolution took place.","The planet evolved from one dominated by microbial life to one containing the plants and animals that are most familiar today.","The activities of many key cellular inventions evolved early in the history of life, collectively defining the nature of our biosphere and underpinning human survival.","There is a critical need for a new disciplinary synthesis to reveal how microbes and their molecular systems survived ever changing global conditions over deep time.","This review critically examines our current understanding of early microbial life and describes the foundations of an emerging area in microbiology and evolutionary synthetic biology to reconstruct the earliest microbial innovations."],"url":"http://arxiv.org/abs/2406.09354v1","category":"q-bio.PE"}
{"created":"2024-06-13 16:09:54","title":"Discovery and Extensive Follow-Up of SN 2024ggi, a nearby type IIP supernova in NGC 3621","abstract":"We present the discovery and early observations of the nearby Type II supernova (SN) 2024ggi in NGC 3621 at 6.64 +/- 0.3 Mpc. The SN was caught 5.8 (+1.9 -2.9) hours after its explosion by the ATLAS survey. Early-phase, high-cadence, and multi-band photometric follow-up was performed by the Kinder (Kilonova Finder) project, collecting over 1000 photometric data points within a week. The combined o- and r-band light curves show a rapid rise of 3.3 magnitudes in 13.7 hours, much faster than SN 2023ixf (another recent, nearby, and well-observed SN II). Between 13.8 and 18.8 hours after explosion SN 2024ggi became bluer, with u-g colour dropping from 0.53 to 0.15 mag. The rapid blueward evolution indicates a wind shock breakout (SBO) scenario. No hour-long brightening expected for the SBO from a bare stellar surface was detected during our observations. The classification spectrum, taken 17 hours after the SN explosion, shows flash features of high-ionization species such as Balmer lines, He I, C III, and N III. Detailed light curve modeling reveals critical insights into the properties of the circumstellar material (CSM). Our favoured model has an explosion energy of 2 x 10^51 erg, a mass-loss rate of 10^-3 solar_mass/yr (with an assumed 10 km/s wind), and a confined CSM radius of 6 x 10^14 cm. The corresponding CSM mass is 0.4 solar_mass. Comparisons with SN 2023ixf highlight that SN 2024ggi has a smaller CSM density, resulting in a faster rise and fainter UV flux. The extensive dataset and the involvement of citizen astronomers underscore that a collaborative network is essential for SBO searches, leading to more precise and comprehensive SN characterizations.","sentences":["We present the discovery and early observations of the nearby Type II supernova (SN) 2024ggi in NGC 3621 at 6.64 +/- 0.3 Mpc.","The SN was caught 5.8 (+1.9 -2.9) hours after its explosion by the ATLAS survey.","Early-phase, high-cadence, and multi-band photometric follow-up was performed by the Kinder (Kilonova Finder) project, collecting over 1000 photometric data points within a week.","The combined o- and r-band light curves show a rapid rise of 3.3 magnitudes in 13.7 hours, much faster than SN 2023ixf (another recent, nearby, and well-observed SN II).","Between 13.8 and 18.8 hours after explosion SN 2024ggi became bluer, with u-g colour dropping from 0.53 to 0.15 mag.","The rapid blueward evolution indicates a wind shock breakout (SBO) scenario.","No hour-long brightening expected for the SBO from a bare stellar surface was detected during our observations.","The classification spectrum, taken 17 hours after the SN explosion, shows flash features of high-ionization species such as Balmer lines, He I, C III, and N III.","Detailed light curve modeling reveals critical insights into the properties of the circumstellar material (CSM).","Our favoured model has an explosion energy of 2 x 10^51 erg, a mass-loss rate of 10^-3 solar_mass/yr (with an assumed 10 km/s wind), and a confined CSM radius of 6 x 10^14 cm.","The corresponding CSM mass is 0.4 solar_mass.","Comparisons with SN 2023ixf highlight that SN 2024ggi has a smaller CSM density, resulting in a faster rise and fainter UV flux.","The extensive dataset and the involvement of citizen astronomers underscore that a collaborative network is essential for SBO searches, leading to more precise and comprehensive SN characterizations."],"url":"http://arxiv.org/abs/2406.09270v1","category":"astro-ph.HE"}
{"created":"2024-06-13 15:46:46","title":"Two problems on submodules of $H^2(\\mathbb{D}^n)$","abstract":"Given any shift-invariant closed subspace $\\mathcal{S}$ (aka submodule) of the Hardy space over the unit polydisc $H^2(\\mathbb{D}^n)$ (where $n \\geq 2$), let $R_{z_j}:=M_{z_j}|_{\\mathcal{S}}$, and $E_{z_j}:=P_{\\mathcal{S}}\\circ ev_{z_j}$, for each $j \\in \\{1,\\ldots,n\\}$. Here, $ev_{z_j}$ is the operator evaluating at $0$ in the $z_j$-th variable. In this article, we prove that given any subset $\\Lambda \\subseteq \\{1,\\ldots,n\\}$, there exists a collection of one-variable inner functions $\\{\\phi_\\lambda (z_\\lambda)\\}_{\\lambda \\in \\Lambda}$ on $\\mathbb{D}^n$, such that \\[ \\mathcal{S} = \\sum_{\\lambda \\in \\Lambda} \\phi_\\lambda (z_\\lambda)H^2(\\mathbb{D}^n), \\] if and only if the conditions $ (I_{\\mathcal{S}}-E_{z_k}E_{z_k}^*)(I_{\\mathcal{S}}-R_{z_k}R_{z_k}^*)=0$ for all $k \\in \\{1,\\dots,n\\} \\setminus \\Lambda$, and $(I_{\\mathcal{S}}-E_{z_{i}}E_{z_{i}}^*)(I_{\\mathcal{S}}-R_{z_{i}}R_{z_{i}}^*)(I_{\\mathcal{S}}-E_{z_{j}}E_{z_{j}}^*)(I_{\\mathcal{S}}-R_{z_{j}}R_{z_{j}}^*)=0$ for all distinct $i,j \\in \\Lambda$ are satisfied. Following this, we study R.G. Douglas's question on the commutativity of orthogonal projections onto the corresponding quotient modules.","sentences":["Given any shift-invariant closed subspace $\\mathcal{S}$ (aka submodule) of the Hardy space over the unit polydisc $H^2(\\mathbb{D}^n)$ (where $n \\geq 2$), let $R_{z_j}:=M_{z_j}|_{\\mathcal{S}}$, and $E_{z_j}:=P_{\\mathcal{S}}\\circ ev_{z_j}$, for each $j \\in \\{1,\\ldots,n\\}$. Here, $ev_{z_j}$ is the operator evaluating at $0$ in the $z_j$-th variable.","In this article, we prove that given any subset $\\Lambda \\subseteq \\{1,\\ldots,n\\}$, there exists a collection of one-variable inner functions $\\{\\phi_\\lambda (z_\\lambda)\\}_{\\lambda \\in \\Lambda}$ on $\\mathbb{D}^n$, such that \\[ \\mathcal{S} = \\sum_{\\lambda \\in \\Lambda} \\phi_\\lambda (z_\\lambda)H^2(\\mathbb{D}^n), \\] if and only if the conditions $ (I_{\\mathcal{S}}-E_{z_k}E_{z_k}^*)(I_{\\mathcal{S}}-R_{z_k}R_{z_k}^*)=0$ for all $k \\in \\{1,\\dots,n\\} \\setminus \\Lambda$, and $(I_{\\mathcal{S}}-E_{z_{i}}E_{z_{i}}^*)(I_{\\mathcal{S}}-R_{z_{i}}R_{z_{i}}^*)(I_{\\mathcal{S}}-E_{z_{j}}E_{z_{j}}^*)(I_{\\mathcal{S}}-R_{z_{j}}R_{z_{j}}^*)=0$ for all distinct $i,j \\in \\Lambda$ are satisfied.","Following this, we study R.G. Douglas's question on the commutativity of orthogonal projections onto the corresponding quotient modules."],"url":"http://arxiv.org/abs/2406.09245v1","category":"math.FA"}
{"created":"2024-06-13 15:15:07","title":"WildlifeReID-10k: Wildlife re-identification dataset with 10k individual animals","abstract":"We introduce a new wildlife re-identification dataset WildlifeReID-10k with more than 214k images of 10k individual animals. It is a collection of 30 existing wildlife re-identification datasets with additional processing steps. WildlifeReID-10k contains animals as diverse as marine turtles, primates, birds, African herbivores, marine mammals and domestic animals. Due to the ubiquity of similar images in datasets, we argue that the standard (random) splits into training and testing sets are inadequate for wildlife re-identification and propose a new similarity-aware split based on the similarity of extracted features. To promote fair method comparison, we include similarity-aware splits both for closed-set and open-set settings, use MegaDescriptor - a foundational model for wildlife re-identification - for baseline performance and host a leaderboard with the best results. We publicly publish the dataset and the codes used to create it in the wildlife-datasets library, making WildlifeReID-10k both highly curated and easy to use.","sentences":["We introduce a new wildlife re-identification dataset WildlifeReID-10k with more than 214k images of 10k individual animals.","It is a collection of 30 existing wildlife re-identification datasets with additional processing steps.","WildlifeReID-10k contains animals as diverse as marine turtles, primates, birds, African herbivores, marine mammals and domestic animals.","Due to the ubiquity of similar images in datasets, we argue that the standard (random) splits into training and testing sets are inadequate for wildlife re-identification and propose a new similarity-aware split based on the similarity of extracted features.","To promote fair method comparison, we include similarity-aware splits both for closed-set and open-set settings, use MegaDescriptor - a foundational model for wildlife re-identification - for baseline performance and host a leaderboard with the best results.","We publicly publish the dataset and the codes used to create it in the wildlife-datasets library, making WildlifeReID-10k both highly curated and easy to use."],"url":"http://arxiv.org/abs/2406.09211v1","category":"cs.CV"}
{"created":"2024-06-13 14:55:02","title":"When Pearson $\u03c7^2$ and other divisible statistics are not goodness-of-fit tests","abstract":"Thousands of experiments are analyzed and papers are published each year involving the statistical analysis of grouped data. While this area of statistics is often perceived - somewhat naively - as saturated, several misconceptions still affect everyday practice, and new frontiers have so far remained unexplored. Researchers must be aware of the limitations affecting their analyses and what are the new possibilities in their hands.   Motivated by this need, the article introduces a unifying approach to the analysis of grouped data which allows us to study the class of divisible statistics - that includes Pearson's $\\chi^2$, the likelihood ratio as special cases - with a fresh perspective. The contributions collected in this manuscript span from modeling and estimation to distribution-free goodness-of-fit tests.   Perhaps the most surprising result presented here is that, in a sparse regime, all tests proposed in the literature are dominated by a class of weighted linear statistics.","sentences":["Thousands of experiments are analyzed and papers are published each year involving the statistical analysis of grouped data.","While this area of statistics is often perceived - somewhat naively - as saturated, several misconceptions still affect everyday practice, and new frontiers have so far remained unexplored.","Researchers must be aware of the limitations affecting their analyses and what are the new possibilities in their hands.   ","Motivated by this need, the article introduces a unifying approach to the analysis of grouped data which allows us to study the class of divisible statistics - that includes Pearson's $\\chi^2$, the likelihood ratio as special cases - with a fresh perspective.","The contributions collected in this manuscript span from modeling and estimation to distribution-free goodness-of-fit tests.   ","Perhaps the most surprising result presented here is that, in a sparse regime, all tests proposed in the literature are dominated by a class of weighted linear statistics."],"url":"http://arxiv.org/abs/2406.09195v1","category":"stat.ME"}
{"created":"2024-06-13 14:31:41","title":"Schur Quantization and Complex Chern-Simons theory","abstract":"Any four-dimensional Supersymmetric Quantum Field Theory with eight supercharges can be associated to a certain complex symplectic manifold called the \"K-theoretic Coulomb branch\" of the theory. The collection of K-theoretic Coulomb branches include many complex phase spaces of great interest, including in particular the \"character varieties\" of complex flat connections on a Riemann surface. The SQFT definition endows K-theoretic Coulomb branches with a variety of canonical structures, including a deformation quantization. In this paper we introduce a canonical \"Schur\" quantization of K-theoretic Coulomb branches. It is defined by a variant of the Gelfand-Naimark-Segal construction, applied to protected Schur correlation functions of half-BPS line defects. Schur quantization produces an actual quantization of the complex phase space. As a concrete application, we apply this construction to character varieties in order to quantize Chern-Simons gauge theory with a complex gauge group. Other applications include the definition of a new quantum deformation of the Lorentz group, and the solution of certain spectral problems via dualities.","sentences":["Any four-dimensional Supersymmetric Quantum Field Theory with eight supercharges can be associated to a certain complex symplectic manifold called the \"K-theoretic Coulomb branch\" of the theory.","The collection of K-theoretic Coulomb branches include many complex phase spaces of great interest, including in particular the \"character varieties\" of complex flat connections on a Riemann surface.","The SQFT definition endows K-theoretic Coulomb branches with a variety of canonical structures, including a deformation quantization.","In this paper we introduce a canonical \"Schur\" quantization of K-theoretic Coulomb branches.","It is defined by a variant of the Gelfand-Naimark-Segal construction, applied to protected Schur correlation functions of half-BPS line defects.","Schur quantization produces an actual quantization of the complex phase space.","As a concrete application, we apply this construction to character varieties in order to quantize Chern-Simons gauge theory with a complex gauge group.","Other applications include the definition of a new quantum deformation of the Lorentz group, and the solution of certain spectral problems via dualities."],"url":"http://arxiv.org/abs/2406.09171v1","category":"hep-th"}
{"created":"2024-06-13 14:18:56","title":"Towards Multilingual Audio-Visual Question Answering","abstract":"In this paper, we work towards extending Audio-Visual Question Answering (AVQA) to multilingual settings. Existing AVQA research has predominantly revolved around English and replicating it for addressing AVQA in other languages requires a substantial allocation of resources. As a scalable solution, we leverage machine translation and present two multilingual AVQA datasets for eight languages created from existing benchmark AVQA datasets. This prevents extra human annotation efforts of collecting questions and answers manually. To this end, we propose, MERA framework, by leveraging state-of-the-art (SOTA) video, audio, and textual foundation models for AVQA in multiple languages. We introduce a suite of models namely MERA-L, MERA-C, MERA-T with varied model architectures to benchmark the proposed datasets. We believe our work will open new research directions and act as a reference benchmark for future works in multilingual AVQA.","sentences":["In this paper, we work towards extending Audio-Visual Question Answering (AVQA) to multilingual settings.","Existing AVQA research has predominantly revolved around English and replicating it for addressing AVQA in other languages requires a substantial allocation of resources.","As a scalable solution, we leverage machine translation and present two multilingual AVQA datasets for eight languages created from existing benchmark AVQA datasets.","This prevents extra human annotation efforts of collecting questions and answers manually.","To this end, we propose, MERA framework, by leveraging state-of-the-art (SOTA) video, audio, and textual foundation models for AVQA in multiple languages.","We introduce a suite of models namely MERA-L, MERA-C, MERA-T with varied model architectures to benchmark the proposed datasets.","We believe our work will open new research directions and act as a reference benchmark for future works in multilingual AVQA."],"url":"http://arxiv.org/abs/2406.09156v1","category":"cs.LG"}
{"created":"2024-06-13 14:08:56","title":"Investigating the translation capabilities of Large Language Models trained on parallel data only","abstract":"In recent years, Large Language Models (LLMs) have demonstrated exceptional proficiency across a broad spectrum of Natural Language Processing (NLP) tasks, including Machine Translation. However, previous methods predominantly relied on iterative processes such as instruction fine-tuning or continual pre-training, leaving unexplored the challenges of training LLMs solely on parallel data. In this work, we introduce PLUME (Parallel Language Model), a collection of three 2B LLMs featuring varying vocabulary sizes (32k, 128k, and 256k) trained exclusively on Catalan-centric parallel examples. These models perform comparably to previous encoder-decoder architectures on 16 supervised translation directions and 56 zero-shot ones. Utilizing this set of models, we conduct a thorough investigation into the translation capabilities of LLMs, probing their performance, the impact of the different elements of the prompt, and their cross-lingual representation space.","sentences":["In recent years, Large Language Models (LLMs) have demonstrated exceptional proficiency across a broad spectrum of Natural Language Processing (NLP) tasks, including Machine Translation.","However, previous methods predominantly relied on iterative processes such as instruction fine-tuning or continual pre-training, leaving unexplored the challenges of training LLMs solely on parallel data.","In this work, we introduce PLUME (Parallel Language Model), a collection of three 2B LLMs featuring varying vocabulary sizes (32k, 128k, and 256k) trained exclusively on Catalan-centric parallel examples.","These models perform comparably to previous encoder-decoder architectures on 16 supervised translation directions and 56 zero-shot ones.","Utilizing this set of models, we conduct a thorough investigation into the translation capabilities of LLMs, probing their performance, the impact of the different elements of the prompt, and their cross-lingual representation space."],"url":"http://arxiv.org/abs/2406.09140v1","category":"cs.CL"}
{"created":"2024-06-13 13:29:07","title":"Emergence of superradiance in dissipative dipolar-coupled spin systems","abstract":"In the superradiance phenomenon, a collection of non-interacting atoms exhibits collective dissipation due to interaction with a common radiation field, resulting in a non-monotonic decay profile. This work shows that dissipative dipolar-coupled systems exhibit an identical collective dissipation aided by the nonsecular part of the dipolar coupling. We consider a simplified dipolar network where the dipolar interaction between the spin-pairs is assumed to be identical. Hence the dynamics remain confined in the block diagonal Hilbert spaces. For a suitable choice of the initial condition, the resulting dynamics require dealing with a smaller subspace which helps extend the analysis to a larger spin network. To include the nonsecular dipolar relaxation, we use a fluctuation-regulated quantum master equation. We note that a successful observation of superradiance in this system requires a weak system-bath coupling. Moreover, we find that for an ensemble of N spins, the maximum intensity of the radiation exhibits a nearly quadratic scaling (N^2), and the dipolar relaxation time follows an inverse square proportionality (1/N^2); these two observations help characterize the emergence of superradiance. Our results agree well with the standard results of pure spin superradiance observed experimentally in various systems.","sentences":["In the superradiance phenomenon, a collection of non-interacting atoms exhibits collective dissipation due to interaction with a common radiation field, resulting in a non-monotonic decay profile.","This work shows that dissipative dipolar-coupled systems exhibit an identical collective dissipation aided by the nonsecular part of the dipolar coupling.","We consider a simplified dipolar network where the dipolar interaction between the spin-pairs is assumed to be identical.","Hence the dynamics remain confined in the block diagonal Hilbert spaces.","For a suitable choice of the initial condition, the resulting dynamics require dealing with a smaller subspace which helps extend the analysis to a larger spin network.","To include the nonsecular dipolar relaxation, we use a fluctuation-regulated quantum master equation.","We note that a successful observation of superradiance in this system requires a weak system-bath coupling.","Moreover, we find that for an ensemble of N spins, the maximum intensity of the radiation exhibits a nearly quadratic scaling (N^2), and the dipolar relaxation time follows an inverse square proportionality (1/N^2); these two observations help characterize the emergence of superradiance.","Our results agree well with the standard results of pure spin superradiance observed experimentally in various systems."],"url":"http://arxiv.org/abs/2406.09100v1","category":"quant-ph"}
{"created":"2024-06-13 13:28:58","title":"Towards a Function-as-a-Service Choreographic Programming Language: Examples and Applications","abstract":"Choreographic Programming (CP) is a language paradigm whereby software artefacts, called choreographies, specify the behaviour of communicating participants. CP is famous for its correctness-by-construction approach to the development of concurrent, distributed systems. In this paper, we illustrate FaaSChal, a proposal for a CP language tailored for the case of serverless Function-as-a-Service (FaaS). In FaaS, developers define a distributed architecture as a collection of stateless functions, leaving to the serverless platform the management of deployment and scaling. We provide a first account of a CP language tailored for the FaaS case via examples that present some of its relevant features, including projection. In addition, we showcase a novel application of CP. We use the choreography as a source to extract information on the infrastructural relations among functions so that we can synthesise policies that strive to minimise their latency while guaranteeing the respect of user-defined constraints.","sentences":["Choreographic Programming (CP) is a language paradigm whereby software artefacts, called choreographies, specify the behaviour of communicating participants.","CP is famous for its correctness-by-construction approach to the development of concurrent, distributed systems.","In this paper, we illustrate FaaSChal, a proposal for a CP language tailored for the case of serverless Function-as-a-Service (FaaS).","In FaaS, developers define a distributed architecture as a collection of stateless functions, leaving to the serverless platform the management of deployment and scaling.","We provide a first account of a CP language tailored for the FaaS case via examples that present some of its relevant features, including projection.","In addition, we showcase a novel application of CP.","We use the choreography as a source to extract information on the infrastructural relations among functions so that we can synthesise policies that strive to minimise their latency while guaranteeing the respect of user-defined constraints."],"url":"http://arxiv.org/abs/2406.09099v1","category":"cs.PL"}
{"created":"2024-06-13 12:20:26","title":"Evaluating Privacy, Security, and Trust Perceptions in Conversational AI: A Systematic Review","abstract":"Conversational AI (CAI) systems which encompass voice- and text-based assistants are on the rise and have been largely integrated into people's everyday lives. Despite their widespread adoption, users voice concerns regarding privacy, security and trust in these systems. However, the composition of these perceptions, their impact on technology adoption and usage and the relationship between privacy, security and trust perceptions in the CAI context remain open research challenges. This study contributes to the field by conducting a Systematic Literature Review and offers insights into the current state of research on privacy, security and trust perceptions in the context of CAI systems. The review covers application fields and user groups and sheds light on empirical methods and tools used for assessment. Moreover, it provides insights into the reliability and validity of privacy, security and trust scales, as well as extensively investigating the subconstructs of each item as well as additional concepts which are concurrently collected. We point out that the perceptions of trust, privacy and security overlap based on the subconstructs we identified. While the majority of studies investigate one of these concepts, only a few studies were found exploring privacy, security and trust perceptions jointly. Our research aims to inform on directions to develop and use reliable scales for users' privacy, security and trust perceptions and contribute to the development of trustworthy CAI systems.","sentences":["Conversational AI (CAI) systems which encompass voice- and text-based assistants are on the rise and have been largely integrated into people's everyday lives.","Despite their widespread adoption, users voice concerns regarding privacy, security and trust in these systems.","However, the composition of these perceptions, their impact on technology adoption and usage and the relationship between privacy, security and trust perceptions in the CAI context remain open research challenges.","This study contributes to the field by conducting a Systematic Literature Review and offers insights into the current state of research on privacy, security and trust perceptions in the context of CAI systems.","The review covers application fields and user groups and sheds light on empirical methods and tools used for assessment.","Moreover, it provides insights into the reliability and validity of privacy, security and trust scales, as well as extensively investigating the subconstructs of each item as well as additional concepts which are concurrently collected.","We point out that the perceptions of trust, privacy and security overlap based on the subconstructs we identified.","While the majority of studies investigate one of these concepts, only a few studies were found exploring privacy, security and trust perceptions jointly.","Our research aims to inform on directions to develop and use reliable scales for users' privacy, security and trust perceptions and contribute to the development of trustworthy CAI systems."],"url":"http://arxiv.org/abs/2406.09037v1","category":"cs.HC"}
{"created":"2024-06-13 11:38:58","title":"Deep learning empowered sensor fusion to improve infant movement classification","abstract":"There is a recent boom in the development of AI solutions to facilitate and enhance diagnostic procedures for established clinical tools. To assess the integrity of the developing nervous system, the Prechtl general movement assessment (GMA) is recognized for its clinical value in the diagnosis of neurological impairments in early infancy. GMA has been increasingly augmented through machine learning approaches intending to scale-up its application, circumvent costs in the training of human assessors and further standardize classification of spontaneous motor patterns. Available deep learning tools, all of which are based on single sensor modalities, are however still considerably inferior to that of well-trained human assessors. These approaches are hardly comparable as all models are designed, trained and evaluated on proprietary/ silo-data sets. We propose a sensor fusion approach for assessing fidgety movements (FMs) comparing three different sensor modalities (pressure, inertial, and visual sensors). Various combinations and two sensor fusion approaches (late and early fusion) for infant movement classification were tested to evaluate whether a multi-sensor system outperforms single modality assessments. The performance of the three-sensor fusion (classification accuracy of 94.5\\%) was significantly higher than that of any single modality evaluated, suggesting the sensor fusion approach is a promising avenue for automated classification of infant motor patterns. The development of a robust sensor fusion system may significantly enhance AI-based early recognition of neurofunctions, ultimately facilitating early implementation of automated detection of neurodevelopmental conditions.","sentences":["There is a recent boom in the development of AI solutions to facilitate and enhance diagnostic procedures for established clinical tools.","To assess the integrity of the developing nervous system, the Prechtl general movement assessment (GMA) is recognized for its clinical value in the diagnosis of neurological impairments in early infancy.","GMA has been increasingly augmented through machine learning approaches intending to scale-up its application, circumvent costs in the training of human assessors and further standardize classification of spontaneous motor patterns.","Available deep learning tools, all of which are based on single sensor modalities, are however still considerably inferior to that of well-trained human assessors.","These approaches are hardly comparable as all models are designed, trained and evaluated on proprietary/ silo-data sets.","We propose a sensor fusion approach for assessing fidgety movements (FMs) comparing three different sensor modalities (pressure, inertial, and visual sensors).","Various combinations and two sensor fusion approaches (late and early fusion) for infant movement classification were tested to evaluate whether a multi-sensor system outperforms single modality assessments.","The performance of the three-sensor fusion (classification accuracy of 94.5\\%) was significantly higher than that of any single modality evaluated, suggesting the sensor fusion approach is a promising avenue for automated classification of infant motor patterns.","The development of a robust sensor fusion system may significantly enhance AI-based early recognition of neurofunctions, ultimately facilitating early implementation of automated detection of neurodevelopmental conditions."],"url":"http://arxiv.org/abs/2406.09014v1","category":"cs.LG"}
{"created":"2024-06-13 11:29:21","title":"Fredformer: Frequency Debiased Transformer for Time Series Forecasting","abstract":"The Transformer model has shown leading performance in time series forecasting. Nevertheless, in some complex scenarios, it tends to learn low-frequency features in the data and overlook high-frequency features, showing a frequency bias. This bias prevents the model from accurately capturing important high-frequency data features. In this paper, we undertook empirical analyses to understand this bias and discovered that frequency bias results from the model disproportionately focusing on frequency features with higher energy. Based on our analysis, we formulate this bias and propose Fredformer, a Transformer-based framework designed to mitigate frequency bias by learning features equally across different frequency bands. This approach prevents the model from overlooking lower amplitude features important for accurate forecasting. Extensive experiments show the effectiveness of our proposed approach, which can outperform other baselines in different real-world time-series datasets. Furthermore, we introduce a lightweight variant of the Fredformer with an attention matrix approximation, which achieves comparable performance but with much fewer parameters and lower computation costs. The code is available at: https://github.com/chenzRG/Fredformer","sentences":["The Transformer model has shown leading performance in time series forecasting.","Nevertheless, in some complex scenarios, it tends to learn low-frequency features in the data and overlook high-frequency features, showing a frequency bias.","This bias prevents the model from accurately capturing important high-frequency data features.","In this paper, we undertook empirical analyses to understand this bias and discovered that frequency bias results from the model disproportionately focusing on frequency features with higher energy.","Based on our analysis, we formulate this bias and propose Fredformer, a Transformer-based framework designed to mitigate frequency bias by learning features equally across different frequency bands.","This approach prevents the model from overlooking lower amplitude features important for accurate forecasting.","Extensive experiments show the effectiveness of our proposed approach, which can outperform other baselines in different real-world time-series datasets.","Furthermore, we introduce a lightweight variant of the Fredformer with an attention matrix approximation, which achieves comparable performance but with much fewer parameters and lower computation costs.","The code is available at: https://github.com/chenzRG/Fredformer"],"url":"http://arxiv.org/abs/2406.09009v1","category":"cs.LG"}
{"created":"2024-06-13 10:54:03","title":"Introducing Brain-like Concepts to Embodied Hand-crafted Dialog Management System","abstract":"Along with the development of chatbot, language models and speech technologies, there is a growing possibility and interest of creating systems able to interface with humans seamlessly through natural language or directly via speech. In this paper, we want to demonstrate that placing the research on dialog system in the broader context of embodied intelligence allows to introduce concepts taken from neurobiology and neuropsychology to define behavior architecture that reconcile hand-crafted design and artificial neural network and open the gate to future new learning approaches like imitation or learning by instruction. To do so, this paper presents a neural behavior engine that allows creation of mixed initiative dialog and action generation based on hand-crafted models using a graphical language. A demonstration of the usability of such brain-like inspired architecture together with a graphical dialog model is described through a virtual receptionist application running on a semi-public space.","sentences":["Along with the development of chatbot, language models and speech technologies, there is a growing possibility and interest of creating systems able to interface with humans seamlessly through natural language or directly via speech.","In this paper, we want to demonstrate that placing the research on dialog system in the broader context of embodied intelligence allows to introduce concepts taken from neurobiology and neuropsychology to define behavior architecture that reconcile hand-crafted design and artificial neural network and open the gate to future new learning approaches like imitation or learning by instruction.","To do so, this paper presents a neural behavior engine that allows creation of mixed initiative dialog and action generation based on hand-crafted models using a graphical language.","A demonstration of the usability of such brain-like inspired architecture together with a graphical dialog model is described through a virtual receptionist application running on a semi-public space."],"url":"http://arxiv.org/abs/2406.08996v1","category":"cs.AI"}
{"created":"2024-06-13 10:38:53","title":"Ups and downs in the X-ray emission of the colliding wind binaries HD 168112 and HD 167971","abstract":"The long-period O-star binary system HD 168112 and the triple O-star system HD 167971 are well-known sources of non-thermal radio emission that arises from a colliding wind interaction. The wind-wind collisions in these systems should result in phase-dependent X-ray emissions. The presence of a population of relativistic electrons in the wind interaction zone could affect the properties of the X-ray emission and make it deviate from the behaviour expected for adiabatic shocks. We investigate the X-ray emission of these systems with the goals of quantifying the fraction of the X-ray flux arising from wind interactions and determining whether these emissions follow the predictions for adiabatic wind-wind collisions. Six X-ray observations were collected with XMM-Newton. Three observations were scheduled around the most recent periastron passage of HD 168112. Spectra and light curves were analysed and compared with simple predictions of model calculations for X-ray emission from colliding wind systems. The X-ray emission of HD 168112 varies as the inverse of the orbital separation, as expected for an adiabatic wind interaction zone. The relative contribution of intrinsic X-ray emission from wind-embedded shocks varies between 38% at periastron to 81% at apastron. The wind-wind collision zone remains adiabatic even around periastron passage. The X-ray emission of HD 167971 displays variations on the orbital timescale of the inner eclipsing binary. The existing data of this system do not allow us to probe variations on the timescale of the outer orbit. Shock modification due to the action of relativistic electrons does not seem to be efficiently operating in the HD 168112 system. In the existing observations, a significant part of the emission of HD 167971 must arise in the inner eclipsing binary. The origin of this emission is as yet unclear.","sentences":["The long-period O-star binary system HD 168112 and the triple O-star system HD 167971 are well-known sources of non-thermal radio emission that arises from a colliding wind interaction.","The wind-wind collisions in these systems should result in phase-dependent X-ray emissions.","The presence of a population of relativistic electrons in the wind interaction zone could affect the properties of the X-ray emission and make it deviate from the behaviour expected for adiabatic shocks.","We investigate the X-ray emission of these systems with the goals of quantifying the fraction of the X-ray flux arising from wind interactions and determining whether these emissions follow the predictions for adiabatic wind-wind collisions.","Six X-ray observations were collected with XMM-Newton.","Three observations were scheduled around the most recent periastron passage of HD 168112.","Spectra and light curves were analysed and compared with simple predictions of model calculations for X-ray emission from colliding wind systems.","The X-ray emission of HD 168112 varies as the inverse of the orbital separation, as expected for an adiabatic wind interaction zone.","The relative contribution of intrinsic X-ray emission from wind-embedded shocks varies between 38% at periastron to 81% at apastron.","The wind-wind collision zone remains adiabatic even around periastron passage.","The X-ray emission of HD 167971 displays variations on the orbital timescale of the inner eclipsing binary.","The existing data of this system do not allow us to probe variations on the timescale of the outer orbit.","Shock modification due to the action of relativistic electrons does not seem to be efficiently operating in the HD 168112 system.","In the existing observations, a significant part of the emission of HD 167971 must arise in the inner eclipsing binary.","The origin of this emission is as yet unclear."],"url":"http://arxiv.org/abs/2406.08991v1","category":"astro-ph.SR"}
{"created":"2024-06-13 10:26:14","title":"A Novel Quantum LSTM Network","abstract":"The rapid evolution of artificial intelligence has led to the widespread adoption of Long Short-Term Memory (LSTM) networks, known for their effectiveness in processing sequential data. However, LSTMs are constrained by inherent limitations such as the vanishing gradient problem and substantial computational demands. The advent of quantum computing presents a revolutionary approach to overcoming these obstacles. This paper introduces the Quantum LSTM (qLSTM) model, which integrates quantum computing principles with traditional LSTM networks to significantly enhance computational efficiency and model performance in sequence learning tasks. Quantum computing leverages qubits, which can exist in multiple states simultaneously through superposition and entangle these states to represent complex correlations without direct physical interaction, offering a profound advancement over classical binary computing. Our qLSTM model aims to address the limitations of traditional LSTMs, providing a robust framework for more efficient and effective sequential data processing.","sentences":["The rapid evolution of artificial intelligence has led to the widespread adoption of Long Short-Term Memory (LSTM) networks, known for their effectiveness in processing sequential data.","However, LSTMs are constrained by inherent limitations such as the vanishing gradient problem and substantial computational demands.","The advent of quantum computing presents a revolutionary approach to overcoming these obstacles.","This paper introduces the Quantum LSTM (qLSTM) model, which integrates quantum computing principles with traditional LSTM networks to significantly enhance computational efficiency and model performance in sequence learning tasks.","Quantum computing leverages qubits, which can exist in multiple states simultaneously through superposition and entangle these states to represent complex correlations without direct physical interaction, offering a profound advancement over classical binary computing.","Our qLSTM model aims to address the limitations of traditional LSTMs, providing a robust framework for more efficient and effective sequential data processing."],"url":"http://arxiv.org/abs/2406.08982v1","category":"quant-ph"}
{"created":"2024-06-13 10:18:36","title":"Multi-Agent Software Development through Cross-Team Collaboration","abstract":"The latest breakthroughs in Large Language Models (LLMs), eg., ChatDev, have catalyzed profound transformations, particularly through multi-agent collaboration for software development. LLM agents can collaborate in teams like humans, and follow the waterfall model to sequentially work on requirements analysis, development, review, testing, and other phases to perform autonomous software generation. However, for an agent team, each phase in a single development process yields only one possible outcome. This results in the completion of only one development chain, thereby losing the opportunity to explore multiple potential decision paths within the solution space. Consequently, this may lead to obtaining suboptimal results. To address this challenge, we introduce Cross-Team Collaboration (CTC), a scalable multi-team framework that enables orchestrated teams to jointly propose various decisions and communicate with their insights in a cross-team collaboration environment for superior content generation. Experimental results in software development reveal a notable increase in quality compared to state-of-the-art baselines, underscoring the efficacy of our framework. The significant improvements in story generation demonstrate the promising generalization ability of our framework across various domains. We anticipate that our work will guide LLM agents towards a cross-team paradigm and contribute to their significant growth in but not limited to software development. The code and data will be available at https://github.com/OpenBMB/ChatDev.","sentences":["The latest breakthroughs in Large Language Models (LLMs), eg., ChatDev, have catalyzed profound transformations, particularly through multi-agent collaboration for software development.","LLM agents can collaborate in teams like humans, and follow the waterfall model to sequentially work on requirements analysis, development, review, testing, and other phases to perform autonomous software generation.","However, for an agent team, each phase in a single development process yields only one possible outcome.","This results in the completion of only one development chain, thereby losing the opportunity to explore multiple potential decision paths within the solution space.","Consequently, this may lead to obtaining suboptimal results.","To address this challenge, we introduce Cross-Team Collaboration (CTC), a scalable multi-team framework that enables orchestrated teams to jointly propose various decisions and communicate with their insights in a cross-team collaboration environment for superior content generation.","Experimental results in software development reveal a notable increase in quality compared to state-of-the-art baselines, underscoring the efficacy of our framework.","The significant improvements in story generation demonstrate the promising generalization ability of our framework across various domains.","We anticipate that our work will guide LLM agents towards a cross-team paradigm and contribute to their significant growth in but not limited to software development.","The code and data will be available at https://github.com/OpenBMB/ChatDev."],"url":"http://arxiv.org/abs/2406.08979v1","category":"cs.CL"}
{"created":"2024-06-13 10:04:17","title":"XLand-100B: A Large-Scale Multi-Task Dataset for In-Context Reinforcement Learning","abstract":"Following the success of the in-context learning paradigm in large-scale language and computer vision models, the recently emerging field of in-context reinforcement learning is experiencing a rapid growth. However, its development has been held back by the lack of challenging benchmarks, as all the experiments have been carried out in simple environments and on small-scale datasets. We present \\textbf{XLand-100B}, a large-scale dataset for in-context reinforcement learning based on the XLand-MiniGrid environment, as a first step to alleviate this problem. It contains complete learning histories for nearly $30,000$ different tasks, covering $100$B transitions and $2.5$B episodes. It took $50,000$ GPU hours to collect the dataset, which is beyond the reach of most academic labs. Along with the dataset, we provide the utilities to reproduce or expand it even further. With this substantial effort, we aim to democratize research in the rapidly growing field of in-context reinforcement learning and provide a solid foundation for further scaling. The code is open-source and available under Apache 2.0 licence at https://github.com/dunno-lab/xland-minigrid-datasets.","sentences":["Following the success of the in-context learning paradigm in large-scale language and computer vision models, the recently emerging field of in-context reinforcement learning is experiencing a rapid growth.","However, its development has been held back by the lack of challenging benchmarks, as all the experiments have been carried out in simple environments and on small-scale datasets.","We present \\textbf{XLand-100B}, a large-scale dataset for in-context reinforcement learning based on the XLand-MiniGrid environment, as a first step to alleviate this problem.","It contains complete learning histories for nearly $30,000$ different tasks, covering $100$B transitions and $2.5$B episodes.","It took $50,000$ GPU hours to collect the dataset, which is beyond the reach of most academic labs.","Along with the dataset, we provide the utilities to reproduce or expand it even further.","With this substantial effort, we aim to democratize research in the rapidly growing field of in-context reinforcement learning and provide a solid foundation for further scaling.","The code is open-source and available under Apache 2.0 licence at https://github.com/dunno-lab/xland-minigrid-datasets."],"url":"http://arxiv.org/abs/2406.08973v1","category":"cs.LG"}
{"created":"2024-06-13 09:52:44","title":"Separation Power of Equivariant Neural Networks","abstract":"The separation power of a machine learning model refers to its capacity to distinguish distinct inputs, and it is often employed as a proxy for its expressivity. In this paper, we propose a theoretical framework to investigate the separation power of equivariant neural networks with point-wise activations. Using the proposed framework, we can derive an explicit description of inputs indistinguishable by a family of neural networks with given architecture, demonstrating that it remains unaffected by the choice of non-polynomial activation function employed. We are able to understand the role played by activation functions in separability. Indeed, we show that all non-polynomial activations, such as ReLU and sigmoid, are equivalent in terms of expressivity, and that they reach maximum discrimination capacity. We demonstrate how assessing the separation power of an equivariant neural network can be simplified to evaluating the separation power of minimal representations. We conclude by illustrating how these minimal components form a hierarchy in separation power.","sentences":["The separation power of a machine learning model refers to its capacity to distinguish distinct inputs, and it is often employed as a proxy for its expressivity.","In this paper, we propose a theoretical framework to investigate the separation power of equivariant neural networks with point-wise activations.","Using the proposed framework, we can derive an explicit description of inputs indistinguishable by a family of neural networks with given architecture, demonstrating that it remains unaffected by the choice of non-polynomial activation function employed.","We are able to understand the role played by activation functions in separability.","Indeed, we show that all non-polynomial activations, such as ReLU and sigmoid, are equivalent in terms of expressivity, and that they reach maximum discrimination capacity.","We demonstrate how assessing the separation power of an equivariant neural network can be simplified to evaluating the separation power of minimal representations.","We conclude by illustrating how these minimal components form a hierarchy in separation power."],"url":"http://arxiv.org/abs/2406.08966v1","category":"cs.LG"}
{"created":"2024-06-13 09:44:04","title":"Beyond Recommendations: From Backward to Forward AI Support of Pilots' Decision-Making Process","abstract":"AI is anticipated to enhance human decision-making in high-stakes domains like aviation, but adoption is often hindered by challenges such as inappropriate reliance and poor alignment with users' decision-making. Recent research suggests that a core underlying issue is the recommendation-centric design of many AI systems, i.e., they give end-to-end recommendations and ignore the rest of the decision-making process. Alternative support paradigms are rare, and it remains unclear how the few that do exist compare to recommendation-centric support. In this work, we aimed to empirically compare recommendation-centric support to an alternative paradigm, continuous support, in the context of diversions in aviation. We conducted a mixed-methods study with 32 professional pilots in a realistic setting. To ensure the quality of our study scenarios, we conducted a focus group with four additional pilots prior to the study. We found that continuous support can support pilots' decision-making in a forward direction, allowing them to think more beyond the limits of the system and make faster decisions when combined with recommendations, though the forward support can be disrupted. Participants' statements further suggest a shift in design goal away from providing recommendations, to supporting quick information gathering. Our results show ways to design more helpful and effective AI decision support that goes beyond end-to-end recommendations.","sentences":["AI is anticipated to enhance human decision-making in high-stakes domains like aviation, but adoption is often hindered by challenges such as inappropriate reliance and poor alignment with users' decision-making.","Recent research suggests that a core underlying issue is the recommendation-centric design of many AI systems, i.e., they give end-to-end recommendations and ignore the rest of the decision-making process.","Alternative support paradigms are rare, and it remains unclear how the few that do exist compare to recommendation-centric support.","In this work, we aimed to empirically compare recommendation-centric support to an alternative paradigm, continuous support, in the context of diversions in aviation.","We conducted a mixed-methods study with 32 professional pilots in a realistic setting.","To ensure the quality of our study scenarios, we conducted a focus group with four additional pilots prior to the study.","We found that continuous support can support pilots' decision-making in a forward direction, allowing them to think more beyond the limits of the system and make faster decisions when combined with recommendations, though the forward support can be disrupted.","Participants' statements further suggest a shift in design goal away from providing recommendations, to supporting quick information gathering.","Our results show ways to design more helpful and effective AI decision support that goes beyond end-to-end recommendations."],"url":"http://arxiv.org/abs/2406.08959v1","category":"cs.HC"}
{"created":"2024-06-13 09:36:13","title":"Tool Wear Prediction in CNC Turning Operations using Ultrasonic Microphone Arrays and CNNs","abstract":"This paper introduces a novel method for predicting tool wear in CNC turning operations, combining ultrasonic microphone arrays and convolutional neural networks (CNNs). High-frequency acoustic emissions between 0 kHz and 60 kHz are enhanced using beamforming techniques to improve the signal- to-noise ratio. The processed acoustic data is then analyzed by a CNN, which predicts the Remaining Useful Life (RUL) of cutting tools. Trained on data from 350 workpieces machined with a single carbide insert, the model can accurately predict the RUL of the carbide insert. Our results demonstrate the potential gained by integrating advanced ultrasonic sensors with deep learning for accurate predictive maintenance tasks in CNC machining.","sentences":["This paper introduces a novel method for predicting tool wear in CNC turning operations, combining ultrasonic microphone arrays and convolutional neural networks (CNNs).","High-frequency acoustic emissions between 0 kHz and 60 kHz are enhanced using beamforming techniques to improve the signal- to-noise ratio.","The processed acoustic data is then analyzed by a CNN, which predicts the Remaining Useful Life (RUL) of cutting tools.","Trained on data from 350 workpieces machined with a single carbide insert, the model can accurately predict the RUL of the carbide insert.","Our results demonstrate the potential gained by integrating advanced ultrasonic sensors with deep learning for accurate predictive maintenance tasks in CNC machining."],"url":"http://arxiv.org/abs/2406.08957v1","category":"eess.AS"}
{"created":"2024-06-13 09:17:10","title":"Human-Robot Interface for Teleoperated Robotized Planetary Sample Collection and Assembly","abstract":"As human space exploration evolves toward longer voyages farther from our home planet, in-situ resource utilization (ISRU) becomes increasingly important. Haptic teleoperations are one of the technologies by which such activities can be carried out remotely by humans, whose expertise is still necessary for complex activities. In order to perform precision tasks with effectiveness, the operator must experience ease of use and accuracy. The same features are demanded to reduce the complexity of the training procedures and the associated learning time for operators without a specific background in robotic teleoperations. Haptic teleoperation systems, that allow for a natural feeling of forces, need to cope with the trade-off between accurate movements and workspace extension. Clearly, both of them are required for typical ISRU tasks. In this work, we develop a new concept of operations and suitable human-robot interfaces to achieve sample collection and assembly with ease of use and accuracy. In the proposed operational concept, the teleoperation space is extended by executing automated trajectories, offline planned at the control station. In three different experimental scenarios, we validate the end-to-end system involving the control station and the robotic asset, by assessing the contribution of haptics to mission success, the system robustness to consistent delays, and the ease of training new operators.","sentences":["As human space exploration evolves toward longer voyages farther from our home planet, in-situ resource utilization (ISRU) becomes increasingly important.","Haptic teleoperations are one of the technologies by which such activities can be carried out remotely by humans, whose expertise is still necessary for complex activities.","In order to perform precision tasks with effectiveness, the operator must experience ease of use and accuracy.","The same features are demanded to reduce the complexity of the training procedures and the associated learning time for operators without a specific background in robotic teleoperations.","Haptic teleoperation systems, that allow for a natural feeling of forces, need to cope with the trade-off between accurate movements and workspace extension.","Clearly, both of them are required for typical ISRU tasks.","In this work, we develop a new concept of operations and suitable human-robot interfaces to achieve sample collection and assembly with ease of use and accuracy.","In the proposed operational concept, the teleoperation space is extended by executing automated trajectories, offline planned at the control station.","In three different experimental scenarios, we validate the end-to-end system involving the control station and the robotic asset, by assessing the contribution of haptics to mission success, the system robustness to consistent delays, and the ease of training new operators."],"url":"http://arxiv.org/abs/2406.08946v1","category":"cs.RO"}
{"created":"2024-06-13 09:00:14","title":"Exploring Multilingual Unseen Speaker Emotion Recognition: Leveraging Co-Attention Cues in Multitask Learning","abstract":"Advent of modern deep learning techniques has given rise to advancements in the field of Speech Emotion Recognition (SER). However, most systems prevalent in the field fail to generalize to speakers not seen during training. This study focuses on handling challenges of multilingual SER, specifically on unseen speakers. We introduce CAMuLeNet, a novel architecture leveraging co-attention based fusion and multitask learning to address this problem. Additionally, we benchmark pretrained encoders of Whisper, HuBERT, Wav2Vec2.0, and WavLM using 10-fold leave-speaker-out cross-validation on five existing multilingual benchmark datasets: IEMOCAP, RAVDESS, CREMA-D, EmoDB and CaFE and, release a novel dataset for SER on the Hindi language (BhavVani). CAMuLeNet shows an average improvement of approximately 8% over all benchmarks on unseen speakers determined by our cross-validation strategy.","sentences":["Advent of modern deep learning techniques has given rise to advancements in the field of Speech Emotion Recognition (SER).","However, most systems prevalent in the field fail to generalize to speakers not seen during training.","This study focuses on handling challenges of multilingual SER, specifically on unseen speakers.","We introduce CAMuLeNet, a novel architecture leveraging co-attention based fusion and multitask learning to address this problem.","Additionally, we benchmark pretrained encoders of Whisper, HuBERT, Wav2Vec2.0, and WavLM using 10-fold leave-speaker-out cross-validation on five existing multilingual benchmark datasets: IEMOCAP, RAVDESS, CREMA-D, EmoDB and CaFE and, release a novel dataset for SER on the Hindi language (BhavVani).","CAMuLeNet shows an average improvement of approximately 8% over all benchmarks on unseen speakers determined by our cross-validation strategy."],"url":"http://arxiv.org/abs/2406.08931v1","category":"cs.CL"}
{"created":"2024-06-13 08:58:59","title":"Efficient Multi-View Fusion and Flexible Adaptation to View Missing in Cardiovascular System Signals","abstract":"The progression of deep learning and the widespread adoption of sensors have facilitated automatic multi-view fusion (MVF) about the cardiovascular system (CVS) signals. However, prevalent MVF model architecture often amalgamates CVS signals from the same temporal step but different views into a unified representation, disregarding the asynchronous nature of cardiovascular events and the inherent heterogeneity across views, leading to catastrophic view confusion. Efficient training strategies specifically tailored for MVF models to attain comprehensive representations need simultaneous consideration. Crucially, real-world data frequently arrives with incomplete views, an aspect rarely noticed by researchers. Thus, the View-Centric Transformer (VCT) and Multitask Masked Autoencoder (M2AE) are specifically designed to emphasize the centrality of each view and harness unlabeled data to achieve superior fused representations. Additionally, we systematically define the missing-view problem for the first time and introduce prompt techniques to aid pretrained MVF models in flexibly adapting to various missing-view scenarios. Rigorous experiments involving atrial fibrillation detection, blood pressure estimation, and sleep staging-typical health monitoring tasks-demonstrate the remarkable advantage of our method in MVF compared to prevailing methodologies. Notably, the prompt technique requires finetuning less than 3% of the entire model's data, substantially fortifying the model's resilience to view missing while circumventing the need for complete retraining. The results demonstrate the effectiveness of our approaches, highlighting their potential for practical applications in cardiovascular health monitoring. Codes and models are released at URL.","sentences":["The progression of deep learning and the widespread adoption of sensors have facilitated automatic multi-view fusion (MVF) about the cardiovascular system (CVS) signals.","However, prevalent MVF model architecture often amalgamates CVS signals from the same temporal step but different views into a unified representation, disregarding the asynchronous nature of cardiovascular events and the inherent heterogeneity across views, leading to catastrophic view confusion.","Efficient training strategies specifically tailored for MVF models to attain comprehensive representations need simultaneous consideration.","Crucially, real-world data frequently arrives with incomplete views, an aspect rarely noticed by researchers.","Thus, the View-Centric Transformer (VCT) and Multitask Masked Autoencoder (M2AE) are specifically designed to emphasize the centrality of each view and harness unlabeled data to achieve superior fused representations.","Additionally, we systematically define the missing-view problem for the first time and introduce prompt techniques to aid pretrained MVF models in flexibly adapting to various missing-view scenarios.","Rigorous experiments involving atrial fibrillation detection, blood pressure estimation, and sleep staging-typical health monitoring tasks-demonstrate the remarkable advantage of our method in MVF compared to prevailing methodologies.","Notably, the prompt technique requires finetuning less than 3% of the entire model's data, substantially fortifying the model's resilience to view missing while circumventing the need for complete retraining.","The results demonstrate the effectiveness of our approaches, highlighting their potential for practical applications in cardiovascular health monitoring.","Codes and models are released at URL."],"url":"http://arxiv.org/abs/2406.08930v1","category":"cs.LG"}
{"created":"2024-06-13 08:58:45","title":"Step-by-Step Diffusion: An Elementary Tutorial","abstract":"We present an accessible first course on diffusion models and flow matching for machine learning, aimed at a technical audience with no diffusion experience. We try to simplify the mathematical details as much as possible (sometimes heuristically), while retaining enough precision to derive correct algorithms.","sentences":["We present an accessible first course on diffusion models and flow matching for machine learning, aimed at a technical audience with no diffusion experience.","We try to simplify the mathematical details as much as possible (sometimes heuristically), while retaining enough precision to derive correct algorithms."],"url":"http://arxiv.org/abs/2406.08929v1","category":"cs.LG"}
{"created":"2024-06-13 08:44:12","title":"Learning Images Across Scales Using Adversarial Training","abstract":"The real world exhibits rich structure and detail across many scales of observation. It is difficult, however, to capture and represent a broad spectrum of scales using ordinary images. We devise a novel paradigm for learning a representation that captures an orders-of-magnitude variety of scales from an unstructured collection of ordinary images. We treat this collection as a distribution of scale-space slices to be learned using adversarial training, and additionally enforce coherency across slices. Our approach relies on a multiscale generator with carefully injected procedural frequency content, which allows to interactively explore the emerging continuous scale space. Training across vastly different scales poses challenges regarding stability, which we tackle using a supervision scheme that involves careful sampling of scales. We show that our generator can be used as a multiscale generative model, and for reconstructions of scale spaces from unstructured patches. Significantly outperforming the state of the art, we demonstrate zoom-in factors of up to 256x at high quality and scale consistency.","sentences":["The real world exhibits rich structure and detail across many scales of observation.","It is difficult, however, to capture and represent a broad spectrum of scales using ordinary images.","We devise a novel paradigm for learning a representation that captures an orders-of-magnitude variety of scales from an unstructured collection of ordinary images.","We treat this collection as a distribution of scale-space slices to be learned using adversarial training, and additionally enforce coherency across slices.","Our approach relies on a multiscale generator with carefully injected procedural frequency content, which allows to interactively explore the emerging continuous scale space.","Training across vastly different scales poses challenges regarding stability, which we tackle using a supervision scheme that involves careful sampling of scales.","We show that our generator can be used as a multiscale generative model, and for reconstructions of scale spaces from unstructured patches.","Significantly outperforming the state of the art, we demonstrate zoom-in factors of up to 256x at high quality and scale consistency."],"url":"http://arxiv.org/abs/2406.08924v1","category":"cs.GR"}
{"created":"2024-06-13 08:37:01","title":"Navigating the Shadows: Unveiling Effective Disturbances for Modern AI Content Detectors","abstract":"With the launch of ChatGPT, large language models (LLMs) have attracted global attention. In the realm of article writing, LLMs have witnessed extensive utilization, giving rise to concerns related to intellectual property protection, personal privacy, and academic integrity. In response, AI-text detection has emerged to distinguish between human and machine-generated content. However, recent research indicates that these detection systems often lack robustness and struggle to effectively differentiate perturbed texts. Currently, there is a lack of systematic evaluations regarding detection performance in real-world applications, and a comprehensive examination of perturbation techniques and detector robustness is also absent. To bridge this gap, our work simulates real-world scenarios in both informal and professional writing, exploring the out-of-the-box performance of current detectors. Additionally, we have constructed 12 black-box text perturbation methods to assess the robustness of current detection models across various perturbation granularities. Furthermore, through adversarial learning experiments, we investigate the impact of perturbation data augmentation on the robustness of AI-text detectors. We have released our code and data at https://github.com/zhouying20/ai-text-detector-evaluation.","sentences":["With the launch of ChatGPT, large language models (LLMs) have attracted global attention.","In the realm of article writing, LLMs have witnessed extensive utilization, giving rise to concerns related to intellectual property protection, personal privacy, and academic integrity.","In response, AI-text detection has emerged to distinguish between human and machine-generated content.","However, recent research indicates that these detection systems often lack robustness and struggle to effectively differentiate perturbed texts.","Currently, there is a lack of systematic evaluations regarding detection performance in real-world applications, and a comprehensive examination of perturbation techniques and detector robustness is also absent.","To bridge this gap, our work simulates real-world scenarios in both informal and professional writing, exploring the out-of-the-box performance of current detectors.","Additionally, we have constructed 12 black-box text perturbation methods to assess the robustness of current detection models across various perturbation granularities.","Furthermore, through adversarial learning experiments, we investigate the impact of perturbation data augmentation on the robustness of AI-text detectors.","We have released our code and data at https://github.com/zhouying20/ai-text-detector-evaluation."],"url":"http://arxiv.org/abs/2406.08922v1","category":"cs.CL"}
{"created":"2024-06-13 08:34:12","title":"AV-GS: Learning Material and Geometry Aware Priors for Novel View Acoustic Synthesis","abstract":"Novel view acoustic synthesis (NVAS) aims to render binaural audio at any target viewpoint, given a mono audio emitted by a sound source at a 3D scene. Existing methods have proposed NeRF-based implicit models to exploit visual cues as a condition for synthesizing binaural audio. However, in addition to low efficiency originating from heavy NeRF rendering, these methods all have a limited ability of characterizing the entire scene environment such as room geometry, material properties, and the spatial relation between the listener and sound source. To address these issues, we propose a novel Audio-Visual Gaussian Splatting (AV-GS) model. To obtain a material-aware and geometry-aware condition for audio synthesis, we learn an explicit point-based scene representation with an audio-guidance parameter on locally initialized Gaussian points, taking into account the space relation from the listener and sound source. To make the visual scene model audio adaptive, we propose a point densification and pruning strategy to optimally distribute the Gaussian points, with the per-point contribution in sound propagation (e.g., more points needed for texture-less wall surfaces as they affect sound path diversion). Extensive experiments validate the superiority of our AV-GS over existing alternatives on the real-world RWAS and simulation-based SoundSpaces datasets.","sentences":["Novel view acoustic synthesis (NVAS) aims to render binaural audio at any target viewpoint, given a mono audio emitted by a sound source at a 3D scene.","Existing methods have proposed NeRF-based implicit models to exploit visual cues as a condition for synthesizing binaural audio.","However, in addition to low efficiency originating from heavy NeRF rendering, these methods all have a limited ability of characterizing the entire scene environment such as room geometry, material properties, and the spatial relation between the listener and sound source.","To address these issues, we propose a novel Audio-Visual Gaussian Splatting (AV-GS) model.","To obtain a material-aware and geometry-aware condition for audio synthesis, we learn an explicit point-based scene representation with an audio-guidance parameter on locally initialized Gaussian points, taking into account the space relation from the listener and sound source.","To make the visual scene model audio adaptive, we propose a point densification and pruning strategy to optimally distribute the Gaussian points, with the per-point contribution in sound propagation (e.g., more points needed for texture-less wall surfaces as they affect sound path diversion).","Extensive experiments validate the superiority of our AV-GS over existing alternatives on the real-world RWAS and simulation-based SoundSpaces datasets."],"url":"http://arxiv.org/abs/2406.08920v1","category":"cs.SD"}
{"created":"2024-06-13 08:30:29","title":"Beyond the Calibration Point: Mechanism Comparison in Differential Privacy","abstract":"In differentially private (DP) machine learning, the privacy guarantees of DP mechanisms are often reported and compared on the basis of a single $(\\varepsilon, \\delta)$-pair. This practice overlooks that DP guarantees can vary substantially \\emph{even between mechanisms sharing a given $(\\varepsilon, \\delta)$}, and potentially introduces privacy vulnerabilities which can remain undetected. This motivates the need for robust, rigorous methods for comparing DP guarantees in such cases. Here, we introduce the $\\Delta$-divergence between mechanisms which quantifies the worst-case excess privacy vulnerability of choosing one mechanism over another in terms of $(\\varepsilon, \\delta)$, $f$-DP and in terms of a newly presented Bayesian interpretation. Moreover, as a generalisation of the Blackwell theorem, it is endowed with strong decision-theoretic foundations. Through application examples, we show that our techniques can facilitate informed decision-making and reveal gaps in the current understanding of privacy risks, as current practices in DP-SGD often result in choosing mechanisms with high excess privacy vulnerabilities.","sentences":["In differentially private (DP) machine learning, the privacy guarantees of DP mechanisms are often reported and compared on the basis of a single $(\\varepsilon, \\delta)$-pair.","This practice overlooks that DP guarantees can vary substantially \\emph{even between mechanisms sharing a given $(\\varepsilon, \\delta)$}, and potentially introduces privacy vulnerabilities which can remain undetected.","This motivates the need for robust, rigorous methods for comparing DP guarantees in such cases.","Here, we introduce the $\\Delta$-divergence between mechanisms which quantifies the worst-case excess privacy vulnerability of choosing one mechanism over another in terms of $(\\varepsilon, \\delta)$, $f$-DP and in terms of a newly presented Bayesian interpretation.","Moreover, as a generalisation of the Blackwell theorem, it is endowed with strong decision-theoretic foundations.","Through application examples, we show that our techniques can facilitate informed decision-making and reveal gaps in the current understanding of privacy risks, as current practices in DP-SGD often result in choosing mechanisms with high excess privacy vulnerabilities."],"url":"http://arxiv.org/abs/2406.08918v1","category":"cs.CR"}
{"created":"2024-06-13 08:20:58","title":"Transcription-Free Fine-Tuning of Speech Separation Models for Noisy and Reverberant Multi-Speaker Automatic Speech Recognition","abstract":"One solution to automatic speech recognition (ASR) of overlapping speakers is to separate speech and then perform ASR on the separated signals. Commonly, the separator produces artefacts which often degrade ASR performance. Addressing this issue typically requires reference transcriptions to jointly train the separation and ASR networks. This is often not viable for training on real-world in-domain audio where reference transcript information is not always available. This paper proposes a transcription-free method for joint training using only audio signals. The proposed method uses embedding differences of pre-trained ASR encoders as a loss with a proposed modification to permutation invariant training (PIT) called guided PIT (GPIT). The method achieves a 6.4% improvement in word error rate (WER) measures over a signal-level loss and also shows enhancement improvements in perceptual measures such as short-time objective intelligibility (STOI).","sentences":["One solution to automatic speech recognition (ASR) of overlapping speakers is to separate speech and then perform ASR on the separated signals.","Commonly, the separator produces artefacts which often degrade ASR performance.","Addressing this issue typically requires reference transcriptions to jointly train the separation and ASR networks.","This is often not viable for training on real-world in-domain audio where reference transcript information is not always available.","This paper proposes a transcription-free method for joint training using only audio signals.","The proposed method uses embedding differences of pre-trained ASR encoders as a loss with a proposed modification to permutation invariant training (PIT) called guided PIT (GPIT).","The method achieves a 6.4% improvement in word error rate (WER) measures over a signal-level loss and also shows enhancement improvements in perceptual measures such as short-time objective intelligibility (STOI)."],"url":"http://arxiv.org/abs/2406.08914v1","category":"cs.SD"}
{"created":"2024-06-13 08:16:52","title":"An Initial Investigation of Language Adaptation for TTS Systems under Low-resource Scenarios","abstract":"Self-supervised learning (SSL) representations from massively multilingual models offer a promising solution for low-resource language speech tasks. Despite advancements, language adaptation in TTS systems remains an open problem. This paper explores the language adaptation capability of ZMM-TTS, a recent SSL-based multilingual TTS system proposed in our previous work. We conducted experiments on 12 languages using limited data with various fine-tuning configurations. We demonstrate that the similarity in phonetics between the pre-training and target languages, as well as the language category, affects the target language's adaptation performance. Additionally, we find that the fine-tuning dataset size and number of speakers influence adaptability. Surprisingly, we also observed that using paired data for fine-tuning is not always optimal compared to audio-only data. Beyond speech intelligibility, our analysis covers speaker similarity, language identification, and predicted MOS.","sentences":["Self-supervised learning (SSL) representations from massively multilingual models offer a promising solution for low-resource language speech tasks.","Despite advancements, language adaptation in TTS systems remains an open problem.","This paper explores the language adaptation capability of ZMM-TTS, a recent SSL-based multilingual TTS system proposed in our previous work.","We conducted experiments on 12 languages using limited data with various fine-tuning configurations.","We demonstrate that the similarity in phonetics between the pre-training and target languages, as well as the language category, affects the target language's adaptation performance.","Additionally, we find that the fine-tuning dataset size and number of speakers influence adaptability.","Surprisingly, we also observed that using paired data for fine-tuning is not always optimal compared to audio-only data.","Beyond speech intelligibility, our analysis covers speaker similarity, language identification, and predicted MOS."],"url":"http://arxiv.org/abs/2406.08911v1","category":"cs.CL"}
{"created":"2024-06-13 08:06:57","title":"Dual Attribute-Spatial Relation Alignment for 3D Visual Grounding","abstract":"3D visual grounding is an emerging research area dedicated to making connections between the 3D physical world and natural language, which is crucial for achieving embodied intelligence. In this paper, we propose DASANet, a Dual Attribute-Spatial relation Alignment Network that separately models and aligns object attributes and spatial relation features between language and 3D vision modalities. We decompose both the language and 3D point cloud input into two separate parts and design a dual-branch attention module to separately model the decomposed inputs while preserving global context in attribute-spatial feature fusion by cross attentions. Our DASANet achieves the highest grounding accuracy 65.1% on the Nr3D dataset, 1.3% higher than the best competitor. Besides, the visualization of the two branches proves that our method is efficient and highly interpretable.","sentences":["3D visual grounding is an emerging research area dedicated to making connections between the 3D physical world and natural language, which is crucial for achieving embodied intelligence.","In this paper, we propose DASANet, a Dual Attribute-Spatial relation Alignment Network that separately models and aligns object attributes and spatial relation features between language and 3D vision modalities.","We decompose both the language and 3D point cloud input into two separate parts and design a dual-branch attention module to separately model the decomposed inputs while preserving global context in attribute-spatial feature fusion by cross attentions.","Our DASANet achieves the highest grounding accuracy 65.1% on the Nr3D dataset, 1.3% higher than the best competitor.","Besides, the visualization of the two branches proves that our method is efficient and highly interpretable."],"url":"http://arxiv.org/abs/2406.08907v1","category":"cs.CV"}
{"created":"2024-06-13 07:57:14","title":"Mutual dipolar drag in a bilayer Fermi gas","abstract":"We consider two-dimensional spin-polarized dipolar Fermi gases confined in a double-layer system and calculate the momentum transfer between the layers as a function of temperature to investigate the transport properties of the system. We use the Hubbard approximation to describe the correlation effects and the screening between the dipoles within a single layer. The effective interlayer interaction between the dipoles across the layers is obtained by the random-phase approximation. We calculate the interaction strength and the layer separation distance dependence of the drag rate, and we show that there is a critical distance below which the system is unstable. In addition, we calculate the typical behavior of the collective modes related to the density fluctuations.","sentences":["We consider two-dimensional spin-polarized dipolar Fermi gases confined in a double-layer system and calculate the momentum transfer between the layers as a function of temperature to investigate the transport properties of the system.","We use the Hubbard approximation to describe the correlation effects and the screening between the dipoles within a single layer.","The effective interlayer interaction between the dipoles across the layers is obtained by the random-phase approximation.","We calculate the interaction strength and the layer separation distance dependence of the drag rate, and we show that there is a critical distance below which the system is unstable.","In addition, we calculate the typical behavior of the collective modes related to the density fluctuations."],"url":"http://arxiv.org/abs/2406.08902v1","category":"cond-mat.quant-gas"}
{"created":"2024-06-13 07:51:08","title":"Computer Vision Approaches for Automated Bee Counting Application","abstract":"Many application from the bee colony health state monitoring could be efficiently solved using a computer vision techniques. One of such challenges is an efficient way for counting the number of incoming and outcoming bees, which could be used to further analyse many trends, such as the bee colony health state, blooming periods, or for investigating the effects of agricultural spraying. In this paper, we compare three methods for the automated bee counting over two own datasets. The best performing method is based on the ResNet-50 convolutional neural network classifier, which achieved accuracy of 87% over the BUT1 dataset and the accuracy of 93% over the BUT2 dataset.","sentences":["Many application from the bee colony health state monitoring could be efficiently solved using a computer vision techniques.","One of such challenges is an efficient way for counting the number of incoming and outcoming bees, which could be used to further analyse many trends, such as the bee colony health state, blooming periods, or for investigating the effects of agricultural spraying.","In this paper, we compare three methods for the automated bee counting over two own datasets.","The best performing method is based on the ResNet-50 convolutional neural network classifier, which achieved accuracy of 87% over the BUT1 dataset and the accuracy of 93% over the BUT2 dataset."],"url":"http://arxiv.org/abs/2406.08898v1","category":"cs.CV"}
{"created":"2024-06-13 07:50:15","title":"Blind Super-Resolution via Meta-learning and Markov Chain Monte Carlo Simulation","abstract":"Learning-based approaches have witnessed great successes in blind single image super-resolution (SISR) tasks, however, handcrafted kernel priors and learning based kernel priors are typically required. In this paper, we propose a Meta-learning and Markov Chain Monte Carlo (MCMC) based SISR approach to learn kernel priors from organized randomness. In concrete, a lightweight network is adopted as kernel generator, and is optimized via learning from the MCMC simulation on random Gaussian distributions. This procedure provides an approximation for the rational blur kernel, and introduces a network-level Langevin dynamics into SISR optimization processes, which contributes to preventing bad local optimal solutions for kernel estimation. Meanwhile, a meta-learning-based alternating optimization procedure is proposed to optimize the kernel generator and image restorer, respectively. In contrast to the conventional alternating minimization strategy, a meta-learning-based framework is applied to learn an adaptive optimization strategy, which is less-greedy and results in better convergence performance. These two procedures are iteratively processed in a plug-and-play fashion, for the first time, realizing a learning-based but plug-and-play blind SISR solution in unsupervised inference. Extensive simulations demonstrate the superior performance and generalization ability of the proposed approach when comparing with state-of-the-arts on synthesis and real-world datasets. The code is available at https://github.com/XYLGroup/MLMC.","sentences":["Learning-based approaches have witnessed great successes in blind single image super-resolution (SISR) tasks, however, handcrafted kernel priors and learning based kernel priors are typically required.","In this paper, we propose a Meta-learning and Markov Chain Monte Carlo (MCMC) based SISR approach to learn kernel priors from organized randomness.","In concrete, a lightweight network is adopted as kernel generator, and is optimized via learning from the MCMC simulation on random Gaussian distributions.","This procedure provides an approximation for the rational blur kernel, and introduces a network-level Langevin dynamics into SISR optimization processes, which contributes to preventing bad local optimal solutions for kernel estimation.","Meanwhile, a meta-learning-based alternating optimization procedure is proposed to optimize the kernel generator and image restorer, respectively.","In contrast to the conventional alternating minimization strategy, a meta-learning-based framework is applied to learn an adaptive optimization strategy, which is less-greedy and results in better convergence performance.","These two procedures are iteratively processed in a plug-and-play fashion, for the first time, realizing a learning-based but plug-and-play blind SISR solution in unsupervised inference.","Extensive simulations demonstrate the superior performance and generalization ability of the proposed approach when comparing with state-of-the-arts on synthesis and real-world datasets.","The code is available at https://github.com/XYLGroup/MLMC."],"url":"http://arxiv.org/abs/2406.08896v1","category":"eess.IV"}
{"created":"2024-06-13 07:31:29","title":"CIMRL: Combining IMitiation and Reinforcement Learning for Safe Autonomous Driving","abstract":"Modern approaches to autonomous driving rely heavily on learned components trained with large amounts of human driving data via imitation learning. However, these methods require large amounts of expensive data collection and even then face challenges with safely handling long-tail scenarios and compounding errors over time. At the same time, pure Reinforcement Learning (RL) methods can fail to learn performant policies in sparse, constrained, and challenging-to-define reward settings like driving. Both of these challenges make deploying purely cloned policies in safety critical applications like autonomous vehicles challenging. In this paper we propose Combining IMitation and Reinforcement Learning (CIMRL) approach -- a framework that enables training driving policies in simulation through leveraging imitative motion priors and safety constraints. CIMRL does not require extensive reward specification and improves on the closed loop behavior of pure cloning methods. By combining RL and imitation, we demonstrate that our method achieves state-of-the-art results in closed loop simulation driving benchmarks.","sentences":["Modern approaches to autonomous driving rely heavily on learned components trained with large amounts of human driving data via imitation learning.","However, these methods require large amounts of expensive data collection and even then face challenges with safely handling long-tail scenarios and compounding errors over time.","At the same time, pure Reinforcement Learning (RL) methods can fail to learn performant policies in sparse, constrained, and challenging-to-define reward settings like driving.","Both of these challenges make deploying purely cloned policies in safety critical applications like autonomous vehicles challenging.","In this paper we propose Combining IMitation and Reinforcement Learning (CIMRL) approach -- a framework that enables training driving policies in simulation through leveraging imitative motion priors and safety constraints.","CIMRL does not require extensive reward specification and improves on the closed loop behavior of pure cloning methods.","By combining RL and imitation, we demonstrate that our method achieves state-of-the-art results in closed loop simulation driving benchmarks."],"url":"http://arxiv.org/abs/2406.08878v1","category":"cs.LG"}
{"created":"2024-06-13 07:09:41","title":"Zoom and Shift are All You Need","abstract":"Feature alignment serves as the primary mechanism for fusing multimodal data. We put forth a feature alignment approach that achieves full integration of multimodal information. This is accomplished via an alternating process of shifting and expanding feature representations across modalities to obtain a consistent unified representation in a joint feature space. The proposed technique can reliably capture high-level interplay between features originating from distinct modalities. Consequently, substantial gains in multimodal learning performance are attained. Additionally, we demonstrate the superiority of our approach over other prevalent multimodal fusion schemes on a range of tasks. Extensive experimental evaluation conducted on multimodal datasets comprising time series, image, and text demonstrates that our method achieves state-of-the-art results.","sentences":["Feature alignment serves as the primary mechanism for fusing multimodal data.","We put forth a feature alignment approach that achieves full integration of multimodal information.","This is accomplished via an alternating process of shifting and expanding feature representations across modalities to obtain a consistent unified representation in a joint feature space.","The proposed technique can reliably capture high-level interplay between features originating from distinct modalities.","Consequently, substantial gains in multimodal learning performance are attained.","Additionally, we demonstrate the superiority of our approach over other prevalent multimodal fusion schemes on a range of tasks.","Extensive experimental evaluation conducted on multimodal datasets comprising time series, image, and text demonstrates that our method achieves state-of-the-art results."],"url":"http://arxiv.org/abs/2406.08866v1","category":"cs.CV"}
{"created":"2024-06-13 07:04:22","title":"Research on Early Warning Model of Cardiovascular Disease Based on Computer Deep Learning","abstract":"This project intends to study a cardiovascular disease risk early warning model based on one-dimensional convolutional neural networks. First, the missing values of 13 physiological and symptom indicators such as patient age, blood glucose, cholesterol, and chest pain were filled and Z-score was standardized. The convolutional neural network is converted into a 2D matrix, the convolution function of 1,3, and 5 is used for the first-order convolution operation, and the Max Pooling algorithm is adopted for dimension reduction. Set the learning rate and output rate. It is optimized by the Adam algorithm. The result of classification is output by a soft classifier. This study was conducted based on Statlog in the UCI database and heart disease database respectively. The empirical data indicate that the forecasting precision of this technique has been enhanced by 11.2%, relative to conventional approaches, while there is a significant improvement in the logarithmic curve fitting. The efficacy and applicability of the novel approach are corroborated through the examination employing a one-dimensional convolutional neural network.","sentences":["This project intends to study a cardiovascular disease risk early warning model based on one-dimensional convolutional neural networks.","First, the missing values of 13 physiological and symptom indicators such as patient age, blood glucose, cholesterol, and chest pain were filled and Z-score was standardized.","The convolutional neural network is converted into a 2D matrix, the convolution function of 1,3, and 5 is used for the first-order convolution operation, and the Max Pooling algorithm is adopted for dimension reduction.","Set the learning rate and output rate.","It is optimized by the Adam algorithm.","The result of classification is output by a soft classifier.","This study was conducted based on Statlog in the UCI database and heart disease database respectively.","The empirical data indicate that the forecasting precision of this technique has been enhanced by 11.2%, relative to conventional approaches, while there is a significant improvement in the logarithmic curve fitting.","The efficacy and applicability of the novel approach are corroborated through the examination employing a one-dimensional convolutional neural network."],"url":"http://arxiv.org/abs/2406.08864v1","category":"cs.LG"}
{"created":"2024-06-13 06:56:49","title":"Self-supervised Graph Neural Network for Mechanical CAD Retrieval","abstract":"CAD (Computer-Aided Design) plays a crucial role in mechanical industry, where large numbers of similar-shaped CAD parts are often created. Efficiently reusing these parts is key to reducing design and production costs for enterprises. Retrieval systems are vital for achieving CAD reuse, but the complex shapes of CAD models are difficult to accurately describe using text or keywords, making traditional retrieval methods ineffective. While existing representation learning approaches have been developed for CAD, manually labeling similar samples in these methods is expensive. Additionally, CAD models' unique parameterized data structure presents challenges for applying existing 3D shape representation learning techniques directly. In this work, we propose GC-CAD, a self-supervised contrastive graph neural network-based method for mechanical CAD retrieval that directly models parameterized CAD raw files. GC-CAD consists of two key modules: structure-aware representation learning and contrastive graph learning framework. The method leverages graph neural networks to extract both geometric and topological information from CAD models, generating feature representations. We then introduce a simple yet effective contrastive graph learning framework approach, enabling the model to train without manual labels and generate retrieval-ready representations. Experimental results on four datasets including human evaluation demonstrate that the proposed method achieves significant accuracy improvements and up to 100 times efficiency improvement over the baseline methods.","sentences":["CAD (Computer-Aided Design) plays a crucial role in mechanical industry, where large numbers of similar-shaped CAD parts are often created.","Efficiently reusing these parts is key to reducing design and production costs for enterprises.","Retrieval systems are vital for achieving CAD reuse, but the complex shapes of CAD models are difficult to accurately describe using text or keywords, making traditional retrieval methods ineffective.","While existing representation learning approaches have been developed for CAD, manually labeling similar samples in these methods is expensive.","Additionally, CAD models' unique parameterized data structure presents challenges for applying existing 3D shape representation learning techniques directly.","In this work, we propose GC-CAD, a self-supervised contrastive graph neural network-based method for mechanical CAD retrieval that directly models parameterized CAD raw files.","GC-CAD consists of two key modules: structure-aware representation learning and contrastive graph learning framework.","The method leverages graph neural networks to extract both geometric and topological information from CAD models, generating feature representations.","We then introduce a simple yet effective contrastive graph learning framework approach, enabling the model to train without manual labels and generate retrieval-ready representations.","Experimental results on four datasets including human evaluation demonstrate that the proposed method achieves significant accuracy improvements and up to 100 times efficiency improvement over the baseline methods."],"url":"http://arxiv.org/abs/2406.08863v1","category":"cs.IR"}
{"created":"2024-06-13 06:54:37","title":"Cognitively Inspired Energy-Based World Models","abstract":"One of the predominant methods for training world models is autoregressive prediction in the output space of the next element of a sequence. In Natural Language Processing (NLP), this takes the form of Large Language Models (LLMs) predicting the next token; in Computer Vision (CV), this takes the form of autoregressive models predicting the next frame/token/pixel. However, this approach differs from human cognition in several respects. First, human predictions about the future actively influence internal cognitive processes. Second, humans naturally evaluate the plausibility of predictions regarding future states. Based on this capability, and third, by assessing when predictions are sufficient, humans allocate a dynamic amount of time to make a prediction. This adaptive process is analogous to System 2 thinking in psychology. All these capabilities are fundamental to the success of humans at high-level reasoning and planning. Therefore, to address the limitations of traditional autoregressive models lacking these human-like capabilities, we introduce Energy-Based World Models (EBWM). EBWM involves training an Energy-Based Model (EBM) to predict the compatibility of a given context and a predicted future state. In doing so, EBWM enables models to achieve all three facets of human cognition described. Moreover, we developed a variant of the traditional autoregressive transformer tailored for Energy-Based models, termed the Energy-Based Transformer (EBT). Our results demonstrate that EBWM scales better with data and GPU Hours than traditional autoregressive transformers in CV, and that EBWM offers promising early scaling in NLP. Consequently, this approach offers an exciting path toward training future models capable of System 2 thinking and intelligently searching across state spaces.","sentences":["One of the predominant methods for training world models is autoregressive prediction in the output space of the next element of a sequence.","In Natural Language Processing (NLP), this takes the form of Large Language Models (LLMs) predicting the next token; in Computer Vision (CV), this takes the form of autoregressive models predicting the next frame/token/pixel.","However, this approach differs from human cognition in several respects.","First, human predictions about the future actively influence internal cognitive processes.","Second, humans naturally evaluate the plausibility of predictions regarding future states.","Based on this capability, and third, by assessing when predictions are sufficient, humans allocate a dynamic amount of time to make a prediction.","This adaptive process is analogous to System 2 thinking in psychology.","All these capabilities are fundamental to the success of humans at high-level reasoning and planning.","Therefore, to address the limitations of traditional autoregressive models lacking these human-like capabilities, we introduce Energy-Based World Models (EBWM).","EBWM involves training an Energy-Based Model (EBM) to predict the compatibility of a given context and a predicted future state.","In doing so, EBWM enables models to achieve all three facets of human cognition described.","Moreover, we developed a variant of the traditional autoregressive transformer tailored for Energy-Based models, termed the Energy-Based Transformer (EBT).","Our results demonstrate that EBWM scales better with data and GPU Hours than traditional autoregressive transformers in CV, and that EBWM offers promising early scaling in NLP.","Consequently, this approach offers an exciting path toward training future models capable of System 2 thinking and intelligently searching across state spaces."],"url":"http://arxiv.org/abs/2406.08862v1","category":"cs.LG"}
{"created":"2024-06-13 06:39:49","title":"Trajectory Planning for Autonomous Driving in Unstructured Scenarios Based on Graph Neural Network and Numerical Optimization","abstract":"In unstructured environments, obstacles are diverse and lack lane markings, making trajectory planning for intelligent vehicles a challenging task. Traditional trajectory planning methods typically involve multiple stages, including path planning, speed planning, and trajectory optimization. These methods require the manual design of numerous parameters for each stage, resulting in significant workload and computational burden. While end-to-end trajectory planning methods are simple and efficient, they often fail to ensure that the trajectory meets vehicle dynamics and obstacle avoidance constraints in unstructured scenarios. Therefore, this paper proposes a novel trajectory planning method based on Graph Neural Networks (GNN) and numerical optimization. The proposed method consists of two stages: (1) initial trajectory prediction using the GNN, (2) trajectory optimization using numerical optimization. First, the graph neural network processes the environment information and predicts a rough trajectory, replacing traditional path and speed planning. This predicted trajectory serves as the initial solution for the numerical optimization stage, which optimizes the trajectory to ensure compliance with vehicle dynamics and obstacle avoidance constraints. We conducted simulation experiments to validate the feasibility of the proposed algorithm and compared it with other mainstream planning algorithms. The results demonstrate that the proposed method simplifies the trajectory planning process and significantly improves planning efficiency.","sentences":["In unstructured environments, obstacles are diverse and lack lane markings, making trajectory planning for intelligent vehicles a challenging task.","Traditional trajectory planning methods typically involve multiple stages, including path planning, speed planning, and trajectory optimization.","These methods require the manual design of numerous parameters for each stage, resulting in significant workload and computational burden.","While end-to-end trajectory planning methods are simple and efficient, they often fail to ensure that the trajectory meets vehicle dynamics and obstacle avoidance constraints in unstructured scenarios.","Therefore, this paper proposes a novel trajectory planning method based on Graph Neural Networks (GNN) and numerical optimization.","The proposed method consists of two stages: (1) initial trajectory prediction using the GNN, (2) trajectory optimization using numerical optimization.","First, the graph neural network processes the environment information and predicts a rough trajectory, replacing traditional path and speed planning.","This predicted trajectory serves as the initial solution for the numerical optimization stage, which optimizes the trajectory to ensure compliance with vehicle dynamics and obstacle avoidance constraints.","We conducted simulation experiments to validate the feasibility of the proposed algorithm and compared it with other mainstream planning algorithms.","The results demonstrate that the proposed method simplifies the trajectory planning process and significantly improves planning efficiency."],"url":"http://arxiv.org/abs/2406.08855v1","category":"cs.RO"}
{"created":"2024-06-13 06:38:09","title":"Current applications and potential future directions of reinforcement learning-based Digital Twins in agriculture","abstract":"Digital Twins have gained attention in various industries for simulation, monitoring, and decision-making, relying on ever-improving machine learning models. However, agricultural Digital Twin implementations are limited compared to other industries. Meanwhile, machine learning, particularly reinforcement learning, has shown potential in agricultural applications like optimizing decision-making, task automation, and resource management. A key aspect of Digital Twins is representing physical assets or systems in a virtual environment, which aligns well with reinforcement learning's need for environment representations to learn the best policy for a task. Reinforcement learning in agriculture can thus enable various Digital Twin applications in agricultural domains. This review aims to categorize existing research employing reinforcement learning in agricultural settings by application domains like robotics, greenhouse management, irrigation systems, and crop management, identifying potential future areas for reinforcement learning-based Digital Twins. It also categorizes the reinforcement learning techniques used, including tabular methods, Deep Q-Networks (DQN), Policy Gradient methods, and Actor-Critic algorithms, to overview currently employed models. The review seeks to provide insights into the state-of-the-art in integrating Digital Twins and reinforcement learning in agriculture, identifying gaps and opportunities for future research, and exploring synergies to tackle agricultural challenges and optimize farming, paving the way for more efficient and sustainable farming methodologies.","sentences":["Digital Twins have gained attention in various industries for simulation, monitoring, and decision-making, relying on ever-improving machine learning models.","However, agricultural Digital Twin implementations are limited compared to other industries.","Meanwhile, machine learning, particularly reinforcement learning, has shown potential in agricultural applications like optimizing decision-making, task automation, and resource management.","A key aspect of Digital Twins is representing physical assets or systems in a virtual environment, which aligns well with reinforcement learning's need for environment representations to learn the best policy for a task.","Reinforcement learning in agriculture can thus enable various Digital Twin applications in agricultural domains.","This review aims to categorize existing research employing reinforcement learning in agricultural settings by application domains like robotics, greenhouse management, irrigation systems, and crop management, identifying potential future areas for reinforcement learning-based Digital Twins.","It also categorizes the reinforcement learning techniques used, including tabular methods, Deep Q-Networks (DQN), Policy Gradient methods, and Actor-Critic algorithms, to overview currently employed models.","The review seeks to provide insights into the state-of-the-art in integrating Digital Twins and reinforcement learning in agriculture, identifying gaps and opportunities for future research, and exploring synergies to tackle agricultural challenges and optimize farming, paving the way for more efficient and sustainable farming methodologies."],"url":"http://arxiv.org/abs/2406.08854v1","category":"cs.LG"}
{"created":"2024-06-13 06:24:52","title":"An Approach to Build Zero-Shot Slot-Filling System for Industry-Grade Conversational Assistants","abstract":"We present an approach to build Large Language Model (LLM) based slot-filling system to perform Dialogue State Tracking in conversational assistants serving across a wide variety of industry-grade applications. Key requirements of this system include: 1) usage of smaller-sized models to meet low latency requirements and to enable convenient and cost-effective cloud and customer premise deployments, and 2) zero-shot capabilities to serve across a wide variety of domains, slot types and conversational scenarios. We adopt a fine-tuning approach where a pre-trained LLM is fine-tuned into a slot-filling model using task specific data. The fine-tuning data is prepared carefully to cover a wide variety of slot-filling task scenarios that the model is expected to face across various domains. We give details of the data preparation and model building process. We also give a detailed analysis of the results of our experimental evaluations. Results show that our prescribed approach for slot-filling model building has resulted in 6.9% relative improvement of F1 metric over the best baseline on a realistic benchmark, while at the same time reducing the latency by 57%. More over, the data we prepared has helped improve F1 on an average by 4.2% relative across various slot-types.","sentences":["We present an approach to build Large Language Model (LLM) based slot-filling system to perform Dialogue State Tracking in conversational assistants serving across a wide variety of industry-grade applications.","Key requirements of this system include: 1) usage of smaller-sized models to meet low latency requirements and to enable convenient and cost-effective cloud and customer premise deployments, and 2) zero-shot capabilities to serve across a wide variety of domains, slot types and conversational scenarios.","We adopt a fine-tuning approach where a pre-trained LLM is fine-tuned into a slot-filling model using task specific data.","The fine-tuning data is prepared carefully to cover a wide variety of slot-filling task scenarios that the model is expected to face across various domains.","We give details of the data preparation and model building process.","We also give a detailed analysis of the results of our experimental evaluations.","Results show that our prescribed approach for slot-filling model building has resulted in 6.9% relative improvement of F1 metric over the best baseline on a realistic benchmark, while at the same time reducing the latency by 57%.","More over, the data we prepared has helped improve F1 on an average by 4.2% relative across various slot-types."],"url":"http://arxiv.org/abs/2406.08848v1","category":"cs.CL"}
{"created":"2024-06-13 06:03:59","title":"Research on Optimization of Natural Language Processing Model Based on Multimodal Deep Learning","abstract":"This project intends to study the image representation based on attention mechanism and multimodal data. By adding multiple pattern layers to the attribute model, the semantic and hidden layers of image content are integrated. The word vector is quantified by the Word2Vec method and then evaluated by a word embedding convolutional neural network. The published experimental results of the two groups were tested. The experimental results show that this method can convert discrete features into continuous characters, thus reducing the complexity of feature preprocessing. Word2Vec and natural language processing technology are integrated to achieve the goal of direct evaluation of missing image features. The robustness of the image feature evaluation model is improved by using the excellent feature analysis characteristics of a convolutional neural network. This project intends to improve the existing image feature identification methods and eliminate the subjective influence in the evaluation process. The findings from the simulation indicate that the novel approach has developed is viable, effectively augmenting the features within the produced representations.","sentences":["This project intends to study the image representation based on attention mechanism and multimodal data.","By adding multiple pattern layers to the attribute model, the semantic and hidden layers of image content are integrated.","The word vector is quantified by the Word2Vec method and then evaluated by a word embedding convolutional neural network.","The published experimental results of the two groups were tested.","The experimental results show that this method can convert discrete features into continuous characters, thus reducing the complexity of feature preprocessing.","Word2Vec and natural language processing technology are integrated to achieve the goal of direct evaluation of missing image features.","The robustness of the image feature evaluation model is improved by using the excellent feature analysis characteristics of a convolutional neural network.","This project intends to improve the existing image feature identification methods and eliminate the subjective influence in the evaluation process.","The findings from the simulation indicate that the novel approach has developed is viable, effectively augmenting the features within the produced representations."],"url":"http://arxiv.org/abs/2406.08838v1","category":"cs.CL"}
{"created":"2024-06-13 05:55:13","title":"Interaction and entanglement engineering in driven giant atoms setup with coupled resonator waveguide","abstract":"We investigate the coherent interactions mediated by the coupled resonator waveguide between two types of giant atoms. We find that the effective coupling and collective dissipation can be controlled on demand by adjusting the configuration of the giant atoms. As a result, the external driving gives birth to a substantial entanglement between two giant atoms, which exhibits a Rabi splitting character. {In the three giant atom setup, we find that the nonzero next neighbour atomic entanglement can surpass the neighbour ones, and is able to be adjust by tuning the driving phase, which serves as an artificial magnetic field. The enhancement of next neighbour atomic entanglement can not be realized in the small atom setup.} We hope these controllable interactions in giant atom array are of great applications in the quantum information process.","sentences":["We investigate the coherent interactions mediated by the coupled resonator waveguide between two types of giant atoms.","We find that the effective coupling and collective dissipation can be controlled on demand by adjusting the configuration of the giant atoms.","As a result, the external driving gives birth to a substantial entanglement between two giant atoms, which exhibits a Rabi splitting character.","{In the three giant atom setup, we find that the nonzero next neighbour atomic entanglement can surpass the neighbour ones, and is able to be adjust by tuning the driving phase, which serves as an artificial magnetic field.","The enhancement of next neighbour atomic entanglement can not be realized in the small atom setup.}","We hope these controllable interactions in giant atom array are of great applications in the quantum information process."],"url":"http://arxiv.org/abs/2406.08834v1","category":"quant-ph"}
{"created":"2024-06-13 05:54:28","title":"Multiplexed Quantum Communication with Surface and Hypergraph Product Codes","abstract":"Connecting multiple processors via quantum interconnect technologies could help to overcome issues of scalability in single-processor quantum computers. Transmission via these interconnects can be performed more efficiently using quantum multiplexing, where information is encoded in high-dimensional photonic degrees of freedom. We explore the effects of multiplexing on logical error rates in surface codes and hypergraph product codes. We show that, although multiplexing makes loss errors more damaging, assigning qubits to photons in an intelligent manner can minimize these effects, and the ability to encode higher-distance codes in a smaller number of photons can result in overall lower logical error rates. This multiplexing technique can also be adapted to quantum communication and multimode quantum memory with high-dimensional qudit systems.","sentences":["Connecting multiple processors via quantum interconnect technologies could help to overcome issues of scalability in single-processor quantum computers.","Transmission via these interconnects can be performed more efficiently using quantum multiplexing, where information is encoded in high-dimensional photonic degrees of freedom.","We explore the effects of multiplexing on logical error rates in surface codes and hypergraph product codes.","We show that, although multiplexing makes loss errors more damaging, assigning qubits to photons in an intelligent manner can minimize these effects, and the ability to encode higher-distance codes in a smaller number of photons can result in overall lower logical error rates.","This multiplexing technique can also be adapted to quantum communication and multimode quantum memory with high-dimensional qudit systems."],"url":"http://arxiv.org/abs/2406.08832v1","category":"quant-ph"}
{"created":"2024-06-13 05:49:29","title":"Center-Sensitive Kernel Optimization for Efficient On-Device Incremental Learning","abstract":"To facilitate the evolution of edge intelligence in ever-changing environments, we study on-device incremental learning constrained in limited computation resource in this paper. Current on-device training methods just focus on efficient training without considering the catastrophic forgetting, preventing the model getting stronger when continually exploring the world. To solve this problem, a direct solution is to involve the existing incremental learning mechanisms into the on-device training framework. Unfortunately, such a manner cannot work well as those mechanisms usually introduce large additional computational cost to the network optimization process, which would inevitably exceed the memory capacity of the edge devices. To address this issue, this paper makes an early effort to propose a simple but effective edge-friendly incremental learning framework. Based on an empirical study on the knowledge intensity of the kernel elements of the neural network, we find that the center kernel is the key for maximizing the knowledge intensity for learning new data, while freezing the other kernel elements would get a good balance on the model's capacity for overcoming catastrophic forgetting. Upon this finding, we further design a center-sensitive kernel optimization framework to largely alleviate the cost of the gradient computation and back-propagation. Besides, a dynamic channel element selection strategy is also proposed to facilitate a sparse orthogonal gradient projection for further reducing the optimization complexity, upon the knowledge explored from the new task data. Extensive experiments validate our method is efficient and effective, e.g., our method achieves average accuracy boost of 38.08% with even less memory and approximate computation compared to existing on-device training methods, indicating its significant potential for on-device incremental learning.","sentences":["To facilitate the evolution of edge intelligence in ever-changing environments, we study on-device incremental learning constrained in limited computation resource in this paper.","Current on-device training methods just focus on efficient training without considering the catastrophic forgetting, preventing the model getting stronger when continually exploring the world.","To solve this problem, a direct solution is to involve the existing incremental learning mechanisms into the on-device training framework.","Unfortunately, such a manner cannot work well as those mechanisms usually introduce large additional computational cost to the network optimization process, which would inevitably exceed the memory capacity of the edge devices.","To address this issue, this paper makes an early effort to propose a simple but effective edge-friendly incremental learning framework.","Based on an empirical study on the knowledge intensity of the kernel elements of the neural network, we find that the center kernel is the key for maximizing the knowledge intensity for learning new data, while freezing the other kernel elements would get a good balance on the model's capacity for overcoming catastrophic forgetting.","Upon this finding, we further design a center-sensitive kernel optimization framework to largely alleviate the cost of the gradient computation and back-propagation.","Besides, a dynamic channel element selection strategy is also proposed to facilitate a sparse orthogonal gradient projection for further reducing the optimization complexity, upon the knowledge explored from the new task data.","Extensive experiments validate our method is efficient and effective, e.g., our method achieves average accuracy boost of 38.08% with even less memory and approximate computation compared to existing on-device training methods, indicating its significant potential for on-device incremental learning."],"url":"http://arxiv.org/abs/2406.08830v1","category":"cs.LG"}
{"created":"2024-06-13 05:38:20","title":"Estimating Difficulty Levels of Programming Problems with Pre-trained Model","abstract":"As the demand for programming skills grows across industries and academia, students often turn to Programming Online Judge (POJ) platforms for coding practice and competition. The difficulty level of each programming problem serves as an essential reference for guiding students' adaptive learning. However, current methods of determining difficulty levels either require extensive expert annotations or take a long time to accumulate enough student solutions for each problem. To address this issue, we formulate the problem of automatic difficulty level estimation of each programming problem, given its textual description and a solution example of code. For tackling this problem, we propose to couple two pre-trained models, one for text modality and the other for code modality, into a unified model. We built two POJ datasets for the task and the results demonstrate the effectiveness of the proposed approach and the contributions of both modalities.","sentences":["As the demand for programming skills grows across industries and academia, students often turn to Programming Online Judge (POJ) platforms for coding practice and competition.","The difficulty level of each programming problem serves as an essential reference for guiding students' adaptive learning.","However, current methods of determining difficulty levels either require extensive expert annotations or take a long time to accumulate enough student solutions for each problem.","To address this issue, we formulate the problem of automatic difficulty level estimation of each programming problem, given its textual description and a solution example of code.","For tackling this problem, we propose to couple two pre-trained models, one for text modality and the other for code modality, into a unified model.","We built two POJ datasets for the task and the results demonstrate the effectiveness of the proposed approach and the contributions of both modalities."],"url":"http://arxiv.org/abs/2406.08828v1","category":"cs.SE"}
{"created":"2024-06-13 05:31:49","title":"LLM-Driven Robots Risk Enacting Discrimination, Violence, and Unlawful Actions","abstract":"Members of the Human-Robot Interaction (HRI) and Artificial Intelligence (AI) communities have proposed Large Language Models (LLMs) as a promising resource for robotics tasks such as natural language interactions, doing household and workplace tasks, approximating `common sense reasoning', and modeling humans. However, recent research has raised concerns about the potential for LLMs to produce discriminatory outcomes and unsafe behaviors in real-world robot experiments and applications. To address these concerns, we conduct an HRI-based evaluation of discrimination and safety criteria on several highly-rated LLMs. Our evaluation reveals that LLMs currently lack robustness when encountering people across a diverse range of protected identity characteristics (e.g., race, gender, disability status, nationality, religion, and their intersections), producing biased outputs consistent with directly discriminatory outcomes -- e.g. `gypsy' and `mute' people are labeled untrustworthy, but not `european' or `able-bodied' people. Furthermore, we test models in settings with unconstrained natural language (open vocabulary) inputs, and find they fail to act safely, generating responses that accept dangerous, violent, or unlawful instructions -- such as incident-causing misstatements, taking people's mobility aids, and sexual predation. Our results underscore the urgent need for systematic, routine, and comprehensive risk assessments and assurances to improve outcomes and ensure LLMs only operate on robots when it is safe, effective, and just to do so. Data and code will be made available.","sentences":["Members of the Human-Robot Interaction (HRI) and Artificial Intelligence (AI) communities have proposed Large Language Models (LLMs) as a promising resource for robotics tasks such as natural language interactions, doing household and workplace tasks, approximating `common sense reasoning', and modeling humans.","However, recent research has raised concerns about the potential for LLMs to produce discriminatory outcomes and unsafe behaviors in real-world robot experiments and applications.","To address these concerns, we conduct an HRI-based evaluation of discrimination and safety criteria on several highly-rated LLMs.","Our evaluation reveals that LLMs currently lack robustness when encountering people across a diverse range of protected identity characteristics (e.g., race, gender, disability status, nationality, religion, and their intersections), producing biased outputs consistent with directly discriminatory outcomes -- e.g. `gypsy' and `mute' people are labeled untrustworthy, but not `european' or `able-bodied' people.","Furthermore, we test models in settings with unconstrained natural language (open vocabulary) inputs, and find they fail to act safely, generating responses that accept dangerous, violent, or unlawful instructions -- such as incident-causing misstatements, taking people's mobility aids, and sexual predation.","Our results underscore the urgent need for systematic, routine, and comprehensive risk assessments and assurances to improve outcomes and ensure LLMs only operate on robots when it is safe, effective, and just to do so.","Data and code will be made available."],"url":"http://arxiv.org/abs/2406.08824v1","category":"cs.RO"}
{"created":"2024-06-13 05:28:53","title":"Computer vision-based model for detecting turning lane features on Florida's public roadways","abstract":"Efficient and current roadway geometry data collection is critical to transportation agencies in road planning, maintenance, design, and rehabilitation. Data collection methods are divided into land-based and aerial-based. Land-based methods for extensive highway networks are tedious, costly, pose safety risks. Therefore, there is the need for efficient, safe, and economical data acquisition methodologies. The rise of computer vision and object detection technologies have made automated extraction of roadway geometry features feasible. This study detects roadway features on Florida's public roads from high-resolution aerial images using AI. The developed model achieved an average accuracy of 80.4 percent when compared with ground truth data. The extracted roadway geometry data can be integrated with crash and traffic data to provide valuable insights to policymakers and roadway users.","sentences":["Efficient and current roadway geometry data collection is critical to transportation agencies in road planning, maintenance, design, and rehabilitation.","Data collection methods are divided into land-based and aerial-based.","Land-based methods for extensive highway networks are tedious, costly, pose safety risks.","Therefore, there is the need for efficient, safe, and economical data acquisition methodologies.","The rise of computer vision and object detection technologies have made automated extraction of roadway geometry features feasible.","This study detects roadway features on Florida's public roads from high-resolution aerial images using AI.","The developed model achieved an average accuracy of 80.4 percent when compared with ground truth data.","The extracted roadway geometry data can be integrated with crash and traffic data to provide valuable insights to policymakers and roadway users."],"url":"http://arxiv.org/abs/2406.08822v1","category":"cs.CV"}
{"created":"2024-06-13 05:21:10","title":"AIM: Attributing, Interpreting, Mitigating Data Unfairness","abstract":"Data collected in the real world often encapsulates historical discrimination against disadvantaged groups and individuals. Existing fair machine learning (FairML) research has predominantly focused on mitigating discriminative bias in the model prediction, with far less effort dedicated towards exploring how to trace biases present in the data, despite its importance for the transparency and interpretability of FairML. To fill this gap, we investigate a novel research problem: discovering samples that reflect biases/prejudices from the training data. Grounding on the existing fairness notions, we lay out a sample bias criterion and propose practical algorithms for measuring and countering sample bias. The derived bias score provides intuitive sample-level attribution and explanation of historical bias in data. On this basis, we further design two FairML strategies via sample-bias-informed minimal data editing. They can mitigate both group and individual unfairness at the cost of minimal or zero predictive utility loss. Extensive experiments and analyses on multiple real-world datasets demonstrate the effectiveness of our methods in explaining and mitigating unfairness. Code is available at https://github.com/ZhiningLiu1998/AIM.","sentences":["Data collected in the real world often encapsulates historical discrimination against disadvantaged groups and individuals.","Existing fair machine learning (FairML) research has predominantly focused on mitigating discriminative bias in the model prediction, with far less effort dedicated towards exploring how to trace biases present in the data, despite its importance for the transparency and interpretability of FairML.","To fill this gap, we investigate a novel research problem: discovering samples that reflect biases/prejudices from the training data.","Grounding on the existing fairness notions, we lay out a sample bias criterion and propose practical algorithms for measuring and countering sample bias.","The derived bias score provides intuitive sample-level attribution and explanation of historical bias in data.","On this basis, we further design two FairML strategies via sample-bias-informed minimal data editing.","They can mitigate both group and individual unfairness at the cost of minimal or zero predictive utility loss.","Extensive experiments and analyses on multiple real-world datasets demonstrate the effectiveness of our methods in explaining and mitigating unfairness.","Code is available at https://github.com/ZhiningLiu1998/AIM."],"url":"http://arxiv.org/abs/2406.08819v1","category":"cs.LG"}
{"created":"2024-06-13 05:06:30","title":"Generating Speakers by Prompting Listener Impressions for Pre-trained Multi-Speaker Text-to-Speech Systems","abstract":"This paper proposes a speech synthesis system that allows users to specify and control the acoustic characteristics of a speaker by means of prompts describing the speaker's traits of synthesized speech. Unlike previous approaches, our method utilizes listener impressions to construct prompts, which are easier to collect and align more naturally with everyday descriptions of speaker traits. We adopt the Low-rank Adaptation (LoRA) technique to swiftly tailor a pre-trained language model to our needs, facilitating the extraction of speaker-related traits from the prompt text. Besides, different from other prompt-driven text-to-speech (TTS) systems, we separate the prompt-to-speaker module from the multi-speaker TTS system, enhancing system flexibility and compatibility with various pre-trained multi-speaker TTS systems. Moreover, for the prompt-to-speaker characteristic module, we also compared the discriminative method and flow-matching based generative method and we found that combining both methods can help the system simultaneously capture speaker-related information from prompts better and generate speech with higher fidelity.","sentences":["This paper proposes a speech synthesis system that allows users to specify and control the acoustic characteristics of a speaker by means of prompts describing the speaker's traits of synthesized speech.","Unlike previous approaches, our method utilizes listener impressions to construct prompts, which are easier to collect and align more naturally with everyday descriptions of speaker traits.","We adopt the Low-rank Adaptation (LoRA) technique to swiftly tailor a pre-trained language model to our needs, facilitating the extraction of speaker-related traits from the prompt text.","Besides, different from other prompt-driven text-to-speech (TTS) systems, we separate the prompt-to-speaker module from the multi-speaker TTS system, enhancing system flexibility and compatibility with various pre-trained multi-speaker TTS systems.","Moreover, for the prompt-to-speaker characteristic module, we also compared the discriminative method and flow-matching based generative method and we found that combining both methods can help the system simultaneously capture speaker-related information from prompts better and generate speech with higher fidelity."],"url":"http://arxiv.org/abs/2406.08812v1","category":"cs.SD"}
{"created":"2024-06-13 05:00:27","title":"Are we there yet? A brief survey of Music Emotion Prediction Datasets, Models and Outstanding Challenges","abstract":"Deep learning models for music have advanced drastically in the last few years. But how good are machine learning models at capturing emotion these days and what challenges are researchers facing? In this paper, we provide a comprehensive overview of the available music-emotion datasets and discuss evaluation standards as well as competitions in the field. We also provide a brief overview of various types of music emotion prediction models that have been built over the years, offering insights into the diverse approaches within the field. Through this examination, we highlight the challenges that persist in accurately capturing emotion in music. Recognizing the dynamic nature of this field, we have complemented our findings with an accompanying GitHub repository. This repository contains a comprehensive list of music emotion datasets and recent predictive models.","sentences":["Deep learning models for music have advanced drastically in the last few years.","But how good are machine learning models at capturing emotion these days and what challenges are researchers facing?","In this paper, we provide a comprehensive overview of the available music-emotion datasets and discuss evaluation standards as well as competitions in the field.","We also provide a brief overview of various types of music emotion prediction models that have been built over the years, offering insights into the diverse approaches within the field.","Through this examination, we highlight the challenges that persist in accurately capturing emotion in music.","Recognizing the dynamic nature of this field, we have complemented our findings with an accompanying GitHub repository.","This repository contains a comprehensive list of music emotion datasets and recent predictive models."],"url":"http://arxiv.org/abs/2406.08809v1","category":"cs.SD"}
{"created":"2024-06-13 04:39:42","title":"A Dual Approach to Imitation Learning from Observations with Offline Datasets","abstract":"Demonstrations are an effective alternative to task specification for learning agents in settings where designing a reward function is difficult. However, demonstrating expert behavior in the action space of the agent becomes unwieldy when robots have complex, unintuitive morphologies. We consider the practical setting where an agent has a dataset of prior interactions with the environment and is provided with observation-only expert demonstrations. Typical learning from observations approaches have required either learning an inverse dynamics model or a discriminator as intermediate steps of training. Errors in these intermediate one-step models compound during downstream policy learning or deployment. We overcome these limitations by directly learning a multi-step utility function that quantifies how each action impacts the agent's divergence from the expert's visitation distribution. Using the principle of duality, we derive DILO(Dual Imitation Learning from Observations), an algorithm that can leverage arbitrary suboptimal data to learn imitating policies without requiring expert actions. DILO reduces the learning from observations problem to that of simply learning an actor and a critic, bearing similar complexity to vanilla offline RL. This allows DILO to gracefully scale to high dimensional observations, and demonstrate improved performance across the board. Project page (code and videos): $\\href{https://hari-sikchi.github.io/dilo/}{\\text{hari-sikchi.github.io/dilo/}}$","sentences":["Demonstrations are an effective alternative to task specification for learning agents in settings where designing a reward function is difficult.","However, demonstrating expert behavior in the action space of the agent becomes unwieldy when robots have complex, unintuitive morphologies.","We consider the practical setting where an agent has a dataset of prior interactions with the environment and is provided with observation-only expert demonstrations.","Typical learning from observations approaches have required either learning an inverse dynamics model or a discriminator as intermediate steps of training.","Errors in these intermediate one-step models compound during downstream policy learning or deployment.","We overcome these limitations by directly learning a multi-step utility function that quantifies how each action impacts the agent's divergence from the expert's visitation distribution.","Using the principle of duality, we derive DILO(Dual Imitation Learning from Observations), an algorithm that can leverage arbitrary suboptimal data to learn imitating policies without requiring expert actions.","DILO reduces the learning from observations problem to that of simply learning an actor and a critic, bearing similar complexity to vanilla offline RL.","This allows DILO to gracefully scale to high dimensional observations, and demonstrate improved performance across the board.","Project page (code and videos): $\\href{https://hari-sikchi.github.io/dilo/}{\\text{hari-sikchi.github.io/dilo/}}$"],"url":"http://arxiv.org/abs/2406.08805v1","category":"cs.LG"}
{"created":"2024-06-13 04:39:16","title":"DIET: Customized Slimming for Incompatible Networks in Sequential Recommendation","abstract":"Due to the continuously improving capabilities of mobile edges, recommender systems start to deploy models on edges to alleviate network congestion caused by frequent mobile requests. Several studies have leveraged the proximity of edge-side to real-time data, fine-tuning them to create edge-specific models. Despite their significant progress, these methods require substantial on-edge computational resources and frequent network transfers to keep the model up to date. The former may disrupt other processes on the edge to acquire computational resources, while the latter consumes network bandwidth, leading to a decrease in user satisfaction. In response to these challenges, we propose a customizeD slImming framework for incompatiblE neTworks(DIET). DIET deploys the same generic backbone (potentially incompatible for a specific edge) to all devices. To minimize frequent bandwidth usage and storage consumption in personalization, DIET tailors specific subnets for each edge based on its past interactions, learning to generate slimming subnets(diets) within incompatible networks for efficient transfer. It also takes the inter-layer relationships into account, empirically reducing inference time while obtaining more suitable diets. We further explore the repeated modules within networks and propose a more storage-efficient framework, DIETING, which utilizes a single layer of parameters to represent the entire network, achieving comparably excellent performance. The experiments across four state-of-the-art datasets and two widely used models demonstrate the superior accuracy in recommendation and efficiency in transmission and storage of our framework.","sentences":["Due to the continuously improving capabilities of mobile edges, recommender systems start to deploy models on edges to alleviate network congestion caused by frequent mobile requests.","Several studies have leveraged the proximity of edge-side to real-time data, fine-tuning them to create edge-specific models.","Despite their significant progress, these methods require substantial on-edge computational resources and frequent network transfers to keep the model up to date.","The former may disrupt other processes on the edge to acquire computational resources, while the latter consumes network bandwidth, leading to a decrease in user satisfaction.","In response to these challenges, we propose a customizeD slImming framework for incompatiblE neTworks(DIET).","DIET deploys the same generic backbone (potentially incompatible for a specific edge) to all devices.","To minimize frequent bandwidth usage and storage consumption in personalization, DIET tailors specific subnets for each edge based on its past interactions, learning to generate slimming subnets(diets) within incompatible networks for efficient transfer.","It also takes the inter-layer relationships into account, empirically reducing inference time while obtaining more suitable diets.","We further explore the repeated modules within networks and propose a more storage-efficient framework, DIETING, which utilizes a single layer of parameters to represent the entire network, achieving comparably excellent performance.","The experiments across four state-of-the-art datasets and two widely used models demonstrate the superior accuracy in recommendation and efficiency in transmission and storage of our framework."],"url":"http://arxiv.org/abs/2406.08804v1","category":"cs.DC"}
{"created":"2024-06-13 04:28:00","title":"Pareto Front-Diverse Batch Multi-Objective Bayesian Optimization","abstract":"We consider the problem of multi-objective optimization (MOO) of expensive black-box functions with the goal of discovering high-quality and diverse Pareto fronts where we are allowed to evaluate a batch of inputs. This problem arises in many real-world applications including penicillin production where diversity of solutions is critical. We solve this problem in the framework of Bayesian optimization (BO) and propose a novel approach referred to as Pareto front-Diverse Batch Multi-Objective BO (PDBO). PDBO tackles two important challenges: 1) How to automatically select the best acquisition function in each BO iteration, and 2) How to select a diverse batch of inputs by considering multiple objectives. We propose principled solutions to address these two challenges. First, PDBO employs a multi-armed bandit approach to select one acquisition function from a given library. We solve a cheap MOO problem by assigning the selected acquisition function for each expensive objective function to obtain a candidate set of inputs for evaluation. Second, it utilizes Determinantal Point Processes (DPPs) to choose a Pareto-front-diverse batch of inputs for evaluation from the candidate set obtained from the first step. The key parameters for the methods behind these two steps are updated after each round of function evaluations. Experiments on multiple MOO benchmarks demonstrate that PDBO outperforms prior methods in terms of both the quality and diversity of Pareto solutions.","sentences":["We consider the problem of multi-objective optimization (MOO) of expensive black-box functions with the goal of discovering high-quality and diverse Pareto fronts where we are allowed to evaluate a batch of inputs.","This problem arises in many real-world applications including penicillin production where diversity of solutions is critical.","We solve this problem in the framework of Bayesian optimization (BO) and propose a novel approach referred to as Pareto front-Diverse Batch Multi-Objective BO (PDBO).","PDBO tackles two important challenges: 1) How to automatically select the best acquisition function in each BO iteration, and 2) How to select a diverse batch of inputs by considering multiple objectives.","We propose principled solutions to address these two challenges.","First, PDBO employs a multi-armed bandit approach to select one acquisition function from a given library.","We solve a cheap MOO problem by assigning the selected acquisition function for each expensive objective function to obtain a candidate set of inputs for evaluation.","Second, it utilizes Determinantal Point Processes (DPPs) to choose a Pareto-front-diverse batch of inputs for evaluation from the candidate set obtained from the first step.","The key parameters for the methods behind these two steps are updated after each round of function evaluations.","Experiments on multiple MOO benchmarks demonstrate that PDBO outperforms prior methods in terms of both the quality and diversity of Pareto solutions."],"url":"http://arxiv.org/abs/2406.08799v1","category":"cs.LG"}
{"created":"2024-06-13 04:25:18","title":"Joint Hybrid Transceiver and Reflection Matrix Design for RIS-Aided mmWave MIMO Cognitive Radio Systems","abstract":"In this work, a reconfigurable intelligent surface (RIS)-aided millimeter wave (mmWave) multiple-input multiple-output (MIMO) cognitive radio (CR) downlink operating in the underlay mode is investigated. The cognitive base station (CBS) communicates with multiple secondary users (SUs), each having multiple RF chains in the presence of a primary user (PU). We conceive a joint hybrid transmit precoder (TPC), receiver combiner (RC), and RIS reflection matrix (RM) design, which maximizes the sum spectral efficiency (SE) of the secondary system while maintaining the interference induced at the PU below a specified threshold. To this end, we formulate the sum-SE maximization problem considering the total transmit power (TP), the interference power (IP), and the non-convex unity modulus constraints of the RF TPC, RF RC, and RM. To solve this highly non-convex problem, we propose a two-stage hybrid transceiver design in conjunction with a novel block coordinate descent (BCD)-successive Riemannian conjugate gradient (SRCG) algorithm. We initially decompose the RF TPC, RC, and RM optimization problem into a series of sub-problems and subsequently design pairs of RF TPC and RC vectors, followed by successively optimizing the elements of the RM using the iterative BCD-SRCG algorithm. Furthermore, based on the effective baseband (BB) channel, the BB TPC and BB RC are designed using the proposed direct singular value decomposition (D-SVD) and projection based SVD (P-SVD) methods. Subsequently, the proportional water-filling solution is proposed for optimizing the power, which maximizes the weighted sum-SE of the system. Finally, simulation results are provided to compare our proposed schemes to several benchmarks and quantify the impact of other parameters on the sum-SE of the system.","sentences":["In this work, a reconfigurable intelligent surface (RIS)-aided millimeter wave (mmWave) multiple-input multiple-output (MIMO) cognitive radio (CR) downlink operating in the underlay mode is investigated.","The cognitive base station (CBS) communicates with multiple secondary users (SUs), each having multiple RF chains in the presence of a primary user (PU).","We conceive a joint hybrid transmit precoder (TPC), receiver combiner (RC), and RIS reflection matrix (RM) design, which maximizes the sum spectral efficiency (SE) of the secondary system while maintaining the interference induced at the PU below a specified threshold.","To this end, we formulate the sum-SE maximization problem considering the total transmit power (TP), the interference power (IP), and the non-convex unity modulus constraints of the RF TPC, RF RC, and RM.","To solve this highly non-convex problem, we propose a two-stage hybrid transceiver design in conjunction with a novel block coordinate descent (BCD)-successive Riemannian conjugate gradient (SRCG) algorithm.","We initially decompose the RF TPC, RC, and RM optimization problem into a series of sub-problems and subsequently design pairs of RF TPC and RC vectors, followed by successively optimizing the elements of the RM using the iterative BCD-SRCG algorithm.","Furthermore, based on the effective baseband (BB) channel, the BB TPC and BB RC are designed using the proposed direct singular value decomposition (D-SVD) and projection based SVD (P-SVD) methods.","Subsequently, the proportional water-filling solution is proposed for optimizing the power, which maximizes the weighted sum-SE of the system.","Finally, simulation results are provided to compare our proposed schemes to several benchmarks and quantify the impact of other parameters on the sum-SE of the system."],"url":"http://arxiv.org/abs/2406.08797v1","category":"eess.SP"}
{"created":"2024-06-13 03:47:12","title":"Understanding the Generalizability of Link Predictors Under Distribution Shifts on Graphs","abstract":"Recently, multiple models proposed for link prediction (LP) demonstrate impressive results on benchmark datasets. However, many popular benchmark datasets often assume that dataset samples are drawn from the same distribution (i.e., IID samples). In real-world situations, this assumption is often incorrect; since uncontrolled factors may lead train and test samples to come from separate distributions. To tackle the distribution shift problem, recent work focuses on creating datasets that feature distribution shifts and designing generalization methods that perform well on the new data. However, those studies only consider distribution shifts that affect {\\it node-} and {\\it graph-level} tasks, thus ignoring link-level tasks. Furthermore, relatively few LP generalization methods exist. To bridge this gap, we introduce a set of LP-specific data splits which utilizes structural properties to induce a controlled distribution shift. We verify the shift's effect empirically through evaluation of different SOTA LP methods and subsequently couple these methods with generalization techniques. Interestingly, LP-specific methods frequently generalize poorly relative to heuristics or basic GNN methods. Finally, this work provides analysis to uncover insights for enhancing LP generalization. Our code is available at: \\href{https://github.com/revolins/LPStructGen}{https://github.com/revolins/LPStructGen}","sentences":["Recently, multiple models proposed for link prediction (LP) demonstrate impressive results on benchmark datasets.","However, many popular benchmark datasets often assume that dataset samples are drawn from the same distribution (i.e., IID samples).","In real-world situations, this assumption is often incorrect; since uncontrolled factors may lead train and test samples to come from separate distributions.","To tackle the distribution shift problem, recent work focuses on creating datasets that feature distribution shifts and designing generalization methods that perform well on the new data.","However, those studies only consider distribution shifts that affect {\\it node-} and {\\it graph-level} tasks, thus ignoring link-level tasks.","Furthermore, relatively few LP generalization methods exist.","To bridge this gap, we introduce a set of LP-specific data splits which utilizes structural properties to induce a controlled distribution shift.","We verify the shift's effect empirically through evaluation of different SOTA LP methods and subsequently couple these methods with generalization techniques.","Interestingly, LP-specific methods frequently generalize poorly relative to heuristics or basic GNN methods.","Finally, this work provides analysis to uncover insights for enhancing LP generalization.","Our code is available at: \\href{https://github.com/revolins/LPStructGen}{https://github.com/revolins/LPStructGen}"],"url":"http://arxiv.org/abs/2406.08788v1","category":"cs.LG"}
{"created":"2024-06-13 03:46:21","title":"A Survey on Compositional Learning of AI Models: Theoretical and Experimetnal Practices","abstract":"Compositional learning, mastering the ability to combine basic concepts and construct more intricate ones, is crucial for human cognition, especially in human language comprehension and visual perception. This notion is tightly connected to generalization over unobserved situations. Despite its integral role in intelligence, there is a lack of systematic theoretical and experimental research methodologies, making it difficult to analyze the compositional learning abilities of computational models. In this paper, we survey the literature on compositional learning of AI models and the connections made to cognitive studies. We identify abstract concepts of compositionality in cognitive and linguistic studies and connect these to the computational challenges faced by language and vision models in compositional reasoning. We overview the formal definitions, tasks, evaluation benchmarks, variety of computational models, and theoretical findings. We cover modern studies on large language models to provide a deeper understanding of the cutting-edge compositional capabilities exhibited by state-of-the-art AI models and pinpoint important directions for future research.","sentences":["Compositional learning, mastering the ability to combine basic concepts and construct more intricate ones, is crucial for human cognition, especially in human language comprehension and visual perception.","This notion is tightly connected to generalization over unobserved situations.","Despite its integral role in intelligence, there is a lack of systematic theoretical and experimental research methodologies, making it difficult to analyze the compositional learning abilities of computational models.","In this paper, we survey the literature on compositional learning of AI models and the connections made to cognitive studies.","We identify abstract concepts of compositionality in cognitive and linguistic studies and connect these to the computational challenges faced by language and vision models in compositional reasoning.","We overview the formal definitions, tasks, evaluation benchmarks, variety of computational models, and theoretical findings.","We cover modern studies on large language models to provide a deeper understanding of the cutting-edge compositional capabilities exhibited by state-of-the-art AI models and pinpoint important directions for future research."],"url":"http://arxiv.org/abs/2406.08787v1","category":"cs.AI"}
{"created":"2024-06-13 03:03:02","title":"MFF-EINV2: Multi-scale Feature Fusion across Spectral-Spatial-Temporal Domains for Sound Event Localization and Detection","abstract":"Sound Event Localization and Detection (SELD) involves detecting and localizing sound events using multichannel sound recordings. Previously proposed Event-Independent Network V2 (EINV2) has achieved outstanding performance on SELD. However, it still faces challenges in effectively extracting features across spectral, spatial, and temporal domains. This paper proposes a three-stage network structure named Multi-scale Feature Fusion (MFF) module to fully extract multi-scale features across spectral, spatial, and temporal domains. The MFF module utilizes parallel subnetworks architecture to generate multi-scale spectral and spatial features. The TF-Convolution Module is employed to provide multi-scale temporal features. We incorporated MFF into EINV2 and term the proposed method as MFF-EINV2. Experimental results in 2022 and 2023 DCASE challenge task3 datasets show the effectiveness of our MFF-EINV2, which achieves state-of-the-art (SOTA) performance compared to published methods.","sentences":["Sound Event Localization and Detection (SELD) involves detecting and localizing sound events using multichannel sound recordings.","Previously proposed Event-Independent Network V2 (EINV2) has achieved outstanding performance on SELD.","However, it still faces challenges in effectively extracting features across spectral, spatial, and temporal domains.","This paper proposes a three-stage network structure named Multi-scale Feature Fusion (MFF) module to fully extract multi-scale features across spectral, spatial, and temporal domains.","The MFF module utilizes parallel subnetworks architecture to generate multi-scale spectral and spatial features.","The TF-Convolution Module is employed to provide multi-scale temporal features.","We incorporated MFF into EINV2 and term the proposed method as MFF-EINV2.","Experimental results in 2022 and 2023 DCASE challenge task3 datasets show the effectiveness of our MFF-EINV2, which achieves state-of-the-art (SOTA) performance compared to published methods."],"url":"http://arxiv.org/abs/2406.08771v1","category":"cs.SD"}
{"created":"2024-06-13 02:55:08","title":"Injecting Combinatorial Optimization into MCTS: Application to the Board Game boop","abstract":"Games, including abstract board games, constitute a convenient ground to create, design, and improve new AI methods. In this field, Monte Carlo Tree Search is a popular algorithm family, aiming to build game trees and explore them efficiently. Combinatorial Optimization, on the other hand, aims to model and solve problems with an objective to optimize and constraints to satisfy, and is less common in Game AI. We believe however that both methods can be combined efficiently, by injecting Combinatorial Optimization into Monte Carlo Tree Search to help the tree search, leading to a novel combination of these two techniques. Tested on the board game boop., our method beats 96% of the time the Monte Carlo Tree Search algorithm baseline. We conducted an ablation study to isolate and analyze which injections and combinations of injections lead to such performances. Finally, we opposed our AI method against human players on the Board Game Arena platform, and reached a 373 ELO rating after 51 boop. games, with a 69% win rate and finishing ranked 56th worldwide on the platform over 5,316 boop. players.","sentences":["Games, including abstract board games, constitute a convenient ground to create, design, and improve new AI methods.","In this field, Monte Carlo Tree Search is a popular algorithm family, aiming to build game trees and explore them efficiently.","Combinatorial Optimization, on the other hand, aims to model and solve problems with an objective to optimize and constraints to satisfy, and is less common in Game AI.","We believe however that both methods can be combined efficiently, by injecting Combinatorial Optimization into Monte Carlo Tree Search to help the tree search, leading to a novel combination of these two techniques.","Tested on the board game boop., our method beats 96% of the time the Monte Carlo Tree Search algorithm baseline.","We conducted an ablation study to isolate and analyze which injections and combinations of injections lead to such performances.","Finally, we opposed our AI method against human players on the Board Game Arena platform, and reached a 373 ELO rating after 51 boop.","games, with a 69% win rate and finishing ranked 56th worldwide on the platform over 5,316 boop.","players."],"url":"http://arxiv.org/abs/2406.08766v1","category":"cs.AI"}
{"created":"2024-06-13 02:35:55","title":"SRFUND: A Multi-Granularity Hierarchical Structure Reconstruction Benchmark in Form Understanding","abstract":"Accurately identifying and organizing textual content is crucial for the automation of document processing in the field of form understanding. Existing datasets, such as FUNSD and XFUND, support entity classification and relationship prediction tasks but are typically limited to local and entity-level annotations. This limitation overlooks the hierarchically structured representation of documents, constraining comprehensive understanding of complex forms. To address this issue, we present the SRFUND, a hierarchically structured multi-task form understanding benchmark. SRFUND provides refined annotations on top of the original FUNSD and XFUND datasets, encompassing five tasks: (1) word to text-line merging, (2) text-line to entity merging, (3) entity category classification, (4) item table localization, and (5) entity-based full-document hierarchical structure recovery. We meticulously supplemented the original dataset with missing annotations at various levels of granularity and added detailed annotations for multi-item table regions within the forms. Additionally, we introduce global hierarchical structure dependencies for entity relation prediction tasks, surpassing traditional local key-value associations. The SRFUND dataset includes eight languages including English, Chinese, Japanese, German, French, Spanish, Italian, and Portuguese, making it a powerful tool for cross-lingual form understanding. Extensive experimental results demonstrate that the SRFUND dataset presents new challenges and significant opportunities in handling diverse layouts and global hierarchical structures of forms, thus providing deep insights into the field of form understanding. The original dataset and implementations of baseline methods are available at https://sprateam-ustc.github.io/SRFUND","sentences":["Accurately identifying and organizing textual content is crucial for the automation of document processing in the field of form understanding.","Existing datasets, such as FUNSD and XFUND, support entity classification and relationship prediction tasks but are typically limited to local and entity-level annotations.","This limitation overlooks the hierarchically structured representation of documents, constraining comprehensive understanding of complex forms.","To address this issue, we present the SRFUND, a hierarchically structured multi-task form understanding benchmark.","SRFUND provides refined annotations on top of the original FUNSD and XFUND datasets, encompassing five tasks: (1) word to text-line merging, (2) text-line to entity merging, (3) entity category classification, (4) item table localization, and (5) entity-based full-document hierarchical structure recovery.","We meticulously supplemented the original dataset with missing annotations at various levels of granularity and added detailed annotations for multi-item table regions within the forms.","Additionally, we introduce global hierarchical structure dependencies for entity relation prediction tasks, surpassing traditional local key-value associations.","The SRFUND dataset includes eight languages including English, Chinese, Japanese, German, French, Spanish, Italian, and Portuguese, making it a powerful tool for cross-lingual form understanding.","Extensive experimental results demonstrate that the SRFUND dataset presents new challenges and significant opportunities in handling diverse layouts and global hierarchical structures of forms, thus providing deep insights into the field of form understanding.","The original dataset and implementations of baseline methods are available at https://sprateam-ustc.github.io/SRFUND"],"url":"http://arxiv.org/abs/2406.08757v1","category":"cs.CL"}
{"created":"2024-06-13 02:21:07","title":"3D Building Generation in Minecraft via Large Language Models","abstract":"Recently, procedural content generation has exhibited considerable advancements in the domain of 2D game level generation such as Super Mario Bros. and Sokoban through large language models (LLMs). To further validate the capabilities of LLMs, this paper explores how LLMs contribute to the generation of 3D buildings in a sandbox game, Minecraft. We propose a Text to Building in Minecraft (T2BM) model, which involves refining prompts, decoding interlayer representation and repairing. Facade, indoor scene and functional blocks like doors are supported in the generation. Experiments are conducted to evaluate the completeness and satisfaction of buildings generated via LLMs. It shows that LLMs hold significant potential for 3D building generation. Given appropriate prompts, LLMs can generate correct buildings in Minecraft with complete structures and incorporate specific building blocks such as windows and beds, meeting the specified requirements of human users.","sentences":["Recently, procedural content generation has exhibited considerable advancements in the domain of 2D game level generation such as Super Mario Bros. and Sokoban through large language models (LLMs).","To further validate the capabilities of LLMs, this paper explores how LLMs contribute to the generation of 3D buildings in a sandbox game, Minecraft.","We propose a Text to Building in Minecraft (T2BM) model, which involves refining prompts, decoding interlayer representation and repairing.","Facade, indoor scene and functional blocks like doors are supported in the generation.","Experiments are conducted to evaluate the completeness and satisfaction of buildings generated via LLMs.","It shows that LLMs hold significant potential for 3D building generation.","Given appropriate prompts, LLMs can generate correct buildings in Minecraft with complete structures and incorporate specific building blocks such as windows and beds, meeting the specified requirements of human users."],"url":"http://arxiv.org/abs/2406.08751v1","category":"cs.AI"}
{"created":"2024-06-13 02:12:18","title":"Learning in Feature Spaces via Coupled Covariances: Asymmetric Kernel SVD and Nystr\u00f6m method","abstract":"In contrast with Mercer kernel-based approaches as used e.g., in Kernel Principal Component Analysis (KPCA), it was previously shown that Singular Value Decomposition (SVD) inherently relates to asymmetric kernels and Asymmetric Kernel Singular Value Decomposition (KSVD) has been proposed. However, the existing formulation to KSVD cannot work with infinite-dimensional feature mappings, the variational objective can be unbounded, and needs further numerical evaluation and exploration towards machine learning. In this work, i) we introduce a new asymmetric learning paradigm based on coupled covariance eigenproblem (CCE) through covariance operators, allowing infinite-dimensional feature maps. The solution to CCE is ultimately obtained from the SVD of the induced asymmetric kernel matrix, providing links to KSVD. ii) Starting from the integral equations corresponding to a pair of coupled adjoint eigenfunctions, we formalize the asymmetric Nystr\\\"om method through a finite sample approximation to speed up training. iii) We provide the first empirical evaluations verifying the practical utility and benefits of KSVD and compare with methods resorting to symmetrization or linear SVD across multiple tasks.","sentences":["In contrast with Mercer kernel-based approaches as used e.g., in Kernel Principal Component Analysis (KPCA), it was previously shown that Singular Value Decomposition (SVD) inherently relates to asymmetric kernels and Asymmetric Kernel Singular Value Decomposition (KSVD) has been proposed.","However, the existing formulation to KSVD cannot work with infinite-dimensional feature mappings, the variational objective can be unbounded, and needs further numerical evaluation and exploration towards machine learning.","In this work, i) we introduce a new asymmetric learning paradigm based on coupled covariance eigenproblem (CCE) through covariance operators, allowing infinite-dimensional feature maps.","The solution to CCE is ultimately obtained from the SVD of the induced asymmetric kernel matrix, providing links to KSVD.","ii)","Starting from the integral equations corresponding to a pair of coupled adjoint eigenfunctions, we formalize the asymmetric Nystr\\\"om method through a finite sample approximation to speed up training.","iii)","We provide the first empirical evaluations verifying the practical utility and benefits of KSVD and compare with methods resorting to symmetrization or linear SVD across multiple tasks."],"url":"http://arxiv.org/abs/2406.08748v1","category":"cs.LG"}
{"created":"2024-06-13 02:07:06","title":"UruBots Autonomous Cars Team One Description Paper for FIRA 2024","abstract":"This document presents the design of an autonomous car developed by the UruBots team for the 2024 FIRA Autonomous Cars Race Challenge. The project involves creating an RC-car sized electric vehicle capable of navigating race tracks with in an autonomous manner. It integrates mechanical and electronic systems alongside artificial intelligence based algorithms for the navigation and real-time decision-making. The core of our project include the utilization of an AI-based algorithm to learn information from a camera and act in the robot to perform the navigation. We show that by creating a dataset with more than five thousand samples and a five-layered CNN we managed to achieve promissing performance we our proposed hardware setup. Overall, this paper aims to demonstrate the autonomous capabilities of our car, highlighting its readiness for the 2024 FIRA challenge, helping to contribute to the field of autonomous vehicle research.","sentences":["This document presents the design of an autonomous car developed by the UruBots team for the 2024 FIRA Autonomous Cars Race Challenge.","The project involves creating an RC-car sized electric vehicle capable of navigating race tracks with in an autonomous manner.","It integrates mechanical and electronic systems alongside artificial intelligence based algorithms for the navigation and real-time decision-making.","The core of our project include the utilization of an AI-based algorithm to learn information from a camera and act in the robot to perform the navigation.","We show that by creating a dataset with more than five thousand samples and a five-layered CNN we managed to achieve promissing performance we our proposed hardware setup.","Overall, this paper aims to demonstrate the autonomous capabilities of our car, highlighting its readiness for the 2024 FIRA challenge, helping to contribute to the field of autonomous vehicle research."],"url":"http://arxiv.org/abs/2406.08745v1","category":"cs.RO"}
{"created":"2024-06-13 01:36:55","title":"A Tangible Multi-Display Toolkit to Support the Collaborative Design Exploration of AV-Pedestrian Interfaces","abstract":"The advent of cyber-physical systems, such as robots and autonomous vehicles (AVs), brings new opportunities and challenges for the domain of interaction design. Though there is consensus about the value of human-centred development, there is a lack of documented tailored methods and tools for involving multiple stakeholders in design exploration processes. In this paper we present a novel approach using a tangible multi-display toolkit. Orchestrating computer-generated imagery across multiple displays, the toolkit enables multiple viewing angles and perspectives to be captured simultaneously (e.g. top-view, first-person pedestrian view). Participants are able to directly interact with the simulated environment through tangible objects. At the same time, the objects physically simulate the interface's behaviour (e.g. through an integrated LED display). We evaluated the toolkit in design sessions with experts to collect feedback and input on the design of an AV-pedestrian interface. The paper reports on how the combination of tangible objects and multiple displays supports collaborative design explorations.","sentences":["The advent of cyber-physical systems, such as robots and autonomous vehicles (AVs), brings new opportunities and challenges for the domain of interaction design.","Though there is consensus about the value of human-centred development, there is a lack of documented tailored methods and tools for involving multiple stakeholders in design exploration processes.","In this paper we present a novel approach using a tangible multi-display toolkit.","Orchestrating computer-generated imagery across multiple displays, the toolkit enables multiple viewing angles and perspectives to be captured simultaneously (e.g. top-view, first-person pedestrian view).","Participants are able to directly interact with the simulated environment through tangible objects.","At the same time, the objects physically simulate the interface's behaviour (e.g. through an integrated LED display).","We evaluated the toolkit in design sessions with experts to collect feedback and input on the design of an AV-pedestrian interface.","The paper reports on how the combination of tangible objects and multiple displays supports collaborative design explorations."],"url":"http://arxiv.org/abs/2406.08733v1","category":"cs.HC"}
{"created":"2024-06-13 00:59:55","title":"ECBD: Evidence-Centered Benchmark Design for NLP","abstract":"Benchmarking is seen as critical to assessing progress in NLP. However, creating a benchmark involves many design decisions (e.g., which datasets to include, which metrics to use) that often rely on tacit, untested assumptions about what the benchmark is intended to measure or is actually measuring. There is currently no principled way of analyzing these decisions and how they impact the validity of the benchmark's measurements. To address this gap, we draw on evidence-centered design in educational assessments and propose Evidence-Centered Benchmark Design (ECBD), a framework which formalizes the benchmark design process into five modules. ECBD specifies the role each module plays in helping practitioners collect evidence about capabilities of interest. Specifically, each module requires benchmark designers to describe, justify, and support benchmark design choices -- e.g., clearly specifying the capabilities the benchmark aims to measure or how evidence about those capabilities is collected from model responses. To demonstrate the use of ECBD, we conduct case studies with three benchmarks: BoolQ, SuperGLUE, and HELM. Our analysis reveals common trends in benchmark design and documentation that could threaten the validity of benchmarks' measurements.","sentences":["Benchmarking is seen as critical to assessing progress in NLP.","However, creating a benchmark involves many design decisions (e.g., which datasets to include, which metrics to use) that often rely on tacit, untested assumptions about what the benchmark is intended to measure or is actually measuring.","There is currently no principled way of analyzing these decisions and how they impact the validity of the benchmark's measurements.","To address this gap, we draw on evidence-centered design in educational assessments and propose Evidence-Centered Benchmark Design (ECBD), a framework which formalizes the benchmark design process into five modules.","ECBD specifies the role each module plays in helping practitioners collect evidence about capabilities of interest.","Specifically, each module requires benchmark designers to describe, justify, and support benchmark design choices -- e.g., clearly specifying the capabilities the benchmark aims to measure or how evidence about those capabilities is collected from model responses.","To demonstrate the use of ECBD, we conduct case studies with three benchmarks: BoolQ, SuperGLUE, and HELM.","Our analysis reveals common trends in benchmark design and documentation that could threaten the validity of benchmarks' measurements."],"url":"http://arxiv.org/abs/2406.08723v1","category":"cs.CL"}
{"created":"2024-06-13 00:33:29","title":"Batch-Instructed Gradient for Prompt Evolution:Systematic Prompt Optimization for Enhanced Text-to-Image Synthesis","abstract":"Text-to-image models have shown remarkable progress in generating high-quality images from user-provided prompts. Despite this, the quality of these images varies due to the models' sensitivity to human language nuances. With advancements in large language models, there are new opportunities to enhance prompt design for image generation tasks. Existing research primarily focuses on optimizing prompts for direct interaction, while less attention is given to scenarios involving intermediary agents, like the Stable Diffusion model. This study proposes a Multi-Agent framework to optimize input prompts for text-to-image generation models. Central to this framework is a prompt generation mechanism that refines initial queries using dynamic instructions, which evolve through iterative performance feedback. High-quality prompts are then fed into a state-of-the-art text-to-image model. A professional prompts database serves as a benchmark to guide the instruction modifier towards generating high-caliber prompts. A scoring system evaluates the generated images, and an LLM generates new instructions based on calculated gradients. This iterative process is managed by the Upper Confidence Bound (UCB) algorithm and assessed using the Human Preference Score version 2 (HPS v2). Preliminary ablation studies highlight the effectiveness of various system components and suggest areas for future improvements.","sentences":["Text-to-image models have shown remarkable progress in generating high-quality images from user-provided prompts.","Despite this, the quality of these images varies due to the models' sensitivity to human language nuances.","With advancements in large language models, there are new opportunities to enhance prompt design for image generation tasks.","Existing research primarily focuses on optimizing prompts for direct interaction, while less attention is given to scenarios involving intermediary agents, like the Stable Diffusion model.","This study proposes a Multi-Agent framework to optimize input prompts for text-to-image generation models.","Central to this framework is a prompt generation mechanism that refines initial queries using dynamic instructions, which evolve through iterative performance feedback.","High-quality prompts are then fed into a state-of-the-art text-to-image model.","A professional prompts database serves as a benchmark to guide the instruction modifier towards generating high-caliber prompts.","A scoring system evaluates the generated images, and an LLM generates new instructions based on calculated gradients.","This iterative process is managed by the Upper Confidence Bound (UCB) algorithm and assessed using the Human Preference Score version 2 (HPS v2).","Preliminary ablation studies highlight the effectiveness of various system components and suggest areas for future improvements."],"url":"http://arxiv.org/abs/2406.08713v1","category":"cs.AI"}
{"created":"2024-06-13 00:07:09","title":"Linear spectroscopy of collective modes and the gap structure in two-dimensional superconductors","abstract":"We consider optical response in multi-band, multi-layer two-dimensional superconductors. Within a simple model, we show that collective modes of the condensate, such as Leggett and clapping modes, can be detected in linear response. We show how trigonal warping of the superconducting order parameter can help facilitate detection of clapping modes. Taking rhombohedral trilayer graphene as an example, we consider several possible pairing mechanisms and show that all-electronic mechanisms may produce in-gap clapping modes. These modes, if present, should be detectable in the absorption of microwaves applied via gate electrodes; their detection would constitute strong evidence for unconventional pairing. Lastly, we show that absorption at frequencies above the superconducting gap $2|\\Delta|$ also contains a wealth of information about the gap structure. Our results suggest that linear spectroscopy can be a powerful tool for the characterization of unconventional two-dimensional superconductors.","sentences":["We consider optical response in multi-band, multi-layer two-dimensional superconductors.","Within a simple model, we show that collective modes of the condensate, such as Leggett and clapping modes, can be detected in linear response.","We show how trigonal warping of the superconducting order parameter can help facilitate detection of clapping modes.","Taking rhombohedral trilayer graphene as an example, we consider several possible pairing mechanisms and show that all-electronic mechanisms may produce in-gap clapping modes.","These modes, if present, should be detectable in the absorption of microwaves applied via gate electrodes; their detection would constitute strong evidence for unconventional pairing.","Lastly, we show that absorption at frequencies above the superconducting gap $2|\\Delta|$ also contains a wealth of information about the gap structure.","Our results suggest that linear spectroscopy can be a powerful tool for the characterization of unconventional two-dimensional superconductors."],"url":"http://arxiv.org/abs/2406.08706v1","category":"cond-mat.supr-con"}
{"created":"2024-06-13 00:00:20","title":"VLind-Bench: Measuring Language Priors in Large Vision-Language Models","abstract":"Large Vision-Language Models (LVLMs) have demonstrated outstanding performance across various multimodal tasks. However, they suffer from a problem known as language prior, where responses are generated based solely on textual patterns while disregarding image information. Addressing the issue of language prior is crucial, as it can lead to undesirable biases or hallucinations when dealing with images that are out of training distribution. Despite its importance, current methods for accurately measuring language priors in LVLMs are poorly studied. Although existing benchmarks based on counterfactual or out-of-distribution images can partially be used to measure language priors, they fail to disentangle language priors from other confounding factors. To this end, we propose a new benchmark called VLind-Bench, which is the first benchmark specifically designed to measure the language priors, or blindness, of LVLMs. It not only includes tests on counterfactual images to assess language priors but also involves a series of tests to evaluate more basic capabilities such as commonsense knowledge, visual perception, and commonsense biases. For each instance in our benchmark, we ensure that all these basic tests are passed before evaluating the language priors, thereby minimizing the influence of other factors on the assessment. The evaluation and analysis of recent LVLMs in our benchmark reveal that almost all models exhibit a significant reliance on language priors, presenting a strong challenge in the field.","sentences":["Large Vision-Language Models (LVLMs) have demonstrated outstanding performance across various multimodal tasks.","However, they suffer from a problem known as language prior, where responses are generated based solely on textual patterns while disregarding image information.","Addressing the issue of language prior is crucial, as it can lead to undesirable biases or hallucinations when dealing with images that are out of training distribution.","Despite its importance, current methods for accurately measuring language priors in LVLMs are poorly studied.","Although existing benchmarks based on counterfactual or out-of-distribution images can partially be used to measure language priors, they fail to disentangle language priors from other confounding factors.","To this end, we propose a new benchmark called VLind-Bench, which is the first benchmark specifically designed to measure the language priors, or blindness, of LVLMs.","It not only includes tests on counterfactual images to assess language priors but also involves a series of tests to evaluate more basic capabilities such as commonsense knowledge, visual perception, and commonsense biases.","For each instance in our benchmark, we ensure that all these basic tests are passed before evaluating the language priors, thereby minimizing the influence of other factors on the assessment.","The evaluation and analysis of recent LVLMs in our benchmark reveal that almost all models exhibit a significant reliance on language priors, presenting a strong challenge in the field."],"url":"http://arxiv.org/abs/2406.08702v1","category":"cs.AI"}
{"created":"2024-06-12 23:36:16","title":"Global AI Governance in Healthcare: A Cross-Jurisdictional Regulatory Analysis","abstract":"Artificial Intelligence (AI) is being adopted across the world and promises a new revolution in healthcare. While AI-enabled medical devices in North America dominate 42.3% of the global market, the use of AI-enabled medical devices in other countries is still a story waiting to be unfolded. We aim to delve deeper into global regulatory approaches towards AI use in healthcare, with a focus on how common themes are emerging globally. We compare these themes to the World Health Organization's (WHO) regulatory considerations and principles on ethical use of AI for healthcare applications. Our work seeks to take a global perspective on AI policy by analyzing 14 legal jurisdictions including countries representative of various regions in the world (North America, South America, South East Asia, Middle East, Africa, Australia, and the Asia-Pacific). Our eventual goal is to foster a global conversation on the ethical use of AI in healthcare and the regulations that will guide it. We propose solutions to promote international harmonization of AI regulations and examine the requirements for regulating generative AI, using China and Singapore as examples of countries with well-developed policies in this area.","sentences":["Artificial Intelligence (AI) is being adopted across the world and promises a new revolution in healthcare.","While AI-enabled medical devices in North America dominate 42.3% of the global market, the use of AI-enabled medical devices in other countries is still a story waiting to be unfolded.","We aim to delve deeper into global regulatory approaches towards AI use in healthcare, with a focus on how common themes are emerging globally.","We compare these themes to the World Health Organization's (WHO) regulatory considerations and principles on ethical use of AI for healthcare applications.","Our work seeks to take a global perspective on AI policy by analyzing 14 legal jurisdictions including countries representative of various regions in the world (North America, South America, South East Asia, Middle East, Africa, Australia, and the Asia-Pacific).","Our eventual goal is to foster a global conversation on the ethical use of AI in healthcare and the regulations that will guide it.","We propose solutions to promote international harmonization of AI regulations and examine the requirements for regulating generative AI, using China and Singapore as examples of countries with well-developed policies in this area."],"url":"http://arxiv.org/abs/2406.08695v1","category":"cs.CY"}
{"created":"2024-06-12 23:22:23","title":"UnO: Unsupervised Occupancy Fields for Perception and Forecasting","abstract":"Perceiving the world and forecasting its future state is a critical task for self-driving. Supervised approaches leverage annotated object labels to learn a model of the world -- traditionally with object detections and trajectory predictions, or temporal bird's-eye-view (BEV) occupancy fields. However, these annotations are expensive and typically limited to a set of predefined categories that do not cover everything we might encounter on the road. Instead, we learn to perceive and forecast a continuous 4D (spatio-temporal) occupancy field with self-supervision from LiDAR data. This unsupervised world model can be easily and effectively transferred to downstream tasks. We tackle point cloud forecasting by adding a lightweight learned renderer and achieve state-of-the-art performance in Argoverse 2, nuScenes, and KITTI. To further showcase its transferability, we fine-tune our model for BEV semantic occupancy forecasting and show that it outperforms the fully supervised state-of-the-art, especially when labeled data is scarce. Finally, when compared to prior state-of-the-art on spatio-temporal geometric occupancy prediction, our 4D world model achieves a much higher recall of objects from classes relevant to self-driving.","sentences":["Perceiving the world and forecasting its future state is a critical task for self-driving.","Supervised approaches leverage annotated object labels to learn a model of the world -- traditionally with object detections and trajectory predictions, or temporal bird's-eye-view (BEV) occupancy fields.","However, these annotations are expensive and typically limited to a set of predefined categories that do not cover everything we might encounter on the road.","Instead, we learn to perceive and forecast a continuous 4D (spatio-temporal) occupancy field with self-supervision from LiDAR data.","This unsupervised world model can be easily and effectively transferred to downstream tasks.","We tackle point cloud forecasting by adding a lightweight learned renderer and achieve state-of-the-art performance in Argoverse 2, nuScenes, and KITTI.","To further showcase its transferability, we fine-tune our model for BEV semantic occupancy forecasting and show that it outperforms the fully supervised state-of-the-art, especially when labeled data is scarce.","Finally, when compared to prior state-of-the-art on spatio-temporal geometric occupancy prediction, our 4D world model achieves a much higher recall of objects from classes relevant to self-driving."],"url":"http://arxiv.org/abs/2406.08691v1","category":"cs.CV"}
{"created":"2024-06-12 23:16:45","title":"Security of AI Agents","abstract":"The study and development of AI agents have been boosted by large language models. AI agents can function as intelligent assistants and complete tasks on behalf of their users with access to tools and the ability to execute commands in their environments, Through studying and experiencing the workflow of typical AI agents, we have raised several concerns regarding their security. These potential vulnerabilities are not addressed by the frameworks used to build the agents, nor by research aimed at improving the agents. In this paper, we identify and describe these vulnerabilities in detail from a system security perspective, emphasizing their causes and severe effects. Furthermore, we introduce defense mechanisms corresponding to each vulnerability with meticulous design and experiments to evaluate their viability. Altogether, this paper contextualizes the security issues in the current development of AI agents and delineates methods to make AI agents safer and more reliable.","sentences":["The study and development of AI agents have been boosted by large language models.","AI agents can function as intelligent assistants and complete tasks on behalf of their users with access to tools and the ability to execute commands in their environments, Through studying and experiencing the workflow of typical AI agents, we have raised several concerns regarding their security.","These potential vulnerabilities are not addressed by the frameworks used to build the agents, nor by research aimed at improving the agents.","In this paper, we identify and describe these vulnerabilities in detail from a system security perspective, emphasizing their causes and severe effects.","Furthermore, we introduce defense mechanisms corresponding to each vulnerability with meticulous design and experiments to evaluate their viability.","Altogether, this paper contextualizes the security issues in the current development of AI agents and delineates methods to make AI agents safer and more reliable."],"url":"http://arxiv.org/abs/2406.08689v1","category":"cs.CR"}
{"created":"2024-06-12 23:04:13","title":"On Security Weaknesses and Vulnerabilities in Deep Learning Systems","abstract":"The security guarantee of AI-enabled software systems (particularly using deep learning techniques as a functional core) is pivotal against the adversarial attacks exploiting software vulnerabilities. However, little attention has been paid to a systematic investigation of vulnerabilities in such systems. A common situation learned from the open source software community is that deep learning engineers frequently integrate off-the-shelf or open-source learning frameworks into their ecosystems. In this work, we specifically look into deep learning (DL) framework and perform the first systematic study of vulnerabilities in DL systems through a comprehensive analysis of identified vulnerabilities from Common Vulnerabilities and Exposures (CVE) and open-source DL tools, including TensorFlow, Caffe, OpenCV, Keras, and PyTorch. We propose a two-stream data analysis framework to explore vulnerability patterns from various databases. We investigate the unique DL frameworks and libraries development ecosystems that appear to be decentralized and fragmented. By revisiting the Common Weakness Enumeration (CWE) List, which provides the traditional software vulnerability related practices, we observed that it is more challenging to detect and fix the vulnerabilities throughout the DL systems lifecycle. Moreover, we conducted a large-scale empirical study of 3,049 DL vulnerabilities to better understand the patterns of vulnerability and the challenges in fixing them. We have released the full replication package at https://github.com/codelzz/Vulnerabilities4DLSystem. We anticipate that our study can advance the development of secure DL systems.","sentences":["The security guarantee of AI-enabled software systems (particularly using deep learning techniques as a functional core) is pivotal against the adversarial attacks exploiting software vulnerabilities.","However, little attention has been paid to a systematic investigation of vulnerabilities in such systems.","A common situation learned from the open source software community is that deep learning engineers frequently integrate off-the-shelf or open-source learning frameworks into their ecosystems.","In this work, we specifically look into deep learning (DL) framework and perform the first systematic study of vulnerabilities in DL systems through a comprehensive analysis of identified vulnerabilities from Common Vulnerabilities and Exposures (CVE) and open-source DL tools, including TensorFlow, Caffe, OpenCV, Keras, and PyTorch.","We propose a two-stream data analysis framework to explore vulnerability patterns from various databases.","We investigate the unique DL frameworks and libraries development ecosystems that appear to be decentralized and fragmented.","By revisiting the Common Weakness Enumeration (CWE) List, which provides the traditional software vulnerability related practices, we observed that it is more challenging to detect and fix the vulnerabilities throughout the DL systems lifecycle.","Moreover, we conducted a large-scale empirical study of 3,049 DL vulnerabilities to better understand the patterns of vulnerability and the challenges in fixing them.","We have released the full replication package at https://github.com/codelzz/Vulnerabilities4DLSystem.","We anticipate that our study can advance the development of secure DL systems."],"url":"http://arxiv.org/abs/2406.08688v1","category":"cs.SE"}
{"created":"2024-06-12 23:00:59","title":"AlphaZeroES: Direct score maximization outperforms planning loss minimization","abstract":"Planning at execution time has been shown to dramatically improve performance for agents in both single-agent and multi-agent settings. A well-known family of approaches to planning at execution time are AlphaZero and its variants, which use Monte Carlo Tree Search together with a neural network that guides the search by predicting state values and action probabilities. AlphaZero trains these networks by minimizing a planning loss that makes the value prediction match the episode return, and the policy prediction at the root of the search tree match the output of the full tree expansion. AlphaZero has been applied to both single-agent environments (such as Sokoban) and multi-agent environments (such as chess and Go) with great success. In this paper, we explore an intriguing question: In single-agent environments, can we outperform AlphaZero by directly maximizing the episode score instead of minimizing this planning loss, while leaving the MCTS algorithm and neural architecture unchanged? To directly maximize the episode score, we use evolution strategies, a family of algorithms for zeroth-order blackbox optimization. Our experiments indicate that, across multiple environments, directly maximizing the episode score outperforms minimizing the planning loss.","sentences":["Planning at execution time has been shown to dramatically improve performance for agents in both single-agent and multi-agent settings.","A well-known family of approaches to planning at execution time are AlphaZero and its variants, which use Monte Carlo Tree Search together with a neural network that guides the search by predicting state values and action probabilities.","AlphaZero trains these networks by minimizing a planning loss that makes the value prediction match the episode return, and the policy prediction at the root of the search tree match the output of the full tree expansion.","AlphaZero has been applied to both single-agent environments (such as Sokoban) and multi-agent environments (such as chess and Go) with great success.","In this paper, we explore an intriguing question: In single-agent environments, can we outperform AlphaZero by directly maximizing the episode score instead of minimizing this planning loss, while leaving the MCTS algorithm and neural architecture unchanged?","To directly maximize the episode score, we use evolution strategies, a family of algorithms for zeroth-order blackbox optimization.","Our experiments indicate that, across multiple environments, directly maximizing the episode score outperforms minimizing the planning loss."],"url":"http://arxiv.org/abs/2406.08687v1","category":"cs.AI"}
{"created":"2024-06-12 22:28:08","title":"HelpSteer2: Open-source dataset for training top-performing reward models","abstract":"High-quality preference datasets are essential for training reward models that can effectively guide large language models (LLMs) in generating high-quality responses aligned with human preferences. As LLMs become stronger and better aligned, permissively licensed preference datasets, such as Open Assistant, HH-RLHF, and HelpSteer need to be updated to remain effective for reward modeling. Methods that distil preference data from proprietary LLMs such as GPT-4 have restrictions on commercial usage imposed by model providers. To improve upon both generated responses and attribute labeling quality, we release HelpSteer2, a permissively licensed preference dataset (CC-BY-4.0). Using a powerful internal base model trained on HelpSteer2, we are able to achieve the SOTA score (92.0%) on Reward-Bench's primary dataset, outperforming currently listed open and proprietary models, as of June 12th, 2024. Notably, HelpSteer2 consists of only ten thousand response pairs, an order of magnitude fewer than existing preference datasets (e.g., HH-RLHF), which makes it highly efficient for training reward models. Our extensive experiments demonstrate that reward models trained with HelpSteer2 are effective in aligning LLMs. In particular, we propose SteerLM 2.0, a model alignment approach that can effectively make use of the rich multi-attribute score predicted by our reward models. HelpSteer2 is available at https://huggingface.co/datasets/nvidia/HelpSteer2 and code is available at https://github.com/NVIDIA/NeMo-Aligner","sentences":["High-quality preference datasets are essential for training reward models that can effectively guide large language models (LLMs) in generating high-quality responses aligned with human preferences.","As LLMs become stronger and better aligned, permissively licensed preference datasets, such as Open Assistant, HH-RLHF, and HelpSteer need to be updated to remain effective for reward modeling.","Methods that distil preference data from proprietary LLMs such as GPT-4 have restrictions on commercial usage imposed by model providers.","To improve upon both generated responses and attribute labeling quality, we release HelpSteer2, a permissively licensed preference dataset (CC-BY-4.0).","Using a powerful internal base model trained on HelpSteer2, we are able to achieve the SOTA score (92.0%) on Reward-Bench's primary dataset, outperforming currently listed open and proprietary models, as of June 12th, 2024.","Notably, HelpSteer2 consists of only ten thousand response pairs, an order of magnitude fewer than existing preference datasets (e.g., HH-RLHF), which makes it highly efficient for training reward models.","Our extensive experiments demonstrate that reward models trained with HelpSteer2 are effective in aligning LLMs.","In particular, we propose SteerLM 2.0, a model alignment approach that can effectively make use of the rich multi-attribute score predicted by our reward models.","HelpSteer2 is available at https://huggingface.co/datasets/nvidia/HelpSteer2 and code is available at https://github.com/NVIDIA/NeMo-Aligner"],"url":"http://arxiv.org/abs/2406.08673v1","category":"cs.CL"}
{"created":"2024-06-12 22:14:45","title":"Scaling Analysis of the Swirling Wake of a Porous Disc: Application to Wind Turbines","abstract":"We report a comprehensive study of the wake of a porous disc, the design of which has been modified to incorporate a swirling motion at an inexpensive cost. The swirl intensity is passively controlled by varying the internal disc geometry, i.e. the pitch angle of the blades. A swirl number is introduced to characterise the competition between the linear (drag) and the azimuthal (swirl) momentums on the wake recovery. Assuming that swirl dominates the near wake and non-equilibrium turbulence theory applies, new scaling laws of the mean wake properties are derived. To assess these theoretical predictions, an in-depth analysis of the aerodynamics of these original porous discs has been conducted experimentally. It is found that at the early stage of wake recovery, the swirling motion induces a low-pressure core, which controls the mean velocity deficit properties. The measurements collected in the swirling wake of the porous discs support the new scaling laws proposed in this work. Finally, it is shown that, as far as swirl is injected in the wake, the characteristics of the mean velocity deficit profiles match very well those of both lab-scale and real-scale wind turbine data extracted from the literature. Overall, our results emphasise that by setting the initial conditions of the wake recovery, swirl is a key ingredient to be taken into account in order to faithfully replicate the mean wake of wind turbines.","sentences":["We report a comprehensive study of the wake of a porous disc, the design of which has been modified to incorporate a swirling motion at an inexpensive cost.","The swirl intensity is passively controlled by varying the internal disc geometry, i.e. the pitch angle of the blades.","A swirl number is introduced to characterise the competition between the linear (drag) and the azimuthal (swirl) momentums on the wake recovery.","Assuming that swirl dominates the near wake and non-equilibrium turbulence theory applies, new scaling laws of the mean wake properties are derived.","To assess these theoretical predictions, an in-depth analysis of the aerodynamics of these original porous discs has been conducted experimentally.","It is found that at the early stage of wake recovery, the swirling motion induces a low-pressure core, which controls the mean velocity deficit properties.","The measurements collected in the swirling wake of the porous discs support the new scaling laws proposed in this work.","Finally, it is shown that, as far as swirl is injected in the wake, the characteristics of the mean velocity deficit profiles match very well those of both lab-scale and real-scale wind turbine data extracted from the literature.","Overall, our results emphasise that by setting the initial conditions of the wake recovery, swirl is a key ingredient to be taken into account in order to faithfully replicate the mean wake of wind turbines."],"url":"http://arxiv.org/abs/2406.08667v1","category":"physics.flu-dyn"}
{"created":"2024-06-12 22:12:03","title":"Interventional Causal Discovery in a Mixture of DAGs","abstract":"Causal interactions among a group of variables are often modeled by a single causal graph. In some domains, however, these interactions are best described by multiple co-existing causal graphs, e.g., in dynamical systems or genomics. This paper addresses the hitherto unknown role of interventions in learning causal interactions among variables governed by a mixture of causal systems, each modeled by one directed acyclic graph (DAG). Causal discovery from mixtures is fundamentally more challenging than single-DAG causal discovery. Two major difficulties stem from (i) inherent uncertainty about the skeletons of the component DAGs that constitute the mixture and (ii) possibly cyclic relationships across these component DAGs. This paper addresses these challenges and aims to identify edges that exist in at least one component DAG of the mixture, referred to as true edges. First, it establishes matching necessary and sufficient conditions on the size of interventions required to identify the true edges. Next, guided by the necessity results, an adaptive algorithm is designed that learns all true edges using ${\\cal O}(n^2)$ interventions, where $n$ is the number of nodes. Remarkably, the size of the interventions is optimal if the underlying mixture model does not contain cycles across its components. More generally, the gap between the intervention size used by the algorithm and the optimal size is quantified. It is shown to be bounded by the cyclic complexity number of the mixture model, defined as the size of the minimal intervention that can break the cycles in the mixture, which is upper bounded by the number of cycles among the ancestors of a node.","sentences":["Causal interactions among a group of variables are often modeled by a single causal graph.","In some domains, however, these interactions are best described by multiple co-existing causal graphs, e.g., in dynamical systems or genomics.","This paper addresses the hitherto unknown role of interventions in learning causal interactions among variables governed by a mixture of causal systems, each modeled by one directed acyclic graph (DAG).","Causal discovery from mixtures is fundamentally more challenging than single-DAG causal discovery.","Two major difficulties stem from (i) inherent uncertainty about the skeletons of the component DAGs that constitute the mixture and (ii) possibly cyclic relationships across these component DAGs.","This paper addresses these challenges and aims to identify edges that exist in at least one component DAG of the mixture, referred to as true edges.","First, it establishes matching necessary and sufficient conditions on the size of interventions required to identify the true edges.","Next, guided by the necessity results, an adaptive algorithm is designed that learns all true edges using ${\\cal O}(n^2)$ interventions, where $n$ is the number of nodes.","Remarkably, the size of the interventions is optimal if the underlying mixture model does not contain cycles across its components.","More generally, the gap between the intervention size used by the algorithm and the optimal size is quantified.","It is shown to be bounded by the cyclic complexity number of the mixture model, defined as the size of the minimal intervention that can break the cycles in the mixture, which is upper bounded by the number of cycles among the ancestors of a node."],"url":"http://arxiv.org/abs/2406.08666v1","category":"cs.LG"}
{"created":"2024-06-12 22:09:27","title":"Exploring Fuzzing as Data Augmentation for Neural Test Generation","abstract":"Testing is an essential part of modern software engineering to build reliable programs. As testing the software is important but expensive, automatic test case generation methods have become popular in software development. Unlike traditional search-based coverage-guided test generation like fuzzing, neural test generation backed by large language models can write tests that are semantically meaningful and can be understood by other maintainers. However, compared to regular code corpus, unit tests in the datasets are limited in amount and diversity. In this paper, we present a novel data augmentation technique **FuzzAug**, that combines the advantages of fuzzing and large language models. FuzzAug not only keeps valid program semantics in the augmented data, but also provides more diverse inputs to the function under test, helping the model to associate correct inputs embedded with the function's dynamic behaviors with the function under test. We evaluate FuzzAug's benefits by using it on a neural test generation dataset to train state-of-the-art code generation models. By augmenting the training set, our model generates test cases with $11\\%$ accuracy increases. Models trained with FuzzAug generate unit test functions with double the branch coverage compared to those without it. FuzzAug can be used across various datasets to train advanced code generation models, enhancing their utility in automated software testing. Our work shows the benefits of using dynamic analysis results to enhance neural test generation. Code and data will be publicly available.","sentences":["Testing is an essential part of modern software engineering to build reliable programs.","As testing the software is important but expensive, automatic test case generation methods have become popular in software development.","Unlike traditional search-based coverage-guided test generation like fuzzing, neural test generation backed by large language models can write tests that are semantically meaningful and can be understood by other maintainers.","However, compared to regular code corpus, unit tests in the datasets are limited in amount and diversity.","In this paper, we present a novel data augmentation technique **FuzzAug**, that combines the advantages of fuzzing and large language models.","FuzzAug not only keeps valid program semantics in the augmented data, but also provides more diverse inputs to the function under test, helping the model to associate correct inputs embedded with the function's dynamic behaviors with the function under test.","We evaluate FuzzAug's benefits by using it on a neural test generation dataset to train state-of-the-art code generation models.","By augmenting the training set, our model generates test cases with $11\\%$ accuracy increases.","Models trained with FuzzAug generate unit test functions with double the branch coverage compared to those without it.","FuzzAug can be used across various datasets to train advanced code generation models, enhancing their utility in automated software testing.","Our work shows the benefits of using dynamic analysis results to enhance neural test generation.","Code and data will be publicly available."],"url":"http://arxiv.org/abs/2406.08665v1","category":"cs.SE"}
{"created":"2024-06-12 21:46:13","title":"Fine-Tuned 'Small' LLMs (Still) Significantly Outperform Zero-Shot Generative AI Models in Text Classification","abstract":"Generative AI offers a simple, prompt-based alternative to fine-tuning smaller BERT-style LLMs for text classification tasks. This promises to eliminate the need for manually labeled training data and task-specific model training. However, it remains an open question whether tools like ChatGPT can deliver on this promise. In this paper, we show that smaller, fine-tuned LLMs (still) consistently and significantly outperform larger, zero-shot prompted models in text classification. We compare three major generative AI models (ChatGPT with GPT-3.5/GPT-4 and Claude Opus) with several fine-tuned LLMs across a diverse set of classification tasks (sentiment, approval/disapproval, emotions, party positions) and text categories (news, tweets, speeches). We find that fine-tuning with application-specific training data achieves superior performance in all cases. To make this approach more accessible to a broader audience, we provide an easy-to-use toolkit alongside this paper. Our toolkit, accompanied by non-technical step-by-step guidance, enables users to select and fine-tune BERT-like LLMs for any classification task with minimal technical and computational effort.","sentences":["Generative AI offers a simple, prompt-based alternative to fine-tuning smaller BERT-style LLMs for text classification tasks.","This promises to eliminate the need for manually labeled training data and task-specific model training.","However, it remains an open question whether tools like ChatGPT can deliver on this promise.","In this paper, we show that smaller, fine-tuned LLMs (still) consistently and significantly outperform larger, zero-shot prompted models in text classification.","We compare three major generative AI models (ChatGPT with GPT-3.5/GPT-4 and Claude Opus) with several fine-tuned LLMs across a diverse set of classification tasks (sentiment, approval/disapproval, emotions, party positions) and text categories (news, tweets, speeches).","We find that fine-tuning with application-specific training data achieves superior performance in all cases.","To make this approach more accessible to a broader audience, we provide an easy-to-use toolkit alongside this paper.","Our toolkit, accompanied by non-technical step-by-step guidance, enables users to select and fine-tune BERT-like LLMs for any classification task with minimal technical and computational effort."],"url":"http://arxiv.org/abs/2406.08660v1","category":"cs.CL"}
{"created":"2024-06-12 21:41:32","title":"TC-Bench: Benchmarking Temporal Compositionality in Text-to-Video and Image-to-Video Generation","abstract":"Video generation has many unique challenges beyond those of image generation. The temporal dimension introduces extensive possible variations across frames, over which consistency and continuity may be violated. In this study, we move beyond evaluating simple actions and argue that generated videos should incorporate the emergence of new concepts and their relation transitions like in real-world videos as time progresses. To assess the Temporal Compositionality of video generation models, we propose TC-Bench, a benchmark of meticulously crafted text prompts, corresponding ground truth videos, and robust evaluation metrics. The prompts articulate the initial and final states of scenes, effectively reducing ambiguities for frame development and simplifying the assessment of transition completion. In addition, by collecting aligned real-world videos corresponding to the prompts, we expand TC-Bench's applicability from text-conditional models to image-conditional ones that can perform generative frame interpolation. We also develop new metrics to measure the completeness of component transitions in generated videos, which demonstrate significantly higher correlations with human judgments than existing metrics. Our comprehensive experimental results reveal that most video generators achieve less than 20% of the compositional changes, highlighting enormous space for future improvement. Our analysis indicates that current video generation models struggle to interpret descriptions of compositional changes and synthesize various components across different time steps.","sentences":["Video generation has many unique challenges beyond those of image generation.","The temporal dimension introduces extensive possible variations across frames, over which consistency and continuity may be violated.","In this study, we move beyond evaluating simple actions and argue that generated videos should incorporate the emergence of new concepts and their relation transitions like in real-world videos as time progresses.","To assess the Temporal Compositionality of video generation models, we propose TC-Bench, a benchmark of meticulously crafted text prompts, corresponding ground truth videos, and robust evaluation metrics.","The prompts articulate the initial and final states of scenes, effectively reducing ambiguities for frame development and simplifying the assessment of transition completion.","In addition, by collecting aligned real-world videos corresponding to the prompts, we expand TC-Bench's applicability from text-conditional models to image-conditional ones that can perform generative frame interpolation.","We also develop new metrics to measure the completeness of component transitions in generated videos, which demonstrate significantly higher correlations with human judgments than existing metrics.","Our comprehensive experimental results reveal that most video generators achieve less than 20% of the compositional changes, highlighting enormous space for future improvement.","Our analysis indicates that current video generation models struggle to interpret descriptions of compositional changes and synthesize various components across different time steps."],"url":"http://arxiv.org/abs/2406.08656v1","category":"cs.CV"}
{"created":"2024-06-12 21:23:27","title":"How to Distinguish AI-Generated Images from Authentic Photographs","abstract":"The high level of photorealism in state-of-the-art diffusion models like Midjourney, Stable Diffusion, and Firefly makes it difficult for untrained humans to distinguish between real photographs and AI-generated images. To address this problem, we designed a guide to help readers develop a more critical eye toward identifying artifacts, inconsistencies, and implausibilities that often appear in AI-generated images. The guide is organized into five categories of artifacts and implausibilities: anatomical, stylistic, functional, violations of physics, and sociocultural. For this guide, we generated 138 images with diffusion models, curated 9 images from social media, and curated 42 real photographs. These images showcase the kinds of cues that prompt suspicion towards the possibility an image is AI-generated and why it is often difficult to draw conclusions about an image's provenance without any context beyond the pixels in an image. Human-perceptible artifacts are not always present in AI-generated images, but this guide reveals artifacts and implausibilities that often emerge. By drawing attention to these kinds of artifacts and implausibilities, we aim to better equip people to distinguish AI-generated images from real photographs in the future.","sentences":["The high level of photorealism in state-of-the-art diffusion models like Midjourney, Stable Diffusion, and Firefly makes it difficult for untrained humans to distinguish between real photographs and AI-generated images.","To address this problem, we designed a guide to help readers develop a more critical eye toward identifying artifacts, inconsistencies, and implausibilities that often appear in AI-generated images.","The guide is organized into five categories of artifacts and implausibilities: anatomical, stylistic, functional, violations of physics, and sociocultural.","For this guide, we generated 138 images with diffusion models, curated 9 images from social media, and curated 42 real photographs.","These images showcase the kinds of cues that prompt suspicion towards the possibility an image is AI-generated and why it is often difficult to draw conclusions about an image's provenance without any context beyond the pixels in an image.","Human-perceptible artifacts are not always present in AI-generated images, but this guide reveals artifacts and implausibilities that often emerge.","By drawing attention to these kinds of artifacts and implausibilities, we aim to better equip people to distinguish AI-generated images from real photographs in the future."],"url":"http://arxiv.org/abs/2406.08651v1","category":"cs.HC"}
{"created":"2024-06-12 21:08:12","title":"Toward Fully-End-to-End Listened Speech Decoding from EEG Signals","abstract":"Speech decoding from EEG signals is a challenging task, where brain activity is modeled to estimate salient characteristics of acoustic stimuli. We propose FESDE, a novel framework for Fully-End-to-end Speech Decoding from EEG signals. Our approach aims to directly reconstruct listened speech waveforms given EEG signals, where no intermediate acoustic feature processing step is required. The proposed method consists of an EEG module and a speech module along with a connector. The EEG module learns to better represent EEG signals, while the speech module generates speech waveforms from model representations. The connector learns to bridge the distributions of the latent spaces of EEG and speech. The proposed framework is both simple and efficient, by allowing single-step inference, and outperforms prior works on objective metrics. A fine-grained phoneme analysis is conducted to unveil model characteristics of speech decoding. The source code is available here: github.com/lee-jhwn/fesde.","sentences":["Speech decoding from EEG signals is a challenging task, where brain activity is modeled to estimate salient characteristics of acoustic stimuli.","We propose FESDE, a novel framework for Fully-End-to-end Speech Decoding from EEG signals.","Our approach aims to directly reconstruct listened speech waveforms given EEG signals, where no intermediate acoustic feature processing step is required.","The proposed method consists of an EEG module and a speech module along with a connector.","The EEG module learns to better represent EEG signals, while the speech module generates speech waveforms from model representations.","The connector learns to bridge the distributions of the latent spaces of EEG and speech.","The proposed framework is both simple and efficient, by allowing single-step inference, and outperforms prior works on objective metrics.","A fine-grained phoneme analysis is conducted to unveil model characteristics of speech decoding.","The source code is available here: github.com/lee-jhwn/fesde."],"url":"http://arxiv.org/abs/2406.08644v1","category":"eess.SP"}
{"created":"2024-06-12 20:50:26","title":"A Game Between Two Identical Dubins Cars: Evading a Conic Sensor in Minimum Time","abstract":"A fundamental task in mobile robotics is keeping an intelligent agent under surveillance with an autonomous robot as it travels in the environment. This work studies a version of that problem involving one of the most popular vehicle platforms in robotics. In particular, we consider two identical Dubins cars moving on a plane without obstacles. One of them plays as the pursuer, and it is equipped with a limited field-of-view detection region modeled as a semi-infinite cone with its apex at the pursuer's position. The pursuer aims to maintain the other Dubins car, which plays as the evader, as much time as possible inside its detection region. On the contrary, the evader wants to escape as soon as possible. In this work, employing differential game theory, we find the time-optimal motion strategies near the game's end. The analysis of those trajectories reveals the existence of at least two singular surfaces: a Transition Surface and an Evader's Universal Surface. We also found that the barrier's standard construction produces a surface that partially lies outside the playing space and fails to define a closed region, implying that an additional procedure is required to determine all configurations where the evader escapes.","sentences":["A fundamental task in mobile robotics is keeping an intelligent agent under surveillance with an autonomous robot as it travels in the environment.","This work studies a version of that problem involving one of the most popular vehicle platforms in robotics.","In particular, we consider two identical Dubins cars moving on a plane without obstacles.","One of them plays as the pursuer, and it is equipped with a limited field-of-view detection region modeled as a semi-infinite cone with its apex at the pursuer's position.","The pursuer aims to maintain the other Dubins car, which plays as the evader, as much time as possible inside its detection region.","On the contrary, the evader wants to escape as soon as possible.","In this work, employing differential game theory, we find the time-optimal motion strategies near the game's end.","The analysis of those trajectories reveals the existence of at least two singular surfaces: a Transition Surface and an Evader's Universal Surface.","We also found that the barrier's standard construction produces a surface that partially lies outside the playing space and fails to define a closed region, implying that an additional procedure is required to determine all configurations where the evader escapes."],"url":"http://arxiv.org/abs/2406.08637v1","category":"cs.RO"}
{"created":"2024-06-12 20:29:14","title":"Coupled Ocean-Atmosphere Dynamics in a Machine Learning Earth System Model","abstract":"Seasonal climate forecasts are socioeconomically important for managing the impacts of extreme weather events and for planning in sectors like agriculture and energy. Climate predictability on seasonal timescales is tied to boundary effects of the ocean on the atmosphere and coupled interactions in the ocean-atmosphere system. We present the Ocean-linked-atmosphere (Ola) model, a high-resolution (0.25{\\deg}) Artificial Intelligence/ Machine Learning (AI/ML) coupled earth-system model which separately models the ocean and atmosphere dynamics using an autoregressive Spherical Fourier Neural Operator architecture, with a view towards enabling fast, accurate, large ensemble forecasts on the seasonal timescale. We find that Ola exhibits learned characteristics of ocean-atmosphere coupled dynamics including tropical oceanic waves with appropriate phase speeds, and an internally generated El Ni\\~no/Southern Oscillation (ENSO) having realistic amplitude, geographic structure, and vertical structure within the ocean mixed layer. We present initial evidence of skill in forecasting the ENSO which compares favorably to the SPEAR model of the Geophysical Fluid Dynamics Laboratory.","sentences":["Seasonal climate forecasts are socioeconomically important for managing the impacts of extreme weather events and for planning in sectors like agriculture and energy.","Climate predictability on seasonal timescales is tied to boundary effects of the ocean on the atmosphere and coupled interactions in the ocean-atmosphere system.","We present the Ocean-linked-atmosphere (Ola) model, a high-resolution (0.25{\\deg})","Artificial Intelligence/ Machine Learning (AI/ML) coupled earth-system model which separately models the ocean and atmosphere dynamics using an autoregressive Spherical Fourier Neural Operator architecture, with a view towards enabling fast, accurate, large ensemble forecasts on the seasonal timescale.","We find that Ola exhibits learned characteristics of ocean-atmosphere coupled dynamics including tropical oceanic waves with appropriate phase speeds, and an internally generated El Ni\\~no/Southern Oscillation (ENSO) having realistic amplitude, geographic structure, and vertical structure within the ocean mixed layer.","We present initial evidence of skill in forecasting the ENSO which compares favorably to the SPEAR model of the Geophysical Fluid Dynamics Laboratory."],"url":"http://arxiv.org/abs/2406.08632v1","category":"physics.ao-ph"}
{"created":"2024-06-12 20:12:29","title":"Emotion Manipulation Through Music -- A Deep Learning Interactive Visual Approach","abstract":"Music evokes emotion in many people. We introduce a novel way to manipulate the emotional content of a song using AI tools. Our goal is to achieve the desired emotion while leaving the original melody as intact as possible. For this, we create an interactive pipeline capable of shifting an input song into a diametrically opposed emotion and visualize this result through Russel's Circumplex model. Our approach is a proof-of-concept for Semantic Manipulation of Music, a novel field aimed at modifying the emotional content of existing music. We design a deep learning model able to assess the accuracy of our modifications to key, SoundFont instrumentation, and other musical features. The accuracy of our model is in-line with the current state of the art techniques on the 4Q Emotion dataset. With further refinement, this research may contribute to on-demand custom music generation, the automated remixing of existing work, and music playlists tuned for emotional progression.","sentences":["Music evokes emotion in many people.","We introduce a novel way to manipulate the emotional content of a song using AI tools.","Our goal is to achieve the desired emotion while leaving the original melody as intact as possible.","For this, we create an interactive pipeline capable of shifting an input song into a diametrically opposed emotion and visualize this result through Russel's Circumplex model.","Our approach is a proof-of-concept for Semantic Manipulation of Music, a novel field aimed at modifying the emotional content of existing music.","We design a deep learning model able to assess the accuracy of our modifications to key, SoundFont instrumentation, and other musical features.","The accuracy of our model is in-line with the current state of the art techniques on the 4Q Emotion dataset.","With further refinement, this research may contribute to on-demand custom music generation, the automated remixing of existing work, and music playlists tuned for emotional progression."],"url":"http://arxiv.org/abs/2406.08623v1","category":"cs.SD"}
{"created":"2024-06-12 19:52:39","title":"Enhancing Path Selections with Interference Graphs in Multihop Relay Wireless Networks","abstract":"The multihop relay wireless networks have gained traction due to the emergence of Reconfigurable Intelligent Surfaces (RISs) which can be used as relays in high frequency range wireless network, including THz or mmWave. To select paths in these networks, the transmission performance plays the key network in these networks. In this paper, we enhance and greatly simplify the path selection in multihop relay RIS enabled wireless networks with what we refer to as interference graphs. Interference graphs are created based on SNR model, conical and cylindrical beam shapes in the transmission and the related interference model. Once created, they can be simply and efficiently used to select valid paths, without overestimation of the effect of interference. The results show that decreased ordering of conflict selections in the graphs yields the best results, as compared to conservative approach that tolerates no interference.","sentences":["The multihop relay wireless networks have gained traction due to the emergence of Reconfigurable Intelligent Surfaces (RISs) which can be used as relays in high frequency range wireless network, including THz or mmWave.","To select paths in these networks, the transmission performance plays the key network in these networks.","In this paper, we enhance and greatly simplify the path selection in multihop relay RIS enabled wireless networks with what we refer to as interference graphs.","Interference graphs are created based on SNR model, conical and cylindrical beam shapes in the transmission and the related interference model.","Once created, they can be simply and efficiently used to select valid paths, without overestimation of the effect of interference.","The results show that decreased ordering of conflict selections in the graphs yields the best results, as compared to conservative approach that tolerates no interference."],"url":"http://arxiv.org/abs/2406.08616v1","category":"cs.NI"}
{"created":"2024-06-12 19:41:01","title":"LayeredDoc: Domain Adaptive Document Restoration with a Layer Separation Approach","abstract":"The rapid evolution of intelligent document processing systems demands robust solutions that adapt to diverse domains without extensive retraining. Traditional methods often falter with variable document types, leading to poor performance. To overcome these limitations, this paper introduces a text-graphic layer separation approach that enhances domain adaptability in document image restoration (DIR) systems. We propose LayeredDoc, which utilizes two layers of information: the first targets coarse-grained graphic components, while the second refines machine-printed textual content. This hierarchical DIR framework dynamically adjusts to the characteristics of the input document, facilitating effective domain adaptation. We evaluated our approach both qualitatively and quantitatively using a new real-world dataset, LayeredDocDB, developed for this study. Initially trained on a synthetically generated dataset, our model demonstrates strong generalization capabilities for the DIR task, offering a promising solution for handling variability in real-world data. Our code is accessible on GitHub.","sentences":["The rapid evolution of intelligent document processing systems demands robust solutions that adapt to diverse domains without extensive retraining.","Traditional methods often falter with variable document types, leading to poor performance.","To overcome these limitations, this paper introduces a text-graphic layer separation approach that enhances domain adaptability in document image restoration (DIR) systems.","We propose LayeredDoc, which utilizes two layers of information: the first targets coarse-grained graphic components, while the second refines machine-printed textual content.","This hierarchical DIR framework dynamically adjusts to the characteristics of the input document, facilitating effective domain adaptation.","We evaluated our approach both qualitatively and quantitatively using a new real-world dataset, LayeredDocDB, developed for this study.","Initially trained on a synthetically generated dataset, our model demonstrates strong generalization capabilities for the DIR task, offering a promising solution for handling variability in real-world data.","Our code is accessible on GitHub."],"url":"http://arxiv.org/abs/2406.08610v1","category":"cs.CV"}
{"created":"2024-06-12 19:26:35","title":"Reversing the Forget-Retain Objectives: An Efficient LLM Unlearning Framework from Logit Difference","abstract":"As Large Language Models (LLMs) demonstrate extensive capability in learning from documents, LLM unlearning becomes an increasingly important research area to address concerns of LLMs in terms of privacy, copyright, etc. A conventional LLM unlearning task typically involves two goals: (1) The target LLM should forget the knowledge in the specified forget documents, and (2) it should retain the other knowledge that the LLM possesses, for which we assume access to a small number of retain documents. To achieve both goals, a mainstream class of LLM unlearning methods introduces an optimization framework with a combination of two objectives - maximizing the prediction loss on the forget documents while minimizing that on the retain documents, which suffers from two challenges, degenerated output and catastrophic forgetting. In this paper, we propose a novel unlearning framework called Unlearning from Logit Difference (ULD), which introduces an assistant LLM that aims to achieve the opposite of the unlearning goals: remembering the forget documents and forgetting the retain knowledge. ULD then derives the unlearned LLM by computing the logit difference between the target and the assistant LLMs. We show that such reversed objectives would naturally resolve both aforementioned challenges while significantly improving the training efficiency. Extensive experiments demonstrate that our method efficiently achieves the intended forgetting while preserving the LLM's overall capabilities, reducing training time by more than threefold. Notably, our method loses 0% of model utility on the ToFU benchmark, whereas baseline methods may sacrifice 17% of utility on average to achieve comparable forget quality. Our code will be publicly available at https://github.com/UCSB-NLP-Chang/ULD.","sentences":["As Large Language Models (LLMs) demonstrate extensive capability in learning from documents, LLM unlearning becomes an increasingly important research area to address concerns of LLMs in terms of privacy, copyright, etc.","A conventional LLM unlearning task typically involves two goals: (1) The target LLM should forget the knowledge in the specified forget documents, and (2) it should retain the other knowledge that the LLM possesses, for which we assume access to a small number of retain documents.","To achieve both goals, a mainstream class of LLM unlearning methods introduces an optimization framework with a combination of two objectives - maximizing the prediction loss on the forget documents while minimizing that on the retain documents, which suffers from two challenges, degenerated output and catastrophic forgetting.","In this paper, we propose a novel unlearning framework called Unlearning from Logit Difference (ULD), which introduces an assistant LLM that aims to achieve the opposite of the unlearning goals: remembering the forget documents and forgetting the retain knowledge.","ULD then derives the unlearned LLM by computing the logit difference between the target and the assistant LLMs.","We show that such reversed objectives would naturally resolve both aforementioned challenges while significantly improving the training efficiency.","Extensive experiments demonstrate that our method efficiently achieves the intended forgetting while preserving the LLM's overall capabilities, reducing training time by more than threefold.","Notably, our method loses 0% of model utility on the ToFU benchmark, whereas baseline methods may sacrifice 17% of utility on average to achieve comparable forget quality.","Our code will be publicly available at https://github.com/UCSB-NLP-Chang/ULD."],"url":"http://arxiv.org/abs/2406.08607v1","category":"cs.CL"}
{"created":"2024-06-12 19:22:29","title":"End-to-End Argument Mining as Augmented Natural Language Generation","abstract":"Argument Mining (AM) is a crucial aspect of computational argumentation, which deals with the identification and extraction of Argumentative Components (ACs) and their corresponding Argumentative Relations (ARs). Most prior works have solved these problems by dividing them into multiple subtasks. And the available end-to-end setups are mostly based on the dependency parsing approach. This work proposes a unified end-to-end framework based on a generative paradigm, in which the argumentative structures are framed into label-augmented text, called Augmented Natural Language (ANL). Additionally, we explore the role of different types of markers in solving AM tasks. Through different marker-based fine-tuning strategies, we present an extensive study by integrating marker knowledge into our generative model. The proposed framework achieves competitive results to the state-of-the-art (SoTA) model and outperforms several baselines.","sentences":["Argument Mining (AM) is a crucial aspect of computational argumentation, which deals with the identification and extraction of Argumentative Components (ACs) and their corresponding Argumentative Relations (ARs).","Most prior works have solved these problems by dividing them into multiple subtasks.","And the available end-to-end setups are mostly based on the dependency parsing approach.","This work proposes a unified end-to-end framework based on a generative paradigm, in which the argumentative structures are framed into label-augmented text, called Augmented Natural Language (ANL).","Additionally, we explore the role of different types of markers in solving AM tasks.","Through different marker-based fine-tuning strategies, we present an extensive study by integrating marker knowledge into our generative model.","The proposed framework achieves competitive results to the state-of-the-art (SoTA) model and outperforms several baselines."],"url":"http://arxiv.org/abs/2406.08606v1","category":"cs.CL"}
{"created":"2024-06-12 19:14:58","title":"FakeInversion: Learning to Detect Images from Unseen Text-to-Image Models by Inverting Stable Diffusion","abstract":"Due to the high potential for abuse of GenAI systems, the task of detecting synthetic images has recently become of great interest to the research community. Unfortunately, existing image-space detectors quickly become obsolete as new high-fidelity text-to-image models are developed at blinding speed. In this work, we propose a new synthetic image detector that uses features obtained by inverting an open-source pre-trained Stable Diffusion model. We show that these inversion features enable our detector to generalize well to unseen generators of high visual fidelity (e.g., DALL-E 3) even when the detector is trained only on lower fidelity fake images generated via Stable Diffusion. This detector achieves new state-of-the-art across multiple training and evaluation setups. Moreover, we introduce a new challenging evaluation protocol that uses reverse image search to mitigate stylistic and thematic biases in the detector evaluation. We show that the resulting evaluation scores align well with detectors' in-the-wild performance, and release these datasets as public benchmarks for future research.","sentences":["Due to the high potential for abuse of GenAI systems, the task of detecting synthetic images has recently become of great interest to the research community.","Unfortunately, existing image-space detectors quickly become obsolete as new high-fidelity text-to-image models are developed at blinding speed.","In this work, we propose a new synthetic image detector that uses features obtained by inverting an open-source pre-trained Stable Diffusion model.","We show that these inversion features enable our detector to generalize well to unseen generators of high visual fidelity (e.g., DALL-E 3) even when the detector is trained only on lower fidelity fake images generated via Stable Diffusion.","This detector achieves new state-of-the-art across multiple training and evaluation setups.","Moreover, we introduce a new challenging evaluation protocol that uses reverse image search to mitigate stylistic and thematic biases in the detector evaluation.","We show that the resulting evaluation scores align well with detectors' in-the-wild performance, and release these datasets as public benchmarks for future research."],"url":"http://arxiv.org/abs/2406.08603v1","category":"cs.CV"}
{"created":"2024-06-12 19:09:04","title":"A Refinement of the McCreight-Meyer Union Theorem","abstract":"Using properties of Blum complexity measures and certain complexity class operators, we exhibit a total computable and non-decreasing function $t_{\\mathsf{poly}}$ such that for all $k$, $\\Sigma_k\\mathsf{P} = \\Sigma_k\\mathsf{TIME}(t_{\\mathsf{poly}})$, $\\mathsf{BPP} = \\mathsf{BPTIME}(t_{\\mathsf{poly}})$, $\\mathsf{RP} = \\mathsf{RTIME}(t_{\\mathsf{poly}})$, $\\mathsf{UP} = \\mathsf{UTIME}(t_{\\mathsf{poly}})$, $\\mathsf{PP} = \\mathsf{PTIME}(t_{\\mathsf{poly}})$, $\\mathsf{Mod}_k\\mathsf{P} = \\mathsf{Mod}_k\\mathsf{TIME}(t_{\\mathsf{poly}})$, $\\mathsf{PSPACE} = \\mathsf{DSPACE}(t_{\\mathsf{poly}})$, and so forth. A similar statement holds for any collection of language classes, provided that each class is definable by applying a certain complexity class operator to some Blum complexity class.","sentences":["Using properties of Blum complexity measures and certain complexity class operators, we exhibit a total computable and non-decreasing function $t_{\\mathsf{poly}}$ such that for all $k$, $\\Sigma_k\\mathsf{P} = \\Sigma_k\\mathsf{TIME}(t_{\\mathsf{poly}})$, $\\mathsf{BPP} = \\mathsf{BPTIME}(t_{\\mathsf{poly}})$, $\\mathsf{RP} = \\mathsf{RTIME}(t_{\\mathsf{poly}})$, $\\mathsf{UP} = \\mathsf{UTIME}(t_{\\mathsf{poly}})$, $\\mathsf{PP} = \\mathsf{PTIME}(t_{\\mathsf{poly}})$, $\\mathsf{Mod}_k\\mathsf{P} = \\mathsf{Mod}_k\\mathsf{TIME}(t_{\\mathsf{poly}})$, $\\mathsf{PSPACE} = \\mathsf{DSPACE}(t_{\\mathsf{poly}})$, and so forth.","A similar statement holds for any collection of language classes, provided that each class is definable by applying a certain complexity class operator to some Blum complexity class."],"url":"http://arxiv.org/abs/2406.08600v1","category":"cs.CC"}
{"created":"2024-06-12 19:05:43","title":"Language Model Council: Benchmarking Foundation Models on Highly Subjective Tasks by Consensus","abstract":"The rapid advancement of Large Language Models (LLMs) necessitates robust and challenging benchmarks. Leaderboards like Chatbot Arena rank LLMs based on how well their responses align with human preferences. However, many tasks such as those related to emotional intelligence, creative writing, or persuasiveness, are highly subjective and often lack majoritarian human agreement. Judges may have irreconcilable disagreements about what constitutes a better response. To address the challenge of ranking LLMs on highly subjective tasks, we propose a novel benchmarking framework, the Language Model Council (LMC). The LMC operates through a democratic process to: 1) formulate a test set through equal participation, 2) administer the test among council members, and 3) evaluate responses as a collective jury. We deploy a council of 20 newest LLMs on an open-ended emotional intelligence task: responding to interpersonal dilemmas. Our results show that the LMC produces rankings that are more separable, robust, and less biased than those from any individual LLM judge, and is more consistent with a human-established leaderboard compared to other benchmarks.","sentences":["The rapid advancement of Large Language Models (LLMs) necessitates robust and challenging benchmarks.","Leaderboards like Chatbot Arena rank LLMs based on how well their responses align with human preferences.","However, many tasks such as those related to emotional intelligence, creative writing, or persuasiveness, are highly subjective and often lack majoritarian human agreement.","Judges may have irreconcilable disagreements about what constitutes a better response.","To address the challenge of ranking LLMs on highly subjective tasks, we propose a novel benchmarking framework, the Language Model Council (LMC).","The LMC operates through a democratic process to: 1) formulate a test set through equal participation, 2) administer the test among council members, and 3) evaluate responses as a collective jury.","We deploy a council of 20 newest LLMs on an open-ended emotional intelligence task: responding to interpersonal dilemmas.","Our results show that the LMC produces rankings that are more separable, robust, and less biased than those from any individual LLM judge, and is more consistent with a human-established leaderboard compared to other benchmarks."],"url":"http://arxiv.org/abs/2406.08598v1","category":"cs.CL"}
{"created":"2024-06-12 18:59:01","title":"Intelligent Multi-View Test Time Augmentation","abstract":"In this study, we introduce an intelligent Test Time Augmentation (TTA) algorithm designed to enhance the robustness and accuracy of image classification models against viewpoint variations. Unlike traditional TTA methods that indiscriminately apply augmentations, our approach intelligently selects optimal augmentations based on predictive uncertainty metrics. This selection is achieved via a two-stage process: the first stage identifies the optimal augmentation for each class by evaluating uncertainty levels, while the second stage implements an uncertainty threshold to determine when applying TTA would be advantageous. This methodological advancement ensures that augmentations contribute to classification more effectively than a uniform application across the dataset. Experimental validation across several datasets and neural network architectures validates our approach, yielding an average accuracy improvement of 1.73% over methods that use single-view images. This research underscores the potential of adaptive, uncertainty-aware TTA in improving the robustness of image classification in the presence of viewpoint variations, paving the way for further exploration into intelligent augmentation strategies.","sentences":["In this study, we introduce an intelligent Test Time Augmentation (TTA) algorithm designed to enhance the robustness and accuracy of image classification models against viewpoint variations.","Unlike traditional TTA methods that indiscriminately apply augmentations, our approach intelligently selects optimal augmentations based on predictive uncertainty metrics.","This selection is achieved via a two-stage process: the first stage identifies the optimal augmentation for each class by evaluating uncertainty levels, while the second stage implements an uncertainty threshold to determine when applying TTA would be advantageous.","This methodological advancement ensures that augmentations contribute to classification more effectively than a uniform application across the dataset.","Experimental validation across several datasets and neural network architectures validates our approach, yielding an average accuracy improvement of 1.73% over methods that use single-view images.","This research underscores the potential of adaptive, uncertainty-aware TTA in improving the robustness of image classification in the presence of viewpoint variations, paving the way for further exploration into intelligent augmentation strategies."],"url":"http://arxiv.org/abs/2406.08593v1","category":"eess.IV"}
{"created":"2024-06-12 18:48:46","title":"Composition Tracking for Collisions Between Differentiated Bodies in REBOUND","abstract":"Previous research suggests that impacts between planetary embryos and planetesimals during the late stages of planet formation can often determine the percentages of core and mantle material that compose the newly formed planets in a system. Previous studies have attempted to include the composition-changing effects of these collisions in N-body simulations of planet formation, often as post-processing codes. In this paper, we present the Differentiated Body Composition Tracker, a new post-processing tool that uses collisional data collected from the N-body integrator REBOUND to determine the amount of core and mantle material that is transferred between colliding objects and the resulting fragments during an impact. We demonstrate how this code works using the data from 50 REBOUND simulations of planet formation and explore how the parameters in the code affect the core mass fractions of the remaining objects from these simulations. We then investigate how non-uniform distributions of core material across an initial disc affect the final core mass fractions of planets. Under ideal conditions, we find that a combination of giant impacts and planetary embryos enriched in core material could create some of the iron-rich planets that have been discovered.","sentences":["Previous research suggests that impacts between planetary embryos and planetesimals during the late stages of planet formation can often determine the percentages of core and mantle material that compose the newly formed planets in a system.","Previous studies have attempted to include the composition-changing effects of these collisions in N-body simulations of planet formation, often as post-processing codes.","In this paper, we present the Differentiated Body Composition Tracker, a new post-processing tool that uses collisional data collected from the N-body integrator REBOUND to determine the amount of core and mantle material that is transferred between colliding objects and the resulting fragments during an impact.","We demonstrate how this code works using the data from 50 REBOUND simulations of planet formation and explore how the parameters in the code affect the core mass fractions of the remaining objects from these simulations.","We then investigate how non-uniform distributions of core material across an initial disc affect the final core mass fractions of planets.","Under ideal conditions, we find that a combination of giant impacts and planetary embryos enriched in core material could create some of the iron-rich planets that have been discovered."],"url":"http://arxiv.org/abs/2406.08588v1","category":"astro-ph.EP"}
{"created":"2024-06-12 18:47:28","title":"CS-Bench: A Comprehensive Benchmark for Large Language Models towards Computer Science Mastery","abstract":"Computer Science (CS) stands as a testament to the intricacies of human intelligence, profoundly advancing the development of artificial intelligence and modern society. However, the current community of large language models (LLMs) overly focuses on benchmarks for analyzing specific foundational skills (e.g. mathematics and code generation), neglecting an all-round evaluation of the computer science field. To bridge this gap, we introduce CS-Bench, the first bilingual (Chinese-English) benchmark dedicated to evaluating the performance of LLMs in computer science. CS-Bench comprises approximately 5K meticulously curated test samples, covering 26 subfields across 4 key areas of computer science, encompassing various task forms and divisions of knowledge and reasoning. Utilizing CS-Bench, we conduct a comprehensive evaluation of over 30 mainstream LLMs, revealing the relationship between CS performance and model scales. We also quantitatively analyze the reasons for failures in existing LLMs and highlight directions for improvements, including knowledge supplementation and CS-specific reasoning. Further cross-capability experiments show a high correlation between LLMs' capabilities in computer science and their abilities in mathematics and coding. Moreover, expert LLMs specialized in mathematics and coding also demonstrate strong performances in several CS subfields. Looking ahead, we envision CS-Bench serving as a cornerstone for LLM applications in the CS field and paving new avenues in assessing LLMs' diverse reasoning capabilities. The CS-Bench data and evaluation code are available at https://github.com/csbench/csbench.","sentences":["Computer Science (CS) stands as a testament to the intricacies of human intelligence, profoundly advancing the development of artificial intelligence and modern society.","However, the current community of large language models (LLMs) overly focuses on benchmarks for analyzing specific foundational skills (e.g. mathematics and code generation), neglecting an all-round evaluation of the computer science field.","To bridge this gap, we introduce CS-Bench, the first bilingual (Chinese-English) benchmark dedicated to evaluating the performance of LLMs in computer science.","CS-Bench comprises approximately 5K meticulously curated test samples, covering 26 subfields across 4 key areas of computer science, encompassing various task forms and divisions of knowledge and reasoning.","Utilizing CS-Bench, we conduct a comprehensive evaluation of over 30 mainstream LLMs, revealing the relationship between CS performance and model scales.","We also quantitatively analyze the reasons for failures in existing LLMs and highlight directions for improvements, including knowledge supplementation and CS-specific reasoning.","Further cross-capability experiments show a high correlation between LLMs' capabilities in computer science and their abilities in mathematics and coding.","Moreover, expert LLMs specialized in mathematics and coding also demonstrate strong performances in several CS subfields.","Looking ahead, we envision CS-Bench serving as a cornerstone for LLM applications in the CS field and paving new avenues in assessing LLMs' diverse reasoning capabilities.","The CS-Bench data and evaluation code are available at https://github.com/csbench/csbench."],"url":"http://arxiv.org/abs/2406.08587v1","category":"cs.CL"}
{"created":"2024-06-12 18:39:43","title":"Defining a Reference Architecture for Edge Systems in Highly-Uncertain Environments","abstract":"Increasing rate of progress in hardware and artificial intelligence (AI) solutions is enabling a range of software systems to be deployed closer to their users, increasing application of edge software system paradigms. Edge systems support scenarios in which computation is placed closer to where data is generated and needed, and provide benefits such as reduced latency, bandwidth optimization, and higher resiliency and availability. Users who operate in highly-uncertain and resource-constrained environments, such as first responders, law enforcement, and soldiers, can greatly benefit from edge systems to support timelier decision making. Unfortunately, understanding how different architecture approaches for edge systems impact priority quality concerns is largely neglected by industry and research, yet crucial for national and local safety, optimal resource utilization, and timely decision making. Much of industry is focused on the hardware and networking aspects of edge systems, with very little attention to the software that enables edge capabilities. This paper presents our work to fill this gap, defining a reference architecture for edge systems in highly-uncertain environments, and showing examples of how it has been implemented in practice.","sentences":["Increasing rate of progress in hardware and artificial intelligence (AI) solutions is enabling a range of software systems to be deployed closer to their users, increasing application of edge software system paradigms.","Edge systems support scenarios in which computation is placed closer to where data is generated and needed, and provide benefits such as reduced latency, bandwidth optimization, and higher resiliency and availability.","Users who operate in highly-uncertain and resource-constrained environments, such as first responders, law enforcement, and soldiers, can greatly benefit from edge systems to support timelier decision making.","Unfortunately, understanding how different architecture approaches for edge systems impact priority quality concerns is largely neglected by industry and research, yet crucial for national and local safety, optimal resource utilization, and timely decision making.","Much of industry is focused on the hardware and networking aspects of edge systems, with very little attention to the software that enables edge capabilities.","This paper presents our work to fill this gap, defining a reference architecture for edge systems in highly-uncertain environments, and showing examples of how it has been implemented in practice."],"url":"http://arxiv.org/abs/2406.08583v1","category":"cs.SE"}
{"created":"2024-06-12 18:27:38","title":"Laser induced $\\mathcal{PT}$-symmetry breaking in the fluctuations of electronic fluids","abstract":"Electronic fluids can display exciting dynamical properties. In particular, due to Landau damping, the collective modes spectrum of an electronic system with multipolar interactions is non-hermitian, and can present non-hermitian degeneracies called $\\textit{exceptional points}$. In this work, we want to explore the dynamical properties of these degeneracies using laser control. We show that by using a light pulse, we can control the collective mode spectrum and tune a non-hermitian $\\mathcal{PT}$ phase transition in which two exceptional points anhilate each other. At this transition, the gap closes with a cubic root signature, what defines a third order exceptional point.","sentences":["Electronic fluids can display exciting dynamical properties.","In particular, due to Landau damping, the collective modes spectrum of an electronic system with multipolar interactions is non-hermitian, and can present non-hermitian degeneracies called $\\textit{exceptional points}$.","In this work, we want to explore the dynamical properties of these degeneracies using laser control.","We show that by using a light pulse, we can control the collective mode spectrum and tune a non-hermitian $\\mathcal{PT}$ phase transition in which two exceptional points anhilate each other.","At this transition, the gap closes with a cubic root signature, what defines a third order exceptional point."],"url":"http://arxiv.org/abs/2406.08576v1","category":"cond-mat.str-el"}
{"created":"2024-06-12 18:26:42","title":"Using Quality Attribute Scenarios for ML Model Test Case Generation","abstract":"Testing of machine learning (ML) models is a known challenge identified by researchers and practitioners alike. Unfortunately, current practice for ML model testing prioritizes testing for model performance, while often neglecting the requirements and constraints of the ML-enabled system that integrates the model. This limited view of testing leads to failures during integration, deployment, and operations, contributing to the difficulties of moving models from development to production. This paper presents an approach based on quality attribute (QA) scenarios to elicit and define system- and model-relevant test cases for ML models. The QA-based approach described in this paper has been integrated into MLTE, a process and tool to support ML model test and evaluation. Feedback from users of MLTE highlights its effectiveness in testing beyond model performance and identifying failures early in the development process.","sentences":["Testing of machine learning (ML) models is a known challenge identified by researchers and practitioners alike.","Unfortunately, current practice for ML model testing prioritizes testing for model performance, while often neglecting the requirements and constraints of the ML-enabled system that integrates the model.","This limited view of testing leads to failures during integration, deployment, and operations, contributing to the difficulties of moving models from development to production.","This paper presents an approach based on quality attribute (QA) scenarios to elicit and define system- and model-relevant test cases for ML models.","The QA-based approach described in this paper has been integrated into MLTE, a process and tool to support ML model test and evaluation.","Feedback from users of MLTE highlights its effectiveness in testing beyond model performance and identifying failures early in the development process."],"url":"http://arxiv.org/abs/2406.08575v1","category":"cs.SE"}
{"created":"2024-06-12 18:19:37","title":"LLM-assisted Concept Discovery: Automatically Identifying and Explaining Neuron Functions","abstract":"Providing textual concept-based explanations for neurons in deep neural networks (DNNs) is of importance in understanding how a DNN model works. Prior works have associated concepts with neurons based on examples of concepts or a pre-defined set of concepts, thus limiting possible explanations to what the user expects, especially in discovering new concepts. Furthermore, defining the set of concepts requires manual work from the user, either by directly specifying them or collecting examples. To overcome these, we propose to leverage multimodal large language models for automatic and open-ended concept discovery. We show that, without a restricted set of pre-defined concepts, our method gives rise to novel interpretable concepts that are more faithful to the model's behavior. To quantify this, we validate each concept by generating examples and counterexamples and evaluating the neuron's response on this new set of images. Collectively, our method can discover concepts and simultaneously validate them, providing a credible automated tool to explain deep neural networks.","sentences":["Providing textual concept-based explanations for neurons in deep neural networks (DNNs) is of importance in understanding how a DNN model works.","Prior works have associated concepts with neurons based on examples of concepts or a pre-defined set of concepts, thus limiting possible explanations to what the user expects, especially in discovering new concepts.","Furthermore, defining the set of concepts requires manual work from the user, either by directly specifying them or collecting examples.","To overcome these, we propose to leverage multimodal large language models for automatic and open-ended concept discovery.","We show that, without a restricted set of pre-defined concepts, our method gives rise to novel interpretable concepts that are more faithful to the model's behavior.","To quantify this, we validate each concept by generating examples and counterexamples and evaluating the neuron's response on this new set of images.","Collectively, our method can discover concepts and simultaneously validate them, providing a credible automated tool to explain deep neural networks."],"url":"http://arxiv.org/abs/2406.08572v1","category":"cs.CV"}
{"created":"2024-06-12 18:11:32","title":"HDNet: Physics-Inspired Neural Network for Flow Estimation based on Helmholtz Decomposition","abstract":"Flow estimation problems are ubiquitous in scientific imaging. Often, the underlying flows are subject to physical constraints that can be exploited in the flow estimation; for example, incompressible (divergence-free) flows are expected for many fluid experiments, while irrotational (curl-free) flows arise in the analysis of optical distortions and wavefront sensing. In this work, we propose a Physics- Inspired Neural Network (PINN) named HDNet, which performs a Helmholtz decomposition of an arbitrary flow field, i.e., it decomposes the input flow into a divergence-only and a curl-only component. HDNet can be trained exclusively on synthetic data generated by reverse Helmholtz decomposition, which we call Helmholtz synthesis. As a PINN, HDNet is fully differentiable and can easily be integrated into arbitrary flow estimation problems.","sentences":["Flow estimation problems are ubiquitous in scientific imaging.","Often, the underlying flows are subject to physical constraints that can be exploited in the flow estimation; for example, incompressible (divergence-free) flows are expected for many fluid experiments, while irrotational (curl-free) flows arise in the analysis of optical distortions and wavefront sensing.","In this work, we propose a Physics- Inspired Neural Network (PINN) named HDNet, which performs a Helmholtz decomposition of an arbitrary flow field, i.e., it decomposes the input flow into a divergence-only and a curl-only component.","HDNet can be trained exclusively on synthetic data generated by reverse Helmholtz decomposition, which we call Helmholtz synthesis.","As a PINN, HDNet is fully differentiable and can easily be integrated into arbitrary flow estimation problems."],"url":"http://arxiv.org/abs/2406.08570v1","category":"cs.LG"}
{"created":"2024-06-12 18:07:06","title":"A new approach for predicting the Quality of Experience in multimedia services using machine learning","abstract":"In today's world, the Internet is recognized as one of the essentials of human life, playing a significant role in communications, business, and lifestyle. The quality of internet services can have widespread negative impacts on individual and social levels. Consequently, Quality of Service (QoS) has become a fundamental necessity for service providers in a competitive market aiming to offer superior services. The success and survival of these providers depend on their ability to maintain high service quality and ensure satisfaction.Alongside QoS, the concept of Quality of Experience (QoE) has emerged with the development of telephony networks. QoE focuses on the user's satisfaction with the service, helping operators adjust their services to meet user expectations. Recent research shows a trend towards utilizing machine learning and deep learning techniques to predict QoE. Researchers aim to develop accurate models by leveraging large volumes of data from network and user interactions, considering various real-world scenarios. Despite the complexity of network environments, this research provides a practical framework for improving and evaluating QoE. This study presents a comprehensive framework for evaluating QoE in multimedia services, adhering to the ITU-T P.1203 standard which includes automated data collection processes and uses machine learning algorithms to predict user satisfaction based on key network parameters. By collecting over 20,000 data records from different network conditions and users, the Random Forest model achieved a prediction accuracy of 95.8% for user satisfaction. This approach allows operators to dynamically allocate network resources in real-time, maintaining high levels of customer satisfaction with minimal costs.","sentences":["In today's world, the Internet is recognized as one of the essentials of human life, playing a significant role in communications, business, and lifestyle.","The quality of internet services can have widespread negative impacts on individual and social levels.","Consequently, Quality of Service (QoS) has become a fundamental necessity for service providers in a competitive market aiming to offer superior services.","The success and survival of these providers depend on their ability to maintain high service quality and ensure satisfaction.","Alongside QoS, the concept of Quality of Experience (QoE) has emerged with the development of telephony networks.","QoE focuses on the user's satisfaction with the service, helping operators adjust their services to meet user expectations.","Recent research shows a trend towards utilizing machine learning and deep learning techniques to predict QoE.","Researchers aim to develop accurate models by leveraging large volumes of data from network and user interactions, considering various real-world scenarios.","Despite the complexity of network environments, this research provides a practical framework for improving and evaluating QoE.","This study presents a comprehensive framework for evaluating QoE in multimedia services, adhering to the ITU-T P.1203 standard which includes automated data collection processes and uses machine learning algorithms to predict user satisfaction based on key network parameters.","By collecting over 20,000 data records from different network conditions and users, the Random Forest model achieved a prediction accuracy of 95.8% for user satisfaction.","This approach allows operators to dynamically allocate network resources in real-time, maintaining high levels of customer satisfaction with minimal costs."],"url":"http://arxiv.org/abs/2406.08564v1","category":"cs.NI"}
{"created":"2024-06-12 18:02:27","title":"Theory and Observation of Winds from Star-Forming Galaxies","abstract":"Galactic winds shape the stellar, gas, and metal content of galaxies. To quantify their impact, we must understand their physics. We review potential wind-driving mechanisms and observed wind properties, with a focus on the warm ionized and hot X-ray-emitting gas. Energy and momentum injection by supernovae (SNe), cosmic rays, radiation pressure, and magnetic fields are considered in the light of observations: (1) Emission and absorption line measurements of cool/warm gas provide our best physical diagnostics of galactic outflows. (2) The critical unsolved problem is how to accelerate cool gas to the high velocities observed. Although conclusive evidence for no one mechanism exists, the momentum, energy, and mass-loading budgets observed compare well with theory. (3) A model where star formation provides a force $\\sim L/c$, where $L$ is the bolometric luminosity, and cool gas is pushed out of the galaxy's gravitational potential, compares well with available data. The wind power is $\\sim0.1$ that provided by SNe. (4) The very hot X-ray emitting phase, may be a (or the) prime mover. Momentum and energy exchange between the hot and cooler phases is critical to the gas dynamics. (5) Gaps in our observational knowledge include the hot gas kinematics and the size and structure of the outflows probed with UV absorption lines. Simulations are needed to more fully understand mixing, cloud-radiation, cloud-cosmic ray, and cloud-hot wind interactions, the collective effects of star clusters, and both distributed and clustered SNe. Observational works should seek secondary correlations in the wind data that provide evidence for specific mechanisms and compare spectroscopy with the column density-velocity results from theory.","sentences":["Galactic winds shape the stellar, gas, and metal content of galaxies.","To quantify their impact, we must understand their physics.","We review potential wind-driving mechanisms and observed wind properties, with a focus on the warm ionized and hot X-ray-emitting gas.","Energy and momentum injection by supernovae (SNe), cosmic rays, radiation pressure, and magnetic fields are considered in the light of observations: (1) Emission and absorption line measurements of cool/warm gas provide our best physical diagnostics of galactic outflows.","(2) The critical unsolved problem is how to accelerate cool gas to the high velocities observed.","Although conclusive evidence for no one mechanism exists, the momentum, energy, and mass-loading budgets observed compare well with theory.","(3) A model where star formation provides a force $\\sim L/c$, where $L$ is the bolometric luminosity, and cool gas is pushed out of the galaxy's gravitational potential, compares well with available data.","The wind power is $\\sim0.1$ that provided by SNe.","(4) The very hot X-ray emitting phase, may be a (or the) prime mover.","Momentum and energy exchange between the hot and cooler phases is critical to the gas dynamics.","(5) Gaps in our observational knowledge include the hot gas kinematics and the size and structure of the outflows probed with UV absorption lines.","Simulations are needed to more fully understand mixing, cloud-radiation, cloud-cosmic ray, and cloud-hot wind interactions, the collective effects of star clusters, and both distributed and clustered SNe.","Observational works should seek secondary correlations in the wind data that provide evidence for specific mechanisms and compare spectroscopy with the column density-velocity results from theory."],"url":"http://arxiv.org/abs/2406.08561v1","category":"astro-ph.GA"}
{"created":"2024-06-12 18:00:47","title":"Macroscopic Tunneling Probe of Moir\u00e9 Spin Textures in Twisted CrI$_3$","abstract":"Various noncollinear spin textures and magnetic phases have been predicted in twisted two-dimensional CrI$_3$ due to competing ferromagnetic (FM) and antiferromagnetic (AFM) interlayer exchange from moir\\'e stacking - with potential spintronic applications even when the underlying material possesses a negligible Dzyaloshinskii-Moriya or dipole-dipole interaction. Recent measurements have shown evidence of coexisting FM and AFM layer order in small-twist-angle CrI$_3$ bilayers and double bilayers. Yet, the nature of the magnetic textures remains unresolved and possibilities for their manipulation and electrical readout are unexplored. Here, we use tunneling magnetoresistance to investigate the collective spin states of twisted double-bilayer CrI$_3$ under both out-of-plane and in-plane magnetic fields together with detailed micromagnetic simulations of domain dynamics based on magnetic circular dichroism. Our results capture hysteretic and anisotropic field evolutions of the magnetic states and we further uncover two distinct non-volatile spin textures (out-of-plane and in-plane domains) at $\\approx$ 1{\\deg} twist angle, with a different global tunneling resistance that can be switched by magnetic field.","sentences":["Various noncollinear spin textures and magnetic phases have been predicted in twisted two-dimensional CrI$_3$ due to competing ferromagnetic (FM) and antiferromagnetic (AFM) interlayer exchange from moir\\'e stacking - with potential spintronic applications even when the underlying material possesses a negligible Dzyaloshinskii-Moriya or dipole-dipole interaction.","Recent measurements have shown evidence of coexisting FM and AFM layer order in small-twist-angle CrI$_3$ bilayers and double bilayers.","Yet, the nature of the magnetic textures remains unresolved and possibilities for their manipulation and electrical readout are unexplored.","Here, we use tunneling magnetoresistance to investigate the collective spin states of twisted double-bilayer CrI$_3$ under both out-of-plane and in-plane magnetic fields together with detailed micromagnetic simulations of domain dynamics based on magnetic circular dichroism.","Our results capture hysteretic and anisotropic field evolutions of the magnetic states and we further uncover two distinct non-volatile spin textures (out-of-plane and in-plane domains) at $\\approx$ 1{\\deg} twist angle, with a different global tunneling resistance that can be switched by magnetic field."],"url":"http://arxiv.org/abs/2406.08556v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-12 18:00:01","title":"RVT-2: Learning Precise Manipulation from Few Demonstrations","abstract":"In this work, we study how to build a robotic system that can solve multiple 3D manipulation tasks given language instructions. To be useful in industrial and household domains, such a system should be capable of learning new tasks with few demonstrations and solving them precisely. Prior works, like PerAct and RVT, have studied this problem, however, they often struggle with tasks requiring high precision. We study how to make them more effective, precise, and fast. Using a combination of architectural and system-level improvements, we propose RVT-2, a multitask 3D manipulation model that is 6X faster in training and 2X faster in inference than its predecessor RVT. RVT-2 achieves a new state-of-the-art on RLBench, improving the success rate from 65% to 82%. RVT-2 is also effective in the real world, where it can learn tasks requiring high precision, like picking up and inserting plugs, with just 10 demonstrations. Visual results, code, and trained model are provided at: https://robotic-view-transformer-2.github.io/.","sentences":["In this work, we study how to build a robotic system that can solve multiple 3D manipulation tasks given language instructions.","To be useful in industrial and household domains, such a system should be capable of learning new tasks with few demonstrations and solving them precisely.","Prior works, like PerAct and RVT, have studied this problem, however, they often struggle with tasks requiring high precision.","We study how to make them more effective, precise, and fast.","Using a combination of architectural and system-level improvements, we propose RVT-2, a multitask 3D manipulation model that is 6X faster in training and 2X faster in inference than its predecessor RVT.","RVT-2 achieves a new state-of-the-art on RLBench, improving the success rate from 65% to 82%.","RVT-2 is also effective in the real world, where it can learn tasks requiring high precision, like picking up and inserting plugs, with just 10 demonstrations.","Visual results, code, and trained model are provided at: https://robotic-view-transformer-2.github.io/."],"url":"http://arxiv.org/abs/2406.08545v1","category":"cs.RO"}
{"created":"2024-06-13 17:59:44","title":"Towards Evaluating the Robustness of Visual State Space Models","abstract":"Vision State Space Models (VSSMs), a novel architecture that combines the strengths of recurrent neural networks and latent variable models, have demonstrated remarkable performance in visual perception tasks by efficiently capturing long-range dependencies and modeling complex visual dynamics. However, their robustness under natural and adversarial perturbations remains a critical concern. In this work, we present a comprehensive evaluation of VSSMs' robustness under various perturbation scenarios, including occlusions, image structure, common corruptions, and adversarial attacks, and compare their performance to well-established architectures such as transformers and Convolutional Neural Networks. Furthermore, we investigate the resilience of VSSMs to object-background compositional changes on sophisticated benchmarks designed to test model performance in complex visual scenes. We also assess their robustness on object detection and segmentation tasks using corrupted datasets that mimic real-world scenarios. To gain a deeper understanding of VSSMs' adversarial robustness, we conduct a frequency analysis of adversarial attacks, evaluating their performance against low-frequency and high-frequency perturbations. Our findings highlight the strengths and limitations of VSSMs in handling complex visual corruptions, offering valuable insights for future research and improvements in this promising field. Our code and models will be available at https://github.com/HashmatShadab/MambaRobustness.","sentences":["Vision State Space Models (VSSMs), a novel architecture that combines the strengths of recurrent neural networks and latent variable models, have demonstrated remarkable performance in visual perception tasks by efficiently capturing long-range dependencies and modeling complex visual dynamics.","However, their robustness under natural and adversarial perturbations remains a critical concern.","In this work, we present a comprehensive evaluation of VSSMs' robustness under various perturbation scenarios, including occlusions, image structure, common corruptions, and adversarial attacks, and compare their performance to well-established architectures such as transformers and Convolutional Neural Networks.","Furthermore, we investigate the resilience of VSSMs to object-background compositional changes on sophisticated benchmarks designed to test model performance in complex visual scenes.","We also assess their robustness on object detection and segmentation tasks using corrupted datasets that mimic real-world scenarios.","To gain a deeper understanding of VSSMs' adversarial robustness, we conduct a frequency analysis of adversarial attacks, evaluating their performance against low-frequency and high-frequency perturbations.","Our findings highlight the strengths and limitations of VSSMs in handling complex visual corruptions, offering valuable insights for future research and improvements in this promising field.","Our code and models will be available at https://github.com/HashmatShadab/MambaRobustness."],"url":"http://arxiv.org/abs/2406.09407v1","category":"cs.CV"}
{"created":"2024-06-13 17:59:35","title":"Why Warmup the Learning Rate? Underlying Mechanisms and Improvements","abstract":"It is common in deep learning to warm up the learning rate $\\eta$, often by a linear schedule between $\\eta_{\\text{init}} = 0$ and a predetermined target $\\eta_{\\text{trgt}}$. In this paper, we show through systematic experiments using SGD and Adam that the overwhelming benefit of warmup arises from allowing the network to tolerate larger $\\eta_{\\text{trgt}}$ by forcing the network to more well-conditioned areas of the loss landscape. The ability to handle larger $\\eta_{\\text{trgt}}$ makes hyperparameter tuning more robust while improving the final performance. We uncover different regimes of operation during the warmup period, depending on whether training starts off in a progressive sharpening or sharpness reduction phase, which in turn depends on the initialization and parameterization. Using these insights, we show how $\\eta_{\\text{init}}$ can be properly chosen by utilizing the loss catapult mechanism, which saves on the number of warmup steps, in some cases completely eliminating the need for warmup. We also suggest an initialization for the variance in Adam which provides benefits similar to warmup.","sentences":["It is common in deep learning to warm up the learning rate $\\eta$, often by a linear schedule between $\\eta_{\\text{init}} = 0$ and a predetermined target $\\eta_{\\text{trgt}}$. In this paper, we show through systematic experiments using SGD and Adam that the overwhelming benefit of warmup arises from allowing the network to tolerate larger $\\eta_{\\text{trgt}}$ by forcing the network to more well-conditioned areas of the loss landscape.","The ability to handle larger $\\eta_{\\text{trgt}}$ makes hyperparameter tuning more robust while improving the final performance.","We uncover different regimes of operation during the warmup period, depending on whether training starts off in a progressive sharpening or sharpness reduction phase, which in turn depends on the initialization and parameterization.","Using these insights, we show how $\\eta_{\\text{init}}$ can be properly chosen by utilizing the loss catapult mechanism, which saves on the number of warmup steps, in some cases completely eliminating the need for warmup.","We also suggest an initialization for the variance in Adam which provides benefits similar to warmup."],"url":"http://arxiv.org/abs/2406.09405v1","category":"cs.LG"}
{"created":"2024-06-13 17:59:31","title":"Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models","abstract":"Humans draw to facilitate reasoning: we draw auxiliary lines when solving geometry problems; we mark and circle when reasoning on maps; we use sketches to amplify our ideas and relieve our limited-capacity working memory. However, such actions are missing in current multimodal language models (LMs). Current chain-of-thought and tool-use paradigms only use text as intermediate reasoning steps. In this work, we introduce Sketchpad, a framework that gives multimodal LMs a visual sketchpad and tools to draw on the sketchpad. The LM conducts planning and reasoning according to the visual artifacts it has drawn. Different from prior work, which uses text-to-image models to enable LMs to draw, Sketchpad enables LMs to draw with lines, boxes, marks, etc., which is closer to human sketching and better facilitates reasoning. Sketchpad can also use specialist vision models during the sketching process (e.g., draw bounding boxes with object detection models, draw masks with segmentation models), to further enhance visual perception and reasoning. We experiment with a wide range of math tasks (including geometry, functions, graphs, and chess) and complex visual reasoning tasks. Sketchpad substantially improves performance on all tasks over strong base models with no sketching, yielding an average gain of 12.7% on math tasks, and 8.6% on vision tasks. GPT-4o with Sketchpad sets a new state of the art on all tasks, including V*Bench (80.3%), BLINK spatial reasoning (83.9%), and visual correspondence (80.8%). All codes and data are in https://visualsketchpad.github.io/.","sentences":["Humans draw to facilitate reasoning: we draw auxiliary lines when solving geometry problems; we mark and circle when reasoning on maps; we use sketches to amplify our ideas and relieve our limited-capacity working memory.","However, such actions are missing in current multimodal language models (LMs).","Current chain-of-thought and tool-use paradigms only use text as intermediate reasoning steps.","In this work, we introduce Sketchpad, a framework that gives multimodal LMs a visual sketchpad and tools to draw on the sketchpad.","The LM conducts planning and reasoning according to the visual artifacts it has drawn.","Different from prior work, which uses text-to-image models to enable LMs to draw, Sketchpad enables LMs to draw with lines, boxes, marks, etc., which is closer to human sketching and better facilitates reasoning.","Sketchpad can also use specialist vision models during the sketching process (e.g., draw bounding boxes with object detection models, draw masks with segmentation models), to further enhance visual perception and reasoning.","We experiment with a wide range of math tasks (including geometry, functions, graphs, and chess) and complex visual reasoning tasks.","Sketchpad substantially improves performance on all tasks over strong base models with no sketching, yielding an average gain of 12.7% on math tasks, and 8.6% on vision tasks.","GPT-4o with Sketchpad sets a new state of the art on all tasks, including V*Bench (80.3%), BLINK spatial reasoning (83.9%), and visual correspondence (80.8%).","All codes and data are in https://visualsketchpad.github.io/."],"url":"http://arxiv.org/abs/2406.09403v1","category":"cs.CV"}
{"created":"2024-06-13 17:59:07","title":"Entanglement dynamics and eigenstate correlations in strongly disordered quantum many-body systems","abstract":"The many-body localised phase of quantum systems is an unusual dynamical phase wherein the system fails to thermalise and yet, entanglement grows unboundedly albeit very slowly in time. We present a microscopic theory of this ultraslow growth of entanglement in terms of dynamical eigenstate correlations of strongly disordered, interacting quantum systems in the many-body localised regime. These correlations involve sets of four or more eigenstates and hence, go beyond correlations involving pairs of eigenstates which are usually studied in the context of eigenstate thermalisation or lack thereof. We consider the minimal case, namely the second R\\'enyi entropy of entanglement, of an initial product state as well as that of the time-evolution operator, wherein the correlations involve quartets of four eigenstates. We identify that the dynamics of the entanglement entropy is dominated by the spectral correlations within certain special quartets of eigenstates. We uncover the spatial structure of these special quartets and the ensuing statistics of the spectral correlations amongst the eigenstates therein, which reveals a hierarchy of timescales or equivalently, energyscales. We show that the hierarchy of these timescales along with their non-trivial distributions conspire to produce the logarithmic in time growth of entanglement, characteristic of the many-body localised regime. The underlying spatial structures in the set of special quartets also provides a microscopic understanding of the spacetime picture of the entanglement growth. The theory therefore provides a much richer perspective on entanglement growth in strongly disordered systems compared to the commonly employed phenomenological approach based on the $\\ell$-bit picture.","sentences":["The many-body localised phase of quantum systems is an unusual dynamical phase wherein the system fails to thermalise and yet, entanglement grows unboundedly albeit very slowly in time.","We present a microscopic theory of this ultraslow growth of entanglement in terms of dynamical eigenstate correlations of strongly disordered, interacting quantum systems in the many-body localised regime.","These correlations involve sets of four or more eigenstates and hence, go beyond correlations involving pairs of eigenstates which are usually studied in the context of eigenstate thermalisation or lack thereof.","We consider the minimal case, namely the second R\\'enyi entropy of entanglement, of an initial product state as well as that of the time-evolution operator, wherein the correlations involve quartets of four eigenstates.","We identify that the dynamics of the entanglement entropy is dominated by the spectral correlations within certain special quartets of eigenstates.","We uncover the spatial structure of these special quartets and the ensuing statistics of the spectral correlations amongst the eigenstates therein, which reveals a hierarchy of timescales or equivalently, energyscales.","We show that the hierarchy of these timescales along with their non-trivial distributions conspire to produce the logarithmic in time growth of entanglement, characteristic of the many-body localised regime.","The underlying spatial structures in the set of special quartets also provides a microscopic understanding of the spacetime picture of the entanglement growth.","The theory therefore provides a much richer perspective on entanglement growth in strongly disordered systems compared to the commonly employed phenomenological approach based on the $\\ell$-bit picture."],"url":"http://arxiv.org/abs/2406.09392v1","category":"cond-mat.dis-nn"}
{"created":"2024-06-13 17:51:10","title":"Investigation of Adaptive Hotspot-Aware Indexes for Oscillating Write-Heavy and Read-Heavy Workloads -- An Experimental Study","abstract":"HTAP systems are designed to handle transactional and analytical workloads. Besides a mixed workload at any given time, the workload can also change over time. A popular kind of continuously changing workload is one that oscillates between being write-heavy and being read-heavy. These oscillating workloads can be observed in many applications. Indexes, e.g., the B+-tree and the LSM-Tree cannot perform equally well all the time. Conventional adaptive indexing does not solve this issue either as it focuses on adapting in one direction. This paper investigates how to support oscillating workloads with adaptive indexes that adapt the underlying index structures in both directions. With the observation that real-world datasets are skewed, we focus on optimizing the indexes within the hotspot regions. We encapsulate the adaptation techniques into the Adaptive Hotspot-Aware Tree adaptive index. We compare the indexes and discuss the insights of each adaptation technique. Our investigation highlights the trade-offs of AHA-tree as well as the pros and cons of each design choice. AHA-tree can behave competitively as compared to an LSM-tree for write-heavy transactional workloads. Upon switching to a read-heavy analytical workload, and after some transient adaptation period, AHA-tree can behave as a B+-tree and can match the B+-trees read performance.","sentences":["HTAP systems are designed to handle transactional and analytical workloads.","Besides a mixed workload at any given time, the workload can also change over time.","A popular kind of continuously changing workload is one that oscillates between being write-heavy and being read-heavy.","These oscillating workloads can be observed in many applications.","Indexes, e.g., the B+-tree and the LSM-Tree cannot perform equally well all the time.","Conventional adaptive indexing does not solve this issue either as it focuses on adapting in one direction.","This paper investigates how to support oscillating workloads with adaptive indexes that adapt the underlying index structures in both directions.","With the observation that real-world datasets are skewed, we focus on optimizing the indexes within the hotspot regions.","We encapsulate the adaptation techniques into the Adaptive Hotspot-Aware Tree adaptive index.","We compare the indexes and discuss the insights of each adaptation technique.","Our investigation highlights the trade-offs of AHA-tree as well as the pros and cons of each design choice.","AHA-tree can behave competitively as compared to an LSM-tree for write-heavy transactional workloads.","Upon switching to a read-heavy analytical workload, and after some transient adaptation period, AHA-tree can behave as a B+-tree and can match the B+-trees read performance."],"url":"http://arxiv.org/abs/2406.09372v1","category":"cs.DB"}
{"created":"2024-06-13 17:51:00","title":"LRM-Zero: Training Large Reconstruction Models with Synthesized Data","abstract":"We present LRM-Zero, a Large Reconstruction Model (LRM) trained entirely on synthesized 3D data, achieving high-quality sparse-view 3D reconstruction. The core of LRM-Zero is our procedural 3D dataset, Zeroverse, which is automatically synthesized from simple primitive shapes with random texturing and augmentations (e.g., height fields, boolean differences, and wireframes). Unlike previous 3D datasets (e.g., Objaverse) which are often captured or crafted by humans to approximate real 3D data, Zeroverse completely ignores realistic global semantics but is rich in complex geometric and texture details that are locally similar to or even more intricate than real objects. We demonstrate that our LRM-Zero, trained with our fully synthesized Zeroverse, can achieve high visual quality in the reconstruction of real-world objects, competitive with models trained on Objaverse. We also analyze several critical design choices of Zeroverse that contribute to LRM-Zero's capability and training stability. Our work demonstrates that 3D reconstruction, one of the core tasks in 3D vision, can potentially be addressed without the semantics of real-world objects. The Zeroverse's procedural synthesis code and interactive visualization are available at: https://desaixie.github.io/lrm-zero/.","sentences":["We present LRM-Zero, a Large Reconstruction Model (LRM) trained entirely on synthesized 3D data, achieving high-quality sparse-view 3D reconstruction.","The core of LRM-Zero is our procedural 3D dataset, Zeroverse, which is automatically synthesized from simple primitive shapes with random texturing and augmentations (e.g., height fields, boolean differences, and wireframes).","Unlike previous 3D datasets (e.g., Objaverse) which are often captured or crafted by humans to approximate real 3D data, Zeroverse completely ignores realistic global semantics but is rich in complex geometric and texture details that are locally similar to or even more intricate than real objects.","We demonstrate that our LRM-Zero, trained with our fully synthesized Zeroverse, can achieve high visual quality in the reconstruction of real-world objects, competitive with models trained on Objaverse.","We also analyze several critical design choices of Zeroverse that contribute to LRM-Zero's capability and training stability.","Our work demonstrates that 3D reconstruction, one of the core tasks in 3D vision, can potentially be addressed without the semantics of real-world objects.","The Zeroverse's procedural synthesis code and interactive visualization are available at: https://desaixie.github.io/lrm-zero/."],"url":"http://arxiv.org/abs/2406.09371v1","category":"cs.CV"}
{"created":"2024-06-13 17:44:06","title":"More Likely Than You Think: Inclination-Driving Secular Resonances are Common in Known Exoplanet Systems","abstract":"Multi-planet systems face significant challenges to detection. For example, further orbiting planets have reduced signal-to-noise ratio in radial velocity detection methods, and small mutual inclinations between planets can prevent them from all transiting. One mechanism to excite mutual inclination between planets is secular resonance, where the nodal precession frequencies of the planets align such as to greatly increase the efficiency of angular momentum transport between planets. These resonances can significantly misalign planets from one another, hindering detection, and typically can only occur when there are three or more planets in the system. Naively, systems can only be in resonance for particular combinations of planet semimajor axes and masses; however, effects that alter the nodal precession frequencies of the planets, such as the decay of stellar oblateness, can significantly expand the region of parameter space where resonances occur. In this work, we explore known three-planet systems, determine whether they are in (or were in) secular resonance due to evolving stellar oblateness, and demonstrate the implications of resonance on their detectability and stability. We show that about 20% of a sample of three planet transiting systems seem to undergo these resonances early in their lives.","sentences":["Multi-planet systems face significant challenges to detection.","For example, further orbiting planets have reduced signal-to-noise ratio in radial velocity detection methods, and small mutual inclinations between planets can prevent them from all transiting.","One mechanism to excite mutual inclination between planets is secular resonance, where the nodal precession frequencies of the planets align such as to greatly increase the efficiency of angular momentum transport between planets.","These resonances can significantly misalign planets from one another, hindering detection, and typically can only occur when there are three or more planets in the system.","Naively, systems can only be in resonance for particular combinations of planet semimajor axes and masses; however, effects that alter the nodal precession frequencies of the planets, such as the decay of stellar oblateness, can significantly expand the region of parameter space where resonances occur.","In this work, we explore known three-planet systems, determine whether they are in (or were in) secular resonance due to evolving stellar oblateness, and demonstrate the implications of resonance on their detectability and stability.","We show that about 20% of a sample of three planet transiting systems seem to undergo these resonances early in their lives."],"url":"http://arxiv.org/abs/2406.09359v1","category":"astro-ph.EP"}
{"created":"2024-06-13 17:38:26","title":"On the Expressibility of the Reconstructional Color Refinement","abstract":"One of the most basic facts related to the famous Ulam reconstruction conjecture is that the connectedness of a graph can be determined by the deck of its vertex-deleted subgraphs, which are considered up to isomorphism. We strengthen this result by proving that connectedness can still be determined when the subgraphs in the deck are given up to equivalence under the color refinement isomorphism test. Consequently, this implies that connectedness is recognizable by Reconstruction Graph Neural Networks, a recently introduced GNN architecture inspired by the reconstruction conjecture (Cotta, Morris, Ribeiro 2021).","sentences":["One of the most basic facts related to the famous Ulam reconstruction conjecture is that the connectedness of a graph can be determined by the deck of its vertex-deleted subgraphs, which are considered up to isomorphism.","We strengthen this result by proving that connectedness can still be determined when the subgraphs in the deck are given up to equivalence under the color refinement isomorphism test.","Consequently, this implies that connectedness is recognizable by Reconstruction Graph Neural Networks, a recently introduced GNN architecture inspired by the reconstruction conjecture (Cotta, Morris, Ribeiro 2021)."],"url":"http://arxiv.org/abs/2406.09351v1","category":"cs.CC"}
{"created":"2024-06-13 17:33:16","title":"Measuring the CMB spectral distortions with COSMO: the multi-mode antenna system","abstract":"In this work, we present the design and manufacturing of the two multi-mode antenna arrays of the COSMO experiment and the preliminary beam pattern measurements of their fundamental mode compared with simulations.   COSMO is a cryogenic Martin-Puplett Fourier Transform Spectrometer that aims at measuring the isotropic y-type spectral distortion of the Cosmic Microwave Background from Antarctica, by performing differential measurements between the sky and an internal, cryogenic reference blackbody. To reduce the atmospheric contribution, a spinning wedge mirror performs fast sky-dips at varying elevations while fast, low-noise Kinetic Inductance detectors scan the interferogram.   Two arrays of antennas couple the radiation to the detectors. Each array consists of nine smooth-walled multi-mode feed-horns, operating in the $120-180$ GHz and $210-300$ GHz range, respectively. The multi-mode propagation helps increase the instrumental sensitivity without employing large focal planes with hundreds of detectors. The two arrays have a step-linear and a linear profile, respectively, and are obtained by superimposing aluminum plates made with CNC milling. The simulated multi-mode beam pattern has a $\\sim 20^{\\circ} - 26^{\\circ}$ FWHM for the low-frequency array and $\\sim 16^{\\circ}$ FWHM for the high-frequency one. The side lobes are below $-15$ dB.   To characterize the antenna response, we measured the beam pattern of the fundamental mode using a Vector Network Analyzer, in far-field conditions inside an anechoic chamber at room temperature. We completed the measurements of the low-frequency array and found a good agreement with the simulations. We also identified a few non-idealities that we attribute to the measuring setup and will further investigate. A comprehensive multi-mode measurement will be feasible at cryogenic temperature once the full receiver is integrated.","sentences":["In this work, we present the design and manufacturing of the two multi-mode antenna arrays of the COSMO experiment and the preliminary beam pattern measurements of their fundamental mode compared with simulations.   ","COSMO is a cryogenic Martin-Puplett Fourier Transform Spectrometer that aims at measuring the isotropic y-type spectral distortion of the Cosmic Microwave Background from Antarctica, by performing differential measurements between the sky and an internal, cryogenic reference blackbody.","To reduce the atmospheric contribution, a spinning wedge mirror performs fast sky-dips at varying elevations while fast, low-noise Kinetic Inductance detectors scan the interferogram.   ","Two arrays of antennas couple the radiation to the detectors.","Each array consists of nine smooth-walled multi-mode feed-horns, operating in the $120-180$ GHz and $210-300$ GHz range, respectively.","The multi-mode propagation helps increase the instrumental sensitivity without employing large focal planes with hundreds of detectors.","The two arrays have a step-linear and a linear profile, respectively, and are obtained by superimposing aluminum plates made with CNC milling.","The simulated multi-mode beam pattern has a $\\sim 20^{\\circ} - 26^{\\circ}$ FWHM for the low-frequency array and $\\sim 16^{\\circ}$ FWHM for the high-frequency one.","The side lobes are below $-15$ dB.   To characterize the antenna response, we measured the beam pattern of the fundamental mode using a Vector Network Analyzer, in far-field conditions inside an anechoic chamber at room temperature.","We completed the measurements of the low-frequency array and found a good agreement with the simulations.","We also identified a few non-idealities that we attribute to the measuring setup and will further investigate.","A comprehensive multi-mode measurement will be feasible at cryogenic temperature once the full receiver is integrated."],"url":"http://arxiv.org/abs/2406.09349v1","category":"astro-ph.IM"}
{"created":"2024-06-13 17:32:28","title":"Emergence of Fluctuation Relations in UNO","abstract":"Fluctuation theorems are generalisations of the second law that describe the relations between work, temperature, and free energy in thermodynamic processes and are used extensively in studies of irreversibility and entropy. Many experiments have verified these relations for different physical systems in the setting of thermodynamics. In this study, we observe the same behavior away from physical thermodynamics, namely for the card game UNO, by performing numerical simulations of the game. As the analog of work, we choose the number of steps one player needs to effect a transition in her deck; the other players and the remaining cards play the role of a finite, non-Markovian bath. We also compare our observation with is expected for a Markovian random walk.","sentences":["Fluctuation theorems are generalisations of the second law that describe the relations between work, temperature, and free energy in thermodynamic processes and are used extensively in studies of irreversibility and entropy.","Many experiments have verified these relations for different physical systems in the setting of thermodynamics.","In this study, we observe the same behavior away from physical thermodynamics, namely for the card game UNO, by performing numerical simulations of the game.","As the analog of work, we choose the number of steps one player needs to effect a transition in her deck; the other players and the remaining cards play the role of a finite, non-Markovian bath.","We also compare our observation with is expected for a Markovian random walk."],"url":"http://arxiv.org/abs/2406.09348v1","category":"physics.soc-ph"}
{"created":"2024-06-13 17:31:30","title":"Separations in the Representational Capabilities of Transformers and Recurrent Architectures","abstract":"Transformer architectures have been widely adopted in foundation models. Due to their high inference costs, there is renewed interest in exploring the potential of efficient recurrent architectures (RNNs). In this paper, we analyze the differences in the representational capabilities of Transformers and RNNs across several tasks of practical relevance, including index lookup, nearest neighbor, recognizing bounded Dyck languages, and string equality. For the tasks considered, our results show separations based on the size of the model required for different architectures. For example, we show that a one-layer Transformer of logarithmic width can perform index lookup, whereas an RNN requires a hidden state of linear size. Conversely, while constant-size RNNs can recognize bounded Dyck languages, we show that one-layer Transformers require a linear size for this task. Furthermore, we show that two-layer Transformers of logarithmic size can perform decision tasks such as string equality or disjointness, whereas both one-layer Transformers and recurrent models require linear size for these tasks. We also show that a log-size two-layer Transformer can implement the nearest neighbor algorithm in its forward pass; on the other hand recurrent models require linear size. Our constructions are based on the existence of $N$ nearly orthogonal vectors in $O(\\log N)$ dimensional space and our lower bounds are based on reductions from communication complexity problems. We supplement our theoretical results with experiments that highlight the differences in the performance of these architectures on practical-size sequences.","sentences":["Transformer architectures have been widely adopted in foundation models.","Due to their high inference costs, there is renewed interest in exploring the potential of efficient recurrent architectures (RNNs).","In this paper, we analyze the differences in the representational capabilities of Transformers and RNNs across several tasks of practical relevance, including index lookup, nearest neighbor, recognizing bounded Dyck languages, and string equality.","For the tasks considered, our results show separations based on the size of the model required for different architectures.","For example, we show that a one-layer Transformer of logarithmic width can perform index lookup, whereas an RNN requires a hidden state of linear size.","Conversely, while constant-size RNNs can recognize bounded Dyck languages, we show that one-layer Transformers require a linear size for this task.","Furthermore, we show that two-layer Transformers of logarithmic size can perform decision tasks such as string equality or disjointness, whereas both one-layer Transformers and recurrent models require linear size for these tasks.","We also show that a log-size two-layer Transformer can implement the nearest neighbor algorithm in its forward pass; on the other hand recurrent models require linear size.","Our constructions are based on the existence of $N$ nearly orthogonal vectors in $O(\\log N)$ dimensional space and our lower bounds are based on reductions from communication complexity problems.","We supplement our theoretical results with experiments that highlight the differences in the performance of these architectures on practical-size sequences."],"url":"http://arxiv.org/abs/2406.09347v1","category":"cs.LG"}
{"created":"2024-06-13 17:23:20","title":"Wavefront shaping simulations with augmented partial factorization","abstract":"Wavefront shaping can tailor multipath interference to control multiple scattering of waves in complex optical systems. However, full-wave simulations that capture multiple scattering are computationally demanding given the large system size and the large number of input channels. Recently, an \"augmented partial factorization\" (APF) method was proposed to significantly speed-up such full-wave simulations. In this tutorial, we illustrate how to perform wavefront shaping simulations with the APF method using the open-source frequency-domain electromagnetic scattering solver MESTI. We present the foundational concepts and then walk through four examples: computing the scattering matrix of a slab with random permittivities, open high-transmission channels through disorder, focusing inside disorder with phase conjugation, and reflection matrix computation in a spatial focused-beam basis. The goal is to lower the barrier for researchers to use simulations to explore the rich phenomena enabled by wavefront shaping.","sentences":["Wavefront shaping can tailor multipath interference to control multiple scattering of waves in complex optical systems.","However, full-wave simulations that capture multiple scattering are computationally demanding given the large system size and the large number of input channels.","Recently, an \"augmented partial factorization\" (APF) method was proposed to significantly speed-up such full-wave simulations.","In this tutorial, we illustrate how to perform wavefront shaping simulations with the APF method using the open-source frequency-domain electromagnetic scattering solver MESTI.","We present the foundational concepts and then walk through four examples: computing the scattering matrix of a slab with random permittivities, open high-transmission channels through disorder, focusing inside disorder with phase conjugation, and reflection matrix computation in a spatial focused-beam basis.","The goal is to lower the barrier for researchers to use simulations to explore the rich phenomena enabled by wavefront shaping."],"url":"http://arxiv.org/abs/2406.09342v1","category":"physics.optics"}
{"created":"2024-06-13 17:19:43","title":"Learning the Influence Graph of a High-Dimensional Markov Process with Memory","abstract":"Motivated by multiple applications in social networks, nervous systems, and financial risk analysis, we consider the problem of learning the underlying (directed) influence graph or causal graph of a high-dimensional multivariate discrete-time Markov process with memory. At any discrete time instant, each observed variable of the multivariate process is a binary string of random length, which is parameterized by an unobservable or hidden [0,1]-valued scalar. The hidden scalars corresponding to the variables evolve according to discrete-time linear stochastic dynamics dictated by the underlying influence graph whose nodes are the variables. We extend an existing algorithm for learning i.i.d. graphical models to this Markovian setting with memory and prove that it can learn the influence graph based on the binary observations using logarithmic (in number of variables or nodes) samples when the degree of the influence graph is bounded. The crucial analytical contribution of this work is the derivation of the sample complexity result by upper and lower bounding the rate of convergence of the observed Markov process with memory to its stationary distribution in terms of the parameters of the influence graph.","sentences":["Motivated by multiple applications in social networks, nervous systems, and financial risk analysis, we consider the problem of learning the underlying (directed) influence graph or causal graph of a high-dimensional multivariate discrete-time Markov process with memory.","At any discrete time instant, each observed variable of the multivariate process is a binary string of random length, which is parameterized by an unobservable or hidden [0,1]-valued scalar.","The hidden scalars corresponding to the variables evolve according to discrete-time linear stochastic dynamics dictated by the underlying influence graph whose nodes are the variables.","We extend an existing algorithm for learning i.i.d. graphical models to this Markovian setting with memory and prove that it can learn the influence graph based on the binary observations using logarithmic (in number of variables or nodes) samples when the degree of the influence graph is bounded.","The crucial analytical contribution of this work is the derivation of the sample complexity result by upper and lower bounding the rate of convergence of the observed Markov process with memory to its stationary distribution in terms of the parameters of the influence graph."],"url":"http://arxiv.org/abs/2406.09338v1","category":"cs.LG"}
{"created":"2024-06-13 17:18:25","title":"Perfectly hidden order and Z2 confinement transition in a fully packed monopole liquid","abstract":"We investigate a simple variant of spin ice whose degenerate ground states are densely packed monopole configurations. An applied field drives this model through a Z2 confinement transition, in absence of any local order parameter. Instead, this hidden order turns out to be diagnosed by a string order invisible to any local probe. We describe the transition in terms of a bosonic field theory with a pairing term, as well as a Kramers-Wannier duality into a 3D Ising model, which establishes its Z2 nature. This topological transition can be thought of as a variant of the celebrated U(1) Kasteleyn transition; but instead of the traditional '3/2'-order kink, the system shows critical scaling expected near a 3D Ising transition. Remarkably, however, the magnetic response scales with the critical exponent not of the susceptibility, but of the specific heat.","sentences":["We investigate a simple variant of spin ice whose degenerate ground states are densely packed monopole configurations.","An applied field drives this model through a Z2 confinement transition, in absence of any local order parameter.","Instead, this hidden order turns out to be diagnosed by a string order invisible to any local probe.","We describe the transition in terms of a bosonic field theory with a pairing term, as well as a Kramers-Wannier duality into a 3D Ising model, which establishes its Z2 nature.","This topological transition can be thought of as a variant of the celebrated U(1)","Kasteleyn transition; but instead of the traditional '3/2'-order kink, the system shows critical scaling expected near a 3D Ising transition.","Remarkably, however, the magnetic response scales with the critical exponent not of the susceptibility, but of the specific heat."],"url":"http://arxiv.org/abs/2406.09336v1","category":"cond-mat.str-el"}
{"created":"2024-06-13 17:14:17","title":"RoTipBot: Robotic Handling of Thin and Flexible Objects using Rotatable Tactile Sensors","abstract":"This paper introduces RoTipBot, a novel robotic system for handling thin, flexible objects. Different from previous works that are limited to singulating them using suction cups or soft grippers, RoTipBot can grasp and count multiple layers simultaneously, emulating human handling in various environments. Specifically, we develop a novel vision-based tactile sensor named RoTip that can rotate and sense contact information around its tip. Equipped with two RoTip sensors, RoTipBot feeds multiple layers of thin, flexible objects into the centre between its fingers, enabling effective grasping and counting. RoTip's tactile sensing ensures both fingers maintain good contact with the object, and an adjustment approach is designed to allow the gripper to adapt to changes in the object. Extensive experiments demonstrate the efficacy of the RoTip sensor and the RoTipBot approach. The results show that RoTipBot not only achieves a higher success rate but also grasps and counts multiple layers simultaneously -- capabilities not possible with previous methods. Furthermore, RoTipBot operates up to three times faster than state-of-the-art methods. The success of RoTipBot paves the way for future research in object manipulation using mobilised tactile sensors. All the materials used in this paper are available at \\url{https://sites.google.com/view/rotipbot}.","sentences":["This paper introduces RoTipBot, a novel robotic system for handling thin, flexible objects.","Different from previous works that are limited to singulating them using suction cups or soft grippers, RoTipBot can grasp and count multiple layers simultaneously, emulating human handling in various environments.","Specifically, we develop a novel vision-based tactile sensor named RoTip that can rotate and sense contact information around its tip.","Equipped with two RoTip sensors, RoTipBot feeds multiple layers of thin, flexible objects into the centre between its fingers, enabling effective grasping and counting.","RoTip's tactile sensing ensures both fingers maintain good contact with the object, and an adjustment approach is designed to allow the gripper to adapt to changes in the object.","Extensive experiments demonstrate the efficacy of the RoTip sensor and the RoTipBot approach.","The results show that RoTipBot not only achieves a higher success rate but also grasps and counts multiple layers simultaneously -- capabilities not possible with previous methods.","Furthermore, RoTipBot operates up to three times faster than state-of-the-art methods.","The success of RoTipBot paves the way for future research in object manipulation using mobilised tactile sensors.","All the materials used in this paper are available at \\url{https://sites.google.com/view/rotipbot}."],"url":"http://arxiv.org/abs/2406.09332v1","category":"cs.RO"}
{"created":"2024-06-13 17:06:15","title":"Towards AI Lesion Tracking in PET/CT Imaging: A Siamese-based CNN Pipeline applied on PSMA PET/CT Scans","abstract":"Assessing tumor response to systemic therapies is one of the main applications of PET/CT. Routinely, only a small subset of index lesions out of multiple lesions is analyzed. However, this operator dependent selection may bias the results due to possible significant inter-metastatic heterogeneity of response to therapy. Automated, AI based approaches for lesion tracking hold promise in enabling the analysis of many more lesions and thus providing a better assessment of tumor response. This work introduces a Siamese CNN approach for lesion tracking between PET/CT scans. Our approach is applied on the laborious task of tracking a high number of bone lesions in full-body baseline and follow-up [68Ga]Ga- or [18F]F-PSMA PET/CT scans after two cycles of [177Lu]Lu-PSMA therapy of metastatic castration resistant prostate cancer patients. Data preparation includes lesion segmentation and affine registration. Our algorithm extracts suitable lesion patches and forwards them into a Siamese CNN trained to classify the lesion patch pairs as corresponding or non-corresponding lesions. Experiments have been performed with different input patch types and a Siamese network in 2D and 3D. The CNN model successfully learned to classify lesion assignments, reaching a lesion tracking accuracy of 83 % in its best configuration with an AUC = 0.91. For remaining lesions the pipeline accomplished a re-identification rate of 89 %. We proved that a CNN may facilitate the tracking of multiple lesions in PSMA PET/CT scans. Future clinical studies are necessary if this improves the prediction of the outcome of therapies.","sentences":["Assessing tumor response to systemic therapies is one of the main applications of PET/CT.","Routinely, only a small subset of index lesions out of multiple lesions is analyzed.","However, this operator dependent selection may bias the results due to possible significant inter-metastatic heterogeneity of response to therapy.","Automated, AI based approaches for lesion tracking hold promise in enabling the analysis of many more lesions and thus providing a better assessment of tumor response.","This work introduces a Siamese CNN approach for lesion tracking between PET/CT scans.","Our approach is applied on the laborious task of tracking a high number of bone lesions in full-body baseline and follow-up [68Ga]Ga- or [18F]F-PSMA PET/CT scans after two cycles of [177Lu]Lu-PSMA therapy of metastatic castration resistant prostate cancer patients.","Data preparation includes lesion segmentation and affine registration.","Our algorithm extracts suitable lesion patches and forwards them into a Siamese CNN trained to classify the lesion patch pairs as corresponding or non-corresponding lesions.","Experiments have been performed with different input patch types and a Siamese network in 2D and 3D.","The CNN model successfully learned to classify lesion assignments, reaching a lesion tracking accuracy of 83 % in its best configuration with an AUC = 0.91.","For remaining lesions the pipeline accomplished a re-identification rate of 89 %.","We proved that a CNN may facilitate the tracking of multiple lesions in PSMA PET/CT scans.","Future clinical studies are necessary if this improves the prediction of the outcome of therapies."],"url":"http://arxiv.org/abs/2406.09327v1","category":"eess.IV"}
{"created":"2024-06-13 17:01:28","title":"Master of Disaster: A Disaster-Related Event Monitoring System From News Streams","abstract":"The need for a disaster-related event monitoring system has arisen due to the societal and economic impact caused by the increasing number of severe disaster events. An event monitoring system should be able to extract event-related information from texts, and discriminates event instances. We demonstrate our open-source event monitoring system, namely, Master of Disaster (MoD), which receives news streams, extracts event information, links extracted information to a knowledge graph (KG), in this case Wikidata, and discriminates event instances visually. The goal of event visualization is to group event mentions referring to the same real-world event instance so that event instance discrimination can be achieved by visual screening.","sentences":["The need for a disaster-related event monitoring system has arisen due to the societal and economic impact caused by the increasing number of severe disaster events.","An event monitoring system should be able to extract event-related information from texts, and discriminates event instances.","We demonstrate our open-source event monitoring system, namely, Master of Disaster (MoD), which receives news streams, extracts event information, links extracted information to a knowledge graph (KG), in this case Wikidata, and discriminates event instances visually.","The goal of event visualization is to group event mentions referring to the same real-world event instance so that event instance discrimination can be achieved by visual screening."],"url":"http://arxiv.org/abs/2406.09323v1","category":"cs.IR"}
{"created":"2024-06-13 16:44:58","title":"Learning High-dimensional Latent Variable Models via Doubly Stochastic Optimisation by Unadjusted Langevin","abstract":"Latent variable models are widely used in social and behavioural sciences, such as education, psychology, and political science. In recent years, high-dimensional latent variable models have become increasingly common for analysing large and complex data. Estimating high-dimensional latent variable models using marginal maximum likelihood is computationally demanding due to the complexity of integrals involved. To address this challenge, stochastic optimisation, which combines stochastic approximation and sampling techniques, has been shown to be effective. This method iterates between two steps -- (1) sampling the latent variables from their posterior distribution based on the current parameter estimate, and (2) updating the fixed parameters using an approximate stochastic gradient constructed from the latent variable samples. In this paper, we propose a computationally more efficient stochastic optimisation algorithm. This improvement is achieved through the use of a minibatch of observations when sampling latent variables and constructing stochastic gradients, and an unadjusted Langevin sampler that utilises the gradient of the negative complete-data log-likelihood to sample latent variables. Theoretical results are established for the proposed algorithm, showing that the iterative parameter update converges to the marginal maximum likelihood estimate as the number of iterations goes to infinity. Furthermore, the proposed algorithm is shown to scale well to high-dimensional settings through simulation studies and a personality test application with 30,000 respondents, 300 items, and 30 latent dimensions.","sentences":["Latent variable models are widely used in social and behavioural sciences, such as education, psychology, and political science.","In recent years, high-dimensional latent variable models have become increasingly common for analysing large and complex data.","Estimating high-dimensional latent variable models using marginal maximum likelihood is computationally demanding due to the complexity of integrals involved.","To address this challenge, stochastic optimisation, which combines stochastic approximation and sampling techniques, has been shown to be effective.","This method iterates between two steps -- (1) sampling the latent variables from their posterior distribution based on the current parameter estimate, and (2) updating the fixed parameters using an approximate stochastic gradient constructed from the latent variable samples.","In this paper, we propose a computationally more efficient stochastic optimisation algorithm.","This improvement is achieved through the use of a minibatch of observations when sampling latent variables and constructing stochastic gradients, and an unadjusted Langevin sampler that utilises the gradient of the negative complete-data log-likelihood to sample latent variables.","Theoretical results are established for the proposed algorithm, showing that the iterative parameter update converges to the marginal maximum likelihood estimate as the number of iterations goes to infinity.","Furthermore, the proposed algorithm is shown to scale well to high-dimensional settings through simulation studies and a personality test application with 30,000 respondents, 300 items, and 30 latent dimensions."],"url":"http://arxiv.org/abs/2406.09311v1","category":"stat.CO"}
{"created":"2024-06-13 16:39:44","title":"All-optically tunable enantio-selectivity and chirality transfer","abstract":"Detecting and controlling the chirality of materials play an essential role in exploring nature, providing new avenues for material creation, discrimination, and manipulation. In such tasks, chiral reagents are essential in defining or enhancing the chiral dichroism response. However, ignoring their influences on the symmetry of the medium hamper the ability to control and induce asymmetric synthesis. Here, we propose a simple but versatile chirality transfer method for synthesizing and manipulating the chirality of medium. The proposed method induces the dispersion of light in a neutral atomic system, allowing to deterministically and tunably control the chirality transfer using a helical field. First, we theoretically analyze the mechanism for this optically induced chirality transfer. Afterwards, we experimentally study the enantio-sensitive feature of the medium exposed to the auxiliary chiral field. This result can be suppressed or enhanced in a deterministic enantio-selection, opening up an efficient way to manipulate asymmetric synthesis.","sentences":["Detecting and controlling the chirality of materials play an essential role in exploring nature, providing new avenues for material creation, discrimination, and manipulation.","In such tasks, chiral reagents are essential in defining or enhancing the chiral dichroism response.","However, ignoring their influences on the symmetry of the medium hamper the ability to control and induce asymmetric synthesis.","Here, we propose a simple but versatile chirality transfer method for synthesizing and manipulating the chirality of medium.","The proposed method induces the dispersion of light in a neutral atomic system, allowing to deterministically and tunably control the chirality transfer using a helical field.","First, we theoretically analyze the mechanism for this optically induced chirality transfer.","Afterwards, we experimentally study the enantio-sensitive feature of the medium exposed to the auxiliary chiral field.","This result can be suppressed or enhanced in a deterministic enantio-selection, opening up an efficient way to manipulate asymmetric synthesis."],"url":"http://arxiv.org/abs/2406.09303v1","category":"physics.optics"}
{"created":"2024-06-13 16:39:05","title":"The reflection complexity of sequences over finite alphabets","abstract":"In combinatorics on words, the well-studied factor complexity function $\\rho_{\\bf x}$ of a sequence ${\\bf x}$ over a finite alphabet counts, for any nonnegative integer $n$, the number of distinct length-$n$ factors of ${\\bf x}$. In this paper, we introduce the \\emph{reflection complexity} function $r_{\\bf x}$ to enumerate the factors occurring in a sequence ${\\bf x}$, up to reversing the order of symbols in a word. We introduce and prove results on $r_{\\bf x}$ regarding its growth properties and relationship with other complexity functions. We prove that if ${\\bf x}$ is $k$-automatic, then $r_{\\bf x}$ is computably $k$-regular, and we use the software {\\tt Walnut} to evaluate the reflection complexity of automatic sequences, such as the Thue--Morse sequence. We prove a Morse--Hedlund-type result characterizing eventually periodic sequences in terms of their reflection complexity, and we deduce a characterization of Sturmian sequences. Furthermore, we investigate the reflection complexity of episturmian, $(s+1)$-dimensional billiard, and Rote sequences. There are still many unanswered questions about this measure.","sentences":["In combinatorics on words, the well-studied factor complexity function $\\rho_{\\bf x}$ of a sequence ${\\bf x}$ over a finite alphabet counts, for any nonnegative integer $n$, the number of distinct length-$n$ factors of ${\\bf x}$. In this paper, we introduce the \\emph{reflection complexity} function $r_{\\bf x}$ to enumerate the factors occurring in a sequence ${\\bf x}$, up to reversing the order of symbols in a word.","We introduce and prove results on $r_{\\bf x}$ regarding its growth properties and relationship with other complexity functions.","We prove that if ${\\bf x}$ is $k$-automatic, then $r_{\\bf x}$ is computably $k$-regular, and we use the software {\\tt Walnut} to evaluate the reflection complexity of automatic sequences, such as the Thue--Morse sequence.","We prove a Morse--Hedlund-type result characterizing eventually periodic sequences in terms of their reflection complexity, and we deduce a characterization of Sturmian sequences.","Furthermore, we investigate the reflection complexity of episturmian, $(s+1)$-dimensional billiard, and Rote sequences.","There are still many unanswered questions about this measure."],"url":"http://arxiv.org/abs/2406.09302v1","category":"math.CO"}
{"created":"2024-06-13 16:36:23","title":"Nested Sequents for Quasi-transitive Modal Logics","abstract":"Previous works by Gor\\'e, Postniece and Tiu have provided sound and cut-free complete proof systems for modal logics extended with path axioms using the formalism of nested sequent. Our aim is to provide (i) a constructive cut-elimination procedure and (ii) alternative modular formulations for these systems. We present our methodology to achieve these two goals on a subclass of path axioms, namely quasi-transitivity axioms.","sentences":["Previous works by Gor\\'e, Postniece and Tiu have provided sound and cut-free complete proof systems for modal logics extended with path axioms using the formalism of nested sequent.","Our aim is to provide (i) a constructive cut-elimination procedure and (ii) alternative modular formulations for these systems.","We present our methodology to achieve these two goals on a subclass of path axioms, namely quasi-transitivity axioms."],"url":"http://arxiv.org/abs/2406.09300v1","category":"cs.LO"}
{"created":"2024-06-13 16:27:56","title":"Exploring Spoken Language Identification Strategies for Automatic Transcription of Multilingual Broadcast and Institutional Speech","abstract":"This paper addresses spoken language identification (SLI) and speech recognition of multilingual broadcast and institutional speech, real application scenarios that have been rarely addressed in the SLI literature. Observing that in these domains language changes are mostly associated with speaker changes, we propose a cascaded system consisting of speaker diarization and language identification and compare it with more traditional language identification and language diarization systems. Results show that the proposed system often achieves lower language classification and language diarization error rates (up to 10% relative language diarization error reduction and 60% relative language confusion reduction) and leads to lower WERs on multilingual test sets (more than 8% relative WER reduction), while at the same time does not negatively affect speech recognition on monolingual audio (with an absolute WER increase between 0.1% and 0.7% w.r.t. monolingual ASR).","sentences":["This paper addresses spoken language identification (SLI) and speech recognition of multilingual broadcast and institutional speech, real application scenarios that have been rarely addressed in the SLI literature.","Observing that in these domains language changes are mostly associated with speaker changes, we propose a cascaded system consisting of speaker diarization and language identification and compare it with more traditional language identification and language diarization systems.","Results show that the proposed system often achieves lower language classification and language diarization error rates (up to 10% relative language diarization error reduction and 60% relative language confusion reduction) and leads to lower WERs on multilingual test sets (more than 8% relative WER reduction), while at the same time does not negatively affect speech recognition on monolingual audio (with an absolute WER increase between 0.1% and 0.7% w.r.t.","monolingual ASR)."],"url":"http://arxiv.org/abs/2406.09290v1","category":"eess.AS"}
{"created":"2024-06-13 16:26:34","title":"Optimal magnetization switching via spin-orbit torque on the surface of a topological insulator","abstract":"An optimal protocol for the current-induced switching of a perpendicularly magnetized nanoelement placed on the surface of a topological insulator is presented. The time dependence of both in-plane components of the surface current that induces the magnetization reversal via Dirac spin-orbit torque with minimal Joule heating is derived analytically as a function of the required switching time and material properties. It is demonstrated that a particularly energy-efficient switching is realized for vanishing dampinglike torque. The optimal reversal time providing a tradeoff between the switching speed and energy efficiency is derived. The obtained switching protocol is contrasted with the one realized in heavy-metal systems. Topological insulators provide a highly tunable platform for the realization of energy-efficient magnetization switching.","sentences":["An optimal protocol for the current-induced switching of a perpendicularly magnetized nanoelement placed on the surface of a topological insulator is presented.","The time dependence of both in-plane components of the surface current that induces the magnetization reversal via Dirac spin-orbit torque with minimal Joule heating is derived analytically as a function of the required switching time and material properties.","It is demonstrated that a particularly energy-efficient switching is realized for vanishing dampinglike torque.","The optimal reversal time providing a tradeoff between the switching speed and energy efficiency is derived.","The obtained switching protocol is contrasted with the one realized in heavy-metal systems.","Topological insulators provide a highly tunable platform for the realization of energy-efficient magnetization switching."],"url":"http://arxiv.org/abs/2406.09287v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-13 16:23:46","title":"The O-minimal Zilber Conjecture in Higher Dimensions","abstract":"We prove the higher dimensional case of the o-minimal variant of Zilber's Restricted Trichotomy Conjecture. More precisely, let $\\mathcal R$ be an o-minimal expansion of a real closed field, let $M$ be an interpretable set in $\\mathcal R$, and let $\\mathcal M=(M,...)$ be a reduct of the induced structure on $M$. If $\\mathcal M$ is strongly minimal and not locally modular, then $\\dim_{\\mathcal R}(M)=2$. As an application, we prove the Zilber trichotomy for all strongly minimal structures interpreted in the theory of compact complex manifolds.","sentences":["We prove the higher dimensional case of the o-minimal variant of Zilber's Restricted Trichotomy Conjecture.","More precisely, let $\\mathcal R$ be an o-minimal expansion of a real closed field, let $M$ be an interpretable set in $\\mathcal R$, and let $\\mathcal M=(M,...",")$ be a reduct of the induced structure on $M$. If $\\mathcal M$ is strongly minimal and not locally modular, then $\\dim_{\\mathcal R}(M)=2$. As an application, we prove the Zilber trichotomy for all strongly minimal structures interpreted in the theory of compact complex manifolds."],"url":"http://arxiv.org/abs/2406.09285v1","category":"math.LO"}
{"created":"2024-06-13 16:14:34","title":"Multigrid preconditioning for discontinuous Galerkin discretizations of an elliptic optimal control problem with a convection-dominated state equation","abstract":"We consider discontinuous Galerkin methods for an elliptic distributed optimal control problem constrained by a convection-dominated problem. We prove global optimal convergence rates using an inf-sup condition, with the diffusion parameter $\\varepsilon$ and regularization parameter $\\beta$ explicitly tracked. We then propose a multilevel preconditioner based on downwind ordering to solve the discretized system. The preconditioner only requires two approximate solves of single convection-dominated equations using multigrid methods. Moreover, for the strongly convection-dominated case, only two sweeps of block Gauss-Seidel iterations are needed. We also derive a simple bound indicating the role played by the multigrid preconditioner. Numerical results are shown to support our findings.","sentences":["We consider discontinuous Galerkin methods for an elliptic distributed optimal control problem constrained by a convection-dominated problem.","We prove global optimal convergence rates using an inf-sup condition, with the diffusion parameter $\\varepsilon$ and regularization parameter $\\beta$ explicitly tracked.","We then propose a multilevel preconditioner based on downwind ordering to solve the discretized system.","The preconditioner only requires two approximate solves of single convection-dominated equations using multigrid methods.","Moreover, for the strongly convection-dominated case, only two sweeps of block Gauss-Seidel iterations are needed.","We also derive a simple bound indicating the role played by the multigrid preconditioner.","Numerical results are shown to support our findings."],"url":"http://arxiv.org/abs/2406.09276v1","category":"math.NA"}
{"created":"2024-06-13 16:02:03","title":"Flexible Heteroscedastic Count Regression with Deep Double Poisson Networks","abstract":"Neural networks that can produce accurate, input-conditional uncertainty representations are critical for real-world applications. Recent progress on heteroscedastic continuous regression has shown great promise for calibrated uncertainty quantification on complex tasks, like image regression. However, when these methods are applied to discrete regression tasks, such as crowd counting, ratings prediction, or inventory estimation, they tend to produce predictive distributions with numerous pathologies. We propose to address these issues by training a neural network to output the parameters of a Double Poisson distribution, which we call the Deep Double Poisson Network (DDPN). In contrast to existing methods that are trained to minimize Gaussian negative log likelihood (NLL), DDPNs produce a proper probability mass function over discrete output. Additionally, DDPNs naturally model under-, over-, and equi-dispersion, unlike networks trained with the more rigid Poisson and Negative Binomial parameterizations. We show DDPNs 1) vastly outperform existing discrete models; 2) meet or exceed the accuracy and flexibility of networks trained with Gaussian NLL; 3) produce proper predictive distributions over discrete counts; and 4) exhibit superior out-of-distribution detection. DDPNs can easily be applied to a variety of count regression datasets including tabular, image, point cloud, and text data.","sentences":["Neural networks that can produce accurate, input-conditional uncertainty representations are critical for real-world applications.","Recent progress on heteroscedastic continuous regression has shown great promise for calibrated uncertainty quantification on complex tasks, like image regression.","However, when these methods are applied to discrete regression tasks, such as crowd counting, ratings prediction, or inventory estimation, they tend to produce predictive distributions with numerous pathologies.","We propose to address these issues by training a neural network to output the parameters of a Double Poisson distribution, which we call the Deep Double Poisson Network (DDPN).","In contrast to existing methods that are trained to minimize Gaussian negative log likelihood (NLL), DDPNs produce a proper probability mass function over discrete output.","Additionally, DDPNs naturally model under-, over-, and equi-dispersion, unlike networks trained with the more rigid Poisson and Negative Binomial parameterizations.","We show DDPNs 1) vastly outperform existing discrete models; 2) meet or exceed the accuracy and flexibility of networks trained with Gaussian NLL; 3) produce proper predictive distributions over discrete counts; and 4) exhibit superior out-of-distribution detection.","DDPNs can easily be applied to a variety of count regression datasets including tabular, image, point cloud, and text data."],"url":"http://arxiv.org/abs/2406.09262v1","category":"cs.LG"}
{"created":"2024-06-13 15:58:37","title":"Integral solutions to systems of diagonal equations","abstract":"In this paper, we obtain the asymptotic formula for the number of integral solutions to a system of diagonal equations. We obtain the asymptotic formula for the number of solutions with variables restricted to smooth numbers as well. We improve the required number of variables compared to previous results by incorporating the recent progress on Waring's problem and the resolution of Vinogradov's mean value theorem.","sentences":["In this paper, we obtain the asymptotic formula for the number of integral solutions to a system of diagonal equations.","We obtain the asymptotic formula for the number of solutions with variables restricted to smooth numbers as well.","We improve the required number of variables compared to previous results by incorporating the recent progress on Waring's problem and the resolution of Vinogradov's mean value theorem."],"url":"http://arxiv.org/abs/2406.09256v1","category":"math.NT"}
{"created":"2024-06-13 15:46:41","title":"On averages of completely multiplicative functions over co-prime integer pairs","abstract":"Recently, Donoso, Le, Moreira and Sun studied the asymptotic behavior of the averages of completely multiplicative functions over the Gaussian integers. They derived Wirsing's theorem for Gaussian integers, answered a question of Frantzikinakis and Host for sum of two squares, and obtained a variant of a theorem of Bergelson and Richter on ergodic averages along the number of prime factors of integers. In this paper, we will show the analogue of these results for co-prime integer pairs. Moreover, building on Frantzikinakis and Host's results, we obtain some convergences on the multilinear averages of multiplicative functions over primitive lattice points.","sentences":["Recently, Donoso, Le, Moreira and Sun studied the asymptotic behavior of the averages of completely multiplicative functions over the Gaussian integers.","They derived Wirsing's theorem for Gaussian integers, answered a question of Frantzikinakis and Host for sum of two squares, and obtained a variant of a theorem of Bergelson and Richter on ergodic averages along the number of prime factors of integers.","In this paper, we will show the analogue of these results for co-prime integer pairs.","Moreover, building on Frantzikinakis and Host's results, we obtain some convergences on the multilinear averages of multiplicative functions over primitive lattice points."],"url":"http://arxiv.org/abs/2406.09243v1","category":"math.NT"}
{"created":"2024-06-13 15:41:38","title":"EHAZOP: A Proof of Concept Ethical Hazard Analysis of an Assistive Robot","abstract":"The use of assistive robots in domestic environments can raise significant ethical concerns, from the risk of individual ethical harm to wider societal ethical impacts including culture flattening and compromise of human dignity. It is therefore essential to ensure that technological development of these robots is informed by robust and inclusive techniques for mitigating ethical concerns. This paper presents EHAZOP, a method for conducting an ethical hazard analysis on an assistive robot. EHAZOP draws upon collaborative, creative and structured processes originating within safety engineering, using these to identify ethical concerns associated with the operation of a given assistive robot. We present the results of a proof of concept study of EHAZOP, demonstrating the potential for this process to identify diverse ethical hazards in these systems.","sentences":["The use of assistive robots in domestic environments can raise significant ethical concerns, from the risk of individual ethical harm to wider societal ethical impacts including culture flattening and compromise of human dignity.","It is therefore essential to ensure that technological development of these robots is informed by robust and inclusive techniques for mitigating ethical concerns.","This paper presents EHAZOP, a method for conducting an ethical hazard analysis on an assistive robot.","EHAZOP draws upon collaborative, creative and structured processes originating within safety engineering, using these to identify ethical concerns associated with the operation of a given assistive robot.","We present the results of a proof of concept study of EHAZOP, demonstrating the potential for this process to identify diverse ethical hazards in these systems."],"url":"http://arxiv.org/abs/2406.09239v1","category":"cs.RO"}
{"created":"2024-06-13 15:36:43","title":"Variational Mode Decomposition as Trusted Data Augmentation in ML-based Power System Stability Assessment","abstract":"Balanced data is required for deep neural networks (DNNs) when learning to perform power system stability assessment. However, power system measurement data contains relatively few events from where power system dynamics can be learnt. To mitigate this imbalance, we propose a novel data augmentation strategy preserving the dynamic characteristics to be learnt. The augmentation is performed using Variational Mode Decomposition. The detrended and the augmented data are tested for distributions similarity using Kernel Maximum Mean Discrepancy test. In addition, the effectiveness of the augmentation methodology is validated via training an Encoder DNN utilizing original data, testing using the augmented data, and evaluating the Encoder's performance employing several metrics.","sentences":["Balanced data is required for deep neural networks (DNNs) when learning to perform power system stability assessment.","However, power system measurement data contains relatively few events from where power system dynamics can be learnt.","To mitigate this imbalance, we propose a novel data augmentation strategy preserving the dynamic characteristics to be learnt.","The augmentation is performed using Variational Mode Decomposition.","The detrended and the augmented data are tested for distributions similarity using Kernel Maximum Mean Discrepancy test.","In addition, the effectiveness of the augmentation methodology is validated via training an Encoder DNN utilizing original data, testing using the augmented data, and evaluating the Encoder's performance employing several metrics."],"url":"http://arxiv.org/abs/2406.09235v1","category":"eess.SP"}
{"created":"2024-06-13 15:31:38","title":"Elastic scattering on a quantum computer","abstract":"Scattering probes the internal structure of quantum systems. We calculate the two-particle elastic scattering phase shift for a short-ranged interaction on a quantum computer. Short-ranged interactions with a large scattering length or shallow bound state describe a universality class that is of interest in atomic, condensed matter, nuclear, and particle physics. The phase shift is calculated by relating the ground state energy of the interacting particles in a harmonic trap. The relaxation method is used as the variational quantum eigensolver for the ground state calculation. Schmidt decomposition is used to reduce quantum circuits nominally requiring tens of qubits to 2-qubit circuits, thus reducing the noise in quantum measurements. Calculations in multi-particle systems with many-body interactions would benefit from this reduction of qubits in noisy quantum processors.","sentences":["Scattering probes the internal structure of quantum systems.","We calculate the two-particle elastic scattering phase shift for a short-ranged interaction on a quantum computer.","Short-ranged interactions with a large scattering length or shallow bound state describe a universality class that is of interest in atomic, condensed matter, nuclear, and particle physics.","The phase shift is calculated by relating the ground state energy of the interacting particles in a harmonic trap.","The relaxation method is used as the variational quantum eigensolver for the ground state calculation.","Schmidt decomposition is used to reduce quantum circuits nominally requiring tens of qubits to 2-qubit circuits, thus reducing the noise in quantum measurements.","Calculations in multi-particle systems with many-body interactions would benefit from this reduction of qubits in noisy quantum processors."],"url":"http://arxiv.org/abs/2406.09231v1","category":"nucl-th"}
{"created":"2024-06-13 15:27:47","title":"Well-posedness of aggregation-diffusion systems with irregular kernels","abstract":"We consider aggregation-diffusion equations with merely bounded nonlocal interaction potential $K$. We are interested in establishing their well-posedness theory when the nonlocal interaction potential $K$ is neither differentiable nor positive (semi-)definite, thus preventing application of classical arguments. We prove the existence of weak solutions in two cases: if the mass of the initial data is sufficiently small, or if the interaction potential is symmetric and of bounded variation without any smallness assumption. The latter allows one to exploit the dissipation of the free energy in an optimal way, which is an entirely new approach. Remarkably, in both cases, under the additional condition that $\\nabla K\\ast K$ is in $L^2$, we can prove that the solution is smooth and unique. When $K$ is a characteristic function of a ball, we construct the classical unique solution. Under additional structural conditions we extend these results to the $n$-species system.","sentences":["We consider aggregation-diffusion equations with merely bounded nonlocal interaction potential $K$. We are interested in establishing their well-posedness theory when the nonlocal interaction potential $K$ is neither differentiable nor positive (semi-)definite, thus preventing application of classical arguments.","We prove the existence of weak solutions in two cases: if the mass of the initial data is sufficiently small, or if the interaction potential is symmetric and of bounded variation without any smallness assumption.","The latter allows one to exploit the dissipation of the free energy in an optimal way, which is an entirely new approach.","Remarkably, in both cases, under the additional condition that $\\nabla K\\ast K$ is in $L^2$, we can prove that the solution is smooth and unique.","When $K$ is a characteristic function of a ball, we construct the classical unique solution.","Under additional structural conditions we extend these results to the $n$-species system."],"url":"http://arxiv.org/abs/2406.09227v1","category":"math.AP"}
{"created":"2024-06-13 15:24:53","title":"Solving the Synthetic Riddle of Colloidal 2D PbTe Nanoplatelets with Tunable Near-Infrared Emission","abstract":"Near-infrared emitting colloidal two-dimensional (2D) PbX (X=S, Se) nanoplatelets have emerged as interesting materials with strong size quantisation in the thickness dimension. They act as model systems for efficient charge carrier multiplication and hold potential as intriguing candidates for finer-based photonic quantum applications. However, synthetic access to the third family member, 2D PbTe, remains elusive due to a challenging precursor chemistry. Here, we report a direct synthesis for 2D PbTe nanoplatelets (NPLs) with unable photoluminescence (PL, 910-1460 nm (1.36-0.85 eV), PLQY 1-15 %), based on aminophosphine precursor chemistry. Ex-situ transamination of tris(dimethylamino)phosphine telluride with octylamine is confirmed by 31P NMR and yields a reactive tellurium precursor for the formation of 2D PbTe NPLs at temperatures as low as 0 {\\deg}C. The PL position of the PbTe NPLs is unable by controlling the Pb:Te ration in the reaction. GIWAXS confirms the 2D geometry of the NPLs and the formation of superlattices. The importance of a post-synthetic passivation of the PbTe NPLs by PbI2 to ensure colloidal stability of the otherwise oxygen sensitive samples is supported by X-ray photoelectron spectroscopy. Our results expand and complete the row of lead chalcogenide-based 2D NPLs, opening up new ways for further pushing the optical properties of 2D NPLs into the infrared and toward technologically relevant wavelengths.","sentences":["Near-infrared emitting colloidal two-dimensional (2D) PbX (X=S, Se) nanoplatelets have emerged as interesting materials with strong size quantisation in the thickness dimension.","They act as model systems for efficient charge carrier multiplication and hold potential as intriguing candidates for finer-based photonic quantum applications.","However, synthetic access to the third family member, 2D PbTe, remains elusive due to a challenging precursor chemistry.","Here, we report a direct synthesis for 2D PbTe nanoplatelets (NPLs) with unable photoluminescence (PL, 910-1460 nm (1.36-0.85 eV), PLQY 1-15 %), based on aminophosphine precursor chemistry.","Ex-situ transamination of tris(dimethylamino)phosphine telluride with octylamine is confirmed by 31P NMR and yields a reactive tellurium precursor for the formation of 2D PbTe NPLs at temperatures as low as 0 {\\deg}C.","The PL position of the PbTe NPLs is unable by controlling the Pb:Te ration in the reaction.","GIWAXS confirms the 2D geometry of the NPLs and the formation of superlattices.","The importance of a post-synthetic passivation of the PbTe NPLs by PbI2 to ensure colloidal stability of the otherwise oxygen sensitive samples is supported by X-ray photoelectron spectroscopy.","Our results expand and complete the row of lead chalcogenide-based 2D NPLs, opening up new ways for further pushing the optical properties of 2D NPLs into the infrared and toward technologically relevant wavelengths."],"url":"http://arxiv.org/abs/2406.09223v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-13 15:16:34","title":"Three New Galactic Globular Cluster Candidates: FSR1700, Teutsch67, and CWNU4193","abstract":"The VISTA Variables in the Via L\\'actea Extended Survey (VVVX) enables exploration of previously uncharted territories within the inner Milky Way (MW), particularly those obscured by stellar crowding and intense extinction. Our objective is to identify and investigate new star clusters to elucidate their intrinsic characteristics. Specifically, we are focused on uncovering new candidate Globular Clusters (GCs) situated at low Galactic latitudes, with the ultimate goal of completing the census of the MW GC system. Leveraging a combination of Near-InfraRed (NIR) data from the VVVX survey and Two Micron All Sky Survey (2MASS), along with optical photometry and precise proper motions (PMs) from the Gaia Data Release 3 (DR3), we are conducting a systematic characterisation of new GCs. As a result, we report the discovery and characterisation of four new Galactic clusters named FSR 1700, FSR 1415, CWNU 4193, and Teutsch 67, all located within the MW disk. We estimate a wide range of reddening, with values ranging from 0.44 to 0.73 mag for E(J-Ks). The heliocentric distances span from 10.3 to 13.2 kpc. Additionally, we determine their metallicities and ages, finding a range of -0.85 to -0.75 dex for [Fe/H] and ages approximately close to 11 Gyr, respectively. FSR 1415 is an exception, it is an old open cluster with age = 3 Gyr and [Fe/H] = -0.10. Furthermore, we fitted the radial density profiles to derive their structural parameters like tidal radius, core radius, and concentration parameters. In conclusion, based on their positions, kinematics, metallicities, and ages, and comparing our findings with existing literature, we categorise FSR 1700, Teutsch 67 and CWNU 4193 as genuine GC candidates, while FSR 1415 is an old open cluster exhibiting characteristics of a post core-collapse cluster.","sentences":["The VISTA Variables in the Via L\\'actea Extended Survey (VVVX) enables exploration of previously uncharted territories within the inner Milky Way (MW), particularly those obscured by stellar crowding and intense extinction.","Our objective is to identify and investigate new star clusters to elucidate their intrinsic characteristics.","Specifically, we are focused on uncovering new candidate Globular Clusters (GCs) situated at low Galactic latitudes, with the ultimate goal of completing the census of the MW GC system.","Leveraging a combination of Near-InfraRed (NIR) data from the VVVX survey and Two Micron All Sky Survey (2MASS), along with optical photometry and precise proper motions (PMs) from the Gaia Data Release 3 (DR3), we are conducting a systematic characterisation of new GCs.","As a result, we report the discovery and characterisation of four new Galactic clusters named FSR 1700, FSR 1415, CWNU 4193, and Teutsch 67, all located within the MW disk.","We estimate a wide range of reddening, with values ranging from 0.44 to 0.73 mag for E(J-Ks).","The heliocentric distances span from 10.3 to 13.2 kpc.","Additionally, we determine their metallicities and ages, finding a range of -0.85 to -0.75 dex for [Fe/H] and ages approximately close to 11 Gyr, respectively.","FSR 1415 is an exception, it is an old open cluster with age = 3 Gyr and [Fe/H] = -0.10.","Furthermore, we fitted the radial density profiles to derive their structural parameters like tidal radius, core radius, and concentration parameters.","In conclusion, based on their positions, kinematics, metallicities, and ages, and comparing our findings with existing literature, we categorise FSR 1700, Teutsch 67 and CWNU 4193 as genuine GC candidates, while FSR 1415 is an old open cluster exhibiting characteristics of a post core-collapse cluster."],"url":"http://arxiv.org/abs/2406.09216v1","category":"astro-ph.GA"}
{"created":"2024-06-13 15:00:17","title":"Optimizing Visual Question Answering Models for Driving: Bridging the Gap Between Human and Machine Attention Patterns","abstract":"Visual Question Answering (VQA) models play a critical role in enhancing the perception capabilities of autonomous driving systems by allowing vehicles to analyze visual inputs alongside textual queries, fostering natural interaction and trust between the vehicle and its occupants or other road users. This study investigates the attention patterns of humans compared to a VQA model when answering driving-related questions, revealing disparities in the objects observed. We propose an approach integrating filters to optimize the model's attention mechanisms, prioritizing relevant objects and improving accuracy. Utilizing the LXMERT model for a case study, we compare attention patterns of the pre-trained and Filter Integrated models, alongside human answers using images from the NuImages dataset, gaining insights into feature prioritization. We evaluated the models using a Subjective scoring framework which shows that the integration of the feature encoder filter has enhanced the performance of the VQA model by refining its attention mechanisms.","sentences":["Visual Question Answering (VQA) models play a critical role in enhancing the perception capabilities of autonomous driving systems by allowing vehicles to analyze visual inputs alongside textual queries, fostering natural interaction and trust between the vehicle and its occupants or other road users.","This study investigates the attention patterns of humans compared to a VQA model when answering driving-related questions, revealing disparities in the objects observed.","We propose an approach integrating filters to optimize the model's attention mechanisms, prioritizing relevant objects and improving accuracy.","Utilizing the LXMERT model for a case study, we compare attention patterns of the pre-trained and Filter Integrated models, alongside human answers using images from the NuImages dataset, gaining insights into feature prioritization.","We evaluated the models using a Subjective scoring framework which shows that the integration of the feature encoder filter has enhanced the performance of the VQA model by refining its attention mechanisms."],"url":"http://arxiv.org/abs/2406.09203v1","category":"cs.CV"}
{"created":"2024-06-13 14:59:45","title":"Enhanced Object Detection: A Study on Vast Vocabulary Object Detection Track for V3Det Challenge 2024","abstract":"In this technical report, we present our findings from the research conducted on the Vast Vocabulary Visual Detection (V3Det) dataset for Supervised Vast Vocabulary Visual Detection task. How to deal with complex categories and detection boxes has become a difficulty in this track. The original supervised detector is not suitable for this task. We have designed a series of improvements, including adjustments to the network structure, changes to the loss function, and design of training strategies. Our model has shown improvement over the baseline and achieved excellent rankings on the Leaderboard for both the Vast Vocabulary Object Detection (Supervised) track and the Open Vocabulary Object Detection (OVD) track of the V3Det Challenge 2024.","sentences":["In this technical report, we present our findings from the research conducted on the Vast Vocabulary Visual Detection (V3Det) dataset for Supervised Vast Vocabulary Visual Detection task.","How to deal with complex categories and detection boxes has become a difficulty in this track.","The original supervised detector is not suitable for this task.","We have designed a series of improvements, including adjustments to the network structure, changes to the loss function, and design of training strategies.","Our model has shown improvement over the baseline and achieved excellent rankings on the Leaderboard for both the Vast Vocabulary Object Detection (Supervised) track and the Open Vocabulary Object Detection (OVD) track of the V3Det Challenge 2024."],"url":"http://arxiv.org/abs/2406.09201v1","category":"cs.CV"}
{"created":"2024-06-13 14:55:11","title":"Adaptive Slot Attention: Object Discovery with Dynamic Slot Number","abstract":"Object-centric learning (OCL) extracts the representation of objects with slots, offering an exceptional blend of flexibility and interpretability for abstracting low-level perceptual features. A widely adopted method within OCL is slot attention, which utilizes attention mechanisms to iteratively refine slot representations. However, a major drawback of most object-centric models, including slot attention, is their reliance on predefining the number of slots. This not only necessitates prior knowledge of the dataset but also overlooks the inherent variability in the number of objects present in each instance. To overcome this fundamental limitation, we present a novel complexity-aware object auto-encoder framework. Within this framework, we introduce an adaptive slot attention (AdaSlot) mechanism that dynamically determines the optimal number of slots based on the content of the data. This is achieved by proposing a discrete slot sampling module that is responsible for selecting an appropriate number of slots from a candidate list. Furthermore, we introduce a masked slot decoder that suppresses unselected slots during the decoding process. Our framework, tested extensively on object discovery tasks with various datasets, shows performance matching or exceeding top fixed-slot models. Moreover, our analysis substantiates that our method exhibits the capability to dynamically adapt the slot number according to each instance's complexity, offering the potential for further exploration in slot attention research. Project will be available at https://kfan21.github.io/AdaSlot/","sentences":["Object-centric learning (OCL) extracts the representation of objects with slots, offering an exceptional blend of flexibility and interpretability for abstracting low-level perceptual features.","A widely adopted method within OCL is slot attention, which utilizes attention mechanisms to iteratively refine slot representations.","However, a major drawback of most object-centric models, including slot attention, is their reliance on predefining the number of slots.","This not only necessitates prior knowledge of the dataset but also overlooks the inherent variability in the number of objects present in each instance.","To overcome this fundamental limitation, we present a novel complexity-aware object auto-encoder framework.","Within this framework, we introduce an adaptive slot attention (AdaSlot) mechanism that dynamically determines the optimal number of slots based on the content of the data.","This is achieved by proposing a discrete slot sampling module that is responsible for selecting an appropriate number of slots from a candidate list.","Furthermore, we introduce a masked slot decoder that suppresses unselected slots during the decoding process.","Our framework, tested extensively on object discovery tasks with various datasets, shows performance matching or exceeding top fixed-slot models.","Moreover, our analysis substantiates that our method exhibits the capability to dynamically adapt the slot number according to each instance's complexity, offering the potential for further exploration in slot attention research.","Project will be available at https://kfan21.github.io/AdaSlot/"],"url":"http://arxiv.org/abs/2406.09196v1","category":"cs.CV"}
{"created":"2024-06-13 14:42:17","title":"Detection-Rate-Emphasized Multi-objective Evolutionary Feature Selection for Network Intrusion Detection","abstract":"Network intrusion detection is one of the most important issues in the field of cyber security, and various machine learning techniques have been applied to build intrusion detection systems. However, since the number of features to describe the network connections is often large, where some features are redundant or noisy, feature selection is necessary in such scenarios, which can both improve the efficiency and accuracy. Recently, some researchers focus on using multi-objective evolutionary algorithms (MOEAs) to select features. But usually, they only consider the number of features and classification accuracy as the objectives, resulting in unsatisfactory performance on a critical metric, detection rate. This will lead to the missing of many real attacks and bring huge losses to the network system. In this paper, we propose DR-MOFS to model the feature selection problem in network intrusion detection as a three-objective optimization problem, where the number of features, accuracy and detection rate are optimized simultaneously, and use MOEAs to solve it. Experiments on two popular network intrusion detection datasets NSL-KDD and UNSW-NB15 show that in most cases the proposed method can outperform previous methods, i.e., lead to fewer features, higher accuracy and detection rate.","sentences":["Network intrusion detection is one of the most important issues in the field of cyber security, and various machine learning techniques have been applied to build intrusion detection systems.","However, since the number of features to describe the network connections is often large, where some features are redundant or noisy, feature selection is necessary in such scenarios, which can both improve the efficiency and accuracy.","Recently, some researchers focus on using multi-objective evolutionary algorithms (MOEAs) to select features.","But usually, they only consider the number of features and classification accuracy as the objectives, resulting in unsatisfactory performance on a critical metric, detection rate.","This will lead to the missing of many real attacks and bring huge losses to the network system.","In this paper, we propose DR-MOFS to model the feature selection problem in network intrusion detection as a three-objective optimization problem, where the number of features, accuracy and detection rate are optimized simultaneously, and use MOEAs to solve it.","Experiments on two popular network intrusion detection datasets NSL-KDD and UNSW-NB15 show that in most cases the proposed method can outperform previous methods, i.e., lead to fewer features, higher accuracy and detection rate."],"url":"http://arxiv.org/abs/2406.09180v1","category":"cs.LG"}
{"created":"2024-06-13 14:39:40","title":"Scalable and Flexible Causal Discovery with an Efficient Test for Adjacency","abstract":"To make accurate predictions, understand mechanisms, and design interventions in systems of many variables, we wish to learn causal graphs from large scale data. Unfortunately the space of all possible causal graphs is enormous so scalably and accurately searching for the best fit to the data is a challenge. In principle we could substantially decrease the search space, or learn the graph entirely, by testing the conditional independence of variables. However, deciding if two variables are adjacent in a causal graph may require an exponential number of tests. Here we build a scalable and flexible method to evaluate if two variables are adjacent in a causal graph, the Differentiable Adjacency Test (DAT). DAT replaces an exponential number of tests with a provably equivalent relaxed problem. It then solves this problem by training two neural networks. We build a graph learning method based on DAT, DAT-Graph, that can also learn from data with interventions. DAT-Graph can learn graphs of 1000 variables with state of the art accuracy. Using the graph learned by DAT-Graph, we also build models that make much more accurate predictions of the effects of interventions on large scale RNA sequencing data.","sentences":["To make accurate predictions, understand mechanisms, and design interventions in systems of many variables, we wish to learn causal graphs from large scale data.","Unfortunately the space of all possible causal graphs is enormous so scalably and accurately searching for the best fit to the data is a challenge.","In principle we could substantially decrease the search space, or learn the graph entirely, by testing the conditional independence of variables.","However, deciding if two variables are adjacent in a causal graph may require an exponential number of tests.","Here we build a scalable and flexible method to evaluate if two variables are adjacent in a causal graph, the Differentiable Adjacency Test (DAT).","DAT replaces an exponential number of tests with a provably equivalent relaxed problem.","It then solves this problem by training two neural networks.","We build a graph learning method based on DAT, DAT-Graph, that can also learn from data with interventions.","DAT-Graph can learn graphs of 1000 variables with state of the art accuracy.","Using the graph learned by DAT-Graph, we also build models that make much more accurate predictions of the effects of interventions on large scale RNA sequencing data."],"url":"http://arxiv.org/abs/2406.09177v1","category":"stat.ML"}
{"created":"2024-06-13 14:35:11","title":"Potion: Towards Poison Unlearning","abstract":"Adversarial attacks by malicious actors on machine learning systems, such as introducing poison triggers into training datasets, pose significant risks. The challenge in resolving such an attack arises in practice when only a subset of the poisoned data can be identified. This necessitates the development of methods to remove, i.e. unlearn, poison triggers from already trained models with only a subset of the poison data available. The requirements for this task significantly deviate from privacy-focused unlearning where all of the data to be forgotten by the model is known. Previous work has shown that the undiscovered poisoned samples lead to a failure of established unlearning methods, with only one method, Selective Synaptic Dampening (SSD), showing limited success. Even full retraining, after the removal of the identified poison, cannot address this challenge as the undiscovered poison samples lead to a reintroduction of the poison trigger in the model. Our work addresses two key challenges to advance the state of the art in poison unlearning. First, we introduce a novel outlier-resistant method, based on SSD, that significantly improves model protection and unlearning performance. Second, we introduce Poison Trigger Neutralisation (PTN) search, a fast, parallelisable, hyperparameter search that utilises the characteristic \"unlearning versus model protection\" trade-off to find suitable hyperparameters in settings where the forget set size is unknown and the retain set is contaminated. We benchmark our contributions using ResNet-9 on CIFAR10 and WideResNet-28x10 on CIFAR100. Experimental results show that our method heals 93.72% of poison compared to SSD with 83.41% and full retraining with 40.68%. We achieve this while also lowering the average model accuracy drop caused by unlearning from 5.68% (SSD) to 1.41% (ours).","sentences":["Adversarial attacks by malicious actors on machine learning systems, such as introducing poison triggers into training datasets, pose significant risks.","The challenge in resolving such an attack arises in practice when only a subset of the poisoned data can be identified.","This necessitates the development of methods to remove, i.e. unlearn, poison triggers from already trained models with only a subset of the poison data available.","The requirements for this task significantly deviate from privacy-focused unlearning where all of the data to be forgotten by the model is known.","Previous work has shown that the undiscovered poisoned samples lead to a failure of established unlearning methods, with only one method, Selective Synaptic Dampening (SSD), showing limited success.","Even full retraining, after the removal of the identified poison, cannot address this challenge as the undiscovered poison samples lead to a reintroduction of the poison trigger in the model.","Our work addresses two key challenges to advance the state of the art in poison unlearning.","First, we introduce a novel outlier-resistant method, based on SSD, that significantly improves model protection and unlearning performance.","Second, we introduce Poison Trigger Neutralisation (PTN) search, a fast, parallelisable, hyperparameter search that utilises the characteristic \"unlearning versus model protection\" trade-off to find suitable hyperparameters in settings where the forget set size is unknown and the retain set is contaminated.","We benchmark our contributions using ResNet-9 on CIFAR10 and WideResNet-28x10 on CIFAR100.","Experimental results show that our method heals 93.72% of poison compared to SSD with 83.41% and full retraining with 40.68%.","We achieve this while also lowering the average model accuracy drop caused by unlearning from 5.68% (SSD) to 1.41% (ours)."],"url":"http://arxiv.org/abs/2406.09173v1","category":"cs.LG"}
{"created":"2024-06-13 14:31:19","title":"Test of Time: A Benchmark for Evaluating LLMs on Temporal Reasoning","abstract":"Large language models (LLMs) have showcased remarkable reasoning capabilities, yet they remain susceptible to errors, particularly in temporal reasoning tasks involving complex temporal logic. Existing research has explored LLM performance on temporal reasoning using diverse datasets and benchmarks. However, these studies often rely on real-world data that LLMs may have encountered during pre-training or employ anonymization techniques that can inadvertently introduce factual inconsistencies. In this work, we address these limitations by introducing novel synthetic datasets specifically designed to assess LLM temporal reasoning abilities in various scenarios. The diversity of question types across these datasets enables systematic investigation into the impact of the problem structure, size, question type, fact order, and other factors on LLM performance. Our findings provide valuable insights into the strengths and weaknesses of current LLMs in temporal reasoning tasks. To foster further research in this area, we are open-sourcing the datasets and evaluation framework used in our experiments: https://huggingface.co/datasets/baharef/ToT.","sentences":["Large language models (LLMs) have showcased remarkable reasoning capabilities, yet they remain susceptible to errors, particularly in temporal reasoning tasks involving complex temporal logic.","Existing research has explored LLM performance on temporal reasoning using diverse datasets and benchmarks.","However, these studies often rely on real-world data that LLMs may have encountered during pre-training or employ anonymization techniques that can inadvertently introduce factual inconsistencies.","In this work, we address these limitations by introducing novel synthetic datasets specifically designed to assess LLM temporal reasoning abilities in various scenarios.","The diversity of question types across these datasets enables systematic investigation into the impact of the problem structure, size, question type, fact order, and other factors on LLM performance.","Our findings provide valuable insights into the strengths and weaknesses of current LLMs in temporal reasoning tasks.","To foster further research in this area, we are open-sourcing the datasets and evaluation framework used in our experiments: https://huggingface.co/datasets/baharef/ToT."],"url":"http://arxiv.org/abs/2406.09170v1","category":"cs.CL"}
{"created":"2024-06-13 14:30:39","title":"Empirical Networks are Sparse: Enhancing Multi-Edge Models with Zero-Inflation","abstract":"Real-world networks are sparse. As we show in this article, even when a large number of interactions is observed most node pairs remain disconnected. We demonstrate that classical multi-edge network models, such as the $G(N,p)$, configuration models, and stochastic block models, fail to accurately capture this phenomenon. To mitigate this issue, zero-inflation must be integrated into these traditional models. Through zero-inflation, we incorporate a mechanism that accounts for the excess number of zeroes (disconnected pairs) observed in empirical data. By performing an analysis on all the datasets from the Sociopatterns repository, we illustrate how zero-inflated models more accurately reflect the sparsity and heavy-tailed edge count distributions observed in empirical data. Our findings underscore that failing to account for these ubiquitous properties in real-world networks inadvertently leads to biased models which do not accurately represent complex systems and their dynamics.","sentences":["Real-world networks are sparse.","As we show in this article, even when a large number of interactions is observed most node pairs remain disconnected.","We demonstrate that classical multi-edge network models, such as the $G(N,p)$, configuration models, and stochastic block models, fail to accurately capture this phenomenon.","To mitigate this issue, zero-inflation must be integrated into these traditional models.","Through zero-inflation, we incorporate a mechanism that accounts for the excess number of zeroes (disconnected pairs) observed in empirical data.","By performing an analysis on all the datasets from the Sociopatterns repository, we illustrate how zero-inflated models more accurately reflect the sparsity and heavy-tailed edge count distributions observed in empirical data.","Our findings underscore that failing to account for these ubiquitous properties in real-world networks inadvertently leads to biased models which do not accurately represent complex systems and their dynamics."],"url":"http://arxiv.org/abs/2406.09169v1","category":"cs.SI"}
{"created":"2024-06-13 14:27:40","title":"Gluon scattering on the self-dual dyon","abstract":"The computation of scattering amplitudes in the presence of non-trivial background gauge fields is an important but extremely difficult problem in quantum field theory. In even the simplest backgrounds, obtaining explicit formulae for processes involving more than a few external particles is often intractable. Recently, it has been shown that remarkable progress can be made by considering background fields which are chiral in nature. In this paper, we obtain a compact expression for the tree-level, maximal helicity violating (MHV) scattering amplitude of an arbitrary number of gluons in the background of a self-dual dyon. This is a Cartan-valued, complex gauge field sourced by a point particle with equal electric and magnetic charges, and can be viewed as the self-dual version of a Coulomb field. Twistor theory enables us to manifest the underlying integrability of the self-dual dyon, trivializing the perturbative expansion in the MHV sector. The formula contains a single position-space integral over a spatial slice, which can be evaluated explicitly in simple cases. As an application of the formula, we show that the holomorphic collinear splitting functions of gluons in the self-dual dyon background are un-deformed from a trivial background, meaning that holomorphic celestial OPE coefficients and the associated chiral algebra are similarly un-deformed. We also comment on extensions of our MHV formula to the full tree-level gluon S-matrix.","sentences":["The computation of scattering amplitudes in the presence of non-trivial background gauge fields is an important but extremely difficult problem in quantum field theory.","In even the simplest backgrounds, obtaining explicit formulae for processes involving more than a few external particles is often intractable.","Recently, it has been shown that remarkable progress can be made by considering background fields which are chiral in nature.","In this paper, we obtain a compact expression for the tree-level, maximal helicity violating (MHV) scattering amplitude of an arbitrary number of gluons in the background of a self-dual dyon.","This is a Cartan-valued, complex gauge field sourced by a point particle with equal electric and magnetic charges, and can be viewed as the self-dual version of a Coulomb field.","Twistor theory enables us to manifest the underlying integrability of the self-dual dyon, trivializing the perturbative expansion in the MHV sector.","The formula contains a single position-space integral over a spatial slice, which can be evaluated explicitly in simple cases.","As an application of the formula, we show that the holomorphic collinear splitting functions of gluons in the self-dual dyon background are un-deformed from a trivial background, meaning that holomorphic celestial OPE coefficients and the associated chiral algebra are similarly un-deformed.","We also comment on extensions of our MHV formula to the full tree-level gluon S-matrix."],"url":"http://arxiv.org/abs/2406.09165v1","category":"hep-th"}
{"created":"2024-06-13 14:13:24","title":"Controlling the Magnetic Properties of the van der Waals Multiferroic Crystals Co$_{1-x}$Ni$_{x}$I$_2$","abstract":"The structurally related compounds NiI$_2$ and CoI$_2$ are multiferroic van der Waals materials, in which helimagnetic orders exist simultaneously with electric polarization. Here, we report on the evolution of the crystal structure and of the magnetic properties across the solid solution Co$_{1-x}$Ni$_{x}$I$_2$. We have successfully grown crystals of the whole range of the solid solution, i.e. $x = 0-1$, by employing the self-selecting vapor growth (SSVG) technique and by carefully tuning the synthesis conditions according to the chemical composition. Our structural investigations show that the crystal symmetry changes from $P\\bar{3}m1$ to $R\\bar{3}m$ when Ni substitutes for Co beyond $x = 0.2$. Both the lattice parameters and magnetic properties evolve continuously and smoothly from one end member to the other, showing that they can be finely tuned by the chemical composition. We also observe that the Ni substitution degree in the solid solution affects the metamagnetic transition typical for CoI$_2$ at high magnetic fields. In particular, we find the existence of the metamagnetic transition similar to that for CoI$_2$ in the NiI$_2$ structure. Based on magnetic measurements we construct the phase diagram of the Co$_{1-x}$Ni$_{x}$I$_2$ system. Controlling the magnetic properties by the chemical composition may open new pathways for the fabrication of electronic devices made of two-dimensional (2D) flakes of multiferroic van der Waals materials.","sentences":["The structurally related compounds NiI$_2$ and CoI$_2$ are multiferroic van der Waals materials, in which helimagnetic orders exist simultaneously with electric polarization.","Here, we report on the evolution of the crystal structure and of the magnetic properties across the solid solution Co$_{1-x}$Ni$_{x}$I$_2$.","We have successfully grown crystals of the whole range of the solid solution, i.e. $x = 0-1$, by employing the self-selecting vapor growth (SSVG) technique and by carefully tuning the synthesis conditions according to the chemical composition.","Our structural investigations show that the crystal symmetry changes from $P\\bar{3}m1$ to $R\\bar{3}m$ when Ni substitutes for Co beyond $x = 0.2$. Both the lattice parameters and magnetic properties evolve continuously and smoothly from one end member to the other, showing that they can be finely tuned by the chemical composition.","We also observe that the Ni substitution degree in the solid solution affects the metamagnetic transition typical for CoI$_2$ at high magnetic fields.","In particular, we find the existence of the metamagnetic transition similar to that for CoI$_2$ in the NiI$_2","$ structure.","Based on magnetic measurements we construct the phase diagram of the Co$_{1-x}$Ni$_{x}$I$_2$ system.","Controlling the magnetic properties by the chemical composition may open new pathways for the fabrication of electronic devices made of two-dimensional (2D) flakes of multiferroic van der Waals materials."],"url":"http://arxiv.org/abs/2406.09146v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-13 14:11:48","title":"fast-resolve: Fast Bayesian Radio Interferometric Imaging","abstract":"Context: Interferometric imaging is algorithmically and computationally challenging as there is no unique inversion from the measurement data back to the sky maps, and the datasets can be very large. Many imaging methods already exist, but most of them focus either on the accuracy or the computational aspect. Aims: This paper aims to reduce the computational complexity of the Bayesian imaging algorithm resolve, enabling the application of Bayesian imaging for larger datasets. Methods: By combining computational shortcuts of the CLEAN algorithm with the Bayesian imaging algorithm resolve we developed an accurate and fast imaging algorithm which we name fast-resolve. Results: We validate the accuracy of the presented fast-resolve algorithm by comparing it with results from resolve on VLA Cygnus A data. Furthermore, we demonstrate the computational advantages of fast-resolve on a large MeerKAT ESO 137-006 dataset which is computationally out of reach for resolve. Conclusions: The presented algorithm is significantly faster than previous Bayesian imaging algorithms, broadening the applicability of Bayesian interferometric imaging. Specifically for the single channel VLA Cygnus A datasets fast-resolve is about $144$ times faster than resolve. For the MeerKAT dataset with multiple channels the computational speedup of fast-resolve is even larger.","sentences":["Context: Interferometric imaging is algorithmically and computationally challenging as there is no unique inversion from the measurement data back to the sky maps, and the datasets can be very large.","Many imaging methods already exist, but most of them focus either on the accuracy or the computational aspect.","Aims:","This paper aims to reduce the computational complexity of the Bayesian imaging algorithm resolve, enabling the application of Bayesian imaging for larger datasets.","Methods: By combining computational shortcuts of the CLEAN algorithm with the Bayesian imaging algorithm resolve we developed an accurate and fast imaging algorithm which we name fast-resolve.","Results: We validate the accuracy of the presented fast-resolve algorithm by comparing it with results from resolve on VLA Cygnus A data.","Furthermore, we demonstrate the computational advantages of fast-resolve on a large MeerKAT ESO 137-006 dataset which is computationally out of reach for resolve.","Conclusions: The presented algorithm is significantly faster than previous Bayesian imaging algorithms, broadening the applicability of Bayesian interferometric imaging.","Specifically for the single channel VLA Cygnus A datasets fast-resolve is about $144$ times faster than resolve.","For the MeerKAT dataset with multiple channels the computational speedup of fast-resolve is even larger."],"url":"http://arxiv.org/abs/2406.09144v1","category":"astro-ph.IM"}
{"created":"2024-06-13 14:10:57","title":"Optimal Control of Agent-Based Dynamics under Deep Galerkin Feedback Laws","abstract":"Ever since the concepts of dynamic programming were introduced, one of the most difficult challenges has been to adequately address high-dimensional control problems. With growing dimensionality, the utilisation of Deep Neural Networks promises to circumvent the issue of an otherwise exponentially increasing complexity. The paper specifically investigates the sampling issues the Deep Galerkin Method is subjected to. It proposes a drift relaxation-based sampling approach to alleviate the symptoms of high-variance policy approximations. This is validated on mean-field control problems; namely, the variations of the opinion dynamics presented by the Sznajd and the Hegselmann-Krause model. The resulting policies induce a significant cost reduction over manually optimised control functions and show improvements on the Linear-Quadratic Regulator problem over the Deep FBSDE approach.","sentences":["Ever since the concepts of dynamic programming were introduced, one of the most difficult challenges has been to adequately address high-dimensional control problems.","With growing dimensionality, the utilisation of Deep Neural Networks promises to circumvent the issue of an otherwise exponentially increasing complexity.","The paper specifically investigates the sampling issues the Deep Galerkin Method is subjected to.","It proposes a drift relaxation-based sampling approach to alleviate the symptoms of high-variance policy approximations.","This is validated on mean-field control problems; namely, the variations of the opinion dynamics presented by the Sznajd and the Hegselmann-Krause model.","The resulting policies induce a significant cost reduction over manually optimised control functions and show improvements on the Linear-Quadratic Regulator problem over the Deep FBSDE approach."],"url":"http://arxiv.org/abs/2406.09141v1","category":"cs.LG"}
{"created":"2024-06-13 14:04:34","title":"RH-SQL: Refined Schema and Hardness Prompt for Text-to-SQL","abstract":"Text-to-SQL is a technology that converts natural language queries into the structured query language SQL. A novel research approach that has recently gained attention focuses on methods based on the complexity of SQL queries, achieving notable performance improvements. However, existing methods entail significant storage and training costs, which hampers their practical application. To address this issue, this paper introduces a method for Text-to-SQL based on Refined Schema and Hardness Prompt. By filtering out low-relevance schema information with a refined schema and identifying query hardness through a Language Model (LM) to form prompts, this method reduces storage and training costs while maintaining performance. It's worth mentioning that this method is applicable to any sequence-to-sequence (seq2seq) LM. Our experiments on the Spider dataset, specifically with large-scale LMs, achieved an exceptional Execution accuracy (EX) of 82.6%, demonstrating the effectiveness and greater suitability of our method for real-world applications.","sentences":["Text-to-SQL is a technology that converts natural language queries into the structured query language SQL.","A novel research approach that has recently gained attention focuses on methods based on the complexity of SQL queries, achieving notable performance improvements.","However, existing methods entail significant storage and training costs, which hampers their practical application.","To address this issue, this paper introduces a method for Text-to-SQL based on Refined Schema and Hardness Prompt.","By filtering out low-relevance schema information with a refined schema and identifying query hardness through a Language Model (LM) to form prompts, this method reduces storage and training costs while maintaining performance.","It's worth mentioning that this method is applicable to any sequence-to-sequence (seq2seq) LM.","Our experiments on the Spider dataset, specifically with large-scale LMs, achieved an exceptional Execution accuracy (EX) of 82.6%, demonstrating the effectiveness and greater suitability of our method for real-world applications."],"url":"http://arxiv.org/abs/2406.09133v1","category":"cs.CL"}
{"created":"2024-06-13 14:01:08","title":"CoastTerm: a Corpus for Multidisciplinary Term Extraction in Coastal Scientific Literature","abstract":"The growing impact of climate change on coastal areas, particularly active but fragile regions, necessitates collaboration among diverse stakeholders and disciplines to formulate effective environmental protection policies. We introduce a novel specialized corpus comprising 2,491 sentences from 410 scientific abstracts concerning coastal areas, for the Automatic Term Extraction (ATE) and Classification (ATC) tasks. Inspired by the ARDI framework, focused on the identification of Actors, Resources, Dynamics and Interactions, we automatically extract domain terms and their distinct roles in the functioning of coastal systems by leveraging monolingual and multilingual transformer models. The evaluation demonstrates consistent results, achieving an F1 score of approximately 80\\% for automated term extraction and F1 of 70\\% for extracting terms and their labels. These findings are promising and signify an initial step towards the development of a specialized Knowledge Base dedicated to coastal areas.","sentences":["The growing impact of climate change on coastal areas, particularly active but fragile regions, necessitates collaboration among diverse stakeholders and disciplines to formulate effective environmental protection policies.","We introduce a novel specialized corpus comprising 2,491 sentences from 410 scientific abstracts concerning coastal areas, for the Automatic Term Extraction (ATE) and Classification (ATC) tasks.","Inspired by the ARDI framework, focused on the identification of Actors, Resources, Dynamics and Interactions, we automatically extract domain terms and their distinct roles in the functioning of coastal systems by leveraging monolingual and multilingual transformer models.","The evaluation demonstrates consistent results, achieving an F1 score of approximately 80\\% for automated term extraction and F1 of 70\\% for extracting terms and their labels.","These findings are promising and signify an initial step towards the development of a specialized Knowledge Base dedicated to coastal areas."],"url":"http://arxiv.org/abs/2406.09128v1","category":"cs.CL"}
{"created":"2024-06-13 13:51:45","title":"Direct Imitation Learning-based Visual Servoing using the Large Projection Formulation","abstract":"Today robots must be safe, versatile, and user-friendly to operate in unstructured and human-populated environments. Dynamical system-based imitation learning enables robots to perform complex tasks stably and without explicit programming, greatly simplifying their real-world deployment. To exploit the full potential of these systems it is crucial to implement closed loops that use visual feedback. Vision permits to cope with environmental changes, but is complex to handle due to the high dimension of the image space. This study introduces a dynamical system-based imitation learning for direct visual servoing. It leverages off-the-shelf deep learning-based perception backbones to extract robust features from the raw input image, and an imitation learning strategy to execute sophisticated robot motions. The learning blocks are integrated using the large projection task priority formulation. As demonstrated through extensive experimental analysis, the proposed method realizes complex tasks with a robotic manipulator.","sentences":["Today robots must be safe, versatile, and user-friendly to operate in unstructured and human-populated environments.","Dynamical system-based imitation learning enables robots to perform complex tasks stably and without explicit programming, greatly simplifying their real-world deployment.","To exploit the full potential of these systems it is crucial to implement closed loops that use visual feedback.","Vision permits to cope with environmental changes, but is complex to handle due to the high dimension of the image space.","This study introduces a dynamical system-based imitation learning for direct visual servoing.","It leverages off-the-shelf deep learning-based perception backbones to extract robust features from the raw input image, and an imitation learning strategy to execute sophisticated robot motions.","The learning blocks are integrated using the large projection task priority formulation.","As demonstrated through extensive experimental analysis, the proposed method realizes complex tasks with a robotic manipulator."],"url":"http://arxiv.org/abs/2406.09120v1","category":"cs.RO"}
{"created":"2024-06-13 13:43:41","title":"Quantum space-time Poincar\u00e9 inequality for Lindblad dynamics","abstract":"We investigate the mixing properties of primitive hypocoercive Lindblad dynamics. By extending the variational framework originally developed for underdamped Langevin dynamics, we derive fully explicit and constructive exponential decay estimates for the convergence in the noncommutative $L^2$-norm. Our analysis relies on establishing a quantum analog of space-time Poincar\\'{e} inequalities. To complement these hypocoercive estimates, we also analyze the limiting behavior of the spectral gap for Lindblad dynamics with a large coherent contribution, providing sharper convergence rate estimates in this asymptotic regime. A number of concrete examples are provided as applications of our theoretical results.","sentences":["We investigate the mixing properties of primitive hypocoercive Lindblad dynamics.","By extending the variational framework originally developed for underdamped Langevin dynamics, we derive fully explicit and constructive exponential decay estimates for the convergence in the noncommutative $L^2$-norm.","Our analysis relies on establishing a quantum analog of space-time Poincar\\'{e} inequalities.","To complement these hypocoercive estimates, we also analyze the limiting behavior of the spectral gap for Lindblad dynamics with a large coherent contribution, providing sharper convergence rate estimates in this asymptotic regime.","A number of concrete examples are provided as applications of our theoretical results."],"url":"http://arxiv.org/abs/2406.09115v1","category":"quant-ph"}
{"created":"2024-06-13 13:36:23","title":"The Brownian loop measure on Riemann surfaces and applications to length spectra","abstract":"We prove a simple identity relating the length spectrum of a Riemann surface to that of the same surface with an arbitrary number of additional cusps. Our proof uses the Brownian loop measure introduced by Lawler and Werner. In particular, we express the total mass of Brownian loops in a fixed free homotopy class on any Riemann surface in terms of the length of the geodesic representative for the complete constant curvature metric. This expression also allows us to write the electrical thickness of a compact set in $\\mathbb C$ separating $0$ and $\\infty$, or the Velling--Kirillov K\\\"ahler potential, in terms of the Brownian loop measure and the zeta-regularized determinant of Laplacian as a renormalization of the Brownian loop measure with respect to the length spectrum.","sentences":["We prove a simple identity relating the length spectrum of a Riemann surface to that of the same surface with an arbitrary number of additional cusps.","Our proof uses the Brownian loop measure introduced by Lawler and Werner.","In particular, we express the total mass of Brownian loops in a fixed free homotopy class on any Riemann surface in terms of the length of the geodesic representative for the complete constant curvature metric.","This expression also allows us to write the electrical thickness of a compact set in $\\mathbb C$ separating $0$ and $\\infty$, or the Velling--Kirillov K\\\"ahler potential, in terms of the Brownian loop measure and the zeta-regularized determinant of Laplacian as a renormalization of the Brownian loop measure with respect to the length spectrum."],"url":"http://arxiv.org/abs/2406.09108v1","category":"math.GT"}
{"created":"2024-06-13 13:33:52","title":"Square-roots and lattices","abstract":"We construct a point set in the Euclidean plane that elucidates the relationship between the fine-scale statistics of the fractional parts of $\\sqrt n$ and directional statistics for a shifted lattice. We show that the randomly rotated, and then stretched, point set converges in distribution to a lattice-like random point process. This follows closely the arguments in Elkies and McMullen's original analysis for the gap statistics of $\\sqrt{n}$ mod 1 in terms of random affine lattices [Duke Math. J. 123 (2004), 95-139]. There is, however, a curious subtlety: the limit process emerging in our construction is NOT invariant under the standard $\\mathrm{SL}(2,\\mathbb{R})$-action on $\\mathbb{R}^2$.","sentences":["We construct a point set in the Euclidean plane that elucidates the relationship between the fine-scale statistics of the fractional parts of $\\sqrt n$ and directional statistics for a shifted lattice.","We show that the randomly rotated, and then stretched, point set converges in distribution to a lattice-like random point process.","This follows closely the arguments in Elkies and McMullen's original analysis for the gap statistics of $\\sqrt{n}$ mod 1 in terms of random affine lattices [Duke Math.","J. 123 (2004), 95-139].","There is, however, a curious subtlety: the limit process emerging in our construction is NOT invariant under the standard $\\mathrm{SL}(2,\\mathbb{R})$-action on $\\mathbb{R}^2$."],"url":"http://arxiv.org/abs/2406.09107v1","category":"math.NT"}
{"created":"2024-06-13 13:27:27","title":"Adaptive Actor-Critic Based Optimal Regulation for Drift-Free Uncertain Nonlinear Systems","abstract":"In this paper, a continuous-time adaptive actor-critic reinforcement learning (RL) controller is developed for drift-free nonlinear systems. Practical examples of such systems are image-based visual servoing (IBVS) and wheeled mobile robots (WMR), where the system dynamics includes a parametric uncertainty in the control effectiveness matrix with no drift term. The uncertainty in the input term poses a challenge for developing a continuous-time RL controller using existing methods. In this paper, an actor-critic or synchronous policy iteration (PI)-based RL controller is presented with a concurrent learning (CL)-based parameter update law for estimating the unknown parameters of the control effectiveness matrix. An infinite-horizon value function minimization objective is achieved by regulating the current states to the desired with near-optimal control efforts. The proposed controller guarantees closed-loop stability and simulation results validate the proposed theory using IBVS and WMR examples.","sentences":["In this paper, a continuous-time adaptive actor-critic reinforcement learning (RL) controller is developed for drift-free nonlinear systems.","Practical examples of such systems are image-based visual servoing (IBVS) and wheeled mobile robots (WMR), where the system dynamics includes a parametric uncertainty in the control effectiveness matrix with no drift term.","The uncertainty in the input term poses a challenge for developing a continuous-time RL controller using existing methods.","In this paper, an actor-critic or synchronous policy iteration (PI)-based RL controller is presented with a concurrent learning (CL)-based parameter update law for estimating the unknown parameters of the control effectiveness matrix.","An infinite-horizon value function minimization objective is achieved by regulating the current states to the desired with near-optimal control efforts.","The proposed controller guarantees closed-loop stability and simulation results validate the proposed theory using IBVS and WMR examples."],"url":"http://arxiv.org/abs/2406.09097v1","category":"eess.SY"}
{"created":"2024-06-13 13:19:04","title":"Simulations of distributed-phase-reference quantum key distribution protocols","abstract":"Quantum technology can enable secure communication for cryptography purposes using quantum key distribution. Quantum key distribution protocols provide a secret key between two users with security guaranteed by the laws of quantum mechanics. To define the proper implementation of a quantum key distribution system using a particular cryptography protocol, it is crucial to critically and meticulously assess the device's performance due to technological limitations in the components used. We perform simulations on the ANSYS Interconnect platform to characterise the practical implementation of these devices using distributed-phase-reference protocols differential-phase-shift and coherent-one-way quantum key distribution. Further, we briefly describe and simulate some possible eavesdropping attempts, backflash attack, trojan-horse attack and detector-blinding attack exploiting the device imperfections.","sentences":["Quantum technology can enable secure communication for cryptography purposes using quantum key distribution.","Quantum key distribution protocols provide a secret key between two users with security guaranteed by the laws of quantum mechanics.","To define the proper implementation of a quantum key distribution system using a particular cryptography protocol, it is crucial to critically and meticulously assess the device's performance due to technological limitations in the components used.","We perform simulations on the ANSYS Interconnect platform to characterise the practical implementation of these devices using distributed-phase-reference protocols differential-phase-shift and coherent-one-way quantum key distribution.","Further, we briefly describe and simulate some possible eavesdropping attempts, backflash attack, trojan-horse attack and detector-blinding attack exploiting the device imperfections."],"url":"http://arxiv.org/abs/2406.09091v1","category":"quant-ph"}
{"created":"2024-06-13 13:18:21","title":"Potential systems with singular $\u03a6$-Laplacian","abstract":"We are concerned with solvability of the boundary value problem $$-\\left[ \\phi(u^{\\prime}) \\right] ^{\\prime}=\\nabla_u F(t,u), \\quad \\left ( \\phi \\left( u^{\\prime }\\right)(0), -\\phi \\left( u^{\\prime }\\right)(T)\\right )\\in \\partial j(u(0), u(T)),$$ where $\\phi$ is a homeomorphism from $B_a$ -- the open ball of radius $a$ centered at $0_{\\mathbb{R}^N},$ onto $\\mathbb{R}^N$, satisfying $\\phi(0_{\\mathbb{R}^N})=0_{\\mathbb{R}^N}$, $\\phi =\\nabla \\Phi$, with $\\Phi: \\overline{B}_a \\to (-\\infty, 0]$ of class $C^1$ on $B_a$, continuous and strictly convex on $\\overline{B}_a.$ The potential $F:[0,T] \\times \\mathbb{R}^N \\to \\mathbb{R}$ is of class $C^1$ with respect to the second variable and $j:\\mathbb{R}^N \\times \\mathbb{R}^N \\rightarrow (-\\infty, +\\infty]$ is proper, convex and lower semicontinuous. We first provide a variational formulation in the frame of critical point theory for convex, lower semicontinuous perturbations of $C^1$-functionals. Then, taking the advantage of this key step, we obtain existence of minimum energy as well as saddle-point solutions of the problem. Some concrete illustrative examples of applications are provided.","sentences":["We are concerned with solvability of the boundary value problem $$-\\left[ \\phi(u^{\\prime}) \\right] ^{\\prime}=\\nabla_u F(t,u), \\quad \\left ( \\phi \\left( u^{\\prime }\\right)(0), -\\phi \\left( u^{\\prime }\\right)(T)\\right )","\\in \\partial j(u(0), u(T)),$$ where $\\phi$ is a homeomorphism from $B_a$ -- the open ball of radius $a$ centered at $0_{\\mathbb{R}^N},$ onto $\\mathbb{R}^N$, satisfying $\\phi(0_{\\mathbb{R}^N})=0_{\\mathbb{R}^N}$, $\\phi =\\nabla \\Phi$, with $\\Phi: \\overline{B}_a \\to (-\\infty, 0]$ of class $C^1$ on $B_a$, continuous and strictly convex on $\\overline{B}_a.$ The potential $F:[0,T] \\times \\mathbb{R}^N \\to \\mathbb{R}$ is of class $C^1$ with respect to the second variable and $j:\\mathbb{R}^N \\times \\mathbb{R}^N","\\rightarrow (-\\infty, +\\infty]$ is proper, convex and lower semicontinuous.","We first provide a variational formulation in the frame of critical point theory for convex, lower semicontinuous perturbations of $C^1$-functionals.","Then, taking the advantage of this key step, we obtain existence of minimum energy as well as saddle-point solutions of the problem.","Some concrete illustrative examples of applications are provided."],"url":"http://arxiv.org/abs/2406.09090v1","category":"math.AP"}
{"created":"2024-06-13 13:13:29","title":"Dyadic obligations: proofs and countermodels via hypersequents","abstract":"The basic system E of dyadic deontic logic proposed by {\\AA}qvist offers a simple solution to contrary-to-duty paradoxes and allows to represent norms with exceptions. We investigate E from a proof-theoretical viewpoint. We propose a hypersequent calculus with good properties, the most important of which is cut-elimination, and the consequent subformula property. The calculus is refined to obtain a decision procedure for E and an effective countermodel computation in case of failure of proof search. Using the refined calculus, we prove that validity in E is Co-NP and countermodels have polynomial size.","sentences":["The basic system E of dyadic deontic logic proposed by {\\AA}qvist offers a simple solution to contrary-to-duty paradoxes and allows to represent norms with exceptions.","We investigate E from a proof-theoretical viewpoint.","We propose a hypersequent calculus with good properties, the most important of which is cut-elimination, and the consequent subformula property.","The calculus is refined to obtain a decision procedure for E and an effective countermodel computation in case of failure of proof search.","Using the refined calculus, we prove that validity in E is Co-NP and countermodels have polynomial size."],"url":"http://arxiv.org/abs/2406.09088v1","category":"cs.LO"}
{"created":"2024-06-13 13:04:14","title":"Multifractal analysis of the growth rate of digits in Schneider's $p$-adic continued fraction dynamical system","abstract":"Let $\\mathbb{Z}_p$ be the ring of $p$-adic integers and $a_n(x)$ be the $n$-th digit of Schneider's $p$-adic continued fraction of $x\\in p\\mathbb{Z}_p$. We study the growth rate of the digits $\\{a_n(x)\\}_{n\\geq1}$ from the viewpoint of multifractal analysis. The Hausdorff dimension of the set \\[E_{\\sup}(\\psi)=\\Big\\{x\\in p\\mathbb{Z}_p:\\ \\limsup\\limits_{n\\to\\infty}\\frac{a_n(x)}{\\psi(n)}=1\\Big\\}\\] is completely determined for any $\\psi:\\mathbb{N}\\to\\mathbb{R}^{+}$ satisfying $\\psi(n)\\to \\infty$ as $n\\to\\infty$. As an application, we also calculate the Hausdorff dimension of the intersection sets \\[E^{\\sup}_{\\inf}(\\psi,\\alpha_1,\\alpha_2)=\\left\\{x\\in p\\mathbb{Z}_p:\\liminf_{n\\rightarrow\\infty}\\dfrac{a_n(x)}{\\psi(n)}=\\alpha_1,~\\limsup_{n\\rightarrow\\infty}\\dfrac{a_n(x)}{\\psi(n)}=\\alpha_2\\right\\}\\] for the above function $\\psi$ and $0\\leq\\alpha_1<\\alpha_2\\leq\\infty$.","sentences":["Let $\\mathbb{Z}_p$ be the ring of $p$-adic integers and $a_n(x)$ be the $n$-th digit of Schneider's $p$-adic continued fraction of $x\\in p\\mathbb{Z}_p$. We study the growth rate of the digits $\\{a_n(x)\\}_{n\\geq1}$ from the viewpoint of multifractal analysis.","The Hausdorff dimension of the set \\[E_{\\sup}(\\psi)=\\Big\\{x\\in p\\mathbb{Z}_p:\\ \\limsup\\limits_{n\\to\\infty}\\frac{a_n(x)}{\\psi(n)}=1\\Big\\}\\] is completely determined for any $\\psi:\\mathbb{N}\\to\\mathbb{R}^{+}$ satisfying $\\psi(n)\\to \\infty$ as $n\\to\\infty$. As an application, we also calculate the Hausdorff dimension of the intersection sets \\[E^{\\sup}_{\\inf}(\\psi,\\alpha_1,\\alpha_2)=\\left\\{x\\in p\\mathbb{Z}_p:\\liminf_{n\\rightarrow\\infty}\\dfrac{a_n(x)}{\\psi(n)}=\\alpha_1,~\\limsup_{n\\rightarrow\\infty}\\dfrac{a_n(x)}{\\psi(n)}=\\alpha_2\\right\\}\\] for the above function $\\psi$ and $0\\leq\\alpha_1<\\alpha_2\\leq\\infty$."],"url":"http://arxiv.org/abs/2406.09081v1","category":"math.NT"}
{"created":"2024-06-13 13:02:25","title":"ONNX-to-Hardware Design Flow for Adaptive Neural-Network Inference on FPGAs","abstract":"The challenges involved in executing neural networks (NNs) at the edge include providing diversity, flexibility, and sustainability. That implies, for instance, supporting evolving applications and algorithms energy-efficiently. Using hardware or software accelerators can deliver fast and efficient computation of the NNs, while flexibility can be exploited to support long-term adaptivity. Nonetheless, handcrafting an NN for a specific device, despite the possibility of leading to an optimal solution, takes time and experience, and that's why frameworks for hardware accelerators are being developed. This work, starting from a preliminary semi-integrated ONNX-to-hardware toolchain [21], focuses on enabling approximate computing leveraging the distinctive ability of the original toolchain to favor adaptivity. The goal is to allow lightweight adaptable NN inference on FPGAs at the edge.","sentences":["The challenges involved in executing neural networks (NNs) at the edge include providing diversity, flexibility, and sustainability.","That implies, for instance, supporting evolving applications and algorithms energy-efficiently.","Using hardware or software accelerators can deliver fast and efficient computation of the NNs, while flexibility can be exploited to support long-term adaptivity.","Nonetheless, handcrafting an NN for a specific device, despite the possibility of leading to an optimal solution, takes time and experience, and that's why frameworks for hardware accelerators are being developed.","This work, starting from a preliminary semi-integrated ONNX-to-hardware toolchain [21], focuses on enabling approximate computing leveraging the distinctive ability of the original toolchain to favor adaptivity.","The goal is to allow lightweight adaptable NN inference on FPGAs at the edge."],"url":"http://arxiv.org/abs/2406.09078v1","category":"cs.AR"}
{"created":"2024-06-13 13:01:50","title":"Spectroscopy of two-dimensional interacting lattice electrons using symmetry-aware neural backflow transformations","abstract":"Neural networks have shown to be a powerful tool to represent ground state of quantum many-body systems, including for fermionic systems. In this work, we introduce a framework for embedding lattice symmetries in Neural Slater-Backflow-Jastrow wavefunction ansatzes, and demonstrate how our model allows us to target the ground state and low-lying excited states. To capture the Hamiltonian symmetries, we introduce group-equivariant backflow transformations. We study the low-energy excitation spectrum of the t-V model on a square lattice away from half-filling, and find that our symmetry-aware backflow significantly improves the ground-state energies, and yields accurate low-lying excited states for up to 10x10 lattices. We additionally compute the two-point density-correlation function and the structure factor to detect the phase transition and determine the critical point. Finally, we quantify the variational accuracy of our model using the V-score.","sentences":["Neural networks have shown to be a powerful tool to represent ground state of quantum many-body systems, including for fermionic systems.","In this work, we introduce a framework for embedding lattice symmetries in Neural Slater-Backflow-Jastrow wavefunction ansatzes, and demonstrate how our model allows us to target the ground state and low-lying excited states.","To capture the Hamiltonian symmetries, we introduce group-equivariant backflow transformations.","We study the low-energy excitation spectrum of the t-V model on a square lattice away from half-filling, and find that our symmetry-aware backflow significantly improves the ground-state energies, and yields accurate low-lying excited states for up to 10x10 lattices.","We additionally compute the two-point density-correlation function and the structure factor to detect the phase transition and determine the critical point.","Finally, we quantify the variational accuracy of our model using the V-score."],"url":"http://arxiv.org/abs/2406.09077v1","category":"cond-mat.str-el"}
{"created":"2024-06-13 12:58:53","title":"3M: Multi-modal Multi-task Multi-teacher Learning for Game Event Detection","abstract":"Esports has rapidly emerged as a global phenomenon with an ever-expanding audience via platforms, like YouTube. Due to the inherent complexity nature of the game, it is challenging for newcomers to comprehend what the event entails. The chaotic nature of online chat, the fast-paced speech of the game commentator, and the game-specific user interface further compound the difficulty for users in comprehending the gameplay. To overcome these challenges, it is crucial to integrate the Multi-Modal (MM) information from the platform and understand the event. The paper introduces a new MM multi-teacher-based game event detection framework, with the ultimate goal of constructing a comprehensive framework that enhances the comprehension of the ongoing game situation. While conventional MM models typically prioritise aligning MM data through concurrent training towards a unified objective, our framework leverages multiple teachers trained independently on different tasks to accomplish the Game Event Detection. The experiment clearly shows the effectiveness of the proposed MM multi-teacher framework.","sentences":["Esports has rapidly emerged as a global phenomenon with an ever-expanding audience via platforms, like YouTube.","Due to the inherent complexity nature of the game, it is challenging for newcomers to comprehend what the event entails.","The chaotic nature of online chat, the fast-paced speech of the game commentator, and the game-specific user interface further compound the difficulty for users in comprehending the gameplay.","To overcome these challenges, it is crucial to integrate the Multi-Modal (MM) information from the platform and understand the event.","The paper introduces a new MM multi-teacher-based game event detection framework, with the ultimate goal of constructing a comprehensive framework that enhances the comprehension of the ongoing game situation.","While conventional MM models typically prioritise aligning MM data through concurrent training towards a unified objective, our framework leverages multiple teachers trained independently on different tasks to accomplish the Game Event Detection.","The experiment clearly shows the effectiveness of the proposed MM multi-teacher framework."],"url":"http://arxiv.org/abs/2406.09076v1","category":"cs.CL"}
{"created":"2024-06-13 12:58:11","title":"Entanglement properties of optomagnonic crystal from nonlinear perspective","abstract":"Optomagnonics is a new field of research in condensed matter physics and quantum optics focused on strong magnon-photon interactions. Particular interest concerns realistic, experimentally feasible materials and prototype cheap elements for futuristic nanodevices implemented in the processing or storing of quantum information. Quantifying the entanglement between two continuous bosonic modes, such as magnons and photons, is not trivial. The state-of-the-art for today is the logarithmic negativity, calculated through the quantum Langevin equations subjected to thermal noise. However, due to its complexity, this method requires further approximation. In the present work, we propose a new procedure that avoids the linearization of dynamics. Prior analyzing the quantum entanglement, we explore the nonlinear semiclassical dynamics in detail and precisely define the phase space. The typical nonlinear dynamical system holds bifurcation points and fixed points of different characters in its phase space. Our main finding is that entanglement is not defined in the Saddle Point region. On the other hand, the maximum of the entanglement corresponds to the region near the border between the Stable node and Stable spiral regions. In numerical calculations, we considered a particular system: optomagnonic crystal based on the yttrium iron garnet (YIG) slab with the periodic air holes drilled in the slab. In our case, Magnon-photon interaction occurs due to the magneto-electric effect in YIG. We provide explicit derivation of the coupling term. Besides, we calculate photon modes for a particular geometry of the optomagnonic crystal. We analyzed the amplitude-frequency characteristics of the optomagnonic crystal and showed that due to the instability region, one could efficiently switch the mean magnon numbers in the system and control entanglement in the system.","sentences":["Optomagnonics is a new field of research in condensed matter physics and quantum optics focused on strong magnon-photon interactions.","Particular interest concerns realistic, experimentally feasible materials and prototype cheap elements for futuristic nanodevices implemented in the processing or storing of quantum information.","Quantifying the entanglement between two continuous bosonic modes, such as magnons and photons, is not trivial.","The state-of-the-art for today is the logarithmic negativity, calculated through the quantum Langevin equations subjected to thermal noise.","However, due to its complexity, this method requires further approximation.","In the present work, we propose a new procedure that avoids the linearization of dynamics.","Prior analyzing the quantum entanglement, we explore the nonlinear semiclassical dynamics in detail and precisely define the phase space.","The typical nonlinear dynamical system holds bifurcation points and fixed points of different characters in its phase space.","Our main finding is that entanglement is not defined in the Saddle Point region.","On the other hand, the maximum of the entanglement corresponds to the region near the border between the Stable node and Stable spiral regions.","In numerical calculations, we considered a particular system: optomagnonic crystal based on the yttrium iron garnet (YIG) slab with the periodic air holes drilled in the slab.","In our case, Magnon-photon interaction occurs due to the magneto-electric effect in YIG.","We provide explicit derivation of the coupling term.","Besides, we calculate photon modes for a particular geometry of the optomagnonic crystal.","We analyzed the amplitude-frequency characteristics of the optomagnonic crystal and showed that due to the instability region, one could efficiently switch the mean magnon numbers in the system and control entanglement in the system."],"url":"http://arxiv.org/abs/2406.09074v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-13 12:53:59","title":"Engineering Digital Systems for Humanity: Challenges and Opportunities","abstract":"As testified by new regulations like the European AI act, the worries about the societal impact of (autonomous) software technologies are becoming of public concern. Social and human values, besides the traditional software behaviour and quality, are increasingly recognized as important for sustainability and long-term well-being. In this paper, we identify the macro and technological challenges and opportunities of present and future digital systems that should be engineered for humanity. Our specific perspective in identifying the challenges is to focus on humans and on their role in their co-existence with digital systems. The first challenge considers humans in a proactive role when interacting with the digital systems, i.e., taking initiative in making things happening instead of reacting to events. The second concerns humans having an active role in the interaction with the digital systems i.e., on humans that interact with digital systems as a reaction to events. The third challenge focuses on humans that have a passive role i.e., they experience, enjoy or even suffer the decisions and/or actions of digital systems. Two further transversal challenges are considered: the duality of trust and trustworthiness and the compliance to legislation that both may seriously affect the deployment and use of digital systems.","sentences":["As testified by new regulations like the European AI act, the worries about the societal impact of (autonomous) software technologies are becoming of public concern.","Social and human values, besides the traditional software behaviour and quality, are increasingly recognized as important for sustainability and long-term well-being.","In this paper, we identify the macro and technological challenges and opportunities of present and future digital systems that should be engineered for humanity.","Our specific perspective in identifying the challenges is to focus on humans and on their role in their co-existence with digital systems.","The first challenge considers humans in a proactive role when interacting with the digital systems, i.e., taking initiative in making things happening instead of reacting to events.","The second concerns humans having an active role in the interaction with the digital systems i.e., on humans that interact with digital systems as a reaction to events.","The third challenge focuses on humans that have a passive role i.e., they experience, enjoy or even suffer the decisions and/or actions of digital systems.","Two further transversal challenges are considered: the duality of trust and trustworthiness and the compliance to legislation that both may seriously affect the deployment and use of digital systems."],"url":"http://arxiv.org/abs/2406.09065v1","category":"cs.SE"}
{"created":"2024-06-13 12:53:55","title":"The nature of the accretion physics in quiescent black hole system LB-1","abstract":"LB-1 is a binary system that has drawn great attention since its discovery in 2019. The nature of the two components of LB-1 is not very clear, which however is suggested very possibly to be a B-type star plus a black hole (BH). In this paper, we first calculate the wind mass-loss rate of the B-type star. We then calculate the mass capture rate by the BH, with which as the initial mass accretion rate, we calculate the truncation radius of the accretion disk and the corresponding emergent spectra of the accretion flow (comprising an inner advection-dominated accretion flow (ADAF) + an outer truncated accretion disk) within the framework of the disk evaporation model. It is found that the predicted truncation radius of the accretion disk with appropriate model parameters is consistent with observations inferred from the observed broad H$_\\alpha$ emission line. The predicted X-ray luminosity is definitely below the estimated upper limits with the sensitivity of Chandra X-ray Observatory of the X-ray luminosity $\\sim 2\\times 10^{31}$ erg/s. Finally, we argue that if the disk evaporation model indeed reflects the intrinsic physics of the accretion flow, the value of the viscosity parameter $\\alpha$ is constrained to be $\\alpha \\gtrsim 0.05$ (with BH mass being $68M_{\\rm \\odot}$), or $\\alpha \\gtrsim 0.003$ (with BH mass being $21M_{\\rm \\odot}$) to match the observed upper limit of the X-ray luminosity of LB-1.","sentences":["LB-1 is a binary system that has drawn great attention since its discovery in 2019.","The nature of the two components of LB-1 is not very clear, which however is suggested very possibly to be a B-type star plus a black hole (BH).","In this paper, we first calculate the wind mass-loss rate of the B-type star.","We then calculate the mass capture rate by the BH, with which as the initial mass accretion rate, we calculate the truncation radius of the accretion disk and the corresponding emergent spectra of the accretion flow (comprising an inner advection-dominated accretion flow (ADAF)","+ an outer truncated accretion disk) within the framework of the disk evaporation model.","It is found that the predicted truncation radius of the accretion disk with appropriate model parameters is consistent with observations inferred from the observed broad H$_\\alpha$ emission line.","The predicted X-ray luminosity is definitely below the estimated upper limits with the sensitivity of Chandra X-ray Observatory of the X-ray luminosity $\\sim 2\\times 10^{31}$ erg/s. Finally, we argue that if the disk evaporation model indeed reflects the intrinsic physics of the accretion flow, the value of the viscosity parameter $\\alpha$ is constrained to be $\\alpha \\gtrsim 0.05$ (with BH mass being $68M_{\\rm \\odot}$), or $\\alpha \\gtrsim 0.003$ (with BH mass being $21M_{\\rm \\odot}$) to match the observed upper limit of the X-ray luminosity of LB-1."],"url":"http://arxiv.org/abs/2406.09064v1","category":"astro-ph.HE"}
{"created":"2024-06-13 12:51:45","title":"On the Measurement of the Unruh Effect Through Extended Quantum Thermometers","abstract":"The Unruh effect, predicting a thermal reservoir for accelerating systems, calls for a more refined understanding of measurement processes involving quantum systems as thermometers. Conventional models fail to account for the inherent spatial extent of the thermometer, neglecting the complexities associated with accelerated extended quantum systems. Our work builds upon the seminal work of Bell, Hughes, and Leinaas. We propose a refined thermometer model incorporating a spin-1/2 particle where the spin acts as a temperature indicator. This refined model demonstrates the ability to effectively measure the temperature under specific, realistic conditions, providing a unique value that essentially averages the local Unruh temperatures throughout the extended quantum system acting as the thermometer.","sentences":["The Unruh effect, predicting a thermal reservoir for accelerating systems, calls for a more refined understanding of measurement processes involving quantum systems as thermometers.","Conventional models fail to account for the inherent spatial extent of the thermometer, neglecting the complexities associated with accelerated extended quantum systems.","Our work builds upon the seminal work of Bell, Hughes, and Leinaas.","We propose a refined thermometer model incorporating a spin-1/2 particle where the spin acts as a temperature indicator.","This refined model demonstrates the ability to effectively measure the temperature under specific, realistic conditions, providing a unique value that essentially averages the local Unruh temperatures throughout the extended quantum system acting as the thermometer."],"url":"http://arxiv.org/abs/2406.09063v1","category":"quant-ph"}
{"created":"2024-06-13 12:50:57","title":"Joint Observer Gain and Input Design for Asymptotic Active Fault Diagnosis","abstract":"This paper proposes a joint gain and input design method for observer-based asymptotic active fault diagnosis, which is based on a newly-defined notion named the excluding degree of the origin from a zonotope. Using the excluding degree, a quantitative specification is obtained to characterize the performance of set-based robust fault diagnosis. Furthermore, a single gain design method and a joint gain and input design method are proposed, respectively. This is the first work to achieve a joint observer gain and input design for set-based active fault diagnosis. Compared with the existing methods that design gains and input separately, the proposed joint gain and input design method has advantages to exploit the fault diagnosis potential of observer-based schemes. Finally, several examples are used to illustrate the effectiveness of the proposed methods.","sentences":["This paper proposes a joint gain and input design method for observer-based asymptotic active fault diagnosis, which is based on a newly-defined notion named the excluding degree of the origin from a zonotope.","Using the excluding degree, a quantitative specification is obtained to characterize the performance of set-based robust fault diagnosis.","Furthermore, a single gain design method and a joint gain and input design method are proposed, respectively.","This is the first work to achieve a joint observer gain and input design for set-based active fault diagnosis.","Compared with the existing methods that design gains and input separately, the proposed joint gain and input design method has advantages to exploit the fault diagnosis potential of observer-based schemes.","Finally, several examples are used to illustrate the effectiveness of the proposed methods."],"url":"http://arxiv.org/abs/2406.09061v1","category":"eess.SY"}
{"created":"2024-06-13 12:38:55","title":"Joint Channel Estimation and Prediction for Massive MIMO with Frequency Hopping Sounding","abstract":"In massive multiple-input multiple-output (MIMO) systems, the downlink transmission performance heavily relies on accurate channel state information (CSI). Constrained by the transmitted power, user equipment always transmits sounding reference signals (SRSs) to the base station through frequency hopping, which will be leveraged to estimate uplink CSI and subsequently predict downlink CSI. This paper aims to investigate joint channel estimation and prediction (JCEP) for massive MIMO with frequency hopping sounding (FHS). Specifically, we present a multiple-subband (MS) delay-angle-Doppler (DAD) domain channel model with off-grid basis to tackle the energy leakage problem. Furthermore, we formulate the JCEP problem with FHS as a multiple measurement vector (MMV) problem, facilitating the sharing of common CSI across different subbands. To solve this problem, we propose an efficient Off-Grid-MS hybrid message passing (HMP) algorithm under the constrained Bethe free energy (BFE) framework. Aiming to address the lack of prior CSI in practical scenarios, the proposed algorithm can adaptively learn the hyper-parameters of the channel by minimizing the corresponding terms in the BFE expression. To alleviate the complexity of channel hyper-parameter learning, we leverage the approximations of the off-grid matrices to simplify the off-grid hyper-parameter estimation. Numerical results illustrate that the proposed algorithm can effectively mitigate the energy leakage issue and exploit the common CSI across different subbands, acquiring more accurate CSI compared to state-of-the-art counterparts.","sentences":["In massive multiple-input multiple-output (MIMO) systems, the downlink transmission performance heavily relies on accurate channel state information (CSI).","Constrained by the transmitted power, user equipment always transmits sounding reference signals (SRSs) to the base station through frequency hopping, which will be leveraged to estimate uplink CSI and subsequently predict downlink CSI.","This paper aims to investigate joint channel estimation and prediction (JCEP) for massive MIMO with frequency hopping sounding (FHS).","Specifically, we present a multiple-subband (MS) delay-angle-Doppler (DAD) domain channel model with off-grid basis to tackle the energy leakage problem.","Furthermore, we formulate the JCEP problem with FHS as a multiple measurement vector (MMV) problem, facilitating the sharing of common CSI across different subbands.","To solve this problem, we propose an efficient Off-Grid-MS hybrid message passing (HMP) algorithm under the constrained Bethe free energy (BFE) framework.","Aiming to address the lack of prior CSI in practical scenarios, the proposed algorithm can adaptively learn the hyper-parameters of the channel by minimizing the corresponding terms in the BFE expression.","To alleviate the complexity of channel hyper-parameter learning, we leverage the approximations of the off-grid matrices to simplify the off-grid hyper-parameter estimation.","Numerical results illustrate that the proposed algorithm can effectively mitigate the energy leakage issue and exploit the common CSI across different subbands, acquiring more accurate CSI compared to state-of-the-art counterparts."],"url":"http://arxiv.org/abs/2406.09053v1","category":"eess.SP"}
{"created":"2024-06-13 12:23:20","title":"Language-Driven Closed-Loop Grasping with Model-Predictive Trajectory Replanning","abstract":"Combining a vision module inside a closed-loop control system for a \\emph{seamless movement} of a robot in a manipulation task is challenging due to the inconsistent update rates between utilized modules. This task is even more difficult in a dynamic environment, e.g., objects are moving. This paper presents a \\emph{modular} zero-shot framework for language-driven manipulation of (dynamic) objects through a closed-loop control system with real-time trajectory replanning and an online 6D object pose localization. We segment an object within $\\SI{0.5}{\\second}$ by leveraging a vision language model via language commands. Then, guided by natural language commands, a closed-loop system, including a unified pose estimation and tracking and online trajectory planning, is utilized to continuously track this object and compute the optimal trajectory in real-time. Our proposed zero-shot framework provides a smooth trajectory that avoids jerky movements and ensures the robot can grasp a non-stationary object. Experiment results exhibit the real-time capability of the proposed zero-shot modular framework for the trajectory optimization module to accurately and efficiently grasp moving objects, i.e., up to \\SI{30}{\\hertz} update rates for the online 6D pose localization module and \\SI{10}{\\hertz} update rates for the receding-horizon trajectory optimization. These advantages highlight the modular framework's potential applications in robotics and human-robot interaction; see the video in \\href{https://www.acin.tuwien.ac.at/en/6e64/}{https://www.acin.tuwien.ac.at/en/6e64/}","sentences":["Combining a vision module inside a closed-loop control system for a \\emph{seamless movement} of a robot in a manipulation task is challenging due to the inconsistent update rates between utilized modules.","This task is even more difficult in a dynamic environment, e.g., objects are moving.","This paper presents a \\emph{modular} zero-shot framework for language-driven manipulation of (dynamic) objects through a closed-loop control system with real-time trajectory replanning and an online 6D object pose localization.","We segment an object within $\\SI{0.5}{\\second}$ by leveraging a vision language model via language commands.","Then, guided by natural language commands, a closed-loop system, including a unified pose estimation and tracking and online trajectory planning, is utilized to continuously track this object and compute the optimal trajectory in real-time.","Our proposed zero-shot framework provides a smooth trajectory that avoids jerky movements and ensures the robot can grasp a non-stationary object.","Experiment results exhibit the real-time capability of the proposed zero-shot modular framework for the trajectory optimization module to accurately and efficiently grasp moving objects, i.e., up to \\SI{30}{\\hertz} update rates for the online 6D pose localization module and \\SI{10}{\\hertz} update rates for the receding-horizon trajectory optimization.","These advantages highlight the modular framework's potential applications in robotics and human-robot interaction; see the video in \\href{https://www.acin.tuwien.ac.at/en/6e64/}{https://www.acin.tuwien.ac.at/en/6e64/}"],"url":"http://arxiv.org/abs/2406.09039v1","category":"cs.RO"}
{"created":"2024-06-13 12:03:29","title":"Fair by design: A sociotechnical approach to justifying the fairness of AI-enabled systems across the lifecycle","abstract":"Fairness is one of the most commonly identified ethical principles in existing AI guidelines, and the development of fair AI-enabled systems is required by new and emerging AI regulation. But most approaches to addressing the fairness of AI-enabled systems are limited in scope in two significant ways: their substantive content focuses on statistical measures of fairness, and they do not emphasize the need to identify and address fairness considerations across the whole AI lifecycle. Our contribution is to present an assurance framework and tool that can enable a practical and transparent method for widening the scope of fairness considerations across the AI lifecycle and move the discussion beyond mere statistical notions of fairness to consider a richer analysis in a practical and context-dependent manner. To illustrate this approach, we first describe and then apply the framework of Trustworthy and Ethical Assurance (TEA) to an AI-enabled clinical diagnostic support system (CDSS) whose purpose is to help clinicians predict the risk of developing hypertension in patients with Type 2 diabetes, a context in which several fairness considerations arise (e.g., discrimination against patient subgroups). This is supplemented by an open-source tool and a fairness considerations map to help facilitate reasoning about the fairness of AI-enabled systems in a participatory way. In short, by using a shared framework for identifying, documenting and justifying fairness considerations, and then using this deliberative exercise to structure an assurance case, research on AI fairness becomes reusable and generalizable for others in the ethical AI community and for sharing best practices for achieving fairness and equity in digital health and healthcare in particular.","sentences":["Fairness is one of the most commonly identified ethical principles in existing AI guidelines, and the development of fair AI-enabled systems is required by new and emerging AI regulation.","But most approaches to addressing the fairness of AI-enabled systems are limited in scope in two significant ways: their substantive content focuses on statistical measures of fairness, and they do not emphasize the need to identify and address fairness considerations across the whole AI lifecycle.","Our contribution is to present an assurance framework and tool that can enable a practical and transparent method for widening the scope of fairness considerations across the AI lifecycle and move the discussion beyond mere statistical notions of fairness to consider a richer analysis in a practical and context-dependent manner.","To illustrate this approach, we first describe and then apply the framework of Trustworthy and Ethical Assurance (TEA) to an AI-enabled clinical diagnostic support system (CDSS) whose purpose is to help clinicians predict the risk of developing hypertension in patients with Type 2 diabetes, a context in which several fairness considerations arise (e.g., discrimination against patient subgroups).","This is supplemented by an open-source tool and a fairness considerations map to help facilitate reasoning about the fairness of AI-enabled systems in a participatory way.","In short, by using a shared framework for identifying, documenting and justifying fairness considerations, and then using this deliberative exercise to structure an assurance case, research on AI fairness becomes reusable and generalizable for others in the ethical AI community and for sharing best practices for achieving fairness and equity in digital health and healthcare in particular."],"url":"http://arxiv.org/abs/2406.09029v1","category":"cs.CY"}
{"created":"2024-06-13 11:55:40","title":"Contextual Distillation Model for Diversified Recommendation","abstract":"The diversity of recommendation is equally crucial as accuracy in improving user experience. Existing studies, e.g., Determinantal Point Process (DPP) and Maximal Marginal Relevance (MMR), employ a greedy paradigm to iteratively select items that optimize both accuracy and diversity. However, prior methods typically exhibit quadratic complexity, limiting their applications to the re-ranking stage and are not applicable to other recommendation stages with a larger pool of candidate items, such as the pre-ranking and ranking stages. In this paper, we propose Contextual Distillation Model (CDM), an efficient recommendation model that addresses diversification, suitable for the deployment in all stages of industrial recommendation pipelines. Specifically, CDM utilizes the candidate items in the same user request as context to enhance the diversification of the results. We propose a contrastive context encoder that employs attention mechanisms to model both positive and negative contexts. For the training of CDM, we compare each target item with its context embedding and utilize the knowledge distillation framework to learn the win probability of each target item under the MMR algorithm, where the teacher is derived from MMR outputs. During inference, ranking is performed through a linear combination of the recommendation and student model scores, ensuring both diversity and efficiency. We perform offline evaluations on two industrial datasets and conduct online A/B test of CDM on the short-video platform KuaiShou. The considerable enhancements observed in both recommendation quality and diversity, as shown by metrics, provide strong superiority for the effectiveness of CDM.","sentences":["The diversity of recommendation is equally crucial as accuracy in improving user experience.","Existing studies, e.g., Determinantal Point Process (DPP) and Maximal Marginal Relevance (MMR), employ a greedy paradigm to iteratively select items that optimize both accuracy and diversity.","However, prior methods typically exhibit quadratic complexity, limiting their applications to the re-ranking stage and are not applicable to other recommendation stages with a larger pool of candidate items, such as the pre-ranking and ranking stages.","In this paper, we propose Contextual Distillation Model (CDM), an efficient recommendation model that addresses diversification, suitable for the deployment in all stages of industrial recommendation pipelines.","Specifically, CDM utilizes the candidate items in the same user request as context to enhance the diversification of the results.","We propose a contrastive context encoder that employs attention mechanisms to model both positive and negative contexts.","For the training of CDM, we compare each target item with its context embedding and utilize the knowledge distillation framework to learn the win probability of each target item under the MMR algorithm, where the teacher is derived from MMR outputs.","During inference, ranking is performed through a linear combination of the recommendation and student model scores, ensuring both diversity and efficiency.","We perform offline evaluations on two industrial datasets and conduct online A/B test of CDM on the short-video platform KuaiShou.","The considerable enhancements observed in both recommendation quality and diversity, as shown by metrics, provide strong superiority for the effectiveness of CDM."],"url":"http://arxiv.org/abs/2406.09021v1","category":"cs.IR"}
{"created":"2024-06-13 11:44:22","title":"Ground state energy of a dilute Bose gas with three-body hard-core interactions","abstract":"We consider a gas of bosons interacting through a three-body hard-core potential in the thermodynamic limit. We derive an upper bound on the ground state energy of the system at the leading order using a Jastrow factor. Our result matches the lower bound proven by Nam-Ricaud-Triay and therefore resolves the leading order. Moreover, a straightforward adaptation of our proof can be used for systems interacting via combined two-body and three-body interactions to generalise Theorem 1.2. from arXiv:2402.05646 to hard-core potentials.","sentences":["We consider a gas of bosons interacting through a three-body hard-core potential in the thermodynamic limit.","We derive an upper bound on the ground state energy of the system at the leading order using a Jastrow factor.","Our result matches the lower bound proven by Nam-Ricaud-Triay and therefore resolves the leading order.","Moreover, a straightforward adaptation of our proof can be used for systems interacting via combined two-body and three-body interactions to generalise Theorem 1.2.","from arXiv:2402.05646 to hard-core potentials."],"url":"http://arxiv.org/abs/2406.09019v1","category":"math-ph"}
{"created":"2024-06-13 11:39:02","title":"AMSA-UNet: An Asymmetric Multiple Scales U-net Based on Self-attention for Deblurring","abstract":"The traditional ingle-scale U-Net often leads to the loss of spatial information during deblurring, which affects the deblurring accracy. Additionally, due to the convolutional method's limitation in capturing long-range dependencies, the quality of the recovered image is degraded. To address the above problems, an asymmetric multiple scales U-net based on self-attention (AMSA-UNet) is proposed to improve the accuracy and computational complexity. By introducing a multiple-scales U shape architecture, the network can focus on blurry regions at the global level and better recover image details at the local level. In order to overcome the limitations of traditional convolutional methods in capturing the long-range dependencies of information, a self-attention mechanism is introduced into the decoder part of the backbone network, which significantly increases the model's receptive field, enabling it to pay more attention to semantic information of the image, thereby producing more accurate and visually pleasing deblurred images. What's more, a frequency domain-based computation method was introduced to reduces the computation amount. The experimental results demonstrate that the proposed method exhibits significant improvements in both accuracy and speed compared to eight excellent methods","sentences":["The traditional ingle-scale U-Net often leads to the loss of spatial information during deblurring, which affects the deblurring accracy.","Additionally, due to the convolutional method's limitation in capturing long-range dependencies, the quality of the recovered image is degraded.","To address the above problems, an asymmetric multiple scales U-net based on self-attention (AMSA-UNet) is proposed to improve the accuracy and computational complexity.","By introducing a multiple-scales U shape architecture, the network can focus on blurry regions at the global level and better recover image details at the local level.","In order to overcome the limitations of traditional convolutional methods in capturing the long-range dependencies of information, a self-attention mechanism is introduced into the decoder part of the backbone network, which significantly increases the model's receptive field, enabling it to pay more attention to semantic information of the image, thereby producing more accurate and visually pleasing deblurred images.","What's more, a frequency domain-based computation method was introduced to reduces the computation amount.","The experimental results demonstrate that the proposed method exhibits significant improvements in both accuracy and speed compared to eight excellent methods"],"url":"http://arxiv.org/abs/2406.09015v1","category":"cs.CV"}
{"created":"2024-06-13 11:19:40","title":"Smooth structures on four-manifolds with finite cyclic fundamental groups","abstract":"For each nonnegative integer m we show that any closed, oriented topological four-manifold with fundamental group Z_{4m+2} and odd intersection form, with possibly seven exceptions, either admits no smooth structure or admits infinitely many distinct smooth structures up to diffeomorphism. Moreover, we construct infinite families of non-complex irreducible fake projective planes with diverse fundamental groups.","sentences":["For each nonnegative integer m we show that any closed, oriented topological four-manifold with fundamental group Z_{4m+2} and odd intersection form, with possibly seven exceptions, either admits no smooth structure or admits infinitely many distinct smooth structures up to diffeomorphism.","Moreover, we construct infinite families of non-complex irreducible fake projective planes with diverse fundamental groups."],"url":"http://arxiv.org/abs/2406.09007v1","category":"math.GT"}
{"created":"2024-06-13 11:18:49","title":"Privacy Aware Memory Forensics","abstract":"In recent years, insider threats and attacks have been increasing in terms of frequency and cost to the corporate business. The utilization of end-to-end encrypted instant messaging applications (WhatsApp, Telegram, VPN) by malicious insiders raised data breach incidents exponentially. The Securities and Exchange Board of India (SEBI) investigated reports on such data leak incidents and reported about twelve companies where earnings data and financial information were leaked using WhatsApp messages. Recent surveys indicate that 60% of data breaches are primarily caused by malicious insider threats. Especially, in the case of the defense environment, information leaks by insiders will jeopardize the countrys national security. Sniffing of network and host-based activities will not work in an insider threat detection environment due to end-to-end encryption. Memory forensics allows access to the messages sent or received over an end-to-end encrypted environment but with a total compromise of the users privacy. In this research, we present a novel solution to detect data leakages by insiders in an organization. Our approach captures the RAM of the insiders device and analyses it for sensitive information leaks from a host system while maintaining the users privacy. Sensitive data leaks are identified with context using a deep learning model. The feasibility and effectiveness of the proposed idea have been demonstrated with the help of a military use case. The proposed architecture can however be used across various use cases with minor modifications.","sentences":["In recent years, insider threats and attacks have been increasing in terms of frequency and cost to the corporate business.","The utilization of end-to-end encrypted instant messaging applications (WhatsApp, Telegram, VPN) by malicious insiders raised data breach incidents exponentially.","The Securities and Exchange Board of India (SEBI) investigated reports on such data leak incidents and reported about twelve companies where earnings data and financial information were leaked using WhatsApp messages.","Recent surveys indicate that 60% of data breaches are primarily caused by malicious insider threats.","Especially, in the case of the defense environment, information leaks by insiders will jeopardize the countrys national security.","Sniffing of network and host-based activities will not work in an insider threat detection environment due to end-to-end encryption.","Memory forensics allows access to the messages sent or received over an end-to-end encrypted environment but with a total compromise of the users privacy.","In this research, we present a novel solution to detect data leakages by insiders in an organization.","Our approach captures the RAM of the insiders device and analyses it for sensitive information leaks from a host system while maintaining the users privacy.","Sensitive data leaks are identified with context using a deep learning model.","The feasibility and effectiveness of the proposed idea have been demonstrated with the help of a military use case.","The proposed architecture can however be used across various use cases with minor modifications."],"url":"http://arxiv.org/abs/2406.09005v1","category":"cs.CR"}
{"created":"2024-06-13 11:14:22","title":"Effect of measurements on quantum speed limit","abstract":"Given the initial and final states of a quantum system, the speed of transportation of state vector in the projective Hilbert space governs the quantum speed limit. Here, we ask the question what happens to the quantum speed limit under continuous measurement process. We model the continuous measurement process by a non-Hermitian Hamiltonian which keeps the evolution of the system Schr{\\\"o}dinger-like even under the process of measurement. Using this specific measurement model, we prove that under continuous measurement, the speed of transportation of a quantum system tends to zero. Interestingly, we also find that for small time scale, there is an enhancement of quantum speed even if the measurement strength is finite. Our findings can have applications in quantum computing and quantum control where dynamics is governed by both unitary and measurement processes.","sentences":["Given the initial and final states of a quantum system, the speed of transportation of state vector in the projective Hilbert space governs the quantum speed limit.","Here, we ask the question what happens to the quantum speed limit under continuous measurement process.","We model the continuous measurement process by a non-Hermitian Hamiltonian which keeps the evolution of the system Schr{\\\"o}dinger-like even under the process of measurement.","Using this specific measurement model, we prove that under continuous measurement, the speed of transportation of a quantum system tends to zero.","Interestingly, we also find that for small time scale, there is an enhancement of quantum speed even if the measurement strength is finite.","Our findings can have applications in quantum computing and quantum control where dynamics is governed by both unitary and measurement processes."],"url":"http://arxiv.org/abs/2406.09004v1","category":"quant-ph"}
{"created":"2024-06-13 11:04:46","title":"Evaluation of Sparse Acoustic Array Geometries for the Application in Indoor Localization","abstract":"Angle-of-Arrival estimation technology, with its potential advantages, emerges as an intriguing choice for indoor localization. Notably, it holds the promise of reducing installation costs. In contrast to ToF/TDoA based systems, AoA-based approaches require a reduced number of nodes for effective localization. This characteristic establishes a trade-off between installation costs and the complexity of hardware and software. Moreover, the appeal of acoustic localization is further heightened by its capacity to provide cost-effective hardware solutions while maintaining a high degree of accuracy. Consequently, acoustic AoA estimation technology stands out as a feasible and compelling option in the field of indoor localization. Sparse arrays additionally have the ability to estimate the DoA of more sources than available sensors by placing sensors in a specific geometry. In this contribution, we introduce a measurement platform designed to evaluate various sparse array geometries experimentally. The acoustic microphone array comprises 64 microphones arranged in an 8x8 grid, following an Uniform Rectangular Array (URA) configuration, with a grid spacing of 8.255 mm. This configuration achieves a spatial Nyquist frequency of approximately 20.8 kHz in the acoustic domain at room temperature. Notably, the array exhibits a mean spherical error of 1.26{\\deg} when excluding higher elevation angles. The platform allows for masking sensors to simulate sparse array configurations. We assess four array geometries through simulations and experimental data, identifying the Open-Box and Nested array geometries as robust candidates. Additionally, we demonstrate the array's capability to concurrently estimate the directions of three emitting sources using experimental data, employing waveforms consisting of orthogonal codes.","sentences":["Angle-of-Arrival estimation technology, with its potential advantages, emerges as an intriguing choice for indoor localization.","Notably, it holds the promise of reducing installation costs.","In contrast to ToF/TDoA based systems, AoA-based approaches require a reduced number of nodes for effective localization.","This characteristic establishes a trade-off between installation costs and the complexity of hardware and software.","Moreover, the appeal of acoustic localization is further heightened by its capacity to provide cost-effective hardware solutions while maintaining a high degree of accuracy.","Consequently, acoustic AoA estimation technology stands out as a feasible and compelling option in the field of indoor localization.","Sparse arrays additionally have the ability to estimate the DoA of more sources than available sensors by placing sensors in a specific geometry.","In this contribution, we introduce a measurement platform designed to evaluate various sparse array geometries experimentally.","The acoustic microphone array comprises 64 microphones arranged in an 8x8 grid, following an Uniform Rectangular Array (URA) configuration, with a grid spacing of 8.255 mm.","This configuration achieves a spatial Nyquist frequency of approximately 20.8 kHz in the acoustic domain at room temperature.","Notably, the array exhibits a mean spherical error of 1.26{\\deg} when excluding higher elevation angles.","The platform allows for masking sensors to simulate sparse array configurations.","We assess four array geometries through simulations and experimental data, identifying the Open-Box and Nested array geometries as robust candidates.","Additionally, we demonstrate the array's capability to concurrently estimate the directions of three emitting sources using experimental data, employing waveforms consisting of orthogonal codes."],"url":"http://arxiv.org/abs/2406.09001v1","category":"eess.SP"}
{"created":"2024-06-13 10:58:25","title":"A Passwordless MFA Utlizing Biometrics, Proximity and Contactless Communication","abstract":"Despite being more secure and strongly promoted, two-factor (2FA) or multi-factor (MFA) schemes either fail to protect against recent phishing threats such as real-time MITM, controls/relay MITM, malicious browser extension-based phishing attacks, and/or need the users to purchase and carry other hardware for additional account protection. Leveraging the unprecedented popularity of NFC and BLE-enabled smartphones, we explore a new horizon for designing an MFA scheme. This paper introduces an advanced authentication method for user verification that utilizes the user's real-time facial biometric identity, which serves as an inherent factor, together with BLE- NFC-enabled mobile devices, which operate as an ownership factor. We have implemented a prototype authentication system on a BLE-NFC-enabled Android device, and initial threat modeling suggests that it is safe against known phishing attacks. The scheme has been compared with other popular schemes using the Bonneau et al. assessment framework in terms of usability, deployability, and security.","sentences":["Despite being more secure and strongly promoted, two-factor (2FA) or multi-factor (MFA) schemes either fail to protect against recent phishing threats such as real-time MITM, controls/relay MITM, malicious browser extension-based phishing attacks, and/or need the users to purchase and carry other hardware for additional account protection.","Leveraging the unprecedented popularity of NFC and BLE-enabled smartphones, we explore a new horizon for designing an MFA scheme.","This paper introduces an advanced authentication method for user verification that utilizes the user's real-time facial biometric identity, which serves as an inherent factor, together with BLE-","NFC-enabled mobile devices, which operate as an ownership factor.","We have implemented a prototype authentication system on a BLE-NFC-enabled Android device, and initial threat modeling suggests that it is safe against known phishing attacks.","The scheme has been compared with other popular schemes using the Bonneau et al. assessment framework in terms of usability, deployability, and security."],"url":"http://arxiv.org/abs/2406.09000v1","category":"cs.CR"}
{"created":"2024-06-13 10:57:35","title":"Variational quantum Hamiltonian engineering","abstract":"The Hamiltonian of a quantum system is represented in terms of operators corresponding to the kinetic and potential energies of the system. The expectation value of a Hamiltonian and Hamiltonian simulation are two of the most fundamental tasks in quantum computation. The overheads for realizing the two tasks are determined by the Pauli norm of Hamiltonian, which sums over all the absolute values of Pauli coefficients. In this work, we propose a variational quantum algorithm (VQA) called variational quantum Hamiltonian engineering (VQHE) to minimize the Pauli norm of Hamiltonian, such that the overhead for executing expectation value estimation and Hamiltonian simulation can be reduced. First, we develop a theory to encode the Pauli norm optimization problem into the vector L1-norm minimization problem. Then we devise an appropriate cost function and utilize the parameterized quantum circuits (PQC) to minimize the cost function. We also conduct numerical experiments to reduce the Pauli norm of the Ising Hamiltonian and molecules' Hamiltonian to show the efficiency of the proposed VQHE.","sentences":["The Hamiltonian of a quantum system is represented in terms of operators corresponding to the kinetic and potential energies of the system.","The expectation value of a Hamiltonian and Hamiltonian simulation are two of the most fundamental tasks in quantum computation.","The overheads for realizing the two tasks are determined by the Pauli norm of Hamiltonian, which sums over all the absolute values of Pauli coefficients.","In this work, we propose a variational quantum algorithm (VQA) called variational quantum Hamiltonian engineering (VQHE) to minimize the Pauli norm of Hamiltonian, such that the overhead for executing expectation value estimation and Hamiltonian simulation can be reduced.","First, we develop a theory to encode the Pauli norm optimization problem into the vector L1-norm minimization problem.","Then we devise an appropriate cost function and utilize the parameterized quantum circuits (PQC) to minimize the cost function.","We also conduct numerical experiments to reduce the Pauli norm of the Ising Hamiltonian and molecules' Hamiltonian to show the efficiency of the proposed VQHE."],"url":"http://arxiv.org/abs/2406.08998v1","category":"quant-ph"}
{"created":"2024-06-13 10:53:41","title":"Asymptotic Stability and Strict Passivity of Port-Hamiltonian Descriptor Systems via State Feedback","abstract":"While port-Hamiltonian descriptor systems are known to be stable and passive, they may not be asymptotically stable or strictly passive. Necessary and sufficient conditions are presented when these properties as well as the regularity and the index one property can be achieved via state feedback while preserving the port-Hamiltonian structure.","sentences":["While port-Hamiltonian descriptor systems are known to be stable and passive, they may not be asymptotically stable or strictly passive.","Necessary and sufficient conditions are presented when these properties as well as the regularity and the index one property can be achieved via state feedback while preserving the port-Hamiltonian structure."],"url":"http://arxiv.org/abs/2406.08994v1","category":"math.OC"}
{"created":"2024-06-13 10:26:04","title":"Bayesian Inference of General Noise Model Parameters from Surface Code's Syndrome Statistics","abstract":"Active research on the surface code shows that its decoding performance can be significantly enhanced by utilizing the information of the noise model and optimizing the grid shape and decoding algorithm. Usually, the parameters in the noise model for the quantum error correction code must be prepared separately using some method, such as the quantum process tomography. There is a strong need to perform noise model estimation in parallel with the syndrome measurement during decoding to avoid the demanding prior tomography procedure. While noise model estimation based on syndrome measurement statistics is well-explored for Pauli noise, it remains under-studied for more complex noise models like amplitude damping. In this paper, we propose general noise model Bayesian inference methods that integrate the surface code's tensor network simulator, which can efficiently simulate various noise models, with Monte Carlo sampling techniques. For stationary noise, where the noise parameters are constant and do not change, we propose a method based on the Markov chain Monte Carlo. For time-varying noise, which is a more realistic situation, we introduce another method based on the sequential Monte Carlo. We present the numerical results of applying the proposed methods to various noise models, such as static, time-varying, and non-uniform cases, and evaluate their performance in detail.","sentences":["Active research on the surface code shows that its decoding performance can be significantly enhanced by utilizing the information of the noise model and optimizing the grid shape and decoding algorithm.","Usually, the parameters in the noise model for the quantum error correction code must be prepared separately using some method, such as the quantum process tomography.","There is a strong need to perform noise model estimation in parallel with the syndrome measurement during decoding to avoid the demanding prior tomography procedure.","While noise model estimation based on syndrome measurement statistics is well-explored for Pauli noise, it remains under-studied for more complex noise models like amplitude damping.","In this paper, we propose general noise model Bayesian inference methods that integrate the surface code's tensor network simulator, which can efficiently simulate various noise models, with Monte Carlo sampling techniques.","For stationary noise, where the noise parameters are constant and do not change, we propose a method based on the Markov chain Monte Carlo.","For time-varying noise, which is a more realistic situation, we introduce another method based on the sequential Monte Carlo.","We present the numerical results of applying the proposed methods to various noise models, such as static, time-varying, and non-uniform cases, and evaluate their performance in detail."],"url":"http://arxiv.org/abs/2406.08981v1","category":"quant-ph"}
{"created":"2024-06-13 10:23:52","title":"From Theory to Therapy: Reframing SBDD Model Evaluation via Practical Metrics","abstract":"Recent advancements in structure-based drug design (SBDD) have significantly enhanced the efficiency and precision of drug discovery by generating molecules tailored to bind specific protein pockets. Despite these technological strides, their practical application in real-world drug development remains challenging due to the complexities of synthesizing and testing these molecules. The reliability of the Vina docking score, the current standard for assessing binding abilities, is increasingly questioned due to its susceptibility to overfitting. To address these limitations, we propose a comprehensive evaluation framework that includes assessing the similarity of generated molecules to known active compounds, introducing a virtual screening-based metric for practical deployment capabilities, and re-evaluating binding affinity more rigorously. Our experiments reveal that while current SBDD models achieve high Vina scores, they fall short in practical usability metrics, highlighting a significant gap between theoretical predictions and real-world applicability. Our proposed metrics and dataset aim to bridge this gap, enhancing the practical applicability of future SBDD models and aligning them more closely with the needs of pharmaceutical research and development.","sentences":["Recent advancements in structure-based drug design (SBDD) have significantly enhanced the efficiency and precision of drug discovery by generating molecules tailored to bind specific protein pockets.","Despite these technological strides, their practical application in real-world drug development remains challenging due to the complexities of synthesizing and testing these molecules.","The reliability of the Vina docking score, the current standard for assessing binding abilities, is increasingly questioned due to its susceptibility to overfitting.","To address these limitations, we propose a comprehensive evaluation framework that includes assessing the similarity of generated molecules to known active compounds, introducing a virtual screening-based metric for practical deployment capabilities, and re-evaluating binding affinity more rigorously.","Our experiments reveal that while current SBDD models achieve high Vina scores, they fall short in practical usability metrics, highlighting a significant gap between theoretical predictions and real-world applicability.","Our proposed metrics and dataset aim to bridge this gap, enhancing the practical applicability of future SBDD models and aligning them more closely with the needs of pharmaceutical research and development."],"url":"http://arxiv.org/abs/2406.08980v1","category":"q-bio.BM"}
{"created":"2024-06-13 10:16:29","title":"Signature of non-trivial band topology in Shubnikov--de Haas oscillations","abstract":"We investigate the Shubnikov-de Haas (SdH) magneto-oscillations in the resistivity of two-dimensional topological insulators (TIs). Within the Bernevig-Hughes-Zhang (BHZ) model for TIs in the presence of a quantizing magnetic field, we obtain analytical expressions for the SdH oscillations by combining a semiclassical approach for the resistivity and a trace formula for the density of states. We show that when the non-trivial topology is produced by inverted bands with ''Mexican-hat'' shape, SdH oscillations show an anomalous beating pattern that is {\\it solely} due to the non-trivial topology of the system. These beatings are robust against, and distinct from beatings originating from spin-orbit interactions. This provides a direct way to experimentally probe the non-trivial topology of 2D TIs entirely from a bulk measurement. Furthermore, the Fourier transform of the SdH oscillations as a function of the Fermi energy and quantum capacitance models allows for extracting both the topological gap and gap at zero momentum.","sentences":["We investigate the Shubnikov-de Haas (SdH) magneto-oscillations in the resistivity of two-dimensional topological insulators (TIs).","Within the Bernevig-Hughes-Zhang (BHZ) model for TIs in the presence of a quantizing magnetic field, we obtain analytical expressions for the SdH oscillations by combining a semiclassical approach for the resistivity and a trace formula for the density of states.","We show that when the non-trivial topology is produced by inverted bands with ''Mexican-hat'' shape, SdH oscillations show an anomalous beating pattern that is {\\it solely} due to the non-trivial topology of the system.","These beatings are robust against, and distinct from beatings originating from spin-orbit interactions.","This provides a direct way to experimentally probe the non-trivial topology of 2D TIs entirely from a bulk measurement.","Furthermore, the Fourier transform of the SdH oscillations as a function of the Fermi energy and quantum capacitance models allows for extracting both the topological gap and gap at zero momentum."],"url":"http://arxiv.org/abs/2406.08977v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-13 10:13:22","title":"Drift-diffusive resetting search process with stochastic returns: speed-up beyond optimal instantaneous return","abstract":"Stochastic resetting has emerged as a useful strategy to reduce the completion time for a broad class of first passage processes. In the canonical setup, one intermittently resets a given system to its initial configuration only to start afresh and continue evolving in time until the target goal is met. This is, however, an instantaneous process and thus less feasible for any practical purposes. A crucial generalization in this regard is to consider a finite-time return process which has significant ramifications to the first passage properties. Intriguingly, it has recently been shown that for diffusive search processes, returning in finite but stochastic time can gain significant speed-up over the instantaneous resetting process. Unlike diffusion which has a diverging mean completion time, in this paper, we ask whether this phenomena can also be observed for a first passage process with finite mean completion time. To this end, we explore the set-up of a classical drift-diffusive search process in one dimension with stochastic resetting and further assume that the return phase is modulated by a potential $U(x)=\\lambda |x|$ with $\\lambda>0$. For this process, we compute the mean first passage time exactly and underpin its characteristics with respect to the resetting rate and potential strength. We find a unified phase space that allows us to explore and identify the system parameter regions where stochastic return supersedes over both the underlying process and the process under instantaneous resetting. Furthermore and quite interestingly, we find that for a range of parameters the mean completion time under stochastic return protocol can be reduced further than the \\textit{optimally restarted} instantaneous processes. We thus believe that resetting with stochastic returns can serve as a better optimization strategy owing to its dominance over classical first passage under resetting.","sentences":["Stochastic resetting has emerged as a useful strategy to reduce the completion time for a broad class of first passage processes.","In the canonical setup, one intermittently resets a given system to its initial configuration only to start afresh and continue evolving in time until the target goal is met.","This is, however, an instantaneous process and thus less feasible for any practical purposes.","A crucial generalization in this regard is to consider a finite-time return process which has significant ramifications to the first passage properties.","Intriguingly, it has recently been shown that for diffusive search processes, returning in finite but stochastic time can gain significant speed-up over the instantaneous resetting process.","Unlike diffusion which has a diverging mean completion time, in this paper, we ask whether this phenomena can also be observed for a first passage process with finite mean completion time.","To this end, we explore the set-up of a classical drift-diffusive search process in one dimension with stochastic resetting and further assume that the return phase is modulated by a potential $U(x)=\\lambda |x|$ with $\\lambda>0$. For this process, we compute the mean first passage time exactly and underpin its characteristics with respect to the resetting rate and potential strength.","We find a unified phase space that allows us to explore and identify the system parameter regions where stochastic return supersedes over both the underlying process and the process under instantaneous resetting.","Furthermore and quite interestingly, we find that for a range of parameters the mean completion time under stochastic return protocol can be reduced further than the \\textit{optimally restarted} instantaneous processes.","We thus believe that resetting with stochastic returns can serve as a better optimization strategy owing to its dominance over classical first passage under resetting."],"url":"http://arxiv.org/abs/2406.08975v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-13 09:52:44","title":"The Sz\u00e1sz inequality for matrix polynomials and functional calculus","abstract":"The Sz\\'asz inequality is a classical result providing a bound for polynomials with zeros in the upper half of the complex plane in terms of its low-order coefficients. Some generalisations of this result to polynomials in several variables were done by Borcea-Br\\\"and\\'en and Knese. In this article the inequality is discussed in the context of matrix polynomials and matrix variables. As a byproduct we improve the scalar Sz\\'asz inequality by relaxing the assumption of the location of zeros.","sentences":["The Sz\\'asz inequality is a classical result providing a bound for polynomials with zeros in the upper half of the complex plane in terms of its low-order coefficients.","Some generalisations of this result to polynomials in several variables were done by Borcea-Br\\\"and\\'en and Knese.","In this article the inequality is discussed in the context of matrix polynomials and matrix variables.","As a byproduct we improve the scalar Sz\\'asz inequality by relaxing the assumption of the location of zeros."],"url":"http://arxiv.org/abs/2406.08965v1","category":"math.FA"}
{"created":"2024-06-13 09:51:54","title":"Kinetics-Optimized Enhanced Sampling Using Mean First Passage Times","abstract":"Molecular dynamics simulations have become essential in many areas of atomistic modelling from drug discovery to materials science. They provide critical atomic-level insights into key dynamical events experiments cannot easily capture. However, their impact often falls short as the timescales of the important processes are inaccessible using standard molecular dynamics. Enhanced sampling methods provided avenues to access such crucial rare events, for example key slow conformational changes of biomolecules. However, the bias in enhanced sampling simulations is rarely optimized, and even if they are, the optimization criteria is based on the thermodynamics or Hamiltonian of the system, but do not directly consider molecular kinetics. Here, we introduce a novel enhanced sampling algorithm that adaptively optimizes the bias based on the kinetics of the system for the first time. We identify the optimal bias that minimizes a key physical observable, the mean first passage time (MFPT) from a starting state to a target state. Our algorithm makes use of the relation between biased and unbiased kinetics obtained from discretized Markov state models (MSMs), as established in the dynamic histogram analysis method (DHAM). We demonstrate the applicability of the method for different 1D and 2D analytical potential-based model examples, NaCl dissociation in explicit water, and phosphate unbinding in Ras GTPase. Our algorithm has excellent performance compared with state-of-art enhanced sampling methods in terms of the timescales required to reach the final state in the benchmarking systems. Our findings provide a novel, kinetics-driven enhanced sampling strategy, signatured by a targeted approach to facilitate mapping rare events, with the potential for breakthrough applications in drug discovery and materials science.","sentences":["Molecular dynamics simulations have become essential in many areas of atomistic modelling from drug discovery to materials science.","They provide critical atomic-level insights into key dynamical events experiments cannot easily capture.","However, their impact often falls short as the timescales of the important processes are inaccessible using standard molecular dynamics.","Enhanced sampling methods provided avenues to access such crucial rare events, for example key slow conformational changes of biomolecules.","However, the bias in enhanced sampling simulations is rarely optimized, and even if they are, the optimization criteria is based on the thermodynamics or Hamiltonian of the system, but do not directly consider molecular kinetics.","Here, we introduce a novel enhanced sampling algorithm that adaptively optimizes the bias based on the kinetics of the system for the first time.","We identify the optimal bias that minimizes a key physical observable, the mean first passage time (MFPT) from a starting state to a target state.","Our algorithm makes use of the relation between biased and unbiased kinetics obtained from discretized Markov state models (MSMs), as established in the dynamic histogram analysis method (DHAM).","We demonstrate the applicability of the method for different 1D and 2D analytical potential-based model examples, NaCl dissociation in explicit water, and phosphate unbinding in Ras GTPase.","Our algorithm has excellent performance compared with state-of-art enhanced sampling methods in terms of the timescales required to reach the final state in the benchmarking systems.","Our findings provide a novel, kinetics-driven enhanced sampling strategy, signatured by a targeted approach to facilitate mapping rare events, with the potential for breakthrough applications in drug discovery and materials science."],"url":"http://arxiv.org/abs/2406.08964v1","category":"physics.bio-ph"}
{"created":"2024-06-13 09:50:54","title":"On quantum Stochastic Master equations","abstract":"Stochastic Master equations or quantum filtering equations for mixed states are well known objects in quantum physics. Building a mathematically rigorous theory of these equations in infinite-dimensional spaces is a long standing open problem. The first objective of this paper is to give a solution to this problem under the assumption of bounded operators providing coupling with environment (or a measurement devise). Furthermore, recently the author built the theory of the law of large number limit for continuously observed interacting quantum particle systems leading to quantum mean-field games. These limits are described by certain nontrivial extensions of quantum stochastic master equations that can be looked at as infinite-dimensional operator-valued McKean-Vlasov diffusions. The second objective of this paper is to provide a well-posedness result for these new class of McKean-Vlasov diffusions.","sentences":["Stochastic Master equations or quantum filtering equations for mixed states are well known objects in quantum physics.","Building a mathematically rigorous theory of these equations in infinite-dimensional spaces is a long standing open problem.","The first objective of this paper is to give a solution to this problem under the assumption of bounded operators providing coupling with environment (or a measurement devise).","Furthermore, recently the author built the theory of the law of large number limit for continuously observed interacting quantum particle systems leading to quantum mean-field games.","These limits are described by certain nontrivial extensions of quantum stochastic master equations that can be looked at as infinite-dimensional operator-valued McKean-Vlasov diffusions.","The second objective of this paper is to provide a well-posedness result for these new class of McKean-Vlasov diffusions."],"url":"http://arxiv.org/abs/2406.08962v1","category":"math.PR"}
{"created":"2024-06-13 09:23:43","title":"Discovery of knowledge of wall-bounded turbulence via symbolic regression","abstract":"With the development of high performance computer and experimental technology, the study of turbulence has accumulated a large number of high fidelity data. However, few general turbulence knowledge has been found from the data. So we use the symbolic regression(SR) method to find a new mixing length formula which is generally valid in wall-bounded turbulence, and this formula has physical interpretation that it has correct asymptotic relationships in viscous sublayer,buffer layer ,log-law region and outer region . Coupled with Reynolds averaged Navier-Stokes(RANS) solver, we test several classic cases. The prediction results fully demonstrate the accuracy and generalization of the formula. So far, we have found that SR method can help us find general laws from complex turbulent systems, and it is expected that through this 'white box' machine learning method, more turbulence knowledge with physical interpretation can be found in the future.","sentences":["With the development of high performance computer and experimental technology, the study of turbulence has accumulated a large number of high fidelity data.","However, few general turbulence knowledge has been found from the data.","So we use the symbolic regression(SR) method to find a new mixing length formula which is generally valid in wall-bounded turbulence, and this formula has physical interpretation that it has correct asymptotic relationships in viscous sublayer,buffer layer ,log-law region and outer region .","Coupled with Reynolds averaged Navier-Stokes(RANS) solver, we test several classic cases.","The prediction results fully demonstrate the accuracy and generalization of the formula.","So far, we have found that SR method can help us find general laws from complex turbulent systems, and it is expected that through this 'white box' machine learning method, more turbulence knowledge with physical interpretation can be found in the future."],"url":"http://arxiv.org/abs/2406.08950v1","category":"physics.flu-dyn"}
{"created":"2024-06-13 09:23:23","title":"TOI-837 b: characterisation, formation and evolutionary history of an infant warm Saturn-mass planet","abstract":"We aim to determine the fundamental properties of the $\\sim$35 Myr old star TOI-837 and its close-in Saturn-sized planet, and to investigate the system's formation and evolutionary history. We analysed TESS photometry and HARPS spectroscopic data, measured stellar and planetary parameters, and characterised the stellar activity. We performed population synthesis simulations to track the formation history of TOI-837 $b$, and to reconstruct its possible internal structure. We investigated the planetary atmospheric evolution through photo-evaporation, and quantified the prospects for atmospheric characterisation with JWST. TOI-837 $b$ has radius and mass similar to those of Saturn ($r_b$=9.71$^{+0.93}_{-0.60}$ \\rearth, $m_b$=116$^{+17}_{-18}$ M$_\\odot$, and $\\rho_b$=0.68$^{+0.20}_{-0.18}$ gcm$^{-3}$), on a primordial circular orbit. Population synthesis and early migration simulations suggest that the planet could have originated between 2-4 au, and have either a large and massive core, or a smaller Saturn-like core, depending on the opacity of the protoplanetary gas and on the growth rate of the core. We found that photo-evaporation produced negligible effects even at early ages (3-10 Myr). Transmission spectroscopy with JWST is very promising, and expected to provide constraints on atmospheric metallicity, abundance of H$_2$O, CO$_2$, CH$_4$ molecules, and to probe the presence of refractory elements. TOI-837 offers valuable prospects for follow-up observations, which are needed for a thorough characterisation. JWST will help to better constraining the formation and evolution history of the system, and understand whether TOI-837 $b$ is a Saturn-analogue.","sentences":["We aim to determine the fundamental properties of the $\\sim$35 Myr old star TOI-837 and its close-in Saturn-sized planet, and to investigate the system's formation and evolutionary history.","We analysed TESS photometry and HARPS spectroscopic data, measured stellar and planetary parameters, and characterised the stellar activity.","We performed population synthesis simulations to track the formation history of TOI-837 $b$, and to reconstruct its possible internal structure.","We investigated the planetary atmospheric evolution through photo-evaporation, and quantified the prospects for atmospheric characterisation with JWST.","TOI-837 $b$ has radius and mass similar to those of Saturn ($r_b$=9.71$^{+0.93}_{-0.60}$ \\rearth, $m_b$=116$^{+17}_{-18}$ M$_\\odot$, and $\\rho_b$=0.68$^{+0.20}_{-0.18}$ gcm$^{-3}$), on a primordial circular orbit.","Population synthesis and early migration simulations suggest that the planet could have originated between 2-4 au, and have either a large and massive core, or a smaller Saturn-like core, depending on the opacity of the protoplanetary gas and on the growth rate of the core.","We found that photo-evaporation produced negligible effects even at early ages (3-10 Myr).","Transmission spectroscopy with JWST is very promising, and expected to provide constraints on atmospheric metallicity, abundance of H$_2$O, CO$_2$, CH$_4$ molecules, and to probe the presence of refractory elements.","TOI-837 offers valuable prospects for follow-up observations, which are needed for a thorough characterisation.","JWST will help to better constraining the formation and evolution history of the system, and understand whether TOI-837 $b$ is a Saturn-analogue."],"url":"http://arxiv.org/abs/2406.08949v1","category":"astro-ph.EP"}
{"created":"2024-06-13 09:18:34","title":"Validity of the Lieb-Schultz-Mattis Theorem in Long-Range Interacting Systems","abstract":"The Lieb-Schultz-Mattis (LSM) theorem asserts that microscopic details of the system can impose non-trivial constraints on the system's low-energy properties. While traditionally applied to short-range interaction systems, where locality ensures a vanishing spectral gap in large system size limit, the impact of long-range interactions on the LSM theorem remains an open question. Long-range interactions are prevalent in experimental platforms such as Rydberg atoms, dipolar quantum gases, polar molecules, optical cavities, and trapped ions, where the interaction decay exponent can be experimentally tuned. We extend the LSM theorem in one dimension to long-range interacting systems and find that the LSM theorem holds for exponentially or power-law two-body interactions with a decay exponent $\\alpha > 2$. However, for power-law interactions with $\\alpha < 2$, the constraints of the LSM theorem on the ground state do not apply. Numerical simulations of long-range versions of the Heisenberg and Majumdar-Ghosh models, both satisfying the LSM symmetry requirements, are also provided. Our results suggest promising directions for experimental validation of the LSM theorem in systems with tunable long-range interactions.","sentences":["The Lieb-Schultz-Mattis (LSM) theorem asserts that microscopic details of the system can impose non-trivial constraints on the system's low-energy properties.","While traditionally applied to short-range interaction systems, where locality ensures a vanishing spectral gap in large system size limit, the impact of long-range interactions on the LSM theorem remains an open question.","Long-range interactions are prevalent in experimental platforms such as Rydberg atoms, dipolar quantum gases, polar molecules, optical cavities, and trapped ions, where the interaction decay exponent can be experimentally tuned.","We extend the LSM theorem in one dimension to long-range interacting systems and find that the LSM theorem holds for exponentially or power-law two-body interactions with a decay exponent $\\alpha > 2$.","However, for power-law interactions with $\\alpha < 2$, the constraints of the LSM theorem on the ground state do not apply.","Numerical simulations of long-range versions of the Heisenberg and Majumdar-Ghosh models, both satisfying the LSM symmetry requirements, are also provided.","Our results suggest promising directions for experimental validation of the LSM theorem in systems with tunable long-range interactions."],"url":"http://arxiv.org/abs/2406.08948v1","category":"cond-mat.str-el"}
{"created":"2024-06-13 09:17:31","title":"Incorporating static intersite correlation effects in vanadium dioxide through DFT$+V$","abstract":"We analyze the effects on the structural and electronic properties of vanadium dioxide (VO$_2$) of adding an empirical inter-atomic potential within the density-functional theory$+V$ (DFT$+V$) framework. We use the DFT$+V$ machinery founded on the extended Hubbard model to apply an empirical self-energy correction between nearest-neighbor vanadium atoms in both rutile and monoclinic phases, and for a set of structures interpolating between these two cases. We observe that imposing an explicit intersite interaction $V$ along the vanadium-vanadium chains enhances the characteristic bonding-antibonding splitting of the relevant bands in the monoclinic phase, thus favoring electronic dimerization and the formation of a band gap. We then explore the effect of $V$ on the structural properties and the relative energies of the two phases, finding an insulating global energy minimum for the monoclinic phase, consistent with experimental observations. With increasing $V$, this minimum becomes deeper relative to the rutile structure, and the transition from the metallic to the insulating state becomes sharper. We also analyze the effect of applying the $+V$ correction either to all or only to selected vanadium-vanadium pairs, and both in the monoclinic as well as in the metallic rutile phase. Our results suggest that DFT$+V$ can indeed serve as a computationally inexpensive unbiased way of modeling VO$_2$ which is well suited for studies that, e.g., require large system sizes.","sentences":["We analyze the effects on the structural and electronic properties of vanadium dioxide (VO$_2$) of adding an empirical inter-atomic potential within the density-functional theory$+V$ (DFT$+V$) framework.","We use the DFT$+V$ machinery founded on the extended Hubbard model to apply an empirical self-energy correction between nearest-neighbor vanadium atoms in both rutile and monoclinic phases, and for a set of structures interpolating between these two cases.","We observe that imposing an explicit intersite interaction $V$ along the vanadium-vanadium chains enhances the characteristic bonding-antibonding splitting of the relevant bands in the monoclinic phase, thus favoring electronic dimerization and the formation of a band gap.","We then explore the effect of $V$ on the structural properties and the relative energies of the two phases, finding an insulating global energy minimum for the monoclinic phase, consistent with experimental observations.","With increasing $V$, this minimum becomes deeper relative to the rutile structure, and the transition from the metallic to the insulating state becomes sharper.","We also analyze the effect of applying the $+V$ correction either to all or only to selected vanadium-vanadium pairs, and both in the monoclinic as well as in the metallic rutile phase.","Our results suggest that DFT$+V$ can indeed serve as a computationally inexpensive unbiased way of modeling VO$_2$ which is well suited for studies that, e.g., require large system sizes."],"url":"http://arxiv.org/abs/2406.08947v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-13 09:03:53","title":"LaCoOT: Layer Collapse through Optimal Transport","abstract":"Although deep neural networks are well-known for their remarkable performance in tackling complex tasks, their hunger for computational resources remains a significant hurdle, posing energy-consumption issues and restricting their deployment on resource-constrained devices, which stalls their widespread adoption. In this paper, we present an optimal transport method to reduce the depth of over-parametrized deep neural networks, alleviating their computational burden. More specifically, we propose a new regularization strategy based on the Max-Sliced Wasserstein distance to minimize the distance between the intermediate feature distributions in the neural network. We show that minimizing this distance enables the complete removal of intermediate layers in the network, with almost no performance loss and without requiring any finetuning. We assess the effectiveness of our method on traditional image classification setups. We commit to releasing the source code upon acceptance of the article.","sentences":["Although deep neural networks are well-known for their remarkable performance in tackling complex tasks, their hunger for computational resources remains a significant hurdle, posing energy-consumption issues and restricting their deployment on resource-constrained devices, which stalls their widespread adoption.","In this paper, we present an optimal transport method to reduce the depth of over-parametrized deep neural networks, alleviating their computational burden.","More specifically, we propose a new regularization strategy based on the Max-Sliced Wasserstein distance to minimize the distance between the intermediate feature distributions in the neural network.","We show that minimizing this distance enables the complete removal of intermediate layers in the network, with almost no performance loss and without requiring any finetuning.","We assess the effectiveness of our method on traditional image classification setups.","We commit to releasing the source code upon acceptance of the article."],"url":"http://arxiv.org/abs/2406.08933v1","category":"cs.LG"}
{"created":"2024-06-13 08:51:57","title":"Multiple Prior Representation Learning for Self-Supervised Monocular Depth Estimation via Hybrid Transformer","abstract":"Self-supervised monocular depth estimation aims to infer depth information without relying on labeled data. However, the lack of labeled information poses a significant challenge to the model's representation, limiting its ability to capture the intricate details of the scene accurately. Prior information can potentially mitigate this issue, enhancing the model's understanding of scene structure and texture. Nevertheless, solely relying on a single type of prior information often falls short when dealing with complex scenes, necessitating improvements in generalization performance. To address these challenges, we introduce a novel self-supervised monocular depth estimation model that leverages multiple priors to bolster representation capabilities across spatial, context, and semantic dimensions. Specifically, we employ a hybrid transformer and a lightweight pose network to obtain long-range spatial priors in the spatial dimension. Then, the context prior attention is designed to improve generalization, particularly in complex structures or untextured areas. In addition, semantic priors are introduced by leveraging semantic boundary loss, and semantic prior attention is supplemented, further refining the semantic features extracted by the decoder. Experiments on three diverse datasets demonstrate the effectiveness of the proposed model. It integrates multiple priors to comprehensively enhance the representation ability, improving the accuracy and reliability of depth estimation. Codes are available at: \\url{https://github.com/MVME-HBUT/MPRLNet}","sentences":["Self-supervised monocular depth estimation aims to infer depth information without relying on labeled data.","However, the lack of labeled information poses a significant challenge to the model's representation, limiting its ability to capture the intricate details of the scene accurately.","Prior information can potentially mitigate this issue, enhancing the model's understanding of scene structure and texture.","Nevertheless, solely relying on a single type of prior information often falls short when dealing with complex scenes, necessitating improvements in generalization performance.","To address these challenges, we introduce a novel self-supervised monocular depth estimation model that leverages multiple priors to bolster representation capabilities across spatial, context, and semantic dimensions.","Specifically, we employ a hybrid transformer and a lightweight pose network to obtain long-range spatial priors in the spatial dimension.","Then, the context prior attention is designed to improve generalization, particularly in complex structures or untextured areas.","In addition, semantic priors are introduced by leveraging semantic boundary loss, and semantic prior attention is supplemented, further refining the semantic features extracted by the decoder.","Experiments on three diverse datasets demonstrate the effectiveness of the proposed model.","It integrates multiple priors to comprehensively enhance the representation ability, improving the accuracy and reliability of depth estimation.","Codes are available at: \\url{https://github.com/MVME-HBUT/MPRLNet}"],"url":"http://arxiv.org/abs/2406.08928v1","category":"cs.CV"}
{"created":"2024-06-13 08:28:14","title":"Predicting Fault-Ride-Through Probability of Inverter-Dominated Power Grids using Machine Learning","abstract":"Due to the increasing share of renewables, the analysis of the dynamical behavior of power grids gains importance. Effective risk assessments necessitate the analysis of large number of fault scenarios. The computational costs inherent in dynamic simulations impose constraints on the number of configurations that can be analyzed. Machine Learning (ML) has proven to efficiently predict complex power grid properties. Hence, we analyze the potential of ML for predicting dynamic stability of future power grids with large shares of inverters. For this purpose, we generate a new dataset consisting of synthetic power grid models and perform dynamical simulations. As targets for the ML training, we calculate the fault-ride-through probability, which we define as the probability of staying within a ride-through curve after a fault at a bus has been cleared. Importantly, we demonstrate that ML models accurately predict the fault-ride-through probability of synthetic power grids. Finally, we also show that the ML models generalize to an IEEE-96 Test System, which emphasizes the potential of deploying ML methods to study probabilistic stability of power grids.","sentences":["Due to the increasing share of renewables, the analysis of the dynamical behavior of power grids gains importance.","Effective risk assessments necessitate the analysis of large number of fault scenarios.","The computational costs inherent in dynamic simulations impose constraints on the number of configurations that can be analyzed.","Machine Learning (ML) has proven to efficiently predict complex power grid properties.","Hence, we analyze the potential of ML for predicting dynamic stability of future power grids with large shares of inverters.","For this purpose, we generate a new dataset consisting of synthetic power grid models and perform dynamical simulations.","As targets for the ML training, we calculate the fault-ride-through probability, which we define as the probability of staying within a ride-through curve after a fault at a bus has been cleared.","Importantly, we demonstrate that ML models accurately predict the fault-ride-through probability of synthetic power grids.","Finally, we also show that the ML models generalize to an IEEE-96 Test System, which emphasizes the potential of deploying ML methods to study probabilistic stability of power grids."],"url":"http://arxiv.org/abs/2406.08917v1","category":"eess.SY"}
{"created":"2024-06-13 08:23:05","title":"GluPredKit: Development and User Evaluation of a Standardization Software for Blood Glucose Prediction","abstract":"Blood glucose prediction is an important component of biomedical technology for managing diabetes with automated insulin delivery systems. Machine learning and deep learning algorithms hold the potential to advance this technology. However, the lack of standardized methodologies impedes direct comparisons of emerging algorithms. This study addresses this challenge by developing GluPredKit, a software platform designed to standardize the training, testing, and comparison of blood glucose prediction algorithms. GluPredKit features a modular, open-source architecture, complemented by a command-line interface, comprehensive documentation, and a video tutorial to enhance usability. To ensure the platform's effectiveness and user-friendliness, we conducted preliminary testing and a user study. In this study, four participants interacted with GluPredKit and provided feedback through the System Usability Scale (SUS) and open-ended questions. The findings indicate that GluPredKit effectively addresses the standardization challenge and offers high usability, facilitating direct comparisons between different algorithms. Additionally, it serves an educational purpose by making advanced methodologies more accessible. Future directions include continuously enhancing the software based on user feedback. We also invite community contributions to further expand GluPredKit with state-of-the-art components and foster a collaborative effort in standardizing blood glucose prediction research, leading to more comparable studies.","sentences":["Blood glucose prediction is an important component of biomedical technology for managing diabetes with automated insulin delivery systems.","Machine learning and deep learning algorithms hold the potential to advance this technology.","However, the lack of standardized methodologies impedes direct comparisons of emerging algorithms.","This study addresses this challenge by developing GluPredKit, a software platform designed to standardize the training, testing, and comparison of blood glucose prediction algorithms.","GluPredKit features a modular, open-source architecture, complemented by a command-line interface, comprehensive documentation, and a video tutorial to enhance usability.","To ensure the platform's effectiveness and user-friendliness, we conducted preliminary testing and a user study.","In this study, four participants interacted with GluPredKit and provided feedback through the System Usability Scale (SUS) and open-ended questions.","The findings indicate that GluPredKit effectively addresses the standardization challenge and offers high usability, facilitating direct comparisons between different algorithms.","Additionally, it serves an educational purpose by making advanced methodologies more accessible.","Future directions include continuously enhancing the software based on user feedback.","We also invite community contributions to further expand GluPredKit with state-of-the-art components and foster a collaborative effort in standardizing blood glucose prediction research, leading to more comparable studies."],"url":"http://arxiv.org/abs/2406.08915v1","category":"cs.SE"}
{"created":"2024-06-13 08:17:30","title":"The Dissipative Effect of Caputo--Time-Fractional Derivatives and its Implications for the Solutions of Nonlinear Wave Equations","abstract":"In honor of the great Russian mathematician A. N. Kolmogorov, we would like to draw attention in the present paper to a curious mathematical observation concerning fractional differential equations describing physical systems, whose time evolution for integer derivatives has a time-honored conservative form. This observation, although known to the general mathematical community, has not, in our view, been satisfactorily addressed. More specifically, we follow the recent exploration of Caputo-Riesz time-space-fractional nonlinear wave equation, in which two of the present authors introduced an energy-type functional and proposed a finite-difference scheme to approximate the solutions of the continuous model. The relevant Klein-Gordon equation considered here has the form: \\begin{equation} \\frac {\\partial ^\\beta \\phi (x , t)} {\\partial t ^\\beta} - \\Delta ^\\alpha \\phi (x , t) + F ^\\prime (\\phi (x , t)) = 0, \\quad \\forall (x , t) \\in (-\\infty,\\infty) \\end{equation} where we explore the sine-Gordon nonlinearity $F(\\phi)=1-\\cos(\\phi)$ with smooth initial data. For $\\alpha=\\beta=2$, we naturally retrieve the exact, analytical form of breather waves expected from the literature. Focusing on the Caputo temporal derivative variation within $1< \\beta < 2$ values for $\\alpha=2$, however, we observe artificial dissipative effects, which lead to complete breather disappearance, over a time scale depending on the value of $\\beta$. We compare such findings to single degree-of-freedom linear and nonlinear oscillators in the presence of Caputo temporal derivatives and also consider anti-damping mechanisms to counter the relevant effect. These findings also motivate some interesting directions for further study, e.g., regarding the consideration of topological solitary waves, such as kinks/antikinks and their dynamical evolution in this model.","sentences":["In honor of the great Russian mathematician A. N. Kolmogorov, we would like to draw attention in the present paper to a curious mathematical observation concerning fractional differential equations describing physical systems, whose time evolution for integer derivatives has a time-honored conservative form.","This observation, although known to the general mathematical community, has not, in our view, been satisfactorily addressed.","More specifically, we follow the recent exploration of Caputo-Riesz time-space-fractional nonlinear wave equation, in which two of the present authors introduced an energy-type functional and proposed a finite-difference scheme to approximate the solutions of the continuous model.","The relevant Klein-Gordon equation considered here has the form: \\begin{equation} \\frac {\\partial ^\\beta \\phi (x , t)} {\\partial t ^\\beta} - \\Delta ^\\alpha \\phi (x , t) +","F ^\\prime (\\phi (x , t))","= 0, \\quad \\forall (x , t) \\in (-\\infty,\\infty) \\end{equation} where we explore the sine-Gordon nonlinearity $F(\\phi)=1-\\cos(\\phi)$ with smooth initial data.","For $\\alpha=\\beta=2$, we naturally retrieve the exact, analytical form of breather waves expected from the literature.","Focusing on the Caputo temporal derivative variation within $1< \\beta < 2$ values for $\\alpha=2$, however, we observe artificial dissipative effects, which lead to complete breather disappearance, over a time scale depending on the value of $\\beta$. We compare such findings to single degree-of-freedom linear and nonlinear oscillators in the presence of Caputo temporal derivatives and also consider anti-damping mechanisms to counter the relevant effect.","These findings also motivate some interesting directions for further study, e.g., regarding the consideration of topological solitary waves, such as kinks/antikinks and their dynamical evolution in this model."],"url":"http://arxiv.org/abs/2406.08912v1","category":"nlin.PS"}
{"created":"2024-06-13 08:14:23","title":"Impact of potential and temperature fluctuations on charge and heat transport in quantum Hall edges in the heat Coulomb blockade regime","abstract":"We present a broad study of charge and heat transport in a mesoscopic system where one or several quantum Hall edge channels are strongly coupled to a floating Ohmic contact (OC). It is well known that charge-current fluctuations emanating from the OC along the edge channels are highly susceptible to the OC charge capacitance in the heat Coulomb blockade regime (an impeded ability of the OC to equilibrate edge channels). Here, we demonstrate how potential- and temperature fluctuations due to finite OC charge and heat capacities impact the heat-current fluctuations emitted from the OC. First, by assuming an infinite OC heat capacity, we show that the output heat-current noise is strongly dependent on the OC charge capacitance, following from a close relation between one-dimensional charge- and heat currents. When also the OC heat capacity is finite, an interplay of potential- and temperature fluctuations influences the heat transport. Concretely, we find that the effect of the charge capacitance on heat transport manifests in terms of a strongly increased energy relaxation time in the heat Coulomb blockade regime. Furthermore, we find expressions for a broad set of output observables, such as charge and heat auto- and cross correlations, as functions of input and OC fluctuations, depending on the relation between charge and energy relaxation times compared to the frequency of fluctuations and inverse (local) temperatures as well as on the number of edge channels attached to the OC. Finally, we show that a finite OC heat capacity transforms the full counting statistics of the output charge from Gaussian to non-Gaussian. Our findings provide novel opportunities to experimentally probe and harness the quantum nature of heat transport in strongly coupled electron circuits.","sentences":["We present a broad study of charge and heat transport in a mesoscopic system where one or several quantum Hall edge channels are strongly coupled to a floating Ohmic contact (OC).","It is well known that charge-current fluctuations emanating from the OC along the edge channels are highly susceptible to the OC charge capacitance in the heat Coulomb blockade regime (an impeded ability of the OC to equilibrate edge channels).","Here, we demonstrate how potential- and temperature fluctuations due to finite OC charge and heat capacities impact the heat-current fluctuations emitted from the OC.","First, by assuming an infinite OC heat capacity, we show that the output heat-current noise is strongly dependent on the OC charge capacitance, following from a close relation between one-dimensional charge- and heat currents.","When also the OC heat capacity is finite, an interplay of potential- and temperature fluctuations influences the heat transport.","Concretely, we find that the effect of the charge capacitance on heat transport manifests in terms of a strongly increased energy relaxation time in the heat Coulomb blockade regime.","Furthermore, we find expressions for a broad set of output observables, such as charge and heat auto- and cross correlations, as functions of input and OC fluctuations, depending on the relation between charge and energy relaxation times compared to the frequency of fluctuations and inverse (local) temperatures as well as on the number of edge channels attached to the OC.","Finally, we show that a finite OC heat capacity transforms the full counting statistics of the output charge from Gaussian to non-Gaussian.","Our findings provide novel opportunities to experimentally probe and harness the quantum nature of heat transport in strongly coupled electron circuits."],"url":"http://arxiv.org/abs/2406.08910v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-13 08:09:27","title":"Ultra-low Frequency Noise External Cavity Diode Laser Systems for Quantum Applications","abstract":"We present two distinct ultra-low frequency noise lasers at 729 nm with a fast frequency noise of 30 Hz^2/Hz, corresponding to a Lorentzian linewidth of 0.1 kHz. The characteristics of both lasers, which are based on different types of laser diodes, are investigated using experimental and theoretical analysis with a focus on identifying the advantages and disadvantages of each type of system. Specifically, we study the differences and similarities in mode behaviour while tuning frequency noise and linewidth reduction. Furthermore, we demonstrate the locking capability of these systems on medium-finesse cavities. The results provide insights into the unique operational characteristics of these ultra-low noise lasers and their potential applications in quantum technology that require high levels of control fidelity.","sentences":["We present two distinct ultra-low frequency noise lasers at 729 nm with a fast frequency noise of 30 Hz^2/Hz, corresponding to a Lorentzian linewidth of 0.1 kHz.","The characteristics of both lasers, which are based on different types of laser diodes, are investigated using experimental and theoretical analysis with a focus on identifying the advantages and disadvantages of each type of system.","Specifically, we study the differences and similarities in mode behaviour while tuning frequency noise and linewidth reduction.","Furthermore, we demonstrate the locking capability of these systems on medium-finesse cavities.","The results provide insights into the unique operational characteristics of these ultra-low noise lasers and their potential applications in quantum technology that require high levels of control fidelity."],"url":"http://arxiv.org/abs/2406.08908v1","category":"physics.optics"}
{"created":"2024-06-13 08:05:36","title":"Kinematics and star formation of hub-filament systems in W49A","abstract":"W49A is a prominent giant molecular cloud (GMC) that exhibits strong star formation activities, yet its structural and kinematic properties remain uncertain. Our study aims to investigate the large-scale structure and kinematics of W49A, and elucidate the role of filaments and hub-filament systems (HFSs) in its star formation activity. We utilized continuum data from Herschel and the James Clerk Maxwell Telescope (JCMT) as well as the molecular lines 12CO (3-2), 13CO (3-2), and C18O (3-2) to identify filaments and HFS structures within W49A. Further analysis focused on the physical properties, kinematics, and mass transport within these structures. Additionally, recombination line emission from the H I/OH/Recombination (THOR) line survey was employed to trace the central H II region and ionized gas. Our findings reveal that W49A comprises one blue-shifted (B-S) HFS and one red-shifted (R-S) HFS, each with multiple filaments and dense hubs. Notably, significant velocity gradients were detected along these filaments, indicative of material transport toward the hubs. High mass accretion rates along the filaments facilitate the formation of massive stars in the HFSs. Furthermore, the presence of V-shaped structures around clumps in position-velocity diagrams suggests ongoing gravitational collapse and local star formation within the filaments. Our results indicate that W49A consists of one R-S HFS and one B-S HFS, and that the material transport from filaments to the hub promotes the formation of massive stars in the hub. These findings underscore the significance of HFSs in shaping the star formation history of W49A.","sentences":["W49A is a prominent giant molecular cloud (GMC) that exhibits strong star formation activities, yet its structural and kinematic properties remain uncertain.","Our study aims to investigate the large-scale structure and kinematics of W49A, and elucidate the role of filaments and hub-filament systems (HFSs) in its star formation activity.","We utilized continuum data from Herschel and the James Clerk Maxwell Telescope (JCMT) as well as the molecular lines 12CO (3-2), 13CO (3-2), and C18O (3-2) to identify filaments and HFS structures within W49A. Further analysis focused on the physical properties, kinematics, and mass transport within these structures.","Additionally, recombination line emission from the H I/OH/Recombination (THOR) line survey was employed to trace the central H II region and ionized gas.","Our findings reveal that W49A comprises one blue-shifted (B-S) HFS and one red-shifted (R-S) HFS, each with multiple filaments and dense hubs.","Notably, significant velocity gradients were detected along these filaments, indicative of material transport toward the hubs.","High mass accretion rates along the filaments facilitate the formation of massive stars in the HFSs.","Furthermore, the presence of V-shaped structures around clumps in position-velocity diagrams suggests ongoing gravitational collapse and local star formation within the filaments.","Our results indicate that W49A consists of one R-S HFS and one B-S HFS, and that the material transport from filaments to the hub promotes the formation of massive stars in the hub.","These findings underscore the significance of HFSs in shaping the star formation history of W49A."],"url":"http://arxiv.org/abs/2406.08906v1","category":"astro-ph.GA"}
{"created":"2024-06-13 07:57:27","title":"Delta-CoMe: Training-Free Delta-Compression with Mixed-Precision for Large Language Models","abstract":"Fine-tuning is a crucial process for adapting large language models (LLMs) to diverse applications. In certain scenarios, such as multi-tenant serving, deploying multiple LLMs becomes necessary to meet complex demands. Recent studies suggest decomposing a fine-tuned LLM into a base model and corresponding delta weights, which are then compressed using low-rank or low-bit approaches to reduce costs. In this work, we observe that existing low-rank and low-bit compression methods can significantly harm the model performance for task-specific fine-tuned LLMs (e.g., WizardMath for math problems). Motivated by the long-tail distribution of singular values in the delta weights, we propose a delta quantization approach using mixed-precision. This method employs higher-bit representation for singular vectors corresponding to larger singular values. We evaluate our approach on various fine-tuned LLMs, including math LLMs, code LLMs, chat LLMs, and even VLMs. Experimental results demonstrate that our approach performs comparably to full fine-tuned LLMs, surpassing both low-rank and low-bit baselines by a considerable margin. Additionally, we show that our method is compatible with various backbone LLMs, such as Llama-2, Llama-3, and Mistral, highlighting its generalizability.","sentences":["Fine-tuning is a crucial process for adapting large language models (LLMs) to diverse applications.","In certain scenarios, such as multi-tenant serving, deploying multiple LLMs becomes necessary to meet complex demands.","Recent studies suggest decomposing a fine-tuned LLM into a base model and corresponding delta weights, which are then compressed using low-rank or low-bit approaches to reduce costs.","In this work, we observe that existing low-rank and low-bit compression methods can significantly harm the model performance for task-specific fine-tuned LLMs (e.g., WizardMath for math problems).","Motivated by the long-tail distribution of singular values in the delta weights, we propose a delta quantization approach using mixed-precision.","This method employs higher-bit representation for singular vectors corresponding to larger singular values.","We evaluate our approach on various fine-tuned LLMs, including math LLMs, code LLMs, chat LLMs, and even VLMs.","Experimental results demonstrate that our approach performs comparably to full fine-tuned LLMs, surpassing both low-rank and low-bit baselines by a considerable margin.","Additionally, we show that our method is compatible with various backbone LLMs, such as Llama-2, Llama-3, and Mistral, highlighting its generalizability."],"url":"http://arxiv.org/abs/2406.08903v1","category":"cs.CL"}
{"created":"2024-06-13 07:55:01","title":"Fractional Chern insulator candidate in twisted bilayer checkboard lattice","abstract":"We investigate a fractional Chern insulator (FCI) candidate arising from Moir\\'e bands with higher Chern number C=2 on a magic angle twisted bilayer checkboard lattice (MATBCB). There are two nearly flat low lying bands in the single particle energy spectrum under the first magic angle $\\phi\\approx 1.608^{\\circ}$ and chiral limit. We find MATBCB hosts a nearly uniform Berry curvature distribution and exhibits tiny violation of quantum geometric trace condition in the first moir\\'e Brillourin Zone (mBZ), indicating that there is a nearly ideal quantum geometry in MATBCB in single particle level. Turning on projected Coulomb interactions, we perform exact diagonalization and find a ten-fold ground state quasi-degeneracy in many body energy spectrum with filling fraction $\\nu=1/5$. The ten-fold quasi-degenrate ground states further show spectra flow under flux pumping. By diagnosing the particle entanglement spectrum (PES) of the ground states, we obtain a clear PES gap and quasi-hole state counting consistent with Halperin spin singlet generalized Pauli principle, suggesting that a fractional Chern insulator is realized in this system.","sentences":["We investigate a fractional Chern insulator (FCI) candidate arising from Moir\\'e bands with higher Chern number C=2 on a magic angle twisted bilayer checkboard lattice (MATBCB).","There are two nearly flat low lying bands in the single particle energy spectrum under the first magic angle $\\phi\\approx 1.608^{\\circ}$ and chiral limit.","We find MATBCB hosts a nearly uniform Berry curvature distribution and exhibits tiny violation of quantum geometric trace condition in the first moir\\'e Brillourin Zone (mBZ), indicating that there is a nearly ideal quantum geometry in MATBCB in single particle level.","Turning on projected Coulomb interactions, we perform exact diagonalization and find a ten-fold ground state quasi-degeneracy in many body energy spectrum with filling fraction $\\nu=1/5$. The ten-fold quasi-degenrate ground states further show spectra flow under flux pumping.","By diagnosing the particle entanglement spectrum (PES) of the ground states, we obtain a clear PES gap and quasi-hole state counting consistent with Halperin spin singlet generalized Pauli principle, suggesting that a fractional Chern insulator is realized in this system."],"url":"http://arxiv.org/abs/2406.08901v1","category":"cond-mat.str-el"}
{"created":"2024-06-13 07:53:54","title":"On Improving Error Resilience of Neural End-to-End Speech Coders","abstract":"Error resilient tools like Packet Loss Concealment (PLC) and Forward Error Correction (FEC) are essential to maintain a reliable speech communication for applications like Voice over Internet Protocol (VoIP), where packets are frequently delayed and lost. In recent times, end-to-end neural speech codecs have seen a significant rise, due to their ability to transmit speech signal at low bitrates but few considerations were made about their error resilience in a real system. Recently introduced Neural End-to-End Speech Codec (NESC) can reproduce high quality natural speech at low bitrates. We extend its robustness to packet losses by adding a low complexity network to predict the codebook indices in latent space. Furthermore, we propose a method to add an in-band FEC at an additional bitrate of 0.8 kbps. Both subjective and objective assessment indicate the effectiveness of proposed methods, and demonstrate that coupling PLC and FEC provide significant robustness against packet losses.","sentences":["Error resilient tools like Packet Loss Concealment (PLC) and Forward Error Correction (FEC) are essential to maintain a reliable speech communication for applications like Voice over Internet Protocol (VoIP), where packets are frequently delayed and lost.","In recent times, end-to-end neural speech codecs have seen a significant rise, due to their ability to transmit speech signal at low bitrates but few considerations were made about their error resilience in a real system.","Recently introduced Neural End-to-End Speech Codec (NESC) can reproduce high quality natural speech at low bitrates.","We extend its robustness to packet losses by adding a low complexity network to predict the codebook indices in latent space.","Furthermore, we propose a method to add an in-band FEC at an additional bitrate of 0.8 kbps.","Both subjective and objective assessment indicate the effectiveness of proposed methods, and demonstrate that coupling PLC and FEC provide significant robustness against packet losses."],"url":"http://arxiv.org/abs/2406.08900v1","category":"eess.AS"}
{"created":"2024-06-13 07:46:17","title":"OpenMaterial: A Comprehensive Dataset of Complex Materials for 3D Reconstruction","abstract":"Recent advances in deep learning such as neural radiance fields and implicit neural representations have significantly propelled the field of 3D reconstruction. However, accurately reconstructing objects with complex optical properties, such as metals and glass, remains a formidable challenge due to their unique specular and light-transmission characteristics. To facilitate the development of solutions to these challenges, we introduce the OpenMaterial dataset, comprising 1001 objects made of 295 distinct materials-including conductors, dielectrics, plastics, and their roughened variants- and captured under 723 diverse lighting conditions. To this end, we utilized physics-based rendering with laboratory-measured Indices of Refraction (IOR) and generated high-fidelity multiview images that closely replicate real-world objects. OpenMaterial provides comprehensive annotations, including 3D shape, material type, camera pose, depth, and object mask. It stands as the first large-scale dataset enabling quantitative evaluations of existing algorithms on objects with diverse and challenging materials, thereby paving the way for the development of 3D reconstruction algorithms capable of handling complex material properties.","sentences":["Recent advances in deep learning such as neural radiance fields and implicit neural representations have significantly propelled the field of 3D reconstruction.","However, accurately reconstructing objects with complex optical properties, such as metals and glass, remains a formidable challenge due to their unique specular and light-transmission characteristics.","To facilitate the development of solutions to these challenges, we introduce the OpenMaterial dataset, comprising 1001 objects made of 295 distinct materials-including conductors, dielectrics, plastics, and their roughened variants- and captured under 723 diverse lighting conditions.","To this end, we utilized physics-based rendering with laboratory-measured Indices of Refraction (IOR) and generated high-fidelity multiview images that closely replicate real-world objects.","OpenMaterial provides comprehensive annotations, including 3D shape, material type, camera pose, depth, and object mask.","It stands as the first large-scale dataset enabling quantitative evaluations of existing algorithms on objects with diverse and challenging materials, thereby paving the way for the development of 3D reconstruction algorithms capable of handling complex material properties."],"url":"http://arxiv.org/abs/2406.08894v1","category":"cs.CV"}
{"created":"2024-06-13 07:45:58","title":"Modeling Nonlinear Dynamics from Videos","abstract":"We introduce a method for constructing reduced-order models directly from videos of dynamical systems. The method uses a non-intrusive tracking to isolate the motion of a user-selected part in the video of an autonomous dynamical system. In the space of delayed observations of this motion, we reconstruct a low-dimensional attracting spectral submanifold (SSM) whose internal dynamics serves as a mathematically justified reduced-order model for nearby motions of the full system. We obtain this model in a simple polynomial form that allows explicit identification of important physical system parameters, such as natural frequencies, linear and nonlinear damping and nonlinear stiffness. Beyond faithfully reproducing attracting steady states and limit cycles, our SSM-reduced models can also uncover hidden motion not seen in the video, such as unstable fixed points and unstable limit cycles forming basin boundaries. We demonstrate all these features on experimental videos of five physical systems: a double pendulum, an inverted flag in counter-flow, water sloshing in tank, a wing exhibiting aeroelastic flutter and a shimmying wheel.","sentences":["We introduce a method for constructing reduced-order models directly from videos of dynamical systems.","The method uses a non-intrusive tracking to isolate the motion of a user-selected part in the video of an autonomous dynamical system.","In the space of delayed observations of this motion, we reconstruct a low-dimensional attracting spectral submanifold (SSM) whose internal dynamics serves as a mathematically justified reduced-order model for nearby motions of the full system.","We obtain this model in a simple polynomial form that allows explicit identification of important physical system parameters, such as natural frequencies, linear and nonlinear damping and nonlinear stiffness.","Beyond faithfully reproducing attracting steady states and limit cycles, our SSM-reduced models can also uncover hidden motion not seen in the video, such as unstable fixed points and unstable limit cycles forming basin boundaries.","We demonstrate all these features on experimental videos of five physical systems: a double pendulum, an inverted flag in counter-flow, water sloshing in tank, a wing exhibiting aeroelastic flutter and a shimmying wheel."],"url":"http://arxiv.org/abs/2406.08893v1","category":"math.DS"}
{"created":"2024-06-13 07:44:21","title":"Robust Information Retrieval","abstract":"Beyond effectiveness, the robustness of an information retrieval (IR) system is increasingly attracting attention. When deployed, a critical technology such as IR should not only deliver strong performance on average but also have the ability to handle a variety of exceptional situations. In recent years, research into the robustness of IR has seen significant growth, with numerous researchers offering extensive analyses and proposing myriad strategies to address robustness challenges. In this tutorial, we first provide background information covering the basics and a taxonomy of robustness in IR. Then, we examine adversarial robustness and out-of-distribution (OOD) robustness within IR-specific contexts, extensively reviewing recent progress in methods to enhance robustness. The tutorial concludes with a discussion on the robustness of IR in the context of large language models (LLMs), highlighting ongoing challenges and promising directions for future research. This tutorial aims to generate broader attention to robustness issues in IR, facilitate an understanding of the relevant literature, and lower the barrier to entry for interested researchers and practitioners.","sentences":["Beyond effectiveness, the robustness of an information retrieval (IR) system is increasingly attracting attention.","When deployed, a critical technology such as IR should not only deliver strong performance on average but also have the ability to handle a variety of exceptional situations.","In recent years, research into the robustness of IR has seen significant growth, with numerous researchers offering extensive analyses and proposing myriad strategies to address robustness challenges.","In this tutorial, we first provide background information covering the basics and a taxonomy of robustness in IR.","Then, we examine adversarial robustness and out-of-distribution (OOD) robustness within IR-specific contexts, extensively reviewing recent progress in methods to enhance robustness.","The tutorial concludes with a discussion on the robustness of IR in the context of large language models (LLMs), highlighting ongoing challenges and promising directions for future research.","This tutorial aims to generate broader attention to robustness issues in IR, facilitate an understanding of the relevant literature, and lower the barrier to entry for interested researchers and practitioners."],"url":"http://arxiv.org/abs/2406.08891v1","category":"cs.IR"}
{"created":"2024-06-13 07:42:25","title":"Low-Overhead Channel Estimation via 3D Extrapolation for TDD mmWave Massive MIMO Systems Under High-Mobility Scenarios","abstract":"In TDD mmWave massive MIMO systems, the downlink CSI can be attained through uplink channel estimation thanks to the uplink-downlink channel reciprocity. However, the channel aging issue is significant under high-mobility scenarios and thus necessitates frequent uplink channel estimation. In addition, large amounts of antennas and subcarriers lead to high-dimensional CSI matrices, aggravating the pilot training overhead. To systematically reduce the pilot overhead, a spatial, frequency, and temporal domain (3D) channel extrapolation framework is proposed in this paper. Considering the marginal effects of pilots in the spatial and frequency domains and the effectiveness of traditional knowledge-driven channel estimation methods, we first propose a knowledge-and-data driven spatial-frequency channel extrapolation network (KDD-SFCEN) for uplink channel estimation by exploiting the least square estimator for coarse channel estimation and joint spatial-frequency channel extrapolation to reduce the spatial-frequency domain pilot overhead. Then, resorting to the uplink-downlink channel reciprocity and temporal domain dependencies of downlink channels, a temporal uplink-downlink channel extrapolation network (TUDCEN) is proposed for slot-level channel extrapolation, aiming to enlarge the pilot signal period and thus reduce the temporal domain pilot overhead under high-mobility scenarios. Specifically, we propose the spatial-frequency sampling embedding module to reduce the representation dimension and consequent computational complexity, and we propose to exploit the autoregressive generative Transformer for generating downlink channels autoregressively. Numerical results demonstrate the superiority of the proposed framework in significantly reducing the pilot training overhead by more than 16 times and improving the system's spectral efficiency under high-mobility scenarios.","sentences":["In TDD mmWave massive MIMO systems, the downlink CSI can be attained through uplink channel estimation thanks to the uplink-downlink channel reciprocity.","However, the channel aging issue is significant under high-mobility scenarios and thus necessitates frequent uplink channel estimation.","In addition, large amounts of antennas and subcarriers lead to high-dimensional CSI matrices, aggravating the pilot training overhead.","To systematically reduce the pilot overhead, a spatial, frequency, and temporal domain (3D) channel extrapolation framework is proposed in this paper.","Considering the marginal effects of pilots in the spatial and frequency domains and the effectiveness of traditional knowledge-driven channel estimation methods, we first propose a knowledge-and-data driven spatial-frequency channel extrapolation network (KDD-SFCEN) for uplink channel estimation by exploiting the least square estimator for coarse channel estimation and joint spatial-frequency channel extrapolation to reduce the spatial-frequency domain pilot overhead.","Then, resorting to the uplink-downlink channel reciprocity and temporal domain dependencies of downlink channels, a temporal uplink-downlink channel extrapolation network (TUDCEN) is proposed for slot-level channel extrapolation, aiming to enlarge the pilot signal period and thus reduce the temporal domain pilot overhead under high-mobility scenarios.","Specifically, we propose the spatial-frequency sampling embedding module to reduce the representation dimension and consequent computational complexity, and we propose to exploit the autoregressive generative Transformer for generating downlink channels autoregressively.","Numerical results demonstrate the superiority of the proposed framework in significantly reducing the pilot training overhead by more than 16 times and improving the system's spectral efficiency under high-mobility scenarios."],"url":"http://arxiv.org/abs/2406.08887v1","category":"eess.SP"}
{"created":"2024-06-13 07:37:16","title":"The Penalized Inverse Probability Measure for Conformal Classification","abstract":"The deployment of safe and trustworthy machine learning systems, and particularly complex black box neural networks, in real-world applications requires reliable and certified guarantees on their performance. The conformal prediction framework offers such formal guarantees by transforming any point into a set predictor with valid, finite-set, guarantees on the coverage of the true at a chosen level of confidence. Central to this methodology is the notion of the nonconformity score function that assigns to each example a measure of ''strangeness'' in comparison with the previously seen observations. While the coverage guarantees are maintained regardless of the nonconformity measure, the point predictor and the dataset, previous research has shown that the performance of a conformal model, as measured by its efficiency (the average size of the predicted sets) and its informativeness (the proportion of prediction sets that are singletons), is influenced by the choice of the nonconformity score function. The current work introduces the Penalized Inverse Probability (PIP) nonconformity score, and its regularized version RePIP, that allow the joint optimization of both efficiency and informativeness. Through toy examples and empirical results on the task of crop and weed image classification in agricultural robotics, the current work shows how PIP-based conformal classifiers exhibit precisely the desired behavior in comparison with other nonconformity measures and strike a good balance between informativeness and efficiency.","sentences":["The deployment of safe and trustworthy machine learning systems, and particularly complex black box neural networks, in real-world applications requires reliable and certified guarantees on their performance.","The conformal prediction framework offers such formal guarantees by transforming any point into a set predictor with valid, finite-set, guarantees on the coverage of the true at a chosen level of confidence.","Central to this methodology is the notion of the nonconformity score function that assigns to each example a measure of ''strangeness'' in comparison with the previously seen observations.","While the coverage guarantees are maintained regardless of the nonconformity measure, the point predictor and the dataset, previous research has shown that the performance of a conformal model, as measured by its efficiency (the average size of the predicted sets) and its informativeness (the proportion of prediction sets that are singletons), is influenced by the choice of the nonconformity score function.","The current work introduces the Penalized Inverse Probability (PIP) nonconformity score, and its regularized version RePIP, that allow the joint optimization of both efficiency and informativeness.","Through toy examples and empirical results on the task of crop and weed image classification in agricultural robotics, the current work shows how PIP-based conformal classifiers exhibit precisely the desired behavior in comparison with other nonconformity measures and strike a good balance between informativeness and efficiency."],"url":"http://arxiv.org/abs/2406.08884v1","category":"cs.CV"}
{"created":"2024-06-13 07:21:02","title":"Two-component system modelling shallow-water waves with constant vorticity under the Camassa-Holm scaling","abstract":"This paper is concerned with the derivation of a two-component system modelling shallow-water waves with constant vorticity under the Camassa-Holm scaling from our newly established Green-Naghdi equations with a linear shear. It is worth pointing out that the $\\rho$ component in this new system is quite different from the previous two-component system due to the effects of both vorticity and larger amplitude. We then establish the local well-posedness of this new system in Besov spaces, and present a blow-up criterion. We finally give a sufficient condition for global strong solutions to the system in some special case.","sentences":["This paper is concerned with the derivation of a two-component system modelling shallow-water waves with constant vorticity under the Camassa-Holm scaling from our newly established Green-Naghdi equations with a linear shear.","It is worth pointing out that the $\\rho$ component in this new system is quite different from the previous two-component system due to the effects of both vorticity and larger amplitude.","We then establish the local well-posedness of this new system in Besov spaces, and present a blow-up criterion.","We finally give a sufficient condition for global strong solutions to the system in some special case."],"url":"http://arxiv.org/abs/2406.08874v1","category":"math.AP"}
{"created":"2024-06-13 07:20:26","title":"Electronic and magnetic ground state of 4$d^3$ double perovskite ruthenates A$_2$LaRuO$_6$ (A $=$ Ca, Sr, Ba)","abstract":"4$d$ transition metal oxide (TMO) offers an intriguing puzzle for their electronic and magnetic ground state. They are in the cross-over regime of strong spin orbit interaction (SOI) and electron-electron correlation ($U$) with quenched orbital angular momentum. Our work unravels the electronic and magnetic ground state of the less investigated 4$d^{3}$ double perovskite ruthenates A$_{2}$LaRuO$_6$ (A = Ca, Ba). The negligible effect of SOI is evident from the bulk magnetic, specific heat measurements and density functional theory (DFT) calculations, indicating a classical spin-only magnetic ground state (${S}$ = 3/2) for the materials. Magnetization measurements show that both materials have long range antiferromagnetic order with high degree of magnetic frustration ($f$ $\\approx$13 -15). Interestingly, a near $T^2$- behavior is observed in low-$T$ magnetic heat capacity measurement, indicating the presence of low-dimensional spin-wave exciation and magnetic frustration in both materials. The temperature dependent resistivity measurements and electronic band structure calculations confirm a conventional Mott insulating ground state in these two systems. Moreover, our experimental investigation and DFT calculations highlight the reason for the nonexistence of Sr$_2$LaRuO$_6$.","sentences":["4$d$ transition metal oxide (TMO) offers an intriguing puzzle for their electronic and magnetic ground state.","They are in the cross-over regime of strong spin orbit interaction (SOI) and electron-electron correlation ($U$) with quenched orbital angular momentum.","Our work unravels the electronic and magnetic ground state of the less investigated 4$d^{3}$ double perovskite ruthenates A$_{2}$LaRuO$_6$ (A = Ca, Ba).","The negligible effect of SOI is evident from the bulk magnetic, specific heat measurements and density functional theory (DFT) calculations, indicating a classical spin-only magnetic ground state (${S}$ = 3/2) for the materials.","Magnetization measurements show that both materials have long range antiferromagnetic order with high degree of magnetic frustration ($f$ $\\approx$13 -15).","Interestingly, a near $T^2$- behavior is observed in low-$T$ magnetic heat capacity measurement, indicating the presence of low-dimensional spin-wave exciation and magnetic frustration in both materials.","The temperature dependent resistivity measurements and electronic band structure calculations confirm a conventional Mott insulating ground state in these two systems.","Moreover, our experimental investigation and DFT calculations highlight the reason for the nonexistence of Sr$_2$LaRuO$_6$."],"url":"http://arxiv.org/abs/2406.08873v1","category":"cond-mat.str-el"}
{"created":"2024-06-13 06:49:03","title":"Plan, Generate and Complicate: Improving Low-resource Dialogue State Tracking via Easy-to-Difficult Zero-shot Data Augmentation","abstract":"Data augmentation methods have been a promising direction to improve the performance of small models for low-resource dialogue state tracking. However, traditional methods rely on pre-defined user goals and neglect the importance of data complexity in this task. In this paper, we propose EDZ-DA, an Easy-to-Difficult Zero-shot Data Augmentation framework for low-resource dialogue state tracking that utilizes large language models to automatically catch the relationships of different domains and then generate the dialogue data. We also complicate the dialogues based on the domain relation to enhance the model's capability for co-reference slot tracking. Furthermore, we permute slot values to mitigate the influence of output orders and the problem of incomplete value generation. Experimental results illustrate the superiority of our proposed method compared to previous strong data augmentation baselines on MultiWOZ.","sentences":["Data augmentation methods have been a promising direction to improve the performance of small models for low-resource dialogue state tracking.","However, traditional methods rely on pre-defined user goals and neglect the importance of data complexity in this task.","In this paper, we propose EDZ-DA, an Easy-to-Difficult Zero-shot Data Augmentation framework for low-resource dialogue state tracking that utilizes large language models to automatically catch the relationships of different domains and then generate the dialogue data.","We also complicate the dialogues based on the domain relation to enhance the model's capability for co-reference slot tracking.","Furthermore, we permute slot values to mitigate the influence of output orders and the problem of incomplete value generation.","Experimental results illustrate the superiority of our proposed method compared to previous strong data augmentation baselines on MultiWOZ."],"url":"http://arxiv.org/abs/2406.08860v1","category":"cs.CL"}
{"created":"2024-06-13 06:44:46","title":"OmniH2O: Universal and Dexterous Human-to-Humanoid Whole-Body Teleoperation and Learning","abstract":"We present OmniH2O (Omni Human-to-Humanoid), a learning-based system for whole-body humanoid teleoperation and autonomy. Using kinematic pose as a universal control interface, OmniH2O enables various ways for a human to control a full-sized humanoid with dexterous hands, including using real-time teleoperation through VR headset, verbal instruction, and RGB camera. OmniH2O also enables full autonomy by learning from teleoperated demonstrations or integrating with frontier models such as GPT-4. OmniH2O demonstrates versatility and dexterity in various real-world whole-body tasks through teleoperation or autonomy, such as playing multiple sports, moving and manipulating objects, and interacting with humans. We develop an RL-based sim-to-real pipeline, which involves large-scale retargeting and augmentation of human motion datasets, learning a real-world deployable policy with sparse sensor input by imitating a privileged teacher policy, and reward designs to enhance robustness and stability. We release the first humanoid whole-body control dataset, OmniH2O-6, containing six everyday tasks, and demonstrate humanoid whole-body skill learning from teleoperated datasets.","sentences":["We present OmniH2O (Omni Human-to-Humanoid), a learning-based system for whole-body humanoid teleoperation and autonomy.","Using kinematic pose as a universal control interface, OmniH2O enables various ways for a human to control a full-sized humanoid with dexterous hands, including using real-time teleoperation through VR headset, verbal instruction, and RGB camera.","OmniH2O also enables full autonomy by learning from teleoperated demonstrations or integrating with frontier models such as GPT-4.","OmniH2O demonstrates versatility and dexterity in various real-world whole-body tasks through teleoperation or autonomy, such as playing multiple sports, moving and manipulating objects, and interacting with humans.","We develop an RL-based sim-to-real pipeline, which involves large-scale retargeting and augmentation of human motion datasets, learning a real-world deployable policy with sparse sensor input by imitating a privileged teacher policy, and reward designs to enhance robustness and stability.","We release the first humanoid whole-body control dataset, OmniH2O-6, containing six everyday tasks, and demonstrate humanoid whole-body skill learning from teleoperated datasets."],"url":"http://arxiv.org/abs/2406.08858v1","category":"cs.RO"}
{"created":"2024-06-13 06:41:43","title":"Cooperative decay of an ensemble of atoms in a one-dimensional chain with a single excitation","abstract":"We propose a new approximated expression of the cooperative decay rate of a one-dimensional chain of $N$ two-level atoms in the single-excitation configuration. From it, the interference nature of superradiance and subradiance arises naturally, without the need of solving the eigenvalue problem of the atom-atom interaction Green function. The cooperative decay rate can be interpreted as the imaginary part of the expectation value of the effective non-Hermitian Hamiltonian of the system, evaluated over a generalized Dicke state of N atoms in the single-excitation manifold. Whereas the subradiant decay rate is zero for an infinite chain, it decreases as 1/N for a finite chain. A simple approximated expression for the cooperative decay rate is obtained as a function of the lattice constant d and the atomic number N. The results are obtained first for the scalar model and then extended to the vectorial light model, assuming all the dipoles aligned.","sentences":["We propose a new approximated expression of the cooperative decay rate of a one-dimensional chain of $N$ two-level atoms in the single-excitation configuration.","From it, the interference nature of superradiance and subradiance arises naturally, without the need of solving the eigenvalue problem of the atom-atom interaction Green function.","The cooperative decay rate can be interpreted as the imaginary part of the expectation value of the effective non-Hermitian Hamiltonian of the system, evaluated over a generalized Dicke state of N atoms in the single-excitation manifold.","Whereas the subradiant decay rate is zero for an infinite chain, it decreases as 1/N for a finite chain.","A simple approximated expression for the cooperative decay rate is obtained as a function of the lattice constant d and the atomic number N.","The results are obtained first for the scalar model and then extended to the vectorial light model, assuming all the dipoles aligned."],"url":"http://arxiv.org/abs/2406.08856v1","category":"quant-ph"}
{"created":"2024-06-13 06:36:19","title":"Assessment of Uncertainty Quantification in Universal Differential Equations","abstract":"Scientific Machine Learning is a new class of approaches that integrate physical knowledge and mechanistic models with data-driven techniques for uncovering governing equations of complex processes. Among the available approaches, Universal Differential Equations (UDEs) are used to combine prior knowledge in the form of mechanistic formulations with universal function approximators, like neural networks. Integral to the efficacy of UDEs is the joint estimation of parameters within mechanistic formulations and the universal function approximators using empirical data. The robustness and applicability of resultant models, however, hinge upon the rigorous quantification of uncertainties associated with these parameters, as well as the predictive capabilities of the overall model or its constituent components. With this work, we provide a formalisation of uncertainty quantification (UQ) for UDEs and investigate important frequentist and Bayesian methods. By analysing three synthetic examples of varying complexity, we evaluate the validity and efficiency of ensembles, variational inference and Markov chain Monte Carlo sampling as epistemic UQ methods for UDEs.","sentences":["Scientific Machine Learning is a new class of approaches that integrate physical knowledge and mechanistic models with data-driven techniques for uncovering governing equations of complex processes.","Among the available approaches, Universal Differential Equations (UDEs) are used to combine prior knowledge in the form of mechanistic formulations with universal function approximators, like neural networks.","Integral to the efficacy of UDEs is the joint estimation of parameters within mechanistic formulations and the universal function approximators using empirical data.","The robustness and applicability of resultant models, however, hinge upon the rigorous quantification of uncertainties associated with these parameters, as well as the predictive capabilities of the overall model or its constituent components.","With this work, we provide a formalisation of uncertainty quantification (UQ) for UDEs and investigate important frequentist and Bayesian methods.","By analysing three synthetic examples of varying complexity, we evaluate the validity and efficiency of ensembles, variational inference and Markov chain Monte Carlo sampling as epistemic UQ methods for UDEs."],"url":"http://arxiv.org/abs/2406.08853v1","category":"stat.ML"}
{"created":"2024-06-13 06:24:54","title":"Electronic processes in collisions between nitrogen impurity ions and hydrogen atoms","abstract":"In order to interpret and predict the behavior and properties of fusion plasma, accurate cross sections for electronic processes in collisions between plasma impurities and atomic hydrogen are required. In this work, we investigate the electron capture, target excitation, and ionization processes occurring in collision of ${\\rm N}^{4+}$ with atomic hydrogen in a broad energy domain ranging from 0.06 to 225 keV/u. We consider ${\\rm N}^{4+}$ ground state ${\\rm N}^{4+} (2s)$ and also ${\\rm N}^{4+} (2p)$ since the impurities in the edge plasma environment may be excited due to collisions with electrons and ions/atoms. Total and partial cross sections in both spin-averaged and spin-resolved cases are calculated using a two-active-electron semiclassical asymptotic-state close-coupling approach. For electron capture cross sections the present results show the best overall agreement with available experimental data for both total and partial cross sections, and the origins of observed discrepancies are discussed. Furthermore, we provide new data for target excitation and ionization processes, which are essential to improve our understanding of this relevant collision system. The International Atomic Energy Agency (IAEA) has recently published a report highlighting the importance and the scarcity of such data. Our work therefore will allow a better modeling and thus understanding of magnetically confined fusion plasma.","sentences":["In order to interpret and predict the behavior and properties of fusion plasma, accurate cross sections for electronic processes in collisions between plasma impurities and atomic hydrogen are required.","In this work, we investigate the electron capture, target excitation, and ionization processes occurring in collision of ${\\rm N}^{4+}$ with atomic hydrogen in a broad energy domain ranging from 0.06 to 225 keV/u. We consider ${\\rm N}^{4+}$ ground state ${\\rm N}^{4+} (2s)$ and also ${\\rm N}^{4+} (2p)$ since the impurities in the edge plasma environment may be excited due to collisions with electrons and ions/atoms.","Total and partial cross sections in both spin-averaged and spin-resolved cases are calculated using a two-active-electron semiclassical asymptotic-state close-coupling approach.","For electron capture cross sections the present results show the best overall agreement with available experimental data for both total and partial cross sections, and the origins of observed discrepancies are discussed.","Furthermore, we provide new data for target excitation and ionization processes, which are essential to improve our understanding of this relevant collision system.","The International Atomic Energy Agency (IAEA) has recently published a report highlighting the importance and the scarcity of such data.","Our work therefore will allow a better modeling and thus understanding of magnetically confined fusion plasma."],"url":"http://arxiv.org/abs/2406.08849v1","category":"physics.atom-ph"}
{"created":"2024-06-13 06:09:16","title":"Input-Gen: Guided Generation of Stateful Inputs for Testing, Tuning, and Training","abstract":"The size and complexity of software applications is increasing at an accelerating pace. Source code repositories (along with their dependencies) require vast amounts of labor to keep them tested, maintained, and up to date. As the discipline now begins to also incorporate automatically generated programs, automation in testing and tuning is required to keep up with the pace - let alone reduce the present level of complexity. While machine learning has been used to understand and generate code in various contexts, machine learning models themselves are trained almost exclusively on static code without inputs, traces, or other execution time information. This lack of training data limits the ability of these models to understand real-world problems in software. In this work we show that inputs, like code, can be generated automatically at scale. Our generated inputs are stateful, and appear to faithfully reproduce the arbitrary data structures and system calls required to rerun a program function. By building our tool within the compiler, it both can be applied to arbitrary programming languages and architectures and can leverage static analysis and transformations for improved performance. Our approach is able to produce valid inputs, including initial memory states, for 90% of the ComPile dataset modules we explored, for a total of 21.4 million executable functions. Further, we find that a single generated input results in an average block coverage of 37%, whereas guided generation of five inputs improves it to 45%.","sentences":["The size and complexity of software applications is increasing at an accelerating pace.","Source code repositories (along with their dependencies) require vast amounts of labor to keep them tested, maintained, and up to date.","As the discipline now begins to also incorporate automatically generated programs, automation in testing and tuning is required to keep up with the pace - let alone reduce the present level of complexity.","While machine learning has been used to understand and generate code in various contexts, machine learning models themselves are trained almost exclusively on static code without inputs, traces, or other execution time information.","This lack of training data limits the ability of these models to understand real-world problems in software.","In this work we show that inputs, like code, can be generated automatically at scale.","Our generated inputs are stateful, and appear to faithfully reproduce the arbitrary data structures and system calls required to rerun a program function.","By building our tool within the compiler, it both can be applied to arbitrary programming languages and architectures and can leverage static analysis and transformations for improved performance.","Our approach is able to produce valid inputs, including initial memory states, for 90% of the ComPile dataset modules we explored, for a total of 21.4 million executable functions.","Further, we find that a single generated input results in an average block coverage of 37%, whereas guided generation of five inputs improves it to 45%."],"url":"http://arxiv.org/abs/2406.08843v1","category":"cs.SE"}
{"created":"2024-06-13 06:06:47","title":"Engineering bound state in continuum via giant atom in photonic waveguide","abstract":"The bound state in the continuum (BIC) in photonic system has been widely used in the field of lasing and sensing. We here find the controllable BIC in an artificial giant atom-dressed one-dimensional photonic waveguide. The giant atom couples to the waveguide via two distant sites. We find that the energy and the photonic distribution in the BIC can be controlled on demand by tuning the frequency and the size of the giant atom as well as its coupling phase with the waveguide. More interestingly, we predict the quantum beats in the atomic and photonic dynamical evolution, which is induced by the oscillation between the BIC and bound state outside the continuum (BOC). These findings provide an approach to manipulate the waveguide system via the bound states, and can be applied in the quantum information processing.","sentences":["The bound state in the continuum (BIC) in photonic system has been widely used in the field of lasing and sensing.","We here find the controllable BIC in an artificial giant atom-dressed one-dimensional photonic waveguide.","The giant atom couples to the waveguide via two distant sites.","We find that the energy and the photonic distribution in the BIC can be controlled on demand by tuning the frequency and the size of the giant atom as well as its coupling phase with the waveguide.","More interestingly, we predict the quantum beats in the atomic and photonic dynamical evolution, which is induced by the oscillation between the BIC and bound state outside the continuum (BOC).","These findings provide an approach to manipulate the waveguide system via the bound states, and can be applied in the quantum information processing."],"url":"http://arxiv.org/abs/2406.08841v1","category":"quant-ph"}
{"created":"2024-06-13 17:56:31","title":"Novel azimuthal observables from two-photon collision at $e^+e^-$ colliders","abstract":"In this work we advocate a set of novel azimuthal-angle-related observables associated with exclusive hadron production from two-photon fusion at $e^+ e^-$ colliders, taking the $\\gamma\\gamma\\to \\pi\\pi$ as a benchmark process. As a direct consequence of the linearly polarized quasi-real photons emitted off the electron and positron beams, the $\\cos 2\\phi$ azimuthal asymmetry in dipion production is predicted within the transverse-momentum-dependent (TMD) factorization framework. In numerical analysis, we take the helicity amplitudes of $\\gamma\\gamma\\to \\pi\\pi$ determined from the partial wave solutions in dispersion relation as input, and find that the predicted $\\cos2\\phi$ azimuthal modulation may reach 40\\% for the typical kinematical setup of {\\tt Belle} experiment. Future accurate measurement of this azimuthal asymmetry may facilitate the direct extraction of the relative phase between two helicity amplitudes with photon helicity configurations $++$ and $+-$. This knowledge provides a valuable input for the dispersive determination of the hadronic light-by-light (Hlbl) contributions, which constitutes one of the largest theoretical uncertainties in predictions for the muon anomalous magnetic moment.","sentences":["In this work we advocate a set of novel azimuthal-angle-related observables associated with exclusive hadron production from two-photon fusion at $e^+ e^-$ colliders, taking the $\\gamma\\gamma\\to \\pi\\pi$ as a benchmark process.","As a direct consequence of the linearly polarized quasi-real photons emitted off the electron and positron beams, the $\\cos 2\\phi$ azimuthal asymmetry in dipion production is predicted within the transverse-momentum-dependent (TMD) factorization framework.","In numerical analysis, we take the helicity amplitudes of $\\gamma\\gamma\\to \\pi\\pi$ determined from the partial wave solutions in dispersion relation as input, and find that the predicted $\\cos2\\phi$ azimuthal modulation may reach 40\\% for the typical kinematical setup of {\\tt Belle} experiment.","Future accurate measurement of this azimuthal asymmetry may facilitate the direct extraction of the relative phase between two helicity amplitudes with photon helicity configurations $++$ and $+-$.","This knowledge provides a valuable input for the dispersive determination of the hadronic light-by-light (Hlbl) contributions, which constitutes one of the largest theoretical uncertainties in predictions for the muon anomalous magnetic moment."],"url":"http://arxiv.org/abs/2406.09381v1","category":"hep-ph"}
{"created":"2024-06-13 17:40:15","title":"Enhancing Domain Adaptation through Prompt Gradient Alignment","abstract":"Prior Unsupervised Domain Adaptation (UDA) methods often aim to train a domain-invariant feature extractor, which may hinder the model from learning sufficiently discriminative features. To tackle this, a line of works based on prompt learning leverages the power of large-scale pre-trained vision-language models to learn both domain-invariant and specific features through a set of domain-agnostic and domain-specific learnable prompts. Those studies typically enforce invariant constraints on representation, output, or prompt space to learn such prompts. Differently, we cast UDA as a multiple-objective optimization problem in which each objective is represented by a domain loss. Under this new framework, we propose aligning per-objective gradients to foster consensus between them. Additionally, to prevent potential overfitting when fine-tuning this deep learning architecture, we penalize the norm of these gradients. To achieve these goals, we devise a practical gradient update procedure that can work under both single-source and multi-source UDA. Empirically, our method consistently surpasses other prompt-based baselines by a large margin on different UDA benchmarks","sentences":["Prior Unsupervised Domain Adaptation (UDA) methods often aim to train a domain-invariant feature extractor, which may hinder the model from learning sufficiently discriminative features.","To tackle this, a line of works based on prompt learning leverages the power of large-scale pre-trained vision-language models to learn both domain-invariant and specific features through a set of domain-agnostic and domain-specific learnable prompts.","Those studies typically enforce invariant constraints on representation, output, or prompt space to learn such prompts.","Differently, we cast UDA as a multiple-objective optimization problem in which each objective is represented by a domain loss.","Under this new framework, we propose aligning per-objective gradients to foster consensus between them.","Additionally, to prevent potential overfitting when fine-tuning this deep learning architecture, we penalize the norm of these gradients.","To achieve these goals, we devise a practical gradient update procedure that can work under both single-source and multi-source UDA.","Empirically, our method consistently surpasses other prompt-based baselines by a large margin on different UDA benchmarks"],"url":"http://arxiv.org/abs/2406.09353v1","category":"cs.LG"}
{"created":"2024-06-13 16:41:30","title":"A tutorial on fairness in machine learning in healthcare","abstract":"OBJECTIVE: Ensuring that machine learning (ML) algorithms are safe and effective within all patient groups, and do not disadvantage particular patients, is essential to clinical decision making and preventing the reinforcement of existing healthcare inequities. The objective of this tutorial is to introduce the medical informatics community to the common notions of fairness within ML, focusing on clinical applications and implementation in practice.   TARGET AUDIENCE: As gaps in fairness arise in a variety of healthcare applications, this tutorial is designed to provide an understanding of fairness, without assuming prior knowledge, to researchers and clinicians who make use of modern clinical data.   SCOPE: We describe the fundamental concepts and methods used to define fairness in ML, including an overview of why models in healthcare may be unfair, a summary and comparison of the metrics used to quantify fairness, and a discussion of some ongoing research. We illustrate some of the fairness methods introduced through a case study of mortality prediction in a publicly available electronic health record dataset. Finally, we provide a user-friendly R package for comprehensive group fairness evaluation, enabling researchers and clinicians to assess fairness in their own ML work.","sentences":["OBJECTIVE:","Ensuring that machine learning (ML) algorithms are safe and effective within all patient groups, and do not disadvantage particular patients, is essential to clinical decision making and preventing the reinforcement of existing healthcare inequities.","The objective of this tutorial is to introduce the medical informatics community to the common notions of fairness within ML, focusing on clinical applications and implementation in practice.   ","TARGET AUDIENCE:","As gaps in fairness arise in a variety of healthcare applications, this tutorial is designed to provide an understanding of fairness, without assuming prior knowledge, to researchers and clinicians who make use of modern clinical data.   ","SCOPE:","We describe the fundamental concepts and methods used to define fairness in ML, including an overview of why models in healthcare may be unfair, a summary and comparison of the metrics used to quantify fairness, and a discussion of some ongoing research.","We illustrate some of the fairness methods introduced through a case study of mortality prediction in a publicly available electronic health record dataset.","Finally, we provide a user-friendly R package for comprehensive group fairness evaluation, enabling researchers and clinicians to assess fairness in their own ML work."],"url":"http://arxiv.org/abs/2406.09307v1","category":"cs.LG"}
{"created":"2024-06-13 15:14:22","title":"Davenport constant and its variants for some non-abelian groups","abstract":"We define two variants $e(G)$, $f(G)$ of the Davenport constant $d(G)$ of a finite group $G$, that is not necessarily abelian. These naturally arising constants aid in computing $d(G)$ and are of potential independent interest. We compute the constants $d(G)$, $e(G)$, $f(G)$ for some nonabelian groups G, and demonstrate that, unlike abelian groups where these constants are identical, they can each be distinct. As a byproduct of our results, we also obtain some cases of a conjecture of J. Bass. We compute the $k$-th Davenport constant for several classes of groups as well. We also make a conjecture on $f(G)$ for metacyclic groups and provide evidence towards it.","sentences":["We define two variants $e(G)$, $f(G)$ of the Davenport constant $d(G)$ of a finite group $G$, that is not necessarily abelian.","These naturally arising constants aid in computing $d(G)$ and are of potential independent interest.","We compute the constants $d(G)$, $e(G)$, $f(G)$ for some nonabelian groups G, and demonstrate that, unlike abelian groups where these constants are identical, they can each be distinct.","As a byproduct of our results, we also obtain some cases of a conjecture of J. Bass.","We compute the $k$-th Davenport constant for several classes of groups as well.","We also make a conjecture on $f(G)$ for metacyclic groups and provide evidence towards it."],"url":"http://arxiv.org/abs/2406.09210v1","category":"math.CO"}
{"created":"2024-06-13 15:02:26","title":"Extending MGCAMB tests of gravity to nonlinear scales","abstract":"Modified Growth with CAMB (MGCAMB) is a patch for the Einstein-Boltzmann solver CAMB for cosmological tests of gravity. Until now, MGCAMB was limited to scales well-described by linear perturbation theory. In this work, we extend the framework with a phenomenological model that can capture nonlinear corrections in a broad range of modified gravity theories. The extension employs the publicly available halo model reaction code ReACT, developed for modeling the nonlinear corrections to cosmological observables in extensions of the $\\Lambda$CDM model. The nonlinear extension makes it possible to use a wider range of data from large scale structure surveys, without applying a linear scale cut. We demonstrate that, with the 3$\\times$2pt Dark Energy Survey data, we achieve a stronger constraint on the linear phenomenological functions $\\mu$ and $\\Sigma$, after marginalizing over the additional nonlinear parameter $p_1$, compared to the case without the nonlinear extension and using a linear cut. The new version of MGCAMB is now forked with CAMB on GitHub allowing for compatibility with future upgrades.","sentences":["Modified Growth with CAMB (MGCAMB) is a patch for the Einstein-Boltzmann solver CAMB for cosmological tests of gravity.","Until now, MGCAMB was limited to scales well-described by linear perturbation theory.","In this work, we extend the framework with a phenomenological model that can capture nonlinear corrections in a broad range of modified gravity theories.","The extension employs the publicly available halo model reaction code ReACT, developed for modeling the nonlinear corrections to cosmological observables in extensions of the $\\Lambda$CDM model.","The nonlinear extension makes it possible to use a wider range of data from large scale structure surveys, without applying a linear scale cut.","We demonstrate that, with the 3$\\times$2pt Dark Energy Survey data, we achieve a stronger constraint on the linear phenomenological functions $\\mu$ and $\\Sigma$, after marginalizing over the additional nonlinear parameter $p_1$, compared to the case without the nonlinear extension and using a linear cut.","The new version of MGCAMB is now forked with CAMB on GitHub allowing for compatibility with future upgrades."],"url":"http://arxiv.org/abs/2406.09204v1","category":"astro-ph.CO"}
{"created":"2024-06-13 14:57:18","title":"Orthogonality and isotropy of speaker and phonetic information in self-supervised speech representations","abstract":"Self-supervised speech representations can hugely benefit downstream speech technologies, yet the properties that make them useful are still poorly understood. Two candidate properties related to the geometry of the representation space have been hypothesized to correlate well with downstream tasks: (1) the degree of orthogonality between the subspaces spanned by the speaker centroids and phone centroids, and (2) the isotropy of the space, i.e., the degree to which all dimensions are effectively utilized. To study them, we introduce a new measure, Cumulative Residual Variance (CRV), which can be used to assess both properties. Using linear classifiers for speaker and phone ID to probe the representations of six different self-supervised models and two untrained baselines, we ask whether either orthogonality or isotropy correlate with linear probing accuracy. We find that both measures correlate with phonetic probing accuracy, though our results on isotropy are more nuanced.","sentences":["Self-supervised speech representations can hugely benefit downstream speech technologies, yet the properties that make them useful are still poorly understood.","Two candidate properties related to the geometry of the representation space have been hypothesized to correlate well with downstream tasks: (1) the degree of orthogonality between the subspaces spanned by the speaker centroids and phone centroids, and (2) the isotropy of the space, i.e., the degree to which all dimensions are effectively utilized.","To study them, we introduce a new measure, Cumulative Residual Variance (CRV), which can be used to assess both properties.","Using linear classifiers for speaker and phone ID to probe the representations of six different self-supervised models and two untrained baselines, we ask whether either orthogonality or isotropy correlate with linear probing accuracy.","We find that both measures correlate with phonetic probing accuracy, though our results on isotropy are more nuanced."],"url":"http://arxiv.org/abs/2406.09200v1","category":"cs.CL"}
{"created":"2024-06-13 14:50:58","title":"Lie Symmetry Net: Preserving Conservation Laws in Modelling Financial Market Dynamics via Differential Equations","abstract":"This paper employs a novel Lie symmetry-based framework to model the intrinsic symmetries within financial market. Specifically, we introduce {\\it Lie symmetry net} (LSN), which characterises the Lie symmetry of the differential equations (DE) estimating financial market dynamics, such as the Black-Scholes equation and the Va\\v{s}i\\v{c}ek equation. To simulate these differential equations in a symmetry-aware manner, LSN incorporates a Lie symmetry risk derived from the conservation laws associated with the Lie symmetry operators of the target differential equations. This risk measures how well the Lie symmetry is realised and guides the training of LSN under the structural risk minimisation framework. Extensive numerical experiments demonstrate that LSN effectively realises the Lie symmetry and achieves an error reduction of more than {\\it one order of magnitude} compared to state-of-the-art methods. The code is available at \\href{https://github.com/Jxl163/LSN_code}{https://github.com/Jxl163/LSN$\\_$code}.","sentences":["This paper employs a novel Lie symmetry-based framework to model the intrinsic symmetries within financial market.","Specifically, we introduce {\\it Lie symmetry net} (LSN), which characterises the Lie symmetry of the differential equations (DE) estimating financial market dynamics, such as the Black-Scholes equation and the Va\\v{s}i\\v{c}ek equation.","To simulate these differential equations in a symmetry-aware manner, LSN incorporates a Lie symmetry risk derived from the conservation laws associated with the Lie symmetry operators of the target differential equations.","This risk measures how well the Lie symmetry is realised and guides the training of LSN under the structural risk minimisation framework.","Extensive numerical experiments demonstrate that LSN effectively realises the Lie symmetry and achieves an error reduction of more than {\\it one order of magnitude} compared to state-of-the-art methods.","The code is available at \\href{https://github.com/Jxl163/LSN_code}{https://github.com/Jxl163/LSN$\\_$code}."],"url":"http://arxiv.org/abs/2406.09189v1","category":"math.AP"}
{"created":"2024-06-13 14:16:50","title":"EncCluster: Scalable Functional Encryption in Federated Learning through Weight Clustering and Probabilistic Filters","abstract":"Federated Learning (FL) enables model training across decentralized devices by communicating solely local model updates to an aggregation server. Although such limited data sharing makes FL more secure than centralized approached, FL remains vulnerable to inference attacks during model update transmissions. Existing secure aggregation approaches rely on differential privacy or cryptographic schemes like Functional Encryption (FE) to safeguard individual client data. However, such strategies can reduce performance or introduce unacceptable computational and communication overheads on clients running on edge devices with limited resources. In this work, we present EncCluster, a novel method that integrates model compression through weight clustering with recent decentralized FE and privacy-enhancing data encoding using probabilistic filters to deliver strong privacy guarantees in FL without affecting model performance or adding unnecessary burdens to clients. We performed a comprehensive evaluation, spanning various datasets and architectures, to demonstrate EncCluster's scalability across encryption levels. Our findings reveal that EncCluster significantly reduces communication costs - below even conventional FedAvg - and accelerates encryption by more than four times over all baselines; at the same time, it maintains high model accuracy and enhanced privacy assurances.","sentences":["Federated Learning (FL) enables model training across decentralized devices by communicating solely local model updates to an aggregation server.","Although such limited data sharing makes FL more secure than centralized approached, FL remains vulnerable to inference attacks during model update transmissions.","Existing secure aggregation approaches rely on differential privacy or cryptographic schemes like Functional Encryption (FE) to safeguard individual client data.","However, such strategies can reduce performance or introduce unacceptable computational and communication overheads on clients running on edge devices with limited resources.","In this work, we present EncCluster, a novel method that integrates model compression through weight clustering with recent decentralized FE and privacy-enhancing data encoding using probabilistic filters to deliver strong privacy guarantees in FL without affecting model performance or adding unnecessary burdens to clients.","We performed a comprehensive evaluation, spanning various datasets and architectures, to demonstrate EncCluster's scalability across encryption levels.","Our findings reveal that EncCluster significantly reduces communication costs - below even conventional FedAvg - and accelerates encryption by more than four times over all baselines; at the same time, it maintains high model accuracy and enhanced privacy assurances."],"url":"http://arxiv.org/abs/2406.09152v1","category":"cs.CR"}
{"created":"2024-06-13 14:16:14","title":"Reducing the Space Used by the Sieve of Eratosthenes When Factoring","abstract":"We present a version of the sieve of Eratosthenes that can factor all integers $\\le x$ in $O(x \\log\\log x)$ arithmetic operations using at most $O(\\sqrt{x}/\\log\\log x)$ bits of space. This is an improved space bound under the condition that the algorithm takes at most $O(x\\log\\log x)$ time. We also show our algorithm performs well in practice.","sentences":["We present a version of the sieve of Eratosthenes that can factor all integers $\\le x$ in $O(x \\log\\log x)$ arithmetic operations using at most $O(\\sqrt{x}/\\log\\log x)$ bits of space.","This is an improved space bound under the condition that the algorithm takes at most $O(x\\log\\log x)$ time.","We also show our algorithm performs well in practice."],"url":"http://arxiv.org/abs/2406.09150v1","category":"cs.DS"}
{"created":"2024-06-13 14:01:02","title":"The Milky Way as Seen by Classical Cepheids II: Spiral Structure","abstract":"As a relatively young and bright population, and the archetype of standard candles, classical Cepheids make an ideal population to trace non-axisymmetric structure in the young stellar disk to large distances. We use the new distances derived in Paper I based on mid-IR WISE photometry for a selected sample of 2857 dynamically young Cepheids to trace the spiral arms of the Milky Way. The Perseus and Sagittarius-Carina arms are clearly evident in the third and fourth Galactic quadrants, while the Local and Scutum arms are much weaker, with extinction severely limiting our view of the latter, inner-most spiral arm. Pitch angles are derived for each arm over various ranges of galactic azimuth, each covering at least 90{\\deg} in azimuth. Our method of detecting spiral arms and deriving pitch angles does not rely on pre-assigning sources to specific arms. While spiral structure in the first and second quadrant is not obvious, in part due to extinction effects, it is not inconsistent with that seen in the third and fourth quadrants. In summary, the Cepheids allow us to map spiral structure in the third and fourth Galactic quadrants where there are currently few masers with astrometric parallaxes, thus significantly extending our picture of the Milky Way on large-scales.","sentences":["As a relatively young and bright population, and the archetype of standard candles, classical Cepheids make an ideal population to trace non-axisymmetric structure in the young stellar disk to large distances.","We use the new distances derived in Paper I based on mid-IR WISE photometry for a selected sample of 2857 dynamically young Cepheids to trace the spiral arms of the Milky Way.","The Perseus and Sagittarius-Carina arms are clearly evident in the third and fourth Galactic quadrants, while the Local and Scutum arms are much weaker, with extinction severely limiting our view of the latter, inner-most spiral arm.","Pitch angles are derived for each arm over various ranges of galactic azimuth, each covering at least 90{\\deg} in azimuth.","Our method of detecting spiral arms and deriving pitch angles does not rely on pre-assigning sources to specific arms.","While spiral structure in the first and second quadrant is not obvious, in part due to extinction effects, it is not inconsistent with that seen in the third and fourth quadrants.","In summary, the Cepheids allow us to map spiral structure in the third and fourth Galactic quadrants where there are currently few masers with astrometric parallaxes, thus significantly extending our picture of the Milky Way on large-scales."],"url":"http://arxiv.org/abs/2406.09127v1","category":"astro-ph.GA"}
{"created":"2024-06-13 13:48:15","title":"On Modulation and Translation Invariant Operators and the Heisenberg Module","abstract":"We investigate spaces of operators which are invariant under translations or modulations by lattices in phase space. The natural connection to the Heisenberg module is considered, giving results on the characterisation of such operators as limits of finite--rank operators. Discrete representations of these operators in terms of elementary objects and the composition calculus are given. Different quantisation schemes are discussed with respect to the results.","sentences":["We investigate spaces of operators which are invariant under translations or modulations by lattices in phase space.","The natural connection to the Heisenberg module is considered, giving results on the characterisation of such operators as limits of finite--rank operators.","Discrete representations of these operators in terms of elementary objects and the composition calculus are given.","Different quantisation schemes are discussed with respect to the results."],"url":"http://arxiv.org/abs/2406.09119v1","category":"math.FA"}
{"created":"2024-06-13 13:43:16","title":"The Milky Way as seen by Classical Cepheids I: distances based on mid-infrared photometry","abstract":"Classical Cepheids are the archetype of the standard candle, thanks to the period-luminosity relation which allows to measure their intrinsic brightness. They are also relatively young and bright, potentially making them excellent tracers of the young stellar population that is responsible for shaping the visible aspect of our Galaxy, the Milky Way. However, being observers embedded in the dusty interstellar medium of the Galaxy, deriving reliable photometric distances to classical Cepheids of the Milky Way is a challenge. The typical approach is to use reddening-free indices, such as Wesenheit magnitudes, to obviate the need for an extinction correction. However, this approach is not reliable - especially toward the inner Galaxy - as its assumption of a universal total-to-selective extinction ratio is not satisfied, particularly in lines-of-sight where the extinction is high and crosses spiral arms. We instead estimate new distances for 3425 Cepheids based on mid-IR photometry from WISE, which suffers minimally from extinction, and by adopting a 3D extinction map to calculate the necessary (albeit small) extinction corrections. We show that our distances are consistent with Gaia's parallaxes for the subset with relative parallax errors smaller than 10%, verifying that our mean distance errors are of the order of 13% and that the mean parallax zero point for this subsample is 7 $\\mu$as. The catalog of Cepheid distances is made available online.","sentences":["Classical Cepheids are the archetype of the standard candle, thanks to the period-luminosity relation which allows to measure their intrinsic brightness.","They are also relatively young and bright, potentially making them excellent tracers of the young stellar population that is responsible for shaping the visible aspect of our Galaxy, the Milky Way.","However, being observers embedded in the dusty interstellar medium of the Galaxy, deriving reliable photometric distances to classical Cepheids of the Milky Way is a challenge.","The typical approach is to use reddening-free indices, such as Wesenheit magnitudes, to obviate the need for an extinction correction.","However, this approach is not reliable - especially toward the inner Galaxy - as its assumption of a universal total-to-selective extinction ratio is not satisfied, particularly in lines-of-sight where the extinction is high and crosses spiral arms.","We instead estimate new distances for 3425 Cepheids based on mid-IR photometry from WISE, which suffers minimally from extinction, and by adopting a 3D extinction map to calculate the necessary (albeit small) extinction corrections.","We show that our distances are consistent with Gaia's parallaxes for the subset with relative parallax errors smaller than 10%, verifying that our mean distance errors are of the order of 13% and that the mean parallax zero point for this subsample is 7 $\\mu$as.","The catalog of Cepheid distances is made available online."],"url":"http://arxiv.org/abs/2406.09113v1","category":"astro-ph.GA"}
{"created":"2024-06-13 13:33:07","title":"Selecting Alternative Metals for Advanced Interconnects","abstract":"Today, interconnect resistance and reliability are key limiters for the performance of advanced CMOS circuits. As transistor scaling is slowing, interconnect scaling has become the main driver for circuit miniaturization, and interconnect limitations are expected to become even more stringent in future CMOS technology nodes. Current Cu dual-damascene metallization is also becoming increasingly challenging as critical interconnect dimensions approach 10 nm, alternative metallization schemes are researched with increasing intensity for about a decade. The selection of alternative metals is a highly multifaceted task and includes many criteria, covering the resistivity at reduced dimension, reliability and thermal aspects, as well as a sustainability perspective. In this tutorial, we introduce the basic criteria for alternative metal benchmarking and selection, and discuss the current state of the art of the field. The tutorial covers materials close to manufacturing introduction, materials under actual research, as well as future directions for fundamental research. While first alternatives to Cu metallization in commercial CMOS devices have become a reality recently, research for the ultimate interconnect metal is ongoing.","sentences":["Today, interconnect resistance and reliability are key limiters for the performance of advanced CMOS circuits.","As transistor scaling is slowing, interconnect scaling has become the main driver for circuit miniaturization, and interconnect limitations are expected to become even more stringent in future CMOS technology nodes.","Current Cu dual-damascene metallization is also becoming increasingly challenging as critical interconnect dimensions approach 10 nm, alternative metallization schemes are researched with increasing intensity for about a decade.","The selection of alternative metals is a highly multifaceted task and includes many criteria, covering the resistivity at reduced dimension, reliability and thermal aspects, as well as a sustainability perspective.","In this tutorial, we introduce the basic criteria for alternative metal benchmarking and selection, and discuss the current state of the art of the field.","The tutorial covers materials close to manufacturing introduction, materials under actual research, as well as future directions for fundamental research.","While first alternatives to Cu metallization in commercial CMOS devices have become a reality recently, research for the ultimate interconnect metal is ongoing."],"url":"http://arxiv.org/abs/2406.09106v1","category":"physics.app-ph"}
{"created":"2024-06-13 12:58:47","title":"Strong External Difference Families and Classification of $\u03b1$-valuations","abstract":"One method of constructing $(a^2+1, 2,a, 1)$-SEDFs (i.e., strong external difference families) in $\\mathbb{Z}_{a^2+1}$ makes use of $\\alpha$-valuations of complete bipartite graphs $K_{a,a}$. We explore this approach and we provide a classification theorem which shows that all such $\\alpha$-valuations can be constructed recursively via a sequence of ``blow-up'' operations. We also enumerate all $(a^2+1, 2,a, 1)$-SEDFs in $\\mathbb{Z}_{a^2+1}$ for $a \\leq 14$ and we show that all these SEDFs are equivalent to $\\alpha$-valuations via affine transformations. Whether this holds for all $a > 14$ as well is an interesting open problem. We also study SEDFs in dihedral groups, where we show that two known constructions are equivalent.","sentences":["One method of constructing $(a^2+1, 2,a, 1)$-SEDFs (i.e., strong external difference families) in $\\mathbb{Z}_{a^2+1}$ makes use of $\\alpha$-valuations of complete bipartite graphs $K_{a,a}$. We explore this approach and we provide a classification theorem which shows that all such $\\alpha$-valuations can be constructed recursively via a sequence of ``blow-up'' operations.","We also enumerate all $(a^2+1, 2,a, 1)$-SEDFs in $\\mathbb{Z}_{a^2+1}$ for $a \\leq 14$ and we show that all these SEDFs are equivalent to $\\alpha$-valuations via affine transformations.","Whether this holds for all $a > 14$ as well is an interesting open problem.","We also study SEDFs in dihedral groups, where we show that two known constructions are equivalent."],"url":"http://arxiv.org/abs/2406.09075v1","category":"math.CO"}
{"created":"2024-06-13 12:58:00","title":"Are we making progress in unlearning? Findings from the first NeurIPS unlearning competition","abstract":"We present the findings of the first NeurIPS competition on unlearning, which sought to stimulate the development of novel algorithms and initiate discussions on formal and robust evaluation methodologies. The competition was highly successful: nearly 1,200 teams from across the world participated, and a wealth of novel, imaginative solutions with different characteristics were contributed. In this paper, we analyze top solutions and delve into discussions on benchmarking unlearning, which itself is a research problem. The evaluation methodology we developed for the competition measures forgetting quality according to a formal notion of unlearning, while incorporating model utility for a holistic evaluation. We analyze the effectiveness of different instantiations of this evaluation framework vis-a-vis the associated compute cost, and discuss implications for standardizing evaluation. We find that the ranking of leading methods remains stable under several variations of this framework, pointing to avenues for reducing the cost of evaluation. Overall, our findings indicate progress in unlearning, with top-performing competition entries surpassing existing algorithms under our evaluation framework. We analyze trade-offs made by different algorithms and strengths or weaknesses in terms of generalizability to new datasets, paving the way for advancing both benchmarking and algorithm development in this important area.","sentences":["We present the findings of the first NeurIPS competition on unlearning, which sought to stimulate the development of novel algorithms and initiate discussions on formal and robust evaluation methodologies.","The competition was highly successful: nearly 1,200 teams from across the world participated, and a wealth of novel, imaginative solutions with different characteristics were contributed.","In this paper, we analyze top solutions and delve into discussions on benchmarking unlearning, which itself is a research problem.","The evaluation methodology we developed for the competition measures forgetting quality according to a formal notion of unlearning, while incorporating model utility for a holistic evaluation.","We analyze the effectiveness of different instantiations of this evaluation framework vis-a-vis the associated compute cost, and discuss implications for standardizing evaluation.","We find that the ranking of leading methods remains stable under several variations of this framework, pointing to avenues for reducing the cost of evaluation.","Overall, our findings indicate progress in unlearning, with top-performing competition entries surpassing existing algorithms under our evaluation framework.","We analyze trade-offs made by different algorithms and strengths or weaknesses in terms of generalizability to new datasets, paving the way for advancing both benchmarking and algorithm development in this important area."],"url":"http://arxiv.org/abs/2406.09073v1","category":"cs.LG"}
{"created":"2024-06-13 12:54:20","title":"How structured are the representations in transformer-based vision encoders? An analysis of multi-object representations in vision-language models","abstract":"Forming and using symbol-like structured representations for reasoning has been considered essential for generalising over novel inputs. The primary tool that allows generalisation outside training data distribution is the ability to abstract away irrelevant information into a compact form relevant to the task. An extreme form of such abstract representations is symbols. Humans make use of symbols to bind information while abstracting away irrelevant parts to utilise the information consistently and meaningfully. This work estimates the state of such structured representations in vision encoders. Specifically, we evaluate image encoders in large vision-language pre-trained models to address the question of which desirable properties their representations lack by applying the criteria of symbolic structured reasoning described for LLMs to the image models. We test the representation space of image encoders like VIT, BLIP, CLIP, and FLAVA to characterise the distribution of the object representations in these models. In particular, we create decoding tasks using multi-object scenes from the COCO dataset, relating the token space to its input content for various objects in the scene. We use these tasks to characterise the network's token and layer-wise information modelling. Our analysis highlights that the CLS token, used for the downstream task, only focuses on a few objects necessary for the trained downstream task. Still, other individual objects are well-modelled separately by the tokens in the network originating from those objects. We further observed a widespread distribution of scene information. This demonstrates that information is far more entangled in tokens than optimal for representing objects similar to symbols. Given these symbolic properties, we show the network dynamics that cause failure modes of these models on basic downstream tasks in a multi-object scene.","sentences":["Forming and using symbol-like structured representations for reasoning has been considered essential for generalising over novel inputs.","The primary tool that allows generalisation outside training data distribution is the ability to abstract away irrelevant information into a compact form relevant to the task.","An extreme form of such abstract representations is symbols.","Humans make use of symbols to bind information while abstracting away irrelevant parts to utilise the information consistently and meaningfully.","This work estimates the state of such structured representations in vision encoders.","Specifically, we evaluate image encoders in large vision-language pre-trained models to address the question of which desirable properties their representations lack by applying the criteria of symbolic structured reasoning described for LLMs to the image models.","We test the representation space of image encoders like VIT, BLIP, CLIP, and FLAVA to characterise the distribution of the object representations in these models.","In particular, we create decoding tasks using multi-object scenes from the COCO dataset, relating the token space to its input content for various objects in the scene.","We use these tasks to characterise the network's token and layer-wise information modelling.","Our analysis highlights that the CLS token, used for the downstream task, only focuses on a few objects necessary for the trained downstream task.","Still, other individual objects are well-modelled separately by the tokens in the network originating from those objects.","We further observed a widespread distribution of scene information.","This demonstrates that information is far more entangled in tokens than optimal for representing objects similar to symbols.","Given these symbolic properties, we show the network dynamics that cause failure modes of these models on basic downstream tasks in a multi-object scene."],"url":"http://arxiv.org/abs/2406.09067v1","category":"cs.CV"}
{"created":"2024-06-13 12:30:02","title":"MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM Finetuning","abstract":"Efficient finetuning of large language models (LLMs) aims to adapt the LLMs with reduced computation and memory cost. Previous LoRA-based approaches initialize the low-rank matrices with gaussian distribution and zero values, while keeping the original weight matrices frozen. However, the trainable model parameters optimized in an unguided subspace might have interference with the well-learned subspace of the pretrained weight matrix. In this paper, we propose MiLoRA, a simple yet effective LLM finetuning approach that only updates the minor singular components of the weight matrix while keeping the principle singular components frozen. It is observed that the minor matrix corresponds to the noisy or long-tail information, while the principle matrix contains important knowledge. The MiLoRA initializes the low-rank matrices within a subspace that is orthogonal to the principle matrix, thus the pretrained knowledge is expected to be well preserved. During finetuning, MiLoRA makes the most use of the less-optimized subspace for learning the finetuning dataset. Extensive experiments on commonsense reasoning, math reasoning and instruction following benchmarks present the superior performance of our method.","sentences":["Efficient finetuning of large language models (LLMs) aims to adapt the LLMs with reduced computation and memory cost.","Previous LoRA-based approaches initialize the low-rank matrices with gaussian distribution and zero values, while keeping the original weight matrices frozen.","However, the trainable model parameters optimized in an unguided subspace might have interference with the well-learned subspace of the pretrained weight matrix.","In this paper, we propose MiLoRA, a simple yet effective LLM finetuning approach that only updates the minor singular components of the weight matrix while keeping the principle singular components frozen.","It is observed that the minor matrix corresponds to the noisy or long-tail information, while the principle matrix contains important knowledge.","The MiLoRA initializes the low-rank matrices within a subspace that is orthogonal to the principle matrix, thus the pretrained knowledge is expected to be well preserved.","During finetuning, MiLoRA makes the most use of the less-optimized subspace for learning the finetuning dataset.","Extensive experiments on commonsense reasoning, math reasoning and instruction following benchmarks present the superior performance of our method."],"url":"http://arxiv.org/abs/2406.09044v1","category":"cs.CL"}
{"created":"2024-06-13 11:40:06","title":"Cross-Modal Learning for Anomaly Detection in Fused Magnesium Smelting Process: Methodology and Benchmark","abstract":"Fused Magnesium Furnace (FMF) is a crucial industrial equipment in the production of magnesia, and anomaly detection plays a pivotal role in ensuring its efficient, stable, and secure operation. Existing anomaly detection methods primarily focus on analyzing dominant anomalies using the process variables (such as arc current) or constructing neural networks based on abnormal visual features, while overlooking the intrinsic correlation of cross-modal information. This paper proposes a cross-modal Transformer (dubbed FmFormer), designed to facilitate anomaly detection in fused magnesium smelting processes by exploring the correlation between visual features (video) and process variables (current). Our approach introduces a novel tokenization paradigm to effectively bridge the substantial dimensionality gap between the 3D video modality and the 1D current modality in a multiscale manner, enabling a hierarchical reconstruction of pixel-level anomaly detection. Subsequently, the FmFormer leverages self-attention to learn internal features within each modality and bidirectional cross-attention to capture correlations across modalities. To validate the effectiveness of the proposed method, we also present a pioneering cross-modal benchmark of the fused magnesium smelting process, featuring synchronously acquired video and current data for over 2.2 million samples. Leveraging cross-modal learning, the proposed FmFormer achieves state-of-the-art performance in detecting anomalies, particularly under extreme interferences such as current fluctuations and visual occlusion caused by heavy water mist. The presented methodology and benchmark may be applicable to other industrial applications with some amendments. The benchmark will be released at https://github.com/GaochangWu/FMF-Benchmark.","sentences":["Fused Magnesium Furnace (FMF) is a crucial industrial equipment in the production of magnesia, and anomaly detection plays a pivotal role in ensuring its efficient, stable, and secure operation.","Existing anomaly detection methods primarily focus on analyzing dominant anomalies using the process variables (such as arc current) or constructing neural networks based on abnormal visual features, while overlooking the intrinsic correlation of cross-modal information.","This paper proposes a cross-modal Transformer (dubbed FmFormer), designed to facilitate anomaly detection in fused magnesium smelting processes by exploring the correlation between visual features (video) and process variables (current).","Our approach introduces a novel tokenization paradigm to effectively bridge the substantial dimensionality gap between the 3D video modality and the 1D current modality in a multiscale manner, enabling a hierarchical reconstruction of pixel-level anomaly detection.","Subsequently, the FmFormer leverages self-attention to learn internal features within each modality and bidirectional cross-attention to capture correlations across modalities.","To validate the effectiveness of the proposed method, we also present a pioneering cross-modal benchmark of the fused magnesium smelting process, featuring synchronously acquired video and current data for over 2.2 million samples.","Leveraging cross-modal learning, the proposed FmFormer achieves state-of-the-art performance in detecting anomalies, particularly under extreme interferences such as current fluctuations and visual occlusion caused by heavy water mist.","The presented methodology and benchmark may be applicable to other industrial applications with some amendments.","The benchmark will be released at https://github.com/GaochangWu/FMF-Benchmark."],"url":"http://arxiv.org/abs/2406.09016v1","category":"cs.CV"}
{"created":"2024-06-13 11:33:30","title":"Bayesian Statistical Modeling with Predictors from LLMs","abstract":"State of the art large language models (LLMs) have shown impressive performance on a variety of benchmark tasks and are increasingly used as components in larger applications, where LLM-based predictions serve as proxies for human judgements or decision. This raises questions about the human-likeness of LLM-derived information, alignment with human intuition, and whether LLMs could possibly be considered (parts of) explanatory models of (aspects of) human cognition or language use. To shed more light on these issues, we here investigate the human-likeness of LLMs' predictions for multiple-choice decision tasks from the perspective of Bayesian statistical modeling. Using human data from a forced-choice experiment on pragmatic language use, we find that LLMs do not capture the variance in the human data at the item-level. We suggest different ways of deriving full distributional predictions from LLMs for aggregate, condition-level data, and find that some, but not all ways of obtaining condition-level predictions yield adequate fits to human data. These results suggests that assessment of LLM performance depends strongly on seemingly subtle choices in methodology, and that LLMs are at best predictors of human behavior at the aggregate, condition-level, for which they are, however, not designed to, or usually used to, make predictions in the first place.","sentences":["State of the art large language models (LLMs) have shown impressive performance on a variety of benchmark tasks and are increasingly used as components in larger applications, where LLM-based predictions serve as proxies for human judgements or decision.","This raises questions about the human-likeness of LLM-derived information, alignment with human intuition, and whether LLMs could possibly be considered (parts of) explanatory models of (aspects of) human cognition or language use.","To shed more light on these issues, we here investigate the human-likeness of LLMs' predictions for multiple-choice decision tasks from the perspective of Bayesian statistical modeling.","Using human data from a forced-choice experiment on pragmatic language use, we find that LLMs do not capture the variance in the human data at the item-level.","We suggest different ways of deriving full distributional predictions from LLMs for aggregate, condition-level data, and find that some, but not all ways of obtaining condition-level predictions yield adequate fits to human data.","These results suggests that assessment of LLM performance depends strongly on seemingly subtle choices in methodology, and that LLMs are at best predictors of human behavior at the aggregate, condition-level, for which they are, however, not designed to, or usually used to, make predictions in the first place."],"url":"http://arxiv.org/abs/2406.09012v1","category":"cs.CL"}
{"created":"2024-06-13 11:30:29","title":"A geometric approach to informed MCMC sampling","abstract":"A Riemannian geometric framework for Markov chain Monte Carlo (MCMC) is developed where using the Fisher-Rao metric on the manifold of probability density functions (pdfs) informed proposal densities for Metropolis-Hastings (MH) algorithms are constructed. We exploit the square-root representation of pdfs under which the Fisher-Rao metric boils down to the standard $L^2$ metric on the positive orthant of the unit hypersphere. The square-root representation allows us to easily compute the geodesic distance between densities, resulting in a straightforward implementation of the proposed geometric MCMC methodology. Unlike the random walk MH that blindly proposes a candidate state using no information about the target, the geometric MH algorithms effectively move an uninformed base density (e.g., a random walk proposal density) towards different global/local approximations of the target density. We compare the proposed geometric MH algorithm with other MCMC algorithms for various Markov chain orderings, namely the covariance, efficiency, Peskun, and spectral gap orderings. The superior performance of the geometric algorithms over other MH algorithms like the random walk Metropolis, independent MH and variants of Metropolis adjusted Langevin algorithms is demonstrated in the context of various multimodal, nonlinear and high dimensional examples. In particular, we use extensive simulation and real data applications to compare these algorithms for analyzing mixture models, logistic regression models and ultra-high dimensional Bayesian variable selection models. A publicly available R package accompanies the article.","sentences":["A Riemannian geometric framework for Markov chain Monte Carlo (MCMC) is developed where using the Fisher-Rao metric on the manifold of probability density functions (pdfs) informed proposal densities for Metropolis-Hastings (MH) algorithms are constructed.","We exploit the square-root representation of pdfs under which the Fisher-Rao metric boils down to the standard $L^2$ metric on the positive orthant of the unit hypersphere.","The square-root representation allows us to easily compute the geodesic distance between densities, resulting in a straightforward implementation of the proposed geometric MCMC methodology.","Unlike the random walk MH that blindly proposes a candidate state using no information about the target, the geometric MH algorithms effectively move an uninformed base density (e.g., a random walk proposal density) towards different global/local approximations of the target density.","We compare the proposed geometric MH algorithm with other MCMC algorithms for various Markov chain orderings, namely the covariance, efficiency, Peskun, and spectral gap orderings.","The superior performance of the geometric algorithms over other MH algorithms like the random walk Metropolis, independent MH and variants of Metropolis adjusted Langevin algorithms is demonstrated in the context of various multimodal, nonlinear and high dimensional examples.","In particular, we use extensive simulation and real data applications to compare these algorithms for analyzing mixture models, logistic regression models and ultra-high dimensional Bayesian variable selection models.","A publicly available R package accompanies the article."],"url":"http://arxiv.org/abs/2406.09010v1","category":"stat.ME"}
{"created":"2024-06-13 10:38:38","title":"BTS: Building Timeseries Dataset: Empowering Large-Scale Building Analytics","abstract":"Buildings play a crucial role in human well-being, influencing occupant comfort, health, and safety. Additionally, they contribute significantly to global energy consumption, accounting for one-third of total energy usage, and carbon emissions. Optimizing building performance presents a vital opportunity to combat climate change and promote human flourishing. However, research in building analytics has been hampered by the lack of accessible, available, and comprehensive real-world datasets on multiple building operations. In this paper, we introduce the Building TimeSeries (BTS) dataset. Our dataset covers three buildings over a three-year period, comprising more than ten thousand timeseries data points with hundreds of unique ontologies. Moreover, the metadata is standardized using the Brick schema. To demonstrate the utility of this dataset, we performed benchmarks on two tasks: timeseries ontology classification and zero-shot forecasting. These tasks represent an essential initial step in addressing challenges related to interoperability in building analytics. Access to the dataset and the code used for benchmarking are available here: https://github.com/cruiseresearchgroup/DIEF_BTS .","sentences":["Buildings play a crucial role in human well-being, influencing occupant comfort, health, and safety.","Additionally, they contribute significantly to global energy consumption, accounting for one-third of total energy usage, and carbon emissions.","Optimizing building performance presents a vital opportunity to combat climate change and promote human flourishing.","However, research in building analytics has been hampered by the lack of accessible, available, and comprehensive real-world datasets on multiple building operations.","In this paper, we introduce the Building TimeSeries (BTS) dataset.","Our dataset covers three buildings over a three-year period, comprising more than ten thousand timeseries data points with hundreds of unique ontologies.","Moreover, the metadata is standardized using the Brick schema.","To demonstrate the utility of this dataset, we performed benchmarks on two tasks: timeseries ontology classification and zero-shot forecasting.","These tasks represent an essential initial step in addressing challenges related to interoperability in building analytics.","Access to the dataset and the code used for benchmarking are available here: https://github.com/cruiseresearchgroup/DIEF_BTS ."],"url":"http://arxiv.org/abs/2406.08990v1","category":"cs.LG"}
{"created":"2024-06-13 10:35:27","title":"Probing the polarized emission from SMC X-1: the brightest X-ray pulsar observed by IXPE","abstract":"Recent observations of X-ray pulsars (XRPs) performed by the Imaging X-ray Polarimetry Explorer (IXPE) have made it possible to investigate the intricate details of these objects in a new way, thanks to the added value of X-ray polarimetry. Here we present the results of the IXPE observations of SMC X-1, a member of the small group of XRPs displaying super-orbital variability. SMC X-1 was observed by IXPE three separate times during the high state of its super-orbital period. The observed luminosity in the 2-8 keV energy band of $L=2\\times10^{38}$ erg/s makes SMC X-1 the brightest XRP ever observed by IXPE. We detect significant polarization in all three observations, with values of the phase-averaged polarization degree (PD) and polarization angle (PA) of $3.2\\pm0.8$% and $97\\deg\\pm8\\deg$ for Observation 1, $3.0\\pm0.9$% and $90\\deg\\pm8\\deg$ for Observation 2, and $5.5\\pm1.1$% and $80\\deg\\pm6\\deg$ for Observation 3, for the spectro-polarimetric analysis. The observed PD shows an increase over time with decreasing luminosity, while the PA decreases in decrements of 10\\deg. The phase-resolved spectro-polarimetric analysis reveals significant detection of polarization in three out of seven phase bins, with the PD ranging between 2% and 10%, and a corresponding range in the PA from $\\sim$70\\deg\\ to $\\sim$100\\deg. The pulse-phase resolved PD displays an apparent anti-correlation with the flux. Using the rotating vector model, we obtain constraints on the pulsar's geometrical properties for the individual observations. The position angle of the pulsar displays an evolution over time supporting the idea that we observe changes related to different super-orbital phases. Scattering in the wind of the precessing accretion disk may be responsible for the behavior of the polarimetric properties observed during the high-state of SMC X-1's super-orbital period.","sentences":["Recent observations of X-ray pulsars (XRPs) performed by the Imaging X-ray Polarimetry Explorer (IXPE) have made it possible to investigate the intricate details of these objects in a new way, thanks to the added value of X-ray polarimetry.","Here we present the results of the IXPE observations of SMC X-1, a member of the small group of XRPs displaying super-orbital variability.","SMC X-1 was observed by IXPE three separate times during the high state of its super-orbital period.","The observed luminosity in the 2-8 keV energy band of $L=2\\times10^{38}$ erg/s makes SMC X-1 the brightest XRP ever observed by IXPE.","We detect significant polarization in all three observations, with values of the phase-averaged polarization degree (PD) and polarization angle (PA) of $3.2\\pm0.8$% and $97\\deg\\pm8\\deg$ for Observation 1, $3.0\\pm0.9$% and $90\\deg\\pm8\\deg$ for Observation 2, and $5.5\\pm1.1$% and $80\\deg\\pm6\\deg$ for Observation 3, for the spectro-polarimetric analysis.","The observed PD shows an increase over time with decreasing luminosity, while the PA decreases in decrements of 10\\deg.","The phase-resolved spectro-polarimetric analysis reveals significant detection of polarization in three out of seven phase bins, with the PD ranging between 2% and 10%, and a corresponding range in the PA from $\\sim$70\\deg\\ to $\\sim$100\\deg.","The pulse-phase resolved PD displays an apparent anti-correlation with the flux.","Using the rotating vector model, we obtain constraints on the pulsar's geometrical properties for the individual observations.","The position angle of the pulsar displays an evolution over time supporting the idea that we observe changes related to different super-orbital phases.","Scattering in the wind of the precessing accretion disk may be responsible for the behavior of the polarimetric properties observed during the high-state of SMC X-1's super-orbital period."],"url":"http://arxiv.org/abs/2406.08988v1","category":"astro-ph.HE"}
{"created":"2024-06-13 10:32:01","title":"The Behavior of Tree-Width and Path-Width under Graph Operations and Graph Transformations","abstract":"Tree-width and path-width are well-known graph parameters. Many NP-hard graph problems allow polynomial-time solutions, when restricted to graphs of bounded tree-width or bounded path-width. In this work, we study the behavior of tree-width and path-width under various unary and binary graph transformations. Doing so, for considered transformations we provide upper and lower bounds for the tree-width and path-width of the resulting graph in terms of the tree-width and path-width of the initial graphs or argue why such bounds are impossible to specify. Among the studied, unary transformations are vertex addition, vertex deletion, edge addition, edge deletion, subgraphs, vertex identification, edge contraction, edge subdivision, minors, powers of graphs, line graphs, edge complements, local complements, Seidel switching, and Seidel complementation. Among the studied, binary transformations we consider the disjoint union, join, union, substitution, graph product, 1-sum, and corona of two graphs.","sentences":["Tree-width and path-width are well-known graph parameters.","Many NP-hard graph problems allow polynomial-time solutions, when restricted to graphs of bounded tree-width or bounded path-width.","In this work, we study the behavior of tree-width and path-width under various unary and binary graph transformations.","Doing so, for considered transformations we provide upper and lower bounds for the tree-width and path-width of the resulting graph in terms of the tree-width and path-width of the initial graphs or argue why such bounds are impossible to specify.","Among the studied, unary transformations are vertex addition, vertex deletion, edge addition, edge deletion, subgraphs, vertex identification, edge contraction, edge subdivision, minors, powers of graphs, line graphs, edge complements, local complements, Seidel switching, and Seidel complementation.","Among the studied, binary transformations we consider the disjoint union, join, union, substitution, graph product, 1-sum, and corona of two graphs."],"url":"http://arxiv.org/abs/2406.08985v1","category":"cs.DS"}
{"created":"2024-06-13 10:28:37","title":"Effective removal of global tilt from topography images of vicinal surfaces with narrow terraces","abstract":"The main feature of vicinal surfaces of crystals characterized by the Miller indices $(h\\,h\\,m)$, is rather small width (less than 10 nm) and substantially large length (more than 200 nm) of atomically-flat terraces on sample surface. This makes difficult to apply standard methods of image processing and correct visualization of crystalline lattice at the terraces and multiatomic steps. Here we consider two procedures allowing us to minimize effects of both small-scale noise and global tilt of sample: (i) analysis of the difference of two Gaussian blurred images, and (ii) subtraction of the plane, whose parameters are determined by optimization of the histogram of the visible heights, from raw topography image. It is shown that both methods provide non-distorted images demonstrating atomic structures on vicinal ${\\rm Si}(5\\,5\\,6)$ and ${\\rm Si}(5\\,5\\,7)$ surfaces.","sentences":["The main feature of vicinal surfaces of crystals characterized by the Miller indices $(h\\,h\\,m)$, is rather small width (less than 10 nm) and substantially large length (more than 200 nm) of atomically-flat terraces on sample surface.","This makes difficult to apply standard methods of image processing and correct visualization of crystalline lattice at the terraces and multiatomic steps.","Here we consider two procedures allowing us to minimize effects of both small-scale noise and global tilt of sample: (i) analysis of the difference of two Gaussian blurred images, and (ii) subtraction of the plane, whose parameters are determined by optimization of the histogram of the visible heights, from raw topography image.","It is shown that both methods provide non-distorted images demonstrating atomic structures on vicinal ${\\rm Si}(5\\,5\\,6)$ and ${\\rm Si}(5\\,5\\,7)$ surfaces."],"url":"http://arxiv.org/abs/2406.08984v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-13 10:06:35","title":"Cascaded noise reduction and acoustic echo cancellation based on an extended noise reduction","abstract":"In many speech recording applications, the recorded desired speech is corrupted by both noise and acoustic echo, such that combined noise reduction (NR) and acoustic echo cancellation (AEC) is called for. A common cascaded design corresponds to NR filters preceding AEC filters. These NR filters aim at reducing the near-end room noise (and possibly partially the echo) and operate on the microphones only, consequently requiring the AEC filters to model both the echo paths and the NR filters. In this paper, however, we propose a design with extended NR (NRext) filters preceding AEC filters under the assumption of the echo paths being additive maps, thus preserving the addition operation. Here, the NRext filters aim at reducing both the near-end room noise and the far-end room noise component in the echo, and operate on both the microphones and loudspeakers. We show that the succeeding AEC filters remarkably become independent of the NRext filters, such that the AEC filters are only required to model the echo paths, improving the AEC performance. Further, the degrees of freedom in the NRext filters scale with the number of loudspeakers, which is not the case for the NR filters, resulting in an improved NR performance.","sentences":["In many speech recording applications, the recorded desired speech is corrupted by both noise and acoustic echo, such that combined noise reduction (NR) and acoustic echo cancellation (AEC) is called for.","A common cascaded design corresponds to NR filters preceding AEC filters.","These NR filters aim at reducing the near-end room noise (and possibly partially the echo) and operate on the microphones only, consequently requiring the AEC filters to model both the echo paths and the NR filters.","In this paper, however, we propose a design with extended NR (NRext) filters preceding AEC filters under the assumption of the echo paths being additive maps, thus preserving the addition operation.","Here, the NRext filters aim at reducing both the near-end room noise and the far-end room noise component in the echo, and operate on both the microphones and loudspeakers.","We show that the succeeding AEC filters remarkably become independent of the NRext filters, such that the AEC filters are only required to model the echo paths, improving the AEC performance.","Further, the degrees of freedom in the NRext filters scale with the number of loudspeakers, which is not the case for the NR filters, resulting in an improved NR performance."],"url":"http://arxiv.org/abs/2406.08974v1","category":"eess.AS"}
{"created":"2024-06-13 09:33:53","title":"S-SOS: Stochastic Sum-Of-Squares for Parametric Polynomial Optimization","abstract":"Global polynomial optimization is an important tool across applied mathematics, with many applications in operations research, engineering, and physical sciences. In various settings, the polynomials depend on external parameters that may be random. We discuss a stochastic sum-of-squares (S-SOS) algorithm based on the sum-of squares hierarchy that constructs a series of semidefinite programs to jointly find strict lower bounds on the global minimum and extract candidates for parameterized global minimizers. We prove quantitative convergence of the hierarchy as the degree increases and use it to solve unconstrained and constrained polynomial optimization problems parameterized by random variables. By employing $n$-body priors from condensed matter physics to induce sparsity, we can use S-SOS to produce solutions and uncertainty intervals for sensor network localization problems containing up to 40 variables and semidefinite matrix sizes surpassing $800 \\times 800$.","sentences":["Global polynomial optimization is an important tool across applied mathematics, with many applications in operations research, engineering, and physical sciences.","In various settings, the polynomials depend on external parameters that may be random.","We discuss a stochastic sum-of-squares (S-SOS) algorithm based on the sum-of squares hierarchy that constructs a series of semidefinite programs to jointly find strict lower bounds on the global minimum and extract candidates for parameterized global minimizers.","We prove quantitative convergence of the hierarchy as the degree increases and use it to solve unconstrained and constrained polynomial optimization problems parameterized by random variables.","By employing $n$-body priors from condensed matter physics to induce sparsity, we can use S-SOS to produce solutions and uncertainty intervals for sensor network localization problems containing up to 40 variables and semidefinite matrix sizes surpassing $800 \\times 800$."],"url":"http://arxiv.org/abs/2406.08954v1","category":"math.OC"}
{"created":"2024-06-13 09:32:40","title":"Preserving Identity with Variational Score for General-purpose 3D Editing","abstract":"We present Piva (Preserving Identity with Variational Score Distillation), a novel optimization-based method for editing images and 3D models based on diffusion models. Specifically, our approach is inspired by the recently proposed method for 2D image editing - Delta Denoising Score (DDS). We pinpoint the limitations in DDS for 2D and 3D editing, which causes detail loss and over-saturation. To address this, we propose an additional score distillation term that enforces identity preservation. This results in a more stable editing process, gradually optimizing NeRF models to match target prompts while retaining crucial input characteristics. We demonstrate the effectiveness of our approach in zero-shot image and neural field editing. Our method successfully alters visual attributes, adds both subtle and substantial structural elements, translates shapes, and achieves competitive results on standard 2D and 3D editing benchmarks. Additionally, our method imposes no constraints like masking or pre-training, making it compatible with a wide range of pre-trained diffusion models. This allows for versatile editing without needing neural field-to-mesh conversion, offering a more user-friendly experience.","sentences":["We present Piva (Preserving Identity with Variational Score Distillation), a novel optimization-based method for editing images and 3D models based on diffusion models.","Specifically, our approach is inspired by the recently proposed method for 2D image editing - Delta Denoising Score (DDS).","We pinpoint the limitations in DDS for 2D and 3D editing, which causes detail loss and over-saturation.","To address this, we propose an additional score distillation term that enforces identity preservation.","This results in a more stable editing process, gradually optimizing NeRF models to match target prompts while retaining crucial input characteristics.","We demonstrate the effectiveness of our approach in zero-shot image and neural field editing.","Our method successfully alters visual attributes, adds both subtle and substantial structural elements, translates shapes, and achieves competitive results on standard 2D and 3D editing benchmarks.","Additionally, our method imposes no constraints like masking or pre-training, making it compatible with a wide range of pre-trained diffusion models.","This allows for versatile editing without needing neural field-to-mesh conversion, offering a more user-friendly experience."],"url":"http://arxiv.org/abs/2406.08953v1","category":"cs.CV"}
{"created":"2024-06-13 09:27:26","title":"Magnetic reconnection, plasmoids and numerical resolution","abstract":"Explaining fast magnetic reconnection in electrically conducting plasmas has been a theoretical challenge in plasma physics since its first description by Eugene N. Parker. In the recent years the observed reconnection rate has been shown by numerical simulations to be explained by the plasmoid instability that appears in highly conductive plasmas. In this work we show that the plasmoid instability is very sensitive to the numerical resolution used. It is shown that well resolved runs display no plasmoid instability even at Lundquist number as large as $5\\cdot10^5$ achieved at resolutions of $32\\,768^2$ grid points. On the contrary in simulations that are under-resolved below a threshold, the plasmoid instability manifests itself with the formation of larger plasmoids the larger the under-resolving is. The present results thus question the description of the plasmoid instability as a mechanism for fast magnetic reconnection.","sentences":["Explaining fast magnetic reconnection in electrically conducting plasmas has been a theoretical challenge in plasma physics since its first description by Eugene N. Parker.","In the recent years the observed reconnection rate has been shown by numerical simulations to be explained by the plasmoid instability that appears in highly conductive plasmas.","In this work we show that the plasmoid instability is very sensitive to the numerical resolution used.","It is shown that well resolved runs display no plasmoid instability even at Lundquist number as large as $5\\cdot10^5$ achieved at resolutions of $32\\,768^2$ grid points.","On the contrary in simulations that are under-resolved below a threshold, the plasmoid instability manifests itself with the formation of larger plasmoids the larger the under-resolving is.","The present results thus question the description of the plasmoid instability as a mechanism for fast magnetic reconnection."],"url":"http://arxiv.org/abs/2406.08951v1","category":"physics.plasm-ph"}
{"created":"2024-06-13 09:12:08","title":"Quotient-convergence of Submodular Setfunctions","abstract":"We introduce the concept of quotient-convergence for sequences of submodular set functions, providing, among others, a new framework for the study of convergence of matroids through their rank functions. Extending the limit theory of bounded degree graphs, which analyzes graph sequences via neighborhood sampling, we address the challenge posed by the absence of a neighborhood concept in matroids. We show that any bounded set function can be approximated by a sequence of finite set functions that quotient-converges to it. In addition, we explicitly construct such sequences for increasing, submodular, and upper continuous set functions, and prove the completeness of the space under quotient-convergence.","sentences":["We introduce the concept of quotient-convergence for sequences of submodular set functions, providing, among others, a new framework for the study of convergence of matroids through their rank functions.","Extending the limit theory of bounded degree graphs, which analyzes graph sequences via neighborhood sampling, we address the challenge posed by the absence of a neighborhood concept in matroids.","We show that any bounded set function can be approximated by a sequence of finite set functions that quotient-converges to it.","In addition, we explicitly construct such sequences for increasing, submodular, and upper continuous set functions, and prove the completeness of the space under quotient-convergence."],"url":"http://arxiv.org/abs/2406.08942v1","category":"math.CO"}
{"created":"2024-06-13 09:10:16","title":"Word Order in English-Japanese Simultaneous Interpretation: Analyses and Evaluation using Chunk-wise Monotonic Translation","abstract":"This paper analyzes the features of monotonic translations, which follow the word order of the source language, in simultaneous interpreting (SI). The word order differences are one of the biggest challenges in SI, especially for language pairs with significant structural differences like English and Japanese. We analyzed the characteristics of monotonic translations using the NAIST English-to-Japanese Chunk-wise Monotonic Translation Evaluation Dataset and found some grammatical structures that make monotonic translation difficult in English-Japanese SI. We further investigated the features of monotonic translations through evaluating the output from the existing speech translation (ST) and simultaneous speech translation (simulST) models on NAIST English-to-Japanese Chunk-wise Monotonic Translation Evaluation Dataset as well as on existing test sets. The results suggest that the existing SI-based test set underestimates the model performance. We also found that the monotonic-translation-based dataset would better evaluate simulST models, while using an offline-based test set for evaluating simulST models underestimates the model performance.","sentences":["This paper analyzes the features of monotonic translations, which follow the word order of the source language, in simultaneous interpreting (SI).","The word order differences are one of the biggest challenges in SI, especially for language pairs with significant structural differences like English and Japanese.","We analyzed the characteristics of monotonic translations using the NAIST English-to-Japanese Chunk-wise Monotonic Translation Evaluation Dataset and found some grammatical structures that make monotonic translation difficult in English-Japanese SI.","We further investigated the features of monotonic translations through evaluating the output from the existing speech translation (ST) and simultaneous speech translation (simulST) models on NAIST English-to-Japanese Chunk-wise Monotonic Translation Evaluation Dataset as well as on existing test sets.","The results suggest that the existing SI-based test set underestimates the model performance.","We also found that the monotonic-translation-based dataset would better evaluate simulST models, while using an offline-based test set for evaluating simulST models underestimates the model performance."],"url":"http://arxiv.org/abs/2406.08940v1","category":"cs.CL"}
{"created":"2024-06-13 09:08:43","title":"Generation of cyclotomic Hecke fields by $L$-values of cusp forms on $\\mathrm{GL}(2)$ with certain $\\mathbb{Z}_p$ twist","abstract":"Let $F$ be a number field, $f$ an algebraic automorphic newform on $\\mathrm{GL}(2)$ over $F$, $p$ an odd prime does not divide the class number of $F$ and the level of $f$. We prove that $f$ is determined by its $L$-values twisted by Galois characters $\\phi$ of certain $\\mathbb{Z}_p$-extension of $F$. Furthermore, if $F$ is totally real or CM, then under some mild assumption on $f$, the compositum of the Hecke field of $f$ and the cyclotomic field $\\mathbb{Q}(\\phi)$ is generated by the algebraic $L$-values of $f$ twisted by Galois characters $\\phi$ of certain $\\mathbb{Z}_p$-extension of $F$.","sentences":["Let $F$ be a number field, $f$ an algebraic automorphic newform on $\\mathrm{GL}(2)$ over $F$, $p$ an odd prime does not divide the class number of $F$ and the level of $f$. We prove that $f$ is determined by its $L$-values twisted by Galois characters $\\phi$ of certain $\\mathbb{Z}_p$-extension of $F$.","Furthermore, if $F$ is totally real or CM, then under some mild assumption on $f$, the compositum of the Hecke field of $f$ and the cyclotomic field $\\mathbb{Q}(\\phi)$ is generated by the algebraic $L$-values of $f$ twisted by Galois characters $\\phi$ of certain $\\mathbb{Z}_p$-extension of $F$."],"url":"http://arxiv.org/abs/2406.08939v1","category":"math.NT"}
{"created":"2024-06-13 09:07:22","title":"Mirror and Preconditioned Gradient Descent in Wasserstein Space","abstract":"As the problem of minimizing functionals on the Wasserstein space encompasses many applications in machine learning, different optimization algorithms on $\\mathbb{R}^d$ have received their counterpart analog on the Wasserstein space. We focus here on lifting two explicit algorithms: mirror descent and preconditioned gradient descent. These algorithms have been introduced to better capture the geometry of the function to minimize and are provably convergent under appropriate (namely relative) smoothness and convexity conditions. Adapting these notions to the Wasserstein space, we prove guarantees of convergence of some Wasserstein-gradient-based discrete-time schemes for new pairings of objective functionals and regularizers. The difficulty here is to carefully select along which curves the functionals should be smooth and convex. We illustrate the advantages of adapting the geometry induced by the regularizer on ill-conditioned optimization tasks, and showcase the improvement of choosing different discrepancies and geometries in a computational biology task of aligning single-cells.","sentences":["As the problem of minimizing functionals on the Wasserstein space encompasses many applications in machine learning, different optimization algorithms on $\\mathbb{R}^d$ have received their counterpart analog on the Wasserstein space.","We focus here on lifting two explicit algorithms: mirror descent and preconditioned gradient descent.","These algorithms have been introduced to better capture the geometry of the function to minimize and are provably convergent under appropriate (namely relative) smoothness and convexity conditions.","Adapting these notions to the Wasserstein space, we prove guarantees of convergence of some Wasserstein-gradient-based discrete-time schemes for new pairings of objective functionals and regularizers.","The difficulty here is to carefully select along which curves the functionals should be smooth and convex.","We illustrate the advantages of adapting the geometry induced by the regularizer on ill-conditioned optimization tasks, and showcase the improvement of choosing different discrepancies and geometries in a computational biology task of aligning single-cells."],"url":"http://arxiv.org/abs/2406.08938v1","category":"math.OC"}
{"created":"2024-06-13 09:06:41","title":"Mechanism Design by a Politician","abstract":"A set of agents has to make a decision about the provision of a public good and its financing. Agents have heterogeneous values for the public good and each agent's value is private information. An agenda-setter has the right to make a proposal about a public-good level and a vector of contributions. For the proposal to be approved, only the favourable votes of a subset of agents are needed. If the proposal is not approved, a type-dependent outside option is implemented. I characterize the optimal public-good provision and the coalition-formation for any outside option in dominant strategies. Optimal public-good provision might be a non-monotonic function of the outside option public-good level. Moreover, the optimal coalition might be a non-convex set of types.","sentences":["A set of agents has to make a decision about the provision of a public good and its financing.","Agents have heterogeneous values for the public good and each agent's value is private information.","An agenda-setter has the right to make a proposal about a public-good level and a vector of contributions.","For the proposal to be approved, only the favourable votes of a subset of agents are needed.","If the proposal is not approved, a type-dependent outside option is implemented.","I characterize the optimal public-good provision and the coalition-formation for any outside option in dominant strategies.","Optimal public-good provision might be a non-monotonic function of the outside option public-good level.","Moreover, the optimal coalition might be a non-convex set of types."],"url":"http://arxiv.org/abs/2406.08936v1","category":"econ.TH"}
{"created":"2024-06-13 08:50:03","title":"Effective affinity for generic currents in nonequilibrium processes","abstract":"In mesoscopic experiments it is common to observe a single fluctuating current, such as the position of a molecular motor, while the complete set of currents is inaccessible. For such scenarios with partial information we introduce an effective affinity for generic currents in Markov processes. The effective affinity quantifies dissipative and fluctuation properties of fluctuating currents. Notably, the effective affinity multiplied by the current lower bounds the rate of dissipation, and the effective affinity determines first-passage and extreme value statistics of fluctuating currents. In addition, we determine the conditions under which the effective affinity has a stalling force interpretation. To derive these results we introduce a family of martingales associated with generic currents.","sentences":["In mesoscopic experiments it is common to observe a single fluctuating current, such as the position of a molecular motor, while the complete set of currents is inaccessible.","For such scenarios with partial information we introduce an effective affinity for generic currents in Markov processes.","The effective affinity quantifies dissipative and fluctuation properties of fluctuating currents.","Notably, the effective affinity multiplied by the current lower bounds the rate of dissipation, and the effective affinity determines first-passage and extreme value statistics of fluctuating currents.","In addition, we determine the conditions under which the effective affinity has a stalling force interpretation.","To derive these results we introduce a family of martingales associated with generic currents."],"url":"http://arxiv.org/abs/2406.08926v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-13 08:45:17","title":"The higher algebra of weighted colimits","abstract":"We develop a theory of weighted colimits in the framework of weakly bi-enriched $\\infty$-categories, an extension of Lurie's notion of enriched $\\infty$-categories. We prove an existence result for weighted colimits, study weighted colimits of diagrams of enriched functors, express weighted colimits via enriched coends, characterize the enriched $\\infty$-category of enriched presheaves as the free cocompletion under weighted colimits and develop a theory of universally adjoining weighted colimits to an enriched $\\infty$-category. We use the latter technique to construct for every presentably $\\mathbb{E}_{k+1}$-monoidal $\\infty$-category $\\mathcal{V}$ for $1 \\leq k \\leq \\infty$ and class $\\mathcal{H}$ of $\\mathcal{V}$-weights, with respect to which weighted colimits are defined, a presentably $\\mathbb{E}_k$-monoidal structure on the $\\infty$-category of $\\mathcal{V}$-enriched $\\infty$-categories that admit $\\mathcal{H}$-weighted colimits. Varying $\\mathcal{H}$ this $\\mathbb{E}_k$-monoidal structure interpolates between the tensor product for $\\mathcal{V}$-enriched $\\infty$-categories and the relative tensor product for $\\infty$-categories presentably left tensored over $\\mathcal{V}$. As an application we prove that forming $\\mathcal{V}$-enriched presheaves is $\\mathbb{E}_k$-monoidal, construct a $\\mathcal{V}$-enriched version of Day-convolution and give a new construction of the tensor product for $\\infty$-categories presentably left tensored over $\\mathcal{V}$ as a $\\mathcal{V}$-enriched localization of Day-convolution. As further applications we construct a tensor product for Cauchy-complete $\\mathcal{V}$-enriched $\\infty$-categories, a tensor product for $(\\infty,2)$-categories with (op)lax colimits and a tensor product for stable $(\\infty,n)$-categories.","sentences":["We develop a theory of weighted colimits in the framework of weakly bi-enriched $\\infty$-categories, an extension of Lurie's notion of enriched $\\infty$-categories.","We prove an existence result for weighted colimits, study weighted colimits of diagrams of enriched functors, express weighted colimits via enriched coends, characterize the enriched $\\infty$-category of enriched presheaves as the free cocompletion under weighted colimits and develop a theory of universally adjoining weighted colimits to an enriched $\\infty$-category.","We use the latter technique to construct for every presentably $\\mathbb{E}_{k+1}$-monoidal $\\infty$-category $\\mathcal{V}$ for $1 \\leq k \\leq \\infty$ and class $\\mathcal{H}$ of $\\mathcal{V}$-weights, with respect to which weighted colimits are defined, a presentably $\\mathbb{E}_k$-monoidal structure on the $\\infty$-category of $\\mathcal{V}$-enriched $\\infty$-categories that admit $\\mathcal{H}$-weighted colimits.","Varying $\\mathcal{H}$ this $\\mathbb{E}_k$-monoidal structure interpolates between the tensor product for $\\mathcal{V}$-enriched $\\infty$-categories and the relative tensor product for $\\infty$-categories presentably left tensored over $\\mathcal{V}$. As an application we prove that forming $\\mathcal{V}$-enriched presheaves is $\\mathbb{E}_k$-monoidal, construct a $\\mathcal{V}$-enriched version of Day-convolution and give a new construction of the tensor product for $\\infty$-categories presentably left tensored over $\\mathcal{V}$ as a $\\mathcal{V}$-enriched localization of Day-convolution.","As further applications we construct a tensor product for Cauchy-complete $\\mathcal{V}$-enriched $\\infty$-categories, a tensor product for $(\\infty,2)$-categories with (op)lax colimits and a tensor product for stable $(\\infty,n)$-categories."],"url":"http://arxiv.org/abs/2406.08925v1","category":"math.CT"}
{"created":"2024-06-13 07:58:15","title":"AdaPTwin: Low-Cost Adaptive Compression of Product Twins in Transformers","abstract":"While large transformer-based models have exhibited remarkable performance in speaker-independent speech recognition, their large size and computational requirements make them expensive or impractical to use in resource-constrained settings. In this work, we propose a low-rank adaptive compression technique called AdaPTwin that jointly compresses product-dependent pairs of weight matrices in the transformer attention layer. Our approach can prioritize the compressed model's performance on a specific speaker while maintaining generalizability to new speakers and acoustic conditions. Notably, our technique requires only 8 hours of speech data for fine-tuning, which can be accomplished in under 20 minutes, making it highly cost-effective compared to other compression methods. We demonstrate the efficacy of our approach by compressing the Whisper and Distil-Whisper models by up to 45% while incurring less than a 2% increase in word error rate.","sentences":["While large transformer-based models have exhibited remarkable performance in speaker-independent speech recognition, their large size and computational requirements make them expensive or impractical to use in resource-constrained settings.","In this work, we propose a low-rank adaptive compression technique called AdaPTwin that jointly compresses product-dependent pairs of weight matrices in the transformer attention layer.","Our approach can prioritize the compressed model's performance on a specific speaker while maintaining generalizability to new speakers and acoustic conditions.","Notably, our technique requires only 8 hours of speech data for fine-tuning, which can be accomplished in under 20 minutes, making it highly cost-effective compared to other compression methods.","We demonstrate the efficacy of our approach by compressing the Whisper and Distil-Whisper models by up to 45% while incurring less than a 2% increase in word error rate."],"url":"http://arxiv.org/abs/2406.08904v1","category":"cs.LG"}
{"created":"2024-06-13 07:44:44","title":"Minimaxity under the half-Cauchy prior","abstract":"This is a follow-up paper of Polson and Scott (2012, Bayesian Analysis), which claimed that the half-Cauchy prior is a sensible default prior for a scale parameter in hierarchical models. For estimation of a p-variate normal mean under the quadratic loss, they demonstrated that the Bayes estimator with respect to the half-Cauchy prior seems to be minimax through numerical experiments. In this paper, we theoretically establish the minimaxity of the corresponding Bayes estimator using the interval arithmetric.","sentences":["This is a follow-up paper of Polson and Scott (2012, Bayesian Analysis), which claimed that the half-Cauchy prior is a sensible default prior for a scale parameter in hierarchical models.","For estimation of a p-variate normal mean under the quadratic loss, they demonstrated that the Bayes estimator with respect to the half-Cauchy prior seems to be minimax through numerical experiments.","In this paper, we theoretically establish the minimaxity of the corresponding Bayes estimator using the interval arithmetric."],"url":"http://arxiv.org/abs/2406.08892v1","category":"math.ST"}
{"created":"2024-06-13 07:36:05","title":"Semigroups generated in Lp-spaces by some dispersal process including semi-permeability conditions at the interface","abstract":"We study an elliptic differential equation set in two habitats under semi-permeability conditions at the interface. This equation describes some dispersal process in population dynamics. Using functional calculus and results in Lutz Weis [22] among others, we show that the associated space operator generates an analytic semigroup in Lp-spaces.","sentences":["We study an elliptic differential equation set in two habitats under semi-permeability conditions at the interface.","This equation describes some dispersal process in population dynamics.","Using functional calculus and results in Lutz Weis","[22] among others, we show that the associated space operator generates an analytic semigroup in Lp-spaces."],"url":"http://arxiv.org/abs/2406.08883v1","category":"math.AP"}
{"created":"2024-06-13 07:35:37","title":"No perspective, no perception!! Perspective-aware Healthcare Answer Summarization","abstract":"Healthcare Community Question Answering (CQA) forums offer an accessible platform for individuals seeking information on various healthcare-related topics. People find such platforms suitable for self-disclosure, seeking medical opinions, finding simplified explanations for their medical conditions, and answering others' questions. However, answers on these forums are typically diverse and prone to off-topic discussions. It can be challenging for readers to sift through numerous answers and extract meaningful insights, making answer summarization a crucial task for CQA forums. While several efforts have been made to summarize the community answers, most of them are limited to the open domain and overlook the different perspectives offered by these answers. To address this problem, this paper proposes a novel task of perspective-specific answer summarization. We identify various perspectives, within healthcare-related responses and frame a perspective-driven abstractive summary covering all responses. To achieve this, we annotate 3167 CQA threads with 6193 perspective-aware summaries in our PUMA dataset. Further, we propose PLASMA, a prompt-driven controllable summarization model. To encapsulate the perspective-specific conditions, we design an energy-controlled loss function for the optimization. We also leverage the prefix tuner to learn the intricacies of the health-care perspective summarization. Our evaluation against five baselines suggests the superior performance of PLASMA by a margin of 1.5-21% improvement. We supplement our experiments with ablation and qualitative analysis.","sentences":["Healthcare Community Question Answering (CQA) forums offer an accessible platform for individuals seeking information on various healthcare-related topics.","People find such platforms suitable for self-disclosure, seeking medical opinions, finding simplified explanations for their medical conditions, and answering others' questions.","However, answers on these forums are typically diverse and prone to off-topic discussions.","It can be challenging for readers to sift through numerous answers and extract meaningful insights, making answer summarization a crucial task for CQA forums.","While several efforts have been made to summarize the community answers, most of them are limited to the open domain and overlook the different perspectives offered by these answers.","To address this problem, this paper proposes a novel task of perspective-specific answer summarization.","We identify various perspectives, within healthcare-related responses and frame a perspective-driven abstractive summary covering all responses.","To achieve this, we annotate 3167 CQA threads with 6193 perspective-aware summaries in our PUMA dataset.","Further, we propose PLASMA, a prompt-driven controllable summarization model.","To encapsulate the perspective-specific conditions, we design an energy-controlled loss function for the optimization.","We also leverage the prefix tuner to learn the intricacies of the health-care perspective summarization.","Our evaluation against five baselines suggests the superior performance of PLASMA by a margin of 1.5-21% improvement.","We supplement our experiments with ablation and qualitative analysis."],"url":"http://arxiv.org/abs/2406.08881v1","category":"cs.CL"}
{"created":"2024-06-13 07:11:51","title":"A Robust Bayesian approach for reliability prognosis of one-shot devices under cumulative risk model","abstract":"The reliability prognosis of one-shot devices is drawing increasing attention because of their wide applicability. The present study aims to determine the lifetime prognosis of highly durable one-shot device units under a step-stress accelerated life testing (SSALT) experiment applying a cumulative risk model (CRM). In an SSALT experiment, CRM retains the continuity of hazard function by allowing the lag period before the effects of stress change emerge. In an analysis of such lifetime data, plentiful datasets might have outliers where conventional methods like maximum likelihood estimation or likelihood-based Bayesian estimation frequently fail. This work develops a robust estimation method based on density power divergence in classical and Bayesian frameworks. The hypothesis is tested by implementing the Bayes factor based on a robustified posterior. In Bayesian estimation, we exploit Hamiltonian Monte Carlo, which has certain advantages over the conventional Metropolis-Hastings algorithms. Further, the influence functions are examined to evaluate the robust behaviour of the estimators and the Bayes factor. Finally, the analytical development is validated through a simulation study and a real data analysis.","sentences":["The reliability prognosis of one-shot devices is drawing increasing attention because of their wide applicability.","The present study aims to determine the lifetime prognosis of highly durable one-shot device units under a step-stress accelerated life testing (SSALT) experiment applying a cumulative risk model (CRM).","In an SSALT experiment, CRM retains the continuity of hazard function by allowing the lag period before the effects of stress change emerge.","In an analysis of such lifetime data, plentiful datasets might have outliers where conventional methods like maximum likelihood estimation or likelihood-based Bayesian estimation frequently fail.","This work develops a robust estimation method based on density power divergence in classical and Bayesian frameworks.","The hypothesis is tested by implementing the Bayes factor based on a robustified posterior.","In Bayesian estimation, we exploit Hamiltonian Monte Carlo, which has certain advantages over the conventional Metropolis-Hastings algorithms.","Further, the influence functions are examined to evaluate the robust behaviour of the estimators and the Bayes factor.","Finally, the analytical development is validated through a simulation study and a real data analysis."],"url":"http://arxiv.org/abs/2406.08867v1","category":"stat.ME"}
{"created":"2024-06-13 06:15:44","title":"Roping in Uncertainty: Robustness and Regularization in Markov Games","abstract":"We study robust Markov games (RMG) with $s$-rectangular uncertainty. We show a general equivalence between computing a robust Nash equilibrium (RNE) of a $s$-rectangular RMG and computing a Nash equilibrium (NE) of an appropriately constructed regularized MG. The equivalence result yields a planning algorithm for solving $s$-rectangular RMGs, as well as provable robustness guarantees for policies computed using regularized methods. However, we show that even for just reward-uncertain two-player zero-sum matrix games, computing an RNE is PPAD-hard. Consequently, we derive a special uncertainty structure called efficient player-decomposability and show that RNE for two-player zero-sum RMG in this class can be provably solved in polynomial time. This class includes commonly used uncertainty sets such as $L_1$ and $L_\\infty$ ball uncertainty sets.","sentences":["We study robust Markov games (RMG) with $s$-rectangular uncertainty.","We show a general equivalence between computing a robust Nash equilibrium (RNE) of a $s$-rectangular RMG and computing a Nash equilibrium (NE) of an appropriately constructed regularized MG.","The equivalence result yields a planning algorithm for solving $s$-rectangular RMGs, as well as provable robustness guarantees for policies computed using regularized methods.","However, we show that even for just reward-uncertain two-player zero-sum matrix games, computing an RNE is PPAD-hard.","Consequently, we derive a special uncertainty structure called efficient player-decomposability and show that RNE for two-player zero-sum RMG in this class can be provably solved in polynomial time.","This class includes commonly used uncertainty sets such as $L_1$ and $L_\\infty$ ball uncertainty sets."],"url":"http://arxiv.org/abs/2406.08847v1","category":"cs.GT"}
{"created":"2024-06-13 06:10:43","title":"Observation of Extraordinary Vibration Scatterings Induced by Strong Anharmonicity in Lead-Free Halide Double Perovskites","abstract":"Lead-free halide double perovskites provide a promising solution for the long-standing issues of lead-containing halide perovskites, i.e., the toxicity of Pb and the low stability under ambient conditions and high-intensity illumination. Their light-to-electricity or thermal-to-electricity conversion is strongly determined by the dynamics of the corresponding lattice vibrations. Here, we present the measurement of lattice dynamics in a prototypical lead-free halide double perovskite, i.e., Cs2NaInCl6. Our quantitative measurements and first-principles calculations show that the scatterings among lattice vibrations at room temperature are at the timescale of ~ 1 ps, which stems from the extraordinarily strong anharmonicity in Cs2NaInCl6. We further quantitatively characterize the degree of anharmonicity of all the ions in the single Cs2NaInCl6 crystal, and demonstrate that this strong anharmonicity is synergistically contributed by the bond hierarchy, the tilting of the NaCl6 and InCl6 octahedral units, and the rattling of Cs+ ions. Consequently, the crystalline Cs2NaInCl6 possesses an ultralow thermal conductivity of ~0.43 W/mK at room temperature, and a weak temperature dependence of T-0.41. Our findings here uncovered the underlying mechanisms behind the dynamics of lattice vibrations in double perovskites, which could largely benefit the design of optoelectronics and thermoelectrics based on halide double perovskites.","sentences":["Lead-free halide double perovskites provide a promising solution for the long-standing issues of lead-containing halide perovskites, i.e., the toxicity of Pb and the low stability under ambient conditions and high-intensity illumination.","Their light-to-electricity or thermal-to-electricity conversion is strongly determined by the dynamics of the corresponding lattice vibrations.","Here, we present the measurement of lattice dynamics in a prototypical lead-free halide double perovskite, i.e., Cs2NaInCl6.","Our quantitative measurements and first-principles calculations show that the scatterings among lattice vibrations at room temperature are at the timescale of ~ 1 ps, which stems from the extraordinarily strong anharmonicity in Cs2NaInCl6.","We further quantitatively characterize the degree of anharmonicity of all the ions in the single Cs2NaInCl6 crystal, and demonstrate that this strong anharmonicity is synergistically contributed by the bond hierarchy, the tilting of the NaCl6 and InCl6 octahedral units, and the rattling of Cs+ ions.","Consequently, the crystalline Cs2NaInCl6 possesses an ultralow thermal conductivity of ~0.43 W/mK at room temperature, and a weak temperature dependence of T-0.41.","Our findings here uncovered the underlying mechanisms behind the dynamics of lattice vibrations in double perovskites, which could largely benefit the design of optoelectronics and thermoelectrics based on halide double perovskites."],"url":"http://arxiv.org/abs/2406.08846v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-13 06:04:34","title":"Conceptual Learning via Embedding Approximations for Reinforcing Interpretability and Transparency","abstract":"Concept bottleneck models (CBMs) have emerged as critical tools in domains where interpretability is paramount. These models rely on predefined textual descriptions, referred to as concepts, to inform their decision-making process and offer more accurate reasoning. As a result, the selection of concepts used in the model is of utmost significance. This study proposes \\underline{\\textbf{C}}onceptual \\underline{\\textbf{L}}earning via \\underline{\\textbf{E}}mbedding \\underline{\\textbf{A}}pproximations for \\underline{\\textbf{R}}einforcing Interpretability and Transparency, abbreviated as CLEAR, a framework for constructing a CBM for image classification. Using score matching and Langevin sampling, we approximate the embedding of concepts within the latent space of a vision-language model (VLM) by learning the scores associated with the joint distribution of images and concepts. A concept selection process is then employed to optimize the similarity between the learned embeddings and the predefined ones. The derived bottleneck offers insights into the CBM's decision-making process, enabling more comprehensive interpretations. Our approach was evaluated through extensive experiments and achieved state-of-the-art performance on various benchmarks. The code for our experiments is available at https://github.com/clearProject/CLEAR/tree/main","sentences":["Concept bottleneck models (CBMs) have emerged as critical tools in domains where interpretability is paramount.","These models rely on predefined textual descriptions, referred to as concepts, to inform their decision-making process and offer more accurate reasoning.","As a result, the selection of concepts used in the model is of utmost significance.","This study proposes \\underline{\\textbf{C}}onceptual \\underline{\\textbf{L}}earning via \\underline{\\textbf{E}}mbedding \\underline{\\textbf{A}}pproximations for \\underline{\\textbf{R}}einforcing Interpretability and Transparency, abbreviated as CLEAR, a framework for constructing a CBM for image classification.","Using score matching and Langevin sampling, we approximate the embedding of concepts within the latent space of a vision-language model (VLM) by learning the scores associated with the joint distribution of images and concepts.","A concept selection process is then employed to optimize the similarity between the learned embeddings and the predefined ones.","The derived bottleneck offers insights into the CBM's decision-making process, enabling more comprehensive interpretations.","Our approach was evaluated through extensive experiments and achieved state-of-the-art performance on various benchmarks.","The code for our experiments is available at https://github.com/clearProject/CLEAR/tree/main"],"url":"http://arxiv.org/abs/2406.08840v1","category":"cs.CV"}
{"created":"2024-06-13 06:04:19","title":"NeRF Director: Revisiting View Selection in Neural Volume Rendering","abstract":"Neural Rendering representations have significantly contributed to the field of 3D computer vision. Given their potential, considerable efforts have been invested to improve their performance. Nonetheless, the essential question of selecting training views is yet to be thoroughly investigated. This key aspect plays a vital role in achieving high-quality results and aligns with the well-known tenet of deep learning: \"garbage in, garbage out\". In this paper, we first illustrate the importance of view selection by demonstrating how a simple rotation of the test views within the most pervasive NeRF dataset can lead to consequential shifts in the performance rankings of state-of-the-art techniques. To address this challenge, we introduce a unified framework for view selection methods and devise a thorough benchmark to assess its impact. Significant improvements can be achieved without leveraging error or uncertainty estimation but focusing on uniform view coverage of the reconstructed object, resulting in a training-free approach. Using this technique, we show that high-quality renderings can be achieved faster by using fewer views. We conduct extensive experiments on both synthetic datasets and realistic data to demonstrate the effectiveness of our proposed method compared with random, conventional error-based, and uncertainty-guided view selection.","sentences":["Neural Rendering representations have significantly contributed to the field of 3D computer vision.","Given their potential, considerable efforts have been invested to improve their performance.","Nonetheless, the essential question of selecting training views is yet to be thoroughly investigated.","This key aspect plays a vital role in achieving high-quality results and aligns with the well-known tenet of deep learning: \"garbage in, garbage out\".","In this paper, we first illustrate the importance of view selection by demonstrating how a simple rotation of the test views within the most pervasive NeRF dataset can lead to consequential shifts in the performance rankings of state-of-the-art techniques.","To address this challenge, we introduce a unified framework for view selection methods and devise a thorough benchmark to assess its impact.","Significant improvements can be achieved without leveraging error or uncertainty estimation but focusing on uniform view coverage of the reconstructed object, resulting in a training-free approach.","Using this technique, we show that high-quality renderings can be achieved faster by using fewer views.","We conduct extensive experiments on both synthetic datasets and realistic data to demonstrate the effectiveness of our proposed method compared with random, conventional error-based, and uncertainty-guided view selection."],"url":"http://arxiv.org/abs/2406.08839v1","category":"cs.CV"}
{"created":"2024-06-13 05:58:03","title":"The strong convergence of the trajectory of a Tikhonov regularized inertial primal-dual dynamical system with a slow damping","abstract":"We propose a Tikhonov regularized inertial primal-dual dynamical system with a slow damping $\\frac{\\alpha}{t^q}$, where the inertial term is introduced only for the primal variable, for the linearly constrained convex optimization problem in Hilbert spaces. Under a suitable assumption on the underlying parameters, by a Lyapunov analysis approach, we prove the strong convergence of the trajectory of the proposed system to the minimal norm primal-dual solution of the problem, along with convergence rate results for the primal-dual gap, the objective residual and the feasibility violation. In Section 4, , we perform some numerical experiments to illustrate the theoretical results. Finaly, we give a conclusion in Section 5.","sentences":["We propose a Tikhonov regularized inertial primal-dual dynamical system with a slow damping $\\frac{\\alpha}{t^q}$, where the inertial term is introduced only for the primal variable, for the linearly constrained convex optimization problem in Hilbert spaces.","Under a suitable assumption on the underlying parameters, by a Lyapunov analysis approach, we prove the strong convergence of the trajectory of the proposed system to the minimal norm primal-dual solution of the problem, along with convergence rate results for the primal-dual gap, the objective residual and the feasibility violation.","In Section 4, , we perform some numerical experiments to illustrate the theoretical results.","Finaly, we give a conclusion in Section 5."],"url":"http://arxiv.org/abs/2406.08836v1","category":"math.OC"}
{"created":"2024-06-13 05:36:01","title":"Interpretable Temporal Class Activation Representation for Audio Spoofing Detection","abstract":"Explaining the decisions made by audio spoofing detection models is crucial for fostering trust in detection outcomes. However, current research on the interpretability of detection models is limited to applying XAI tools to post-trained models. In this paper, we utilize the wav2vec 2.0 model and attentive utterance-level features to integrate interpretability directly into the model's architecture, thereby enhancing transparency of the decision-making process. Specifically, we propose a class activation representation to localize the discriminative frames contributing to detection. Furthermore, we demonstrate that multi-label training based on spoofing types, rather than binary labels as bonafide and spoofed, enables the model to learn distinct characteristics of different attacks, significantly improving detection performance. Our model achieves state-of-the-art results, with an EER of 0.51% and a min t-DCF of 0.0165 on the ASVspoof2019-LA set.","sentences":["Explaining the decisions made by audio spoofing detection models is crucial for fostering trust in detection outcomes.","However, current research on the interpretability of detection models is limited to applying XAI tools to post-trained models.","In this paper, we utilize the wav2vec 2.0 model and attentive utterance-level features to integrate interpretability directly into the model's architecture, thereby enhancing transparency of the decision-making process.","Specifically, we propose a class activation representation to localize the discriminative frames contributing to detection.","Furthermore, we demonstrate that multi-label training based on spoofing types, rather than binary labels as bonafide and spoofed, enables the model to learn distinct characteristics of different attacks, significantly improving detection performance.","Our model achieves state-of-the-art results, with an EER of 0.51% and a min t-DCF of 0.0165 on the ASVspoof2019-LA set."],"url":"http://arxiv.org/abs/2406.08825v1","category":"cs.SD"}
{"created":"2024-06-13 05:23:22","title":"DisfluencySpeech -- Single-Speaker Conversational Speech Dataset with Paralanguage","abstract":"Laughing, sighing, stuttering, and other forms of paralanguage do not contribute any direct lexical meaning to speech, but they provide crucial propositional context that aids semantic and pragmatic processes such as irony. It is thus important for artificial social agents to both understand and be able to generate speech with semantically-important paralanguage. Most speech datasets do not include transcribed non-lexical speech sounds and disfluencies, while those that do are typically multi-speaker datasets where each speaker provides relatively little audio. This makes it challenging to train conversational Text-to-Speech (TTS) synthesis models that include such paralinguistic components.   We thus present DisfluencySpeech, a studio-quality labeled English speech dataset with paralanguage. A single speaker recreates nearly 10 hours of expressive utterances from the Switchboard-1 Telephone Speech Corpus (Switchboard), simulating realistic informal conversations. To aid the development of a TTS model that is able to predictively synthesise paralanguage from text without such components, we provide three different transcripts at different levels of information removal (removal of non-speech events, removal of non-sentence elements, and removal of false starts), as well as benchmark TTS models trained on each of these levels.","sentences":["Laughing, sighing, stuttering, and other forms of paralanguage do not contribute any direct lexical meaning to speech, but they provide crucial propositional context that aids semantic and pragmatic processes such as irony.","It is thus important for artificial social agents to both understand and be able to generate speech with semantically-important paralanguage.","Most speech datasets do not include transcribed non-lexical speech sounds and disfluencies, while those that do are typically multi-speaker datasets where each speaker provides relatively little audio.","This makes it challenging to train conversational Text-to-Speech (TTS) synthesis models that include such paralinguistic components.   ","We thus present DisfluencySpeech, a studio-quality labeled English speech dataset with paralanguage.","A single speaker recreates nearly 10 hours of expressive utterances from the Switchboard-1 Telephone Speech Corpus (Switchboard), simulating realistic informal conversations.","To aid the development of a TTS model that is able to predictively synthesise paralanguage from text without such components, we provide three different transcripts at different levels of information removal (removal of non-speech events, removal of non-sentence elements, and removal of false starts), as well as benchmark TTS models trained on each of these levels."],"url":"http://arxiv.org/abs/2406.08820v1","category":"eess.AS"}
{"created":"2024-06-13 05:17:20","title":"Deep Reinforcement Learning-based Quadcopter Controller: A Practical Approach and Experiments","abstract":"Quadcopters have been studied for decades thanks to their maneuverability and capability of operating in a variety of circumstances. However, quadcopters suffer from dynamical nonlinearity, actuator saturation, as well as sensor noise that make it challenging and time consuming to obtain accurate dynamic models and achieve satisfactory control performance. Fortunately, deep reinforcement learning came and has shown significant potential in system modelling and control of autonomous multirotor aerial vehicles, with recent advancements in deployment, performance enhancement, and generalization. In this paper, an end-to-end deep reinforcement learning-based controller for quadcopters is proposed that is secure for real-world implementation, data-efficient, and free of human gain adjustments. First, a novel actor-critic-based architecture is designed to map the robot states directly to the motor outputs. Then, a quadcopter dynamics-based simulator was devised to facilitate the training of the controller policy. Finally, the trained policy is deployed on a real Crazyflie nano quadrotor platform, without any additional fine-tuning process. Experimental results show that the quadcopter exhibits satisfactory performance as it tracks a given complicated trajectory, which demonstrates the effectiveness and feasibility of the proposed method and signifies its capability in filling the simulation-to-reality gap.","sentences":["Quadcopters have been studied for decades thanks to their maneuverability and capability of operating in a variety of circumstances.","However, quadcopters suffer from dynamical nonlinearity, actuator saturation, as well as sensor noise that make it challenging and time consuming to obtain accurate dynamic models and achieve satisfactory control performance.","Fortunately, deep reinforcement learning came and has shown significant potential in system modelling and control of autonomous multirotor aerial vehicles, with recent advancements in deployment, performance enhancement, and generalization.","In this paper, an end-to-end deep reinforcement learning-based controller for quadcopters is proposed that is secure for real-world implementation, data-efficient, and free of human gain adjustments.","First, a novel actor-critic-based architecture is designed to map the robot states directly to the motor outputs.","Then, a quadcopter dynamics-based simulator was devised to facilitate the training of the controller policy.","Finally, the trained policy is deployed on a real Crazyflie nano quadrotor platform, without any additional fine-tuning process.","Experimental results show that the quadcopter exhibits satisfactory performance as it tracks a given complicated trajectory, which demonstrates the effectiveness and feasibility of the proposed method and signifies its capability in filling the simulation-to-reality gap."],"url":"http://arxiv.org/abs/2406.08815v1","category":"cs.RO"}
{"created":"2024-06-13 05:11:14","title":"Frozen boson stars in an infinite tower of higher-derivative gravity","abstract":"In this paper, we present a solution for a five-dimensional boson star under gravity with infinite tower of higher curvature corrections. We discover that when the coupling constant exceeds a certain threshold, an alternative configuration emerges, distinct from the conventional five-dimensional boson star. This new structure is characterized by a broader frequency range, with its minimum value approaching zero. At a truncation of $n=2$ for the correction order, the solution and its scalar curvature diverge as the frequency approaches zero. However, as the order of higher curvature corrections increases, the singularity at the center vanishes, resulting in a globally regular solution. Additionally, as the frequency approaches zero, the scalar field's radial distribution becomes concentrated within the critical radius $r_c$, forming what we term a ``frozen star\". Beyond this radius, the metric of the frozen star almost degenerates into that of an extreme black hole. The solutions for such frozen stars offer a new avenue for exploring the enigmatic interiors of compact celestial bodies, enhancing our understanding of the internal structure of black holes under semi-classical conditions and potentially addressing the series of paradoxes associated with information loss due to singularities and horizons.","sentences":["In this paper, we present a solution for a five-dimensional boson star under gravity with infinite tower of higher curvature corrections.","We discover that when the coupling constant exceeds a certain threshold, an alternative configuration emerges, distinct from the conventional five-dimensional boson star.","This new structure is characterized by a broader frequency range, with its minimum value approaching zero.","At a truncation of $n=2$ for the correction order, the solution and its scalar curvature diverge as the frequency approaches zero.","However, as the order of higher curvature corrections increases, the singularity at the center vanishes, resulting in a globally regular solution.","Additionally, as the frequency approaches zero, the scalar field's radial distribution becomes concentrated within the critical radius $r_c$, forming what we term a ``frozen star\".","Beyond this radius, the metric of the frozen star almost degenerates into that of an extreme black hole.","The solutions for such frozen stars offer a new avenue for exploring the enigmatic interiors of compact celestial bodies, enhancing our understanding of the internal structure of black holes under semi-classical conditions and potentially addressing the series of paradoxes associated with information loss due to singularities and horizons."],"url":"http://arxiv.org/abs/2406.08813v1","category":"gr-qc"}
{"created":"2024-06-13 05:01:28","title":"Mixture-of-Skills: Learning to Optimize Data Usage for Fine-Tuning Large Language Models","abstract":"Large language models (LLMs) are typically fine-tuned on diverse and extensive datasets sourced from various origins to develop a comprehensive range of skills, such as writing, reasoning, chatting, coding, and more. Each skill has unique characteristics, and these datasets are often heterogeneous and imbalanced, making the fine-tuning process highly challenging. Balancing the development of each skill while ensuring the model maintains its overall performance requires sophisticated techniques and careful dataset curation. In this work, we propose a general, model-agnostic, reinforcement learning framework, Mixture-of-Skills (MoS), that learns to optimize data usage automatically during the fine-tuning process. This framework ensures the optimal comprehensive skill development of LLMs by dynamically adjusting the focus on different datasets based on their current learning state. To validate the effectiveness of MoS, we conduct extensive experiments using three diverse LLM backbones on two widely used benchmarks and demonstrate that MoS substantially enhances model performance. Building on the success of MoS, we propose MoSpec, an adaptation for task-specific fine-tuning, which harnesses the utilities of various datasets for a specific purpose. Our work underlines the significance of dataset rebalancing and present MoS as a powerful, general solution for optimizing data usage in the fine-tuning of LLMs for various purposes.","sentences":["Large language models (LLMs) are typically fine-tuned on diverse and extensive datasets sourced from various origins to develop a comprehensive range of skills, such as writing, reasoning, chatting, coding, and more.","Each skill has unique characteristics, and these datasets are often heterogeneous and imbalanced, making the fine-tuning process highly challenging.","Balancing the development of each skill while ensuring the model maintains its overall performance requires sophisticated techniques and careful dataset curation.","In this work, we propose a general, model-agnostic, reinforcement learning framework, Mixture-of-Skills (MoS), that learns to optimize data usage automatically during the fine-tuning process.","This framework ensures the optimal comprehensive skill development of LLMs by dynamically adjusting the focus on different datasets based on their current learning state.","To validate the effectiveness of MoS, we conduct extensive experiments using three diverse LLM backbones on two widely used benchmarks and demonstrate that MoS substantially enhances model performance.","Building on the success of MoS, we propose MoSpec, an adaptation for task-specific fine-tuning, which harnesses the utilities of various datasets for a specific purpose.","Our work underlines the significance of dataset rebalancing and present MoS as a powerful, general solution for optimizing data usage in the fine-tuning of LLMs for various purposes."],"url":"http://arxiv.org/abs/2406.08811v1","category":"cs.CL"}
{"created":"2024-06-13 04:58:36","title":"Smoothed NPMLEs in nonparametric Poisson mixtures and beyond","abstract":"We discuss nonparametric mixing distribution estimation under the Gaussian-smoothed optimal transport (GOT) distance. It is shown that a recently formulated conjecture -- that the Poisson nonparametric maximum likelihood estimator can achieve root-$n$ rate of convergence under the GOT distance -- holds up to some logarithmic terms. We also establish the same conclusion for other minimum-distance estimators, and discuss mixture models beyond the Poisson.","sentences":["We discuss nonparametric mixing distribution estimation under the Gaussian-smoothed optimal transport (GOT) distance.","It is shown that a recently formulated conjecture -- that the Poisson nonparametric maximum likelihood estimator can achieve root-$n$ rate of convergence under the GOT distance -- holds up to some logarithmic terms.","We also establish the same conclusion for other minimum-distance estimators, and discuss mixture models beyond the Poisson."],"url":"http://arxiv.org/abs/2406.08808v1","category":"math.ST"}
{"created":"2024-06-13 04:38:43","title":"Asymptotic Birkhoff-Violation in Operational Theories: Thermodynamic Implications and Information Processing","abstract":"In accordance with the entropy principle of thermodynamics, under spontaneous evolutions, physical systems always evolve towards states with equal or greater randomness. But, where does this randomness originate? Renowned Birkhoff-von Neumann theorem, often referred to as Birkhoff theorem, identifies source of this randomness to be the stochastic application of reversible operations on the system under study, thereby ensuring its epistemic origin. Analogue of this theorem is known to fail in the quantum case. Here, we extend this investigation beyond quantum mechanics to a broader class of operational theories described within the framework of general probabilistic theories (GPTs). In this generalized framework, we establish Birkhoff-violation as the prevalent trait; in fact the asymptotic variant of the theorem gets violated. We then demonstrate that Birkhoff-violation in GPTs can lead to consequences that are atypical to quantum theory. For instance, we report manifestation of Birkhoff-violation in a communication task, which otherwise is not observed in quantum world. We also show that, unlike the quantum case, in other operational theories the state transformation criteria can be distinct under mixtures of reversible transformations and doubly stochastic evolutions, leading to different resource theories of purity. Despite these exotic implications, we analyze how to define a coherent notion of entropy in this generalized framework, while upholding alignment with von Neumann's thought experiment.","sentences":["In accordance with the entropy principle of thermodynamics, under spontaneous evolutions, physical systems always evolve towards states with equal or greater randomness.","But, where does this randomness originate?","Renowned Birkhoff-von Neumann theorem, often referred to as Birkhoff theorem, identifies source of this randomness to be the stochastic application of reversible operations on the system under study, thereby ensuring its epistemic origin.","Analogue of this theorem is known to fail in the quantum case.","Here, we extend this investigation beyond quantum mechanics to a broader class of operational theories described within the framework of general probabilistic theories (GPTs).","In this generalized framework, we establish Birkhoff-violation as the prevalent trait; in fact the asymptotic variant of the theorem gets violated.","We then demonstrate that Birkhoff-violation in GPTs can lead to consequences that are atypical to quantum theory.","For instance, we report manifestation of Birkhoff-violation in a communication task, which otherwise is not observed in quantum world.","We also show that, unlike the quantum case, in other operational theories the state transformation criteria can be distinct under mixtures of reversible transformations and doubly stochastic evolutions, leading to different resource theories of purity.","Despite these exotic implications, we analyze how to define a coherent notion of entropy in this generalized framework, while upholding alignment with von Neumann's thought experiment."],"url":"http://arxiv.org/abs/2406.08803v1","category":"quant-ph"}
{"created":"2024-06-13 03:57:45","title":"Topology of the charged AdS black hole in restricted phase space","abstract":"The local topological properties of black hole systems can be expressed by the winding numbers as the defects. As so far, AdS black hole thermodynamics is often depicted by the dual parameters of $(T,S),~ (P,V), (\\Phi, Q)$ in the extended phase space, while there is several study on the black hole thermodynamics in the restricted phase space. In this paper, we analyze the topological properties of the charged AdS black holes in the restricted phase space under the higher dimensions and higher order curvature gravities frame. The results show that the topological number of the charged black hole in the same canonical ensembles is a constant and is independent of the concrete dual thermodynamical parameters. However, the topological number in the grand canonical ensemble is different from that in the canonical ensemble for the same black hole system. Furthermore, these results are independent of the dimension $d$, the highest order $k$ of the Lanczos-Lovelock densities.","sentences":["The local topological properties of black hole systems can be expressed by the winding numbers as the defects.","As so far, AdS black hole thermodynamics is often depicted by the dual parameters of $(T,S),~ (P,V), (\\Phi, Q)$ in the extended phase space, while there is several study on the black hole thermodynamics in the restricted phase space.","In this paper, we analyze the topological properties of the charged AdS black holes in the restricted phase space under the higher dimensions and higher order curvature gravities frame.","The results show that the topological number of the charged black hole in the same canonical ensembles is a constant and is independent of the concrete dual thermodynamical parameters.","However, the topological number in the grand canonical ensemble is different from that in the canonical ensemble for the same black hole system.","Furthermore, these results are independent of the dimension $d$, the highest order $k$ of the Lanczos-Lovelock densities."],"url":"http://arxiv.org/abs/2406.08793v1","category":"hep-th"}
{"created":"2024-06-13 03:10:56","title":"Learning Joint and Individual Structure in Network Data with Covariates","abstract":"Datasets consisting of a network and covariates associated with its vertices have become ubiquitous. One problem pertaining to this type of data is to identify information unique to the network, information unique to the vertex covariates and information that is shared between the network and the vertex covariates. Existing techniques for network data and vertex covariates focus on capturing structure that is shared but are usually not able to differentiate structure that is unique to each dataset. This work formulates a low-rank model that simultaneously captures joint and individual information in network data with vertex covariates. A two-step estimation procedure is proposed, composed of an efficient spectral method followed by a refinement optimization step. Theoretically, we show that the spectral method is able to consistently recover the joint and individual components under a general signal-plus-noise model.   Simulations and real data examples demonstrate the ability of the methods to recover accurate and interpretable components. In particular, the application of the methodology to a food trade network between countries with economic, developmental and geographical country-level indicators as covariates yields joint and individual factors that explain the trading patterns.","sentences":["Datasets consisting of a network and covariates associated with its vertices have become ubiquitous.","One problem pertaining to this type of data is to identify information unique to the network, information unique to the vertex covariates and information that is shared between the network and the vertex covariates.","Existing techniques for network data and vertex covariates focus on capturing structure that is shared but are usually not able to differentiate structure that is unique to each dataset.","This work formulates a low-rank model that simultaneously captures joint and individual information in network data with vertex covariates.","A two-step estimation procedure is proposed, composed of an efficient spectral method followed by a refinement optimization step.","Theoretically, we show that the spectral method is able to consistently recover the joint and individual components under a general signal-plus-noise model.   ","Simulations and real data examples demonstrate the ability of the methods to recover accurate and interpretable components.","In particular, the application of the methodology to a food trade network between countries with economic, developmental and geographical country-level indicators as covariates yields joint and individual factors that explain the trading patterns."],"url":"http://arxiv.org/abs/2406.08776v1","category":"stat.ME"}
{"created":"2024-06-13 03:05:36","title":"DenoiseReID: Denoising Model for Representation Learning of Person Re-Identification","abstract":"In this paper, we propose a novel Denoising Model for Representation Learning and take Person Re-Identification (ReID) as a benchmark task, named DenoiseReID, to improve feature discriminative with joint feature extraction and denoising. In the deep learning epoch, backbones which consists of cascaded embedding layers (e.g. convolutions or transformers) to progressively extract useful features, becomes popular. We first view each embedding layer in a backbone as a denoising layer, processing the cascaded embedding layers as if we are recursively denoise features step-by-step. This unifies the frameworks of feature extraction and feature denoising, where the former progressively embeds features from low-level to high-level, and the latter recursively denoises features step-by-step. Then we design a novel Feature Extraction and Feature Denoising Fusion Algorithm (FEFDFA) and \\textit{theoretically demonstrate} its equivalence before and after fusion. FEFDFA merges parameters of the denoising layers into existing embedding layers, thus making feature denoising computation-free. This is a label-free algorithm to incrementally improve feature also complementary to the label if available. Besides, it enjoys two advantages: 1) it's a computation-free and label-free plugin for incrementally improving ReID features. 2) it is complementary to the label if the label is available. Experimental results on various tasks (large-scale image classification, fine-grained image classification, image retrieval) and backbones (transformers and convolutions) show the scalability and stability of our method. Experimental results on 4 ReID datasets and various of backbones show the stability and impressive improvements. We also extend the proposed method to large-scale (ImageNet) and fine-grained (e.g. CUB200) classification tasks, similar improvements are proven.","sentences":["In this paper, we propose a novel Denoising Model for Representation Learning and take Person Re-Identification (ReID) as a benchmark task, named DenoiseReID, to improve feature discriminative with joint feature extraction and denoising.","In the deep learning epoch, backbones which consists of cascaded embedding layers (e.g. convolutions or transformers) to progressively extract useful features, becomes popular.","We first view each embedding layer in a backbone as a denoising layer, processing the cascaded embedding layers as if we are recursively denoise features step-by-step.","This unifies the frameworks of feature extraction and feature denoising, where the former progressively embeds features from low-level to high-level, and the latter recursively denoises features step-by-step.","Then we design a novel Feature Extraction and Feature Denoising Fusion Algorithm (FEFDFA) and \\textit{theoretically demonstrate} its equivalence before and after fusion.","FEFDFA merges parameters of the denoising layers into existing embedding layers, thus making feature denoising computation-free.","This is a label-free algorithm to incrementally improve feature also complementary to the label if available.","Besides, it enjoys two advantages: 1) it's a computation-free and label-free plugin for incrementally improving ReID features.","2) it is complementary to the label if the label is available.","Experimental results on various tasks (large-scale image classification, fine-grained image classification, image retrieval) and backbones (transformers and convolutions) show the scalability and stability of our method.","Experimental results on 4 ReID datasets and various of backbones show the stability and impressive improvements.","We also extend the proposed method to large-scale (ImageNet) and fine-grained (e.g. CUB200) classification tasks, similar improvements are proven."],"url":"http://arxiv.org/abs/2406.08773v1","category":"cs.CV"}
{"created":"2024-06-13 03:05:36","title":"From an Integrated Usability Framework to Lessons on Usability and Performance of Open Government Data Portals: A Comparative Study of European Union and Gulf Cooperation Council Countries","abstract":"Open Government Data (OGD) initiatives aim to enhance public participation and collaboration by making government data accessible to diverse stakeholders, fostering social, environmental, and economic benefits through public value generation. However, challenges such as declining popularity, lack of OGD portal usability, and private interests overshadowing public accessibility persist. This study proposes an integrated usability framework for evaluating OGD portals, focusing on inclusivity, user collaboration, and data exploration. Employing Design Science Research (DSR), the framework is developed and applied to 33 OGD portals from the European Union (EU) and Gulf Cooperation Council (GCC) countries. The quantitative analysis is complemented by qualitative analysis and clustering, enabling assessment of portal performance, identification of best practices, and common weaknesses. This results in 19 high-level recommendations for improving the open data ecosystem. Key findings highlight the competitive nature of EU portals and the innovative features of GCC portals, emphasizing the need for multilingual support, better communication mechanisms, and improved dataset usability. The study stresses trends towards exposing data quality indicators and incorporating advanced functionalities such as AI systems. This framework serves as a baseline for OGD portal requirements elicitation, offering practical implications for developing sustainable, collaborative, and robust OGD portals, ultimately contributing to a more transparent and equitable world.","sentences":["Open Government Data (OGD) initiatives aim to enhance public participation and collaboration by making government data accessible to diverse stakeholders, fostering social, environmental, and economic benefits through public value generation.","However, challenges such as declining popularity, lack of OGD portal usability, and private interests overshadowing public accessibility persist.","This study proposes an integrated usability framework for evaluating OGD portals, focusing on inclusivity, user collaboration, and data exploration.","Employing Design Science Research (DSR), the framework is developed and applied to 33 OGD portals from the European Union (EU) and Gulf Cooperation Council (GCC) countries.","The quantitative analysis is complemented by qualitative analysis and clustering, enabling assessment of portal performance, identification of best practices, and common weaknesses.","This results in 19 high-level recommendations for improving the open data ecosystem.","Key findings highlight the competitive nature of EU portals and the innovative features of GCC portals, emphasizing the need for multilingual support, better communication mechanisms, and improved dataset usability.","The study stresses trends towards exposing data quality indicators and incorporating advanced functionalities such as AI systems.","This framework serves as a baseline for OGD portal requirements elicitation, offering practical implications for developing sustainable, collaborative, and robust OGD portals, ultimately contributing to a more transparent and equitable world."],"url":"http://arxiv.org/abs/2406.08774v1","category":"cs.HC"}
{"created":"2024-06-13 03:04:28","title":"MMFakeBench: A Mixed-Source Multimodal Misinformation Detection Benchmark for LVLMs","abstract":"Current multimodal misinformation detection (MMD) methods often assume a single source and type of forgery for each sample, which is insufficient for real-world scenarios where multiple forgery sources coexist. The lack of a benchmark for mixed-source misinformation has hindered progress in this field. To address this, we introduce MMFakeBench, the first comprehensive benchmark for mixed-source MMD. MMFakeBench includes 3 critical sources: textual veracity distortion, visual veracity distortion, and cross-modal consistency distortion, along with 12 sub-categories of misinformation forgery types. We further conduct an extensive evaluation of 6 prevalent detection methods and 15 large vision-language models (LVLMs) on MMFakeBench under a zero-shot setting. The results indicate that current methods struggle under this challenging and realistic mixed-source MMD setting. Additionally, we propose an innovative unified framework, which integrates rationales, actions, and tool-use capabilities of LVLM agents, significantly enhancing accuracy and generalization. We believe this study will catalyze future research into more realistic mixed-source multimodal misinformation and provide a fair evaluation of misinformation detection methods.","sentences":["Current multimodal misinformation detection (MMD) methods often assume a single source and type of forgery for each sample, which is insufficient for real-world scenarios where multiple forgery sources coexist.","The lack of a benchmark for mixed-source misinformation has hindered progress in this field.","To address this, we introduce MMFakeBench, the first comprehensive benchmark for mixed-source MMD.","MMFakeBench includes 3 critical sources: textual veracity distortion, visual veracity distortion, and cross-modal consistency distortion, along with 12 sub-categories of misinformation forgery types.","We further conduct an extensive evaluation of 6 prevalent detection methods and 15 large vision-language models (LVLMs) on MMFakeBench under a zero-shot setting.","The results indicate that current methods struggle under this challenging and realistic mixed-source MMD setting.","Additionally, we propose an innovative unified framework, which integrates rationales, actions, and tool-use capabilities of LVLM agents, significantly enhancing accuracy and generalization.","We believe this study will catalyze future research into more realistic mixed-source multimodal misinformation and provide a fair evaluation of misinformation detection methods."],"url":"http://arxiv.org/abs/2406.08772v1","category":"cs.CV"}
{"created":"2024-06-13 02:47:38","title":"LGB: Language Model and Graph Neural Network-Driven Social Bot Detection","abstract":"Malicious social bots achieve their malicious purposes by spreading misinformation and inciting social public opinion, seriously endangering social security, making their detection a critical concern. Recently, graph-based bot detection methods have achieved state-of-the-art (SOTA) performance. However, our research finds many isolated and poorly linked nodes in social networks, as shown in Fig.1, which graph-based methods cannot effectively detect. To address this problem, our research focuses on effectively utilizing node semantics and network structure to jointly detect sparsely linked nodes. Given the excellent performance of language models (LMs) in natural language understanding (NLU), we propose a novel social bot detection framework LGB, which consists of two main components: language model (LM) and graph neural network (GNN). Specifically, the social account information is first extracted into unified user textual sequences, which is then used to perform supervised fine-tuning (SFT) of the language model to improve its ability to understand social account semantics. Next, the semantically enriched node representation is fed into the pre-trained GNN to further enhance the node representation by aggregating information from neighbors. Finally, LGB fuses the information from both modalities to improve the detection performance of sparsely linked nodes. Extensive experiments on two real-world datasets demonstrate that LGB consistently outperforms state-of-the-art baseline models by up to 10.95%. LGB is already online: https://botdetection.aminer.cn/robotmain.","sentences":["Malicious social bots achieve their malicious purposes by spreading misinformation and inciting social public opinion, seriously endangering social security, making their detection a critical concern.","Recently, graph-based bot detection methods have achieved state-of-the-art (SOTA) performance.","However, our research finds many isolated and poorly linked nodes in social networks, as shown in Fig.1, which graph-based methods cannot effectively detect.","To address this problem, our research focuses on effectively utilizing node semantics and network structure to jointly detect sparsely linked nodes.","Given the excellent performance of language models (LMs) in natural language understanding (NLU), we propose a novel social bot detection framework LGB, which consists of two main components: language model (LM) and graph neural network (GNN).","Specifically, the social account information is first extracted into unified user textual sequences, which is then used to perform supervised fine-tuning (SFT) of the language model to improve its ability to understand social account semantics.","Next, the semantically enriched node representation is fed into the pre-trained GNN to further enhance the node representation by aggregating information from neighbors.","Finally, LGB fuses the information from both modalities to improve the detection performance of sparsely linked nodes.","Extensive experiments on two real-world datasets demonstrate that LGB consistently outperforms state-of-the-art baseline models by up to 10.95%.","LGB is already online: https://botdetection.aminer.cn/robotmain."],"url":"http://arxiv.org/abs/2406.08762v1","category":"cs.SI"}
{"created":"2024-06-13 02:27:16","title":"Solving Fractional Differential Equations on a Quantum Computer: A Variational Approach","abstract":"We introduce an efficient variational hybrid quantum-classical algorithm designed for solving Caputo time-fractional partial differential equations. Our method employs an iterable cost function incorporating a linear combination of overlap history states. The proposed algorithm is not only efficient in time complexity, but has lower memory costs compared to classical methods. Our results indicate that solution fidelity is insensitive to the fractional index and that gradient evaluation cost scales economically with the number of time steps. As a proof of concept, we apply our algorithm to solve a range of fractional partial differential equations commonly encountered in engineering applications, such as the sub-diffusion equation, the non-linear Burgers' equation and a coupled diffusive epidemic model. We assess quantum hardware performance under realistic noise conditions, further validating the practical utility of our algorithm.","sentences":["We introduce an efficient variational hybrid quantum-classical algorithm designed for solving Caputo time-fractional partial differential equations.","Our method employs an iterable cost function incorporating a linear combination of overlap history states.","The proposed algorithm is not only efficient in time complexity, but has lower memory costs compared to classical methods.","Our results indicate that solution fidelity is insensitive to the fractional index and that gradient evaluation cost scales economically with the number of time steps.","As a proof of concept, we apply our algorithm to solve a range of fractional partial differential equations commonly encountered in engineering applications, such as the sub-diffusion equation, the non-linear Burgers' equation and a coupled diffusive epidemic model.","We assess quantum hardware performance under realistic noise conditions, further validating the practical utility of our algorithm."],"url":"http://arxiv.org/abs/2406.08755v1","category":"quant-ph"}
{"created":"2024-06-13 02:19:40","title":"The expressway network design problem for multiple urban subregions based on the macroscopic fundamental diagram","abstract":"As urbanization advances, cities are expanding, leading to a more decentralized urban structure and longer average commuting durations. The construction of an urban expressway system emerges as a critical strategy to tackle this challenge. However, the traditional link-level network design method faces modeling and solution challenges when dealing with the large-scale expressway network design problem (ENDP). To address the challenges, this paper proposes an expressway network design method for multiple urban subregions based on the macroscopic fundamental diagram (MFD). Initially, a mixed road network traffic model that describes traffic dynamics of multiple subregions and candidate expressways is developed by integrating the MFD and the cell transmission model (CTM). Then, treating urban subregions and candidate expressways as route nodes in the mixed road network, a route choice model is established based on stochastic user equilibrium. Finally, a decision model for ENDP is proposed to minimize vehicle travel time under the construction budget constraint. The impact of financial investment and traffic demand on expressway network design schemes in the case study is explored separately. The simulation results indicate that during the initial stages of expressway planning, the construction of new expressways can significantly alleviate traffic congestion. However, as the expressway network expands further, the effectiveness of improving traffic conditions through new expressway construction gradually diminishes if traffic demand does not continue to increase. Additionally, variations in traffic demand between subregions result in different construction schemes, emphasizing the importance of adjusting budget allocations based on specific traffic demands.","sentences":["As urbanization advances, cities are expanding, leading to a more decentralized urban structure and longer average commuting durations.","The construction of an urban expressway system emerges as a critical strategy to tackle this challenge.","However, the traditional link-level network design method faces modeling and solution challenges when dealing with the large-scale expressway network design problem (ENDP).","To address the challenges, this paper proposes an expressway network design method for multiple urban subregions based on the macroscopic fundamental diagram (MFD).","Initially, a mixed road network traffic model that describes traffic dynamics of multiple subregions and candidate expressways is developed by integrating the MFD and the cell transmission model (CTM).","Then, treating urban subregions and candidate expressways as route nodes in the mixed road network, a route choice model is established based on stochastic user equilibrium.","Finally, a decision model for ENDP is proposed to minimize vehicle travel time under the construction budget constraint.","The impact of financial investment and traffic demand on expressway network design schemes in the case study is explored separately.","The simulation results indicate that during the initial stages of expressway planning, the construction of new expressways can significantly alleviate traffic congestion.","However, as the expressway network expands further, the effectiveness of improving traffic conditions through new expressway construction gradually diminishes if traffic demand does not continue to increase.","Additionally, variations in traffic demand between subregions result in different construction schemes, emphasizing the importance of adjusting budget allocations based on specific traffic demands."],"url":"http://arxiv.org/abs/2406.08750v1","category":"eess.SY"}
{"created":"2024-06-13 02:17:19","title":"Mathematical models for off-ball scoring prediction in basketball","abstract":"In professional basketball, the accurate prediction of scoring opportunities based on strategic decision-making is crucial for space and player evaluations. However, traditional models often face challenges in accounting for the complexities of off-ball movements, which are essential for accurate predictive performance. In this study, we propose two mathematical models to predict off-ball scoring opportunities in basketball, considering both pass-to-score and dribble-to-score movements: the Ball Movement for Off-ball Scoring (BMOS) and the Ball Intercept and Movement for Off-ball Scoring (BIMOS) models. The BMOS adapts principles from the Off-Ball Scoring Opportunities (OBSO) model, originally designed for soccer, to basketball, whereas the BIMOS also incorporates the likelihood of interception during ball movements. We evaluated these models using player tracking data from 630 NBA games in the 2015-2016 regular season, demonstrating that the BIMOS outperforms the BMOS in terms of scoring prediction accuracy. Thus, our models provide valuable insights for tactical analysis and player evaluation in basketball.","sentences":["In professional basketball, the accurate prediction of scoring opportunities based on strategic decision-making is crucial for space and player evaluations.","However, traditional models often face challenges in accounting for the complexities of off-ball movements, which are essential for accurate predictive performance.","In this study, we propose two mathematical models to predict off-ball scoring opportunities in basketball, considering both pass-to-score and dribble-to-score movements: the Ball Movement for Off-ball Scoring (BMOS) and the Ball Intercept and Movement for Off-ball Scoring (BIMOS) models.","The BMOS adapts principles from the Off-Ball Scoring Opportunities (OBSO) model, originally designed for soccer, to basketball, whereas the BIMOS also incorporates the likelihood of interception during ball movements.","We evaluated these models using player tracking data from 630 NBA games in the 2015-2016 regular season, demonstrating that the BIMOS outperforms the BMOS in terms of scoring prediction accuracy.","Thus, our models provide valuable insights for tactical analysis and player evaluation in basketball."],"url":"http://arxiv.org/abs/2406.08749v1","category":"cs.LG"}
{"created":"2024-06-13 02:07:34","title":"The AHA-Tree: An Adaptive Index for HTAP Workloads","abstract":"In this demo, we realize data indexes that can morph from being write-optimized at times to being read-optimized at other times nonstop with zero-down time during the workload transitioning. These data indexes are useful for HTAP systems (Hybrid Transactional and Analytical Processing Systems), where transactional workloads are write-heavy while analytical workloads are read-heavy. Traditional indexes, e.g., B+-tree and LSM-Tree, although optimized for one kind of workload, cannot perform equally well under all workloads. To migrate from the write-optimized LSM-Tree to a read-optimized B+-tree is costly and mandates some system down time to reorganize data. We design adaptive indexes that can dynamically morph from a pure LSM-tree to a pure buffered B-tree back and forth, and has interesting states in-between. There are two challenges: allowing concurrent operations and avoiding system down time. This demo benchmarks the proposed AHA-Tree index under dynamic workloads and shows how the index evolves from one state to another without blocking.","sentences":["In this demo, we realize data indexes that can morph from being write-optimized at times to being read-optimized at other times nonstop with zero-down time during the workload transitioning.","These data indexes are useful for HTAP systems (Hybrid Transactional and Analytical Processing Systems), where transactional workloads are write-heavy while analytical workloads are read-heavy.","Traditional indexes, e.g., B+-tree and LSM-Tree, although optimized for one kind of workload, cannot perform equally well under all workloads.","To migrate from the write-optimized LSM-Tree to a read-optimized B+-tree is costly and mandates some system down time to reorganize data.","We design adaptive indexes that can dynamically morph from a pure LSM-tree to a pure buffered B-tree back and forth, and has interesting states in-between.","There are two challenges: allowing concurrent operations and avoiding system down time.","This demo benchmarks the proposed AHA-Tree index under dynamic workloads and shows how the index evolves from one state to another without blocking."],"url":"http://arxiv.org/abs/2406.08746v1","category":"cs.DB"}
{"created":"2024-06-13 02:03:22","title":"Generalizable Implicit Neural Representation As a Universal Spatiotemporal Traffic Data Learner","abstract":"$\\textbf{This is the conference version of our paper: Spatiotemporal Implicit Neural Representation as a Generalized Traffic Data Learner}$. Spatiotemporal Traffic Data (STTD) measures the complex dynamical behaviors of the multiscale transportation system. Existing methods aim to reconstruct STTD using low-dimensional models. However, they are limited to data-specific dimensions or source-dependent patterns, restricting them from unifying representations. Here, we present a novel paradigm to address the STTD learning problem by parameterizing STTD as an implicit neural representation. To discern the underlying dynamics in low-dimensional regimes, coordinate-based neural networks that can encode high-frequency structures are employed to directly map coordinates to traffic variables. To unravel the entangled spatial-temporal interactions, the variability is decomposed into separate processes. We further enable modeling in irregular spaces such as sensor graphs using spectral embedding. Through continuous representations, our approach enables the modeling of a variety of STTD with a unified input, thereby serving as a generalized learner of the underlying traffic dynamics. It is also shown that it can learn implicit low-rank priors and smoothness regularization from the data, making it versatile for learning different dominating data patterns. We validate its effectiveness through extensive experiments in real-world scenarios, showcasing applications from corridor to network scales. Empirical results not only indicate that our model has significant superiority over conventional low-rank models, but also highlight that the versatility of the approach. We anticipate that this pioneering modeling perspective could lay the foundation for universal representation of STTD in various real-world tasks. $\\textbf{The full version can be found at:}$ https://doi.org/10.48550/arXiv.2405.03185.","sentences":["$\\textbf{This is the conference version of our paper: Spatiotemporal Implicit Neural Representation as a Generalized Traffic Data Learner}$. Spatiotemporal Traffic Data (STTD) measures the complex dynamical behaviors of the multiscale transportation system.","Existing methods aim to reconstruct STTD using low-dimensional models.","However, they are limited to data-specific dimensions or source-dependent patterns, restricting them from unifying representations.","Here, we present a novel paradigm to address the STTD learning problem by parameterizing STTD as an implicit neural representation.","To discern the underlying dynamics in low-dimensional regimes, coordinate-based neural networks that can encode high-frequency structures are employed to directly map coordinates to traffic variables.","To unravel the entangled spatial-temporal interactions, the variability is decomposed into separate processes.","We further enable modeling in irregular spaces such as sensor graphs using spectral embedding.","Through continuous representations, our approach enables the modeling of a variety of STTD with a unified input, thereby serving as a generalized learner of the underlying traffic dynamics.","It is also shown that it can learn implicit low-rank priors and smoothness regularization from the data, making it versatile for learning different dominating data patterns.","We validate its effectiveness through extensive experiments in real-world scenarios, showcasing applications from corridor to network scales.","Empirical results not only indicate that our model has significant superiority over conventional low-rank models, but also highlight that the versatility of the approach.","We anticipate that this pioneering modeling perspective could lay the foundation for universal representation of STTD in various real-world tasks.","$\\textbf{The full version can be found at:}$ https://doi.org/10.48550/arXiv.2405.03185."],"url":"http://arxiv.org/abs/2406.08743v1","category":"cs.LG"}
{"created":"2024-06-13 02:00:13","title":"An AI Architecture with the Capability to Explain Recognition Results","abstract":"Explainability is needed to establish confidence in machine learning results. Some explainable methods take a post hoc approach to explain the weights of machine learning models, others highlight areas of the input contributing to decisions. These methods do not adequately explain decisions, in plain terms. Explainable property-based systems have been shown to provide explanations in plain terms, however, they have not performed as well as leading unexplainable machine learning methods. This research focuses on the importance of metrics to explainability and contributes two methods yielding performance gains. The first method introduces a combination of explainable and unexplainable flows, proposing a metric to characterize explainability of a decision. The second method compares classic metrics for estimating the effectiveness of neural networks in the system, posing a new metric as the leading performer. Results from the new methods and examples from handwritten datasets are presented.","sentences":["Explainability is needed to establish confidence in machine learning results.","Some explainable methods take a post hoc approach to explain the weights of machine learning models, others highlight areas of the input contributing to decisions.","These methods do not adequately explain decisions, in plain terms.","Explainable property-based systems have been shown to provide explanations in plain terms, however, they have not performed as well as leading unexplainable machine learning methods.","This research focuses on the importance of metrics to explainability and contributes two methods yielding performance gains.","The first method introduces a combination of explainable and unexplainable flows, proposing a metric to characterize explainability of a decision.","The second method compares classic metrics for estimating the effectiveness of neural networks in the system, posing a new metric as the leading performer.","Results from the new methods and examples from handwritten datasets are presented."],"url":"http://arxiv.org/abs/2406.08740v1","category":"cs.LG"}
{"created":"2024-06-13 01:51:59","title":"Volatility Forecasting Using Similarity-based Parameter Correction and Aggregated Shock Information","abstract":"We develop a procedure for forecasting the volatility of a time series immediately following a news shock. Adapting the similarity-based framework of Lin and Eck (2020), we exploit series that have experienced similar shocks. We aggregate their shock-induced excess volatilities by positing the shocks to be affine functions of exogenous covariates. The volatility shocks are modeled as random effects and estimated as fixed effects. The aggregation of these estimates is done in service of adjusting the $h$-step-ahead GARCH forecast of the time series under study by an additive term. The adjusted and unadjusted forecasts are evaluated using the unobservable but easily-estimated realized volatility (RV). A real-world application is provided, as are simulation results suggesting the conditions and hyperparameters under which our method thrives.","sentences":["We develop a procedure for forecasting the volatility of a time series immediately following a news shock.","Adapting the similarity-based framework of Lin and Eck (2020), we exploit series that have experienced similar shocks.","We aggregate their shock-induced excess volatilities by positing the shocks to be affine functions of exogenous covariates.","The volatility shocks are modeled as random effects and estimated as fixed effects.","The aggregation of these estimates is done in service of adjusting the $h$-step-ahead GARCH forecast of the time series under study by an additive term.","The adjusted and unadjusted forecasts are evaluated using the unobservable but easily-estimated realized volatility (RV).","A real-world application is provided, as are simulation results suggesting the conditions and hyperparameters under which our method thrives."],"url":"http://arxiv.org/abs/2406.08738v1","category":"stat.AP"}
{"created":"2024-06-13 01:47:09","title":"Field investigation of 3D snow settling dynamics under weak atmospheric turbulence","abstract":"Research on settling dynamics of snow particles, considering their complex morphologies and real atmospheric conditions, remains scarce despite extensive simulations and laboratory studies. Our study bridges the gap through a comprehensive field investigation into the three-dimensional (3D) snow settling dynamics under weak atmospheric turbulence, enabled by a 3D particle tracking velocimetry (PTV) system to record > a million trajectories, coupled with a snow particle analyzer for simultaneous aerodynamic property characterization of four distinct snow types (aggregates, graupels, dendrites, needles). Our findings indicate that while the terminal velocity predicted by the aerodynamic model aligns well with PTV-measured settling velocity for graupels, significant discrepancies arise for non-spherical particles, particularly dendrites, which exhibit higher drag coefficients than predicted. Qualitative observations of 3D settling trajectories highlight pronounced meandering in aggregates and dendrites, in contrast to the subtler meandering observed in needles and graupels, attributable to their smaller frontal areas. This meandering in aggregates and dendrites occurs at lower frequencies compared to that of graupels. Further quantification of trajectory acceleration and curvature suggests that the meandering frequencies in aggregates and dendrites are smaller than that of morphology-induced vortex shedding of disks, likely due to their rotational inertia, and those of graupels align with the small-scale atmospheric turbulence. Moreover, our analysis of vertical acceleration along trajectories elucidates that the orientation changes in dendrites and aggregates enhance their settling velocity. Such insights into settling dynamics refine models of snow settling velocity under weak atmospheric turbulence, with broader implications for more accurately predicting ground snow accumulation.","sentences":["Research on settling dynamics of snow particles, considering their complex morphologies and real atmospheric conditions, remains scarce despite extensive simulations and laboratory studies.","Our study bridges the gap through a comprehensive field investigation into the three-dimensional (3D) snow settling dynamics under weak atmospheric turbulence, enabled by a 3D particle tracking velocimetry (PTV) system to record > a million trajectories, coupled with a snow particle analyzer for simultaneous aerodynamic property characterization of four distinct snow types (aggregates, graupels, dendrites, needles).","Our findings indicate that while the terminal velocity predicted by the aerodynamic model aligns well with PTV","-measured settling velocity for graupels, significant discrepancies arise for non-spherical particles, particularly dendrites, which exhibit higher drag coefficients than predicted.","Qualitative observations of 3D settling trajectories highlight pronounced meandering in aggregates and dendrites, in contrast to the subtler meandering observed in needles and graupels, attributable to their smaller frontal areas.","This meandering in aggregates and dendrites occurs at lower frequencies compared to that of graupels.","Further quantification of trajectory acceleration and curvature suggests that the meandering frequencies in aggregates and dendrites are smaller than that of morphology-induced vortex shedding of disks, likely due to their rotational inertia, and those of graupels align with the small-scale atmospheric turbulence.","Moreover, our analysis of vertical acceleration along trajectories elucidates that the orientation changes in dendrites and aggregates enhance their settling velocity.","Such insights into settling dynamics refine models of snow settling velocity under weak atmospheric turbulence, with broader implications for more accurately predicting ground snow accumulation."],"url":"http://arxiv.org/abs/2406.08737v1","category":"physics.flu-dyn"}
{"created":"2024-06-13 01:34:33","title":"Relative belief inferences from decision theory","abstract":"Relative belief inferences are shown to arise as Bayes rules or limiting Bayes rules. These inferences are invariant under reparameterizations and possess a number of optimal properties. In particular, relative belief inferences are based on a direct measure of statistical evidence.","sentences":["Relative belief inferences are shown to arise as Bayes rules or limiting Bayes rules.","These inferences are invariant under reparameterizations and possess a number of optimal properties.","In particular, relative belief inferences are based on a direct measure of statistical evidence."],"url":"http://arxiv.org/abs/2406.08732v1","category":"math.ST"}
{"created":"2024-06-13 01:29:52","title":"Where Do Large Language Models Fail When Generating Code?","abstract":"Large Language Models (LLMs) have shown great potential in code generation. However, current LLMs still cannot reliably generate correct code. Moreover, it is unclear what kinds of code generation errors LLMs can make. To address this, we conducted an empirical study to analyze incorrect code snippets generated by six popular LLMs on the HumanEval dataset. We analyzed these errors alongside two dimensions of error characteristics -- semantic characteristics and syntactic characteristics -- to derive a comprehensive code generation error taxonomy for LLMs through open coding and thematic analysis. We then labeled all 558 incorrect code snippets based on this taxonomy. Our results showed that the six LLMs exhibited different distributions of semantic and syntactic characteristics. Furthermore, we analyzed the correlation between different error characteristics and factors such as prompt length, code length, and test-pass rate. Finally, we highlight the challenges that LLMs may encounter when generating code and propose implications for future research on reliable code generation with LLMs.","sentences":["Large Language Models (LLMs) have shown great potential in code generation.","However, current LLMs still cannot reliably generate correct code.","Moreover, it is unclear what kinds of code generation errors LLMs can make.","To address this, we conducted an empirical study to analyze incorrect code snippets generated by six popular LLMs on the HumanEval dataset.","We analyzed these errors alongside two dimensions of error characteristics -- semantic characteristics and syntactic characteristics -- to derive a comprehensive code generation error taxonomy for LLMs through open coding and thematic analysis.","We then labeled all 558 incorrect code snippets based on this taxonomy.","Our results showed that the six LLMs exhibited different distributions of semantic and syntactic characteristics.","Furthermore, we analyzed the correlation between different error characteristics and factors such as prompt length, code length, and test-pass rate.","Finally, we highlight the challenges that LLMs may encounter when generating code and propose implications for future research on reliable code generation with LLMs."],"url":"http://arxiv.org/abs/2406.08731v1","category":"cs.SE"}
{"created":"2024-06-13 01:12:08","title":"Structure Phase Change Induced by Nonequilibrium Effects in Molecular Scale Junctions","abstract":"The interrelationship between a material's structure and its properties lies at the heart of materials-related research. Finding how the changes of one affect the other is of primary importance in theoretical and computational materials studies. In this work, based on Hershfield nonequilibrium quantum statistics and the mean-field approach with steady-state density functional theory, we derive a first-principles method to calculate nonequilibrium effects induced forces acting on atoms, enabling structure optimizations and molecular dynamics simulations for molecular junctions under external biases. By applying the method to a few molecular devices, we found that in general, the external bias can induce profound nonequilibrium effects on both electronic/transport properties and the geometric structure of these devices, and consequent changes in electronic properties and geometric structure are closely interrelated. Particularly, when the bias voltage is above 1.0 V, significant structure phase changes could occur, causing dramatic changes in I-V characteristics and vibrational spectra. These findings greatly broaden our understanding of quantum electronic devices and provide a new avenue for discovering novel transport phenomena at molecular scale.","sentences":["The interrelationship between a material's structure and its properties lies at the heart of materials-related research.","Finding how the changes of one affect the other is of primary importance in theoretical and computational materials studies.","In this work, based on Hershfield nonequilibrium quantum statistics and the mean-field approach with steady-state density functional theory, we derive a first-principles method to calculate nonequilibrium effects induced forces acting on atoms, enabling structure optimizations and molecular dynamics simulations for molecular junctions under external biases.","By applying the method to a few molecular devices, we found that in general, the external bias can induce profound nonequilibrium effects on both electronic/transport properties and the geometric structure of these devices, and consequent changes in electronic properties and geometric structure are closely interrelated.","Particularly, when the bias voltage is above 1.0 V, significant structure phase changes could occur, causing dramatic changes in I-V characteristics and vibrational spectra.","These findings greatly broaden our understanding of quantum electronic devices and provide a new avenue for discovering novel transport phenomena at molecular scale."],"url":"http://arxiv.org/abs/2406.08729v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-13 01:11:46","title":"Primordial magnetic relics and their signatures","abstract":"Primordial black holes bearing magnetic charges may bypass the constraints imposed by Hawking radiation, thereby enabling reasonable present-day populations, even for masses below $10^{15}\\,\\text{g}$ -- a range previously considered improbable. They could, therefore, conceivably contribute to a component of dark matter. We investigate novel Faraday rotation signatures exhibited by primordial magnetic black holes while also establishing new Parker-type bounds on their populations. For the latter, we bound the dark matter fraction from intergalactic magnetic fields in cosmic voids $\\left(f_{\\text{DM}} \\lesssim 10^{-8}\\right)$ and cosmic web filaments $\\left(f_{\\text{ DM}} \\lesssim 10^{-4}\\right)$, notably eclipsing previous bounds. Exploring Faraday rotation effects, we discern a pronounced rotation of the polarization angle and the rotation measure values for extremal primordial magnetic black holes with masses $M^{\\text{ ex.}}_{\\text{ BH}}\\gtrsim 10^{-6}~ \\text{M}_\\odot$. This makes them potentially detectable in current observations. A comparative investigation finds that the effects are notably greater than for a neutron star, like a Magnetar, with a similar magnetic field at the surface. Moreover, the polarization angle maps for primordial magnetic black holes exhibit unique features, notably absent in other astrophysical magnetic configurations. In this context, we also introduce a simple integral measure, offering a quantitative measure for their discrimination in many scenarios. These traits potentially suggest a robust avenue for their observational detection and differentiation.","sentences":["Primordial black holes bearing magnetic charges may bypass the constraints imposed by Hawking radiation, thereby enabling reasonable present-day populations, even for masses below $10^{15}\\,\\text{g}$ -- a range previously considered improbable.","They could, therefore, conceivably contribute to a component of dark matter.","We investigate novel Faraday rotation signatures exhibited by primordial magnetic black holes while also establishing new Parker-type bounds on their populations.","For the latter, we bound the dark matter fraction from intergalactic magnetic fields in cosmic voids $\\left(f_{\\text{DM}} \\lesssim 10^{-8}\\right)$ and cosmic web filaments $\\left(f_{\\text{ DM}} \\lesssim 10^{-4}\\right)$, notably eclipsing previous bounds.","Exploring Faraday rotation effects, we discern a pronounced rotation of the polarization angle and the rotation measure values for extremal primordial magnetic black holes with masses $M^{\\text{ ex.}}_{\\text{ BH}}\\gtrsim 10^{-6}~ \\text{M}_\\odot$.","This makes them potentially detectable in current observations.","A comparative investigation finds that the effects are notably greater than for a neutron star, like a Magnetar, with a similar magnetic field at the surface.","Moreover, the polarization angle maps for primordial magnetic black holes exhibit unique features, notably absent in other astrophysical magnetic configurations.","In this context, we also introduce a simple integral measure, offering a quantitative measure for their discrimination in many scenarios.","These traits potentially suggest a robust avenue for their observational detection and differentiation."],"url":"http://arxiv.org/abs/2406.08728v1","category":"astro-ph.CO"}
{"created":"2024-06-13 00:58:07","title":"Restricted Open-shell cluster Mean-Field theory for Strongly Correlated Systems","abstract":"The cluster-based Mean Field method (cMF) and it's second order perturbative correction[1], was introduced by Jim\\'enez-Hoyos and Scuseria to reduce the cost of modeling strongly correlated systems by dividing an active space up into small clusters, which are individually solved in the mean-field presence of each other. In that work, clusters with unpaired electrons are treated naturally, by allowing the $\\alpha$ and $\\beta$ orbitals to spin polarize. While that provided significant energetic stabilization, the resulting cMF wavefunction was spin-contaminated, making it difficult to use as a reference state for spin-pure post-cMF methods. In this work, we propose the Restricted Open-shell cMF (RO-cMF) method, extending the cMF approach to systems with open-shell clusters, while not permitting spin-polarization. While the resulting RO-cMF energies are necessarily higher in energy than the unrestricted orbital cMF, the new RO-cMF provides a simple reference state for post-cMF methods that recover the missing inter-cluster correlations. We provide a detailed explanation of the method, and report demonstrative calculations of exchange coupling constants for three systems: a di-iron complex, a di-chromium complex, and a dimerized organic radical. We also report the first perturbatively corrected RO-cMF-PT2 results as well.","sentences":["The cluster-based Mean Field method (cMF) and it's second order perturbative correction[1], was introduced by Jim\\'enez-Hoyos and Scuseria to reduce the cost of modeling strongly correlated systems by dividing an active space up into small clusters, which are individually solved in the mean-field presence of each other.","In that work, clusters with unpaired electrons are treated naturally, by allowing the $\\alpha$ and $\\beta$ orbitals to spin polarize.","While that provided significant energetic stabilization, the resulting cMF wavefunction was spin-contaminated, making it difficult to use as a reference state for spin-pure post-cMF methods.","In this work, we propose the Restricted Open-shell cMF (RO-cMF) method, extending the cMF approach to systems with open-shell clusters, while not permitting spin-polarization.","While the resulting RO-cMF energies are necessarily higher in energy than the unrestricted orbital cMF, the new RO-cMF provides a simple reference state for post-cMF methods that recover the missing inter-cluster correlations.","We provide a detailed explanation of the method, and report demonstrative calculations of exchange coupling constants for three systems: a di-iron complex, a di-chromium complex, and a dimerized organic radical.","We also report the first perturbatively corrected RO-cMF-PT2 results as well."],"url":"http://arxiv.org/abs/2406.08721v1","category":"physics.chem-ph"}
{"created":"2024-06-13 00:48:51","title":"TikTag: Breaking ARM's Memory Tagging Extension with Speculative Execution","abstract":"ARM Memory Tagging Extension (MTE) is a new hardware feature introduced in ARMv8.5-A architecture, aiming to detect memory corruption vulnerabilities. The low overhead of MTE makes it an attractive solution to mitigate memory corruption attacks in modern software systems and is considered the most promising path forward for improving C/C++ software security. This paper explores the potential security risks posed by speculative execution attacks against MTE. Specifically, this paper identifies new TikTag gadgets capable of leaking the MTE tags from arbitrary memory addresses through speculative execution. With TikTag gadgets, attackers can bypass the probabilistic defense of MTE, increasing the attack success rate by close to 100%. We demonstrate that TikTag gadgets can be used to bypass MTE-based mitigations in real-world systems, Google Chrome and the Linux kernel. Experimental results show that TikTag gadgets can successfully leak an MTE tag with a success rate higher than 95% in less than 4 seconds. We further propose new defense mechanisms to mitigate the security risks posed by TikTag gadgets.","sentences":["ARM Memory Tagging Extension (MTE) is a new hardware feature introduced in ARMv8.5-A architecture, aiming to detect memory corruption vulnerabilities.","The low overhead of MTE makes it an attractive solution to mitigate memory corruption attacks in modern software systems and is considered the most promising path forward for improving C/C++ software security.","This paper explores the potential security risks posed by speculative execution attacks against MTE.","Specifically, this paper identifies new TikTag gadgets capable of leaking the MTE tags from arbitrary memory addresses through speculative execution.","With TikTag gadgets, attackers can bypass the probabilistic defense of MTE, increasing the attack success rate by close to 100%.","We demonstrate that TikTag gadgets can be used to bypass MTE-based mitigations in real-world systems, Google Chrome and the Linux kernel.","Experimental results show that TikTag gadgets can successfully leak an MTE tag with a success rate higher than 95% in less than 4 seconds.","We further propose new defense mechanisms to mitigate the security risks posed by TikTag gadgets."],"url":"http://arxiv.org/abs/2406.08719v1","category":"cs.CR"}
{"created":"2024-06-13 00:43:48","title":"TSE-PI: Target Sound Extraction under Reverberant Environments with Pitch Information","abstract":"Target sound extraction (TSE) separates the target sound from the mixture signals based on provided clues. However, the performance of existing models significantly degrades under reverberant conditions. Inspired by auditory scene analysis (ASA), this work proposes a TSE model provided with pitch information named TSE-PI. Conditional pitch extraction is achieved through the Feature-wise Linearly Modulated layer with the sound-class label. A modified Waveformer model combined with pitch information, employing a learnable Gammatone filterbank in place of the convolutional encoder, is used for target sound extraction. The inclusion of pitch information is aimed at improving the model's performance. The experimental results on the FSD50K dataset illustrate 2.4 dB improvements of target sound extraction under reverberant environments when incorporating pitch information and Gammatone filterbank.","sentences":["Target sound extraction (TSE) separates the target sound from the mixture signals based on provided clues.","However, the performance of existing models significantly degrades under reverberant conditions.","Inspired by auditory scene analysis (ASA), this work proposes a TSE model provided with pitch information named TSE-PI.","Conditional pitch extraction is achieved through the Feature-wise Linearly Modulated layer with the sound-class label.","A modified Waveformer model combined with pitch information, employing a learnable Gammatone filterbank in place of the convolutional encoder, is used for target sound extraction.","The inclusion of pitch information is aimed at improving the model's performance.","The experimental results on the FSD50K dataset illustrate 2.4 dB improvements of target sound extraction under reverberant environments when incorporating pitch information and Gammatone filterbank."],"url":"http://arxiv.org/abs/2406.08716v1","category":"cs.SD"}
{"created":"2024-06-13 00:39:13","title":"A non-circular concept of number inspired by Gottlob Frege's definition","abstract":"Gottlob Frege ingeniously presented a purely logical definition of the concept of number. However, one can claim that his definition is, in some way, circular, as it relies on the concept of one-to-one relation. The concept of number only makes sense when it presents the property of projection/reflection or binding. When we consider a number as an abstraction of objects, whatever they may be, saying that a number that belongs to the concept F is the same as that which belongs to the concept G means there is a projection/reflection, or binding, between the objects in F and the objects in G. We present a definition based on both equivalent approaches. First, we introduce the definition based on the relations of projection and reflection; then, we present the definition based on the relation of binding.","sentences":["Gottlob Frege ingeniously presented a purely logical definition of the concept of number.","However, one can claim that his definition is, in some way, circular, as it relies on the concept of one-to-one relation.","The concept of number only makes sense when it presents the property of projection/reflection or binding.","When we consider a number as an abstraction of objects, whatever they may be, saying that a number that belongs to the concept F is the same as that which belongs to the concept G means there is a projection/reflection, or binding, between the objects in F and the objects in G. We present a definition based on both equivalent approaches.","First, we introduce the definition based on the relations of projection and reflection; then, we present the definition based on the relation of binding."],"url":"http://arxiv.org/abs/2406.08715v1","category":"math.HO"}
{"created":"2024-06-13 00:13:32","title":"mOSCAR: A Large-scale Multilingual and Multimodal Document-level Corpus","abstract":"Multimodal Large Language Models (mLLMs) are trained on a large amount of text-image data. While most mLLMs are trained on caption-like data only, Alayrac et al. [2022] showed that additionally training them on interleaved sequences of text and images can lead to the emergence of in-context learning capabilities. However, the dataset they used, M3W, is not public and is only in English. There have been attempts to reproduce their results but the released datasets are English-only. In contrast, current multilingual and multimodal datasets are either composed of caption-like only or medium-scale or fully private data. This limits mLLM research for the 7,000 other languages spoken in the world. We therefore introduce mOSCAR, to the best of our knowledge the first large-scale multilingual and multimodal document corpus crawled from the web. It covers 163 languages, 315M documents, 214B tokens and 1.2B images. We carefully conduct a set of filtering and evaluation steps to make sure mOSCAR is sufficiently safe, diverse and of good quality. We additionally train two types of multilingual model to prove the benefits of mOSCAR: (1) a model trained on a subset of mOSCAR and captioning data and (2) a model train on captioning data only. The model additionally trained on mOSCAR shows a strong boost in few-shot learning performance across various multilingual image-text tasks and benchmarks, confirming previous findings for English-only mLLMs.","sentences":["Multimodal Large Language Models (mLLMs) are trained on a large amount of text-image data.","While most mLLMs are trained on caption-like data only, Alayrac et al.","[2022] showed that additionally training them on interleaved sequences of text and images can lead to the emergence of in-context learning capabilities.","However, the dataset they used, M3W, is not public and is only in English.","There have been attempts to reproduce their results but the released datasets are English-only.","In contrast, current multilingual and multimodal datasets are either composed of caption-like only or medium-scale or fully private data.","This limits mLLM research for the 7,000 other languages spoken in the world.","We therefore introduce mOSCAR, to the best of our knowledge the first large-scale multilingual and multimodal document corpus crawled from the web.","It covers 163 languages, 315M documents, 214B tokens and 1.2B images.","We carefully conduct a set of filtering and evaluation steps to make sure mOSCAR is sufficiently safe, diverse and of good quality.","We additionally train two types of multilingual model to prove the benefits of mOSCAR: (1) a model trained on a subset of mOSCAR and captioning data and (2) a model train on captioning data only.","The model additionally trained on mOSCAR shows a strong boost in few-shot learning performance across various multilingual image-text tasks and benchmarks, confirming previous findings for English-only mLLMs."],"url":"http://arxiv.org/abs/2406.08707v1","category":"cs.CL"}
{"created":"2024-06-12 23:43:07","title":"Matching with a Status Quo: The Agreeable Core","abstract":"We provide a framework to unify classic models of two-sided matching with recent models of recontracting. In the classic model, agents from two sides must be matched; in models of recontracting, agents improve a status quo match. We generalize the core (matches not blocked by any coalition) from cooperative game theory to our setting by restricting the set of permissible coalitions to coalitions containing neither or both agents in a status quo match, dubbed \"agreeable\" coalitions. The agreeable core is the set of all weak improvements of the status quo that are not blocked by any agreeable coalition. Our main result is that the agreeable core is nonempty and can be found through a computationally efficient and economically meaningful algorithm: our Propose-Exchange algorithm. The applications of the agreeable core include early decision, out-of-match agreements in the NRMP, matching with minimum constraints, and efficiency in school choice.","sentences":["We provide a framework to unify classic models of two-sided matching with recent models of recontracting.","In the classic model, agents from two sides must be matched; in models of recontracting, agents improve a status quo match.","We generalize the core (matches not blocked by any coalition) from cooperative game theory to our setting by restricting the set of permissible coalitions to coalitions containing neither or both agents in a status quo match, dubbed \"agreeable\" coalitions.","The agreeable core is the set of all weak improvements of the status quo that are not blocked by any agreeable coalition.","Our main result is that the agreeable core is nonempty and can be found through a computationally efficient and economically meaningful algorithm: our Propose-Exchange algorithm.","The applications of the agreeable core include early decision, out-of-match agreements in the NRMP, matching with minimum constraints, and efficiency in school choice."],"url":"http://arxiv.org/abs/2406.08700v1","category":"econ.TH"}
{"created":"2024-06-12 23:41:43","title":"Orthogonalized Estimation of Difference of $Q$-functions","abstract":"Offline reinforcement learning is important in many settings with available observational data but the inability to deploy new policies online due to safety, cost, and other concerns. Many recent advances in causal inference and machine learning target estimation of causal contrast functions such as CATE, which is sufficient for optimizing decisions and can adapt to potentially smoother structure. We develop a dynamic generalization of the R-learner (Nie and Wager 2021, Lewis and Syrgkanis 2021) for estimating and optimizing the difference of $Q^\\pi$-functions, $Q^\\pi(s,1)-Q^\\pi(s,0)$ (which can be used to optimize multiple-valued actions). We leverage orthogonal estimation to improve convergence rates in the presence of slower nuisance estimation rates and prove consistency of policy optimization under a margin condition. The method can leverage black-box nuisance estimators of the $Q$-function and behavior policy to target estimation of a more structured $Q$-function contrast.","sentences":["Offline reinforcement learning is important in many settings with available observational data but the inability to deploy new policies online due to safety, cost, and other concerns.","Many recent advances in causal inference and machine learning target estimation of causal contrast functions such as CATE, which is sufficient for optimizing decisions and can adapt to potentially smoother structure.","We develop a dynamic generalization of the R-learner (Nie and Wager 2021, Lewis and Syrgkanis 2021) for estimating and optimizing the difference of $Q^\\pi$-functions, $Q^\\pi(s,1)-Q^\\pi(s,0)$ (which can be used to optimize multiple-valued actions).","We leverage orthogonal estimation to improve convergence rates in the presence of slower nuisance estimation rates and prove consistency of policy optimization under a margin condition.","The method can leverage black-box nuisance estimators of the $Q$-function and behavior policy to target estimation of a more structured $Q$-function contrast."],"url":"http://arxiv.org/abs/2406.08697v1","category":"stat.ML"}
{"created":"2024-06-12 23:34:23","title":"On the archetypal `flavours', indices and teleconnections of ENSO revealed by global sea surface temperatures","abstract":"El Ni\\~no-Southern Oscillation global (ENSO) imprint on sea surface temperature comes in many guises. To identify its tropical fingerprints and impacts on the rest of the climate system, we propose a global approach based on archetypal analysis (AA), a pattern recognition method based on the identification of extreme configurations in the dataset under investigation. Relying on detrended sea surface temperature monthly anomalies over the 1982 to 2022 period, the technique recovers central and eastern Pacific ENSO types identified by more traditional methods and allows one to hierarchically add extra flavours and nuances to both persistent and transient phases of the phenomenon. Archetypal patterns found compare favorably to phase identification from K-means, fuzzy C-means and recently published network-based machine-learning algorithms. The AA implementation is modified for the identification of ENSO phases in sub-seasonal-to-seasonal prediction systems and complements current alert systems in characterising the diversity of ENSO and its teleconnections. Tropical and extra-tropical teleconnection composites from various oceanic and atmospheric fields derived from the analysis are shown to be robust and physically relevant. Extending AA to sub-surface ocean fields improves the discrimination between phases when the characterisation of ENSO based on sea surface temperature is uncertain. We show that AA on detrended sea-level monthly anomalies provides a clearer expression of ENSO types.","sentences":["El Ni\\~no-Southern Oscillation global (ENSO) imprint on sea surface temperature comes in many guises.","To identify its tropical fingerprints and impacts on the rest of the climate system, we propose a global approach based on archetypal analysis (AA), a pattern recognition method based on the identification of extreme configurations in the dataset under investigation.","Relying on detrended sea surface temperature monthly anomalies over the 1982 to 2022 period, the technique recovers central and eastern Pacific ENSO types identified by more traditional methods and allows one to hierarchically add extra flavours and nuances to both persistent and transient phases of the phenomenon.","Archetypal patterns found compare favorably to phase identification from K-means, fuzzy C-means and recently published network-based machine-learning algorithms.","The AA implementation is modified for the identification of ENSO phases in sub-seasonal-to-seasonal prediction systems and complements current alert systems in characterising the diversity of ENSO and its teleconnections.","Tropical and extra-tropical teleconnection composites from various oceanic and atmospheric fields derived from the analysis are shown to be robust and physically relevant.","Extending AA to sub-surface ocean fields improves the discrimination between phases when the characterisation of ENSO based on sea surface temperature is uncertain.","We show that AA on detrended sea-level monthly anomalies provides a clearer expression of ENSO types."],"url":"http://arxiv.org/abs/2406.08694v1","category":"physics.ao-ph"}
{"created":"2024-06-12 23:29:12","title":"Infinity inner products and open Gromov--Witten invariants","abstract":"The open Gromov--Witten (OGW) potential is a function from the set of weak bounding cochains on a closed Lagrangian in a closed symplectic manifold to the Novikov ring. Existing definitions of the OGW potential assume that the ground field of the Novikov ring is either $\\mathbb{R}$ or $\\mathbb{C}$. In this paper, we give an alternate definition of the OGW potential in the pearly model for Lagrangian Floer theory which yields an invariant valued in the Novikov ring over any field of characteristic zero. We work under simplifying regularity hypotheses which are satisfied, for instance, by any monotone Lagrangian. Our OGW potential is defined in terms of an appropriate weakening of a strictly cyclic pairing on a curved $A_{\\infty}$-algebra, which can be thought of as a version of a proper Calabi--Yau structure. Such a structure is obtained by constructing a version of the cyclic open-closed map on the pearly Lagrangian Floer cochain complex. We also explain an analogue of our construction in de Rham cohomology, and show that it recovers the OGW potential constructed by Solomon and Tukachinsky.","sentences":["The open Gromov--Witten (OGW) potential is a function from the set of weak bounding cochains on a closed Lagrangian in a closed symplectic manifold to the Novikov ring.","Existing definitions of the OGW potential assume that the ground field of the Novikov ring is either $\\mathbb{R}$ or $\\mathbb{C}$. In this paper, we give an alternate definition of the OGW potential in the pearly model for Lagrangian Floer theory which yields an invariant valued in the Novikov ring over any field of characteristic zero.","We work under simplifying regularity hypotheses which are satisfied, for instance, by any monotone Lagrangian.","Our OGW potential is defined in terms of an appropriate weakening of a strictly cyclic pairing on a curved $A_{\\infty}$-algebra, which can be thought of as a version of a proper Calabi--Yau structure.","Such a structure is obtained by constructing a version of the cyclic open-closed map on the pearly Lagrangian Floer cochain complex.","We also explain an analogue of our construction in de Rham cohomology, and show that it recovers the OGW potential constructed by Solomon and Tukachinsky."],"url":"http://arxiv.org/abs/2406.08693v1","category":"math.SG"}
{"created":"2024-06-12 22:58:45","title":"Opportunities in deep learning methods development for computational biology","abstract":"Advances in molecular technologies underlie an enormous growth in the size of data sets pertaining to biology and biomedicine. These advances parallel those in the deep learning subfield of machine learning. Components in the differentiable programming toolbox that makes deep learning possible are allowing computer scientists to address an increasingly large array of problems with flexible and effective tools. However many of these tools have not fully proliferated into the computational biology and bioinformatics fields. In this perspective we survey some of these advances and highlight exemplary examples of their utilization in the biosciences, with the goal of increasing awareness among practitioners of emerging opportunities to blend expert knowledge with newly emerging deep learning architectural tools.","sentences":["Advances in molecular technologies underlie an enormous growth in the size of data sets pertaining to biology and biomedicine.","These advances parallel those in the deep learning subfield of machine learning.","Components in the differentiable programming toolbox that makes deep learning possible are allowing computer scientists to address an increasingly large array of problems with flexible and effective tools.","However many of these tools have not fully proliferated into the computational biology and bioinformatics fields.","In this perspective we survey some of these advances and highlight exemplary examples of their utilization in the biosciences, with the goal of increasing awareness among practitioners of emerging opportunities to blend expert knowledge with newly emerging deep learning architectural tools."],"url":"http://arxiv.org/abs/2406.08686v1","category":"q-bio.QM"}
{"created":"2024-06-12 22:49:45","title":"FIP-GNN: Graph neural networks for scalable prediction of grain-level fatigue indicator parameters","abstract":"High-cycle fatigue is a critical performance metric of structural alloys for many applications. The high cost, time, and labor involved in experimental fatigue testing call for efficient and accurate computer models of fatigue life. We present graph neural networks for polycrystals that, for the first time, can (i) predict fatigue indicator parameters -- grain-level responses to cyclic loading well beyond monotonic elastic and inelastic regimes reported in literature; and (ii) generalize these predictions to large microstructure volume elements with grain populations well beyond those used in training. These advances can make significant contributions to statistically rigorous and computationally efficient modeling of high-cycle fatigue -- a long-standing challenge in the field.","sentences":["High-cycle fatigue is a critical performance metric of structural alloys for many applications.","The high cost, time, and labor involved in experimental fatigue testing call for efficient and accurate computer models of fatigue life.","We present graph neural networks for polycrystals that, for the first time, can (i) predict fatigue indicator parameters -- grain-level responses to cyclic loading well beyond monotonic elastic and inelastic regimes reported in literature; and (ii) generalize these predictions to large microstructure volume elements with grain populations well beyond those used in training.","These advances can make significant contributions to statistically rigorous and computationally efficient modeling of high-cycle fatigue -- a long-standing challenge in the field."],"url":"http://arxiv.org/abs/2406.08682v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-12 22:48:19","title":"Quantitative determination of twist angle and strain in Van der Waals moir\u00e9 superlattices","abstract":"Scanning probe techniques are popular, non-destructive ways to visualize the real space structure of Van der Waals moir\\'es. The high lateral spatial resolution provided by these techniques enables extracting the moir\\'e lattice vectors from a scanning probe image. We have found that the extracted values, while precise, are not necessarily accurate. Scan-to-scan variations in the behavior of the piezos which drive the scanning probe, and thermally-driven slow relative drift between probe and sample, produce systematic errors in the extraction of lattice vectors. In this Letter, we identify the errors and provide a protocol to correct for them. Applying this protocol to an ensemble of ten successive scans of near-magic-angle twisted bilayer graphene, we are able to reduce our errors in extracting lattice vectors to less than 1%. This translates to extracting twist angles with a statistical uncertainty less than 0.001{\\deg} and uniaxial heterostrain with uncertainty on the order of 0.002%.","sentences":["Scanning probe techniques are popular, non-destructive ways to visualize the real space structure of Van der Waals moir\\'es.","The high lateral spatial resolution provided by these techniques enables extracting the moir\\'e lattice vectors from a scanning probe image.","We have found that the extracted values, while precise, are not necessarily accurate.","Scan-to-scan variations in the behavior of the piezos which drive the scanning probe, and thermally-driven slow relative drift between probe and sample, produce systematic errors in the extraction of lattice vectors.","In this Letter, we identify the errors and provide a protocol to correct for them.","Applying this protocol to an ensemble of ten successive scans of near-magic-angle twisted bilayer graphene, we are able to reduce our errors in extracting lattice vectors to less than 1%.","This translates to extracting twist angles with a statistical uncertainty less than 0.001{\\deg} and uniaxial heterostrain with uncertainty on the order of 0.002%."],"url":"http://arxiv.org/abs/2406.08681v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-12 22:36:37","title":"Forming the Trappist-1 system in two steps during the recession of the disc inner edge","abstract":"Trappist-1 hosts 7 planets where period ratios of neighbouring pairs are close to the 8:5, 5:3, 3:2, 3:2, 4:3, and 3:2 ratios in increasing distance from the star. The Laplace angles associated with neighbouring triplets are observed to be librating, proving the resonant nature of the system. This compact, resonant configuration is a manifest sign of disc-driven migration; however, the preferred outcome of such evolution is the establishment of first-order resonances, not the high-order resonances observed in the inner system. Here, we explain the observed orbital configuration in a model that is largely independent on the specific disc migration and orbital circularisation efficiencies. Together with migration, the two key elements of our model are: i) the inner border of the protoplanetary disc receded with time; and ii) the system was initially separated in two sub-systems. Specifically, the inner b, c d and e planets were initially placed in a 3:2 resonance chain and then evolved to the 8:5 -- 5:3 commensurability between planets b, c and d under the effect of the recession of the inner edge of the disc, whereas the outer planets migrated to the inner edge at a later time, establishing the remaining resonances. Our results pivot on the dynamical role of the presently unobservable recession of the inner edge of protoplanetary discs. They also reveal the role of recurring phases of convergent migration followed by resonant repulsion with associated orbital circularisation when resonant chains interact with migration barriers.","sentences":["Trappist-1 hosts 7 planets where period ratios of neighbouring pairs are close to the 8:5, 5:3, 3:2, 3:2, 4:3, and 3:2 ratios in increasing distance from the star.","The Laplace angles associated with neighbouring triplets are observed to be librating, proving the resonant nature of the system.","This compact, resonant configuration is a manifest sign of disc-driven migration; however, the preferred outcome of such evolution is the establishment of first-order resonances, not the high-order resonances observed in the inner system.","Here, we explain the observed orbital configuration in a model that is largely independent on the specific disc migration and orbital circularisation efficiencies.","Together with migration, the two key elements of our model are: i) the inner border of the protoplanetary disc receded with time; and ii) the system was initially separated in two sub-systems.","Specifically, the inner b, c d and e planets were initially placed in a 3:2 resonance chain and then evolved to the 8:5 -- 5:3 commensurability between planets b, c and d under the effect of the recession of the inner edge of the disc, whereas the outer planets migrated to the inner edge at a later time, establishing the remaining resonances.","Our results pivot on the dynamical role of the presently unobservable recession of the inner edge of protoplanetary discs.","They also reveal the role of recurring phases of convergent migration followed by resonant repulsion with associated orbital circularisation when resonant chains interact with migration barriers."],"url":"http://arxiv.org/abs/2406.08677v1","category":"astro-ph.EP"}
{"created":"2024-06-12 21:56:47","title":"Dark Neutrino Moments From Light Loops","abstract":"Active and sterile neutrinos may acquire \"dark moments\" via one-loop diagrams with a massless dark photon and new light particles in the loop. Due to the kinetic mixing between the dark photon and the Standard Model photon, neutrinos would obtain effective electromagnetic moments. This mechanism allows for enhanced electromagnetic moments that can evade constraints on particles directly charged under electromagnetism. We show that in a wide region of parameter space, the model features testable predictions for the anapole and magnetic moment of active and sterile neutrinos with dark matter direct detection experiments sensitive to the solar neutrino flux.","sentences":["Active and sterile neutrinos may acquire \"dark moments\" via one-loop diagrams with a massless dark photon and new light particles in the loop.","Due to the kinetic mixing between the dark photon and the Standard Model photon, neutrinos would obtain effective electromagnetic moments.","This mechanism allows for enhanced electromagnetic moments that can evade constraints on particles directly charged under electromagnetism.","We show that in a wide region of parameter space, the model features testable predictions for the anapole and magnetic moment of active and sterile neutrinos with dark matter direct detection experiments sensitive to the solar neutrino flux."],"url":"http://arxiv.org/abs/2406.08663v1","category":"hep-ph"}
{"created":"2024-06-12 21:47:33","title":"On Fox's trapezoidal conjecture","abstract":"We investigate Fox's trapezoidal conjecture for alternating links. We show that it holds for diagrammatic Murasugi sums of special alternating links, where all sums involved have length less than three (which includes diagrammatic plumbing). It also holds for links containing a large twist region, which we call twist-concentrated. Furthermore, we show some weaker inequalities between consecutive coefficients of the Alexander polynomial of an alternating 3-braid closure, and extend this to arbitrary alternating links. We then study an extension of the trapezoidal conjecture due to Hirasawa and Murasugi, which states that the stable length of the Alexander polynomial of an alternating link can be bounded from above using the signature. We estabilish this and determine when equality holds for diagrammatic Murasugi sums of special alternating knots where each sum has length less than three, and also for twist-concentrated 3-braids. Finally, we study the behavior of the Hirasawa-Murasugi inequality under concordance.","sentences":["We investigate Fox's trapezoidal conjecture for alternating links.","We show that it holds for diagrammatic Murasugi sums of special alternating links, where all sums involved have length less than three (which includes diagrammatic plumbing).","It also holds for links containing a large twist region, which we call twist-concentrated.","Furthermore, we show some weaker inequalities between consecutive coefficients of the Alexander polynomial of an alternating 3-braid closure, and extend this to arbitrary alternating links.","We then study an extension of the trapezoidal conjecture due to Hirasawa and Murasugi, which states that the stable length of the Alexander polynomial of an alternating link can be bounded from above using the signature.","We estabilish this and determine when equality holds for diagrammatic Murasugi sums of special alternating knots where each sum has length less than three, and also for twist-concentrated 3-braids.","Finally, we study the behavior of the Hirasawa-Murasugi inequality under concordance."],"url":"http://arxiv.org/abs/2406.08662v1","category":"math.GT"}
{"created":"2024-06-12 21:47:19","title":"Towards minimal self-testing of qubit states and measurements in prepare-and-measure scenarios","abstract":"Self-testing is a promising approach to certifying quantum states or measurements. Originally, it relied solely on the outcome statistics of the measurements involved in a device-independent (DI) setup. Extra physical assumptions about the system make the setup semi-DI. In the latter approach, we consider a prepare-and-measure scenario in which the dimension of the mediating particle is assumed to be two. In a setup involving four (three) preparations and three (two) projective measurements in addition to the target, we exemplify how to self-test any four- (three-) outcome extremal positive operator-valued measure using a linear witness. One of our constructions also achieves self-testing of any number of states with the help of as many projective measurements as the dimensionality of the space spanned by the corresponding Bloch vectors. These constructions are conjectured to be minimal in terms of the number of preparations and measurements required. In addition, we implement one of our prepare-and-measure constructions on IBM and IonQ quantum processors and certify the existence of a complex qubit Hilbert space based on the data obtained from these experiments.","sentences":["Self-testing is a promising approach to certifying quantum states or measurements.","Originally, it relied solely on the outcome statistics of the measurements involved in a device-independent (DI) setup.","Extra physical assumptions about the system make the setup semi-DI.","In the latter approach, we consider a prepare-and-measure scenario in which the dimension of the mediating particle is assumed to be two.","In a setup involving four (three) preparations and three (two) projective measurements in addition to the target, we exemplify how to self-test any four- (three-) outcome extremal positive operator-valued measure using a linear witness.","One of our constructions also achieves self-testing of any number of states with the help of as many projective measurements as the dimensionality of the space spanned by the corresponding Bloch vectors.","These constructions are conjectured to be minimal in terms of the number of preparations and measurements required.","In addition, we implement one of our prepare-and-measure constructions on IBM and IonQ quantum processors and certify the existence of a complex qubit Hilbert space based on the data obtained from these experiments."],"url":"http://arxiv.org/abs/2406.08661v1","category":"quant-ph"}
{"created":"2024-06-12 21:33:22","title":"Large Stepsize Gradient Descent for Non-Homogeneous Two-Layer Networks: Margin Improvement and Fast Optimization","abstract":"The typical training of neural networks using large stepsize gradient descent (GD) under the logistic loss often involves two distinct phases, where the empirical risk oscillates in the first phase but decreases monotonically in the second phase. We investigate this phenomenon in two-layer networks that satisfy a near-homogeneity condition. We show that the second phase begins once the empirical risk falls below a certain threshold, dependent on the stepsize. Additionally, we show that the normalized margin grows nearly monotonically in the second phase, demonstrating an implicit bias of GD in training non-homogeneous predictors. If the dataset is linearly separable and the derivative of the activation function is bounded away from zero, we show that the average empirical risk decreases, implying that the first phase must stop in finite steps. Finally, we demonstrate that by choosing a suitably large stepsize, GD that undergoes this phase transition is more efficient than GD that monotonically decreases the risk. Our analysis applies to networks of any width, beyond the well-known neural tangent kernel and mean-field regimes.","sentences":["The typical training of neural networks using large stepsize gradient descent (GD) under the logistic loss often involves two distinct phases, where the empirical risk oscillates in the first phase but decreases monotonically in the second phase.","We investigate this phenomenon in two-layer networks that satisfy a near-homogeneity condition.","We show that the second phase begins once the empirical risk falls below a certain threshold, dependent on the stepsize.","Additionally, we show that the normalized margin grows nearly monotonically in the second phase, demonstrating an implicit bias of GD in training non-homogeneous predictors.","If the dataset is linearly separable and the derivative of the activation function is bounded away from zero, we show that the average empirical risk decreases, implying that the first phase must stop in finite steps.","Finally, we demonstrate that by choosing a suitably large stepsize, GD that undergoes this phase transition is more efficient than GD that monotonically decreases the risk.","Our analysis applies to networks of any width, beyond the well-known neural tangent kernel and mean-field regimes."],"url":"http://arxiv.org/abs/2406.08654v1","category":"stat.ML"}
{"created":"2024-06-12 21:18:14","title":"MOTI$\\mathcal{VE}$: A Drug-Target Interaction Graph For Inductive Link Prediction","abstract":"Drug-target interaction (DTI) prediction is crucial for identifying new therapeutics and detecting mechanisms of action. While structure-based methods accurately model physical interactions between a drug and its protein target, cell-based assays such as Cell Painting can better capture complex DTI interactions. This paper introduces MOTI$\\mathcal{VE}$, a Morphological cOmpound Target Interaction Graph dataset that comprises Cell Painting features for $11,000$ genes and $3,600$ compounds along with their relationships extracted from seven publicly available databases. We provide random, cold-source (new drugs), and cold-target (new genes) data splits to enable rigorous evaluation under realistic use cases. Our benchmark results show that graph neural networks that use Cell Painting features consistently outperform those that learn from graph structure alone, feature-based models, and topological heuristics. MOTI$\\mathcal{VE}$ accelerates both graph ML research and drug discovery by promoting the development of more reliable DTI prediction models. MOTI$\\mathcal{VE}$ resources are available at https://github.com/carpenter-singh-lab/motive.","sentences":["Drug-target interaction (DTI) prediction is crucial for identifying new therapeutics and detecting mechanisms of action.","While structure-based methods accurately model physical interactions between a drug and its protein target, cell-based assays such as Cell Painting can better capture complex DTI interactions.","This paper introduces MOTI$\\mathcal{VE}$, a Morphological cOmpound Target Interaction Graph dataset that comprises Cell Painting features for $11,000$ genes and $3,600$ compounds along with their relationships extracted from seven publicly available databases.","We provide random, cold-source (new drugs), and cold-target (new genes) data splits to enable rigorous evaluation under realistic use cases.","Our benchmark results show that graph neural networks that use Cell Painting features consistently outperform those that learn from graph structure alone, feature-based models, and topological heuristics.","MOTI$\\mathcal{VE}$ accelerates both graph ML research and drug discovery by promoting the development of more reliable DTI prediction models.","MOTI$\\mathcal{VE}$ resources are available at https://github.com/carpenter-singh-lab/motive."],"url":"http://arxiv.org/abs/2406.08649v1","category":"cs.LG"}
{"created":"2024-06-12 21:09:00","title":"ODIN: Identifying Protoclusters and Cosmic Filaments Traced by Ly$\u03b1$-emitting Galaxies","abstract":"To understand the formation and evolution of massive cosmic structures, studying them at high redshift, in the epoch when they formed the majority of their mass is essential. The One-hundred-deg$^2$ DECam Imaging in Narrowbands (ODIN) survey is undertaking the widest-area narrowband program to date, to use Ly$\\alpha$-emitting galaxies (LAEs) to trace the large-scale structure (LSS) of the Universe at three cosmic epochs. In this work, we present results at $z$ = 3.1 based on early ODIN data in the COSMOS field. We identify and characterize protoclusters and cosmic filaments using multiple methods and discuss their strengths and weaknesses. We then compare our observations against the IllustrisTNG suite of cosmological hydrodynamical simulations. The two are in excellent agreement, with a similar number and angular size of structures identified above a specified density threshold. We are able to recover the simulated protoclusters with $\\log$(M$_{z=0}$/$M_\\odot$) $\\gtrsim$ 14.4 in $\\sim$ 60\\% of the cases. With these objects we show that the descendant masses of the protoclusters in our sample can be estimated purely based on our 2D measurements, finding a median $z$ = 0 mass of $\\sim10^{14.5}$M$_\\odot$. The lack of information on the radial extent of each protocluster introduces a $\\sim$0.4~dex uncertainty in its descendant mass. Finally, we show that the recovery of the cosmic web in the vicinity of protoclusters is both efficient and accurate. The similarity of our observations and the simulations imply that our structure selection is likewise robust and efficient, demonstrating that LAEs are reliable tracers of the LSS.","sentences":["To understand the formation and evolution of massive cosmic structures, studying them at high redshift, in the epoch when they formed the majority of their mass is essential.","The One-hundred-deg$^2$ DECam Imaging in Narrowbands (ODIN) survey is undertaking the widest-area narrowband program to date, to use Ly$\\alpha$-emitting galaxies (LAEs) to trace the large-scale structure (LSS) of the Universe at three cosmic epochs.","In this work, we present results at $z$ = 3.1 based on early ODIN data in the COSMOS field.","We identify and characterize protoclusters and cosmic filaments using multiple methods and discuss their strengths and weaknesses.","We then compare our observations against the IllustrisTNG suite of cosmological hydrodynamical simulations.","The two are in excellent agreement, with a similar number and angular size of structures identified above a specified density threshold.","We are able to recover the simulated protoclusters with $\\log$(M$_{z=0}$/$M_\\odot$) $\\gtrsim$ 14.4 in $\\sim$ 60\\% of the cases.","With these objects we show that the descendant masses of the protoclusters in our sample can be estimated purely based on our 2D measurements, finding a median $z$ = 0 mass of $\\sim10^{14.5}$M$_\\odot$. The lack of information on the radial extent of each protocluster introduces a $\\sim$0.4~dex uncertainty in its descendant mass.","Finally, we show that the recovery of the cosmic web in the vicinity of protoclusters is both efficient and accurate.","The similarity of our observations and the simulations imply that our structure selection is likewise robust and efficient, demonstrating that LAEs are reliable tracers of the LSS."],"url":"http://arxiv.org/abs/2406.08645v1","category":"astro-ph.GA"}
{"created":"2024-06-12 20:47:17","title":"Towards Integrating Personal Knowledge into Test-Time Predictions","abstract":"Machine learning (ML) models can make decisions based on large amounts of data, but they can be missing personal knowledge available to human users about whom predictions are made. For example, a model trained to predict psychiatric outcomes may know nothing about a patient's social support system, and social support may look different for different patients. In this work, we introduce the problem of human feature integration, which provides a way to incorporate important personal-knowledge from users without domain expertise into ML predictions. We characterize this problem through illustrative user stories and comparisons to existing approaches; we formally describe this problem in a way that paves the ground for future technical solutions; and we provide a proof-of-concept study of a simple version of a solution to this problem in a semi-realistic setting.","sentences":["Machine learning (ML) models can make decisions based on large amounts of data, but they can be missing personal knowledge available to human users about whom predictions are made.","For example, a model trained to predict psychiatric outcomes may know nothing about a patient's social support system, and social support may look different for different patients.","In this work, we introduce the problem of human feature integration, which provides a way to incorporate important personal-knowledge from users without domain expertise into ML predictions.","We characterize this problem through illustrative user stories and comparisons to existing approaches; we formally describe this problem in a way that paves the ground for future technical solutions; and we provide a proof-of-concept study of a simple version of a solution to this problem in a semi-realistic setting."],"url":"http://arxiv.org/abs/2406.08636v1","category":"cs.LG"}
{"created":"2024-06-12 20:21:09","title":"Empirical Evidence That There Is No Such Thing As A Validated Prediction Model","abstract":"Background: External validations are essential to assess clinical prediction models (CPMs) before deployment. Apart from model misspecification, differences in patient population and other factors influence a model's AUC (c-statistic). We aimed to quantify variation in AUCs across external validation studies and adjust expectations of a model's performance in a new setting.   Methods: The Tufts-PACE CPM Registry contains CPMs for cardiovascular disease prognosis. We analyzed the AUCs of 469 CPMs with a total of 1,603 external validations. For each CPM, we performed a random effects meta-analysis to estimate the between-study standard deviation $\\tau$ among the AUCs. Since the majority of these meta-analyses has only a handful of validations, this leads to very poor estimates of $\\tau$. So, we estimated a log normal distribution of $\\tau$ across all CPMs and used this as an empirical prior. We compared this empirical Bayesian approach with frequentist meta-analyses using cross-validation.   Results: The 469 CPMs had a median of 2 external validations (IQR: [1-3]). The estimated distribution of $\\tau$ had a mean of 0.055 and a standard deviation of 0.015. If $\\tau$ = 0.05, the 95% prediction interval for the AUC in a new setting is at least +/- 0.1, regardless of the number of validations. Frequentist methods underestimate the uncertainty about the AUC in a new setting. Accounting for $\\tau$ in a Bayesian approach achieved near nominal coverage.   Conclusion: Due to large heterogeneity among the validated AUC values of a CPM, there is great irreducible uncertainty in predicting the AUC in a new setting. This uncertainty is underestimated by existing methods. The proposed empirical Bayes approach addresses this problem which merits wide application in judging the validity of prediction models.","sentences":["Background: External validations are essential to assess clinical prediction models (CPMs) before deployment.","Apart from model misspecification, differences in patient population and other factors influence a model's AUC (c-statistic).","We aimed to quantify variation in AUCs across external validation studies and adjust expectations of a model's performance in a new setting.   ","Methods: The Tufts-PACE CPM Registry contains CPMs for cardiovascular disease prognosis.","We analyzed the AUCs of 469 CPMs with a total of 1,603 external validations.","For each CPM, we performed a random effects meta-analysis to estimate the between-study standard deviation $\\tau$ among the AUCs.","Since the majority of these meta-analyses has only a handful of validations, this leads to very poor estimates of $\\tau$. So, we estimated a log normal distribution of $\\tau$ across all CPMs and used this as an empirical prior.","We compared this empirical Bayesian approach with frequentist meta-analyses using cross-validation.   ","Results:","The 469 CPMs had a median of 2 external validations (IQR: [1-3]).","The estimated distribution of $\\tau$ had a mean of 0.055 and a standard deviation of 0.015.","If $\\tau$ = 0.05, the 95% prediction interval for the AUC in a new setting is at least +/- 0.1, regardless of the number of validations.","Frequentist methods underestimate the uncertainty about the AUC in a new setting.","Accounting for $\\tau$ in a Bayesian approach achieved near nominal coverage.   ","Conclusion: Due to large heterogeneity among the validated AUC values of a CPM, there is great irreducible uncertainty in predicting the AUC in a new setting.","This uncertainty is underestimated by existing methods.","The proposed empirical Bayes approach addresses this problem which merits wide application in judging the validity of prediction models."],"url":"http://arxiv.org/abs/2406.08628v1","category":"stat.ME"}
{"created":"2024-06-12 20:15:29","title":"Safety-Driven Battery Charging: A Fisher Information-guided Adaptive MPC with Real-time Parameter Identification","abstract":"Lithium-ion (Li-ion) batteries are ubiquitous in modern energy storage systems, highlighting the critical need to comprehend and optimize their performance. Yet, battery models often exhibit poor parameter identifiability which hinders the development of effective battery management strategies and impacts their overall performance, longevity, and safety. This manuscript explores the integration of Fisher Information (FI) theory with Model Predictive Control (MPC) for battery charging. The study addresses the inherent hurdles in accurately estimating battery model parameters due to nonlinear dynamics and uncertainty. Our proposed method aims to ensure safe battery charging and enhance real-time parameter estimation capabilities by leveraging adaptive control strategies guided by FI metrics. Simulation results underscore the effectiveness of our approach in mitigating parameter identifiability issues, offering promising solutions for improving the control of batteries during safe charging process.","sentences":["Lithium-ion (Li-ion) batteries are ubiquitous in modern energy storage systems, highlighting the critical need to comprehend and optimize their performance.","Yet, battery models often exhibit poor parameter identifiability which hinders the development of effective battery management strategies and impacts their overall performance, longevity, and safety.","This manuscript explores the integration of Fisher Information (FI) theory with Model Predictive Control (MPC) for battery charging.","The study addresses the inherent hurdles in accurately estimating battery model parameters due to nonlinear dynamics and uncertainty.","Our proposed method aims to ensure safe battery charging and enhance real-time parameter estimation capabilities by leveraging adaptive control strategies guided by FI metrics.","Simulation results underscore the effectiveness of our approach in mitigating parameter identifiability issues, offering promising solutions for improving the control of batteries during safe charging process."],"url":"http://arxiv.org/abs/2406.08626v1","category":"eess.SY"}
{"created":"2024-06-12 20:15:00","title":"FSBI: Deepfakes Detection with Frequency Enhanced Self-Blended Images","abstract":"Advances in deepfake research have led to the creation of almost perfect manipulations undetectable by human eyes and some deepfakes detection tools. Recently, several techniques have been proposed to differentiate deepfakes from realistic images and videos. This paper introduces a Frequency Enhanced Self-Blended Images (FSBI) approach for deepfakes detection. This proposed approach utilizes Discrete Wavelet Transforms (DWT) to extract discriminative features from the self-blended images (SBI) to be used for training a convolutional network architecture model. The SBIs blend the image with itself by introducing several forgery artifacts in a copy of the image before blending it. This prevents the classifier from overfitting specific artifacts by learning more generic representations. These blended images are then fed into the frequency features extractor to detect artifacts that can not be detected easily in the time domain. The proposed approach has been evaluated on FF++ and Celeb-DF datasets and the obtained results outperformed the state-of-the-art techniques with the cross-dataset evaluation protocol.","sentences":["Advances in deepfake research have led to the creation of almost perfect manipulations undetectable by human eyes and some deepfakes detection tools.","Recently, several techniques have been proposed to differentiate deepfakes from realistic images and videos.","This paper introduces a Frequency Enhanced Self-Blended Images (FSBI) approach for deepfakes detection.","This proposed approach utilizes Discrete Wavelet Transforms (DWT) to extract discriminative features from the self-blended images (SBI) to be used for training a convolutional network architecture model.","The SBIs blend the image with itself by introducing several forgery artifacts in a copy of the image before blending it.","This prevents the classifier from overfitting specific artifacts by learning more generic representations.","These blended images are then fed into the frequency features extractor to detect artifacts that can not be detected easily in the time domain.","The proposed approach has been evaluated on FF++ and Celeb-DF datasets and the obtained results outperformed the state-of-the-art techniques with the cross-dataset evaluation protocol."],"url":"http://arxiv.org/abs/2406.08625v1","category":"cs.CV"}
{"created":"2024-06-12 20:01:54","title":"A thermodynamic approach to adhesion and deformation of DNA-bound droplets","abstract":"Here we derive and experimentally test a free energy functional that captures the adhesion of DNA-coated emulsion droplets. Generalizing previous approaches, the theory combines important energetic and entropic effects of microscopic DNA mechanics and droplet elasticity. It simultaneously predicts adhesion size, morphology, and binder concentration as a function of experimental control parameters. Notably, droplets transition from undeformed binding to flat droplet interfaces at a characteristic DNA coverage. These equilibrium predictions agree quantitatively with experiments on droplet-substrate and droplet-droplet binding, revealing a weak effective binding strength of $3.7\\pm0.3\\text{k}_{\\text{B}}\\text{T}$ owing to entropic costs. Our results open the path to rich design strategies for making colloidal architectures.","sentences":["Here we derive and experimentally test a free energy functional that captures the adhesion of DNA-coated emulsion droplets.","Generalizing previous approaches, the theory combines important energetic and entropic effects of microscopic DNA mechanics and droplet elasticity.","It simultaneously predicts adhesion size, morphology, and binder concentration as a function of experimental control parameters.","Notably, droplets transition from undeformed binding to flat droplet interfaces at a characteristic DNA coverage.","These equilibrium predictions agree quantitatively with experiments on droplet-substrate and droplet-droplet binding, revealing a weak effective binding strength of $3.7\\pm0.3\\text{k}_{\\text{B}}\\text{T}$ owing to entropic costs.","Our results open the path to rich design strategies for making colloidal architectures."],"url":"http://arxiv.org/abs/2406.08618v1","category":"cond-mat.soft"}
{"created":"2024-06-13 17:57:10","title":"Reflecting on the State of Rehearsal-free Continual Learning with Pretrained Models","abstract":"With the advent and recent ubiquity of foundation models, continual learning (CL) has recently shifted from continual training from scratch to the continual adaptation of pretrained models, seeing particular success on rehearsal-free CL benchmarks (RFCL). To achieve this, most proposed methods adapt and restructure parameter-efficient finetuning techniques (PEFT) to suit the continual nature of the problem. Based most often on input-conditional query-mechanisms or regularizations on top of prompt- or adapter-based PEFT, these PEFT-style RFCL (P-RFCL) approaches report peak performances; often convincingly outperforming existing CL techniques. However, on the other end, critical studies have recently highlighted competitive results by training on just the first task or via simple non-parametric baselines. Consequently, questions arise about the relationship between methodological choices in P-RFCL and their reported high benchmark scores. In this work, we tackle these questions to better understand the true drivers behind strong P-RFCL performances, their placement w.r.t. recent first-task adaptation studies, and their relation to preceding CL standards such as EWC or SI. In particular, we show: (1) P-RFCL techniques relying on input-conditional query mechanisms work not because, but rather despite them by collapsing towards standard PEFT shortcut solutions. (2) Indeed, we show how most often, P-RFCL techniques can be matched by a simple and lightweight PEFT baseline. (3) Using this baseline, we identify the implicit bound on tunable parameters when deriving RFCL approaches from PEFT methods as a potential denominator behind P-RFCL efficacy. Finally, we (4) better disentangle continual versus first-task adaptation, and (5) motivate standard RFCL techniques s.a. EWC or SI in light of recent P-RFCL methods.","sentences":["With the advent and recent ubiquity of foundation models, continual learning (CL) has recently shifted from continual training from scratch to the continual adaptation of pretrained models, seeing particular success on rehearsal-free CL benchmarks (RFCL).","To achieve this, most proposed methods adapt and restructure parameter-efficient finetuning techniques (PEFT) to suit the continual nature of the problem.","Based most often on input-conditional query-mechanisms or regularizations on top of prompt- or adapter-based PEFT, these PEFT-style RFCL (P-RFCL) approaches report peak performances; often convincingly outperforming existing CL techniques.","However, on the other end, critical studies have recently highlighted competitive results by training on just the first task or via simple non-parametric baselines.","Consequently, questions arise about the relationship between methodological choices in P-RFCL and their reported high benchmark scores.","In this work, we tackle these questions to better understand the true drivers behind strong P-RFCL performances, their placement w.r.t.","recent first-task adaptation studies, and their relation to preceding CL standards such as EWC or SI.","In particular, we show: (1) P-RFCL techniques relying on input-conditional query mechanisms work not because, but rather despite them by collapsing towards standard PEFT shortcut solutions.","(2) Indeed, we show how most often, P-RFCL techniques can be matched by a simple and lightweight PEFT baseline.","(3) Using this baseline, we identify the implicit bound on tunable parameters when deriving RFCL approaches from PEFT methods as a potential denominator behind P-RFCL efficacy.","Finally, we (4) better disentangle continual versus first-task adaptation, and (5) motivate standard RFCL techniques s.a. EWC or SI in light of recent P-RFCL methods."],"url":"http://arxiv.org/abs/2406.09384v1","category":"cs.LG"}
{"created":"2024-06-13 17:55:18","title":"A continuous model of transportation in the Heisenberg group","abstract":"We adapt to the Heisenberg group a minimization problem with horizontal divergence-type constraint. We investigate its dual formulation and its connection with the congested optimal transport problem, for $1<p<+\\infty$, and the Monge-Kantorovich problem, in the limit case $p=1$.","sentences":["We adapt to the Heisenberg group a minimization problem with horizontal divergence-type constraint.","We investigate its dual formulation and its connection with the congested optimal transport problem, for $1<p<+\\infty$, and the Monge-Kantorovich problem, in the limit case $p=1$."],"url":"http://arxiv.org/abs/2406.09380v1","category":"math.AP"}
{"created":"2024-06-13 17:53:47","title":"Learning conditional distributions on continuous spaces","abstract":"We investigate sample-based learning of conditional distributions on multi-dimensional unit boxes, allowing for different dimensions of the feature and target spaces. Our approach involves clustering data near varying query points in the feature space to create empirical measures in the target space. We employ two distinct clustering schemes: one based on a fixed-radius ball and the other on nearest neighbors. We establish upper bounds for the convergence rates of both methods and, from these bounds, deduce optimal configurations for the radius and the number of neighbors. We propose to incorporate the nearest neighbors method into neural network training, as our empirical analysis indicates it has better performance in practice. For efficiency, our training process utilizes approximate nearest neighbors search with random binary space partitioning. Additionally, we employ the Sinkhorn algorithm and a sparsity-enforced transport plan. Our empirical findings demonstrate that, with a suitably designed structure, the neural network has the ability to adapt to a suitable level of Lipschitz continuity locally. For reproducibility, our code is available at \\url{https://github.com/zcheng-a/LCD_kNN}.","sentences":["We investigate sample-based learning of conditional distributions on multi-dimensional unit boxes, allowing for different dimensions of the feature and target spaces.","Our approach involves clustering data near varying query points in the feature space to create empirical measures in the target space.","We employ two distinct clustering schemes: one based on a fixed-radius ball and the other on nearest neighbors.","We establish upper bounds for the convergence rates of both methods and, from these bounds, deduce optimal configurations for the radius and the number of neighbors.","We propose to incorporate the nearest neighbors method into neural network training, as our empirical analysis indicates it has better performance in practice.","For efficiency, our training process utilizes approximate nearest neighbors search with random binary space partitioning.","Additionally, we employ the Sinkhorn algorithm and a sparsity-enforced transport plan.","Our empirical findings demonstrate that, with a suitably designed structure, the neural network has the ability to adapt to a suitable level of Lipschitz continuity locally.","For reproducibility, our code is available at \\url{https://github.com/zcheng-a/LCD_kNN}."],"url":"http://arxiv.org/abs/2406.09375v1","category":"stat.ML"}
{"created":"2024-06-13 17:15:33","title":"ProxyLM: Predicting Language Model Performance on Multilingual Tasks via Proxy Models","abstract":"Performance prediction is a method to estimate the performance of multilingual language models (LMs), mitigating computational costs associated with model capacity and data for fine-tuning. Our paper introduces ProxyLM, a scalable framework for predicting LM performance using proxy models in multilingual tasks. These proxy models act as surrogates, approximating the performance of fine-tuned LMs on specific downstream natural language processing (NLP) tasks. By leveraging proxy models, ProxyLM significantly reduces computational overhead on task evaluations, achieving up to a 37.08x speedup compared to traditional methods, even with our smallest proxy models. Additionally, our methodology showcases adaptability to previously unseen languages in pre-trained LMs, outperforming the state-of-the-art performance by 1.89x as measured by root-mean-square-error (RMSE). This framework streamlines model selection, enabling efficient deployment and iterative LM enhancements without extensive computational resources.","sentences":["Performance prediction is a method to estimate the performance of multilingual language models (LMs), mitigating computational costs associated with model capacity and data for fine-tuning.","Our paper introduces ProxyLM, a scalable framework for predicting LM performance using proxy models in multilingual tasks.","These proxy models act as surrogates, approximating the performance of fine-tuned LMs on specific downstream natural language processing (NLP) tasks.","By leveraging proxy models, ProxyLM significantly reduces computational overhead on task evaluations, achieving up to a 37.08x speedup compared to traditional methods, even with our smallest proxy models.","Additionally, our methodology showcases adaptability to previously unseen languages in pre-trained LMs, outperforming the state-of-the-art performance by 1.89x as measured by root-mean-square-error (RMSE).","This framework streamlines model selection, enabling efficient deployment and iterative LM enhancements without extensive computational resources."],"url":"http://arxiv.org/abs/2406.09334v1","category":"cs.CL"}
{"created":"2024-06-13 15:57:49","title":"Compact Parallel Hash Tables on the GPU","abstract":"On the GPU, hash table operation speed is determined in large part by cache line efficiency, and state-of-the-art hashing schemes thus divide tables into cache line-sized buckets. This raises the question whether performance can be further improved by increasing the number of entries that fit in such buckets. Known compact hashing techniques have not yet been adapted to the massively parallel setting, nor have they been evaluated on the GPU. We consider a compact version of bucketed cuckoo hashing, and a version of compact iceberg hashing suitable for the GPU. We discuss the tables from a theoretical perspective, and provide an open source implementation of both schemes in CUDA for comparative benchmarking. In terms of performance, the state-of-the-art cuckoo hashing benefits from compactness on lookups and insertions (most experiments show at least 10-20% increase in throughput), and the iceberg table benefits significantly, to the point of being comparable to compact cuckoo hashing--while supporting performant dynamic operation.","sentences":["On the GPU, hash table operation speed is determined in large part by cache line efficiency, and state-of-the-art hashing schemes thus divide tables into cache line-sized buckets.","This raises the question whether performance can be further improved by increasing the number of entries that fit in such buckets.","Known compact hashing techniques have not yet been adapted to the massively parallel setting, nor have they been evaluated on the GPU.","We consider a compact version of bucketed cuckoo hashing, and a version of compact iceberg hashing suitable for the GPU.","We discuss the tables from a theoretical perspective, and provide an open source implementation of both schemes in CUDA for comparative benchmarking.","In terms of performance, the state-of-the-art cuckoo hashing benefits from compactness on lookups and insertions (most experiments show at least 10-20% increase in throughput), and the iceberg table benefits significantly, to the point of being comparable to compact cuckoo hashing--while supporting performant dynamic operation."],"url":"http://arxiv.org/abs/2406.09255v1","category":"cs.DS"}
{"created":"2024-06-13 14:06:12","title":"AdaRevD: Adaptive Patch Exiting Reversible Decoder Pushes the Limit of Image Deblurring","abstract":"Despite the recent progress in enhancing the efficacy of image deblurring, the limited decoding capability constrains the upper limit of State-Of-The-Art (SOTA) methods. This paper proposes a pioneering work, Adaptive Patch Exiting Reversible Decoder (AdaRevD), to explore their insufficient decoding capability. By inheriting the weights of the well-trained encoder, we refactor a reversible decoder which scales up the single-decoder training to multi-decoder training while remaining GPU memory-friendly. Meanwhile, we show that our reversible structure gradually disentangles high-level degradation degree and low-level blur pattern (residual of the blur image and its sharp counterpart) from compact degradation representation. Besides, due to the spatially-variant motion blur kernels, different blur patches have various deblurring difficulties. We further introduce a classifier to learn the degradation degree of image patches, enabling them to exit at different sub-decoders for speedup. Experiments show that our AdaRevD pushes the limit of image deblurring, e.g., achieving 34.60 dB in PSNR on GoPro dataset.","sentences":["Despite the recent progress in enhancing the efficacy of image deblurring, the limited decoding capability constrains the upper limit of State-Of-The-Art (SOTA) methods.","This paper proposes a pioneering work, Adaptive Patch Exiting Reversible Decoder (AdaRevD), to explore their insufficient decoding capability.","By inheriting the weights of the well-trained encoder, we refactor a reversible decoder which scales up the single-decoder training to multi-decoder training while remaining GPU memory-friendly.","Meanwhile, we show that our reversible structure gradually disentangles high-level degradation degree and low-level blur pattern (residual of the blur image and its sharp counterpart) from compact degradation representation.","Besides, due to the spatially-variant motion blur kernels, different blur patches have various deblurring difficulties.","We further introduce a classifier to learn the degradation degree of image patches, enabling them to exit at different sub-decoders for speedup.","Experiments show that our AdaRevD pushes the limit of image deblurring, e.g., achieving 34.60 dB in PSNR on GoPro dataset."],"url":"http://arxiv.org/abs/2406.09135v1","category":"cs.CV"}
{"created":"2024-06-13 12:54:02","title":"Impermanent Identifiers: Enhanced Source Code Comprehension and Refactoring","abstract":"In response to the prevailing challenges in contemporary software development, this article introduces an innovative approach to code augmentation centered around Impermanent Identifiers. The primary goal is to enhance the software development experience by introducing dynamic identifiers that adapt to changing contexts, facilitating more efficient interactions between developers and source code, ultimately advancing comprehension, maintenance, and collaboration in software development. Additionally, this study rigorously evaluates the adoption and acceptance of Impermanent Identifiers within the software development landscape. Through a comprehensive empirical examination, we investigate how developers perceive and integrate this approach into their daily programming practices, exploring perceived benefits, potential barriers, and factors influencing its adoption. In summary, this article charts a new course for code augmentation, proposing Impermanent Identifiers as its cornerstone while assessing their feasibility and acceptance among developers. This interdisciplinary research seeks to contribute to the continuous improvement of software development practices and the progress of code augmentation technology.","sentences":["In response to the prevailing challenges in contemporary software development, this article introduces an innovative approach to code augmentation centered around Impermanent Identifiers.","The primary goal is to enhance the software development experience by introducing dynamic identifiers that adapt to changing contexts, facilitating more efficient interactions between developers and source code, ultimately advancing comprehension, maintenance, and collaboration in software development.","Additionally, this study rigorously evaluates the adoption and acceptance of Impermanent Identifiers within the software development landscape.","Through a comprehensive empirical examination, we investigate how developers perceive and integrate this approach into their daily programming practices, exploring perceived benefits, potential barriers, and factors influencing its adoption.","In summary, this article charts a new course for code augmentation, proposing Impermanent Identifiers as its cornerstone while assessing their feasibility and acceptance among developers.","This interdisciplinary research seeks to contribute to the continuous improvement of software development practices and the progress of code augmentation technology."],"url":"http://arxiv.org/abs/2406.09066v1","category":"cs.SE"}
{"created":"2024-06-13 10:57:24","title":"Adaptive Temporal Motion Guided Graph Convolution Network for Micro-expression Recognition","abstract":"Micro-expressions serve as essential cues for understanding individuals' genuine emotional states. Recognizing micro-expressions attracts increasing research attention due to its various applications in fields such as business negotiation and psychotherapy. However, the intricate and transient nature of micro-expressions poses a significant challenge to their accurate recognition. Most existing works either neglect temporal dependencies or suffer from redundancy issues in clip-level recognition. In this work, we propose a novel framework for micro-expression recognition, named the Adaptive Temporal Motion Guided Graph Convolution Network (ATM-GCN). Our framework excels at capturing temporal dependencies between frames across the entire clip, thereby enhancing micro-expression recognition at the clip level. Specifically, the integration of Adaptive Temporal Motion layers empowers our method to aggregate global and local motion features inherent in micro-expressions. Experimental results demonstrate that ATM-GCN not only surpasses existing state-of-the-art methods, particularly on the Composite dataset, but also achieves superior performance on the latest micro-expression dataset CAS(ME)$^3$.","sentences":["Micro-expressions serve as essential cues for understanding individuals' genuine emotional states.","Recognizing micro-expressions attracts increasing research attention due to its various applications in fields such as business negotiation and psychotherapy.","However, the intricate and transient nature of micro-expressions poses a significant challenge to their accurate recognition.","Most existing works either neglect temporal dependencies or suffer from redundancy issues in clip-level recognition.","In this work, we propose a novel framework for micro-expression recognition, named the Adaptive Temporal Motion Guided Graph Convolution Network (ATM-GCN).","Our framework excels at capturing temporal dependencies between frames across the entire clip, thereby enhancing micro-expression recognition at the clip level.","Specifically, the integration of Adaptive Temporal Motion layers empowers our method to aggregate global and local motion features inherent in micro-expressions.","Experimental results demonstrate that ATM-GCN not only surpasses existing state-of-the-art methods, particularly on the Composite dataset, but also achieves superior performance on the latest micro-expression dataset CAS(ME)$^3$."],"url":"http://arxiv.org/abs/2406.08997v1","category":"cs.CV"}
{"created":"2024-06-13 10:17:53","title":"MURCA driven Bulk viscosity in neutrino trapped baryonic matter","abstract":"We examine bulk viscosity, taking into account trapped neutrinos in baryonic matter, in the context of binary neutron star mergers. Following the merging event, the binary star can yield a remnant compact object with densities up to $5$ nuclear saturation density and temperature upto $50$ MeV resulting in the retention of neutrinos. We employ two relativistic mean field models, NL3 and DDME2, to describe the neutrino-trapped baryonic matter. The dissipation coefficient is determined by evaluating the Modified URCA interaction rate in the dense baryonic medium, and accounting for perturbations caused by density oscillations. We observe the resonant behavior of bulk viscosity as it varies with the temperature of the medium. The bulk viscosity peak remains within the temperature range of $\\sim 13-50$ MeV, depending upon the underlying equation of states and lepton fractions. This temperature range corresponds to the relevant domain of binary neutron star mergers. We also note that in presence of neutrinos in the medium the bulk viscosity peak shifts towards higher temperature and the peak value of bulk viscosity also changes. The time scale of viscous dissipation is dictated by the beta-off-equilibrium susceptibilities derived from the nuclear equation of state. The resulting viscous decay time scale ranges from $32-100$ milliseconds, which aligns with the order of magnitude of the post-merger object's survival time in some specific scenarios.","sentences":["We examine bulk viscosity, taking into account trapped neutrinos in baryonic matter, in the context of binary neutron star mergers.","Following the merging event, the binary star can yield a remnant compact object with densities up to $5$ nuclear saturation density and temperature upto $50$ MeV resulting in the retention of neutrinos.","We employ two relativistic mean field models, NL3 and DDME2, to describe the neutrino-trapped baryonic matter.","The dissipation coefficient is determined by evaluating the Modified URCA interaction rate in the dense baryonic medium, and accounting for perturbations caused by density oscillations.","We observe the resonant behavior of bulk viscosity as it varies with the temperature of the medium.","The bulk viscosity peak remains within the temperature range of $\\sim 13-50$ MeV, depending upon the underlying equation of states and lepton fractions.","This temperature range corresponds to the relevant domain of binary neutron star mergers.","We also note that in presence of neutrinos in the medium the bulk viscosity peak shifts towards higher temperature and the peak value of bulk viscosity also changes.","The time scale of viscous dissipation is dictated by the beta-off-equilibrium susceptibilities derived from the nuclear equation of state.","The resulting viscous decay time scale ranges from $32-100$ milliseconds, which aligns with the order of magnitude of the post-merger object's survival time in some specific scenarios."],"url":"http://arxiv.org/abs/2406.08978v1","category":"nucl-th"}
{"created":"2024-06-13 09:57:51","title":"Covariate Selection for Optimizing Balance with Covariate-Adjusted Response-Adaptive Randomization","abstract":"Balancing influential covariates is crucial for valid treatment comparisons in clinical studies. While covariate-adaptive randomization is commonly used to achieve balance, its performance can be inadequate when the number of baseline covariates is large. It is therefore essential to identify the influential factors associated with the outcome and ensure balance among these critical covariates. In this article, we propose a novel covariate-adjusted response-adaptive randomization that integrates the patients' responses and covariates information to select sequentially significant covariates and maintain their balance. We establish theoretically the consistency of our covariate selection method and demonstrate that the improved covariate balancing, as evidenced by a faster convergence rate of the imbalance measure, leads to higher efficiency in estimating treatment effects. Furthermore, we provide extensive numerical and empirical studies to illustrate the benefits of our proposed method across various settings.","sentences":["Balancing influential covariates is crucial for valid treatment comparisons in clinical studies.","While covariate-adaptive randomization is commonly used to achieve balance, its performance can be inadequate when the number of baseline covariates is large.","It is therefore essential to identify the influential factors associated with the outcome and ensure balance among these critical covariates.","In this article, we propose a novel covariate-adjusted response-adaptive randomization that integrates the patients' responses and covariates information to select sequentially significant covariates and maintain their balance.","We establish theoretically the consistency of our covariate selection method and demonstrate that the improved covariate balancing, as evidenced by a faster convergence rate of the imbalance measure, leads to higher efficiency in estimating treatment effects.","Furthermore, we provide extensive numerical and empirical studies to illustrate the benefits of our proposed method across various settings."],"url":"http://arxiv.org/abs/2406.08968v1","category":"stat.ME"}
{"created":"2024-06-13 08:00:25","title":"SingOMD: Singing Oriented Multi-resolution Discrete Representation Construction from Speech Models","abstract":"Discrete representation has shown advantages in speech generation tasks, wherein discrete tokens are derived by discretizing hidden features from self-supervised learning (SSL) pre-trained models. However, the direct application of speech SSL models to singing generation encounters domain gaps between speech and singing. Furthermore, singing generation necessitates a more refined representation than typical speech. To address these challenges, we introduce SingOMD, a novel method to extract singing-oriented multi-resolution discrete representations from speech SSL models. Specifically, we first adapt the features from speech SSL through a resynthesis task and incorporate multi-resolution modules based on resampling to better serve singing generation. These adapted multi-resolution features are then discretized via clustering. Extensive experiments demonstrate the robustness, efficiency, and effectiveness of these representations in singing vocoders and singing voice synthesis.","sentences":["Discrete representation has shown advantages in speech generation tasks, wherein discrete tokens are derived by discretizing hidden features from self-supervised learning (SSL) pre-trained models.","However, the direct application of speech SSL models to singing generation encounters domain gaps between speech and singing.","Furthermore, singing generation necessitates a more refined representation than typical speech.","To address these challenges, we introduce SingOMD, a novel method to extract singing-oriented multi-resolution discrete representations from speech SSL models.","Specifically, we first adapt the features from speech SSL through a resynthesis task and incorporate multi-resolution modules based on resampling to better serve singing generation.","These adapted multi-resolution features are then discretized via clustering.","Extensive experiments demonstrate the robustness, efficiency, and effectiveness of these representations in singing vocoders and singing voice synthesis."],"url":"http://arxiv.org/abs/2406.08905v1","category":"cs.SD"}
{"created":"2024-06-13 07:50:44","title":"Motif-driven Subgraph Structure Learning for Graph Classification","abstract":"To mitigate the suboptimal nature of graph structure, Graph Structure Learning (GSL) has emerged as a promising approach to improve graph structure and boost performance in downstream tasks. Despite the proposal of numerous GSL methods, the progresses in this field mostly concentrated on node-level tasks, while graph-level tasks (e.g., graph classification) remain largely unexplored. Notably, applying node-level GSL to graph classification is non-trivial due to the lack of find-grained guidance for intricate structure learning. Inspired by the vital role of subgraph in graph classification, in this paper we explore the potential of subgraph structure learning for graph classification by tackling the challenges of key subgraph selection and structure optimization. We propose a novel Motif-driven Subgraph Structure Learning method for Graph Classification (MOSGSL). Specifically, MOSGSL incorporates a subgraph structure learning module which can adaptively select important subgraphs. A motif-driven structure guidance module is further introduced to capture key subgraph-level structural patterns (motifs) and facilitate personalized structure learning. Extensive experiments demonstrate a significant and consistent improvement over baselines, as well as its flexibility and generalizability for various backbones and learning procedures.","sentences":["To mitigate the suboptimal nature of graph structure, Graph Structure Learning (GSL) has emerged as a promising approach to improve graph structure and boost performance in downstream tasks.","Despite the proposal of numerous GSL methods, the progresses in this field mostly concentrated on node-level tasks, while graph-level tasks (e.g., graph classification) remain largely unexplored.","Notably, applying node-level GSL to graph classification is non-trivial due to the lack of find-grained guidance for intricate structure learning.","Inspired by the vital role of subgraph in graph classification, in this paper we explore the potential of subgraph structure learning for graph classification by tackling the challenges of key subgraph selection and structure optimization.","We propose a novel Motif-driven Subgraph Structure Learning method for Graph Classification (MOSGSL).","Specifically, MOSGSL incorporates a subgraph structure learning module which can adaptively select important subgraphs.","A motif-driven structure guidance module is further introduced to capture key subgraph-level structural patterns (motifs) and facilitate personalized structure learning.","Extensive experiments demonstrate a significant and consistent improvement over baselines, as well as its flexibility and generalizability for various backbones and learning procedures."],"url":"http://arxiv.org/abs/2406.08897v1","category":"cs.LG"}
{"created":"2024-06-13 07:13:07","title":"Dissipative Superfluidity in a Molecular Bose-Einstein Condensate","abstract":"Motivated by recent experimental realization of a Bose-Einstein condensate (BEC) of dipolar molecules, we develop superfluid transport theory for a dissipative BEC to show that a weak uniform two-body loss can induce phase rigidity, leading to superfluid transport of bosons. A generalized f-sum rule is shown to hold for a dissipative superfluid as a consequence of weak U(1) symmetry. It is also demonstrated that dissipation enhances the stability of a molecular BEC with dipolar interactions. Possible experimental situations for measuring the superfluid fraction and the spectral function are discussed.","sentences":["Motivated by recent experimental realization of a Bose-Einstein condensate (BEC) of dipolar molecules, we develop superfluid transport theory for a dissipative BEC to show that a weak uniform two-body loss can induce phase rigidity, leading to superfluid transport of bosons.","A generalized f-sum rule is shown to hold for a dissipative superfluid as a consequence of weak U(1) symmetry.","It is also demonstrated that dissipation enhances the stability of a molecular BEC with dipolar interactions.","Possible experimental situations for measuring the superfluid fraction and the spectral function are discussed."],"url":"http://arxiv.org/abs/2406.08868v1","category":"cond-mat.quant-gas"}
{"created":"2024-06-13 05:37:54","title":"How Powerful is Graph Filtering for Recommendation","abstract":"It has been shown that the effectiveness of graph convolutional network (GCN) for recommendation is attributed to the spectral graph filtering. Most GCN-based methods consist of a graph filter or followed by a low-rank mapping optimized based on supervised training. However, we show two limitations suppressing the power of graph filtering: (1) Lack of generality. Due to the varied noise distribution, graph filters fail to denoise sparse data where noise is scattered across all frequencies, while supervised training results in worse performance on dense data where noise is concentrated in middle frequencies that can be removed by graph filters without training. (2) Lack of expressive power. We theoretically show that linear GCN (LGCN) that is effective on collaborative filtering (CF) cannot generate arbitrary embeddings, implying the possibility that optimal data representation might be unreachable.   To tackle the first limitation, we show close relation between noise distribution and the sharpness of spectrum where a sharper spectral distribution is more desirable causing data noise to be separable from important features without training. Based on this observation, we propose a generalized graph normalization G^2N to adjust the sharpness of spectral distribution in order to redistribute data noise to assure that it can be removed by graph filtering without training. As for the second limitation, we propose an individualized graph filter (IGF) adapting to the different confidence levels of the user preference that interactions can reflect, which is proved to be able to generate arbitrary embeddings. By simplifying LGCN, we further propose a simplified graph filtering (SGFCF) which only requires the top-K singular values for recommendation. Finally, experimental results on four datasets with different density settings demonstrate the effectiveness and efficiency of our proposed methods.","sentences":["It has been shown that the effectiveness of graph convolutional network (GCN) for recommendation is attributed to the spectral graph filtering.","Most GCN-based methods consist of a graph filter or followed by a low-rank mapping optimized based on supervised training.","However, we show two limitations suppressing the power of graph filtering: (1) Lack of generality.","Due to the varied noise distribution, graph filters fail to denoise sparse data where noise is scattered across all frequencies, while supervised training results in worse performance on dense data where noise is concentrated in middle frequencies that can be removed by graph filters without training.","(2) Lack of expressive power.","We theoretically show that linear GCN (LGCN) that is effective on collaborative filtering (CF) cannot generate arbitrary embeddings, implying the possibility that optimal data representation might be unreachable.   ","To tackle the first limitation, we show close relation between noise distribution and the sharpness of spectrum where a sharper spectral distribution is more desirable causing data noise to be separable from important features without training.","Based on this observation, we propose a generalized graph normalization G^2N to adjust the sharpness of spectral distribution in order to redistribute data noise to assure that it can be removed by graph filtering without training.","As for the second limitation, we propose an individualized graph filter (IGF) adapting to the different confidence levels of the user preference that interactions can reflect, which is proved to be able to generate arbitrary embeddings.","By simplifying LGCN, we further propose a simplified graph filtering (SGFCF) which only requires the top-K singular values for recommendation.","Finally, experimental results on four datasets with different density settings demonstrate the effectiveness and efficiency of our proposed methods."],"url":"http://arxiv.org/abs/2406.08827v1","category":"cs.IR"}
{"created":"2024-06-13 05:15:52","title":"Skim then Focus: Integrating Contextual and Fine-grained Views for Repetitive Action Counting","abstract":"The key to action counting is accurately locating each video's repetitive actions. Instead of estimating the probability of each frame belonging to an action directly, we propose a dual-branch network, i.e., SkimFocusNet, working in a two-step manner. The model draws inspiration from empirical observations indicating that humans typically engage in coarse skimming of entire sequences to grasp the general action pattern initially, followed by a finer, frame-by-frame focus to determine if it aligns with the target action. Specifically, SkimFocusNet incorporates a skim branch and a focus branch. The skim branch scans the global contextual information throughout the sequence to identify potential target action for guidance. Subsequently, the focus branch utilizes the guidance to diligently identify repetitive actions using a long-short adaptive guidance (LSAG) block. Additionally, we have observed that videos in existing datasets often feature only one type of repetitive action, which inadequately represents real-world scenarios. To more accurately describe real-life situations, we establish the Multi-RepCount dataset, which includes videos containing multiple repetitive motions. On Multi-RepCount, our SkimFoucsNet can perform specified action counting, that is, to enable counting a particular action type by referencing an exemplary video. This capability substantially exhibits the robustness of our method. Extensive experiments demonstrate that SkimFocusNet achieves state-of-the-art performances with significant improvements. We also conduct a thorough ablation study to evaluate the network components. The source code will be published upon acceptance.","sentences":["The key to action counting is accurately locating each video's repetitive actions.","Instead of estimating the probability of each frame belonging to an action directly, we propose a dual-branch network, i.e., SkimFocusNet, working in a two-step manner.","The model draws inspiration from empirical observations indicating that humans typically engage in coarse skimming of entire sequences to grasp the general action pattern initially, followed by a finer, frame-by-frame focus to determine if it aligns with the target action.","Specifically, SkimFocusNet incorporates a skim branch and a focus branch.","The skim branch scans the global contextual information throughout the sequence to identify potential target action for guidance.","Subsequently, the focus branch utilizes the guidance to diligently identify repetitive actions using a long-short adaptive guidance (LSAG) block.","Additionally, we have observed that videos in existing datasets often feature only one type of repetitive action, which inadequately represents real-world scenarios.","To more accurately describe real-life situations, we establish the Multi-RepCount dataset, which includes videos containing multiple repetitive motions.","On Multi-RepCount, our SkimFoucsNet can perform specified action counting, that is, to enable counting a particular action type by referencing an exemplary video.","This capability substantially exhibits the robustness of our method.","Extensive experiments demonstrate that SkimFocusNet achieves state-of-the-art performances with significant improvements.","We also conduct a thorough ablation study to evaluate the network components.","The source code will be published upon acceptance."],"url":"http://arxiv.org/abs/2406.08814v1","category":"cs.CV"}
{"created":"2024-06-13 04:44:06","title":"Adaptive Cooperative Streaming of Holographic Video Over Wireless Networks: A Proximal Policy Optimization Solution","abstract":"Adapting holographic video streaming to fluctuating wireless channels is essential to maintain consistent and satisfactory Quality of Experience (QoE) for users, which, however, is a challenging task due to the dynamic and uncertain characteristics of wireless networks. To address this issue, we propose a holographic video cooperative streaming framework designed for a generic wireless network in which multiple access points can cooperatively transmit video with different bitrates to multiple users. Additionally, we model a novel QoE metric tailored specifically for holographic video streaming, which can effectively encapsulate the nuances of holographic video quality, quality fluctuations, and rebuffering occurrences simultaneously. Furthermore, we formulate a formidable QoE maximization problem, which is a non-convex mixed integer nonlinear programming problem. Using proximal policy optimization (PPO), a new class of reinforcement learning algorithms, we devise a joint beamforming and bitrate control scheme, which can be wisely adapted to fluctuations in the wireless channel. The numerical results demonstrate the superiority of the proposed scheme over representative baselines.","sentences":["Adapting holographic video streaming to fluctuating wireless channels is essential to maintain consistent and satisfactory Quality of Experience (QoE) for users, which, however, is a challenging task due to the dynamic and uncertain characteristics of wireless networks.","To address this issue, we propose a holographic video cooperative streaming framework designed for a generic wireless network in which multiple access points can cooperatively transmit video with different bitrates to multiple users.","Additionally, we model a novel QoE metric tailored specifically for holographic video streaming, which can effectively encapsulate the nuances of holographic video quality, quality fluctuations, and rebuffering occurrences simultaneously.","Furthermore, we formulate a formidable QoE maximization problem, which is a non-convex mixed integer nonlinear programming problem.","Using proximal policy optimization (PPO), a new class of reinforcement learning algorithms, we devise a joint beamforming and bitrate control scheme, which can be wisely adapted to fluctuations in the wireless channel.","The numerical results demonstrate the superiority of the proposed scheme over representative baselines."],"url":"http://arxiv.org/abs/2406.08806v1","category":"eess.SY"}
{"created":"2024-06-13 04:33:20","title":"Hallo: Hierarchical Audio-Driven Visual Synthesis for Portrait Image Animation","abstract":"The field of portrait image animation, driven by speech audio input, has experienced significant advancements in the generation of realistic and dynamic portraits. This research delves into the complexities of synchronizing facial movements and creating visually appealing, temporally consistent animations within the framework of diffusion-based methodologies. Moving away from traditional paradigms that rely on parametric models for intermediate facial representations, our innovative approach embraces the end-to-end diffusion paradigm and introduces a hierarchical audio-driven visual synthesis module to enhance the precision of alignment between audio inputs and visual outputs, encompassing lip, expression, and pose motion. Our proposed network architecture seamlessly integrates diffusion-based generative models, a UNet-based denoiser, temporal alignment techniques, and a reference network. The proposed hierarchical audio-driven visual synthesis offers adaptive control over expression and pose diversity, enabling more effective personalization tailored to different identities. Through a comprehensive evaluation that incorporates both qualitative and quantitative analyses, our approach demonstrates obvious enhancements in image and video quality, lip synchronization precision, and motion diversity. Further visualization and access to the source code can be found at: https://fudan-generative-vision.github.io/hallo.","sentences":["The field of portrait image animation, driven by speech audio input, has experienced significant advancements in the generation of realistic and dynamic portraits.","This research delves into the complexities of synchronizing facial movements and creating visually appealing, temporally consistent animations within the framework of diffusion-based methodologies.","Moving away from traditional paradigms that rely on parametric models for intermediate facial representations, our innovative approach embraces the end-to-end diffusion paradigm and introduces a hierarchical audio-driven visual synthesis module to enhance the precision of alignment between audio inputs and visual outputs, encompassing lip, expression, and pose motion.","Our proposed network architecture seamlessly integrates diffusion-based generative models, a UNet-based denoiser, temporal alignment techniques, and a reference network.","The proposed hierarchical audio-driven visual synthesis offers adaptive control over expression and pose diversity, enabling more effective personalization tailored to different identities.","Through a comprehensive evaluation that incorporates both qualitative and quantitative analyses, our approach demonstrates obvious enhancements in image and video quality, lip synchronization precision, and motion diversity.","Further visualization and access to the source code can be found at: https://fudan-generative-vision.github.io/hallo."],"url":"http://arxiv.org/abs/2406.08801v1","category":"cs.CV"}
{"created":"2024-06-13 04:27:37","title":"FouRA: Fourier Low Rank Adaptation","abstract":"While Low-Rank Adaptation (LoRA) has proven beneficial for efficiently fine-tuning large models, LoRA fine-tuned text-to-image diffusion models lack diversity in the generated images, as the model tends to copy data from the observed training samples. This effect becomes more pronounced at higher values of adapter strength and for adapters with higher ranks which are fine-tuned on smaller datasets. To address these challenges, we present FouRA, a novel low-rank method that learns projections in the Fourier domain along with learning a flexible input-dependent adapter rank selection strategy. Through extensive experiments and analysis, we show that FouRA successfully solves the problems related to data copying and distribution collapse while significantly improving the generated image quality. We demonstrate that FouRA enhances the generalization of fine-tuned models thanks to its adaptive rank selection. We further show that the learned projections in the frequency domain are decorrelated and prove effective when merging multiple adapters. While FouRA is motivated for vision tasks, we also demonstrate its merits for language tasks on the GLUE benchmark.","sentences":["While Low-Rank Adaptation (LoRA) has proven beneficial for efficiently fine-tuning large models, LoRA fine-tuned text-to-image diffusion models lack diversity in the generated images, as the model tends to copy data from the observed training samples.","This effect becomes more pronounced at higher values of adapter strength and for adapters with higher ranks which are fine-tuned on smaller datasets.","To address these challenges, we present FouRA, a novel low-rank method that learns projections in the Fourier domain along with learning a flexible input-dependent adapter rank selection strategy.","Through extensive experiments and analysis, we show that FouRA successfully solves the problems related to data copying and distribution collapse while significantly improving the generated image quality.","We demonstrate that FouRA enhances the generalization of fine-tuned models thanks to its adaptive rank selection.","We further show that the learned projections in the frequency domain are decorrelated and prove effective when merging multiple adapters.","While FouRA is motivated for vision tasks, we also demonstrate its merits for language tasks on the GLUE benchmark."],"url":"http://arxiv.org/abs/2406.08798v1","category":"cs.CV"}
{"created":"2024-06-13 04:04:52","title":"H\u00f6lder Continuity for Fully Fractional Parabolic Equations with Space-time Nonlocal Operators","abstract":"We study the local H\\\"{o}lder regularity of weak solutions to the fully fractional parabolic equations involving spatial fractional diffusion and fractional time derivatives of the Marchaud type. It is worth noting that we do not impose boundedness assumptions on the weak solutions and nonhomogeneous terms. Within the space-time nonlocal framework, it is crucial to consider both space-dependent nonlocal tail terms and the first introduced time-dependent nonlocal tail term. By adapting a nonlocal variant of the parabolic De Giorgi iterative technique, we initially establish a priori local boundedness with tail terms for weak solutions and then prove the local H\\\"{o}lder continuity.","sentences":["We study the local H\\\"{o}lder regularity of weak solutions to the fully fractional parabolic equations involving spatial fractional diffusion and fractional time derivatives of the Marchaud type.","It is worth noting that we do not impose boundedness assumptions on the weak solutions and nonhomogeneous terms.","Within the space-time nonlocal framework, it is crucial to consider both space-dependent nonlocal tail terms and the first introduced time-dependent nonlocal tail term.","By adapting a nonlocal variant of the parabolic De Giorgi iterative technique, we initially establish a priori local boundedness with tail terms for weak solutions and then prove the local H\\\"{o}lder continuity."],"url":"http://arxiv.org/abs/2406.08795v1","category":"math.AP"}
{"created":"2024-06-13 03:33:36","title":"BEVSpread: Spread Voxel Pooling for Bird's-Eye-View Representation in Vision-based Roadside 3D Object Detection","abstract":"Vision-based roadside 3D object detection has attracted rising attention in autonomous driving domain, since it encompasses inherent advantages in reducing blind spots and expanding perception range. While previous work mainly focuses on accurately estimating depth or height for 2D-to-3D mapping, ignoring the position approximation error in the voxel pooling process. Inspired by this insight, we propose a novel voxel pooling strategy to reduce such error, dubbed BEVSpread. Specifically, instead of bringing the image features contained in a frustum point to a single BEV grid, BEVSpread considers each frustum point as a source and spreads the image features to the surrounding BEV grids with adaptive weights. To achieve superior propagation performance, a specific weight function is designed to dynamically control the decay speed of the weights according to distance and depth. Aided by customized CUDA parallel acceleration, BEVSpread achieves comparable inference time as the original voxel pooling. Extensive experiments on two large-scale roadside benchmarks demonstrate that, as a plug-in, BEVSpread can significantly improve the performance of existing frustum-based BEV methods by a large margin of (1.12, 5.26, 3.01) AP in vehicle, pedestrian and cyclist.","sentences":["Vision-based roadside 3D object detection has attracted rising attention in autonomous driving domain, since it encompasses inherent advantages in reducing blind spots and expanding perception range.","While previous work mainly focuses on accurately estimating depth or height for 2D-to-3D mapping, ignoring the position approximation error in the voxel pooling process.","Inspired by this insight, we propose a novel voxel pooling strategy to reduce such error, dubbed BEVSpread.","Specifically, instead of bringing the image features contained in a frustum point to a single BEV grid, BEVSpread considers each frustum point as a source and spreads the image features to the surrounding BEV grids with adaptive weights.","To achieve superior propagation performance, a specific weight function is designed to dynamically control the decay speed of the weights according to distance and depth.","Aided by customized CUDA parallel acceleration, BEVSpread achieves comparable inference time as the original voxel pooling.","Extensive experiments on two large-scale roadside benchmarks demonstrate that, as a plug-in, BEVSpread can significantly improve the performance of existing frustum-based BEV methods by a large margin of (1.12, 5.26, 3.01) AP in vehicle, pedestrian and cyclist."],"url":"http://arxiv.org/abs/2406.08785v1","category":"cs.CV"}
{"created":"2024-06-13 03:30:29","title":"Improved methods for empirical Bayes multivariate multiple testing and effect size estimation","abstract":"Estimating the sharing of genetic effects across different conditions is important to many statistical analyses of genomic data. The patterns of sharing arising from these data are often highly heterogeneous. To flexibly model these heterogeneous sharing patterns, Urbut et al. (2019) proposed the multivariate adaptive shrinkage (MASH) method to jointly analyze genetic effects across multiple conditions. However, multivariate analyses using MASH (as well as other multivariate analyses) require good estimates of the sharing patterns, and estimating these patterns efficiently and accurately remains challenging. Here we describe new empirical Bayes methods that provide improvements in speed and accuracy over existing methods. The two key ideas are: (1) adaptive regularization to improve accuracy in settings with many conditions; (2) improving the speed of the model fitting algorithms by exploiting analytical results on covariance estimation. In simulations, we show that the new methods provide better model fits, better out-of-sample performance, and improved power and accuracy in detecting the true underlying signals. In an analysis of eQTLs in 49 human tissues, our new analysis pipeline achieves better model fits and better out-of-sample performance than the existing MASH analysis pipeline. We have implemented the new methods, which we call ``Ultimate Deconvolution'', in an R package, udr, available on GitHub.","sentences":["Estimating the sharing of genetic effects across different conditions is important to many statistical analyses of genomic data.","The patterns of sharing arising from these data are often highly heterogeneous.","To flexibly model these heterogeneous sharing patterns, Urbut et al. (2019) proposed the multivariate adaptive shrinkage (MASH) method to jointly analyze genetic effects across multiple conditions.","However, multivariate analyses using MASH (as well as other multivariate analyses) require good estimates of the sharing patterns, and estimating these patterns efficiently and accurately remains challenging.","Here we describe new empirical Bayes methods that provide improvements in speed and accuracy over existing methods.","The two key ideas are: (1) adaptive regularization to improve accuracy in settings with many conditions; (2) improving the speed of the model fitting algorithms by exploiting analytical results on covariance estimation.","In simulations, we show that the new methods provide better model fits, better out-of-sample performance, and improved power and accuracy in detecting the true underlying signals.","In an analysis of eQTLs in 49 human tissues, our new analysis pipeline achieves better model fits and better out-of-sample performance than the existing MASH analysis pipeline.","We have implemented the new methods, which we call ``Ultimate Deconvolution'', in an R package, udr, available on GitHub."],"url":"http://arxiv.org/abs/2406.08784v1","category":"stat.ME"}
{"created":"2024-06-13 02:41:11","title":"Gaussian-Forest: Hierarchical-Hybrid 3D Gaussian Splatting for Compressed Scene Modeling","abstract":"The field of novel-view synthesis has recently witnessed the emergence of 3D Gaussian Splatting, which represents scenes in a point-based manner and renders through rasterization. This methodology, in contrast to Radiance Fields that rely on ray tracing, demonstrates superior rendering quality and speed. However, the explicit and unstructured nature of 3D Gaussians poses a significant storage challenge, impeding its broader application. To address this challenge, we introduce the Gaussian-Forest modeling framework, which hierarchically represents a scene as a forest of hybrid 3D Gaussians. Each hybrid Gaussian retains its unique explicit attributes while sharing implicit ones with its sibling Gaussians, thus optimizing parameterization with significantly fewer variables. Moreover, adaptive growth and pruning strategies are designed, ensuring detailed representation in complex regions and a notable reduction in the number of required Gaussians. Extensive experiments demonstrate that Gaussian-Forest not only maintains comparable speed and quality but also achieves a compression rate surpassing 10 times, marking a significant advancement in efficient scene modeling. Codes are available at https://github.com/Xian-Bei/GaussianForest.","sentences":["The field of novel-view synthesis has recently witnessed the emergence of 3D Gaussian Splatting, which represents scenes in a point-based manner and renders through rasterization.","This methodology, in contrast to Radiance Fields that rely on ray tracing, demonstrates superior rendering quality and speed.","However, the explicit and unstructured nature of 3D Gaussians poses a significant storage challenge, impeding its broader application.","To address this challenge, we introduce the Gaussian-Forest modeling framework, which hierarchically represents a scene as a forest of hybrid 3D Gaussians.","Each hybrid Gaussian retains its unique explicit attributes while sharing implicit ones with its sibling Gaussians, thus optimizing parameterization with significantly fewer variables.","Moreover, adaptive growth and pruning strategies are designed, ensuring detailed representation in complex regions and a notable reduction in the number of required Gaussians.","Extensive experiments demonstrate that Gaussian-Forest not only maintains comparable speed and quality but also achieves a compression rate surpassing 10 times, marking a significant advancement in efficient scene modeling.","Codes are available at https://github.com/Xian-Bei/GaussianForest."],"url":"http://arxiv.org/abs/2406.08759v1","category":"cs.CV"}
{"created":"2024-06-13 02:08:28","title":"StreamBench: Towards Benchmarking Continuous Improvement of Language Agents","abstract":"Recent works have shown that large language model (LLM) agents are able to improve themselves from experience, which is an important ability for continuous enhancement post-deployment. However, existing benchmarks primarily evaluate their innate capabilities and do not assess their ability to improve over time. To address this gap, we introduce StreamBench, a pioneering benchmark designed to evaluate the continuous improvement of LLM agents over an input-feedback sequence. StreamBench simulates an online learning environment where LLMs receive a continuous flow of feedback stream and iteratively enhance their performance. In addition, we propose several simple yet effective baselines for improving LLMs on StreamBench, and provide a comprehensive analysis to identify critical components that contribute to successful streaming strategies. Our work serves as a stepping stone towards developing effective online learning strategies for LLMs, paving the way for more adaptive AI systems in streaming scenarios.","sentences":["Recent works have shown that large language model (LLM) agents are able to improve themselves from experience, which is an important ability for continuous enhancement post-deployment.","However, existing benchmarks primarily evaluate their innate capabilities and do not assess their ability to improve over time.","To address this gap, we introduce StreamBench, a pioneering benchmark designed to evaluate the continuous improvement of LLM agents over an input-feedback sequence.","StreamBench simulates an online learning environment where LLMs receive a continuous flow of feedback stream and iteratively enhance their performance.","In addition, we propose several simple yet effective baselines for improving LLMs on StreamBench, and provide a comprehensive analysis to identify critical components that contribute to successful streaming strategies.","Our work serves as a stepping stone towards developing effective online learning strategies for LLMs, paving the way for more adaptive AI systems in streaming scenarios."],"url":"http://arxiv.org/abs/2406.08747v1","category":"cs.CL"}
{"created":"2024-06-13 00:58:55","title":"Uniform Large Deviation Principles of Fractional Reaction-Diffusion Equations Driven by Superlinear Multiplicative Noise on R^n","abstract":"In this paper, we investigate the uniform large deviation principle of the fractional stochastic reaction-diffusion equation on the entire space R^n as the noise intensity approaches zero. The nonlinear drift term is dissipative and has a polynomial growth of any order. The nonlinear diffusion term is locally Lipschitz continuous and has a superlinear growth rate. By the weak convergence method, we establish the Freidlin-Wentzell uniform large deviations over bounded initial data as well as the Dembo-Zeitouni uniform large deviations over compact initial data. The main difficulties are caused by the superlinear growth of noise coefficients and the non-compactness of Sobolev embeddings on unbounded domains. The dissipativeness of the drift term and the idea of uniform tail-ends estimates of solutions are employed to circumvent these difficulties.","sentences":["In this paper, we investigate the uniform large deviation principle of the fractional stochastic reaction-diffusion equation on the entire space R^n as the noise intensity approaches zero.","The nonlinear drift term is dissipative and has a polynomial growth of any order.","The nonlinear diffusion term is locally Lipschitz continuous and has a superlinear growth rate.","By the weak convergence method, we establish the Freidlin-Wentzell uniform large deviations over bounded initial data as well as the Dembo-Zeitouni uniform large deviations over compact initial data.","The main difficulties are caused by the superlinear growth of noise coefficients and the non-compactness of Sobolev embeddings on unbounded domains.","The dissipativeness of the drift term and the idea of uniform tail-ends estimates of solutions are employed to circumvent these difficulties."],"url":"http://arxiv.org/abs/2406.08722v1","category":"math.PR"}
{"created":"2024-06-12 22:23:26","title":"Superconducting Diode Effect in Quantum Spin Hall Insulator-based Josephson Junctions","abstract":"The superconducting diode effect (SDE) is a magneto-electric phenomenon where an external magnetic field imparts a non-zero center-of-mass momentum to Cooper pairs, either facilitating or hindering the flow of supercurrent depending on its direction. We propose that quantum spin Hall insulator (QSHI)-based Josephson junctions can serve as versatile platforms for non-dissipative electronics exhibiting the SDE when triggered by a phase bias and an out-of-plane magnetic field. By computing the contributions from Andreev bound states and the continuum of quasi-particle states, we provide both numerical and analytical results scrutinizing various aspects of the SDE, including its quality Q-factor. The maximum value of the $Q$-factor is found to be universal at low (zero) temperatures, which ties its origin to underlying topological properties that are independent of the junction's specific details. As the magnetic field increases, the SDE diminishes due to the closing of the induced superconducting gap caused by orbital effects. To observe the SDE, the QSHI-based Josephson junction must be designed so that its edges are transport-wise non-equivalent. Additionally, we explore the SDE in a more exotic yet realistic scenario, where the fermionic ground-state parity of the Josephson junction remains conserved while driving a current. In this 4$\\pi$-periodic situation, we predict an enhancement of the SDE compared to its 2$\\pi$-periodic, parity-unconstrained counterpart.","sentences":["The superconducting diode effect (SDE) is a magneto-electric phenomenon where an external magnetic field imparts a non-zero center-of-mass momentum to Cooper pairs, either facilitating or hindering the flow of supercurrent depending on its direction.","We propose that quantum spin Hall insulator (QSHI)-based Josephson junctions can serve as versatile platforms for non-dissipative electronics exhibiting the SDE when triggered by a phase bias and an out-of-plane magnetic field.","By computing the contributions from Andreev bound states and the continuum of quasi-particle states, we provide both numerical and analytical results scrutinizing various aspects of the SDE, including its quality Q-factor.","The maximum value of the $Q$-factor is found to be universal at low (zero) temperatures, which ties its origin to underlying topological properties that are independent of the junction's specific details.","As the magnetic field increases, the SDE diminishes due to the closing of the induced superconducting gap caused by orbital effects.","To observe the SDE, the QSHI-based Josephson junction must be designed so that its edges are transport-wise non-equivalent.","Additionally, we explore the SDE in a more exotic yet realistic scenario, where the fermionic ground-state parity of the Josephson junction remains conserved while driving a current.","In this 4$\\pi$-periodic situation, we predict an enhancement of the SDE compared to its 2$\\pi$-periodic, parity-unconstrained counterpart."],"url":"http://arxiv.org/abs/2406.08669v1","category":"cond-mat.supr-con"}
{"created":"2024-06-12 21:42:13","title":"Mistral-C2F: Coarse to Fine Actor for Analytical and Reasoning Enhancement in RLHF and Effective-Merged LLMs","abstract":"Despite the advances in Large Language Models (LLMs), exemplified by models like GPT-4 and Claude, smaller-scale LLMs such as Llama and Mistral often struggle with generating in-depth and coherent dialogues. This paper presents a novel two-step Coarse-to-Fine Actor model to address the inherent limitations in conversational and analytical capabilities of small-sized LLMs. Our approach begins with the Policy-based Coarse Actor, employing a technique we term \"Continuous Maximization\". The Coarse Actor establishes an enhanced, knowledge-rich pool adept at aligning with human preference styles in analysis and reasoning. Through the RLHF process, it employs Continuous Maximization, a strategy that dynamically and adaptively extends the output length limit, enabling the generation of more detailed and analytical content. Subsequently, the Fine Actor refines this analytical content, addressing the generation of excessively redundant information from the Coarse Actor. We introduce a \"Knowledge Residue Merger\" approach, refining the content from the Coarse Actor and merging it with an existing Instruction model to improve quality, correctness, and reduce redundancies. We applied our methodology to the popular Mistral model, creating Mistral-C2F, which has demonstrated exceptional performance across 11 general language tasks and the MT-Bench Dialogue task, outperforming similar-scale models and even larger models with 13B and 30B parameters. Our model has significantly improved conversational and analytical reasoning abilities.","sentences":["Despite the advances in Large Language Models (LLMs), exemplified by models like GPT-4 and Claude, smaller-scale LLMs such as Llama and Mistral often struggle with generating in-depth and coherent dialogues.","This paper presents a novel two-step Coarse-to-Fine Actor model to address the inherent limitations in conversational and analytical capabilities of small-sized LLMs.","Our approach begins with the Policy-based Coarse Actor, employing a technique we term \"Continuous Maximization\".","The Coarse Actor establishes an enhanced, knowledge-rich pool adept at aligning with human preference styles in analysis and reasoning.","Through the RLHF process, it employs Continuous Maximization, a strategy that dynamically and adaptively extends the output length limit, enabling the generation of more detailed and analytical content.","Subsequently, the Fine Actor refines this analytical content, addressing the generation of excessively redundant information from the Coarse Actor.","We introduce a \"Knowledge Residue Merger\" approach, refining the content from the Coarse Actor and merging it with an existing Instruction model to improve quality, correctness, and reduce redundancies.","We applied our methodology to the popular Mistral model, creating Mistral-C2F, which has demonstrated exceptional performance across 11 general language tasks and the MT-Bench Dialogue task, outperforming similar-scale models and even larger models with 13B and 30B parameters.","Our model has significantly improved conversational and analytical reasoning abilities."],"url":"http://arxiv.org/abs/2406.08657v1","category":"cs.CL"}
{"created":"2024-06-12 21:20:00","title":"Adaptive Nonlinear Model Predictive Control for a Real-World Labyrinth Game","abstract":"We present a nonlinear non-convex model predictive control approach to solving a real-world labyrinth game. We introduce adaptive nonlinear constraints, representing the non-convex obstacles within the labyrinth. Our method splits the computation-heavy optimization problem into two layers; first, a high-level model predictive controller which incorporates the full problem formulation and finds pseudo-global optimal trajectories at a low frequency. Secondly, a low-level model predictive controller that receives a reduced, computationally optimized version of the optimization problem to follow the given high-level path in real-time. Further, a map of the labyrinth surface irregularities is learned. Our controller is able to handle the major disturbances and model inaccuracies encountered on the labyrinth and outperforms other classical control methods.","sentences":["We present a nonlinear non-convex model predictive control approach to solving a real-world labyrinth game.","We introduce adaptive nonlinear constraints, representing the non-convex obstacles within the labyrinth.","Our method splits the computation-heavy optimization problem into two layers; first, a high-level model predictive controller which incorporates the full problem formulation and finds pseudo-global optimal trajectories at a low frequency.","Secondly, a low-level model predictive controller that receives a reduced, computationally optimized version of the optimization problem to follow the given high-level path in real-time.","Further, a map of the labyrinth surface irregularities is learned.","Our controller is able to handle the major disturbances and model inaccuracies encountered on the labyrinth and outperforms other classical control methods."],"url":"http://arxiv.org/abs/2406.08650v1","category":"cs.RO"}
{"created":"2024-06-12 21:01:26","title":"ML-SUPERB 2.0: Benchmarking Multilingual Speech Models Across Modeling Constraints, Languages, and Datasets","abstract":"ML-SUPERB evaluates self-supervised learning (SSL) models on the tasks of language identification and automatic speech recognition (ASR). This benchmark treats the models as feature extractors and uses a single shallow downstream model, which can be fine-tuned for a downstream task. However, real-world use cases may require different configurations. This paper presents ML-SUPERB~2.0, which is a new benchmark for evaluating pre-trained SSL and supervised speech models across downstream models, fine-tuning setups, and efficient model adaptation approaches. We find performance improvements over the setup of ML-SUPERB. However, performance depends on the downstream model design. Also, we find large performance differences between languages and datasets, suggesting the need for more targeted approaches to improve multilingual ASR performance.","sentences":["ML-SUPERB evaluates self-supervised learning (SSL) models on the tasks of language identification and automatic speech recognition (ASR).","This benchmark treats the models as feature extractors and uses a single shallow downstream model, which can be fine-tuned for a downstream task.","However, real-world use cases may require different configurations.","This paper presents ML-SUPERB~2.0, which is a new benchmark for evaluating pre-trained SSL and supervised speech models across downstream models, fine-tuning setups, and efficient model adaptation approaches.","We find performance improvements over the setup of ML-SUPERB.","However, performance depends on the downstream model design.","Also, we find large performance differences between languages and datasets, suggesting the need for more targeted approaches to improve multilingual ASR performance."],"url":"http://arxiv.org/abs/2406.08641v1","category":"cs.SD"}
{"created":"2024-06-12 20:59:57","title":"Adaptively Implicit Advection for Atmospheric Flows","abstract":"Implicit time-stepping for advection is applied locally in space and time where Courant numbers are large, but standard explicit time-stepping is used for the remaining solution which is typically the majority. This adaptively implicit advection scheme facilitates efficient and robust integrations with long time-steps while having negligible impact on the overall accuracy, and achieving monotonicity and local conservation on general meshes. A novel and important aspect for the efficiency of the approach is that only one linear solver iteration is needed for each advection solve.   The implementation in this paper uses a second-order Runge-Kutta implicit/explicit time-stepping in combination with a second/third-order finite volume spatial discretisation. We demonstrate the adaptively implicit advection in the context of deformational flow advection on the sphere and a fully compressible model for atmospheric flows. Tracers are advected over the poles of latitude-longitude grids with very large Courant numbers and through hexagonal and cubed-sphere meshes with the same algorithm. Buoyant flow simulations with strong local updrafts also benefit from adaptively implicit advection. Stably stratified flow simulations require a stable combination of implicit treatment of gravity and acoustic waves as well as advection in order to achieve long stable time-steps.","sentences":["Implicit time-stepping for advection is applied locally in space and time where Courant numbers are large, but standard explicit time-stepping is used for the remaining solution which is typically the majority.","This adaptively implicit advection scheme facilitates efficient and robust integrations with long time-steps while having negligible impact on the overall accuracy, and achieving monotonicity and local conservation on general meshes.","A novel and important aspect for the efficiency of the approach is that only one linear solver iteration is needed for each advection solve.   ","The implementation in this paper uses a second-order Runge-Kutta implicit/explicit time-stepping in combination with a second/third-order finite volume spatial discretisation.","We demonstrate the adaptively implicit advection in the context of deformational flow advection on the sphere and a fully compressible model for atmospheric flows.","Tracers are advected over the poles of latitude-longitude grids with very large Courant numbers and through hexagonal and cubed-sphere meshes with the same algorithm.","Buoyant flow simulations with strong local updrafts also benefit from adaptively implicit advection.","Stably stratified flow simulations require a stable combination of implicit treatment of gravity and acoustic waves as well as advection in order to achieve long stable time-steps."],"url":"http://arxiv.org/abs/2406.08640v1","category":"physics.flu-dyn"}
{"created":"2024-06-12 20:24:03","title":"Chemical Timescale Effects on Detonation Convergence","abstract":"Numerical simulations of detonation-containing flows have emerged as crucial tools for designing next-generation power and propulsion devices. As these tools mature, it is important for the combustion community to properly understand and isolate grid resolution effects when simulating detonations. To this end, this work provides a comprehensive analysis of the numerical convergence of unsteady detonation simulations, with focus on isolating the impacts of chemical timescale modifications on convergence characteristics in the context of operator splitting. With the aid of an adaptive mesh refinement based flow solver, the convergence analysis is conducted using two kinetics configurations: (1) a simplified three-step model mechanism, in which chemical timescales in the detonation are modified by adjusting activation energies, and (2) a detailed hydrogen mechanism, in which chemical timescales are adjusted through ambient pressure modifications. The convergence of unsteady self-sustained detonations in one-dimensional channels is then analyzed with reference to steady-state theoretical baseline solutions using these mechanisms. The goal of the analysis is to provide a detailed comparison of the effects of grid resolution on both macroscopic (peak pressures and detonation wave speeds) and microscopic (detonation wave structure) quantities of interest, drawing connections between the deviations from steady-state baselines and minimum chemical timescales. This work uncovers resolution-dependent unsteady detonation regimes, and highlights the important role played by not only the chemical timescales, but also the ratio between chemical and induction timescales in the detonation wave structure on simulation convergence properties.","sentences":["Numerical simulations of detonation-containing flows have emerged as crucial tools for designing next-generation power and propulsion devices.","As these tools mature, it is important for the combustion community to properly understand and isolate grid resolution effects when simulating detonations.","To this end, this work provides a comprehensive analysis of the numerical convergence of unsteady detonation simulations, with focus on isolating the impacts of chemical timescale modifications on convergence characteristics in the context of operator splitting.","With the aid of an adaptive mesh refinement based flow solver, the convergence analysis is conducted using two kinetics configurations: (1) a simplified three-step model mechanism, in which chemical timescales in the detonation are modified by adjusting activation energies, and (2) a detailed hydrogen mechanism, in which chemical timescales are adjusted through ambient pressure modifications.","The convergence of unsteady self-sustained detonations in one-dimensional channels is then analyzed with reference to steady-state theoretical baseline solutions using these mechanisms.","The goal of the analysis is to provide a detailed comparison of the effects of grid resolution on both macroscopic (peak pressures and detonation wave speeds) and microscopic (detonation wave structure) quantities of interest, drawing connections between the deviations from steady-state baselines and minimum chemical timescales.","This work uncovers resolution-dependent unsteady detonation regimes, and highlights the important role played by not only the chemical timescales, but also the ratio between chemical and induction timescales in the detonation wave structure on simulation convergence properties."],"url":"http://arxiv.org/abs/2406.08631v1","category":"physics.flu-dyn"}
{"created":"2024-06-12 19:12:51","title":"Interpolation in Weighted Projective Spaces","abstract":"Over an algebraically closed field, the $\\textit{double point interpolation}$ problem asks for the vector space dimension of the projective hypersurfaces of degree $d$ singular at a given set of points. After being open for 90 years, a series of papers by J. Alexander and A. Hirschowitz in 1992--1995 settled this question in what is referred to as the Alexander-Hirschowitz theorem. In this paper we primarily use commutative algebra to lay the groundwork necessary to prove analogous statements in the $\\textit{weighted projective space}$, a natural generalization of the projective space. We show the Hilbert function of general simple points in any $n$-dimensional weighted projective space exhibits the expected behavior. We give an inductive procedure for weighted projective space, similar to that originally due to A. Terracini from 1915, to demonstrate an example of a weighted projective plane where the analogue of the Alexander-Hirschowitz theorem holds without exceptions. We further adapt Terracini's lemma regarding secant varieties to give an interpolation bound for an infinite family of weighted projective planes.","sentences":["Over an algebraically closed field, the $\\textit{double point interpolation}$ problem asks for the vector space dimension of the projective hypersurfaces of degree $d$ singular at a given set of points.","After being open for 90 years, a series of papers by J. Alexander and A. Hirschowitz in 1992--1995 settled this question in what is referred to as the Alexander-Hirschowitz theorem.","In this paper we primarily use commutative algebra to lay the groundwork necessary to prove analogous statements in the $\\textit{weighted projective space}$, a natural generalization of the projective space.","We show the Hilbert function of general simple points in any $n$-dimensional weighted projective space exhibits the expected behavior.","We give an inductive procedure for weighted projective space, similar to that originally due to A. Terracini from 1915, to demonstrate an example of a weighted projective plane where the analogue of the Alexander-Hirschowitz theorem holds without exceptions.","We further adapt Terracini's lemma regarding secant varieties to give an interpolation bound for an infinite family of weighted projective planes."],"url":"http://arxiv.org/abs/2406.08602v1","category":"math.AC"}
{"created":"2024-06-12 19:07:27","title":"Canard explosions in turbulent thermo-fluid systems","abstract":"A sudden transition to a state of high amplitude limit cycle oscillations is catastrophic in a thermo-fluid system. Conventionally, upon varying the control parameter, a sudden transition is observed as an abrupt jump in the amplitude of the fluctuations in these systems. In contrast, we present an experimental discovery of a canard explosion in a turbulent reactive flow system where we observe a continuous bifurcation with a rapid rise in the amplitude of the fluctuations within a narrow range of control parameters. The observed transition is facilitated via a state of bursting, consisting of the epochs of large amplitude periodic oscillations amidst the epochs of low amplitude periodic oscillations. The amplitude of the bursts is higher than the amplitude of the bursts of intermittency state in a conventional gradual transition, as reported in turbulent reactive flow systems. During the bursting state, we observe that temperature fluctuations of exhaust gas vary at a slower time scale in correlation with the amplitude envelope of the bursts. We also present a phenomenological model for thermoacoustic systems to describe the observed canard explosion. Using the model, we explain that the large amplitude bursts occur due to the slow-fast dynamics at the bifurcation regime of the canard explosion.","sentences":["A sudden transition to a state of high amplitude limit cycle oscillations is catastrophic in a thermo-fluid system.","Conventionally, upon varying the control parameter, a sudden transition is observed as an abrupt jump in the amplitude of the fluctuations in these systems.","In contrast, we present an experimental discovery of a canard explosion in a turbulent reactive flow system where we observe a continuous bifurcation with a rapid rise in the amplitude of the fluctuations within a narrow range of control parameters.","The observed transition is facilitated via a state of bursting, consisting of the epochs of large amplitude periodic oscillations amidst the epochs of low amplitude periodic oscillations.","The amplitude of the bursts is higher than the amplitude of the bursts of intermittency state in a conventional gradual transition, as reported in turbulent reactive flow systems.","During the bursting state, we observe that temperature fluctuations of exhaust gas vary at a slower time scale in correlation with the amplitude envelope of the bursts.","We also present a phenomenological model for thermoacoustic systems to describe the observed canard explosion.","Using the model, we explain that the large amplitude bursts occur due to the slow-fast dynamics at the bifurcation regime of the canard explosion."],"url":"http://arxiv.org/abs/2406.08599v1","category":"physics.flu-dyn"}
{"created":"2024-06-12 19:00:56","title":"Whispering in the dark: faint X-ray emission from black holes with OB star companions","abstract":"Context. Recent astrometric and spectroscopic surveys of OB stars have revealed a few stellar-mass black holes (BHs) with orbital periods as low as 10 days. No X-ray counterpart has been detected, due to the absence of a radiatively efficient accretion disk around the BH. Yet, dissipative processes in the hot, dilute and strongly magnetized plasma around the BH (so-called BH corona) can still lead to non-thermal X-ray emission (e.g. synchrotron).   Aims. We determine the X-ray luminosity distribution from BH+OB star binaries up to orbital periods of a few thousand days.   Methods. We use detailed binary evolution models computed with MESA for initial primary masses of 10-90 $M_{\\odot}$ and orbital periods from 1-3000 d. The X-ray luminosity is computed for a broad range of radiative efficiencies.   Results. We show that particle acceleration through magnetic reconnection can heat the BH corona. A substantial fraction of the gravitational potential energy from the accreted plasma is converted into non-thermal X-ray emission. Our population synthesis analysis predicts at least 28 (up to 72) BH+OB star binaries in the Large Magellanic Cloud (LMC) to produce X-ray luminosity above 10$^{31}$ erg$\\,$s$^{-1}$, observable through focused Chandra observations. We identify a population of SB1 systems in the LMC and HD96670 in the Milky Way comprising O stars with unseen companions of masses above 2.3 $M_{\\odot}$ that aligns well with our predictions. The predicted luminosities of the OB companions to these X-ray-emitting BHs are 10$^{4.5-5.5}$ $L_{\\odot}$.   Conclusions. These results make the case for long-time exposure in X-rays of the stellar-mass BH candidates identified around OB stars. It will constrain the underlying population of X-ray-faint BHs, the evolution from single to double degenerate binaries, and the progenitors of gravitational wave mergers. (Abridged)","sentences":["Context.","Recent astrometric and spectroscopic surveys of OB stars have revealed a few stellar-mass black holes (BHs) with orbital periods as low as 10 days.","No X-ray counterpart has been detected, due to the absence of a radiatively efficient accretion disk around the BH.","Yet, dissipative processes in the hot, dilute and strongly magnetized plasma around the BH (so-called BH corona) can still lead to non-thermal X-ray emission (e.g. synchrotron).   ","Aims.","We determine the X-ray luminosity distribution from BH+OB star binaries up to orbital periods of a few thousand days.   Methods.","We use detailed binary evolution models computed with MESA for initial primary masses of 10-90 $M_{\\odot}$ and orbital periods from 1-3000 d.","The X-ray luminosity is computed for a broad range of radiative efficiencies.   ","Results.","We show that particle acceleration through magnetic reconnection can heat the BH corona.","A substantial fraction of the gravitational potential energy from the accreted plasma is converted into non-thermal X-ray emission.","Our population synthesis analysis predicts at least 28 (up to 72) BH+OB star binaries in the Large Magellanic Cloud (LMC) to produce X-ray luminosity above 10$^{31}$ erg$\\,$s$^{-1}$, observable through focused Chandra observations.","We identify a population of SB1 systems in the LMC and HD96670 in the Milky Way comprising O stars with unseen companions of masses above 2.3 $M_{\\odot}$ that aligns well with our predictions.","The predicted luminosities of the OB companions to these X-ray-emitting BHs are 10$^{4.5-5.5}$ $L_{\\odot}$.   Conclusions.","These results make the case for long-time exposure in X-rays of the stellar-mass BH candidates identified around OB stars.","It will constrain the underlying population of X-ray-faint BHs, the evolution from single to double degenerate binaries, and the progenitors of gravitational wave mergers.","(Abridged)"],"url":"http://arxiv.org/abs/2406.08596v1","category":"astro-ph.HE"}
{"created":"2024-06-12 18:44:34","title":"Family of Exact and Inexact Quantum Speed Limits for Completely Positive and Trace-Preserving Dynamics","abstract":"Traditional quantum speed limits formulated in density matrix space perform poorly for dynamics beyond unitary, as they are generally unattainable and fail to characterize the fastest possible dynamics. To address this, we derive two distinct quantum speed limits in Liouville space for Completely Positive and Trace-Preserving (CPTP) dynamics that outperform previous bounds. The first bound saturates for time-optimal CPTP dynamics, while the second bound is exact for all states and all CPTP dynamics. Our bounds have a clear physical and geometric interpretation arising from the uncertainty of superoperators and the geometry of quantum evolution in Liouville space. They can be regarded as the generalization of the Mandelstam-Tamm bound, providing uncertainty relations between time, energy, and dissipation for open quantum dynamics. Additionally, our bounds are significantly simpler to estimate and experimentally more feasible as they require to compute or measure the overlap of density matrices and the variance of the Liouvillian. We have also obtained the form of the Liouvillian, which generates the time-optimal (fastest) CPTP dynamics for given initial and final states. We give two important applications of our bounds. First, we show that the speed of evolution in Liouville space bounds the growth of the spectral form factor and Krylov complexity of states, which are crucial for studying information scrambling and quantum chaos. Second, using our bounds, we explain the Mpemba effect in non-equilibrium open quantum dynamics.","sentences":["Traditional quantum speed limits formulated in density matrix space perform poorly for dynamics beyond unitary, as they are generally unattainable and fail to characterize the fastest possible dynamics.","To address this, we derive two distinct quantum speed limits in Liouville space for Completely Positive and Trace-Preserving (CPTP) dynamics that outperform previous bounds.","The first bound saturates for time-optimal CPTP dynamics, while the second bound is exact for all states and all CPTP dynamics.","Our bounds have a clear physical and geometric interpretation arising from the uncertainty of superoperators and the geometry of quantum evolution in Liouville space.","They can be regarded as the generalization of the Mandelstam-Tamm bound, providing uncertainty relations between time, energy, and dissipation for open quantum dynamics.","Additionally, our bounds are significantly simpler to estimate and experimentally more feasible as they require to compute or measure the overlap of density matrices and the variance of the Liouvillian.","We have also obtained the form of the Liouvillian, which generates the time-optimal (fastest) CPTP dynamics for given initial and final states.","We give two important applications of our bounds.","First, we show that the speed of evolution in Liouville space bounds the growth of the spectral form factor and Krylov complexity of states, which are crucial for studying information scrambling and quantum chaos.","Second, using our bounds, we explain the Mpemba effect in non-equilibrium open quantum dynamics."],"url":"http://arxiv.org/abs/2406.08584v1","category":"quant-ph"}
{"created":"2024-06-12 18:38:40","title":"Exploring Fact Memorization and Style Imitation in LLMs Using QLoRA: An Experimental Study and Quality Assessment Methods","abstract":"There are various methods for adapting LLMs to different domains. The most common methods are prompting, finetuning, and RAG. In this work, we explore the possibility of adapting a model using one of the PEFT methods - QLoRA. The experiment aims to simulate human responses based on their interviews. The simulation quality is assessed by comparing the quality of the style and the quality of the generated facts.","sentences":["There are various methods for adapting LLMs to different domains.","The most common methods are prompting, finetuning, and RAG.","In this work, we explore the possibility of adapting a model using one of the PEFT methods - QLoRA.","The experiment aims to simulate human responses based on their interviews.","The simulation quality is assessed by comparing the quality of the style and the quality of the generated facts."],"url":"http://arxiv.org/abs/2406.08582v1","category":"cs.CL"}
{"created":"2024-06-12 15:23:53","title":"The observer effect in quantum: the case of classification","abstract":"The observer effect in quantum physics states that observation inevitably influences the system being observed. Our proposed epistemic framework treats the observer as an integral part of sensory information processing within entangled quantum systems, highlighting the subjective and probabilistic aspects of observation and inference. Our study introduces a hierarchical model for fuzzy instance classification, which aligns sensory input with an observer's pre-existing beliefs and associated quantum probability-based truth values. Sensory data evolves via interaction with observer states, as described by the Lindblad master equation, and is then classified adaptively using positive operator-valued measures (POVM). Our parametrization employs measures of concurrent similarity and dissimilarity, facilitating perceptual associations and asymmetric cognition. The observer's position on a skeptic-believer spectrum modulates ambiguous matching of noisy perceptions. We show that sensory information becomes intricately entangled with observer states, yielding a wide array of probabilistic classification results. This framework lays the groundwork for a quantum-probability-based understanding of the observer effect, encouraging further exploration of quantum correlations and properties in cognitive processes.","sentences":["The observer effect in quantum physics states that observation inevitably influences the system being observed.","Our proposed epistemic framework treats the observer as an integral part of sensory information processing within entangled quantum systems, highlighting the subjective and probabilistic aspects of observation and inference.","Our study introduces a hierarchical model for fuzzy instance classification, which aligns sensory input with an observer's pre-existing beliefs and associated quantum probability-based truth values.","Sensory data evolves via interaction with observer states, as described by the Lindblad master equation, and is then classified adaptively using positive operator-valued measures (POVM).","Our parametrization employs measures of concurrent similarity and dissimilarity, facilitating perceptual associations and asymmetric cognition.","The observer's position on a skeptic-believer spectrum modulates ambiguous matching of noisy perceptions.","We show that sensory information becomes intricately entangled with observer states, yielding a wide array of probabilistic classification results.","This framework lays the groundwork for a quantum-probability-based understanding of the observer effect, encouraging further exploration of quantum correlations and properties in cognitive processes."],"url":"http://arxiv.org/abs/2406.08533v1","category":"quant-ph"}
{"created":"2024-06-12 08:51:08","title":"Adaptive Teaching with a Shared Classifier for Knowledge Distillation","abstract":"Knowledge distillation (KD) is a technique used to transfer knowledge from an overparameterized teacher network to a less-parameterized student network, thereby minimizing the incurred performance loss. KD methods can be categorized into offline and online approaches. Offline KD leverages a powerful pretrained teacher network, while online KD allows the teacher network to be adjusted dynamically to enhance the learning effectiveness of the student network. Recently, it has been discovered that sharing the classifier of the teacher network can significantly boost the performance of the student network with only a minimal increase in the number of network parameters. Building on these insights, we propose adaptive teaching with a shared classifier (ATSC). In ATSC, the pretrained teacher network self-adjusts to better align with the learning needs of the student network based on its capabilities, and the student network benefits from the shared classifier, enhancing its performance. Additionally, we extend ATSC to environments with multiple teachers. We conduct extensive experiments, demonstrating the effectiveness of the proposed KD method. Our approach achieves state-of-the-art results on the CIFAR-100 and ImageNet datasets in both single-teacher and multiteacher scenarios, with only a modest increase in the number of required model parameters. The source code is publicly available at https://github.com/random2314235/ATSC.","sentences":["Knowledge distillation (KD) is a technique used to transfer knowledge from an overparameterized teacher network to a less-parameterized student network, thereby minimizing the incurred performance loss.","KD methods can be categorized into offline and online approaches.","Offline KD leverages a powerful pretrained teacher network, while online KD allows the teacher network to be adjusted dynamically to enhance the learning effectiveness of the student network.","Recently, it has been discovered that sharing the classifier of the teacher network can significantly boost the performance of the student network with only a minimal increase in the number of network parameters.","Building on these insights, we propose adaptive teaching with a shared classifier (ATSC).","In ATSC, the pretrained teacher network self-adjusts to better align with the learning needs of the student network based on its capabilities, and the student network benefits from the shared classifier, enhancing its performance.","Additionally, we extend ATSC to environments with multiple teachers.","We conduct extensive experiments, demonstrating the effectiveness of the proposed KD method.","Our approach achieves state-of-the-art results on the CIFAR-100 and ImageNet datasets in both single-teacher and multiteacher scenarios, with only a modest increase in the number of required model parameters.","The source code is publicly available at https://github.com/random2314235/ATSC."],"url":"http://arxiv.org/abs/2406.08528v1","category":"cs.CV"}
{"created":"2024-06-11 17:59:53","title":"Image and Video Tokenization with Binary Spherical Quantization","abstract":"We propose a new transformer-based image and video tokenizer with Binary Spherical Quantization (BSQ). BSQ projects the high-dimensional visual embedding to a lower-dimensional hypersphere and then applies binary quantization. BSQ is (1) parameter-efficient without an explicit codebook, (2) scalable to arbitrary token dimensions, and (3) compact: compressing visual data by up to 100$\\times$ with minimal distortion. Our tokenizer uses a transformer encoder and decoder with simple block-wise causal masking to support variable-length videos as input. The resulting BSQ-ViT achieves state-of-the-art visual reconstruction quality on image and video reconstruction benchmarks with 2.4$\\times$ throughput compared to the best prior methods. Furthermore, by learning an autoregressive prior for adaptive arithmetic coding, BSQ-ViT achieves comparable results on video compression with state-of-the-art video compression standards. BSQ-ViT also enables masked language models to achieve competitive image synthesis quality to GAN- and diffusion-based methods.","sentences":["We propose a new transformer-based image and video tokenizer with Binary Spherical Quantization (BSQ).","BSQ projects the high-dimensional visual embedding to a lower-dimensional hypersphere and then applies binary quantization.","BSQ is (1) parameter-efficient without an explicit codebook, (2) scalable to arbitrary token dimensions, and (3) compact: compressing visual data by up to 100$\\times$ with minimal distortion.","Our tokenizer uses a transformer encoder and decoder with simple block-wise causal masking to support variable-length videos as input.","The resulting BSQ-ViT achieves state-of-the-art visual reconstruction quality on image and video reconstruction benchmarks with 2.4$\\times$ throughput compared to the best prior methods.","Furthermore, by learning an autoregressive prior for adaptive arithmetic coding, BSQ-ViT achieves comparable results on video compression with state-of-the-art video compression standards.","BSQ-ViT also enables masked language models to achieve competitive image synthesis quality to GAN- and diffusion-based methods."],"url":"http://arxiv.org/abs/2406.07548v1","category":"cs.CV"}
{"created":"2024-06-11 17:55:25","title":"MAP: Low-compute Model Merging with Amortized Pareto Fronts via Quadratic Approximation","abstract":"Model merging has emerged as an effective approach to combine multiple single-task models, fine-tuned from the same pre-trained model, into a multitask model. This process typically involves computing a weighted average of the model parameters without any additional training. Existing model-merging methods focus on enhancing average task accuracy. However, interference and conflicts between the objectives of different tasks can lead to trade-offs during model merging. In real-world applications, a set of solutions with various trade-offs can be more informative, helping practitioners make decisions based on diverse preferences. In this paper, we introduce a novel low-compute algorithm, Model Merging with Amortized Pareto Front (MAP). MAP identifies a Pareto set of scaling coefficients for merging multiple models to reflect the trade-offs. The core component of MAP is approximating the evaluation metrics of the various tasks using a quadratic approximation surrogate model derived from a pre-selected set of scaling coefficients, enabling amortized inference. Experimental results on vision and natural language processing tasks show that MAP can accurately identify the Pareto front. To further reduce the required computation of MAP, we propose (1) a Bayesian adaptive sampling algorithm and (2) a nested merging scheme with multiple stages.","sentences":["Model merging has emerged as an effective approach to combine multiple single-task models, fine-tuned from the same pre-trained model, into a multitask model.","This process typically involves computing a weighted average of the model parameters without any additional training.","Existing model-merging methods focus on enhancing average task accuracy.","However, interference and conflicts between the objectives of different tasks can lead to trade-offs during model merging.","In real-world applications, a set of solutions with various trade-offs can be more informative, helping practitioners make decisions based on diverse preferences.","In this paper, we introduce a novel low-compute algorithm, Model Merging with Amortized Pareto Front (MAP).","MAP identifies a Pareto set of scaling coefficients for merging multiple models to reflect the trade-offs.","The core component of MAP is approximating the evaluation metrics of the various tasks using a quadratic approximation surrogate model derived from a pre-selected set of scaling coefficients, enabling amortized inference.","Experimental results on vision and natural language processing tasks show that MAP can accurately identify the Pareto front.","To further reduce the required computation of MAP, we propose (1) a Bayesian adaptive sampling algorithm and (2) a nested merging scheme with multiple stages."],"url":"http://arxiv.org/abs/2406.07529v1","category":"cs.LG"}
{"created":"2024-06-11 17:47:27","title":"Instant 3D Human Avatar Generation using Image Diffusion Models","abstract":"We present AvatarPopUp, a method for fast, high quality 3D human avatar generation from different input modalities, such as images and text prompts and with control over the generated pose and shape. The common theme is the use of diffusion-based image generation networks that are specialized for each particular task, followed by a 3D lifting network. We purposefully decouple the generation from the 3D modeling which allow us to leverage powerful image synthesis priors, trained on billions of text-image pairs. We fine-tune latent diffusion networks with additional image conditioning to solve tasks such as image generation and back-view prediction, and to support qualitatively different multiple 3D hypotheses. Our partial fine-tuning approach allows to adapt the networks for each task without inducing catastrophic forgetting. In our experiments, we demonstrate that our method produces accurate, high-quality 3D avatars with diverse appearance that respect the multimodal text, image, and body control signals. Our approach can produce a 3D model in as few as 2 seconds, a four orders of magnitude speedup w.r.t. the vast majority of existing methods, most of which solve only a subset of our tasks, and with fewer controls, thus enabling applications that require the controlled 3D generation of human avatars at scale. The project website can be found at https://www.nikoskolot.com/avatarpopup/.","sentences":["We present AvatarPopUp, a method for fast, high quality 3D human avatar generation from different input modalities, such as images and text prompts and with control over the generated pose and shape.","The common theme is the use of diffusion-based image generation networks that are specialized for each particular task, followed by a 3D lifting network.","We purposefully decouple the generation from the 3D modeling which allow us to leverage powerful image synthesis priors, trained on billions of text-image pairs.","We fine-tune latent diffusion networks with additional image conditioning to solve tasks such as image generation and back-view prediction, and to support qualitatively different multiple 3D hypotheses.","Our partial fine-tuning approach allows to adapt the networks for each task without inducing catastrophic forgetting.","In our experiments, we demonstrate that our method produces accurate, high-quality 3D avatars with diverse appearance that respect the multimodal text, image, and body control signals.","Our approach can produce a 3D model in as few as 2 seconds, a four orders of magnitude speedup w.r.t.","the vast majority of existing methods, most of which solve only a subset of our tasks, and with fewer controls, thus enabling applications that require the controlled 3D generation of human avatars at scale.","The project website can be found at https://www.nikoskolot.com/avatarpopup/."],"url":"http://arxiv.org/abs/2406.07516v1","category":"cs.CV"}
{"created":"2024-06-11 17:29:51","title":"Transition from decaying to decayless kink oscillations of solar coronal loops","abstract":"The transition of an impulsively excited kink oscillation of a solar coronal loop to an oscillation with a stationary amplitude, i.e., the damping pattern, is determined using the low-dimensional self-oscillation model. In the model, the decayless kink oscillations are sustained by the interaction of the oscillating loop with an external quasi-steady flow. The analytical solution is based on the assumption that the combined effect of the effective dissipation, for example, by resonant absorption, and interaction with an external flow, is weak. The effect is characterised by a dimensionless coupling parameter. The damping pattern is found to depend upon the initial amplitude and the coupling parameter. The approximate expression shows a good agreement with a numerical solution of the self-oscillation equation. The plausibility of the established damping pattern is demonstrated by an observational example. Notably, the damping pattern is not exponential, and the characteristic decay time is different from the time determined by the traditionally used exponential damping fit. Implications of this finding for seismology of the solar coronal plasmas are discussed. In particular, it is suggested that a very rapid, in less than the oscillation period, decay of the oscillation to the stationary level, achieved for larger values of the coupling parameter, can explain the relative rareness of the kink oscillation events.","sentences":["The transition of an impulsively excited kink oscillation of a solar coronal loop to an oscillation with a stationary amplitude, i.e., the damping pattern, is determined using the low-dimensional self-oscillation model.","In the model, the decayless kink oscillations are sustained by the interaction of the oscillating loop with an external quasi-steady flow.","The analytical solution is based on the assumption that the combined effect of the effective dissipation, for example, by resonant absorption, and interaction with an external flow, is weak.","The effect is characterised by a dimensionless coupling parameter.","The damping pattern is found to depend upon the initial amplitude and the coupling parameter.","The approximate expression shows a good agreement with a numerical solution of the self-oscillation equation.","The plausibility of the established damping pattern is demonstrated by an observational example.","Notably, the damping pattern is not exponential, and the characteristic decay time is different from the time determined by the traditionally used exponential damping fit.","Implications of this finding for seismology of the solar coronal plasmas are discussed.","In particular, it is suggested that a very rapid, in less than the oscillation period, decay of the oscillation to the stationary level, achieved for larger values of the coupling parameter, can explain the relative rareness of the kink oscillation events."],"url":"http://arxiv.org/abs/2406.07490v1","category":"astro-ph.SR"}
{"created":"2024-06-11 17:27:23","title":"GLAD: Towards Better Reconstruction with Global and Local Adaptive Diffusion Models for Unsupervised Anomaly Detection","abstract":"Diffusion models have shown superior performance on unsupervised anomaly detection tasks. Since trained with normal data only, diffusion models tend to reconstruct normal counterparts of test images with certain noises added. However, these methods treat all potential anomalies equally, which may cause two main problems. From the global perspective, the difficulty of reconstructing images with different anomalies is uneven. Therefore, instead of utilizing the same setting for all samples, we propose to predict a particular denoising step for each sample by evaluating the difference between image contents and the priors extracted from diffusion models. From the local perspective, reconstructing abnormal regions differs from normal areas even in the same image. Theoretically, the diffusion model predicts a noise for each step, typically following a standard Gaussian distribution. However, due to the difference between the anomaly and its potential normal counterpart, the predicted noise in abnormal regions will inevitably deviate from the standard Gaussian distribution. To this end, we propose introducing synthetic abnormal samples in training to encourage the diffusion models to break through the limitation of standard Gaussian distribution, and a spatial-adaptive feature fusion scheme is utilized during inference. With the above modifications, we propose a global and local adaptive diffusion model (abbreviated to GLAD) for unsupervised anomaly detection, which introduces appealing flexibility and achieves anomaly-free reconstruction while retaining as much normal information as possible. Extensive experiments are conducted on three commonly used anomaly detection datasets (MVTec-AD, MPDD, and VisA) and a printed circuit board dataset (PCB-Bank) we integrated, showing the effectiveness of the proposed method.","sentences":["Diffusion models have shown superior performance on unsupervised anomaly detection tasks.","Since trained with normal data only, diffusion models tend to reconstruct normal counterparts of test images with certain noises added.","However, these methods treat all potential anomalies equally, which may cause two main problems.","From the global perspective, the difficulty of reconstructing images with different anomalies is uneven.","Therefore, instead of utilizing the same setting for all samples, we propose to predict a particular denoising step for each sample by evaluating the difference between image contents and the priors extracted from diffusion models.","From the local perspective, reconstructing abnormal regions differs from normal areas even in the same image.","Theoretically, the diffusion model predicts a noise for each step, typically following a standard Gaussian distribution.","However, due to the difference between the anomaly and its potential normal counterpart, the predicted noise in abnormal regions will inevitably deviate from the standard Gaussian distribution.","To this end, we propose introducing synthetic abnormal samples in training to encourage the diffusion models to break through the limitation of standard Gaussian distribution, and a spatial-adaptive feature fusion scheme is utilized during inference.","With the above modifications, we propose a global and local adaptive diffusion model (abbreviated to GLAD) for unsupervised anomaly detection, which introduces appealing flexibility and achieves anomaly-free reconstruction while retaining as much normal information as possible.","Extensive experiments are conducted on three commonly used anomaly detection datasets (MVTec-AD, MPDD, and VisA) and a printed circuit board dataset (PCB-Bank) we integrated, showing the effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2406.07487v1","category":"cs.CV"}
{"created":"2024-06-11 17:26:14","title":"Towards Generalized Hydrological Forecasting using Transformer Models for 120-Hour Streamflow Prediction","abstract":"This study explores the efficacy of a Transformer model for 120-hour streamflow prediction across 125 diverse locations in Iowa, US. Utilizing data from the preceding 72 hours, including precipitation, evapotranspiration, and discharge values, we developed a generalized model to predict future streamflow. Our approach contrasts with traditional methods that typically rely on location-specific models. We benchmarked the Transformer model's performance against three deep learning models (LSTM, GRU, and Seq2Seq) and the Persistence approach, employing Nash-Sutcliffe Efficiency (NSE), Kling-Gupta Efficiency (KGE), Pearson's r, and Normalized Root Mean Square Error (NRMSE) as metrics. The study reveals the Transformer model's superior performance, maintaining higher median NSE and KGE scores and exhibiting the lowest NRMSE values. This indicates its capability to accurately simulate and predict streamflow, adapting effectively to varying hydrological conditions and geographical variances. Our findings underscore the Transformer model's potential as an advanced tool in hydrological modeling, offering significant improvements over traditional and contemporary approaches.","sentences":["This study explores the efficacy of a Transformer model for 120-hour streamflow prediction across 125 diverse locations in Iowa, US.","Utilizing data from the preceding 72 hours, including precipitation, evapotranspiration, and discharge values, we developed a generalized model to predict future streamflow.","Our approach contrasts with traditional methods that typically rely on location-specific models.","We benchmarked the Transformer model's performance against three deep learning models (LSTM, GRU, and Seq2Seq) and the Persistence approach, employing Nash-Sutcliffe Efficiency (NSE), Kling-Gupta Efficiency (KGE), Pearson's r, and Normalized Root Mean Square Error (NRMSE) as metrics.","The study reveals the Transformer model's superior performance, maintaining higher median NSE and KGE scores and exhibiting the lowest NRMSE values.","This indicates its capability to accurately simulate and predict streamflow, adapting effectively to varying hydrological conditions and geographical variances.","Our findings underscore the Transformer model's potential as an advanced tool in hydrological modeling, offering significant improvements over traditional and contemporary approaches."],"url":"http://arxiv.org/abs/2406.07484v1","category":"cs.LG"}
{"created":"2024-06-11 17:20:01","title":"Choreographing the Rhythms of Observation: Dynamics for Ranged Observer Bipartite-Unipartite SpatioTemporal (ROBUST) Networks","abstract":"Existing network analysis methods struggle to optimize observer placements in dynamic environments with limited visibility. This dissertation introduces the novel ROBUST (Ranged Observer Bipartite-Unipartite SpatioTemporal) framework, offering a significant advancement in modeling, analyzing, and optimizing observer networks within complex spatiotemporal domains. ROBUST leverages a unique bipartite-unipartite approach, distinguishing between observer and observable entities while incorporating spatial constraints and temporal dynamics.   This research extends spatiotemporal network theory by introducing novel graph-based measures, including myopic degree, spatial closeness centrality, and edge length proportion. These measures, coupled with advanced clustering techniques like Proximal Recurrence, provide insights into network structure, resilience, and the effectiveness of observer placements. The ROBUST framework demonstrates superior resource allocation and strategic responsiveness compared to conventional models. Case studies in oceanographic monitoring, urban safety networks, and multi-agent path planning showcases its practical applicability and adaptability. Results demonstrate significant improvements in coverage, response times, and overall network efficiency.   This work paves the way for future research in incorporating imperfect knowledge, refining temporal pathing methodologies, and expanding the scope of applications. By bridging theoretical advancements with practical solutions, ROBUST stands as a significant contribution to the field, promising to inform and inspire ongoing and future endeavors in network optimization and multi-agent system planning.","sentences":["Existing network analysis methods struggle to optimize observer placements in dynamic environments with limited visibility.","This dissertation introduces the novel ROBUST (Ranged Observer Bipartite-Unipartite SpatioTemporal) framework, offering a significant advancement in modeling, analyzing, and optimizing observer networks within complex spatiotemporal domains.","ROBUST leverages a unique bipartite-unipartite approach, distinguishing between observer and observable entities while incorporating spatial constraints and temporal dynamics.   ","This research extends spatiotemporal network theory by introducing novel graph-based measures, including myopic degree, spatial closeness centrality, and edge length proportion.","These measures, coupled with advanced clustering techniques like Proximal Recurrence, provide insights into network structure, resilience, and the effectiveness of observer placements.","The ROBUST framework demonstrates superior resource allocation and strategic responsiveness compared to conventional models.","Case studies in oceanographic monitoring, urban safety networks, and multi-agent path planning showcases its practical applicability and adaptability.","Results demonstrate significant improvements in coverage, response times, and overall network efficiency.   ","This work paves the way for future research in incorporating imperfect knowledge, refining temporal pathing methodologies, and expanding the scope of applications.","By bridging theoretical advancements with practical solutions, ROBUST stands as a significant contribution to the field, promising to inform and inspire ongoing and future endeavors in network optimization and multi-agent system planning."],"url":"http://arxiv.org/abs/2406.07473v1","category":"cs.MA"}
{"created":"2024-06-11 17:09:31","title":"Reconfigurable Intelligent Surfaces in Dynamic Rich Scattering Environments: BiLSTM-Based Optimization for Accurate User Localization","abstract":"The integration of reconfigurable intelligent surfaces (RIS) in wireless environments offers channel programmability and dynamic control over propagation channels, which is expected to play a crucial role in sixth generation (6G) networks. The majority of RIS-related research has focused on simpler, quasi-free-space conditions, where wireless channels are typically modeled analytically. However, many practical localization scenarios unfold in environments characterized by rich scattering that also change over time. These dynamic and complex conditions pose significant challenges in determining the optimal RIS configuration to maximize localization accuracy. In this paper, we present our approach to overcoming this challenge. This paper introduces a novel approach that leverages a bidirectional long-short term memory (biLSTM) network, trained with a simulator that accurately reflects wave physics, to capture the relationship between wireless channels and the RIS configuration under dynamic, rich-scattering conditions. We use this approach to optimize RIS configurations for enhanced user equipment (UE) localization, measured by mean squared error (MSE). Through extensive simulations, we demonstrate that our approach adapts RIS configurations to significantly improve localization accuracy in such dynamically changing rich scattering environments.","sentences":["The integration of reconfigurable intelligent surfaces (RIS) in wireless environments offers channel programmability and dynamic control over propagation channels, which is expected to play a crucial role in sixth generation (6G) networks.","The majority of RIS-related research has focused on simpler, quasi-free-space conditions, where wireless channels are typically modeled analytically.","However, many practical localization scenarios unfold in environments characterized by rich scattering that also change over time.","These dynamic and complex conditions pose significant challenges in determining the optimal RIS configuration to maximize localization accuracy.","In this paper, we present our approach to overcoming this challenge.","This paper introduces a novel approach that leverages a bidirectional long-short term memory (biLSTM) network, trained with a simulator that accurately reflects wave physics, to capture the relationship between wireless channels and the RIS configuration under dynamic, rich-scattering conditions.","We use this approach to optimize RIS configurations for enhanced user equipment (UE) localization, measured by mean squared error (MSE).","Through extensive simulations, we demonstrate that our approach adapts RIS configurations to significantly improve localization accuracy in such dynamically changing rich scattering environments."],"url":"http://arxiv.org/abs/2406.07463v1","category":"eess.SP"}
{"created":"2024-06-11 17:01:45","title":"fKAN: Fractional Kolmogorov-Arnold Networks with trainable Jacobi basis functions","abstract":"Recent advancements in neural network design have given rise to the development of Kolmogorov-Arnold Networks (KANs), which enhance speed, interpretability, and precision. This paper presents the Fractional Kolmogorov-Arnold Network (fKAN), a novel neural network architecture that incorporates the distinctive attributes of KANs with a trainable adaptive fractional-orthogonal Jacobi function as its basis function. By leveraging the unique mathematical properties of fractional Jacobi functions, including simple derivative formulas, non-polynomial behavior, and activity for both positive and negative input values, this approach ensures efficient learning and enhanced accuracy. The proposed architecture is evaluated across a range of tasks in deep learning and physics-informed deep learning. Precision is tested on synthetic regression data, image classification, image denoising, and sentiment analysis. Additionally, the performance is measured on various differential equations, including ordinary, partial, and fractional delay differential equations. The results demonstrate that integrating fractional Jacobi functions into KANs significantly improves training speed and performance across diverse fields and applications.","sentences":["Recent advancements in neural network design have given rise to the development of Kolmogorov-Arnold Networks (KANs), which enhance speed, interpretability, and precision.","This paper presents the Fractional Kolmogorov-Arnold Network (fKAN), a novel neural network architecture that incorporates the distinctive attributes of KANs with a trainable adaptive fractional-orthogonal Jacobi function as its basis function.","By leveraging the unique mathematical properties of fractional Jacobi functions, including simple derivative formulas, non-polynomial behavior, and activity for both positive and negative input values, this approach ensures efficient learning and enhanced accuracy.","The proposed architecture is evaluated across a range of tasks in deep learning and physics-informed deep learning.","Precision is tested on synthetic regression data, image classification, image denoising, and sentiment analysis.","Additionally, the performance is measured on various differential equations, including ordinary, partial, and fractional delay differential equations.","The results demonstrate that integrating fractional Jacobi functions into KANs significantly improves training speed and performance across diverse fields and applications."],"url":"http://arxiv.org/abs/2406.07456v1","category":"cs.LG"}
{"created":"2024-06-11 17:01:41","title":"Reinforcement Learning from Human Feedback without Reward Inference: Model-Free Algorithm and Instance-Dependent Analysis","abstract":"In this paper, we study reinforcement learning from human feedback (RLHF) under an episodic Markov decision process with a general trajectory-wise reward model. We developed a model-free RLHF best policy identification algorithm, called $\\mathsf{BSAD}$, without explicit reward model inference, which is a critical intermediate step in the contemporary RLHF paradigms for training large language models (LLM). The algorithm identifies the optimal policy directly from human preference information in a backward manner, employing a dueling bandit sub-routine that constantly duels actions to identify the superior one. $\\mathsf{BSAD}$ adopts a reward-free exploration and best-arm-identification-like adaptive stopping criteria to equalize the visitation among all states in the same decision step while moving to the previous step as soon as the optimal action is identifiable, leading to a provable, instance-dependent sample complexity $\\tilde{\\mathcal{O}}(c_{\\mathcal{M}}SA^3H^3M\\log\\frac{1}{\\delta})$ which resembles the result in classic RL, where $c_{\\mathcal{M}}$ is the instance-dependent constant and $M$ is the batch size. Moreover, $\\mathsf{BSAD}$ can be transformed into an explore-then-commit algorithm with logarithmic regret and generalized to discounted MDPs using a frame-based approach. Our results show: (i) sample-complexity-wise, RLHF is not significantly harder than classic RL and (ii) end-to-end RLHF may deliver improved performance by avoiding pitfalls in reward inferring such as overfit and distribution shift.","sentences":["In this paper, we study reinforcement learning from human feedback (RLHF) under an episodic Markov decision process with a general trajectory-wise reward model.","We developed a model-free RLHF best policy identification algorithm, called $\\mathsf{BSAD}$, without explicit reward model inference, which is a critical intermediate step in the contemporary RLHF paradigms for training large language models (LLM).","The algorithm identifies the optimal policy directly from human preference information in a backward manner, employing a dueling bandit sub-routine that constantly duels actions to identify the superior one.","$\\mathsf{BSAD}$ adopts a reward-free exploration and best-arm-identification-like adaptive stopping criteria to equalize the visitation among all states in the same decision step while moving to the previous step as soon as the optimal action is identifiable, leading to a provable, instance-dependent sample complexity $\\tilde{\\mathcal{O}}(c_{\\mathcal{M}}SA^3H^3M\\log\\frac{1}{\\delta})$ which resembles the result in classic RL, where $c_{\\mathcal{M}}$ is the instance-dependent constant and $M$ is the batch size.","Moreover, $\\mathsf{BSAD}$ can be transformed into an explore-then-commit algorithm with logarithmic regret and generalized to discounted MDPs using a frame-based approach.","Our results show: (i) sample-complexity-wise, RLHF is not significantly harder than classic RL and (ii) end-to-end RLHF may deliver improved performance by avoiding pitfalls in reward inferring such as overfit and distribution shift."],"url":"http://arxiv.org/abs/2406.07455v1","category":"cs.LG"}
{"created":"2024-06-11 16:34:02","title":"Learning Domain-Invariant Features for Out-of-Context News Detection","abstract":"Multimodal out-of-context news is a common type of misinformation on online media platforms. This involves posting a caption, alongside an invalid out-of-context news image. Reflecting its importance, researchers have developed models to detect such misinformation. However, a common limitation of these models is that they only consider the scenario where pre-labeled data is available for each domain, failing to address the out-of-context news detection on unlabeled domains (e.g., unverified news on new topics or agencies). In this work, we therefore focus on domain adaptive out-of-context news detection. In order to effectively adapt the detection model to unlabeled news topics or agencies, we propose ConDA-TTA (Contrastive Domain Adaptation with Test-Time Adaptation) which applies contrastive learning and maximum mean discrepancy (MMD) to learn the domain-invariant feature. In addition, it leverages target domain statistics during test-time to further assist domain adaptation. Experimental results show that our approach outperforms baselines in 5 out of 7 domain adaptation settings on two public datasets, by as much as 2.93% in F1 and 2.08% in accuracy.","sentences":["Multimodal out-of-context news is a common type of misinformation on online media platforms.","This involves posting a caption, alongside an invalid out-of-context news image.","Reflecting its importance, researchers have developed models to detect such misinformation.","However, a common limitation of these models is that they only consider the scenario where pre-labeled data is available for each domain, failing to address the out-of-context news detection on unlabeled domains (e.g., unverified news on new topics or agencies).","In this work, we therefore focus on domain adaptive out-of-context news detection.","In order to effectively adapt the detection model to unlabeled news topics or agencies, we propose ConDA-TTA (Contrastive Domain Adaptation with Test-Time Adaptation) which applies contrastive learning and maximum mean discrepancy (MMD) to learn the domain-invariant feature.","In addition, it leverages target domain statistics during test-time to further assist domain adaptation.","Experimental results show that our approach outperforms baselines in 5 out of 7 domain adaptation settings on two public datasets, by as much as 2.93% in F1 and 2.08% in accuracy."],"url":"http://arxiv.org/abs/2406.07430v1","category":"cs.CL"}
{"created":"2024-06-11 16:30:30","title":"GemNet: Menu-Based, Strategy-Proof Multi-Bidder Auctions Through Deep Learning","abstract":"Differentiable economics uses deep learning for automated mechanism design. Despite strong progress, it has remained an open problem to learn multi-bidder, general, and fully strategy-proof (SP) auctions. We introduce GEneral Menu-based NETwork (GemNet), which significantly extends the menu-based approach of RochetNet [D\\\"utting et al., 2023] to the multi-bidder setting. The challenge in achieving SP is to learn bidder-independent menus that are feasible, so that the optimal menu choices for each bidder do not over-allocate items when taken together (we call this menu compatibility). GemNet penalizes the failure of menu compatibility during training, and transforms learned menus after training through price changes, by considering a set of discretized bidder values and reasoning about Lipschitz smoothness to guarantee menu compatibility on the entire value space. This approach is general, leaving undisturbed trained menus that already satisfy menu compatibility and reducing to RochetNet for a single bidder. Mixed-integer linear programs are used for menu transforms and through a number of optimizations, including adaptive grids and methods to skip menu elements, we scale to large auction design problems. GemNet learns auctions with better revenue than affine maximization methods, achieves exact SP whereas previous general multi-bidder methods are approximately SP, and offers greatly enhanced interpretability.","sentences":["Differentiable economics uses deep learning for automated mechanism design.","Despite strong progress, it has remained an open problem to learn multi-bidder, general, and fully strategy-proof (SP) auctions.","We introduce GEneral Menu-based NETwork (GemNet), which significantly extends the menu-based approach of RochetNet [D\\\"utting et al., 2023] to the multi-bidder setting.","The challenge in achieving SP is to learn bidder-independent menus that are feasible, so that the optimal menu choices for each bidder do not over-allocate items when taken together (we call this menu compatibility).","GemNet penalizes the failure of menu compatibility during training, and transforms learned menus after training through price changes, by considering a set of discretized bidder values and reasoning about Lipschitz smoothness to guarantee menu compatibility on the entire value space.","This approach is general, leaving undisturbed trained menus that already satisfy menu compatibility and reducing to RochetNet for a single bidder.","Mixed-integer linear programs are used for menu transforms and through a number of optimizations, including adaptive grids and methods to skip menu elements, we scale to large auction design problems.","GemNet learns auctions with better revenue than affine maximization methods, achieves exact SP whereas previous general multi-bidder methods are approximately SP, and offers greatly enhanced interpretability."],"url":"http://arxiv.org/abs/2406.07428v1","category":"cs.GT"}
{"created":"2024-06-11 16:21:57","title":"Graph Reasoning for Explainable Cold Start Recommendation","abstract":"The cold start problem, where new users or items have no interaction history, remains a critical challenge in recommender systems (RS). A common solution involves using Knowledge Graphs (KG) to train entity embeddings or Graph Neural Networks (GNNs). Since KGs incorporate auxiliary data and not just user/item interactions, these methods can make relevant recommendations for cold users or items. Graph Reasoning (GR) methods, however, find paths from users to items to recommend using relations in the KG and, in the context of RS, have been used for interpretability. In this study, we propose GRECS: a framework for adapting GR to cold start recommendations. By utilizing explicit paths starting for users rather than relying only on entity embeddings, GRECS can find items corresponding to users' preferences by navigating the graph, even when limited information about users is available. Our experiments show that GRECS mitigates the cold start problem and outperforms competitive baselines across 5 standard datasets while being explainable. This study highlights the potential of GR for developing explainable recommender systems better suited for managing cold users and items.","sentences":["The cold start problem, where new users or items have no interaction history, remains a critical challenge in recommender systems (RS).","A common solution involves using Knowledge Graphs (KG) to train entity embeddings or Graph Neural Networks (GNNs).","Since KGs incorporate auxiliary data and not just user/item interactions, these methods can make relevant recommendations for cold users or items.","Graph Reasoning (GR) methods, however, find paths from users to items to recommend using relations in the KG and, in the context of RS, have been used for interpretability.","In this study, we propose GRECS: a framework for adapting GR to cold start recommendations.","By utilizing explicit paths starting for users rather than relying only on entity embeddings, GRECS can find items corresponding to users' preferences by navigating the graph, even when limited information about users is available.","Our experiments show that GRECS mitigates the cold start problem and outperforms competitive baselines across 5 standard datasets while being explainable.","This study highlights the potential of GR for developing explainable recommender systems better suited for managing cold users and items."],"url":"http://arxiv.org/abs/2406.07420v1","category":"cs.IR"}
{"created":"2024-06-11 16:21:48","title":"Single and merger soliton dynamics in scalar field dark matter with and without self-interactions","abstract":"(abridged)Scalar field dark matter (SFDM) made of bosons has become a popular alternative to the CDM paradigm, especially for its potential to cure the so-called \"small-scale problems\" of CDM. Cosmological simulations have determined that SFDM halos exhibit a core-envelope structure, but they are computationally expensive. Halo cores have been found to be well approximated by \"solitons\". The study of single soliton and multiple soliton merger dynamics constitutes a more feasible approach to investigate in detail the genuine quantum dynamics of SFDM and its interplay with self-gravity for a multitude of free boson parameters. In this paper, we present dedicated simulations of single solitons and binary soliton mergers, for models without and with a 2-boson, repulsive, weak to intermediate self-interaction (SI), as well as multiple soliton mergers without SI. We adapt the open-source code Pyultralight to simulate solitons with SI. We derive numerical scaling relations between the central density and mass of solitons for several values of SI and find deviations from the monotonic relations known from fuzzy dark matter (no SI), or the strongly repulsive Thomas-Fermi regime. Solitons with SI exemplify larger cores and lower central densities, compared to solitons without SI. Using our simulations, we extract numerical density profiles for solitons and post-merger objects, and fit them to analytic functions of previous literature. We find a mild preference for Gaussian cores for objects with SI, while the envelopes of post-mergers can be fit to NFW profiles albeit with some caution as we discuss. Similar to previous work, we find global, persistent oscillations for solitons as well as post-mergers, confirming that self-gravitating SFDM has very long relaxation times, although objects with SI exhibit oscillations of comparatively smaller amplitude.","sentences":["(abridged)Scalar field dark matter (SFDM) made of bosons has become a popular alternative to the CDM paradigm, especially for its potential to cure the so-called \"small-scale problems\" of CDM.","Cosmological simulations have determined that SFDM halos exhibit a core-envelope structure, but they are computationally expensive.","Halo cores have been found to be well approximated by \"solitons\".","The study of single soliton and multiple soliton merger dynamics constitutes a more feasible approach to investigate in detail the genuine quantum dynamics of SFDM and its interplay with self-gravity for a multitude of free boson parameters.","In this paper, we present dedicated simulations of single solitons and binary soliton mergers, for models without and with a 2-boson, repulsive, weak to intermediate self-interaction (SI), as well as multiple soliton mergers without SI.","We adapt the open-source code Pyultralight to simulate solitons with SI.","We derive numerical scaling relations between the central density and mass of solitons for several values of SI and find deviations from the monotonic relations known from fuzzy dark matter (no SI), or the strongly repulsive Thomas-Fermi regime.","Solitons with SI exemplify larger cores and lower central densities, compared to solitons without SI.","Using our simulations, we extract numerical density profiles for solitons and post-merger objects, and fit them to analytic functions of previous literature.","We find a mild preference for Gaussian cores for objects with SI, while the envelopes of post-mergers can be fit to NFW profiles albeit with some caution as we discuss.","Similar to previous work, we find global, persistent oscillations for solitons as well as post-mergers, confirming that self-gravitating SFDM has very long relaxation times, although objects with SI exhibit oscillations of comparatively smaller amplitude."],"url":"http://arxiv.org/abs/2406.07419v1","category":"astro-ph.CO"}
{"created":"2024-06-11 16:21:33","title":"Enhanced Gene Selection in Single-Cell Genomics: Pre-Filtering Synergy and Reinforced Optimization","abstract":"Recent advancements in single-cell genomics necessitate precision in gene panel selection to interpret complex biological data effectively. Those methods aim to streamline the analysis of scRNA-seq data by focusing on the most informative genes that contribute significantly to the specific analysis task. Traditional selection methods, which often rely on expert domain knowledge, embedded machine learning models, or heuristic-based iterative optimization, are prone to biases and inefficiencies that may obscure critical genomic signals. Recognizing the limitations of traditional methods, we aim to transcend these constraints with a refined strategy. In this study, we introduce an iterative gene panel selection strategy that is applicable to clustering tasks in single-cell genomics. Our method uniquely integrates results from other gene selection algorithms, providing valuable preliminary boundaries or prior knowledge as initial guides in the search space to enhance the efficiency of our framework. Furthermore, we incorporate the stochastic nature of the exploration process in reinforcement learning (RL) and its capability for continuous optimization through reward-based feedback. This combination mitigates the biases inherent in the initial boundaries and harnesses RL's adaptability to refine and target gene panel selection dynamically. To illustrate the effectiveness of our method, we conducted detailed comparative experiments, case studies, and visualization analysis.","sentences":["Recent advancements in single-cell genomics necessitate precision in gene panel selection to interpret complex biological data effectively.","Those methods aim to streamline the analysis of scRNA-seq data by focusing on the most informative genes that contribute significantly to the specific analysis task.","Traditional selection methods, which often rely on expert domain knowledge, embedded machine learning models, or heuristic-based iterative optimization, are prone to biases and inefficiencies that may obscure critical genomic signals.","Recognizing the limitations of traditional methods, we aim to transcend these constraints with a refined strategy.","In this study, we introduce an iterative gene panel selection strategy that is applicable to clustering tasks in single-cell genomics.","Our method uniquely integrates results from other gene selection algorithms, providing valuable preliminary boundaries or prior knowledge as initial guides in the search space to enhance the efficiency of our framework.","Furthermore, we incorporate the stochastic nature of the exploration process in reinforcement learning (RL) and its capability for continuous optimization through reward-based feedback.","This combination mitigates the biases inherent in the initial boundaries and harnesses RL's adaptability to refine and target gene panel selection dynamically.","To illustrate the effectiveness of our method, we conducted detailed comparative experiments, case studies, and visualization analysis."],"url":"http://arxiv.org/abs/2406.07418v1","category":"cs.AI"}
{"created":"2024-06-11 16:10:37","title":"Enhancing Tabular Data Optimization with a Flexible Graph-based Reinforced Exploration Strategy","abstract":"Tabular data optimization methods aim to automatically find an optimal feature transformation process that generates high-value features and improves the performance of downstream machine learning tasks. Current frameworks for automated feature transformation rely on iterative sequence generation tasks, optimizing decision strategies through performance feedback from downstream tasks. However, these approaches fail to effectively utilize historical decision-making experiences and overlook potential relationships among generated features, thus limiting the depth of knowledge extraction. Moreover, the granularity of the decision-making process lacks dynamic backtracking capabilities for individual features, leading to insufficient adaptability when encountering inefficient pathways, adversely affecting overall robustness and exploration efficiency. To address the limitations observed in current automatic feature engineering frameworks, we introduce a novel method that utilizes a feature-state transformation graph to effectively preserve the entire feature transformation journey, where each node represents a specific transformation state. During exploration, three cascading agents iteratively select nodes and idea mathematical operations to generate new transformation states. This strategy leverages the inherent properties of the graph structure, allowing for the preservation and reuse of valuable transformations. It also enables backtracking capabilities through graph pruning techniques, which can rectify inefficient transformation paths. To validate the efficacy and flexibility of our approach, we conducted comprehensive experiments and detailed case studies, demonstrating superior performance in diverse scenarios.","sentences":["Tabular data optimization methods aim to automatically find an optimal feature transformation process that generates high-value features and improves the performance of downstream machine learning tasks.","Current frameworks for automated feature transformation rely on iterative sequence generation tasks, optimizing decision strategies through performance feedback from downstream tasks.","However, these approaches fail to effectively utilize historical decision-making experiences and overlook potential relationships among generated features, thus limiting the depth of knowledge extraction.","Moreover, the granularity of the decision-making process lacks dynamic backtracking capabilities for individual features, leading to insufficient adaptability when encountering inefficient pathways, adversely affecting overall robustness and exploration efficiency.","To address the limitations observed in current automatic feature engineering frameworks, we introduce a novel method that utilizes a feature-state transformation graph to effectively preserve the entire feature transformation journey, where each node represents a specific transformation state.","During exploration, three cascading agents iteratively select nodes and idea mathematical operations to generate new transformation states.","This strategy leverages the inherent properties of the graph structure, allowing for the preservation and reuse of valuable transformations.","It also enables backtracking capabilities through graph pruning techniques, which can rectify inefficient transformation paths.","To validate the efficacy and flexibility of our approach, we conducted comprehensive experiments and detailed case studies, demonstrating superior performance in diverse scenarios."],"url":"http://arxiv.org/abs/2406.07404v1","category":"cs.LG"}
{"created":"2024-06-11 16:07:08","title":"Redefining Automotive Radar Imaging: A Domain-Informed 1D Deep Learning Approach for High-Resolution and Efficient Performance","abstract":"Millimeter-wave (mmWave) radars are indispensable for perception tasks of autonomous vehicles, thanks to their resilience in challenging weather conditions. Yet, their deployment is often limited by insufficient spatial resolution for precise semantic scene interpretation. Classical super-resolution techniques adapted from optical imaging inadequately address the distinct characteristics of radar signal data. In response, our study redefines radar imaging super-resolution as a one-dimensional (1D) signal super-resolution spectra estimation problem by harnessing the radar signal processing domain knowledge, introducing innovative data normalization and a domain-informed signal-to-noise ratio (SNR)-guided loss function. Our tailored deep learning network for automotive radar imaging exhibits remarkable scalability, parameter efficiency and fast inference speed, alongside enhanced performance in terms of radar imaging quality and resolution. Extensive testing confirms that our SR-SPECNet sets a new benchmark in producing high-resolution radar range-azimuth images, outperforming existing methods across varied antenna configurations and dataset sizes. Source code and new radar dataset will be made publicly available online.","sentences":["Millimeter-wave (mmWave) radars are indispensable for perception tasks of autonomous vehicles, thanks to their resilience in challenging weather conditions.","Yet, their deployment is often limited by insufficient spatial resolution for precise semantic scene interpretation.","Classical super-resolution techniques adapted from optical imaging inadequately address the distinct characteristics of radar signal data.","In response, our study redefines radar imaging super-resolution as a one-dimensional (1D) signal super-resolution spectra estimation problem by harnessing the radar signal processing domain knowledge, introducing innovative data normalization and a domain-informed signal-to-noise ratio (SNR)-guided loss function.","Our tailored deep learning network for automotive radar imaging exhibits remarkable scalability, parameter efficiency and fast inference speed, alongside enhanced performance in terms of radar imaging quality and resolution.","Extensive testing confirms that our SR-SPECNet sets a new benchmark in producing high-resolution radar range-azimuth images, outperforming existing methods across varied antenna configurations and dataset sizes.","Source code and new radar dataset will be made publicly available online."],"url":"http://arxiv.org/abs/2406.07399v1","category":"cs.LG"}
{"created":"2024-06-11 15:50:35","title":"Fast Adaptive Meta-Heuristic for Large-Scale Facility Location Problem","abstract":"Facility location problems have been a major research area of interest in the last several decades. In particular, uncapacitated location problems (ULP) have enormous applications. Variations of ULP often appear, especially as large-scale subproblems in more complex combinatorial optimization problems. Although many researchers have studied different versions of ULP (e.g., uncapacitated facility location problem (UCFLP) and p-Median problem), most of these authors have considered small to moderately sized problems. In this paper, we address the ULP and provide a fast adaptive meta-heuristic for large-scale problems. The approach is based on critical event memory tabu search. For the diversification component of the algorithm, we have chosen a procedure based on a sequencing problem commonly used for traveling salesman-type problems. The efficacy of this approach is evaluated across a diverse range of benchmark problems sourced from the Internet, with a comprehensive comparison against four prominent algorithms in the literature. The proposed adaptive critical event tabu search (ACETS) demonstrates remarkable effectiveness for large-scale problems. The algorithm successfully solved all problems optimally within a short computing time. Notably, ACETS discovered three best new solutions for benchmark problems, specifically for Asymmetric 500A-1, Asymmetric 750A-1, and Symmetric 750B-4, underscoring its innovative and robust nature.","sentences":["Facility location problems have been a major research area of interest in the last several decades.","In particular, uncapacitated location problems (ULP) have enormous applications.","Variations of ULP often appear, especially as large-scale subproblems in more complex combinatorial optimization problems.","Although many researchers have studied different versions of ULP (e.g., uncapacitated facility location problem (UCFLP) and p-Median problem), most of these authors have considered small to moderately sized problems.","In this paper, we address the ULP and provide a fast adaptive meta-heuristic for large-scale problems.","The approach is based on critical event memory tabu search.","For the diversification component of the algorithm, we have chosen a procedure based on a sequencing problem commonly used for traveling salesman-type problems.","The efficacy of this approach is evaluated across a diverse range of benchmark problems sourced from the Internet, with a comprehensive comparison against four prominent algorithms in the literature.","The proposed adaptive critical event tabu search (ACETS) demonstrates remarkable effectiveness for large-scale problems.","The algorithm successfully solved all problems optimally within a short computing time.","Notably, ACETS discovered three best new solutions for benchmark problems, specifically for Asymmetric 500A-1, Asymmetric 750A-1, and Symmetric 750B-4, underscoring its innovative and robust nature."],"url":"http://arxiv.org/abs/2406.07382v1","category":"math.OC"}
{"created":"2024-06-11 15:32:39","title":"Fast and accurate evaluation of Biot-Savart integrals over spatial curves","abstract":"The Biot-Savart law is relevant in physical contexts including electromagnetism and fluid dynamics. In the latter case, when the rotation of a fluid is confined to a set of very thin vortex filaments, this law describes the velocity field induced by the spatial arrangement of these objects. The Biot-Savart law is at the core of vortex methods used in the simulation of classical and quantum fluid flows. Naive methods are inefficient when dealing with large numbers of vortex elements, which makes them inadequate for simulating turbulent vortex flows. Here we exploit a direct analogy between the Biot-Savart law and electrostatics to adapt Ewald summation methods, routinely used in molecular dynamics simulations, to vortex filament simulations in three-dimensional periodic domains. In this context, the basic idea is to split the induced velocity onto (i) a coarse-grained velocity generated by a Gaussian-filtered vorticity field, and (ii) a short-range correction accounting for near-singular behaviour near the vortices. The former component can be accurately and efficiently evaluated using the nonuniform fast Fourier transform algorithm. Analytical accuracy estimates are provided as a function of the parameters entering the method. We also discuss how to properly account for the finite vortex core size in kinetic energy estimations. Using numerical experiments, we verify the accuracy and the conservation properties of the proposed approach. Moreover, we demonstrate the $O(N \\log N)$ complexity of the method over a wide range of problem sizes $N$, considerably better than the $O(N^2)$ cost of a naive approach.","sentences":["The Biot-Savart law is relevant in physical contexts including electromagnetism and fluid dynamics.","In the latter case, when the rotation of a fluid is confined to a set of very thin vortex filaments, this law describes the velocity field induced by the spatial arrangement of these objects.","The Biot-Savart law is at the core of vortex methods used in the simulation of classical and quantum fluid flows.","Naive methods are inefficient when dealing with large numbers of vortex elements, which makes them inadequate for simulating turbulent vortex flows.","Here we exploit a direct analogy between the Biot-Savart law and electrostatics to adapt Ewald summation methods, routinely used in molecular dynamics simulations, to vortex filament simulations in three-dimensional periodic domains.","In this context, the basic idea is to split the induced velocity onto (i) a coarse-grained velocity generated by a Gaussian-filtered vorticity field, and (ii) a short-range correction accounting for near-singular behaviour near the vortices.","The former component can be accurately and efficiently evaluated using the nonuniform fast Fourier transform algorithm.","Analytical accuracy estimates are provided as a function of the parameters entering the method.","We also discuss how to properly account for the finite vortex core size in kinetic energy estimations.","Using numerical experiments, we verify the accuracy and the conservation properties of the proposed approach.","Moreover, we demonstrate the $O(N \\log N)$ complexity of the method over a wide range of problem sizes $N$, considerably better than the $O(N^2)$ cost of a naive approach."],"url":"http://arxiv.org/abs/2406.07366v1","category":"physics.comp-ph"}
{"created":"2024-06-11 15:32:32","title":"BvSP: Broad-view Soft Prompting for Few-Shot Aspect Sentiment Quad Prediction","abstract":"Aspect sentiment quad prediction (ASQP) aims to predict four aspect-based elements, including aspect term, opinion term, aspect category, and sentiment polarity. In practice, unseen aspects, due to distinct data distribution, impose many challenges for a trained neural model. Motivated by this, this work formulates ASQP into the few-shot scenario, which aims for fast adaptation in real applications. Therefore, we first construct a few-shot ASQP dataset (FSQP) that contains richer categories and is more balanced for the few-shot study. Moreover, recent methods extract quads through a generation paradigm, which involves converting the input sentence into a templated target sequence. However, they primarily focus on the utilization of a single template or the consideration of different template orders, thereby overlooking the correlations among various templates. To tackle this issue, we further propose a Broadview Soft Prompting (BvSP) method that aggregates multiple templates with a broader view by taking into account the correlation between the different templates. Specifically, BvSP uses the pre-trained language model to select the most relevant k templates with Jensen-Shannon divergence. BvSP further introduces soft prompts to guide the pre-trained language model using the selected templates. Then, we aggregate the results of multi-templates by voting mechanism. Empirical results demonstrate that BvSP significantly outperforms the stateof-the-art methods under four few-shot settings and other public datasets. Our code and dataset are available at https://github.com/byinhao/BvSP.","sentences":["Aspect sentiment quad prediction (ASQP) aims to predict four aspect-based elements, including aspect term, opinion term, aspect category, and sentiment polarity.","In practice, unseen aspects, due to distinct data distribution, impose many challenges for a trained neural model.","Motivated by this, this work formulates ASQP into the few-shot scenario, which aims for fast adaptation in real applications.","Therefore, we first construct a few-shot ASQP dataset (FSQP) that contains richer categories and is more balanced for the few-shot study.","Moreover, recent methods extract quads through a generation paradigm, which involves converting the input sentence into a templated target sequence.","However, they primarily focus on the utilization of a single template or the consideration of different template orders, thereby overlooking the correlations among various templates.","To tackle this issue, we further propose a Broadview Soft Prompting (BvSP) method that aggregates multiple templates with a broader view by taking into account the correlation between the different templates.","Specifically, BvSP uses the pre-trained language model to select the most relevant k templates with Jensen-Shannon divergence.","BvSP further introduces soft prompts to guide the pre-trained language model using the selected templates.","Then, we aggregate the results of multi-templates by voting mechanism.","Empirical results demonstrate that BvSP significantly outperforms the stateof-the-art methods under four few-shot settings and other public datasets.","Our code and dataset are available at https://github.com/byinhao/BvSP."],"url":"http://arxiv.org/abs/2406.07365v1","category":"cs.CL"}
{"created":"2024-06-11 15:27:02","title":"A mechanical qubit","abstract":"Strong nonlinear interactions between quantized excitations are an important resource for quantum technologies based on bosonic oscillator modes. However, most electromagnetic and mechanical nonlinearities arising from intrinsic material properties are far too weak compared to dissipation in the system to allow for nonlinear effects to be observed on the single-quantum level. To overcome this limitation, electromagnetic resonators in both the optical and microwave frequency regimes have been coupled to other strongly nonlinear quantum systems such as atoms and superconducting qubits, allowing for the demonstration of effects such as photon blockade and coherent quantum protocols using the Kerr effect. Here, we demonstrate the realization of the single-phonon nonlinear regime in a solid-state mechanical system. The single-phonon anharmonicity in our system exceeds the decoherence rate by a factor of 6.8, allowing us to use the lowest two energy levels of the resonator as a mechanical qubit, for which we show initialization, readout, and a complete set of direct single qubit gates. Our work adds another unique capability to a powerful quantum acoustics platform for quantum simulations, sensing, and information processing.","sentences":["Strong nonlinear interactions between quantized excitations are an important resource for quantum technologies based on bosonic oscillator modes.","However, most electromagnetic and mechanical nonlinearities arising from intrinsic material properties are far too weak compared to dissipation in the system to allow for nonlinear effects to be observed on the single-quantum level.","To overcome this limitation, electromagnetic resonators in both the optical and microwave frequency regimes have been coupled to other strongly nonlinear quantum systems such as atoms and superconducting qubits, allowing for the demonstration of effects such as photon blockade and coherent quantum protocols using the Kerr effect.","Here, we demonstrate the realization of the single-phonon nonlinear regime in a solid-state mechanical system.","The single-phonon anharmonicity in our system exceeds the decoherence rate by a factor of 6.8, allowing us to use the lowest two energy levels of the resonator as a mechanical qubit, for which we show initialization, readout, and a complete set of direct single qubit gates.","Our work adds another unique capability to a powerful quantum acoustics platform for quantum simulations, sensing, and information processing."],"url":"http://arxiv.org/abs/2406.07360v1","category":"quant-ph"}
{"created":"2024-06-11 15:08:14","title":"EdgeTimer: Adaptive Multi-Timescale Scheduling in Mobile Edge Computing with Deep Reinforcement Learning","abstract":"In mobile edge computing (MEC), resource scheduling is crucial to task requests' performance and service providers' cost, involving multi-layer heterogeneous scheduling decisions. Existing schedulers typically adopt static timescales to regularly update scheduling decisions of each layer, without adaptive adjustment of timescales for different layers, resulting in potentially poor performance in practice.   We notice that the adaptive timescales would significantly improve the trade-off between the operation cost and delay performance. Based on this insight, we propose EdgeTimer, the first work to automatically generate adaptive timescales to update multi-layer scheduling decisions using deep reinforcement learning (DRL). First, EdgeTimer uses a three-layer hierarchical DRL framework to decouple the multi-layer decision-making task into a hierarchy of independent sub-tasks for improving learning efficiency. Second, to cope with each sub-task, EdgeTimer adopts a safe multi-agent DRL algorithm for decentralized scheduling while ensuring system reliability. We apply EdgeTimer to a wide range of Kubernetes scheduling rules, and evaluate it using production traces with different workload patterns. Extensive trace-driven experiments demonstrate that EdgeTimer can learn adaptive timescales, irrespective of workload patterns and built-in scheduling rules. It obtains up to 9.1x more profit than existing approaches without sacrificing the delay performance.","sentences":["In mobile edge computing (MEC), resource scheduling is crucial to task requests' performance and service providers' cost, involving multi-layer heterogeneous scheduling decisions.","Existing schedulers typically adopt static timescales to regularly update scheduling decisions of each layer, without adaptive adjustment of timescales for different layers, resulting in potentially poor performance in practice.   ","We notice that the adaptive timescales would significantly improve the trade-off between the operation cost and delay performance.","Based on this insight, we propose EdgeTimer, the first work to automatically generate adaptive timescales to update multi-layer scheduling decisions using deep reinforcement learning (DRL).","First, EdgeTimer uses a three-layer hierarchical DRL framework to decouple the multi-layer decision-making task into a hierarchy of independent sub-tasks for improving learning efficiency.","Second, to cope with each sub-task, EdgeTimer adopts a safe multi-agent DRL algorithm for decentralized scheduling while ensuring system reliability.","We apply EdgeTimer to a wide range of Kubernetes scheduling rules, and evaluate it using production traces with different workload patterns.","Extensive trace-driven experiments demonstrate that EdgeTimer can learn adaptive timescales, irrespective of workload patterns and built-in scheduling rules.","It obtains up to 9.1x more profit than existing approaches without sacrificing the delay performance."],"url":"http://arxiv.org/abs/2406.07342v1","category":"cs.NI"}
{"created":"2024-06-11 15:06:15","title":"Transferring Knowledge from Large Foundation Models to Small Downstream Models","abstract":"How do we transfer the relevant knowledge from ever larger foundation models into small, task-specific downstream models that can run at much lower costs? Standard transfer learning using pre-trained weights as the initialization transfers limited information and commits us to often massive pre-trained architectures. This procedure also precludes combining multiple pre-trained models that learn complementary information. To address these shortcomings, we introduce Adaptive Feature Transfer (AFT). Instead of transferring weights, AFT operates purely on features, thereby decoupling the choice of the pre-trained model from the smaller downstream model. Rather than indiscriminately compressing all pre-trained features, AFT adaptively transfers pre-trained features that are most useful for performing the downstream task, using a simple regularization that adds minimal overhead. Across multiple vision, language, and multi-modal datasets, AFT achieves significantly better downstream performance compared to alternatives with a similar computational cost. Furthermore, AFT reliably translates improvement in pre-trained models into improvement in downstream performance, even if the downstream model is over $50\\times$ smaller, and can effectively transfer complementary information learned by multiple pre-trained models.","sentences":["How do we transfer the relevant knowledge from ever larger foundation models into small, task-specific downstream models that can run at much lower costs?","Standard transfer learning using pre-trained weights as the initialization transfers limited information and commits us to often massive pre-trained architectures.","This procedure also precludes combining multiple pre-trained models that learn complementary information.","To address these shortcomings, we introduce Adaptive Feature Transfer (AFT).","Instead of transferring weights, AFT operates purely on features, thereby decoupling the choice of the pre-trained model from the smaller downstream model.","Rather than indiscriminately compressing all pre-trained features, AFT adaptively transfers pre-trained features that are most useful for performing the downstream task, using a simple regularization that adds minimal overhead.","Across multiple vision, language, and multi-modal datasets, AFT achieves significantly better downstream performance compared to alternatives with a similar computational cost.","Furthermore, AFT reliably translates improvement in pre-trained models into improvement in downstream performance, even if the downstream model is over $50\\times$ smaller, and can effectively transfer complementary information learned by multiple pre-trained models."],"url":"http://arxiv.org/abs/2406.07337v1","category":"cs.LG"}
{"created":"2024-06-11 15:01:20","title":"Minimizing Energy Costs in Deep Learning Model Training: The Gaussian Sampling Approach","abstract":"Computing the loss gradient via backpropagation consumes considerable energy during deep learning (DL) model training. In this paper, we propose a novel approach to efficiently compute DL models' gradients to mitigate the substantial energy overhead associated with backpropagation. Exploiting the over-parameterized nature of DL models and the smoothness of their loss landscapes, we propose a method called {\\em GradSamp} for sampling gradient updates from a Gaussian distribution. Specifically, we update model parameters at a given epoch (chosen periodically or randomly) by perturbing the parameters (element-wise) from the previous epoch with Gaussian ``noise''. The parameters of the Gaussian distribution are estimated using the error between the model parameter values from the two previous epochs. {\\em GradSamp} not only streamlines gradient computation but also enables skipping entire epochs, thereby enhancing overall efficiency. We rigorously validate our hypothesis across a diverse set of standard and non-standard CNN and transformer-based models, spanning various computer vision tasks such as image classification, object detection, and image segmentation. Additionally, we explore its efficacy in out-of-distribution scenarios such as Domain Adaptation (DA), Domain Generalization (DG), and decentralized settings like Federated Learning (FL). Our experimental results affirm the effectiveness of {\\em GradSamp} in achieving notable energy savings without compromising performance, underscoring its versatility and potential impact in practical DL applications.","sentences":["Computing the loss gradient via backpropagation consumes considerable energy during deep learning (DL) model training.","In this paper, we propose a novel approach to efficiently compute DL models' gradients to mitigate the substantial energy overhead associated with backpropagation.","Exploiting the over-parameterized nature of DL models and the smoothness of their loss landscapes, we propose a method called {\\em GradSamp} for sampling gradient updates from a Gaussian distribution.","Specifically, we update model parameters at a given epoch (chosen periodically or randomly) by perturbing the parameters (element-wise) from the previous epoch with Gaussian ``noise''.","The parameters of the Gaussian distribution are estimated using the error between the model parameter values from the two previous epochs.","{\\em GradSamp} not only streamlines gradient computation but also enables skipping entire epochs, thereby enhancing overall efficiency.","We rigorously validate our hypothesis across a diverse set of standard and non-standard CNN and transformer-based models, spanning various computer vision tasks such as image classification, object detection, and image segmentation.","Additionally, we explore its efficacy in out-of-distribution scenarios such as Domain Adaptation (DA), Domain Generalization (DG), and decentralized settings like Federated Learning (FL).","Our experimental results affirm the effectiveness of {\\em GradSamp} in achieving notable energy savings without compromising performance, underscoring its versatility and potential impact in practical DL applications."],"url":"http://arxiv.org/abs/2406.07332v1","category":"cs.CV"}
{"created":"2024-06-13 17:59:46","title":"CodedEvents: Optimal Point-Spread-Function Engineering for 3D-Tracking with Event Cameras","abstract":"Point-spread-function (PSF) engineering is a well-established computational imaging technique that uses phase masks and other optical elements to embed extra information (e.g., depth) into the images captured by conventional CMOS image sensors. To date, however, PSF-engineering has not been applied to neuromorphic event cameras; a powerful new image sensing technology that responds to changes in the log-intensity of light.   This paper establishes theoretical limits (Cram\\'er Rao bounds) on 3D point localization and tracking with PSF-engineered event cameras. Using these bounds, we first demonstrate that existing Fisher phase masks are already near-optimal for localizing static flashing point sources (e.g., blinking fluorescent molecules). We then demonstrate that existing designs are sub-optimal for tracking moving point sources and proceed to use our theory to design optimal phase masks and binary amplitude masks for this task. To overcome the non-convexity of the design problem, we leverage novel implicit neural representation based parameterizations of the phase and amplitude masks. We demonstrate the efficacy of our designs through extensive simulations. We also validate our method with a simple prototype.","sentences":["Point-spread-function (PSF) engineering is a well-established computational imaging technique that uses phase masks and other optical elements to embed extra information (e.g., depth) into the images captured by conventional CMOS image sensors.","To date, however, PSF-engineering has not been applied to neuromorphic event cameras; a powerful new image sensing technology that responds to changes in the log-intensity of light.   ","This paper establishes theoretical limits (Cram\\'er Rao bounds) on 3D point localization and tracking with PSF-engineered event cameras.","Using these bounds, we first demonstrate that existing Fisher phase masks are already near-optimal for localizing static flashing point sources (e.g., blinking fluorescent molecules).","We then demonstrate that existing designs are sub-optimal for tracking moving point sources and proceed to use our theory to design optimal phase masks and binary amplitude masks for this task.","To overcome the non-convexity of the design problem, we leverage novel implicit neural representation based parameterizations of the phase and amplitude masks.","We demonstrate the efficacy of our designs through extensive simulations.","We also validate our method with a simple prototype."],"url":"http://arxiv.org/abs/2406.09409v1","category":"cs.CV"}
{"created":"2024-06-13 17:55:00","title":"Existence and partial regularity of Legendrian area-minimizing currents","abstract":"We show that Legendrian integral currents in a contact manifold that locally minimize the mass among Legendrian competitors have a regular set which is open and dense in their support. We apply this to show existence and partial regularity of solutions of the Legendrian Plateau problem in the $n$th Heisenberg group for an arbitrary horizontal $(n-1)$-cycle as prescribed boundary, and of mass-minimizing Legendrian integral currents in any $n$-dimensional homology class of a closed contact $(2n+1)$-manifold. In the case of the Heisenberg group, our result applies to Ambrosio--Kirchheim metric currents with respect to the Carnot--Carath\\'eodory distance. Our results do not assume any compatibility between the subriemannian metric and the symplectic form.","sentences":["We show that Legendrian integral currents in a contact manifold that locally minimize the mass among Legendrian competitors have a regular set which is open and dense in their support.","We apply this to show existence and partial regularity of solutions of the Legendrian Plateau problem in the $n$th Heisenberg group for an arbitrary horizontal $(n-1)$-cycle as prescribed boundary, and of mass-minimizing Legendrian integral currents in any $n$-dimensional homology class of a closed contact $(2n+1)$-manifold.","In the case of the Heisenberg group, our result applies to Ambrosio--Kirchheim metric currents with respect to the Carnot--Carath\\'eodory distance.","Our results do not assume any compatibility between the subriemannian metric and the symplectic form."],"url":"http://arxiv.org/abs/2406.09378v1","category":"math.DG"}
{"created":"2024-06-13 17:49:25","title":"On the existence of magic squares of powers","abstract":"For any $d \\geq 2$, we prove that there exists an integer $n_0(d)$ such that there exists an $n \\times n$ magic square of $d^\\text{th}$ powers for all $n \\geq n_0(d)$. In particular, we establish the existence of an $n \\times n$ magic square of squares for all $n \\geq 4$, which settles a conjecture of V\\'{a}rilly-Alvarado.   All previous approaches had been based on constructive methods and the existence of $n \\times n$ magic squares of $d^\\text{th}$ powers had only been known for sparse values of $n$. We prove our result by the Hardy-Littlewood circle method, which in this setting essentially reduces the problem to finding a sufficient number of disjoint linearly independent subsets of the columns of the coefficient matrix of the equations defining magic squares. We prove an optimal (up to a constant) lower bound for this quantity.","sentences":["For any $d \\geq 2$, we prove that there exists an integer $n_0(d)$ such that there exists an $n \\times n$ magic square of $d^\\text{th}$ powers for all $n \\geq n_0(d)$. In particular, we establish the existence of an $n \\times n$ magic square of squares for all $n \\geq 4$, which settles a conjecture of V\\'{a}rilly-Alvarado.   ","All previous approaches had been based on constructive methods and the existence of $n \\times n$ magic squares of $d^\\text{th}$ powers had only been known for sparse values of $n$. We prove our result by the Hardy-Littlewood circle method, which in this setting essentially reduces the problem to finding a sufficient number of disjoint linearly independent subsets of the columns of the coefficient matrix of the equations defining magic squares.","We prove an optimal (up to a constant) lower bound for this quantity."],"url":"http://arxiv.org/abs/2406.09364v1","category":"math.NT"}
{"created":"2024-06-13 17:26:02","title":"Hamiltonian stationary maps with infinitely many singularities","abstract":"For any $k\\in \\mathbb{N}$ we construct an Hamiltonian stationary Lagrangian map from a disc to $\\mathbb{C}^2$ with infinitely many Schoen-Wolfson singularities which is of class $C^k$ up to the boundary and has smooth trace.","sentences":["For any $k\\in \\mathbb{N}$ we construct an Hamiltonian stationary Lagrangian map from a disc to $\\mathbb{C}^2$ with infinitely many Schoen-Wolfson singularities which is of class $C^k$ up to the boundary and has smooth trace."],"url":"http://arxiv.org/abs/2406.09344v1","category":"math.DG"}
{"created":"2024-06-13 16:46:34","title":"Hodge decomposition theorem on compact $d$-K\u00e4hler manifolds","abstract":"In this article, we will explore the fundamental concepts, including various basic concepts on $d$-complex manifolds, along with several differential operators and examine the relationships between them. A $d$-K\\\"ahler manifold is a $d$-complex manifold equipped with a metric that satisfies a specific condition. We prove the Hodge decomposition theorem on compact $d$-K\\\"ahler manifolds, which establishes a crucial relationship between certain de-Rham cohomology groups and Dolbeault cohomology groups on a compact $d$-K\\\"ahler manifold .","sentences":["In this article, we will explore the fundamental concepts, including various basic concepts on $d$-complex manifolds, along with several differential operators and examine the relationships between them.","A $d$-K\\\"ahler manifold is a $d$-complex manifold equipped with a metric that satisfies a specific condition.","We prove the Hodge decomposition theorem on compact $d$-K\\\"ahler manifolds, which establishes a crucial relationship between certain de-Rham cohomology groups and Dolbeault cohomology groups on a compact $d$-K\\\"ahler manifold ."],"url":"http://arxiv.org/abs/2406.09312v1","category":"math.DG"}
{"created":"2024-06-13 16:41:22","title":"Single-photon and multi-photon Fano lines for helium and neon using tRecX-haCC","abstract":"Single-photon and multi-photon ionization of helium and neon atoms by ultrashort extreme ultraviolet radiation is studied using the tRecX-haCC package developed by the authors. The tRecX-haCC package solves the time dependent Schrodinger equation in the presence of a laser pulse and computes the photoelectron spectra using the iSurf technique to efficiently capture the slow decay of the resonant states. Wavelength of the radiation is chosen to expose doubly excited states embedded in the continuum. The lineshapes in the spectra that arise as a result of these doubly excited states are analyzed. The computed peak positions and linewidths are in good agreement with the literature. In addition, the dependence of the Fano q parameter on the order of the multi-photon process is demonstrated.","sentences":["Single-photon and multi-photon ionization of helium and neon atoms by ultrashort extreme ultraviolet radiation is studied using the tRecX-haCC package developed by the authors.","The tRecX-haCC package solves the time dependent Schrodinger equation in the presence of a laser pulse and computes the photoelectron spectra using the iSurf technique to efficiently capture the slow decay of the resonant states.","Wavelength of the radiation is chosen to expose doubly excited states embedded in the continuum.","The lineshapes in the spectra that arise as a result of these doubly excited states are analyzed.","The computed peak positions and linewidths are in good agreement with the literature.","In addition, the dependence of the Fano q parameter on the order of the multi-photon process is demonstrated."],"url":"http://arxiv.org/abs/2406.09306v1","category":"physics.atom-ph"}
{"created":"2024-06-13 16:12:52","title":"Doubled Shapiro steps in a dynamic axion insulator Josephson junction","abstract":"Dynamic axion insulators feature a time-dependent axion field that can be induced by antiferromagnetic resonance. Here, we show that a Josephson junction incorporating this dynamic axion insulator between two superconductors exhibits a striking doubled Shapiro steps wherein all odd steps are completely suppressed in the jointly presence of a DC bias and a static magnetic field. The resistively shunted junction simulation confirms that these doubled Shapiro steps originate from the distinctive axion electrodynamics driven by the antiferromagnetic resonance, which thus not only furnishes a hallmark to identify the dynamic axion insulator but also provides a method to evaluate its mass term. Furthermore, the experimentally feasible differential conductance is also determined. Our work holds significant importance in condensed matter physics and materials science for understanding the dynamic axion insulator, paving the way for its further exploration and applications.","sentences":["Dynamic axion insulators feature a time-dependent axion field that can be induced by antiferromagnetic resonance.","Here, we show that a Josephson junction incorporating this dynamic axion insulator between two superconductors exhibits a striking doubled Shapiro steps wherein all odd steps are completely suppressed in the jointly presence of a DC bias and a static magnetic field.","The resistively shunted junction simulation confirms that these doubled Shapiro steps originate from the distinctive axion electrodynamics driven by the antiferromagnetic resonance, which thus not only furnishes a hallmark to identify the dynamic axion insulator but also provides a method to evaluate its mass term.","Furthermore, the experimentally feasible differential conductance is also determined.","Our work holds significant importance in condensed matter physics and materials science for understanding the dynamic axion insulator, paving the way for its further exploration and applications."],"url":"http://arxiv.org/abs/2406.09274v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-13 16:11:10","title":"QCD constraints on isospin-dense matter and the nuclear equation of state","abstract":"Understanding the behavior of dense hadronic matter is a central goal in nuclear physics as it governs the nature and dynamics of astrophysical objects such as supernovae and neutron stars. Because of the non-perturbative nature of quantum chromodynamics (QCD), little is known rigorously about hadronic matter in these extreme conditions. Here, lattice QCD calculations are used to compute thermodynamic quantities and the equation of state of QCD over a wide range of isospin chemical potentials. Agreement is seen with chiral perturbation theory predictions when the chemical potential is small. Comparison to perturbative QCD calculations at large chemical potential allows for an estimate of the gap in the superconducting phase, and this quantity is seen to agree with perturbative determinations. Since the partition function for an isospin chemical potential, $\\mu_I$, bounds the partition function for a baryon chemical potential $\\mu_B=3/2\\mu_I$, these calculations also provide rigorous non-perturbative QCD bounds on the nuclear equation of state over a wide range of baryon densities for the first time.","sentences":["Understanding the behavior of dense hadronic matter is a central goal in nuclear physics as it governs the nature and dynamics of astrophysical objects such as supernovae and neutron stars.","Because of the non-perturbative nature of quantum chromodynamics (QCD), little is known rigorously about hadronic matter in these extreme conditions.","Here, lattice QCD calculations are used to compute thermodynamic quantities and the equation of state of QCD over a wide range of isospin chemical potentials.","Agreement is seen with chiral perturbation theory predictions when the chemical potential is small.","Comparison to perturbative QCD calculations at large chemical potential allows for an estimate of the gap in the superconducting phase, and this quantity is seen to agree with perturbative determinations.","Since the partition function for an isospin chemical potential, $\\mu_I$, bounds the partition function for a baryon chemical potential $\\mu_B=3/2\\mu_I$, these calculations also provide rigorous non-perturbative QCD bounds on the nuclear equation of state over a wide range of baryon densities for the first time."],"url":"http://arxiv.org/abs/2406.09273v1","category":"hep-lat"}
{"created":"2024-06-13 16:06:42","title":"Global smooth solutions by transport noise of 3D Navier-Stokes equations with small hyperviscosity","abstract":"The existence of global smooth solutions to the Navier-Stokes equations (NSEs) with hyperviscosity $(-\\Delta)^{\\gamma}$ is open unless $\\gamma $ is close to the J.-L. Lions exponent $ \\frac{5}{4}$ at which the energy balance is strong enough to prevent singularity formation. If $1<\\gamma \\ll \\frac{5}{4}$, then the global well-posedness of the hyperviscous NSEs is widely open as for the usual NSEs. In this paper, for all $\\gamma>1$, we show the existence of a transport noise for which global smooth solutions to the stochastic hyperviscous NSEs on the three-dimensional torus exist with high probability. In particular, a suitable transport noise considerably improves the known well-posedness results in the deterministic setting.","sentences":["The existence of global smooth solutions to the Navier-Stokes equations (NSEs) with hyperviscosity $(-\\Delta)^{\\gamma}$ is open unless $\\gamma $ is close to the J.-L. Lions exponent $ \\frac{5}{4}$ at which the energy balance is strong enough to prevent singularity formation.","If $1<\\gamma \\ll \\frac{5}{4}$, then the global well-posedness of the hyperviscous NSEs is widely open as for the usual NSEs.","In this paper, for all $\\gamma>1$, we show the existence of a transport noise for which global smooth solutions to the stochastic hyperviscous NSEs on the three-dimensional torus exist with high probability.","In particular, a suitable transport noise considerably improves the known well-posedness results in the deterministic setting."],"url":"http://arxiv.org/abs/2406.09267v1","category":"math.AP"}
{"created":"2024-06-13 15:56:55","title":"Deep Sketched Output Kernel Regression for Structured Prediction","abstract":"By leveraging the kernel trick in the output space, kernel-induced losses provide a principled way to define structured output prediction tasks for a wide variety of output modalities. In particular, they have been successfully used in the context of surrogate non-parametric regression, where the kernel trick is typically exploited in the input space as well. However, when inputs are images or texts, more expressive models such as deep neural networks seem more suited than non-parametric methods. In this work, we tackle the question of how to train neural networks to solve structured output prediction tasks, while still benefiting from the versatility and relevance of kernel-induced losses. We design a novel family of deep neural architectures, whose last layer predicts in a data-dependent finite-dimensional subspace of the infinite-dimensional output feature space deriving from the kernel-induced loss. This subspace is chosen as the span of the eigenfunctions of a randomly-approximated version of the empirical kernel covariance operator. Interestingly, this approach unlocks the use of gradient descent algorithms (and consequently of any neural architecture) for structured prediction. Experiments on synthetic tasks as well as real-world supervised graph prediction problems show the relevance of our method.","sentences":["By leveraging the kernel trick in the output space, kernel-induced losses provide a principled way to define structured output prediction tasks for a wide variety of output modalities.","In particular, they have been successfully used in the context of surrogate non-parametric regression, where the kernel trick is typically exploited in the input space as well.","However, when inputs are images or texts, more expressive models such as deep neural networks seem more suited than non-parametric methods.","In this work, we tackle the question of how to train neural networks to solve structured output prediction tasks, while still benefiting from the versatility and relevance of kernel-induced losses.","We design a novel family of deep neural architectures, whose last layer predicts in a data-dependent finite-dimensional subspace of the infinite-dimensional output feature space deriving from the kernel-induced loss.","This subspace is chosen as the span of the eigenfunctions of a randomly-approximated version of the empirical kernel covariance operator.","Interestingly, this approach unlocks the use of gradient descent algorithms (and consequently of any neural architecture) for structured prediction.","Experiments on synthetic tasks as well as real-world supervised graph prediction problems show the relevance of our method."],"url":"http://arxiv.org/abs/2406.09253v1","category":"stat.ML"}
{"created":"2024-06-13 15:27:08","title":"Probing atmospheric escape through metastable He I triplet lines in 15 exoplanets observed with SPIRou","abstract":"For several years, the metastable helium triplet line has been successfully used as a tracer to probe atmospheric escape in transiting exoplanets. This absorption in the near-infrared (1083.3 nm) can be observed from the ground using high-resolution spectroscopy, providing new constraints on the mass-loss rate and the temperature characterizing the upper atmosphere of close-in exoplanets.   The aim of this work is to search for the He triplet signature in 15 transiting exoplanets -- ranging from super-Earths to ultrahot Jupiters -- observed with SPIRou, a high-resolution (R~70 000) near-infrared spectropolarimeter at the CFHT, in order to bring new constraints or to improve existing ones regarding atmospheric escape through a homogeneous study.   We developed a full data processing and analysis pipeline to correct for the residual telluric and stellar contributions. We then used two different 1D models based on the Parker-wind equations and nonlocal thermodynamic equilibrium (NLTE) radiative transfer to interpret the observational results.   We confirm published He triplet detections for HAT-P-11 b, HD 189733 b, and WASP-69 b. We tentatively detect the signature of escaping He in HD 209458 b, GJ 3470 b, and WASP-76 b. We report new constraints on the mass-loss rate and temperature for our three detections and set upper limits for the tentative and nondetections. We notably report improved constraints on the mass-loss rate and temperature of the escaping gas for TOI-1807 b, and report a nondetection for the debated atmospheric escape in GJ 1214 b. We also conducted the first search for the He signature in GJ 486 b since its discovery and report a nondetection of the He triplet. Finally, we studied the impact of important model assumptions on our retrieved parameters, notably the limitations of 1D models and the influence of the H/He ratio on the derived constraints.","sentences":["For several years, the metastable helium triplet line has been successfully used as a tracer to probe atmospheric escape in transiting exoplanets.","This absorption in the near-infrared (1083.3 nm) can be observed from the ground using high-resolution spectroscopy, providing new constraints on the mass-loss rate and the temperature characterizing the upper atmosphere of close-in exoplanets.   ","The aim of this work is to search for the He triplet signature in 15 transiting exoplanets -- ranging from super-Earths to ultrahot Jupiters -- observed with SPIRou, a high-resolution (R~70 000) near-infrared spectropolarimeter at the CFHT, in order to bring new constraints or to improve existing ones regarding atmospheric escape through a homogeneous study.   ","We developed a full data processing and analysis pipeline to correct for the residual telluric and stellar contributions.","We then used two different 1D models based on the Parker-wind equations and nonlocal thermodynamic equilibrium (NLTE) radiative transfer to interpret the observational results.   ","We confirm published He triplet detections for HAT-P-11 b, HD 189733 b, and WASP-69 b.","We tentatively detect the signature of escaping He in HD 209458 b, GJ 3470 b, and WASP-76 b.","We report new constraints on the mass-loss rate and temperature for our three detections and set upper limits for the tentative and nondetections.","We notably report improved constraints on the mass-loss rate and temperature of the escaping gas for TOI-1807 b, and report a nondetection for the debated atmospheric escape in GJ 1214 b.","We also conducted the first search for the He signature in GJ 486 b since its discovery and report a nondetection of the He triplet.","Finally, we studied the impact of important model assumptions on our retrieved parameters, notably the limitations of 1D models and the influence of the H/He ratio on the derived constraints."],"url":"http://arxiv.org/abs/2406.09225v1","category":"astro-ph.EP"}
{"created":"2024-06-13 15:24:22","title":"Well-posedness and regularity of solutions to neural field problems with dendritic processing","abstract":"We study solutions to a recently proposed neural field model in which dendrites are modelled as a continuum of vertical fibres stemming from a somatic layer. Since voltage propagates along the dendritic direction via a cable equation with nonlocal sources, the model features an anisotropic diffusion operator, as well as an integral term for synaptic coupling. The corresponding Cauchy problem is thus markedly different from classical neural field equations. We prove that the weak formulation of the problem admits a unique solution, with embedding estimates similar to the ones of nonlinear local reaction-diffusion equations. Our analysis relies on perturbing weak solutions to the diffusion-less problem, that is, a standard neural field, for which weak problems have not been studied to date. We find rigorous asymptotic estimates for the problem with and without diffusion, and prove that the solutions of the two models stay close, in a suitable norm, on finite time intervals. We provide numerical evidence of our perturbative results.","sentences":["We study solutions to a recently proposed neural field model in which dendrites are modelled as a continuum of vertical fibres stemming from a somatic layer.","Since voltage propagates along the dendritic direction via a cable equation with nonlocal sources, the model features an anisotropic diffusion operator, as well as an integral term for synaptic coupling.","The corresponding Cauchy problem is thus markedly different from classical neural field equations.","We prove that the weak formulation of the problem admits a unique solution, with embedding estimates similar to the ones of nonlinear local reaction-diffusion equations.","Our analysis relies on perturbing weak solutions to the diffusion-less problem, that is, a standard neural field, for which weak problems have not been studied to date.","We find rigorous asymptotic estimates for the problem with and without diffusion, and prove that the solutions of the two models stay close, in a suitable norm, on finite time intervals.","We provide numerical evidence of our perturbative results."],"url":"http://arxiv.org/abs/2406.09222v1","category":"math.AP"}
{"created":"2024-06-13 15:15:30","title":"Charged massive spin 2 and 3/2 propagation in a constant electromagnetic background","abstract":"We present two methods for deriving the equations of motion for charged massive spin-3/2 particles. The first approach involves utilizing the Euler-Lagrange equations derived from a Lagrangian that describes the propagation of the first massive excitation of open superstrings. The second method entails enforcing the conditions that the trace vanishes and that the covariant derivative of the equations vanish. We very briefly comment about other spins.","sentences":["We present two methods for deriving the equations of motion for charged massive spin-3/2 particles.","The first approach involves utilizing the Euler-Lagrange equations derived from a Lagrangian that describes the propagation of the first massive excitation of open superstrings.","The second method entails enforcing the conditions that the trace vanishes and that the covariant derivative of the equations vanish.","We very briefly comment about other spins."],"url":"http://arxiv.org/abs/2406.09213v1","category":"hep-th"}
{"created":"2024-06-13 14:54:30","title":"Bengining overfitting in Fixed Dimension via Physics-Informed Learning with Smooth Iductive Bias","abstract":"Recent advances in machine learning theory showed that interpolation to noisy samples using over-parameterized machine learning algorithms always leads to inconsistency. However, this work surprisingly discovers that interpolated machine learning can exhibit benign overfitting and consistency when using physics-informed learning for supervised tasks governed by partial differential equations (PDEs) describing laws of physics. An analysis provides an asymptotic Sobolev norm learning curve for kernel ridge(less) regression addressing linear inverse problems involving elliptic PDEs. The results reveal that the PDE operators can stabilize variance and lead to benign overfitting for fixed-dimensional problems, contrasting standard regression settings. The impact of various inductive biases introduced by minimizing different Sobolev norms as implicit regularization is also examined. Notably, the convergence rate is independent of the specific (smooth) inductive bias for both ridge and ridgeless regression. For regularized least squares estimators, all (smooth enough) inductive biases can achieve optimal convergence rates when the regularization parameter is properly chosen. The smoothness requirement recovers a condition previously found in the Bayesian setting and extends conclusions to minimum norm interpolation estimators.","sentences":["Recent advances in machine learning theory showed that interpolation to noisy samples using over-parameterized machine learning algorithms always leads to inconsistency.","However, this work surprisingly discovers that interpolated machine learning can exhibit benign overfitting and consistency when using physics-informed learning for supervised tasks governed by partial differential equations (PDEs) describing laws of physics.","An analysis provides an asymptotic Sobolev norm learning curve for kernel ridge(less) regression addressing linear inverse problems involving elliptic PDEs.","The results reveal that the PDE operators can stabilize variance and lead to benign overfitting for fixed-dimensional problems, contrasting standard regression settings.","The impact of various inductive biases introduced by minimizing different Sobolev norms as implicit regularization is also examined.","Notably, the convergence rate is independent of the specific (smooth) inductive bias for both ridge and ridgeless regression.","For regularized least squares estimators, all (smooth enough) inductive biases can achieve optimal convergence rates when the regularization parameter is properly chosen.","The smoothness requirement recovers a condition previously found in the Bayesian setting and extends conclusions to minimum norm interpolation estimators."],"url":"http://arxiv.org/abs/2406.09194v1","category":"stat.ML"}
{"created":"2024-06-13 14:04:34","title":"Jacobian-Enhanced Neural Networks","abstract":"Jacobian-Enhanced Neural Networks (JENN) are densely connected multi-layer perceptrons, whose training process is modified to predict partial derivatives accurately. Their main benefit is better accuracy with fewer training points compared to standard neural networks. These attributes are particularly desirable in the field of computer-aided design, where there is often the need to replace computationally expensive, physics-based models with fast running approximations, known as surrogate models or meta-models. Since a surrogate emulates the original model accurately in near-real time, it yields a speed benefit that can be used to carry out orders of magnitude more function calls quickly. However, in the special case of gradient-enhanced methods, there is the additional value proposition that partial derivatives are accurate, which is a critical property for one important use-case: surrogate-based optimization. This work derives the complete theory and exemplifies its superiority over standard neural nets for surrogate-based optimization.","sentences":["Jacobian-Enhanced Neural Networks (JENN) are densely connected multi-layer perceptrons, whose training process is modified to predict partial derivatives accurately.","Their main benefit is better accuracy with fewer training points compared to standard neural networks.","These attributes are particularly desirable in the field of computer-aided design, where there is often the need to replace computationally expensive, physics-based models with fast running approximations, known as surrogate models or meta-models.","Since a surrogate emulates the original model accurately in near-real time, it yields a speed benefit that can be used to carry out orders of magnitude more function calls quickly.","However, in the special case of gradient-enhanced methods, there is the additional value proposition that partial derivatives are accurate, which is a critical property for one important use-case: surrogate-based optimization.","This work derives the complete theory and exemplifies its superiority over standard neural nets for surrogate-based optimization."],"url":"http://arxiv.org/abs/2406.09132v1","category":"cs.LG"}
{"created":"2024-06-13 13:46:46","title":"Second Order Shape Optimization for an Interface Identification Problem constrained by Nonlocal Models","abstract":"Since shape optimization methods have been proven useful for identifying interfaces in models governed by partial differential equations, we show how shape optimization techniques can also be applied to an interface identification problem constrained by a nonlocal Dirichlet problem. Here, we focus on deriving the second shape derivative of the corresponding reduced functional and we further investigate a second order optimization algorithm.","sentences":["Since shape optimization methods have been proven useful for identifying interfaces in models governed by partial differential equations, we show how shape optimization techniques can also be applied to an interface identification problem constrained by a nonlocal Dirichlet problem.","Here, we focus on deriving the second shape derivative of the corresponding reduced functional and we further investigate a second order optimization algorithm."],"url":"http://arxiv.org/abs/2406.09118v1","category":"math.OC"}
{"created":"2024-06-13 13:29:45","title":"Analytic smoothing effect of the Cauchy problem for a class of ultra-parabolic equations","abstract":"In this paper, we study a class of strongly degenerate ultraparabolic equations with analytic coefficients. We demonstrate that the Cauchy problem exhibits an analytic smoothing effect. This means that, with an initial datum belonging to the Sobolev space $H^s$ (of real index s), the associated Cauchy problem admits a unique solution that is analytic in all spatial variables for any strictly positive time. This smoothing effect property is similar to that of the Cauchy problem for uniformly parabolic equations with analytic coefficients.","sentences":["In this paper, we study a class of strongly degenerate ultraparabolic equations with analytic coefficients.","We demonstrate that the Cauchy problem exhibits an analytic smoothing effect.","This means that, with an initial datum belonging to the Sobolev space $H^s$ (of real index s), the associated Cauchy problem admits a unique solution that is analytic in all spatial variables for any strictly positive time.","This smoothing effect property is similar to that of the Cauchy problem for uniformly parabolic equations with analytic coefficients."],"url":"http://arxiv.org/abs/2406.09102v1","category":"math.AP"}
{"created":"2024-06-13 13:07:52","title":"Operator-informed score matching for Markov diffusion models","abstract":"Diffusion models are typically trained using score matching, yet score matching is agnostic to the particular forward process that defines the model. This paper argues that Markov diffusion models enjoy an advantage over other types of diffusion model, as their associated operators can be exploited to improve the training process. In particular, (i) there exists an explicit formal solution to the forward process as a sequence of time-dependent kernel mean embeddings; and (ii) the derivation of score-matching and related estimators can be streamlined. Building upon (i), we propose Riemannian diffusion kernel smoothing, which ameliorates the need for neural score approximation, at least in the low-dimensional context; Building upon (ii), we propose operator-informed score matching, a variance reduction technique that is straightforward to implement in both low- and high-dimensional diffusion modeling and is demonstrated to improve score matching in an empirical proof-of-concept.","sentences":["Diffusion models are typically trained using score matching, yet score matching is agnostic to the particular forward process that defines the model.","This paper argues that Markov diffusion models enjoy an advantage over other types of diffusion model, as their associated operators can be exploited to improve the training process.","In particular, (i) there exists an explicit formal solution to the forward process as a sequence of time-dependent kernel mean embeddings; and (ii) the derivation of score-matching and related estimators can be streamlined.","Building upon (i), we propose Riemannian diffusion kernel smoothing, which ameliorates the need for neural score approximation, at least in the low-dimensional context; Building upon (ii), we propose operator-informed score matching, a variance reduction technique that is straightforward to implement in both low- and high-dimensional diffusion modeling and is demonstrated to improve score matching in an empirical proof-of-concept."],"url":"http://arxiv.org/abs/2406.09084v1","category":"stat.ML"}
{"created":"2024-06-13 13:03:37","title":"Latent Assistance Networks: Rediscovering Hyperbolic Tangents in RL","abstract":"Activation functions are one of the key components of a neural network. The most commonly used activation functions can be classed into the category of continuously differentiable (e.g. tanh) and linear-unit functions (e.g. ReLU), both having their own strengths and drawbacks with respect to downstream performance and representation capacity through learning (e.g. measured by the number of dead neurons and the effective rank). In reinforcement learning, the performance of continuously differentiable activations often falls short as compared to linear-unit functions. From the perspective of the activations in the last hidden layer, this paper provides insights regarding this sub-optimality and explores how activation functions influence the occurrence of dead neurons and the magnitude of the effective rank. Additionally, a novel neural architecture is proposed that leverages the product of independent activation values. In the Atari domain, we show faster learning, a reduction in dead neurons and increased effective rank.","sentences":["Activation functions are one of the key components of a neural network.","The most commonly used activation functions can be classed into the category of continuously differentiable (e.g. tanh) and linear-unit functions (e.g. ReLU), both having their own strengths and drawbacks with respect to downstream performance and representation capacity through learning (e.g. measured by the number of dead neurons and the effective rank).","In reinforcement learning, the performance of continuously differentiable activations often falls short as compared to linear-unit functions.","From the perspective of the activations in the last hidden layer, this paper provides insights regarding this sub-optimality and explores how activation functions influence the occurrence of dead neurons and the magnitude of the effective rank.","Additionally, a novel neural architecture is proposed that leverages the product of independent activation values.","In the Atari domain, we show faster learning, a reduction in dead neurons and increased effective rank."],"url":"http://arxiv.org/abs/2406.09079v1","category":"cs.LG"}
{"created":"2024-06-13 12:07:03","title":"An alternative to purification in CFT","abstract":"In conformal field theories, in contrast to \\emph{adding} some auxiliary states into the bipartite mixed state $\\rho_{AB}$ as the usual purifications do, we show a pure entangled state $\\psi_{AB}$ can be constructed by \\emph{subtracting} the undetectable regions. In this pure state $\\psi_{AB}$, the von Neumann entropy $S_{\\text{vN}}(A)$ naturally captures quantum entanglement between $A$ and $B$. We verify that $S_{\\text{vN}}(A)$ is equal to the entanglement wedge cross-section $E_{W}$ in AdS spacetime, which is conjectured to be the holographic dual of the entanglement of purification. We show such constructed entanglement entropy has a phase transition. The ordinary entanglement entropies of critical and non-critical QFTs are simply limits of the two phases.","sentences":["In conformal field theories, in contrast to \\emph{adding} some auxiliary states into the bipartite mixed state $\\rho_{AB}$ as the usual purifications do, we show a pure entangled state $\\psi_{AB}$ can be constructed by \\emph{subtracting} the undetectable regions.","In this pure state $\\psi_{AB}$, the von Neumann entropy $S_{\\text{vN}}(A)$ naturally captures quantum entanglement between $A$ and $B$. We verify that $S_{\\text{vN}}(A)$ is equal to the entanglement wedge cross-section $E_{W}$ in AdS spacetime, which is conjectured to be the holographic dual of the entanglement of purification.","We show such constructed entanglement entropy has a phase transition.","The ordinary entanglement entropies of critical and non-critical QFTs are simply limits of the two phases."],"url":"http://arxiv.org/abs/2406.09033v1","category":"hep-th"}
{"created":"2024-06-13 11:56:58","title":"E(2)-Equivariant Features in Machine Learning for Morphological Classification of Radio Galaxies","abstract":"With the growth of data from new radio telescope facilities, machine-learning approaches to the morphological classification of radio galaxies are increasingly being utilised. However, while widely employed deep-learning models using convolutional neural networks (CNNs) are equivariant to translations within images, neither CNNs nor most other machine-learning approaches are equivariant to additional isometries of the Euclidean plane, such as rotations and reflections. Recent work has attempted to address this by using G-steerable CNNs, designed to be equivariant to a specified subset of 2-dimensional Euclidean, E(2), transformations. Although this approach improved model performance, the computational costs were a recognised drawback. Here we consider the use of directly extracted E(2)-equivariant features for the classification of radio galaxies. Specifically, we investigate the use of Minkowski functionals (MFs), Haralick features (HFs) and elliptical Fourier descriptors (EFDs). We show that, while these features do not perform equivalently well to CNNs in terms of accuracy, they are able to inform the classification of radio galaxies, requiring ~50 times less computational runtime. We demonstrate that MFs are the most informative, EFDs the least informative, and show that combinations of all three result in only incrementally improved performance, which we suggest is due to information overlap between feature sets.","sentences":["With the growth of data from new radio telescope facilities, machine-learning approaches to the morphological classification of radio galaxies are increasingly being utilised.","However, while widely employed deep-learning models using convolutional neural networks (CNNs) are equivariant to translations within images, neither CNNs nor most other machine-learning approaches are equivariant to additional isometries of the Euclidean plane, such as rotations and reflections.","Recent work has attempted to address this by using G-steerable CNNs, designed to be equivariant to a specified subset of 2-dimensional Euclidean, E(2), transformations.","Although this approach improved model performance, the computational costs were a recognised drawback.","Here we consider the use of directly extracted E(2)-equivariant features for the classification of radio galaxies.","Specifically, we investigate the use of Minkowski functionals (MFs), Haralick features (HFs) and elliptical Fourier descriptors (EFDs).","We show that, while these features do not perform equivalently well to CNNs in terms of accuracy, they are able to inform the classification of radio galaxies, requiring ~50 times less computational runtime.","We demonstrate that MFs are the most informative, EFDs the least informative, and show that combinations of all three result in only incrementally improved performance, which we suggest is due to information overlap between feature sets."],"url":"http://arxiv.org/abs/2406.09024v1","category":"astro-ph.IM"}
{"created":"2024-06-13 10:53:33","title":"Classic GNNs are Strong Baselines: Reassessing GNNs for Node Classification","abstract":"Graph Transformers (GTs) have recently emerged as popular alternatives to traditional message-passing Graph Neural Networks (GNNs), due to their theoretically superior expressiveness and impressive performance reported on standard node classification benchmarks, often significantly outperforming GNNs. In this paper, we conduct a thorough empirical analysis to reevaluate the performance of three classic GNN models (GCN, GAT, and GraphSAGE) against GTs. Our findings suggest that the previously reported superiority of GTs may have been overstated due to suboptimal hyperparameter configurations in GNNs. Remarkably, with slight hyperparameter tuning, these classic GNN models achieve state-of-the-art performance, matching or even exceeding that of recent GTs across 17 out of the 18 diverse datasets examined. Additionally, we conduct detailed ablation studies to investigate the influence of various GNN configurations, such as normalization, dropout, residual connections, network depth, and jumping knowledge mode, on node classification performance. Our study aims to promote a higher standard of empirical rigor in the field of graph machine learning, encouraging more accurate comparisons and evaluations of model capabilities.","sentences":["Graph Transformers (GTs) have recently emerged as popular alternatives to traditional message-passing Graph Neural Networks (GNNs), due to their theoretically superior expressiveness and impressive performance reported on standard node classification benchmarks, often significantly outperforming GNNs.","In this paper, we conduct a thorough empirical analysis to reevaluate the performance of three classic GNN models (GCN, GAT, and GraphSAGE) against GTs.","Our findings suggest that the previously reported superiority of GTs may have been overstated due to suboptimal hyperparameter configurations in GNNs.","Remarkably, with slight hyperparameter tuning, these classic GNN models achieve state-of-the-art performance, matching or even exceeding that of recent GTs across 17 out of the 18 diverse datasets examined.","Additionally, we conduct detailed ablation studies to investigate the influence of various GNN configurations, such as normalization, dropout, residual connections, network depth, and jumping knowledge mode, on node classification performance.","Our study aims to promote a higher standard of empirical rigor in the field of graph machine learning, encouraging more accurate comparisons and evaluations of model capabilities."],"url":"http://arxiv.org/abs/2406.08993v1","category":"cs.LG"}
{"created":"2024-06-13 10:35:16","title":"Towards Next Era of Multi-objective Optimization: Large Language Models as Architects of Evolutionary Operators","abstract":"Multi-objective optimization problems (MOPs) are prevalent in various real-world applications, necessitating sophisticated solutions that balance conflicting objectives. Traditional evolutionary algorithms (EAs), while effective, often rely on domain-specific expert knowledge and iterative tuning, which can impede innovation when encountering novel MOPs. Very recently, the emergence of Large Language Models (LLMs) has revolutionized software engineering by enabling the autonomous development and refinement of programs. Capitalizing on this advancement, we propose a new LLM-based framework for evolving EA operators, designed to address a wide array of MOPs. This framework facilitates the production of EA operators without the extensive demands for expert intervention, thereby streamlining the design process. To validate the efficacy of our approach, we have conducted extensive empirical studies across various categories of MOPs. The results demonstrate the robustness and superior performance of our LLM-evolved operators.","sentences":["Multi-objective optimization problems (MOPs) are prevalent in various real-world applications, necessitating sophisticated solutions that balance conflicting objectives.","Traditional evolutionary algorithms (EAs), while effective, often rely on domain-specific expert knowledge and iterative tuning, which can impede innovation when encountering novel MOPs.","Very recently, the emergence of Large Language Models (LLMs) has revolutionized software engineering by enabling the autonomous development and refinement of programs.","Capitalizing on this advancement, we propose a new LLM-based framework for evolving EA operators, designed to address a wide array of MOPs.","This framework facilitates the production of EA operators without the extensive demands for expert intervention, thereby streamlining the design process.","To validate the efficacy of our approach, we have conducted extensive empirical studies across various categories of MOPs.","The results demonstrate the robustness and superior performance of our LLM-evolved operators."],"url":"http://arxiv.org/abs/2406.08987v1","category":"cs.NE"}
{"created":"2024-06-13 10:03:15","title":"On the block size spectrum of the multiplicative Beta coalescent","abstract":"In this work we introduce the (exchangeable, but non-consistent) multiplicative $\\Lambda$-coalescent, which accounts for the connected components (blocks) of the dynamic $\\Lambda$-random graph. This is the random graph analogue of the classical $\\Lambda$-coalescent studied in mathematical population genetics; it is an exchangeable and consistent random graph process. This work considers the case where $\\Lambda$ is the beta measure. We prove a dynamic law of large numbers for the numbers of blocks containing $1,...,d$ elements. In addition, we provide a functional limit theorem for the fluctuations around its deterministic trajectory. The limit process satisfies a stochastic differential equation of Ornstein-Uhlenbeck type.","sentences":["In this work we introduce the (exchangeable, but non-consistent) multiplicative $\\Lambda$-coalescent, which accounts for the connected components (blocks) of the dynamic $\\Lambda$-random graph.","This is the random graph analogue of the classical $\\Lambda$-coalescent studied in mathematical population genetics; it is an exchangeable and consistent random graph process.","This work considers the case where $\\Lambda$ is the beta measure.","We prove a dynamic law of large numbers for the numbers of blocks containing $1,...,d$ elements.","In addition, we provide a functional limit theorem for the fluctuations around its deterministic trajectory.","The limit process satisfies a stochastic differential equation of Ornstein-Uhlenbeck type."],"url":"http://arxiv.org/abs/2406.08972v1","category":"math.PR"}
{"created":"2024-06-13 09:54:26","title":"Small-scale and large-scale dynamos in global convection simulations of solar-like stars","abstract":"It has been recently shown that a small-scale dynamo (SSD) instability could be possible in solar-like low magnetic Prandtl number Pm plasmas. It has been proposed that the presence of SSD can potentially have a significant impact on the dynamics of the large-scale dynamo (LSD) in the stellar convection zones. Studying these two dynamos, SSD and LSD, together in a global magnetoconvection model requires high-resolution simulations and large amounts of computational resources. Starting from a well-studied global convective dynamo model that produces cyclic magnetic fields, we systematically increased the resolution and lowered the diffusivities to enter the regime of Reynolds numbers that allow for the excitation of SSD on top of the LSD. We studied how the properties of convection, generated differential rotation profiles, and LSD solutions change with the presence of SSD. We performed convective dynamo simulations in a spherical wedge with the Pencil Code. The resolutions of the models were increased in 4 steps by a total factor of 16 to achieve maximal fluid and magnetic Reynolds numbers of over 500. We found that the differential rotation is strongly quenched by the presence of the LSD and SSD. Even though the small-scale magnetic field only mildly decreases increasing Re, the large-scale field strength decreases significantly. We do not find the SSD dynamo significantly quenching the convective flows as claimed recently by other authors; in contrast, the convective flows first grow and then saturate for increasing Re. Furthermore, the angular momentum transport is highly affected by the presence of small-scale magnetic fields, which are mostly generated by LSD. These fields not only change the Reynolds stresses, but also generate dynamically important Maxwell stresses. The LSD evolution in terms of its pattern and field distribution is rather independent of the increase in Rm.","sentences":["It has been recently shown that a small-scale dynamo (SSD) instability could be possible in solar-like low magnetic Prandtl number Pm plasmas.","It has been proposed that the presence of SSD can potentially have a significant impact on the dynamics of the large-scale dynamo (LSD) in the stellar convection zones.","Studying these two dynamos, SSD and LSD, together in a global magnetoconvection model requires high-resolution simulations and large amounts of computational resources.","Starting from a well-studied global convective dynamo model that produces cyclic magnetic fields, we systematically increased the resolution and lowered the diffusivities to enter the regime of Reynolds numbers that allow for the excitation of SSD on top of the LSD.","We studied how the properties of convection, generated differential rotation profiles, and LSD","solutions change with the presence of SSD.","We performed convective dynamo simulations in a spherical wedge with the Pencil Code.","The resolutions of the models were increased in 4 steps by a total factor of 16 to achieve maximal fluid and magnetic Reynolds numbers of over 500.","We found that the differential rotation is strongly quenched by the presence of the LSD and SSD.","Even though the small-scale magnetic field only mildly decreases increasing Re, the large-scale field strength decreases significantly.","We do not find the SSD dynamo significantly quenching the convective flows as claimed recently by other authors; in contrast, the convective flows first grow and then saturate for increasing Re.","Furthermore, the angular momentum transport is highly affected by the presence of small-scale magnetic fields, which are mostly generated by LSD.","These fields not only change the Reynolds stresses, but also generate dynamically important Maxwell stresses.","The LSD evolution in terms of its pattern and field distribution is rather independent of the increase in Rm."],"url":"http://arxiv.org/abs/2406.08967v1","category":"astro-ph.SR"}
{"created":"2024-06-13 09:12:26","title":"Neural NeRF Compression","abstract":"Neural Radiance Fields (NeRFs) have emerged as powerful tools for capturing detailed 3D scenes through continuous volumetric representations. Recent NeRFs utilize feature grids to improve rendering quality and speed; however, these representations introduce significant storage overhead. This paper presents a novel method for efficiently compressing a grid-based NeRF model, addressing the storage overhead concern. Our approach is based on the non-linear transform coding paradigm, employing neural compression for compressing the model's feature grids. Due to the lack of training data involving many i.i.d scenes, we design an encoder-free, end-to-end optimized approach for individual scenes, using lightweight decoders. To leverage the spatial inhomogeneity of the latent feature grids, we introduce an importance-weighted rate-distortion objective and a sparse entropy model employing a masking mechanism. Our experimental results validate that our proposed method surpasses existing works in terms of grid-based NeRF compression efficacy and reconstruction quality.","sentences":["Neural Radiance Fields (NeRFs) have emerged as powerful tools for capturing detailed 3D scenes through continuous volumetric representations.","Recent NeRFs utilize feature grids to improve rendering quality and speed; however, these representations introduce significant storage overhead.","This paper presents a novel method for efficiently compressing a grid-based NeRF model, addressing the storage overhead concern.","Our approach is based on the non-linear transform coding paradigm, employing neural compression for compressing the model's feature grids.","Due to the lack of training data involving many i.i.d scenes, we design an encoder-free, end-to-end optimized approach for individual scenes, using lightweight decoders.","To leverage the spatial inhomogeneity of the latent feature grids, we introduce an importance-weighted rate-distortion objective and a sparse entropy model employing a masking mechanism.","Our experimental results validate that our proposed method surpasses existing works in terms of grid-based NeRF compression efficacy and reconstruction quality."],"url":"http://arxiv.org/abs/2406.08943v1","category":"cs.CV"}
{"created":"2024-06-13 08:33:30","title":"Strong gravitational lenses from the Vera C. Rubin Observatory","abstract":"Like many areas of astrophysics and cosmology, the Vera C. Rubin Observatory will be transformational for almost all the applications of strong lensing, thanks to the dramatic increase in the number of known strong lenses by two orders of magnitude or more and the readily available time-domain data for the lenses with transient sources. In this article, we provide an overview of the forecasted number of discovered lenses of different types and describe the primary science cases these large lens samples will enable. We provide an updated forecast on the joint constraint for the dark energy equation-of-state parameters, $w_0$ and $w_a$, from combining all strong lensing probes of dark energy. We update the previous forecast from the Rubin Observatory Dark Energy Science Collaboration's Science Review Document by adding two new crucial strong lensing samples: lensed Type Ia supernovae and single-deflector lenses with measured stellar kinematics. Finally, we describe the current and near-future activities and collaborative efforts within the strong lensing community in preparation for the arrival of the first real dataset from Rubin in early 2026.","sentences":["Like many areas of astrophysics and cosmology, the Vera C. Rubin Observatory will be transformational for almost all the applications of strong lensing, thanks to the dramatic increase in the number of known strong lenses by two orders of magnitude or more and the readily available time-domain data for the lenses with transient sources.","In this article, we provide an overview of the forecasted number of discovered lenses of different types and describe the primary science cases these large lens samples will enable.","We provide an updated forecast on the joint constraint for the dark energy equation-of-state parameters, $w_0$ and $w_a$, from combining all strong lensing probes of dark energy.","We update the previous forecast from the Rubin Observatory Dark Energy Science Collaboration's Science Review Document by adding two new crucial strong lensing samples: lensed Type Ia supernovae and single-deflector lenses with measured stellar kinematics.","Finally, we describe the current and near-future activities and collaborative efforts within the strong lensing community in preparation for the arrival of the first real dataset from Rubin in early 2026."],"url":"http://arxiv.org/abs/2406.08919v1","category":"astro-ph.CO"}
{"created":"2024-06-13 07:47:45","title":"Beyond mean-field effects in dynamics of BEC in the double-well potential","abstract":"The nonlinear dynamics of a quasi-1D BEC loaded in a double-well potential is studied. The beyond mean-field corrections to the energy in the form of the Lee-Huang-Yang term are taken into account. One-dimensional geometry is considered. The problem is described in the scalar approximation by the extended Gross-Pitaevskii (EGP) equation with the attractive quadratic nonlinearity, due to the Lee-Huang-Yang correction, and the effective cubic mean-field nonlinearity describing the residual intra- and inter-species interactions. To describe tunneling and localization phenomena, a two-mode model was obtained. The frequencies of the Josephson oscillations are found and confirmed by the full numerical simulations of the EGP equation. The parametric resonance in the Josephson oscillations, when the height of the barrier is periodically modulated, is studied. The predictions of the dimer model, including the case of the one-dimensional Lee-Huang-Yang superfluid, have been proven.","sentences":["The nonlinear dynamics of a quasi-1D BEC loaded in a double-well potential is studied.","The beyond mean-field corrections to the energy in the form of the Lee-Huang-Yang term are taken into account.","One-dimensional geometry is considered.","The problem is described in the scalar approximation by the extended Gross-Pitaevskii (EGP) equation with the attractive quadratic nonlinearity, due to the Lee-Huang-Yang correction, and the effective cubic mean-field nonlinearity describing the residual intra- and inter-species interactions.","To describe tunneling and localization phenomena, a two-mode model was obtained.","The frequencies of the Josephson oscillations are found and confirmed by the full numerical simulations of the EGP equation.","The parametric resonance in the Josephson oscillations, when the height of the barrier is periodically modulated, is studied.","The predictions of the dimer model, including the case of the one-dimensional Lee-Huang-Yang superfluid, have been proven."],"url":"http://arxiv.org/abs/2406.08895v1","category":"cond-mat.quant-gas"}
{"created":"2024-06-13 07:35:42","title":"SA-DQAS: Self-attention Enhanced Differentiable Quantum Architecture Search","abstract":"We introduce SA-DQAS in this paper, a novel framework that enhances the gradient-based Differentiable Quantum Architecture Search (DQAS) with a self-attention mechanism, aimed at optimizing circuit design for Quantum Machine Learning (QML) challenges. Analogous to a sequence of words in a sentence, a quantum circuit can be viewed as a sequence of placeholders containing quantum gates. Unlike DQAS, each placeholder is independent, while the self-attention mechanism in SA-DQAS helps to capture relation and dependency information among each operation candidate placed on placeholders in a circuit. To evaluate and verify, we conduct experiments on job-shop scheduling problems (JSSP), Max-cut problems, and quantum fidelity. Incorporating self-attention improves the stability and performance of the resulting quantum circuits and refines their structural design with higher noise resilience and fidelity. Our research demonstrates the first successful integration of self-attention with DQAS.","sentences":["We introduce SA-DQAS in this paper, a novel framework that enhances the gradient-based Differentiable Quantum Architecture Search (DQAS) with a self-attention mechanism, aimed at optimizing circuit design for Quantum Machine Learning (QML) challenges.","Analogous to a sequence of words in a sentence, a quantum circuit can be viewed as a sequence of placeholders containing quantum gates.","Unlike DQAS, each placeholder is independent, while the self-attention mechanism in SA-DQAS helps to capture relation and dependency information among each operation candidate placed on placeholders in a circuit.","To evaluate and verify, we conduct experiments on job-shop scheduling problems (JSSP), Max-cut problems, and quantum fidelity.","Incorporating self-attention improves the stability and performance of the resulting quantum circuits and refines their structural design with higher noise resilience and fidelity.","Our research demonstrates the first successful integration of self-attention with DQAS."],"url":"http://arxiv.org/abs/2406.08882v1","category":"quant-ph"}
{"created":"2024-06-13 06:29:16","title":"Inverse Probability of Treatment Weighting with Deep Sequence Models Enables Accurate treatment effect Estimation from Electronic Health Records","abstract":"Observational data have been actively used to estimate treatment effect, driven by the growing availability of electronic health records (EHRs). However, EHRs typically consist of longitudinal records, often introducing time-dependent confoundings that hinder the unbiased estimation of treatment effect. Inverse probability of treatment weighting (IPTW) is a widely used propensity score method since it provides unbiased treatment effect estimation and its derivation is straightforward. In this study, we aim to utilize IPTW to estimate treatment effect in the presence of time-dependent confounding using claims records. Previous studies have utilized propensity score methods with features derived from claims records through feature processing, which generally requires domain knowledge and additional resources to extract information to accurately estimate propensity scores. Deep sequence models, particularly recurrent neural networks and self-attention-based architectures, have demonstrated good performance in modeling EHRs for various downstream tasks. We propose that these deep sequence models can provide accurate IPTW estimation of treatment effect by directly estimating the propensity scores from claims records without the need for feature processing. We empirically demonstrate this by conducting comprehensive evaluations using synthetic and semi-synthetic datasets.","sentences":["Observational data have been actively used to estimate treatment effect, driven by the growing availability of electronic health records (EHRs).","However, EHRs typically consist of longitudinal records, often introducing time-dependent confoundings that hinder the unbiased estimation of treatment effect.","Inverse probability of treatment weighting (IPTW) is a widely used propensity score method since it provides unbiased treatment effect estimation and its derivation is straightforward.","In this study, we aim to utilize IPTW to estimate treatment effect in the presence of time-dependent confounding using claims records.","Previous studies have utilized propensity score methods with features derived from claims records through feature processing, which generally requires domain knowledge and additional resources to extract information to accurately estimate propensity scores.","Deep sequence models, particularly recurrent neural networks and self-attention-based architectures, have demonstrated good performance in modeling EHRs for various downstream tasks.","We propose that these deep sequence models can provide accurate IPTW estimation of treatment effect by directly estimating the propensity scores from claims records without the need for feature processing.","We empirically demonstrate this by conducting comprehensive evaluations using synthetic and semi-synthetic datasets."],"url":"http://arxiv.org/abs/2406.08851v1","category":"cs.LG"}
{"created":"2024-06-13 06:00:28","title":"Research on Deep Learning Model of Feature Extraction Based on Convolutional Neural Network","abstract":"Neural networks with relatively shallow layers and simple structures may have limited ability in accurately identifying pneumonia. In addition, deep neural networks also have a large demand for computing resources, which may cause convolutional neural networks to be unable to be implemented on terminals. Therefore, this paper will carry out the optimal classification of convolutional neural networks. Firstly, according to the characteristics of pneumonia images, AlexNet and InceptionV3 were selected to obtain better image recognition results. Combining the features of medical images, the forward neural network with deeper and more complex structure is learned. Finally, knowledge extraction technology is used to extract the obtained data into the AlexNet model to achieve the purpose of improving computing efficiency and reducing computing costs. The results showed that the prediction accuracy, specificity, and sensitivity of the trained AlexNet model increased by 4.25 percentage points, 7.85 percentage points, and 2.32 percentage points, respectively. The graphics processing usage has decreased by 51% compared to the InceptionV3 mode.","sentences":["Neural networks with relatively shallow layers and simple structures may have limited ability in accurately identifying pneumonia.","In addition, deep neural networks also have a large demand for computing resources, which may cause convolutional neural networks to be unable to be implemented on terminals.","Therefore, this paper will carry out the optimal classification of convolutional neural networks.","Firstly, according to the characteristics of pneumonia images, AlexNet and InceptionV3 were selected to obtain better image recognition results.","Combining the features of medical images, the forward neural network with deeper and more complex structure is learned.","Finally, knowledge extraction technology is used to extract the obtained data into the AlexNet model to achieve the purpose of improving computing efficiency and reducing computing costs.","The results showed that the prediction accuracy, specificity, and sensitivity of the trained AlexNet model increased by 4.25 percentage points, 7.85 percentage points, and 2.32 percentage points, respectively.","The graphics processing usage has decreased by 51% compared to the InceptionV3 mode."],"url":"http://arxiv.org/abs/2406.08837v1","category":"eess.IV"}
{"created":"2024-06-13 05:38:30","title":"Improving Adversarial Robustness via Feature Pattern Consistency Constraint","abstract":"Convolutional Neural Networks (CNNs) are well-known for their vulnerability to adversarial attacks, posing significant security concerns. In response to these threats, various defense methods have emerged to bolster the model's robustness. However, most existing methods either focus on learning from adversarial perturbations, leading to overfitting to the adversarial examples, or aim to eliminate such perturbations during inference, inevitably increasing computational burdens. Conversely, clean training, which strengthens the model's robustness by relying solely on clean examples, can address the aforementioned issues. In this paper, we align with this methodological stream and enhance its generalizability to unknown adversarial examples. This enhancement is achieved by scrutinizing the behavior of latent features within the network. Recognizing that a correct prediction relies on the correctness of the latent feature's pattern, we introduce a novel and effective Feature Pattern Consistency Constraint (FPCC) method to reinforce the latent feature's capacity to maintain the correct feature pattern. Specifically, we propose Spatial-wise Feature Modification and Channel-wise Feature Selection to enhance latent features. Subsequently, we employ the Pattern Consistency Loss to constrain the similarity between the feature pattern of the latent features and the correct feature pattern. Our experiments demonstrate that the FPCC method empowers latent features to uphold correct feature patterns even in the face of adversarial examples, resulting in inherent adversarial robustness surpassing state-of-the-art models.","sentences":["Convolutional Neural Networks (CNNs) are well-known for their vulnerability to adversarial attacks, posing significant security concerns.","In response to these threats, various defense methods have emerged to bolster the model's robustness.","However, most existing methods either focus on learning from adversarial perturbations, leading to overfitting to the adversarial examples, or aim to eliminate such perturbations during inference, inevitably increasing computational burdens.","Conversely, clean training, which strengthens the model's robustness by relying solely on clean examples, can address the aforementioned issues.","In this paper, we align with this methodological stream and enhance its generalizability to unknown adversarial examples.","This enhancement is achieved by scrutinizing the behavior of latent features within the network.","Recognizing that a correct prediction relies on the correctness of the latent feature's pattern, we introduce a novel and effective Feature Pattern Consistency Constraint (FPCC) method to reinforce the latent feature's capacity to maintain the correct feature pattern.","Specifically, we propose Spatial-wise Feature Modification and Channel-wise Feature Selection to enhance latent features.","Subsequently, we employ the Pattern Consistency Loss to constrain the similarity between the feature pattern of the latent features and the correct feature pattern.","Our experiments demonstrate that the FPCC method empowers latent features to uphold correct feature patterns even in the face of adversarial examples, resulting in inherent adversarial robustness surpassing state-of-the-art models."],"url":"http://arxiv.org/abs/2406.08829v1","category":"cs.CV"}
{"created":"2024-06-13 03:54:20","title":"Revealing hidden medium-range order in silicate glass-formers using many-body correlation functions","abstract":"The medium range order (MRO) in amorphous systems has been linked to complex features such as the dynamic heterogeneity of supercooled liquids or the plastic deformation of glasses. However, the nature of the MRO in these materials has remained elusive, primarily due to the lack of methods capable of characterizing this order. Here, we leverage standard two-body structural correlators and advanced many-body correlation functions to probe numerically the MRO in prototypical network glassformers, i.e., silica and sodium silicates, systems that are of importance in natural as well as industrial settings. With increasing Na concentration, one finds that the local environment of Na becomes more structured and the spatial distribution of Na on intermediate length scales changes from blob-like to channel-like, indicating a growing inhomogeneity in the spatial Na arrangement. In parallel, we find that the Si-O network becomes increasingly depolymerized, resulting in a ring size distribution that broadens. The radius of gyration of the rings is well described by a power-law with an exponent around 0.75, indicating that the rings are progressively more crumbled with increasing size. Using a recently proposed four-point correlation function, we reveal that the relative orientation of the tetrahedra shows a transition at a distance around 4 Angstroms, a structural modification that is not seen in standard two-point correlation functions. Furthermore, we find that the length scale characterizing the MRO is non-monotonic as a function of temperature, caused by the competition between energetic and entropic terms. Finally, we demonstrate that the structural correlation lengths as obtained from the correlation functions that quantify the MRO are correlated with macroscopic observables such as the kinetic fragility of the liquids and the elastic properties of the glasses.","sentences":["The medium range order (MRO) in amorphous systems has been linked to complex features such as the dynamic heterogeneity of supercooled liquids or the plastic deformation of glasses.","However, the nature of the MRO in these materials has remained elusive, primarily due to the lack of methods capable of characterizing this order.","Here, we leverage standard two-body structural correlators and advanced many-body correlation functions to probe numerically the MRO in prototypical network glassformers, i.e., silica and sodium silicates, systems that are of importance in natural as well as industrial settings.","With increasing Na concentration, one finds that the local environment of Na becomes more structured and the spatial distribution of Na on intermediate length scales changes from blob-like to channel-like, indicating a growing inhomogeneity in the spatial Na arrangement.","In parallel, we find that the Si-O network becomes increasingly depolymerized, resulting in a ring size distribution that broadens.","The radius of gyration of the rings is well described by a power-law with an exponent around 0.75, indicating that the rings are progressively more crumbled with increasing size.","Using a recently proposed four-point correlation function, we reveal that the relative orientation of the tetrahedra shows a transition at a distance around 4 Angstroms, a structural modification that is not seen in standard two-point correlation functions.","Furthermore, we find that the length scale characterizing the MRO is non-monotonic as a function of temperature, caused by the competition between energetic and entropic terms.","Finally, we demonstrate that the structural correlation lengths as obtained from the correlation functions that quantify the MRO are correlated with macroscopic observables such as the kinetic fragility of the liquids and the elastic properties of the glasses."],"url":"http://arxiv.org/abs/2406.08792v1","category":"cond-mat.dis-nn"}
{"created":"2024-06-13 03:40:37","title":"Laser-target symmetry-breaking in high harmonic generation: from frequency shift to odd-even intensity modulation","abstract":"Although the frequency shift and odd-even intensity modulation in high-order harmonic generation (HHG) have both been observed for asymmetric laser-target systems, they are typically studied as two separate phenomena. In this Letter, we provide a comprehensive picture of these two nonlinear optical phenomena, unifying them through a common origin - asymmetry of the laser-target system. By tuning asymmetric laser-target systems, we discover a transition from the harmonic frequency shift to the odd-even intensity modulation upon increasing the duration of the driving laser pulse. Specifically, these phenomena are observed simultaneously for laser pulses with intermediate pulse duration. For numerical evidence, we solve the time-dependent Schr\\\"{o}dinger equation, while insight into the underlying physics is obtained from a simplified analytically tractable model. Understanding the asymmetric characteristics reflected in the HHG as provided is crucial for retrieving laser-target information, sampling external fields, and probing molecular dynamics.","sentences":["Although the frequency shift and odd-even intensity modulation in high-order harmonic generation (HHG) have both been observed for asymmetric laser-target systems, they are typically studied as two separate phenomena.","In this Letter, we provide a comprehensive picture of these two nonlinear optical phenomena, unifying them through a common origin - asymmetry of the laser-target system.","By tuning asymmetric laser-target systems, we discover a transition from the harmonic frequency shift to the odd-even intensity modulation upon increasing the duration of the driving laser pulse.","Specifically, these phenomena are observed simultaneously for laser pulses with intermediate pulse duration.","For numerical evidence, we solve the time-dependent Schr\\\"{o}dinger equation, while insight into the underlying physics is obtained from a simplified analytically tractable model.","Understanding the asymmetric characteristics reflected in the HHG as provided is crucial for retrieving laser-target information, sampling external fields, and probing molecular dynamics."],"url":"http://arxiv.org/abs/2406.08786v1","category":"physics.optics"}
{"created":"2024-06-13 03:27:01","title":"Hybrid Spatial-spectral Neural Network for Hyperspectral Image Denoising","abstract":"Hyperspectral image (HSI) denoising is an essential procedure for HSI applications. Unfortunately, the existing Transformer-based methods mainly focus on non-local modeling, neglecting the importance of locality in image denoising. Moreover, deep learning methods employ complex spectral learning mechanisms, thus introducing large computation costs.   To address these problems, we propose a hybrid spatial-spectral denoising network (HSSD), in which we design a novel hybrid dual-path network inspired by CNN and Transformer characteristics, leading to capturing both local and non-local spatial details while suppressing noise efficiently. Furthermore, to reduce computational complexity, we adopt a simple but effective decoupling strategy that disentangles the learning of space and spectral channels, where multilayer perception with few parameters is utilized to learn the global correlations among spectra. The synthetic and real experiments demonstrate that our proposed method outperforms state-of-the-art methods on spatial and spectral reconstruction. The code and details are available on https://github.com/HLImg/HSSD.","sentences":["Hyperspectral image (HSI) denoising is an essential procedure for HSI applications.","Unfortunately, the existing Transformer-based methods mainly focus on non-local modeling, neglecting the importance of locality in image denoising.","Moreover, deep learning methods employ complex spectral learning mechanisms, thus introducing large computation costs.   ","To address these problems, we propose a hybrid spatial-spectral denoising network (HSSD), in which we design a novel hybrid dual-path network inspired by CNN and Transformer characteristics, leading to capturing both local and non-local spatial details while suppressing noise efficiently.","Furthermore, to reduce computational complexity, we adopt a simple but effective decoupling strategy that disentangles the learning of space and spectral channels, where multilayer perception with few parameters is utilized to learn the global correlations among spectra.","The synthetic and real experiments demonstrate that our proposed method outperforms state-of-the-art methods on spatial and spectral reconstruction.","The code and details are available on https://github.com/HLImg/HSSD."],"url":"http://arxiv.org/abs/2406.08782v1","category":"eess.IV"}
{"created":"2024-06-13 03:18:27","title":"Regularizing property of the twisted conical K\u00e4hler-Ricci flow","abstract":"In this paper, we show the regularity and uniqueness of the twisted conical K\\\"ahler-Ricci flow running from a positive closed current with zero Lelong number, which extends the regularizing property of the smooth twisted K\\\"ahler-Ricci flow, known as Guedj-Zeriahi's existence theorem and Di Nezza-Lu's uniqueness theorem, to the conical singularity case.","sentences":["In this paper, we show the regularity and uniqueness of the twisted conical K\\\"ahler-Ricci flow running from a positive closed current with zero Lelong number, which extends the regularizing property of the smooth twisted K\\\"ahler-Ricci flow, known as Guedj-Zeriahi's existence theorem and Di Nezza-Lu's uniqueness theorem, to the conical singularity case."],"url":"http://arxiv.org/abs/2406.08778v1","category":"math.CV"}
{"created":"2024-06-13 03:14:16","title":"Finite Time Blowup of Integer- and Fractional-Order Time-Delayed Diffusion Equations","abstract":"In this work, exact solutions are derived for an integer- and fractional-order time-delayed diffusion equation with arbitrary initial conditions. The solutions are obtained using Fourier transform methods in conjunction with the known properties of delay functions. It is observed that the solutions do not exhibit infinite speed of propagation for smooth initial conditions that are bounded and positive. Sufficient conditions on the initial condition are also established such that the finite time blowup of the solutions can be explicitly calculated. Examples are provided that highlight the contrasting behaviours of these exact solutions with the known dynamics of solutions to the standard diffusion equation.","sentences":["In this work, exact solutions are derived for an integer- and fractional-order time-delayed diffusion equation with arbitrary initial conditions.","The solutions are obtained using Fourier transform methods in conjunction with the known properties of delay functions.","It is observed that the solutions do not exhibit infinite speed of propagation for smooth initial conditions that are bounded and positive.","Sufficient conditions on the initial condition are also established such that the finite time blowup of the solutions can be explicitly calculated.","Examples are provided that highlight the contrasting behaviours of these exact solutions with the known dynamics of solutions to the standard diffusion equation."],"url":"http://arxiv.org/abs/2406.08777v1","category":"math.AP"}
{"created":"2024-06-13 02:51:18","title":"LLM-based Knowledge Pruning for Time Series Data Analytics on Edge-computing Devices","abstract":"Limited by the scale and diversity of time series data, the neural networks trained on time series data often overfit and show unsatisfacotry performances. In comparison, large language models (LLMs) recently exhibit impressive generalization in diverse fields. Although massive LLM based approaches are proposed for time series tasks, these methods require to load the whole LLM in both training and reference. This high computational demands limit practical applications in resource-constrained settings, like edge-computing and IoT devices. To address this issue, we propose Knowledge Pruning (KP), a novel paradigm for time series learning in this paper. For a specific downstream task, we argue that the world knowledge learned by LLMs is much redundant and only the related knowledge termed as \"pertinent knowledge\" is useful. Unlike other methods, our KP targets to prune the redundant knowledge and only distill the pertinent knowledge into the target model. This reduces model size and computational costs significantly. Additionally, different from existing LLM based approaches, our KP does not require to load the LLM in the process of training and testing, further easing computational burdens. With our proposed KP, a lightweight network can effectively learn the pertinent knowledge, achieving satisfactory performances with a low computation cost. To verify the effectiveness of our KP, two fundamental tasks on edge-computing devices are investigated in our experiments, where eight diverse environments or benchmarks with different networks are used to verify the generalization of our KP. Through experiments, our KP demonstrates effective learning of pertinent knowledge, achieving notable performance improvements in regression (19.7% on average) and classification (up to 13.7%) tasks, showcasing state-of-the-art results.","sentences":["Limited by the scale and diversity of time series data, the neural networks trained on time series data often overfit and show unsatisfacotry performances.","In comparison, large language models (LLMs) recently exhibit impressive generalization in diverse fields.","Although massive LLM based approaches are proposed for time series tasks, these methods require to load the whole LLM in both training and reference.","This high computational demands limit practical applications in resource-constrained settings, like edge-computing and IoT devices.","To address this issue, we propose Knowledge Pruning (KP), a novel paradigm for time series learning in this paper.","For a specific downstream task, we argue that the world knowledge learned by LLMs is much redundant and only the related knowledge termed as \"pertinent knowledge\" is useful.","Unlike other methods, our KP targets to prune the redundant knowledge and only distill the pertinent knowledge into the target model.","This reduces model size and computational costs significantly.","Additionally, different from existing LLM based approaches, our KP does not require to load the LLM in the process of training and testing, further easing computational burdens.","With our proposed KP, a lightweight network can effectively learn the pertinent knowledge, achieving satisfactory performances with a low computation cost.","To verify the effectiveness of our KP, two fundamental tasks on edge-computing devices are investigated in our experiments, where eight diverse environments or benchmarks with different networks are used to verify the generalization of our KP.","Through experiments, our KP demonstrates effective learning of pertinent knowledge, achieving notable performance improvements in regression (19.7% on average) and classification (up to 13.7%) tasks, showcasing state-of-the-art results."],"url":"http://arxiv.org/abs/2406.08765v1","category":"cs.LG"}
{"created":"2024-06-13 02:02:16","title":"UruBots Autonomous Car Team Two: Team Description Paper for FIRA 2024","abstract":"This paper proposes a mini autonomous car to be used by the team UruBots for the 2024 FIRA Autonomous Cars Race Challenge. The vehicle is proposed focusing on a low cost and light weight setup. Powered by a Raspberry PI4 and with a total weight of 1.15 Kilograms, we show that our vehicle manages to race a track of approximately 13 meters in 11 seconds at the best evaluation that was carried out, with an average speed of 1.2m/s in average. That performance was achieved after training a convolutional neural network with 1500 samples for a total amount of 60 epochs. Overall, we believe that our vehicle are suited to perform at the FIRA Autonomous Cars Race Challenge 2024, helping the development of the field of study and the category in the competition.","sentences":["This paper proposes a mini autonomous car to be used by the team UruBots for the 2024 FIRA Autonomous Cars Race Challenge.","The vehicle is proposed focusing on a low cost and light weight setup.","Powered by a Raspberry PI4 and with a total weight of 1.15 Kilograms, we show that our vehicle manages to race a track of approximately 13 meters in 11 seconds at the best evaluation that was carried out, with an average speed of 1.2m/s in average.","That performance was achieved after training a convolutional neural network with 1500 samples for a total amount of 60 epochs.","Overall, we believe that our vehicle are suited to perform at the FIRA Autonomous Cars Race Challenge 2024, helping the development of the field of study and the category in the competition."],"url":"http://arxiv.org/abs/2406.08741v1","category":"cs.RO"}
{"created":"2024-06-13 00:18:20","title":"Introducing Diminutive Causal Structure into Graph Representation Learning","abstract":"When engaging in end-to-end graph representation learning with Graph Neural Networks (GNNs), the intricate causal relationships and rules inherent in graph data pose a formidable challenge for the model in accurately capturing authentic data relationships. A proposed mitigating strategy involves the direct integration of rules or relationships corresponding to the graph data into the model. However, within the domain of graph representation learning, the inherent complexity of graph data obstructs the derivation of a comprehensive causal structure that encapsulates universal rules or relationships governing the entire dataset. Instead, only specialized diminutive causal structures, delineating specific causal relationships within constrained subsets of graph data, emerge as discernible. Motivated by empirical insights, it is observed that GNN models exhibit a tendency to converge towards such specialized causal structures during the training process. Consequently, we posit that the introduction of these specific causal structures is advantageous for the training of GNN models. Building upon this proposition, we introduce a novel method that enables GNN models to glean insights from these specialized diminutive causal structures, thereby enhancing overall performance. Our method specifically extracts causal knowledge from the model representation of these diminutive causal structures and incorporates interchange intervention to optimize the learning process. Theoretical analysis serves to corroborate the efficacy of our proposed method. Furthermore, empirical experiments consistently demonstrate significant performance improvements across diverse datasets.","sentences":["When engaging in end-to-end graph representation learning with Graph Neural Networks (GNNs), the intricate causal relationships and rules inherent in graph data pose a formidable challenge for the model in accurately capturing authentic data relationships.","A proposed mitigating strategy involves the direct integration of rules or relationships corresponding to the graph data into the model.","However, within the domain of graph representation learning, the inherent complexity of graph data obstructs the derivation of a comprehensive causal structure that encapsulates universal rules or relationships governing the entire dataset.","Instead, only specialized diminutive causal structures, delineating specific causal relationships within constrained subsets of graph data, emerge as discernible.","Motivated by empirical insights, it is observed that GNN models exhibit a tendency to converge towards such specialized causal structures during the training process.","Consequently, we posit that the introduction of these specific causal structures is advantageous for the training of GNN models.","Building upon this proposition, we introduce a novel method that enables GNN models to glean insights from these specialized diminutive causal structures, thereby enhancing overall performance.","Our method specifically extracts causal knowledge from the model representation of these diminutive causal structures and incorporates interchange intervention to optimize the learning process.","Theoretical analysis serves to corroborate the efficacy of our proposed method.","Furthermore, empirical experiments consistently demonstrate significant performance improvements across diverse datasets."],"url":"http://arxiv.org/abs/2406.08709v1","category":"cs.LG"}
{"created":"2024-06-13 00:16:28","title":"Data-driven Thermal Modeling for Electrically Excited Synchronous Motors -- A Supervised Machine Learning Approach","abstract":"This paper proposes a data-driven supervised machine learning (ML) for online thermal modeling of electrically excited synchronous motors (EESMs). EESMs are desired for EVs due to their high performance, efficiency, and durability at a relatively low cost. Therefore, obtaining precise EESM temperature estimations are significantly important, because online accurate temperature estimation can lead to EESM performance improvement and guaranteeing its safety and reliability. In this study, in addition to the default inputs' data, EESM losses data is leveraged to improve the performance of the proposed ML approach for thermal modeling. Exponentially weighted moving averages and standard deviations of the inputs are also incorporated in the learning process to consider the memory effect for modeling a dynamical thermal model. Using the experimental data of an EESM prototype, the performance of ordinary least squares (OLS) method is evaluated through a complete training, testing and cross-validation process. Finally, simulation results will provide the key performance metrics of OLS for EESM thermal modeling.","sentences":["This paper proposes a data-driven supervised machine learning (ML) for online thermal modeling of electrically excited synchronous motors (EESMs).","EESMs are desired for EVs due to their high performance, efficiency, and durability at a relatively low cost.","Therefore, obtaining precise EESM temperature estimations are significantly important, because online accurate temperature estimation can lead to EESM performance improvement and guaranteeing its safety and reliability.","In this study, in addition to the default inputs' data, EESM losses data is leveraged to improve the performance of the proposed ML approach for thermal modeling.","Exponentially weighted moving averages and standard deviations of the inputs are also incorporated in the learning process to consider the memory effect for modeling a dynamical thermal model.","Using the experimental data of an EESM prototype, the performance of ordinary least squares (OLS) method is evaluated through a complete training, testing and cross-validation process.","Finally, simulation results will provide the key performance metrics of OLS for EESM thermal modeling."],"url":"http://arxiv.org/abs/2406.08708v1","category":"eess.SY"}
{"created":"2024-06-13 00:02:41","title":"Differentiable Optics with dLux II: Optical Design Maximising Fisher Information","abstract":"The design of astronomical hardware operating at the diffraction limit requires optimization of physical optical simulations of the instrument with respect to desired figures of merit, such as throughput or astrometric accuracy. These systems can be high dimensional, with highly nonlinear relationships between outputs and the adjustable parameters of the hardware. In this series of papers we present and apply dLux, an open-source end-to-end differentiable optical modelling framework. Automatic differentiation enables not just efficient high-dimensional optimization of astronomical hardware designs, but also Bayesian experimental design directly targeting the precision of experimental outcomes. Automatic second derivatives enable the exact and numerically stable calculation of parameter covariance forecasts, and higher derivatives of these enable direct optimization of these forecasts. We validate this method against analytic theory and illustrate its utility in evaluating the astrometric precision of a parametrized telescope model, and the design of a diffractive pupil to achieve optimal astrometric performance for exoplanet searches. The source code and tutorial software are open source and publicly available, targeting researchers who may wish to harness dLux for their own optical simulation problems.","sentences":["The design of astronomical hardware operating at the diffraction limit requires optimization of physical optical simulations of the instrument with respect to desired figures of merit, such as throughput or astrometric accuracy.","These systems can be high dimensional, with highly nonlinear relationships between outputs and the adjustable parameters of the hardware.","In this series of papers we present and apply dLux, an open-source end-to-end differentiable optical modelling framework.","Automatic differentiation enables not just efficient high-dimensional optimization of astronomical hardware designs, but also Bayesian experimental design directly targeting the precision of experimental outcomes.","Automatic second derivatives enable the exact and numerically stable calculation of parameter covariance forecasts, and higher derivatives of these enable direct optimization of these forecasts.","We validate this method against analytic theory and illustrate its utility in evaluating the astrometric precision of a parametrized telescope model, and the design of a diffractive pupil to achieve optimal astrometric performance for exoplanet searches.","The source code and tutorial software are open source and publicly available, targeting researchers who may wish to harness dLux for their own optical simulation problems."],"url":"http://arxiv.org/abs/2406.08704v1","category":"astro-ph.IM"}
{"created":"2024-06-13 00:02:32","title":"Differentiable Optics with dLux I: Deep calibration of Flat Field and Phase Retrieval with Automatic Differentiation","abstract":"The sensitivity limits of space telescopes are imposed by uncalibrated errors in the point spread function, photon-noise, background light, and detector sensitivity. These are typically calibrated with specialized wavefront sensor hardware and with flat fields obtained on the ground or with calibration sources, but these leave vulnerabilities to residual time-varying or non-common path aberrations and variations in the detector conditions. It is therefore desirable to infer these from science data alone, facing the prohibitively high dimensional problems of phase retrieval and pixel-level calibration. We introduce a new Python package for physical optics simulation, dLux, which uses the machine learning framework JAX to achieve GPU acceleration and automatic differentiation (autodiff), and apply this to simulating astronomical imaging. In this first of a series of papers, we show that gradient descent enabled by autodiff can be used to simultaneously perform phase retrieval and calibration of detector sensitivity, scaling efficiently to inferring millions of parameters. This new framework enables high dimensional optimization and inference in data analysis and hardware design in astronomy and beyond, which we explore in subsequent papers in this series.","sentences":["The sensitivity limits of space telescopes are imposed by uncalibrated errors in the point spread function, photon-noise, background light, and detector sensitivity.","These are typically calibrated with specialized wavefront sensor hardware and with flat fields obtained on the ground or with calibration sources, but these leave vulnerabilities to residual time-varying or non-common path aberrations and variations in the detector conditions.","It is therefore desirable to infer these from science data alone, facing the prohibitively high dimensional problems of phase retrieval and pixel-level calibration.","We introduce a new Python package for physical optics simulation, dLux, which uses the machine learning framework JAX to achieve GPU acceleration and automatic differentiation (autodiff), and apply this to simulating astronomical imaging.","In this first of a series of papers, we show that gradient descent enabled by autodiff can be used to simultaneously perform phase retrieval and calibration of detector sensitivity, scaling efficiently to inferring millions of parameters.","This new framework enables high dimensional optimization and inference in data analysis and hardware design in astronomy and beyond, which we explore in subsequent papers in this series."],"url":"http://arxiv.org/abs/2406.08703v1","category":"astro-ph.IM"}
{"created":"2024-06-12 23:43:02","title":"Positive-energy Dirac particles in cosmology","abstract":"We provide general relativistic treatment of the single-component field described by Dirac's positive-energy wave equation of 1971. It is motivated by Bogomolny's proposal to regard that field as a possible candidate for dark matter. Our emphasis is on standard (flat) Friedmann-Robertson-Walker cosmology. The early universe is not considered, though there is a strong indication that the field may play a role in internally driven generation of cosmic anisotropy.","sentences":["We provide general relativistic treatment of the single-component field described by Dirac's positive-energy wave equation of 1971.","It is motivated by Bogomolny's proposal to regard that field as a possible candidate for dark matter.","Our emphasis is on standard (flat) Friedmann-Robertson-Walker cosmology.","The early universe is not considered, though there is a strong indication that the field may play a role in internally driven generation of cosmic anisotropy."],"url":"http://arxiv.org/abs/2406.08699v1","category":"gr-qc"}
{"created":"2024-06-12 22:23:41","title":"Investigating meV-scale Equilibrium Atomic Dynamics with X-Rays: focus on disordered materials","abstract":"This paper reviews non-resonant meV-resolved inelastic x-ray scattering (IXS) as a complementary method to inelastic neutron scattering (INS). Two aspects of IXS that are notable in this context are (1) that IXS allows straightforward measurements of phonons in small (sub-mm, and even $\\sim 0.01$ mm) samples with $\\sim 1$ meV resolution and excellent Q resolution and (2) that IXS avoids the kinematic constraints of INS. The first allows both new geometries (thin films, diamond anvil cells) and easy access to new materials, while the second allows high quality data on disordered materials, such as scans with sub-meV resolution to arbitrarily high energy transfer become possible , even at momentum transfers as small as 1/nm. The review briefly discusses the spectrometers and compares the practical forms for the dynamic structure factor for crystals, glasses and liquids. This is followed by a longer review of work on liquids and then shorter discussion of work on glasses, crystals, and discussion of specific geometries. This paper complements the longer introduction/review of IXS at arxiv:1504.01098 .","sentences":["This paper reviews non-resonant meV-resolved inelastic x-ray scattering (IXS) as a complementary method to inelastic neutron scattering (INS).","Two aspects of IXS that are notable in this context are (1) that IXS allows straightforward measurements of phonons in small (sub-mm, and even $\\sim 0.01$ mm) samples with $\\sim 1$ meV resolution and excellent Q resolution and (2) that IXS avoids the kinematic constraints of INS.","The first allows both new geometries (thin films, diamond anvil cells) and easy access to new materials, while the second allows high quality data on disordered materials, such as scans with sub-meV resolution to arbitrarily high energy transfer become possible , even at momentum transfers as small as 1/nm.","The review briefly discusses the spectrometers and compares the practical forms for the dynamic structure factor for crystals, glasses and liquids.","This is followed by a longer review of work on liquids and then shorter discussion of work on glasses, crystals, and discussion of specific geometries.","This paper complements the longer introduction/review of IXS at arxiv:1504.01098 ."],"url":"http://arxiv.org/abs/2406.08670v1","category":"cond-mat.soft"}
{"created":"2024-06-12 22:15:48","title":"Causal Inference on Missing Exposure via Robust Estimation","abstract":"How to deal with missing data in observational studies is a common concern for causal inference. When the covariates are missing at random (MAR), multiple approaches have been provided to help solve the issue. However, if the exposure is MAR, few approaches are available and careful adjustments on both missingness and confounding issues are required to ensure a consistent estimate of the true causal effect on the response. In this article, a new inverse probability weighting (IPW) estimator based on weighted estimating equations (WEE) is proposed to incorporate weights from both the missingness and propensity score (PS) models, which can reduce the joint effect of extreme weights in finite samples. Additionally, we develop a triple robust (TR) estimator via WEE to further protect against the misspecification of the missingness model. The asymptotic properties of WEE estimators are proved using properties of estimating equations. Based on the simulation studies, WEE methods outperform others including imputation-based approaches in terms of bias and variability. Finally, an application study is conducted to identify the causal effect of the presence of cardiovascular disease on mortality for COVID-19 patients.","sentences":["How to deal with missing data in observational studies is a common concern for causal inference.","When the covariates are missing at random (MAR), multiple approaches have been provided to help solve the issue.","However, if the exposure is MAR, few approaches are available and careful adjustments on both missingness and confounding issues are required to ensure a consistent estimate of the true causal effect on the response.","In this article, a new inverse probability weighting (IPW) estimator based on weighted estimating equations (WEE) is proposed to incorporate weights from both the missingness and propensity score (PS) models, which can reduce the joint effect of extreme weights in finite samples.","Additionally, we develop a triple robust (TR) estimator via WEE to further protect against the misspecification of the missingness model.","The asymptotic properties of WEE estimators are proved using properties of estimating equations.","Based on the simulation studies, WEE methods outperform others including imputation-based approaches in terms of bias and variability.","Finally, an application study is conducted to identify the causal effect of the presence of cardiovascular disease on mortality for COVID-19 patients."],"url":"http://arxiv.org/abs/2406.08668v1","category":"stat.ME"}
{"created":"2024-06-12 21:43:12","title":"Pruning is Optimal for Learning Sparse Features in High-Dimensions","abstract":"While it is commonly observed in practice that pruning networks to a certain level of sparsity can improve the quality of the features, a theoretical explanation of this phenomenon remains elusive. In this work, we investigate this by demonstrating that a broad class of statistical models can be optimally learned using pruned neural networks trained with gradient descent, in high-dimensions.   We consider learning both single-index and multi-index models of the form $y = \\sigma^*(\\boldsymbol{V}^{\\top} \\boldsymbol{x}) + \\epsilon$, where $\\sigma^*$ is a degree-$p$ polynomial, and $\\boldsymbol{V} \\in \\mathbbm{R}^{d \\times r}$ with $r \\ll d$, is the matrix containing relevant model directions. We assume that $\\boldsymbol{V}$ satisfies a certain $\\ell_q$-sparsity condition for matrices and show that pruning neural networks proportional to the sparsity level of $\\boldsymbol{V}$ improves their sample complexity compared to unpruned networks. Furthermore, we establish Correlational Statistical Query (CSQ) lower bounds in this setting, which take the sparsity level of $\\boldsymbol{V}$ into account. We show that if the sparsity level of $\\boldsymbol{V}$ exceeds a certain threshold, training pruned networks with a gradient descent algorithm achieves the sample complexity suggested by the CSQ lower bound. In the same scenario, however, our results imply that basis-independent methods such as models trained via standard gradient descent initialized with rotationally invariant random weights can provably achieve only suboptimal sample complexity.","sentences":["While it is commonly observed in practice that pruning networks to a certain level of sparsity can improve the quality of the features, a theoretical explanation of this phenomenon remains elusive.","In this work, we investigate this by demonstrating that a broad class of statistical models can be optimally learned using pruned neural networks trained with gradient descent, in high-dimensions.   ","We consider learning both single-index and multi-index models of the form $y = \\sigma^*(\\boldsymbol{V}^{\\top} \\boldsymbol{x})","+ \\epsilon$, where $\\sigma^*$ is a degree-$p$ polynomial, and $\\boldsymbol{V} \\in \\mathbbm{R}^{d \\times r}$ with $r \\ll d$, is the matrix containing relevant model directions.","We assume that $\\boldsymbol{V}$ satisfies a certain $\\ell_q$-sparsity condition for matrices and show that pruning neural networks proportional to the sparsity level of $\\boldsymbol{V}$ improves their sample complexity compared to unpruned networks.","Furthermore, we establish Correlational Statistical Query (CSQ) lower bounds in this setting, which take the sparsity level of $\\boldsymbol{V}$ into account.","We show that if the sparsity level of $\\boldsymbol{V}$ exceeds a certain threshold, training pruned networks with a gradient descent algorithm achieves the sample complexity suggested by the CSQ lower bound.","In the same scenario, however, our results imply that basis-independent methods such as models trained via standard gradient descent initialized with rotationally invariant random weights can provably achieve only suboptimal sample complexity."],"url":"http://arxiv.org/abs/2406.08658v1","category":"stat.ML"}
{"created":"2024-06-12 21:31:32","title":"BaSeNet: A Learning-based Mobile Manipulator Base Pose Sequence Planning for Pickup Tasks","abstract":"In many applications, a mobile manipulator robot is required to grasp a set of objects distributed in space. This may not be feasible from a single base pose and the robot must plan the sequence of base poses for grasping all objects, minimizing the total navigation and grasping time. This is a Combinatorial Optimization problem that can be solved using exact methods, which provide optimal solutions but are computationally expensive, or approximate methods, which offer computationally efficient but sub-optimal solutions. Recent studies have shown that learning-based methods can solve Combinatorial Optimization problems, providing near-optimal and computationally efficient solutions.   In this work, we present BASENET - a learning-based approach to plan the sequence of base poses for the robot to grasp all the objects in the scene. We propose a Reinforcement Learning based solution that learns the base poses for grasping individual objects and the sequence in which the objects should be grasped to minimize the total navigation and grasping costs using Layered Learning. As the problem has a varying number of states and actions, we represent states and actions as a graph and use Graph Neural Networks for learning. We show that the proposed method can produce comparable solutions to exact and approximate methods with significantly less computation time.","sentences":["In many applications, a mobile manipulator robot is required to grasp a set of objects distributed in space.","This may not be feasible from a single base pose and the robot must plan the sequence of base poses for grasping all objects, minimizing the total navigation and grasping time.","This is a Combinatorial Optimization problem that can be solved using exact methods, which provide optimal solutions but are computationally expensive, or approximate methods, which offer computationally efficient but sub-optimal solutions.","Recent studies have shown that learning-based methods can solve Combinatorial Optimization problems, providing near-optimal and computationally efficient solutions.   ","In this work, we present BASENET - a learning-based approach to plan the sequence of base poses for the robot to grasp all the objects in the scene.","We propose a Reinforcement Learning based solution that learns the base poses for grasping individual objects and the sequence in which the objects should be grasped to minimize the total navigation and grasping costs using Layered Learning.","As the problem has a varying number of states and actions, we represent states and actions as a graph and use Graph Neural Networks for learning.","We show that the proposed method can produce comparable solutions to exact and approximate methods with significantly less computation time."],"url":"http://arxiv.org/abs/2406.08653v1","category":"cs.RO"}
{"created":"2024-06-12 21:11:46","title":"PETSc/TAO Developments for Early Exascale Systems","abstract":"The Portable Extensible Toolkit for Scientific Computation (PETSc) library provides scalable solvers for nonlinear time-dependent differential and algebraic equations and for numerical optimization via the Toolkit for Advanced Optimization (TAO). PETSc is used in dozens of scientific fields and is an important building block for many simulation codes. During the U.S. Department of Energy's Exascale Computing Project, the PETSc team has made substantial efforts to enable efficient utilization of the massive fine-grain parallelism present within exascale compute nodes and to enable performance portability across exascale architectures. We recap some of the challenges that designers of numerical libraries face in such an endeavor, and then discuss the many developments we have made, which include the addition of new GPU backends, features supporting efficient on-device matrix assembly, better support for asynchronicity and GPU kernel concurrency, and new communication infrastructure. We evaluate the performance of these developments on some pre-exascale systems as well the early exascale systems Frontier and Aurora, using compute kernel, communication layer, solver, and mini-application benchmark studies, and then close with a few observations drawn from our experiences on the tension between portable performance and other goals of numerical libraries.","sentences":["The Portable Extensible Toolkit for Scientific Computation (PETSc) library provides scalable solvers for nonlinear time-dependent differential and algebraic equations and for numerical optimization via the Toolkit for Advanced Optimization (TAO).","PETSc is used in dozens of scientific fields and is an important building block for many simulation codes.","During the U.S. Department of Energy's Exascale Computing Project, the PETSc team has made substantial efforts to enable efficient utilization of the massive fine-grain parallelism present within exascale compute nodes and to enable performance portability across exascale architectures.","We recap some of the challenges that designers of numerical libraries face in such an endeavor, and then discuss the many developments we have made, which include the addition of new GPU backends, features supporting efficient on-device matrix assembly, better support for asynchronicity and GPU kernel concurrency, and new communication infrastructure.","We evaluate the performance of these developments on some pre-exascale systems as well the early exascale systems Frontier and Aurora, using compute kernel, communication layer, solver, and mini-application benchmark studies, and then close with a few observations drawn from our experiences on the tension between portable performance and other goals of numerical libraries."],"url":"http://arxiv.org/abs/2406.08646v1","category":"cs.MS"}
{"created":"2024-06-12 21:01:58","title":"Operational Calculus for the 1st Level General Fractional Derivatives and its Applications","abstract":"The 1st level General Fractional Derivatives (GFDs) combine in one definition the GFDs of the Riemann-Liouville type and the regularized GFDs (or the GFDs of the Caputo type) that have been recently introduced and actively studied in the Fractional Calculus literature. In this paper, we first construct an operational calculus of Mikusi\\'nski type for the 1st level GFDs. In particular, it includes the operational calculi for the GFDs of the Riemann-Liouville type and for the regularized GFDs as its particular cases. In the second part of the paper, this calculus is applied for derivation of the closed form solution formulas to the initial-value problems for the linear fractional differential equations with the 1st level GFDs.","sentences":["The 1st level General Fractional Derivatives (GFDs) combine in one definition the GFDs of the Riemann-Liouville type and the regularized GFDs (or the GFDs of the Caputo type) that have been recently introduced and actively studied in the Fractional Calculus literature.","In this paper, we first construct an operational calculus of Mikusi\\'nski type for the 1st level GFDs.","In particular, it includes the operational calculi for the GFDs of the Riemann-Liouville type and for the regularized GFDs as its particular cases.","In the second part of the paper, this calculus is applied for derivation of the closed form solution formulas to the initial-value problems for the linear fractional differential equations with the 1st level GFDs."],"url":"http://arxiv.org/abs/2406.08642v1","category":"math.AP"}
{"created":"2024-06-12 19:47:46","title":"Some applications of canonical metrics to Landau-Ginzburg models","abstract":"It is known that a given smooth del Pezzo surface or Fano threefold $X$ admits a choice of log Calabi-Yau compactified mirror toric Landau-Ginzburg model (with respect to certain fixed K\\\"ahler classes and Gorenstein toric degenerations). Here we consider the problem of constructing a corresponding map $\\Theta$ from a domain in the complexified K\\\"ahler cone of $X$ to a well-defined, separated moduli space $\\mathfrak{M}$ of polarised manifolds endowed with a canonical metric. We prove a complete result for del Pezzos and a partial result for some special Fano threefolds. The construction uses some fundamental results in the theory of constant scalar curvature K\\\"ahler metrics. As a consequence $\\mathfrak{M}$ parametrises $K$-stable manifolds and the domain of $\\Theta$ is endowed with the pullback of a Weil-Petersson form.","sentences":["It is known that a given smooth del Pezzo surface or Fano threefold $X$ admits a choice of log Calabi-Yau compactified mirror toric Landau-Ginzburg model (with respect to certain fixed K\\\"ahler classes and Gorenstein toric degenerations).","Here we consider the problem of constructing a corresponding map $\\Theta$ from a domain in the complexified K\\\"ahler cone of $X$ to a well-defined, separated moduli space $\\mathfrak{M}$ of polarised manifolds endowed with a canonical metric.","We prove a complete result for del Pezzos and a partial result for some special Fano threefolds.","The construction uses some fundamental results in the theory of constant scalar curvature K\\\"ahler metrics.","As a consequence $\\mathfrak{M}$ parametrises $K$-stable manifolds and the domain of $\\Theta$ is endowed with the pullback of a Weil-Petersson form."],"url":"http://arxiv.org/abs/2406.08613v1","category":"math.AG"}
{"created":"2024-06-12 19:30:00","title":"Integral representation and approximation of L-functions associated to Hecke cusp eigenforms","abstract":"We derive a family of approximations for L-functions of Hecke cusp eigenforms, according to a recipe first described by Matiyasevich for the Riemann xi function. We show that these approximations converge to the true L-function, and along the way we demonstrate error formulas which may be used to investigate analytic properties of the L-function and its derivatives. Together with the Euler product expansion of the L-function, the family of approximations also encodes some of the key features of the L-function such as its functional equation. Finally, we derive via Mellin transforms a convolution-type formula which leads to precise error bounds in terms of the incomplete gamma function. This formula can be interpreted as an alternative definition for the approximation and sheds light on Matiyasevich's procedure.","sentences":["We derive a family of approximations for L-functions of Hecke cusp eigenforms, according to a recipe first described by Matiyasevich for the Riemann xi function.","We show that these approximations converge to the true L-function, and along the way we demonstrate error formulas which may be used to investigate analytic properties of the L-function and its derivatives.","Together with the Euler product expansion of the L-function, the family of approximations also encodes some of the key features of the L-function such as its functional equation.","Finally, we derive via Mellin transforms a convolution-type formula which leads to precise error bounds in terms of the incomplete gamma function.","This formula can be interpreted as an alternative definition for the approximation and sheds light on Matiyasevich's procedure."],"url":"http://arxiv.org/abs/2406.08608v1","category":"math.NT"}
{"created":"2024-06-12 18:58:17","title":"More Extreme Limits of Manifolds with Positive Scalar Curvature","abstract":"In this article, we extend the example constructed in the paper by Sormani-Tian-Wang to build new examples that satisfy the assumptions of the conjecture by Gromov. Each of these new examples of sequence converges to a limit space with infinitely many poles in $\\mathbb{S}^2$. These examples can be used to test various notions of weak scalar curvature.","sentences":["In this article, we extend the example constructed in the paper by Sormani-Tian-Wang to build new examples that satisfy the assumptions of the conjecture by Gromov.","Each of these new examples of sequence converges to a limit space with infinitely many poles in $\\mathbb{S}^2$. These examples can be used to test various notions of weak scalar curvature."],"url":"http://arxiv.org/abs/2406.08592v1","category":"math.DG"}
{"created":"2024-06-12 18:50:06","title":"Jet Flavour Tagging at FCC-ee with a Transformer-based Neural Network: DeepJetTransformer","abstract":"Jet flavour tagging is crucial in experimental high-energy physics. A tagging algorithm, \\texttt{DeepJetTransformer}, is presented, which exploits a transformer-based neural network that is substantially faster to train.   The \\texttt{DeepJetTransformer} network uses information from particle flow-style objects and secondary vertex reconstruction as is standard for $b$- and $c$-jet identification supplemented by additional information, such as reconstructed V$^0$s and $K^{\\pm}/\\pi^{\\pm}$ discrimination, typically not included in tagging algorithms at the LHC. The model is trained as a multiclassifier to identify all quark flavours separately and performs excellently in identifying $b$- and $c$-jets. An $s$-tagging efficiency of $40\\%$ can be achieved with a $10\\%$ $ud$-jet background efficiency. The impact of including V$^0$s and $K^{\\pm}/\\pi^{\\pm}$ discrimination is presented.   The network is applied on exclusive $Z \\to q\\bar{q}$ samples to examine the physics potential and is shown to isolate $Z \\to s\\bar{s}$ events. Assuming all other backgrounds can be efficiently rejected, a $5\\sigma$ discovery significance for $Z \\to s\\bar{s}$ can be achieved with an integrated luminosity of $60~\\text{nb}^{-1}$, corresponding to less than a second of the FCC-ee run plan at the $Z$ resonance.","sentences":["Jet flavour tagging is crucial in experimental high-energy physics.","A tagging algorithm, \\texttt{DeepJetTransformer}, is presented, which exploits a transformer-based neural network that is substantially faster to train.   ","The \\texttt{DeepJetTransformer} network uses information from particle flow-style objects and secondary vertex reconstruction as is standard for $b$- and $c$-jet identification supplemented by additional information, such as reconstructed V$^0$s and $K^{\\pm}/\\pi^{\\pm}$ discrimination, typically not included in tagging algorithms at the LHC.","The model is trained as a multiclassifier to identify all quark flavours separately and performs excellently in identifying $b$- and $c$-jets.","An $s$-tagging efficiency of $40\\%$ can be achieved with a $10\\%$ $ud$-jet background efficiency.","The impact of including V$^0$s and $K^{\\pm}/\\pi^{\\pm}$ discrimination is presented.   ","The network is applied on exclusive $Z \\to q\\bar{q}$ samples to examine the physics potential and is shown to isolate $Z \\to s\\bar{s}$ events.","Assuming all other backgrounds can be efficiently rejected, a $5\\sigma$ discovery significance for $Z \\to s\\bar{s}$ can be achieved with an integrated luminosity of $60~\\text{nb}^{-1}$, corresponding to less than a second of the FCC-ee run plan at the $Z$ resonance."],"url":"http://arxiv.org/abs/2406.08590v1","category":"hep-ex"}
{"created":"2024-06-12 18:38:09","title":"Exploiting the packing-field route to craft custom time crystals","abstract":"Time crystals are many-body systems that spontaneously break time-translation symmetry, and thus exhibit long-range spatiotemporal order and robust periodic motion. Recent results have demonstrated how to build time-crystal phases in driven diffusive fluids using an external packing field coupled to density fluctuations. Here we exploit this mechanism to engineer and control on-demand custom continuous time crystals characterized by an arbitrary number of rotating condensates, which can be further enhanced with higher-order modes. We elucidate the underlying critical point, as well as general properties of the condensates density profiles and velocities, demonstrating a scaling property of higher-order traveling condensates in terms of first-order ones. We illustrate our findings by solving the hydrodynamic equations for various paradigmatic driven diffusive systems, obtaining along the way a number of remarkable results, e.g. the possibility of explosive time crystal phases characterized by an abrupt, first-order-type transition. Overall, these results demonstrate the versatility and broad possibilities of this promising route to time crystals.","sentences":["Time crystals are many-body systems that spontaneously break time-translation symmetry, and thus exhibit long-range spatiotemporal order and robust periodic motion.","Recent results have demonstrated how to build time-crystal phases in driven diffusive fluids using an external packing field coupled to density fluctuations.","Here we exploit this mechanism to engineer and control on-demand custom continuous time crystals characterized by an arbitrary number of rotating condensates, which can be further enhanced with higher-order modes.","We elucidate the underlying critical point, as well as general properties of the condensates density profiles and velocities, demonstrating a scaling property of higher-order traveling condensates in terms of first-order ones.","We illustrate our findings by solving the hydrodynamic equations for various paradigmatic driven diffusive systems, obtaining along the way a number of remarkable results, e.g. the possibility of explosive time crystal phases characterized by an abrupt, first-order-type transition.","Overall, these results demonstrate the versatility and broad possibilities of this promising route to time crystals."],"url":"http://arxiv.org/abs/2406.08581v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-12 18:36:08","title":"Shape optimization problems involving nonlocal and nonlinear operators","abstract":"In this research, we investigate a general shape optimization problem in which the state equation is expressed using a nonlocal and nonlinear operator. We prove the existence of a minimum point for a functional $F$ defined on the family of all 'quasi-open' subsets of a bounded open set $\\Omega$ in $\\mathbb{R}^n$. This is ensured under the condition that $F$ demonstrates decreasing behavior concerning set inclusion and is lower semicontinuous with respect to a suitable topology associated with the fractional $p$-Laplacian under Dirichlet boundary conditions. Moreover, we study the asymptotic behavior of the solutions when $s\\to1$ and extend this result to the anisotropic case.","sentences":["In this research, we investigate a general shape optimization problem in which the state equation is expressed using a nonlocal and nonlinear operator.","We prove the existence of a minimum point for a functional $F$ defined on the family of all 'quasi-open' subsets of a bounded open set $\\Omega$ in $\\mathbb{R}^n$. This is ensured under the condition that $F$ demonstrates decreasing behavior concerning set inclusion and is lower semicontinuous with respect to a suitable topology associated with the fractional $p$-Laplacian under Dirichlet boundary conditions.","Moreover, we study the asymptotic behavior of the solutions when $s\\to1$ and extend this result to the anisotropic case."],"url":"http://arxiv.org/abs/2406.08579v1","category":"math.AP"}
{"created":"2024-06-12 18:24:59","title":"Alternative representation of Magnus series by exact proper operator exponent","abstract":"In this report the emphasis is on an alternative representation of the Magnus series by proper operator (matrix) exponential solutions to differential equations (systems), both linear and nonlinear ODEs and PDEs. The main idea here is in \\emph{exact} \\emph{linear} representations of the \\emph{nonlinear} DEs. We proceeded from Dyson's time-ordered solutions, and using only generalizations of the well-known Baker-Campbell- Hausdorff (BCH) and Zassenhaus formulae for $t$-dependent operators directly converted them to simple proper operator exponents. The method being explicit both in terms of the operator and in terms of expressing the formal solution as an ordinary exponential, makes it quite easy to calculate analytical expressions to solutions in the form of a Taylor function series in one variable $t$. If introduce a mutually invertible change of variable $t$ into the original equations and then find a solution to this new equation in the form with ordinary exponential, one can obtain a completely different Taylor expansion of the desired function. The essence of this method comes down to resuming the series.","sentences":["In this report the emphasis is on an alternative representation of the Magnus series by proper operator (matrix) exponential solutions to differential equations (systems), both linear and nonlinear ODEs and PDEs.","The main idea here is in \\emph{exact} \\emph{linear} representations of the \\emph{nonlinear} DEs.","We proceeded from Dyson's time-ordered solutions, and using only generalizations of the well-known Baker-Campbell- Hausdorff (BCH) and Zassenhaus formulae for $t$-dependent operators directly converted them to simple proper operator exponents.","The method being explicit both in terms of the operator and in terms of expressing the formal solution as an ordinary exponential, makes it quite easy to calculate analytical expressions to solutions in the form of a Taylor function series in one variable $t$. If introduce a mutually invertible change of variable $t$ into the original equations and then find a solution to this new equation in the form with ordinary exponential, one can obtain a completely different Taylor expansion of the desired function.","The essence of this method comes down to resuming the series."],"url":"http://arxiv.org/abs/2406.08574v1","category":"math-ph"}
{"created":"2024-06-12 18:20:49","title":"Refined cyclic renormalization group in Russian Doll model","abstract":"We discuss the Russian Doll Model (RDM) of superconductivity for finite energy levels. Previously, cyclic renormalization group (RG) and Efimov scaling were found in RDM for part of the equidistant spectrum and we generalize this observation in a few directions. We find that when the whole spectrum is considered, equidistancy condition is removed or diagonal disorder is added, the cyclicity of RG survives but the period of RG becomes energy dependent. The analytic analysis is supported with exact diagonalization.","sentences":["We discuss the Russian Doll Model (RDM) of superconductivity for finite energy levels.","Previously, cyclic renormalization group (RG) and Efimov scaling were found in RDM for part of the equidistant spectrum and we generalize this observation in a few directions.","We find that when the whole spectrum is considered, equidistancy condition is removed or diagonal disorder is added, the cyclicity of RG survives but the period of RG becomes energy dependent.","The analytic analysis is supported with exact diagonalization."],"url":"http://arxiv.org/abs/2406.08573v1","category":"cond-mat.dis-nn"}
{"created":"2024-06-12 18:19:06","title":"The verification of periodicity with the use of recurrent neural networks","abstract":"The ability to automatically and robustly self-verify periodicity present in time-series astronomical data is becoming more important as data sets rapidly increase in size. The age of large astronomical surveys has rendered manual inspection of time-series data less practical. Previous efforts in generating a false alarm probability to verify the periodicity of stars have been aimed towards the analysis of a constructed periodogram. However, these methods feature correlations with features that do not pertain to periodicity, such as light curve shape, slow trends and stochastic variability. The common assumption that photometric errors are Gaussian and well determined is also a limitation of analytic methods. We present a novel machine learning based technique which directly analyses the phase folded light curve for its false alarm probability. We show that the results of this method are largely insensitive to the shape of the light curve, and we establish minimum values for the number of data points and the amplitude to noise ratio.","sentences":["The ability to automatically and robustly self-verify periodicity present in time-series astronomical data is becoming more important as data sets rapidly increase in size.","The age of large astronomical surveys has rendered manual inspection of time-series data less practical.","Previous efforts in generating a false alarm probability to verify the periodicity of stars have been aimed towards the analysis of a constructed periodogram.","However, these methods feature correlations with features that do not pertain to periodicity, such as light curve shape, slow trends and stochastic variability.","The common assumption that photometric errors are Gaussian and well determined is also a limitation of analytic methods.","We present a novel machine learning based technique which directly analyses the phase folded light curve for its false alarm probability.","We show that the results of this method are largely insensitive to the shape of the light curve, and we establish minimum values for the number of data points and the amplitude to noise ratio."],"url":"http://arxiv.org/abs/2406.08571v1","category":"astro-ph.IM"}
{"created":"2024-06-12 18:11:24","title":"Noise-Aware Differentially Private Regression via Meta-Learning","abstract":"Many high-stakes applications require machine learning models that protect user privacy and provide well-calibrated, accurate predictions. While Differential Privacy (DP) is the gold standard for protecting user privacy, standard DP mechanisms typically significantly impair performance. One approach to mitigating this issue is pre-training models on simulated data before DP learning on the private data. In this work we go a step further, using simulated data to train a meta-learning model that combines the Convolutional Conditional Neural Process (ConvCNP) with an improved functional DP mechanism of Hall et al. [2013] yielding the DPConvCNP. DPConvCNP learns from simulated data how to map private data to a DP predictive model in one forward pass, and then provides accurate, well-calibrated predictions. We compare DPConvCNP with a DP Gaussian Process (GP) baseline with carefully tuned hyperparameters. The DPConvCNP outperforms the GP baseline, especially on non-Gaussian data, yet is much faster at test time and requires less tuning.","sentences":["Many high-stakes applications require machine learning models that protect user privacy and provide well-calibrated, accurate predictions.","While Differential Privacy (DP) is the gold standard for protecting user privacy, standard DP mechanisms typically significantly impair performance.","One approach to mitigating this issue is pre-training models on simulated data before DP learning on the private data.","In this work we go a step further, using simulated data to train a meta-learning model that combines the Convolutional Conditional Neural Process (ConvCNP) with an improved functional DP mechanism of Hall et al.","[2013] yielding the DPConvCNP.","DPConvCNP learns from simulated data how to map private data to a DP predictive model in one forward pass, and then provides accurate, well-calibrated predictions.","We compare DPConvCNP with a DP Gaussian Process (GP) baseline with carefully tuned hyperparameters.","The DPConvCNP outperforms the GP baseline, especially on non-Gaussian data, yet is much faster at test time and requires less tuning."],"url":"http://arxiv.org/abs/2406.08569v1","category":"cs.LG"}
{"created":"2024-06-12 18:00:09","title":"Quantum Hardware-Enabled Molecular Dynamics via Transfer Learning","abstract":"The ability to perform ab initio molecular dynamics simulations using potential energies calculated on quantum computers would allow virtually exact dynamics for chemical and biochemical systems, with substantial impacts on the fields of catalysis and biophysics. However, noisy hardware, the costs of computing gradients, and the number of qubits required to simulate large systems present major challenges to realizing the potential of dynamical simulations using quantum hardware. Here, we demonstrate that some of these issues can be mitigated by recent advances in machine learning. By combining transfer learning with techniques for building machine-learned potential energy surfaces, we propose a new path forward for molecular dynamics simulations on quantum hardware. We use transfer learning to reduce the number of energy evaluations that use quantum hardware by first training models on larger, less accurate classical datasets and then refining them on smaller, more accurate quantum datasets. We demonstrate this approach by training machine learning models to predict a molecule's potential energy using Behler-Parrinello neural networks. When successfully trained, the model enables energy gradient predictions necessary for dynamics simulations that cannot be readily obtained directly from quantum hardware. To reduce the quantum resources needed, the model is initially trained with data derived from low-cost techniques, such as Density Functional Theory, and subsequently refined with a smaller dataset obtained from the optimization of the Unitary Coupled Cluster ansatz. We show that this approach significantly reduces the size of the quantum training dataset while capturing the high accuracies needed for quantum chemistry simulations.","sentences":["The ability to perform ab initio molecular dynamics simulations using potential energies calculated on quantum computers would allow virtually exact dynamics for chemical and biochemical systems, with substantial impacts on the fields of catalysis and biophysics.","However, noisy hardware, the costs of computing gradients, and the number of qubits required to simulate large systems present major challenges to realizing the potential of dynamical simulations using quantum hardware.","Here, we demonstrate that some of these issues can be mitigated by recent advances in machine learning.","By combining transfer learning with techniques for building machine-learned potential energy surfaces, we propose a new path forward for molecular dynamics simulations on quantum hardware.","We use transfer learning to reduce the number of energy evaluations that use quantum hardware by first training models on larger, less accurate classical datasets and then refining them on smaller, more accurate quantum datasets.","We demonstrate this approach by training machine learning models to predict a molecule's potential energy using Behler-Parrinello neural networks.","When successfully trained, the model enables energy gradient predictions necessary for dynamics simulations that cannot be readily obtained directly from quantum hardware.","To reduce the quantum resources needed, the model is initially trained with data derived from low-cost techniques, such as Density Functional Theory, and subsequently refined with a smaller dataset obtained from the optimization of the Unitary Coupled Cluster ansatz.","We show that this approach significantly reduces the size of the quantum training dataset while capturing the high accuracies needed for quantum chemistry simulations."],"url":"http://arxiv.org/abs/2406.08554v1","category":"physics.chem-ph"}
{"created":"2024-06-12 16:47:45","title":"Optimizing Container Loading and Unloading through Dual-Cycling and Dockyard Rehandle Reduction Using a Hybrid Genetic Algorithm","abstract":"This paper addresses the optimization of container unloading and loading operations at ports, integrating quay-crane dual-cycling with dockyard rehandle minimization. We present a unified model encompassing both operations: ship container unloading and loading by quay crane, and the other is reducing dockyard rehandles while loading the ship. We recognize that optimizing one aspect in isolation can lead to suboptimal outcomes due to interdependencies. Specifically, optimizing unloading sequences for minimal operation time may inadvertently increase dockyard rehandles during loading and vice versa. To address this NP-hard problem, we propose a hybrid genetic algorithm (GA) QCDC-DR-GA comprising one-dimensional and two-dimensional GA components. Our model, QCDC-DR-GA, consistently outperforms four state-of-the-art methods in maximizing dual cycles and minimizing dockyard rehandles. Compared to those methods, it reduced 15-20% of total operation time for large vessels. Statistical validation through a two-tailed paired t-test confirms the superiority of QCDC-DR-GA at a 5% significance level. The approach effectively combines QCDC optimization with dockyard rehandle minimization, optimizing the total unloading-loading time. Results underscore the inefficiency of separately optimizing QCDC and dockyard rehandles. Fragmented approaches, such as QCDC Scheduling Optimized by bi-level GA and GA-ILSRS (Scenario 2), show limited improvement compared to QCDC-DR-GA. As in GA-ILSRS (Scenario 1), neglecting dual-cycle optimization leads to inferior performance than QCDC-DR-GA. This emphasizes the necessity of simultaneously considering both aspects for optimal resource utilization and overall operational efficiency.","sentences":["This paper addresses the optimization of container unloading and loading operations at ports, integrating quay-crane dual-cycling with dockyard rehandle minimization.","We present a unified model encompassing both operations: ship container unloading and loading by quay crane, and the other is reducing dockyard rehandles while loading the ship.","We recognize that optimizing one aspect in isolation can lead to suboptimal outcomes due to interdependencies.","Specifically, optimizing unloading sequences for minimal operation time may inadvertently increase dockyard rehandles during loading and vice versa.","To address this NP-hard problem, we propose a hybrid genetic algorithm (GA) QCDC-DR-GA comprising one-dimensional and two-dimensional GA components.","Our model, QCDC-DR-GA, consistently outperforms four state-of-the-art methods in maximizing dual cycles and minimizing dockyard rehandles.","Compared to those methods, it reduced 15-20% of total operation time for large vessels.","Statistical validation through a two-tailed paired t-test confirms the superiority of QCDC-DR-GA at a 5% significance level.","The approach effectively combines QCDC optimization with dockyard rehandle minimization, optimizing the total unloading-loading time.","Results underscore the inefficiency of separately optimizing QCDC and dockyard rehandles.","Fragmented approaches, such as QCDC Scheduling Optimized by bi-level GA and GA-ILSRS (Scenario 2), show limited improvement compared to QCDC-DR-GA.","As in GA-ILSRS (Scenario 1), neglecting dual-cycle optimization leads to inferior performance than QCDC-DR-GA.","This emphasizes the necessity of simultaneously considering both aspects for optimal resource utilization and overall operational efficiency."],"url":"http://arxiv.org/abs/2406.08534v1","category":"cs.NE"}
{"created":"2024-06-13 17:59:05","title":"LLAVIDAL: Benchmarking Large Language Vision Models for Daily Activities of Living","abstract":"Large Language Vision Models (LLVMs) have demonstrated effectiveness in processing internet videos, yet they struggle with the visually perplexing dynamics present in Activities of Daily Living (ADL) due to limited pertinent datasets and models tailored to relevant cues. To this end, we propose a framework for curating ADL multiview datasets to fine-tune LLVMs, resulting in the creation of ADL-X, comprising 100K RGB video-instruction pairs, language descriptions, 3D skeletons, and action-conditioned object trajectories. We introduce LLAVIDAL, an LLVM capable of incorporating 3D poses and relevant object trajectories to understand the intricate spatiotemporal relationships within ADLs. Furthermore, we present a novel benchmark, ADLMCQ, for quantifying LLVM effectiveness in ADL scenarios. When trained on ADL-X, LLAVIDAL consistently achieves state-of-the-art performance across all ADL evaluation metrics. Qualitative analysis reveals LLAVIDAL's temporal reasoning capabilities in understanding ADL. The link to the dataset is provided at: https://adl-x.github.io/","sentences":["Large Language Vision Models (LLVMs) have demonstrated effectiveness in processing internet videos, yet they struggle with the visually perplexing dynamics present in Activities of Daily Living (ADL) due to limited pertinent datasets and models tailored to relevant cues.","To this end, we propose a framework for curating ADL multiview datasets to fine-tune LLVMs, resulting in the creation of ADL-X, comprising 100K RGB video-instruction pairs, language descriptions, 3D skeletons, and action-conditioned object trajectories.","We introduce LLAVIDAL, an LLVM capable of incorporating 3D poses and relevant object trajectories to understand the intricate spatiotemporal relationships within ADLs.","Furthermore, we present a novel benchmark, ADLMCQ, for quantifying LLVM effectiveness in ADL scenarios.","When trained on ADL-X, LLAVIDAL consistently achieves state-of-the-art performance across all ADL evaluation metrics.","Qualitative analysis reveals LLAVIDAL's temporal reasoning capabilities in understanding ADL.","The link to the dataset is provided at: https://adl-x.github.io/"],"url":"http://arxiv.org/abs/2406.09390v1","category":"cs.CV"}
{"created":"2024-06-13 17:49:56","title":"Towards an Improved Understanding and Utilization of Maximum Manifold Capacity Representations","abstract":"Maximum Manifold Capacity Representations (MMCR) is a recent multi-view self-supervised learning (MVSSL) method that matches or surpasses other leading MVSSL methods. MMCR is intriguing because it does not fit neatly into any of the commonplace MVSSL lineages, instead originating from a statistical mechanical perspective on the linear separability of data manifolds. In this paper, we seek to improve our understanding and our utilization of MMCR. To better understand MMCR, we leverage tools from high dimensional probability to demonstrate that MMCR incentivizes alignment and uniformity of learned embeddings. We then leverage tools from information theory to show that such embeddings maximize a well-known lower bound on mutual information between views, thereby connecting the geometric perspective of MMCR to the information-theoretic perspective commonly discussed in MVSSL. To better utilize MMCR, we mathematically predict and experimentally confirm non-monotonic changes in the pretraining loss akin to double descent but with respect to atypical hyperparameters. We also discover compute scaling laws that enable predicting the pretraining loss as a function of gradients steps, batch size, embedding dimension and number of views. We then show that MMCR, originally applied to image data, is performant on multimodal image-text data. By more deeply understanding the theoretical and empirical behavior of MMCR, our work reveals insights on improving MVSSL methods.","sentences":["Maximum Manifold Capacity Representations (MMCR) is a recent multi-view self-supervised learning (MVSSL) method that matches or surpasses other leading MVSSL methods.","MMCR is intriguing because it does not fit neatly into any of the commonplace MVSSL lineages, instead originating from a statistical mechanical perspective on the linear separability of data manifolds.","In this paper, we seek to improve our understanding and our utilization of MMCR.","To better understand MMCR, we leverage tools from high dimensional probability to demonstrate that MMCR incentivizes alignment and uniformity of learned embeddings.","We then leverage tools from information theory to show that such embeddings maximize a well-known lower bound on mutual information between views, thereby connecting the geometric perspective of MMCR to the information-theoretic perspective commonly discussed in MVSSL.","To better utilize MMCR, we mathematically predict and experimentally confirm non-monotonic changes in the pretraining loss akin to double descent but with respect to atypical hyperparameters.","We also discover compute scaling laws that enable predicting the pretraining loss as a function of gradients steps, batch size, embedding dimension and number of views.","We then show that MMCR, originally applied to image data, is performant on multimodal image-text data.","By more deeply understanding the theoretical and empirical behavior of MMCR, our work reveals insights on improving MVSSL methods."],"url":"http://arxiv.org/abs/2406.09366v1","category":"cs.LG"}
{"created":"2024-06-13 16:35:16","title":"Pauli Noise Learning for Mid-Circuit Measurements","abstract":"Current benchmarks for mid-circuit measurements (MCMs) are limited in scalability or the types of error they can quantify, necessitating new techniques for quantifying their performance. Here, we introduce a theory for learning Pauli noise in MCMs and use it to create MCM cycle benchmarking, a scalable method for benchmarking MCMs. MCM cycle benchmarking extracts detailed information about the rates of errors in randomly compiled layers of MCMs and Clifford gates, and we demonstrate how its results can be used to quantify correlated errors during MCMs on current quantum hardware. Our method can be integrated into existing Pauli noise learning techniques to scalably characterize and benchmark wide classes of circuits containing MCMs.","sentences":["Current benchmarks for mid-circuit measurements (MCMs) are limited in scalability or the types of error they can quantify, necessitating new techniques for quantifying their performance.","Here, we introduce a theory for learning Pauli noise in MCMs and use it to create MCM cycle benchmarking, a scalable method for benchmarking MCMs.","MCM cycle benchmarking extracts detailed information about the rates of errors in randomly compiled layers of MCMs and Clifford gates, and we demonstrate how its results can be used to quantify correlated errors during MCMs on current quantum hardware.","Our method can be integrated into existing Pauli noise learning techniques to scalably characterize and benchmark wide classes of circuits containing MCMs."],"url":"http://arxiv.org/abs/2406.09299v1","category":"quant-ph"}
{"created":"2024-06-13 16:33:44","title":"MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer Decoding","abstract":"Auto-regressive inference of transformers benefit greatly from Key-Value (KV) caching, but can lead to major memory bottlenecks as model size, batch size, and sequence length grow at scale. We introduce Multi-Layer Key-Value (MLKV) sharing, a novel approach extending KV sharing across transformer layers to reduce memory usage beyond what was possible with Multi-Query Attention (MQA) and Grouped-Query Attention (GQA). Evaluations on various NLP benchmarks and inference metrics using uptrained Pythia-160M variants demonstrate that MLKV significantly reduces memory usage with minimal performance loss, reducing KV cache size down to a factor of 6x compared to MQA. These results highlight MLKV's potential for efficient deployment of transformer models at scale. We provide code at https://github.com/zaydzuhri/pythia-mlkv","sentences":["Auto-regressive inference of transformers benefit greatly from Key-Value (KV) caching, but can lead to major memory bottlenecks as model size, batch size, and sequence length grow at scale.","We introduce Multi-Layer Key-Value (MLKV) sharing, a novel approach extending KV sharing across transformer layers to reduce memory usage beyond what was possible with Multi-Query Attention (MQA) and Grouped-Query Attention (GQA).","Evaluations on various NLP benchmarks and inference metrics using uptrained Pythia-160M variants demonstrate that MLKV significantly reduces memory usage with minimal performance loss, reducing KV cache size down to a factor of 6x compared to MQA.","These results highlight MLKV's potential for efficient deployment of transformer models at scale.","We provide code at https://github.com/zaydzuhri/pythia-mlkv"],"url":"http://arxiv.org/abs/2406.09297v1","category":"cs.LG"}
{"created":"2024-06-13 14:51:52","title":"Comprehensive Machine Learning Model Comparison for Cherenkov and Scintillation Light Separation due to Particle Interactions","abstract":"The demand for novel detector mediums such as Water-based Liquid Scintillator (WbLS) has increased over the last few decades due to their capability for both low energy particle interactions and higher light yield. Recently, the usage of machine learning (ML) methods in high-energy physics has also been increasing. The ML and AI methods are used in many physics projects in the field since they provide effective and sensitive results. In this study, we aimed to develop a comprehensive analysis of water Cherenkov detectors and perform physics analyses to efficiently separate Cherenkov and scintillation photons with ML algorithms using the data from the WbLS detector environment. The main goal of this study was to produce more precise solutions to physics problems, such as signal classification, by applying ML techniques to the simulation and experimental data. Here, we trained more than 20 ML models, and our results revealed that three machine learning models, XGBoost, Light GBM, and Random Forest models, and their ensemble model gave us more than 95\\% accuracy for separating Cherenkov and scintillation photons with balanced and unbalanced datasets. This is a significant increase in efficiency as compared with the results of the classical method by applying simple time cuts.","sentences":["The demand for novel detector mediums such as Water-based Liquid Scintillator (WbLS) has increased over the last few decades due to their capability for both low energy particle interactions and higher light yield.","Recently, the usage of machine learning (ML) methods in high-energy physics has also been increasing.","The ML and AI methods are used in many physics projects in the field since they provide effective and sensitive results.","In this study, we aimed to develop a comprehensive analysis of water Cherenkov detectors and perform physics analyses to efficiently separate Cherenkov and scintillation photons with ML algorithms using the data from the WbLS detector environment.","The main goal of this study was to produce more precise solutions to physics problems, such as signal classification, by applying ML techniques to the simulation and experimental data.","Here, we trained more than 20 ML models, and our results revealed that three machine learning models, XGBoost, Light GBM, and Random Forest models, and their ensemble model gave us more than 95\\% accuracy for separating Cherenkov and scintillation photons with balanced and unbalanced datasets.","This is a significant increase in efficiency as compared with the results of the classical method by applying simple time cuts."],"url":"http://arxiv.org/abs/2406.09191v1","category":"hep-ex"}
{"created":"2024-06-13 14:30:35","title":"SR-CACO-2: A Dataset for Confocal Fluorescence Microscopy Image Super-Resolution","abstract":"Confocal fluorescence microscopy is one of the most accessible and widely used imaging techniques for the study of biological processes. Scanning confocal microscopy allows the capture of high-quality images from 3D samples, yet suffers from well-known limitations such as photobleaching and phototoxicity of specimens caused by intense light exposure, which limits its use in some applications, especially for living cells. Cellular damage can be alleviated by changing imaging parameters to reduce light exposure, often at the expense of image quality. Machine/deep learning methods for single-image super-resolution (SISR) can be applied to restore image quality by upscaling lower-resolution (LR) images to produce high-resolution images (HR). These SISR methods have been successfully applied to photo-realistic images due partly to the abundance of publicly available data. In contrast, the lack of publicly available data partly limits their application and success in scanning confocal microscopy. In this paper, we introduce a large scanning confocal microscopy dataset named SR-CACO-2 that is comprised of low- and high-resolution image pairs marked for three different fluorescent markers. It allows the evaluation of performance of SISR methods on three different upscaling levels (X2, X4, X8). SR-CACO-2 contains the human epithelial cell line Caco-2 (ATCC HTB-37), and it is composed of 22 tiles that have been translated in the form of 9,937 image patches for experiments with SISR methods. Given the new SR-CACO-2 dataset, we also provide benchmarking results for 15 state-of-the-art methods that are representative of the main SISR families. Results show that these methods have limited success in producing high-resolution textures, indicating that SR-CACO-2 represents a challenging problem. Our dataset, code and pretrained weights are available: https://github.com/sbelharbi/sr-caco-2.","sentences":["Confocal fluorescence microscopy is one of the most accessible and widely used imaging techniques for the study of biological processes.","Scanning confocal microscopy allows the capture of high-quality images from 3D samples, yet suffers from well-known limitations such as photobleaching and phototoxicity of specimens caused by intense light exposure, which limits its use in some applications, especially for living cells.","Cellular damage can be alleviated by changing imaging parameters to reduce light exposure, often at the expense of image quality.","Machine/deep learning methods for single-image super-resolution (SISR) can be applied to restore image quality by upscaling lower-resolution (LR) images to produce high-resolution images (HR).","These SISR methods have been successfully applied to photo-realistic images due partly to the abundance of publicly available data.","In contrast, the lack of publicly available data partly limits their application and success in scanning confocal microscopy.","In this paper, we introduce a large scanning confocal microscopy dataset named SR-CACO-2 that is comprised of low- and high-resolution image pairs marked for three different fluorescent markers.","It allows the evaluation of performance of SISR methods on three different upscaling levels (X2, X4, X8).","SR-CACO-2 contains the human epithelial cell line Caco-2 (ATCC HTB-37), and it is composed of 22 tiles that have been translated in the form of 9,937 image patches for experiments with SISR methods.","Given the new SR-CACO-2 dataset, we also provide benchmarking results for 15 state-of-the-art methods that are representative of the main SISR families.","Results show that these methods have limited success in producing high-resolution textures, indicating that SR-CACO-2 represents a challenging problem.","Our dataset, code and pretrained weights are available: https://github.com/sbelharbi/sr-caco-2."],"url":"http://arxiv.org/abs/2406.09168v1","category":"eess.IV"}
{"created":"2024-06-13 14:17:47","title":"LASER: Learning by Aligning Self-supervised Representations of Speech for Improving Content-related Tasks","abstract":"Self-supervised learning (SSL)-based speech models are extensively used for full-stack speech processing. However, it has been observed that improving SSL-based speech representations using unlabeled speech for content-related tasks is challenging and computationally expensive. Recent attempts have been made to address this issue with cost-effective self-supervised fine-tuning (SSFT) approaches. Continuing in this direction, a cost-effective SSFT method named \"LASER: Learning by Aligning Self-supervised Representations\" is presented. LASER is based on the soft-DTW alignment loss with temporal regularisation term. Experiments are conducted with HuBERT and WavLM models and evaluated on the SUPERB benchmark for two content-related tasks: automatic speech recognition (ASR) and phoneme recognition (PR). A relative improvement of 3.7% and 8.2% for HuBERT, and 4.1% and 11.7% for WavLM are observed, for the ASR and PR tasks respectively, with only < 3 hours of fine-tuning on a single GPU.","sentences":["Self-supervised learning (SSL)-based speech models are extensively used for full-stack speech processing.","However, it has been observed that improving SSL-based speech representations using unlabeled speech for content-related tasks is challenging and computationally expensive.","Recent attempts have been made to address this issue with cost-effective self-supervised fine-tuning (SSFT) approaches.","Continuing in this direction, a cost-effective SSFT method named \"LASER: Learning by Aligning Self-supervised Representations\" is presented.","LASER is based on the soft-DTW alignment loss with temporal regularisation term.","Experiments are conducted with HuBERT and WavLM models and evaluated on the SUPERB benchmark for two content-related tasks: automatic speech recognition (ASR) and phoneme recognition (PR).","A relative improvement of 3.7% and 8.2% for HuBERT, and 4.1% and 11.7% for WavLM are observed, for the ASR and PR tasks respectively, with only < 3 hours of fine-tuning on a single GPU."],"url":"http://arxiv.org/abs/2406.09153v1","category":"cs.CL"}
{"created":"2024-06-13 14:14:27","title":"Weakly-supervised anomaly detection for multimodal data distributions","abstract":"Weakly-supervised anomaly detection can outperform existing unsupervised methods with the assistance of a very small number of labeled anomalies, which attracts increasing attention from researchers. However, existing weakly-supervised anomaly detection methods are limited as these methods do not factor in the multimodel nature of the real-world data distribution. To mitigate this, we propose the Weakly-supervised Variational-mixture-model-based Anomaly Detector (WVAD). WVAD excels in multimodal datasets. It consists of two components: a deep variational mixture model, and an anomaly score estimator. The deep variational mixture model captures various features of the data from different clusters, then these features are delivered to the anomaly score estimator to assess the anomaly levels. Experimental results on three real-world datasets demonstrate WVAD's superiority.","sentences":["Weakly-supervised anomaly detection can outperform existing unsupervised methods with the assistance of a very small number of labeled anomalies, which attracts increasing attention from researchers.","However, existing weakly-supervised anomaly detection methods are limited as these methods do not factor in the multimodel nature of the real-world data distribution.","To mitigate this, we propose the Weakly-supervised Variational-mixture-model-based Anomaly Detector (WVAD).","WVAD excels in multimodal datasets.","It consists of two components: a deep variational mixture model, and an anomaly score estimator.","The deep variational mixture model captures various features of the data from different clusters, then these features are delivered to the anomaly score estimator to assess the anomaly levels.","Experimental results on three real-world datasets demonstrate WVAD's superiority."],"url":"http://arxiv.org/abs/2406.09147v1","category":"cs.LG"}
{"created":"2024-06-13 14:07:15","title":"Dynamic Correlation Clustering in Sublinear Update Time","abstract":"We study the classic problem of correlation clustering in dynamic node streams. In this setting, nodes are either added or randomly deleted over time, and each node pair is connected by a positive or negative edge. The objective is to continuously find a partition which minimizes the sum of positive edges crossing clusters and negative edges within clusters. We present an algorithm that maintains an $O(1)$-approximation with $O$(polylog $n$) amortized update time. Prior to our work, Behnezhad, Charikar, Ma, and L. Tan achieved a $5$-approximation with $O(1)$ expected update time in edge streams which translates in node streams to an $O(D)$-update time where $D$ is the maximum possible degree. Finally we complement our theoretical analysis with experiments on real world data.","sentences":["We study the classic problem of correlation clustering in dynamic node streams.","In this setting, nodes are either added or randomly deleted over time, and each node pair is connected by a positive or negative edge.","The objective is to continuously find a partition which minimizes the sum of positive edges crossing clusters and negative edges within clusters.","We present an algorithm that maintains an $O(1)$-approximation with $O$(polylog $n$) amortized update time.","Prior to our work, Behnezhad, Charikar, Ma, and L. Tan achieved a $5$-approximation with $O(1)$ expected update time in edge streams which translates in node streams to an $O(D)$-update time where $D$ is the maximum possible degree.","Finally we complement our theoretical analysis with experiments on real world data."],"url":"http://arxiv.org/abs/2406.09137v1","category":"cs.DS"}
{"created":"2024-06-13 14:02:18","title":"OLGA: One-cLass Graph Autoencoder","abstract":"One-class learning (OCL) comprises a set of techniques applied when real-world problems have a single class of interest. The usual procedure for OCL is learning a hypersphere that comprises instances of this class and, ideally, repels unseen instances from any other classes. Besides, several OCL algorithms for graphs have been proposed since graph representation learning has succeeded in various fields. These methods may use a two-step strategy, initially representing the graph and, in a second step, classifying its nodes. On the other hand, end-to-end methods learn the node representations while classifying the nodes in one learning process. We highlight three main gaps in the literature on OCL for graphs: (i) non-customized representations for OCL; (ii) the lack of constraints on hypersphere parameters learning; and (iii) the methods' lack of interpretability and visualization. We propose One-cLass Graph Autoencoder (OLGA). OLGA is end-to-end and learns the representations for the graph nodes while encapsulating the interest instances by combining two loss functions. We propose a new hypersphere loss function to encapsulate the interest instances. OLGA combines this new hypersphere loss with the graph autoencoder reconstruction loss to improve model learning. OLGA achieved state-of-the-art results and outperformed six other methods with a statistically significant difference from five methods. Moreover, OLGA learns low-dimensional representations maintaining the classification performance with an interpretable model representation learning and results.","sentences":["One-class learning (OCL) comprises a set of techniques applied when real-world problems have a single class of interest.","The usual procedure for OCL is learning a hypersphere that comprises instances of this class and, ideally, repels unseen instances from any other classes.","Besides, several OCL algorithms for graphs have been proposed since graph representation learning has succeeded in various fields.","These methods may use a two-step strategy, initially representing the graph and, in a second step, classifying its nodes.","On the other hand, end-to-end methods learn the node representations while classifying the nodes in one learning process.","We highlight three main gaps in the literature on OCL for graphs: (i) non-customized representations for OCL; (ii) the lack of constraints on hypersphere parameters learning; and (iii) the methods' lack of interpretability and visualization.","We propose One-cLass Graph Autoencoder (OLGA).","OLGA is end-to-end and learns the representations for the graph nodes while encapsulating the interest instances by combining two loss functions.","We propose a new hypersphere loss function to encapsulate the interest instances.","OLGA combines this new hypersphere loss with the graph autoencoder reconstruction loss to improve model learning.","OLGA achieved state-of-the-art results and outperformed six other methods with a statistically significant difference from five methods.","Moreover, OLGA learns low-dimensional representations maintaining the classification performance with an interpretable model representation learning and results."],"url":"http://arxiv.org/abs/2406.09131v1","category":"cs.LG"}
{"created":"2024-06-13 13:31:04","title":"Chain-of-Though (CoT) prompting strategies for medical error detection and correction","abstract":"This paper describes our submission to the MEDIQA-CORR 2024 shared task for automatically detecting and correcting medical errors in clinical notes. We report results for three methods of few-shot In-Context Learning (ICL) augmented with Chain-of-Thought (CoT) and reason prompts using a large language model (LLM). In the first method, we manually analyse a subset of train and validation dataset to infer three CoT prompts by examining error types in the clinical notes. In the second method, we utilise the training dataset to prompt the LLM to deduce reasons about their correctness or incorrectness. The constructed CoTs and reasons are then augmented with ICL examples to solve the tasks of error detection, span identification, and error correction. Finally, we combine the two methods using a rule-based ensemble method. Across the three sub-tasks, our ensemble method achieves a ranking of 3rd for both sub-task 1 and 2, while securing 7th place in sub-task 3 among all submissions.","sentences":["This paper describes our submission to the MEDIQA-CORR 2024 shared task for automatically detecting and correcting medical errors in clinical notes.","We report results for three methods of few-shot In-Context Learning (ICL) augmented with Chain-of-Thought (CoT) and reason prompts using a large language model (LLM).","In the first method, we manually analyse a subset of train and validation dataset to infer three CoT prompts by examining error types in the clinical notes.","In the second method, we utilise the training dataset to prompt the LLM to deduce reasons about their correctness or incorrectness.","The constructed CoTs and reasons are then augmented with ICL examples to solve the tasks of error detection, span identification, and error correction.","Finally, we combine the two methods using a rule-based ensemble method.","Across the three sub-tasks, our ensemble method achieves a ranking of 3rd for both sub-task 1 and 2, while securing 7th place in sub-task 3 among all submissions."],"url":"http://arxiv.org/abs/2406.09103v1","category":"cs.CL"}
{"created":"2024-06-13 12:54:53","title":"On the Robustness of Global Feature Effect Explanations","abstract":"We study the robustness of global post-hoc explanations for predictive models trained on tabular data. Effects of predictor features in black-box supervised learning are an essential diagnostic tool for model debugging and scientific discovery in applied sciences. However, how vulnerable they are to data and model perturbations remains an open research question. We introduce several theoretical bounds for evaluating the robustness of partial dependence plots and accumulated local effects. Our experimental results with synthetic and real-world datasets quantify the gap between the best and worst-case scenarios of (mis)interpreting machine learning predictions globally.","sentences":["We study the robustness of global post-hoc explanations for predictive models trained on tabular data.","Effects of predictor features in black-box supervised learning are an essential diagnostic tool for model debugging and scientific discovery in applied sciences.","However, how vulnerable they are to data and model perturbations remains an open research question.","We introduce several theoretical bounds for evaluating the robustness of partial dependence plots and accumulated local effects.","Our experimental results with synthetic and real-world datasets quantify the gap between the best and worst-case scenarios of (mis)interpreting machine learning predictions globally."],"url":"http://arxiv.org/abs/2406.09069v1","category":"cs.LG"}
{"created":"2024-06-13 11:32:16","title":"Finite-temperature properties of antiferroelectric perovskite $\\rm PbZrO_3$ from deep learning interatomic potential","abstract":"The prototypical antiferroelectric perovskite $\\rm PbZrO_3$ (PZO) has garnered considerable attentions in recent years due to its significance in technological applications and fundamental research. Many unresolved issues in PZO are associated with large spatial and time scales, as well as finite temperatures, presenting significant challenges for first-principles density functional theory studies. Here, we introduce a deep learning interatomic potential of PZO, enabling investigation of finite-temperature properties through large-scale atomistic simulations. Trained using an elaborately designed dataset, the model successfully reproduces a large number of phases, in particular, the recently discovered 80-atom antiferroelectric $Pnam$ phase and ferrielectric $Ima2$ phase, providing precise predictions for their structural and dynamical properties. Using this model, we investigated phase transitions of multiple phases, including $Pbam$/$Pnam$, $Ima2$ and $R3c$, which show high similarity to the experimental observation. Our simulation results also highlight the crucial role of free-energy in determining the low-temperature phase of PZO, reconciling the apparent contradiction: $Pbam$ is the most commonly observed phase in experiments, while theoretical calculations predict other phases exhibiting even lower energy. Furthermore, in the temperature range where the $Pbam$ phase is thermodynamically stable, typical double polarization hysteresis loops for antiferroelectrics were obtained, along with a detailed elucidation of the dynamical evolution upon electric-field induced transitions between the non-polar $Pbam$ and polar $R3c$ phases.","sentences":["The prototypical antiferroelectric perovskite $\\rm PbZrO_3$ (PZO) has garnered considerable attentions in recent years due to its significance in technological applications and fundamental research.","Many unresolved issues in PZO are associated with large spatial and time scales, as well as finite temperatures, presenting significant challenges for first-principles density functional theory studies.","Here, we introduce a deep learning interatomic potential of PZO, enabling investigation of finite-temperature properties through large-scale atomistic simulations.","Trained using an elaborately designed dataset, the model successfully reproduces a large number of phases, in particular, the recently discovered 80-atom antiferroelectric $Pnam$ phase and ferrielectric $Ima2$ phase, providing precise predictions for their structural and dynamical properties.","Using this model, we investigated phase transitions of multiple phases, including $Pbam$/$Pnam$, $Ima2$ and $R3c$, which show high similarity to the experimental observation.","Our simulation results also highlight the crucial role of free-energy in determining the low-temperature phase of PZO, reconciling the apparent contradiction: $Pbam$ is the most commonly observed phase in experiments, while theoretical calculations predict other phases exhibiting even lower energy.","Furthermore, in the temperature range where the $Pbam$ phase is thermodynamically stable, typical double polarization hysteresis loops for antiferroelectrics were obtained, along with a detailed elucidation of the dynamical evolution upon electric-field induced transitions between the non-polar $Pbam$ and polar $R3c$ phases."],"url":"http://arxiv.org/abs/2406.09011v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-13 11:12:46","title":"Enhancing Cross-Modal Fine-Tuning with Gradually Intermediate Modality Generation","abstract":"Large-scale pretrained models have proven immensely valuable in handling data-intensive modalities like text and image. However, fine-tuning these models for certain specialized modalities, such as protein sequence and cosmic ray, poses challenges due to the significant modality discrepancy and scarcity of labeled data. In this paper, we propose an end-to-end method, PaRe, to enhance cross-modal fine-tuning, aiming to transfer a large-scale pretrained model to various target modalities. PaRe employs a gating mechanism to select key patches from both source and target data. Through a modality-agnostic Patch Replacement scheme, these patches are preserved and combined to construct data-rich intermediate modalities ranging from easy to hard. By gradually intermediate modality generation, we can not only effectively bridge the modality gap to enhance stability and transferability of cross-modal fine-tuning, but also address the challenge of limited data in the target modality by leveraging enriched intermediate modality data. Compared with hand-designed, general-purpose, task-specific, and state-of-the-art cross-modal fine-tuning approaches, PaRe demonstrates superior performance across three challenging benchmarks, encompassing more than ten modalities.","sentences":["Large-scale pretrained models have proven immensely valuable in handling data-intensive modalities like text and image.","However, fine-tuning these models for certain specialized modalities, such as protein sequence and cosmic ray, poses challenges due to the significant modality discrepancy and scarcity of labeled data.","In this paper, we propose an end-to-end method, PaRe, to enhance cross-modal fine-tuning, aiming to transfer a large-scale pretrained model to various target modalities.","PaRe employs a gating mechanism to select key patches from both source and target data.","Through a modality-agnostic Patch Replacement scheme, these patches are preserved and combined to construct data-rich intermediate modalities ranging from easy to hard.","By gradually intermediate modality generation, we can not only effectively bridge the modality gap to enhance stability and transferability of cross-modal fine-tuning, but also address the challenge of limited data in the target modality by leveraging enriched intermediate modality data.","Compared with hand-designed, general-purpose, task-specific, and state-of-the-art cross-modal fine-tuning approaches, PaRe demonstrates superior performance across three challenging benchmarks, encompassing more than ten modalities."],"url":"http://arxiv.org/abs/2406.09003v1","category":"cs.CV"}
{"created":"2024-06-13 10:36:18","title":"ToneUnit: A Speech Discretization Approach for Tonal Language Speech Synthesis","abstract":"Representing speech as discretized units has numerous benefits in supporting downstream spoken language processing tasks. However, the approach has been less explored in speech synthesis of tonal languages like Mandarin Chinese. Our preliminary experiments on Chinese speech synthesis reveal the issue of \"tone shift\", where a synthesized speech utterance contains correct base syllables but incorrect tones. To address the issue, we propose the ToneUnit framework, which leverages annotated data with tone labels as CTC supervision to learn tone-aware discrete speech units for Mandarin Chinese speech. Our findings indicate that the discrete units acquired through the TonUnit resolve the \"tone shift\" issue in synthesized Chinese speech and yield favorable results in English synthesis. Moreover, the experimental results suggest that finite scalar quantization enhances the effectiveness of ToneUnit. Notably, ToneUnit can work effectively even with minimal annotated data.","sentences":["Representing speech as discretized units has numerous benefits in supporting downstream spoken language processing tasks.","However, the approach has been less explored in speech synthesis of tonal languages like Mandarin Chinese.","Our preliminary experiments on Chinese speech synthesis reveal the issue of \"tone shift\", where a synthesized speech utterance contains correct base syllables but incorrect tones.","To address the issue, we propose the ToneUnit framework, which leverages annotated data with tone labels as CTC supervision to learn tone-aware discrete speech units for Mandarin Chinese speech.","Our findings indicate that the discrete units acquired through the TonUnit resolve the \"tone shift\" issue in synthesized Chinese speech and yield favorable results in English synthesis.","Moreover, the experimental results suggest that finite scalar quantization enhances the effectiveness of ToneUnit.","Notably, ToneUnit can work effectively even with minimal annotated data."],"url":"http://arxiv.org/abs/2406.08989v1","category":"eess.AS"}
{"created":"2024-06-13 09:49:58","title":"SIU: A Million-Scale Structural Small Molecule-Protein Interaction Dataset for Unbiased Bioactivity Prediction","abstract":"Small molecules play a pivotal role in modern medicine, and scrutinizing their interactions with protein targets is essential for the discovery and development of novel, life-saving therapeutics. The term \"bioactivity\" encompasses various biological effects resulting from these interactions, including both binding and functional responses. The magnitude of bioactivity dictates the therapeutic or toxic pharmacological outcomes of small molecules, rendering accurate bioactivity prediction crucial for the development of safe and effective drugs. However, existing structural datasets of small molecule-protein interactions are often limited in scale and lack systematically organized bioactivity labels, thereby impeding our understanding of these interactions and precise bioactivity prediction. In this study, we introduce a comprehensive dataset of small molecule-protein interactions, consisting of over a million binding structures, each annotated with real biological activity labels. This dataset is designed to facilitate unbiased bioactivity prediction. We evaluated several classical models on this dataset, and the results demonstrate that the task of unbiased bioactivity prediction is challenging yet essential.","sentences":["Small molecules play a pivotal role in modern medicine, and scrutinizing their interactions with protein targets is essential for the discovery and development of novel, life-saving therapeutics.","The term \"bioactivity\" encompasses various biological effects resulting from these interactions, including both binding and functional responses.","The magnitude of bioactivity dictates the therapeutic or toxic pharmacological outcomes of small molecules, rendering accurate bioactivity prediction crucial for the development of safe and effective drugs.","However, existing structural datasets of small molecule-protein interactions are often limited in scale and lack systematically organized bioactivity labels, thereby impeding our understanding of these interactions and precise bioactivity prediction.","In this study, we introduce a comprehensive dataset of small molecule-protein interactions, consisting of over a million binding structures, each annotated with real biological activity labels.","This dataset is designed to facilitate unbiased bioactivity prediction.","We evaluated several classical models on this dataset, and the results demonstrate that the task of unbiased bioactivity prediction is challenging yet essential."],"url":"http://arxiv.org/abs/2406.08961v1","category":"q-bio.BM"}
{"created":"2024-06-13 09:36:27","title":"An Unsupervised Approach to Achieve Supervised-Level Explainability in Healthcare Records","abstract":"Electronic healthcare records are vital for patient safety as they document conditions, plans, and procedures in both free text and medical codes. Language models have significantly enhanced the processing of such records, streamlining workflows and reducing manual data entry, thereby saving healthcare providers significant resources. However, the black-box nature of these models often leaves healthcare professionals hesitant to trust them. State-of-the-art explainability methods increase model transparency but rely on human-annotated evidence spans, which are costly. In this study, we propose an approach to produce plausible and faithful explanations without needing such annotations. We demonstrate on the automated medical coding task that adversarial robustness training improves explanation plausibility and introduce AttInGrad, a new explanation method superior to previous ones. By combining both contributions in a fully unsupervised setup, we produce explanations of comparable quality, or better, to that of a supervised approach. We release our code and model weights.","sentences":["Electronic healthcare records are vital for patient safety as they document conditions, plans, and procedures in both free text and medical codes.","Language models have significantly enhanced the processing of such records, streamlining workflows and reducing manual data entry, thereby saving healthcare providers significant resources.","However, the black-box nature of these models often leaves healthcare professionals hesitant to trust them.","State-of-the-art explainability methods increase model transparency but rely on human-annotated evidence spans, which are costly.","In this study, we propose an approach to produce plausible and faithful explanations without needing such annotations.","We demonstrate on the automated medical coding task that adversarial robustness training improves explanation plausibility and introduce AttInGrad, a new explanation method superior to previous ones.","By combining both contributions in a fully unsupervised setup, we produce explanations of comparable quality, or better, to that of a supervised approach.","We release our code and model weights."],"url":"http://arxiv.org/abs/2406.08958v1","category":"cs.LG"}
{"created":"2024-06-13 08:41:17","title":"Stencil Computations on AMD and Nvidia Graphics Processors: Performance and Tuning Strategies","abstract":"Over the last ten years, graphics processors have become the de facto accelerator for data-parallel tasks in various branches of high-performance computing, including machine learning and computational sciences. However, with the recent introduction of AMD-manufactured graphics processors to the world's fastest supercomputers, tuning strategies established for previous hardware generations must be re-evaluated. In this study, we evaluate the performance and energy efficiency of stencil computations on modern datacenter graphics processors, and propose a tuning strategy for fusing cache-heavy stencil kernels. The studied cases comprise both synthetic and practical applications, which involve the evaluation of linear and nonlinear stencil functions in one to three dimensions. Our experiments reveal that AMD and Nvidia graphics processors exhibit key differences in both hardware and software, necessitating platform-specific tuning to reach their full computational potential.","sentences":["Over the last ten years, graphics processors have become the de facto accelerator for data-parallel tasks in various branches of high-performance computing, including machine learning and computational sciences.","However, with the recent introduction of AMD-manufactured graphics processors to the world's fastest supercomputers, tuning strategies established for previous hardware generations must be re-evaluated.","In this study, we evaluate the performance and energy efficiency of stencil computations on modern datacenter graphics processors, and propose a tuning strategy for fusing cache-heavy stencil kernels.","The studied cases comprise both synthetic and practical applications, which involve the evaluation of linear and nonlinear stencil functions in one to three dimensions.","Our experiments reveal that AMD and Nvidia graphics processors exhibit key differences in both hardware and software, necessitating platform-specific tuning to reach their full computational potential."],"url":"http://arxiv.org/abs/2406.08923v1","category":"cs.DC"}
{"created":"2024-06-13 06:09:20","title":"Equilibrium Selection for Multi-agent Reinforcement Learning: A Unified Framework","abstract":"While there are numerous works in multi-agent reinforcement learning (MARL), most of them focus on designing algorithms and proving convergence to a Nash equilibrium (NE) or other equilibrium such as coarse correlated equilibrium. However, NEs can be non-unique and their performance varies drastically. Thus, it is important to design algorithms that converge to Nash equilibrium with better rewards or social welfare. In contrast, classical game theory literature has extensively studied equilibrium selection for multi-agent learning in normal-form games, demonstrating that decentralized learning algorithms can asymptotically converge to potential-maximizing or Pareto-optimal NEs. These insights motivate this paper to investigate equilibrium selection in the MARL setting. We focus on the stochastic game model, leveraging classical equilibrium selection results from normal-form games to propose a unified framework for equilibrium selection in stochastic games. The proposed framework is highly modular and can extend various learning rules and their corresponding equilibrium selection results from normal-form games to the stochastic game setting.","sentences":["While there are numerous works in multi-agent reinforcement learning (MARL), most of them focus on designing algorithms and proving convergence to a Nash equilibrium (NE) or other equilibrium such as coarse correlated equilibrium.","However, NEs can be non-unique and their performance varies drastically.","Thus, it is important to design algorithms that converge to Nash equilibrium with better rewards or social welfare.","In contrast, classical game theory literature has extensively studied equilibrium selection for multi-agent learning in normal-form games, demonstrating that decentralized learning algorithms can asymptotically converge to potential-maximizing or Pareto-optimal NEs.","These insights motivate this paper to investigate equilibrium selection in the MARL setting.","We focus on the stochastic game model, leveraging classical equilibrium selection results from normal-form games to propose a unified framework for equilibrium selection in stochastic games.","The proposed framework is highly modular and can extend various learning rules and their corresponding equilibrium selection results from normal-form games to the stochastic game setting."],"url":"http://arxiv.org/abs/2406.08844v1","category":"cs.GT"}
{"created":"2024-06-13 17:41:37","title":"CMC-Bench: Towards a New Paradigm of Visual Signal Compression","abstract":"Ultra-low bitrate image compression is a challenging and demanding topic. With the development of Large Multimodal Models (LMMs), a Cross Modality Compression (CMC) paradigm of Image-Text-Image has emerged. Compared with traditional codecs, this semantic-level compression can reduce image data size to 0.1\\% or even lower, which has strong potential applications. However, CMC has certain defects in consistency with the original image and perceptual quality. To address this problem, we introduce CMC-Bench, a benchmark of the cooperative performance of Image-to-Text (I2T) and Text-to-Image (T2I) models for image compression. This benchmark covers 18,000 and 40,000 images respectively to verify 6 mainstream I2T and 12 T2I models, including 160,000 subjective preference scores annotated by human experts. At ultra-low bitrates, this paper proves that the combination of some I2T and T2I models has surpassed the most advanced visual signal codecs; meanwhile, it highlights where LMMs can be further optimized toward the compression task. We encourage LMM developers to participate in this test to promote the evolution of visual signal codec protocols.","sentences":["Ultra-low bitrate image compression is a challenging and demanding topic.","With the development of Large Multimodal Models (LMMs), a Cross Modality Compression (CMC) paradigm of Image-Text-Image has emerged.","Compared with traditional codecs, this semantic-level compression can reduce image data size to 0.1\\% or even lower, which has strong potential applications.","However, CMC has certain defects in consistency with the original image and perceptual quality.","To address this problem, we introduce CMC-Bench, a benchmark of the cooperative performance of Image-to-Text (I2T) and Text-to-Image (T2I) models for image compression.","This benchmark covers 18,000 and 40,000 images respectively to verify 6 mainstream I2T and 12 T2I models, including 160,000 subjective preference scores annotated by human experts.","At ultra-low bitrates, this paper proves that the combination of some I2T and T2I models has surpassed the most advanced visual signal codecs; meanwhile, it highlights where LMMs can be further optimized toward the compression task.","We encourage LMM developers to participate in this test to promote the evolution of visual signal codec protocols."],"url":"http://arxiv.org/abs/2406.09356v1","category":"cs.CV"}
{"created":"2024-06-13 15:39:14","title":"Near-Field Multiuser Communications based on Sparse Arrays","abstract":"This paper considers near-field multiuser communications based on sparse arrays (SAs). First, for the uniform SAs (USAs), we analyze the beam gains of channel steering vectors, which shows that increasing the antenna spacings can effectively improve the spatial resolution of the antenna arrays to enhance the sum rate of multiuser communications. Then, we investigate nonuniform SAs (NSAs) to mitigate the high multiuser interference from the grating lobes of the USAs. To maximize the sum rate of near-field multiuser communications, we optimize the antenna positions of the NSAs, where a successive convex approximation-based antenna position optimization algorithm is proposed. Moreover, we find that the channels of both the USAs and the NSAs show uniform sparsity in the defined surrogate distance-angle (SD-A) domain. Based on the channel sparsity, an on-grid SD-A-domain orthogonal matching pursuit (SDA-OMP) algorithm is developed to estimate multiuser channels. To further improve the resolution of the SDA-OMP, we also design an off-grid SD-A-domain iterative super-resolution channel estimation algorithm. Simulation results demonstrate the superior performance of the proposed methods.","sentences":["This paper considers near-field multiuser communications based on sparse arrays (SAs).","First, for the uniform SAs (USAs), we analyze the beam gains of channel steering vectors, which shows that increasing the antenna spacings can effectively improve the spatial resolution of the antenna arrays to enhance the sum rate of multiuser communications.","Then, we investigate nonuniform SAs (NSAs) to mitigate the high multiuser interference from the grating lobes of the USAs.","To maximize the sum rate of near-field multiuser communications, we optimize the antenna positions of the NSAs, where a successive convex approximation-based antenna position optimization algorithm is proposed.","Moreover, we find that the channels of both the USAs and the NSAs show uniform sparsity in the defined surrogate distance-angle (SD-A) domain.","Based on the channel sparsity, an on-grid SD-A-domain orthogonal matching pursuit (SDA-OMP) algorithm is developed to estimate multiuser channels.","To further improve the resolution of the SDA-OMP, we also design an off-grid SD-A-domain iterative super-resolution channel estimation algorithm.","Simulation results demonstrate the superior performance of the proposed methods."],"url":"http://arxiv.org/abs/2406.09238v1","category":"cs.IT"}
{"created":"2024-06-13 10:39:11","title":"Bilevel Optimization of the Kantorovich Problem and its Quadratic Regularization Part III: The Finite-Dimensional Case","abstract":"As the title suggests, this is the third paper in a series addressing bilevel optimization problems that are governed by the Kantorovich problem of optimal transport. These tasks can be reformulated as mathematical problems with complementarity constraints in the space of regular Borel measures. Due to the nonsmoothness that is introduced by the complementarity constraints, such problems are often regularized, for instance, using entropic regularization. In this series of papers, however, we apply a quadratic regularization to the Kantorovich problem. By doing so, we enhance its numerical properties while preserving the sparsity structure of the optimal transportation plan as much as possible. While the first two papers in this series focus on the well-posedness of the regularized bilevel problems and the approximation of solutions to the bilevel optimization problem in the infinite-dimensional case, in this paper, we reproduce these results for the finite-dimensional case and present findings that go well beyond the ones of the previous papers and pave the way for the numerical treatment of the bilevel problems.","sentences":["As the title suggests, this is the third paper in a series addressing bilevel optimization problems that are governed by the Kantorovich problem of optimal transport.","These tasks can be reformulated as mathematical problems with complementarity constraints in the space of regular Borel measures.","Due to the nonsmoothness that is introduced by the complementarity constraints, such problems are often regularized, for instance, using entropic regularization.","In this series of papers, however, we apply a quadratic regularization to the Kantorovich problem.","By doing so, we enhance its numerical properties while preserving the sparsity structure of the optimal transportation plan as much as possible.","While the first two papers in this series focus on the well-posedness of the regularized bilevel problems and the approximation of solutions to the bilevel optimization problem in the infinite-dimensional case, in this paper, we reproduce these results for the finite-dimensional case and present findings that go well beyond the ones of the previous papers and pave the way for the numerical treatment of the bilevel problems."],"url":"http://arxiv.org/abs/2406.08992v1","category":"math.OC"}
{"created":"2024-06-13 07:19:00","title":"Testing MOND using the dynamics of nearby stellar streams","abstract":"The stellar halo of the Milky Way is built up, at least in part, from debris from past mergers. Stars from such merger events define substructures in phase-space, for example in the form of streams, which are groups of stars moving on similar trajectories. The nearby Helmi streams discovered more than two decades ago are a well-known example. Using 6D phase-space information from the Gaia space mission, Dodd et al. (2022) have recently reported that the Helmi streams are split into two clumps in angular momentum space. Such substructure can be explained and sustained in time if the dark matter halo of the Milky Way takes a prolate shape in the region probed by the orbits of the stars in the streams. Here, we explore the behaviour of the two clumps identified in the Helmi streams in a Modified Newtonian Dynamics (MOND) framework to test this alternative model of gravity. We perform orbit integrations of Helmi streams member stars in a simplified MOND model of the Milky Way and using the more sophisticated Phantom of RAMSES simulation framework. We find with both approaches that the two Helmi streams clumps do not retain their identity and dissolve after merely 100 Myr. This extremely short timescale would render the detection of two separate clumps as very unlikely in MONDian gravity. The observational constraints provided by the streams, which MOND fails to reproduce in its current formulation, could potentially also be used to test other alternative gravity models.","sentences":["The stellar halo of the Milky Way is built up, at least in part, from debris from past mergers.","Stars from such merger events define substructures in phase-space, for example in the form of streams, which are groups of stars moving on similar trajectories.","The nearby Helmi streams discovered more than two decades ago are a well-known example.","Using 6D phase-space information from the Gaia space mission, Dodd et al.","(2022) have recently reported that the Helmi streams are split into two clumps in angular momentum space.","Such substructure can be explained and sustained in time if the dark matter halo of the Milky Way takes a prolate shape in the region probed by the orbits of the stars in the streams.","Here, we explore the behaviour of the two clumps identified in the Helmi streams in a Modified Newtonian Dynamics (MOND) framework to test this alternative model of gravity.","We perform orbit integrations of Helmi streams member stars in a simplified MOND model of the Milky Way and using the more sophisticated Phantom of RAMSES simulation framework.","We find with both approaches that the two Helmi streams clumps do not retain their identity and dissolve after merely 100 Myr.","This extremely short timescale would render the detection of two separate clumps as very unlikely in MONDian gravity.","The observational constraints provided by the streams, which MOND fails to reproduce in its current formulation, could potentially also be used to test other alternative gravity models."],"url":"http://arxiv.org/abs/2406.08872v1","category":"astro-ph.GA"}
{"created":"2024-06-13 07:15:21","title":"MEGA: Maximum-Entropy Genetic Algorithm for Router Nodes Placement in Wireless Mesh Networks","abstract":"Over the past decade, Wireless Mesh Networks (WMNs) have seen significant advancements due to their simple deployment, cost-effectiveness, ease of implementation and reliable service coverage. However, despite these advantages, the placement of nodes in WMNs presents a critical challenge that significantly impacts their performance. This issue is recognized as an NP-hard problem, underscoring the necessity of development optimization algorithms, such as heuristic and metaheuristic approaches. This motivates us to develop the Maximum Entropy Genetic Algorithm (MEGA) to address the issue of mesh router node placement in WMNs. To assess the proposed method, we conducted experiments across various scenarios with different settings, focusing on key metrics such as network connectivity and user coverage. The simulation results show a comparison of MEGA with other prominent algorithms, such as the Coyote Optimization Algorithm (COA), Firefly Algorithm (FA), Genetic Algorithm (GA), and Particle Swarm Optimization (PSO), revealing MEGA's effectiveness and usability in determining optimal locations for mesh routers.","sentences":["Over the past decade, Wireless Mesh Networks (WMNs) have seen significant advancements due to their simple deployment, cost-effectiveness, ease of implementation and reliable service coverage.","However, despite these advantages, the placement of nodes in WMNs presents a critical challenge that significantly impacts their performance.","This issue is recognized as an NP-hard problem, underscoring the necessity of development optimization algorithms, such as heuristic and metaheuristic approaches.","This motivates us to develop the Maximum Entropy Genetic Algorithm (MEGA) to address the issue of mesh router node placement in WMNs.","To assess the proposed method, we conducted experiments across various scenarios with different settings, focusing on key metrics such as network connectivity and user coverage.","The simulation results show a comparison of MEGA with other prominent algorithms, such as the Coyote Optimization Algorithm (COA), Firefly Algorithm (FA), Genetic Algorithm (GA), and Particle Swarm Optimization (PSO), revealing MEGA's effectiveness and usability in determining optimal locations for mesh routers."],"url":"http://arxiv.org/abs/2406.08870v1","category":"cs.NI"}
{"created":"2024-06-13 06:27:13","title":"COVE: Unleashing the Diffusion Feature Correspondence for Consistent Video Editing","abstract":"Video editing is an emerging task, in which most current methods adopt the pre-trained text-to-image (T2I) diffusion model to edit the source video in a zero-shot manner. Despite extensive efforts, maintaining the temporal consistency of edited videos remains challenging due to the lack of temporal constraints in the regular T2I diffusion model. To address this issue, we propose COrrespondence-guided Video Editing (COVE), leveraging the inherent diffusion feature correspondence to achieve high-quality and consistent video editing. Specifically, we propose an efficient sliding-window-based strategy to calculate the similarity among tokens in the diffusion features of source videos, identifying the tokens with high correspondence across frames. During the inversion and denoising process, we sample the tokens in noisy latent based on the correspondence and then perform self-attention within them. To save GPU memory usage and accelerate the editing process, we further introduce the temporal-dimensional token merging strategy, which can effectively reduce redundancy. COVE can be seamlessly integrated into the pre-trained T2I diffusion model without the need for extra training or optimization. Extensive experiment results demonstrate that COVE achieves the start-of-the-art performance in various video editing scenarios, outperforming existing methods both quantitatively and qualitatively. The code will be release at https://github.com/wangjiangshan0725/COVE","sentences":["Video editing is an emerging task, in which most current methods adopt the pre-trained text-to-image (T2I) diffusion model to edit the source video in a zero-shot manner.","Despite extensive efforts, maintaining the temporal consistency of edited videos remains challenging due to the lack of temporal constraints in the regular T2I diffusion model.","To address this issue, we propose COrrespondence-guided Video Editing (COVE), leveraging the inherent diffusion feature correspondence to achieve high-quality and consistent video editing.","Specifically, we propose an efficient sliding-window-based strategy to calculate the similarity among tokens in the diffusion features of source videos, identifying the tokens with high correspondence across frames.","During the inversion and denoising process, we sample the tokens in noisy latent based on the correspondence and then perform self-attention within them.","To save GPU memory usage and accelerate the editing process, we further introduce the temporal-dimensional token merging strategy, which can effectively reduce redundancy.","COVE can be seamlessly integrated into the pre-trained T2I diffusion model without the need for extra training or optimization.","Extensive experiment results demonstrate that COVE achieves the start-of-the-art performance in various video editing scenarios, outperforming existing methods both quantitatively and qualitatively.","The code will be release at https://github.com/wangjiangshan0725/COVE"],"url":"http://arxiv.org/abs/2406.08850v1","category":"cs.CV"}
{"created":"2024-06-13 06:08:04","title":"ContraSolver: Self-Alignment of Language Models by Resolving Internal Preference Contradictions","abstract":"While substantial advancements have been made in developing large language models (LLMs), achieving control over their behavior can be difficult. Direct preference optimization (DPO) assumes the existence of a latent reward function to evaluate the responses of LLMs. This assumption indicates a strict preference ordering of different responses to the same input. However, there always exist contradictions of preference in LLMs according to our experimental observations. In this paper, we construct a graph structure of the preference relationship among different responses with self-annotation to find contradictions in the preference order. We propose ContraSolver, an algorithm that traverses all edges on the preference graph to identify those that might cause contradictions. ContraSolver initializes the graph with a maximum spanning tree and identifies contradictory edges, prioritizing the resolution of low-confidence preferences while preserving high-confidence ones. Experimental results on four different generation tasks show that the performance of different LLMs can be largely improved through our completely unsupervised self-alignment. Furthermore, by analyzing the preference graphs of LLMs with and without self-alignment by ContraSolver, we quantify the reduction in contradictions, suggesting that resolving preference contradictions is crucial for achieving better alignment performance.","sentences":["While substantial advancements have been made in developing large language models (LLMs), achieving control over their behavior can be difficult.","Direct preference optimization (DPO) assumes the existence of a latent reward function to evaluate the responses of LLMs.","This assumption indicates a strict preference ordering of different responses to the same input.","However, there always exist contradictions of preference in LLMs according to our experimental observations.","In this paper, we construct a graph structure of the preference relationship among different responses with self-annotation to find contradictions in the preference order.","We propose ContraSolver, an algorithm that traverses all edges on the preference graph to identify those that might cause contradictions.","ContraSolver initializes the graph with a maximum spanning tree and identifies contradictory edges, prioritizing the resolution of low-confidence preferences while preserving high-confidence ones.","Experimental results on four different generation tasks show that the performance of different LLMs can be largely improved through our completely unsupervised self-alignment.","Furthermore, by analyzing the preference graphs of LLMs with and without self-alignment by ContraSolver, we quantify the reduction in contradictions, suggesting that resolving preference contradictions is crucial for achieving better alignment performance."],"url":"http://arxiv.org/abs/2406.08842v1","category":"cs.CL"}
{"created":"2024-06-13 02:48:25","title":"Cavitated Ag Paste for Cost-Effective Solar Cell","abstract":"This paper reports on the investigation of cavitated silver paste produced by cavitation technology as a cost-effective alternative to traditional three-roll milling (TRM). Cavitation, utilizing high-frequency sound waves, enhances metal paste dispersion, reduces oxidation, extends shelf life, and minimizes waste. Passivated Emitter and Rear Cell (PERC) solar cells made with cavitated silver paste achieved a 21% energy conversion efficiency, slightly lower than the 22% efficiency of conventional paste. Cavitated paste produced finer gridlines, reducing silver usage and costs but increasing contact resistance, leading to a lower fill factor. Despite this, cavitation technology shows promise for more efficient and cost-effective solar cell production. Further research is needed to optimize efficiency and resistance, highlighting the potential for cavitation technology in commercial applications.","sentences":["This paper reports on the investigation of cavitated silver paste produced by cavitation technology as a cost-effective alternative to traditional three-roll milling (TRM).","Cavitation, utilizing high-frequency sound waves, enhances metal paste dispersion, reduces oxidation, extends shelf life, and minimizes waste.","Passivated Emitter and Rear Cell (PERC) solar cells made with cavitated silver paste achieved a 21% energy conversion efficiency, slightly lower than the 22% efficiency of conventional paste.","Cavitated paste produced finer gridlines, reducing silver usage and costs but increasing contact resistance, leading to a lower fill factor.","Despite this, cavitation technology shows promise for more efficient and cost-effective solar cell production.","Further research is needed to optimize efficiency and resistance, highlighting the potential for cavitation technology in commercial applications."],"url":"http://arxiv.org/abs/2406.08763v1","category":"physics.app-ph"}
{"created":"2024-06-13 02:31:36","title":"Optimizing Large Model Training through Overlapped Activation Recomputation","abstract":"Large model training has been using recomputation to alleviate the memory pressure and pipelining to exploit the parallelism of data, tensor, and devices. The existing recomputation approaches may incur up to 40% overhead when training real-world models, e.g., the GPT model with 22B parameters. This is because they are executed on demand in the critical training path. In this paper, we design a new recomputation framework, Lynx, to reduce the overhead by overlapping the recomputation with communication occurring in training pipelines. It consists of an optimal scheduling algorithm (OPT) and a heuristic-based scheduling algorithm (HEU). OPT achieves a global optimum but suffers from a long search time. HEU was designed based on our observation that there are identical structures in large DNN models so that we can apply the same scheduling policy to all identical structures. HEU achieves a local optimum but reduces the search time by 99% compared to OPT. Our comprehensive evaluation using GPT models with 1.3B-20B parameters shows that both OPT and HEU outperform the state-of-the-art recomputation approaches (e.g., Megatron-LM and Checkmake) by 1.02-1.53x. HEU achieves a similar performance as OPT with a search time of 0.16s on average.","sentences":["Large model training has been using recomputation to alleviate the memory pressure and pipelining to exploit the parallelism of data, tensor, and devices.","The existing recomputation approaches may incur up to 40% overhead when training real-world models, e.g., the GPT model with 22B parameters.","This is because they are executed on demand in the critical training path.","In this paper, we design a new recomputation framework, Lynx, to reduce the overhead by overlapping the recomputation with communication occurring in training pipelines.","It consists of an optimal scheduling algorithm (OPT) and a heuristic-based scheduling algorithm (HEU).","OPT achieves a global optimum but suffers from a long search time.","HEU was designed based on our observation that there are identical structures in large DNN models so that we can apply the same scheduling policy to all identical structures.","HEU achieves a local optimum but reduces the search time by 99% compared to OPT.","Our comprehensive evaluation using GPT models with 1.3B-20B parameters shows that both OPT and HEU outperform the state-of-the-art recomputation approaches (e.g., Megatron-LM and Checkmake) by 1.02-1.53x.","HEU achieves a similar performance as OPT with a search time of 0.16s on average."],"url":"http://arxiv.org/abs/2406.08756v1","category":"cs.DC"}
{"created":"2024-06-13 00:27:51","title":"A Novel Diamond-like Carbon based photocathode for PICOSEC Micromegas detector","abstract":"The PICOSEC Micromegas (MM) is a Cherenkov photodetector based on the MM detector operating in a two-stage amplification mode. Prototypes equipped with a cesium iodide (CsI) photocathode have shown promising time resolutions as low as 24 picoseconds (ps) for Minimum Ionizing Particles. However, due to the high hygroscopicity and susceptibility to ion bombardment of the CsI photocathode, alternative photocathode materials such as pure aluminum, pure chromium, as well as Diamond-like Carbon (DLC) have been studied to improve the robustness of detector. Among these, DLC has yielded the most favorable results. A batch of DLC photocathodes with different thicknesses were produced and evaluated by using ultraviolet light and particle beam. The results of both quantum efficiency measurement and beam test indicate that the optimized thickness of the DLC photocathode is approximately 3 nm. The PICOSEC MM prototype equipped with a 3 nm DLC photocathode has reached a time resolution of around 42 ps with a detection efficiency of 97% for 150 GeV/c muons. Furthermore, the DLC photocathodes have shown good resistance to ion bombardment in the aging test compared to CsI photocathode. These results confirm the great potential of DLC as a photocathode for the PICOSEC MM detector.","sentences":["The PICOSEC Micromegas (MM) is a Cherenkov photodetector based on the MM detector operating in a two-stage amplification mode.","Prototypes equipped with a cesium iodide (CsI) photocathode have shown promising time resolutions as low as 24 picoseconds (ps) for Minimum Ionizing Particles.","However, due to the high hygroscopicity and susceptibility to ion bombardment of the CsI photocathode, alternative photocathode materials such as pure aluminum, pure chromium, as well as Diamond-like Carbon (DLC) have been studied to improve the robustness of detector.","Among these, DLC has yielded the most favorable results.","A batch of DLC photocathodes with different thicknesses were produced and evaluated by using ultraviolet light and particle beam.","The results of both quantum efficiency measurement and beam test indicate that the optimized thickness of the DLC photocathode is approximately 3 nm.","The PICOSEC MM prototype equipped with a 3 nm DLC photocathode has reached a time resolution of around 42 ps with a detection efficiency of 97% for 150 GeV/c muons.","Furthermore, the DLC photocathodes have shown good resistance to ion bombardment in the aging test compared to CsI photocathode.","These results confirm the great potential of DLC as a photocathode for the PICOSEC MM detector."],"url":"http://arxiv.org/abs/2406.08712v1","category":"physics.ins-det"}
{"created":"2024-06-13 00:25:36","title":"Matching with Nested and Bundled Pandora Boxes","abstract":"We consider max-weighted matching with costs for learning the weights, modeled as a \"Pandora's Box\" on each endpoint of an edge. Each vertex has an initially-unknown value for being matched to a neighbor, and an algorithm must pay some cost to observe this value. The goal is to maximize the total matched value minus costs. Our model is inspired by two-sided settings, such as matching employees to employers. Importantly for such settings, we allow for negative values which cause existing approaches to fail.   We first prove upper bounds for algorithms in two natural classes. Any algorithm that \"bundles\" the two Pandora boxes incident to an edge is an $o(1)$-approximation. Likewise, any \"vertex-based\" algorithm, which uses properties of the separate Pandora's boxes but does not consider the interaction of their value distributions, is an $o(1)$-approximation. Instead, we utilize Pandora's Nested-Box Problem, i.e. multiple stages of inspection. We give a self-contained, fully constructive optimal solution to the nested-boxes problem, which may have structural observations of interest compared to prior work. By interpreting each edge as a nested box, we leverage this solution to obtain a constant-factor approximation algorithm. Finally, we show any ``edge-based'' algorithm, which considers the interactions of values along an edge but not with the rest of the graph, is also an $o(1)$-approximation.","sentences":["We consider max-weighted matching with costs for learning the weights, modeled as a \"Pandora's Box\" on each endpoint of an edge.","Each vertex has an initially-unknown value for being matched to a neighbor, and an algorithm must pay some cost to observe this value.","The goal is to maximize the total matched value minus costs.","Our model is inspired by two-sided settings, such as matching employees to employers.","Importantly for such settings, we allow for negative values which cause existing approaches to fail.   ","We first prove upper bounds for algorithms in two natural classes.","Any algorithm that \"bundles\" the two Pandora boxes incident to an edge is an $o(1)$-approximation.","Likewise, any \"vertex-based\" algorithm, which uses properties of the separate Pandora's boxes but does not consider the interaction of their value distributions, is an $o(1)$-approximation.","Instead, we utilize Pandora's Nested-Box Problem, i.e. multiple stages of inspection.","We give a self-contained, fully constructive optimal solution to the nested-boxes problem, which may have structural observations of interest compared to prior work.","By interpreting each edge as a nested box, we leverage this solution to obtain a constant-factor approximation algorithm.","Finally, we show any ``edge-based'' algorithm, which considers the interactions of values along an edge but not with the rest of the graph, is also an $o(1)$-approximation."],"url":"http://arxiv.org/abs/2406.08711v1","category":"cs.DS"}
{"created":"2024-06-13 00:04:15","title":"When LLM Meets DRL: Advancing Jailbreaking Efficiency via DRL-guided Search","abstract":"Recent studies developed jailbreaking attacks, which construct jailbreaking prompts to ``fool'' LLMs into responding to harmful questions. Early-stage jailbreaking attacks require access to model internals or significant human efforts. More advanced attacks utilize genetic algorithms for automatic and black-box attacks. However, the random nature of genetic algorithms significantly limits the effectiveness of these attacks. In this paper, we propose RLbreaker, a black-box jailbreaking attack driven by deep reinforcement learning (DRL). We model jailbreaking as a search problem and design an RL agent to guide the search, which is more effective and has less randomness than stochastic search, such as genetic algorithms. Specifically, we design a customized DRL system for the jailbreaking problem, including a novel reward function and a customized proximal policy optimization (PPO) algorithm. Through extensive experiments, we demonstrate that RLbreaker is much more effective than existing jailbreaking attacks against six state-of-the-art (SOTA) LLMs. We also show that RLbreaker is robust against three SOTA defenses and its trained agents can transfer across different LLMs. We further validate the key design choices of RLbreaker via a comprehensive ablation study.","sentences":["Recent studies developed jailbreaking attacks, which construct jailbreaking prompts to ``fool'' LLMs into responding to harmful questions.","Early-stage jailbreaking attacks require access to model internals or significant human efforts.","More advanced attacks utilize genetic algorithms for automatic and black-box attacks.","However, the random nature of genetic algorithms significantly limits the effectiveness of these attacks.","In this paper, we propose RLbreaker, a black-box jailbreaking attack driven by deep reinforcement learning (DRL).","We model jailbreaking as a search problem and design an RL agent to guide the search, which is more effective and has less randomness than stochastic search, such as genetic algorithms.","Specifically, we design a customized DRL system for the jailbreaking problem, including a novel reward function and a customized proximal policy optimization (PPO) algorithm.","Through extensive experiments, we demonstrate that RLbreaker is much more effective than existing jailbreaking attacks against six state-of-the-art (SOTA) LLMs.","We also show that RLbreaker is robust against three SOTA defenses and its trained agents can transfer across different LLMs.","We further validate the key design choices of RLbreaker via a comprehensive ablation study."],"url":"http://arxiv.org/abs/2406.08705v1","category":"cs.CR"}
{"created":"2024-06-12 21:13:52","title":"Optimized Dual-Volumes for Tetrahedral Meshes","abstract":"Constructing well-behaved Laplacian and mass matrices is essential for tetrahedral mesh processing. Unfortunately, the \\emph{de facto} standard linear finite elements exhibit bias on tetrahedralized regular grids, motivating the development of finite-volume methods. In this paper, we place existing methods into a common construction, showing how their differences amount to the choice of simplex centers. These choices lead to satisfaction or breakdown of important properties: continuity with respect to vertex positions, positive semi-definiteness of the implied Dirichlet energy, positivity of the mass matrix, and unbiased-ness on regular grids. Based on this analysis, we propose a new method for constructing dual-volumes which explicitly satisfy all of these properties via convex optimization.","sentences":["Constructing well-behaved Laplacian and mass matrices is essential for tetrahedral mesh processing.","Unfortunately, the \\emph{de facto} standard linear finite elements exhibit bias on tetrahedralized regular grids, motivating the development of finite-volume methods.","In this paper, we place existing methods into a common construction, showing how their differences amount to the choice of simplex centers.","These choices lead to satisfaction or breakdown of important properties: continuity with respect to vertex positions, positive semi-definiteness of the implied Dirichlet energy, positivity of the mass matrix, and unbiased-ness on regular grids.","Based on this analysis, we propose a new method for constructing dual-volumes which explicitly satisfy all of these properties via convex optimization."],"url":"http://arxiv.org/abs/2406.08647v1","category":"cs.GR"}
{"created":"2024-06-12 20:53:07","title":"Conditional Similarity Triplets Enable Covariate-Informed Representations of Single-Cell Data","abstract":"Single-cell technologies enable comprehensive profiling of diverse immune cell-types through the measurement of multiple genes or proteins per cell. In order to translate data from immune profiling assays into powerful diagnostics, machine learning approaches are used to compute per-sample immunological summaries, or featurizations that can be used as inputs to models for outcomes of interest. Current supervised learning approaches for computing per-sample representations are optimized based only on the outcome variable to be predicted and do not take into account clinically-relevant covariates that are likely to also be measured. Here we expand the optimization problem to also take into account such additional patient covariates to directly inform the learned per-sample representations. To do this, we introduce CytoCoSet, a set-based encoding method, which formulates a loss function with an additional triplet term penalizing samples with similar covariates from having disparate embedding results in per-sample representations. Overall, incorporating clinical covariates leads to improved prediction of clinical phenotypes.","sentences":["Single-cell technologies enable comprehensive profiling of diverse immune cell-types through the measurement of multiple genes or proteins per cell.","In order to translate data from immune profiling assays into powerful diagnostics, machine learning approaches are used to compute per-sample immunological summaries, or featurizations that can be used as inputs to models for outcomes of interest.","Current supervised learning approaches for computing per-sample representations are optimized based only on the outcome variable to be predicted and do not take into account clinically-relevant covariates that are likely to also be measured.","Here we expand the optimization problem to also take into account such additional patient covariates to directly inform the learned per-sample representations.","To do this, we introduce CytoCoSet, a set-based encoding method, which formulates a loss function with an additional triplet term penalizing samples with similar covariates from having disparate embedding results in per-sample representations.","Overall, incorporating clinical covariates leads to improved prediction of clinical phenotypes."],"url":"http://arxiv.org/abs/2406.08638v1","category":"cs.LG"}
{"created":"2024-06-12 19:03:44","title":"Optimizing the auxetic behavior of anisotropic laminates","abstract":"Anisotropic laminates with a negative Poisson's ratio for at least some directions are called auxetic. In this paper, we consider the conditions for optimizing the auxeticity of an orthotropic laminate, namely: for a laminate composed by a given material, (i) how to obtain the lowest, i.e. the highest negative, Poisson's ratio and (ii) how to maximize the auxetic zone, i.e. the set of directions where the Poisson's ratio is negative. It is shown that in both the cases the optimal solution is found on the boundary of the feasible domain and in particular that it can be obtained using angle-ply sequences of identical layers. The polar method with dimensionless moduli is employed for representing the anisotropic behavior of the laminate, which allows, on the one hand, to reduce the dimensionality of the problem and, on the other hand, to have an effective mathematical representation of anisotropy by dimensionless invariants.","sentences":["Anisotropic laminates with a negative Poisson's ratio for at least some directions are called auxetic.","In this paper, we consider the conditions for optimizing the auxeticity of an orthotropic laminate, namely: for a laminate composed by a given material, (i) how to obtain the lowest, i.e. the highest negative, Poisson's ratio and (ii) how to maximize the auxetic zone, i.e. the set of directions where the Poisson's ratio is negative.","It is shown that in both the cases the optimal solution is found on the boundary of the feasible domain and in particular that it can be obtained using angle-ply sequences of identical layers.","The polar method with dimensionless moduli is employed for representing the anisotropic behavior of the laminate, which allows, on the one hand, to reduce the dimensionality of the problem and, on the other hand, to have an effective mathematical representation of anisotropy by dimensionless invariants."],"url":"http://arxiv.org/abs/2406.08597v1","category":"math-ph"}
{"created":"2024-06-12 19:00:27","title":"Approximating Maximum Matching Requires Almost Quadratic Time","abstract":"We study algorithms for estimating the size of maximum matching. This problem has been subject to extensive research. For $n$-vertex graphs, Bhattacharya, Kiss, and Saranurak [FOCS'23] (BKS) showed that an estimate that is within $\\varepsilon n$ of the optimal solution can be achieved in $n^{2-\\Omega_\\varepsilon(1)}$ time, where $n$ is the number of vertices. While this is subquadratic in $n$ for any fixed $\\varepsilon > 0$, it gets closer and closer to the trivial $\\Theta(n^2)$ time algorithm that reads the entire input as $\\varepsilon$ is made smaller and smaller.   In this work, we close this gap and show that the algorithm of BKS is close to optimal. In particular, we prove that for any fixed $\\delta > 0$, there is another fixed $\\varepsilon = \\varepsilon(\\delta) > 0$ such that estimating the size of maximum matching within an additive error of $\\varepsilon n$ requires $\\Omega(n^{2-\\delta})$ time in the adjacency list model.","sentences":["We study algorithms for estimating the size of maximum matching.","This problem has been subject to extensive research.","For $n$-vertex graphs, Bhattacharya, Kiss, and Saranurak","[FOCS'23] (BKS) showed that an estimate that is within $\\varepsilon n$ of the optimal solution can be achieved in $n^{2-\\Omega_\\varepsilon(1)}$ time, where $n$ is the number of vertices.","While this is subquadratic in $n$ for any fixed $\\varepsilon > 0$, it gets closer and closer to the trivial $\\Theta(n^2)$ time algorithm that reads the entire input as $\\varepsilon$ is made smaller and smaller.   ","In this work, we close this gap and show that the algorithm of BKS is close to optimal.","In particular, we prove that for any fixed $\\delta > 0$, there is another fixed $\\varepsilon = \\varepsilon(\\delta) > 0$ such that estimating the size of maximum matching within an additive error of $\\varepsilon n$ requires $\\Omega(n^{2-\\delta})$ time in the adjacency list model."],"url":"http://arxiv.org/abs/2406.08595v1","category":"cs.DS"}
{"created":"2024-06-12 18:59:51","title":"Limiting behaviour of Branching Processes and Online Social Networks","abstract":"The literature considers multi-type Markov branching processes (BPs), where the offspring distribution depends only on the living (current) population. We analyse the total-current population-dependent BPs where the offspring distribution can also depend on the total (dead and living) population. Such a generalization is inspired by the need to accurately model content propagation over online social networks (OSNs). The key question investigated is the time-asymptotic proportion of the populations, which translates to the proportional visibility of the posts on the OSN. We provide the answer using a stochastic approximation (SA) technique, which has not been used in the existing BP literature. The analysis is derived using a non-trivial autonomous measurable ODE. Interestingly, we prove the possibility of a new limiting behaviour for the stochastic trajectory, named as hovering around. Such a result is not just new to the theory of BPs but also to the SA based literature.   Later, we explore three new variants of BPs: (i) any living individual of a population can attack and acquire the living individuals of the other population, in addition to producing its offspring; (ii) the individuals can die due to abnormal circumstances, and not just at the completion of their lifetimes; (iii) the expected number of offspring decreases as the total-population increases, leading to the saturation of the total-population.   Such variants aid in analysing unexplored aspects of content propagation over OSNs: (i) competition in advertisement posts for similar products; (ii) controlling fake-post propagation, while not affecting the sharing of real-post; (iii) impact of re-forwarding the posts. We also designed and analysed a participation (mean-field) game where the OSN lures the users with a reward-based scheme to provide their opinion about the actuality of the post (fake or real).","sentences":["The literature considers multi-type Markov branching processes (BPs), where the offspring distribution depends only on the living (current) population.","We analyse the total-current population-dependent BPs where the offspring distribution can also depend on the total (dead and living) population.","Such a generalization is inspired by the need to accurately model content propagation over online social networks (OSNs).","The key question investigated is the time-asymptotic proportion of the populations, which translates to the proportional visibility of the posts on the OSN.","We provide the answer using a stochastic approximation (SA) technique, which has not been used in the existing BP literature.","The analysis is derived using a non-trivial autonomous measurable ODE.","Interestingly, we prove the possibility of a new limiting behaviour for the stochastic trajectory, named as hovering around.","Such a result is not just new to the theory of BPs but also to the SA based literature.   ","Later, we explore three new variants of BPs: (i) any living individual of a population can attack and acquire the living individuals of the other population, in addition to producing its offspring; (ii) the individuals can die due to abnormal circumstances, and not just at the completion of their lifetimes; (iii) the expected number of offspring decreases as the total-population increases, leading to the saturation of the total-population.   ","Such variants aid in analysing unexplored aspects of content propagation over OSNs: (i) competition in advertisement posts for similar products; (ii) controlling fake-post propagation, while not affecting the sharing of real-post; (iii) impact of re-forwarding the posts.","We also designed and analysed a participation (mean-field) game where the OSN lures the users with a reward-based scheme to provide their opinion about the actuality of the post (fake or real)."],"url":"http://arxiv.org/abs/2406.08594v1","category":"math.PR"}
{"created":"2024-06-12 18:49:08","title":"FireBench: A High-fidelity Ensemble Simulation Framework for Exploring Wildfire Behavior and Data-driven Modeling","abstract":"Background. Wildfire research uses ensemble methods to analyze fire behaviors and assess uncertainties. Nonetheless, current research methods are either confined to simple models or complex simulations with limits. Modern computing tools could allow for efficient, high-fidelity ensemble simulations. Aims. This study proposes a high-fidelity ensemble wildfire simulation framework for studying wildfire behavior, ML tasks, fire-risk assessment, and uncertainty analysis. Methods. In this research, we present a simulation framework that integrates the Swirl-Fire large-eddy simulation tool for wildfire predictions with the Vizier optimization platform for automated run-time management of ensemble simulations and large-scale batch processing. All simulations are executed on tensor-processing units to enhance computational efficiency. Key results. A dataset of 117 simulations is created, each with 1.35 billion mesh points. The simulations are compared to existing experimental data and show good agreement in terms of fire rate of spread. Computations are done for fire acceleration, mean rate of spread, and fireline intensity. Conclusions. Strong coupling between these 2 parameters are observed for the fire spread and intermittency. A critical Froude number that delineates fires from plume-driven to convection-driven is identified and confirmed with literature observations. Implications. The ensemble simulation framework is efficient in facilitating parametric wildfire studies.","sentences":["Background.","Wildfire research uses ensemble methods to analyze fire behaviors and assess uncertainties.","Nonetheless, current research methods are either confined to simple models or complex simulations with limits.","Modern computing tools could allow for efficient, high-fidelity ensemble simulations.","Aims.","This study proposes a high-fidelity ensemble wildfire simulation framework for studying wildfire behavior, ML tasks, fire-risk assessment, and uncertainty analysis.","Methods.","In this research, we present a simulation framework that integrates the Swirl-Fire large-eddy simulation tool for wildfire predictions with the Vizier optimization platform for automated run-time management of ensemble simulations and large-scale batch processing.","All simulations are executed on tensor-processing units to enhance computational efficiency.","Key results.","A dataset of 117 simulations is created, each with 1.35 billion mesh points.","The simulations are compared to existing experimental data and show good agreement in terms of fire rate of spread.","Computations are done for fire acceleration, mean rate of spread, and fireline intensity.","Conclusions.","Strong coupling between these 2 parameters are observed for the fire spread and intermittency.","A critical Froude number that delineates fires from plume-driven to convection-driven is identified and confirmed with literature observations.","Implications.","The ensemble simulation framework is efficient in facilitating parametric wildfire studies."],"url":"http://arxiv.org/abs/2406.08589v1","category":"physics.comp-ph"}
{"created":"2024-06-12 18:46:55","title":"The Monge-Kantorovich problem on Wasserstein space","abstract":"We consider the Monge-Kantorovich problem between two random measuress. More precisely, given probability measures $\\mathbb{P}_1,\\mathbb{P}_2\\in\\mathcal{P}(\\mathcal{P}(M))$ on the space $\\mathcal{P}(M)$ of probability measures on a smooth compact manifold, we study the optimal transport problem between $\\mathbb{P}_1$ and $\\mathbb{P}_2 $ where the cost function is given by the squared Wasserstein distance $W_2^2(\\mu,\\nu)$ between $\\mu,\\nu \\in \\mathcal{P}(M)$. Under appropriate assumptions on $\\mathbb{P}_1$, we prove that there exists a unique optimal plan and that it takes the form of an optimal map. An extension of this result to cost functions of the form $h(W_2(\\mu,\\nu))$, for strictly convex and strictly increasing functions $h$, is also established. The proofs rely heavily on a recent result of Schiavo \\cite{schiavo2020rademacher}, which establishes a version of Rademacher's theorem on Wasserstein space.","sentences":["We consider the Monge-Kantorovich problem between two random measuress.","More precisely, given probability measures $\\mathbb{P}_1,\\mathbb{P}_2\\in\\mathcal{P}(\\mathcal{P}(M))$ on the space $\\mathcal{P}(M)$ of probability measures on a smooth compact manifold, we study the optimal transport problem between $\\mathbb{P}_1$ and $\\mathbb{P}_2 $ where the cost function is given by the squared Wasserstein distance $W_2^2(\\mu,\\nu)$ between $\\mu,\\nu \\in \\mathcal{P}(M)$. Under appropriate assumptions on $\\mathbb{P}_1$, we prove that there exists a unique optimal plan and that it takes the form of an optimal map.","An extension of this result to cost functions of the form $h(W_2(\\mu,\\nu))$, for strictly convex and strictly increasing functions $h$, is also established.","The proofs rely heavily on a recent result of Schiavo \\cite{schiavo2020rademacher}, which establishes a version of Rademacher's theorem on Wasserstein space."],"url":"http://arxiv.org/abs/2406.08585v1","category":"math.PR"}
{"created":"2024-06-12 18:00:06","title":"Realization of topological Thouless pumping in a synthetic Rydberg dimension","abstract":"The simulation of synthetic dimensions by manipulating internal states of atoms and molecules has opened the door to investigate regimes outside those of more traditional quantum many-body platforms. Highly excited Rydberg states of atoms are a particularly promising platform to engineer Hamiltonians in such synthetic dimensions due to their large number of addressable states and the readily available technologies for manipulating their couplings and for detecting them. In this letter, we demonstrate the realization of topological quantum pumping in synthetic dimensions by engineering a one-dimensional Rice-Mele chain from the Rydberg states of cesium atoms, and manipulating their couplings in a time-dependent fashion through radio-frequency fields. We implement Thouless protocols for topological pumping and investigate the efficiency for pumping an effective quantum particle as a function of the period of pumping and other parameters while allowing for rates of change that are not necessarily adiabatic. We demonstrate that optimal pumping efficiencies of up to 90% can be achieved when the pump is operated in the topological Thouless regime, even when the pumping is accompanied by the wave-packet spread that arises from the energy dispersion of the particle along the synthetic dimension.","sentences":["The simulation of synthetic dimensions by manipulating internal states of atoms and molecules has opened the door to investigate regimes outside those of more traditional quantum many-body platforms.","Highly excited Rydberg states of atoms are a particularly promising platform to engineer Hamiltonians in such synthetic dimensions due to their large number of addressable states and the readily available technologies for manipulating their couplings and for detecting them.","In this letter, we demonstrate the realization of topological quantum pumping in synthetic dimensions by engineering a one-dimensional Rice-Mele chain from the Rydberg states of cesium atoms, and manipulating their couplings in a time-dependent fashion through radio-frequency fields.","We implement Thouless protocols for topological pumping and investigate the efficiency for pumping an effective quantum particle as a function of the period of pumping and other parameters while allowing for rates of change that are not necessarily adiabatic.","We demonstrate that optimal pumping efficiencies of up to 90% can be achieved when the pump is operated in the topological Thouless regime, even when the pumping is accompanied by the wave-packet spread that arises from the energy dispersion of the particle along the synthetic dimension."],"url":"http://arxiv.org/abs/2406.08551v1","category":"quant-ph"}
{"created":"2024-06-12 18:00:01","title":"A practical framework for analyzing high-dimensional QKD setups","abstract":"High-dimensional (HD) entanglement promises both enhanced key rates and overcoming obstacles faced by modern-day quantum communication. However, modern convex optimization-based security arguments are limited by computational constraints; thus, accessible dimensions are far exceeded by progress in HD photonics, bringing forth a need for efficient methods to compute key rates for large encoding dimensions. In response to this problem, we present a flexible analytic framework facilitated by the dual of a semi-definite program and diagonalizing operators inspired by entanglement-witness theory, enabling the efficient computation of key rates in high-dimensional systems. To facilitate the latter, we show how matrix completion techniques can be incorporated to effectively yield improved, computable bounds on the key rate in paradigmatic high-dimensional systems of time- or frequency-bin entangled photons and beyond.","sentences":["High-dimensional (HD) entanglement promises both enhanced key rates and overcoming obstacles faced by modern-day quantum communication.","However, modern convex optimization-based security arguments are limited by computational constraints; thus, accessible dimensions are far exceeded by progress in HD photonics, bringing forth a need for efficient methods to compute key rates for large encoding dimensions.","In response to this problem, we present a flexible analytic framework facilitated by the dual of a semi-definite program and diagonalizing operators inspired by entanglement-witness theory, enabling the efficient computation of key rates in high-dimensional systems.","To facilitate the latter, we show how matrix completion techniques can be incorporated to effectively yield improved, computable bounds on the key rate in paradigmatic high-dimensional systems of time- or frequency-bin entangled photons and beyond."],"url":"http://arxiv.org/abs/2406.08544v1","category":"quant-ph"}
{"created":"2024-06-13 17:49:30","title":"Is every knot isotopic to the unknot?","abstract":"In 1974, D. Rolfsen asked: Is every knot in $S^3$ isotopic (=homotopic through embeddings) to a PL knot or, equivalently, to the unknot? In particular, is the Bing sling isotopic to a PL knot? We show that the Bing sling is not isotopic to any PL knot: (1) by an isotopy which extends to an isotopy of $2$-component links with $lk=1$; (2) through knots that are intersections of nested sequences of solid tori.   There are also stronger versions of these results. In (1), the additional component may be allowed to self-intersect, and even to get replaced by a new one as long as it represents the same conjugacy class in $G/[G',G'']$, where $G$ is the fundamental group of the complement to the original component. In (2), the \"solid tori\" can be replaced by \"boundary-link-like handlebodies\", where a handlebody $V\\subset S^3$ of genus $g$ is called boundary-link-like if $\\pi_1(\\overline{S^3-V})$ admits a homomorphism to the free group $F_g$ such that the composition $\\pi_1(\\partial V)\\to\\pi_1(\\overline{S^3-V})\\to F_g$ is surjective.","sentences":["In 1974, D. Rolfsen asked: Is every knot in $S^3$ isotopic (=homotopic through embeddings) to a PL knot or, equivalently, to the unknot?","In particular, is the Bing sling isotopic to a PL knot?","We show that the Bing sling is not isotopic to any PL knot: (1) by an isotopy which extends to an isotopy of $2$-component links with $lk=1$; (2) through knots that are intersections of nested sequences of solid tori.   ","There are also stronger versions of these results.","In (1), the additional component may be allowed to self-intersect, and even to get replaced by a new one as long as it represents the same conjugacy class in $G/[G',G'']$, where $G$ is the fundamental group of the complement to the original component.","In (2), the \"solid tori\" can be replaced by \"boundary-link-like handlebodies\", where a handlebody $V\\subset S^3$ of genus $g$ is called boundary-link-like if $\\pi_1(\\overline{S^3-V})$ admits a homomorphism to the free group $F_g$ such that the composition $\\pi_1(\\partial V)\\to\\pi_1(\\overline{S^3-V})\\to F_g$ is surjective."],"url":"http://arxiv.org/abs/2406.09365v1","category":"math.GT"}
{"created":"2024-06-13 17:40:06","title":"An optical atomic clock using $4D_J$ states of rubidium","abstract":"We analyze an optical atomic clock using two-photon $5S_{1/2} \\rightarrow 4D_J$ transitions in rubidium. Four one- and two-color excitation schemes to probe the fine-structure states $4D_{3/2}$ and $4D_{5/2}$ are considered in detail. We compare key characteristics of Rb $4D_J$ and $5D_{5/2}$ two-photon clocks. The $4D_J$ clock features a high signal-to-noise ratio due to two-photon decay at favorable wavelengths, low dc electric and magnetic susceptibilities, and minimal black-body shifts. Ac Stark shifts from the clock interrogation lasers are compensated by two-color Rabi-frequency matching. We identify a \"magic\" wavelength near 1060~nm, which allows for in-trap, Doppler-free clock-transition interrogation with lattice-trapped cold atoms. From our analysis of clock statistics and systematics, we project a quantum-noise-limited relative clock stability at the $10^{-13}/\\sqrt{\\tau(s)}$-level, with integration time $\\tau$ in seconds, and a relative accuracy of $\\sim 10^{-13}$. We describe a potential architecture for implementing the proposed clock using a single telecom clock laser at 1550~nm, which is conducive to optical communication and long-distance clock comparisons. Our work could be of interest in efforts to realize small and portable Rb clocks and in high-precision measurements of atomic properties of Rb $4D_J$-states.","sentences":["We analyze an optical atomic clock using two-photon $5S_{1/2} \\rightarrow 4D_J$ transitions in rubidium.","Four one-","and two-color excitation schemes to probe the fine-structure states $4D_{3/2}$ and $4D_{5/2}$ are considered in detail.","We compare key characteristics of Rb $4D_J$ and $5D_{5/2}$ two-photon clocks.","The $4D_J$ clock features a high signal-to-noise ratio due to two-photon decay at favorable wavelengths, low dc electric and magnetic susceptibilities, and minimal black-body shifts.","Ac Stark shifts from the clock interrogation lasers are compensated by two-color Rabi-frequency matching.","We identify a \"magic\" wavelength near 1060~nm, which allows for in-trap, Doppler-free clock-transition interrogation with lattice-trapped cold atoms.","From our analysis of clock statistics and systematics, we project a quantum-noise-limited relative clock stability at the $10^{-13}/\\sqrt{\\tau(s)}$-level, with integration time $\\tau$ in seconds, and a relative accuracy of $\\sim 10^{-13}$.","We describe a potential architecture for implementing the proposed clock using a single telecom clock laser at 1550~nm, which is conducive to optical communication and long-distance clock comparisons.","Our work could be of interest in efforts to realize small and portable Rb clocks and in high-precision measurements of atomic properties of Rb $4D_J$-states."],"url":"http://arxiv.org/abs/2406.09352v1","category":"physics.atom-ph"}
{"created":"2024-06-13 16:37:51","title":"Hands-free teleoperation of a nearby manipulator through a virtual body-to-robot link","abstract":"This paper introduces an innovative control approach for teleoperating a robot in close proximity to a human operator, which could be useful to control robots embedded on wheelchairs. The method entails establishing a virtual connection between a specific body part and the robot's end-effector, visually displayed through an Augmented Reality (AR) headset. This linkage enables the transformation of body rotations into amplified effector translations, extending the robot's workspace beyond the capabilities of direct one-to-one mapping. Moreover, the linkage can be reconfigured using a joystick, resulting in a hybrid position/velocity control mode using the body/joystick motions respectively. After providing a comprehensive overview of the control methodology, we present the results of an experimental campaign designed to elucidate the advantages and drawbacks of our approach compared to the conventional joystick-based teleoperation method. The body-link control demonstrates slightly faster task completion and is naturally preferred over joystick velocity control, albeit being more physically demanding for tasks with a large range. The hybrid mode, where participants could simultaneously utilize both modes, emerges as a compromise, combining the intuitiveness of the body mode with the extensive task range of the velocity mode. Finally, we provide preliminary observations on potential assistive applications using head motions, especially for operators with limited range of motion in their bodies.","sentences":["This paper introduces an innovative control approach for teleoperating a robot in close proximity to a human operator, which could be useful to control robots embedded on wheelchairs.","The method entails establishing a virtual connection between a specific body part and the robot's end-effector, visually displayed through an Augmented Reality (AR) headset.","This linkage enables the transformation of body rotations into amplified effector translations, extending the robot's workspace beyond the capabilities of direct one-to-one mapping.","Moreover, the linkage can be reconfigured using a joystick, resulting in a hybrid position/velocity control mode using the body/joystick motions respectively.","After providing a comprehensive overview of the control methodology, we present the results of an experimental campaign designed to elucidate the advantages and drawbacks of our approach compared to the conventional joystick-based teleoperation method.","The body-link control demonstrates slightly faster task completion and is naturally preferred over joystick velocity control, albeit being more physically demanding for tasks with a large range.","The hybrid mode, where participants could simultaneously utilize both modes, emerges as a compromise, combining the intuitiveness of the body mode with the extensive task range of the velocity mode.","Finally, we provide preliminary observations on potential assistive applications using head motions, especially for operators with limited range of motion in their bodies."],"url":"http://arxiv.org/abs/2406.09301v1","category":"cs.RO"}
{"created":"2024-06-13 16:23:10","title":"Surface and curvature tensions of relativistic models","abstract":"In the present paper, we show a simple method to obtain fittings for the surface and curvature tensions. The method uses the nuclear mass of a spherical fully ionized atom and a simple expression for the binding energy such that a least square fit is found when confronted with the Atomic Mass Evaluation (AME) 2020. The fittings are then used to evaluate the pasta phase free energy per particle, which is confronted with the one obtained with a Thomas-Fermi fitting. The results are very encouraging and suggest that this recipe can be safely used whenever the surface and curvature tensions are necessary.","sentences":["In the present paper, we show a simple method to obtain fittings for the surface and curvature tensions.","The method uses the nuclear mass of a spherical fully ionized atom and a simple expression for the binding energy such that a least square fit is found when confronted with the Atomic Mass Evaluation (AME) 2020.","The fittings are then used to evaluate the pasta phase free energy per particle, which is confronted with the one obtained with a Thomas-Fermi fitting.","The results are very encouraging and suggest that this recipe can be safely used whenever the surface and curvature tensions are necessary."],"url":"http://arxiv.org/abs/2406.09284v1","category":"nucl-th"}
{"created":"2024-06-13 16:17:48","title":"Jet formation in post-AGB binaries: Confronting cold magnetohydrodynamic disc wind wind models with observations","abstract":"Aims: We consider cold self-similar magnetohydrodynamic (MHD) disc wind solutions to describe jets launching from the circumcompanion accretion discs in post-AGB binaries. Resulting predictions are matched to observations for five different post-AGB binaries. This both tests the physical validity of the MHD disc wind paradigm and reveals the accretion disc properties.   Results: Many of the time-series' properties are reproduced well by the models, though systematic mismatches, such as overestimated rotation, remain. Four targets imply accretion discs that reach close to the secondary's stellar surface, while one is fitted with an unrealistically large inner radius of about 20 stellar radii. Some fits imply inner disc temperatures over 10 000 K, seemingly discrepant with a previous observational estimate from H band interferometry. This estimate is, however, shown to be biased. Fitted mass-accretion rates range from about 10^-6 to 10^-3 solar masses per year. Relative to jets launched from young stellar objects (YSOs), all targets prefer winds with higher ejection efficiencies, lower magnetizations and thicker discs.   Conclusions: Our models show that current cold MHD disc wind solutions can explain many of the jet-related Balmer alpha features seen in post-AGB binaries, though systematic discrepancies remain. This includes, but is not limited to, overestimated rotation and underestimated post-AGB circumbinary disc lifetimes. The consideration of thicker discs and the inclusion of irradiation from the post-AGB primary, leading to warm magnetothermal wind launching, might alleviate these.","sentences":["Aims: We consider cold self-similar magnetohydrodynamic (MHD) disc wind solutions to describe jets launching from the circumcompanion accretion discs in post-AGB binaries.","Resulting predictions are matched to observations for five different post-AGB binaries.","This both tests the physical validity of the MHD disc wind paradigm and reveals the accretion disc properties.   ","Results:","Many of the time-series' properties are reproduced well by the models, though systematic mismatches, such as overestimated rotation, remain.","Four targets imply accretion discs that reach close to the secondary's stellar surface, while one is fitted with an unrealistically large inner radius of about 20 stellar radii.","Some fits imply inner disc temperatures over 10 000 K, seemingly discrepant with a previous observational estimate from H band interferometry.","This estimate is, however, shown to be biased.","Fitted mass-accretion rates range from about 10^-6 to 10^-3 solar masses per year.","Relative to jets launched from young stellar objects (YSOs), all targets prefer winds with higher ejection efficiencies, lower magnetizations and thicker discs.   ","Conclusions: Our models show that current cold MHD disc wind solutions can explain many of the jet-related Balmer alpha features seen in post-AGB binaries, though systematic discrepancies remain.","This includes, but is not limited to, overestimated rotation and underestimated post-AGB circumbinary disc lifetimes.","The consideration of thicker discs and the inclusion of irradiation from the post-AGB primary, leading to warm magnetothermal wind launching, might alleviate these."],"url":"http://arxiv.org/abs/2406.09280v1","category":"astro-ph.SR"}
{"created":"2024-06-13 16:16:59","title":"Boosting information transfer in a quantum correlated medium","abstract":"Sharing and receiving information plays a pivotal role in science and technology. Quantum communication relies on the principles of quantum mechanics to transmit information in a nonclassical manner. Existing quantum communication protocols are commonly based on shared entangled states between sender and receiver, while the transmitting medium is classical. We here demonstrate that information transfer may be enhanced in a quantum correlated medium without entanglement distribution. We concretely show that nonclassical correlations, with nonzero discord, between the first two spins of a spin chain that acts as a quantum wire can increase the information flow and reduce the propagation time. We relate this effect to the breaking of the spatial symmetry of the out-of-time-order correlator that characterizes the spread of information through the medium.","sentences":["Sharing and receiving information plays a pivotal role in science and technology.","Quantum communication relies on the principles of quantum mechanics to transmit information in a nonclassical manner.","Existing quantum communication protocols are commonly based on shared entangled states between sender and receiver, while the transmitting medium is classical.","We here demonstrate that information transfer may be enhanced in a quantum correlated medium without entanglement distribution.","We concretely show that nonclassical correlations, with nonzero discord, between the first two spins of a spin chain that acts as a quantum wire can increase the information flow and reduce the propagation time.","We relate this effect to the breaking of the spatial symmetry of the out-of-time-order correlator that characterizes the spread of information through the medium."],"url":"http://arxiv.org/abs/2406.09278v1","category":"quant-ph"}
{"created":"2024-06-13 16:10:13","title":"Spin excitations in Nd1-xSrxNiO2 and YBa2Cu3O7-delta: the influence of Hubbard U","abstract":"We use Resonant Inelastic X-ray Scattering (RIXS) to compare the doping dependence of magnetic excitations of an Infinite-Layer nickelate to those of a prototypical superconducting cuprate. The polarization analysis of RIXS spectra establishes the dominant spin-flip nature of the mid-infrared peak in both cases. Hole doping leads to opposite behavior of the magnetic energy in the two materials. By fitting the data with an original Hubbard-based model for dynamic susceptibility, we find that t is comparable in the two materials while U is about twice larger in the nickelate. This finding accounts for the smaller magnetic bandwidth of nickelates and for its decrease upon doping.","sentences":["We use Resonant Inelastic X-ray Scattering (RIXS) to compare the doping dependence of magnetic excitations of an Infinite-Layer nickelate to those of a prototypical superconducting cuprate.","The polarization analysis of RIXS spectra establishes the dominant spin-flip nature of the mid-infrared peak in both cases.","Hole doping leads to opposite behavior of the magnetic energy in the two materials.","By fitting the data with an original Hubbard-based model for dynamic susceptibility, we find that t is comparable in the two materials while U is about twice larger in the nickelate.","This finding accounts for the smaller magnetic bandwidth of nickelates and for its decrease upon doping."],"url":"http://arxiv.org/abs/2406.09271v1","category":"cond-mat.str-el"}
{"created":"2024-06-13 16:08:01","title":"CMB bounds on primordial black holes with dark matter mini-halos: the role of radiative feedback","abstract":"Observations of the cosmic microwave background constrain the abundance of primordial black holes, as these would accrete gas and inject energy into the cosmological medium. We have revisited these constraints, taking into account the local heating and ionisation of the gas around the black holes. While constraints for \\textit{naked} black holes are not significantly affected, bounds including dark matter mini-halos are drastically relaxed. This result suggests that previous analysis may have significantly overestimated the role of dark matter mini-halos in boosting the accretion rates.","sentences":["Observations of the cosmic microwave background constrain the abundance of primordial black holes, as these would accrete gas and inject energy into the cosmological medium.","We have revisited these constraints, taking into account the local heating and ionisation of the gas around the black holes.","While constraints for \\textit{naked} black holes are not significantly affected, bounds including dark matter mini-halos are drastically relaxed.","This result suggests that previous analysis may have significantly overestimated the role of dark matter mini-halos in boosting the accretion rates."],"url":"http://arxiv.org/abs/2406.09269v1","category":"astro-ph.CO"}
{"created":"2024-06-13 16:01:02","title":"Freudenthal Duality in Conformal Field Theory","abstract":"Rotational Freudenthal duality (RFD) relates two extremal Kerr-Newman (KN) black holes (BHs) with different angular momenta and electric-magnetic charges, but with the same Bekenstein-Hawking entropy. Through the Kerr/CFT correspondence (and its KN extension), a four-dimensional, asymptotically flat extremal KN BH is endowed with a dual thermal, two-dimensional conformal field theory (CFT) such that the Cardy entropy of the CFT is the same as the Bekenstein-Hawking entropy of the KN BH itself. Using this connection, we study the effect of the RFD on the thermal CFT dual to the KN extremal BH. We find that the RFD maps two different thermal, two-dimensional CFTs with different temperatures and central charges, but with the same asymptotic density of states, thereby matching the Cardy entropy. In an appendix, we discuss the action of the RFD on doubly-extremal rotating BHs, finding a spurious branch in the non-rotating limit, and determining that for this class of BH solutions the image of the RFD necessarily over-rotates.","sentences":["Rotational Freudenthal duality (RFD) relates two extremal Kerr-Newman (KN) black holes (BHs) with different angular momenta and electric-magnetic charges, but with the same Bekenstein-Hawking entropy.","Through the Kerr/CFT correspondence (and its KN extension), a four-dimensional, asymptotically flat extremal KN BH is endowed with a dual thermal, two-dimensional conformal field theory (CFT) such that the Cardy entropy of the CFT is the same as the Bekenstein-Hawking entropy of the KN BH itself.","Using this connection, we study the effect of the RFD on the thermal CFT dual to the KN extremal BH.","We find that the RFD maps two different thermal, two-dimensional CFTs with different temperatures and central charges, but with the same asymptotic density of states, thereby matching the Cardy entropy.","In an appendix, we discuss the action of the RFD on doubly-extremal rotating BHs, finding a spurious branch in the non-rotating limit, and determining that for this class of BH solutions the image of the RFD necessarily over-rotates."],"url":"http://arxiv.org/abs/2406.09259v1","category":"hep-th"}
{"created":"2024-06-13 15:39:08","title":"Absorptive Corrections to the Electromagnetic Form Factor in High-Energy Elastic Proton-Proton Scattering","abstract":"Recently, it was noted that absorptive corrections to the electromagnetic form factor in high-energy proton-proton scattering are important for the theoretical interpretation of the $p^\\uparrow{p}$ and $p^\\uparrow{A}$ analyzing power $A_\\text{N}(t)$ measurements with the Hydrogen Jet Target polarimeter (HJET) at RHIC. Here, a concise expression for the absorptive correction was derived within the eikonal approach. The resulting analysis reveals a systematic bias, nearly independent of the beam energy, in the experimental determination of the real-to-imaginary ratio $\\rho$ when absorption effects are overlooked in the data analysis. Quantification of this bias, as $\\rho^\\text{meas}=\\rho + (0.036\\pm0.016)_\\text{bias}$, was achieved using a Regge fit applied to available proton-proton measurements of $\\rho^\\text{meas}(s)$ and $\\sigma^\\text{meas}_\\text{tot}(s)$. Considering the potential impact of such an effect on the experimentally determined $A_\\text{N}(t)$, one may enhance consistency between the HJET and STAR measurements of the hadronic spin-flip amplitude. While the sign of the bias in the value of $\\rho$ aligns with the anticipated effective increase in the proton charge radius in $pp$ scattering due to absorption, it amplifies the observed discrepancy between $\\sigma^\\text{meas}_\\text{tot}$ and $\\rho^\\text{meas}$ values at $\\sqrt{s}=13\\,\\text{TeV}$ as measured in the TOTEM experiment. Evaluation (using published TOTEM data) of the measured proton-proton $d\\sigma/dt$ dependence on the absorptive corrections indicated that possible soft photon corrections to the hadronic amplitude slope may be essential for such data analysis.","sentences":["Recently, it was noted that absorptive corrections to the electromagnetic form factor in high-energy proton-proton scattering are important for the theoretical interpretation of the $p^\\uparrow{p}$ and $p^\\uparrow{A}$ analyzing power $A_\\text{N}(t)$ measurements with the Hydrogen Jet Target polarimeter (HJET) at RHIC.","Here, a concise expression for the absorptive correction was derived within the eikonal approach.","The resulting analysis reveals a systematic bias, nearly independent of the beam energy, in the experimental determination of the real-to-imaginary ratio $\\rho$ when absorption effects are overlooked in the data analysis.","Quantification of this bias, as $\\rho^\\text{meas}=\\rho + (0.036\\pm0.016)_\\text{bias}$, was achieved using a Regge fit applied to available proton-proton measurements of $\\rho^\\text{meas}(s)$ and $\\sigma^\\text{meas}_\\text{tot}(s)$. Considering the potential impact of such an effect on the experimentally determined $A_\\text{N}(t)$, one may enhance consistency between the HJET and STAR measurements of the hadronic spin-flip amplitude.","While the sign of the bias in the value of $\\rho$ aligns with the anticipated effective increase in the proton charge radius in $pp$ scattering due to absorption, it amplifies the observed discrepancy between $\\sigma^\\text{meas}_\\text{tot}$ and $\\rho^\\text{meas}$ values at $\\sqrt{s}=13\\,\\text{TeV}$ as measured in the TOTEM experiment.","Evaluation (using published TOTEM data) of the measured proton-proton $d\\sigma/dt$ dependence on the absorptive corrections indicated that possible soft photon corrections to the hadronic amplitude slope may be essential for such data analysis."],"url":"http://arxiv.org/abs/2406.09237v1","category":"hep-ph"}
{"created":"2024-06-13 15:20:04","title":"Measuring glitch recoveries and braking indices with Bayesian model selection","abstract":"For a selection of 35 pulsars with large spin-up glitches ($\\Delta{\\nu}/\\nu\\geq10^{-6}$), which are monitored by the Jodrell Bank Observatory, we analyse 157 glitches and their recoveries. All parameters are measured consistently and we choose the best model to describe the post-glitch recovery based on Bayesian evidence. We present updated glitch epochs, sizes, changes of spin down rate, exponentially recovering components (amplitude and corresponding timescale) when present, as well as pulsars' second frequency derivatives and their glitch associated changes if detected. We discuss the different observed styles of post-glitch recovery as well as some particularly interesting sources. Several correlations are revealed between glitch parameters and pulsar spin parameters, including a very strong correlation between a pulsar's interglitch $|\\ddot{\\nu}|$ and $\\dot{\\nu}$, as well as between the glitch-induced spin-down rate change $\\Delta\\dot{\\nu}_{\\rm p}$ that does not relax exponentially and $\\dot{\\nu}$. We find that the ratio $\\left|\\Delta \\dot{\\nu}_{\\mathrm{p}}/\\ddot{\\nu}\\right|$ can be used as an estimate of glitch recurrence times, especially for those pulsars for which there are indications of a characteristic glitch size and interglitch waiting time. We calculate the interglitch braking index $n$ and find that pulsars with large glitches typically have $n$ greater than $3$, suggesting that internal torques dominate the rotational evolution between glitches. The external torque, e.g. from electromagnetic dipole radiation, could dominate the observed $\\ddot{\\nu}$ for the youngest pulsars ($\\lesssim10^{4}\\;\\mathrm{yr}$), which may be expected to display $n\\sim3$.","sentences":["For a selection of 35 pulsars with large spin-up glitches ($\\Delta{\\nu}/\\nu\\geq10^{-6}$), which are monitored by the Jodrell Bank Observatory, we analyse 157 glitches and their recoveries.","All parameters are measured consistently","and we choose the best model to describe the post-glitch recovery based on Bayesian evidence.","We present updated glitch epochs, sizes, changes of spin down rate, exponentially recovering components (amplitude and corresponding timescale) when present, as well as pulsars' second frequency derivatives and their glitch associated changes if detected.","We discuss the different observed styles of post-glitch recovery as well as some particularly interesting sources.","Several correlations are revealed between glitch parameters and pulsar spin parameters, including a very strong correlation between a pulsar's interglitch $|\\ddot{\\nu}|$ and $\\dot{\\nu}$, as well as between the glitch-induced spin-down rate change $\\Delta\\dot{\\nu}_{\\rm p}$ that does not relax exponentially and $\\dot{\\nu}$. We find that the ratio $\\left|\\Delta \\dot{\\nu}_{\\mathrm{p}}/\\ddot{\\nu}\\right|$ can be used as an estimate of glitch recurrence times, especially for those pulsars for which there are indications of a characteristic glitch size and interglitch waiting time.","We calculate the interglitch braking index $n$ and find that pulsars with large glitches typically have $n$ greater than $3$, suggesting that internal torques dominate the rotational evolution between glitches.","The external torque, e.g. from electromagnetic dipole radiation, could dominate the observed $\\ddot{\\nu}$ for the youngest pulsars ($\\lesssim10^{4}\\;\\mathrm{yr}$), which may be expected to display $n\\sim3$."],"url":"http://arxiv.org/abs/2406.09219v1","category":"astro-ph.HE"}
{"created":"2024-06-13 14:53:57","title":"Vector-like quarks: status and new directions at the LHC","abstract":"Experimental searches for vector-like quarks have until now only considered their decays into Standard Model particles. However, various new physics scenarios predict additional scalars, so that these vector-like quarks can decay to new channels. These new channels reduce the branching ratios into Standard Model final states, significantly affecting current mass bounds. In this article, we quantitatively assess the relevance and observability of single and pair production processes of vector-like quarks, followed by decays into both standard and exotic final states. We highlight the importance of large widths and the relative interaction strengths with Standard Model particles and new scalars. Then, we review the post-Moriond 2024 status of these models in light of available LHC data and discuss potential future strategies to enhance the scope of vector-like quark searches.","sentences":["Experimental searches for vector-like quarks have until now only considered their decays into Standard Model particles.","However, various new physics scenarios predict additional scalars, so that these vector-like quarks can decay to new channels.","These new channels reduce the branching ratios into Standard Model final states, significantly affecting current mass bounds.","In this article, we quantitatively assess the relevance and observability of single and pair production processes of vector-like quarks, followed by decays into both standard and exotic final states.","We highlight the importance of large widths and the relative interaction strengths with Standard Model particles and new scalars.","Then, we review the post-Moriond 2024 status of these models in light of available LHC data and discuss potential future strategies to enhance the scope of vector-like quark searches."],"url":"http://arxiv.org/abs/2406.09193v1","category":"hep-ph"}
{"created":"2024-06-13 14:48:10","title":"A formation pathway for terrestrial planets with moderate water content involving atmospheric-volatile recycling","abstract":"Of the many recently discovered terrestrial exoplanets, some are expected to harbor moderate water mass fractions of a few percent. The formation pathways that can produce planets with these water mass fractions are not fully understood. Here, we use the code chemcomp, which consists of a semi-analytical 1D protoplanetary disk model harboring a migrating and accreting planet, to model the growth and composition of planets with moderate water mass fractions by pebble accretion in a protoplanetary disk around a TRAPPIST-1 analog star. This star is accompanied by seven terrestrial planets, of which the outer four planets likely contain water mass fractions of between 1\\% and 10\\%. We adopt a published model that considers the evaporation of pebbles in the planetary envelope, from where recycling flows can transport the volatile vapor back into the disk. We find that with this model, the planetary water content depends on the influx rate of pebbles onto the planet. A decreasing pebble influx with time reduces the envelope temperature and consequently allows the formation of planets with moderate water mass fractions as inferred for the outer TRAPPIST-1 planets for a number of different simulation configurations. This is further evidence that the recycling of vapor is an important component of planet formation needed to explain the vast and diverse population of exoplanets.","sentences":["Of the many recently discovered terrestrial exoplanets, some are expected to harbor moderate water mass fractions of a few percent.","The formation pathways that can produce planets with these water mass fractions are not fully understood.","Here, we use the code chemcomp, which consists of a semi-analytical 1D protoplanetary disk model harboring a migrating and accreting planet, to model the growth and composition of planets with moderate water mass fractions by pebble accretion in a protoplanetary disk around a TRAPPIST-1 analog star.","This star is accompanied by seven terrestrial planets, of which the outer four planets likely contain water mass fractions of between 1\\% and 10\\%.","We adopt a published model that considers the evaporation of pebbles in the planetary envelope, from where recycling flows can transport the volatile vapor back into the disk.","We find that with this model, the planetary water content depends on the influx rate of pebbles onto the planet.","A decreasing pebble influx with time reduces the envelope temperature and consequently allows the formation of planets with moderate water mass fractions as inferred for the outer TRAPPIST-1 planets for a number of different simulation configurations.","This is further evidence that the recycling of vapor is an important component of planet formation needed to explain the vast and diverse population of exoplanets."],"url":"http://arxiv.org/abs/2406.09186v1","category":"astro-ph.EP"}
{"created":"2024-06-13 14:36:15","title":"An attractive way to correct for missing singles excitations in unitary coupled cluster doubles theory","abstract":"Coupled cluster methods based exclusively on double excitations are comparatively \"cheap\" and interesting model chemistries, as they are typically able to capture the bulk of the dynamical electron correlation effects. The trade-off in such approximations is that the effect of neglected excitations, particularly single excitations, can be considerable. Using standard and electron pair-restricted $T_2$ operators to define two flavors of unitary coupled cluster doubles (UCCD) methods, we investigate the extent in which missing single excitations can be recovered from low-order corrections in many-body perturbation theory (MBPT) within the unitary coupled cluster (UCC) formalism. Our analysis includes the derivations of finite-order, UCC energy functionals which are used as a basis to define perturbative estimates of missed single excitations. This leads to the novel UCCD[4S] and UCCD[6S] methods, which consider energy corrections for missing singles excitations through fourth- and sixth-order in MBPT, respectively. We also apply the same methodology to the electron pair-restricted ansatz, but the improvements are only marginal. Our findings show that augmenting UCCD with these post hoc perturbative corrections can lead to UCCSD-quality results.","sentences":["Coupled cluster methods based exclusively on double excitations are comparatively \"cheap\" and interesting model chemistries, as they are typically able to capture the bulk of the dynamical electron correlation effects.","The trade-off in such approximations is that the effect of neglected excitations, particularly single excitations, can be considerable.","Using standard and electron pair-restricted $T_2$ operators to define two flavors of unitary coupled cluster doubles (UCCD) methods, we investigate the extent in which missing single excitations can be recovered from low-order corrections in many-body perturbation theory (MBPT) within the unitary coupled cluster (UCC) formalism.","Our analysis includes the derivations of finite-order, UCC energy functionals which are used as a basis to define perturbative estimates of missed single excitations.","This leads to the novel UCCD[4S] and UCCD[6S] methods, which consider energy corrections for missing singles excitations through fourth- and sixth-order in MBPT, respectively.","We also apply the same methodology to the electron pair-restricted ansatz, but the improvements are only marginal.","Our findings show that augmenting UCCD with these post hoc perturbative corrections can lead to UCCSD-quality results."],"url":"http://arxiv.org/abs/2406.09174v1","category":"quant-ph"}
{"created":"2024-06-13 14:27:01","title":"Covariate balancing with measurement error","abstract":"In recent years, there is a growing body of causal inference literature focusing on covariate balancing methods. These methods eliminate observed confounding by equalizing covariate moments between the treated and control groups. The validity of covariate balancing relies on an implicit assumption that all covariates are accurately measured, which is frequently violated in observational studies. Nevertheless, the impact of measurement error on covariate balancing is unclear, and there is no existing work on balancing mismeasured covariates adequately. In this article, we show that naively ignoring measurement error reversely increases the magnitude of covariate imbalance and induces bias to treatment effect estimation. We then propose a class of measurement error correction strategies for the existing covariate balancing methods. Theoretically, we show that these strategies successfully recover balance for all covariates, and eliminate bias of treatment effect estimation. We assess the proposed correction methods in simulation studies and real data analysis.","sentences":["In recent years, there is a growing body of causal inference literature focusing on covariate balancing methods.","These methods eliminate observed confounding by equalizing covariate moments between the treated and control groups.","The validity of covariate balancing relies on an implicit assumption that all covariates are accurately measured, which is frequently violated in observational studies.","Nevertheless, the impact of measurement error on covariate balancing is unclear, and there is no existing work on balancing mismeasured covariates adequately.","In this article, we show that naively ignoring measurement error reversely increases the magnitude of covariate imbalance and induces bias to treatment effect estimation.","We then propose a class of measurement error correction strategies for the existing covariate balancing methods.","Theoretically, we show that these strategies successfully recover balance for all covariates, and eliminate bias of treatment effect estimation.","We assess the proposed correction methods in simulation studies and real data analysis."],"url":"http://arxiv.org/abs/2406.09163v1","category":"stat.ME"}
{"created":"2024-06-13 14:16:24","title":"Higher spin swampland conjecture for massive AdS$_{3}$ gravity","abstract":"In this paper, we show that a possible version of the swampland weak gravity conjecture for higher spin (HS) massive topological AdS$_{3}$ gravity can be expressed in terms of mass $M_{hs}$, charge $Q_{hs}$ and coupling constant $g_{hs}$ of 3D gravity coupled to higher spin fields as $M_{hs} \\leq \\sqrt{2}$ $Q_{hs}$ $g_{hs}$ $M_{Pl}$. The higher spin charge is given by the $SO(1,2)$ quadratic Casimir $Q_{hs}^{2}=s\\left (s-1\\right) $ and the HS coupling constant by ${\\large g}_{hs} ^{2}=2/\\left (M_{Pl}^{2} l_{AdS_{3}}^{2}\\right )$ while the mass expressed like $\\left( l_{AdS_{3}} \\text{M}_{hs}\\right) ^{2}$ is defined as $ \\left (1+\\mu l_{AdS_{3}} \\right ) ^{2} s \\left ( s-1 \\right ) +[1- \\left ( \\mu l_{AdS_{3}} \\right ) ^{2} \\left ( s-1 \\right ) ]$.","sentences":["In this paper, we show that a possible version of the swampland weak gravity conjecture for higher spin (HS) massive topological AdS$_{3}$ gravity can be expressed in terms of mass $M_{hs}$, charge $Q_{hs}$ and coupling constant $g_{hs}$ of 3D gravity coupled to higher spin fields as $M_{hs} \\leq \\sqrt{2}$ $Q_{hs}$ $g_{hs}$ $M_{Pl}$.","The higher spin charge is given by the $SO(1,2)$ quadratic Casimir $Q_{hs}^{2}=s\\left (s-1\\right) $ and the HS coupling constant by ${\\large g}_{hs} ^{2}=2/\\left (M_{Pl}^{2} l_{AdS_{3}}^{2}\\right )$ while the mass expressed like $\\left( l_{AdS_{3}} \\text{M}_{hs}\\right) ^{2}$ is defined as $ \\left (1+\\mu l_{AdS_{3}} \\right ) ^{2} s \\left ( s-1 \\right )","+","[1- \\left ( \\mu l_{AdS_{3}} \\right ) ^{2} \\left ( s-1 \\right ) ]$."],"url":"http://arxiv.org/abs/2406.09151v1","category":"hep-th"}
{"created":"2024-06-13 14:15:59","title":"Structural changes in Ge1-xSnx and Si1-x-yGeySnx thin films on SOI substrates treated by pulse laser annealing","abstract":"Ge1-xSnx and Si1-x-yGeySnx alloys are promising materials for future opto- and nanoelectronics applications. These alloys enable effective band-gap engineering, broad adjustability of their lattice parameter, exhibit much higher carrier mobility than pure Si, and are compatible with the CMOS technology. Unfortunately, the equilibrium solid solubility of Sn in Si1-xGex is less than 1% and the pseudomorphic growth of Si1-x-yGeySnx on Ge or Si can cause in-plane compressive strain in the grown layer, degrading the superior properties of these alloys. Therefore, post-growth strain engineering by ultrafast non-equilibrium thermal treatments like pulse laser annealing (PLA) is needed to improve the layer quality. In this article, Ge0.94Sn0.06 and Si0.14Ge0.8Sn0.06 thin films grown on silicon-on-insulator substrates by molecular beam epitaxy were post growth thermally treated by PLA. The material is analyzed before and after the thermal treatments by transmission electron microscopy, X-ray diffraction (XRD), Rutherford backscattering spectrometry, secondary ion mass spectrometry, and Hall effect measurements. It is shown that after annealing, the material is single-crystalline with improved crystallinity than the as-grown layer. This is reflected in a significantly increased XRD reflection intensity, well-ordered atomic pillars, and increased active carrier concentrations up to 4x1019 cm-3.","sentences":["Ge1-xSnx and Si1-x-yGeySnx alloys are promising materials for future opto- and nanoelectronics applications.","These alloys enable effective band-gap engineering, broad adjustability of their lattice parameter, exhibit much higher carrier mobility than pure Si, and are compatible with the CMOS technology.","Unfortunately, the equilibrium solid solubility of Sn in Si1-xGex is less than 1% and the pseudomorphic growth of Si1-x-yGeySnx on Ge or Si can cause in-plane compressive strain in the grown layer, degrading the superior properties of these alloys.","Therefore, post-growth strain engineering by ultrafast non-equilibrium thermal treatments like pulse laser annealing (PLA) is needed to improve the layer quality.","In this article, Ge0.94Sn0.06 and Si0.14Ge0.8Sn0.06 thin films grown on silicon-on-insulator substrates by molecular beam epitaxy were post growth thermally treated by PLA.","The material is analyzed before and after the thermal treatments by transmission electron microscopy, X-ray diffraction (XRD), Rutherford backscattering spectrometry, secondary ion mass spectrometry, and Hall effect measurements.","It is shown that after annealing, the material is single-crystalline with improved crystallinity than the as-grown layer.","This is reflected in a significantly increased XRD reflection intensity, well-ordered atomic pillars, and increased active carrier concentrations up to 4x1019 cm-3."],"url":"http://arxiv.org/abs/2406.09149v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-13 13:26:14","title":"Casimir energy of $N$ $\u03b4$-plates with constant conductivity","abstract":"The Casimir energy for $N$ $\\delta$-function plates depends on multiple scattering parameter $\\Delta$. This $N$ body interaction was distributed into two body interactions with nearest neighbour scattering and next-to-nearest neighbour scattering based on partitions of $N-1$ and its permutations. Implementing this methodology, we investigate Casimir energy for multiple plates with constant conductivity relatable to Graphene. We also study Casimir interaction between a perfect magnetic conductor and multiple constant conductivity $\\delta$ plates, which results in Boyer repulsion. In the asymptotic limit for ideal boundary conditions, the results become simple where multiple scattering parameter $\\Delta$ consists only of nearest neighbour scattering term.","sentences":["The Casimir energy for $N$ $\\delta$-function plates depends on multiple scattering parameter $\\Delta$. This $N$ body interaction was distributed into two body interactions with nearest neighbour scattering and next-to-nearest neighbour scattering based on partitions of $N-1$ and its permutations.","Implementing this methodology, we investigate Casimir energy for multiple plates with constant conductivity relatable to Graphene.","We also study Casimir interaction between a perfect magnetic conductor and multiple constant conductivity $\\delta$ plates, which results in Boyer repulsion.","In the asymptotic limit for ideal boundary conditions, the results become simple where multiple scattering parameter $\\Delta$ consists only of nearest neighbour scattering term."],"url":"http://arxiv.org/abs/2406.09096v1","category":"quant-ph"}
{"created":"2024-06-13 12:42:33","title":"Relational event models with global covariates","abstract":"Traditional inference in relational event models from dynamic network data involves only dyadic and node-specific variables, as anything that is global, i.e. constant across dyads, drops out of the partial likelihood. We address this with the use of nested case-control sampling on a time-shifted version of the event process. This leads to a partial likelihood of a degenerate logistic additive model, enabling efficient estimation of global and non-global covariate effects. The method's effectiveness is demonstrated through a simulation study. An application to bike sharing data reveals significant influences of global covariates like weather and time of day on bike-sharing dynamics.","sentences":["Traditional inference in relational event models from dynamic network data involves only dyadic and node-specific variables, as anything that is global, i.e. constant across dyads, drops out of the partial likelihood.","We address this with the use of nested case-control sampling on a time-shifted version of the event process.","This leads to a partial likelihood of a degenerate logistic additive model, enabling efficient estimation of global and non-global covariate effects.","The method's effectiveness is demonstrated through a simulation study.","An application to bike sharing data reveals significant influences of global covariates like weather and time of day on bike-sharing dynamics."],"url":"http://arxiv.org/abs/2406.09055v1","category":"stat.ME"}
{"created":"2024-06-13 12:40:05","title":"Electronic structure of a nodal line semimetal candidate TbSbTe","abstract":"The LnSbTe (Ln = Lanthanides) family, like isostructural ZrSiS type compounds, has emerged as a fertile playground for exploring the interaction of electronic correlations and magnetic ordering with the nodal line band topology. Here, we report a detailed electronic band structure investigation of TbSbTe, corroborated by electrical transport, thermodynamic, and magnetic studies. Temperature-dependent magnetic susceptibility and thermodynamic transport studies indicate the onset of antiferromagnetic ordering below TN = 5.1 K. The electronic band structure study, carried out with high-resolution angle-resolved photoemission spectroscopy (ARPES) measurements aided with density functional theory based first-principles calculations reveals presence of nodal lines in the GammaX high symmetry direction, forming a diamond-shaped nodal plane around Gamma high symmetry point. A strongly photon energy dependent nodal feature located at the X point of the surface Brillouin zone, indicating an extended nodal line along X R direction, is also observed. This study elucidates the intricate interplay among symmetry-protected band characteristics, the influence of spin orbit coupling, magnetism, and topological properties.","sentences":["The LnSbTe (Ln = Lanthanides) family, like isostructural ZrSiS type compounds, has emerged as a fertile playground for exploring the interaction of electronic correlations and magnetic ordering with the nodal line band topology.","Here, we report a detailed electronic band structure investigation of TbSbTe, corroborated by electrical transport, thermodynamic, and magnetic studies.","Temperature-dependent magnetic susceptibility and thermodynamic transport studies indicate the onset of antiferromagnetic ordering below TN = 5.1 K.","The electronic band structure study, carried out with high-resolution angle-resolved photoemission spectroscopy (ARPES) measurements aided with density functional theory based first-principles calculations reveals presence of nodal lines in the GammaX high symmetry direction, forming a diamond-shaped nodal plane around Gamma high symmetry point.","A strongly photon energy dependent nodal feature located at the X point of the surface Brillouin zone, indicating an extended nodal line along X R direction, is also observed.","This study elucidates the intricate interplay among symmetry-protected band characteristics, the influence of spin orbit coupling, magnetism, and topological properties."],"url":"http://arxiv.org/abs/2406.09054v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-13 11:12:19","title":"Gatemonium: A Voltage-Tunable Fluxonium","abstract":"We present a new fluxonium qubit design, gatemonium, based on an all superconductor-semiconductor hybrid platform exhibiting gate voltage tunability of $E_J$. We first show the principle of fluxonium operation in epitaxial Al/InAs heterostructure where the single Josephson junction can be controlled using gate voltage control, effectively tuning the ``weight'' of the fictitious phase particle. The spectroscopy of the qubit shows tunability between plasmons to fluxons and their hybrid spectrum. We study two gatemonium devices with different charging energies and extract inductance of InAs-based Josephson junctions array. We also discuss future directions implementing a gate voltage tunable superinductance.","sentences":["We present a new fluxonium qubit design, gatemonium, based on an all superconductor-semiconductor hybrid platform exhibiting gate voltage tunability of $E_J$. We first show the principle of fluxonium operation in epitaxial Al/InAs heterostructure where the single Josephson junction can be controlled using gate voltage control, effectively tuning the ``weight'' of the fictitious phase particle.","The spectroscopy of the qubit shows tunability between plasmons to fluxons and their hybrid spectrum.","We study two gatemonium devices with different charging energies and extract inductance of InAs-based Josephson junctions array.","We also discuss future directions implementing a gate voltage tunable superinductance."],"url":"http://arxiv.org/abs/2406.09002v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-13 10:57:41","title":"Electric-field-modulated topological phase transition in AlSb/InSe heterobilayers","abstract":"Searching for controllable topological phase by means of external stimuli in two-dimensional (2D) material-based van der Waals (vdW) heterostructures is currently an active field for both the underlying physics and practical applications. Here, using first-principles calculations, we investigate electric-field-modulated topological phase transition in a vdW heterobilayer formed by vertically stacking 2D AlSb and InSe monolayers. The AlSb/InSe heterobilayer studied possesses both dynamical and thermal stabilities, which is a direct bandgap semiconductor and forms a Z-scheme heterojunction. With inclusion of spin-orbit coupling (SOC) and applying external electric field, the bandgap decreases at first and then increase, and a trivial insulator to topological insulator phase transition is observed. For the topological insulator phase, band inversion is ascribed to the strong SOC of p orbitals of Sb. Our work paves the way for the design and application of multifunctional nanoscale devices such as topological field effect transistor.","sentences":["Searching for controllable topological phase by means of external stimuli in two-dimensional (2D) material-based van der Waals (vdW) heterostructures is currently an active field for both the underlying physics and practical applications.","Here, using first-principles calculations, we investigate electric-field-modulated topological phase transition in a vdW heterobilayer formed by vertically stacking 2D AlSb and InSe monolayers.","The AlSb/InSe heterobilayer studied possesses both dynamical and thermal stabilities, which is a direct bandgap semiconductor and forms a Z-scheme heterojunction.","With inclusion of spin-orbit coupling (SOC) and applying external electric field, the bandgap decreases at first and then increase, and a trivial insulator to topological insulator phase transition is observed.","For the topological insulator phase, band inversion is ascribed to the strong SOC of p orbitals of Sb.","Our work paves the way for the design and application of multifunctional nanoscale devices such as topological field effect transistor."],"url":"http://arxiv.org/abs/2406.08999v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-06-13 10:53:44","title":"Non-perturbative determination of the ${\\cal N} = 1$ SUSY Yang-Mills gluino condensate at large $N$","abstract":"We present the first non-perturbative large-$N$ calculation of the $\\mathcal{N}=1$ supersymmetric (SUSY) $\\mathrm{SU}(N)$ Yang-Mills gluino condensate, obtained by means of numerical simulations of the lattice-discretized theory, and exploiting large-$N$ twisted volume reduction. We present two different determinations based, respectively, on the Banks-Casher formula and on the Gell-Mann-Oakes-Renner relation, both giving perfectly consistent results. By matching the lattice and the Novikov-Shifman-Vainshtein-Zakharov (NSVZ) regularization schemes, we are able for the first time to compare numerical and analytic computations. Our most accurate determination gives $\\Sigma_{\\rm RGI} /\\Lambda_{\\rm NSVZ}^3 = 1.55(57)$, in very good agreement with the $N$-dependence and the value predicted by the weak coupling instanton-based approach $\\Sigma_{\\rm RGI} /\\Lambda_{\\rm NSVZ}^3 = 1$.","sentences":["We present the first non-perturbative large-$N$ calculation of the $\\mathcal{N}=1$ supersymmetric (SUSY) $\\mathrm{SU}(N)$ Yang-Mills gluino condensate, obtained by means of numerical simulations of the lattice-discretized theory, and exploiting large-$N$ twisted volume reduction.","We present two different determinations based, respectively, on the Banks-Casher formula and on the Gell-Mann-Oakes-Renner relation, both giving perfectly consistent results.","By matching the lattice and the Novikov-Shifman-Vainshtein-Zakharov (NSVZ) regularization schemes, we are able for the first time to compare numerical and analytic computations.","Our most accurate determination gives $\\Sigma_{\\rm RGI} /\\Lambda_{\\rm NSVZ}^3 = 1.55(57)$, in very good agreement with the $N$-dependence and the value predicted by the weak coupling instanton-based approach $\\Sigma_{\\rm RGI} /\\Lambda_{\\rm NSVZ}^3 = 1$."],"url":"http://arxiv.org/abs/2406.08995v1","category":"hep-th"}
{"created":"2024-06-13 09:59:00","title":"Stabilization of a twisted modulus on a mirror of rigid Calabi-Yau manifold","abstract":"We study the stabilization of a twisted modulus in Type IIB flux compactifications on a mirror of the rigid Calabi-Yau threefold. By analyzing the effective action of twisted and untwisted moduli, we find that three-form fluxes satisfying the tadpole cancellation conditions lead to supersymmetric AdS vacua. We also investigate swampland conjectures on this non-geometric background.","sentences":["We study the stabilization of a twisted modulus in Type IIB flux compactifications on a mirror of the rigid Calabi-Yau threefold.","By analyzing the effective action of twisted and untwisted moduli, we find that three-form fluxes satisfying the tadpole cancellation conditions lead to supersymmetric AdS vacua.","We also investigate swampland conjectures on this non-geometric background."],"url":"http://arxiv.org/abs/2406.08970v1","category":"hep-th"}
{"created":"2024-06-13 09:58:03","title":"New Factorizations of Yang-Mills Amplitudes","abstract":"We propose a new factorization pattern for tree-level Yang-Mills (YM) amplitudes, where they decompose into a sum of products of two lower-point amplitudes by setting specific two-point non-planar Mandelstam variables within a rectangular configuration to zero. This approach manifests the hidden zeros of YM amplitudes recently identified. Furthermore, by setting specific Lorentz products involving polarization vectors to zero, the amplitudes further reduce to a sum of products of three currents. These novel factorizations provide a fresh perspective on the structure of YM amplitudes, potentially enhancing our understanding and calculation of these fundamental quantities.","sentences":["We propose a new factorization pattern for tree-level Yang-Mills (YM) amplitudes, where they decompose into a sum of products of two lower-point amplitudes by setting specific two-point non-planar Mandelstam variables within a rectangular configuration to zero.","This approach manifests the hidden zeros of YM amplitudes recently identified.","Furthermore, by setting specific Lorentz products involving polarization vectors to zero, the amplitudes further reduce to a sum of products of three currents.","These novel factorizations provide a fresh perspective on the structure of YM amplitudes, potentially enhancing our understanding and calculation of these fundamental quantities."],"url":"http://arxiv.org/abs/2406.08969v1","category":"hep-th"}
{"created":"2024-06-13 09:06:29","title":"Dense Outflowing Molecular Gas in Massive Star-forming Regions","abstract":"Dense outflowing gas, traced by transitions of molecules with large dipole moment, is important for understanding mass loss and feedback of massive star formation. HCN 3-2 and HCO$^+$ 3-2 are good tracers of dense outflowing molecular gas, which are closely related to active star formation. In this study, we present on-the-fly (OTF) mapping observations of HCN 3-2 and HCO$^+$ 3-2 toward a sample of 33 massive star-forming regions using the 10-m Submillimeter Telescope (SMT). With the spatial distribution of line wings of HCO$^+$ 3-2 and HCN 3-2, outflows are detected in 25 sources, resulting in a detection rate of 76$\\%$. The optically thin H$^{13}$CN and H$^{13}$CO$^+$ 3-2 lines are used to identify line wings as outflows and estimate core mass. The mass $M_{out}$, momentum $P_{out}$, kinetic energy $E_{K}$, force $F_{out}$ and mass loss rate $\\dot M_{out}$ of outflow and core mass, are obtained for each source. A sublinear tight correlation is found between the mass of dense molecular outflow and core mass, with an index of $\\sim$ 0.8 and a correlation coefficient of 0.88.","sentences":["Dense outflowing gas, traced by transitions of molecules with large dipole moment, is important for understanding mass loss and feedback of massive star formation.","HCN 3-2 and HCO$^+$ 3-2 are good tracers of dense outflowing molecular gas, which are closely related to active star formation.","In this study, we present on-the-fly (OTF) mapping observations of HCN 3-2 and HCO$^+$ 3-2 toward a sample of 33 massive star-forming regions using the 10-m Submillimeter Telescope (SMT).","With the spatial distribution of line wings of HCO$^+$ 3-2 and HCN 3-2, outflows are detected in 25 sources, resulting in a detection rate of 76$\\%$.","The optically thin H$^{13}$CN and H$^{13}$CO$^+$ 3-2 lines are used to identify line wings as outflows and estimate core mass.","The mass $M_{out}$, momentum $P_{out}$, kinetic energy $E_{K}$, force $F_{out}$ and mass loss rate $\\dot M_{out}$ of outflow and core mass, are obtained for each source.","A sublinear tight correlation is found between the mass of dense molecular outflow and core mass, with an index of $\\sim$ 0.8 and a correlation coefficient of 0.88."],"url":"http://arxiv.org/abs/2406.08935v1","category":"astro-ph.GA"}
{"created":"2024-06-13 09:05:18","title":"Tully-Fisher Relation of Late-type Galaxies at $0.6 \\leq z \\leq 2.5$","abstract":"We present a study of the stellar and baryonic Tully-Fisher relation within the redshift range of $0.6 \\leq z \\leq 2.5$ utilizing observations of \\sfgs. This dataset, as explored in \\citet{GS23}, comprises of disk-like galaxies spanning a stellar mass range of $8.89 \\leq \\log(M_{star} \\ [\\mathrm{M_\\odot}]) \\leq 11.5$, baryonic mass range of $9.0 \\leq \\log(M_{bar} [\\mathrm{M_\\odot}]) \\leq 11.5$, and circular velocity range of $1.65 \\leq \\log(V_c \\ [{\\rm km/s}]) \\leq 2.85$. Stellar masses of these objects are estimated using spectral energy distribution fitting techniques, while gas masses are determined via scaling relations. Circular velocities are directly derived from the Rotation Curves (RCs), after meticulously correcting for beam smearing and pressure support. Our analysis confirms that our sample adheres to the fundamental mass-size relations of galaxies and reflects the evolution of velocity dispersion in galaxies, in line with previous findings. This reaffirms the reliability of our photometric and kinematic parameters (i.e., $M_{star}$ and $V_c$), thereby enabling a comprehensive examination of the Tully-Fisher relation. To attain robust results, we employed a novel orthogonal likelihood fitting technique designed to minimize intrinsic scatter around the best-fit line, as required at \\hz. For the STFR, we obtained a slope of $\\alpha=3.03\\pm 0.25$, an offset of $\\beta = 3.34\\pm 0.53$, and an intrinsic scatter of $\\zeta_{int}=0.08$ dex. Correspondingly, the BTFR yielded $\\alpha=3.21\\pm 0.28$, $\\beta=3.16\\pm 0.61$, and $\\zeta_{int}=0.09$ dex. Our findings suggest a subtle deviation in the stellar and baryonic Tully-Fisher relation with respect to local studies, which is most-likely due to the evolutionary processes governing disk formation.","sentences":["We present a study of the stellar and baryonic Tully-Fisher relation within the redshift range of $0.6 \\leq z \\leq 2.5$ utilizing observations of \\sfgs.","This dataset, as explored in \\citet{GS23}, comprises of disk-like galaxies spanning a stellar mass range of $8.89 \\leq \\log(M_{star} \\ [\\mathrm{M_\\odot}]) \\leq 11.5$, baryonic mass range of $9.0 \\leq \\log(M_{bar} [\\mathrm{M_\\odot}]) \\leq 11.5$, and circular velocity range of $1.65 \\leq","\\log(V_c \\","[{\\rm km/s}])","\\leq 2.85$. Stellar masses of these objects are estimated using spectral energy distribution fitting techniques, while gas masses are determined via scaling relations.","Circular velocities are directly derived from the Rotation Curves (RCs), after meticulously correcting for beam smearing and pressure support.","Our analysis confirms that our sample adheres to the fundamental mass-size relations of galaxies and reflects the evolution of velocity dispersion in galaxies, in line with previous findings.","This reaffirms the reliability of our photometric and kinematic parameters (i.e., $M_{star}$ and $V_c$), thereby enabling a comprehensive examination of the Tully-Fisher relation.","To attain robust results, we employed a novel orthogonal likelihood fitting technique designed to minimize intrinsic scatter around the best-fit line, as required at \\hz.","For the STFR, we obtained a slope of $\\alpha=3.03\\pm 0.25$, an offset of $\\beta = 3.34\\pm 0.53$, and an intrinsic scatter of $\\zeta_{int}=0.08$ dex.","Correspondingly, the BTFR yielded $\\alpha=3.21\\pm 0.28$, $\\beta=3.16\\pm 0.61$, and $\\zeta_{int}=0.09$ dex.","Our findings suggest a subtle deviation in the stellar and baryonic Tully-Fisher relation with respect to local studies, which is most-likely due to the evolutionary processes governing disk formation."],"url":"http://arxiv.org/abs/2406.08934v1","category":"astro-ph.GA"}
{"created":"2024-06-13 08:50:31","title":"Overtone Transition $2\u03bd_1$ of $\\text{HCO}^+$ and $\\text{HOC}^+$: Origin, Radiative Lifetime, Collisional Quenching","abstract":"We present spectra of the first overtone vibration transition of $\\text{C-H}$/$\\text{O-H}$ stretch ($2\\nu_1$) in $\\text{HCO}^+$ and $\\text{HOC}^+$, recorded using a laser induced reaction action scheme inside a cryogenic 22 pole radio frequency trap. Band origins have been located at 6078.68411(19) and 6360.17630(26) $\\text{cm}^{-1}$, respectively. We introduce a technique based on mass selective ejection from the ion trap for recording background free action spectra. Varying the number density of the neutral action scheme reactant ($\\text{CO}_2$ and Ar, respectively) and collisional partner reactant inside the ion trap, permitted us to estimate the radiative lifetime of the state to be 1.53(34) and 1.22(34) ms, respectively, and the collisional quenching rates of $\\text{HCO}^+$ ($2\\nu_1$) with He, H$_2$, and N$_2$.","sentences":["We present spectra of the first overtone vibration transition of $\\text{C-H}$/$\\text{O-H}$ stretch ($2\\nu_1$) in $\\text{HCO}^+$ and $\\text{HOC}^+$, recorded using a laser induced reaction action scheme inside a cryogenic 22 pole radio frequency trap.","Band origins have been located at 6078.68411(19) and 6360.17630(26) $\\text{cm}^{-1}$, respectively.","We introduce a technique based on mass selective ejection from the ion trap for recording background free action spectra.","Varying the number density of the neutral action scheme reactant ($\\text{CO}_2$ and Ar, respectively) and collisional partner reactant inside the ion trap, permitted us to estimate the radiative lifetime of the state to be 1.53(34) and 1.22(34) ms, respectively, and the collisional quenching rates of $\\text{HCO}^+$ ($2\\nu_1$) with He, H$_2$, and N$_2$."],"url":"http://arxiv.org/abs/2406.08927v1","category":"physics.atom-ph"}
{"created":"2024-06-13 08:35:39","title":"AuriDESI: Mock Catalogues for the DESI Milky Way Survey","abstract":"The Dark Energy Spectroscopic Instrument Milky Way Survey (DESI MWS) will explore the assembly history of the Milky Way by characterising remnants of ancient dwarf galaxy accretion events and improving constraints on the distribution of dark matter in the outer halo. We present mock catalogues that reproduce the selection criteria of MWS and the format of the final MWS data set. These catalogues can be used to test methods for quantifying the properties of stellar halo substructure and reconstructing the Milky Way's accretion history with the MWS data, including the effects of halo-to-halo variance. The mock catalogues are based on a phase-space kernel expansion technique applied to star particles in the Auriga suite of six high-resolution $\\Lambda$CDM magneto-hydrodynamic zoom-in simulations. They include photometric properties (and associated errors) used in DESI target selection and the outputs of the MWS spectral analysis pipeline (radial velocity, metallicity, surface gravity, and temperature). They also include information from the underlying simulation, such as the total gravitational potential and information on the progenitors of accreted halo stars. We discuss how the subset of halo stars observable by MWS in these simulations corresponds to their true content and properties. These mock Milky Ways have rich accretion histories, resulting in a large number of substructures that span the whole stellar halo out to large distances and have substantial overlap in the space of orbital energy and angular momentum.","sentences":["The Dark Energy Spectroscopic Instrument Milky Way Survey (DESI MWS) will explore the assembly history of the Milky Way by characterising remnants of ancient dwarf galaxy accretion events and improving constraints on the distribution of dark matter in the outer halo.","We present mock catalogues that reproduce the selection criteria of MWS and the format of the final MWS data set.","These catalogues can be used to test methods for quantifying the properties of stellar halo substructure and reconstructing the Milky Way's accretion history with the MWS data, including the effects of halo-to-halo variance.","The mock catalogues are based on a phase-space kernel expansion technique applied to star particles in the Auriga suite of six high-resolution $\\Lambda$CDM magneto-hydrodynamic zoom-in simulations.","They include photometric properties (and associated errors) used in DESI target selection and the outputs of the MWS spectral analysis pipeline (radial velocity, metallicity, surface gravity, and temperature).","They also include information from the underlying simulation, such as the total gravitational potential and information on the progenitors of accreted halo stars.","We discuss how the subset of halo stars observable by MWS in these simulations corresponds to their true content and properties.","These mock Milky Ways have rich accretion histories, resulting in a large number of substructures that span the whole stellar halo out to large distances and have substantial overlap in the space of orbital energy and angular momentum."],"url":"http://arxiv.org/abs/2406.08921v1","category":"astro-ph.GA"}
{"created":"2024-06-13 08:12:48","title":"A Label-Free and Non-Monotonic Metric for Evaluating Denoising in Event Cameras","abstract":"Event cameras are renowned for their high efficiency due to outputting a sparse, asynchronous stream of events. However, they are plagued by noisy events, especially in low light conditions. Denoising is an essential task for event cameras, but evaluating denoising performance is challenging. Label-dependent denoising metrics involve artificially adding noise to clean sequences, complicating evaluations. Moreover, the majority of these metrics are monotonic, which can inflate scores by removing substantial noise and valid events. To overcome these limitations, we propose the first label-free and non-monotonic evaluation metric, the area of the continuous contrast curve (AOCC), which utilizes the area enclosed by event frame contrast curves across different time intervals. This metric is inspired by how events capture the edge contours of scenes or objects with high temporal resolution. An effective denoising method removes noise without eliminating these edge-contour events, thus preserving the contrast of event frames. Consequently, contrast across various time ranges serves as a metric to assess denoising effectiveness. As the time interval lengthens, the curve will initially rise and then fall. The proposed metric is validated through both theoretical and experimental evidence.","sentences":["Event cameras are renowned for their high efficiency due to outputting a sparse, asynchronous stream of events.","However, they are plagued by noisy events, especially in low light conditions.","Denoising is an essential task for event cameras, but evaluating denoising performance is challenging.","Label-dependent denoising metrics involve artificially adding noise to clean sequences, complicating evaluations.","Moreover, the majority of these metrics are monotonic, which can inflate scores by removing substantial noise and valid events.","To overcome these limitations, we propose the first label-free and non-monotonic evaluation metric, the area of the continuous contrast curve (AOCC), which utilizes the area enclosed by event frame contrast curves across different time intervals.","This metric is inspired by how events capture the edge contours of scenes or objects with high temporal resolution.","An effective denoising method removes noise without eliminating these edge-contour events, thus preserving the contrast of event frames.","Consequently, contrast across various time ranges serves as a metric to assess denoising effectiveness.","As the time interval lengthens, the curve will initially rise and then fall.","The proposed metric is validated through both theoretical and experimental evidence."],"url":"http://arxiv.org/abs/2406.08909v1","category":"cs.CV"}
{"created":"2024-06-13 07:41:02","title":"Bistability in filamentous actin through monomer-sequestration of an effector species","abstract":"Filamentous actin, a species of dynamic protein polymers, is one of the main components of the cytoskeleton of eukaryotic cells. We formulate a class of models that predict the possibility of bistable steady states in populations of dynamic actin filaments. They are built upon a basic model of actin dynamics that includes severing and capping in the presence of a finite actin monomer pool. The key additional ingredient is the presence of a single species of effector molecules that is partially sequestered to an inactive state by binding to free G-actin. In its unbound active state, this effector species can \\emph{enhance} the rate of nucleation of filamentous actin or its growth speed, or \\emph{inhibit} the activity of capping or severing proteins. Using an explicit analytical solution of the basic actin dynamics model, we show that bistability is predicted to occur in all of the proposed models. We verify these predictions using particle-based stochastic simulations. In addition, we show that switching between the two stable states can be achieved by transient manipulation of the free G-actin pool size.","sentences":["Filamentous actin, a species of dynamic protein polymers, is one of the main components of the cytoskeleton of eukaryotic cells.","We formulate a class of models that predict the possibility of bistable steady states in populations of dynamic actin filaments.","They are built upon a basic model of actin dynamics that includes severing and capping in the presence of a finite actin monomer pool.","The key additional ingredient is the presence of a single species of effector molecules that is partially sequestered to an inactive state by binding to free G-actin.","In its unbound active state, this effector species can \\emph{enhance} the rate of nucleation of filamentous actin or its growth speed, or \\emph{inhibit} the activity of capping or severing proteins.","Using an explicit analytical solution of the basic actin dynamics model, we show that bistability is predicted to occur in all of the proposed models.","We verify these predictions using particle-based stochastic simulations.","In addition, we show that switching between the two stable states can be achieved by transient manipulation of the free G-actin pool size."],"url":"http://arxiv.org/abs/2406.08886v1","category":"q-bio.BM"}
{"created":"2024-06-13 07:38:09","title":"Relation between Entropy, Diffusion and Relaxation Kinetics","abstract":"Intermolecular correlations lower values of both diffusion and entropy. We present an analysis of the existing relations between long-time diffusion (D) and entropy. S. A recently proposed inequality, a lower bound, by Sorkin et al., expresses the long-time diffusion in terms of diffusion in a reference state and the entropy difference. Such a relationship may provide a measure of intermolecular correlations. We show that for a one-dimensional rugged energy landscape, the lower bound becomes equality only if certain three-site correlations are neglected. When these correlations are included, we can derive an accurate expression that agrees with computer simulations. The strong dimensionality dependence of diffusion of a Brownian particle in a rugged energy landscape also resembles the recently proposed inequality. We show that for interacting colloids, a mode-coupling theory-type calculation of diffusion coefficient can be combined with the new inequality to estimate entropy change from long-range inter-colloid correlations. Interestingly, the rate of barrier crossing in a multidimensional potential energy surface, related to diffusion in a periodic lattice, is shown to admit a relation that depends on the entropy change in a way reminiscent of the relation between D and S.","sentences":["Intermolecular correlations lower values of both diffusion and entropy.","We present an analysis of the existing relations between long-time diffusion (D) and entropy.","S. A recently proposed inequality, a lower bound, by Sorkin et al., expresses the long-time diffusion in terms of diffusion in a reference state and the entropy difference.","Such a relationship may provide a measure of intermolecular correlations.","We show that for a one-dimensional rugged energy landscape, the lower bound becomes equality only if certain three-site correlations are neglected.","When these correlations are included, we can derive an accurate expression that agrees with computer simulations.","The strong dimensionality dependence of diffusion of a Brownian particle in a rugged energy landscape also resembles the recently proposed inequality.","We show that for interacting colloids, a mode-coupling theory-type calculation of diffusion coefficient can be combined with the new inequality to estimate entropy change from long-range inter-colloid correlations.","Interestingly, the rate of barrier crossing in a multidimensional potential energy surface, related to diffusion in a periodic lattice, is shown to admit a relation that depends on the entropy change in a way reminiscent of the relation between D and S."],"url":"http://arxiv.org/abs/2406.08885v1","category":"cond-mat.stat-mech"}
{"created":"2024-06-13 07:31:46","title":"Jackknife inference with two-way clustering","abstract":"For linear regression models with cross-section or panel data, it is natural to assume that the disturbances are clustered in two dimensions. However, the finite-sample properties of two-way cluster-robust tests and confidence intervals are often poor. We discuss several ways to improve inference with two-way clustering. Two of these are existing methods for avoiding, or at least ameliorating, the problem of undefined standard errors when a cluster-robust variance matrix estimator (CRVE) is not positive definite. One is a new method that always avoids the problem. More importantly, we propose a family of new two-way CRVEs based on the cluster jackknife. Simulations for models with two-way fixed effects suggest that, in many cases, the cluster-jackknife CRVE combined with our new method yields surprisingly accurate inferences. We provide a simple software package, twowayjack for Stata, that implements our recommended variance estimator.","sentences":["For linear regression models with cross-section or panel data, it is natural to assume that the disturbances are clustered in two dimensions.","However, the finite-sample properties of two-way cluster-robust tests and confidence intervals are often poor.","We discuss several ways to improve inference with two-way clustering.","Two of these are existing methods for avoiding, or at least ameliorating, the problem of undefined standard errors when a cluster-robust variance matrix estimator (CRVE) is not positive definite.","One is a new method that always avoids the problem.","More importantly, we propose a family of new two-way CRVEs based on the cluster jackknife.","Simulations for models with two-way fixed effects suggest that, in many cases, the cluster-jackknife CRVE combined with our new method yields surprisingly accurate inferences.","We provide a simple software package, twowayjack for Stata, that implements our recommended variance estimator."],"url":"http://arxiv.org/abs/2406.08880v1","category":"econ.EM"}
{"created":"2024-06-13 07:23:02","title":"Heuristics for Influence Maximization with Tiered Influence and Activation thresholds","abstract":"The information flows among the people while they communicate through social media websites. Due to the dependency on digital media, a person shares important information or regular updates with friends and family. The set of persons on social media forms a social network. Influence Maximization (IM) is a known problem in social networks. In social networks, information flows from one person to another using an underlying diffusion model. There are two fundamental diffusion models: the Independent Cascade Model (ICM) and the Linear Threshold Model (LTM). In this paper, we study a variant of the IM problem called Minimum Influential Seeds (MINFS) problem proposed by Qiang et al.[16]. It generalizes the classical IM problem with LTM as the diffusion model. Compared to IM, this variant has additional parameters: the influence threshold for each node and the propagation range. The propagation range is a positive integer that specifies how far the information can propagate from a node. A node on the network is not immediately influenced until it receives the same information from enough number of neighbors (influence threshold). Similarly, any node does not forward information until it receives the same information from a sufficient number of neighbors (activation threshold). Once a node becomes activated, it tries to activate or influence its neighbors. The MINFS problem aims to select the minimum number of initial spreader nodes such that all nodes of the graph are influenced. In this paper, we extend the study of the MINFS problem. We propose heuristics that construct seed sets based on the average degree of non-activated nodes, closest first, and backbone-based heaviest path.","sentences":["The information flows among the people while they communicate through social media websites.","Due to the dependency on digital media, a person shares important information or regular updates with friends and family.","The set of persons on social media forms a social network.","Influence Maximization (IM) is a known problem in social networks.","In social networks, information flows from one person to another using an underlying diffusion model.","There are two fundamental diffusion models: the Independent Cascade Model (ICM) and the Linear Threshold Model (LTM).","In this paper, we study a variant of the IM problem called Minimum Influential Seeds (MINFS) problem proposed by Qiang et al.[16].","It generalizes the classical IM problem with LTM as the diffusion model.","Compared to IM, this variant has additional parameters: the influence threshold for each node and the propagation range.","The propagation range is a positive integer that specifies how far the information can propagate from a node.","A node on the network is not immediately influenced until it receives the same information from enough number of neighbors (influence threshold).","Similarly, any node does not forward information until it receives the same information from a sufficient number of neighbors (activation threshold).","Once a node becomes activated, it tries to activate or influence its neighbors.","The MINFS problem aims to select the minimum number of initial spreader nodes such that all nodes of the graph are influenced.","In this paper, we extend the study of the MINFS problem.","We propose heuristics that construct seed sets based on the average degree of non-activated nodes, closest first, and backbone-based heaviest path."],"url":"http://arxiv.org/abs/2406.08876v1","category":"cs.SI"}
{"created":"2024-06-13 07:14:13","title":"W-boson pair production at lepton colliders in the Feynman-diagram gauge","abstract":"We calculate helicity amplitudes for $e^-e^+\\to W^-W^+$ analytically in the Feynman-diagram (FD) gauge. We show that, unlike in the unitary gauge, there is no energy growth of the individual Feynman amplitudes for the longitudinally polarized W bosons, and the contributions from the associated Goldstone bosons are manifest even without taking the high-energy limit. We also find that the physical distributions can be interpreted by the individual amplitudes in the FD gauge.","sentences":["We calculate helicity amplitudes for $e^-e^+\\to W^-W^+$ analytically in the Feynman-diagram (FD) gauge.","We show that, unlike in the unitary gauge, there is no energy growth of the individual Feynman amplitudes for the longitudinally polarized W bosons, and the contributions from the associated Goldstone bosons are manifest even without taking the high-energy limit.","We also find that the physical distributions can be interpreted by the individual amplitudes in the FD gauge."],"url":"http://arxiv.org/abs/2406.08869v1","category":"hep-ph"}
{"created":"2024-06-13 07:06:46","title":"Quantum phase transition of (1+1)-dimensional O(3) nonlinear sigma model at finite density with tensor renormalization group","abstract":"We study the quantum phase transition of the (1+1)-dimensional O(3) nonlinear sigma model at finite density using the tensor renormalization group method. This model suffers from the sign problem, which has prevented us from investigating the properties of the phase transition. We investigate the properties of the phase transition by changing the chemical potential $\\mu$ at a fixed coupling of $\\beta$. We determine the transition point $\\mu_{\\rm c}$ and the critical exponent $\\nu$ from the $\\mu$ dependence of the number density in the thermodynamic limit. The dynamical critical exponent $z$ is also extracted from the scaling behavior of the temporal correlation length as a function of $\\mu$.","sentences":["We study the quantum phase transition of the (1+1)-dimensional O(3) nonlinear sigma model at finite density using the tensor renormalization group method.","This model suffers from the sign problem, which has prevented us from investigating the properties of the phase transition.","We investigate the properties of the phase transition by changing the chemical potential $\\mu$ at a fixed coupling of $\\beta$. We determine the transition point $\\mu_{\\rm c}$ and the critical exponent $\\nu$ from the $\\mu$ dependence of the number density in the thermodynamic limit.","The dynamical critical exponent $z$ is also extracted from the scaling behavior of the temporal correlation length as a function of $\\mu$."],"url":"http://arxiv.org/abs/2406.08865v1","category":"hep-lat"}
{"created":"2024-06-13 06:50:00","title":"Spatially resolved analysis of Stellar Populations in NGC 2992: Impact of AGN feedback","abstract":"In NGC 2992, a galaxy-scale ionized gas outflow driven by AGN has long been recognized, yet its impact on the host galaxy has remained elusive. In this paper, we utilize data from the archival Very Large Telescope (VLT)/MUSE to present a spatially resolved analysis of stellar populations in this galaxy. Two different stellar population templates are employed to fit the stellar continuum, allowing us to determine the light-weighted stellar age, metallicity, the fraction of the young stellar population (age $<100$ Myr, $P_{\\rm Y}$), and the average age and metallicity of $P_{\\rm Y}$. Our results reveal the presence of a very young stellar population ($\\leq40$ Myr) within the dust lane and nearly along the galaxy's major axis. The light-weighted stellar age and the fraction of $P_{\\rm Y}$ show negative trends along the major and minor axes. The average age and metallicity of $P_{\\rm Y}$ present positive trends with increasing distance, except along the northern direction of the major axis. Within the circumnuclear region ($<1$ kpc), the distribution of the young stellar population is spatially anti-correlated with the AGN outflow cone. The highest fraction of $P_{\\rm Y}$ is observed at the outskirts of the nuclear radio bubble in the northern region near the nucleus. Considering the coupling efficiency and timescales, we propose that the AGN outflow in this galaxy may exert both negative and positive feedback on its host. Additionally, the star formation and the AGN activities could be attributed to the interaction between NGC 2992 and NGC 2993.","sentences":["In NGC 2992, a galaxy-scale ionized gas outflow driven by AGN has long been recognized, yet its impact on the host galaxy has remained elusive.","In this paper, we utilize data from the archival Very Large Telescope (VLT)/MUSE to present a spatially resolved analysis of stellar populations in this galaxy.","Two different stellar population templates are employed to fit the stellar continuum, allowing us to determine the light-weighted stellar age, metallicity, the fraction of the young stellar population (age $<100$ Myr, $P_{\\rm Y}$), and the average age and metallicity of $P_{\\rm Y}$.","Our results reveal the presence of a very young stellar population ($\\leq40$ Myr) within the dust lane and nearly along the galaxy's major axis.","The light-weighted stellar age and the fraction of $P_{\\rm Y}$ show negative trends along the major and minor axes.","The average age and metallicity of $P_{\\rm Y}$ present positive trends with increasing distance, except along the northern direction of the major axis.","Within the circumnuclear region ($<1$ kpc), the distribution of the young stellar population is spatially anti-correlated with the AGN outflow cone.","The highest fraction of $P_{\\rm Y}$ is observed at the outskirts of the nuclear radio bubble in the northern region near the nucleus.","Considering the coupling efficiency and timescales, we propose that the AGN outflow in this galaxy may exert both negative and positive feedback on its host.","Additionally, the star formation and the AGN activities could be attributed to the interaction between NGC 2992 and NGC 2993."],"url":"http://arxiv.org/abs/2406.08861v1","category":"astro-ph.GA"}
{"created":"2024-06-13 06:41:56","title":"Energy-momentum tensor in $\u03a6^4$ theory at one loop","abstract":"The energy-momentum tensor form factors are studied in $\\Phi^4$ theory to one-loop order with particular focus on the $D$-term, a particle property which has attracted a lot of attention in the recent literature. It is shown that the free Klein-Gordon theory value of the $D$-term $D_{\\text{free}}=-1$ is reduced to $D_{\\text{one-loop}} =-\\frac13$ even if the $\\Phi^4$ interaction is infinitesimally small. A companion work in $\\Phi^3$ theory confirms this result which may indicate that it is independent of the type of interaction as long as the scalar theory is renormalizable. Dispersion relations are studied. Various definitions of mean square radii including the mechanical radius are investigated. The findings contribute to a better understanding of the EMT properties of particles and their interpretation.","sentences":["The energy-momentum tensor form factors are studied in $\\Phi^4$ theory to one-loop order with particular focus on the $D$-term, a particle property which has attracted a lot of attention in the recent literature.","It is shown that the free Klein-Gordon theory value of the $D$-term $D_{\\text{free}}=-1$ is reduced to $D_{\\text{one-loop}} =-\\frac13$ even if the $\\Phi^4$ interaction is infinitesimally small.","A companion work in $\\Phi^3$ theory confirms this result which may indicate that it is independent of the type of interaction as long as the scalar theory is renormalizable.","Dispersion relations are studied.","Various definitions of mean square radii including the mechanical radius are investigated.","The findings contribute to a better understanding of the EMT properties of particles and their interpretation."],"url":"http://arxiv.org/abs/2406.08857v1","category":"hep-ph"}
{"created":"2024-06-13 05:57:54","title":"A Single-Step Non-Autoregressive Automatic Speech Recognition Architecture with High Accuracy and Inference Speed","abstract":"Non-autoregressive (NAR) automatic speech recognition (ASR) models predict tokens independently and simultaneously, bringing high inference speed. However, there is still a gap in the accuracy of the NAR models compared to the autoregressive (AR) models. To further narrow the gap between the NAR and AR models, we propose a single-step NAR ASR architecture with high accuracy and inference speed, called EfficientASR. It uses an Index Mapping Vector (IMV) based alignment generator to generate alignments during training, and an alignment predictor to learn the alignments for inference. It can be trained end-to-end (E2E) with cross-entropy loss combined with alignment loss. The proposed EfficientASR achieves competitive results on the AISHELL-1 and AISHELL-2 benchmarks compared to the state-of-the-art (SOTA) models. Specifically, it achieves character error rates (CER) of 4.26%/4.62% on the AISHELL-1 dev/test dataset, which outperforms the SOTA AR Conformer with about 30x inference speedup.","sentences":["Non-autoregressive (NAR) automatic speech recognition (ASR) models predict tokens independently and simultaneously, bringing high inference speed.","However, there is still a gap in the accuracy of the NAR models compared to the autoregressive (AR) models.","To further narrow the gap between the NAR and AR models, we propose a single-step NAR ASR architecture with high accuracy and inference speed, called EfficientASR.","It uses an Index Mapping Vector (IMV) based alignment generator to generate alignments during training, and an alignment predictor to learn the alignments for inference.","It can be trained end-to-end (E2E) with cross-entropy loss combined with alignment loss.","The proposed EfficientASR achieves competitive results on the AISHELL-1 and AISHELL-2 benchmarks compared to the state-of-the-art (SOTA) models.","Specifically, it achieves character error rates (CER) of 4.26%/4.62% on the AISHELL-1 dev/test dataset, which outperforms the SOTA AR Conformer with about 30x inference speedup."],"url":"http://arxiv.org/abs/2406.08835v1","category":"cs.SD"}
{"created":"2024-06-13 05:53:36","title":"Exponential gravity with logarithmic corrections in the presence of axion dark matter","abstract":"An exponential modified gravity with additional logarithmic corrections is considered with the presence of an axion-like scalar field in the role of dark matter. Axion fields are thought to become important at late-times when the axion-like scalar field oscillates around its vacuum expectation value, mimicking dark matter behaviour. The model is compared with the usual pressureless fluid description of dark matter. Both models are tested with observational data including some of the latest sources, providing similar fits in comparison with the $\\Lambda$CDM model. Despite results are not statistically relevant to rule out any model, the number of free parameters still favours $\\Lambda$CDM model, as shown by computing the goodness of the fits.","sentences":["An exponential modified gravity with additional logarithmic corrections is considered with the presence of an axion-like scalar field in the role of dark matter.","Axion fields are thought to become important at late-times when the axion-like scalar field oscillates around its vacuum expectation value, mimicking dark matter behaviour.","The model is compared with the usual pressureless fluid description of dark matter.","Both models are tested with observational data including some of the latest sources, providing similar fits in comparison with the $\\Lambda$CDM model.","Despite results are not statistically relevant to rule out any model, the number of free parameters still favours $\\Lambda$CDM model, as shown by computing the goodness of the fits."],"url":"http://arxiv.org/abs/2406.08831v1","category":"gr-qc"}
{"created":"2024-06-13 05:23:24","title":"TNSA based proton acceleration by two oblique laser pulses in the presence of an axial magnetic field","abstract":"A recently proposed strategy to boost the proton/ion cutoff energy in the target normal sheath acceleration scheme employs two obliquely incident laser pulses simultaneously irradiating the flat target rather than a single normally incident laser pulse of twice the pulse energy. Moreover, the presence of an externally applied magnetic field along the normal of the target's rear surface is known to reduce the angular divergence of hot electrons which results in a more efficient sheath field at the target rear leading to increased cutoff energy of accelerated protons/ions. In the present work, we employ two-dimensional Particle-In-Cell (PIC) simulations to examine, in detail, the effect of such a magnetic field on the cutoff energy of protons/ions in the cases of normal as well as oblique incidence of the laser pulse on a flat target. It is shown that the two-oblique-pulse configuration combined with an external magnetic field results in a stronger enhancement of the cutoff energies as compared to the normal incidence case.","sentences":["A recently proposed strategy to boost the proton/ion cutoff energy in the target normal sheath acceleration scheme employs two obliquely incident laser pulses simultaneously irradiating the flat target rather than a single normally incident laser pulse of twice the pulse energy.","Moreover, the presence of an externally applied magnetic field along the normal of the target's rear surface is known to reduce the angular divergence of hot electrons which results in a more efficient sheath field at the target rear leading to increased cutoff energy of accelerated protons/ions.","In the present work, we employ two-dimensional Particle-In-Cell (PIC) simulations to examine, in detail, the effect of such a magnetic field on the cutoff energy of protons/ions in the cases of normal as well as oblique incidence of the laser pulse on a flat target.","It is shown that the two-oblique-pulse configuration combined with an external magnetic field results in a stronger enhancement of the cutoff energies as compared to the normal incidence case."],"url":"http://arxiv.org/abs/2406.08821v1","category":"physics.plasm-ph"}
{"created":"2024-06-13 04:46:29","title":"Optimising an Array of Cherenkov Telescopes in Australia for the Detection of TeV Gamma-Ray Transients","abstract":"As TeV gamma-ray astronomy progresses into the era of the Cherenkov Telescope Array (CTA), instantaneously following up on gamma-ray transients is becoming more important than ever. To this end, a worldwide network of Imaging Atmospheric Cherenkov Telescopes has been proposed. Australia is ideally suited to provide coverage of part of the Southern Hemisphere sky inaccessible to H.E.S.S. in Namibia and the upcoming CTA-South in Chile. This study assesses the sources detectable by a small, transient-focused array in Australia based on CTA telescope designs. The TeV emission of extragalactic sources (including the majority of gamma-ray transients) can suffer significant absorption by the extragalactic background light. As such, we explored the improvements possible by implementing stereoscopic and topological triggers, as well as lowered image cleaning thresholds, to access lower energies. We modelled flaring gamma-ray sources based on past measurements from the satellite-based gamma-ray telescope Fermi-LAT. We estimate that an array of four Medium-Sized Telescopes (MSTs) would detect $\\sim$24 active galactic nucleus flares >5$\\sigma$ per year, up to a redshift of $z\\approx1.5$. Two MSTs achieved $\\sim$80-90% of the detections of four MSTs. The modelled Galactic transients were detectable within the observation time of one night, 11 of the 21 modelled gamma-ray bursts were detectable, as were $\\sim$10% of unidentified transients. An array of MST-class telescopes would thus be a valuable complementary telescope array for transient TeV gamma-ray astronomy.","sentences":["As TeV gamma-ray astronomy progresses into the era of the Cherenkov Telescope Array (CTA), instantaneously following up on gamma-ray transients is becoming more important than ever.","To this end, a worldwide network of Imaging Atmospheric Cherenkov Telescopes has been proposed.","Australia is ideally suited to provide coverage of part of the Southern Hemisphere sky inaccessible to H.E.S.S. in Namibia and the upcoming CTA-South in Chile.","This study assesses the sources detectable by a small, transient-focused array in Australia based on CTA telescope designs.","The TeV emission of extragalactic sources (including the majority of gamma-ray transients) can suffer significant absorption by the extragalactic background light.","As such, we explored the improvements possible by implementing stereoscopic and topological triggers, as well as lowered image cleaning thresholds, to access lower energies.","We modelled flaring gamma-ray sources based on past measurements from the satellite-based gamma-ray telescope Fermi-LAT.","We estimate that an array of four Medium-Sized Telescopes (MSTs) would detect $\\sim$24 active galactic nucleus flares >5$\\sigma$ per year, up to a redshift of $z\\approx1.5$. Two MSTs achieved $\\sim$80-90% of the detections of four MSTs.","The modelled Galactic transients were detectable within the observation time of one night, 11 of the 21 modelled gamma-ray bursts were detectable, as were $\\sim$10% of unidentified transients.","An array of MST-class telescopes would thus be a valuable complementary telescope array for transient TeV gamma-ray astronomy."],"url":"http://arxiv.org/abs/2406.08807v1","category":"astro-ph.IM"}
{"created":"2024-06-13 04:10:17","title":"Deep Exploration of Cross-Lingual Zero-Shot Generalization in Instruction Tuning","abstract":"Instruction tuning has emerged as a powerful technique, significantly boosting zero-shot performance on unseen tasks. While recent work has explored cross-lingual generalization by applying instruction tuning to multilingual models, previous studies have primarily focused on English, with a limited exploration of non-English tasks. For an in-depth exploration of cross-lingual generalization in instruction tuning, we perform instruction tuning individually for two distinct language meta-datasets. Subsequently, we assess the performance on unseen tasks in a language different from the one used for training. To facilitate this investigation, we introduce a novel non-English meta-dataset named \"KORANI\" (Korean Natural Instruction), comprising 51 Korean benchmarks. Moreover, we design cross-lingual templates to mitigate discrepancies in language and instruction-format of the template between training and inference within the cross-lingual setting. Our experiments reveal consistent improvements through cross-lingual generalization in both English and Korean, outperforming baseline by average scores of 20.7\\% and 13.6\\%, respectively. Remarkably, these enhancements are comparable to those achieved by monolingual instruction tuning and even surpass them in some tasks. The result underscores the significance of relevant data acquisition across languages over linguistic congruence with unseen tasks during instruction tuning.","sentences":["Instruction tuning has emerged as a powerful technique, significantly boosting zero-shot performance on unseen tasks.","While recent work has explored cross-lingual generalization by applying instruction tuning to multilingual models, previous studies have primarily focused on English, with a limited exploration of non-English tasks.","For an in-depth exploration of cross-lingual generalization in instruction tuning, we perform instruction tuning individually for two distinct language meta-datasets.","Subsequently, we assess the performance on unseen tasks in a language different from the one used for training.","To facilitate this investigation, we introduce a novel non-English meta-dataset named \"KORANI\" (Korean Natural Instruction), comprising 51 Korean benchmarks.","Moreover, we design cross-lingual templates to mitigate discrepancies in language and instruction-format of the template between training and inference within the cross-lingual setting.","Our experiments reveal consistent improvements through cross-lingual generalization in both English and Korean, outperforming baseline by average scores of 20.7\\% and 13.6\\%, respectively.","Remarkably, these enhancements are comparable to those achieved by monolingual instruction tuning and even surpass them in some tasks.","The result underscores the significance of relevant data acquisition across languages over linguistic congruence with unseen tasks during instruction tuning."],"url":"http://arxiv.org/abs/2406.08796v1","category":"cs.CL"}
{"created":"2024-06-13 02:57:02","title":"Electric field controlled valley-polarized photocurrent switch based on the circular bulk photovoltaic effect","abstract":"Efficient electric manipulation of valley degrees of freedom is critical and challenging for the advancement of valley-based information science and technology. We put forth an electrical scheme, based on a two-band Dirac model, that can switch the fully valley-polarized photocurrent between K and K' valleys using the circular bulk electro-photovoltaic effect. This is accomplished by applying an out-of-plane electric field to the two-dimensional valley materials, which enables continuous tuning of the Berry curvature and its sign flip. We found that the switch of the fully valley-polarized photocurrent is directly tied to the sign change of Berry curvature, which accompanies a topological phase transition, for instance, the quantum spin Hall effect and the quantum valley Hall effect. This scheme has been confirmed in monolayer BiAsI_2 and germanene through first-principles calculations. Our paper offers a promising strategy for the development of a volatile valley-addressable memory device and could inspire further research in this area.","sentences":["Efficient electric manipulation of valley degrees of freedom is critical and challenging for the advancement of valley-based information science and technology.","We put forth an electrical scheme, based on a two-band Dirac model, that can switch the fully valley-polarized photocurrent between K and K' valleys using the circular bulk electro-photovoltaic effect.","This is accomplished by applying an out-of-plane electric field to the two-dimensional valley materials, which enables continuous tuning of the Berry curvature and its sign flip.","We found that the switch of the fully valley-polarized photocurrent is directly tied to the sign change of Berry curvature, which accompanies a topological phase transition, for instance, the quantum spin Hall effect and the quantum valley Hall effect.","This scheme has been confirmed in monolayer BiAsI_2 and germanene through first-principles calculations.","Our paper offers a promising strategy for the development of a volatile valley-addressable memory device and could inspire further research in this area."],"url":"http://arxiv.org/abs/2406.08768v1","category":"cond-mat.mes-hall"}
{"created":"2024-06-13 02:51:14","title":"Numerical Insights into noise amplification of high-energy mid-infrared supercontinuum generation in normal dispersion multimode fibers","abstract":"We report on the noise properties of high-energy mid-infrared supercontinuum (MIR-SC) generation in normal dispersion multimode fibers from the numerical perspective. Noise amplification in multi-modes is primarily due to the stimulated Raman scattering (SRS) effect. This leads to the emergence of \"incoherent cloud formation\" and \"incoherent optical wave breaking\", similar to those observed in single-mode fibers. Increasing the pump technical noise from 0.1 % to 1 % significantly shortens the lumped coherence length L_C and exacerbates the influence of incoherent broadening dynamics competing with coherent dynamics, resulting in MIR-SC being a strong consistency in the collapse evolution of amplitude noise and phase coherence. To minimize this noise amplification and achieve high-energy low-noise MIR-SC in practical applications, it is essential to use short-pulse pumping with low amplitude noise, ensuring that L_C>>L_OWB (where L_OWB denotes the optical wave breaking length).","sentences":["We report on the noise properties of high-energy mid-infrared supercontinuum (MIR-SC) generation in normal dispersion multimode fibers from the numerical perspective.","Noise amplification in multi-modes is primarily due to the stimulated Raman scattering (SRS) effect.","This leads to the emergence of \"incoherent cloud formation\" and \"incoherent optical wave breaking\", similar to those observed in single-mode fibers.","Increasing the pump technical noise from 0.1 % to 1 % significantly shortens the lumped coherence length L_C and exacerbates the influence of incoherent broadening dynamics competing with coherent dynamics, resulting in MIR-SC being a strong consistency in the collapse evolution of amplitude noise and phase coherence.","To minimize this noise amplification and achieve high-energy low-noise MIR-SC in practical applications, it is essential to use short-pulse pumping with low amplitude noise, ensuring that L_C>>L_OWB (where L_OWB denotes the optical wave breaking length)."],"url":"http://arxiv.org/abs/2406.08764v1","category":"physics.optics"}
{"created":"2024-06-13 02:21:37","title":"Scotogenic gauge mechanism for neutrino mass and dark matter","abstract":"Scotogenic is a scheme for neutrino mass generation through the one-loop contribution of an inert scalar doublet and three sterile neutrinos. This work argues that such inert scalar doublet is a Goldstone boson mode associated with a gauge symmetry breaking. Hence, the resultant scotogenic gauge mechanism is very predictive, generating neutrino mass as contributed by a new gauge boson doublet that eats such Goldstone bosons. The dark matter stability is manifestly ensured by a matter parity as residual gauge symmetry for which a vector dark matter candidate is hinted.","sentences":["Scotogenic is a scheme for neutrino mass generation through the one-loop contribution of an inert scalar doublet and three sterile neutrinos.","This work argues that such inert scalar doublet is a Goldstone boson mode associated with a gauge symmetry breaking.","Hence, the resultant scotogenic gauge mechanism is very predictive, generating neutrino mass as contributed by a new gauge boson doublet that eats such Goldstone bosons.","The dark matter stability is manifestly ensured by a matter parity as residual gauge symmetry for which a vector dark matter candidate is hinted."],"url":"http://arxiv.org/abs/2406.08752v1","category":"hep-ph"}
