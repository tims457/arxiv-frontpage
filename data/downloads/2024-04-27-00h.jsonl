{"created":"2024-04-25 17:59:59","title":"Double Copy of 3D Chern-Simons Theory and 6D Kodaira-Spencer Gravity","abstract":"We apply an algebraic double copy construction of gravity from gauge theory to three-dimensional (3D) Chern-Simons theory. The kinematic algebra ${\\cal K}$ is the 3D de Rham complex of forms equipped, for a choice of metric, with a graded Lie algebra that is equivalent to the Schouten-Nijenhuis bracket on polyvector fields. The double copied gravity is defined on a subspace of ${\\cal K}\\otimes \\bar{\\cal K}$ and yields a topological double field theory for a generalized metric perturbation and two 2-forms. This local and gauge invariant theory is non-Lagrangian but can be rendered Lagrangian by abandoning locality. Upon fixing a gauge this reduces to the double copy of Chern-Simons theory previously proposed by Ben-Shahar and Johansson. Furthermore, using complex coordinates in $\\mathbb{C}^3$ this theory is related to six-dimensional (6D) Kodaira-Spencer gravity in that truncating the two 2-forms and one equation yields the Kodaira-Spencer equations on a 3D real slice of $\\mathbb{C}^3$. The full 6D Kodaira-Spencer theory can instead be obtained as a consistent truncation of a chiral double copy.","sentences":["We apply an algebraic double copy construction of gravity from gauge theory to three-dimensional (3D) Chern-Simons theory.","The kinematic algebra ${\\cal K}$ is the 3D de Rham complex of forms equipped, for a choice of metric, with a graded Lie algebra that is equivalent to the Schouten-Nijenhuis bracket on polyvector fields.","The double copied gravity is defined on a subspace of ${\\cal K}\\otimes \\bar{\\cal K}$ and yields a topological double field theory for a generalized metric perturbation and two 2-forms.","This local and gauge invariant theory is non-Lagrangian but can be rendered Lagrangian by abandoning locality.","Upon fixing a gauge this reduces to the double copy of Chern-Simons theory previously proposed by Ben-Shahar and Johansson.","Furthermore, using complex coordinates in $\\mathbb{C}^3$ this theory is related to six-dimensional (6D) Kodaira-Spencer gravity in that truncating the two 2-forms and one equation yields the Kodaira-Spencer equations on a 3D real slice of $\\mathbb{C}^3$. The full 6D Kodaira-Spencer theory can instead be obtained as a consistent truncation of a chiral double copy."],"url":"http://arxiv.org/abs/2404.16830v1","category":"hep-th"}
{"created":"2024-04-25 17:59:59","title":"The Third Monocular Depth Estimation Challenge","abstract":"This paper discusses the results of the third edition of the Monocular Depth Estimation Challenge (MDEC). The challenge focuses on zero-shot generalization to the challenging SYNS-Patches dataset, featuring complex scenes in natural and indoor settings. As with the previous edition, methods can use any form of supervision, i.e. supervised or self-supervised. The challenge received a total of 19 submissions outperforming the baseline on the test set: 10 among them submitted a report describing their approach, highlighting a diffused use of foundational models such as Depth Anything at the core of their method. The challenge winners drastically improved 3D F-Score performance, from 17.51% to 23.72%.","sentences":["This paper discusses the results of the third edition of the Monocular Depth Estimation Challenge (MDEC).","The challenge focuses on zero-shot generalization to the challenging SYNS-Patches dataset, featuring complex scenes in natural and indoor settings.","As with the previous edition, methods can use any form of supervision, i.e. supervised or self-supervised.","The challenge received a total of 19 submissions outperforming the baseline on the test set: 10 among them submitted a report describing their approach, highlighting a diffused use of foundational models such as Depth Anything at the core of their method.","The challenge winners drastically improved 3D F-Score performance, from 17.51% to 23.72%."],"url":"http://arxiv.org/abs/2404.16831v1","category":"cs.CV"}
{"created":"2024-04-25 17:59:58","title":"Make-it-Real: Unleashing Large Multimodal Model's Ability for Painting 3D Objects with Realistic Materials","abstract":"Physically realistic materials are pivotal in augmenting the realism of 3D assets across various applications and lighting conditions. However, existing 3D assets and generative models often lack authentic material properties. Manual assignment of materials using graphic software is a tedious and time-consuming task. In this paper, we exploit advancements in Multimodal Large Language Models (MLLMs), particularly GPT-4V, to present a novel approach, Make-it-Real: 1) We demonstrate that GPT-4V can effectively recognize and describe materials, allowing the construction of a detailed material library. 2) Utilizing a combination of visual cues and hierarchical text prompts, GPT-4V precisely identifies and aligns materials with the corresponding components of 3D objects. 3) The correctly matched materials are then meticulously applied as reference for the new SVBRDF material generation according to the original diffuse map, significantly enhancing their visual authenticity. Make-it-Real offers a streamlined integration into the 3D content creation workflow, showcasing its utility as an essential tool for developers of 3D assets.","sentences":["Physically realistic materials are pivotal in augmenting the realism of 3D assets across various applications and lighting conditions.","However, existing 3D assets and generative models often lack authentic material properties.","Manual assignment of materials using graphic software is a tedious and time-consuming task.","In this paper, we exploit advancements in Multimodal Large Language Models (MLLMs), particularly GPT-4V, to present a novel approach, Make-it-Real: 1) We demonstrate that GPT-4V can effectively recognize and describe materials, allowing the construction of a detailed material library.","2) Utilizing a combination of visual cues and hierarchical text prompts, GPT-4V precisely identifies and aligns materials with the corresponding components of 3D objects.","3)","The correctly matched materials are then meticulously applied as reference for the new SVBRDF material generation according to the original diffuse map, significantly enhancing their visual authenticity.","Make-it-Real offers a streamlined integration into the 3D content creation workflow, showcasing its utility as an essential tool for developers of 3D assets."],"url":"http://arxiv.org/abs/2404.16829v1","category":"cs.CV"}
{"created":"2024-04-25 17:59:56","title":"Made to Order: Discovering monotonic temporal changes via self-supervised video ordering","abstract":"Our objective is to discover and localize monotonic temporal changes in a sequence of images. To achieve this, we exploit a simple proxy task of ordering a shuffled image sequence, with `time' serving as a supervisory signal since only changes that are monotonic with time can give rise to the correct ordering. We also introduce a flexible transformer-based model for general-purpose ordering of image sequences of arbitrary length with built-in attribution maps. After training, the model successfully discovers and localizes monotonic changes while ignoring cyclic and stochastic ones. We demonstrate applications of the model in multiple video settings covering different scene and object types, discovering both object-level and environmental changes in unseen sequences. We also demonstrate that the attention-based attribution maps function as effective prompts for segmenting the changing regions, and that the learned representations can be used for downstream applications. Finally, we show that the model achieves the state of the art on standard benchmarks for ordering a set of images.","sentences":["Our objective is to discover and localize monotonic temporal changes in a sequence of images.","To achieve this, we exploit a simple proxy task of ordering a shuffled image sequence, with `time' serving as a supervisory signal since only changes that are monotonic with time can give rise to the correct ordering.","We also introduce a flexible transformer-based model for general-purpose ordering of image sequences of arbitrary length with built-in attribution maps.","After training, the model successfully discovers and localizes monotonic changes while ignoring cyclic and stochastic ones.","We demonstrate applications of the model in multiple video settings covering different scene and object types, discovering both object-level and environmental changes in unseen sequences.","We also demonstrate that the attention-based attribution maps function as effective prompts for segmenting the changing regions, and that the learned representations can be used for downstream applications.","Finally, we show that the model achieves the state of the art on standard benchmarks for ordering a set of images."],"url":"http://arxiv.org/abs/2404.16828v1","category":"cs.CV"}
{"created":"2024-04-25 17:59:47","title":"Successive Convexification for Trajectory Optimization with Continuous-Time Constraint Satisfaction","abstract":"We present successive convexification, a real-time-capable solution method for nonconvex trajectory optimization, with continuous-time constraint satisfaction and guaranteed convergence, that only requires first-order information. The proposed framework combines several key methods to solve a large class of nonlinear optimal control problems: (i) exterior penalty-based reformulation of the path constraints; (ii) generalized time-dilation; (iii) multiple-shooting discretization; (iv) $\\ell_1$ exact penalization of the nonconvex constraints; and (v) the prox-linear method, a sequential convex programming (SCP) algorithm for convex-composite minimization. The reformulation of the path constraints enables continuous-time constraint satisfaction even on sparse discretization grids and obviates the need for mesh refinement heuristics. Through the prox-linear method, we guarantee convergence of the solution method to stationary points of the penalized problem and guarantee that the converged solutions that are feasible with respect to the discretized and control-parameterized optimal control problem are also Karush-Kuhn-Tucker (KKT) points. Furthermore, we highlight the specialization of this property to global minimizers of convex optimal control problems, wherein the reformulated path constraints cannot be represented by canonical cones, i.e., in the form required by existing convex optimization solvers. In addition to theoretical analysis, we demonstrate the effectiveness and real-time capability of the proposed framework with numerical examples based on popular optimal control applications: dynamic obstacle avoidance and rocket landing.","sentences":["We present successive convexification, a real-time-capable solution method for nonconvex trajectory optimization, with continuous-time constraint satisfaction and guaranteed convergence, that only requires first-order information.","The proposed framework combines several key methods to solve a large class of nonlinear optimal control problems: (i) exterior penalty-based reformulation of the path constraints; (ii) generalized time-dilation; (iii) multiple-shooting discretization; (iv) $\\ell_1$ exact penalization of the nonconvex constraints; and (v) the prox-linear method, a sequential convex programming (SCP) algorithm for convex-composite minimization.","The reformulation of the path constraints enables continuous-time constraint satisfaction even on sparse discretization grids and obviates the need for mesh refinement heuristics.","Through the prox-linear method, we guarantee convergence of the solution method to stationary points of the penalized problem and guarantee that the converged solutions that are feasible with respect to the discretized and control-parameterized optimal control problem are also Karush-Kuhn-Tucker (KKT) points.","Furthermore, we highlight the specialization of this property to global minimizers of convex optimal control problems, wherein the reformulated path constraints cannot be represented by canonical cones, i.e., in the form required by existing convex optimization solvers.","In addition to theoretical analysis, we demonstrate the effectiveness and real-time capability of the proposed framework with numerical examples based on popular optimal control applications: dynamic obstacle avoidance and rocket landing."],"url":"http://arxiv.org/abs/2404.16826v1","category":"math.OC"}
{"created":"2024-04-25 17:59:45","title":"V2A-Mark: Versatile Deep Visual-Audio Watermarking for Manipulation Localization and Copyright Protection","abstract":"AI-generated video has revolutionized short video production, filmmaking, and personalized media, making video local editing an essential tool. However, this progress also blurs the line between reality and fiction, posing challenges in multimedia forensics. To solve this urgent issue, V2A-Mark is proposed to address the limitations of current video tampering forensics, such as poor generalizability, singular function, and single modality focus. Combining the fragility of video-into-video steganography with deep robust watermarking, our method can embed invisible visual-audio localization watermarks and copyright watermarks into the original video frames and audio, enabling precise manipulation localization and copyright protection. We also design a temporal alignment and fusion module and degradation prompt learning to enhance the localization accuracy and decoding robustness. Meanwhile, we introduce a sample-level audio localization method and a cross-modal copyright extraction mechanism to couple the information of audio and video frames. The effectiveness of V2A-Mark has been verified on a visual-audio tampering dataset, emphasizing its superiority in localization precision and copyright accuracy, crucial for the sustainable development of video editing in the AIGC video era.","sentences":["AI-generated video has revolutionized short video production, filmmaking, and personalized media, making video local editing an essential tool.","However, this progress also blurs the line between reality and fiction, posing challenges in multimedia forensics.","To solve this urgent issue, V2A-Mark is proposed to address the limitations of current video tampering forensics, such as poor generalizability, singular function, and single modality focus.","Combining the fragility of video-into-video steganography with deep robust watermarking, our method can embed invisible visual-audio localization watermarks and copyright watermarks into the original video frames and audio, enabling precise manipulation localization and copyright protection.","We also design a temporal alignment and fusion module and degradation prompt learning to enhance the localization accuracy and decoding robustness.","Meanwhile, we introduce a sample-level audio localization method and a cross-modal copyright extraction mechanism to couple the information of audio and video frames.","The effectiveness of V2A-Mark has been verified on a visual-audio tampering dataset, emphasizing its superiority in localization precision and copyright accuracy, crucial for the sustainable development of video editing in the AIGC video era."],"url":"http://arxiv.org/abs/2404.16824v1","category":"cs.CV"}
{"created":"2024-04-25 17:59:41","title":"Learning Visuotactile Skills with Two Multifingered Hands","abstract":"Aiming to replicate human-like dexterity, perceptual experiences, and motion patterns, we explore learning from human demonstrations using a bimanual system with multifingered hands and visuotactile data. Two significant challenges exist: the lack of an affordable and accessible teleoperation system suitable for a dual-arm setup with multifingered hands, and the scarcity of multifingered hand hardware equipped with touch sensing. To tackle the first challenge, we develop HATO, a low-cost hands-arms teleoperation system that leverages off-the-shelf electronics, complemented with a software suite that enables efficient data collection; the comprehensive software suite also supports multimodal data processing, scalable policy learning, and smooth policy deployment. To tackle the latter challenge, we introduce a novel hardware adaptation by repurposing two prosthetic hands equipped with touch sensors for research. Using visuotactile data collected from our system, we learn skills to complete long-horizon, high-precision tasks which are difficult to achieve without multifingered dexterity and touch feedback. Furthermore, we empirically investigate the effects of dataset size, sensing modality, and visual input preprocessing on policy learning. Our results mark a promising step forward in bimanual multifingered manipulation from visuotactile data. Videos, code, and datasets can be found at https://toruowo.github.io/hato/ .","sentences":["Aiming to replicate human-like dexterity, perceptual experiences, and motion patterns, we explore learning from human demonstrations using a bimanual system with multifingered hands and visuotactile data.","Two significant challenges exist: the lack of an affordable and accessible teleoperation system suitable for a dual-arm setup with multifingered hands, and the scarcity of multifingered hand hardware equipped with touch sensing.","To tackle the first challenge, we develop HATO, a low-cost hands-arms teleoperation system that leverages off-the-shelf electronics, complemented with a software suite that enables efficient data collection; the comprehensive software suite also supports multimodal data processing, scalable policy learning, and smooth policy deployment.","To tackle the latter challenge, we introduce a novel hardware adaptation by repurposing two prosthetic hands equipped with touch sensors for research.","Using visuotactile data collected from our system, we learn skills to complete long-horizon, high-precision tasks which are difficult to achieve without multifingered dexterity and touch feedback.","Furthermore, we empirically investigate the effects of dataset size, sensing modality, and visual input preprocessing on policy learning.","Our results mark a promising step forward in bimanual multifingered manipulation from visuotactile data.","Videos, code, and datasets can be found at https://toruowo.github.io/hato/ ."],"url":"http://arxiv.org/abs/2404.16823v1","category":"cs.RO"}
{"created":"2024-04-25 17:59:40","title":"Cosmological probes of Dark Radiation from Neutrino Mixing","abstract":"Models of stepped dark radiation have recently been found to have an important impact on the anisotropies of the cosmic microwave background, aiding in easing the Hubble tension. In this work, we study models with a sector of dark radiation with a step in its abundance, which thermalizes after big bang nucleosynthesis by mixing with the standard model neutrinos. For this, we extend an earlier work which has focused on the background evolution only until the dark sector thermalizes by deriving the full background and perturbation equations of the model and implementing them in an Einstein-Boltzmann solving code. We expound on the behavior of this model, discussing the wide range of parameters that result in interesting and viable cosmologies that dynamically generate dark radiation during a range of epochs. We find that for the strongly self-coupled regime, there is no large cosmological impact for a tight prior on the mass, whereas larger mass ranges allow a smooth interpolation between a behavior close to the $\\Lambda$CDM cosmological standard model and close to an additional component of strongly self-interacting dark radiation. In the weakly self-coupled regime we find that we can accommodate a parameter space relevant for the neutrino anomalies as well as one relevant to easing the Hubble tension.","sentences":["Models of stepped dark radiation have recently been found to have an important impact on the anisotropies of the cosmic microwave background, aiding in easing the Hubble tension.","In this work, we study models with a sector of dark radiation with a step in its abundance, which thermalizes after big bang nucleosynthesis by mixing with the standard model neutrinos.","For this, we extend an earlier work which has focused on the background evolution only until the dark sector thermalizes by deriving the full background and perturbation equations of the model and implementing them in an Einstein-Boltzmann solving code.","We expound on the behavior of this model, discussing the wide range of parameters that result in interesting and viable cosmologies that dynamically generate dark radiation during a range of epochs.","We find that for the strongly self-coupled regime, there is no large cosmological impact for a tight prior on the mass, whereas larger mass ranges allow a smooth interpolation between a behavior close to the $\\Lambda$CDM cosmological standard model and close to an additional component of strongly self-interacting dark radiation.","In the weakly self-coupled regime we find that we can accommodate a parameter space relevant for the neutrino anomalies as well as one relevant to easing the Hubble tension."],"url":"http://arxiv.org/abs/2404.16822v1","category":"astro-ph.CO"}
{"created":"2024-04-25 17:58:43","title":"Revisiting Text-to-Image Evaluation with Gecko: On Metrics, Prompts, and Human Ratings","abstract":"While text-to-image (T2I) generative models have become ubiquitous, they do not necessarily generate images that align with a given prompt. While previous work has evaluated T2I alignment by proposing metrics, benchmarks, and templates for collecting human judgements, the quality of these components is not systematically measured. Human-rated prompt sets are generally small and the reliability of the ratings -- and thereby the prompt set used to compare models -- is not evaluated. We address this gap by performing an extensive study evaluating auto-eval metrics and human templates. We provide three main contributions: (1) We introduce a comprehensive skills-based benchmark that can discriminate models across different human templates. This skills-based benchmark categorises prompts into sub-skills, allowing a practitioner to pinpoint not only which skills are challenging, but at what level of complexity a skill becomes challenging. (2) We gather human ratings across four templates and four T2I models for a total of >100K annotations. This allows us to understand where differences arise due to inherent ambiguity in the prompt and where they arise due to differences in metric and model quality. (3) Finally, we introduce a new QA-based auto-eval metric that is better correlated with human ratings than existing metrics for our new dataset, across different human templates, and on TIFA160.","sentences":["While text-to-image (T2I) generative models have become ubiquitous, they do not necessarily generate images that align with a given prompt.","While previous work has evaluated T2I alignment by proposing metrics, benchmarks, and templates for collecting human judgements, the quality of these components is not systematically measured.","Human-rated prompt sets are generally small and the reliability of the ratings -- and thereby the prompt set used to compare models -- is not evaluated.","We address this gap by performing an extensive study evaluating auto-eval metrics and human templates.","We provide three main contributions: (1) We introduce a comprehensive skills-based benchmark that can discriminate models across different human templates.","This skills-based benchmark categorises prompts into sub-skills, allowing a practitioner to pinpoint not only which skills are challenging, but at what level of complexity a skill becomes challenging.","(2) We gather human ratings across four templates and four T2I models for a total of >100K annotations.","This allows us to understand where differences arise due to inherent ambiguity in the prompt and where they arise due to differences in metric and model quality.","(3) Finally, we introduce a new QA-based auto-eval metric that is better correlated with human ratings than existing metrics for our new dataset, across different human templates, and on TIFA160."],"url":"http://arxiv.org/abs/2404.16820v1","category":"cs.CV"}
{"created":"2024-04-25 17:57:58","title":"Modified scattering for the cubic Schr\u00f6dinger equation on Diophantine waveguides","abstract":"We consider the cubic Schr\\\"odinger equation posed on product spaces subject to a generic Diophantine condition. Our analysis shows that the small-amplitude solutions undergo modified scattering to an effective dynamics governed by some interactions that do not amplify the Sobolev norms. This is in sharp contrast with the infinite energy cascade scenario observed by Hani--Pausader--Tzvetkov--Visciglia in the absence of Diophantine conditions.","sentences":["We consider the cubic Schr\\\"odinger equation posed on product spaces subject to a generic Diophantine condition.","Our analysis shows that the small-amplitude solutions undergo modified scattering to an effective dynamics governed by some interactions that do not amplify the Sobolev norms.","This is in sharp contrast with the infinite energy cascade scenario observed by Hani--Pausader--Tzvetkov--Visciglia in the absence of Diophantine conditions."],"url":"http://arxiv.org/abs/2404.16817v1","category":"math.AP"}
{"created":"2024-04-25 17:57:36","title":"IndicGenBench: A Multilingual Benchmark to Evaluate Generation Capabilities of LLMs on Indic Languages","abstract":"As large language models (LLMs) see increasing adoption across the globe, it is imperative for LLMs to be representative of the linguistic diversity of the world. India is a linguistically diverse country of 1.4 Billion people. To facilitate research on multilingual LLM evaluation, we release IndicGenBench - the largest benchmark for evaluating LLMs on user-facing generation tasks across a diverse set 29 of Indic languages covering 13 scripts and 4 language families. IndicGenBench is composed of diverse generation tasks like cross-lingual summarization, machine translation, and cross-lingual question answering. IndicGenBench extends existing benchmarks to many Indic languages through human curation providing multi-way parallel evaluation data for many under-represented Indic languages for the first time. We evaluate a wide range of proprietary and open-source LLMs including GPT-3.5, GPT-4, PaLM-2, mT5, Gemma, BLOOM and LLaMA on IndicGenBench in a variety of settings. The largest PaLM-2 models performs the best on most tasks, however, there is a significant performance gap in all languages compared to English showing that further research is needed for the development of more inclusive multilingual language models. IndicGenBench is released at www.github.com/google-research-datasets/indic-gen-bench","sentences":["As large language models (LLMs) see increasing adoption across the globe, it is imperative for LLMs to be representative of the linguistic diversity of the world.","India is a linguistically diverse country of 1.4 Billion people.","To facilitate research on multilingual LLM evaluation, we release IndicGenBench - the largest benchmark for evaluating LLMs on user-facing generation tasks across a diverse set 29 of Indic languages covering 13 scripts and 4 language families.","IndicGenBench is composed of diverse generation tasks like cross-lingual summarization, machine translation, and cross-lingual question answering.","IndicGenBench extends existing benchmarks to many Indic languages through human curation providing multi-way parallel evaluation data for many under-represented Indic languages for the first time.","We evaluate a wide range of proprietary and open-source LLMs including GPT-3.5, GPT-4, PaLM-2, mT5, Gemma, BLOOM and LLaMA on IndicGenBench in a variety of settings.","The largest PaLM-2 models performs the best on most tasks, however, there is a significant performance gap in all languages compared to English showing that further research is needed for the development of more inclusive multilingual language models.","IndicGenBench is released at www.github.com/google-research-datasets/indic-gen-bench"],"url":"http://arxiv.org/abs/2404.16816v1","category":"cs.CL"}
{"created":"2024-04-25 17:56:34","title":"Atmospheric Retrievals of the Phase-resolved Spectra of Irradiated Brown Dwarfs WD-0137B and EPIC-2122B","abstract":"We present an atmospheric retrieval analysis of HST/WFC3/G141 spectroscopic phase curve observations of two brown dwarfs, WD-0137B and EPIC-2122B, in ultra-short period orbits around white dwarf hosts. These systems are analogous to hot and ultra-hot Jupiter systems, enabling a unique and high-precision comparison to exoplanet systems. We use the PETRA retrieval suite to test various analysis setups, including joint-phase retrievals, multiple temperature structures, and non-uniform abundances. We find that WD-0137B has a dayside that closely resembles that of other ultra-hot Jupiters with inverted temperature structures and H$^-$ opacity, but quickly transitions to a mostly non-inverted temperature structure on the nightside. Meanwhile, EPIC-2122B's atmosphere remains inverted at all constrained longitudes, with dominant H$^-$ opacity. Retrievals with multiple temperature profiles and non-uniform vertical abundances were generally not statistically justified for this dataset, but retrievals with dayside-dilution factors were found to be justified. Retrieving all phases simultaneously with a linear combination of a dayside and nightside atmosphere was found to be an adequate representation of the entire phase-curve once a longitudinal temperature gradient free parameter was included in the retrieval. Comparing to global circulation models, we attribute behavior in the 1D retrievals to the inclined viewing geometry of the systems, which results in always-visible irradiated and inverted portions of the atmosphere \"contaminating\" spectra measured from the nightside hemisphere. This study sheds light on the similarities between these irradiated brown dwarf systems and hot and ultra-hot Jupiters, but also their unique differences, including the influence of the inclined viewing geometry.","sentences":["We present an atmospheric retrieval analysis of HST/WFC3/G141 spectroscopic phase curve observations of two brown dwarfs, WD-0137B and EPIC-2122B, in ultra-short period orbits around white dwarf hosts.","These systems are analogous to hot and ultra-hot Jupiter systems, enabling a unique and high-precision comparison to exoplanet systems.","We use the PETRA retrieval suite to test various analysis setups, including joint-phase retrievals, multiple temperature structures, and non-uniform abundances.","We find that WD-0137B has a dayside that closely resembles that of other ultra-hot Jupiters with inverted temperature structures and H$^-$ opacity, but quickly transitions to a mostly non-inverted temperature structure on the nightside.","Meanwhile, EPIC-2122B's atmosphere remains inverted at all constrained longitudes, with dominant H$^-$ opacity.","Retrievals with multiple temperature profiles and non-uniform vertical abundances were generally not statistically justified for this dataset, but retrievals with dayside-dilution factors were found to be justified.","Retrieving all phases simultaneously with a linear combination of a dayside and nightside atmosphere was found to be an adequate representation of the entire phase-curve once a longitudinal temperature gradient free parameter was included in the retrieval.","Comparing to global circulation models, we attribute behavior in the 1D retrievals to the inclined viewing geometry of the systems, which results in always-visible irradiated and inverted portions of the atmosphere \"contaminating\" spectra measured from the nightside hemisphere.","This study sheds light on the similarities between these irradiated brown dwarf systems and hot and ultra-hot Jupiters, but also their unique differences, including the influence of the inclined viewing geometry."],"url":"http://arxiv.org/abs/2404.16813v1","category":"astro-ph.SR"}
{"created":"2024-04-25 17:55:14","title":"Make Your LLM Fully Utilize the Context","abstract":"While many contemporary large language models (LLMs) can process lengthy input, they still struggle to fully utilize information within the long context, known as the lost-in-the-middle challenge. We hypothesize that it stems from insufficient explicit supervision during the long-context training, which fails to emphasize that any position in a long context can hold crucial information. Based on this intuition, our study presents information-intensive (IN2) training, a purely data-driven solution to overcome lost-in-the-middle. Specifically, IN2 training leverages a synthesized long-context question-answer dataset, where the answer requires (1) fine-grained information awareness on a short segment (~128 tokens) within a synthesized long context (4K-32K tokens), and (2) the integration and reasoning of information from two or more short segments. Through applying this information-intensive training on Mistral-7B, we present FILM-7B (FILl-in-the-Middle). To thoroughly assess the ability of FILM-7B for utilizing long contexts, we design three probing tasks that encompass various context styles (document, code, and structured-data context) and information retrieval patterns (forward, backward, and bi-directional retrieval). The probing results demonstrate that FILM-7B can robustly retrieve information from different positions in its 32K context window. Beyond these probing tasks, FILM-7B significantly improves the performance on real-world long-context tasks (e.g., 23.5->26.9 F1 score on NarrativeQA), while maintaining a comparable performance on short-context tasks (e.g., 59.3->59.2 accuracy on MMLU). Github Link: https://github.com/microsoft/FILM.","sentences":["While many contemporary large language models (LLMs) can process lengthy input, they still struggle to fully utilize information within the long context, known as the lost-in-the-middle challenge.","We hypothesize that it stems from insufficient explicit supervision during the long-context training, which fails to emphasize that any position in a long context can hold crucial information.","Based on this intuition, our study presents information-intensive (IN2) training, a purely data-driven solution to overcome lost-in-the-middle.","Specifically, IN2 training leverages a synthesized long-context question-answer dataset, where the answer requires (1) fine-grained information awareness on a short segment (~128 tokens) within a synthesized long context (4K-32K tokens), and (2) the integration and reasoning of information from two or more short segments.","Through applying this information-intensive training on Mistral-7B, we present FILM-7B (FILl-in-the-Middle).","To thoroughly assess the ability of FILM-7B for utilizing long contexts, we design three probing tasks that encompass various context styles (document, code, and structured-data context) and information retrieval patterns (forward, backward, and bi-directional retrieval).","The probing results demonstrate that FILM-7B can robustly retrieve information from different positions in its 32K context window.","Beyond these probing tasks, FILM-7B significantly improves the performance on real-world long-context tasks (e.g., 23.5->26.9 F1 score on NarrativeQA), while maintaining a comparable performance on short-context tasks (e.g., 59.3->59.2 accuracy on MMLU).","Github Link: https://github.com/microsoft/FILM."],"url":"http://arxiv.org/abs/2404.16811v1","category":"cs.CL"}
{"created":"2024-04-25 17:52:39","title":"Improving Diversity of Commonsense Generation by Large Language Models via In-Context Learning","abstract":"Generative Commonsense Reasoning (GCR) requires a model to reason about a situation using commonsense knowledge, while generating coherent sentences. Although the quality of the generated sentences is crucial, the diversity of the generation is equally important because it reflects the model's ability to use a range of commonsense knowledge facts. Large Language Models (LLMs) have shown proficiency in enhancing the generation quality across various tasks through in-context learning (ICL) using given examples without the need for any fine-tuning. However, the diversity aspect in LLM outputs has not been systematically studied before. To address this, we propose a simple method that diversifies the LLM generations, while preserving their quality. Experimental results on three benchmark GCR datasets show that our method achieves an ideal balance between the quality and diversity. Moreover, the sentences generated by our proposed method can be used as training data to improve diversity in existing commonsense generators.","sentences":["Generative Commonsense Reasoning (GCR) requires a model to reason about a situation using commonsense knowledge, while generating coherent sentences.","Although the quality of the generated sentences is crucial, the diversity of the generation is equally important because it reflects the model's ability to use a range of commonsense knowledge facts.","Large Language Models (LLMs) have shown proficiency in enhancing the generation quality across various tasks through in-context learning (ICL) using given examples without the need for any fine-tuning.","However, the diversity aspect in LLM outputs has not been systematically studied before.","To address this, we propose a simple method that diversifies the LLM generations, while preserving their quality.","Experimental results on three benchmark GCR datasets show that our method achieves an ideal balance between the quality and diversity.","Moreover, the sentences generated by our proposed method can be used as training data to improve diversity in existing commonsense generators."],"url":"http://arxiv.org/abs/2404.16807v1","category":"cs.CL"}
{"created":"2024-04-25 17:51:10","title":"AAPL: Adding Attributes to Prompt Learning for Vision-Language Models","abstract":"Recent advances in large pre-trained vision-language models have demonstrated remarkable performance on zero-shot downstream tasks. Building upon this, recent studies, such as CoOp and CoCoOp, have proposed the use of prompt learning, where context within a prompt is replaced with learnable vectors, leading to significant improvements over manually crafted prompts. However, the performance improvement for unseen classes is still marginal, and to tackle this problem, data augmentation has been frequently used in traditional zero-shot learning techniques. Through our experiments, we have identified important issues in CoOp and CoCoOp: the context learned through traditional image augmentation is biased toward seen classes, negatively impacting generalization to unseen classes. To address this problem, we propose adversarial token embedding to disentangle low-level visual augmentation features from high-level class information when inducing bias in learnable prompts. Through our novel mechanism called \"Adding Attributes to Prompt Learning\", AAPL, we guide the learnable context to effectively extract text features by focusing on high-level features for unseen classes. We have conducted experiments across 11 datasets, and overall, AAPL shows favorable performances compared to the existing methods in few-shot learning, zero-shot learning, cross-dataset, and domain generalization tasks.","sentences":["Recent advances in large pre-trained vision-language models have demonstrated remarkable performance on zero-shot downstream tasks.","Building upon this, recent studies, such as CoOp and CoCoOp, have proposed the use of prompt learning, where context within a prompt is replaced with learnable vectors, leading to significant improvements over manually crafted prompts.","However, the performance improvement for unseen classes is still marginal, and to tackle this problem, data augmentation has been frequently used in traditional zero-shot learning techniques.","Through our experiments, we have identified important issues in CoOp and CoCoOp: the context learned through traditional image augmentation is biased toward seen classes, negatively impacting generalization to unseen classes.","To address this problem, we propose adversarial token embedding to disentangle low-level visual augmentation features from high-level class information when inducing bias in learnable prompts.","Through our novel mechanism called \"Adding Attributes to Prompt Learning\", AAPL, we guide the learnable context to effectively extract text features by focusing on high-level features for unseen classes.","We have conducted experiments across 11 datasets, and overall, AAPL shows favorable performances compared to the existing methods in few-shot learning, zero-shot learning, cross-dataset, and domain generalization tasks."],"url":"http://arxiv.org/abs/2404.16804v1","category":"cs.CV"}
{"created":"2024-04-25 17:46:50","title":"Complementary asymptotic analysis for a minimal random walk","abstract":"We discuss a complementary asymptotic analysis of the so called minimal random walk. More precisely, we present a version of the almost sure central limit theorem as well as a generalization of the recently proposed quadratic strong laws. In addition, alternative demonstrations of the functional limit theorems will be supplied based on a P\\'olya urn scheme instead of a martingale approach.","sentences":["We discuss a complementary asymptotic analysis of the so called minimal random walk.","More precisely, we present a version of the almost sure central limit theorem as well as a generalization of the recently proposed quadratic strong laws.","In addition, alternative demonstrations of the functional limit theorems will be supplied based on a P\\'olya urn scheme instead of a martingale approach."],"url":"http://arxiv.org/abs/2404.16800v1","category":"math.PR"}
{"created":"2024-04-25 17:46:36","title":"Model-free inference of memory in conformational dynamics of a multi-domain protein","abstract":"Single-molecule experiments provide insight into the motion (conformational dynamics) of individual protein molecules. Usually, a well-defined but coarse-grained intramolecular coordinate is measured and subsequently analysed with the help of Hidden Markov Models (HMMs) to deduce the kinetics of protein conformational changes. Such approaches rely on the assumption that the microscopic dynamics of the protein evolve according to a Markov-jump process on some network. However, the manifestation and extent of memory in the dynamics of the observable strongly depends on the chosen underlying Markov model, which is generally not known and therefore can lead to misinterpretations. Here, we combine extensive single-molecule plasmon ruler experiments on the heat shock protein Hsp90, computer simulations, and theory to infer and quantify memory in a model-free fashion. Our analysis is based on the bare definition of non-Markovian behaviour and does not require any underlying model. In the case of Hsp90 probed by a plasmon ruler, the Markov assumption is found to be clearly and conclusively violated on timescales up to roughly 50 s, which corresponds roughly to $\\sim$50% of the inferred correlation time of the signal. The extent of memory is striking and reaches biologically relevant timescales. This implies that memory effects penetrate even the slowest observed motions. We provide clear and reproducible guidelines on how to test for the presence and duration of memory in experimental single-molecule data.","sentences":["Single-molecule experiments provide insight into the motion (conformational dynamics) of individual protein molecules.","Usually, a well-defined but coarse-grained intramolecular coordinate is measured and subsequently analysed with the help of Hidden Markov Models (HMMs) to deduce the kinetics of protein conformational changes.","Such approaches rely on the assumption that the microscopic dynamics of the protein evolve according to a Markov-jump process on some network.","However, the manifestation and extent of memory in the dynamics of the observable strongly depends on the chosen underlying Markov model, which is generally not known and therefore can lead to misinterpretations.","Here, we combine extensive single-molecule plasmon ruler experiments on the heat shock protein Hsp90, computer simulations, and theory to infer and quantify memory in a model-free fashion.","Our analysis is based on the bare definition of non-Markovian behaviour and does not require any underlying model.","In the case of Hsp90 probed by a plasmon ruler, the Markov assumption is found to be clearly and conclusively violated on timescales up to roughly 50 s, which corresponds roughly to $\\sim$50% of the inferred correlation time of the signal.","The extent of memory is striking and reaches biologically relevant timescales.","This implies that memory effects penetrate even the slowest observed motions.","We provide clear and reproducible guidelines on how to test for the presence and duration of memory in experimental single-molecule data."],"url":"http://arxiv.org/abs/2404.16799v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-25 17:43:29","title":"Spherical bispectrum expansion and quadratic estimators","abstract":"We describe a general expansion of spherical (full-sky) bispectra into a set of orthogonal modes. For squeezed shapes, the basis separates physically-distinct signals and is dominated by the lowest moments. In terms of reduced bispectra, we identify a set of discrete polynomials that are pairwise orthogonal with respect to the relevant Wigner 3j symbol, and reduce to Chebyshev polynomials in the flat-sky (high-momentum) limit for both parity-even and parity-odd cases. For squeezed shapes, the flat-sky limit is equivalent to previous moment expansions used for CMB bispectra and quadratic estimators, but in general reduces to a distinct expansion in the angular dependence of triangles at fixed total side length (momentum). We use the full-sky expansion to construct a tower of orthogonal CMB lensing quadratic estimators and construct estimators that are immune to foregrounds like point sources or noise inhomogeneities. In parity-even combinations (such as the lensing gradient mode from $TT$, or the lensing curl mode from $EB$) the leading two modes can be identified with information from the magnification and shear respectively, whereas the parity-odd combinations are shear-only. Although not directly separable, we show that these estimators can nonetheless be evaluated numerically sufficiently easily.","sentences":["We describe a general expansion of spherical (full-sky) bispectra into a set of orthogonal modes.","For squeezed shapes, the basis separates physically-distinct signals and is dominated by the lowest moments.","In terms of reduced bispectra, we identify a set of discrete polynomials that are pairwise orthogonal with respect to the relevant Wigner 3j symbol, and reduce to Chebyshev polynomials in the flat-sky (high-momentum) limit for both parity-even and parity-odd cases.","For squeezed shapes, the flat-sky limit is equivalent to previous moment expansions used for CMB bispectra and quadratic estimators, but in general reduces to a distinct expansion in the angular dependence of triangles at fixed total side length (momentum).","We use the full-sky expansion to construct a tower of orthogonal CMB lensing quadratic estimators and construct estimators that are immune to foregrounds like point sources or noise inhomogeneities.","In parity-even combinations (such as the lensing gradient mode from $TT$, or the lensing curl mode from $EB$) the leading two modes can be identified with information from the magnification and shear respectively, whereas the parity-odd combinations are shear-only.","Although not directly separable, we show that these estimators can nonetheless be evaluated numerically sufficiently easily."],"url":"http://arxiv.org/abs/2404.16797v1","category":"astro-ph.CO"}
{"created":"2024-04-25 17:42:32","title":"SAGBI and Gr\u00f6bner Bases Detection","abstract":"We introduce a detection algorithm for SAGBI basis in polynomial rings, analogous to a Gr\\\"obner basis detection algorithm previously proposed by Gritzmann and Sturmfels. We also present two accompanying software packages named SagbiGbDetection for Macaulay2 and Julia. Both packages allow the user to find one or more term orders for which a set of input polynomials form either Gr\\\"obner basis for the ideal they generate or a SAGBI basis for the subalgebra. Additionally, we investigate the computational complexity of homogeneous SAGBI detection and apply our implementation to several novel examples.","sentences":["We introduce a detection algorithm for SAGBI basis in polynomial rings, analogous to a Gr\\\"obner basis detection algorithm previously proposed by Gritzmann and Sturmfels.","We also present two accompanying software packages named SagbiGbDetection for Macaulay2 and Julia.","Both packages allow the user to find one or more term orders for which a set of input polynomials form either Gr\\\"obner basis for the ideal they generate or a SAGBI basis for the subalgebra.","Additionally, we investigate the computational complexity of homogeneous SAGBI detection and apply our implementation to several novel examples."],"url":"http://arxiv.org/abs/2404.16796v1","category":"math.AC"}
{"created":"2024-04-25 17:39:50","title":"Extreme points of general transportation polytopes","abstract":"Transportation matrices are $m\\times n$ non-negative matrices whose row sums and row columns are equal to, or dominated above with given integral vectors $R$ and $C$. Those matrices belong to a convex polytope whose extreme points have been previously characterized. In this article, a more general set of non-negative transportation matrices is considered, whose row sums are bounded by two integral non-negative vectors $R_{min}$ and $R_{max}$ and column sums are bounded by two integral non-negative vectors $C_{min}$ and $C_{max}$. It is shown that this set is also a convex polytope whose extreme points are then fully characterized.","sentences":["Transportation matrices are $m\\times n$ non-negative matrices whose row sums and row columns are equal to, or dominated above with given integral vectors $R$ and $C$.","Those matrices belong to a convex polytope whose extreme points have been previously characterized.","In this article, a more general set of non-negative transportation matrices is considered, whose row sums are bounded by two integral non-negative vectors $R_{min}$ and $R_{max}$ and column sums are bounded by two integral non-negative vectors $C_{min}$ and $C_{max}$. It is shown that this set is also a convex polytope whose extreme points are then fully characterized."],"url":"http://arxiv.org/abs/2404.16791v1","category":"math.CO"}
{"created":"2024-04-25 17:39:50","title":"Weak-to-Strong Extrapolation Expedites Alignment","abstract":"Although the capabilities of large language models (LLMs) ideally scale up with increasing data and compute, they are inevitably constrained by limited resources in reality. Suppose we have a moderately trained LLM (e.g., trained to align with human preference) in hand, can we further exploit its potential and cheaply acquire a stronger model? In this paper, we propose a simple method called ExPO to boost LLMs' alignment with human preference. ExPO assumes that a medium-aligned model can be interpolated between a less-aligned (weaker) model, e.g., the initial SFT model, and a better-aligned (stronger) one, thereby directly obtaining this stronger model by extrapolating from the weights of the former two relatively weaker models. On the AlpacaEval 2.0 benchmark, we show that ExPO pushes models trained with less preference data (e.g., 10% or 20%) to reach and even surpass the fully-trained one, without any additional training. Furthermore, ExPO also significantly improves off-the-shelf DPO/RLHF models and exhibits decent scalability across model sizes from 7B to 70B. Our work demonstrates the efficacy of model extrapolation in exploiting LLMs' capabilities, suggesting a promising direction that deserves future exploration.","sentences":["Although the capabilities of large language models (LLMs) ideally scale up with increasing data and compute, they are inevitably constrained by limited resources in reality.","Suppose we have a moderately trained LLM (e.g., trained to align with human preference) in hand, can we further exploit its potential and cheaply acquire a stronger model?","In this paper, we propose a simple method called ExPO to boost LLMs' alignment with human preference.","ExPO assumes that a medium-aligned model can be interpolated between a less-aligned (weaker) model, e.g., the initial SFT model, and a better-aligned (stronger) one, thereby directly obtaining this stronger model by extrapolating from the weights of the former two relatively weaker models.","On the AlpacaEval 2.0 benchmark, we show that ExPO pushes models trained with less preference data (e.g., 10% or 20%) to reach and even surpass the fully-trained one, without any additional training.","Furthermore, ExPO also significantly improves off-the-shelf DPO/RLHF models and exhibits decent scalability across model sizes from 7B to 70B.","Our work demonstrates the efficacy of model extrapolation in exploiting LLMs' capabilities, suggesting a promising direction that deserves future exploration."],"url":"http://arxiv.org/abs/2404.16792v1","category":"cs.LG"}
{"created":"2024-04-25 17:39:35","title":"SEED-Bench-2-Plus: Benchmarking Multimodal Large Language Models with Text-Rich Visual Comprehension","abstract":"Comprehending text-rich visual content is paramount for the practical application of Multimodal Large Language Models (MLLMs), since text-rich scenarios are ubiquitous in the real world, which are characterized by the presence of extensive texts embedded within images. Recently, the advent of MLLMs with impressive versatility has raised the bar for what we can expect from MLLMs. However, their proficiency in text-rich scenarios has yet to be comprehensively and objectively assessed, since current MLLM benchmarks primarily focus on evaluating general visual comprehension. In this work, we introduce SEED-Bench-2-Plus, a benchmark specifically designed for evaluating \\textbf{text-rich visual comprehension} of MLLMs. Our benchmark comprises 2.3K multiple-choice questions with precise human annotations, spanning three broad categories: Charts, Maps, and Webs, each of which covers a wide spectrum of text-rich scenarios in the real world. These categories, due to their inherent complexity and diversity, effectively simulate real-world text-rich environments. We further conduct a thorough evaluation involving 34 prominent MLLMs (including GPT-4V, Gemini-Pro-Vision and Claude-3-Opus) and emphasize the current limitations of MLLMs in text-rich visual comprehension. We hope that our work can serve as a valuable addition to existing MLLM benchmarks, providing insightful observations and inspiring further research in the area of text-rich visual comprehension with MLLMs. The dataset and evaluation code can be accessed at https://github.com/AILab-CVC/SEED-Bench.","sentences":["Comprehending text-rich visual content is paramount for the practical application of Multimodal Large Language Models (MLLMs), since text-rich scenarios are ubiquitous in the real world, which are characterized by the presence of extensive texts embedded within images.","Recently, the advent of MLLMs with impressive versatility has raised the bar for what we can expect from MLLMs.","However, their proficiency in text-rich scenarios has yet to be comprehensively and objectively assessed, since current MLLM benchmarks primarily focus on evaluating general visual comprehension.","In this work, we introduce SEED-Bench-2-Plus, a benchmark specifically designed for evaluating \\textbf{text-rich visual comprehension} of MLLMs.","Our benchmark comprises 2.3K multiple-choice questions with precise human annotations, spanning three broad categories: Charts, Maps, and Webs, each of which covers a wide spectrum of text-rich scenarios in the real world.","These categories, due to their inherent complexity and diversity, effectively simulate real-world text-rich environments.","We further conduct a thorough evaluation involving 34 prominent MLLMs (including GPT-4V, Gemini-Pro-Vision and Claude-3-Opus) and emphasize the current limitations of MLLMs in text-rich visual comprehension.","We hope that our work can serve as a valuable addition to existing MLLM benchmarks, providing insightful observations and inspiring further research in the area of text-rich visual comprehension with MLLMs.","The dataset and evaluation code can be accessed at https://github.com/AILab-CVC/SEED-Bench."],"url":"http://arxiv.org/abs/2404.16790v1","category":"cs.CV"}
{"created":"2024-04-25 17:38:57","title":"Continual Learning of Large Language Models: A Comprehensive Survey","abstract":"The recent success of large language models (LLMs) trained on static, pre-collected, general datasets has sparked numerous research directions and applications. One such direction addresses the non-trivial challenge of integrating pre-trained LLMs into dynamic data distributions, task structures, and user preferences. Pre-trained LLMs, when tailored for specific needs, often experience significant performance degradation in previous knowledge domains -- a phenomenon known as \"catastrophic forgetting\". While extensively studied in the continual learning (CL) community, it presents new manifestations in the realm of LLMs. In this survey, we provide a comprehensive overview of the current research progress on LLMs within the context of CL. This survey is structured into four main sections: we first describe an overview of continually learning LLMs, consisting of two directions of continuity: vertical continuity (or vertical continual learning), i.e., continual adaptation from general to specific capabilities, and horizontal continuity (or horizontal continual learning), i.e., continual adaptation across time and domains (Section 3). We then summarize three stages of learning LLMs in the context of modern CL: Continual Pre-Training (CPT), Domain-Adaptive Pre-training (DAP), and Continual Fine-Tuning (CFT) (Section 4). Then we provide an overview of evaluation protocols for continual learning with LLMs, along with the current available data sources (Section 5). Finally, we discuss intriguing questions pertaining to continual learning for LLMs (Section 6). The full list of papers examined in this survey is available at https://github.com/Wang-ML-Lab/llm-continual-learning-survey.","sentences":["The recent success of large language models (LLMs) trained on static, pre-collected, general datasets has sparked numerous research directions and applications.","One such direction addresses the non-trivial challenge of integrating pre-trained LLMs into dynamic data distributions, task structures, and user preferences.","Pre-trained LLMs, when tailored for specific needs, often experience significant performance degradation in previous knowledge domains -- a phenomenon known as \"catastrophic forgetting\".","While extensively studied in the continual learning (CL) community, it presents new manifestations in the realm of LLMs.","In this survey, we provide a comprehensive overview of the current research progress on LLMs within the context of CL.","This survey is structured into four main sections: we first describe an overview of continually learning LLMs, consisting of two directions of continuity: vertical continuity (or vertical continual learning), i.e., continual adaptation from general to specific capabilities, and horizontal continuity (or horizontal continual learning), i.e., continual adaptation across time and domains (Section 3).","We then summarize three stages of learning LLMs in the context of modern CL: Continual Pre-Training (CPT), Domain-Adaptive Pre-training (DAP), and Continual Fine-Tuning (CFT) (Section 4).","Then we provide an overview of evaluation protocols for continual learning with LLMs, along with the current available data sources (Section 5).","Finally, we discuss intriguing questions pertaining to continual learning for LLMs (Section 6).","The full list of papers examined in this survey is available at https://github.com/Wang-ML-Lab/llm-continual-learning-survey."],"url":"http://arxiv.org/abs/2404.16789v1","category":"cs.LG"}
{"created":"2024-04-25 17:32:55","title":"Harnessing Inferior Solutions For Superior Outcomes: Obtaining Robust Solutions From Quantum Algorithms","abstract":"In the rapidly advancing domain of quantum optimization, the confluence of quantum algorithms such as Quantum Annealing (QA) and the Quantum Approximate Optimization Algorithm (QAOA) with robust optimization methodologies presents a cutting-edge frontier. Although it seems natural to apply quantum algorithms when facing uncertainty, this has barely been approached.   In this paper we adapt the aforementioned quantum optimization techniques to tackle robust optimization problems. By leveraging the inherent stochasticity of quantum annealing and adjusting the parameters and evaluation functions within QAOA, we present two innovative methods for obtaining robust optimal solutions. These heuristics are applied on two use cases within the energy sector: the unit commitment problem, which is central to the scheduling of power plant operations, and the optimization of charging electric vehicles (EVs) including electricity from photovoltaic (PV) to minimize costs. These examples highlight not only the potential of quantum optimization methods to enhance decision-making in energy management but also the practical relevance of the young field of quantum computing in general. Through careful adaptation of quantum algorithms, we lay the foundation for exploring ways to achieve more reliable and efficient solutions in complex optimization scenarios that occur in the real-world.","sentences":["In the rapidly advancing domain of quantum optimization, the confluence of quantum algorithms such as Quantum Annealing (QA) and the Quantum Approximate Optimization Algorithm (QAOA) with robust optimization methodologies presents a cutting-edge frontier.","Although it seems natural to apply quantum algorithms when facing uncertainty, this has barely been approached.   ","In this paper we adapt the aforementioned quantum optimization techniques to tackle robust optimization problems.","By leveraging the inherent stochasticity of quantum annealing and adjusting the parameters and evaluation functions within QAOA, we present two innovative methods for obtaining robust optimal solutions.","These heuristics are applied on two use cases within the energy sector: the unit commitment problem, which is central to the scheduling of power plant operations, and the optimization of charging electric vehicles (EVs) including electricity from photovoltaic (PV) to minimize costs.","These examples highlight not only the potential of quantum optimization methods to enhance decision-making in energy management but also the practical relevance of the young field of quantum computing in general.","Through careful adaptation of quantum algorithms, we lay the foundation for exploring ways to achieve more reliable and efficient solutions in complex optimization scenarios that occur in the real-world."],"url":"http://arxiv.org/abs/2404.16784v1","category":"quant-ph"}
{"created":"2024-04-25 17:31:31","title":"Dual-isometric Projected Entangled Pair States","abstract":"Efficient characterization of higher dimensional many-body physical states presents significant challenges. In this paper, we propose a new class of Project Entangled Pair State (PEPS) that incorporates two isometric conditions. This new class facilitates the efficient calculation of general local observables and certain two-point correlation functions, which have been previously shown to be intractable for general PEPS, or PEPS with only a single isometric constraint. Despite incorporating two isometric conditions, our class preserves the rich physical structure while enhancing the analytical capabilities. It features a large set of tunable parameters, with only a subleading correction compared to that of general PEPS. Furthermore, we analytically demonstrate that this class can encode universal quantum computations and can represent a transition from topological to trivial order.","sentences":["Efficient characterization of higher dimensional many-body physical states presents significant challenges.","In this paper, we propose a new class of Project Entangled Pair State (PEPS) that incorporates two isometric conditions.","This new class facilitates the efficient calculation of general local observables and certain two-point correlation functions, which have been previously shown to be intractable for general PEPS, or PEPS with only a single isometric constraint.","Despite incorporating two isometric conditions, our class preserves the rich physical structure while enhancing the analytical capabilities.","It features a large set of tunable parameters, with only a subleading correction compared to that of general PEPS.","Furthermore, we analytically demonstrate that this class can encode universal quantum computations and can represent a transition from topological to trivial order."],"url":"http://arxiv.org/abs/2404.16783v1","category":"quant-ph"}
{"created":"2024-04-25 17:30:59","title":"Gromov-Witten Invariants and Mirror Symmetry for Non-Fano Varieties Using Scattering Diagrams","abstract":"Gromov-Witten invariants arise in the topological A-model as counts of worldsheet instantons. On the A-side, these invariants can be computed for a Fano or semi-Fano toric variety using generating functions associated to the toric divisors. On the B-side, the same invariants can be computed from the periods of the mirror. We utilize scattering diagrams (aka wall structures) in the Gross-Siebert mirror symmetry program to extend the calculation of Gromov-Witten invariants to non-Fano toric varieties. Following the work of Carl-Pumperla-Siebert, we compute corrected mirror superpotentials $\\vartheta_1(\\mathbb{F}_m)$ and their periods for the Hirzebruch surfaces $\\mathbb{F}_m$ with $m \\ge 2$.","sentences":["Gromov-Witten invariants arise in the topological A-model as counts of worldsheet instantons.","On the A-side, these invariants can be computed for a Fano or semi-Fano toric variety using generating functions associated to the toric divisors.","On the B-side, the same invariants can be computed from the periods of the mirror.","We utilize scattering diagrams (aka wall structures) in the Gross-Siebert mirror symmetry program to extend the calculation of Gromov-Witten invariants to non-Fano toric varieties.","Following the work of Carl-Pumperla-Siebert, we compute corrected mirror superpotentials $\\vartheta_1(\\mathbb{F}_m)$ and their periods for the Hirzebruch surfaces $\\mathbb{F}_m$ with $m \\ge 2$."],"url":"http://arxiv.org/abs/2404.16782v1","category":"math.AG"}
{"created":"2024-04-25 17:30:37","title":"Rapid thermalization of dissipative many-body dynamics of commuting Hamiltonians","abstract":"Quantum systems typically reach thermal equilibrium rather quickly when coupled to a thermal environment. The usual way of bounding the speed of this process is by estimating the spectral gap of the dissipative generator. However the gap, by itself, does not always yield a reasonable estimate for the thermalization time in many-body systems: without further structure, a uniform lower bound on it only constrains the thermalization time to grow polynomially with system size.   Here, instead, we show that for a large class of geometrically-2-local models of Davies generators with commuting Hamiltonians, the thermalization time is much shorter than one would na\\\"ively estimate from the gap: at most logarithmic in the system size. This yields the so-called rapid mixing of dissipative dynamics. The result is particularly relevant for 1D systems, for which we prove rapid thermalization with a system size independent decay rate only from a positive gap in the generator. We also prove that systems in hypercubic lattices of any dimension, and exponential graphs, such as trees, have rapid mixing at high enough temperatures. We do this by introducing a novel notion of clustering which we call \"strong local indistinguishability\" based on a max-relative entropy, and then proving that it implies a lower bound on the modified logarithmic Sobolev inequality (MLSI) for nearest neighbour commuting models.   This has consequences for the rate of thermalization towards Gibbs states, and also for their relevant Wasserstein distances and transportation cost inequalities. Along the way, we show that several measures of decay of correlations on Gibbs states of commuting Hamiltonians are equivalent, a result of independent interest. At the technical level, we also show a direct relation between properties of Davies and Schmidt dynamics, that allows to transfer results of thermalization between both.","sentences":["Quantum systems typically reach thermal equilibrium rather quickly when coupled to a thermal environment.","The usual way of bounding the speed of this process is by estimating the spectral gap of the dissipative generator.","However the gap, by itself, does not always yield a reasonable estimate for the thermalization time in many-body systems: without further structure, a uniform lower bound on it only constrains the thermalization time to grow polynomially with system size.   ","Here, instead, we show that for a large class of geometrically-2-local models of Davies generators with commuting Hamiltonians, the thermalization time is much shorter than one would na\\\"ively estimate from the gap: at most logarithmic in the system size.","This yields the so-called rapid mixing of dissipative dynamics.","The result is particularly relevant for 1D systems, for which we prove rapid thermalization with a system size independent decay rate only from a positive gap in the generator.","We also prove that systems in hypercubic lattices of any dimension, and exponential graphs, such as trees, have rapid mixing at high enough temperatures.","We do this by introducing a novel notion of clustering which we call \"strong local indistinguishability\" based on a max-relative entropy, and then proving that it implies a lower bound on the modified logarithmic Sobolev inequality (MLSI) for nearest neighbour commuting models.   ","This has consequences for the rate of thermalization towards Gibbs states, and also for their relevant Wasserstein distances and transportation cost inequalities.","Along the way, we show that several measures of decay of correlations on Gibbs states of commuting Hamiltonians are equivalent, a result of independent interest.","At the technical level, we also show a direct relation between properties of Davies and Schmidt dynamics, that allows to transfer results of thermalization between both."],"url":"http://arxiv.org/abs/2404.16780v1","category":"quant-ph"}
{"created":"2024-04-25 17:28:33","title":"DrS: Learning Reusable Dense Rewards for Multi-Stage Tasks","abstract":"The success of many RL techniques heavily relies on human-engineered dense rewards, which typically demand substantial domain expertise and extensive trial and error. In our work, we propose DrS (Dense reward learning from Stages), a novel approach for learning reusable dense rewards for multi-stage tasks in a data-driven manner. By leveraging the stage structures of the task, DrS learns a high-quality dense reward from sparse rewards and demonstrations if given. The learned rewards can be \\textit{reused} in unseen tasks, thus reducing the human effort for reward engineering. Extensive experiments on three physical robot manipulation task families with 1000+ task variants demonstrate that our learned rewards can be reused in unseen tasks, resulting in improved performance and sample efficiency of RL algorithms. The learned rewards even achieve comparable performance to human-engineered rewards on some tasks. See our project page (https://sites.google.com/view/iclr24drs) for more details.","sentences":["The success of many RL techniques heavily relies on human-engineered dense rewards, which typically demand substantial domain expertise and extensive trial and error.","In our work, we propose DrS (Dense reward learning from Stages), a novel approach for learning reusable dense rewards for multi-stage tasks in a data-driven manner.","By leveraging the stage structures of the task, DrS learns a high-quality dense reward from sparse rewards and demonstrations if given.","The learned rewards can be \\textit{reused} in unseen tasks, thus reducing the human effort for reward engineering.","Extensive experiments on three physical robot manipulation task families with 1000+ task variants demonstrate that our learned rewards can be reused in unseen tasks, resulting in improved performance and sample efficiency of RL algorithms.","The learned rewards even achieve comparable performance to human-engineered rewards on some tasks.","See our project page (https://sites.google.com/view/iclr24drs) for more details."],"url":"http://arxiv.org/abs/2404.16779v1","category":"cs.LG"}
{"created":"2024-04-25 17:27:39","title":"Unifying Asynchronous Logics for Hyperproperties","abstract":"We introduce and investigate a powerful hyper logical framework in the linear-time setting, we call generalized HyperLTL with stuttering and contexts (GHyperLTL_SC for short). GHyperLTL_SC unifies known asynchronous extensions of HyperLTL and the well-known extension KLTL of LTL with knowledge modalities under both the synchronous and asynchronous perfect recall semantics. As a main contribution, we individuate a meaningful fragment of GHyperLTL_SC, we call simple GHyperLTL_SC, with a decidable model-checking problem, which is more expressive than HyperLTL and known fragments of asynchronous extensions of HyperLTL with a decidable model-checking problem. Simple GHyperLTL_SC subsumes KLTL under the synchronous semantics and the one-agent fragment of KLTL under the asynchronous semantics, and to the best of our knowledge, it represents the unique hyper logic with a decidable model-checking problem which can express powerful non-regular trace properties when interpreted on singleton sets of traces. We justify the relevance of simple GHyperLTL_SC by showing that it can express diagnosability properties, interesting classes of information-flow security policies, both in the synchronous and asynchronous settings, and bounded termination (more in general, global promptness in the style of Prompt LTL).","sentences":["We introduce and investigate a powerful hyper logical framework in the linear-time setting, we call generalized HyperLTL with stuttering and contexts (GHyperLTL_SC for short).","GHyperLTL_SC unifies known asynchronous extensions of HyperLTL and the well-known extension KLTL of LTL with knowledge modalities under both the synchronous and asynchronous perfect recall semantics.","As a main contribution, we individuate a meaningful fragment of GHyperLTL_SC, we call simple GHyperLTL_SC, with a decidable model-checking problem, which is more expressive than HyperLTL and known fragments of asynchronous extensions of HyperLTL with a decidable model-checking problem.","Simple GHyperLTL_SC subsumes KLTL under the synchronous semantics and the one-agent fragment of KLTL under the asynchronous semantics, and to the best of our knowledge, it represents the unique hyper logic with a decidable model-checking problem which can express powerful non-regular trace properties when interpreted on singleton sets of traces.","We justify the relevance of simple GHyperLTL_SC by showing that it can express diagnosability properties, interesting classes of information-flow security policies, both in the synchronous and asynchronous settings, and bounded termination (more in general, global promptness in the style of Prompt LTL)."],"url":"http://arxiv.org/abs/2404.16778v1","category":"cs.LO"}
{"created":"2024-04-25 17:26:24","title":"Imaginary Stark Skin Effect","abstract":"The non-Hermitian skin effect (NHSE) is a unique phenomenon in non-Hermitian systems. However, studies on NHSE in systems without translational symmetry remain largely unexplored. Here, we unveil a new class of NHSE, dubbed \"imaginary Stark skin effect\" (ISSE), in a one-dimensional lossy lattice with a spatially increasing loss rate. The energy spectrum of this model exhibits a T-shaped feature, with approximately half of the eigenstates localized at the left boundary. These skin modes exhibit peculiar behaviors, expressed as a single stable exponential decay wave within the bulk region. We use the transfer matrix method to analyze the formation of the ISSE in this model. According to the eigen-decomposition of the transfer matrix, the wave function is divided into two parts, one of which dominates the behavior of the skin modes in the bulk. Our findings provide insights into the NHSE in systems without translational symmetry and contribute to the understanding of non-Hermitian systems in general.","sentences":["The non-Hermitian skin effect (NHSE) is a unique phenomenon in non-Hermitian systems.","However, studies on NHSE in systems without translational symmetry remain largely unexplored.","Here, we unveil a new class of NHSE, dubbed \"imaginary Stark skin effect\" (ISSE), in a one-dimensional lossy lattice with a spatially increasing loss rate.","The energy spectrum of this model exhibits a T-shaped feature, with approximately half of the eigenstates localized at the left boundary.","These skin modes exhibit peculiar behaviors, expressed as a single stable exponential decay wave within the bulk region.","We use the transfer matrix method to analyze the formation of the ISSE in this model.","According to the eigen-decomposition of the transfer matrix, the wave function is divided into two parts, one of which dominates the behavior of the skin modes in the bulk.","Our findings provide insights into the NHSE in systems without translational symmetry and contribute to the understanding of non-Hermitian systems in general."],"url":"http://arxiv.org/abs/2404.16774v1","category":"quant-ph"}
{"created":"2024-04-25 17:23:43","title":"ConsistentID: Portrait Generation with Multimodal Fine-Grained Identity Preserving","abstract":"Diffusion-based technologies have made significant strides, particularly in personalized and customized facialgeneration. However, existing methods face challenges in achieving high-fidelity and detailed identity (ID)consistency, primarily due to insufficient fine-grained control over facial areas and the lack of a comprehensive strategy for ID preservation by fully considering intricate facial details and the overall face. To address these limitations, we introduce ConsistentID, an innovative method crafted for diverseidentity-preserving portrait generation under fine-grained multimodal facial prompts, utilizing only a single reference image. ConsistentID comprises two key components: a multimodal facial prompt generator that combines facial features, corresponding facial descriptions and the overall facial context to enhance precision in facial details, and an ID-preservation network optimized through the facial attention localization strategy, aimed at preserving ID consistency in facial regions. Together, these components significantly enhance the accuracy of ID preservation by introducing fine-grained multimodal ID information from facial regions. To facilitate training of ConsistentID, we present a fine-grained portrait dataset, FGID, with over 500,000 facial images, offering greater diversity and comprehensiveness than existing public facial datasets. % such as LAION-Face, CelebA, FFHQ, and SFHQ. Experimental results substantiate that our ConsistentID achieves exceptional precision and diversity in personalized facial generation, surpassing existing methods in the MyStyle dataset. Furthermore, while ConsistentID introduces more multimodal ID information, it maintains a fast inference speed during generation.","sentences":["Diffusion-based technologies have made significant strides, particularly in personalized and customized facialgeneration.","However, existing methods face challenges in achieving high-fidelity and detailed identity (ID)consistency, primarily due to insufficient fine-grained control over facial areas and the lack of a comprehensive strategy for ID preservation by fully considering intricate facial details and the overall face.","To address these limitations, we introduce ConsistentID, an innovative method crafted for diverseidentity-preserving portrait generation under fine-grained multimodal facial prompts, utilizing only a single reference image.","ConsistentID comprises two key components: a multimodal facial prompt generator that combines facial features, corresponding facial descriptions and the overall facial context to enhance precision in facial details, and an ID-preservation network optimized through the facial attention localization strategy, aimed at preserving ID consistency in facial regions.","Together, these components significantly enhance the accuracy of ID preservation by introducing fine-grained multimodal ID information from facial regions.","To facilitate training of ConsistentID, we present a fine-grained portrait dataset, FGID, with over 500,000 facial images, offering greater diversity and comprehensiveness than existing public facial datasets.","% such as LAION-Face, CelebA, FFHQ, and SFHQ.","Experimental results substantiate that our ConsistentID achieves exceptional precision and diversity in personalized facial generation, surpassing existing methods in the MyStyle dataset.","Furthermore, while ConsistentID introduces more multimodal ID information, it maintains a fast inference speed during generation."],"url":"http://arxiv.org/abs/2404.16771v1","category":"cs.CV"}
{"created":"2024-04-25 17:22:43","title":"Redefining Safety for Autonomous Vehicles","abstract":"Existing definitions and associated conceptual frameworks for computer-based system safety should be revisited in light of real-world experiences from deploying autonomous vehicles. Current terminology used by industry safety standards emphasizes mitigation of risk from specifically identified hazards, and carries assumptions based on human-supervised vehicle operation. Operation without a human driver dramatically increases the scope of safety concerns, especially due to operation in an open world environment, a requirement to self-enforce operational limits, participation in an ad hoc sociotechnical system of systems, and a requirement to conform to both legal and ethical constraints. Existing standards and terminology only partially address these new challenges. We propose updated definitions for core system safety concepts that encompass these additional considerations as a starting point for evolving safe-ty approaches to address these additional safety challenges. These results might additionally inform framing safety terminology for other autonomous system applications.","sentences":["Existing definitions and associated conceptual frameworks for computer-based system safety should be revisited in light of real-world experiences from deploying autonomous vehicles.","Current terminology used by industry safety standards emphasizes mitigation of risk from specifically identified hazards, and carries assumptions based on human-supervised vehicle operation.","Operation without a human driver dramatically increases the scope of safety concerns, especially due to operation in an open world environment, a requirement to self-enforce operational limits, participation in an ad hoc sociotechnical system of systems, and a requirement to conform to both legal and ethical constraints.","Existing standards and terminology only partially address these new challenges.","We propose updated definitions for core system safety concepts that encompass these additional considerations as a starting point for evolving safe-ty approaches to address these additional safety challenges.","These results might additionally inform framing safety terminology for other autonomous system applications."],"url":"http://arxiv.org/abs/2404.16768v1","category":"cs.RO"}
{"created":"2024-04-25 17:20:45","title":"REBEL: Reinforcement Learning via Regressing Relative Rewards","abstract":"While originally developed for continuous control problems, Proximal Policy Optimization (PPO) has emerged as the work-horse of a variety of reinforcement learning (RL) applications including the fine-tuning of generative models. Unfortunately, PPO requires multiple heuristics to enable stable convergence (e.g. value networks, clipping) and is notorious for its sensitivity to the precise implementation of these components. In response, we take a step back and ask what a minimalist RL algorithm for the era of generative models would look like. We propose REBEL, an algorithm that cleanly reduces the problem of policy optimization to regressing the relative rewards via a direct policy parameterization between two completions to a prompt, enabling strikingly lightweight implementation. In theory, we prove that fundamental RL algorithms like Natural Policy Gradient can be seen as variants of REBEL, which allows us to match the strongest known theoretical guarantees in terms of convergence and sample complexity in the RL literature. REBEL can also cleanly incorporate offline data and handle the intransitive preferences we frequently see in practice. Empirically, we find that REBEL provides a unified approach to language modeling and image generation with stronger or similar performance as PPO and DPO, all while being simpler to implement and more computationally tractable than PPO.","sentences":["While originally developed for continuous control problems, Proximal Policy Optimization (PPO) has emerged as the work-horse of a variety of reinforcement learning (RL) applications including the fine-tuning of generative models.","Unfortunately, PPO requires multiple heuristics to enable stable convergence (e.g. value networks, clipping) and is notorious for its sensitivity to the precise implementation of these components.","In response, we take a step back and ask what a minimalist RL algorithm for the era of generative models would look like.","We propose REBEL, an algorithm that cleanly reduces the problem of policy optimization to regressing the relative rewards via a direct policy parameterization between two completions to a prompt, enabling strikingly lightweight implementation.","In theory, we prove that fundamental RL algorithms like Natural Policy Gradient can be seen as variants of REBEL, which allows us to match the strongest known theoretical guarantees in terms of convergence and sample complexity in the RL literature.","REBEL can also cleanly incorporate offline data and handle the intransitive preferences we frequently see in practice.","Empirically, we find that REBEL provides a unified approach to language modeling and image generation with stronger or similar performance as PPO and DPO, all while being simpler to implement and more computationally tractable than PPO."],"url":"http://arxiv.org/abs/2404.16767v1","category":"cs.LG"}
{"created":"2024-04-25 17:19:36","title":"Prefix Text as a Yarn: Eliciting Non-English Alignment in Foundation Language Model","abstract":"While supervised fine-tuning (SFT) has been a straightforward approach for tailoring the output of foundation large language model (LLM) to specific preferences, concerns have been raised about the depth of this alignment, with some critiques suggesting it is merely \"superficial\". We critically examine this hypothesis within the scope of cross-lingual generation tasks, proposing that the effectiveness of SFT may be constrained by its reliance on prior tokens to guide cross-lingual generation. Based on this crucial insight, and in response to the challenges posed by the costly and limited availability of non-English data for SFT, we introduce a novel training-free alignment method named PreTTY, which employs minimal task-related prior tokens to bridge the foundation LLM and the SFT LLM, achieving comparable performance without training. Experiments on machine translation and part-of-speech tagging across eight languages demonstrate the efficacy of PreTTY in cross-lingual settings. Remarkably, by initiating the decoding process with only one or two prior tokens, foundation LLMs can achieve performance comparable to their SFT counterparts. This method presents a cost-effective alternative to SFT and advances the democratization of multilingual LLMs.","sentences":["While supervised fine-tuning (SFT) has been a straightforward approach for tailoring the output of foundation large language model (LLM) to specific preferences, concerns have been raised about the depth of this alignment, with some critiques suggesting it is merely \"superficial\".","We critically examine this hypothesis within the scope of cross-lingual generation tasks, proposing that the effectiveness of SFT may be constrained by its reliance on prior tokens to guide cross-lingual generation.","Based on this crucial insight, and in response to the challenges posed by the costly and limited availability of non-English data for SFT, we introduce a novel training-free alignment method named PreTTY, which employs minimal task-related prior tokens to bridge the foundation LLM and the SFT LLM, achieving comparable performance without training.","Experiments on machine translation and part-of-speech tagging across eight languages demonstrate the efficacy of PreTTY in cross-lingual settings.","Remarkably, by initiating the decoding process with only one or two prior tokens, foundation LLMs can achieve performance comparable to their SFT counterparts.","This method presents a cost-effective alternative to SFT and advances the democratization of multilingual LLMs."],"url":"http://arxiv.org/abs/2404.16766v1","category":"cs.CL"}
{"created":"2024-04-25 17:18:54","title":"The asymptotic spectrum distance, graph limits, and the Shannon capacity","abstract":"Determining the Shannon capacity of graphs is a long-standing open problem in information theory, graph theory and combinatorial optimization. Over decades, a wide range of upper and lower bound methods have been developed to analyze this problem. However, despite tremendous effort, even small instances of the problem have remained open.   In recent years, a new dual characterization of the Shannon capacity of graphs, asymptotic spectrum duality, has unified and extended known upper bound methods and structural theorems. In this paper, building on asymptotic spectrum duality, we develop a new theory of graph distance, that we call asymptotic spectrum distance, and corresponding limits (reminiscent of, but different from, the celebrated theory of cut-norm, graphons and flag algebras). We propose a graph limit approach to the Shannon capacity problem: to determine the Shannon capacity of a graph, construct a sequence of easier to analyse graphs converging to it.   (1) We give a very general construction of non-trivial converging sequences of graphs (in a family of circulant graphs).   (2) We construct Cauchy sequences of finite graphs that do not converge to any finite graph, but do converge to an infinite graph. We establish strong connections between convergence questions of finite graphs and the asymptotic properties of Borsuk-like infinite graphs on the circle.   (3) We observe that all best-known lower bound constructions for Shannon capacity of small odd cycles can be obtained from a \"finite\" version of the graph limit approach. We develop computational and theoretical aspects of this approach and use these to obtain a new Shannon capacity lower bound for the fifteen-cycle.   The theory of asymptotic spectrum distance applies not only to Shannon capacity of graphs; indeed, we will develop it for a general class of mathematical objects and their asymptotic properties.","sentences":["Determining the Shannon capacity of graphs is a long-standing open problem in information theory, graph theory and combinatorial optimization.","Over decades, a wide range of upper and lower bound methods have been developed to analyze this problem.","However, despite tremendous effort, even small instances of the problem have remained open.   ","In recent years, a new dual characterization of the Shannon capacity of graphs, asymptotic spectrum duality, has unified and extended known upper bound methods and structural theorems.","In this paper, building on asymptotic spectrum duality, we develop a new theory of graph distance, that we call asymptotic spectrum distance, and corresponding limits (reminiscent of, but different from, the celebrated theory of cut-norm, graphons and flag algebras).","We propose a graph limit approach to the Shannon capacity problem: to determine the Shannon capacity of a graph, construct a sequence of easier to analyse graphs converging to it.   ","(1) We give a very general construction of non-trivial converging sequences of graphs (in a family of circulant graphs).   ","(2) We construct Cauchy sequences of finite graphs that do not converge to any finite graph, but do converge to an infinite graph.","We establish strong connections between convergence questions of finite graphs and the asymptotic properties of Borsuk-like infinite graphs on the circle.   ","(3) We observe that all best-known lower bound constructions for Shannon capacity of small odd cycles can be obtained from a \"finite\" version of the graph limit approach.","We develop computational and theoretical aspects of this approach and use these to obtain a new Shannon capacity lower bound for the fifteen-cycle.   ","The theory of asymptotic spectrum distance applies not only to Shannon capacity of graphs; indeed, we will develop it for a general class of mathematical objects and their asymptotic properties."],"url":"http://arxiv.org/abs/2404.16763v1","category":"math.CO"}
{"created":"2024-04-25 17:16:37","title":"Beyond Boolean networks, a multi-valued approach","abstract":"Boolean networks can be viewed as functions on the set of binary strings of a given length, described via logical rules. They were introduced as dynamic models into biology, in particular as logical models of intracellular regulatory networks involving genes, proteins, and metabolites. Since genes can have several modes of action, depending on their expression levels, binary variables are often not sufficiently rich, requiring the use of multi-valued networks instead. The steady state analysis of Boolean networks is computationally complex, and increasing the number of variable values beyond $2$ adds substantially to this complexity, and no general methods are available beyond simulation. The main contribution of this paper is to give an algorithm to compute the steady states of a multi-valued network that has a complexity that, in many cases, is essentially the same as that for the case of binary values. Our approach is based on a representation of multi-valued networks using multi-valued logic functions, providing a biologically intuitive representation of the network. Furthermore, it uses tools to compute lattice points in rational polytopes, tapping a rich area of algebraic combinatorics as a source for combinatorial algorithms for Boolean network analysis. An implementation of the algorithm is provided.","sentences":["Boolean networks can be viewed as functions on the set of binary strings of a given length, described via logical rules.","They were introduced as dynamic models into biology, in particular as logical models of intracellular regulatory networks involving genes, proteins, and metabolites.","Since genes can have several modes of action, depending on their expression levels, binary variables are often not sufficiently rich, requiring the use of multi-valued networks instead.","The steady state analysis of Boolean networks is computationally complex, and increasing the number of variable values beyond $2$ adds substantially to this complexity, and no general methods are available beyond simulation.","The main contribution of this paper is to give an algorithm to compute the steady states of a multi-valued network that has a complexity that, in many cases, is essentially the same as that for the case of binary values.","Our approach is based on a representation of multi-valued networks using multi-valued logic functions, providing a biologically intuitive representation of the network.","Furthermore, it uses tools to compute lattice points in rational polytopes, tapping a rich area of algebraic combinatorics as a source for combinatorial algorithms for Boolean network analysis.","An implementation of the algorithm is provided."],"url":"http://arxiv.org/abs/2404.16760v1","category":"math.DS"}
{"created":"2024-04-25 17:16:07","title":"Revealing the regularities of electron correlation energies associated with valence electrons in atoms in the first three rows of the periodic table","abstract":"Electronic correlation is a complex many-body effect and the correlation energy depends on the specific electronic structure and spatial distribution of electrons in each atom and molecule. Although the total correlation energy in an atom can be decomposed into different components such as inter-orbital and intra-orbital pair-correlation energies (PCE), it is generally believed that the PCEs in different atoms cannot be the same. In this work, we investigate the correlation energies of the atoms in the first three rows of the periodic table (He to Ar). It is found that when the correlation energy is defined as the difference between the exact ground-state energy and the unrestricted Hartree-Fock (UHF) energy, the inter- and intra-orbital PECs associated with the valence electrons of the atoms in the same row of the periodic table have the same values. These PCEs are not entangled and their values depend only on the electron orbitals. For two specific orbitals, the inter-orbital correlation energy is the same between two electrons of parallel spins or anti-parallel spins. We also show that the effects of orbital relaxation on the correlation energy are surprisingly small.","sentences":["Electronic correlation is a complex many-body effect and the correlation energy depends on the specific electronic structure and spatial distribution of electrons in each atom and molecule.","Although the total correlation energy in an atom can be decomposed into different components such as inter-orbital and intra-orbital pair-correlation energies (PCE), it is generally believed that the PCEs in different atoms cannot be the same.","In this work, we investigate the correlation energies of the atoms in the first three rows of the periodic table (He to Ar).","It is found that when the correlation energy is defined as the difference between the exact ground-state energy and the unrestricted Hartree-Fock (UHF) energy, the inter- and intra-orbital PECs associated with the valence electrons of the atoms in the same row of the periodic table have the same values.","These PCEs are not entangled and their values depend only on the electron orbitals.","For two specific orbitals, the inter-orbital correlation energy is the same between two electrons of parallel spins or anti-parallel spins.","We also show that the effects of orbital relaxation on the correlation energy are surprisingly small."],"url":"http://arxiv.org/abs/2404.16759v1","category":"physics.chem-ph"}
{"created":"2024-04-25 17:12:09","title":"Concentration inequalities for Poisson $U$-statistics","abstract":"In this article we obtain concentration inequalities for Poisson $U$-statistics $F_m(f,\\eta)$ of order $m\\ge 1$ with kernels $f$ under general assumptions on $f$ and the intensity measure $\\gamma \\Lambda$ of underlying Poisson point process $\\eta$. The main result are new concentration bounds of the form \\[   \\mathbb{P}(|F_m ( f , \\eta) -\\mathbb{E} F_m ( f , \\eta)| \\ge t)\\leq 2\\exp(-I(\\gamma,t)), \\]   where $I(\\gamma,t)$ satisfies $I(\\gamma,t)=\\Theta(t^{1\\over m}\\log t)$ as $t\\to\\infty$ and $\\gamma$ is fixed. The function $I(\\gamma,t)$ is given explicitly in terms of parameters of the assumptions satisfied by $f$ and $\\Lambda$. One of the key ingredients of the proof are fine bounds for the centred moments of $F_m(f,\\eta)$. We discuss the optimality of obtained bounds and consider a number of applications related to Gilbert graphs and Poisson hyperplane processes in constant curvature spaces.","sentences":["In this article we obtain concentration inequalities for Poisson $U$-statistics $F_m(f,\\eta)$ of order $m\\ge 1$ with kernels $f$ under general assumptions on $f$ and the intensity measure $\\gamma \\Lambda$ of underlying Poisson point process $\\eta$.","The main result are new concentration bounds of the form \\[   \\mathbb{P}(|F_m ( f , \\eta) -\\mathbb{E","} F_m ( f , \\eta)| \\ge t)\\leq 2\\exp(-I(\\gamma,t)), \\]   where $I(\\gamma,t)$ satisfies $I(\\gamma,t)=\\Theta(t^{1\\over m}\\log t)$ as $t\\to\\infty$ and $\\gamma$ is fixed.","The function $I(\\gamma,t)$ is given explicitly in terms of parameters of the assumptions satisfied by $f$ and $\\Lambda$. One of the key ingredients of the proof are fine bounds for the centred moments of $F_m(f,\\eta)$. We discuss the optimality of obtained bounds and consider a number of applications related to Gilbert graphs and Poisson hyperplane processes in constant curvature spaces."],"url":"http://arxiv.org/abs/2404.16756v1","category":"math.PR"}
{"created":"2024-04-25 17:11:37","title":"RadGenome-Chest CT: A Grounded Vision-Language Dataset for Chest CT Analysis","abstract":"Developing generalist foundation model has recently attracted tremendous attention among researchers in the field of AI for Medicine (AI4Medicine). A pivotal insight in developing these models is their reliance on dataset scaling, which emphasizes the requirements on developing open-source medical image datasets that incorporate diverse supervision signals across various imaging modalities. In this paper, we introduce RadGenome-Chest CT, a comprehensive, large-scale, region-guided 3D chest CT interpretation dataset based on CT-RATE. Specifically, we leverage the latest powerful universal segmentation and large language models, to extend the original datasets (over 25,692 non-contrast 3D chest CT volume and reports from 20,000 patients) from the following aspects: (i) organ-level segmentation masks covering 197 categories, which provide intermediate reasoning visual clues for interpretation; (ii) 665 K multi-granularity grounded reports, where each sentence of the report is linked to the corresponding anatomical region of CT volume in the form of a segmentation mask; (iii) 1.3 M grounded VQA pairs, where questions and answers are all linked with reference segmentation masks, enabling models to associate visual evidence with textual explanations. All grounded reports and VQA pairs in the validation set have gone through manual verification to ensure dataset quality. We believe that RadGenome-Chest CT can significantly advance the development of multimodal medical foundation models, by training to generate texts based on given segmentation regions, which is unattainable with previous relevant datasets. We will release all segmentation masks, grounded reports, and VQA pairs to facilitate further research and development in this field.","sentences":["Developing generalist foundation model has recently attracted tremendous attention among researchers in the field of AI for Medicine (AI4Medicine).","A pivotal insight in developing these models is their reliance on dataset scaling, which emphasizes the requirements on developing open-source medical image datasets that incorporate diverse supervision signals across various imaging modalities.","In this paper, we introduce RadGenome-Chest CT, a comprehensive, large-scale, region-guided 3D chest CT interpretation dataset based on CT-RATE.","Specifically, we leverage the latest powerful universal segmentation and large language models, to extend the original datasets (over 25,692 non-contrast 3D chest CT volume and reports from 20,000 patients) from the following aspects: (i) organ-level segmentation masks covering 197 categories, which provide intermediate reasoning visual clues for interpretation; (ii) 665 K multi-granularity grounded reports, where each sentence of the report is linked to the corresponding anatomical region of CT volume in the form of a segmentation mask; (iii) 1.3 M grounded VQA pairs, where questions and answers are all linked with reference segmentation masks, enabling models to associate visual evidence with textual explanations.","All grounded reports and VQA pairs in the validation set have gone through manual verification to ensure dataset quality.","We believe that RadGenome-Chest CT can significantly advance the development of multimodal medical foundation models, by training to generate texts based on given segmentation regions, which is unattainable with previous relevant datasets.","We will release all segmentation masks, grounded reports, and VQA pairs to facilitate further research and development in this field."],"url":"http://arxiv.org/abs/2404.16754v1","category":"cs.CV"}
{"created":"2024-04-25 17:05:38","title":"TELA: Text to Layer-wise 3D Clothed Human Generation","abstract":"This paper addresses the task of 3D clothed human generation from textural descriptions. Previous works usually encode the human body and clothes as a holistic model and generate the whole model in a single-stage optimization, which makes them struggle for clothing editing and meanwhile lose fine-grained control over the whole generation process. To solve this, we propose a layer-wise clothed human representation combined with a progressive optimization strategy, which produces clothing-disentangled 3D human models while providing control capacity for the generation process. The basic idea is progressively generating a minimal-clothed human body and layer-wise clothes. During clothing generation, a novel stratified compositional rendering method is proposed to fuse multi-layer human models, and a new loss function is utilized to help decouple the clothing model from the human body. The proposed method achieves high-quality disentanglement, which thereby provides an effective way for 3D garment generation. Extensive experiments demonstrate that our approach achieves state-of-the-art 3D clothed human generation while also supporting cloth editing applications such as virtual try-on. Project page: http://jtdong.com/tela_layer/","sentences":["This paper addresses the task of 3D clothed human generation from textural descriptions.","Previous works usually encode the human body and clothes as a holistic model and generate the whole model in a single-stage optimization, which makes them struggle for clothing editing and meanwhile lose fine-grained control over the whole generation process.","To solve this, we propose a layer-wise clothed human representation combined with a progressive optimization strategy, which produces clothing-disentangled 3D human models while providing control capacity for the generation process.","The basic idea is progressively generating a minimal-clothed human body and layer-wise clothes.","During clothing generation, a novel stratified compositional rendering method is proposed to fuse multi-layer human models, and a new loss function is utilized to help decouple the clothing model from the human body.","The proposed method achieves high-quality disentanglement, which thereby provides an effective way for 3D garment generation.","Extensive experiments demonstrate that our approach achieves state-of-the-art 3D clothed human generation while also supporting cloth editing applications such as virtual try-on.","Project page: http://jtdong.com/tela_layer/"],"url":"http://arxiv.org/abs/2404.16748v1","category":"cs.CV"}
{"created":"2024-04-25 17:00:13","title":"Statistical Inference for Covariate-Adjusted and Interpretable Generalized Factor Model with Application to Testing Fairness","abstract":"In the era of data explosion, statisticians have been developing interpretable and computationally efficient statistical methods to measure latent factors (e.g., skills, abilities, and personalities) using large-scale assessment data. In addition to understanding the latent information, the covariate effect on responses controlling for latent factors is also of great scientific interest and has wide applications, such as evaluating the fairness of educational testing, where the covariate effect reflects whether a test question is biased toward certain individual characteristics (e.g., gender and race) taking into account their latent abilities. However, the large sample size, substantial covariate dimension, and great test length pose challenges to developing efficient methods and drawing valid inferences. Moreover, to accommodate the commonly encountered discrete types of responses, nonlinear latent factor models are often assumed, bringing further complexity to the problem. To address these challenges, we consider a covariate-adjusted generalized factor model and develop novel and interpretable conditions to address the identifiability issue. Based on the identifiability conditions, we propose a joint maximum likelihood estimation method and establish estimation consistency and asymptotic normality results for the covariate effects under a practical yet challenging asymptotic regime. Furthermore, we derive estimation and inference results for latent factors and the factor loadings. We illustrate the finite sample performance of the proposed method through extensive numerical studies and an application to an educational assessment dataset obtained from the Programme for International Student Assessment (PISA).","sentences":["In the era of data explosion, statisticians have been developing interpretable and computationally efficient statistical methods to measure latent factors (e.g., skills, abilities, and personalities) using large-scale assessment data.","In addition to understanding the latent information, the covariate effect on responses controlling for latent factors is also of great scientific interest and has wide applications, such as evaluating the fairness of educational testing, where the covariate effect reflects whether a test question is biased toward certain individual characteristics (e.g., gender and race) taking into account their latent abilities.","However, the large sample size, substantial covariate dimension, and great test length pose challenges to developing efficient methods and drawing valid inferences.","Moreover, to accommodate the commonly encountered discrete types of responses, nonlinear latent factor models are often assumed, bringing further complexity to the problem.","To address these challenges, we consider a covariate-adjusted generalized factor model and develop novel and interpretable conditions to address the identifiability issue.","Based on the identifiability conditions, we propose a joint maximum likelihood estimation method and establish estimation consistency and asymptotic normality results for the covariate effects under a practical yet challenging asymptotic regime.","Furthermore, we derive estimation and inference results for latent factors and the factor loadings.","We illustrate the finite sample performance of the proposed method through extensive numerical studies and an application to an educational assessment dataset obtained from the Programme for International Student Assessment (PISA)."],"url":"http://arxiv.org/abs/2404.16745v1","category":"stat.ME"}
{"created":"2024-04-25 16:57:05","title":"Automatic Speech Recognition System-Independent Word Error Rate Estimatio","abstract":"Word error rate (WER) is a metric used to evaluate the quality of transcriptions produced by Automatic Speech Recognition (ASR) systems. In many applications, it is of interest to estimate WER given a pair of a speech utterance and a transcript. Previous work on WER estimation focused on building models that are trained with a specific ASR system in mind (referred to as ASR system-dependent). These are also domain-dependent and inflexible in real-world applications. In this paper, a hypothesis generation method for ASR System-Independent WER estimation (SIWE) is proposed. In contrast to prior work, the WER estimators are trained using data that simulates ASR system output. Hypotheses are generated using phonetically similar or linguistically more likely alternative words. In WER estimation experiments, the proposed method reaches a similar performance to ASR system-dependent WER estimators on in-domain data and achieves state-of-the-art performance on out-of-domain data. On the out-of-domain data, the SIWE model outperformed the baseline estimators in root mean square error and Pearson correlation coefficient by relative 17.58% and 18.21%, respectively, on Switchboard and CALLHOME. The performance was further improved when the WER of the training set was close to the WER of the evaluation dataset.","sentences":["Word error rate (WER) is a metric used to evaluate the quality of transcriptions produced by Automatic Speech Recognition (ASR) systems.","In many applications, it is of interest to estimate WER given a pair of a speech utterance and a transcript.","Previous work on WER estimation focused on building models that are trained with a specific ASR system in mind (referred to as ASR system-dependent).","These are also domain-dependent and inflexible in real-world applications.","In this paper, a hypothesis generation method for ASR System-Independent WER estimation (SIWE) is proposed.","In contrast to prior work, the WER estimators are trained using data that simulates ASR system output.","Hypotheses are generated using phonetically similar or linguistically more likely alternative words.","In WER estimation experiments, the proposed method reaches a similar performance to ASR system-dependent WER estimators on in-domain data and achieves state-of-the-art performance on out-of-domain data.","On the out-of-domain data, the SIWE model outperformed the baseline estimators in root mean square error and Pearson correlation coefficient by relative 17.58% and 18.21%, respectively, on Switchboard and CALLHOME.","The performance was further improved when the WER of the training set was close to the WER of the evaluation dataset."],"url":"http://arxiv.org/abs/2404.16743v1","category":"cs.CL"}
{"created":"2024-04-25 16:49:10","title":"CBRW: A Novel Approach for Cancelable Biometric Template Generation based on","abstract":"Cancelable Biometric is a challenging research field in which security of an original biometric image is ensured by transforming the original biometric into another irreversible domain. Several approaches have been suggested in literature for generating cancelable biometric templates. In this paper, two novel and simple cancelable biometric template generation methods based on Random Walk (CBRW) have been proposed. By employing random walk and other steps given in the proposed two algorithms viz. CBRW-BitXOR and CBRW-BitCMP, the original biometric is transformed into a cancellable template. The performance of the proposed methods is compared with other state-of-the-art methods. Experiments have been performed on eight publicly available gray and color datasets i.e. CP (ear) (gray and color), UTIRIS (iris) (gray and color), ORL (face) (gray), IIT Delhi (iris) (gray and color), and AR (face) (color). Performance of the generated templates is measured in terms of Correlation Coefficient (Cr), Root Mean Square Error (RMSE), Peak Signal to Noise Ratio (PSNR), Structural Similarity (SSIM), Mean Absolute Error (MAE), Number of Pixel Change Rate (NPCR), and Unified Average Changing Intensity (UACI). By experimental results, it has been proved that proposed methods are superior than other state-of-the-art methods in qualitative as well as quantitative analysis. Furthermore, CBRW performs better on both gray as well as color images.","sentences":["Cancelable Biometric is a challenging research field in which security of an original biometric image is ensured by transforming the original biometric into another irreversible domain.","Several approaches have been suggested in literature for generating cancelable biometric templates.","In this paper, two novel and simple cancelable biometric template generation methods based on Random Walk (CBRW) have been proposed.","By employing random walk and other steps given in the proposed two algorithms viz.","CBRW-BitXOR and CBRW-BitCMP, the original biometric is transformed into a cancellable template.","The performance of the proposed methods is compared with other state-of-the-art methods.","Experiments have been performed on eight publicly available gray and color datasets i.e. CP (ear) (gray and color), UTIRIS (iris) (gray and color), ORL (face) (gray), IIT Delhi (iris) (gray and color), and AR (face) (color).","Performance of the generated templates is measured in terms of Correlation Coefficient (Cr), Root Mean Square Error (RMSE), Peak Signal to Noise Ratio (PSNR), Structural Similarity (SSIM), Mean Absolute Error (MAE), Number of Pixel Change Rate (NPCR), and Unified Average Changing Intensity (UACI).","By experimental results, it has been proved that proposed methods are superior than other state-of-the-art methods in qualitative as well as quantitative analysis.","Furthermore, CBRW performs better on both gray as well as color images."],"url":"http://arxiv.org/abs/2404.16739v1","category":"cs.CV"}
{"created":"2024-04-25 16:44:45","title":"Lifts of quantum CSS codes","abstract":"We propose a notion of lift for quantum CSS codes, inspired by the geometrical construction of Freedman and Hastings. It is based on the existence of a canonical complex associated to any CSS code, that we introduce under the name of Tanner cone-complex, and over which we generate covering spaces. As a first application, we describe the classification of lifts of hypergraph product codes (HPC) and demonstrate the equivalence with the lifted product code (LPC) of Panteleev and Kalachev, including when the linear codes, factors of the HPC, are Tanner codes. As a second application, we report several new non-product constructions of quantum CSS codes, and we apply the prescription to generate their lifts which, for certain selected covering maps, are codes with improved relative parameters compared to the initial one.","sentences":["We propose a notion of lift for quantum CSS codes, inspired by the geometrical construction of Freedman and Hastings.","It is based on the existence of a canonical complex associated to any CSS code, that we introduce under the name of Tanner cone-complex, and over which we generate covering spaces.","As a first application, we describe the classification of lifts of hypergraph product codes (HPC) and demonstrate the equivalence with the lifted product code (LPC) of Panteleev and Kalachev, including when the linear codes, factors of the HPC, are Tanner codes.","As a second application, we report several new non-product constructions of quantum CSS codes, and we apply the prescription to generate their lifts which, for certain selected covering maps, are codes with improved relative parameters compared to the initial one."],"url":"http://arxiv.org/abs/2404.16736v1","category":"quant-ph"}
{"created":"2024-04-25 16:42:42","title":"Diagram model for the Okada algebra and monoid","abstract":"It is well known that the Young lattice is the Bratelli diagram of the symmetric groups expressing how irreducible representations restrict from $S_N$ to $S_{N-1}$. In 1988, Stanley discovered a similar lattice called the Young-Fibonacci lattice which was realized as the Bratelli diagram of a family of algebras by Okada in 1994. In this paper, we realize the Okada algebra and its associated monoid using a labeled version of Temperley-Lieb arc-diagrams. We prove in full generality that the dimension of the Okada algebra is $n!$. In particular, we interpret a natural bijection between permutations and labeled arc-diagrams as an instance of Fomin's Robinson-Schensted correspondence for the Young-Fibonacci lattice. We prove that the Okada monoid is aperiodic and describe its Green relations. Lifting those results to the algebra allows us to construct a cellular basis of the Okada algebra. }","sentences":["It is well known that the Young lattice is the Bratelli diagram of the symmetric groups expressing how irreducible representations restrict from $S_N$ to $S_{N-1}$. In 1988, Stanley discovered a similar lattice called the Young-Fibonacci lattice which was realized as the Bratelli diagram of a family of algebras by Okada in 1994.","In this paper, we realize the Okada algebra and its associated monoid using a labeled version of Temperley-Lieb arc-diagrams.","We prove in full generality that the dimension of the Okada algebra is $n!$. In particular, we interpret a natural bijection between permutations and labeled arc-diagrams as an instance of Fomin's Robinson-Schensted correspondence for the Young-Fibonacci lattice.","We prove that the Okada monoid is aperiodic and describe its Green relations.","Lifting those results to the algebra allows us to construct a cellular basis of the Okada algebra. }"],"url":"http://arxiv.org/abs/2404.16733v1","category":"math.RT"}
{"created":"2024-04-25 16:40:31","title":"Tidal reconstruction of neutron star mergers from their late inspiral","abstract":"We investigate the measurement correlation between the effective spin and the effective tidal deformability in gravitational wave signals from binary neutron star mergers. We exploit the fact that the tidal effects in a binary system are prominent when the components are closer during the late-inspiral. Thus, we indicate to a computationally efficient strategy of extracting the tidal information compressed within seconds before the merger. We report our observations for \\texttt{GW170817} and explore the suitability of our approach for the upcoming observation scenarios. Fast and accurate measurements of the tidal deformability parameters can be used to inform astronomers in prioritizing the electromagnetic follow-up efforts for such sources.","sentences":["We investigate the measurement correlation between the effective spin and the effective tidal deformability in gravitational wave signals from binary neutron star mergers.","We exploit the fact that the tidal effects in a binary system are prominent when the components are closer during the late-inspiral.","Thus, we indicate to a computationally efficient strategy of extracting the tidal information compressed within seconds before the merger.","We report our observations for \\texttt{GW170817} and explore the suitability of our approach for the upcoming observation scenarios.","Fast and accurate measurements of the tidal deformability parameters can be used to inform astronomers in prioritizing the electromagnetic follow-up efforts for such sources."],"url":"http://arxiv.org/abs/2404.16729v1","category":"gr-qc"}
{"created":"2024-04-25 16:38:52","title":"Approximation Algorithms for Hop Constrained and Buy-at-Bulk Network Design via Hop Constrained Oblivious Routing","abstract":"We consider two-cost network design models in which edges of the input graph have an associated cost and length. We build upon recent advances in hop-constrained oblivious routing to obtain two sets of results.   We address multicommodity buy-at-bulk network design in the nonuniform setting. Existing poly-logarithmic approximations are based on the junction tree approach [CHKS09,KN11]. We obtain a new polylogarithmic approximation via a natural LP relaxation. This establishes an upper bound on its integrality gap and affirmatively answers an open question raised in [CHKS09]. The rounding is based on recent results in hop-constrained oblivious routing [GHZ21], and this technique yields a polylogarithmic approximation in more general settings such as set connectivity. Our algorithm for buy-at-bulk network design is based on an LP-based reduction to hop constrained network design for which we obtain LP-based bicriteria approximation algorithms.   We also consider a fault-tolerant version of hop constrained network design where one wants to design a low-cost network to guarantee short paths between a given set of source-sink pairs even when k-1 edges can fail. This model has been considered in network design [GL17,GML18,AJL20] but no approximation algorithms were known. We obtain polylogarithmic bicriteria approximation algorithms for the single-source setting for any fixed k. We build upon the single-source algorithm and the junction-tree approach to obtain an approximation algorithm for the multicommodity setting when at most one edge can fail.","sentences":["We consider two-cost network design models in which edges of the input graph have an associated cost and length.","We build upon recent advances in hop-constrained oblivious routing to obtain two sets of results.   ","We address multicommodity buy-at-bulk network design in the nonuniform setting.","Existing poly-logarithmic approximations are based on the junction tree approach [CHKS09,KN11].","We obtain a new polylogarithmic approximation via a natural LP relaxation.","This establishes an upper bound on its integrality gap and affirmatively answers an open question raised in [CHKS09].","The rounding is based on recent results in hop-constrained oblivious routing [GHZ21], and this technique yields a polylogarithmic approximation in more general settings such as set connectivity.","Our algorithm for buy-at-bulk network design is based on an LP-based reduction to hop constrained network design for which we obtain LP-based bicriteria approximation algorithms.   ","We also consider a fault-tolerant version of hop constrained network design where one wants to design a low-cost network to guarantee short paths between a given set of source-sink pairs even when k-1 edges can fail.","This model has been considered in network design [GL17,GML18,AJL20] but no approximation algorithms were known.","We obtain polylogarithmic bicriteria approximation algorithms for the single-source setting for any fixed k. We build upon the single-source algorithm and the junction-tree approach to obtain an approximation algorithm for the multicommodity setting when at most one edge can fail."],"url":"http://arxiv.org/abs/2404.16725v1","category":"cs.DS"}
{"created":"2024-04-25 16:36:54","title":"Constrained Level Planarity is FPT with Respect to the Vertex Cover Number","abstract":"The problem Level Planarity asks for a crossing-free drawing of a graph in the plane such that vertices are placed at prescribed y-coordinates (called levels) and such that every edge is realized as a y-monotone curve. In the variant Constrained Level Planarity, each level y is equipped with a partial order <_y on its vertices and in the desired drawing the left-to-right order of vertices on level y has to be a linear extension of <_y. Constrained Level Planarity is known to be a remarkably difficult problem: previous results by Klemz and Rote [ACM Trans. Alg. 2019] and by Br\\\"uckner and Rutter [SODA 2017] imply that it remains NP-hard even when restricted to graphs whose tree-depth and feedback vertex set number are bounded by a constant and even when the instances are additionally required to be either proper, meaning that each edge spans two consecutive levels, or ordered, meaning that all given partial orders are total orders. In particular, these results rule out the existence of FPT-time (even XP-time) algorithms with respect to these and related graph parameters (unless P=NP). However, the parameterized complexity of Constrained Level Planarity with respect to the vertex cover number of the input graph remained open.   In this paper, we show that Constrained Level Planarity can be solved in FPT-time when parameterized by the vertex cover number. In view of the previous intractability statements, our result is best-possible in several regards: a speed-up to polynomial time or a generalization to the aforementioned smaller graph parameters is not possible, even if restricting to proper or ordered instances.","sentences":["The problem Level Planarity asks for a crossing-free drawing of a graph in the plane such that vertices are placed at prescribed y-coordinates (called levels) and such that every edge is realized as a y-monotone curve.","In the variant Constrained Level Planarity, each level y is equipped with a partial order <_y on its vertices and in the desired drawing the left-to-right order of vertices on level y has to be a linear extension of <_y. Constrained Level Planarity is known to be a remarkably difficult problem: previous results by Klemz and Rote [ACM Trans.","Alg. 2019]","and by Br\\\"uckner and Rutter [SODA 2017] imply that it remains NP-hard even when restricted to graphs whose tree-depth and feedback vertex set number are bounded by a constant and even when the instances are additionally required to be either proper, meaning that each edge spans two consecutive levels, or ordered, meaning that all given partial orders are total orders.","In particular, these results rule out the existence of FPT-time (even XP-time) algorithms with respect to these and related graph parameters (unless P=NP).","However, the parameterized complexity of Constrained Level Planarity with respect to the vertex cover number of the input graph remained open.   ","In this paper, we show that Constrained Level Planarity can be solved in FPT-time when parameterized by the vertex cover number.","In view of the previous intractability statements, our result is best-possible in several regards: a speed-up to polynomial time or a generalization to the aforementioned smaller graph parameters is not possible, even if restricting to proper or ordered instances."],"url":"http://arxiv.org/abs/2404.16723v1","category":"cs.DS"}
{"created":"2024-04-25 16:33:19","title":"Distilling Privileged Information for Dubins Traveling Salesman Problems with Neighborhoods","abstract":"This paper presents a novel learning approach for Dubins Traveling Salesman Problems(DTSP) with Neighborhood (DTSPN) to quickly produce a tour of a non-holonomic vehicle passing through neighborhoods of given task points. The method involves two learning phases: initially, a model-free reinforcement learning approach leverages privileged information to distill knowledge from expert trajectories generated by the LinKernighan heuristic (LKH) algorithm. Subsequently, a supervised learning phase trains an adaptation network to solve problems independently of privileged information. Before the first learning phase, a parameter initialization technique using the demonstration data was also devised to enhance training efficiency. The proposed learning method produces a solution about 50 times faster than LKH and substantially outperforms other imitation learning and RL with demonstration schemes, most of which fail to sense all the task points.","sentences":["This paper presents a novel learning approach for Dubins Traveling Salesman Problems(DTSP) with Neighborhood (DTSPN) to quickly produce a tour of a non-holonomic vehicle passing through neighborhoods of given task points.","The method involves two learning phases: initially, a model-free reinforcement learning approach leverages privileged information to distill knowledge from expert trajectories generated by the LinKernighan heuristic (LKH) algorithm.","Subsequently, a supervised learning phase trains an adaptation network to solve problems independently of privileged information.","Before the first learning phase, a parameter initialization technique using the demonstration data was also devised to enhance training efficiency.","The proposed learning method produces a solution about 50 times faster than LKH and substantially outperforms other imitation learning and RL with demonstration schemes, most of which fail to sense all the task points."],"url":"http://arxiv.org/abs/2404.16721v1","category":"cs.AI"}
{"created":"2024-04-25 16:31:59","title":"Simulations of gravitational collapse in null coordinates: III. Hyperbolicity","abstract":"We investigate the well-posedness of the characteristic initial-boundary value problem for the Einstein equations in Bondi-like coordinates (including Bondi, double-null and affine). We propose a definition of strong hyperbolicity of a system of partial differential equations of any order, and show that the Einstein equations in Bondi-like coordinates in their second-order form used in numerical relativity do not meet it, in agreement with results of Giannakopoulos et al for specific first-order reductions. In the principal part, frozen coefficient approximation that one uses to examine hyperbolicity, we explicitly construct the general solution to identify the solutions that obstruct strong hyperbolicity. Independently, we present a first-order symmetric hyperbolic formulation of the Einstein equations in Bondi gauge, linearised about Schwarzschild, thus completing work by Frittelli. This establishes an energy norm ($L^2$ in the metric perturbations and selected first and second derivatives), in which the initial-boundary value problem, with initial data on an outgoing null cone and boundary data on a timelike cylinder or an ingoing null cone, is well-posed, thus verifying a conjecture by Giannakopoulos et al. Unfortunately, our method does not extend to the pure initial-value problem on a null cone with regular vertex.","sentences":["We investigate the well-posedness of the characteristic initial-boundary value problem for the Einstein equations in Bondi-like coordinates (including Bondi, double-null and affine).","We propose a definition of strong hyperbolicity of a system of partial differential equations of any order, and show that the Einstein equations in Bondi-like coordinates in their second-order form used in numerical relativity do not meet it, in agreement with results of Giannakopoulos et al for specific first-order reductions.","In the principal part, frozen coefficient approximation that one uses to examine hyperbolicity, we explicitly construct the general solution to identify the solutions that obstruct strong hyperbolicity.","Independently, we present a first-order symmetric hyperbolic formulation of the Einstein equations in Bondi gauge, linearised about Schwarzschild, thus completing work by Frittelli.","This establishes an energy norm ($L^2$ in the metric perturbations and selected first and second derivatives), in which the initial-boundary value problem, with initial data on an outgoing null cone and boundary data on a timelike cylinder or an ingoing null cone, is well-posed, thus verifying a conjecture by Giannakopoulos et al.","Unfortunately, our method does not extend to the pure initial-value problem on a null cone with regular vertex."],"url":"http://arxiv.org/abs/2404.16720v1","category":"gr-qc"}
{"created":"2024-04-25 16:30:42","title":"Bulk flows, general relativity and the fundamental role of the \"peculiar\" flux","abstract":"Recent surveys have been reporting bulk peculiar flows considerably faster than expected. Bulk flows are moving matter and matter in motion implies nonzero energy flux. In relativity, as opposed to Newtonian physics, matter fluxes gravitate as well, since they also contribute to the energy-momentum tensor. The gravitational input of the \"peculiar\" flux survives at the linear level and it can drastically change our understanding of the way bulk flows have evolved in time. By default, the Newtonian analysis of peculiar motions bypasses the relativistic flux-contribution to the gravitational field. The problem is that there are also studies, with an otherwise relativistic profile, which inadvertently do the same. As result, these treatments reduce to Newtonian and they misleadingly reproduce the slow Newtonian growth-rate of linear peculiar velocities. In contrast, by accounting for the flux effects, the proper relativistic analysis arrives at a considerably stronger growth. We show that it is the flux input of the moving matter to gravity that separates the relativistic studies of peculiar motions from the rest and, in so doing, it could provide an answer to the bulk-flow question.","sentences":["Recent surveys have been reporting bulk peculiar flows considerably faster than expected.","Bulk flows are moving matter and matter in motion implies nonzero energy flux.","In relativity, as opposed to Newtonian physics, matter fluxes gravitate as well, since they also contribute to the energy-momentum tensor.","The gravitational input of the \"peculiar\" flux survives at the linear level and it can drastically change our understanding of the way bulk flows have evolved in time.","By default, the Newtonian analysis of peculiar motions bypasses the relativistic flux-contribution to the gravitational field.","The problem is that there are also studies, with an otherwise relativistic profile, which inadvertently do the same.","As result, these treatments reduce to Newtonian and they misleadingly reproduce the slow Newtonian growth-rate of linear peculiar velocities.","In contrast, by accounting for the flux effects, the proper relativistic analysis arrives at a considerably stronger growth.","We show that it is the flux input of the moving matter to gravity that separates the relativistic studies of peculiar motions from the rest and, in so doing, it could provide an answer to the bulk-flow question."],"url":"http://arxiv.org/abs/2404.16719v1","category":"gr-qc"}
{"created":"2024-04-25 16:30:30","title":"Features Fusion for Dual-View Mammography Mass Detection","abstract":"Detection of malignant lesions on mammography images is extremely important for early breast cancer diagnosis. In clinical practice, images are acquired from two different angles, and radiologists can fully utilize information from both views, simultaneously locating the same lesion. However, for automatic detection approaches such information fusion remains a challenge. In this paper, we propose a new model called MAMM-Net, which allows the processing of both mammography views simultaneously by sharing information not only on an object level, as seen in existing works, but also on a feature level. MAMM-Net's key component is the Fusion Layer, based on deformable attention and designed to increase detection precision while keeping high recall. Our experiments show superior performance on the public DDSM dataset compared to the previous state-of-the-art model, while introducing new helpful features such as lesion annotation on pixel-level and classification of lesions malignancy.","sentences":["Detection of malignant lesions on mammography images is extremely important for early breast cancer diagnosis.","In clinical practice, images are acquired from two different angles, and radiologists can fully utilize information from both views, simultaneously locating the same lesion.","However, for automatic detection approaches such information fusion remains a challenge.","In this paper, we propose a new model called MAMM-Net, which allows the processing of both mammography views simultaneously by sharing information not only on an object level, as seen in existing works, but also on a feature level.","MAMM-Net's key component is the Fusion Layer, based on deformable attention and designed to increase detection precision while keeping high recall.","Our experiments show superior performance on the public DDSM dataset compared to the previous state-of-the-art model, while introducing new helpful features such as lesion annotation on pixel-level and classification of lesions malignancy."],"url":"http://arxiv.org/abs/2404.16718v1","category":"eess.IV"}
{"created":"2024-04-25 16:29:06","title":"Embracing Diversity: Interpretable Zero-shot classification beyond one vector per class","abstract":"Vision-language models enable open-world classification of objects without the need for any retraining. While this zero-shot paradigm marks a significant advance, even today's best models exhibit skewed performance when objects are dissimilar from their typical depiction. Real world objects such as pears appear in a variety of forms -- from diced to whole, on a table or in a bowl -- yet standard VLM classifiers map all instances of a class to a \\it{single vector based on the class label}. We argue that to represent this rich diversity within a class, zero-shot classification should move beyond a single vector. We propose a method to encode and account for diversity within a class using inferred attributes, still in the zero-shot setting without retraining. We find our method consistently outperforms standard zero-shot classification over a large suite of datasets encompassing hierarchies, diverse object states, and real-world geographic diversity, as well finer-grained datasets where intra-class diversity may be less prevalent. Importantly, our method is inherently interpretable, offering faithful explanations for each inference to facilitate model debugging and enhance transparency. We also find our method scales efficiently to a large number of attributes to account for diversity -- leading to more accurate predictions for atypical instances. Finally, we characterize a principled trade-off between overall and worst class accuracy, which can be tuned via a hyperparameter of our method. We hope this work spurs further research into the promise of zero-shot classification beyond a single class vector for capturing diversity in the world, and building transparent AI systems without compromising performance.","sentences":["Vision-language models enable open-world classification of objects without the need for any retraining.","While this zero-shot paradigm marks a significant advance, even today's best models exhibit skewed performance when objects are dissimilar from their typical depiction.","Real world objects such as pears appear in a variety of forms -- from diced to whole, on a table or in a bowl -- yet standard VLM classifiers map all instances of a class to a \\it{single vector based on the class label}.","We argue that to represent this rich diversity within a class, zero-shot classification should move beyond a single vector.","We propose a method to encode and account for diversity within a class using inferred attributes, still in the zero-shot setting without retraining.","We find our method consistently outperforms standard zero-shot classification over a large suite of datasets encompassing hierarchies, diverse object states, and real-world geographic diversity, as well finer-grained datasets where intra-class diversity may be less prevalent.","Importantly, our method is inherently interpretable, offering faithful explanations for each inference to facilitate model debugging and enhance transparency.","We also find our method scales efficiently to a large number of attributes to account for diversity -- leading to more accurate predictions for atypical instances.","Finally, we characterize a principled trade-off between overall and worst class accuracy, which can be tuned via a hyperparameter of our method.","We hope this work spurs further research into the promise of zero-shot classification beyond a single class vector for capturing diversity in the world, and building transparent AI systems without compromising performance."],"url":"http://arxiv.org/abs/2404.16717v1","category":"cs.CV"}
{"created":"2024-04-25 16:22:44","title":"Geometry of paraquaternionic contact structures","abstract":"We introduce the notion of paraquaternionic contact structures (pqc structures), which turns out to be a generalization of the para 3-Sasakian geometry. We derive a distinguished linear connection preserving the pqc structure. Its torsion tensor is expressed explicitly in terms of the structure tensors. We define pqc-Einstein manifolds and show that para 3-Sasakian spaces are precisely pqc manifolds, which are pqc-Einstein. We introduce the paraquaternionic Heisenberg qroup and show that it is the flat model of the pqc geometry.","sentences":["We introduce the notion of paraquaternionic contact structures (pqc structures), which turns out to be a generalization of the para 3-Sasakian geometry.","We derive a distinguished linear connection preserving the pqc structure.","Its torsion tensor is expressed explicitly in terms of the structure tensors.","We define pqc-Einstein manifolds and show that para 3-Sasakian spaces are precisely pqc manifolds, which are pqc-Einstein.","We introduce the paraquaternionic Heisenberg qroup and show that it is the flat model of the pqc geometry."],"url":"http://arxiv.org/abs/2404.16713v1","category":"math.DG"}
{"created":"2024-04-25 16:20:23","title":"Layer Skip: Enabling Early Exit Inference and Self-Speculative Decoding","abstract":"We present LayerSkip, an end-to-end solution to speed-up inference of large language models (LLMs). First, during training we apply layer dropout, with low dropout rates for earlier layers and higher dropout rates for later layers, and an early exit loss where all transformer layers share the same exit. Second, during inference, we show that this training recipe increases the accuracy of early exit at earlier layers, without adding any auxiliary layers or modules to the model. Third, we present a novel self-speculative decoding solution where we exit at early layers and verify and correct with remaining layers of the model. Our proposed self-speculative decoding approach has less memory footprint than other speculative decoding approaches and benefits from shared compute and activations of the draft and verification stages. We run experiments on different Llama model sizes on different types of training: pretraining from scratch, continual pretraining, finetuning on specific data domain, and finetuning on specific task. We implement our inference solution and show speedups of up to 2.16x on summarization for CNN/DM documents, 1.82x on coding, and 2.0x on TOPv2 semantic parsing task.","sentences":["We present LayerSkip, an end-to-end solution to speed-up inference of large language models (LLMs).","First, during training we apply layer dropout, with low dropout rates for earlier layers and higher dropout rates for later layers, and an early exit loss where all transformer layers share the same exit.","Second, during inference, we show that this training recipe increases the accuracy of early exit at earlier layers, without adding any auxiliary layers or modules to the model.","Third, we present a novel self-speculative decoding solution where we exit at early layers and verify and correct with remaining layers of the model.","Our proposed self-speculative decoding approach has less memory footprint than other speculative decoding approaches and benefits from shared compute and activations of the draft and verification stages.","We run experiments on different Llama model sizes on different types of training: pretraining from scratch, continual pretraining, finetuning on specific data domain, and finetuning on specific task.","We implement our inference solution and show speedups of up to 2.16x on summarization for CNN/DM documents, 1.82x on coding, and 2.0x on TOPv2 semantic parsing task."],"url":"http://arxiv.org/abs/2404.16710v1","category":"cs.CL"}
{"created":"2024-04-25 16:11:46","title":"Efficient and Near-Optimal Noise Generation for Streaming Differential Privacy","abstract":"In the task of differentially private (DP) continual counting, we receive a stream of increments and our goal is to output an approximate running total of these increments, without revealing too much about any specific increment. Despite its simplicity, differentially private continual counting has attracted significant attention both in theory and in practice. Existing algorithms for differentially private continual counting are either inefficient in terms of their space usage or add an excessive amount of noise, inducing suboptimal utility.   The most practical DP continual counting algorithms add carefully correlated Gaussian noise to the values. The task of choosing the covariance for this noise can be expressed in terms of factoring the lower-triangular matrix of ones (which computes prefix sums). We present two approaches from this class (for different parameter regimes) that achieve near-optimal utility for DP continual counting and only require logarithmic or polylogarithmic space (and time).   Our first approach is based on a space-efficient streaming matrix multiplication algorithm for a class of Toeplitz matrices. We show that to instantiate this algorithm for DP continual counting, it is sufficient to find a low-degree rational function that approximates the square root on a circle in the complex plane. We then apply and extend tools from approximation theory to achieve this. We also derive efficient closed-forms for the objective function for arbitrarily many steps, and show direct numerical optimization yields a highly practical solution to the problem. Our second approach combines our first approach with a recursive construction similar to the binary tree mechanism.","sentences":["In the task of differentially private (DP) continual counting, we receive a stream of increments and our goal is to output an approximate running total of these increments, without revealing too much about any specific increment.","Despite its simplicity, differentially private continual counting has attracted significant attention both in theory and in practice.","Existing algorithms for differentially private continual counting are either inefficient in terms of their space usage or add an excessive amount of noise, inducing suboptimal utility.   ","The most practical DP continual counting algorithms add carefully correlated Gaussian noise to the values.","The task of choosing the covariance for this noise can be expressed in terms of factoring the lower-triangular matrix of ones (which computes prefix sums).","We present two approaches from this class (for different parameter regimes) that achieve near-optimal utility for DP continual counting and only require logarithmic or polylogarithmic space (and time).   ","Our first approach is based on a space-efficient streaming matrix multiplication algorithm for a class of Toeplitz matrices.","We show that to instantiate this algorithm for DP continual counting, it is sufficient to find a low-degree rational function that approximates the square root on a circle in the complex plane.","We then apply and extend tools from approximation theory to achieve this.","We also derive efficient closed-forms for the objective function for arbitrarily many steps, and show direct numerical optimization yields a highly practical solution to the problem.","Our second approach combines our first approach with a recursive construction similar to the binary tree mechanism."],"url":"http://arxiv.org/abs/2404.16706v1","category":"cs.DS"}
{"created":"2024-04-25 16:09:46","title":"SHINE: Social Homology Identification for Navigation in Crowded Environments","abstract":"Navigating mobile robots in social environments remains a challenging task due to the intricacies of human-robot interactions. Most of the motion planners designed for crowded and dynamic environments focus on choosing the best velocity to reach the goal while avoiding collisions, but do not explicitly consider the high-level navigation behavior (avoiding through the left or right side, letting others pass or passing before others, etc.). In this work, we present a novel motion planner that incorporates topology distinct paths representing diverse navigation strategies around humans. The planner selects the topology class that imitates human behavior the best using a deep neural network model trained on real-world human motion data, ensuring socially intelligent and contextually aware navigation. Our system refines the chosen path through an optimization-based local planner in real time, ensuring seamless adherence to desired social behaviors. In this way, we decouple perception and local planning from the decision-making process. We evaluate the prediction accuracy of the network with real-world data. In addition, we assess the navigation capabilities in both simulation and a real-world platform, comparing it with other state-of-the-art planners. We demonstrate that our planner exhibits socially desirable behaviors and shows a smooth and remarkable performance.","sentences":["Navigating mobile robots in social environments remains a challenging task due to the intricacies of human-robot interactions.","Most of the motion planners designed for crowded and dynamic environments focus on choosing the best velocity to reach the goal while avoiding collisions, but do not explicitly consider the high-level navigation behavior (avoiding through the left or right side, letting others pass or passing before others, etc.).","In this work, we present a novel motion planner that incorporates topology distinct paths representing diverse navigation strategies around humans.","The planner selects the topology class that imitates human behavior the best using a deep neural network model trained on real-world human motion data, ensuring socially intelligent and contextually aware navigation.","Our system refines the chosen path through an optimization-based local planner in real time, ensuring seamless adherence to desired social behaviors.","In this way, we decouple perception and local planning from the decision-making process.","We evaluate the prediction accuracy of the network with real-world data.","In addition, we assess the navigation capabilities in both simulation and a real-world platform, comparing it with other state-of-the-art planners.","We demonstrate that our planner exhibits socially desirable behaviors and shows a smooth and remarkable performance."],"url":"http://arxiv.org/abs/2404.16705v1","category":"cs.RO"}
{"created":"2024-04-25 16:05:44","title":"Generalized boost transformations in finite volumes and application to Hamiltonian methods","abstract":"The investigation of hadron interactions within lattice QCD has been facilitated by the well-known quantisation condition, linking scattering phase shifts to finite-volume energies. Additionally, the ability to utilise systems at finite total boosts has been pivotal in smoothly charting the energy-dependent behaviour of these phase shifts. The existing implementations of the quantization condition at finite boosts rely on momentum transformations between rest and moving frames, defined directly in terms of the energy eigenvalues. This energy dependence is unsuitable in the formulation of a Hamiltonian.In this work, we introduce a novel approach to generalise the three-momentum boost prescription, enabling the incorporation of energy-independent finite-volume Hamiltonians within moving frames. We demonstrate the application of our method through numerical comparisons, employing a phenomenological $\\pi\\pi$ scattering example.","sentences":["The investigation of hadron interactions within lattice QCD has been facilitated by the well-known quantisation condition, linking scattering phase shifts to finite-volume energies.","Additionally, the ability to utilise systems at finite total boosts has been pivotal in smoothly charting the energy-dependent behaviour of these phase shifts.","The existing implementations of the quantization condition at finite boosts rely on momentum transformations between rest and moving frames, defined directly in terms of the energy eigenvalues.","This energy dependence is unsuitable in the formulation of a Hamiltonian.","In this work, we introduce a novel approach to generalise the three-momentum boost prescription, enabling the incorporation of energy-independent finite-volume Hamiltonians within moving frames.","We demonstrate the application of our method through numerical comparisons, employing a phenomenological $\\pi\\pi$ scattering example."],"url":"http://arxiv.org/abs/2404.16702v1","category":"hep-lat"}
{"created":"2024-04-25 16:00:41","title":"Generalized cyclic symmetric decompositions for the matrix multiplication tensor","abstract":"In this paper we propose a new generalized cyclic symmetric structure in the factor matrices of polyadic decompositions of matrix multiplication tensors for non-square matrix multiplication to reduce the number of variables in the optimization problem and in this way improve the convergence.","sentences":["In this paper we propose a new generalized cyclic symmetric structure in the factor matrices of polyadic decompositions of matrix multiplication tensors for non-square matrix multiplication to reduce the number of variables in the optimization problem and in this way improve the convergence."],"url":"http://arxiv.org/abs/2404.16699v1","category":"math.NA"}
{"created":"2024-04-25 15:59:16","title":"Cooperate or Collapse: Emergence of Sustainability Behaviors in a Society of LLM Agents","abstract":"In the rapidly evolving field of artificial intelligence, ensuring safe decision-making of Large Language Models (LLMs) is a significant challenge. This paper introduces Governance of the Commons Simulation (GovSim), a simulation platform designed to study strategic interactions and cooperative decision-making in LLMs. Through this simulation environment, we explore the dynamics of resource sharing among AI agents, highlighting the importance of ethical considerations, strategic planning, and negotiation skills. GovSim is versatile and supports any text-based agent, including LLMs agents. Using the Generative Agent framework, we create a standard agent that facilitates the integration of different LLMs. Our findings reveal that within GovSim, only two out of 15 tested LLMs managed to achieve a sustainable outcome, indicating a significant gap in the ability of models to manage shared resources. Furthermore, we find that by removing the ability of agents to communicate, they overuse the shared resource, highlighting the importance of communication for cooperation. Interestingly, most LLMs lack the ability to make universalized hypotheses, which highlights a significant weakness in their reasoning skills. We open source the full suite of our research results, including the simulation environment, agent prompts, and a comprehensive web interface.","sentences":["In the rapidly evolving field of artificial intelligence, ensuring safe decision-making of Large Language Models (LLMs) is a significant challenge.","This paper introduces Governance of the Commons Simulation (GovSim), a simulation platform designed to study strategic interactions and cooperative decision-making in LLMs.","Through this simulation environment, we explore the dynamics of resource sharing among AI agents, highlighting the importance of ethical considerations, strategic planning, and negotiation skills.","GovSim is versatile and supports any text-based agent, including LLMs agents.","Using the Generative Agent framework, we create a standard agent that facilitates the integration of different LLMs.","Our findings reveal that within GovSim, only two out of 15 tested LLMs managed to achieve a sustainable outcome, indicating a significant gap in the ability of models to manage shared resources.","Furthermore, we find that by removing the ability of agents to communicate, they overuse the shared resource, highlighting the importance of communication for cooperation.","Interestingly, most LLMs lack the ability to make universalized hypotheses, which highlights a significant weakness in their reasoning skills.","We open source the full suite of our research results, including the simulation environment, agent prompts, and a comprehensive web interface."],"url":"http://arxiv.org/abs/2404.16698v1","category":"cs.CL"}
{"created":"2024-04-25 15:58:09","title":"Report on Candidate Computational Indicators for Conscious Valenced Experience","abstract":"This report enlists 13 functional conditions cashed out in computational terms that have been argued to be constituent of conscious valenced experience. These are extracted from existing empirical and theoretical literature on, among others, animal sentience, medical disorders, anaesthetics, philosophy, evolution, neuroscience, and artificial intelligence.","sentences":["This report enlists 13 functional conditions cashed out in computational terms that have been argued to be constituent of conscious valenced experience.","These are extracted from existing empirical and theoretical literature on, among others, animal sentience, medical disorders, anaesthetics, philosophy, evolution, neuroscience, and artificial intelligence."],"url":"http://arxiv.org/abs/2404.16696v1","category":"q-bio.NC"}
{"created":"2024-04-25 15:58:06","title":"Kernelization Dichotomies for Hitting Subgraphs under Structural Parameterizations","abstract":"For a fixed graph $H$, the $H$-SUBGRAPH HITTING problem consists in deleting the minimum number of vertices from an input graph to obtain a graph without any occurrence of $H$ as a subgraph. This problem can be seen as a generalization of VERTEX COVER, which corresponds to the case $H = K_2$. We initiate a study of $H$-SUBGRAPH HITTING from the point of view of characterizing structural parameterizations that allow for polynomial kernels, within the recently active framework of taking as the parameter the number of vertex deletions to obtain a graph in a \"simple\" class $C$. Our main contribution is to identify graph parameters that, when $H$-SUBGRAPH HITTING is parameterized by the vertex-deletion distance to a class $C$ where any of these parameters is bounded, and assuming standard complexity assumptions and that $H$ is biconnected, allow us to prove the following sharp dichotomy: the problem admits a polynomial kernel if and only if $H$ is a clique. These new graph parameters are inspired by the notion of $C$-elimination distance introduced by Bulian and Dawar [Algorithmica 2016], and generalize it in two directions. Our results also apply to the version of the problem where one wants to hit $H$ as an induced subgraph, and imply in particular, that the problems of hitting minors and hitting (induced) subgraphs have a substantially different behavior with respect to the existence of polynomial kernels under structural parameterizations.","sentences":["For a fixed graph $H$, the $H$-SUBGRAPH HITTING problem consists in deleting the minimum number of vertices from an input graph to obtain a graph without any occurrence of $H$ as a subgraph.","This problem can be seen as a generalization of VERTEX COVER, which corresponds to the case $H = K_2$.","We initiate a study of $H$-SUBGRAPH HITTING from the point of view of characterizing structural parameterizations that allow for polynomial kernels, within the recently active framework of taking as the parameter the number of vertex deletions to obtain a graph in a \"simple\" class $C$.","Our main contribution is to identify graph parameters that, when $H$-SUBGRAPH HITTING is parameterized by the vertex-deletion distance to a class $C$ where any of these parameters is bounded, and assuming standard complexity assumptions and that $H$ is biconnected, allow us to prove the following sharp dichotomy: the problem admits a polynomial kernel if and only if $H$ is a clique.","These new graph parameters are inspired by the notion of $C$-elimination distance introduced by Bulian and Dawar","[Algorithmica 2016], and generalize it in two directions.","Our results also apply to the version of the problem where one wants to hit $H$ as an induced subgraph, and imply in particular, that the problems of hitting minors and hitting (induced) subgraphs have a substantially different behavior with respect to the existence of polynomial kernels under structural parameterizations."],"url":"http://arxiv.org/abs/2404.16695v1","category":"cs.DS"}
{"created":"2024-04-25 15:56:23","title":"A non-separable progressive multivariate WENO-$2r$ point value","abstract":"The weighted essentially non-oscillatory {technique} using a stencil of $2r$ points (WENO-$2r$) is an interpolatory method that consists in obtaining a higher approximation order from the non-linear combination of interpolants of $r+1$ nodes. The result is an interpolant of order $2r$ at the smooth parts and order $r+1$ when an isolated discontinuity falls at any grid interval of the large stencil except at the central one. Recently, a new WENO method based on Aitken-Neville's algorithm has been designed for interpolation of equally spaced data at the mid-points and presents progressive order of accuracy close to discontinuities. This paper is devoted to constructing a general progressive WENO method for non-necessarily uniformly spaced data and several variables interpolating in any point of the central interval. Also, we provide explicit formulas for linear and non-linear weights and prove the order obtained. Finally, some numerical experiments are presented to check the theoretical results.","sentences":["The weighted essentially non-oscillatory {technique} using a stencil of $2r$ points (WENO-$2r$) is an interpolatory method that consists in obtaining a higher approximation order from the non-linear combination of interpolants of $r+1$ nodes.","The result is an interpolant of order $2r$ at the smooth parts and order $r+1$ when an isolated discontinuity falls at any grid interval of the large stencil except at the central one.","Recently, a new WENO method based on Aitken-Neville's algorithm has been designed for interpolation of equally spaced data at the mid-points and presents progressive order of accuracy close to discontinuities.","This paper is devoted to constructing a general progressive WENO method for non-necessarily uniformly spaced data and several variables interpolating in any point of the central interval.","Also, we provide explicit formulas for linear and non-linear weights and prove the order obtained.","Finally, some numerical experiments are presented to check the theoretical results."],"url":"http://arxiv.org/abs/2404.16694v1","category":"math.NA"}
{"created":"2024-04-25 15:53:00","title":"Influence of Solution Efficiency and Valence of Instruction on Additive and Subtractive Solution Strategies in Humans and GPT-4","abstract":"We explored the addition bias, a cognitive tendency to prefer adding elements over removing them to alter an initial state or structure, by conducting four preregistered experiments examining the problem-solving behavior of both humans and OpenAl's GPT-4 large language model. The experiments involved 588 participants from the U.S. and 680 iterations of the GPT-4 model. The problem-solving task was either to create symmetry within a grid (Experiments 1 and 3) or to edit a summary (Experiments 2 and 4). As hypothesized, we found that overall, the addition bias was present. Solution efficiency (Experiments 1 and 2) and valence of the instruction (Experiments 3 and 4) played important roles. Human participants were less likely to use additive strategies when subtraction was relatively more efficient than when addition and subtraction were equally efficient. GPT-4 exhibited the opposite behavior, with a strong addition bias when subtraction was more efficient. In terms of instruction valence, GPT-4 was more likely to add words when asked to \"improve\" compared to \"edit\", whereas humans did not show this effect. When we looked at the addition bias under different conditions, we found more biased responses for GPT-4 compared to humans. Our findings highlight the importance of considering comparable and sometimes superior subtractive alternatives, as well as reevaluating one's own and particularly the language models' problem-solving behavior.","sentences":["We explored the addition bias, a cognitive tendency to prefer adding elements over removing them to alter an initial state or structure, by conducting four preregistered experiments examining the problem-solving behavior of both humans and OpenAl's GPT-4 large language model.","The experiments involved 588 participants from the U.S. and 680 iterations of the GPT-4 model.","The problem-solving task was either to create symmetry within a grid (Experiments 1 and 3) or to edit a summary (Experiments 2 and 4).","As hypothesized, we found that overall, the addition bias was present.","Solution efficiency (Experiments 1 and 2) and valence of the instruction (Experiments 3 and 4) played important roles.","Human participants were less likely to use additive strategies when subtraction was relatively more efficient than when addition and subtraction were equally efficient.","GPT-4 exhibited the opposite behavior, with a strong addition bias when subtraction was more efficient.","In terms of instruction valence, GPT-4 was more likely to add words when asked to \"improve\" compared to \"edit\", whereas humans did not show this effect.","When we looked at the addition bias under different conditions, we found more biased responses for GPT-4 compared to humans.","Our findings highlight the importance of considering comparable and sometimes superior subtractive alternatives, as well as reevaluating one's own and particularly the language models' problem-solving behavior."],"url":"http://arxiv.org/abs/2404.16692v1","category":"cs.CL"}
{"created":"2024-04-25 15:50:16","title":"Neutrino many-body flavor evolution: the full Hamiltonian","abstract":"We study neutrino flavor evolution in the quantum many-body approach using the full neutrino-neutrino Hamiltonian, including the usually neglected terms that mediate non-forward scattering processes. Working in the occupation number representation with plane waves as single-particle states, we explore the time evolution of simple initial states with up to $N=10$ neutrinos. We discuss the time evolution of the Loschmidt echo, one body flavor and kinetic observables, and the one-body entanglement entropy. For the small systems considered, we observe `thermalization' of both flavor and momentum degrees of freedom on comparable time scales, with results converging towards expectation values computed within a microcanonical ensemble. We also observe that the inclusion of non-forward processes generates a faster flavor evolution compared to the one induced by the truncated (forward) Hamiltonian.","sentences":["We study neutrino flavor evolution in the quantum many-body approach using the full neutrino-neutrino Hamiltonian, including the usually neglected terms that mediate non-forward scattering processes.","Working in the occupation number representation with plane waves as single-particle states, we explore the time evolution of simple initial states with up to $N=10$ neutrinos.","We discuss the time evolution of the Loschmidt echo, one body flavor and kinetic observables, and the one-body entanglement entropy.","For the small systems considered, we observe `thermalization' of both flavor and momentum degrees of freedom on comparable time scales, with results converging towards expectation values computed within a microcanonical ensemble.","We also observe that the inclusion of non-forward processes generates a faster flavor evolution compared to the one induced by the truncated (forward) Hamiltonian."],"url":"http://arxiv.org/abs/2404.16690v1","category":"hep-ph"}
{"created":"2024-04-25 15:48:40","title":"Learning to Beat ByteRL: Exploitability of Collectible Card Game Agents","abstract":"While Poker, as a family of games, has been studied extensively in the last decades, collectible card games have seen relatively little attention. Only recently have we seen an agent that can compete with professional human players in Hearthstone, one of the most popular collectible card games. Although artificial agents must be able to work with imperfect information in both of these genres, collectible card games pose another set of distinct challenges. Unlike in many poker variants, agents must deal with state space so vast that even enumerating all states consistent with the agent's beliefs is intractable, rendering the current search methods unusable and requiring the agents to opt for other techniques. In this paper, we investigate the strength of such techniques for this class of games. Namely, we present preliminary analysis results of ByteRL, the state-of-the-art agent in Legends of Code and Magic and Hearthstone. Although ByteRL beat a top-10 Hearthstone player from China, we show that its play in Legends of Code and Magic is highly exploitable.","sentences":["While Poker, as a family of games, has been studied extensively in the last decades, collectible card games have seen relatively little attention.","Only recently have we seen an agent that can compete with professional human players in Hearthstone, one of the most popular collectible card games.","Although artificial agents must be able to work with imperfect information in both of these genres, collectible card games pose another set of distinct challenges.","Unlike in many poker variants, agents must deal with state space so vast that even enumerating all states consistent with the agent's beliefs is intractable, rendering the current search methods unusable and requiring the agents to opt for other techniques.","In this paper, we investigate the strength of such techniques for this class of games.","Namely, we present preliminary analysis results of ByteRL, the state-of-the-art agent in Legends of Code and Magic and Hearthstone.","Although ByteRL beat a top-10 Hearthstone player from China, we show that its play in Legends of Code and Magic is highly exploitable."],"url":"http://arxiv.org/abs/2404.16689v1","category":"cs.AI"}
{"created":"2024-04-25 15:36:18","title":"NTIRE 2024 Quality Assessment of AI-Generated Content Challenge","abstract":"This paper reports on the NTIRE 2024 Quality Assessment of AI-Generated Content Challenge, which will be held in conjunction with the New Trends in Image Restoration and Enhancement Workshop (NTIRE) at CVPR 2024. This challenge is to address a major challenge in the field of image and video processing, namely, Image Quality Assessment (IQA) and Video Quality Assessment (VQA) for AI-Generated Content (AIGC). The challenge is divided into the image track and the video track. The image track uses the AIGIQA-20K, which contains 20,000 AI-Generated Images (AIGIs) generated by 15 popular generative models. The image track has a total of 318 registered participants. A total of 1,646 submissions are received in the development phase, and 221 submissions are received in the test phase. Finally, 16 participating teams submitted their models and fact sheets. The video track uses the T2VQA-DB, which contains 10,000 AI-Generated Videos (AIGVs) generated by 9 popular Text-to-Video (T2V) models. A total of 196 participants have registered in the video track. A total of 991 submissions are received in the development phase, and 185 submissions are received in the test phase. Finally, 12 participating teams submitted their models and fact sheets. Some methods have achieved better results than baseline methods, and the winning methods in both tracks have demonstrated superior prediction performance on AIGC.","sentences":["This paper reports on the NTIRE 2024 Quality Assessment of AI-Generated Content Challenge, which will be held in conjunction with the New Trends in Image Restoration and Enhancement Workshop (NTIRE) at CVPR 2024.","This challenge is to address a major challenge in the field of image and video processing, namely, Image Quality Assessment (IQA) and Video Quality Assessment (VQA) for AI-Generated Content (AIGC).","The challenge is divided into the image track and the video track.","The image track uses the AIGIQA-20K, which contains 20,000 AI-Generated Images (AIGIs) generated by 15 popular generative models.","The image track has a total of 318 registered participants.","A total of 1,646 submissions are received in the development phase, and 221 submissions are received in the test phase.","Finally, 16 participating teams submitted their models and fact sheets.","The video track uses the T2VQA-DB, which contains 10,000 AI-Generated Videos (AIGVs) generated by 9 popular Text-to-Video (T2V) models.","A total of 196 participants have registered in the video track.","A total of 991 submissions are received in the development phase, and 185 submissions are received in the test phase.","Finally, 12 participating teams submitted their models and fact sheets.","Some methods have achieved better results than baseline methods, and the winning methods in both tracks have demonstrated superior prediction performance on AIGC."],"url":"http://arxiv.org/abs/2404.16687v1","category":"cs.CV"}
{"created":"2024-04-25 15:33:23","title":"Multi-scale HSV Color Feature Embedding for High-fidelity NIR-to-RGB Spectrum Translation","abstract":"The NIR-to-RGB spectral domain translation is a formidable task due to the inherent spectral mapping ambiguities within NIR inputs and RGB outputs. Thus, existing methods fail to reconcile the tension between maintaining texture detail fidelity and achieving diverse color variations. In this paper, we propose a Multi-scale HSV Color Feature Embedding Network (MCFNet) that decomposes the mapping process into three sub-tasks, including NIR texture maintenance, coarse geometry reconstruction, and RGB color prediction. Thus, we propose three key modules for each corresponding sub-task: the Texture Preserving Block (TPB), the HSV Color Feature Embedding Module (HSV-CFEM), and the Geometry Reconstruction Module (GRM). These modules contribute to our MCFNet methodically tackling spectral translation through a series of escalating resolutions, progressively enriching images with color and texture fidelity in a scale-coherent fashion. The proposed MCFNet demonstrates substantial performance gains over the NIR image colorization task. Code is released at: https://github.com/AlexYangxx/MCFNet.","sentences":["The NIR-to-RGB spectral domain translation is a formidable task due to the inherent spectral mapping ambiguities within NIR inputs and RGB outputs.","Thus, existing methods fail to reconcile the tension between maintaining texture detail fidelity and achieving diverse color variations.","In this paper, we propose a Multi-scale HSV Color Feature Embedding Network (MCFNet) that decomposes the mapping process into three sub-tasks, including NIR texture maintenance, coarse geometry reconstruction, and RGB color prediction.","Thus, we propose three key modules for each corresponding sub-task: the Texture Preserving Block (TPB), the HSV Color Feature Embedding Module (HSV-CFEM), and the Geometry Reconstruction Module (GRM).","These modules contribute to our MCFNet methodically tackling spectral translation through a series of escalating resolutions, progressively enriching images with color and texture fidelity in a scale-coherent fashion.","The proposed MCFNet demonstrates substantial performance gains over the NIR image colorization task.","Code is released at: https://github.com/AlexYangxx/MCFNet."],"url":"http://arxiv.org/abs/2404.16685v1","category":"cs.CV"}
{"created":"2024-04-25 15:30:15","title":"Unrevealing the existence of nontensorial gravitational-wave polarizations from individual supermassive black hole binaries with pulsar timing arrays","abstract":"With the strong evidence for a gravitational wave (GW) background in the nanohertz frequency band from pulsar timing arrays, the detection of continuous GWs from individual supermassive black hole binaries is already at the dawn. Utilizing continuous GWs to test theories of gravity, especially to test the polarizations of GWs is becoming more and more realistic. In this theoretical study, assuming a detection of signals from individual supermassive binary black holes, we use the null stream to estimate the capability of identifying the nontensorial polarizations of GWs. We consider cases for the nontensorial polarizations where the dipole radiation and quadrupole radiation dominate separately. With a frequentist method, we estimate the threshold of the nontensor-to-tensor relative amplitude above which extra polarizations can be detected. We also conduct Bayesian analysis to do parameter estimation with the null stream data. Our treatment provides a data-analysis methodology using the null stream to probe the nontensorial GW polarizations with pulsar timing arrays.","sentences":["With the strong evidence for a gravitational wave (GW) background in the nanohertz frequency band from pulsar timing arrays, the detection of continuous GWs from individual supermassive black hole binaries is already at the dawn.","Utilizing continuous GWs to test theories of gravity, especially to test the polarizations of GWs is becoming more and more realistic.","In this theoretical study, assuming a detection of signals from individual supermassive binary black holes, we use the null stream to estimate the capability of identifying the nontensorial polarizations of GWs.","We consider cases for the nontensorial polarizations where the dipole radiation and quadrupole radiation dominate separately.","With a frequentist method, we estimate the threshold of the nontensor-to-tensor relative amplitude above which extra polarizations can be detected.","We also conduct Bayesian analysis to do parameter estimation with the null stream data.","Our treatment provides a data-analysis methodology using the null stream to probe the nontensorial GW polarizations with pulsar timing arrays."],"url":"http://arxiv.org/abs/2404.16680v1","category":"gr-qc"}
{"created":"2024-04-25 15:28:22","title":"Multimodal Semantic-Aware Automatic Colorization with Diffusion Prior","abstract":"Colorizing grayscale images offers an engaging visual experience. Existing automatic colorization methods often fail to generate satisfactory results due to incorrect semantic colors and unsaturated colors. In this work, we propose an automatic colorization pipeline to overcome these challenges. We leverage the extraordinary generative ability of the diffusion prior to synthesize color with plausible semantics. To overcome the artifacts introduced by the diffusion prior, we apply the luminance conditional guidance. Moreover, we adopt multimodal high-level semantic priors to help the model understand the image content and deliver saturated colors. Besides, a luminance-aware decoder is designed to restore details and enhance overall visual quality. The proposed pipeline synthesizes saturated colors while maintaining plausible semantics. Experiments indicate that our proposed method considers both diversity and fidelity, surpassing previous methods in terms of perceptual realism and gain most human preference.","sentences":["Colorizing grayscale images offers an engaging visual experience.","Existing automatic colorization methods often fail to generate satisfactory results due to incorrect semantic colors and unsaturated colors.","In this work, we propose an automatic colorization pipeline to overcome these challenges.","We leverage the extraordinary generative ability of the diffusion prior to synthesize color with plausible semantics.","To overcome the artifacts introduced by the diffusion prior, we apply the luminance conditional guidance.","Moreover, we adopt multimodal high-level semantic priors to help the model understand the image content and deliver saturated colors.","Besides, a luminance-aware decoder is designed to restore details and enhance overall visual quality.","The proposed pipeline synthesizes saturated colors while maintaining plausible semantics.","Experiments indicate that our proposed method considers both diversity and fidelity, surpassing previous methods in terms of perceptual realism and gain most human preference."],"url":"http://arxiv.org/abs/2404.16678v1","category":"cs.CV"}
{"created":"2024-04-25 15:25:30","title":"Multilayer Correlation Clustering","abstract":"In this paper, we establish Multilayer Correlation Clustering, a novel generalization of Correlation Clustering (Bansal et al., FOCS '02) to the multilayer setting. In this model, we are given a series of inputs of Correlation Clustering (called layers) over the common set $V$. The goal is then to find a clustering of $V$ that minimizes the $\\ell_p$-norm ($p\\geq 1$) of the disagreements vector, which is defined as the vector (with dimension equal to the number of layers), each element of which represents the disagreements of the clustering on the corresponding layer. For this generalization, we first design an $O(L\\log n)$-approximation algorithm, where $L$ is the number of layers, based on the well-known region growing technique. We then study an important special case of our problem, namely the problem with the probability constraint. For this case, we first give an $(\\alpha+2)$-approximation algorithm, where $\\alpha$ is any possible approximation ratio for the single-layer counterpart. For instance, we can take $\\alpha=2.5$ in general (Ailon et al., JACM '08) and $\\alpha=1.73+\\epsilon$ for the unweighted case (Cohen-Addad et al., FOCS '23). Furthermore, we design a $4$-approximation algorithm, which improves the above approximation ratio of $\\alpha+2=4.5$ for the general probability-constraint case. Computational experiments using real-world datasets demonstrate the effectiveness of our proposed algorithms.","sentences":["In this paper, we establish Multilayer Correlation Clustering, a novel generalization of Correlation Clustering (Bansal et al., FOCS '02) to the multilayer setting.","In this model, we are given a series of inputs of Correlation Clustering (called layers) over the common set $V$. The goal is then to find a clustering of $V$ that minimizes the $\\ell_p$-norm ($p\\geq 1$) of the disagreements vector, which is defined as the vector (with dimension equal to the number of layers), each element of which represents the disagreements of the clustering on the corresponding layer.","For this generalization, we first design an $O(L\\log n)$-approximation algorithm, where $L$ is the number of layers, based on the well-known region growing technique.","We then study an important special case of our problem, namely the problem with the probability constraint.","For this case, we first give an $(\\alpha+2)$-approximation algorithm, where $\\alpha$ is any possible approximation ratio for the single-layer counterpart.","For instance, we can take $\\alpha=2.5$ in general (Ailon et al., JACM '08) and $\\alpha=1.73+\\epsilon$ for the unweighted case (Cohen-Addad et al., FOCS '23).","Furthermore, we design a $4$-approximation algorithm, which improves the above approximation ratio of $\\alpha+2=4.5$ for the general probability-constraint case.","Computational experiments using real-world datasets demonstrate the effectiveness of our proposed algorithms."],"url":"http://arxiv.org/abs/2404.16676v1","category":"cs.DS"}
{"created":"2024-04-25 15:24:10","title":"Operator realizations of non-commutative analytic functions","abstract":"A realization or linearization is a triple, $(A,b,c)$, consisting of a $d-$tuple, $A= (A _1, \\cdots, A_d )$, $d\\in \\mathbb{N}$, of bounded linear operators on a separable, complex Hilbert space, $\\mathcal{H}$, and vectors $b,c \\in \\mathcal{H}$. Any such realization defines a (uniformly) analytic non-commutative (NC) function in an open neighbourhood of the origin, $0:= (0, \\cdots , 0)$, of the NC universe of $d-$tuples of square matrices of any fixed size via the formula $h(X) = I \\otimes b^* ( I \\otimes I _{\\mathcal{H}} - \\sum X_j \\otimes A_j ) ^{-1} I \\otimes c$.   It is well-known that an NC function has a finite-dimensional realization if and only if it is a non-commutative rational function that is defined at $0$. Such finite realizations contain valuable information about the NC rational functions they generate. By considering more general, infinite-dimensional realizations we study, construct and characterize more general classes of uniformly analytic NC functions. In particular, we show that an NC function, $h$, is (uniformly) entire, if and only if it has a jointly compact and quasinilpotent realization. Restricting our results to one variable shows that an analytic Taylor-MacLaurin series extends globally to an entire or meromorphic function if and only if it has a realization whose component operator is compact and quasinilpotent, or compact, respectively. This then motivates our definition of the set of global uniformly meromorphic NC functions as the (universal) skew field (of fractions) generated by NC rational expressions in the (semi-free ideal) ring of NC functions with jointly compact realizations.","sentences":["A realization or linearization is a triple, $(A,b,c)$, consisting of a $d-$tuple, $A= (A _1, \\cdots, A_d )$, $d\\in \\mathbb{N}$, of bounded linear operators on a separable, complex Hilbert space, $\\mathcal{H}$, and vectors $b,c \\in \\mathcal{H}$. Any such realization defines a (uniformly) analytic non-commutative (NC) function in an open neighbourhood of the origin, $0:= (0, \\cdots , 0)$, of the NC universe of $d-$tuples of square matrices of any fixed size via the formula $h(X) = I \\otimes b^*","( I \\otimes I _{\\mathcal{H}} - \\sum X_j \\otimes A_j ) ^{-1} I \\otimes c$.   It is well-known that an NC function has a finite-dimensional realization if and only if it is a non-commutative rational function that is defined at $0$. Such finite realizations contain valuable information about the NC rational functions they generate.","By considering more general, infinite-dimensional realizations we study, construct and characterize more general classes of uniformly analytic NC functions.","In particular, we show that an NC function, $h$, is (uniformly) entire, if and only if it has a jointly compact and quasinilpotent realization.","Restricting our results to one variable shows that an analytic Taylor-MacLaurin series extends globally to an entire or meromorphic function if and only if it has a realization whose component operator is compact and quasinilpotent, or compact, respectively.","This then motivates our definition of the set of global uniformly meromorphic NC functions as the (universal) skew field (of fractions) generated by NC rational expressions in the (semi-free ideal) ring of NC functions with jointly compact realizations."],"url":"http://arxiv.org/abs/2404.16675v1","category":"math.FA"}
{"created":"2024-04-25 15:23:57","title":"Thermodynamic Properties, Shadows and Geodesic Motions of Quantum Corrected Spherically Symmetric AdS Black Hole with Phantom Global Monopoles","abstract":"In this paper, we introduce a metric ansatz for describing spherically symmetric quantum corrected black hole (BH) space-time within an AdS background, incorporating both ordinary and phantom global monopoles. Afterwards, we focus into the thermodynamic properties of this BH, calculating essential parameters such as the Hawking temperature and the specific heat capacity. Moving forward, we analyze the effective potential of the system for both null and time-like geodesics, as well as the shadow radius of the BH. Additionally, we compute the emission rate of particles from this BH. Finally, we explore the geodesic equations of motion and visualize the trajectories of massive particles within the BH. Throughout our investigation, we examine how the presence of ordinary and phantom global monopoles, alongside the quantum corrected parameter, influences various thermal properties, the effective potential of the system, the BH shadow radius, energy emission rate, and the trajectories of massive particles. Importantly, through the generation of figures depicting these phenomena, we highlight the distinctions between results obtained with ordinary global monopoles and phantom ones, across a range of quantum corrected parameter values considering small values of the energy scale parameter.","sentences":["In this paper, we introduce a metric ansatz for describing spherically symmetric quantum corrected black hole (BH) space-time within an AdS background, incorporating both ordinary and phantom global monopoles.","Afterwards, we focus into the thermodynamic properties of this BH, calculating essential parameters such as the Hawking temperature and the specific heat capacity.","Moving forward, we analyze the effective potential of the system for both null and time-like geodesics, as well as the shadow radius of the BH.","Additionally, we compute the emission rate of particles from this BH.","Finally, we explore the geodesic equations of motion and visualize the trajectories of massive particles within the BH.","Throughout our investigation, we examine how the presence of ordinary and phantom global monopoles, alongside the quantum corrected parameter, influences various thermal properties, the effective potential of the system, the BH shadow radius, energy emission rate, and the trajectories of massive particles.","Importantly, through the generation of figures depicting these phenomena, we highlight the distinctions between results obtained with ordinary global monopoles and phantom ones, across a range of quantum corrected parameter values considering small values of the energy scale parameter."],"url":"http://arxiv.org/abs/2404.16674v1","category":"gr-qc"}
{"created":"2024-04-25 15:20:57","title":"Illuminating Black Hole Shadow with Dark Matter Annihilation","abstract":"The Event Horizon Telescope (EHT) has revolutionized our ability to study black holes by providing unprecedented spatial resolution and unveiling horizon-scale details. With advancements leading to the next-generation EHT, there is potential to probe even deeper into the black hole's dark region, especially the inner shadow characterized by low-intensity foreground emissions from the jet, thanks to a significant enhancement in dynamic range by two orders of magnitude. We demonstrate how such enhanced observations could transform supermassive black holes into powerful probes for detecting annihilating dark matter, which can form a dense profile in the vicinity of supermassive black holes, by examining the morphology of the black hole image.","sentences":["The Event Horizon Telescope (EHT) has revolutionized our ability to study black holes by providing unprecedented spatial resolution and unveiling horizon-scale details.","With advancements leading to the next-generation EHT, there is potential to probe even deeper into the black hole's dark region, especially the inner shadow characterized by low-intensity foreground emissions from the jet, thanks to a significant enhancement in dynamic range by two orders of magnitude.","We demonstrate how such enhanced observations could transform supermassive black holes into powerful probes for detecting annihilating dark matter, which can form a dense profile in the vicinity of supermassive black holes, by examining the morphology of the black hole image."],"url":"http://arxiv.org/abs/2404.16673v1","category":"hep-ph"}
{"created":"2024-04-25 15:15:36","title":"EmoVIT: Revolutionizing Emotion Insights with Visual Instruction Tuning","abstract":"Visual Instruction Tuning represents a novel learning paradigm involving the fine-tuning of pre-trained language models using task-specific instructions. This paradigm shows promising zero-shot results in various natural language processing tasks but is still unexplored in vision emotion understanding. In this work, we focus on enhancing the model's proficiency in understanding and adhering to instructions related to emotional contexts. Initially, we identify key visual clues critical to visual emotion recognition. Subsequently, we introduce a novel GPT-assisted pipeline for generating emotion visual instruction data, effectively addressing the scarcity of annotated instruction data in this domain. Expanding on the groundwork established by InstructBLIP, our proposed EmoVIT architecture incorporates emotion-specific instruction data, leveraging the powerful capabilities of Large Language Models to enhance performance. Through extensive experiments, our model showcases its proficiency in emotion classification, adeptness in affective reasoning, and competence in comprehending humor. The comparative analysis provides a robust benchmark for Emotion Visual Instruction Tuning in the era of LLMs, providing valuable insights and opening avenues for future exploration in this domain. Our code is available at \\url{https://github.com/aimmemotion/EmoVIT}.","sentences":["Visual Instruction Tuning represents a novel learning paradigm involving the fine-tuning of pre-trained language models using task-specific instructions.","This paradigm shows promising zero-shot results in various natural language processing tasks but is still unexplored in vision emotion understanding.","In this work, we focus on enhancing the model's proficiency in understanding and adhering to instructions related to emotional contexts.","Initially, we identify key visual clues critical to visual emotion recognition.","Subsequently, we introduce a novel GPT-assisted pipeline for generating emotion visual instruction data, effectively addressing the scarcity of annotated instruction data in this domain.","Expanding on the groundwork established by InstructBLIP, our proposed EmoVIT architecture incorporates emotion-specific instruction data, leveraging the powerful capabilities of Large Language Models to enhance performance.","Through extensive experiments, our model showcases its proficiency in emotion classification, adeptness in affective reasoning, and competence in comprehending humor.","The comparative analysis provides a robust benchmark for Emotion Visual Instruction Tuning in the era of LLMs, providing valuable insights and opening avenues for future exploration in this domain.","Our code is available at \\url{https://github.com/aimmemotion/EmoVIT}."],"url":"http://arxiv.org/abs/2404.16670v1","category":"cs.CV"}
{"created":"2024-04-25 15:04:47","title":"Lu.i -- A low-cost electronic neuron for education and outreach","abstract":"With an increasing presence of science throughout all parts of society, there is a rising expectation for researchers to effectively communicate their work and, equally, for teachers to discuss contemporary findings in their classrooms. While the community can resort to an established set of teaching aids for the fundamental concepts of most natural sciences, there is a need for similarly illustrative experiments and demonstrators in neuroscience. We therefore introduce Lu.i: a parametrizable electronic implementation of the leaky-integrate-and-fire neuron model in an engaging form factor. These palm-sized neurons can be used to visualize and experience the dynamics of individual cells and small spiking neural networks. When stimulated with real or simulated sensory input, Lu.i demonstrates brain-inspired information processing in the hands of a student. As such, it is actively used at workshops, in classrooms, and for science communication. As a versatile tool for teaching and outreach, Lu.i nurtures the comprehension of neuroscience research and neuromorphic engineering among future generations of scientists and in the general public.","sentences":["With an increasing presence of science throughout all parts of society, there is a rising expectation for researchers to effectively communicate their work and, equally, for teachers to discuss contemporary findings in their classrooms.","While the community can resort to an established set of teaching aids for the fundamental concepts of most natural sciences, there is a need for similarly illustrative experiments and demonstrators in neuroscience.","We therefore introduce Lu.i: a parametrizable electronic implementation of the leaky-integrate-and-fire neuron model in an engaging form factor.","These palm-sized neurons can be used to visualize and experience the dynamics of individual cells and small spiking neural networks.","When stimulated with real or simulated sensory input, Lu.i demonstrates brain-inspired information processing in the hands of a student.","As such, it is actively used at workshops, in classrooms, and for science communication.","As a versatile tool for teaching and outreach, Lu.i nurtures the comprehension of neuroscience research and neuromorphic engineering among future generations of scientists and in the general public."],"url":"http://arxiv.org/abs/2404.16664v1","category":"q-bio.NC"}
{"created":"2024-04-25 15:04:27","title":"Formal Specification, Assessment, and Enforcement of Fairness for Generative AIs","abstract":"The risk of reinforcing or exacerbating societal biases and inequalities is growing as generative AI increasingly produces content that resembles human output, from text to images and beyond. Here we formally characterize the notion of fairness for generative AI as a basis for monitoring and enforcing fairness. We define two levels of fairness utilizing the concept of infinite words. The first is the fairness demonstrated on the generated sequences, which is only evaluated on the outputs while agnostic to the prompts/models used. The second is the inherent fairness of the generative AI model, which requires that fairness be manifested when input prompts are neutral, that is, they do not explicitly instruct the generative AI to produce a particular type of output. We also study relative intersectional fairness to counteract the combinatorial explosion of fairness when considering multiple categories together with lazy fairness enforcement. Our implemented specification monitoring and enforcement tool shows interesting results when tested against several generative AI models.","sentences":["The risk of reinforcing or exacerbating societal biases and inequalities is growing as generative AI increasingly produces content that resembles human output, from text to images and beyond.","Here we formally characterize the notion of fairness for generative AI as a basis for monitoring and enforcing fairness.","We define two levels of fairness utilizing the concept of infinite words.","The first is the fairness demonstrated on the generated sequences, which is only evaluated on the outputs while agnostic to the prompts/models used.","The second is the inherent fairness of the generative AI model, which requires that fairness be manifested when input prompts are neutral, that is, they do not explicitly instruct the generative AI to produce a particular type of output.","We also study relative intersectional fairness to counteract the combinatorial explosion of fairness when considering multiple categories together with lazy fairness enforcement.","Our implemented specification monitoring and enforcement tool shows interesting results when tested against several generative AI models."],"url":"http://arxiv.org/abs/2404.16663v1","category":"cs.LG"}
{"created":"2024-04-25 14:56:32","title":"Benchmarking Mobile Device Control Agents across Diverse Configurations","abstract":"Developing autonomous agents for mobile devices can significantly enhance user interactions by offering increased efficiency and accessibility. However, despite the growing interest in mobile device control agents, the absence of a commonly adopted benchmark makes it challenging to quantify scientific progress in this area. In this work, we introduce B-MoCA: a novel benchmark designed specifically for evaluating mobile device control agents. To create a realistic benchmark, we develop B-MoCA based on the Android operating system and define 60 common daily tasks. Importantly, we incorporate a randomization feature that changes various aspects of mobile devices, including user interface layouts and language settings, to assess generalization performance. We benchmark diverse agents, including agents employing large language models (LLMs) or multi-modal LLMs as well as agents trained from scratch using human expert demonstrations. While these agents demonstrate proficiency in executing straightforward tasks, their poor performance on complex tasks highlights significant opportunities for future research to enhance their effectiveness. Our source code is publicly available at https://b-moca.github.io.","sentences":["Developing autonomous agents for mobile devices can significantly enhance user interactions by offering increased efficiency and accessibility.","However, despite the growing interest in mobile device control agents, the absence of a commonly adopted benchmark makes it challenging to quantify scientific progress in this area.","In this work, we introduce B-MoCA: a novel benchmark designed specifically for evaluating mobile device control agents.","To create a realistic benchmark, we develop B-MoCA based on the Android operating system and define 60 common daily tasks.","Importantly, we incorporate a randomization feature that changes various aspects of mobile devices, including user interface layouts and language settings, to assess generalization performance.","We benchmark diverse agents, including agents employing large language models (LLMs) or multi-modal LLMs as well as agents trained from scratch using human expert demonstrations.","While these agents demonstrate proficiency in executing straightforward tasks, their poor performance on complex tasks highlights significant opportunities for future research to enhance their effectiveness.","Our source code is publicly available at https://b-moca.github.io."],"url":"http://arxiv.org/abs/2404.16660v1","category":"cs.HC"}
{"created":"2024-04-25 14:55:07","title":"ProbGate at EHRSQL 2024: Enhancing SQL Query Generation Accuracy through Probabilistic Threshold Filtering and Error Handling","abstract":"Recently, deep learning-based language models have significantly enhanced text-to-SQL tasks, with promising applications in retrieving patient records within the medical domain. One notable challenge in such applications is discerning unanswerable queries. Through fine-tuning model, we demonstrate the feasibility of converting medical record inquiries into SQL queries. Additionally, we introduce an entropy-based method to identify and filter out unanswerable results. We further enhance result quality by filtering low-confidence SQL through log probability-based distribution, while grammatical and schema errors are mitigated by executing queries on the actual database. We experimentally verified that our method can filter unanswerable questions, which can be widely utilized even when the parameters of the model are not accessible, and that it can be effectively utilized in practice.","sentences":["Recently, deep learning-based language models have significantly enhanced text-to-SQL tasks, with promising applications in retrieving patient records within the medical domain.","One notable challenge in such applications is discerning unanswerable queries.","Through fine-tuning model, we demonstrate the feasibility of converting medical record inquiries into SQL queries.","Additionally, we introduce an entropy-based method to identify and filter out unanswerable results.","We further enhance result quality by filtering low-confidence SQL through log probability-based distribution, while grammatical and schema errors are mitigated by executing queries on the actual database.","We experimentally verified that our method can filter unanswerable questions, which can be widely utilized even when the parameters of the model are not accessible, and that it can be effectively utilized in practice."],"url":"http://arxiv.org/abs/2404.16659v1","category":"cs.CL"}
{"created":"2024-04-25 14:49:46","title":"Inferring solid-state diffusivity in lithium-ion battery active materials: improving upon the classical GITT method","abstract":"The Galvanostatic Intermittent Titration Technique (GITT) is a ubiquitous method for determining the solid-state diffusivity in lithium-ion battery materials. However, it is notoriously time-consuming and relies upon assumptions whose applicability is questionable. We propose a novel methodology that allows inference of the diffusivity for a more general class of data that is simpler and faster to harvest. We infer the diffusivity (as a function of stoichiometry) by minimising the residual sum of squares between data and solutions to a spherically-symmetric diffusion model in a single representative active material particle. Using data harvested from the NMC cathode of a commercial LG M50 cell we first demonstrate that our method is able to reproduce the diffusivities inferred by the GITT, which requires ten days of galvanostatic intermittent titration data. We then demonstrate that our method reliably reconstructs diffusivity using significantly less data. Despite arising from quick-to-measure data, our method more accurately infers diffusivities. This work is a contribution towards developing faster and more reliable techniques in parameter inference for lithium-ion batteries.","sentences":["The Galvanostatic Intermittent Titration Technique (GITT) is a ubiquitous method for determining the solid-state diffusivity in lithium-ion battery materials.","However, it is notoriously time-consuming and relies upon assumptions whose applicability is questionable.","We propose a novel methodology that allows inference of the diffusivity for a more general class of data that is simpler and faster to harvest.","We infer the diffusivity (as a function of stoichiometry) by minimising the residual sum of squares between data and solutions to a spherically-symmetric diffusion model in a single representative active material particle.","Using data harvested from the NMC cathode of a commercial LG M50 cell we first demonstrate that our method is able to reproduce the diffusivities inferred by the GITT, which requires ten days of galvanostatic intermittent titration data.","We then demonstrate that our method reliably reconstructs diffusivity using significantly less data.","Despite arising from quick-to-measure data, our method more accurately infers diffusivities.","This work is a contribution towards developing faster and more reliable techniques in parameter inference for lithium-ion batteries."],"url":"http://arxiv.org/abs/2404.16658v1","category":"physics.app-ph"}
{"created":"2024-04-25 14:49:40","title":"$p$-adic Hodge parameters in the crystabelline representations of $\\mathrm{GL}_3(\\mathbb{Q}_p)$","abstract":"We build a one-to-one correspondence between $3$-dimensional (generic) crystabelline representations of the absolute Galois group of $\\mathbb{Q}_p$ and certain locally analytic representations of $\\mathrm{GL}_3(\\mathbb{Q}_p)$. We show that the correspondence can be realized in spaces of $p$-adic automorphic representations.","sentences":["We build a one-to-one correspondence between $3$-dimensional (generic) crystabelline representations of the absolute Galois group of $\\mathbb{Q}_p$ and certain locally analytic representations of $\\mathrm{GL}_3(\\mathbb{Q}_p)$. We show that the correspondence can be realized in spaces of $p$-adic automorphic representations."],"url":"http://arxiv.org/abs/2404.16657v1","category":"math.NT"}
{"created":"2024-04-25 14:48:29","title":"A Self-Organizing Clustering System for Unsupervised Distribution Shift Detection","abstract":"Modeling non-stationary data is a challenging problem in the field of continual learning, and data distribution shifts may result in negative consequences on the performance of a machine learning model. Classic learning tools are often vulnerable to perturbations of the input covariates, and are sensitive to outliers and noise, and some tools are based on rigid algebraic assumptions. Distribution shifts are frequently occurring due to changes in raw materials for production, seasonality, a different user base, or even adversarial attacks. Therefore, there is a need for more effective distribution shift detection techniques.   In this work, we propose a continual learning framework for monitoring and detecting distribution changes. We explore the problem in a latent space generated by a bio-inspired self-organizing clustering and statistical aspects of the latent space. In particular, we investigate the projections made by two topology-preserving maps: the Self-Organizing Map and the Scale Invariant Map. Our method can be applied in both a supervised and an unsupervised context. We construct the assessment of changes in the data distribution as a comparison of Gaussian signals, making the proposed method fast and robust. We compare it to other unsupervised techniques, specifically Principal Component Analysis (PCA) and Kernel-PCA. Our comparison involves conducting experiments using sequences of images (based on MNIST and injected shifts with adversarial samples), chemical sensor measurements, and the environmental variable related to ozone levels. The empirical study reveals the potential of the proposed approach.","sentences":["Modeling non-stationary data is a challenging problem in the field of continual learning, and data distribution shifts may result in negative consequences on the performance of a machine learning model.","Classic learning tools are often vulnerable to perturbations of the input covariates, and are sensitive to outliers and noise, and some tools are based on rigid algebraic assumptions.","Distribution shifts are frequently occurring due to changes in raw materials for production, seasonality, a different user base, or even adversarial attacks.","Therefore, there is a need for more effective distribution shift detection techniques.   ","In this work, we propose a continual learning framework for monitoring and detecting distribution changes.","We explore the problem in a latent space generated by a bio-inspired self-organizing clustering and statistical aspects of the latent space.","In particular, we investigate the projections made by two topology-preserving maps: the Self-Organizing Map and the Scale Invariant Map.","Our method can be applied in both a supervised and an unsupervised context.","We construct the assessment of changes in the data distribution as a comparison of Gaussian signals, making the proposed method fast and robust.","We compare it to other unsupervised techniques, specifically Principal Component Analysis (PCA) and Kernel-PCA.","Our comparison involves conducting experiments using sequences of images (based on MNIST and injected shifts with adversarial samples), chemical sensor measurements, and the environmental variable related to ozone levels.","The empirical study reveals the potential of the proposed approach."],"url":"http://arxiv.org/abs/2404.16656v1","category":"cs.LG"}
{"created":"2024-04-25 14:45:49","title":"Two-state transfer: a generalization of pair and plus state transfer","abstract":"In the study of quantum state transfer, one is interested in being able to transmit a quantum state with high fidelity within a quantum spin network. In most of the literature, the state of interest is taken to be associated with a standard basis vector; however, more general states have recently been considered. Here, we consider a general linear combination of two vertex states, which encompasses the definitions of pair states and plus states in connected weighted graphs. A two-state in a graph $X$ is a quantum state of the form $\\mathbf{e}_u+s\\mathbf{e}_v$, where $u$ and $v$ are two vertices in $X$ and $s$ is a non-zero real number. If $s=-1$ or $s=1$, then such a state is called a pair state or a plus state, respectively.   In this paper, we investigate quantum state transfer between two-states, where the Hamiltonian is taken to be the adjacency, Laplacian or signless Laplacian matrix of the graph. By analyzing the spectral properties of the Hamiltonian, we characterize strongly cospectral two-states built from strongly cospectral vertices. This allows us to characterize perfect state transfer (PST) between two-states in complete graphs, cycles and hypercubes. We also produce infinite families of graphs that admit strong cospectrality and PST between two-states that are neither pair nor plus states. Using singular values and singular vectors, we show that vertex PST in the line graph of $X$ implies PST between the plus states formed by corresponding edges in $X$. Furthermore, we provide conditions such that the converse of the previous statement holds. As an application, we characterize strong cospectrality and PST between vertices in line graphs of trees, unicyclic graphs and Cartesian products.","sentences":["In the study of quantum state transfer, one is interested in being able to transmit a quantum state with high fidelity within a quantum spin network.","In most of the literature, the state of interest is taken to be associated with a standard basis vector; however, more general states have recently been considered.","Here, we consider a general linear combination of two vertex states, which encompasses the definitions of pair states and plus states in connected weighted graphs.","A two-state in a graph $X$ is a quantum state of the form $\\mathbf{e}_u+s\\mathbf{e}_v$, where $u$ and $v$ are two vertices in $X$ and $s$ is a non-zero real number.","If $s=-1$ or $s=1$, then such a state is called a pair state or a plus state, respectively.   ","In this paper, we investigate quantum state transfer between two-states, where the Hamiltonian is taken to be the adjacency, Laplacian or signless Laplacian matrix of the graph.","By analyzing the spectral properties of the Hamiltonian, we characterize strongly cospectral two-states built from strongly cospectral vertices.","This allows us to characterize perfect state transfer (PST) between two-states in complete graphs, cycles and hypercubes.","We also produce infinite families of graphs that admit strong cospectrality and PST between two-states that are neither pair nor plus states.","Using singular values and singular vectors, we show that vertex PST in the line graph of $X$ implies PST between the plus states formed by corresponding edges in $X$.","Furthermore, we provide conditions such that the converse of the previous statement holds.","As an application, we characterize strong cospectrality and PST between vertices in line graphs of trees, unicyclic graphs and Cartesian products."],"url":"http://arxiv.org/abs/2404.16654v1","category":"quant-ph"}
{"created":"2024-04-25 14:45:07","title":"An\u00e1lise de ambiguidade lingu\u00edstica em modelos de linguagem de grande escala (LLMs)","abstract":"Linguistic ambiguity continues to represent a significant challenge for natural language processing (NLP) systems, notwithstanding the advancements in architectures such as Transformers and BERT. Inspired by the recent success of instructional models like ChatGPT and Gemini (In 2023, the artificial intelligence was called Bard.), this study aims to analyze and discuss linguistic ambiguity within these models, focusing on three types prevalent in Brazilian Portuguese: semantic, syntactic, and lexical ambiguity. We create a corpus comprising 120 sentences, both ambiguous and unambiguous, for classification, explanation, and disambiguation. The models capability to generate ambiguous sentences was also explored by soliciting sets of sentences for each type of ambiguity. The results underwent qualitative analysis, drawing on recognized linguistic references, and quantitative assessment based on the accuracy of the responses obtained. It was evidenced that even the most sophisticated models, such as ChatGPT and Gemini, exhibit errors and deficiencies in their responses, with explanations often providing inconsistent. Furthermore, the accuracy peaked at 49.58 percent, indicating the need for descriptive studies for supervised learning.","sentences":["Linguistic ambiguity continues to represent a significant challenge for natural language processing (NLP) systems, notwithstanding the advancements in architectures such as Transformers and BERT.","Inspired by the recent success of instructional models like ChatGPT and Gemini (In 2023, the artificial intelligence was called Bard.)",", this study aims to analyze and discuss linguistic ambiguity within these models, focusing on three types prevalent in Brazilian Portuguese: semantic, syntactic, and lexical ambiguity.","We create a corpus comprising 120 sentences, both ambiguous and unambiguous, for classification, explanation, and disambiguation.","The models capability to generate ambiguous sentences was also explored by soliciting sets of sentences for each type of ambiguity.","The results underwent qualitative analysis, drawing on recognized linguistic references, and quantitative assessment based on the accuracy of the responses obtained.","It was evidenced that even the most sophisticated models, such as ChatGPT and Gemini, exhibit errors and deficiencies in their responses, with explanations often providing inconsistent.","Furthermore, the accuracy peaked at 49.58 percent, indicating the need for descriptive studies for supervised learning."],"url":"http://arxiv.org/abs/2404.16653v1","category":"cs.CL"}
{"created":"2024-04-25 14:40:13","title":"Comparison of adaptive mesh refinement techniques for numerical weather prediction","abstract":"This paper examines the application of adaptive mesh refinement (AMR) in the field of numerical weather prediction (NWP). We implement and assess two distinct AMR approaches and evaluate their performance through standard NWP benchmarks. In both cases, we solve the fully compressible Euler equations, fundamental to many non-hydrostatic weather models.   The first approach utilizes oct-tree cell-based mesh refinement coupled with a high-order discontinuous Galerkin method for spatial discretization. In the second approach, we employ level-based AMR with the finite difference method. Our study provides insights into the accuracy and benefits of employing these AMR methodologies for the multi-scale problem of NWP. Additionally, we explore essential properties including their impact on mass and energy conservation. Moreover, we present and evaluate an AMR solution transfer strategy for the tree-based AMR approach that is simple to implement, memory-efficient, and ensures conservation for both flow in the box and sphere.   Furthermore, we discuss scalability, performance portability, and the practical utility of the AMR methodology within an NWP framework -- crucial considerations in selecting an AMR approach. The current de facto standard for mesh refinement in NWP employs a relatively simplistic approach of static nested grids, either within a general circulation model or a separately operated regional model with loose one-way synchronization. It is our hope that this study will stimulate further interest in the adoption of AMR frameworks like AMReX in NWP. These frameworks offer a triple advantage: a robust dynamic AMR for tracking localized and consequential features such as tropical cyclones, extreme scalability, and performance portability.","sentences":["This paper examines the application of adaptive mesh refinement (AMR) in the field of numerical weather prediction (NWP).","We implement and assess two distinct AMR approaches and evaluate their performance through standard NWP benchmarks.","In both cases, we solve the fully compressible Euler equations, fundamental to many non-hydrostatic weather models.   ","The first approach utilizes oct-tree cell-based mesh refinement coupled with a high-order discontinuous Galerkin method for spatial discretization.","In the second approach, we employ level-based AMR with the finite difference method.","Our study provides insights into the accuracy and benefits of employing these AMR methodologies for the multi-scale problem of NWP.","Additionally, we explore essential properties including their impact on mass and energy conservation.","Moreover, we present and evaluate an AMR solution transfer strategy for the tree-based AMR approach that is simple to implement, memory-efficient, and ensures conservation for both flow in the box and sphere.   ","Furthermore, we discuss scalability, performance portability, and the practical utility of the AMR methodology within an NWP framework -- crucial considerations in selecting an AMR approach.","The current de facto standard for mesh refinement in NWP employs a relatively simplistic approach of static nested grids, either within a general circulation model or a separately operated regional model with loose one-way synchronization.","It is our hope that this study will stimulate further interest in the adoption of AMR frameworks like AMReX in NWP.","These frameworks offer a triple advantage: a robust dynamic AMR for tracking localized and consequential features such as tropical cyclones, extreme scalability, and performance portability."],"url":"http://arxiv.org/abs/2404.16648v1","category":"math.NA"}
{"created":"2024-04-25 14:36:00","title":"Application of RESNET50 Convolution Neural Network for the Extraction of Optical Parameters in Scattering Media","abstract":"Estimation of the optical properties of scattering media such as tissue is important in diagnostics as well as in the development of techniques to image deeper. As light penetrates the sample scattering events occur that alter the propagation direction of the photons in a random manner leading degradation of image quality. The distribution of the scattered light does, however, give a measure of the optical properties such as the reduced scattering coefficient and the absorption coefficient. Unfortunately, inverting scattering patterns to recover the optical properties is not simple, especially in the regime where the light is partially randomized. Machine learning has been proposed by several authors as a means of recovering these properties from either the back scattered or the transmitted light. In the present paper, we train a general purpose convolutional neural network RESNET 50 with simulated data based on Monte Carlo simulations. We show that compared with previous work our approach gives comparable or better reconstruction accuracy with training on a much smaller dataset. Moreover, by training on multiple parameters such as the intensity distribution at multiple planes or the exit angle and spatial distribution one achieves improved performance compared to training on a single input such as the intensity distribution captured at the sample surface. While our approach gives good parameter reconstruction, we identify factors that limit the accuracy of the recovered properties, particularly the absorption coefficient. In the light of these limitations, we suggest how the present approach may be enhanced for even better performance.","sentences":["Estimation of the optical properties of scattering media such as tissue is important in diagnostics as well as in the development of techniques to image deeper.","As light penetrates the sample scattering events occur that alter the propagation direction of the photons in a random manner leading degradation of image quality.","The distribution of the scattered light does, however, give a measure of the optical properties such as the reduced scattering coefficient and the absorption coefficient.","Unfortunately, inverting scattering patterns to recover the optical properties is not simple, especially in the regime where the light is partially randomized.","Machine learning has been proposed by several authors as a means of recovering these properties from either the back scattered or the transmitted light.","In the present paper, we train a general purpose convolutional neural network RESNET 50 with simulated data based on Monte Carlo simulations.","We show that compared with previous work our approach gives comparable or better reconstruction accuracy with training on a much smaller dataset.","Moreover, by training on multiple parameters such as the intensity distribution at multiple planes or the exit angle and spatial distribution one achieves improved performance compared to training on a single input such as the intensity distribution captured at the sample surface.","While our approach gives good parameter reconstruction, we identify factors that limit the accuracy of the recovered properties, particularly the absorption coefficient.","In the light of these limitations, we suggest how the present approach may be enhanced for even better performance."],"url":"http://arxiv.org/abs/2404.16647v1","category":"physics.optics"}
{"created":"2024-04-25 14:34:47","title":"Tele-FLM Technical Report","abstract":"Large language models (LLMs) have showcased profound capabilities in language understanding and generation, facilitating a wide array of applications. However, there is a notable paucity of detailed, open-sourced methodologies on efficiently scaling LLMs beyond 50 billion parameters with minimum trial-and-error cost and computational resources. In this report, we introduce Tele-FLM (aka FLM-2), a 52B open-sourced multilingual large language model that features a stable, efficient pre-training paradigm and enhanced factual judgment capabilities. Tele-FLM demonstrates superior multilingual language modeling abilities, measured by BPB on textual corpus. Besides, in both English and Chinese foundation model evaluation, it is comparable to strong open-sourced models that involve larger pre-training FLOPs, such as Llama2-70B and DeepSeek-67B. In addition to the model weights, we share the core designs, engineering practices, and training details, which we expect to benefit both the academic and industrial communities.","sentences":["Large language models (LLMs) have showcased profound capabilities in language understanding and generation, facilitating a wide array of applications.","However, there is a notable paucity of detailed, open-sourced methodologies on efficiently scaling LLMs beyond 50 billion parameters with minimum trial-and-error cost and computational resources.","In this report, we introduce Tele-FLM (aka FLM-2), a 52B open-sourced multilingual large language model that features a stable, efficient pre-training paradigm and enhanced factual judgment capabilities.","Tele-FLM demonstrates superior multilingual language modeling abilities, measured by BPB on textual corpus.","Besides, in both English and Chinese foundation model evaluation, it is comparable to strong open-sourced models that involve larger pre-training FLOPs, such as Llama2-70B and DeepSeek-67B. In addition to the model weights, we share the core designs, engineering practices, and training details, which we expect to benefit both the academic and industrial communities."],"url":"http://arxiv.org/abs/2404.16645v1","category":"cs.CL"}
{"created":"2024-04-25 14:31:20","title":"Subadditivity of shifts, Eilenberg-Zilber shuffle products and homology of lattices","abstract":"We show that the maximal shifts in the minimal free resolution of the quotients of a polynomial ring by a monomial ideal are subadditive as a function of the homological degree. This answers a question that has received some attention in recent years. To do so, we define and study a new model for the homology of posets, given by the so called synor complex. We also introduce an Eilenberg-Zilber type shuffle product on the simplicial chain complex of lattices.   Combining these concepts we prove that the existence of a non-zero homology class for a lattice forces certain non-zero homology classes in lower intervals. This result then translates into properties of the minimal free resolution. In particular, it implies a generalization of the original question.","sentences":["We show that the maximal shifts in the minimal free resolution of the quotients of a polynomial ring by a monomial ideal are subadditive as a function of the homological degree.","This answers a question that has received some attention in recent years.","To do so, we define and study a new model for the homology of posets, given by the so called synor complex.","We also introduce an Eilenberg-Zilber type shuffle product on the simplicial chain complex of lattices.   ","Combining these concepts we prove that the existence of a non-zero homology class for a lattice forces certain non-zero homology classes in lower intervals.","This result then translates into properties of the minimal free resolution.","In particular, it implies a generalization of the original question."],"url":"http://arxiv.org/abs/2404.16643v1","category":"math.AC"}
{"created":"2024-04-25 14:27:28","title":"Honda-Tate theory for log abelian varieties over finite fields","abstract":"In this article we study the Honda-Tate theory for log abelian varieties over an fs log point $S=(\\mathrm{Spec}(\\mathbf{k}),M_S)$ for $\\mathbf{k}=\\mathbb{F}_q$ a finite field, generalizing the classical Honda-Tate theory for abelian varieties over $\\mathbf{k}$. For the standard log point $S$, we give a complete description of the isogeny classes of such log abelian varieties using Weil $q$-numbers of weight 0,1, and 2. In the general case where $M_S$ admits a global chart $P\\to\\mathbf{k}$ with $P=\\mathbb{N}^k$, we also give a complete description of simple isogeny classes of log abelian varieties over $S$ in terms of rational points in generalized simplices.","sentences":["In this article we study the Honda-Tate theory for log abelian varieties over an fs log point $S=(\\mathrm{Spec}(\\mathbf{k}),M_S)$ for $\\mathbf{k}=\\mathbb{F}_q$ a finite field, generalizing the classical Honda-Tate theory for abelian varieties over $\\mathbf{k}$. For the standard log point $S$, we give a complete description of the isogeny classes of such log abelian varieties using Weil $q$-numbers of weight 0,1, and 2.","In the general case where $M_S$ admits a global chart $P\\to\\mathbf{k}$ with $P=\\mathbb{N}^k$, we also give a complete description of simple isogeny classes of log abelian varieties over $S$ in terms of rational points in generalized simplices."],"url":"http://arxiv.org/abs/2404.16639v1","category":"math.NT"}
{"created":"2024-04-25 14:26:53","title":"Privacy-Preserving Statistical Data Generation: Application to Sepsis Detection","abstract":"The biomedical field is among the sectors most impacted by the increasing regulation of Artificial Intelligence (AI) and data protection legislation, given the sensitivity of patient information. However, the rise of synthetic data generation methods offers a promising opportunity for data-driven technologies. In this study, we propose a statistical approach for synthetic data generation applicable in classification problems. We assess the utility and privacy implications of synthetic data generated by Kernel Density Estimator and K-Nearest Neighbors sampling (KDE-KNN) within a real-world context, specifically focusing on its application in sepsis detection. The detection of sepsis is a critical challenge in clinical practice due to its rapid progression and potentially life-threatening consequences. Moreover, we emphasize the benefits of KDE-KNN compared to current synthetic data generation methodologies. Additionally, our study examines the effects of incorporating synthetic data into model training procedures. This investigation provides valuable insights into the effectiveness of synthetic data generation techniques in mitigating regulatory constraints within the biomedical field.","sentences":["The biomedical field is among the sectors most impacted by the increasing regulation of Artificial Intelligence (AI) and data protection legislation, given the sensitivity of patient information.","However, the rise of synthetic data generation methods offers a promising opportunity for data-driven technologies.","In this study, we propose a statistical approach for synthetic data generation applicable in classification problems.","We assess the utility and privacy implications of synthetic data generated by Kernel Density Estimator and K-Nearest Neighbors sampling (KDE-KNN) within a real-world context, specifically focusing on its application in sepsis detection.","The detection of sepsis is a critical challenge in clinical practice due to its rapid progression and potentially life-threatening consequences.","Moreover, we emphasize the benefits of KDE-KNN compared to current synthetic data generation methodologies.","Additionally, our study examines the effects of incorporating synthetic data into model training procedures.","This investigation provides valuable insights into the effectiveness of synthetic data generation techniques in mitigating regulatory constraints within the biomedical field."],"url":"http://arxiv.org/abs/2404.16638v1","category":"cs.LG"}
{"created":"2024-04-25 14:24:41","title":"Zero-Shot Distillation for Image Encoders: How to Make Effective Use of Synthetic Data","abstract":"Multi-modal foundation models such as CLIP have showcased impressive zero-shot capabilities. However, their applicability in resource-constrained environments is limited due to their large number of parameters and high inference time. While existing approaches have scaled down the entire CLIP architecture, we focus on training smaller variants of the image encoder, which suffices for efficient zero-shot classification. The use of synthetic data has shown promise in distilling representations from larger teachers, resulting in strong few-shot and linear probe performance. However, we find that this approach surprisingly fails in true zero-shot settings when using contrastive losses. We identify the exploitation of spurious features as being responsible for poor generalization between synthetic and real data. However, by using the image feature-based L2 distillation loss, we mitigate these problems and train students that achieve zero-shot performance which on four domain-specific datasets is on-par with a ViT-B/32 teacher model trained on DataCompXL, while featuring up to 92% fewer parameters.","sentences":["Multi-modal foundation models such as CLIP have showcased impressive zero-shot capabilities.","However, their applicability in resource-constrained environments is limited due to their large number of parameters and high inference time.","While existing approaches have scaled down the entire CLIP architecture, we focus on training smaller variants of the image encoder, which suffices for efficient zero-shot classification.","The use of synthetic data has shown promise in distilling representations from larger teachers, resulting in strong few-shot and linear probe performance.","However, we find that this approach surprisingly fails in true zero-shot settings when using contrastive losses.","We identify the exploitation of spurious features as being responsible for poor generalization between synthetic and real data.","However, by using the image feature-based L2 distillation loss, we mitigate these problems and train students that achieve zero-shot performance which on four domain-specific datasets is on-par with a ViT-B/32 teacher model trained on DataCompXL, while featuring up to 92% fewer parameters."],"url":"http://arxiv.org/abs/2404.16637v1","category":"cs.CV"}
{"created":"2024-04-25 14:23:24","title":"TinyChart: Efficient Chart Understanding with Visual Token Merging and Program-of-Thoughts Learning","abstract":"Charts are important for presenting and explaining complex data relationships. Recently, multimodal large language models (MLLMs) have shown remarkable capabilities in various chart understanding tasks. However, the sheer size of these models in terms of parameters and computational requirements limits their use in resource-constrained environments. In this paper, we present TinyChart, an efficient MLLM for chart understanding with only 3B parameters. TinyChart overcomes two key challenges in efficient chart understanding: (1) reduce the burden of learning numerical computations through a Program-of-Thoughts (PoT) learning strategy, which trains the model to generate Python programs for numerical calculations, and (2) reduce lengthy vision feature sequences produced by the vision transformer for high-resolution images through a Vision Token Merging module, which gradually merges most similar vision tokens. Extensive experiments demonstrate that our 3B TinyChart achieves SOTA performance on a variety of chart understanding benchmarks including ChartQA, Chart-to-Text, Chart-to-Table, OpenCQA, and ChartX. It outperforms several chart understanding MLLM with up to 13B parameters such as ChartLlama and ChartAst, and close-sourced general-purpose MLLM GPT-4V on ChartQA. It also demonstrates its superior efficiency with higher throughput during inference due to a smaller model scale and more efficient vision encoding. Our code and model are available at https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/TinyChart.","sentences":["Charts are important for presenting and explaining complex data relationships.","Recently, multimodal large language models (MLLMs) have shown remarkable capabilities in various chart understanding tasks.","However, the sheer size of these models in terms of parameters and computational requirements limits their use in resource-constrained environments.","In this paper, we present TinyChart, an efficient MLLM for chart understanding with only 3B parameters.","TinyChart overcomes two key challenges in efficient chart understanding: (1) reduce the burden of learning numerical computations through a Program-of-Thoughts (PoT) learning strategy, which trains the model to generate Python programs for numerical calculations, and (2) reduce lengthy vision feature sequences produced by the vision transformer for high-resolution images through a Vision Token Merging module, which gradually merges most similar vision tokens.","Extensive experiments demonstrate that our 3B TinyChart achieves SOTA performance on a variety of chart understanding benchmarks including ChartQA, Chart-to-Text, Chart-to-Table, OpenCQA, and ChartX. It outperforms several chart understanding MLLM with up to 13B parameters such as ChartLlama and ChartAst, and close-sourced general-purpose MLLM GPT-4V on ChartQA.","It also demonstrates its superior efficiency with higher throughput during inference due to a smaller model scale and more efficient vision encoding.","Our code and model are available at https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/TinyChart."],"url":"http://arxiv.org/abs/2404.16635v1","category":"cs.CV"}
{"created":"2024-04-25 14:22:44","title":"Self-Balanced R-CNN for Instance Segmentation","abstract":"Current state-of-the-art two-stage models on instance segmentation task suffer from several types of imbalances. In this paper, we address the Intersection over the Union (IoU) distribution imbalance of positive input Regions of Interest (RoIs) during the training of the second stage. Our Self-Balanced R-CNN (SBR-CNN), an evolved version of the Hybrid Task Cascade (HTC) model, brings brand new loop mechanisms of bounding box and mask refinements. With an improved Generic RoI Extraction (GRoIE), we also address the feature-level imbalance at the Feature Pyramid Network (FPN) level, originated by a non-uniform integration between low- and high-level features from the backbone layers. In addition, the redesign of the architecture heads toward a fully convolutional approach with FCC further reduces the number of parameters and obtains more clues to the connection between the task to solve and the layers used. Moreover, our SBR-CNN model shows the same or even better improvements if adopted in conjunction with other state-of-the-art models. In fact, with a lightweight ResNet-50 as backbone, evaluated on COCO minival 2017 dataset, our model reaches 45.3% and 41.5% AP for object detection and instance segmentation, with 12 epochs and without extra tricks. The code is available at https://github.com/IMPLabUniPr/mmdetection/tree/sbr_cnn","sentences":["Current state-of-the-art two-stage models on instance segmentation task suffer from several types of imbalances.","In this paper, we address the Intersection over the Union (IoU) distribution imbalance of positive input Regions of Interest (RoIs) during the training of the second stage.","Our Self-Balanced R-CNN (SBR-CNN), an evolved version of the Hybrid Task Cascade (HTC) model, brings brand new loop mechanisms of bounding box and mask refinements.","With an improved Generic RoI Extraction (GRoIE), we also address the feature-level imbalance at the Feature Pyramid Network (FPN) level, originated by a non-uniform integration between low- and high-level features from the backbone layers.","In addition, the redesign of the architecture heads toward a fully convolutional approach with FCC further reduces the number of parameters and obtains more clues to the connection between the task to solve and the layers used.","Moreover, our SBR-CNN model shows the same or even better improvements if adopted in conjunction with other state-of-the-art models.","In fact, with a lightweight ResNet-50 as backbone, evaluated on COCO minival 2017 dataset, our model reaches 45.3% and 41.5% AP for object detection and instance segmentation, with 12 epochs and without extra tricks.","The code is available at https://github.com/IMPLabUniPr/mmdetection/tree/sbr_cnn"],"url":"http://arxiv.org/abs/2404.16633v1","category":"cs.CV"}
{"created":"2024-04-25 14:17:34","title":"Legal Aspects for Software Developers Interested in Generative AI Applications","abstract":"Recent successes in Generative Artificial Intelligence (GenAI) have led to new technologies capable of generating high-quality code, natural language, and images. The next step is to integrate GenAI technology into products, a task typically conducted by software developers. Such product development always comes with a certain risk of liability. Within this article, we want to shed light on the current state of two such risks: data protection and copyright. Both aspects are crucial for GenAI. This technology deals with data for both model training and generated output. We summarize key aspects regarding our current knowledge that every software developer involved in product development using GenAI should be aware of to avoid critical mistakes that may expose them to liability claims.","sentences":["Recent successes in Generative Artificial Intelligence (GenAI) have led to new technologies capable of generating high-quality code, natural language, and images.","The next step is to integrate GenAI technology into products, a task typically conducted by software developers.","Such product development always comes with a certain risk of liability.","Within this article, we want to shed light on the current state of two such risks: data protection and copyright.","Both aspects are crucial for GenAI.","This technology deals with data for both model training and generated output.","We summarize key aspects regarding our current knowledge that every software developer involved in product development using GenAI should be aware of to avoid critical mistakes that may expose them to liability claims."],"url":"http://arxiv.org/abs/2404.16630v1","category":"cs.SE"}
{"created":"2024-04-25 14:11:45","title":"The quasi-isometry invariance of the Coset Intersection Complex","abstract":"For a pair $(G,\\mathcal{P})$ consisting of a finitely generated group and finite collection of subgroups, we introduce a simplicial $G$-complex $\\mathcal{K}(G,\\mathcal{P})$ called the coset intersection complex. We prove that the quasi-isometry type and the homotopy type of $\\mathcal{K}(G,\\mathcal{P})$ are quasi-isometric invariants of the group pair $(G,\\mathcal{P})$. Classical properties of $\\mathcal{P}$ in $G$ correspond to topological or geometric properties of $\\mathcal{K}(G,\\mathcal{P})$, such as having finite height, having finite width, being almost malnormal, admiting a malnormal core, or having thickness of order one. As applications, we obtain that a number of algebraic properties of $\\mathcal{P}$ in $G$ are quasi-isometry invariants of the pair $(G,\\mathcal{P})$. For a certain class of right-angled Artin groups and their maximal parabolic subgroups, we show that the complex $\\mathcal{K}(G,\\mathcal{P})$ is quasi-isometric to the Deligne complex; in particular, it is hyperbolic.","sentences":["For a pair $(G,\\mathcal{P})$ consisting of a finitely generated group and finite collection of subgroups, we introduce a simplicial $G$-complex $\\mathcal{K}(G,\\mathcal{P})$ called the coset intersection complex.","We prove that the quasi-isometry type and the homotopy type of $\\mathcal{K}(G,\\mathcal{P})$ are quasi-isometric invariants of the group pair $(G,\\mathcal{P})$. Classical properties of $\\mathcal{P}$ in $G$ correspond to topological or geometric properties of $\\mathcal{K}(G,\\mathcal{P})$, such as having finite height, having finite width, being almost malnormal, admiting a malnormal core, or having thickness of order one.","As applications, we obtain that a number of algebraic properties of $\\mathcal{P}$ in $G$ are quasi-isometry invariants of the pair $(G,\\mathcal{P})$. For a certain class of right-angled Artin groups and their maximal parabolic subgroups, we show that the complex $\\mathcal{K}(G,\\mathcal{P})$ is quasi-isometric to the Deligne complex; in particular, it is hyperbolic."],"url":"http://arxiv.org/abs/2404.16628v1","category":"math.GR"}
{"created":"2024-04-25 14:10:00","title":"Understanding small neutrino mass and its implication","abstract":"We have derived previously the relations between the neutrino masses and mixing angles in a dispersive analysis on the mixing of neutral leptonic states. The only involved assumption is that the electroweak symmetry of the Standard Model (SM) is restored at a high energy scale in some new physics scenario, which diminishes the box diagrams responsible for the mixing. Here we include corrections to the analysis up to three loops, arising from exchanges of additional neutral and charged scalars in the electroweak symmetric phase. The solution to the dispersion relation for the $\\mu^-e^+$-$\\mu^+e^-$ mixing generates a typical neutrino mass $m_\\nu\\sim O(1)$ eV in the SM unambiguously. The solution also favors the normal ordering of the neutrino masses over the inverted one, and links the large electroweak symmetry restoration, i.e., new physics scale to the small neutrino mass.","sentences":["We have derived previously the relations between the neutrino masses and mixing angles in a dispersive analysis on the mixing of neutral leptonic states.","The only involved assumption is that the electroweak symmetry of the Standard Model (SM) is restored at a high energy scale in some new physics scenario, which diminishes the box diagrams responsible for the mixing.","Here we include corrections to the analysis up to three loops, arising from exchanges of additional neutral and charged scalars in the electroweak symmetric phase.","The solution to the dispersion relation for the $\\mu^-e^+$-$\\mu^+e^-$ mixing generates a typical neutrino mass $m_\\nu\\sim O(1)$ eV in the SM unambiguously.","The solution also favors the normal ordering of the neutrino masses over the inverted one, and links the large electroweak symmetry restoration, i.e., new physics scale to the small neutrino mass."],"url":"http://arxiv.org/abs/2404.16626v1","category":"hep-ph"}
{"created":"2024-04-25 14:07:52","title":"DAVE -- A Detect-and-Verify Paradigm for Low-Shot Counting","abstract":"Low-shot counters estimate the number of objects corresponding to a selected category, based on only few or no exemplars annotated in the image. The current state-of-the-art estimates the total counts as the sum over the object location density map, but does not provide individual object locations and sizes, which are crucial for many applications. This is addressed by detection-based counters, which, however fall behind in the total count accuracy. Furthermore, both approaches tend to overestimate the counts in the presence of other object classes due to many false positives. We propose DAVE, a low-shot counter based on a detect-and-verify paradigm, that avoids the aforementioned issues by first generating a high-recall detection set and then verifying the detections to identify and remove the outliers. This jointly increases the recall and precision, leading to accurate counts. DAVE outperforms the top density-based counters by ~20% in the total count MAE, it outperforms the most recent detection-based counter by ~20% in detection quality and sets a new state-of-the-art in zero-shot as well as text-prompt-based counting.","sentences":["Low-shot counters estimate the number of objects corresponding to a selected category, based on only few or no exemplars annotated in the image.","The current state-of-the-art estimates the total counts as the sum over the object location density map, but does not provide individual object locations and sizes, which are crucial for many applications.","This is addressed by detection-based counters, which, however fall behind in the total count accuracy.","Furthermore, both approaches tend to overestimate the counts in the presence of other object classes due to many false positives.","We propose DAVE, a low-shot counter based on a detect-and-verify paradigm, that avoids the aforementioned issues by first generating a high-recall detection set and then verifying the detections to identify and remove the outliers.","This jointly increases the recall and precision, leading to accurate counts.","DAVE outperforms the top density-based counters by ~20% in the total count MAE, it outperforms the most recent detection-based counter by ~20% in detection quality and sets a new state-of-the-art in zero-shot as well as text-prompt-based counting."],"url":"http://arxiv.org/abs/2404.16622v1","category":"cs.CV"}
{"created":"2024-04-25 14:06:37","title":"Hippocrates: An Open-Source Framework for Advancing Large Language Models in Healthcare","abstract":"The integration of Large Language Models (LLMs) into healthcare promises to transform medical diagnostics, research, and patient care. Yet, the progression of medical LLMs faces obstacles such as complex training requirements, rigorous evaluation demands, and the dominance of proprietary models that restrict academic exploration. Transparent, comprehensive access to LLM resources is essential for advancing the field, fostering reproducibility, and encouraging innovation in healthcare AI. We present Hippocrates, an open-source LLM framework specifically developed for the medical domain. In stark contrast to previous efforts, it offers unrestricted access to its training datasets, codebase, checkpoints, and evaluation protocols. This open approach is designed to stimulate collaborative research, allowing the community to build upon, refine, and rigorously evaluate medical LLMs within a transparent ecosystem. Also, we introduce Hippo, a family of 7B models tailored for the medical domain, fine-tuned from Mistral and LLaMA2 through continual pre-training, instruction tuning, and reinforcement learning from human and AI feedback. Our models outperform existing open medical LLMs models by a large-margin, even surpassing models with 70B parameters. Through Hippocrates, we aspire to unlock the full potential of LLMs not just to advance medical knowledge and patient care but also to democratize the benefits of AI research in healthcare, making them available across the globe.","sentences":["The integration of Large Language Models (LLMs) into healthcare promises to transform medical diagnostics, research, and patient care.","Yet, the progression of medical LLMs faces obstacles such as complex training requirements, rigorous evaluation demands, and the dominance of proprietary models that restrict academic exploration.","Transparent, comprehensive access to LLM resources is essential for advancing the field, fostering reproducibility, and encouraging innovation in healthcare AI.","We present Hippocrates, an open-source LLM framework specifically developed for the medical domain.","In stark contrast to previous efforts, it offers unrestricted access to its training datasets, codebase, checkpoints, and evaluation protocols.","This open approach is designed to stimulate collaborative research, allowing the community to build upon, refine, and rigorously evaluate medical LLMs within a transparent ecosystem.","Also, we introduce Hippo, a family of 7B models tailored for the medical domain, fine-tuned from Mistral and LLaMA2 through continual pre-training, instruction tuning, and reinforcement learning from human and AI feedback.","Our models outperform existing open medical LLMs models by a large-margin, even surpassing models with 70B parameters.","Through Hippocrates, we aspire to unlock the full potential of LLMs not just to advance medical knowledge and patient care but also to democratize the benefits of AI research in healthcare, making them available across the globe."],"url":"http://arxiv.org/abs/2404.16621v1","category":"cs.LG"}
{"created":"2024-04-25 13:56:54","title":"Denoising: from classical methods to deep CNNs","abstract":"This paper aims to explore the evolution of image denoising in a pedagological way. We briefly review classical methods such as Fourier analysis and wavelet bases, highlighting the challenges they faced until the emergence of neural networks, notably the U-Net, in the 2010s. The remarkable performance of these networks has been demonstrated in studies such as Kadkhodaie et al. (2024). They exhibit adaptability to various image types, including those with fixed regularity, facial images, and bedroom scenes, achieving optimal results and biased towards geometry-adaptive harmonic basis. The introduction of score diffusion has played a crucial role in image generation. In this context, denoising becomes essential as it facilitates the estimation of probability density scores. We discuss the prerequisites for genuine learning of probability densities, offering insights that extend from mathematical research to the implications of universal structures.","sentences":["This paper aims to explore the evolution of image denoising in a pedagological way.","We briefly review classical methods such as Fourier analysis and wavelet bases, highlighting the challenges they faced until the emergence of neural networks, notably the U-Net, in the 2010s.","The remarkable performance of these networks has been demonstrated in studies such as Kadkhodaie et al. (2024).","They exhibit adaptability to various image types, including those with fixed regularity, facial images, and bedroom scenes, achieving optimal results and biased towards geometry-adaptive harmonic basis.","The introduction of score diffusion has played a crucial role in image generation.","In this context, denoising becomes essential as it facilitates the estimation of probability density scores.","We discuss the prerequisites for genuine learning of probability densities, offering insights that extend from mathematical research to the implications of universal structures."],"url":"http://arxiv.org/abs/2404.16617v1","category":"cs.CV"}
{"created":"2024-04-25 13:51:38","title":"MuseumMaker: Continual Style Customization without Catastrophic Forgetting","abstract":"Pre-trained large text-to-image (T2I) models with an appropriate text prompt has attracted growing interests in customized images generation field. However, catastrophic forgetting issue make it hard to continually synthesize new user-provided styles while retaining the satisfying results amongst learned styles. In this paper, we propose MuseumMaker, a method that enables the synthesis of images by following a set of customized styles in a never-end manner, and gradually accumulate these creative artistic works as a Museum. When facing with a new customization style, we develop a style distillation loss module to transfer the style of the whole dataset into generation of images. It can minimize the learning biases caused by content of images, and address the catastrophic overfitting issue induced by few-shot images. To deal with catastrophic forgetting amongst past learned styles, we devise a dual regularization for shared-LoRA module to optimize the direction of model update, which could regularize the diffusion model from both weight and feature aspects, respectively. Meanwhile, a unique token embedding corresponding to this new style is learned by a task-wise token learning module, which could preserve historical knowledge from past styles with the limitation of LoRA parameter quantity. As any new user-provided style come, our MuseumMaker can capture the nuances of the new styles while maintaining the details of learned styles. Experimental results on diverse style datasets validate the effectiveness of our proposed MuseumMaker method, showcasing its robustness and versatility across various scenarios.","sentences":["Pre-trained large text-to-image (T2I) models with an appropriate text prompt has attracted growing interests in customized images generation field.","However, catastrophic forgetting issue make it hard to continually synthesize new user-provided styles while retaining the satisfying results amongst learned styles.","In this paper, we propose MuseumMaker, a method that enables the synthesis of images by following a set of customized styles in a never-end manner, and gradually accumulate these creative artistic works as a Museum.","When facing with a new customization style, we develop a style distillation loss module to transfer the style of the whole dataset into generation of images.","It can minimize the learning biases caused by content of images, and address the catastrophic overfitting issue induced by few-shot images.","To deal with catastrophic forgetting amongst past learned styles, we devise a dual regularization for shared-LoRA module to optimize the direction of model update, which could regularize the diffusion model from both weight and feature aspects, respectively.","Meanwhile, a unique token embedding corresponding to this new style is learned by a task-wise token learning module, which could preserve historical knowledge from past styles with the limitation of LoRA parameter quantity.","As any new user-provided style come, our MuseumMaker can capture the nuances of the new styles while maintaining the details of learned styles.","Experimental results on diverse style datasets validate the effectiveness of our proposed MuseumMaker method, showcasing its robustness and versatility across various scenarios."],"url":"http://arxiv.org/abs/2404.16612v1","category":"cs.CV"}
{"created":"2024-04-25 13:49:59","title":"Conformalized Ordinal Classification with Marginal and Conditional Coverage","abstract":"Conformal prediction is a general distribution-free approach for constructing prediction sets combined with any machine learning algorithm that achieve valid marginal or conditional coverage in finite samples. Ordinal classification is common in real applications where the target variable has natural ordering among the class labels. In this paper, we discuss constructing distribution-free prediction sets for such ordinal classification problems by leveraging the ideas of conformal prediction and multiple testing with FWER control. Newer conformal prediction methods are developed for constructing contiguous and non-contiguous prediction sets based on marginal and conditional (class-specific) conformal $p$-values, respectively. Theoretically, we prove that the proposed methods respectively achieve satisfactory levels of marginal and class-specific conditional coverages. Through simulation study and real data analysis, these proposed methods show promising performance compared to the existing conformal method.","sentences":["Conformal prediction is a general distribution-free approach for constructing prediction sets combined with any machine learning algorithm that achieve valid marginal or conditional coverage in finite samples.","Ordinal classification is common in real applications where the target variable has natural ordering among the class labels.","In this paper, we discuss constructing distribution-free prediction sets for such ordinal classification problems by leveraging the ideas of conformal prediction and multiple testing with FWER control.","Newer conformal prediction methods are developed for constructing contiguous and non-contiguous prediction sets based on marginal and conditional (class-specific) conformal $p$-values, respectively.","Theoretically, we prove that the proposed methods respectively achieve satisfactory levels of marginal and class-specific conditional coverages.","Through simulation study and real data analysis, these proposed methods show promising performance compared to the existing conformal method."],"url":"http://arxiv.org/abs/2404.16610v1","category":"stat.ME"}
{"created":"2024-04-25 13:49:42","title":"SFMViT: SlowFast Meet ViT in Chaotic World","abstract":"The task of spatiotemporal action localization in chaotic scenes is a challenging task toward advanced video understanding. Paving the way with high-quality video feature extraction and enhancing the precision of detector-predicted anchors can effectively improve model performance. To this end, we propose a high-performance dual-stream spatiotemporal feature extraction network SFMViT with an anchor pruning strategy. The backbone of our SFMViT is composed of ViT and SlowFast with prior knowledge of spatiotemporal action localization, which fully utilizes ViT's excellent global feature extraction capabilities and SlowFast's spatiotemporal sequence modeling capabilities. Secondly, we introduce the confidence maximum heap to prune the anchors detected in each frame of the picture to filter out the effective anchors. These designs enable our SFMViT to achieve a mAP of 26.62% in the Chaotic World dataset, far exceeding existing models. Code is available at https://github.com/jfightyr/SlowFast-Meet-ViT.","sentences":["The task of spatiotemporal action localization in chaotic scenes is a challenging task toward advanced video understanding.","Paving the way with high-quality video feature extraction and enhancing the precision of detector-predicted anchors can effectively improve model performance.","To this end, we propose a high-performance dual-stream spatiotemporal feature extraction network SFMViT with an anchor pruning strategy.","The backbone of our SFMViT is composed of ViT and SlowFast with prior knowledge of spatiotemporal action localization, which fully utilizes ViT's excellent global feature extraction capabilities and SlowFast's spatiotemporal sequence modeling capabilities.","Secondly, we introduce the confidence maximum heap to prune the anchors detected in each frame of the picture to filter out the effective anchors.","These designs enable our SFMViT to achieve a mAP of 26.62% in the Chaotic World dataset, far exceeding existing models.","Code is available at https://github.com/jfightyr/SlowFast-Meet-ViT."],"url":"http://arxiv.org/abs/2404.16609v1","category":"cs.CV"}
{"created":"2024-04-25 13:47:37","title":"A Comprehensive Design Framework for UE-side and BS-Side RIS Deployments","abstract":"Integrating reconfigurable intelligent surfaces (RISs) in emerging communication systems is a fast-growing research field that has recently earned much attention. While implementing RISs near the base station (BS), i.e., BS-side RIS, or user equipment (UE), i.e., UE-side RIS, exhibits optimum performance, understanding the differences between these two deployments in terms of the system design perspective needs to be clarified. Critical design parameters, such as RIS size, phase shift adjustment, control link, and element type (passive/active), require greater clarity across these scenarios. Overlooking the intricacies of such critical design parameters in light of 6G demands endangers practical implementation, widening the gap between theoretical insights and practical applications. In this regard, our study investigates the impact of each RIS deployment strategy on the anticipated 6G requirements and offers tailored RIS design recommendations to fulfill these forward-looking requirements. Through this, we clarify the practical distinctions and propose a comprehensive framework for differentiating between BS-side and UE-side RIS scenarios in terms of their design parameters. Highlighting the unique needs of each and the potential challenges ahead, we aim to fuse the theoretical underpinnings of RIS with tangible implementation considerations, propelling progress in both the academic sphere and the industry.","sentences":["Integrating reconfigurable intelligent surfaces (RISs) in emerging communication systems is a fast-growing research field that has recently earned much attention.","While implementing RISs near the base station (BS), i.e., BS-side RIS, or user equipment (UE), i.e., UE-side RIS, exhibits optimum performance, understanding the differences between these two deployments in terms of the system design perspective needs to be clarified.","Critical design parameters, such as RIS size, phase shift adjustment, control link, and element type (passive/active), require greater clarity across these scenarios.","Overlooking the intricacies of such critical design parameters in light of 6G demands endangers practical implementation, widening the gap between theoretical insights and practical applications.","In this regard, our study investigates the impact of each RIS deployment strategy on the anticipated 6G requirements and offers tailored RIS design recommendations to fulfill these forward-looking requirements.","Through this, we clarify the practical distinctions and propose a comprehensive framework for differentiating between BS-side and UE-side RIS scenarios in terms of their design parameters.","Highlighting the unique needs of each and the potential challenges ahead, we aim to fuse the theoretical underpinnings of RIS with tangible implementation considerations, propelling progress in both the academic sphere and the industry."],"url":"http://arxiv.org/abs/2404.16607v1","category":"eess.SP"}
{"created":"2024-04-25 13:45:11","title":"A general polarimetric model for transiting and non-transiting ringed exoplanets","abstract":"We explore the potential of polarimetry as a tool for detecting and characterizing exorings. For that purpose, we have improved the publicly available photometric code Pryngles by adding the results of radiative transfer calculations that fully include polarization and scattering by irregularly shaped particles. With this improved code, we compute the total and polarized fluxes and the degree of polarization of a ringed gas giant along its orbit. We vary key model parameters such as the orbit inclination, ring size and orientation, particle albedo and optical thickness, and demonstrate the versatility of our code by predicting the total and polarized fluxes of the \"puffed-up\" planet HIP41378f assuming this planet has an opaque dusty ring. We find that spatially unresolved dusty rings can significantly modify the flux and polarization signals of the light that is reflected. Rings are expected to have a low polarization signal and will generally decrease the degree of polarization as the ring casts a shadow on the planet and/or blocks part of the light the planet reflects. During ring-plane crossings, when the thin ring is illuminated edge-on, a ringed exoplanet's flux and degree of polarization are close to those of a ring-less planet and generally appear as sharp changes in the flux and polarization curves. Ringed planets in edge-on orbits tend to be difficult to distinguish from ring-less planets in reflected flux and degree of polarization. We show that if HIP41378f is surrounded by a ring, its reflected flux (compared to the star) will be of the order of $10^{-9}$, and the ring would decrease the degree of polarization in a detectable way. The improved version of the photometric code Pryngles that we present here shows that dusty rings may produce distinct polarimetric features in light curves across a wide range of orbital configurations, orientations and ring optical properties.","sentences":["We explore the potential of polarimetry as a tool for detecting and characterizing exorings.","For that purpose, we have improved the publicly available photometric code Pryngles by adding the results of radiative transfer calculations that fully include polarization and scattering by irregularly shaped particles.","With this improved code, we compute the total and polarized fluxes and the degree of polarization of a ringed gas giant along its orbit.","We vary key model parameters such as the orbit inclination, ring size and orientation, particle albedo and optical thickness, and demonstrate the versatility of our code by predicting the total and polarized fluxes of the \"puffed-up\" planet HIP41378f assuming this planet has an opaque dusty ring.","We find that spatially unresolved dusty rings can significantly modify the flux and polarization signals of the light that is reflected.","Rings are expected to have a low polarization signal and will generally decrease the degree of polarization as the ring casts a shadow on the planet and/or blocks part of the light the planet reflects.","During ring-plane crossings, when the thin ring is illuminated edge-on, a ringed exoplanet's flux and degree of polarization are close to those of a ring-less planet and generally appear as sharp changes in the flux and polarization curves.","Ringed planets in edge-on orbits tend to be difficult to distinguish from ring-less planets in reflected flux and degree of polarization.","We show that if HIP41378f is surrounded by a ring, its reflected flux (compared to the star) will be of the order of $10^{-9}$, and the ring would decrease the degree of polarization in a detectable way.","The improved version of the photometric code Pryngles that we present here shows that dusty rings may produce distinct polarimetric features in light curves across a wide range of orbital configurations, orientations and ring optical properties."],"url":"http://arxiv.org/abs/2404.16606v1","category":"astro-ph.EP"}
{"created":"2024-04-25 13:41:43","title":"Markov generators as non-hermitian supersymmetric quantum Hamiltonians: spectral properties via bi-orthogonal basis and Singular Value Decompositions","abstract":"Continuity equations associated to continuous-time Markov processes can be considered as Euclidean Schr\\\"odinger equations, where the non-hermitian quantum Hamiltonian $\\bold{H}={\\bold{div}}{\\bold J}$ is naturally factorized into the product of the divergence operator ${\\bold {div}}$ and the current operator ${\\bold J}$. For non-equilibrium Markov jump processes in a space of $N$ configurations with $M$ links and $C=M-(N-1)\\geq 1$ independent cycles, this factorization of the $N \\times N$ Hamiltonian ${\\bold H}={\\bold I}^{\\dagger}{\\bold J}$ involves the incidence matrix ${\\bold I}$ and the current matrix ${\\bold J}$ of size $M \\times N$, so that the supersymmetric partner ${\\hat{\\bold H}}= {\\bold J}{\\bold I}^{\\dagger}$ governing the dynamics of the currents living on the $M$ links is of size $M \\times M$. To better understand the relations between the spectral decompositions of these two Hamiltonians $\\bold{H}={\\bold I}^{\\dagger}{\\bold J}$ and ${\\hat {\\bold H}} ={\\bold J}{\\bold I}^{\\dagger}$ with respect to their bi-orthogonal basis of right and left eigenvectors that characterize the relaxation dynamics towards the steady state and the steady currents, it is useful to analyze the properties of the Singular Value Decompositions of the two rectangular matrices ${\\bold I}$ and ${\\bold J} $ of size $M \\times N$ and the interpretations in terms of discrete Helmholtz decompositions. This general framework concerning Markov jump processes can be adapted to non-equilibrium diffusion processes governed by Fokker-Planck equations in dimension $d$, where the number $N$ of configurations, the number $M$ of links and the number $C=M-(N-1)$ of independent cycles become infinite, while the two matrices ${\\bold I}$ and ${\\bold J}$ become first-order differential operators acting on scalar functions to produce vector fields.","sentences":["Continuity equations associated to continuous-time Markov processes can be considered as Euclidean Schr\\\"odinger equations, where the non-hermitian quantum Hamiltonian $\\bold{H}={\\bold{div}}{\\bold J}$ is naturally factorized into the product of the divergence operator ${\\bold {div}}$ and the current operator ${\\bold J}$. For non-equilibrium Markov jump processes in a space of $N$ configurations with $M$ links and $C=M-(N-1)\\geq 1$ independent cycles, this factorization of the $N \\times N$ Hamiltonian ${\\bold H}={\\bold","I}^{\\dagger}{\\bold J}$ involves the incidence matrix ${\\bold I}$ and the current matrix ${\\bold J}$ of size $M \\times N$, so that the supersymmetric partner ${\\hat{\\bold H}}= {\\bold J}{\\bold I}^{\\dagger}$ governing the dynamics of the currents living on the $M$ links is of size $M \\times M$. To better understand the relations between the spectral decompositions of these two Hamiltonians $\\bold{H}={\\bold I}^{\\dagger}{\\bold J}$ and ${\\hat {\\bold H}} ={\\bold J}{\\bold I}^{\\dagger}$ with respect to their bi-orthogonal basis of right and left eigenvectors that characterize the relaxation dynamics towards the steady state and the steady currents, it is useful to analyze the properties of the Singular Value Decompositions of the two rectangular matrices ${\\bold I}$ and ${\\bold J} $ of size $M \\times N$ and the interpretations in terms of discrete Helmholtz decompositions.","This general framework concerning Markov jump processes can be adapted to non-equilibrium diffusion processes governed by Fokker-Planck equations in dimension $d$, where the number $N$ of configurations, the number $M$ of links and the number $C=M-(N-1)$ of independent cycles become infinite, while the two matrices ${\\bold I}$ and ${\\bold J}$ become first-order differential operators acting on scalar functions to produce vector fields."],"url":"http://arxiv.org/abs/2404.16605v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-25 13:39:15","title":"Rotating strings and anomalous dimensions in Non-AdS holography","abstract":"In this paper we consider certain rigidly rotating closed string configurations in an asymptotically non-AdS string background. The string background is a deformation of $AdS_3 \\times {\\cal M}_7$. It interpolates between $AdS_3 $ and asymptotically linear dilaton ${\\rm I\\!R} \\times S^1 \\times {\\rm I\\!R}$ spacetime (times the internal compact manifold ${\\cal M}_7$). We compute the quantity $E - J$ (in the large $J$ limit) where $E$ is the energy and $J$ is the angular momentum of the spinning strings. In the two dimensional CFT dual to string theory on $AdS_3$ (times ${\\cal M}_7$) it gives the anomalous dimensions of certain twist two and higher operators. We show in the deformed background that $E - J$ is bounded. At a special value of the deformation coupling we also show that for spinning closed strings containing $n > 2$ cusps or spikes both $E$ and $J$ are bounded. In the CFT dual to string theory on $AdS_3$ (times ${\\cal M}_7$) the spinning cusped strings describe operators with twist $n$ larger than two. In general, at other values of the deformation coupling, we demonstrate that this feature is exhibited only by those cusped strings with $n > n_0$ where $n_0$ is determined only by the deformation coupling. We also give simple exact Regge relations between $E$ and $J$. We also study the closely related cusp anomalous dimension of a light-like Wilson loop. We comment on what $E - J$ measures away from the CFT along the deformation in the coupling space. In the long string sector the deformation is dual to a single trace $T{\\bar T}$ deformed orbifold theory. We determine the associated deformed sinh-Gordon model that classically describes the (long) strings near the boundary. This provides an example of single trace $T{\\bar T}$ deformation in non-orbifold theories.","sentences":["In this paper we consider certain rigidly rotating closed string configurations in an asymptotically non-AdS string background.","The string background is a deformation of $AdS_3 \\times {\\cal M}_7$.","It interpolates between $AdS_3 $ and asymptotically linear dilaton ${\\rm I\\!R} \\times S^1 \\times {\\rm I\\!R}$ spacetime (times the internal compact manifold ${\\cal M}_7$).","We compute the quantity $E - J$ (in the large $J$ limit) where $E$ is the energy and $J$ is the angular momentum of the spinning strings.","In the two dimensional CFT dual to string theory on $AdS_3$ (times ${\\cal M}_7$) it gives the anomalous dimensions of certain twist two and higher operators.","We show in the deformed background that $E - J$ is bounded.","At a special value of the deformation coupling we also show that for spinning closed strings containing $n > 2$ cusps or spikes both $E$ and $J$ are bounded.","In the CFT dual to string theory on $AdS_3$ (times ${\\cal M}_7$) the spinning cusped strings describe operators with twist $n$ larger than two.","In general, at other values of the deformation coupling, we demonstrate that this feature is exhibited only by those cusped strings with $n > n_0$ where $n_0$ is determined only by the deformation coupling.","We also give simple exact Regge relations between $E$ and $J$. We also study the closely related cusp anomalous dimension of a light-like Wilson loop.","We comment on what $E - J$ measures away from the CFT along the deformation in the coupling space.","In the long string sector the deformation is dual to a single trace $T{\\bar T}$ deformed orbifold theory.","We determine the associated deformed sinh-Gordon model that classically describes the (long) strings near the boundary.","This provides an example of single trace $T{\\bar T}$ deformation in non-orbifold theories."],"url":"http://arxiv.org/abs/2404.16601v1","category":"hep-th"}
{"created":"2024-04-25 13:26:17","title":"Waveform systematics in gravitational-wave inference of signals from binary neutron star merger models incorporating higher order modes information","abstract":"Accurate information from gravitational wave signals from coalescing binary neutron stars provides essential input to downstream interpretations, including inference of the neutron star population and equation of state. However, even adopting the currently most accurate and physically motivated models available for parameter estimation (PE) of BNSs, these models remain subject to waveform modeling uncertainty: differences between these models may introduce biases in recovered source properties. In this work, we describe injection studies investigating these systematic differences between the two best waveform models available for BNS currently, NRHybSur3dq8Tidal and TEOBResumS. We demonstrate that for BNS sources observable by current second-generation detectors, differences for low-amplitude signals are significant for certain sources.","sentences":["Accurate information from gravitational wave signals from coalescing binary neutron stars provides essential input to downstream interpretations, including inference of the neutron star population and equation of state.","However, even adopting the currently most accurate and physically motivated models available for parameter estimation (PE) of BNSs, these models remain subject to waveform modeling uncertainty: differences between these models may introduce biases in recovered source properties.","In this work, we describe injection studies investigating these systematic differences between the two best waveform models available for BNS currently, NRHybSur3dq8Tidal and TEOBResumS.","We demonstrate that for BNS sources observable by current second-generation detectors, differences for low-amplitude signals are significant for certain sources."],"url":"http://arxiv.org/abs/2404.16599v1","category":"gr-qc"}
{"created":"2024-04-25 13:25:02","title":"Terahertz Voltage-controlled Oscillator from a Kerr-Induced Synchronized Soliton Microcomb","abstract":"The generation of controlled and arbitrarily tunable terahertz radiation, essential for many applications, has proven challenging due to the complexity of experimental setups and fabrication techniques. We introduce a new strategy involving control over a terahertz repetition rate integrated frequency comb, using Kerr-induced synchronization, that results in a terahertz-voltage-controlled oscillator. By modulating the reference laser, we can transfer any microwave waveform onto the microcomb repetition rate via a linear transfer function based on optical frequency division. The resulting frequency comb with a terahertz carrier can be created using integrated components, with a bandwidth constrained only by the synchronization bandwidth and high coherence resulting from the low-noise soliton microcomb in the Kerr-induced synchronized state.","sentences":["The generation of controlled and arbitrarily tunable terahertz radiation, essential for many applications, has proven challenging due to the complexity of experimental setups and fabrication techniques.","We introduce a new strategy involving control over a terahertz repetition rate integrated frequency comb, using Kerr-induced synchronization, that results in a terahertz-voltage-controlled oscillator.","By modulating the reference laser, we can transfer any microwave waveform onto the microcomb repetition rate via a linear transfer function based on optical frequency division.","The resulting frequency comb with a terahertz carrier can be created using integrated components, with a bandwidth constrained only by the synchronization bandwidth and high coherence resulting from the low-noise soliton microcomb in the Kerr-induced synchronized state."],"url":"http://arxiv.org/abs/2404.16597v1","category":"physics.optics"}
{"created":"2024-04-25 13:20:13","title":"Stimulated Emission Depletion (STED) Magnetic Particle Imaging","abstract":"Magnetic particle imaging (MPI) is an in-vivo imaging method to detect magnetic nanoparticles for blood vessel imaging and molecular target imaging. Compared with conventional molecular imaging devices (such as nuclear medicine imaging PET and SPECT), magnetic nanoparticles have longer storage periods than radionuclides without ionizing radiation. MPI has higher detection sensitivity compared with MRI. To accurately locate molecular probes in living organisms, high-resolution images are needed to meet the requirements of precision medicine. The spatial resolution of the latest domestic and international MPI equipment is 1-6 mm and has not yet met the requirements of medical imaging detection. We previously studied the spatial encoding technology based on pulsed square wave stimulation, which significantly improved the image resolution along the field free line (FFL) direction. This study proposes an innovative idea of high-resolution MPI based on stimulated emission depletion (STED) of magnetic nanoparticle signals. The stimulated emission was implemented by using cosine stimulation on FFL-based MPI scanner systems. The STED signal was generated by adding an offset magnetic field parallel to the FFL, which may form a donut-shaped focal spot or a regular Gaussian focal spot depending on the offset field strength. Focal spot modulation techniques and deconvolution algorithms were developed to improve image resolution.","sentences":["Magnetic particle imaging (MPI) is an in-vivo imaging method to detect magnetic nanoparticles for blood vessel imaging and molecular target imaging.","Compared with conventional molecular imaging devices (such as nuclear medicine imaging PET and SPECT), magnetic nanoparticles have longer storage periods than radionuclides without ionizing radiation.","MPI has higher detection sensitivity compared with MRI.","To accurately locate molecular probes in living organisms, high-resolution images are needed to meet the requirements of precision medicine.","The spatial resolution of the latest domestic and international MPI equipment is 1-6 mm and has not yet met the requirements of medical imaging detection.","We previously studied the spatial encoding technology based on pulsed square wave stimulation, which significantly improved the image resolution along the field free line (FFL) direction.","This study proposes an innovative idea of high-resolution MPI based on stimulated emission depletion (STED) of magnetic nanoparticle signals.","The stimulated emission was implemented by using cosine stimulation on FFL-based MPI scanner systems.","The STED signal was generated by adding an offset magnetic field parallel to the FFL, which may form a donut-shaped focal spot or a regular Gaussian focal spot depending on the offset field strength.","Focal spot modulation techniques and deconvolution algorithms were developed to improve image resolution."],"url":"http://arxiv.org/abs/2404.16596v1","category":"physics.med-ph"}
{"created":"2024-04-25 13:18:08","title":"Linear-optical approach to encoding qubits into harmonic-oscillator modes via quantum walks","abstract":"We propose a linear-optical scheme that allows encoding grid-state quantum bits (qubits) into a bosonic mode using cat state and post-selection as sources of non-Gaussianity in the encoding. As a linear-optical realization of the quantum-walk encoding scheme in [Lin {\\em et al.}, Quantum Info. Processing {\\bf 19}, 272 (2020)], we employ the cat state as a quantum coin that enables encoding approximate Gottesman-Kitaev-Preskill (GKP) qubits through quantum walk of a squeezed vacuum state in phase space. We show that the conditional phase-space displacement necessary for the encoding can be realized through a Mach-Zehnder interferometer (MZI) assisted with ancillary cat-state input under appropriate parameter regimes. By analyzing the fidelity of the MZI-based displacement operation, we identify the region of parameter space over which the proposed linear-optical scheme can generate grid-state qubits with high fidelity. With adequate parameter setting, our proposal should be accessible to current optical and superconducting-circuit platforms in preparing grid-state qubits for bosonic modes in the, respectively, optical and microwave domains.","sentences":["We propose a linear-optical scheme that allows encoding grid-state quantum bits (qubits) into a bosonic mode using cat state and post-selection as sources of non-Gaussianity in the encoding.","As a linear-optical realization of the quantum-walk encoding scheme in [Lin {\\em et al.}, Quantum Info.","Processing {\\bf 19}, 272 (2020)], we employ the cat state as a quantum coin that enables encoding approximate Gottesman-Kitaev-Preskill (GKP) qubits through quantum walk of a squeezed vacuum state in phase space.","We show that the conditional phase-space displacement necessary for the encoding can be realized through a Mach-Zehnder interferometer (MZI) assisted with ancillary cat-state input under appropriate parameter regimes.","By analyzing the fidelity of the MZI-based displacement operation, we identify the region of parameter space over which the proposed linear-optical scheme can generate grid-state qubits with high fidelity.","With adequate parameter setting, our proposal should be accessible to current optical and superconducting-circuit platforms in preparing grid-state qubits for bosonic modes in the, respectively, optical and microwave domains."],"url":"http://arxiv.org/abs/2404.16594v1","category":"quant-ph"}
{"created":"2024-04-25 13:13:29","title":"Preconditioned flow as a solution to the hierarchical growth problem in the generalized Lefschetz thimble method","abstract":"The generalized Lefschetz thimble method is a promising approach that attempts to solve the sign problem in Monte Carlo methods by deforming the integration contour using the flow equation. Here we point out a general problem that occurs due to the property of the flow equation, which extends a region on the original contour exponentially to a region on the deformed contour. Since the growth rate for each eigenmode is governed by the singular values of the Hessian of the action, a huge hierarchy in the singular value spectrum, which typically appears for large systems, leads to various technical problems in numerical simulations. We solve this hierarchical growth problem by preconditioning the flow so that the growth rate becomes identical for every eigenmode. As an example, we show that the preconditioned flow enables us to investigate the real-time quantum evolution of an anharmonic oscillator with the system size that can hardly be achieved by using the original flow.","sentences":["The generalized Lefschetz thimble method is a promising approach that attempts to solve the sign problem in Monte Carlo methods by deforming the integration contour using the flow equation.","Here we point out a general problem that occurs due to the property of the flow equation, which extends a region on the original contour exponentially to a region on the deformed contour.","Since the growth rate for each eigenmode is governed by the singular values of the Hessian of the action, a huge hierarchy in the singular value spectrum, which typically appears for large systems, leads to various technical problems in numerical simulations.","We solve this hierarchical growth problem by preconditioning the flow so that the growth rate becomes identical for every eigenmode.","As an example, we show that the preconditioned flow enables us to investigate the real-time quantum evolution of an anharmonic oscillator with the system size that can hardly be achieved by using the original flow."],"url":"http://arxiv.org/abs/2404.16589v1","category":"hep-lat"}
{"created":"2024-04-25 13:10:48","title":"Understanding Privacy Risks of Embeddings Induced by Large Language Models","abstract":"Large language models (LLMs) show early signs of artificial general intelligence but struggle with hallucinations. One promising solution to mitigate these hallucinations is to store external knowledge as embeddings, aiding LLMs in retrieval-augmented generation. However, such a solution risks compromising privacy, as recent studies experimentally showed that the original text can be partially reconstructed from text embeddings by pre-trained language models. The significant advantage of LLMs over traditional pre-trained models may exacerbate these concerns. To this end, we investigate the effectiveness of reconstructing original knowledge and predicting entity attributes from these embeddings when LLMs are employed. Empirical findings indicate that LLMs significantly improve the accuracy of two evaluated tasks over those from pre-trained models, regardless of whether the texts are in-distribution or out-of-distribution. This underscores a heightened potential for LLMs to jeopardize user privacy, highlighting the negative consequences of their widespread use. We further discuss preliminary strategies to mitigate this risk.","sentences":["Large language models (LLMs) show early signs of artificial general intelligence but struggle with hallucinations.","One promising solution to mitigate these hallucinations is to store external knowledge as embeddings, aiding LLMs in retrieval-augmented generation.","However, such a solution risks compromising privacy, as recent studies experimentally showed that the original text can be partially reconstructed from text embeddings by pre-trained language models.","The significant advantage of LLMs over traditional pre-trained models may exacerbate these concerns.","To this end, we investigate the effectiveness of reconstructing original knowledge and predicting entity attributes from these embeddings when LLMs are employed.","Empirical findings indicate that LLMs significantly improve the accuracy of two evaluated tasks over those from pre-trained models, regardless of whether the texts are in-distribution or out-of-distribution.","This underscores a heightened potential for LLMs to jeopardize user privacy, highlighting the negative consequences of their widespread use.","We further discuss preliminary strategies to mitigate this risk."],"url":"http://arxiv.org/abs/2404.16587v1","category":"cs.CL"}
{"created":"2024-04-25 12:59:56","title":"Numerical integrators for confined Langevin dynamics","abstract":"We derive and analyze numerical methods for weak approximation of underdamped (kinetic) Langevin dynamics in bounded domains. First-order methods are based on an Euler-type scheme interlaced with collisions with the boundary. To achieve second order, composition schemes are derived based on decomposition of the generator into collisional drift, impulse, and stochastic momentum evolution. In a deterministic setting, this approach would typically lead to first-order approximation, even in symmetric compositions, but we find that the stochastic method can provide second-order weak approximation with a single gradient evaluation, both at finite times and in the ergodic limit. We provide theoretical and numerical justification for this observation using model problems and compare and contrast the numerical performance of different choices of the ordering of the terms in the splitting scheme.","sentences":["We derive and analyze numerical methods for weak approximation of underdamped (kinetic)","Langevin dynamics in bounded domains.","First-order methods are based on an Euler-type scheme interlaced with collisions with the boundary.","To achieve second order, composition schemes are derived based on decomposition of the generator into collisional drift, impulse, and stochastic momentum evolution.","In a deterministic setting, this approach would typically lead to first-order approximation, even in symmetric compositions, but we find that the stochastic method can provide second-order weak approximation with a single gradient evaluation, both at finite times and in the ergodic limit.","We provide theoretical and numerical justification for this observation using model problems and compare and contrast the numerical performance of different choices of the ordering of the terms in the splitting scheme."],"url":"http://arxiv.org/abs/2404.16584v1","category":"math.NA"}
{"created":"2024-04-25 12:47:47","title":"Neural Interaction Energy for Multi-Agent Trajectory Prediction","abstract":"Maintaining temporal stability is crucial in multi-agent trajectory prediction. Insufficient regularization to uphold this stability often results in fluctuations in kinematic states, leading to inconsistent predictions and the amplification of errors. In this study, we introduce a framework called Multi-Agent Trajectory prediction via neural interaction Energy (MATE). This framework assesses the interactive motion of agents by employing neural interaction energy, which captures the dynamics of interactions and illustrates their influence on the future trajectories of agents. To bolster temporal stability, we introduce two constraints: inter-agent interaction constraint and intra-agent motion constraint. These constraints work together to ensure temporal stability at both the system and agent levels, effectively mitigating prediction fluctuations inherent in multi-agent systems. Comparative evaluations against previous methods on four diverse datasets highlight the superior prediction accuracy and generalization capabilities of our model.","sentences":["Maintaining temporal stability is crucial in multi-agent trajectory prediction.","Insufficient regularization to uphold this stability often results in fluctuations in kinematic states, leading to inconsistent predictions and the amplification of errors.","In this study, we introduce a framework called Multi-Agent Trajectory prediction via neural interaction Energy (MATE).","This framework assesses the interactive motion of agents by employing neural interaction energy, which captures the dynamics of interactions and illustrates their influence on the future trajectories of agents.","To bolster temporal stability, we introduce two constraints: inter-agent interaction constraint and intra-agent motion constraint.","These constraints work together to ensure temporal stability at both the system and agent levels, effectively mitigating prediction fluctuations inherent in multi-agent systems.","Comparative evaluations against previous methods on four diverse datasets highlight the superior prediction accuracy and generalization capabilities of our model."],"url":"http://arxiv.org/abs/2404.16579v1","category":"cs.AI"}
{"created":"2024-04-25 12:46:23","title":"Road Surface Friction Estimation for Winter Conditions Utilising General Visual Features","abstract":"In below freezing winter conditions, road surface friction can greatly vary based on the mixture of snow, ice, and water on the road. Friction between the road and vehicle tyres is a critical parameter defining vehicle dynamics, and therefore road surface friction information is essential to acquire for several intelligent transportation applications, such as safe control of automated vehicles or alerting drivers of slippery road conditions. This paper explores computer vision-based evaluation of road surface friction from roadside cameras. Previous studies have extensively investigated the application of convolutional neural networks for the task of evaluating the road surface condition from images. Here, we propose a hybrid deep learning architecture, WCamNet, consisting of a pretrained visual transformer model and convolutional blocks. The motivation of the architecture is to combine general visual features provided by the transformer model, as well as finetuned feature extraction properties of the convolutional blocks. To benchmark the approach, an extensive dataset was gathered from national Finnish road infrastructure network of roadside cameras and optical road surface friction sensors. Acquired results highlight that the proposed WCamNet outperforms previous approaches in the task of predicting the road surface friction from the roadside camera images.","sentences":["In below freezing winter conditions, road surface friction can greatly vary based on the mixture of snow, ice, and water on the road.","Friction between the road and vehicle tyres is a critical parameter defining vehicle dynamics, and therefore road surface friction information is essential to acquire for several intelligent transportation applications, such as safe control of automated vehicles or alerting drivers of slippery road conditions.","This paper explores computer vision-based evaluation of road surface friction from roadside cameras.","Previous studies have extensively investigated the application of convolutional neural networks for the task of evaluating the road surface condition from images.","Here, we propose a hybrid deep learning architecture, WCamNet, consisting of a pretrained visual transformer model and convolutional blocks.","The motivation of the architecture is to combine general visual features provided by the transformer model, as well as finetuned feature extraction properties of the convolutional blocks.","To benchmark the approach, an extensive dataset was gathered from national Finnish road infrastructure network of roadside cameras and optical road surface friction sensors.","Acquired results highlight that the proposed WCamNet outperforms previous approaches in the task of predicting the road surface friction from the roadside camera images."],"url":"http://arxiv.org/abs/2404.16578v1","category":"cs.CV"}
{"created":"2024-04-25 12:43:52","title":"Implicit-Explicit schemes for decoupling multicontinuum problems in porous media","abstract":"In this work, we present an efficient way to decouple the multicontinuum problems. To construct decoupled schemes, we propose Implicit-Explicit time approximation in general form and study them for the fine-scale and coarse-scale space approximations. We use a finite-volume method for fine-scale approximation, and the nonlocal multicontinuum (NLMC) method is used to construct an accurate and physically meaningful coarse-scale approximation. The NLMC method is an accurate technique to develop a physically meaningful coarse scale model based on defining the macroscale variables. The multiscale basis functions are constructed in local domains by solving constraint energy minimization problems and projecting the system to the coarse grid. The resulting basis functions have exponential decay properties and lead to the accurate approximation on a coarse grid. We construct a fully Implicit time approximation for semi-discrete systems arising after fine-scale and coarse-scale space approximations. We investigate the stability of the two and three-level schemes for fully Implicit and Implicit-Explicit time approximations schemes for multicontinuum problems in fractured porous media. We show that combining the decoupling technique with multiscale approximation leads to developing an accurate and efficient solver for multicontinuum problems.","sentences":["In this work, we present an efficient way to decouple the multicontinuum problems.","To construct decoupled schemes, we propose Implicit-Explicit time approximation in general form and study them for the fine-scale and coarse-scale space approximations.","We use a finite-volume method for fine-scale approximation, and the nonlocal multicontinuum (NLMC) method is used to construct an accurate and physically meaningful coarse-scale approximation.","The NLMC method is an accurate technique to develop a physically meaningful coarse scale model based on defining the macroscale variables.","The multiscale basis functions are constructed in local domains by solving constraint energy minimization problems and projecting the system to the coarse grid.","The resulting basis functions have exponential decay properties and lead to the accurate approximation on a coarse grid.","We construct a fully Implicit time approximation for semi-discrete systems arising after fine-scale and coarse-scale space approximations.","We investigate the stability of the two and three-level schemes for fully Implicit and Implicit-Explicit time approximations schemes for multicontinuum problems in fractured porous media.","We show that combining the decoupling technique with multiscale approximation leads to developing an accurate and efficient solver for multicontinuum problems."],"url":"http://arxiv.org/abs/2404.16576v1","category":"math.NA"}
{"created":"2024-04-25 12:34:47","title":"ReliK: A Reliability Measure for Knowledge Graph Embeddings","abstract":"Can we assess a priori how well a knowledge graph embedding will perform on a specific downstream task and in a specific part of the knowledge graph? Knowledge graph embeddings (KGEs) represent entities (e.g., \"da Vinci,\" \"Mona Lisa\") and relationships (e.g., \"painted\") of a knowledge graph (KG) as vectors. KGEs are generated by optimizing an embedding score, which assesses whether a triple (e.g., \"da Vinci,\" \"painted,\" \"Mona Lisa\") exists in the graph. KGEs have been proven effective in a variety of web-related downstream tasks, including, for instance, predicting relationships among entities. However, the problem of anticipating the performance of a given KGE in a certain downstream task and locally to a specific individual triple, has not been tackled so far.   In this paper, we fill this gap with ReliK, a Reliability measure for KGEs. ReliK relies solely on KGE embedding scores, is task- and KGE-agnostic, and requires no further KGE training. As such, it is particularly appealing for semantic web applications which call for testing multiple KGE methods on various parts of the KG and on each individual downstream task. Through extensive experiments, we attest that ReliK correlates well with both common downstream tasks, such as tail or relation prediction and triple classification, as well as advanced downstream tasks, such as rule mining and question answering, while preserving locality.","sentences":["Can we assess a priori how well a knowledge graph embedding will perform on a specific downstream task and in a specific part of the knowledge graph?","Knowledge graph embeddings (KGEs) represent entities (e.g., \"da Vinci,\" \"Mona Lisa\") and relationships (e.g., \"painted\") of a knowledge graph (KG) as vectors.","KGEs are generated by optimizing an embedding score, which assesses whether a triple (e.g., \"da Vinci,\" \"painted,\" \"Mona Lisa\") exists in the graph.","KGEs have been proven effective in a variety of web-related downstream tasks, including, for instance, predicting relationships among entities.","However, the problem of anticipating the performance of a given KGE in a certain downstream task and locally to a specific individual triple, has not been tackled so far.   ","In this paper, we fill this gap with ReliK, a Reliability measure for KGEs.","ReliK relies solely on KGE embedding scores, is task- and KGE-agnostic, and requires no further KGE training.","As such, it is particularly appealing for semantic web applications which call for testing multiple KGE methods on various parts of the KG and on each individual downstream task.","Through extensive experiments, we attest that ReliK correlates well with both common downstream tasks, such as tail or relation prediction and triple classification, as well as advanced downstream tasks, such as rule mining and question answering, while preserving locality."],"url":"http://arxiv.org/abs/2404.16572v1","category":"cs.SI"}
{"created":"2024-04-25 12:28:32","title":"Boltzmann Generators and the New Frontier of Computational Sampling in Many-Body Systems","abstract":"The paper by No\\'e et al. [F. No\\'e, S. Olsson, J. K\\\"ohler and H. Wu, Science, 365:6457 (2019)] introduced the concept of Boltzmann Generators (BGs), a deep generative model that can produce unbiased independent samples of many-body systems. They can generate equilibrium configurations from different metastable states, compute relative stabilities between different structures of proteins or other organic molecules, and discover new states. In this commentary, we motivate the necessity for a new generation of sampling methods beyond molecular dynamics, explain the methodology, and give our perspective on the future role of BGs.","sentences":["The paper by No\\'e et al.","[F. No\\'e, S. Olsson, J. K\\\"ohler and H. Wu, Science, 365:6457 (2019)] introduced the concept of Boltzmann Generators (BGs), a deep generative model that can produce unbiased independent samples of many-body systems.","They can generate equilibrium configurations from different metastable states, compute relative stabilities between different structures of proteins or other organic molecules, and discover new states.","In this commentary, we motivate the necessity for a new generation of sampling methods beyond molecular dynamics, explain the methodology, and give our perspective on the future role of BGs."],"url":"http://arxiv.org/abs/2404.16566v1","category":"physics.comp-ph"}
{"created":"2024-04-25 12:27:22","title":"Deep learning-based blind image super-resolution with iterative kernel reconstruction and noise estimation","abstract":"Blind single image super-resolution (SISR) is a challenging task in image processing due to the ill-posed nature of the inverse problem. Complex degradations present in real life images make it difficult to solve this problem using na\\\"ive deep learning approaches, where models are often trained on synthetically generated image pairs. Most of the effort so far has been focused on solving the inverse problem under some constraints, such as for a limited space of blur kernels and/or assuming noise-free input images. Yet, there is a gap in the literature to provide a well-generalized deep learning-based solution that performs well on images with unknown and highly complex degradations. In this paper, we propose IKR-Net (Iterative Kernel Reconstruction Network) for blind SISR. In the proposed approach, kernel and noise estimation and high-resolution image reconstruction are carried out iteratively using dedicated deep models. The iterative refinement provides significant improvement in both the reconstructed image and the estimated blur kernel even for noisy inputs. IKR-Net provides a generalized solution that can handle any type of blur and level of noise in the input low-resolution image. IKR-Net achieves state-of-the-art results in blind SISR, especially for noisy images with motion blur.","sentences":["Blind single image super-resolution (SISR) is a challenging task in image processing due to the ill-posed nature of the inverse problem.","Complex degradations present in real life images make it difficult to solve this problem using na\\\"ive deep learning approaches, where models are often trained on synthetically generated image pairs.","Most of the effort so far has been focused on solving the inverse problem under some constraints, such as for a limited space of blur kernels and/or assuming noise-free input images.","Yet, there is a gap in the literature to provide a well-generalized deep learning-based solution that performs well on images with unknown and highly complex degradations.","In this paper, we propose IKR-Net (Iterative Kernel Reconstruction Network) for blind SISR.","In the proposed approach, kernel and noise estimation and high-resolution image reconstruction are carried out iteratively using dedicated deep models.","The iterative refinement provides significant improvement in both the reconstructed image and the estimated blur kernel even for noisy inputs.","IKR-Net provides a generalized solution that can handle any type of blur and level of noise in the input low-resolution image.","IKR-Net achieves state-of-the-art results in blind SISR, especially for noisy images with motion blur."],"url":"http://arxiv.org/abs/2404.16564v1","category":"eess.IV"}
{"created":"2024-04-25 12:18:04","title":"Research on geometric figure classification algorithm based on Deep Learning","abstract":"In recent years, with the rapid development of computer information technology, the development of artificial intelligence has been accelerating. The traditional geometry recognition technology is relatively backward and the recognition rate is low. In the face of massive information database, the traditional algorithm model inevitably has the problems of low recognition accuracy and poor performance. Deep learning theory has gradually become a very important part of machine learning. The implementation of convolutional neural network (CNN) reduces the difficulty of graphics generation algorithm. In this paper, using the advantages of lenet-5 architecture sharing weights and feature extraction and classification, the proposed geometric pattern recognition algorithm model is faster in the training data set. By constructing the shared feature parameters of the algorithm model, the cross-entropy loss function is used in the recognition process to improve the generalization of the model and improve the average recognition accuracy of the test data set.","sentences":["In recent years, with the rapid development of computer information technology, the development of artificial intelligence has been accelerating.","The traditional geometry recognition technology is relatively backward and the recognition rate is low.","In the face of massive information database, the traditional algorithm model inevitably has the problems of low recognition accuracy and poor performance.","Deep learning theory has gradually become a very important part of machine learning.","The implementation of convolutional neural network (CNN) reduces the difficulty of graphics generation algorithm.","In this paper, using the advantages of lenet-5 architecture sharing weights and feature extraction and classification, the proposed geometric pattern recognition algorithm model is faster in the training data set.","By constructing the shared feature parameters of the algorithm model, the cross-entropy loss function is used in the recognition process to improve the generalization of the model and improve the average recognition accuracy of the test data set."],"url":"http://arxiv.org/abs/2404.16561v1","category":"cs.CV"}
{"created":"2024-04-25 12:16:58","title":"Automated Model Selection for Generalized Linear Models","abstract":"In this paper, we show how mixed-integer conic optimization can be used to combine feature subset selection with holistic generalized linear models to fully automate the model selection process. Concretely, we directly optimize for the Akaike and Bayesian information criteria while imposing constraints designed to deal with multicollinearity in the feature selection task. Specifically, we propose a novel pairwise correlation constraint that combines the sign coherence constraint with ideas from classical statistical models like Ridge regression and the OSCAR model.","sentences":["In this paper, we show how mixed-integer conic optimization can be used to combine feature subset selection with holistic generalized linear models to fully automate the model selection process.","Concretely, we directly optimize for the Akaike and Bayesian information criteria while imposing constraints designed to deal with multicollinearity in the feature selection task.","Specifically, we propose a novel pairwise correlation constraint that combines the sign coherence constraint with ideas from classical statistical models like Ridge regression and the OSCAR model."],"url":"http://arxiv.org/abs/2404.16560v1","category":"stat.ML"}
{"created":"2024-04-25 12:15:11","title":"DeepKalPose: An Enhanced Deep-Learning Kalman Filter for Temporally Consistent Monocular Vehicle Pose Estimation","abstract":"This paper presents DeepKalPose, a novel approach for enhancing temporal consistency in monocular vehicle pose estimation applied on video through a deep-learning-based Kalman Filter. By integrating a Bi-directional Kalman filter strategy utilizing forward and backward time-series processing, combined with a learnable motion model to represent complex motion patterns, our method significantly improves pose accuracy and robustness across various conditions, particularly for occluded or distant vehicles. Experimental validation on the KITTI dataset confirms that DeepKalPose outperforms existing methods in both pose accuracy and temporal consistency.","sentences":["This paper presents DeepKalPose, a novel approach for enhancing temporal consistency in monocular vehicle pose estimation applied on video through a deep-learning-based Kalman Filter.","By integrating a Bi-directional Kalman filter strategy utilizing forward and backward time-series processing, combined with a learnable motion model to represent complex motion patterns, our method significantly improves pose accuracy and robustness across various conditions, particularly for occluded or distant vehicles.","Experimental validation on the KITTI dataset confirms that DeepKalPose outperforms existing methods in both pose accuracy and temporal consistency."],"url":"http://arxiv.org/abs/2404.16558v1","category":"cs.CV"}
{"created":"2024-04-25 12:11:38","title":"Energy-Latency Manipulation of Multi-modal Large Language Models via Verbose Samples","abstract":"Despite the exceptional performance of multi-modal large language models (MLLMs), their deployment requires substantial computational resources. Once malicious users induce high energy consumption and latency time (energy-latency cost), it will exhaust computational resources and harm availability of service. In this paper, we investigate this vulnerability for MLLMs, particularly image-based and video-based ones, and aim to induce high energy-latency cost during inference by crafting an imperceptible perturbation. We find that high energy-latency cost can be manipulated by maximizing the length of generated sequences, which motivates us to propose verbose samples, including verbose images and videos. Concretely, two modality non-specific losses are proposed, including a loss to delay end-of-sequence (EOS) token and an uncertainty loss to increase the uncertainty over each generated token. In addition, improving diversity is important to encourage longer responses by increasing the complexity, which inspires the following modality specific loss. For verbose images, a token diversity loss is proposed to promote diverse hidden states. For verbose videos, a frame feature diversity loss is proposed to increase the feature diversity among frames. To balance these losses, we propose a temporal weight adjustment algorithm. Experiments demonstrate that our verbose samples can largely extend the length of generated sequences.","sentences":["Despite the exceptional performance of multi-modal large language models (MLLMs), their deployment requires substantial computational resources.","Once malicious users induce high energy consumption and latency time (energy-latency cost), it will exhaust computational resources and harm availability of service.","In this paper, we investigate this vulnerability for MLLMs, particularly image-based and video-based ones, and aim to induce high energy-latency cost during inference by crafting an imperceptible perturbation.","We find that high energy-latency cost can be manipulated by maximizing the length of generated sequences, which motivates us to propose verbose samples, including verbose images and videos.","Concretely, two modality non-specific losses are proposed, including a loss to delay end-of-sequence (EOS) token and an uncertainty loss to increase the uncertainty over each generated token.","In addition, improving diversity is important to encourage longer responses by increasing the complexity, which inspires the following modality specific loss.","For verbose images, a token diversity loss is proposed to promote diverse hidden states.","For verbose videos, a frame feature diversity loss is proposed to increase the feature diversity among frames.","To balance these losses, we propose a temporal weight adjustment algorithm.","Experiments demonstrate that our verbose samples can largely extend the length of generated sequences."],"url":"http://arxiv.org/abs/2404.16557v1","category":"cs.CV"}
{"created":"2024-04-25 12:11:28","title":"Conditional Distribution Modelling for Few-Shot Image Synthesis with Diffusion Models","abstract":"Few-shot image synthesis entails generating diverse and realistic images of novel categories using only a few example images. While multiple recent efforts in this direction have achieved impressive results, the existing approaches are dependent only upon the few novel samples available at test time in order to generate new images, which restricts the diversity of the generated images. To overcome this limitation, we propose Conditional Distribution Modelling (CDM) -- a framework which effectively utilizes Diffusion models for few-shot image generation. By modelling the distribution of the latent space used to condition a Diffusion process, CDM leverages the learnt statistics of the training data to get a better approximation of the unseen class distribution, thereby removing the bias arising due to limited number of few shot samples. Simultaneously, we devise a novel inversion based optimization strategy that further improves the approximated unseen class distribution, and ensures the fidelity of the generated samples to the unseen class. The experimental results on four benchmark datasets demonstrate the effectiveness of our proposed CDM for few-shot generation.","sentences":["Few-shot image synthesis entails generating diverse and realistic images of novel categories using only a few example images.","While multiple recent efforts in this direction have achieved impressive results, the existing approaches are dependent only upon the few novel samples available at test time in order to generate new images, which restricts the diversity of the generated images.","To overcome this limitation, we propose Conditional Distribution Modelling (CDM) -- a framework which effectively utilizes Diffusion models for few-shot image generation.","By modelling the distribution of the latent space used to condition a Diffusion process, CDM leverages the learnt statistics of the training data to get a better approximation of the unseen class distribution, thereby removing the bias arising due to limited number of few shot samples.","Simultaneously, we devise a novel inversion based optimization strategy that further improves the approximated unseen class distribution, and ensures the fidelity of the generated samples to the unseen class.","The experimental results on four benchmark datasets demonstrate the effectiveness of our proposed CDM for few-shot generation."],"url":"http://arxiv.org/abs/2404.16556v1","category":"cs.CV"}
{"created":"2024-04-25 12:11:27","title":"MMGRec: Multimodal Generative Recommendation with Transformer Model","abstract":"Multimodal recommendation aims to recommend user-preferred candidates based on her/his historically interacted items and associated multimodal information. Previous studies commonly employ an embed-and-retrieve paradigm: learning user and item representations in the same embedding space, then retrieving similar candidate items for a user via embedding inner product. However, this paradigm suffers from inference cost, interaction modeling, and false-negative issues. Toward this end, we propose a new MMGRec model to introduce a generative paradigm into multimodal recommendation. Specifically, we first devise a hierarchical quantization method Graph RQ-VAE to assign Rec-ID for each item from its multimodal and CF information. Consisting of a tuple of semantically meaningful tokens, Rec-ID serves as the unique identifier of each item. Afterward, we train a Transformer-based recommender to generate the Rec-IDs of user-preferred items based on historical interaction sequences. The generative paradigm is qualified since this model systematically predicts the tuple of tokens identifying the recommended item in an autoregressive manner. Moreover, a relation-aware self-attention mechanism is devised for the Transformer to handle non-sequential interaction sequences, which explores the element pairwise relation to replace absolute positional encoding. Extensive experiments evaluate MMGRec's effectiveness compared with state-of-the-art methods.","sentences":["Multimodal recommendation aims to recommend user-preferred candidates based on her/his historically interacted items and associated multimodal information.","Previous studies commonly employ an embed-and-retrieve paradigm: learning user and item representations in the same embedding space, then retrieving similar candidate items for a user via embedding inner product.","However, this paradigm suffers from inference cost, interaction modeling, and false-negative issues.","Toward this end, we propose a new MMGRec model to introduce a generative paradigm into multimodal recommendation.","Specifically, we first devise a hierarchical quantization method Graph RQ-VAE to assign Rec-ID for each item from its multimodal and CF information.","Consisting of a tuple of semantically meaningful tokens, Rec-ID serves as the unique identifier of each item.","Afterward, we train a Transformer-based recommender to generate the Rec-IDs of user-preferred items based on historical interaction sequences.","The generative paradigm is qualified since this model systematically predicts the tuple of tokens identifying the recommended item in an autoregressive manner.","Moreover, a relation-aware self-attention mechanism is devised for the Transformer to handle non-sequential interaction sequences, which explores the element pairwise relation to replace absolute positional encoding.","Extensive experiments evaluate MMGRec's effectiveness compared with state-of-the-art methods."],"url":"http://arxiv.org/abs/2404.16555v1","category":"cs.IR"}
{"created":"2024-04-25 12:10:29","title":"Generalized Multiscale Finite Element Method for discrete network (graph) models","abstract":"In this paper, we consider a time-dependent discrete network model with highly varying connectivity. The approximation by time is performed using an implicit scheme. We propose the coarse scale approximation construction of network models based on the Generalized Multiscale Finite Element Method. An accurate coarse-scale approximation is generated by solving local spectral problems in sub-networks. Convergence analysis of the proposed method is presented for semi-discrete and discrete network models. We establish the stability of the multiscale discrete network. Numerical results are presented for structured and random heterogeneous networks.","sentences":["In this paper, we consider a time-dependent discrete network model with highly varying connectivity.","The approximation by time is performed using an implicit scheme.","We propose the coarse scale approximation construction of network models based on the Generalized Multiscale Finite Element Method.","An accurate coarse-scale approximation is generated by solving local spectral problems in sub-networks.","Convergence analysis of the proposed method is presented for semi-discrete and discrete network models.","We establish the stability of the multiscale discrete network.","Numerical results are presented for structured and random heterogeneous networks."],"url":"http://arxiv.org/abs/2404.16554v1","category":"math.NA"}
{"created":"2024-04-25 12:03:14","title":"Developing Acoustic Models for Automatic Speech Recognition in Swedish","abstract":"This paper is concerned with automatic continuous speech recognition using trainable systems. The aim of this work is to build acoustic models for spoken Swedish. This is done employing hidden Markov models and using the SpeechDat database to train their parameters. Acoustic modeling has been worked out at a phonetic level, allowing general speech recognition applications, even though a simplified task (digits and natural number recognition) has been considered for model evaluation. Different kinds of phone models have been tested, including context independent models and two variations of context dependent models. Furthermore many experiments have been done with bigram language models to tune some of the system parameters. System performance over various speaker subsets with different sex, age and dialect has also been examined. Results are compared to previous similar studies showing a remarkable improvement.","sentences":["This paper is concerned with automatic continuous speech recognition using trainable systems.","The aim of this work is to build acoustic models for spoken Swedish.","This is done employing hidden Markov models and using the SpeechDat database to train their parameters.","Acoustic modeling has been worked out at a phonetic level, allowing general speech recognition applications, even though a simplified task (digits and natural number recognition) has been considered for model evaluation.","Different kinds of phone models have been tested, including context independent models and two variations of context dependent models.","Furthermore many experiments have been done with bigram language models to tune some of the system parameters.","System performance over various speaker subsets with different sex, age and dialect has also been examined.","Results are compared to previous similar studies showing a remarkable improvement."],"url":"http://arxiv.org/abs/2404.16547v1","category":"eess.AS"}
{"created":"2024-04-25 12:02:37","title":"Lessons from ATLAS and CMS measurements of Higgs boson decays to second generation fermions","abstract":"There is now experimental evidence for Higgs boson decay into a pair of muons, and significant constraints on the Higgs boson decay into a charm quark-antiquark pair. The data on Higgs boson decays into second generation fermions probes various extensions of the Standard Model. We analyze the implications for the Standard Model effective field theory (SMEFT), without and with minimal flavor violation (MFV), for two Higgs doublet models (2HDM) with natural flavor conservation (NFC), for models with vector-like fermions, and for specific models that predict significant modifications of the Yukawa couplings to the light generations.","sentences":["There is now experimental evidence for Higgs boson decay into a pair of muons, and significant constraints on the Higgs boson decay into a charm quark-antiquark pair.","The data on Higgs boson decays into second generation fermions probes various extensions of the Standard Model.","We analyze the implications for the Standard Model effective field theory (SMEFT), without and with minimal flavor violation (MFV), for two Higgs doublet models (2HDM) with natural flavor conservation (NFC), for models with vector-like fermions, and for specific models that predict significant modifications of the Yukawa couplings to the light generations."],"url":"http://arxiv.org/abs/2404.16545v1","category":"hep-ph"}
{"created":"2024-04-25 11:58:06","title":"Optimal depth and a novel approach to variational quantum process tomography","abstract":"In this work, we present two new methods for Variational Quantum Circuit (VQC) Process Tomography onto $n$ qubits systems: PT_VQC and U-VQSVD.   Compared to the state of the art, PT_VQC halves in each run the required amount of qubits for process tomography and decreases the required state initializations from $4^{n}$ to just $2^{n}$, all while ensuring high-fidelity reconstruction of the targeted unitary channel $U$. It is worth noting that, for a fixed reconstruction accuracy, PT_VQC achieves faster convergence per iteration step compared to Quantum Deep Neural Network (QDNN) and tensor network schemes.   The novel U-VQSVD algorithm utilizes variational singular value decomposition to extract eigenvectors (up to a global phase) and their associated eigenvalues from an unknown unitary representing a general channel. We assess the performance of U-VQSVD by executing an attack on a non-unitary channel Quantum Physical Unclonable Function (QPUF). U-VQSVD outperforms an uninformed impersonation attack (using randomly generated input states) by a factor of 2 to 5, depending on the qubit dimension.   For the two presented methods, we propose a new approach to calculate the complexity of the displayed VQC, based on what we denote as optimal depth.","sentences":["In this work, we present two new methods for Variational Quantum Circuit (VQC)","Process Tomography onto $n$ qubits systems: PT_VQC and U-VQSVD.   ","Compared to the state of the art, PT_VQC halves in each run the required amount of qubits for process tomography and decreases the required state initializations from $4^{n}$ to just $2^{n}$, all while ensuring high-fidelity reconstruction of the targeted unitary channel $U$.","It is worth noting that, for a fixed reconstruction accuracy, PT_VQC achieves faster convergence per iteration step compared to Quantum Deep Neural Network (QDNN) and tensor network schemes.   ","The novel U-VQSVD algorithm utilizes variational singular value decomposition to extract eigenvectors (up to a global phase) and their associated eigenvalues from an unknown unitary representing a general channel.","We assess the performance of U-VQSVD by executing an attack on a non-unitary channel Quantum Physical Unclonable Function (QPUF).","U-VQSVD outperforms an uninformed impersonation attack (using randomly generated input states) by a factor of 2 to 5, depending on the qubit dimension.   ","For the two presented methods, we propose a new approach to calculate the complexity of the displayed VQC, based on what we denote as optimal depth."],"url":"http://arxiv.org/abs/2404.16541v1","category":"quant-ph"}
{"created":"2024-04-25 11:54:38","title":"Confluent functions, Laguerre polynomials and their (generalized) bilinear integrals","abstract":"We review properties of confluent functions and the closely related Laguerre polynomials, and determine their bilinear integrals. As is well-known, these integrals are convergent only for a limited range of parameters. However, when one uses the generalized integral they can be computed essentially without restricting the parameters. This gives the (generalized) Gram matrix of Laguerre polynomials. If the parameters are not negative integers, then Laguerre polynomials are orthogonal, or at least pseudo-orthogonal in the case of generalized integrals. For negative integer parameters, the orthogonality relations are more complicated.","sentences":["We review properties of confluent functions and the closely related Laguerre polynomials, and determine their bilinear integrals.","As is well-known, these integrals are convergent only for a limited range of parameters.","However, when one uses the generalized integral they can be computed essentially without restricting the parameters.","This gives the (generalized) Gram matrix of Laguerre polynomials.","If the parameters are not negative integers, then Laguerre polynomials are orthogonal, or at least pseudo-orthogonal in the case of generalized integrals.","For negative integer parameters, the orthogonality relations are more complicated."],"url":"http://arxiv.org/abs/2404.16539v1","category":"math.CA"}
{"created":"2024-04-25 11:53:36","title":"OpenDlign: Enhancing Open-World 3D Learning with Depth-Aligned Images","abstract":"Recent advances in Vision and Language Models (VLMs) have improved open-world 3D representation, facilitating 3D zero-shot capability in unseen categories. Existing open-world methods pre-train an extra 3D encoder to align features from 3D data (e.g., depth maps or point clouds) with CAD-rendered images and corresponding texts. However, the limited color and texture variations in CAD images can compromise the alignment robustness. Furthermore, the volume discrepancy between pre-training datasets of the 3D encoder and VLM leads to sub-optimal 2D to 3D knowledge transfer. To overcome these issues, we propose OpenDlign, a novel framework for learning open-world 3D representations, that leverages depth-aligned images generated from point cloud-projected depth maps. Unlike CAD-rendered images, our generated images provide rich, realistic color and texture diversity while preserving geometric and semantic consistency with the depth maps. OpenDlign also optimizes depth map projection and integrates depth-specific text prompts, improving 2D VLM knowledge adaptation for 3D learning efficient fine-tuning. Experimental results show that OpenDlign significantly outperforms existing benchmarks in zero-shot and few-shot 3D tasks, exceeding prior scores by 8.0% on ModelNet40 and 16.4% on OmniObject3D with just 6 million tuned parameters. Moreover, integrating generated depth-aligned images into existing 3D learning pipelines consistently improves their performance.","sentences":["Recent advances in Vision and Language Models (VLMs) have improved open-world 3D representation, facilitating 3D zero-shot capability in unseen categories.","Existing open-world methods pre-train an extra 3D encoder to align features from 3D data (e.g., depth maps or point clouds) with CAD-rendered images and corresponding texts.","However, the limited color and texture variations in CAD images can compromise the alignment robustness.","Furthermore, the volume discrepancy between pre-training datasets of the 3D encoder and VLM leads to sub-optimal 2D to 3D knowledge transfer.","To overcome these issues, we propose OpenDlign, a novel framework for learning open-world 3D representations, that leverages depth-aligned images generated from point cloud-projected depth maps.","Unlike CAD-rendered images, our generated images provide rich, realistic color and texture diversity while preserving geometric and semantic consistency with the depth maps.","OpenDlign also optimizes depth map projection and integrates depth-specific text prompts, improving 2D VLM knowledge adaptation for 3D learning efficient fine-tuning.","Experimental results show that OpenDlign significantly outperforms existing benchmarks in zero-shot and few-shot 3D tasks, exceeding prior scores by 8.0% on ModelNet40 and 16.4% on OmniObject3D with just 6 million tuned parameters.","Moreover, integrating generated depth-aligned images into existing 3D learning pipelines consistently improves their performance."],"url":"http://arxiv.org/abs/2404.16538v1","category":"cs.CV"}
{"created":"2024-04-25 11:50:47","title":"3D Face Modeling via Weakly-supervised Disentanglement Network joint Identity-consistency Prior","abstract":"Generative 3D face models featuring disentangled controlling factors hold immense potential for diverse applications in computer vision and computer graphics. However, previous 3D face modeling methods face a challenge as they demand specific labels to effectively disentangle these factors. This becomes particularly problematic when integrating multiple 3D face datasets to improve the generalization of the model. Addressing this issue, this paper introduces a Weakly-Supervised Disentanglement Framework, denoted as WSDF, to facilitate the training of controllable 3D face models without an overly stringent labeling requirement. Adhering to the paradigm of Variational Autoencoders (VAEs), the proposed model achieves disentanglement of identity and expression controlling factors through a two-branch encoder equipped with dedicated identity-consistency prior. It then faithfully re-entangles these factors via a tensor-based combination mechanism. Notably, the introduction of the Neutral Bank allows precise acquisition of subject-specific information using only identity labels, thereby averting degeneration due to insufficient supervision. Additionally, the framework incorporates a label-free second-order loss function for the expression factor to regulate deformation space and eliminate extraneous information, resulting in enhanced disentanglement. Extensive experiments have been conducted to substantiate the superior performance of WSDF. Our code is available at https://github.com/liguohao96/WSDF.","sentences":["Generative 3D face models featuring disentangled controlling factors hold immense potential for diverse applications in computer vision and computer graphics.","However, previous 3D face modeling methods face a challenge as they demand specific labels to effectively disentangle these factors.","This becomes particularly problematic when integrating multiple 3D face datasets to improve the generalization of the model.","Addressing this issue, this paper introduces a Weakly-Supervised Disentanglement Framework, denoted as WSDF, to facilitate the training of controllable 3D face models without an overly stringent labeling requirement.","Adhering to the paradigm of Variational Autoencoders (VAEs), the proposed model achieves disentanglement of identity and expression controlling factors through a two-branch encoder equipped with dedicated identity-consistency prior.","It then faithfully re-entangles these factors via a tensor-based combination mechanism.","Notably, the introduction of the Neutral Bank allows precise acquisition of subject-specific information using only identity labels, thereby averting degeneration due to insufficient supervision.","Additionally, the framework incorporates a label-free second-order loss function for the expression factor to regulate deformation space and eliminate extraneous information, resulting in enhanced disentanglement.","Extensive experiments have been conducted to substantiate the superior performance of WSDF.","Our code is available at https://github.com/liguohao96/WSDF."],"url":"http://arxiv.org/abs/2404.16536v1","category":"cs.CV"}
{"created":"2024-04-25 11:47:39","title":"SIDEs: Separating Idealization from Deceptive Explanations in xAI","abstract":"Explainable AI (xAI) methods are important for establishing trust in using black-box models. However, recent criticism has mounted against current xAI methods that they disagree, are necessarily false, and can be manipulated, which has started to undermine the deployment of black-box models. Rudin (2019) goes so far as to say that we should stop using black-box models altogether in high-stakes cases because xAI explanations \"must be wrong\". However, strict fidelity to the truth is historically not a desideratum in science. Idealizations -- the intentional distortions introduced to scientific theories and models -- are commonplace in the natural sciences and are seen as a successful scientific tool. Thus, it is not falsehood qua falsehood that is the issue. In this paper, I outline the need for xAI research to engage in idealization evaluation. Drawing on the use of idealizations in the natural sciences and philosophy of science, I introduce a novel framework for evaluating whether xAI methods engage in successful idealizations or deceptive explanations (SIDEs). SIDEs evaluates whether the limitations of xAI methods, and the distortions that they introduce, can be part of a successful idealization or are indeed deceptive distortions as critics suggest. I discuss the role that existing research can play in idealization evaluation and where innovation is necessary. Through a qualitative analysis we find that leading feature importance methods and counterfactual explanations are subject to idealization failure and suggest remedies for ameliorating idealization failure.","sentences":["Explainable AI (xAI) methods are important for establishing trust in using black-box models.","However, recent criticism has mounted against current xAI methods that they disagree, are necessarily false, and can be manipulated, which has started to undermine the deployment of black-box models.","Rudin (2019) goes so far as to say that we should stop using black-box models altogether in high-stakes cases because xAI explanations \"must be wrong\".","However, strict fidelity to the truth is historically not a desideratum in science.","Idealizations -- the intentional distortions introduced to scientific theories and models -- are commonplace in the natural sciences and are seen as a successful scientific tool.","Thus, it is not falsehood qua falsehood that is the issue.","In this paper, I outline the need for xAI research to engage in idealization evaluation.","Drawing on the use of idealizations in the natural sciences and philosophy of science, I introduce a novel framework for evaluating whether xAI methods engage in successful idealizations or deceptive explanations (SIDEs).","SIDEs evaluates whether the limitations of xAI methods, and the distortions that they introduce, can be part of a successful idealization or are indeed deceptive distortions as critics suggest.","I discuss the role that existing research can play in idealization evaluation and where innovation is necessary.","Through a qualitative analysis we find that leading feature importance methods and counterfactual explanations are subject to idealization failure and suggest remedies for ameliorating idealization failure."],"url":"http://arxiv.org/abs/2404.16534v1","category":"cs.AI"}
{"created":"2024-04-25 11:43:46","title":"Global Concept Explanations for Graphs by Contrastive Learning","abstract":"Beyond improving trust and validating model fairness, xAI practices also have the potential to recover valuable scientific insights in application domains where little to no prior human intuition exists. To that end, we propose a method to extract global concept explanations from the predictions of graph neural networks to develop a deeper understanding of the tasks underlying structure-property relationships. We identify concept explanations as dense clusters in the self-explaining Megan models subgraph latent space. For each concept, we optimize a representative prototype graph and optionally use GPT-4 to provide hypotheses about why each structure has a certain effect on the prediction. We conduct computational experiments on synthetic and real-world graph property prediction tasks. For the synthetic tasks we find that our method correctly reproduces the structural rules by which they were created. For real-world molecular property regression and classification tasks, we find that our method rediscovers established rules of thumb. More specifically, our results for molecular mutagenicity prediction indicate more fine-grained resolution of structural details than existing explainability methods, consistent with previous results from chemistry literature. Overall, our results show promising capability to extract the underlying structure-property relationships for complex graph property prediction tasks.","sentences":["Beyond improving trust and validating model fairness, xAI practices also have the potential to recover valuable scientific insights in application domains where little to no prior human intuition exists.","To that end, we propose a method to extract global concept explanations from the predictions of graph neural networks to develop a deeper understanding of the tasks underlying structure-property relationships.","We identify concept explanations as dense clusters in the self-explaining Megan models subgraph latent space.","For each concept, we optimize a representative prototype graph and optionally use GPT-4 to provide hypotheses about why each structure has a certain effect on the prediction.","We conduct computational experiments on synthetic and real-world graph property prediction tasks.","For the synthetic tasks we find that our method correctly reproduces the structural rules by which they were created.","For real-world molecular property regression and classification tasks, we find that our method rediscovers established rules of thumb.","More specifically, our results for molecular mutagenicity prediction indicate more fine-grained resolution of structural details than existing explainability methods, consistent with previous results from chemistry literature.","Overall, our results show promising capability to extract the underlying structure-property relationships for complex graph property prediction tasks."],"url":"http://arxiv.org/abs/2404.16532v1","category":"cs.LG"}
{"created":"2024-04-25 11:41:24","title":"Generalized Posterior Calibration via Sequential Monte Carlo Sampler","abstract":"For robustness toward model misspecification, the generalized posterior inference approach modifies the likelihood term by raising it to the power of a learning rate, thereby adjusting the spread of the posterior. This paper proposes a computationally efficient strategy for selecting an appropriate learning rate. The proposed approach builds upon the generalized posterior calibration (GPC) algorithm introduced by Syring and Martin (2019) [Biometrika, Volume 106, Issue 2, pp. 479-486], which is designed to select the learning rate to achieve the nominal frequentist coverage. This algorithm, which evaluates the coverage probability based on bootstrap samples, suffers from high computational costs due to the need for repeated posterior simulations for bootstrap samples. To address this limitation, the study proposes an algorithm that combines elements of the GPC algorithm with the sequential Monte Carlo (SMC) sampler. By leveraging the similarity between the learning rate in generalized posterior inference and the inverse temperature in SMC sampling, the proposed algorithm efficiently calibrates the posterior distribution with less computational cost. For demonstration, the proposed algorithm was applied to several statistical learning models.","sentences":["For robustness toward model misspecification, the generalized posterior inference approach modifies the likelihood term by raising it to the power of a learning rate, thereby adjusting the spread of the posterior.","This paper proposes a computationally efficient strategy for selecting an appropriate learning rate.","The proposed approach builds upon the generalized posterior calibration (GPC) algorithm introduced by Syring and Martin (2019)","[Biometrika, Volume 106, Issue 2, pp. 479-486], which is designed to select the learning rate to achieve the nominal frequentist coverage.","This algorithm, which evaluates the coverage probability based on bootstrap samples, suffers from high computational costs due to the need for repeated posterior simulations for bootstrap samples.","To address this limitation, the study proposes an algorithm that combines elements of the GPC algorithm with the sequential Monte Carlo (SMC) sampler.","By leveraging the similarity between the learning rate in generalized posterior inference and the inverse temperature in SMC sampling, the proposed algorithm efficiently calibrates the posterior distribution with less computational cost.","For demonstration, the proposed algorithm was applied to several statistical learning models."],"url":"http://arxiv.org/abs/2404.16528v1","category":"stat.CO"}
{"created":"2024-04-25 11:36:39","title":"An efficient method to generate near-ideal hollow beams of different shapes for box potential of quantum gases","abstract":"Ultracold quantum gases are usually prepared in conservative traps for quantum simulation experiments. The atomic density inhomogeneity, together with the consequent position-dependent energy and time scales of cold atoms in traditional harmonic traps, makes it difficult to manipulate and detect the sample at a better level. These problems are partially solved by optical box traps of blue-detuned hollow beams. However, generating a high-quality hollow beam with high light efficiency for the box trap is challenging. Here, we present a scheme that combines the fixed optics, including axicons and prisms, to pre-shape a Gaussian beam into a hollow beam, with a digital micromirror device (DMD) to improve the quality of the hollow beam further, providing a nearly ideal optical potential of various shapes for preparing highly homogeneous cold atoms. The highest power-law exponent of potential walls can reach a value over 100, and the light efficiency from a Gaussian to a hollow beam is also improved compared to direct optical shaping by a mask or a DMD. Combined with a one-dimensional optical lattice, a nearly ideal two-dimensional uniform quantum gas with different geometrical boundaries can be prepared for exploring quantum many-body physics to an unprecedented level.","sentences":["Ultracold quantum gases are usually prepared in conservative traps for quantum simulation experiments.","The atomic density inhomogeneity, together with the consequent position-dependent energy and time scales of cold atoms in traditional harmonic traps, makes it difficult to manipulate and detect the sample at a better level.","These problems are partially solved by optical box traps of blue-detuned hollow beams.","However, generating a high-quality hollow beam with high light efficiency for the box trap is challenging.","Here, we present a scheme that combines the fixed optics, including axicons and prisms, to pre-shape a Gaussian beam into a hollow beam, with a digital micromirror device (DMD) to improve the quality of the hollow beam further, providing a nearly ideal optical potential of various shapes for preparing highly homogeneous cold atoms.","The highest power-law exponent of potential walls can reach a value over 100, and the light efficiency from a Gaussian to a hollow beam is also improved compared to direct optical shaping by a mask or a DMD.","Combined with a one-dimensional optical lattice, a nearly ideal two-dimensional uniform quantum gas with different geometrical boundaries can be prepared for exploring quantum many-body physics to an unprecedented level."],"url":"http://arxiv.org/abs/2404.16525v1","category":"cond-mat.quant-gas"}
{"created":"2024-04-25 11:22:48","title":"Unbiased Estimating Equation on Inverse Divergence and Its Conditions","abstract":"This paper focuses on the Bregman divergence defined by the reciprocal function, called the inverse divergence. For the loss function defined by the monotonically increasing function $f$ and inverse divergence, the conditions for the statistical model and function $f$ under which the estimating equation is unbiased are clarified. Specifically, we characterize two types of statistical models, an inverse Gaussian type and a mixture of generalized inverse Gaussian type distributions, to show that the conditions for the function $f$ are different for each model. We also define Bregman divergence as a linear sum over the dimensions of the inverse divergence and extend the results to the multi-dimensional case.","sentences":["This paper focuses on the Bregman divergence defined by the reciprocal function, called the inverse divergence.","For the loss function defined by the monotonically increasing function $f$ and inverse divergence, the conditions for the statistical model and function $f$ under which the estimating equation is unbiased are clarified.","Specifically, we characterize two types of statistical models, an inverse Gaussian type and a mixture of generalized inverse Gaussian type distributions, to show that the conditions for the function $f$ are different for each model.","We also define Bregman divergence as a linear sum over the dimensions of the inverse divergence and extend the results to the multi-dimensional case."],"url":"http://arxiv.org/abs/2404.16519v1","category":"cs.IT"}
{"created":"2024-04-25 11:17:56","title":"Nonclassicality in a dispersive atom-cavity field interaction in presence of an external driving field","abstract":"We investigate nonclassical properties of a state generated by the interaction of a three-level atom with a quantized cavity field and an external classical driving field. In this study, the fields being degenerate in frequency, are highly detuned from the atom. The atom interacts with the quantized field in a dispersive manner. The experimental set-up involves a three-level atom passing through a cavity and interacting dispersively with the cavity field mode. Simultaneously, the atom interacts with an external classical field that is in resonance with the cavity field. The three-level atom can enter the cavity in one of the bare states $\\ket{e}$, $\\ket{f}$ or $\\ket{g}$ or in a superposition of two of these states. In this paper, we consider superposition of $\\ket{e}$ and $\\ket{f}$. In our analysis, we focus on the statistical properties of the cavity field after interacting with the atom. The state vector $|\\psi(t)\\rangle$ describes the entire atom-field system but we analyze the properties of the cavity field independently neglecting the atomic component of the system. For this the atom part is traced out from $|\\psi(t)\\rangle$ to acquire the cavity field state only, denoted by $\\ket{\\psi_{ f}(t)}$. We evaluate different nonclassical measures including photon number distribution, Mandel's $Q_M$ parameter, squeezing properties $S_x$ and $S_p$, Wigner distribution, $Q_f$ function, second-order correlation function $g^2(0)$ etc. for the obtained cavity field state.","sentences":["We investigate nonclassical properties of a state generated by the interaction of a three-level atom with a quantized cavity field and an external classical driving field.","In this study, the fields being degenerate in frequency, are highly detuned from the atom.","The atom interacts with the quantized field in a dispersive manner.","The experimental set-up involves a three-level atom passing through a cavity and interacting dispersively with the cavity field mode.","Simultaneously, the atom interacts with an external classical field that is in resonance with the cavity field.","The three-level atom can enter the cavity in one of the bare states $\\ket{e}$, $\\ket{f}$ or $\\ket{g}$ or in a superposition of two of these states.","In this paper, we consider superposition of $\\ket{e}$ and $\\ket{f}$. In our analysis, we focus on the statistical properties of the cavity field after interacting with the atom.","The state vector $|\\psi(t)\\rangle$ describes the entire atom-field system but we analyze the properties of the cavity field independently neglecting the atomic component of the system.","For this the atom part is traced out from $|\\psi(t)\\rangle$ to acquire the cavity field state only, denoted by $\\ket{\\psi_{ f}(t)}$. We evaluate different nonclassical measures including photon number distribution, Mandel's $Q_M$ parameter, squeezing properties $S_x$ and $S_p$, Wigner distribution, $Q_f$ function, second-order correlation function $g^2(0)$ etc. for the obtained cavity field state."],"url":"http://arxiv.org/abs/2404.16515v1","category":"quant-ph"}
{"created":"2024-04-25 11:11:25","title":"Revisiting the apparent horizon finding problem with multigrid methods","abstract":"Apparent horizon plays an important role in numerical relativity as it provides a tool to characterize the existence and properties of black holes on three-dimensional spatial slices in 3+1 numerical spacetimes. Apparent horizon finders based on different techniques have been developed. In this paper, we revisit the apparent horizon finding problem in numerical relativity using multigrid-based algorithms. We formulate the nonlinear elliptic apparent horizon equation as a linear Poisson-type equation with a nonlinear source, and solve it using a multigrid algorithm with Gauss-Seidel line relaxation. A fourth order compact finite difference scheme in spherical coordinates is derived and employed to reduce the complexity of the line relaxation operator to a tri-diagonal matrix inversion. The multigrid-based apparent horizon finder developed in this work is capable of locating apparent horizons in generic spatial hypersurfaces without any symmetries. The finder is tested with both analytic data, such as Brill-Lindquist multiple black hole data, and numerical data, including off-centered Kerr-Schild data and dynamical inspiraling binary black hole data. The obtained results are compared with those generated by the current fastest finder AHFinderDirect (Thornburg, Class. Quantum Grav. 21, 743, 2003), which is the default finder in the open source code Einstein Toolkit. Our finder performs comparatively in terms of accuracy, and starts to outperform AHFinderDirect at high angular resolutions (\\sim 1^\\circ) in terms of speed. Our finder is also more flexible to initial guess, as opposed to the Newton's method used in AHFinderDirect. This suggests that the multigrid algorithm provides an alternative option for studying apparent horizons, especially when high resolutions are needed.","sentences":["Apparent horizon plays an important role in numerical relativity as it provides a tool to characterize the existence and properties of black holes on three-dimensional spatial slices in 3+1 numerical spacetimes.","Apparent horizon finders based on different techniques have been developed.","In this paper, we revisit the apparent horizon finding problem in numerical relativity using multigrid-based algorithms.","We formulate the nonlinear elliptic apparent horizon equation as a linear Poisson-type equation with a nonlinear source, and solve it using a multigrid algorithm with Gauss-Seidel line relaxation.","A fourth order compact finite difference scheme in spherical coordinates is derived and employed to reduce the complexity of the line relaxation operator to a tri-diagonal matrix inversion.","The multigrid-based apparent horizon finder developed in this work is capable of locating apparent horizons in generic spatial hypersurfaces without any symmetries.","The finder is tested with both analytic data, such as Brill-Lindquist multiple black hole data, and numerical data, including off-centered Kerr-Schild data and dynamical inspiraling binary black hole data.","The obtained results are compared with those generated by the current fastest finder AHFinderDirect (Thornburg, Class.","Quantum Grav.","21, 743, 2003), which is the default finder in the open source code Einstein Toolkit.","Our finder performs comparatively in terms of accuracy, and starts to outperform AHFinderDirect at high angular resolutions (\\sim 1^\\circ) in terms of speed.","Our finder is also more flexible to initial guess, as opposed to the Newton's method used in AHFinderDirect.","This suggests that the multigrid algorithm provides an alternative option for studying apparent horizons, especially when high resolutions are needed."],"url":"http://arxiv.org/abs/2404.16511v1","category":"gr-qc"}
{"created":"2024-04-25 11:06:57","title":"Interactive3D: Create What You Want by Interactive 3D Generation","abstract":"3D object generation has undergone significant advancements, yielding high-quality results. However, fall short of achieving precise user control, often yielding results that do not align with user expectations, thus limiting their applicability. User-envisioning 3D object generation faces significant challenges in realizing its concepts using current generative models due to limited interaction capabilities. Existing methods mainly offer two approaches: (i) interpreting textual instructions with constrained controllability, or (ii) reconstructing 3D objects from 2D images. Both of them limit customization to the confines of the 2D reference and potentially introduce undesirable artifacts during the 3D lifting process, restricting the scope for direct and versatile 3D modifications. In this work, we introduce Interactive3D, an innovative framework for interactive 3D generation that grants users precise control over the generative process through extensive 3D interaction capabilities. Interactive3D is constructed in two cascading stages, utilizing distinct 3D representations. The first stage employs Gaussian Splatting for direct user interaction, allowing modifications and guidance of the generative direction at any intermediate step through (i) Adding and Removing components, (ii) Deformable and Rigid Dragging, (iii) Geometric Transformations, and (iv) Semantic Editing. Subsequently, the Gaussian splats are transformed into InstantNGP. We introduce a novel (v) Interactive Hash Refinement module to further add details and extract the geometry in the second stage. Our experiments demonstrate that Interactive3D markedly improves the controllability and quality of 3D generation. Our project webpage is available at \\url{https://interactive-3d.github.io/}.","sentences":["3D object generation has undergone significant advancements, yielding high-quality results.","However, fall short of achieving precise user control, often yielding results that do not align with user expectations, thus limiting their applicability.","User-envisioning 3D object generation faces significant challenges in realizing its concepts using current generative models due to limited interaction capabilities.","Existing methods mainly offer two approaches: (i) interpreting textual instructions with constrained controllability, or (ii) reconstructing 3D objects from 2D images.","Both of them limit customization to the confines of the 2D reference and potentially introduce undesirable artifacts during the 3D lifting process, restricting the scope for direct and versatile 3D modifications.","In this work, we introduce Interactive3D, an innovative framework for interactive 3D generation that grants users precise control over the generative process through extensive 3D interaction capabilities.","Interactive3D is constructed in two cascading stages, utilizing distinct 3D representations.","The first stage employs Gaussian Splatting for direct user interaction, allowing modifications and guidance of the generative direction at any intermediate step through (i) Adding and Removing components, (ii) Deformable and Rigid Dragging, (iii) Geometric Transformations, and (iv) Semantic Editing.","Subsequently, the Gaussian splats are transformed into InstantNGP.","We introduce a novel (v) Interactive Hash Refinement module to further add details and extract the geometry in the second stage.","Our experiments demonstrate that Interactive3D markedly improves the controllability and quality of 3D generation.","Our project webpage is available at \\url{https://interactive-3d.github.io/}."],"url":"http://arxiv.org/abs/2404.16510v1","category":"cs.GR"}
{"created":"2024-04-25 11:05:41","title":"Gauged Q-ball dark matter through a cosmological first-order phase transition","abstract":"As a new type of dynamical dark matter mechanism, we discuss the stability of the gauged Q-ball dark matter and its production mechanism through a cosmological first-order phase transition. This work delves into the study of gauged Q-ball dark matter generated during the cosmic phase transition. We demonstrate detailed discussions on the stability of gauged Q-balls to rigorously constrain their charge and mass ranges. Additionally, employing analytic approximations and the mapping method, we provide qualitative insights of gauged Q-ball. We establish an upper limit on the gauge coupling constant and give the relic density of stable gauged Q-ball dark matter formed during a first-order phase transition. Furthermore, we discuss potential observational signatures or constraints of gauged Q-ball dark matter, including astronomical observations and gravitational wave signals.","sentences":["As a new type of dynamical dark matter mechanism, we discuss the stability of the gauged Q-ball dark matter and its production mechanism through a cosmological first-order phase transition.","This work delves into the study of gauged Q-ball dark matter generated during the cosmic phase transition.","We demonstrate detailed discussions on the stability of gauged Q-balls to rigorously constrain their charge and mass ranges.","Additionally, employing analytic approximations and the mapping method, we provide qualitative insights of gauged Q-ball.","We establish an upper limit on the gauge coupling constant and give the relic density of stable gauged Q-ball dark matter formed during a first-order phase transition.","Furthermore, we discuss potential observational signatures or constraints of gauged Q-ball dark matter, including astronomical observations and gravitational wave signals."],"url":"http://arxiv.org/abs/2404.16509v1","category":"hep-ph"}
{"created":"2024-04-25 11:02:54","title":"Exploring the Dynamics of Data Transmission in 5G Networks: A Conceptual Analysis","abstract":"This conceptual analysis examines the dynamics of data transmission in 5G networks. It addresses various aspects of sending data from cameras and LiDARs installed on a remote-controlled ferry to a land-based control center. The range of topics includes all stages of video and LiDAR data processing from acquisition and encoding to final decoding, all aspects of their transmission and reception via the WebRTC protocol, and all possible types of network problems such as handovers or congestion that could affect the quality of experience for end-users. A series of experiments were conducted to evaluate the key aspects of the data transmission. These include simulation-based reproducible runs and real-world experiments conducted using open-source solutions we developed: \"Gymir5G\" - an OMNeT++-based 5G simulation and \"GstWebRTCApp\" - a GStreamer-based application for adaptive control of media streams over the WebRTC protocol. One of the goals of this study is to formulate the bandwidth and latency requirements for reliable real-time communication and to estimate their approximate values. This goal was achieved through simulation-based experiments involving docking maneuvers in the Bay of Kiel, Germany. The final latency for the entire data processing pipeline was also estimated during the real tests. In addition, a series of simulation-based experiments showed the impact of key WebRTC features and demonstrated the effectiveness of the WebRTC protocol, while the conducted video codec comparison showed that the hardware-accelerated H.264 codec is the best. Finally, the research addresses the topic of adaptive communication, where the traditional congestion avoidance and deep reinforcement learning approaches were analyzed. The comparison in a sandbox scenario shows that the AI-based solution outperforms the WebRTC baseline GCC algorithm in terms of data rates, latency, and packet loss.","sentences":["This conceptual analysis examines the dynamics of data transmission in 5G networks.","It addresses various aspects of sending data from cameras and LiDARs installed on a remote-controlled ferry to a land-based control center.","The range of topics includes all stages of video and LiDAR data processing from acquisition and encoding to final decoding, all aspects of their transmission and reception via the WebRTC protocol, and all possible types of network problems such as handovers or congestion that could affect the quality of experience for end-users.","A series of experiments were conducted to evaluate the key aspects of the data transmission.","These include simulation-based reproducible runs and real-world experiments conducted using open-source solutions we developed: \"Gymir5G\" - an OMNeT++-based 5G simulation and \"GstWebRTCApp\" - a GStreamer-based application for adaptive control of media streams over the WebRTC protocol.","One of the goals of this study is to formulate the bandwidth and latency requirements for reliable real-time communication and to estimate their approximate values.","This goal was achieved through simulation-based experiments involving docking maneuvers in the Bay of Kiel, Germany.","The final latency for the entire data processing pipeline was also estimated during the real tests.","In addition, a series of simulation-based experiments showed the impact of key WebRTC features and demonstrated the effectiveness of the WebRTC protocol, while the conducted video codec comparison showed that the hardware-accelerated H.264 codec is the best.","Finally, the research addresses the topic of adaptive communication, where the traditional congestion avoidance and deep reinforcement learning approaches were analyzed.","The comparison in a sandbox scenario shows that the AI-based solution outperforms the WebRTC baseline GCC algorithm in terms of data rates, latency, and packet loss."],"url":"http://arxiv.org/abs/2404.16508v1","category":"cs.NI"}
{"created":"2024-04-25 10:54:47","title":"Hardware Implementation of Double Pendulum Pseudo Random Number Generator","abstract":"The objective of this project is to utilize an FPGA board which is the CMOD A7 35t to obtain a pseudo random number which can be used for encryption. We aim to achieve this by leveraging the inherent randomness present in environmental data captured by sensors. This data will be used as a seed to initialize an algorithm implemented on the CMOD A7 35t FPGA board. The project will focus on interfacing the sensors with the FPGA and developing suitable algorithms to ensure the generated numbers exhibit strong randomness properties.","sentences":["The objective of this project is to utilize an FPGA board which is the CMOD A7 35t to obtain a pseudo random number which can be used for encryption.","We aim to achieve this by leveraging the inherent randomness present in environmental data captured by sensors.","This data will be used as a seed to initialize an algorithm implemented on the CMOD A7 35t FPGA board.","The project will focus on interfacing the sensors with the FPGA and developing suitable algorithms to ensure the generated numbers exhibit strong randomness properties."],"url":"http://arxiv.org/abs/2404.16504v1","category":"cs.CR"}
{"created":"2024-04-25 10:54:16","title":"Characteristics of FEL-generated THz waves using linear and helical undulators","abstract":"Emission of terahertz waves in free electron lasers (FELs) with both linear and helical undulators has been explored. The analysis employs Lienard-Wiechert fields to characterize FEL radiation within the THz region. Specifically, the study delves into the analytical examination of radiation pattern of THz waves, which provides insight into the angular distribution of radiation energy. The variations of radiation pattern for various parameters such as different harmonics, Lorentz factor, magnetic parameter, and total number of periods of linear and helical undulators were investigated. The Fourier transform of free electron laser electric field is showcased across four distinct sets of FEL parameters. The interplay between electron beam and undulators parameters sheds light on how these factors influence the directionality of THz waves. Simulation results indicate that linear undulators often produce radiation with noticeable side lobes, especially at higher harmonics, while helical undulators typically exhibit reduced side lobes, resulting in a more focused main radiation peak. However, the radiation pattern of THz waves tends to concentrate predominantly in the forward direction and the polarization of the emitted radiation is determined by the properties of the undulator and electron beam, rather than the specific undulator geometry. The study underscores that an optimal selection of undulator and beam parameters can effectively maximize the angular distribution of emitted radiation.","sentences":["Emission of terahertz waves in free electron lasers (FELs) with both linear and helical undulators has been explored.","The analysis employs Lienard-Wiechert fields to characterize FEL radiation within the THz region.","Specifically, the study delves into the analytical examination of radiation pattern of THz waves, which provides insight into the angular distribution of radiation energy.","The variations of radiation pattern for various parameters such as different harmonics, Lorentz factor, magnetic parameter, and total number of periods of linear and helical undulators were investigated.","The Fourier transform of free electron laser electric field is showcased across four distinct sets of FEL parameters.","The interplay between electron beam and undulators parameters sheds light on how these factors influence the directionality of THz waves.","Simulation results indicate that linear undulators often produce radiation with noticeable side lobes, especially at higher harmonics, while helical undulators typically exhibit reduced side lobes, resulting in a more focused main radiation peak.","However, the radiation pattern of THz waves tends to concentrate predominantly in the forward direction and the polarization of the emitted radiation is determined by the properties of the undulator and electron beam, rather than the specific undulator geometry.","The study underscores that an optimal selection of undulator and beam parameters can effectively maximize the angular distribution of emitted radiation."],"url":"http://arxiv.org/abs/2404.16503v1","category":"physics.acc-ph"}
{"created":"2024-04-25 10:49:14","title":"Conformal Prediction of Motion Control Performance for an Automated Vehicle in Presence of Actuator Degradations and Failures","abstract":"Automated driving systems require monitoring mechanisms to ensure safe operation, especially if system components degrade or fail. Their runtime self-representation plays a key role as it provides a-priori knowledge about the system's capabilities and limitations. In this paper, we propose a data-driven approach for deriving such a self-representation model for the motion controller of an automated vehicle. A conformalized prediction model is learned and allows estimating how operational conditions as well as potential degradations and failures of the vehicle's actuators impact motion control performance. During runtime behavior generation, our predictor can provide a heuristic for determining the admissible action space.","sentences":["Automated driving systems require monitoring mechanisms to ensure safe operation, especially if system components degrade or fail.","Their runtime self-representation plays a key role as it provides a-priori knowledge about the system's capabilities and limitations.","In this paper, we propose a data-driven approach for deriving such a self-representation model for the motion controller of an automated vehicle.","A conformalized prediction model is learned and allows estimating how operational conditions as well as potential degradations and failures of the vehicle's actuators impact motion control performance.","During runtime behavior generation, our predictor can provide a heuristic for determining the admissible action space."],"url":"http://arxiv.org/abs/2404.16500v1","category":"cs.RO"}
{"created":"2024-04-25 10:45:22","title":"Switching response and ionic hysteresis in organic electrochemical transistors","abstract":"Hysteresis in organic electrochemical transistors (OECT) is a basic effect in which the measured current depends on the voltage sweep direction and velocity. This phenomenon has an important impact on different aspects of the application of OECT, such as the switching time and the synaptic properties for neuromorphic applications. Here we address the combined ionic and electronic kinetic effects that cause the dominant hysteresis effects. We use a combination of tools consisting on basic analytical models, advanced 2D drift-diffusion simulation, and the experimental measurement of a Poly(3-hexylthiophene) (P3HT) OECT, working in an accumulation mode. We develop a general transmission line model considering drift electronic transport and ionic injection and diffusion from the electrolyte. We provide a basic classification of the transient response to a voltage pulse, according to the dominant ionic or electronic relaxation time, and the correspondent hysteresis effects of the transfer curves according to the general categories of inductive and capacitive hysteresis. These are basically related to the main control phenomenon, either the vertical diffusion of ions during doping and dedoping, or the equilibration of electronic current along the channel length.","sentences":["Hysteresis in organic electrochemical transistors (OECT) is a basic effect in which the measured current depends on the voltage sweep direction and velocity.","This phenomenon has an important impact on different aspects of the application of OECT, such as the switching time and the synaptic properties for neuromorphic applications.","Here we address the combined ionic and electronic kinetic effects that cause the dominant hysteresis effects.","We use a combination of tools consisting on basic analytical models, advanced 2D drift-diffusion simulation, and the experimental measurement of a Poly(3-hexylthiophene) (P3HT) OECT, working in an accumulation mode.","We develop a general transmission line model considering drift electronic transport and ionic injection and diffusion from the electrolyte.","We provide a basic classification of the transient response to a voltage pulse, according to the dominant ionic or electronic relaxation time, and the correspondent hysteresis effects of the transfer curves according to the general categories of inductive and capacitive hysteresis.","These are basically related to the main control phenomenon, either the vertical diffusion of ions during doping and dedoping, or the equilibration of electronic current along the channel length."],"url":"http://arxiv.org/abs/2404.16498v1","category":"physics.app-ph"}
{"created":"2024-04-25 10:40:49","title":"T-Explainer: A Model-Agnostic Explainability Framework Based on Gradients","abstract":"The development of machine learning applications has increased significantly in recent years, motivated by the remarkable ability of learning-powered systems to discover and generalize intricate patterns hidden in massive datasets. Modern learning models, while powerful, often exhibit a level of complexity that renders them opaque black boxes, resulting in a notable lack of transparency that hinders our ability to decipher their decision-making processes. Opacity challenges the interpretability and practical application of machine learning, especially in critical domains where understanding the underlying reasons is essential for informed decision-making. Explainable Artificial Intelligence (XAI) rises to meet that challenge, unraveling the complexity of black boxes by providing elucidating explanations. Among the various XAI approaches, feature attribution/importance XAI stands out for its capacity to delineate the significance of input features in the prediction process. However, most existing attribution methods have limitations, such as instability, when divergent explanations may result from similar or even the same instance. In this work, we introduce T-Explainer, a novel local additive attribution explainer based on Taylor expansion endowed with desirable properties, such as local accuracy and consistency, while stable over multiple runs. We demonstrate T-Explainer's effectiveness through benchmark experiments with well-known attribution methods. In addition, T-Explainer is developed as a comprehensive XAI framework comprising quantitative metrics to assess and visualize attribution explanations.","sentences":["The development of machine learning applications has increased significantly in recent years, motivated by the remarkable ability of learning-powered systems to discover and generalize intricate patterns hidden in massive datasets.","Modern learning models, while powerful, often exhibit a level of complexity that renders them opaque black boxes, resulting in a notable lack of transparency that hinders our ability to decipher their decision-making processes.","Opacity challenges the interpretability and practical application of machine learning, especially in critical domains where understanding the underlying reasons is essential for informed decision-making.","Explainable Artificial Intelligence (XAI) rises to meet that challenge, unraveling the complexity of black boxes by providing elucidating explanations.","Among the various XAI approaches, feature attribution/importance XAI stands out for its capacity to delineate the significance of input features in the prediction process.","However, most existing attribution methods have limitations, such as instability, when divergent explanations may result from similar or even the same instance.","In this work, we introduce T-Explainer, a novel local additive attribution explainer based on Taylor expansion endowed with desirable properties, such as local accuracy and consistency, while stable over multiple runs.","We demonstrate T-Explainer's effectiveness through benchmark experiments with well-known attribution methods.","In addition, T-Explainer is developed as a comprehensive XAI framework comprising quantitative metrics to assess and visualize attribution explanations."],"url":"http://arxiv.org/abs/2404.16495v1","category":"cs.LG"}
{"created":"2024-04-25 10:38:33","title":"Commonsense Prototype for Outdoor Unsupervised 3D Object Detection","abstract":"The prevalent approaches of unsupervised 3D object detection follow cluster-based pseudo-label generation and iterative self-training processes. However, the challenge arises due to the sparsity of LiDAR scans, which leads to pseudo-labels with erroneous size and position, resulting in subpar detection performance. To tackle this problem, this paper introduces a Commonsense Prototype-based Detector, termed CPD, for unsupervised 3D object detection. CPD first constructs Commonsense Prototype (CProto) characterized by high-quality bounding box and dense points, based on commonsense intuition. Subsequently, CPD refines the low-quality pseudo-labels by leveraging the size prior from CProto. Furthermore, CPD enhances the detection accuracy of sparsely scanned objects by the geometric knowledge from CProto. CPD outperforms state-of-the-art unsupervised 3D detectors on Waymo Open Dataset (WOD), PandaSet, and KITTI datasets by a large margin. Besides, by training CPD on WOD and testing on KITTI, CPD attains 90.85% and 81.01% 3D Average Precision on easy and moderate car classes, respectively. These achievements position CPD in close proximity to fully supervised detectors, highlighting the significance of our method. The code will be available at https://github.com/hailanyi/CPD.","sentences":["The prevalent approaches of unsupervised 3D object detection follow cluster-based pseudo-label generation and iterative self-training processes.","However, the challenge arises due to the sparsity of LiDAR scans, which leads to pseudo-labels with erroneous size and position, resulting in subpar detection performance.","To tackle this problem, this paper introduces a Commonsense Prototype-based Detector, termed CPD, for unsupervised 3D object detection.","CPD first constructs Commonsense Prototype (CProto) characterized by high-quality bounding box and dense points, based on commonsense intuition.","Subsequently, CPD refines the low-quality pseudo-labels by leveraging the size prior from CProto.","Furthermore, CPD enhances the detection accuracy of sparsely scanned objects by the geometric knowledge from CProto.","CPD outperforms state-of-the-art unsupervised 3D detectors on Waymo Open Dataset (WOD), PandaSet, and KITTI datasets by a large margin.","Besides, by training CPD on WOD and testing on KITTI, CPD attains 90.85% and 81.01% 3D Average Precision on easy and moderate car classes, respectively.","These achievements position CPD in close proximity to fully supervised detectors, highlighting the significance of our method.","The code will be available at https://github.com/hailanyi/CPD."],"url":"http://arxiv.org/abs/2404.16493v1","category":"cs.CV"}
{"created":"2024-04-25 10:27:25","title":"On Neighbourhood Cross Validation","abstract":"It is shown how to efficiently and accurately compute and optimize a range of cross validation criteria for a wide range of models estimated by minimizing a quadratically penalized smooth loss. Example models include generalized additive models for location scale and shape and smooth additive quantile regression. Example losses include negative log likelihoods and smooth quantile losses. Example cross validation criteria include leave-out-neighbourhood cross validation for dealing with un-modelled short range autocorrelation as well as the more familiar leave-one-out cross validation. For a $p$ coefficient model of $n$ data, estimable at $O(np^2)$ computational cost, the general $O(n^2p^2)$ cost of ordinary cross validation is reduced to $O(np^2)$, computing the cross validation criterion to $O(p^3n^{-2})$ accuracy. This is achieved by directly approximating the model coefficient estimates under data subset omission, via efficiently computed single step Newton updates of the full data coefficient estimates. Optimization of the resulting cross validation criterion, with respect to multiple smoothing/precision parameters, can be achieved efficiently using quasi-Newton optimization, adapted to deal with the indefiniteness that occurs when the optimal value for a smoothing parameter tends to infinity. The link between cross validation and the jackknife can be exploited to achieve reasonably well calibrated uncertainty quantification for the model coefficients in non standard settings such as leaving-out-neighbourhoods under residual autocorrelation or quantile regression. Several practical examples are provided, focussing particularly on dealing with un-modelled auto-correlation.","sentences":["It is shown how to efficiently and accurately compute and optimize a range of cross validation criteria for a wide range of models estimated by minimizing a quadratically penalized smooth loss.","Example models include generalized additive models for location scale and shape and smooth additive quantile regression.","Example losses include negative log likelihoods and smooth quantile losses.","Example cross validation criteria include leave-out-neighbourhood cross validation for dealing with un-modelled short range autocorrelation as well as the more familiar leave-one-out cross validation.","For a $p$ coefficient model of $n$ data, estimable at $O(np^2)$ computational cost, the general $O(n^2p^2)$ cost of ordinary cross validation is reduced to $O(np^2)$, computing the cross validation criterion to $O(p^3n^{-2})$ accuracy.","This is achieved by directly approximating the model coefficient estimates under data subset omission, via efficiently computed single step Newton updates of the full data coefficient estimates.","Optimization of the resulting cross validation criterion, with respect to multiple smoothing/precision parameters, can be achieved efficiently using quasi-Newton optimization, adapted to deal with the indefiniteness that occurs when the optimal value for a smoothing parameter tends to infinity.","The link between cross validation and the jackknife can be exploited to achieve reasonably well calibrated uncertainty quantification for the model coefficients in non standard settings such as leaving-out-neighbourhoods under residual autocorrelation or quantile regression.","Several practical examples are provided, focussing particularly on dealing with un-modelled auto-correlation."],"url":"http://arxiv.org/abs/2404.16490v1","category":"stat.ME"}
{"created":"2024-04-25 10:10:48","title":"CoCoG: Controllable Visual Stimuli Generation based on Human Concept Representations","abstract":"A central question for cognitive science is to understand how humans process visual objects, i.e, to uncover human low-dimensional concept representation space from high-dimensional visual stimuli. Generating visual stimuli with controlling concepts is the key. However, there are currently no generative models in AI to solve this problem. Here, we present the Concept based Controllable Generation (CoCoG) framework. CoCoG consists of two components, a simple yet efficient AI agent for extracting interpretable concept and predicting human decision-making in visual similarity judgment tasks, and a conditional generation model for generating visual stimuli given the concepts. We quantify the performance of CoCoG from two aspects, the human behavior prediction accuracy and the controllable generation ability. The experiments with CoCoG indicate that 1) the reliable concept embeddings in CoCoG allows to predict human behavior with 64.07\\% accuracy in the THINGS-similarity dataset; 2) CoCoG can generate diverse objects through the control of concepts; 3) CoCoG can manipulate human similarity judgment behavior by intervening key concepts. CoCoG offers visual objects with controlling concepts to advance our understanding of causality in human cognition. The code of CoCoG is available at \\url{https://github.com/ncclab-sustech/CoCoG}.","sentences":["A central question for cognitive science is to understand how humans process visual objects, i.e, to uncover human low-dimensional concept representation space from high-dimensional visual stimuli.","Generating visual stimuli with controlling concepts is the key.","However, there are currently no generative models in AI to solve this problem.","Here, we present the Concept based Controllable Generation (CoCoG) framework.","CoCoG consists of two components, a simple yet efficient AI agent for extracting interpretable concept and predicting human decision-making in visual similarity judgment tasks, and a conditional generation model for generating visual stimuli given the concepts.","We quantify the performance of CoCoG from two aspects, the human behavior prediction accuracy and the controllable generation ability.","The experiments with CoCoG indicate that 1) the reliable concept embeddings in CoCoG allows to predict human behavior with 64.07\\% accuracy in the THINGS-similarity dataset; 2) CoCoG can generate diverse objects through the control of concepts; 3) CoCoG can manipulate human similarity judgment behavior by intervening key concepts.","CoCoG offers visual objects with controlling concepts to advance our understanding of causality in human cognition.","The code of CoCoG is available at \\url{https://github.com/ncclab-sustech/CoCoG}."],"url":"http://arxiv.org/abs/2404.16482v1","category":"q-bio.NC"}
{"created":"2024-04-25 10:10:28","title":"Secret Key Generation Rates for Line of Sight Multipath Channels in the Presence of Eavesdroppers","abstract":"In this paper, the feasibility of implementing a lightweight key distribution scheme using physical layer security for secret key generation (SKG) is explored. Specifically, we focus on examining SKG with the received signal strength (RSS) serving as the primary source of shared randomness. Our investigation centers on a frequency-selective line-of-sight (LoS) multipath channel, with a particular emphasis on assessing SKG rates derived from the distributions of RSS. We derive the received signal distributions based on how the multipath components resolve at the receiver. The mutual information (MI) is evaluated based on LoS 3GPP channel models using a numerical estimator. We study how the bandwidth, delay spread, and Rician K-factor impact the estimated MI. This MI then serves as a benchmark setting bounds for the SKG rates in our exploration.","sentences":["In this paper, the feasibility of implementing a lightweight key distribution scheme using physical layer security for secret key generation (SKG) is explored.","Specifically, we focus on examining SKG with the received signal strength (RSS) serving as the primary source of shared randomness.","Our investigation centers on a frequency-selective line-of-sight (LoS) multipath channel, with a particular emphasis on assessing SKG rates derived from the distributions of RSS.","We derive the received signal distributions based on how the multipath components resolve at the receiver.","The mutual information (MI) is evaluated based on LoS 3GPP channel models using a numerical estimator.","We study how the bandwidth, delay spread, and Rician K-factor impact the estimated MI.","This MI then serves as a benchmark setting bounds for the SKG rates in our exploration."],"url":"http://arxiv.org/abs/2404.16481v1","category":"eess.SP"}
{"created":"2024-04-25 10:03:14","title":"Evaluating Consistency and Reasoning Capabilities of Large Language Models","abstract":"Large Language Models (LLMs) are extensively used today across various sectors, including academia, research, business, and finance, for tasks such as text generation, summarization, and translation. Despite their widespread adoption, these models often produce incorrect and misleading information, exhibiting a tendency to hallucinate. This behavior can be attributed to several factors, with consistency and reasoning capabilities being significant contributors. LLMs frequently lack the ability to generate explanations and engage in coherent reasoning, leading to inaccurate responses. Moreover, they exhibit inconsistencies in their outputs. This paper aims to evaluate and compare the consistency and reasoning capabilities of both public and proprietary LLMs. The experiments utilize the Boolq dataset as the ground truth, comprising questions, answers, and corresponding explanations. Queries from the dataset are presented as prompts to the LLMs, and the generated responses are evaluated against the ground truth answers. Additionally, explanations are generated to assess the models' reasoning abilities. Consistency is evaluated by repeatedly presenting the same query to the models and observing for variations in their responses. For measuring reasoning capabilities, the generated explanations are compared to the ground truth explanations using metrics such as BERT, BLEU, and F-1 scores. The findings reveal that proprietary models generally outperform public models in terms of both consistency and reasoning capabilities. However, even when presented with basic general knowledge questions, none of the models achieved a score of 90\\% in both consistency and reasoning. This study underscores the direct correlation between consistency and reasoning abilities in LLMs and highlights the inherent reasoning challenges present in current language models.","sentences":["Large Language Models (LLMs) are extensively used today across various sectors, including academia, research, business, and finance, for tasks such as text generation, summarization, and translation.","Despite their widespread adoption, these models often produce incorrect and misleading information, exhibiting a tendency to hallucinate.","This behavior can be attributed to several factors, with consistency and reasoning capabilities being significant contributors.","LLMs frequently lack the ability to generate explanations and engage in coherent reasoning, leading to inaccurate responses.","Moreover, they exhibit inconsistencies in their outputs.","This paper aims to evaluate and compare the consistency and reasoning capabilities of both public and proprietary LLMs.","The experiments utilize the Boolq dataset as the ground truth, comprising questions, answers, and corresponding explanations.","Queries from the dataset are presented as prompts to the LLMs, and the generated responses are evaluated against the ground truth answers.","Additionally, explanations are generated to assess the models' reasoning abilities.","Consistency is evaluated by repeatedly presenting the same query to the models and observing for variations in their responses.","For measuring reasoning capabilities, the generated explanations are compared to the ground truth explanations using metrics such as BERT, BLEU, and F-1 scores.","The findings reveal that proprietary models generally outperform public models in terms of both consistency and reasoning capabilities.","However, even when presented with basic general knowledge questions, none of the models achieved a score of 90\\% in both consistency and reasoning.","This study underscores the direct correlation between consistency and reasoning abilities in LLMs and highlights the inherent reasoning challenges present in current language models."],"url":"http://arxiv.org/abs/2404.16478v1","category":"cs.CL"}
{"created":"2024-04-25 09:57:52","title":"DiffSeg: A Segmentation Model for Skin Lesions Based on Diffusion Difference","abstract":"Weakly supervised medical image segmentation (MIS) using generative models is crucial for clinical diagnosis. However, the accuracy of the segmentation results is often limited by insufficient supervision and the complex nature of medical imaging. Existing models also only provide a single outcome, which does not allow for the measurement of uncertainty. In this paper, we introduce DiffSeg, a segmentation model for skin lesions based on diffusion difference which exploits diffusion model principles to ex-tract noise-based features from images with diverse semantic information. By discerning difference between these noise features, the model identifies diseased areas. Moreover, its multi-output capability mimics doctors' annotation behavior, facilitating the visualization of segmentation result consistency and ambiguity. Additionally, it quantifies output uncertainty using Generalized Energy Distance (GED), aiding interpretability and decision-making for physicians. Finally, the model integrates outputs through the Dense Conditional Random Field (DenseCRF) algorithm to refine the segmentation boundaries by considering inter-pixel correlations, which improves the accuracy and optimizes the segmentation results. We demonstrate the effectiveness of DiffSeg on the ISIC 2018 Challenge dataset, outperforming state-of-the-art U-Net-based methods.","sentences":["Weakly supervised medical image segmentation (MIS) using generative models is crucial for clinical diagnosis.","However, the accuracy of the segmentation results is often limited by insufficient supervision and the complex nature of medical imaging.","Existing models also only provide a single outcome, which does not allow for the measurement of uncertainty.","In this paper, we introduce DiffSeg, a segmentation model for skin lesions based on diffusion difference which exploits diffusion model principles to ex-tract noise-based features from images with diverse semantic information.","By discerning difference between these noise features, the model identifies diseased areas.","Moreover, its multi-output capability mimics doctors' annotation behavior, facilitating the visualization of segmentation result consistency and ambiguity.","Additionally, it quantifies output uncertainty using Generalized Energy Distance (GED), aiding interpretability and decision-making for physicians.","Finally, the model integrates outputs through the Dense Conditional Random Field (DenseCRF) algorithm to refine the segmentation boundaries by considering inter-pixel correlations, which improves the accuracy and optimizes the segmentation results.","We demonstrate the effectiveness of DiffSeg on the ISIC 2018 Challenge dataset, outperforming state-of-the-art U-Net-based methods."],"url":"http://arxiv.org/abs/2404.16474v1","category":"cs.CV"}
{"created":"2024-04-25 09:56:33","title":"Generation of multiple bound states in the continuum through doubly degenerate quasi-guided modes","abstract":"We present a detailed theoretical analysis of a peculiar generation of multiple bound states in the continuum (BICs) in two-dimensional periodic arrays of dielectric spheres. They emerge in high-symmetry lattices with the $C_{6v}$ and $C_{4v}$ point groups and involve doubly degenerate quasi-guided modes at the $\\Gamma$ point that can couple to external radiation. By tuning a system parameter, the doubly degenerate modes can exhibit accidental BICs at a critical parameter. In the vicinity of the critical parameter, the two bands originating from the degenerate mode exhibit multiple off-$\\Gamma$ BICs. They move and annihilate across the two bands by changing the parameter around the critical one. A ring-like high $Q$ channel pinned with multiple BICs emerges particularly for the $C_{6v}$ case. Across the critical coupling, the total vorticity of the multiple BICs is conserved. The $\\vb*{k}\\cdot\\vb*{p}$ perturbation theory explains some features of the phenomena reasonably well.","sentences":["We present a detailed theoretical analysis of a peculiar generation of multiple bound states in the continuum (BICs) in two-dimensional periodic arrays of dielectric spheres.","They emerge in high-symmetry lattices with the $C_{6v}$ and $C_{4v}$ point groups and involve doubly degenerate quasi-guided modes at the $\\Gamma$ point that can couple to external radiation.","By tuning a system parameter, the doubly degenerate modes can exhibit accidental BICs at a critical parameter.","In the vicinity of the critical parameter, the two bands originating from the degenerate mode exhibit multiple off-$\\Gamma$ BICs.","They move and annihilate across the two bands by changing the parameter around the critical one.","A ring-like high $Q$ channel pinned with multiple BICs emerges particularly for the $C_{6v}$ case.","Across the critical coupling, the total vorticity of the multiple BICs is conserved.","The $\\vb*{k}\\cdot\\vb*{p}$ perturbation theory explains some features of the phenomena reasonably well."],"url":"http://arxiv.org/abs/2404.16472v1","category":"physics.optics"}
{"created":"2024-04-25 09:55:35","title":"COBRA -- COnfidence score Based on shape Regression Analysis for method-independent quality assessment of object pose estimation from single images","abstract":"We present a generic algorithm for scoring pose estimation methods that rely on single image semantic analysis. The algorithm employs a lightweight putative shape representation using a combination of multiple Gaussian Processes. Each Gaussian Process (GP) yields distance normal distributions from multiple reference points in the object's coordinate system to its surface, thus providing a geometric evaluation framework for scoring predicted poses. Our confidence measure comprises the average mixture probability of pixel back-projections onto the shape template. In the reported experiments, we compare the accuracy of our GP based representation of objects versus the actual geometric models and demonstrate the ability of our method to capture the influence of outliers as opposed to the corresponding intrinsic measures that ship with the segmentation and pose estimation methods.","sentences":["We present a generic algorithm for scoring pose estimation methods that rely on single image semantic analysis.","The algorithm employs a lightweight putative shape representation using a combination of multiple Gaussian Processes.","Each Gaussian Process (GP) yields distance normal distributions from multiple reference points in the object's coordinate system to its surface, thus providing a geometric evaluation framework for scoring predicted poses.","Our confidence measure comprises the average mixture probability of pixel back-projections onto the shape template.","In the reported experiments, we compare the accuracy of our GP based representation of objects versus the actual geometric models and demonstrate the ability of our method to capture the influence of outliers as opposed to the corresponding intrinsic measures that ship with the segmentation and pose estimation methods."],"url":"http://arxiv.org/abs/2404.16471v1","category":"cs.CV"}
{"created":"2024-04-25 09:50:57","title":"A Dual Perspective of Reinforcement Learning for Imposing Policy Constraints","abstract":"Model-free reinforcement learning methods lack an inherent mechanism to impose behavioural constraints on the trained policies. While certain extensions exist, they remain limited to specific types of constraints, such as value constraints with additional reward signals or visitation density constraints. In this work we try to unify these existing techniques and bridge the gap with classical optimization and control theory, using a generic primal-dual framework for value-based and actor-critic reinforcement learning methods. The obtained dual formulations turn out to be especially useful for imposing additional constraints on the learned policy, as an intrinsic relationship between such dual constraints (or regularization terms) and reward modifications in the primal is reveiled. Furthermore, using this framework, we are able to introduce some novel types of constraints, allowing to impose bounds on the policy's action density or on costs associated with transitions between consecutive states and actions. From the adjusted primal-dual optimization problems, a practical algorithm is derived that supports various combinations of policy constraints that are automatically handled throughout training using trainable reward modifications. The resulting $\\texttt{DualCRL}$ method is examined in more detail and evaluated under different (combinations of) constraints on two interpretable environments. The results highlight the efficacy of the method, which ultimately provides the designer of such systems with a versatile toolbox of possible policy constraints.","sentences":["Model-free reinforcement learning methods lack an inherent mechanism to impose behavioural constraints on the trained policies.","While certain extensions exist, they remain limited to specific types of constraints, such as value constraints with additional reward signals or visitation density constraints.","In this work we try to unify these existing techniques and bridge the gap with classical optimization and control theory, using a generic primal-dual framework for value-based and actor-critic reinforcement learning methods.","The obtained dual formulations turn out to be especially useful for imposing additional constraints on the learned policy, as an intrinsic relationship between such dual constraints (or regularization terms) and reward modifications in the primal is reveiled.","Furthermore, using this framework, we are able to introduce some novel types of constraints, allowing to impose bounds on the policy's action density or on costs associated with transitions between consecutive states and actions.","From the adjusted primal-dual optimization problems, a practical algorithm is derived that supports various combinations of policy constraints that are automatically handled throughout training using trainable reward modifications.","The resulting $\\texttt{DualCRL}$ method is examined in more detail and evaluated under different (combinations of) constraints on two interpretable environments.","The results highlight the efficacy of the method, which ultimately provides the designer of such systems with a versatile toolbox of possible policy constraints."],"url":"http://arxiv.org/abs/2404.16468v1","category":"cs.LG"}
{"created":"2024-04-25 09:50:33","title":"Riding Wavelets: A Method to Discover New Classes of Price Jumps","abstract":"Cascades of events and extreme occurrences have garnered significant attention across diverse domains such as financial markets, seismology, and social physics. Such events can stem either from the internal dynamics inherent to the system (endogenous), or from external shocks (exogenous). The possibility of separating these two classes of events has critical implications for professionals in those fields. We introduce an unsupervised framework leveraging a representation of jump time-series based on wavelet coefficients and apply it to stock price jumps. In line with previous work, we recover the fact that the time-asymmetry of volatility is a major feature. Mean-reversion and trend are found to be two additional key features, allowing us to identify new classes of jumps. Furthermore, thanks to our wavelet-based representation, we investigate the reflexive properties of co-jumps, which occur when multiple stocks experience price jumps within the same minute. We argue that a significant fraction of co-jumps results from an endogenous contagion mechanism.","sentences":["Cascades of events and extreme occurrences have garnered significant attention across diverse domains such as financial markets, seismology, and social physics.","Such events can stem either from the internal dynamics inherent to the system (endogenous), or from external shocks (exogenous).","The possibility of separating these two classes of events has critical implications for professionals in those fields.","We introduce an unsupervised framework leveraging a representation of jump time-series based on wavelet coefficients and apply it to stock price jumps.","In line with previous work, we recover the fact that the time-asymmetry of volatility is a major feature.","Mean-reversion and trend are found to be two additional key features, allowing us to identify new classes of jumps.","Furthermore, thanks to our wavelet-based representation, we investigate the reflexive properties of co-jumps, which occur when multiple stocks experience price jumps within the same minute.","We argue that a significant fraction of co-jumps results from an endogenous contagion mechanism."],"url":"http://arxiv.org/abs/2404.16467v1","category":"q-fin.GN"}
{"created":"2024-04-25 09:45:44","title":"Blockchain-enabled Energy Trading and Battery-based Sharing in Microgrids","abstract":"Carbon footprint reduction can be achieved through various methods, including the adoption of renewable energy sources. The installation of such sources, like photovoltaic panels, while environmentally beneficial, is cost-prohibitive for many. Those lacking photovoltaic solutions typically resort to purchasing energy from utility grids that often rely on fossil fuels. Moreover, when users produce their own energy, they may generate excess that goes unused, leading to inefficiencies. To address these challenges, this paper proposes innovative blockchain-enabled energy-sharing algorithms that allow consumers -- without financial means -- to access energy through the use of their own energy storage units. We explore two sharing models: a centralized method and a peer-to-peer (P2P) one. Our analysis reveals that the P2P model is more effective, enhancing the sharing process significantly compared to the centralized method. We also demonstrate that, when contrasted with traditional battery-supported trading algorithm, the P2P sharing algorithm substantially reduces wasted energy and energy purchases from the grid by 73.6%, and 12.3% respectively. The proposed system utilizes smart contracts to decentralize its structure, address the single point of failure concern, improve overall system transparency, and facilitate peer-to-peer payments.","sentences":["Carbon footprint reduction can be achieved through various methods, including the adoption of renewable energy sources.","The installation of such sources, like photovoltaic panels, while environmentally beneficial, is cost-prohibitive for many.","Those lacking photovoltaic solutions typically resort to purchasing energy from utility grids that often rely on fossil fuels.","Moreover, when users produce their own energy, they may generate excess that goes unused, leading to inefficiencies.","To address these challenges, this paper proposes innovative blockchain-enabled energy-sharing algorithms that allow consumers -- without financial means -- to access energy through the use of their own energy storage units.","We explore two sharing models: a centralized method and a peer-to-peer (P2P) one.","Our analysis reveals that the P2P model is more effective, enhancing the sharing process significantly compared to the centralized method.","We also demonstrate that, when contrasted with traditional battery-supported trading algorithm, the P2P sharing algorithm substantially reduces wasted energy and energy purchases from the grid by 73.6%, and 12.3% respectively.","The proposed system utilizes smart contracts to decentralize its structure, address the single point of failure concern, improve overall system transparency, and facilitate peer-to-peer payments."],"url":"http://arxiv.org/abs/2404.16462v1","category":"cs.DC"}
{"created":"2024-04-25 09:42:50","title":"Large Language Models Perform on Par with Experts Identifying Mental Health Factors in Adolescent Online Forums","abstract":"Mental health in children and adolescents has been steadily deteriorating over the past few years [ 1 ]. The recent advent of Large Language Models (LLMs) offers much hope for cost and time efficient scaling of monitoring and intervention, yet despite specifically prevalent issues such as school bullying and eating disorders, previous studies on have not investigated performance in this domain or for open information extraction where the set of answers is not predetermined. We create a new dataset of Reddit posts from adolescents aged 12-19 annotated by expert psychiatrists for the following categories: TRAUMA, PRECARITY, CONDITION, SYMPTOMS, SUICIDALITY and TREATMENT and compare expert labels to annotations from two top performing LLMs (GPT3.5 and GPT4). In addition, we create two synthetic datasets to assess whether LLMs perform better when annotating data as they generate it. We find GPT4 to be on par with human inter-annotator agreement and performance on synthetic data to be substantially higher, however we find the model still occasionally errs on issues of negation and factuality and higher performance on synthetic data is driven by greater complexity of real data rather than inherent advantage.","sentences":["Mental health in children and adolescents has been steadily deteriorating over the past few years [ 1 ].","The recent advent of Large Language Models (LLMs) offers much hope for cost and time efficient scaling of monitoring and intervention, yet despite specifically prevalent issues such as school bullying and eating disorders, previous studies on have not investigated performance in this domain or for open information extraction where the set of answers is not predetermined.","We create a new dataset of Reddit posts from adolescents aged 12-19 annotated by expert psychiatrists for the following categories: TRAUMA, PRECARITY, CONDITION, SYMPTOMS, SUICIDALITY and TREATMENT and compare expert labels to annotations from two top performing LLMs (GPT3.5 and GPT4).","In addition, we create two synthetic datasets to assess whether LLMs perform better when annotating data as they generate it.","We find GPT4 to be on par with human inter-annotator agreement and performance on synthetic data to be substantially higher, however we find the model still occasionally errs on issues of negation and factuality and higher performance on synthetic data is driven by greater complexity of real data rather than inherent advantage."],"url":"http://arxiv.org/abs/2404.16461v1","category":"cs.CL"}
{"created":"2024-04-25 09:41:18","title":"The tangent space in sub-Finsler geometry and applications","abstract":"In this paper, we study the tangent space to a sub-Finsler manifold in the measured Gromov-Hausdorff sense. We prove that the metric tangent is described by the nilpotent approximation, generalizing the sub-Riemannian result. Additionally, we study the blow-up of a measure on a sub-Finsler manifold. We identify the new notion of bounded measure which ensures that, in the limit, the blow-up is a scalar multiple of the Lebesgue measure. Our results have applications in the study of the Lott-Sturm-Villani curvature-dimension condition in sub-Finsler manifolds. In particular, we show the failure of the CD condition in equiregular sub-Finsler manifolds with growth vector (2,3), equipped with a bounded measure.","sentences":["In this paper, we study the tangent space to a sub-Finsler manifold in the measured Gromov-Hausdorff sense.","We prove that the metric tangent is described by the nilpotent approximation, generalizing the sub-Riemannian result.","Additionally, we study the blow-up of a measure on a sub-Finsler manifold.","We identify the new notion of bounded measure which ensures that, in the limit, the blow-up is a scalar multiple of the Lebesgue measure.","Our results have applications in the study of the Lott-Sturm-Villani curvature-dimension condition in sub-Finsler manifolds.","In particular, we show the failure of the CD condition in equiregular sub-Finsler manifolds with growth vector (2,3), equipped with a bounded measure."],"url":"http://arxiv.org/abs/2404.16460v1","category":"math.DG"}
{"created":"2024-04-25 09:37:44","title":"Towards Precise Observations of Neural Model Robustness in Classification","abstract":"In deep learning applications, robustness measures the ability of neural models that handle slight changes in input data, which could lead to potential safety hazards, especially in safety-critical applications. Pre-deployment assessment of model robustness is essential, but existing methods often suffer from either high costs or imprecise results. To enhance safety in real-world scenarios, metrics that effectively capture the model's robustness are needed. To address this issue, we compare the rigour and usage conditions of various assessment methods based on different definitions. Then, we propose a straightforward and practical metric utilizing hypothesis testing for probabilistic robustness and have integrated it into the TorchAttacks library. Through a comparative analysis of diverse robustness assessment methods, our approach contributes to a deeper understanding of model robustness in safety-critical applications.","sentences":["In deep learning applications, robustness measures the ability of neural models that handle slight changes in input data, which could lead to potential safety hazards, especially in safety-critical applications.","Pre-deployment assessment of model robustness is essential, but existing methods often suffer from either high costs or imprecise results.","To enhance safety in real-world scenarios, metrics that effectively capture the model's robustness are needed.","To address this issue, we compare the rigour and usage conditions of various assessment methods based on different definitions.","Then, we propose a straightforward and practical metric utilizing hypothesis testing for probabilistic robustness and have integrated it into the TorchAttacks library.","Through a comparative analysis of diverse robustness assessment methods, our approach contributes to a deeper understanding of model robustness in safety-critical applications."],"url":"http://arxiv.org/abs/2404.16457v1","category":"cs.SE"}
{"created":"2024-04-25 09:35:09","title":"Correlation-Decoupled Knowledge Distillation for Multimodal Sentiment Analysis with Incomplete Modalities","abstract":"Multimodal sentiment analysis (MSA) aims to understand human sentiment through multimodal data. Most MSA efforts are based on the assumption of modality completeness. However, in real-world applications, some practical factors cause uncertain modality missingness, which drastically degrades the model's performance. To this end, we propose a Correlation-decoupled Knowledge Distillation (CorrKD) framework for the MSA task under uncertain missing modalities. Specifically, we present a sample-level contrastive distillation mechanism that transfers comprehensive knowledge containing cross-sample correlations to reconstruct missing semantics. Moreover, a category-guided prototype distillation mechanism is introduced to capture cross-category correlations using category prototypes to align feature distributions and generate favorable joint representations. Eventually, we design a response-disentangled consistency distillation strategy to optimize the sentiment decision boundaries of the student network through response disentanglement and mutual information maximization. Comprehensive experiments on three datasets indicate that our framework can achieve favorable improvements compared with several baselines.","sentences":["Multimodal sentiment analysis (MSA) aims to understand human sentiment through multimodal data.","Most MSA efforts are based on the assumption of modality completeness.","However, in real-world applications, some practical factors cause uncertain modality missingness, which drastically degrades the model's performance.","To this end, we propose a Correlation-decoupled Knowledge Distillation (CorrKD) framework for the MSA task under uncertain missing modalities.","Specifically, we present a sample-level contrastive distillation mechanism that transfers comprehensive knowledge containing cross-sample correlations to reconstruct missing semantics.","Moreover, a category-guided prototype distillation mechanism is introduced to capture cross-category correlations using category prototypes to align feature distributions and generate favorable joint representations.","Eventually, we design a response-disentangled consistency distillation strategy to optimize the sentiment decision boundaries of the student network through response disentanglement and mutual information maximization.","Comprehensive experiments on three datasets indicate that our framework can achieve favorable improvements compared with several baselines."],"url":"http://arxiv.org/abs/2404.16456v1","category":"cs.CV"}
{"created":"2024-04-25 09:34:49","title":"Canonical Decision Diagrams Modulo Theories","abstract":"Decision diagrams (DDs) are powerful tools to represent effectively propositional formulas, which are largely used in many domains, in particular in formal verification and in knowledge compilation. Some forms of DDs (e.g., OBDDs, SDDs) are canonical, that is, (under given conditions on the atom list) they univocally represent equivalence classes of formulas. Given the limited expressiveness of propositional logic, a few attempts to leverage DDs to SMT level have been presented in the literature. Unfortunately, these techniques still suffer from some limitations: most procedures are theory-specific; some produce theory DDs (T-DDs) which do not univocally represent T-valid formulas or T-inconsistent formulas; none of these techniques provably produces theory-canonical T-DDs, which (under given conditions on the T-atom list) univocally represent T-equivalence classes of formulas. Also, these procedures are not easy to implement, and very few implementations are actually available. In this paper, we present a novel very-general technique to leverage DDs to SMT level, which has several advantages: it is very easy to implement on top of an AllSMT solver and a DD package, which are used as blackboxes; it works for every form of DDs and every theory, or combination thereof, supported by the AllSMT solver; it produces theory-canonical T-DDs if the propositional DD is canonical. We have implemented a prototype tool for both T-OBDDs and T-SDDs on top of OBDD and SDD packages and the MathSAT SMT solver. Some preliminary empirical evaluation supports the effectiveness of the approach.","sentences":["Decision diagrams (DDs) are powerful tools to represent effectively propositional formulas, which are largely used in many domains, in particular in formal verification and in knowledge compilation.","Some forms of DDs (e.g., OBDDs, SDDs) are canonical, that is, (under given conditions on the atom list) they univocally represent equivalence classes of formulas.","Given the limited expressiveness of propositional logic, a few attempts to leverage DDs to SMT level have been presented in the literature.","Unfortunately, these techniques still suffer from some limitations: most procedures are theory-specific; some produce theory DDs (T-DDs) which do not univocally represent T-valid formulas or T-inconsistent formulas; none of these techniques provably produces theory-canonical T-DDs, which (under given conditions on the T-atom list) univocally represent T-equivalence classes of formulas.","Also, these procedures are not easy to implement, and very few implementations are actually available.","In this paper, we present a novel very-general technique to leverage DDs to SMT level, which has several advantages: it is very easy to implement on top of an AllSMT solver and a DD package, which are used as blackboxes; it works for every form of DDs and every theory, or combination thereof, supported by the AllSMT solver; it produces theory-canonical T-DDs if the propositional DD is canonical.","We have implemented a prototype tool for both T-OBDDs and T-SDDs on top of OBDD and SDD packages and the MathSAT SMT solver.","Some preliminary empirical evaluation supports the effectiveness of the approach."],"url":"http://arxiv.org/abs/2404.16455v1","category":"cs.LO"}
{"created":"2024-04-25 09:32:34","title":"PAD: Patch-Agnostic Defense against Adversarial Patch Attacks","abstract":"Adversarial patch attacks present a significant threat to real-world object detectors due to their practical feasibility. Existing defense methods, which rely on attack data or prior knowledge, struggle to effectively address a wide range of adversarial patches. In this paper, we show two inherent characteristics of adversarial patches, semantic independence and spatial heterogeneity, independent of their appearance, shape, size, quantity, and location. Semantic independence indicates that adversarial patches operate autonomously within their semantic context, while spatial heterogeneity manifests as distinct image quality of the patch area that differs from original clean image due to the independent generation process. Based on these observations, we propose PAD, a novel adversarial patch localization and removal method that does not require prior knowledge or additional training. PAD offers patch-agnostic defense against various adversarial patches, compatible with any pre-trained object detectors. Our comprehensive digital and physical experiments involving diverse patch types, such as localized noise, printable, and naturalistic patches, exhibit notable improvements over state-of-the-art works. Our code is available at https://github.com/Lihua-Jing/PAD.","sentences":["Adversarial patch attacks present a significant threat to real-world object detectors due to their practical feasibility.","Existing defense methods, which rely on attack data or prior knowledge, struggle to effectively address a wide range of adversarial patches.","In this paper, we show two inherent characteristics of adversarial patches, semantic independence and spatial heterogeneity, independent of their appearance, shape, size, quantity, and location.","Semantic independence indicates that adversarial patches operate autonomously within their semantic context, while spatial heterogeneity manifests as distinct image quality of the patch area that differs from original clean image due to the independent generation process.","Based on these observations, we propose PAD, a novel adversarial patch localization and removal method that does not require prior knowledge or additional training.","PAD offers patch-agnostic defense against various adversarial patches, compatible with any pre-trained object detectors.","Our comprehensive digital and physical experiments involving diverse patch types, such as localized noise, printable, and naturalistic patches, exhibit notable improvements over state-of-the-art works.","Our code is available at https://github.com/Lihua-Jing/PAD."],"url":"http://arxiv.org/abs/2404.16452v1","category":"cs.CV"}
{"created":"2024-04-25 09:30:38","title":"Latent Modulated Function for Computational Optimal Continuous Image Representation","abstract":"The recent work Local Implicit Image Function (LIIF) and subsequent Implicit Neural Representation (INR) based works have achieved remarkable success in Arbitrary-Scale Super-Resolution (ASSR) by using MLP to decode Low-Resolution (LR) features. However, these continuous image representations typically implement decoding in High-Resolution (HR) High-Dimensional (HD) space, leading to a quadratic increase in computational cost and seriously hindering the practical applications of ASSR. To tackle this problem, we propose a novel Latent Modulated Function (LMF), which decouples the HR-HD decoding process into shared latent decoding in LR-HD space and independent rendering in HR Low-Dimensional (LD) space, thereby realizing the first computational optimal paradigm of continuous image representation. Specifically, LMF utilizes an HD MLP in latent space to generate latent modulations of each LR feature vector. This enables a modulated LD MLP in render space to quickly adapt to any input feature vector and perform rendering at arbitrary resolution. Furthermore, we leverage the positive correlation between modulation intensity and input image complexity to design a Controllable Multi-Scale Rendering (CMSR) algorithm, offering the flexibility to adjust the decoding efficiency based on the rendering precision. Extensive experiments demonstrate that converting existing INR-based ASSR methods to LMF can reduce the computational cost by up to 99.9%, accelerate inference by up to 57 times, and save up to 76% of parameters, while maintaining competitive performance. The code is available at https://github.com/HeZongyao/LMF.","sentences":["The recent work Local Implicit Image Function (LIIF) and subsequent Implicit Neural Representation (INR) based works have achieved remarkable success in Arbitrary-Scale Super-Resolution (ASSR) by using MLP to decode Low-Resolution (LR) features.","However, these continuous image representations typically implement decoding in High-Resolution (HR) High-Dimensional (HD) space, leading to a quadratic increase in computational cost and seriously hindering the practical applications of ASSR.","To tackle this problem, we propose a novel Latent Modulated Function (LMF), which decouples the HR-HD decoding process into shared latent decoding in LR-HD space and independent rendering in HR Low-Dimensional (LD) space, thereby realizing the first computational optimal paradigm of continuous image representation.","Specifically, LMF utilizes an HD MLP in latent space to generate latent modulations of each LR feature vector.","This enables a modulated LD MLP in render space to quickly adapt to any input feature vector and perform rendering at arbitrary resolution.","Furthermore, we leverage the positive correlation between modulation intensity and input image complexity to design a Controllable Multi-Scale Rendering (CMSR) algorithm, offering the flexibility to adjust the decoding efficiency based on the rendering precision.","Extensive experiments demonstrate that converting existing INR-based ASSR methods to LMF can reduce the computational cost by up to 99.9%, accelerate inference by up to 57 times, and save up to 76% of parameters, while maintaining competitive performance.","The code is available at https://github.com/HeZongyao/LMF."],"url":"http://arxiv.org/abs/2404.16451v1","category":"cs.CV"}
{"created":"2024-04-25 09:27:35","title":"On Software Ageing Indicators in OpenStack","abstract":"Distributed systems in general and cloud systems in particular, are susceptible to failures that can lead to substantial economic and data losses, security breaches, and even potential threats to human safety. Software ageing is an example of one such vulnerability. It emerges due to routine re-usage of computational systems units which induce fatigue within the components, resulting in an increased failure rate and potential system breakdown. Due to its stochastic nature, ageing cannot be directly measured, instead ageing indicators as proxies are used. While there are dozens of studies on different ageing indicators, their comprehensive comparison in different settings remains underexplored. In this paper, we compare two ageing indicators in OpenStack as a use case. Specifically, our evaluation compares memory usage (including swap memory) and request response time, as readily available indicators. By executing multiple OpenStack deployments with varying configurations, we conduct a series of experiments and analyze the ageing indicators. Comparative analysis through statistical tests provides valuable insights into the strengths and weaknesses of the utilised ageing indicators. Finally, through an in-depth analysis of other OpenStack failures, we identify underlying failure patterns and their impact on the studied ageing indicators.","sentences":["Distributed systems in general and cloud systems in particular, are susceptible to failures that can lead to substantial economic and data losses, security breaches, and even potential threats to human safety.","Software ageing is an example of one such vulnerability.","It emerges due to routine re-usage of computational systems units which induce fatigue within the components, resulting in an increased failure rate and potential system breakdown.","Due to its stochastic nature, ageing cannot be directly measured, instead ageing indicators as proxies are used.","While there are dozens of studies on different ageing indicators, their comprehensive comparison in different settings remains underexplored.","In this paper, we compare two ageing indicators in OpenStack as a use case.","Specifically, our evaluation compares memory usage (including swap memory) and request response time, as readily available indicators.","By executing multiple OpenStack deployments with varying configurations, we conduct a series of experiments and analyze the ageing indicators.","Comparative analysis through statistical tests provides valuable insights into the strengths and weaknesses of the utilised ageing indicators.","Finally, through an in-depth analysis of other OpenStack failures, we identify underlying failure patterns and their impact on the studied ageing indicators."],"url":"http://arxiv.org/abs/2404.16446v1","category":"cs.DC"}
{"created":"2024-04-25 09:20:51","title":"Contextual Categorization Enhancement through LLMs Latent-Space","abstract":"Managing the semantic quality of the categorization in large textual datasets, such as Wikipedia, presents significant challenges in terms of complexity and cost. In this paper, we propose leveraging transformer models to distill semantic information from texts in the Wikipedia dataset and its associated categories into a latent space. We then explore different approaches based on these encodings to assess and enhance the semantic identity of the categories. Our graphical approach is powered by Convex Hull, while we utilize Hierarchical Navigable Small Worlds (HNSWs) for the hierarchical approach. As a solution to the information loss caused by the dimensionality reduction, we modulate the following mathematical solution: an exponential decay function driven by the Euclidean distances between the high-dimensional encodings of the textual categories. This function represents a filter built around a contextual category and retrieves items with a certain Reconsideration Probability (RP). Retrieving high-RP items serves as a tool for database administrators to improve data groupings by providing recommendations and identifying outliers within a contextual framework.","sentences":["Managing the semantic quality of the categorization in large textual datasets, such as Wikipedia, presents significant challenges in terms of complexity and cost.","In this paper, we propose leveraging transformer models to distill semantic information from texts in the Wikipedia dataset and its associated categories into a latent space.","We then explore different approaches based on these encodings to assess and enhance the semantic identity of the categories.","Our graphical approach is powered by Convex Hull, while we utilize Hierarchical Navigable Small Worlds (HNSWs) for the hierarchical approach.","As a solution to the information loss caused by the dimensionality reduction, we modulate the following mathematical solution: an exponential decay function driven by the Euclidean distances between the high-dimensional encodings of the textual categories.","This function represents a filter built around a contextual category and retrieves items with a certain Reconsideration Probability (RP).","Retrieving high-RP items serves as a tool for database administrators to improve data groupings by providing recommendations and identifying outliers within a contextual framework."],"url":"http://arxiv.org/abs/2404.16442v1","category":"cs.CL"}
{"created":"2024-04-25 09:15:58","title":"Axial Gravitational Perturbations of Slowly-Rotating Compact Objects in General Relativity and Beyond","abstract":"We study the axial gravitational perturbations of slowly-rotating compact objects which are assumed to be supported by anisotropic fluids. We find that the gravitational perturbations decouple from the matter perturbations for axial sectors. We obtain a master wave equation whose potential is fully determined by the metric functions. This equation makes the calculations of gravitational QNMs for rotating compact objects extremely easy in specific background configurations.","sentences":["We study the axial gravitational perturbations of slowly-rotating compact objects which are assumed to be supported by anisotropic fluids.","We find that the gravitational perturbations decouple from the matter perturbations for axial sectors.","We obtain a master wave equation whose potential is fully determined by the metric functions.","This equation makes the calculations of gravitational QNMs for rotating compact objects extremely easy in specific background configurations."],"url":"http://arxiv.org/abs/2404.16437v1","category":"gr-qc"}
{"created":"2024-04-25 09:12:35","title":"Leveraging tropical reef, bird and unrelated sounds for superior transfer learning in marine bioacoustics","abstract":"Machine learning has the potential to revolutionize passive acoustic monitoring (PAM) for ecological assessments. However, high annotation and compute costs limit the field's efficacy. Generalizable pretrained networks can overcome these costs, but high-quality pretraining requires vast annotated libraries, limiting its current applicability primarily to bird taxa. Here, we identify the optimum pretraining strategy for a data-deficient domain using coral reef bioacoustics. We assemble ReefSet, a large annotated library of reef sounds, though modest compared to bird libraries at 2% of the sample count. Through testing few-shot transfer learning performance, we observe that pretraining on bird audio provides notably superior generalizability compared to pretraining on ReefSet or unrelated audio alone. However, our key findings show that cross-domain mixing which leverages bird, reef and unrelated audio during pretraining maximizes reef generalizability. SurfPerch, our pretrained network, provides a strong foundation for automated analysis of marine PAM data with minimal annotation and compute costs.","sentences":["Machine learning has the potential to revolutionize passive acoustic monitoring (PAM) for ecological assessments.","However, high annotation and compute costs limit the field's efficacy.","Generalizable pretrained networks can overcome these costs, but high-quality pretraining requires vast annotated libraries, limiting its current applicability primarily to bird taxa.","Here, we identify the optimum pretraining strategy for a data-deficient domain using coral reef bioacoustics.","We assemble ReefSet, a large annotated library of reef sounds, though modest compared to bird libraries at 2% of the sample count.","Through testing few-shot transfer learning performance, we observe that pretraining on bird audio provides notably superior generalizability compared to pretraining on ReefSet or unrelated audio alone.","However, our key findings show that cross-domain mixing which leverages bird, reef and unrelated audio during pretraining maximizes reef generalizability.","SurfPerch, our pretrained network, provides a strong foundation for automated analysis of marine PAM data with minimal annotation and compute costs."],"url":"http://arxiv.org/abs/2404.16436v1","category":"cs.SD"}
{"created":"2024-04-25 09:09:23","title":"Exact solutions to macroscopic fluctuation theory through classical integrable systems","abstract":"We give a short overview of recent developments in exact solutions for macroscopic fluctuation theory by using connections to classical integrable systems. A calculation of the cumulant generating function for a tagged particle is also given, agreeing with a previous result obtained from a microscopic analysis.","sentences":["We give a short overview of recent developments in exact solutions for macroscopic fluctuation theory by using connections to classical integrable systems.","A calculation of the cumulant generating function for a tagged particle is also given, agreeing with a previous result obtained from a microscopic analysis."],"url":"http://arxiv.org/abs/2404.16434v1","category":"cond-mat.stat-mech"}
{"created":"2024-04-25 09:05:28","title":"FO logic on cellular automata orbits equals MSO logic","abstract":"We introduce an extension of classical cellular automata (CA) to arbitrary labeled graphs, and show that FO logic on CA orbits is equivalent to MSO logic. We deduce various results from that equivalence, including a characterization of finitely generated groups on which FO model checking for CA orbits is undecidable, and undecidability of satisfiability of a fixed FO property for CA over finite graphs. We also show concrete examples of FO formulas for CA orbits whose model checking problem is equivalent to the domino problem, or its seeded or recurring variants respectively, on any finitely generated group. For the recurring domino problem, we use an extension of the FO signature by a relation found in the well-known Garden of Eden theorem, but we also show a concrete FO formula without the extension and with one quantifier alternation whose model checking problem does not belong to the arithmetical hierarchy on group Z^2.","sentences":["We introduce an extension of classical cellular automata (CA) to arbitrary labeled graphs, and show that FO logic on CA orbits is equivalent to MSO logic.","We deduce various results from that equivalence, including a characterization of finitely generated groups on which FO model checking for CA orbits is undecidable, and undecidability of satisfiability of a fixed FO property for CA over finite graphs.","We also show concrete examples of FO formulas for CA orbits whose model checking problem is equivalent to the domino problem, or its seeded or recurring variants respectively, on any finitely generated group.","For the recurring domino problem, we use an extension of the FO signature by a relation found in the well-known Garden of Eden theorem, but we also show a concrete FO formula without the extension and with one quantifier alternation whose model checking problem does not belong to the arithmetical hierarchy on group Z^2."],"url":"http://arxiv.org/abs/2404.16430v1","category":"cs.DM"}
{"created":"2024-04-25 08:58:57","title":"Costless correction of chain based nested sampling parameter estimation in gravitational wave data and beyond","abstract":"Nested sampling parameter estimation differs from evidence estimation, in that it incurs an additional source of error. This error affects estimates of parameter means and credible intervals in gravitational wave analyses and beyond, and yet, it is typically not accounted for in standard error estimation methods. In this paper, we present two novel methods to quantify this error more accurately for any chain based nested sampler, using the additional likelihood calls made at runtime in producing independent samples. Using injected signals of black hole binary coalescences as an example, we first show concretely that the usual error estimation method is insufficient to capture the true error bar on parameter estimates. We then demonstrate how the extra points in the chains of chain based samplers may be carefully utilised to estimate this error correctly, and provide a way to check the accuracy of the resulting error bars. Finally, we discuss how this error affects $p$-$p$ plots and coverage assessments.","sentences":["Nested sampling parameter estimation differs from evidence estimation, in that it incurs an additional source of error.","This error affects estimates of parameter means and credible intervals in gravitational wave analyses and beyond, and yet, it is typically not accounted for in standard error estimation methods.","In this paper, we present two novel methods to quantify this error more accurately for any chain based nested sampler, using the additional likelihood calls made at runtime in producing independent samples.","Using injected signals of black hole binary coalescences as an example, we first show concretely that the usual error estimation method is insufficient to capture the true error bar on parameter estimates.","We then demonstrate how the extra points in the chains of chain based samplers may be carefully utilised to estimate this error correctly, and provide a way to check the accuracy of the resulting error bars.","Finally, we discuss how this error affects $p$-$p$ plots and coverage assessments."],"url":"http://arxiv.org/abs/2404.16428v1","category":"astro-ph.IM"}
{"created":"2024-04-25 08:55:11","title":"Reciprocity in laser ultrasound revisited: Is wavefield characterisation by scanning laser excitation strictly reciprocal to that by scanning laser detection?","abstract":"The common believe about strict measurement reciprocity between scanning laser detection and scanning laser excitation is disproved by a simple experiment. Nevertheless, a deeper study based on the reciprocity relation reveals correct reciprocal measurement set-ups for both the probe-excitation / laser-detection and the laser-excitation / probe-detection case. Similarly, the all-laser measurement, that is thermoelastic laser excitation with laser vibrometer detection, is not in general reciprocal with respect to the exchange of excitation and detection positions. Again, a substitute for the laser doppler vibrometer out-of-plane displacement measurement was found which ensures measurement reciprocity together with laser excitation. The apparent confusion in literature about strict validity/non-validity of measurement reciprocity is mitigated by classifying the measurement situations systematically.","sentences":["The common believe about strict measurement reciprocity between scanning laser detection and scanning laser excitation is disproved by a simple experiment.","Nevertheless, a deeper study based on the reciprocity relation reveals correct reciprocal measurement set-ups for both the probe-excitation / laser-detection and the laser-excitation / probe-detection case.","Similarly, the all-laser measurement, that is thermoelastic laser excitation with laser vibrometer detection, is not in general reciprocal with respect to the exchange of excitation and detection positions.","Again, a substitute for the laser doppler vibrometer out-of-plane displacement measurement was found which ensures measurement reciprocity together with laser excitation.","The apparent confusion in literature about strict validity/non-validity of measurement reciprocity is mitigated by classifying the measurement situations systematically."],"url":"http://arxiv.org/abs/2404.16424v1","category":"physics.ins-det"}
{"created":"2024-04-25 08:53:23","title":"Neural Assembler: Learning to Generate Fine-Grained Robotic Assembly Instructions from Multi-View Images","abstract":"Image-guided object assembly represents a burgeoning research topic in computer vision. This paper introduces a novel task: translating multi-view images of a structural 3D model (for example, one constructed with building blocks drawn from a 3D-object library) into a detailed sequence of assembly instructions executable by a robotic arm. Fed with multi-view images of the target 3D model for replication, the model designed for this task must address several sub-tasks, including recognizing individual components used in constructing the 3D model, estimating the geometric pose of each component, and deducing a feasible assembly order adhering to physical rules. Establishing accurate 2D-3D correspondence between multi-view images and 3D objects is technically challenging. To tackle this, we propose an end-to-end model known as the Neural Assembler. This model learns an object graph where each vertex represents recognized components from the images, and the edges specify the topology of the 3D model, enabling the derivation of an assembly plan. We establish benchmarks for this task and conduct comprehensive empirical evaluations of Neural Assembler and alternative solutions. Our experiments clearly demonstrate the superiority of Neural Assembler.","sentences":["Image-guided object assembly represents a burgeoning research topic in computer vision.","This paper introduces a novel task: translating multi-view images of a structural 3D model (for example, one constructed with building blocks drawn from a 3D-object library) into a detailed sequence of assembly instructions executable by a robotic arm.","Fed with multi-view images of the target 3D model for replication, the model designed for this task must address several sub-tasks, including recognizing individual components used in constructing the 3D model, estimating the geometric pose of each component, and deducing a feasible assembly order adhering to physical rules.","Establishing accurate 2D-3D correspondence between multi-view images and 3D objects is technically challenging.","To tackle this, we propose an end-to-end model known as the Neural Assembler.","This model learns an object graph where each vertex represents recognized components from the images, and the edges specify the topology of the 3D model, enabling the derivation of an assembly plan.","We establish benchmarks for this task and conduct comprehensive empirical evaluations of Neural Assembler and alternative solutions.","Our experiments clearly demonstrate the superiority of Neural Assembler."],"url":"http://arxiv.org/abs/2404.16423v1","category":"cs.CV"}
{"created":"2024-04-25 08:51:59","title":"SynCellFactory: Generative Data Augmentation for Cell Tracking","abstract":"Cell tracking remains a pivotal yet challenging task in biomedical research. The full potential of deep learning for this purpose is often untapped due to the limited availability of comprehensive and varied training data sets. In this paper, we present SynCellFactory, a generative cell video augmentation. At the heart of SynCellFactory lies the ControlNet architecture, which has been fine-tuned to synthesize cell imagery with photorealistic accuracy in style and motion patterns. This technique enables the creation of synthetic yet realistic cell videos that mirror the complexity of authentic microscopy time-lapses. Our experiments demonstrate that SynCellFactory boosts the performance of well-established deep learning models for cell tracking, particularly when original training data is sparse.","sentences":["Cell tracking remains a pivotal yet challenging task in biomedical research.","The full potential of deep learning for this purpose is often untapped due to the limited availability of comprehensive and varied training data sets.","In this paper, we present SynCellFactory, a generative cell video augmentation.","At the heart of SynCellFactory lies the ControlNet architecture, which has been fine-tuned to synthesize cell imagery with photorealistic accuracy in style and motion patterns.","This technique enables the creation of synthetic yet realistic cell videos that mirror the complexity of authentic microscopy time-lapses.","Our experiments demonstrate that SynCellFactory boosts the performance of well-established deep learning models for cell tracking, particularly when original training data is sparse."],"url":"http://arxiv.org/abs/2404.16421v1","category":"cs.CV"}
{"created":"2024-04-25 08:49:47","title":"Instruction Matters, a Simple yet Effective Task Selection Approach in Instruction Tuning for Specific Tasks","abstract":"Instruction tuning has shown its ability to not only enhance zero-shot generalization across various tasks but also its effectiveness in improving the performance of specific tasks. A crucial aspect in instruction tuning for a particular task is a strategic selection of related tasks that offer meaningful supervision, thereby enhancing efficiency and preventing performance degradation from irrelevant tasks. Our research reveals that leveraging instruction information \\textit{alone} enables the identification of pertinent tasks for instruction tuning. This approach is notably simpler compared to traditional methods that necessitate complex measurements of pairwise transferability between tasks or the creation of data samples for the target task. Furthermore, by additionally learning the unique instructional template style of the meta-dataset, we observe an improvement in task selection accuracy, which contributes to enhanced overall performance. Experimental results demonstrate that training on a small set of tasks, chosen solely based on the instructions, leads to substantial performance improvements on benchmarks like P3, Big-Bench, NIV2, and Big-Bench Hard. Significantly, these improvements exceed those achieved by prior task selection methods, highlighting the efficacy of our approach.","sentences":["Instruction tuning has shown its ability to not only enhance zero-shot generalization across various tasks but also its effectiveness in improving the performance of specific tasks.","A crucial aspect in instruction tuning for a particular task is a strategic selection of related tasks that offer meaningful supervision, thereby enhancing efficiency and preventing performance degradation from irrelevant tasks.","Our research reveals that leveraging instruction information \\textit{alone} enables the identification of pertinent tasks for instruction tuning.","This approach is notably simpler compared to traditional methods that necessitate complex measurements of pairwise transferability between tasks or the creation of data samples for the target task.","Furthermore, by additionally learning the unique instructional template style of the meta-dataset, we observe an improvement in task selection accuracy, which contributes to enhanced overall performance.","Experimental results demonstrate that training on a small set of tasks, chosen solely based on the instructions, leads to substantial performance improvements on benchmarks like P3, Big-Bench, NIV2, and Big-Bench Hard.","Significantly, these improvements exceed those achieved by prior task selection methods, highlighting the efficacy of our approach."],"url":"http://arxiv.org/abs/2404.16418v1","category":"cs.CL"}
{"created":"2024-04-25 08:49:29","title":"Constructing Optimal Noise Channels for Enhanced Robustness in Quantum Machine Learning","abstract":"With the rapid advancement of Quantum Machine Learning (QML), the critical need to enhance security measures against adversarial attacks and protect QML models becomes increasingly evident. In this work, we outline the connection between quantum noise channels and differential privacy (DP), by constructing a family of noise channels which are inherently $\\epsilon$-DP: $(\\alpha, \\gamma)$-channels. Through this approach, we successfully replicate the $\\epsilon$-DP bounds observed for depolarizing and random rotation channels, thereby affirming the broad generality of our framework. Additionally, we use a semi-definite program to construct an optimally robust channel. In a small-scale experimental evaluation, we demonstrate the benefits of using our optimal noise channel over depolarizing noise, particularly in enhancing adversarial accuracy. Moreover, we assess how the variables $\\alpha$ and $\\gamma$ affect the certifiable robustness and investigate how different encoding methods impact the classifier's robustness.","sentences":["With the rapid advancement of Quantum Machine Learning (QML), the critical need to enhance security measures against adversarial attacks and protect QML models becomes increasingly evident.","In this work, we outline the connection between quantum noise channels and differential privacy (DP), by constructing a family of noise channels which are inherently $\\epsilon$-DP: $(\\alpha, \\gamma)$-channels.","Through this approach, we successfully replicate the $\\epsilon$-DP bounds observed for depolarizing and random rotation channels, thereby affirming the broad generality of our framework.","Additionally, we use a semi-definite program to construct an optimally robust channel.","In a small-scale experimental evaluation, we demonstrate the benefits of using our optimal noise channel over depolarizing noise, particularly in enhancing adversarial accuracy.","Moreover, we assess how the variables $\\alpha$ and $\\gamma$ affect the certifiable robustness and investigate how different encoding methods impact the classifier's robustness."],"url":"http://arxiv.org/abs/2404.16417v1","category":"quant-ph"}
{"created":"2024-04-25 08:43:06","title":"Asking and Answering Questions to Extract Event-Argument Structures","abstract":"This paper presents a question-answering approach to extract document-level event-argument structures. We automatically ask and answer questions for each argument type an event may have. Questions are generated using manually defined templates and generative transformers. Template-based questions are generated using predefined role-specific wh-words and event triggers from the context document. Transformer-based questions are generated using large language models trained to formulate questions based on a passage and the expected answer. Additionally, we develop novel data augmentation strategies specialized in inter-sentential event-argument relations. We use a simple span-swapping technique, coreference resolution, and large language models to augment the training instances. Our approach enables transfer learning without any corpora-specific modifications and yields competitive results with the RAMS dataset. It outperforms previous work, and it is especially beneficial to extract arguments that appear in different sentences than the event trigger. We also present detailed quantitative and qualitative analyses shedding light on the most common errors made by our best model.","sentences":["This paper presents a question-answering approach to extract document-level event-argument structures.","We automatically ask and answer questions for each argument type an event may have.","Questions are generated using manually defined templates and generative transformers.","Template-based questions are generated using predefined role-specific wh-words and event triggers from the context document.","Transformer-based questions are generated using large language models trained to formulate questions based on a passage and the expected answer.","Additionally, we develop novel data augmentation strategies specialized in inter-sentential event-argument relations.","We use a simple span-swapping technique, coreference resolution, and large language models to augment the training instances.","Our approach enables transfer learning without any corpora-specific modifications and yields competitive results with the RAMS dataset.","It outperforms previous work, and it is especially beneficial to extract arguments that appear in different sentences than the event trigger.","We also present detailed quantitative and qualitative analyses shedding light on the most common errors made by our best model."],"url":"http://arxiv.org/abs/2404.16413v1","category":"cs.CL"}
{"created":"2024-04-25 08:39:10","title":"Label-Free Topic-Focused Summarization Using Query Augmentation","abstract":"In today's data and information-rich world, summarization techniques are essential in harnessing vast text to extract key information and enhance decision-making and efficiency. In particular, topic-focused summarization is important due to its ability to tailor content to specific aspects of an extended text. However, this usually requires extensive labelled datasets and considerable computational power. This study introduces a novel method, Augmented-Query Summarization (AQS), for topic-focused summarization without the need for extensive labelled datasets, leveraging query augmentation and hierarchical clustering. This approach facilitates the transferability of machine learning models to the task of summarization, circumventing the need for topic-specific training. Through real-world tests, our method demonstrates the ability to generate relevant and accurate summaries, showing its potential as a cost-effective solution in data-rich environments. This innovation paves the way for broader application and accessibility in the field of topic-focused summarization technology, offering a scalable, efficient method for personalized content extraction.","sentences":["In today's data and information-rich world, summarization techniques are essential in harnessing vast text to extract key information and enhance decision-making and efficiency.","In particular, topic-focused summarization is important due to its ability to tailor content to specific aspects of an extended text.","However, this usually requires extensive labelled datasets and considerable computational power.","This study introduces a novel method, Augmented-Query Summarization (AQS), for topic-focused summarization without the need for extensive labelled datasets, leveraging query augmentation and hierarchical clustering.","This approach facilitates the transferability of machine learning models to the task of summarization, circumventing the need for topic-specific training.","Through real-world tests, our method demonstrates the ability to generate relevant and accurate summaries, showing its potential as a cost-effective solution in data-rich environments.","This innovation paves the way for broader application and accessibility in the field of topic-focused summarization technology, offering a scalable, efficient method for personalized content extraction."],"url":"http://arxiv.org/abs/2404.16411v1","category":"cs.AI"}
{"created":"2024-04-25 08:36:09","title":"Cross-sensor super-resolution of irregularly sampled Sentinel-2 time series","abstract":"Satellite imaging generally presents a trade-off between the frequency of acquisitions and the spatial resolution of the images. Super-resolution is often advanced as a way to get the best of both worlds. In this work, we investigate multi-image super-resolution of satellite image time series, i.e. how multiple images of the same area acquired at different dates can help reconstruct a higher resolution observation. In particular, we extend state-of-the-art deep single and multi-image super-resolution algorithms, such as SRDiff and HighRes-net, to deal with irregularly sampled Sentinel-2 time series. We introduce BreizhSR, a new dataset for 4x super-resolution of Sentinel-2 time series using very high-resolution SPOT-6 imagery of Brittany, a French region. We show that using multiple images significantly improves super-resolution performance, and that a well-designed temporal positional encoding allows us to perform super-resolution for different times of the series. In addition, we observe a trade-off between spectral fidelity and perceptual quality of the reconstructed HR images, questioning future directions for super-resolution of Earth Observation data.","sentences":["Satellite imaging generally presents a trade-off between the frequency of acquisitions and the spatial resolution of the images.","Super-resolution is often advanced as a way to get the best of both worlds.","In this work, we investigate multi-image super-resolution of satellite image time series, i.e. how multiple images of the same area acquired at different dates can help reconstruct a higher resolution observation.","In particular, we extend state-of-the-art deep single and multi-image super-resolution algorithms, such as SRDiff and HighRes-net, to deal with irregularly sampled Sentinel-2 time series.","We introduce BreizhSR, a new dataset for 4x super-resolution of Sentinel-2 time series using very high-resolution SPOT-6 imagery of Brittany, a French region.","We show that using multiple images significantly improves super-resolution performance, and that a well-designed temporal positional encoding allows us to perform super-resolution for different times of the series.","In addition, we observe a trade-off between spectral fidelity and perceptual quality of the reconstructed HR images, questioning future directions for super-resolution of Earth Observation data."],"url":"http://arxiv.org/abs/2404.16409v1","category":"cs.CV"}
{"created":"2024-04-25 08:35:24","title":"Event-Triggered Resilient Filtering for 2-D Systems with Asynchronous-Delay: Handling Binary Encoding Decoding with Probabilistic Bit Flips","abstract":"In this paper, the event-triggered resilient filtering problem is investigated for a class of two-dimensional systems with asynchronous-delay under binary encoding-decoding schemes with probabilistic bit flips. To reduce unnecessary communications and computations in complex network systems, alleviate network energy consumption, and optimize the use of network resources, a new event-triggered mechanism is proposed, which focuses on broadcasting necessary measurement information to update innovation only when the event generator function is satisfied. A binary encoding-decoding scheme is used in the communication process to quantify the measurement information into a bit stream, transmit it through a memoryless binary symmetric channel with a certain probability of bit flipping, and restore it at the receiver. In order to utilize the delayed decoded measurement information that a measurement reconstruction approach is proposed. Through generating space equivalence verification, it is found that the reconstructed delay-free decoded measurement sequence contains the same information as the original delayed decoded measurement sequence. In addition, resilient filter is utilized to accommodate possible estimation gain perturbations. Then, a recursive estimator framework is presented based on the reconstructed decoded measurement sequence. By means of the mathematical induction technique, the unbiased property of the proposed estimator is proved. The estimation gain is obtained by minimizing an upper bound on the filtering error covariance. Subsequently, through rigorous mathematical analysis, the monotonicity of filtering performance with respect to triggering parameters is discussed.","sentences":["In this paper, the event-triggered resilient filtering problem is investigated for a class of two-dimensional systems with asynchronous-delay under binary encoding-decoding schemes with probabilistic bit flips.","To reduce unnecessary communications and computations in complex network systems, alleviate network energy consumption, and optimize the use of network resources, a new event-triggered mechanism is proposed, which focuses on broadcasting necessary measurement information to update innovation only when the event generator function is satisfied.","A binary encoding-decoding scheme is used in the communication process to quantify the measurement information into a bit stream, transmit it through a memoryless binary symmetric channel with a certain probability of bit flipping, and restore it at the receiver.","In order to utilize the delayed decoded measurement information that a measurement reconstruction approach is proposed.","Through generating space equivalence verification, it is found that the reconstructed delay-free decoded measurement sequence contains the same information as the original delayed decoded measurement sequence.","In addition, resilient filter is utilized to accommodate possible estimation gain perturbations.","Then, a recursive estimator framework is presented based on the reconstructed decoded measurement sequence.","By means of the mathematical induction technique, the unbiased property of the proposed estimator is proved.","The estimation gain is obtained by minimizing an upper bound on the filtering error covariance.","Subsequently, through rigorous mathematical analysis, the monotonicity of filtering performance with respect to triggering parameters is discussed."],"url":"http://arxiv.org/abs/2404.16408v1","category":"cs.IT"}
{"created":"2024-04-25 08:34:21","title":"U2++ MoE: Scaling 4.7x parameters with minimal impact on RTF","abstract":"Scale has opened new frontiers in natural language processing, but at a high cost. In response, by learning to only activate a subset of parameters in training and inference, Mixture-of-Experts (MoE) have been proposed as an energy efficient path to even larger and more capable language models and this shift towards a new generation of foundation models is gaining momentum, particularly within the field of Automatic Speech Recognition (ASR). Recent works that incorporating MoE into ASR models have complex designs such as routing frames via supplementary embedding network, improving multilingual ability for the experts, and utilizing dedicated auxiliary losses for either expert load balancing or specific language handling. We found that delicate designs are not necessary, while an embarrassingly simple substitution of MoE layers for all Feed-Forward Network (FFN) layers is competent for the ASR task. To be more specific, we benchmark our proposed model on a large scale inner-source dataset (160k hours), the results show that we can scale our baseline Conformer (Dense-225M) to its MoE counterparts (MoE-1B) and achieve Dense-1B level Word Error Rate (WER) while maintaining a Dense-225M level Real Time Factor (RTF). Furthermore, by applying Unified 2-pass framework with bidirectional attention decoders (U2++), we achieve the streaming and non-streaming decoding modes in a single MoE based model, which we call U2++ MoE. We hope that our study can facilitate the research on scaling speech foundation models without sacrificing deployment efficiency.","sentences":["Scale has opened new frontiers in natural language processing, but at a high cost.","In response, by learning to only activate a subset of parameters in training and inference, Mixture-of-Experts (MoE) have been proposed as an energy efficient path to even larger and more capable language models and this shift towards a new generation of foundation models is gaining momentum, particularly within the field of Automatic Speech Recognition (ASR).","Recent works that incorporating MoE into ASR models have complex designs such as routing frames via supplementary embedding network, improving multilingual ability for the experts, and utilizing dedicated auxiliary losses for either expert load balancing or specific language handling.","We found that delicate designs are not necessary, while an embarrassingly simple substitution of MoE layers for all Feed-Forward Network (FFN) layers is competent for the ASR task.","To be more specific, we benchmark our proposed model on a large scale inner-source dataset (160k hours), the results show that we can scale our baseline Conformer (Dense-225M) to its MoE counterparts (MoE-1B) and achieve Dense-1B level Word Error Rate (WER) while maintaining a Dense-225M level Real Time Factor (RTF).","Furthermore, by applying Unified 2-pass framework with bidirectional attention decoders (U2++), we achieve the streaming and non-streaming decoding modes in a single MoE based model, which we call U2++ MoE. We hope that our study can facilitate the research on scaling speech foundation models without sacrificing deployment efficiency."],"url":"http://arxiv.org/abs/2404.16407v1","category":"cs.CL"}
{"created":"2024-04-25 08:33:32","title":"Regular Typed Unification","abstract":"Here we define a new unification algorithm for terms interpreted in semantic domains denoted by a subclass of regular types here called deterministic regular types. This reflects our intention not to handle the semantic universe as a homogeneous collection of values, but instead, to partition it in a way that is similar to data types in programming languages. We first define the new unification algorithm which is based on constraint generation and constraint solving, and then prove its main properties: termination, soundness, and completeness with respect to the semantics. Finally, we discuss how to apply this algorithm to a dynamically typed version of Prolog.","sentences":["Here we define a new unification algorithm for terms interpreted in semantic domains denoted by a subclass of regular types here called deterministic regular types.","This reflects our intention not to handle the semantic universe as a homogeneous collection of values, but instead, to partition it in a way that is similar to data types in programming languages.","We first define the new unification algorithm which is based on constraint generation and constraint solving, and then prove its main properties: termination, soundness, and completeness with respect to the semantics.","Finally, we discuss how to apply this algorithm to a dynamically typed version of Prolog."],"url":"http://arxiv.org/abs/2404.16406v1","category":"cs.LO"}
{"created":"2024-04-25 08:31:40","title":"Counting $U(N)^{\\otimes r}\\otimes O(N)^{\\otimes q}$ invariants and tensor model observables","abstract":"$U(N)^{\\otimes r} \\otimes O(N)^{\\otimes q}$ invariants are constructed by contractions of complex tensors of order $r+q$, also denoted $(r,q)$. These tensors transform under $r$ fundamental representations of the unitary group $U(N)$ and $q$ fundamental representations of the orthogonal group $O(N)$. Therefore, $U(N)^{\\otimes r} \\otimes O(N)^{\\otimes q}$ invariants are tensor model observables endowed with a tensor field of order $(r,q)$. We enumerate these observables using group theoretic formulae, for arbitrary tensor fields of order $(r,q)$. Inspecting lower-order cases reveals that, at order $(1,1)$, the number of invariants corresponds to a number of 2- or 4-ary necklaces that exhibit pattern avoidance, offering insights into enumerative combinatorics. For a general order $(r,q)$, the counting can be interpreted as the partition function of a topological quantum field theory (TQFT) with the symmetric group serving as gauge group. We identify the 2-complex pertaining to the enumeration of the invariants, which in turn defines the TQFT, and establish a correspondence with countings associated with covers of diverse topologies. For $r>1$, the number of invariants matches the number of ($q$-dependent) weighted equivalence classes of branched covers of the 2-sphere with $r$ branched points. At $r=1$, the counting maps to the enumeration of branched covers of the 2-sphere with $q+3$ branched points. The formalism unveils a wide array of novel integer sequences that have not been previously documented. We also provide various codes for running computational experiments.","sentences":["$U(N)^{\\otimes r} \\otimes O(N)^{\\otimes q}$ invariants are constructed by contractions of complex tensors of order $r+q$, also denoted $(r,q)$. These tensors transform under $r$ fundamental representations of the unitary group $U(N)$ and $q$ fundamental representations of the orthogonal group $O(N)$.","Therefore, $U(N)^{\\otimes r} \\otimes O(N)^{\\otimes q}$ invariants are tensor model observables endowed with a tensor field of order $(r,q)$.","We enumerate these observables using group theoretic formulae, for arbitrary tensor fields of order $(r,q)$. Inspecting lower-order cases reveals that, at order $(1,1)$, the number of invariants corresponds to a number of 2- or 4-ary necklaces that exhibit pattern avoidance, offering insights into enumerative combinatorics.","For a general order $(r,q)$, the counting can be interpreted as the partition function of a topological quantum field theory (TQFT) with the symmetric group serving as gauge group.","We identify the 2-complex pertaining to the enumeration of the invariants, which in turn defines the TQFT, and establish a correspondence with countings associated with covers of diverse topologies.","For $r>1$, the number of invariants matches the number of ($q$-dependent) weighted equivalence classes of branched covers of the 2-sphere with $r$ branched points.","At $r=1$, the counting maps to the enumeration of branched covers of the 2-sphere with $q+3$ branched points.","The formalism unveils a wide array of novel integer sequences that have not been previously documented.","We also provide various codes for running computational experiments."],"url":"http://arxiv.org/abs/2404.16404v1","category":"hep-th"}
{"created":"2024-04-25 08:22:47","title":"Offline Reinforcement Learning with Behavioral Supervisor Tuning","abstract":"Offline reinforcement learning (RL) algorithms are applied to learn performant, well-generalizing policies when provided with a static dataset of interactions. Many recent approaches to offline RL have seen substantial success, but with one key caveat: they demand substantial per-dataset hyperparameter tuning to achieve reported performance, which requires policy rollouts in the environment to evaluate; this can rapidly become cumbersome. Furthermore, substantial tuning requirements can hamper the adoption of these algorithms in practical domains. In this paper, we present TD3 with Behavioral Supervisor Tuning (TD3-BST), an algorithm that trains an uncertainty model and uses it to guide the policy to select actions within the dataset support. TD3-BST can learn more effective policies from offline datasets compared to previous methods and achieves the best performance across challenging benchmarks without requiring per-dataset tuning.","sentences":["Offline reinforcement learning (RL) algorithms are applied to learn performant, well-generalizing policies when provided with a static dataset of interactions.","Many recent approaches to offline RL have seen substantial success, but with one key caveat: they demand substantial per-dataset hyperparameter tuning to achieve reported performance, which requires policy rollouts in the environment to evaluate; this can rapidly become cumbersome.","Furthermore, substantial tuning requirements can hamper the adoption of these algorithms in practical domains.","In this paper, we present TD3 with Behavioral Supervisor Tuning (TD3-BST), an algorithm that trains an uncertainty model and uses it to guide the policy to select actions within the dataset support.","TD3-BST can learn more effective policies from offline datasets compared to previous methods and achieves the best performance across challenging benchmarks without requiring per-dataset tuning."],"url":"http://arxiv.org/abs/2404.16399v1","category":"cs.LG"}
{"created":"2024-04-25 08:15:37","title":"Deep Learning-based Prediction of Breast Cancer Tumor and Immune Phenotypes from Histopathology","abstract":"The interactions between tumor cells and the tumor microenvironment (TME) dictate therapeutic efficacy of radiation and many systemic therapies in breast cancer. However, to date, there is not a widely available method to reproducibly measure tumor and immune phenotypes for each patient's tumor. Given this unmet clinical need, we applied multiple instance learning (MIL) algorithms to assess activity of ten biologically relevant pathways from the hematoxylin and eosin (H&E) slide of primary breast tumors. We employed different feature extraction approaches and state-of-the-art model architectures. Using binary classification, our models attained area under the receiver operating characteristic (AUROC) scores above 0.70 for nearly all gene expression pathways and on some cases, exceeded 0.80. Attention maps suggest that our trained models recognize biologically relevant spatial patterns of cell sub-populations from H&E. These efforts represent a first step towards developing computational H&E biomarkers that reflect facets of the TME and hold promise for augmenting precision oncology.","sentences":["The interactions between tumor cells and the tumor microenvironment (TME) dictate therapeutic efficacy of radiation and many systemic therapies in breast cancer.","However, to date, there is not a widely available method to reproducibly measure tumor and immune phenotypes for each patient's tumor.","Given this unmet clinical need, we applied multiple instance learning (MIL) algorithms to assess activity of ten biologically relevant pathways from the hematoxylin and eosin (H&E) slide of primary breast tumors.","We employed different feature extraction approaches and state-of-the-art model architectures.","Using binary classification, our models attained area under the receiver operating characteristic (AUROC) scores above 0.70 for nearly all gene expression pathways and on some cases, exceeded 0.80.","Attention maps suggest that our trained models recognize biologically relevant spatial patterns of cell sub-populations from H&E.","These efforts represent a first step towards developing computational H&E biomarkers that reflect facets of the TME and hold promise for augmenting precision oncology."],"url":"http://arxiv.org/abs/2404.16397v1","category":"eess.IV"}
{"created":"2024-04-25 08:08:54","title":"Fuzzy Inference System for Test Case Prioritization in Software Testing","abstract":"In the realm of software development, testing is crucial for ensuring software quality and adherence to requirements. However, it can be time-consuming and resource-intensive, especially when dealing with large and complex software systems. Test case prioritization (TCP) is a vital strategy to enhance testing efficiency by identifying the most critical test cases for early execution. This paper introduces a novel fuzzy logic-based approach to automate TCP, using fuzzy linguistic variables and expert-derived fuzzy rules to establish a link between test case characteristics and their prioritization. Our methodology utilizes two fuzzy variables - failure rate and execution time - alongside two crisp parameters: Prerequisite Test Case and Recently Updated Flag. Our findings demonstrate the proposed system capacity to rank test cases effectively through experimental validation on a real-world software system. The results affirm the practical applicability of our approach in optimizing the TCP and reducing the resource intensity of software testing.","sentences":["In the realm of software development, testing is crucial for ensuring software quality and adherence to requirements.","However, it can be time-consuming and resource-intensive, especially when dealing with large and complex software systems.","Test case prioritization (TCP) is a vital strategy to enhance testing efficiency by identifying the most critical test cases for early execution.","This paper introduces a novel fuzzy logic-based approach to automate TCP, using fuzzy linguistic variables and expert-derived fuzzy rules to establish a link between test case characteristics and their prioritization.","Our methodology utilizes two fuzzy variables - failure rate and execution time - alongside two crisp parameters: Prerequisite Test Case and Recently Updated Flag.","Our findings demonstrate the proposed system capacity to rank test cases effectively through experimental validation on a real-world software system.","The results affirm the practical applicability of our approach in optimizing the TCP and reducing the resource intensity of software testing."],"url":"http://arxiv.org/abs/2404.16395v1","category":"cs.SE"}
{"created":"2024-04-25 08:03:12","title":"STAR-RIS-Assisted Communication Radar Coexistence: Analysis and Optimization","abstract":"Integrated sensing and communication (ISAC) is expected to play a prominent role among emerging technologies in future wireless communications. In particular, a communication radar coexistence system is degraded significantly by mutual interference. In this work, given the advantages of promising reconfigurable intelligent surface (RIS), we propose a simultaneously transmitting and reflecting RIS (STAR-RIS)-assisted radar coexistence system where a STAR-RIS is introduced to improve the communication performance while suppressing the mutual interference and providing full space coverage. Based on the realistic conditions of correlated fading, and the presence of multiple user equipments (UEs) at both sides of the RIS, we derive the achievable rates at the radar and the communication receiver side in closed forms in terms of statistical channel state information (CSI). Next, we perform alternating optimization (AO) for optimizing the STAR-RIS and the radar beamforming. Regarding the former, we optimize the amplitudes and phase shifts of the STAR-RIS through a projected gradient ascent algorithm (PGAM) simultaneously with respect to the amplitudes and phase shifts of the surface for both energy splitting (ES) and mode switching (MS) operation protocols. The proposed optimization saves enough overhead since it can be performed every several coherence intervals. This property is particularly beneficial compared to reflecting-only RIS because a STAR-RIS includes the double number of variables, which require increased overhead. Finally, simulation results illustrate how the proposed architecture outperforms the conventional RIS counterpart, and show how the various parameters affect the performance. Moreover, a benchmark full instantaneous CSI (I-CSI) based design is provided and shown to result in higher sum-rate but also in large overhead associated with complexity.","sentences":["Integrated sensing and communication (ISAC) is expected to play a prominent role among emerging technologies in future wireless communications.","In particular, a communication radar coexistence system is degraded significantly by mutual interference.","In this work, given the advantages of promising reconfigurable intelligent surface (RIS), we propose a simultaneously transmitting and reflecting RIS (STAR-RIS)-assisted radar coexistence system where a STAR-RIS is introduced to improve the communication performance while suppressing the mutual interference and providing full space coverage.","Based on the realistic conditions of correlated fading, and the presence of multiple user equipments (UEs) at both sides of the RIS, we derive the achievable rates at the radar and the communication receiver side in closed forms in terms of statistical channel state information (CSI).","Next, we perform alternating optimization (AO) for optimizing the STAR-RIS and the radar beamforming.","Regarding the former, we optimize the amplitudes and phase shifts of the STAR-RIS through a projected gradient ascent algorithm (PGAM) simultaneously with respect to the amplitudes and phase shifts of the surface for both energy splitting (ES) and mode switching (MS) operation protocols.","The proposed optimization saves enough overhead since it can be performed every several coherence intervals.","This property is particularly beneficial compared to reflecting-only RIS because a STAR-RIS includes the double number of variables, which require increased overhead.","Finally, simulation results illustrate how the proposed architecture outperforms the conventional RIS counterpart, and show how the various parameters affect the performance.","Moreover, a benchmark full instantaneous CSI (I-CSI) based design is provided and shown to result in higher sum-rate but also in large overhead associated with complexity."],"url":"http://arxiv.org/abs/2404.16394v1","category":"cs.IT"}
{"created":"2024-04-25 08:01:11","title":"Dirigent: Lightweight Serverless Orchestration","abstract":"While Function as a Service (FaaS) platforms can initialize function sandboxes on worker nodes in 10-100s of milliseconds, the latency to schedule functions in real FaaS clusters can be orders of magnitude higher. We find that the current approach of building FaaS cluster managers on top of legacy orchestration systems like Kubernetes leads to high scheduling delay at high sandbox churn, which is typical in FaaS clusters. While generic cluster managers use hierarchical abstractions and multiple internal components to manage and reconcile state with frequent persistent updates, this becomes a bottleneck for FaaS, where cluster state frequently changes as sandboxes are created on the critical path of requests. Based on our root cause analysis of performance issues in existing FaaS cluster managers, we propose Dirigent, a clean-slate system architecture for FaaS orchestration with three key principles. First, Dirigent optimizes internal cluster manager abstractions to simplify state management. Second, it eliminates persistent state updates on the critical path of function invocations, leveraging the fact that FaaS abstracts sandboxes from users to relax exact state reconstruction guarantees. Finally, Dirigent runs monolithic control and data planes to minimize internal communication overheads and maximize throughput. We compare Dirigent to state-of-the-art FaaS platforms and show that Dirigent reduces 99th percentile per-function scheduling latency for a production workload by 2.79x compared to AWS Lambda and can spin up 2500 sandboxes per second at low latency, which is 1250x more than with Knative.","sentences":["While Function as a Service (FaaS) platforms can initialize function sandboxes on worker nodes in 10-100s of milliseconds, the latency to schedule functions in real FaaS clusters can be orders of magnitude higher.","We find that the current approach of building FaaS cluster managers on top of legacy orchestration systems like Kubernetes leads to high scheduling delay at high sandbox churn, which is typical in FaaS clusters.","While generic cluster managers use hierarchical abstractions and multiple internal components to manage and reconcile state with frequent persistent updates, this becomes a bottleneck for FaaS, where cluster state frequently changes as sandboxes are created on the critical path of requests.","Based on our root cause analysis of performance issues in existing FaaS cluster managers, we propose Dirigent, a clean-slate system architecture for FaaS orchestration with three key principles.","First, Dirigent optimizes internal cluster manager abstractions to simplify state management.","Second, it eliminates persistent state updates on the critical path of function invocations, leveraging the fact that FaaS abstracts sandboxes from users to relax exact state reconstruction guarantees.","Finally, Dirigent runs monolithic control and data planes to minimize internal communication overheads and maximize throughput.","We compare Dirigent to state-of-the-art FaaS platforms and show that Dirigent reduces 99th percentile per-function scheduling latency for a production workload by 2.79x compared to AWS Lambda and can spin up 2500 sandboxes per second at low latency, which is 1250x more than with Knative."],"url":"http://arxiv.org/abs/2404.16393v1","category":"cs.DC"}
{"created":"2024-04-25 07:59:23","title":"Stability-Oriented Prediction Horizons Design of Generalized Predictive Control for DC/DC Boost Converter","abstract":"This paper introduces a novel approach in designing prediction horizons on a generalized predictive control for a DC/DC boost converter. This method involves constructing a closed-loop system model and assessing the impact of different prediction horizons on system stability. In contrast to conventional design approaches that often rely on empirical prediction horizon selection or incorporate non-linear observers, the proposed method establishes a rigorous boundary for the prediction horizon to ensure system stability. This approach facilitates the selection of an appropriate prediction horizon while avoiding excessively short horizons that can lead to instability and preventing the adoption of unnecessarily long horizons that would burden the controller with high computational demands. Finally, the accuracy of the design method has been confirmed through experimental testing. Moreover, it has been demonstrated that the prediction horizon determined by this method reduces the computational burden by 10\\%-20\\% compared to the empirically selected prediction horizon.","sentences":["This paper introduces a novel approach in designing prediction horizons on a generalized predictive control for a DC/DC boost converter.","This method involves constructing a closed-loop system model and assessing the impact of different prediction horizons on system stability.","In contrast to conventional design approaches that often rely on empirical prediction horizon selection or incorporate non-linear observers, the proposed method establishes a rigorous boundary for the prediction horizon to ensure system stability.","This approach facilitates the selection of an appropriate prediction horizon while avoiding excessively short horizons that can lead to instability and preventing the adoption of unnecessarily long horizons that would burden the controller with high computational demands.","Finally, the accuracy of the design method has been confirmed through experimental testing.","Moreover, it has been demonstrated that the prediction horizon determined by this method reduces the computational burden by 10\\%-20\\% compared to the empirically selected prediction horizon."],"url":"http://arxiv.org/abs/2404.16391v1","category":"eess.SY"}
{"created":"2024-04-25 07:58:37","title":"Improvement of Geant4 Neutron-HP package: Unresolved Resonance Region description with Probability Tables","abstract":"Whether for shielding applications or for criticality safety studies, solving the neutron transport equation with good accuracy requires to take into account the resonant structure of cross sections in part of the Unresolved Resonance Region (URR). In this energy range even if the resonances can no longer be resolved experimentally, neglecting them can lead to significant numerical biases, namely in flux-based quantities. In Geant4, low energy neutrons are transported using evaluated nuclear data libraries handled by the Neutron High-Precision (Neutron-HP) package. In the version 11.01.p02 of the code, the URR can only be described by average smooth cross sections that do not take into account the statistical resonant structure of the cross sections. To overcome this shortcoming, the treatment of the URR with the use of the probability table method has been implemented in Geant4 and successfully validated with the reference Monte Carlo neutron transport codes MCNP6 (version 6.2) and Tripoli-4 (version 12). These developments will be taken into account in the next release of Geant4. All the validations of Geant4 have been performed with probability tables generated from both the NJOY and CALENDF pre-processing tools. Therefore Geant4 now has this unique feature to study the relative impact of the strategies involved during the production of probability table by the two pre-processing codes. This has been used to show that self-shielding is important also for inelastic cross sections in the example of 238U. The tool to generate probability tables usable by Geant4 either from NJOY or from CALENDF is made available on a dedicated GitLab repository and will be included in Geant4.","sentences":["Whether for shielding applications or for criticality safety studies, solving the neutron transport equation with good accuracy requires to take into account the resonant structure of cross sections in part of the Unresolved Resonance Region (URR).","In this energy range even if the resonances can no longer be resolved experimentally, neglecting them can lead to significant numerical biases, namely in flux-based quantities.","In Geant4, low energy neutrons are transported using evaluated nuclear data libraries handled by the Neutron High-Precision (Neutron-HP) package.","In the version 11.01.p02 of the code, the URR can only be described by average smooth cross sections that do not take into account the statistical resonant structure of the cross sections.","To overcome this shortcoming, the treatment of the URR with the use of the probability table method has been implemented in Geant4 and successfully validated with the reference Monte Carlo neutron transport codes MCNP6 (version 6.2) and Tripoli-4 (version 12).","These developments will be taken into account in the next release of Geant4.","All the validations of Geant4 have been performed with probability tables generated from both the NJOY and CALENDF pre-processing tools.","Therefore Geant4 now has this unique feature to study the relative impact of the strategies involved during the production of probability table by the two pre-processing codes.","This has been used to show that self-shielding is important also for inelastic cross sections in the example of 238U.","The tool to generate probability tables usable by Geant4 either from NJOY or from CALENDF is made available on a dedicated GitLab repository and will be included in Geant4."],"url":"http://arxiv.org/abs/2404.16389v1","category":"physics.comp-ph"}
{"created":"2024-04-25 07:57:11","title":"SwarmRL: Building the Future of Smart Active Systems","abstract":"This work introduces SwarmRL, a Python package designed to study intelligent active particles. SwarmRL provides an easy-to-use interface for developing models to control microscopic colloids using classical control and deep reinforcement learning approaches. These models may be deployed in simulations or real-world environments under a common framework. We explain the structure of the software and its key features and demonstrate how it can be used to accelerate research. With SwarmRL, we aim to streamline research into micro-robotic control while bridging the gap between experimental and simulation-driven sciences. SwarmRL is available open-source on GitHub at https://github.com/SwarmRL/SwarmRL.","sentences":["This work introduces SwarmRL, a Python package designed to study intelligent active particles.","SwarmRL provides an easy-to-use interface for developing models to control microscopic colloids using classical control and deep reinforcement learning approaches.","These models may be deployed in simulations or real-world environments under a common framework.","We explain the structure of the software and its key features and demonstrate how it can be used to accelerate research.","With SwarmRL, we aim to streamline research into micro-robotic control while bridging the gap between experimental and simulation-driven sciences.","SwarmRL is available open-source on GitHub at https://github.com/SwarmRL/SwarmRL."],"url":"http://arxiv.org/abs/2404.16388v1","category":"cs.RO"}
{"created":"2024-04-25 07:51:26","title":"Efficiency in Focus: LayerNorm as a Catalyst for Fine-tuning Medical Visual Language Pre-trained Models","abstract":"In the realm of Medical Visual Language Models (Med-VLMs), the quest for universal efficient fine-tuning mechanisms remains paramount, especially given researchers in interdisciplinary fields are often extremely short of training resources, yet largely unexplored. Given the unique challenges in the medical domain, such as limited data scope and significant domain-specific requirements, evaluating and adapting Parameter-Efficient Fine-Tuning (PEFT) methods specifically for Med-VLMs is essential. Most of the current PEFT methods on Med-VLMs have yet to be comprehensively investigated but mainly focus on adding some components to the model's structure or input. However, fine-tuning intrinsic model components often yields better generality and consistency, and its impact on the ultimate performance of Med-VLMs has been widely overlooked and remains understudied. In this paper, we endeavour to explore an alternative to traditional PEFT methods, especially the impact of fine-tuning LayerNorm layers, FFNs and Attention layers on the Med-VLMs. Our comprehensive studies span both small-scale and large-scale Med-VLMs, evaluating their performance under various fine-tuning paradigms across tasks such as Medical Visual Question Answering and Medical Imaging Report Generation. The findings reveal unique insights into the effects of intrinsic parameter fine-tuning methods on fine-tuning Med-VLMs to downstream tasks and expose fine-tuning solely the LayerNorm layers not only surpasses the efficiency of traditional PEFT methods but also retains the model's accuracy and generalization capabilities across a spectrum of medical downstream tasks. The experiments show LayerNorm fine-tuning's superior adaptability and scalability, particularly in the context of large-scale Med-VLMs.","sentences":["In the realm of Medical Visual Language Models (Med-VLMs), the quest for universal efficient fine-tuning mechanisms remains paramount, especially given researchers in interdisciplinary fields are often extremely short of training resources, yet largely unexplored.","Given the unique challenges in the medical domain, such as limited data scope and significant domain-specific requirements, evaluating and adapting Parameter-Efficient Fine-Tuning (PEFT) methods specifically for Med-VLMs is essential.","Most of the current PEFT methods on Med-VLMs have yet to be comprehensively investigated but mainly focus on adding some components to the model's structure or input.","However, fine-tuning intrinsic model components often yields better generality and consistency, and its impact on the ultimate performance of Med-VLMs has been widely overlooked and remains understudied.","In this paper, we endeavour to explore an alternative to traditional PEFT methods, especially the impact of fine-tuning LayerNorm layers, FFNs and Attention layers on the Med-VLMs.","Our comprehensive studies span both small-scale and large-scale Med-VLMs, evaluating their performance under various fine-tuning paradigms across tasks such as Medical Visual Question Answering and Medical Imaging Report Generation.","The findings reveal unique insights into the effects of intrinsic parameter fine-tuning methods on fine-tuning Med-VLMs to downstream tasks and expose fine-tuning solely the LayerNorm layers not only surpasses the efficiency of traditional PEFT methods but also retains the model's accuracy and generalization capabilities across a spectrum of medical downstream tasks.","The experiments show LayerNorm fine-tuning's superior adaptability and scalability, particularly in the context of large-scale Med-VLMs."],"url":"http://arxiv.org/abs/2404.16385v1","category":"cs.CV"}
{"created":"2024-04-25 07:45:40","title":"Euler band topology in spin-orbit coupled magnetic systems","abstract":"The Euler class characterizes the topology of two real bands isolated from other bands in two-dimensions. Despite various intriguing topological properties predicted up to now, the candidate real materials hosting electronic Euler bands are extremely rare. Here, we show that in a quantum spin Hall insulator with two-fold rotation $C_{2z}$ about the $z$-axis, a pair of bands with nontrivial $\\mathbb{Z}_2$ invariant turn into magnetic Euler bands under in-plane Zeeman field or in-plane ferromagnetic ordering. The resulting magnetic insulator generally carries a nontrivial second Stiefel-Whitney invariant. In particular, when the topmost pair of occupied bands carry a nonzero Euler number, the corresponding magnetic insulator can be called a magnetic Euler insulator. Moreover, the topological phase transition between a trivial magnetic insulator and a magnetic Stiefel-Whitney or Euler insulator is mediated by a stable topological semimetal phase in which Dirac nodes carrying non-Abelian topological charges exhibit braiding processes across the transition. Using the first-principles calculations, we propose various candidate materials hosting magnetic Euler bands. We especially show that ZrTe$_5$ bilayers under in-plane ferromagnetism are a candidate system for magnetic Stiefel-Whitney insulators in which the non-Abelian braiding-induced topological phase transitions can occur under pressure.","sentences":["The Euler class characterizes the topology of two real bands isolated from other bands in two-dimensions.","Despite various intriguing topological properties predicted up to now, the candidate real materials hosting electronic Euler bands are extremely rare.","Here, we show that in a quantum spin Hall insulator with two-fold rotation $C_{2z}$ about the $z$-axis, a pair of bands with nontrivial $\\mathbb{Z}_2$ invariant turn into magnetic Euler bands under in-plane Zeeman field or in-plane ferromagnetic ordering.","The resulting magnetic insulator generally carries a nontrivial second Stiefel-Whitney invariant.","In particular, when the topmost pair of occupied bands carry a nonzero Euler number, the corresponding magnetic insulator can be called a magnetic Euler insulator.","Moreover, the topological phase transition between a trivial magnetic insulator and a magnetic Stiefel-Whitney or Euler insulator is mediated by a stable topological semimetal phase in which Dirac nodes carrying non-Abelian topological charges exhibit braiding processes across the transition.","Using the first-principles calculations, we propose various candidate materials hosting magnetic Euler bands.","We especially show that ZrTe$_5$ bilayers under in-plane ferromagnetism are a candidate system for magnetic Stiefel-Whitney insulators in which the non-Abelian braiding-induced topological phase transitions can occur under pressure."],"url":"http://arxiv.org/abs/2404.16383v1","category":"cond-mat.mes-hall"}
{"created":"2024-04-25 07:42:48","title":"Efficient Higher-order Convolution for Small Kernels in Deep Learning","abstract":"Deep convolutional neural networks (DCNNs) are a class of artificial neural networks, primarily for computer vision tasks such as segmentation and classification. Many nonlinear operations, such as activation functions and pooling strategies, are used in DCNNs to enhance their ability to process different signals with different tasks. Conceptional convolution, a linear filter, is the essential component of DCNNs while nonlinear convolution is generally implemented as higher-order Volterra filters, However, for Volterra filtering, significant memory and computational costs pose a primary limitation for its widespread application in DCNN applications. In this study, we propose a novel method to perform higher-order Volterra filtering with lower memory and computation cost in forward and backward pass in DCNN training. The proposed method demonstrates computational advantages compared with conventional Volterra filter implementation. Furthermore, based on the proposed method, a new attention module called Higher-order Local Attention Block (HLA) is proposed and tested on CIFAR-100 dataset, which shows competitive improvement for classification task. Source code is available at: https://github.com/WinterWen666/Efficient-High-Order-Volterra-Convolution.git","sentences":["Deep convolutional neural networks (DCNNs) are a class of artificial neural networks, primarily for computer vision tasks such as segmentation and classification.","Many nonlinear operations, such as activation functions and pooling strategies, are used in DCNNs to enhance their ability to process different signals with different tasks.","Conceptional convolution, a linear filter, is the essential component of DCNNs while nonlinear convolution is generally implemented as higher-order Volterra filters, However, for Volterra filtering, significant memory and computational costs pose a primary limitation for its widespread application in DCNN applications.","In this study, we propose a novel method to perform higher-order Volterra filtering with lower memory and computation cost in forward and backward pass in DCNN training.","The proposed method demonstrates computational advantages compared with conventional Volterra filter implementation.","Furthermore, based on the proposed method, a new attention module called Higher-order Local Attention Block (HLA) is proposed and tested on CIFAR-100 dataset, which shows competitive improvement for classification task.","Source code is available at: https://github.com/WinterWen666/Efficient-High-Order-Volterra-Convolution.git"],"url":"http://arxiv.org/abs/2404.16380v1","category":"cs.CV"}
{"created":"2024-04-25 07:41:47","title":"Optimal and Bounded Suboptimal Any-Angle Multi-agent Pathfinding","abstract":"Multi-agent pathfinding (MAPF) is the problem of finding a set of conflict-free paths for a set of agents. Typically, the agents' moves are limited to a pre-defined graph of possible locations and allowed transitions between them, e.g. a 4-neighborhood grid. We explore how to solve MAPF problems when each agent can move between any pair of possible locations as long as traversing the line segment connecting them does not lead to the collision with the obstacles. This is known as any-angle pathfinding. We present the first optimal any-angle multi-agent pathfinding algorithm. Our planner is based on the Continuous Conflict-based Search (CCBS) algorithm and an optimal any-angle variant of the Safe Interval Path Planning (TO-AA-SIPP). The straightforward combination of those, however, scales poorly since any-angle path finding induces search trees with a very large branching factor. To mitigate this, we adapt two techniques from classical MAPF to the any-angle setting, namely Disjoint Splitting and Multi-Constraints. Experimental results on different combinations of these techniques show they enable solving over 30% more problems than the vanilla combination of CCBS and TO-AA-SIPP. In addition, we present a bounded-suboptimal variant of our algorithm, that enables trading runtime for solution cost in a controlled manner.","sentences":["Multi-agent pathfinding (MAPF) is the problem of finding a set of conflict-free paths for a set of agents.","Typically, the agents' moves are limited to a pre-defined graph of possible locations and allowed transitions between them, e.g. a 4-neighborhood grid.","We explore how to solve MAPF problems when each agent can move between any pair of possible locations as long as traversing the line segment connecting them does not lead to the collision with the obstacles.","This is known as any-angle pathfinding.","We present the first optimal any-angle multi-agent pathfinding algorithm.","Our planner is based on the Continuous Conflict-based Search (CCBS) algorithm and an optimal any-angle variant of the Safe Interval Path Planning (TO-AA-SIPP).","The straightforward combination of those, however, scales poorly since any-angle path finding induces search trees with a very large branching factor.","To mitigate this, we adapt two techniques from classical MAPF to the any-angle setting, namely Disjoint Splitting and Multi-Constraints.","Experimental results on different combinations of these techniques show they enable solving over 30% more problems than the vanilla combination of CCBS and TO-AA-SIPP.","In addition, we present a bounded-suboptimal variant of our algorithm, that enables trading runtime for solution cost in a controlled manner."],"url":"http://arxiv.org/abs/2404.16379v1","category":"cs.AI"}
{"created":"2024-04-25 07:34:12","title":"Two-dimensional jet flows for compressible full Euler system with general vorticity","abstract":"In this paper, we consider the well-posedness theory of two-dimensional compressible subsonic jet flows for steady full Euler system with general vorticity. Inspired by the analysis in arXiv:2006.05672, we show that the stream function formulation for such system admits a variational structure. Then the existence and uniqueness of a smooth subsonic jet flow can be established by the variational method developed by Alt, Caffarelli and Friedman. Furthermore, the far fields behavior of the flow and the existence of a critical upstream pressure are also obtained.","sentences":["In this paper, we consider the well-posedness theory of two-dimensional compressible subsonic jet flows for steady full Euler system with general vorticity.","Inspired by the analysis in arXiv:2006.05672, we show that the stream function formulation for such system admits a variational structure.","Then the existence and uniqueness of a smooth subsonic jet flow can be established by the variational method developed by Alt, Caffarelli and Friedman.","Furthermore, the far fields behavior of the flow and the existence of a critical upstream pressure are also obtained."],"url":"http://arxiv.org/abs/2404.16377v1","category":"math.AP"}
{"created":"2024-04-25 07:31:42","title":"A Hypergraph Approach to Distributed Broadcast","abstract":"This paper explores the distributed broadcast problem within the context of network communications, a critical challenge in decentralized information dissemination. We put forth a novel hypergraph-based approach to address this issue, focusing on minimizing the number of broadcasts to ensure comprehensive data sharing among all network users. A key contribution of our work is the establishment of a general lower bound for the problem using the min-cut capacity of hypergraphs. Additionally, we present the distributed broadcast for quasi-trees (DBQT) algorithm tailored for the unique structure of quasi-trees, which is proven to be optimal. This paper advances both network communication strategies and hypergraph theory, with implications for a wide range of real-world applications, from vehicular and sensor networks to distributed storage systems.","sentences":["This paper explores the distributed broadcast problem within the context of network communications, a critical challenge in decentralized information dissemination.","We put forth a novel hypergraph-based approach to address this issue, focusing on minimizing the number of broadcasts to ensure comprehensive data sharing among all network users.","A key contribution of our work is the establishment of a general lower bound for the problem using the min-cut capacity of hypergraphs.","Additionally, we present the distributed broadcast for quasi-trees (DBQT) algorithm tailored for the unique structure of quasi-trees, which is proven to be optimal.","This paper advances both network communication strategies and hypergraph theory, with implications for a wide range of real-world applications, from vehicular and sensor networks to distributed storage systems."],"url":"http://arxiv.org/abs/2404.16376v1","category":"cs.IT"}
{"created":"2024-04-25 07:29:17","title":"List Items One by One: A New Data Source and Learning Paradigm for Multimodal LLMs","abstract":"Set-of-Mark (SoM) Prompting unleashes the visual grounding capability of GPT-4V, by enabling the model to associate visual objects with tags inserted on the image. These tags, marked with alphanumerics, can be indexed via text tokens for easy reference. Despite the extraordinary performance from GPT-4V, we observe that other Multimodal Large Language Models (MLLMs) struggle to understand these visual tags. To promote the learning of SoM prompting for open-source models, we propose a new learning paradigm: \"list items one by one,\" which asks the model to enumerate and describe all visual tags placed on the image following the alphanumeric orders of tags. By integrating our curated dataset with other visual instruction tuning datasets, we are able to equip existing MLLMs with the SoM prompting ability. Furthermore, we evaluate our finetuned SoM models on five MLLM benchmarks. We find that this new dataset, even in a relatively small size (10k-30k images with tags), significantly enhances visual reasoning capabilities and reduces hallucinations for MLLMs. Perhaps surprisingly, these improvements persist even when the visual tags are omitted from input images during inference. This suggests the potential of \"list items one by one\" as a new paradigm for training MLLMs, which strengthens the object-text alignment through the use of visual tags in the training stage. Finally, we conduct analyses by probing trained models to understand the working mechanism of SoM. Our code and data are available at \\url{https://github.com/zzxslp/SoM-LLaVA}.","sentences":["Set-of-Mark (SoM)","Prompting unleashes the visual grounding capability of GPT-4V, by enabling the model to associate visual objects with tags inserted on the image.","These tags, marked with alphanumerics, can be indexed via text tokens for easy reference.","Despite the extraordinary performance from GPT-4V, we observe that other Multimodal Large Language Models (MLLMs) struggle to understand these visual tags.","To promote the learning of SoM prompting for open-source models, we propose a new learning paradigm: \"list items one by one,\" which asks the model to enumerate and describe all visual tags placed on the image following the alphanumeric orders of tags.","By integrating our curated dataset with other visual instruction tuning datasets, we are able to equip existing MLLMs with the SoM prompting ability.","Furthermore, we evaluate our finetuned SoM models on five MLLM benchmarks.","We find that this new dataset, even in a relatively small size (10k-30k images with tags), significantly enhances visual reasoning capabilities and reduces hallucinations for MLLMs.","Perhaps surprisingly, these improvements persist even when the visual tags are omitted from input images during inference.","This suggests the potential of \"list items one by one\" as a new paradigm for training MLLMs, which strengthens the object-text alignment through the use of visual tags in the training stage.","Finally, we conduct analyses by probing trained models to understand the working mechanism of SoM.","Our code and data are available at \\url{https://github.com/zzxslp/SoM-LLaVA}."],"url":"http://arxiv.org/abs/2404.16375v1","category":"cs.CV"}
{"created":"2024-04-25 07:15:23","title":"Don't Say No: Jailbreaking LLM by Suppressing Refusal","abstract":"Ensuring the safety alignment of Large Language Models (LLMs) is crucial to generating responses consistent with human values. Despite their ability to recognize and avoid harmful queries, LLMs are vulnerable to \"jailbreaking\" attacks, where carefully crafted prompts elicit them to produce toxic content. One category of jailbreak attacks is reformulating the task as adversarial attacks by eliciting the LLM to generate an affirmative response. However, the typical attack in this category GCG has very limited attack success rate. In this study, to better study the jailbreak attack, we introduce the DSN (Don't Say No) attack, which prompts LLMs to not only generate affirmative responses but also novelly enhance the objective to suppress refusals. In addition, another challenge lies in jailbreak attacks is the evaluation, as it is difficult to directly and accurately assess the harmfulness of the attack. The existing evaluation such as refusal keyword matching has its own limitation as it reveals numerous false positive and false negative instances. To overcome this challenge, we propose an ensemble evaluation pipeline incorporating Natural Language Inference (NLI) contradiction assessment and two external LLM evaluators. Extensive experiments demonstrate the potency of the DSN and the effectiveness of ensemble evaluation compared to baseline methods.","sentences":["Ensuring the safety alignment of Large Language Models (LLMs) is crucial to generating responses consistent with human values.","Despite their ability to recognize and avoid harmful queries, LLMs are vulnerable to \"jailbreaking\" attacks, where carefully crafted prompts elicit them to produce toxic content.","One category of jailbreak attacks is reformulating the task as adversarial attacks by eliciting the LLM to generate an affirmative response.","However, the typical attack in this category GCG has very limited attack success rate.","In this study, to better study the jailbreak attack, we introduce the DSN (Don't Say No) attack, which prompts LLMs to not only generate affirmative responses but also novelly enhance the objective to suppress refusals.","In addition, another challenge lies in jailbreak attacks is the evaluation, as it is difficult to directly and accurately assess the harmfulness of the attack.","The existing evaluation such as refusal keyword matching has its own limitation as it reveals numerous false positive and false negative instances.","To overcome this challenge, we propose an ensemble evaluation pipeline incorporating Natural Language Inference (NLI) contradiction assessment and two external LLM evaluators.","Extensive experiments demonstrate the potency of the DSN and the effectiveness of ensemble evaluation compared to baseline methods."],"url":"http://arxiv.org/abs/2404.16369v1","category":"cs.CL"}
{"created":"2024-04-25 17:59:19","title":"How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites","abstract":"In this report, we introduce InternVL 1.5, an open-source multimodal large language model (MLLM) to bridge the capability gap between open-source and proprietary commercial models in multimodal understanding. We introduce three simple improvements: (1) Strong Vision Encoder: we explored a continuous learning strategy for the large-scale vision foundation model -- InternViT-6B, boosting its visual understanding capabilities, and making it can be transferred and reused in different LLMs. (2) Dynamic High-Resolution: we divide images into tiles ranging from 1 to 40 of 448$\\times$448 pixels according to the aspect ratio and resolution of the input images, which supports up to 4K resolution input. (3) High-Quality Bilingual Dataset: we carefully collected a high-quality bilingual dataset that covers common scenes, document images, and annotated them with English and Chinese question-answer pairs, significantly enhancing performance in OCR- and Chinese-related tasks. We evaluate InternVL 1.5 through a series of benchmarks and comparative studies. Compared to both open-source and proprietary models, InternVL 1.5 shows competitive performance, achieving state-of-the-art results in 8 of 18 benchmarks. Code has been released at https://github.com/OpenGVLab/InternVL.","sentences":["In this report, we introduce InternVL 1.5, an open-source multimodal large language model (MLLM) to bridge the capability gap between open-source and proprietary commercial models in multimodal understanding.","We introduce three simple improvements: (1) Strong Vision Encoder: we explored a continuous learning strategy for the large-scale vision foundation model -- InternViT-6B, boosting its visual understanding capabilities, and making it can be transferred and reused in different LLMs.","(2) Dynamic High-Resolution: we divide images into tiles ranging from 1 to 40 of 448$\\times$448 pixels according to the aspect ratio and resolution of the input images, which supports up to 4K resolution input.","(3) High-Quality Bilingual Dataset: we carefully collected a high-quality bilingual dataset that covers common scenes, document images, and annotated them with English and Chinese question-answer pairs, significantly enhancing performance in OCR- and Chinese-related tasks.","We evaluate InternVL 1.5 through a series of benchmarks and comparative studies.","Compared to both open-source and proprietary models, InternVL 1.5 shows competitive performance, achieving state-of-the-art results in 8 of 18 benchmarks.","Code has been released at https://github.com/OpenGVLab/InternVL."],"url":"http://arxiv.org/abs/2404.16821v1","category":"cs.CV"}
{"created":"2024-04-25 16:40:15","title":"Learning-Based Efficient Approximation of Data-enabled Predictive Control","abstract":"Data-Enabled Predictive Control (DeePC) bypasses the need for system identification by directly leveraging raw data to formulate optimal control policies. However, the size of the optimization problem in DeePC grows linearly with respect to the data size, which prohibits its application due to high computational costs. In this paper, we propose an efficient approximation of DeePC, whose size is invariant with respect to the amount of data collected, via differentiable convex programming. Specifically, the optimization problem in DeePC is decomposed into two parts: a control objective and a scoring function that evaluates the likelihood of a guessed I/O sequence, the latter of which is approximated with a size-invariant learned optimization problem. The proposed method is validated through numerical simulations on a quadruple tank system, illustrating that the learned controller can reduce the computational time of DeePC by 5x while maintaining its control performance.","sentences":["Data-Enabled Predictive Control (DeePC) bypasses the need for system identification by directly leveraging raw data to formulate optimal control policies.","However, the size of the optimization problem in DeePC grows linearly with respect to the data size, which prohibits its application due to high computational costs.","In this paper, we propose an efficient approximation of DeePC, whose size is invariant with respect to the amount of data collected, via differentiable convex programming.","Specifically, the optimization problem in DeePC is decomposed into two parts: a control objective and a scoring function that evaluates the likelihood of a guessed I/O sequence, the latter of which is approximated with a size-invariant learned optimization problem.","The proposed method is validated through numerical simulations on a quadruple tank system, illustrating that the learned controller can reduce the computational time of DeePC by 5x while maintaining its control performance."],"url":"http://arxiv.org/abs/2404.16727v1","category":"eess.SY"}
{"created":"2024-04-25 14:03:26","title":"Intrinsic and extrinsic plasmons in the hard x-ray photoelectron spectra of nearly free electron metals","abstract":"Collective plasmon excitations in solids that result from the process of photoemission are an important area of fundamental research. In this study, we identify a significant number ($n$) of multiple bulk plasmons ($n\\omega_p$) in the hard x-ray photoelectron spectra of the core levels and valence bands (VBs) of two well-known, nearly free electron metals, aluminum (Al) and magnesium (Mg). On the basis of earlier theoretical works, we estimate the contributions of extrinsic, intrinsic, and interference processes to the intensities of 1$s$ to 2$s$ core level plasmons. The intrinsic contribution diminishes from 22% for 1$\\omega_p$, to 4.4% for 2$\\omega_p$, and becomes negligible thereafter (0.5% for 3$\\omega_p$). The extrinsic and intrinsic plasmon contributions do not vary significantly across a broad range of photoelectron kinetic energies, and also between the two metals (Al and Mg). The interference contribution varies from negative to zero as $n$ increases. An asymmetric line shape is observed for the bulk plasmons, which is most pronounced for 1$\\omega_p$. Signature of the surface plasmon is detected in normal emission, and it exhibits a significantly increased intensity in the grazing emission. The VB spectra of Al and Mg, which are dominated by $s$-like states, exhibit excellent agreement with the calculated VB based on density functional theory. The VB exhibits four multiple bulk plasmon peaks in the loss region, which are influenced by an intrinsic process in addition to the extrinsic process. On a completely oxidized aluminum surface, the relative intensity of the Al metal bulk plasmon remains nearly unaltered, while the surface plasmon is completely attenuated.","sentences":["Collective plasmon excitations in solids that result from the process of photoemission are an important area of fundamental research.","In this study, we identify a significant number ($n$) of multiple bulk plasmons ($n\\omega_p$) in the hard x-ray photoelectron spectra of the core levels and valence bands (VBs) of two well-known, nearly free electron metals, aluminum (Al) and magnesium (Mg).","On the basis of earlier theoretical works, we estimate the contributions of extrinsic, intrinsic, and interference processes to the intensities of 1$s$ to 2$s$ core level plasmons.","The intrinsic contribution diminishes from 22% for 1$\\omega_p$, to 4.4% for 2$\\omega_p$, and becomes negligible thereafter (0.5% for 3$\\omega_p$).","The extrinsic and intrinsic plasmon contributions do not vary significantly across a broad range of photoelectron kinetic energies, and also between the two metals (Al and Mg).","The interference contribution varies from negative to zero as $n$ increases.","An asymmetric line shape is observed for the bulk plasmons, which is most pronounced for 1$\\omega_p$. Signature of the surface plasmon is detected in normal emission, and it exhibits a significantly increased intensity in the grazing emission.","The VB spectra of Al and Mg, which are dominated by $s$-like states, exhibit excellent agreement with the calculated VB based on density functional theory.","The VB exhibits four multiple bulk plasmon peaks in the loss region, which are influenced by an intrinsic process in addition to the extrinsic process.","On a completely oxidized aluminum surface, the relative intensity of the Al metal bulk plasmon remains nearly unaltered, while the surface plasmon is completely attenuated."],"url":"http://arxiv.org/abs/2404.16620v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-25 12:09:17","title":"RE-RecSys: An End-to-End system for recommending properties in Real-Estate domain","abstract":"We propose an end-to-end real-estate recommendation system, RE-RecSys, which has been productionized in real-world industry setting. We categorize any user into 4 categories based on available historical data: i) cold-start users; ii) short-term users; iii) long-term users; and iv) short-long term users. For cold-start users, we propose a novel rule-based engine that is based on the popularity of locality and user preferences. For short-term users, we propose to use content-filtering model which recommends properties based on recent interactions of users. For long-term and short-long term users, we propose a novel combination of content and collaborative filtering based approach which can be easily productionized in the real-world scenario. Moreover, based on the conversion rate, we have designed a novel weighing scheme for different impressions done by users on the platform for the training of content and collaborative models. Finally, we show the efficiency of the proposed pipeline, RE-RecSys, on a real-world property and clickstream dataset collected from leading real-estate platform in India. We show that the proposed pipeline is deployable in real-world scenario with an average latency of <40 ms serving 1000 rpm.","sentences":["We propose an end-to-end real-estate recommendation system, RE-RecSys, which has been productionized in real-world industry setting.","We categorize any user into 4 categories based on available historical data: i) cold-start users; ii) short-term users; iii) long-term users; and iv) short-long term users.","For cold-start users, we propose a novel rule-based engine that is based on the popularity of locality and user preferences.","For short-term users, we propose to use content-filtering model which recommends properties based on recent interactions of users.","For long-term and short-long term users, we propose a novel combination of content and collaborative filtering based approach which can be easily productionized in the real-world scenario.","Moreover, based on the conversion rate, we have designed a novel weighing scheme for different impressions done by users on the platform for the training of content and collaborative models.","Finally, we show the efficiency of the proposed pipeline, RE-RecSys, on a real-world property and clickstream dataset collected from leading real-estate platform in India.","We show that the proposed pipeline is deployable in real-world scenario with an average latency of <40 ms serving 1000 rpm."],"url":"http://arxiv.org/abs/2404.16553v1","category":"cs.IR"}
{"created":"2024-04-25 12:04:36","title":"Application of Long-Short Term Memory and Convolutional Neural Networks for Real-Time Bridge Scour Forecast","abstract":"Scour around bridge piers is a critical challenge for infrastructures around the world. In the absence of analytical models and due to the complexity of the scour process, it is difficult for current empirical methods to achieve accurate predictions. In this paper, we exploit the power of deep learning algorithms to forecast the scour depth variations around bridge piers based on historical sensor monitoring data, including riverbed elevation, flow elevation, and flow velocity. We investigated the performance of Long Short-Term Memory (LSTM) and Convolutional Neural Network (CNN) models for real-time scour forecasting using data collected from bridges in Alaska and Oregon from 2006 to 2021. The LSTM models achieved mean absolute error (MAE) ranging from 0.1m to 0.5m for predicting bed level variations a week in advance, showing a reasonable performance. The Fully Convolutional Network (FCN) variant of CNN outperformed other CNN configurations, showing a comparable performance to LSTMs with significantly lower computational costs. We explored various innovative random-search heuristics for hyperparameter tuning and model optimisation which resulted in reduced computational cost compared to grid-search method. The impact of different combinations of sensor features on scour prediction showed the significance of the historical time series of scour for predicting upcoming events. Overall, this study provides a greater understanding of the potential of Deep Learning (DL) for real-time scour forecasting and early warning in bridges with diverse scour and flow characteristics including riverine and tidal/coastal bridges.","sentences":["Scour around bridge piers is a critical challenge for infrastructures around the world.","In the absence of analytical models and due to the complexity of the scour process, it is difficult for current empirical methods to achieve accurate predictions.","In this paper, we exploit the power of deep learning algorithms to forecast the scour depth variations around bridge piers based on historical sensor monitoring data, including riverbed elevation, flow elevation, and flow velocity.","We investigated the performance of Long Short-Term Memory (LSTM) and Convolutional Neural Network (CNN) models for real-time scour forecasting using data collected from bridges in Alaska and Oregon from 2006 to 2021.","The LSTM models achieved mean absolute error (MAE) ranging from 0.1m to 0.5m for predicting bed level variations a week in advance, showing a reasonable performance.","The Fully Convolutional Network (FCN) variant of CNN outperformed other CNN configurations, showing a comparable performance to LSTMs with significantly lower computational costs.","We explored various innovative random-search heuristics for hyperparameter tuning and model optimisation which resulted in reduced computational cost compared to grid-search method.","The impact of different combinations of sensor features on scour prediction showed the significance of the historical time series of scour for predicting upcoming events.","Overall, this study provides a greater understanding of the potential of Deep Learning (DL) for real-time scour forecasting and early warning in bridges with diverse scour and flow characteristics including riverine and tidal/coastal bridges."],"url":"http://arxiv.org/abs/2404.16549v1","category":"cs.LG"}
{"created":"2024-04-25 11:38:54","title":"Energy Efficient Service Placement for IoT Networks","abstract":"In recent years, there has been a significant expansion in the Internet of Things (IoT), with a growing number of devices being connected to the internet. This has led to an increase in data collection and analysis as well as the development of new technologies and applications. The rise of IoT has also brought about new challenges, such as security concerns and energy efficiency. This study investigates a layered IoT architecture that combines fog and cloud computing, aiming to assess the impact of service placement on energy efficiency. Through simulations, we analyse energy use across Access Fog, Metro Fog, and Cloud Data Centre layers for different IoT request volumes. Findings indicate that Access Fog is optimal for single requests, while Metro Fog efficiently manages higher demands from multiple devices. The study emphasizes the need for adaptive service deployment, responsive to network load variations, to improve energy efficiency. Hence, we propose the implementation of dynamic service placement strategies within Internet of Things (IoT) environments.","sentences":["In recent years, there has been a significant expansion in the Internet of Things (IoT), with a growing number of devices being connected to the internet.","This has led to an increase in data collection and analysis as well as the development of new technologies and applications.","The rise of IoT has also brought about new challenges, such as security concerns and energy efficiency.","This study investigates a layered IoT architecture that combines fog and cloud computing, aiming to assess the impact of service placement on energy efficiency.","Through simulations, we analyse energy use across Access Fog, Metro Fog, and Cloud Data Centre layers for different IoT request volumes.","Findings indicate that Access Fog is optimal for single requests, while Metro Fog efficiently manages higher demands from multiple devices.","The study emphasizes the need for adaptive service deployment, responsive to network load variations, to improve energy efficiency.","Hence, we propose the implementation of dynamic service placement strategies within Internet of Things (IoT) environments."],"url":"http://arxiv.org/abs/2404.16527v1","category":"cs.NI"}
{"created":"2024-04-25 10:59:02","title":"Building a Japanese Document-Level Relation Extraction Dataset Assisted by Cross-Lingual Transfer","abstract":"Document-level Relation Extraction (DocRE) is the task of extracting all semantic relationships from a document. While studies have been conducted on English DocRE, limited attention has been given to DocRE in non-English languages. This work delves into effectively utilizing existing English resources to promote DocRE studies in non-English languages, with Japanese as the representative case. As an initial attempt, we construct a dataset by transferring an English dataset to Japanese. However, models trained on such a dataset suffer from low recalls. We investigate the error cases and attribute the failure to different surface structures and semantics of documents translated from English and those written by native speakers. We thus switch to explore if the transferred dataset can assist human annotation on Japanese documents. In our proposal, annotators edit relation predictions from a model trained on the transferred dataset. Quantitative analysis shows that relation recommendations suggested by the model help reduce approximately 50% of the human edit steps compared with the previous approach. Experiments quantify the performance of existing DocRE models on our collected dataset, portraying the challenges of Japanese and cross-lingual DocRE.","sentences":["Document-level Relation Extraction (DocRE) is the task of extracting all semantic relationships from a document.","While studies have been conducted on English DocRE, limited attention has been given to DocRE in non-English languages.","This work delves into effectively utilizing existing English resources to promote DocRE studies in non-English languages, with Japanese as the representative case.","As an initial attempt, we construct a dataset by transferring an English dataset to Japanese.","However, models trained on such a dataset suffer from low recalls.","We investigate the error cases and attribute the failure to different surface structures and semantics of documents translated from English and those written by native speakers.","We thus switch to explore if the transferred dataset can assist human annotation on Japanese documents.","In our proposal, annotators edit relation predictions from a model trained on the transferred dataset.","Quantitative analysis shows that relation recommendations suggested by the model help reduce approximately 50% of the human edit steps compared with the previous approach.","Experiments quantify the performance of existing DocRE models on our collected dataset, portraying the challenges of Japanese and cross-lingual DocRE."],"url":"http://arxiv.org/abs/2404.16506v1","category":"cs.CL"}
{"created":"2024-04-25 10:07:21","title":"TESS Free-floating Planet Candidate Is Likely a Stellar Flare","abstract":"The discovery of a terrestrial-mass free-floating planet candidate in the light curve of the star TIC 107150013 observed by the Transiting Exoplanet Survey Satellite (TESS) has recently been announced. A short-duration (~0.5 day), low-amplitude (~0.06 mag) brightening in the TESS light curve was interpreted as a short-timescale gravitational microlensing event. However, the purported event occurred far from the Galactic center and the Galactic plane (l~ 239 deg, b ~ -5 deg), on a relatively nearby (~3.2 kpc) star, making the microlensing interpretation unlikely. Here, we report the archival photometric observations of TIC 107150013 collected by the Optical Gravitational Lensing Experiment (OGLE) from 2018 through 2020. The archival OGLE light curve reveals periodic variability indicative of starspots on the surface of the star. The presence of starspots indicates magnetic activity of the star, which may also manifest as stellar flares. We interpret the brightening of TIC 107150013 seen in the TESS data as the stellar flare. We present similar flaring stars detected in the archival OGLE data, mimicking short-timescale, low-amplitude microlensing events. Such stars may be a source of non-negligible false positive detections in the planned space-based microlensing surveys.","sentences":["The discovery of a terrestrial-mass free-floating planet candidate in the light curve of the star TIC 107150013 observed by the Transiting Exoplanet Survey Satellite (TESS) has recently been announced.","A short-duration (~0.5 day), low-amplitude (~0.06 mag) brightening in the TESS light curve was interpreted as a short-timescale gravitational microlensing event.","However, the purported event occurred far from the Galactic center and the Galactic plane (l~ 239 deg, b ~ -5 deg), on a relatively nearby (~3.2 kpc) star, making the microlensing interpretation unlikely.","Here, we report the archival photometric observations of TIC 107150013 collected by the Optical Gravitational Lensing Experiment (OGLE) from 2018 through 2020.","The archival OGLE light curve reveals periodic variability indicative of starspots on the surface of the star.","The presence of starspots indicates magnetic activity of the star, which may also manifest as stellar flares.","We interpret the brightening of TIC 107150013 seen in the TESS data as the stellar flare.","We present similar flaring stars detected in the archival OGLE data, mimicking short-timescale, low-amplitude microlensing events.","Such stars may be a source of non-negligible false positive detections in the planned space-based microlensing surveys."],"url":"http://arxiv.org/abs/2404.16480v1","category":"astro-ph.EP"}
{"created":"2024-04-25 09:58:04","title":"Finite-size inertial spherical particles in turbulence","abstract":"We investigate by direct numerical simulations the fluid-solid interaction of non-dilute suspensions of spherical particles moving in triperiodic turbulence, at the relatively large Reynolds number of $Re_\\lambda \\approx 400$. The solid-to-fluid density ratio is varied between $1.3$ and $100$, the particle diameter $D$ ranges between $16 \\le D/\\eta \\le 123$ ($\\eta$ is the Kolmogorov scale), and the volume fraction of the suspension is $0.079$. Turbulence is sustained using the Arnold-Beltrami-Childress cellular-flow forcing. The influence of the solid phase on the largest and energetic scales of the flow changes with the size and density of the particles. Light and large particles modulate all scales in a isotropic way, while heavier and smaller particles modulate the largest scales of the flow towards an anisotropic state. Smaller scales are isotropic and homogeneous for all cases. The mechanism driving the energy transfer across scales changes with the size and the density of the particles. For large and light particles the energy transfer is only marginally influenced by the fluid-solid interaction. For small and heavy particles, instead, the classical energy cascade is subdominant at all scales, and the energy transfer is essentially driven by the fluid-solid coupling. The influence of the solid phase on the flow intermittency is also discussed. Besides, the collective motion of the particles and their preferential location in relation with properties of the carrier flow are analysed. The solid phase exhibits moderate clustering; for large particles the level of clustering decreases with their density, while for small particles it is maximum for intermediate values.","sentences":["We investigate by direct numerical simulations the fluid-solid interaction of non-dilute suspensions of spherical particles moving in triperiodic turbulence, at the relatively large Reynolds number of $Re_\\lambda \\approx 400$.","The solid-to-fluid density ratio is varied between $1.3$ and $100$, the particle diameter $D$ ranges between $16 \\le D/\\eta \\le 123$ ($\\eta$ is the Kolmogorov scale), and the volume fraction of the suspension is $0.079$. Turbulence is sustained using the Arnold-Beltrami-Childress cellular-flow forcing.","The influence of the solid phase on the largest and energetic scales of the flow changes with the size and density of the particles.","Light and large particles modulate all scales in a isotropic way, while heavier and smaller particles modulate the largest scales of the flow towards an anisotropic state.","Smaller scales are isotropic and homogeneous for all cases.","The mechanism driving the energy transfer across scales changes with the size and the density of the particles.","For large and light particles the energy transfer is only marginally influenced by the fluid-solid interaction.","For small and heavy particles, instead, the classical energy cascade is subdominant at all scales, and the energy transfer is essentially driven by the fluid-solid coupling.","The influence of the solid phase on the flow intermittency is also discussed.","Besides, the collective motion of the particles and their preferential location in relation with properties of the carrier flow are analysed.","The solid phase exhibits moderate clustering; for large particles the level of clustering decreases with their density, while for small particles it is maximum for intermediate values."],"url":"http://arxiv.org/abs/2404.16475v1","category":"physics.flu-dyn"}
{"created":"2024-04-25 09:23:03","title":"Automating the Discovery of Partial Differential Equations in Dynamical Systems","abstract":"Identifying partial differential equations (PDEs) from data is crucial for understanding the governing mechanisms of natural phenomena, yet it remains a challenging task. We present an extension to the ARGOS framework, ARGOS-RAL, which leverages sparse regression with the recurrent adaptive lasso to identify PDEs from limited prior knowledge automatically. Our method automates calculating partial derivatives, constructing a candidate library, and estimating a sparse model. We rigorously evaluate the performance of ARGOS-RAL in identifying canonical PDEs under various noise levels and sample sizes, demonstrating its robustness in handling noisy and non-uniformly distributed data. We also test the algorithm's performance on datasets consisting solely of random noise to simulate scenarios with severely compromised data quality. Our results show that ARGOS-RAL effectively and reliably identifies the underlying PDEs from data, outperforming the sequential threshold ridge regression method in most cases. We highlight the potential of combining statistical methods, machine learning, and dynamical systems theory to automatically discover governing equations from collected data, streamlining the scientific modeling process.","sentences":["Identifying partial differential equations (PDEs) from data is crucial for understanding the governing mechanisms of natural phenomena, yet it remains a challenging task.","We present an extension to the ARGOS framework, ARGOS-RAL, which leverages sparse regression with the recurrent adaptive lasso to identify PDEs from limited prior knowledge automatically.","Our method automates calculating partial derivatives, constructing a candidate library, and estimating a sparse model.","We rigorously evaluate the performance of ARGOS-RAL in identifying canonical PDEs under various noise levels and sample sizes, demonstrating its robustness in handling noisy and non-uniformly distributed data.","We also test the algorithm's performance on datasets consisting solely of random noise to simulate scenarios with severely compromised data quality.","Our results show that ARGOS-RAL effectively and reliably identifies the underlying PDEs from data, outperforming the sequential threshold ridge regression method in most cases.","We highlight the potential of combining statistical methods, machine learning, and dynamical systems theory to automatically discover governing equations from collected data, streamlining the scientific modeling process."],"url":"http://arxiv.org/abs/2404.16444v1","category":"cs.LG"}
{"created":"2024-04-25 08:18:18","title":"Revisiting Relevance Feedback for CLIP-based Interactive Image Retrieval","abstract":"Many image retrieval studies use metric learning to train an image encoder. However, metric learning cannot handle differences in users' preferences, and requires data to train an image encoder. To overcome these limitations, we revisit relevance feedback, a classic technique for interactive retrieval systems, and propose an interactive CLIP-based image retrieval system with relevance feedback. Our retrieval system first executes the retrieval, collects each user's unique preferences through binary feedback, and returns images the user prefers. Even when users have various preferences, our retrieval system learns each user's preference through the feedback and adapts to the preference. Moreover, our retrieval system leverages CLIP's zero-shot transferability and achieves high accuracy without training. We empirically show that our retrieval system competes well with state-of-the-art metric learning in category-based image retrieval, despite not training image encoders specifically for each dataset. Furthermore, we set up two additional experimental settings where users have various preferences: one-label-based image retrieval and conditioned image retrieval. In both cases, our retrieval system effectively adapts to each user's preferences, resulting in improved accuracy compared to image retrieval without feedback. Overall, our work highlights the potential benefits of integrating CLIP with classic relevance feedback techniques to enhance image retrieval.","sentences":["Many image retrieval studies use metric learning to train an image encoder.","However, metric learning cannot handle differences in users' preferences, and requires data to train an image encoder.","To overcome these limitations, we revisit relevance feedback, a classic technique for interactive retrieval systems, and propose an interactive CLIP-based image retrieval system with relevance feedback.","Our retrieval system first executes the retrieval, collects each user's unique preferences through binary feedback, and returns images the user prefers.","Even when users have various preferences, our retrieval system learns each user's preference through the feedback and adapts to the preference.","Moreover, our retrieval system leverages CLIP's zero-shot transferability and achieves high accuracy without training.","We empirically show that our retrieval system competes well with state-of-the-art metric learning in category-based image retrieval, despite not training image encoders specifically for each dataset.","Furthermore, we set up two additional experimental settings where users have various preferences: one-label-based image retrieval and conditioned image retrieval.","In both cases, our retrieval system effectively adapts to each user's preferences, resulting in improved accuracy compared to image retrieval without feedback.","Overall, our work highlights the potential benefits of integrating CLIP with classic relevance feedback techniques to enhance image retrieval."],"url":"http://arxiv.org/abs/2404.16398v1","category":"cs.CV"}
{"created":"2024-04-25 07:44:24","title":"Abstracting Effect Systems for Algebraic Effect Handlers","abstract":"Many effect systems for algebraic effect handlers are designed to guarantee that all invoked effects are handled adequately. However, respective researchers have developed their own effect systems that differ in how to represent the collections of effects that may happen. This situation results in blurring what is required for the representation and manipulation of effect collections in a safe effect system.   In this work, we present a language ${\\lambda_{\\mathrm{EA}}}$ equipped with an effect system that abstracts the existing effect systems for algebraic effect handlers. The effect system of ${\\lambda_{\\mathrm{EA}}}$ is parameterized over effect algebras, which abstract the representation and manipulation of effect collections in safe effect systems. We prove the type-and-effect safety of ${\\lambda_{\\mathrm{EA}}}$ by assuming that a given effect algebra meets certain properties called safety conditions. As a result, we can obtain the safety properties of a concrete effect system by proving that an effect algebra corresponding to the concrete system meets the safety conditions. We also show that effect algebras meeting the safety conditions are expressive enough to accommodate some existing effect systems, each of which represents effect collections in a different style. Our framework can also differentiate the safety aspects of the effect collections of the existing effect systems. To this end, we extend ${\\lambda_{\\mathrm{EA}}}$ and the safety conditions to lift coercions and type-erasure semantics, propose other effect algebras including ones for which no effect system has been studied in the literature, and compare which effect algebra is safe and which is not for the extensions.","sentences":["Many effect systems for algebraic effect handlers are designed to guarantee that all invoked effects are handled adequately.","However, respective researchers have developed their own effect systems that differ in how to represent the collections of effects that may happen.","This situation results in blurring what is required for the representation and manipulation of effect collections in a safe effect system.   ","In this work, we present a language ${\\lambda_{\\mathrm{EA}}}$ equipped with an effect system that abstracts the existing effect systems for algebraic effect handlers.","The effect system of ${\\lambda_{\\mathrm{EA}}}$ is parameterized over effect algebras, which abstract the representation and manipulation of effect collections in safe effect systems.","We prove the type-and-effect safety of ${\\lambda_{\\mathrm{EA}}}$ by assuming that a given effect algebra meets certain properties called safety conditions.","As a result, we can obtain the safety properties of a concrete effect system by proving that an effect algebra corresponding to the concrete system meets the safety conditions.","We also show that effect algebras meeting the safety conditions are expressive enough to accommodate some existing effect systems, each of which represents effect collections in a different style.","Our framework can also differentiate the safety aspects of the effect collections of the existing effect systems.","To this end, we extend ${\\lambda_{\\mathrm{EA}}}$ and the safety conditions to lift coercions and type-erasure semantics, propose other effect algebras including ones for which no effect system has been studied in the literature, and compare which effect algebra is safe and which is not for the extensions."],"url":"http://arxiv.org/abs/2404.16381v1","category":"cs.PL"}
{"created":"2024-04-25 07:17:30","title":"MegaParticles: Range-based 6-DoF Monte Carlo Localization with GPU-Accelerated Stein Particle Filter","abstract":"This paper presents a 6-DoF range-based Monte Carlo localization method with a GPU-accelerated Stein particle filter. To update a massive amount of particles, we propose a Gauss-Newton-based Stein variational gradient descent (SVGD) with iterative neighbor particle search. This method uses SVGD to collectively update particle states with gradient and neighborhood information, which provides efficient particle sampling. For an efficient neighbor particle search, it uses locality sensitive hashing and iteratively updates the neighbor list of each particle over time. The neighbor list is then used to propagate the posterior probabilities of particles over the neighbor particle graph. The proposed method is capable of evaluating one million particles in real-time on a single GPU and enables robust pose initialization and re-localization without an initial pose estimate. In experiments, the proposed method showed an extreme robustness to complete sensor occlusion (i.e., kidnapping), and enabled pinpoint sensor localization without any prior information.","sentences":["This paper presents a 6-DoF range-based Monte Carlo localization method with a GPU-accelerated Stein particle filter.","To update a massive amount of particles, we propose a Gauss-Newton-based Stein variational gradient descent (SVGD) with iterative neighbor particle search.","This method uses SVGD to collectively update particle states with gradient and neighborhood information, which provides efficient particle sampling.","For an efficient neighbor particle search, it uses locality sensitive hashing and iteratively updates the neighbor list of each particle over time.","The neighbor list is then used to propagate the posterior probabilities of particles over the neighbor particle graph.","The proposed method is capable of evaluating one million particles in real-time on a single GPU and enables robust pose initialization and re-localization without an initial pose estimate.","In experiments, the proposed method showed an extreme robustness to complete sensor occlusion (i.e., kidnapping), and enabled pinpoint sensor localization without any prior information."],"url":"http://arxiv.org/abs/2404.16370v1","category":"cs.RO"}
{"created":"2024-04-25 07:09:05","title":"Guarding Graph Neural Networks for Unsupervised Graph Anomaly Detection","abstract":"Unsupervised graph anomaly detection aims at identifying rare patterns that deviate from the majority in a graph without the aid of labels, which is important for a variety of real-world applications. Recent advances have utilized Graph Neural Networks (GNNs) to learn effective node representations by aggregating information from neighborhoods. This is motivated by the hypothesis that nodes in the graph tend to exhibit consistent behaviors with their neighborhoods. However, such consistency can be disrupted by graph anomalies in multiple ways. Most existing methods directly employ GNNs to learn representations, disregarding the negative impact of graph anomalies on GNNs, resulting in sub-optimal node representations and anomaly detection performance. While a few recent approaches have redesigned GNNs for graph anomaly detection under semi-supervised label guidance, how to address the adverse effects of graph anomalies on GNNs in unsupervised scenarios and learn effective representations for anomaly detection are still under-explored. To bridge this gap, in this paper, we propose a simple yet effective framework for Guarding Graph Neural Networks for Unsupervised Graph Anomaly Detection (G3AD). Specifically, G3AD introduces two auxiliary networks along with correlation constraints to guard the GNNs from inconsistent information encoding. Furthermore, G3AD introduces an adaptive caching module to guard the GNNs from solely reconstructing the observed data that contains anomalies. Extensive experiments demonstrate that our proposed G3AD can outperform seventeen state-of-the-art methods on both synthetic and real-world datasets.","sentences":["Unsupervised graph anomaly detection aims at identifying rare patterns that deviate from the majority in a graph without the aid of labels, which is important for a variety of real-world applications.","Recent advances have utilized Graph Neural Networks (GNNs) to learn effective node representations by aggregating information from neighborhoods.","This is motivated by the hypothesis that nodes in the graph tend to exhibit consistent behaviors with their neighborhoods.","However, such consistency can be disrupted by graph anomalies in multiple ways.","Most existing methods directly employ GNNs to learn representations, disregarding the negative impact of graph anomalies on GNNs, resulting in sub-optimal node representations and anomaly detection performance.","While a few recent approaches have redesigned GNNs for graph anomaly detection under semi-supervised label guidance, how to address the adverse effects of graph anomalies on GNNs in unsupervised scenarios and learn effective representations for anomaly detection are still under-explored.","To bridge this gap, in this paper, we propose a simple yet effective framework for Guarding Graph Neural Networks for Unsupervised Graph Anomaly Detection (G3AD).","Specifically, G3AD introduces two auxiliary networks along with correlation constraints to guard the GNNs from inconsistent information encoding.","Furthermore, G3AD introduces an adaptive caching module to guard the GNNs from solely reconstructing the observed data that contains anomalies.","Extensive experiments demonstrate that our proposed G3AD can outperform seventeen state-of-the-art methods on both synthetic and real-world datasets."],"url":"http://arxiv.org/abs/2404.16366v1","category":"cs.LG"}
{"created":"2024-04-25 07:08:00","title":"VISLA Benchmark: Evaluating Embedding Sensitivity to Semantic and Lexical Alterations","abstract":"Despite their remarkable successes, state-of-the-art language models face challenges in grasping certain important semantic details. This paper introduces the VISLA (Variance and Invariance to Semantic and Lexical Alterations) benchmark, designed to evaluate the semantic and lexical understanding of language models. VISLA presents a 3-way semantic (in)equivalence task with a triplet of sentences associated with an image, to evaluate both vision-language models (VLMs) and unimodal language models (ULMs). An evaluation involving 34 VLMs and 20 ULMs reveals surprising difficulties in distinguishing between lexical and semantic variations. Spatial semantics encoded by language models also appear to be highly sensitive to lexical information. Notably, text encoders of VLMs demonstrate greater sensitivity to semantic and lexical variations than unimodal text encoders. Our contributions include the unification of image-to-text and text-to-text retrieval tasks, an off-the-shelf evaluation without fine-tuning, and assessing LMs' semantic (in)variance in the presence of lexical alterations. The results highlight strengths and weaknesses across diverse vision and unimodal language models, contributing to a deeper understanding of their capabilities. % VISLA enables a rigorous evaluation, shedding light on language models' capabilities in handling semantic and lexical nuances. Data and code will be made available at https://github.com/Sri-Harsha/visla_benchmark.","sentences":["Despite their remarkable successes, state-of-the-art language models face challenges in grasping certain important semantic details.","This paper introduces the VISLA (Variance and Invariance to Semantic and Lexical Alterations) benchmark, designed to evaluate the semantic and lexical understanding of language models.","VISLA presents a 3-way semantic (in)equivalence task with a triplet of sentences associated with an image, to evaluate both vision-language models (VLMs) and unimodal language models (ULMs).","An evaluation involving 34 VLMs and 20 ULMs reveals surprising difficulties in distinguishing between lexical and semantic variations.","Spatial semantics encoded by language models also appear to be highly sensitive to lexical information.","Notably, text encoders of VLMs demonstrate greater sensitivity to semantic and lexical variations than unimodal text encoders.","Our contributions include the unification of image-to-text and text-to-text retrieval tasks, an off-the-shelf evaluation without fine-tuning, and assessing LMs' semantic (in)variance in the presence of lexical alterations.","The results highlight strengths and weaknesses across diverse vision and unimodal language models, contributing to a deeper understanding of their capabilities.","% VISLA enables a rigorous evaluation, shedding light on language models' capabilities in handling semantic and lexical nuances.","Data and code will be made available at https://github.com/Sri-Harsha/visla_benchmark."],"url":"http://arxiv.org/abs/2404.16365v1","category":"cs.CL"}
{"created":"2024-04-25 07:02:07","title":"ReZero: Boosting MCTS-based Algorithms by Just-in-Time and Speedy Reanalyze","abstract":"MCTS-based algorithms, such as MuZero and its derivatives, have achieved widespread success in various decision-making domains. These algorithms employ the reanalyze process to enhance sample efficiency, albeit at the expense of significant wall-clock time consumption. To address this issue, we propose a general approach named ReZero to boost MCTS-based algorithms. Specifically, we propose a new scheme that simplifies data collecting and reanalyzing, which significantly reduces the search cost while guarantees the performance as well. Furthermore, to accelerate each search process, we conceive a method to reuse the subsequent information in the trajectory. The corresponding analysis conducted on the bandit model also provides auxiliary theoretical substantiation for our design. Experiments conducted on Atari environments and board games demonstrates that ReZero substantially improves training speed while maintaining high sample efficiency. The code is available as part of the LightZero benchmark at https://github.com/opendilab/LightZero.","sentences":["MCTS-based algorithms, such as MuZero and its derivatives, have achieved widespread success in various decision-making domains.","These algorithms employ the reanalyze process to enhance sample efficiency, albeit at the expense of significant wall-clock time consumption.","To address this issue, we propose a general approach named ReZero to boost MCTS-based algorithms.","Specifically, we propose a new scheme that simplifies data collecting and reanalyzing, which significantly reduces the search cost while guarantees the performance as well.","Furthermore, to accelerate each search process, we conceive a method to reuse the subsequent information in the trajectory.","The corresponding analysis conducted on the bandit model also provides auxiliary theoretical substantiation for our design.","Experiments conducted on Atari environments and board games demonstrates that ReZero substantially improves training speed while maintaining high sample efficiency.","The code is available as part of the LightZero benchmark at https://github.com/opendilab/LightZero."],"url":"http://arxiv.org/abs/2404.16364v1","category":"cs.AI"}
{"created":"2024-04-25 06:22:21","title":"Integration of Mixture of Experts and Multimodal Generative AI in Internet of Vehicles: A Survey","abstract":"Generative AI (GAI) can enhance the cognitive, reasoning, and planning capabilities of intelligent modules in the Internet of Vehicles (IoV) by synthesizing augmented datasets, completing sensor data, and making sequential decisions. In addition, the mixture of experts (MoE) can enable the distributed and collaborative execution of AI models without performance degradation between connected vehicles. In this survey, we explore the integration of MoE and GAI to enable Artificial General Intelligence in IoV, which can enable the realization of full autonomy for IoV with minimal human supervision and applicability in a wide range of mobility scenarios, including environment monitoring, traffic management, and autonomous driving. In particular, we present the fundamentals of GAI, MoE, and their interplay applications in IoV. Furthermore, we discuss the potential integration of MoE and GAI in IoV, including distributed perception and monitoring, collaborative decision-making and planning, and generative modeling and simulation. Finally, we present several potential research directions for facilitating the integration.","sentences":["Generative AI (GAI) can enhance the cognitive, reasoning, and planning capabilities of intelligent modules in the Internet of Vehicles (IoV) by synthesizing augmented datasets, completing sensor data, and making sequential decisions.","In addition, the mixture of experts (MoE) can enable the distributed and collaborative execution of AI models without performance degradation between connected vehicles.","In this survey, we explore the integration of MoE and GAI to enable Artificial General Intelligence in IoV, which can enable the realization of full autonomy for IoV with minimal human supervision and applicability in a wide range of mobility scenarios, including environment monitoring, traffic management, and autonomous driving.","In particular, we present the fundamentals of GAI, MoE, and their interplay applications in IoV.","Furthermore, we discuss the potential integration of MoE and GAI in IoV, including distributed perception and monitoring, collaborative decision-making and planning, and generative modeling and simulation.","Finally, we present several potential research directions for facilitating the integration."],"url":"http://arxiv.org/abs/2404.16356v1","category":"cs.NI"}
{"created":"2024-04-25 05:42:41","title":"Light-weight Retinal Layer Segmentation with Global Reasoning","abstract":"Automatic retinal layer segmentation with medical images, such as optical coherence tomography (OCT) images, serves as an important tool for diagnosing ophthalmic diseases. However, it is challenging to achieve accurate segmentation due to low contrast and blood flow noises presented in the images. In addition, the algorithm should be light-weight to be deployed for practical clinical applications. Therefore, it is desired to design a light-weight network with high performance for retinal layer segmentation. In this paper, we propose LightReSeg for retinal layer segmentation which can be applied to OCT images. Specifically, our approach follows an encoder-decoder structure, where the encoder part employs multi-scale feature extraction and a Transformer block for fully exploiting the semantic information of feature maps at all scales and making the features have better global reasoning capabilities, while the decoder part, we design a multi-scale asymmetric attention (MAA) module for preserving the semantic information at each encoder scale. The experiments show that our approach achieves a better segmentation performance compared to the current state-of-the-art method TransUnet with 105.7M parameters on both our collected dataset and two other public datasets, with only 3.3M parameters.","sentences":["Automatic retinal layer segmentation with medical images, such as optical coherence tomography (OCT) images, serves as an important tool for diagnosing ophthalmic diseases.","However, it is challenging to achieve accurate segmentation due to low contrast and blood flow noises presented in the images.","In addition, the algorithm should be light-weight to be deployed for practical clinical applications.","Therefore, it is desired to design a light-weight network with high performance for retinal layer segmentation.","In this paper, we propose LightReSeg for retinal layer segmentation which can be applied to OCT images.","Specifically, our approach follows an encoder-decoder structure, where the encoder part employs multi-scale feature extraction and a Transformer block for fully exploiting the semantic information of feature maps at all scales and making the features have better global reasoning capabilities, while the decoder part, we design a multi-scale asymmetric attention (MAA) module for preserving the semantic information at each encoder scale.","The experiments show that our approach achieves a better segmentation performance compared to the current state-of-the-art method TransUnet with 105.7M parameters on both our collected dataset and two other public datasets, with only 3.3M parameters."],"url":"http://arxiv.org/abs/2404.16346v1","category":"eess.IV"}
{"created":"2024-04-25 05:07:50","title":"Training-Free Unsupervised Prompt for Vision-Language Models","abstract":"Prompt learning has become the most effective paradigm for adapting large pre-trained vision-language models (VLMs) to downstream tasks. Recently, unsupervised prompt tuning methods, such as UPL and POUF, directly leverage pseudo-labels as supervisory information to fine-tune additional adaptation modules on unlabeled data. However, inaccurate pseudo labels easily misguide the tuning process and result in poor representation capabilities. In light of this, we propose Training-Free Unsupervised Prompts (TFUP), which maximally preserves the inherent representation capabilities and enhances them with a residual connection to similarity-based prediction probabilities in a training-free and labeling-free manner. Specifically, we integrate both instance confidence and prototype scores to select representative samples, which are used to customize a reliable Feature Cache Model (FCM) for training-free inference. Then, we design a Multi-level Similarity Measure (MSM) that considers both feature-level and semantic-level similarities to calculate the distance between each test image and the cached sample as the weight of the corresponding cached label to generate similarity-based prediction probabilities. In this way, TFUP achieves surprising performance, even surpassing the training-base method on multiple classification datasets. Based on our TFUP, we propose a training-based approach (TFUP-T) to further boost the adaptation performance. In addition to the standard cross-entropy loss, TFUP-T adopts an additional marginal distribution entropy loss to constrain the model from a global perspective. Our TFUP-T achieves new state-of-the-art classification performance compared to unsupervised and few-shot adaptation approaches on multiple benchmarks. In particular, TFUP-T improves the classification accuracy of POUF by 3.3% on the most challenging Domain-Net dataset.","sentences":["Prompt learning has become the most effective paradigm for adapting large pre-trained vision-language models (VLMs) to downstream tasks.","Recently, unsupervised prompt tuning methods, such as UPL and POUF, directly leverage pseudo-labels as supervisory information to fine-tune additional adaptation modules on unlabeled data.","However, inaccurate pseudo labels easily misguide the tuning process and result in poor representation capabilities.","In light of this, we propose Training-Free Unsupervised Prompts (TFUP), which maximally preserves the inherent representation capabilities and enhances them with a residual connection to similarity-based prediction probabilities in a training-free and labeling-free manner.","Specifically, we integrate both instance confidence and prototype scores to select representative samples, which are used to customize a reliable Feature Cache Model (FCM) for training-free inference.","Then, we design a Multi-level Similarity Measure (MSM) that considers both feature-level and semantic-level similarities to calculate the distance between each test image and the cached sample as the weight of the corresponding cached label to generate similarity-based prediction probabilities.","In this way, TFUP achieves surprising performance, even surpassing the training-base method on multiple classification datasets.","Based on our TFUP, we propose a training-based approach (TFUP-T) to further boost the adaptation performance.","In addition to the standard cross-entropy loss, TFUP-T adopts an additional marginal distribution entropy loss to constrain the model from a global perspective.","Our TFUP-T achieves new state-of-the-art classification performance compared to unsupervised and few-shot adaptation approaches on multiple benchmarks.","In particular, TFUP-T improves the classification accuracy of POUF by 3.3% on the most challenging Domain-Net dataset."],"url":"http://arxiv.org/abs/2404.16339v1","category":"cs.CV"}
{"created":"2024-04-25 04:47:12","title":"Phonon dispersion of quantum paraelectric SrTiO3 in electric fields","abstract":"Here we report on an elastic and inelastic neutron scattering study addressing the effect of electric fields on quantum paraelectric SrTiO3. Our elastic scattering results find small changes as a function of field in a superlattice reflection that sample the octahedral rotations, which is indicative of only weak coupling of octahedral rotation and electric polarization. By collecting not only the change in gap, but also of the dispersion, we can better quantify the changes in the lattice dynamics. The findings are put in context to recent field DFT calculations predicting the E-field effect on the atomic motions of the lowest lying transverse optical (soft) mode. We find hints of non-linear coupling to the acoustic mode and to the phonon with polarization perpendicular to the E-field, which shows the non-linearity in the chemical potential that is also relevant when strongly driving SrTiO3 with E-field (THz) pulses.","sentences":["Here we report on an elastic and inelastic neutron scattering study addressing the effect of electric fields on quantum paraelectric SrTiO3.","Our elastic scattering results find small changes as a function of field in a superlattice reflection that sample the octahedral rotations, which is indicative of only weak coupling of octahedral rotation and electric polarization.","By collecting not only the change in gap, but also of the dispersion, we can better quantify the changes in the lattice dynamics.","The findings are put in context to recent field DFT calculations predicting the E-field effect on the atomic motions of the lowest lying transverse optical (soft) mode.","We find hints of non-linear coupling to the acoustic mode and to the phonon with polarization perpendicular to the E-field, which shows the non-linearity in the chemical potential that is also relevant when strongly driving SrTiO3 with E-field (THz) pulses."],"url":"http://arxiv.org/abs/2404.16334v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-04-25 04:46:02","title":"AI Coders Are Among Us: Rethinking Programming Language Grammar Towards Efficient Code Generation","abstract":"Besides humans and machines, Artificial Intelligence (AI) models have emerged to be another important audience of programming languages, as we come to the era of large language models (LLMs). LLMs can now excel at coding competitions and even program like developers to address various tasks, such as math calculation. Yet, the grammar and layout of existing programs are designed for humans. Particularly, abundant grammar tokens and formatting tokens are included to make the code more readable to humans. While beneficial, such a human-centric design imposes an unnecessary computational burden on LLMs where each token, either consumed or generated, consumes computational resources. To improve inference efficiency and reduce computational costs, we propose the concept of AI-oriented grammar, which aims to represent the code in a way that better suits the working mechanism of AI models. Code written with AI-oriented grammar discards formats and uses a minimum number of tokens to convey code semantics effectively. To demonstrate the feasibility of this concept, we explore and implement the first AI-oriented grammar for Python, named Simple Python (SimPy). SimPy is crafted by revising the original Python grammar through a series of heuristic rules. Programs written in SimPy maintain identical Abstract Syntax Tree (AST) structures to those in standard Python, allowing execution via a modified AST parser. In addition, we explore methods to enable existing LLMs to proficiently understand and use SimPy, and ensure the changes remain imperceptible for human developers. Compared with the original Python, SimPy not only reduces token usage by 13.5% and 10.4% for CodeLlama and GPT-4, but can also achieve equivalent, even improved, performance over the models trained on Python code.","sentences":["Besides humans and machines, Artificial Intelligence (AI) models have emerged to be another important audience of programming languages, as we come to the era of large language models (LLMs).","LLMs can now excel at coding competitions and even program like developers to address various tasks, such as math calculation.","Yet, the grammar and layout of existing programs are designed for humans.","Particularly, abundant grammar tokens and formatting tokens are included to make the code more readable to humans.","While beneficial, such a human-centric design imposes an unnecessary computational burden on LLMs where each token, either consumed or generated, consumes computational resources.","To improve inference efficiency and reduce computational costs, we propose the concept of AI-oriented grammar, which aims to represent the code in a way that better suits the working mechanism of AI models.","Code written with AI-oriented grammar discards formats and uses a minimum number of tokens to convey code semantics effectively.","To demonstrate the feasibility of this concept, we explore and implement the first AI-oriented grammar for Python, named Simple Python (SimPy).","SimPy is crafted by revising the original Python grammar through a series of heuristic rules.","Programs written in SimPy maintain identical Abstract Syntax Tree (AST) structures to those in standard Python, allowing execution via a modified AST parser.","In addition, we explore methods to enable existing LLMs to proficiently understand and use SimPy, and ensure the changes remain imperceptible for human developers.","Compared with the original Python, SimPy not only reduces token usage by 13.5% and 10.4% for CodeLlama and GPT-4, but can also achieve equivalent, even improved, performance over the models trained on Python code."],"url":"http://arxiv.org/abs/2404.16333v1","category":"cs.SE"}
{"created":"2024-04-25 04:37:35","title":"IMWA: Iterative Model Weight Averaging Benefits Class-Imbalanced Learning Tasks","abstract":"Model Weight Averaging (MWA) is a technique that seeks to enhance model's performance by averaging the weights of multiple trained models. This paper first empirically finds that 1) the vanilla MWA can benefit the class-imbalanced learning, and 2) performing model averaging in the early epochs of training yields a greater performance improvement than doing that in later epochs. Inspired by these two observations, in this paper we propose a novel MWA technique for class-imbalanced learning tasks named Iterative Model Weight Averaging (IMWA). Specifically, IMWA divides the entire training stage into multiple episodes. Within each episode, multiple models are concurrently trained from the same initialized model weight, and subsequently averaged into a singular model. Then, the weight of this average model serves as a fresh initialization for the ensuing episode, thus establishing an iterative learning paradigm. Compared to vanilla MWA, IMWA achieves higher performance improvements with the same computational cost. Moreover, IMWA can further enhance the performance of those methods employing EMA strategy, demonstrating that IMWA and EMA can complement each other. Extensive experiments on various class-imbalanced learning tasks, i.e., class-imbalanced image classification, semi-supervised class-imbalanced image classification and semi-supervised object detection tasks showcase the effectiveness of our IMWA.","sentences":["Model Weight Averaging (MWA) is a technique that seeks to enhance model's performance by averaging the weights of multiple trained models.","This paper first empirically finds that 1) the vanilla MWA can benefit the class-imbalanced learning, and 2) performing model averaging in the early epochs of training yields a greater performance improvement than doing that in later epochs.","Inspired by these two observations, in this paper we propose a novel MWA technique for class-imbalanced learning tasks named Iterative Model Weight Averaging (IMWA).","Specifically, IMWA divides the entire training stage into multiple episodes.","Within each episode, multiple models are concurrently trained from the same initialized model weight, and subsequently averaged into a singular model.","Then, the weight of this average model serves as a fresh initialization for the ensuing episode, thus establishing an iterative learning paradigm.","Compared to vanilla MWA, IMWA achieves higher performance improvements with the same computational cost.","Moreover, IMWA can further enhance the performance of those methods employing EMA strategy, demonstrating that IMWA and EMA can complement each other.","Extensive experiments on various class-imbalanced learning tasks, i.e., class-imbalanced image classification, semi-supervised class-imbalanced image classification and semi-supervised object detection tasks showcase the effectiveness of our IMWA."],"url":"http://arxiv.org/abs/2404.16331v1","category":"cs.CV"}
{"created":"2024-04-25 04:21:57","title":"Semantic Segmentation Refiner for Ultrasound Applications with Zero-Shot Foundation Models","abstract":"Despite the remarkable success of deep learning in medical imaging analysis, medical image segmentation remains challenging due to the scarcity of high-quality labeled images for supervision. Further, the significant domain gap between natural and medical images in general and ultrasound images in particular hinders fine-tuning models trained on natural images to the task at hand. In this work, we address the performance degradation of segmentation models in low-data regimes and propose a prompt-less segmentation method harnessing the ability of segmentation foundation models to segment abstract shapes. We do that via our novel prompt point generation algorithm which uses coarse semantic segmentation masks as input and a zero-shot prompt-able foundation model as an optimization target. We demonstrate our method on a segmentation findings task (pathologic anomalies) in ultrasound images. Our method's advantages are brought to light in varying degrees of low-data regime experiments on a small-scale musculoskeletal ultrasound images dataset, yielding a larger performance gain as the training set size decreases.","sentences":["Despite the remarkable success of deep learning in medical imaging analysis, medical image segmentation remains challenging due to the scarcity of high-quality labeled images for supervision.","Further, the significant domain gap between natural and medical images in general and ultrasound images in particular hinders fine-tuning models trained on natural images to the task at hand.","In this work, we address the performance degradation of segmentation models in low-data regimes and propose a prompt-less segmentation method harnessing the ability of segmentation foundation models to segment abstract shapes.","We do that via our novel prompt point generation algorithm which uses coarse semantic segmentation masks as input and a zero-shot prompt-able foundation model as an optimization target.","We demonstrate our method on a segmentation findings task (pathologic anomalies) in ultrasound images.","Our method's advantages are brought to light in varying degrees of low-data regime experiments on a small-scale musculoskeletal ultrasound images dataset, yielding a larger performance gain as the training set size decreases."],"url":"http://arxiv.org/abs/2404.16325v1","category":"cs.CV"}
{"created":"2024-04-25 03:23:28","title":"WorldValuesBench: A Large-Scale Benchmark Dataset for Multi-Cultural Value Awareness of Language Models","abstract":"The awareness of multi-cultural human values is critical to the ability of language models (LMs) to generate safe and personalized responses. However, this awareness of LMs has been insufficiently studied, since the computer science community lacks access to the large-scale real-world data about multi-cultural values. In this paper, we present WorldValuesBench, a globally diverse, large-scale benchmark dataset for the multi-cultural value prediction task, which requires a model to generate a rating response to a value question based on demographic contexts. Our dataset is derived from an influential social science project, World Values Survey (WVS), that has collected answers to hundreds of value questions (e.g., social, economic, ethical) from 94,728 participants worldwide. We have constructed more than 20 million examples of the type \"(demographic attributes, value question) $\\rightarrow$ answer\" from the WVS responses. We perform a case study using our dataset and show that the task is challenging for strong open and closed-source models. On merely $11.1\\%$, $25.0\\%$, $72.2\\%$, and $75.0\\%$ of the questions, Alpaca-7B, Vicuna-7B-v1.5, Mixtral-8x7B-Instruct-v0.1, and GPT-3.5 Turbo can respectively achieve $<0.2$ Wasserstein 1-distance from the human normalized answer distributions. WorldValuesBench opens up new research avenues in studying limitations and opportunities in multi-cultural value awareness of LMs.","sentences":["The awareness of multi-cultural human values is critical to the ability of language models (LMs) to generate safe and personalized responses.","However, this awareness of LMs has been insufficiently studied, since the computer science community lacks access to the large-scale real-world data about multi-cultural values.","In this paper, we present WorldValuesBench, a globally diverse, large-scale benchmark dataset for the multi-cultural value prediction task, which requires a model to generate a rating response to a value question based on demographic contexts.","Our dataset is derived from an influential social science project, World Values Survey (WVS), that has collected answers to hundreds of value questions (e.g., social, economic, ethical) from 94,728 participants worldwide.","We have constructed more than 20 million examples of the type \"(demographic attributes, value question) $\\rightarrow$ answer\" from the WVS responses.","We perform a case study using our dataset and show that the task is challenging for strong open and closed-source models.","On merely $11.1\\%$, $25.0\\%$, $72.2\\%$, and $75.0\\%$ of the questions, Alpaca-7B, Vicuna-7B-v1.5, Mixtral-8x7B-Instruct-v0.1, and GPT-3.5","Turbo can respectively achieve $<0.2$ Wasserstein 1-distance from the human normalized answer distributions.","WorldValuesBench opens up new research avenues in studying limitations and opportunities in multi-cultural value awareness of LMs."],"url":"http://arxiv.org/abs/2404.16308v1","category":"cs.CL"}
{"created":"2024-04-25 02:37:56","title":"When Fuzzing Meets LLMs: Challenges and Opportunities","abstract":"Fuzzing, a widely-used technique for bug detection, has seen advancements through Large Language Models (LLMs). Despite their potential, LLMs face specific challenges in fuzzing. In this paper, we identified five major challenges of LLM-assisted fuzzing. To support our findings, we revisited the most recent papers from top-tier conferences, confirming that these challenges are widespread. As a remedy, we propose some actionable recommendations to help improve applying LLM in Fuzzing and conduct preliminary evaluations on DBMS fuzzing. The results demonstrate that our recommendations effectively address the identified challenges.","sentences":["Fuzzing, a widely-used technique for bug detection, has seen advancements through Large Language Models (LLMs).","Despite their potential, LLMs face specific challenges in fuzzing.","In this paper, we identified five major challenges of LLM-assisted fuzzing.","To support our findings, we revisited the most recent papers from top-tier conferences, confirming that these challenges are widespread.","As a remedy, we propose some actionable recommendations to help improve applying LLM in Fuzzing and conduct preliminary evaluations on DBMS fuzzing.","The results demonstrate that our recommendations effectively address the identified challenges."],"url":"http://arxiv.org/abs/2404.16297v1","category":"cs.SE"}
{"created":"2024-04-25 02:28:16","title":"Research on Splicing Image Detection Algorithms Based on Natural Image Statistical Characteristics","abstract":"With the development and widespread application of digital image processing technology, image splicing has become a common method of image manipulation, raising numerous security and legal issues. This paper introduces a new splicing image detection algorithm based on the statistical characteristics of natural images, aimed at improving the accuracy and efficiency of splicing image detection. By analyzing the limitations of traditional methods, we have developed a detection framework that integrates advanced statistical analysis techniques and machine learning methods. The algorithm has been validated using multiple public datasets, showing high accuracy in detecting spliced edges and locating tampered areas, as well as good robustness. Additionally, we explore the potential applications and challenges faced by the algorithm in real-world scenarios. This research not only provides an effective technological means for the field of image tampering detection but also offers new ideas and methods for future related research.","sentences":["With the development and widespread application of digital image processing technology, image splicing has become a common method of image manipulation, raising numerous security and legal issues.","This paper introduces a new splicing image detection algorithm based on the statistical characteristics of natural images, aimed at improving the accuracy and efficiency of splicing image detection.","By analyzing the limitations of traditional methods, we have developed a detection framework that integrates advanced statistical analysis techniques and machine learning methods.","The algorithm has been validated using multiple public datasets, showing high accuracy in detecting spliced edges and locating tampered areas, as well as good robustness.","Additionally, we explore the potential applications and challenges faced by the algorithm in real-world scenarios.","This research not only provides an effective technological means for the field of image tampering detection but also offers new ideas and methods for future related research."],"url":"http://arxiv.org/abs/2404.16296v1","category":"cs.CV"}
{"created":"2024-04-25 02:25:35","title":"LLM-Based Section Identifiers Excel on Open Source but Stumble in Real World Applications","abstract":"Electronic health records (EHR) even though a boon for healthcare practitioners, are growing convoluted and longer every day. Sifting around these lengthy EHRs is taxing and becomes a cumbersome part of physician-patient interaction. Several approaches have been proposed to help alleviate this prevalent issue either via summarization or sectioning, however, only a few approaches have truly been helpful in the past. With the rise of automated methods, machine learning (ML) has shown promise in solving the task of identifying relevant sections in EHR. However, most ML methods rely on labeled data which is difficult to get in healthcare. Large language models (LLMs) on the other hand, have performed impressive feats in natural language processing (NLP), that too in a zero-shot manner, i.e. without any labeled data. To that end, we propose using LLMs to identify relevant section headers. We find that GPT-4 can effectively solve the task on both zero and few-shot settings as well as segment dramatically better than state-of-the-art methods. Additionally, we also annotate a much harder real world dataset and find that GPT-4 struggles to perform well, alluding to further research and harder benchmarks.","sentences":["Electronic health records (EHR) even though a boon for healthcare practitioners, are growing convoluted and longer every day.","Sifting around these lengthy EHRs is taxing and becomes a cumbersome part of physician-patient interaction.","Several approaches have been proposed to help alleviate this prevalent issue either via summarization or sectioning, however, only a few approaches have truly been helpful in the past.","With the rise of automated methods, machine learning (ML) has shown promise in solving the task of identifying relevant sections in EHR.","However, most ML methods rely on labeled data which is difficult to get in healthcare.","Large language models (LLMs) on the other hand, have performed impressive feats in natural language processing (NLP), that too in a zero-shot manner, i.e. without any labeled data.","To that end, we propose using LLMs to identify relevant section headers.","We find that GPT-4 can effectively solve the task on both zero and few-shot settings as well as segment dramatically better than state-of-the-art methods.","Additionally, we also annotate a much harder real world dataset and find that GPT-4 struggles to perform well, alluding to further research and harder benchmarks."],"url":"http://arxiv.org/abs/2404.16294v1","category":"cs.CL"}
{"created":"2024-04-25 01:48:44","title":"An Efficient Reconstructed Differential Evolution Variant by Some of the Current State-of-the-art Strategies for Solving Single Objective Bound Constrained Problems","abstract":"Complex single-objective bounded problems are often difficult to solve. In evolutionary computation methods, since the proposal of differential evolution algorithm in 1997, it has been widely studied and developed due to its simplicity and efficiency. These developments include various adaptive strategies, operator improvements, and the introduction of other search methods. After 2014, research based on LSHADE has also been widely studied by researchers. However, although recently proposed improvement strategies have shown superiority over their previous generation's first performance, adding all new strategies may not necessarily bring the strongest performance. Therefore, we recombine some effective advances based on advanced differential evolution variants in recent years and finally determine an effective combination scheme to further promote the performance of differential evolution. In this paper, we propose a strategy recombination and reconstruction differential evolution algorithm called reconstructed differential evolution (RDE) to solve single-objective bounded optimization problems. Based on the benchmark suite of the 2024 IEEE Congress on Evolutionary Computation (CEC2024), we tested RDE and several other advanced differential evolution variants. The experimental results show that RDE has superior performance in solving complex optimization problems.","sentences":["Complex single-objective bounded problems are often difficult to solve.","In evolutionary computation methods, since the proposal of differential evolution algorithm in 1997, it has been widely studied and developed due to its simplicity and efficiency.","These developments include various adaptive strategies, operator improvements, and the introduction of other search methods.","After 2014, research based on LSHADE has also been widely studied by researchers.","However, although recently proposed improvement strategies have shown superiority over their previous generation's first performance, adding all new strategies may not necessarily bring the strongest performance.","Therefore, we recombine some effective advances based on advanced differential evolution variants in recent years and finally determine an effective combination scheme to further promote the performance of differential evolution.","In this paper, we propose a strategy recombination and reconstruction differential evolution algorithm called reconstructed differential evolution (RDE) to solve single-objective bounded optimization problems.","Based on the benchmark suite of the 2024 IEEE Congress on Evolutionary Computation (CEC2024), we tested RDE and several other advanced differential evolution variants.","The experimental results show that RDE has superior performance in solving complex optimization problems."],"url":"http://arxiv.org/abs/2404.16280v1","category":"cs.NE"}
{"created":"2024-04-25 01:09:45","title":"Nonsmooth, Nonconvex Optimization Using Functional Encoding and Component Transition Information","abstract":"We have developed novel algorithms for optimizing nonsmooth, nonconvex functions in which the nonsmoothness is caused by nonsmooth operators presented in the analytical form of the objective. The algorithms are based on encoding the active branch of each nonsmooth operator such that the active smooth component function and its code can be extracted at any given point, and the transition of the solution from one smooth piece to another can be detected via tracking the change of active branches of all the operators. This mechanism enables the possibility of collecting the information about the sequence of active component functions encountered in the previous iterations (i.e., the component transition information), and using it in the construction of a current local model or identification of a descent direction in a very economic and effective manner. Based on this novel idea, we have developed a trust-region method and a joint gradient descent method driven by the component information for optimizing the encodable piecewise-smooth, nonconvex functions. It has further been shown that the joint gradient descent method using a technique called proactive component function accessing can achieve a linear rate of convergence if a so-called multi-component Polyak-Lojasiewicz inequality and some other regularity conditions hold at a neighborhood of a local minimizer.","sentences":["We have developed novel algorithms for optimizing nonsmooth, nonconvex functions in which the nonsmoothness is caused by nonsmooth operators presented in the analytical form of the objective.","The algorithms are based on encoding the active branch of each nonsmooth operator such that the active smooth component function and its code can be extracted at any given point, and the transition of the solution from one smooth piece to another can be detected via tracking the change of active branches of all the operators.","This mechanism enables the possibility of collecting the information about the sequence of active component functions encountered in the previous iterations (i.e., the component transition information), and using it in the construction of a current local model or identification of a descent direction in a very economic and effective manner.","Based on this novel idea, we have developed a trust-region method and a joint gradient descent method driven by the component information for optimizing the encodable piecewise-smooth, nonconvex functions.","It has further been shown that the joint gradient descent method using a technique called proactive component function accessing can achieve a linear rate of convergence if a so-called multi-component Polyak-Lojasiewicz inequality and some other regularity conditions hold at a neighborhood of a local minimizer."],"url":"http://arxiv.org/abs/2404.16273v1","category":"math.OC"}
{"created":"2024-04-25 00:10:25","title":"OmniSearchSage: Multi-Task Multi-Entity Embeddings for Pinterest Search","abstract":"In this paper, we present OmniSearchSage, a versatile and scalable system for understanding search queries, pins, and products for Pinterest search. We jointly learn a unified query embedding coupled with pin and product embeddings, leading to an improvement of $>8\\%$ relevance, $>7\\%$ engagement, and $>5\\%$ ads CTR in Pinterest's production search system. The main contributors to these gains are improved content understanding, better multi-task learning, and real-time serving. We enrich our entity representations using diverse text derived from image captions from a generative LLM, historical engagement, and user-curated boards. Our multitask learning setup produces a single search query embedding in the same space as pin and product embeddings and compatible with pre-existing pin and product embeddings. We show the value of each feature through ablation studies, and show the effectiveness of a unified model compared to standalone counterparts. Finally, we share how these embeddings have been deployed across the Pinterest search stack, from retrieval to ranking, scaling to serve $300k$ requests per second at low latency. Our implementation of this work is available at https://github.com/pinterest/atg-research/tree/main/omnisearchsage.","sentences":["In this paper, we present OmniSearchSage, a versatile and scalable system for understanding search queries, pins, and products for Pinterest search.","We jointly learn a unified query embedding coupled with pin and product embeddings, leading to an improvement of $>8\\%$ relevance, $>7\\%$ engagement, and $>5\\%$ ads CTR in Pinterest's production search system.","The main contributors to these gains are improved content understanding, better multi-task learning, and real-time serving.","We enrich our entity representations using diverse text derived from image captions from a generative LLM, historical engagement, and user-curated boards.","Our multitask learning setup produces a single search query embedding in the same space as pin and product embeddings and compatible with pre-existing pin and product embeddings.","We show the value of each feature through ablation studies, and show the effectiveness of a unified model compared to standalone counterparts.","Finally, we share how these embeddings have been deployed across the Pinterest search stack, from retrieval to ranking, scaling to serve $300k$ requests per second at low latency.","Our implementation of this work is available at https://github.com/pinterest/atg-research/tree/main/omnisearchsage."],"url":"http://arxiv.org/abs/2404.16260v1","category":"cs.IR"}
{"created":"2024-04-25 00:06:28","title":"An Experiment with Electric Guitar Signals for Exploring the Virtuosity based on the Entropy of Music","abstract":"We analyze the concept of virtuosity as a collective attribute in music and its relationship with the entropy based on an experiment that compares two sets of digital signals played by composer-performer electric guitarists. Based on an interdisciplinary approach related to the complex systems, we computed the spectrum of signals, identified statistical distributions that best describe them, and measured the Shannon entropy to establish their diversity. Findings suggested that virtuosity might be related to a range of entropy values that identify levels of diversity of the frequency components of audio signals. Despite the presence of different values of entropy in the two sets of signals, they are statistically similar. Therefore, entropy values can be interpreted as levels of virtuosity in music.","sentences":["We analyze the concept of virtuosity as a collective attribute in music and its relationship with the entropy based on an experiment that compares two sets of digital signals played by composer-performer electric guitarists.","Based on an interdisciplinary approach related to the complex systems, we computed the spectrum of signals, identified statistical distributions that best describe them, and measured the Shannon entropy to establish their diversity.","Findings suggested that virtuosity might be related to a range of entropy values that identify levels of diversity of the frequency components of audio signals.","Despite the presence of different values of entropy in the two sets of signals, they are statistically similar.","Therefore, entropy values can be interpreted as levels of virtuosity in music."],"url":"http://arxiv.org/abs/2404.16259v1","category":"cs.SD"}
{"created":"2024-04-25 00:05:19","title":"Translation of Multifaceted Data without Re-Training of Machine Translation Systems","abstract":"Translating major language resources to build minor language resources becomes a widely-used approach. Particularly in translating complex data points composed of multiple components, it is common to translate each component separately. However, we argue that this practice often overlooks the interrelation between components within the same data point. To address this limitation, we propose a novel MT pipeline that considers the intra-data relation in implementing MT for training data. In our MT pipeline, all the components in a data point are concatenated to form a single translation sequence and subsequently reconstructed to the data components after translation. We introduce a Catalyst Statement (CS) to enhance the intra-data relation, and Indicator Token (IT) to assist the decomposition of a translated sequence into its respective data components. Through our approach, we have achieved a considerable improvement in translation quality itself, along with its effectiveness as training data. Compared with the conventional approach that translates each data component separately, our method yields better training data that enhances the performance of the trained model by 2.690 points for the web page ranking (WPR) task, and 0.845 for the question generation (QG) task in the XGLUE benchmark.","sentences":["Translating major language resources to build minor language resources becomes a widely-used approach.","Particularly in translating complex data points composed of multiple components, it is common to translate each component separately.","However, we argue that this practice often overlooks the interrelation between components within the same data point.","To address this limitation, we propose a novel MT pipeline that considers the intra-data relation in implementing MT for training data.","In our MT pipeline, all the components in a data point are concatenated to form a single translation sequence and subsequently reconstructed to the data components after translation.","We introduce a Catalyst Statement (CS) to enhance the intra-data relation, and Indicator Token (IT) to assist the decomposition of a translated sequence into its respective data components.","Through our approach, we have achieved a considerable improvement in translation quality itself, along with its effectiveness as training data.","Compared with the conventional approach that translates each data component separately, our method yields better training data that enhances the performance of the trained model by 2.690 points for the web page ranking (WPR) task, and 0.845 for the question generation (QG) task in the XGLUE benchmark."],"url":"http://arxiv.org/abs/2404.16257v1","category":"cs.CL"}
{"created":"2024-04-24 23:49:23","title":"Mitigating Automotive Radar Interference using Onboard Intelligent Reflective Surface","abstract":"The use of automotive radars is gaining popularity as a means to enhance a vehicle's sensing capabilities. However, these radars can suffer from interference caused by transmissions from other radars mounted on nearby vehicles. To address this issue, we investigate the use of an onboard intelligent reflective surface (IRS) to artificially increase a vehicle's effective radar cross section (RCS), or its \"electromagnetic visibility.\" Our proposed method utilizes the IRS's ability to form a coherent reflection of the incident radar waveform back towards the source radar, thereby improving radar performance under interference. We evaluated both passive and active IRS options. Passive IRS, which does not support reflection amplification, was found to be counter-productive and actually decreased the vehicle's effective RCS instead of enhancing it. In contrast, active IRS, which can amplify the reflection power of individual elements, effectively combats all types of automotive radar interference when the reflective elements are configured with a 15-35 dB reflection gain.","sentences":["The use of automotive radars is gaining popularity as a means to enhance a vehicle's sensing capabilities.","However, these radars can suffer from interference caused by transmissions from other radars mounted on nearby vehicles.","To address this issue, we investigate the use of an onboard intelligent reflective surface (IRS) to artificially increase a vehicle's effective radar cross section (RCS), or its \"electromagnetic visibility.\"","Our proposed method utilizes the IRS's ability to form a coherent reflection of the incident radar waveform back towards the source radar, thereby improving radar performance under interference.","We evaluated both passive and active IRS options.","Passive IRS, which does not support reflection amplification, was found to be counter-productive and actually decreased the vehicle's effective RCS instead of enhancing it.","In contrast, active IRS, which can amplify the reflection power of individual elements, effectively combats all types of automotive radar interference when the reflective elements are configured with a 15-35 dB reflection gain."],"url":"http://arxiv.org/abs/2404.16253v1","category":"eess.SP"}
{"created":"2024-04-24 23:39:58","title":"Investigating the prompt leakage effect and black-box defenses for multi-turn LLM interactions","abstract":"Prompt leakage in large language models (LLMs) poses a significant security and privacy threat, particularly in retrieval-augmented generation (RAG) systems. However, leakage in multi-turn LLM interactions along with mitigation strategies has not been studied in a standardized manner. This paper investigates LLM vulnerabilities against prompt leakage across 4 diverse domains and 10 closed- and open-source LLMs. Our unique multi-turn threat model leverages the LLM's sycophancy effect and our analysis dissects task instruction and knowledge leakage in the LLM response. In a multi-turn setting, our threat model elevates the average attack success rate (ASR) to 86.2%, including a 99% leakage with GPT-4 and claude-1.3. We find that some black-box LLMs like Gemini show variable susceptibility to leakage across domains - they are more likely to leak contextual knowledge in the news domain compared to the medical domain. Our experiments measure specific effects of 6 black-box defense strategies, including a query-rewriter in the RAG scenario. Our proposed multi-tier combination of defenses still has an ASR of 5.3% for black-box LLMs, indicating room for enhancement and future direction for LLM security research.","sentences":["Prompt leakage in large language models (LLMs) poses a significant security and privacy threat, particularly in retrieval-augmented generation (RAG) systems.","However, leakage in multi-turn LLM interactions along with mitigation strategies has not been studied in a standardized manner.","This paper investigates LLM vulnerabilities against prompt leakage across 4 diverse domains and 10 closed- and open-source LLMs.","Our unique multi-turn threat model leverages the LLM's sycophancy effect and our analysis dissects task instruction and knowledge leakage in the LLM response.","In a multi-turn setting, our threat model elevates the average attack success rate (ASR) to 86.2%, including a 99% leakage with GPT-4 and claude-1.3.","We find that some black-box LLMs like Gemini show variable susceptibility to leakage across domains - they are more likely to leak contextual knowledge in the news domain compared to the medical domain.","Our experiments measure specific effects of 6 black-box defense strategies, including a query-rewriter in the RAG scenario.","Our proposed multi-tier combination of defenses still has an ASR of 5.3% for black-box LLMs, indicating room for enhancement and future direction for LLM security research."],"url":"http://arxiv.org/abs/2404.16251v1","category":"cs.CR"}
{"created":"2024-04-24 23:37:15","title":"URL: Universal Referential Knowledge Linking via Task-instructed Representation Compression","abstract":"Linking a claim to grounded references is a critical ability to fulfill human demands for authentic and reliable information. Current studies are limited to specific tasks like information retrieval or semantic matching, where the claim-reference relationships are unique and fixed, while the referential knowledge linking (RKL) in real-world can be much more diverse and complex. In this paper, we propose universal referential knowledge linking (URL), which aims to resolve diversified referential knowledge linking tasks by one unified model. To this end, we propose a LLM-driven task-instructed representation compression, as well as a multi-view learning approach, in order to effectively adapt the instruction following and semantic understanding abilities of LLMs to referential knowledge linking. Furthermore, we also construct a new benchmark to evaluate ability of models on referential knowledge linking tasks across different scenarios. Experiments demonstrate that universal RKL is challenging for existing approaches, while the proposed framework can effectively resolve the task across various scenarios, and therefore outperforms previous approaches by a large margin.","sentences":["Linking a claim to grounded references is a critical ability to fulfill human demands for authentic and reliable information.","Current studies are limited to specific tasks like information retrieval or semantic matching, where the claim-reference relationships are unique and fixed, while the referential knowledge linking (RKL) in real-world can be much more diverse and complex.","In this paper, we propose universal referential knowledge linking (URL), which aims to resolve diversified referential knowledge linking tasks by one unified model.","To this end, we propose a LLM-driven task-instructed representation compression, as well as a multi-view learning approach, in order to effectively adapt the instruction following and semantic understanding abilities of LLMs to referential knowledge linking.","Furthermore, we also construct a new benchmark to evaluate ability of models on referential knowledge linking tasks across different scenarios.","Experiments demonstrate that universal RKL is challenging for existing approaches, while the proposed framework can effectively resolve the task across various scenarios, and therefore outperforms previous approaches by a large margin."],"url":"http://arxiv.org/abs/2404.16248v1","category":"cs.CL"}
{"created":"2024-04-24 23:27:45","title":"An Analog of the Rothe Method for Some Ill-Posed Problems for Parabolic Equations","abstract":"The classical method of Rothe proves existence and uniqueness theorems for initial boundary value problems for parabolic equations using the explicit finite difference scheme with respect to time. In this method, an elliptic boundary value problem is investigated on each time step. On the other hand, time dependent experimental data are always collected on discrete time grids, and the grid step size cannot be arranged to be infinitely small. The same is true for numerical studies. Therefore, it makes an applied sense to consider both unique continuation problems and coefficient inverse problems for parabolic equations, which are written in the form of finite differences with respect to time and without allowing the grid step size to tend to zero. This leads to a boundary value problem for a coupled system of elliptic equations with both Dirichlet and Neumann boundary data, which is somewhat similar to the Rothe's method. Dissimilarities are named as well. Two long standing open questions are addressed within this framework. A specific applied example of monitoring epidemics is discussed. In particular, a numerical method for this problem is constructed and its global convergence analysis is provided.","sentences":["The classical method of Rothe proves existence and uniqueness theorems for initial boundary value problems for parabolic equations using the explicit finite difference scheme with respect to time.","In this method, an elliptic boundary value problem is investigated on each time step.","On the other hand, time dependent experimental data are always collected on discrete time grids, and the grid step size cannot be arranged to be infinitely small.","The same is true for numerical studies.","Therefore, it makes an applied sense to consider both unique continuation problems and coefficient inverse problems for parabolic equations, which are written in the form of finite differences with respect to time and without allowing the grid step size to tend to zero.","This leads to a boundary value problem for a coupled system of elliptic equations with both Dirichlet and Neumann boundary data, which is somewhat similar to the Rothe's method.","Dissimilarities are named as well.","Two long standing open questions are addressed within this framework.","A specific applied example of monitoring epidemics is discussed.","In particular, a numerical method for this problem is constructed and its global convergence analysis is provided."],"url":"http://arxiv.org/abs/2404.16246v1","category":"math-ph"}
{"created":"2024-04-24 22:56:26","title":"A communication protocol based on NK boolean networks for coordinating collective action","abstract":"In this paper, I describe a digital social communication protocol (Gridt) based on Kauffman's NK boolean networks. The main assertion is that a communication network with this topology supports infinitely scalable self-organization of collective action without requiring hierarchy or central control. The paper presents the functionality of this protocol and substantiates the following propositions about its function and implications: (1) Communication via NK boolean networks facilitates coordination on collective action games for any variable number of users, and justifies the assumption that the game's payoff structure is common knowledge; (2) Use of this protocol increases its users' transfer empowerment, a form of intrinsic motivation that motivates coordinated action independent of the task or outcome; (3) Communication via this network can be considered 'cheap talk' and benefits the strategy of players with aligned interests, but not of players with conflicting interests; (4) Absence of significant barriers for its realization warrants a timely and continuing discussion on the ethics and implications of this technology; (5) Full realization of the technology's potential calls for a free-to-use service with maximal transparency of design and associated economic incentives.","sentences":["In this paper, I describe a digital social communication protocol (Gridt) based on Kauffman's NK boolean networks.","The main assertion is that a communication network with this topology supports infinitely scalable self-organization of collective action without requiring hierarchy or central control.","The paper presents the functionality of this protocol and substantiates the following propositions about its function and implications: (1) Communication via NK boolean networks facilitates coordination on collective action games for any variable number of users, and justifies the assumption that the game's payoff structure is common knowledge; (2) Use of this protocol increases its users' transfer empowerment, a form of intrinsic motivation that motivates coordinated action independent of the task or outcome; (3) Communication via this network can be considered 'cheap talk' and benefits the strategy of players with aligned interests, but not of players with conflicting interests; (4) Absence of significant barriers for its realization warrants a timely and continuing discussion on the ethics and implications of this technology; (5) Full realization of the technology's potential calls for a free-to-use service with maximal transparency of design and associated economic incentives."],"url":"http://arxiv.org/abs/2404.16240v1","category":"cs.SI"}
{"created":"2024-04-24 22:28:12","title":"AutoGluon-Multimodal (AutoMM): Supercharging Multimodal AutoML with Foundation Models","abstract":"AutoGluon-Multimodal (AutoMM) is introduced as an open-source AutoML library designed specifically for multimodal learning. Distinguished by its exceptional ease of use, AutoMM enables fine-tuning of foundational models with just three lines of code. Supporting various modalities including image, text, and tabular data, both independently and in combination, the library offers a comprehensive suite of functionalities spanning classification, regression, object detection, semantic matching, and image segmentation. Experiments across diverse datasets and tasks showcases AutoMM's superior performance in basic classification and regression tasks compared to existing AutoML tools, while also demonstrating competitive results in advanced tasks, aligning with specialized toolboxes designed for such purposes.","sentences":["AutoGluon-Multimodal (AutoMM) is introduced as an open-source AutoML library designed specifically for multimodal learning.","Distinguished by its exceptional ease of use, AutoMM enables fine-tuning of foundational models with just three lines of code.","Supporting various modalities including image, text, and tabular data, both independently and in combination, the library offers a comprehensive suite of functionalities spanning classification, regression, object detection, semantic matching, and image segmentation.","Experiments across diverse datasets and tasks showcases AutoMM's superior performance in basic classification and regression tasks compared to existing AutoML tools, while also demonstrating competitive results in advanced tasks, aligning with specialized toolboxes designed for such purposes."],"url":"http://arxiv.org/abs/2404.16233v1","category":"cs.LG"}
{"created":"2024-04-24 21:33:17","title":"Efficient NAS with FaDE on Hierarchical Spaces","abstract":"Neural architecture search (NAS) is a challenging problem. Hierarchical search spaces allow for cheap evaluations of neural network sub modules to serve as surrogate for architecture evaluations. Yet, sometimes the hierarchy is too restrictive or the surrogate fails to generalize. We present FaDE which uses differentiable architecture search to obtain relative performance predictions on finite regions of a hierarchical NAS space. The relative nature of these ranks calls for a memory-less, batch-wise outer search algorithm for which we use an evolutionary algorithm with pseudo-gradient descent. FaDE is especially suited on deep hierarchical, respectively multi-cell search spaces, which it can explore by linear instead of exponential cost and therefore eliminates the need for a proxy search space.   Our experiments show that firstly, FaDE-ranks on finite regions of the search space correlate with corresponding architecture performances and secondly, the ranks can empower a pseudo-gradient evolutionary search on the complete neural architecture search space.","sentences":["Neural architecture search (NAS) is a challenging problem.","Hierarchical search spaces allow for cheap evaluations of neural network sub modules to serve as surrogate for architecture evaluations.","Yet, sometimes the hierarchy is too restrictive or the surrogate fails to generalize.","We present FaDE which uses differentiable architecture search to obtain relative performance predictions on finite regions of a hierarchical NAS space.","The relative nature of these ranks calls for a memory-less, batch-wise outer search algorithm for which we use an evolutionary algorithm with pseudo-gradient descent.","FaDE is especially suited on deep hierarchical, respectively multi-cell search spaces, which it can explore by linear instead of exponential cost and therefore eliminates the need for a proxy search space.   ","Our experiments show that firstly, FaDE-ranks on finite regions of the search space correlate with corresponding architecture performances and secondly, the ranks can empower a pseudo-gradient evolutionary search on the complete neural architecture search space."],"url":"http://arxiv.org/abs/2404.16218v1","category":"cs.NE"}
{"created":"2024-04-24 21:30:01","title":"ActiveRIR: Active Audio-Visual Exploration for Acoustic Environment Modeling","abstract":"An environment acoustic model represents how sound is transformed by the physical characteristics of an indoor environment, for any given source/receiver location. Traditional methods for constructing acoustic models involve expensive and time-consuming collection of large quantities of acoustic data at dense spatial locations in the space, or rely on privileged knowledge of scene geometry to intelligently select acoustic data sampling locations. We propose active acoustic sampling, a new task for efficiently building an environment acoustic model of an unmapped environment in which a mobile agent equipped with visual and acoustic sensors jointly constructs the environment acoustic model and the occupancy map on-the-fly. We introduce ActiveRIR, a reinforcement learning (RL) policy that leverages information from audio-visual sensor streams to guide agent navigation and determine optimal acoustic data sampling positions, yielding a high quality acoustic model of the environment from a minimal set of acoustic samples. We train our policy with a novel RL reward based on information gain in the environment acoustic model. Evaluating on diverse unseen indoor environments from a state-of-the-art acoustic simulation platform, ActiveRIR outperforms an array of methods--both traditional navigation agents based on spatial novelty and visual exploration as well as existing state-of-the-art methods.","sentences":["An environment acoustic model represents how sound is transformed by the physical characteristics of an indoor environment, for any given source/receiver location.","Traditional methods for constructing acoustic models involve expensive and time-consuming collection of large quantities of acoustic data at dense spatial locations in the space, or rely on privileged knowledge of scene geometry to intelligently select acoustic data sampling locations.","We propose active acoustic sampling, a new task for efficiently building an environment acoustic model of an unmapped environment in which a mobile agent equipped with visual and acoustic sensors jointly constructs the environment acoustic model and the occupancy map on-the-fly.","We introduce ActiveRIR, a reinforcement learning (RL) policy that leverages information from audio-visual sensor streams to guide agent navigation and determine optimal acoustic data sampling positions, yielding a high quality acoustic model of the environment from a minimal set of acoustic samples.","We train our policy with a novel RL reward based on information gain in the environment acoustic model.","Evaluating on diverse unseen indoor environments from a state-of-the-art acoustic simulation platform, ActiveRIR outperforms an array of methods--both traditional navigation agents based on spatial novelty and visual exploration as well as existing state-of-the-art methods."],"url":"http://arxiv.org/abs/2404.16216v1","category":"cs.CV"}
{"created":"2024-04-24 21:08:21","title":"GPU-RANC: A CUDA Accelerated Simulation Framework for Neuromorphic Architectures","abstract":"Open-source simulation tools play a crucial role for neuromorphic application engineers and hardware architects to investigate performance bottlenecks and explore design optimizations before committing to silicon. Reconfigurable Architecture for Neuromorphic Computing (RANC) is one such tool that offers ability to execute pre-trained Spiking Neural Network (SNN) models within a unified ecosystem through both software-based simulation and FPGA-based emulation. RANC has been utilized by the community with its flexible and highly parameterized design to study implementation bottlenecks, tune architectural parameters or modify neuron behavior based on application insights and study the trade space on hardware performance and network accuracy. In designing architectures for use in neuromorphic computing, there are an incredibly large number of configuration parameters such as number and precision of weights per neuron, neuron and axon counts per core, network topology, and neuron behavior. To accelerate such studies and provide users with a streamlined productive design space exploration, in this paper we introduce the GPU-based implementation of RANC. We summarize our parallelization approach and quantify the speedup gains achieved with GPU-based tick-accurate simulations across various use cases. We demonstrate up to 780 times speedup compared to serial version of the RANC simulator based on a 512 neuromorphic core MNIST inference application. We believe that the RANC ecosystem now provides a much more feasible avenue in the research of exploring different optimizations for accelerating SNNs and performing richer studies by enabling rapid convergence to optimized neuromorphic architectures.","sentences":["Open-source simulation tools play a crucial role for neuromorphic application engineers and hardware architects to investigate performance bottlenecks and explore design optimizations before committing to silicon.","Reconfigurable Architecture for Neuromorphic Computing (RANC) is one such tool that offers ability to execute pre-trained Spiking Neural Network (SNN) models within a unified ecosystem through both software-based simulation and FPGA-based emulation.","RANC has been utilized by the community with its flexible and highly parameterized design to study implementation bottlenecks, tune architectural parameters or modify neuron behavior based on application insights and study the trade space on hardware performance and network accuracy.","In designing architectures for use in neuromorphic computing, there are an incredibly large number of configuration parameters such as number and precision of weights per neuron, neuron and axon counts per core, network topology, and neuron behavior.","To accelerate such studies and provide users with a streamlined productive design space exploration, in this paper we introduce the GPU-based implementation of RANC.","We summarize our parallelization approach and quantify the speedup gains achieved with GPU-based tick-accurate simulations across various use cases.","We demonstrate up to 780 times speedup compared to serial version of the RANC simulator based on a 512 neuromorphic core MNIST inference application.","We believe that the RANC ecosystem now provides a much more feasible avenue in the research of exploring different optimizations for accelerating SNNs and performing richer studies by enabling rapid convergence to optimized neuromorphic architectures."],"url":"http://arxiv.org/abs/2404.16208v1","category":"cs.ET"}
{"created":"2024-04-24 21:04:14","title":"Knowledge Graph Completion using Structural and Textual Embeddings","abstract":"Knowledge Graphs (KGs) are widely employed in artificial intelligence applications, such as question-answering and recommendation systems. However, KGs are frequently found to be incomplete. While much of the existing literature focuses on predicting missing nodes for given incomplete KG triples, there remains an opportunity to complete KGs by exploring relations between existing nodes, a task known as relation prediction. In this study, we propose a relations prediction model that harnesses both textual and structural information within KGs. Our approach integrates walks-based embeddings with language model embeddings to effectively represent nodes. We demonstrate that our model achieves competitive results in the relation prediction task when evaluated on a widely used dataset.","sentences":["Knowledge Graphs (KGs) are widely employed in artificial intelligence applications, such as question-answering and recommendation systems.","However, KGs are frequently found to be incomplete.","While much of the existing literature focuses on predicting missing nodes for given incomplete KG triples, there remains an opportunity to complete KGs by exploring relations between existing nodes, a task known as relation prediction.","In this study, we propose a relations prediction model that harnesses both textual and structural information within KGs.","Our approach integrates walks-based embeddings with language model embeddings to effectively represent nodes.","We demonstrate that our model achieves competitive results in the relation prediction task when evaluated on a widely used dataset."],"url":"http://arxiv.org/abs/2404.16206v1","category":"cs.AI"}
{"created":"2024-04-24 20:42:28","title":"Towards Efficient Patient Recruitment for Clinical Trials: Application of a Prompt-Based Learning Model","abstract":"Objective: Clinical trials are essential for advancing pharmaceutical interventions, but they face a bottleneck in selecting eligible participants. Although leveraging electronic health records (EHR) for recruitment has gained popularity, the complex nature of unstructured medical texts presents challenges in efficiently identifying participants. Natural Language Processing (NLP) techniques have emerged as a solution with a recent focus on transformer models. In this study, we aimed to evaluate the performance of a prompt-based large language model for the cohort selection task from unstructured medical notes collected in the EHR. Methods: To process the medical records, we selected the most related sentences of the records to the eligibility criteria needed for the trial. The SNOMED CT concepts related to each eligibility criterion were collected. Medical records were also annotated with MedCAT based on the SNOMED CT ontology. Annotated sentences including concepts matched with the criteria-relevant terms were extracted. A prompt-based large language model (Generative Pre-trained Transformer (GPT) in this study) was then used with the extracted sentences as the training set. To assess its effectiveness, we evaluated the model's performance using the dataset from the 2018 n2c2 challenge, which aimed to classify medical records of 311 patients based on 13 eligibility criteria through NLP techniques. Results: Our proposed model showed the overall micro and macro F measures of 0.9061 and 0.8060 which were among the highest scores achieved by the experiments performed with this dataset. Conclusion: The application of a prompt-based large language model in this study to classify patients based on eligibility criteria received promising scores. Besides, we proposed a method of extractive summarization with the aid of SNOMED CT ontology that can be also applied to other medical texts.","sentences":["Objective: Clinical trials are essential for advancing pharmaceutical interventions, but they face a bottleneck in selecting eligible participants.","Although leveraging electronic health records (EHR) for recruitment has gained popularity, the complex nature of unstructured medical texts presents challenges in efficiently identifying participants.","Natural Language Processing (NLP) techniques have emerged as a solution with a recent focus on transformer models.","In this study, we aimed to evaluate the performance of a prompt-based large language model for the cohort selection task from unstructured medical notes collected in the EHR.","Methods: To process the medical records, we selected the most related sentences of the records to the eligibility criteria needed for the trial.","The SNOMED CT concepts related to each eligibility criterion were collected.","Medical records were also annotated with MedCAT based on the SNOMED CT ontology.","Annotated sentences including concepts matched with the criteria-relevant terms were extracted.","A prompt-based large language model (Generative Pre-trained Transformer (GPT) in this study) was then used with the extracted sentences as the training set.","To assess its effectiveness, we evaluated the model's performance using the dataset from the 2018 n2c2 challenge, which aimed to classify medical records of 311 patients based on 13 eligibility criteria through NLP techniques.","Results: Our proposed model showed the overall micro and macro F measures of 0.9061 and 0.8060 which were among the highest scores achieved by the experiments performed with this dataset.","Conclusion: The application of a prompt-based large language model in this study to classify patients based on eligibility criteria received promising scores.","Besides, we proposed a method of extractive summarization with the aid of SNOMED CT ontology that can be also applied to other medical texts."],"url":"http://arxiv.org/abs/2404.16198v1","category":"cs.CL"}
{"created":"2024-04-24 20:35:17","title":"ApisTox: a new benchmark dataset for the classification of small molecules toxicity on honey bees","abstract":"The global decline in bee populations poses significant risks to agriculture, biodiversity, and environmental stability. To bridge the gap in existing data, we introduce ApisTox, a comprehensive dataset focusing on the toxicity of pesticides to honey bees (Apis mellifera). This dataset combines and leverages data from existing sources such as ECOTOX and PPDB, providing an extensive, consistent, and curated collection that surpasses the previous datasets. ApisTox incorporates a wide array of data, including toxicity levels for chemicals, details such as time of their publication in literature, and identifiers linking them to external chemical databases. This dataset may serve as an important tool for environmental and agricultural research, but also can support the development of policies and practices aimed at minimizing harm to bee populations. Finally, ApisTox offers a unique resource for benchmarking molecular property prediction methods on agrochemical compounds, facilitating advancements in both environmental science and cheminformatics. This makes it a valuable tool for both academic research and practical applications in bee conservation.","sentences":["The global decline in bee populations poses significant risks to agriculture, biodiversity, and environmental stability.","To bridge the gap in existing data, we introduce ApisTox, a comprehensive dataset focusing on the toxicity of pesticides to honey bees (Apis mellifera).","This dataset combines and leverages data from existing sources such as ECOTOX and PPDB, providing an extensive, consistent, and curated collection that surpasses the previous datasets.","ApisTox incorporates a wide array of data, including toxicity levels for chemicals, details such as time of their publication in literature, and identifiers linking them to external chemical databases.","This dataset may serve as an important tool for environmental and agricultural research, but also can support the development of policies and practices aimed at minimizing harm to bee populations.","Finally, ApisTox offers a unique resource for benchmarking molecular property prediction methods on agrochemical compounds, facilitating advancements in both environmental science and cheminformatics.","This makes it a valuable tool for both academic research and practical applications in bee conservation."],"url":"http://arxiv.org/abs/2404.16196v1","category":"q-bio.QM"}
{"created":"2024-04-24 20:34:27","title":"A Game-Theoretic Analysis of Auditing Differentially Private Algorithms with Epistemically Disparate Herd","abstract":"Privacy-preserving AI algorithms are widely adopted in various domains, but the lack of transparency might pose accountability issues. While auditing algorithms can address this issue, machine-based audit approaches are often costly and time-consuming. Herd audit, on the other hand, offers an alternative solution by harnessing collective intelligence. Nevertheless, the presence of epistemic disparity among auditors, resulting in varying levels of expertise and access to knowledge, may impact audit performance. An effective herd audit will establish a credible accountability threat for algorithm developers, incentivizing them to uphold their claims. In this study, our objective is to develop a systematic framework that examines the impact of herd audits on algorithm developers using the Stackelberg game approach. The optimal strategy for auditors emphasizes the importance of easy access to relevant information, as it increases the auditors' confidence in the audit process. Similarly, the optimal choice for developers suggests that herd audit is viable when auditors face lower costs in acquiring knowledge. By enhancing transparency and accountability, herd audit contributes to the responsible development of privacy-preserving algorithms.","sentences":["Privacy-preserving AI algorithms are widely adopted in various domains, but the lack of transparency might pose accountability issues.","While auditing algorithms can address this issue, machine-based audit approaches are often costly and time-consuming.","Herd audit, on the other hand, offers an alternative solution by harnessing collective intelligence.","Nevertheless, the presence of epistemic disparity among auditors, resulting in varying levels of expertise and access to knowledge, may impact audit performance.","An effective herd audit will establish a credible accountability threat for algorithm developers, incentivizing them to uphold their claims.","In this study, our objective is to develop a systematic framework that examines the impact of herd audits on algorithm developers using the Stackelberg game approach.","The optimal strategy for auditors emphasizes the importance of easy access to relevant information, as it increases the auditors' confidence in the audit process.","Similarly, the optimal choice for developers suggests that herd audit is viable when auditors face lower costs in acquiring knowledge.","By enhancing transparency and accountability, herd audit contributes to the responsible development of privacy-preserving algorithms."],"url":"http://arxiv.org/abs/2404.16195v1","category":"cs.CR"}
{"created":"2024-04-24 20:33:25","title":"Improving Multi-label Recognition using Class Co-Occurrence Probabilities","abstract":"Multi-label Recognition (MLR) involves the identification of multiple objects within an image. To address the additional complexity of this problem, recent works have leveraged information from vision-language models (VLMs) trained on large text-images datasets for the task. These methods learn an independent classifier for each object (class), overlooking correlations in their occurrences. Such co-occurrences can be captured from the training data as conditional probabilities between a pair of classes. We propose a framework to extend the independent classifiers by incorporating the co-occurrence information for object pairs to improve the performance of independent classifiers. We use a Graph Convolutional Network (GCN) to enforce the conditional probabilities between classes, by refining the initial estimates derived from image and text sources obtained using VLMs. We validate our method on four MLR datasets, where our approach outperforms all state-of-the-art methods.","sentences":["Multi-label Recognition (MLR) involves the identification of multiple objects within an image.","To address the additional complexity of this problem, recent works have leveraged information from vision-language models (VLMs) trained on large text-images datasets for the task.","These methods learn an independent classifier for each object (class), overlooking correlations in their occurrences.","Such co-occurrences can be captured from the training data as conditional probabilities between a pair of classes.","We propose a framework to extend the independent classifiers by incorporating the co-occurrence information for object pairs to improve the performance of independent classifiers.","We use a Graph Convolutional Network (GCN) to enforce the conditional probabilities between classes, by refining the initial estimates derived from image and text sources obtained using VLMs.","We validate our method on four MLR datasets, where our approach outperforms all state-of-the-art methods."],"url":"http://arxiv.org/abs/2404.16193v1","category":"cs.CV"}
{"created":"2024-04-24 20:22:48","title":"Pearls from Pebbles: Improved Confidence Functions for Auto-labeling","abstract":"Auto-labeling is an important family of techniques that produce labeled training sets with minimum manual labeling. A prominent variant, threshold-based auto-labeling (TBAL), works by finding a threshold on a model's confidence scores above which it can accurately label unlabeled data points. However, many models are known to produce overconfident scores, leading to poor TBAL performance. While a natural idea is to apply off-the-shelf calibration methods to alleviate the overconfidence issue, such methods still fall short. Rather than experimenting with ad-hoc choices of confidence functions, we propose a framework for studying the \\emph{optimal} TBAL confidence function. We develop a tractable version of the framework to obtain \\texttt{Colander} (Confidence functions for Efficient and Reliable Auto-labeling), a new post-hoc method specifically designed to maximize performance in TBAL systems. We perform an extensive empirical evaluation of our method \\texttt{Colander} and compare it against methods designed for calibration. \\texttt{Colander} achieves up to 60\\% improvements on coverage over the baselines while maintaining auto-labeling error below $5\\%$ and using the same amount of labeled data as the baselines.","sentences":["Auto-labeling is an important family of techniques that produce labeled training sets with minimum manual labeling.","A prominent variant, threshold-based auto-labeling (TBAL), works by finding a threshold on a model's confidence scores above which it can accurately label unlabeled data points.","However, many models are known to produce overconfident scores, leading to poor TBAL performance.","While a natural idea is to apply off-the-shelf calibration methods to alleviate the overconfidence issue, such methods still fall short.","Rather than experimenting with ad-hoc choices of confidence functions, we propose a framework for studying the \\emph{optimal} TBAL confidence function.","We develop a tractable version of the framework to obtain \\texttt{Colander} (Confidence functions for Efficient and Reliable Auto-labeling), a new post-hoc method specifically designed to maximize performance in TBAL systems.","We perform an extensive empirical evaluation of our method \\texttt{Colander} and compare it against methods designed for calibration.","\\texttt{Colander} achieves up to 60\\% improvements on coverage over the baselines while maintaining auto-labeling error below $5\\%$ and using the same amount of labeled data as the baselines."],"url":"http://arxiv.org/abs/2404.16188v1","category":"cs.LG"}
{"created":"2024-04-24 20:15:57","title":"ABCD: Trust enhanced Attention based Convolutional Autoencoder for Risk Assessment","abstract":"Anomaly detection in industrial systems is crucial for preventing equipment failures, ensuring risk identification, and maintaining overall system efficiency. Traditional monitoring methods often rely on fixed thresholds and empirical rules, which may not be sensitive enough to detect subtle changes in system health and predict impending failures. To address this limitation, this paper proposes, a novel Attention-based convolutional autoencoder (ABCD) for risk detection and map the risk value derive to the maintenance planning. ABCD learns the normal behavior of conductivity from historical data of a real-world industrial cooling system and reconstructs the input data, identifying anomalies that deviate from the expected patterns. The framework also employs calibration techniques to ensure the reliability of its predictions. Evaluation results demonstrate that with the attention mechanism in ABCD a 57.4% increase in performance and a reduction of false alarms by 9.37% is seen compared to without attention. The approach can effectively detect risks, the risk priority rank mapped to maintenance, providing valuable insights for cooling system designers and service personnel. Calibration error of 0.03% indicates that the model is well-calibrated and enhances model's trustworthiness, enabling informed decisions about maintenance strategies","sentences":["Anomaly detection in industrial systems is crucial for preventing equipment failures, ensuring risk identification, and maintaining overall system efficiency.","Traditional monitoring methods often rely on fixed thresholds and empirical rules, which may not be sensitive enough to detect subtle changes in system health and predict impending failures.","To address this limitation, this paper proposes, a novel Attention-based convolutional autoencoder (ABCD) for risk detection and map the risk value derive to the maintenance planning.","ABCD learns the normal behavior of conductivity from historical data of a real-world industrial cooling system and reconstructs the input data, identifying anomalies that deviate from the expected patterns.","The framework also employs calibration techniques to ensure the reliability of its predictions.","Evaluation results demonstrate that with the attention mechanism in ABCD a 57.4% increase in performance and a reduction of false alarms by 9.37% is seen compared to without attention.","The approach can effectively detect risks, the risk priority rank mapped to maintenance, providing valuable insights for cooling system designers and service personnel.","Calibration error of 0.03% indicates that the model is well-calibrated and enhances model's trustworthiness, enabling informed decisions about maintenance strategies"],"url":"http://arxiv.org/abs/2404.16183v1","category":"cs.LG"}
{"created":"2024-04-24 20:05:39","title":"Advancing Recommender Systems by mitigating Shilling attacks","abstract":"Considering the premise that the number of products offered grow in an exponential fashion and the amount of data that a user can assimilate before making a decision is relatively small, recommender systems help in categorizing content according to user preferences. Collaborative filtering is a widely used method for computing recommendations due to its good performance. But, this method makes the system vulnerable to attacks which try to bias the recommendations. These attacks, known as 'shilling attacks' are performed to push an item or nuke an item in the system. This paper proposes an algorithm to detect such shilling profiles in the system accurately and also study the effects of such profiles on the recommendations.","sentences":["Considering the premise that the number of products offered grow in an exponential fashion and the amount of data that a user can assimilate before making a decision is relatively small, recommender systems help in categorizing content according to user preferences.","Collaborative filtering is a widely used method for computing recommendations due to its good performance.","But, this method makes the system vulnerable to attacks which try to bias the recommendations.","These attacks, known as 'shilling attacks' are performed to push an item or nuke an item in the system.","This paper proposes an algorithm to detect such shilling profiles in the system accurately and also study the effects of such profiles on the recommendations."],"url":"http://arxiv.org/abs/2404.16177v1","category":"cs.IR"}
{"created":"2024-04-24 20:04:55","title":"MiMICRI: Towards Domain-centered Counterfactual Explanations of Cardiovascular Image Classification Models","abstract":"The recent prevalence of publicly accessible, large medical imaging datasets has led to a proliferation of artificial intelligence (AI) models for cardiovascular image classification and analysis. At the same time, the potentially significant impacts of these models have motivated the development of a range of explainable AI (XAI) methods that aim to explain model predictions given certain image inputs. However, many of these methods are not developed or evaluated with domain experts, and explanations are not contextualized in terms of medical expertise or domain knowledge. In this paper, we propose a novel framework and python library, MiMICRI, that provides domain-centered counterfactual explanations of cardiovascular image classification models. MiMICRI helps users interactively select and replace segments of medical images that correspond to morphological structures. From the counterfactuals generated, users can then assess the influence of each segment on model predictions, and validate the model against known medical facts. We evaluate this library with two medical experts. Our evaluation demonstrates that a domain-centered XAI approach can enhance the interpretability of model explanations, and help experts reason about models in terms of relevant domain knowledge. However, concerns were also surfaced about the clinical plausibility of the counterfactuals generated. We conclude with a discussion on the generalizability and trustworthiness of the MiMICRI framework, as well as the implications of our findings on the development of domain-centered XAI methods for model interpretability in healthcare contexts.","sentences":["The recent prevalence of publicly accessible, large medical imaging datasets has led to a proliferation of artificial intelligence (AI) models for cardiovascular image classification and analysis.","At the same time, the potentially significant impacts of these models have motivated the development of a range of explainable AI (XAI) methods that aim to explain model predictions given certain image inputs.","However, many of these methods are not developed or evaluated with domain experts, and explanations are not contextualized in terms of medical expertise or domain knowledge.","In this paper, we propose a novel framework and python library, MiMICRI, that provides domain-centered counterfactual explanations of cardiovascular image classification models.","MiMICRI helps users interactively select and replace segments of medical images that correspond to morphological structures.","From the counterfactuals generated, users can then assess the influence of each segment on model predictions, and validate the model against known medical facts.","We evaluate this library with two medical experts.","Our evaluation demonstrates that a domain-centered XAI approach can enhance the interpretability of model explanations, and help experts reason about models in terms of relevant domain knowledge.","However, concerns were also surfaced about the clinical plausibility of the counterfactuals generated.","We conclude with a discussion on the generalizability and trustworthiness of the MiMICRI framework, as well as the implications of our findings on the development of domain-centered XAI methods for model interpretability in healthcare contexts."],"url":"http://arxiv.org/abs/2404.16174v1","category":"cs.HC"}
{"created":"2024-04-24 19:55:50","title":"The Over-Certainty Phenomenon in Modern UDA Algorithms","abstract":"When neural networks are confronted with unfamiliar data that deviate from their training set, this signifies a domain shift. While these networks output predictions on their inputs, they typically fail to account for their level of familiarity with these novel observations. This challenge becomes even more pronounced in resource-constrained settings, such as embedded systems or edge devices. To address such challenges, we aim to recalibrate a neural network's decision boundaries in relation to its cognizance of the data it observes, introducing an approach we coin as certainty distillation. While prevailing works navigate unsupervised domain adaptation (UDA) with the goal of curtailing model entropy, they unintentionally birth models that grapple with calibration inaccuracies - a dilemma we term the over-certainty phenomenon. In this paper, we probe the drawbacks of this traditional learning model. As a solution to the issue, we propose a UDA algorithm that not only augments accuracy but also assures model calibration, all while maintaining suitability for environments with limited computational resources.","sentences":["When neural networks are confronted with unfamiliar data that deviate from their training set, this signifies a domain shift.","While these networks output predictions on their inputs, they typically fail to account for their level of familiarity with these novel observations.","This challenge becomes even more pronounced in resource-constrained settings, such as embedded systems or edge devices.","To address such challenges, we aim to recalibrate a neural network's decision boundaries in relation to its cognizance of the data it observes, introducing an approach we coin as certainty distillation.","While prevailing works navigate unsupervised domain adaptation (UDA) with the goal of curtailing model entropy, they unintentionally birth models that grapple with calibration inaccuracies - a dilemma we term the over-certainty phenomenon.","In this paper, we probe the drawbacks of this traditional learning model.","As a solution to the issue, we propose a UDA algorithm that not only augments accuracy but also assures model calibration, all while maintaining suitability for environments with limited computational resources."],"url":"http://arxiv.org/abs/2404.16168v1","category":"cs.LG"}
{"created":"2024-04-24 19:50:19","title":"Fast photon-mediated entanglement of continuously-cooled trapped ions for quantum networking","abstract":"We entangle two co-trapped atomic barium ion qubits by collecting single visible photons from each ion through in-vacuo 0.8 NA objectives, interfering them through an integrated fiber-beamsplitter and detecting them in coincidence. This projects the qubits into an entangled Bell state with an observed fidelity lower bound of F > 94%. We also introduce an ytterbium ion for sympathetic cooling to remove the need for recooling interruptions and achieve a continuous entanglement rate of 250 1/s.","sentences":["We entangle two co-trapped atomic barium ion qubits by collecting single visible photons from each ion through in-vacuo 0.8 NA objectives, interfering them through an integrated fiber-beamsplitter and detecting them in coincidence.","This projects the qubits into an entangled Bell state with an observed fidelity lower bound of F > 94%.","We also introduce an ytterbium ion for sympathetic cooling to remove the need for recooling interruptions and achieve a continuous entanglement rate of 250 1/s."],"url":"http://arxiv.org/abs/2404.16167v1","category":"quant-ph"}
{"created":"2024-04-24 19:40:01","title":"Towards a Holistic Evaluation of LLMs on Factual Knowledge Recall","abstract":"Large language models (LLMs) have shown remarkable performance on a variety of NLP tasks, and are being rapidly adopted in a wide range of use cases. It is therefore of vital importance to holistically evaluate the factuality of their generated outputs, as hallucinations remain a challenging issue.   In this work, we focus on assessing LLMs' ability to recall factual knowledge learned from pretraining, and the factors that affect this ability. To that end, we construct FACT-BENCH, a representative benchmark covering 20 domains, 134 property types, 3 answer types, and different knowledge popularity levels. We benchmark 31 models from 10 model families and provide a holistic assessment of their strengths and weaknesses. We observe that instruction-tuning hurts knowledge recall, as pretraining-only models consistently outperform their instruction-tuned counterparts, and positive effects of model scaling, as larger models outperform smaller ones for all model families. However, the best performance from GPT-4 still represents a large gap with the upper-bound. We additionally study the role of in-context exemplars using counterfactual demonstrations, which lead to significant degradation of factual knowledge recall for large models. By further decoupling model known and unknown knowledge, we find the degradation is attributed to exemplars that contradict a model's known knowledge, as well as the number of such exemplars. Lastly, we fine-tune LLaMA-7B in different settings of known and unknown knowledge. In particular, fine-tuning on a model's known knowledge is beneficial, and consistently outperforms fine-tuning on unknown and mixed knowledge. We will make our benchmark publicly available.","sentences":["Large language models (LLMs) have shown remarkable performance on a variety of NLP tasks, and are being rapidly adopted in a wide range of use cases.","It is therefore of vital importance to holistically evaluate the factuality of their generated outputs, as hallucinations remain a challenging issue.   ","In this work, we focus on assessing LLMs' ability to recall factual knowledge learned from pretraining, and the factors that affect this ability.","To that end, we construct FACT-BENCH, a representative benchmark covering 20 domains, 134 property types, 3 answer types, and different knowledge popularity levels.","We benchmark 31 models from 10 model families and provide a holistic assessment of their strengths and weaknesses.","We observe that instruction-tuning hurts knowledge recall, as pretraining-only models consistently outperform their instruction-tuned counterparts, and positive effects of model scaling, as larger models outperform smaller ones for all model families.","However, the best performance from GPT-4 still represents a large gap with the upper-bound.","We additionally study the role of in-context exemplars using counterfactual demonstrations, which lead to significant degradation of factual knowledge recall for large models.","By further decoupling model known and unknown knowledge, we find the degradation is attributed to exemplars that contradict a model's known knowledge, as well as the number of such exemplars.","Lastly, we fine-tune LLaMA-7B in different settings of known and unknown knowledge.","In particular, fine-tuning on a model's known knowledge is beneficial, and consistently outperforms fine-tuning on unknown and mixed knowledge.","We will make our benchmark publicly available."],"url":"http://arxiv.org/abs/2404.16164v1","category":"cs.CL"}
{"created":"2024-04-24 19:37:18","title":"Scaling Lifelong Multi-Agent Path Finding to More Realistic Settings: Research Challenges and Opportunities","abstract":"Multi-Agent Path Finding (MAPF) is the problem of moving multiple agents from starts to goals without collisions. Lifelong MAPF (LMAPF) extends MAPF by continuously assigning new goals to agents. We present our winning approach to the 2023 League of Robot Runners LMAPF competition, which leads us to several interesting research challenges and future directions. In this paper, we outline three main research challenges. The first challenge is to search for high-quality LMAPF solutions within a limited planning time (e.g., 1s per step) for a large number of agents (e.g., 10,000) or extremely high agent density (e.g., 97.7%). We present future directions such as developing more competitive rule-based and anytime MAPF algorithms and parallelizing state-of-the-art MAPF algorithms. The second challenge is to alleviate congestion and the effect of myopic behaviors in LMAPF algorithms. We present future directions, such as developing moving guidance and traffic rules to reduce congestion, incorporating future prediction and real-time search, and determining the optimal agent number. The third challenge is to bridge the gaps between the LMAPF models used in the literature and real-world applications. We present future directions, such as dealing with more realistic kinodynamic models, execution uncertainty, and evolving systems.","sentences":["Multi-Agent Path Finding (MAPF) is the problem of moving multiple agents from starts to goals without collisions.","Lifelong MAPF (LMAPF) extends MAPF by continuously assigning new goals to agents.","We present our winning approach to the 2023 League of Robot Runners LMAPF competition, which leads us to several interesting research challenges and future directions.","In this paper, we outline three main research challenges.","The first challenge is to search for high-quality LMAPF solutions within a limited planning time (e.g., 1s per step) for a large number of agents (e.g., 10,000) or extremely high agent density (e.g., 97.7%).","We present future directions such as developing more competitive rule-based and anytime MAPF algorithms and parallelizing state-of-the-art MAPF algorithms.","The second challenge is to alleviate congestion and the effect of myopic behaviors in LMAPF algorithms.","We present future directions, such as developing moving guidance and traffic rules to reduce congestion, incorporating future prediction and real-time search, and determining the optimal agent number.","The third challenge is to bridge the gaps between the LMAPF models used in the literature and real-world applications.","We present future directions, such as dealing with more realistic kinodynamic models, execution uncertainty, and evolving systems."],"url":"http://arxiv.org/abs/2404.16162v1","category":"cs.MA"}
{"created":"2024-04-24 19:30:18","title":"Domain-Specific Improvement on Psychotherapy Chatbot Using Assistant","abstract":"Large language models (LLMs) have demonstrated impressive generalization capabilities on specific tasks with human-written instruction data. However, the limited quantity, diversity, and professional expertise of such instruction data raise concerns about the performance of LLMs in psychotherapy tasks when provided with domain-specific instructions. To address this, we firstly propose Domain-Specific Assistant Instructions based on AlexanderStreet therapy, and secondly, we use an adaption fine-tuning method and retrieval augmented generation method to improve pre-trained LLMs. Through quantitative evaluation of linguistic quality using automatic and human evaluation, we observe that pre-trained LLMs on Psychotherapy Assistant Instructions outperform state-of-the-art LLMs response baselines. Our Assistant-Instruction approach offers a half-annotation method to align pre-trained LLMs with instructions and provide pre-trained LLMs with more psychotherapy knowledge.","sentences":["Large language models (LLMs) have demonstrated impressive generalization capabilities on specific tasks with human-written instruction data.","However, the limited quantity, diversity, and professional expertise of such instruction data raise concerns about the performance of LLMs in psychotherapy tasks when provided with domain-specific instructions.","To address this, we firstly propose Domain-Specific Assistant Instructions based on AlexanderStreet therapy, and secondly, we use an adaption fine-tuning method and retrieval augmented generation method to improve pre-trained LLMs.","Through quantitative evaluation of linguistic quality using automatic and human evaluation, we observe that pre-trained LLMs on Psychotherapy Assistant Instructions outperform state-of-the-art LLMs response baselines.","Our Assistant-Instruction approach offers a half-annotation method to align pre-trained LLMs with instructions and provide pre-trained LLMs with more psychotherapy knowledge."],"url":"http://arxiv.org/abs/2404.16160v1","category":"cs.CL"}
{"created":"2024-04-24 19:30:03","title":"AFU: Actor-Free critic Updates in off-policy RL for continuous control","abstract":"This paper presents AFU, an off-policy deep RL algorithm addressing in a new way the challenging \"max-Q problem\" in Q-learning for continuous action spaces, with a solution based on regression and conditional gradient scaling. AFU has an actor but its critic updates are entirely independent from it. As a consequence, the actor can be chosen freely. In the initial version, AFU-alpha, we employ the same stochastic actor as in Soft Actor-Critic (SAC), but we then study a simple failure mode of SAC and show how AFU can be modified to make actor updates less likely to become trapped in local optima, resulting in a second version of the algorithm, AFU-beta. Experimental results demonstrate the sample efficiency of both versions of AFU, marking it as the first model-free off-policy algorithm competitive with state-of-the-art actor-critic methods while departing from the actor-critic perspective.","sentences":["This paper presents AFU, an off-policy deep RL algorithm addressing in a new way the challenging \"max-Q problem\" in Q-learning for continuous action spaces, with a solution based on regression and conditional gradient scaling.","AFU has an actor but its critic updates are entirely independent from it.","As a consequence, the actor can be chosen freely.","In the initial version, AFU-alpha, we employ the same stochastic actor as in Soft Actor-Critic (SAC), but we then study a simple failure mode of SAC and show how AFU can be modified to make actor updates less likely to become trapped in local optima, resulting in a second version of the algorithm, AFU-beta.","Experimental results demonstrate the sample efficiency of both versions of AFU, marking it as the first model-free off-policy algorithm competitive with state-of-the-art actor-critic methods while departing from the actor-critic perspective."],"url":"http://arxiv.org/abs/2404.16159v1","category":"cs.LG"}
{"created":"2024-04-24 18:38:11","title":"From Local to Global: A Graph RAG Approach to Query-Focused Summarization","abstract":"The use of retrieval-augmented generation (RAG) to retrieve relevant information from an external knowledge source enables large language models (LLMs) to answer questions over private and/or previously unseen document collections. However, RAG fails on global questions directed at an entire text corpus, such as \"What are the main themes in the dataset?\", since this is inherently a query-focused summarization (QFS) task, rather than an explicit retrieval task. Prior QFS methods, meanwhile, fail to scale to the quantities of text indexed by typical RAG systems. To combine the strengths of these contrasting methods, we propose a Graph RAG approach to question answering over private text corpora that scales with both the generality of user questions and the quantity of source text to be indexed. Our approach uses an LLM to build a graph-based text index in two stages: first to derive an entity knowledge graph from the source documents, then to pregenerate community summaries for all groups of closely-related entities. Given a question, each community summary is used to generate a partial response, before all partial responses are again summarized in a final response to the user. For a class of global sensemaking questions over datasets in the 1 million token range, we show that Graph RAG leads to substantial improvements over a na\\\"ive RAG baseline for both the comprehensiveness and diversity of generated answers. An open-source, Python-based implementation of both global and local Graph RAG approaches is forthcoming at https://aka.ms/graphrag.","sentences":["The use of retrieval-augmented generation (RAG) to retrieve relevant information from an external knowledge source enables large language models (LLMs) to answer questions over private and/or previously unseen document collections.","However, RAG fails on global questions directed at an entire text corpus, such as \"What are the main themes in the dataset?\", since this is inherently a query-focused summarization (QFS) task, rather than an explicit retrieval task.","Prior QFS methods, meanwhile, fail to scale to the quantities of text indexed by typical RAG systems.","To combine the strengths of these contrasting methods, we propose a Graph RAG approach to question answering over private text corpora that scales with both the generality of user questions and the quantity of source text to be indexed.","Our approach uses an LLM to build a graph-based text index in two stages: first to derive an entity knowledge graph from the source documents, then to pregenerate community summaries for all groups of closely-related entities.","Given a question, each community summary is used to generate a partial response, before all partial responses are again summarized in a final response to the user.","For a class of global sensemaking questions over datasets in the 1 million token range, we show that Graph RAG leads to substantial improvements over a na\\\"ive RAG baseline for both the comprehensiveness and diversity of generated answers.","An open-source, Python-based implementation of both global and local Graph RAG approaches is forthcoming at https://aka.ms/graphrag."],"url":"http://arxiv.org/abs/2404.16130v1","category":"cs.CL"}
{"created":"2024-04-24 18:28:17","title":"FairDeDup: Detecting and Mitigating Vision-Language Fairness Disparities in Semantic Dataset Deduplication","abstract":"Recent dataset deduplication techniques have demonstrated that content-aware dataset pruning can dramatically reduce the cost of training Vision-Language Pretrained (VLP) models without significant performance losses compared to training on the original dataset. These results have been based on pruning commonly used image-caption datasets collected from the web -- datasets that are known to harbor harmful social biases that may then be codified in trained models. In this work, we evaluate how deduplication affects the prevalence of these biases in the resulting trained models and introduce an easy-to-implement modification to the recent SemDeDup algorithm that can reduce the negative effects that we observe. When examining CLIP-style models trained on deduplicated variants of LAION-400M, we find our proposed FairDeDup algorithm consistently leads to improved fairness metrics over SemDeDup on the FairFace and FACET datasets while maintaining zero-shot performance on CLIP benchmarks.","sentences":["Recent dataset deduplication techniques have demonstrated that content-aware dataset pruning can dramatically reduce the cost of training Vision-Language Pretrained (VLP) models without significant performance losses compared to training on the original dataset.","These results have been based on pruning commonly used image-caption datasets collected from the web -- datasets that are known to harbor harmful social biases that may then be codified in trained models.","In this work, we evaluate how deduplication affects the prevalence of these biases in the resulting trained models and introduce an easy-to-implement modification to the recent SemDeDup algorithm that can reduce the negative effects that we observe.","When examining CLIP-style models trained on deduplicated variants of LAION-400M, we find our proposed FairDeDup algorithm consistently leads to improved fairness metrics over SemDeDup on the FairFace and FACET datasets while maintaining zero-shot performance on CLIP benchmarks."],"url":"http://arxiv.org/abs/2404.16123v1","category":"cs.CV"}
{"created":"2024-04-24 18:18:09","title":"Cybersecurity Assessment of the Polar Bluetooth Low Energy Heart-rate Sensor","abstract":"Wireless communications among wearable and implantable devices implement the information exchange around the human body. Wireless body area network (WBAN) technology enables non-invasive applications in our daily lives. Wireless connected devices improve the quality of many services, and they make procedures easier. On the other hand, they open up large attack surfaces and introduces potential security vulnerabilities. Bluetooth low energy (BLE) is a low-power protocol widely used in wireless personal area networks (WPANs). This paper analyzes the security vulnerabilities of a BLE heart-rate sensor. By observing the received signal strength indicator (RSSI) variations, it is possible to detect anomalies in the BLE connection. The case-study shows that an attacker can easily intercept and manipulate the data transmitted between the mobile app and the BLE device. With this research, the author would raise awareness about the security of the heart-rate information that we can receive from our wireless body sensors.","sentences":["Wireless communications among wearable and implantable devices implement the information exchange around the human body.","Wireless body area network (WBAN) technology enables non-invasive applications in our daily lives.","Wireless connected devices improve the quality of many services, and they make procedures easier.","On the other hand, they open up large attack surfaces and introduces potential security vulnerabilities.","Bluetooth low energy (BLE) is a low-power protocol widely used in wireless personal area networks (WPANs).","This paper analyzes the security vulnerabilities of a BLE heart-rate sensor.","By observing the received signal strength indicator (RSSI) variations, it is possible to detect anomalies in the BLE connection.","The case-study shows that an attacker can easily intercept and manipulate the data transmitted between the mobile app and the BLE device.","With this research, the author would raise awareness about the security of the heart-rate information that we can receive from our wireless body sensors."],"url":"http://arxiv.org/abs/2404.16117v1","category":"cs.CR"}
{"created":"2024-04-24 18:13:29","title":"Classifying Human-Generated and AI-Generated Election Claims in Social Media","abstract":"Politics is one of the most prevalent topics discussed on social media platforms, particularly during major election cycles, where users engage in conversations about candidates and electoral processes. Malicious actors may use this opportunity to disseminate misinformation to undermine trust in the electoral process. The emergence of Large Language Models (LLMs) exacerbates this issue by enabling malicious actors to generate misinformation at an unprecedented scale. Artificial intelligence (AI)-generated content is often indistinguishable from authentic user content, raising concerns about the integrity of information on social networks. In this paper, we present a novel taxonomy for characterizing election-related claims. This taxonomy provides an instrument for analyzing election-related claims, with granular categories related to jurisdiction, equipment, processes, and the nature of claims. We introduce ElectAI, a novel benchmark dataset that consists of 9,900 tweets, each labeled as human- or AI-generated. For AI-generated tweets, the specific LLM variant that produced them is specified. We annotated a subset of 1,550 tweets using the proposed taxonomy to capture the characteristics of election-related claims. We explored the capabilities of LLMs in extracting the taxonomy attributes and trained various machine learning models using ElectAI to distinguish between human- and AI-generated posts and identify the specific LLM variant.","sentences":["Politics is one of the most prevalent topics discussed on social media platforms, particularly during major election cycles, where users engage in conversations about candidates and electoral processes.","Malicious actors may use this opportunity to disseminate misinformation to undermine trust in the electoral process.","The emergence of Large Language Models (LLMs) exacerbates this issue by enabling malicious actors to generate misinformation at an unprecedented scale.","Artificial intelligence (AI)-generated content is often indistinguishable from authentic user content, raising concerns about the integrity of information on social networks.","In this paper, we present a novel taxonomy for characterizing election-related claims.","This taxonomy provides an instrument for analyzing election-related claims, with granular categories related to jurisdiction, equipment, processes, and the nature of claims.","We introduce ElectAI, a novel benchmark dataset that consists of 9,900 tweets, each labeled as human- or AI-generated.","For AI-generated tweets, the specific LLM variant that produced them is specified.","We annotated a subset of 1,550 tweets using the proposed taxonomy to capture the characteristics of election-related claims.","We explored the capabilities of LLMs in extracting the taxonomy attributes and trained various machine learning models using ElectAI to distinguish between human- and AI-generated posts and identify the specific LLM variant."],"url":"http://arxiv.org/abs/2404.16116v1","category":"cs.CL"}
{"created":"2024-04-24 18:13:12","title":"Online Personalizing White-box LLMs Generation with Neural Bandits","abstract":"The advent of personalized content generation by LLMs presents a novel challenge: how to efficiently adapt text to meet individual preferences without the unsustainable demand of creating a unique model for each user. This study introduces an innovative online method that employs neural bandit algorithms to dynamically optimize soft instruction embeddings based on user feedback, enhancing the personalization of open-ended text generation by white-box LLMs. Through rigorous experimentation on various tasks, we demonstrate significant performance improvements over baseline strategies. NeuralTS, in particular, leads to substantial enhancements in personalized news headline generation, achieving up to a 62.9% improvement in terms of best ROUGE scores and up to 2.76% increase in LLM-agent evaluation against the baseline.","sentences":["The advent of personalized content generation by LLMs presents a novel challenge: how to efficiently adapt text to meet individual preferences without the unsustainable demand of creating a unique model for each user.","This study introduces an innovative online method that employs neural bandit algorithms to dynamically optimize soft instruction embeddings based on user feedback, enhancing the personalization of open-ended text generation by white-box LLMs.","Through rigorous experimentation on various tasks, we demonstrate significant performance improvements over baseline strategies.","NeuralTS, in particular, leads to substantial enhancements in personalized news headline generation, achieving up to a 62.9% improvement in terms of best ROUGE scores and up to 2.76% increase in LLM-agent evaluation against the baseline."],"url":"http://arxiv.org/abs/2404.16115v1","category":"cs.CL"}
{"created":"2024-04-24 18:10:31","title":"Mamba-360: Survey of State Space Models as Transformer Alternative for Long Sequence Modelling: Methods, Applications, and Challenges","abstract":"Sequence modeling is a crucial area across various domains, including Natural Language Processing (NLP), speech recognition, time series forecasting, music generation, and bioinformatics. Recurrent Neural Networks (RNNs) and Long Short Term Memory Networks (LSTMs) have historically dominated sequence modeling tasks like Machine Translation, Named Entity Recognition (NER), etc. However, the advancement of transformers has led to a shift in this paradigm, given their superior performance. Yet, transformers suffer from $O(N^2)$ attention complexity and challenges in handling inductive bias. Several variations have been proposed to address these issues which use spectral networks or convolutions and have performed well on a range of tasks. However, they still have difficulty in dealing with long sequences. State Space Models(SSMs) have emerged as promising alternatives for sequence modeling paradigms in this context, especially with the advent of S4 and its variants, such as S4nd, Hippo, Hyena, Diagnol State Spaces (DSS), Gated State Spaces (GSS), Linear Recurrent Unit (LRU), Liquid-S4, Mamba, etc. In this survey, we categorize the foundational SSMs based on three paradigms namely, Gating architectures, Structural architectures, and Recurrent architectures. This survey also highlights diverse applications of SSMs across domains such as vision, video, audio, speech, language (especially long sequence modeling), medical (including genomics), chemical (like drug design), recommendation systems, and time series analysis, including tabular data. Moreover, we consolidate the performance of SSMs on benchmark datasets like Long Range Arena (LRA), WikiText, Glue, Pile, ImageNet, Kinetics-400, sstv2, as well as video datasets such as Breakfast, COIN, LVU, and various time series datasets. The project page for Mamba-360 work is available on this webpage.\\url{https://github.com/badripatro/mamba360}.","sentences":["Sequence modeling is a crucial area across various domains, including Natural Language Processing (NLP), speech recognition, time series forecasting, music generation, and bioinformatics.","Recurrent Neural Networks (RNNs) and Long Short Term Memory Networks (LSTMs) have historically dominated sequence modeling tasks like Machine Translation, Named Entity Recognition (NER), etc.","However, the advancement of transformers has led to a shift in this paradigm, given their superior performance.","Yet, transformers suffer from $O(N^2)$ attention complexity and challenges in handling inductive bias.","Several variations have been proposed to address these issues which use spectral networks or convolutions and have performed well on a range of tasks.","However, they still have difficulty in dealing with long sequences.","State Space Models(SSMs) have emerged as promising alternatives for sequence modeling paradigms in this context, especially with the advent of S4 and its variants, such as S4nd, Hippo, Hyena, Diagnol State Spaces (DSS), Gated State Spaces (GSS), Linear Recurrent Unit (LRU), Liquid-S4, Mamba, etc.","In this survey, we categorize the foundational SSMs based on three paradigms namely, Gating architectures, Structural architectures, and Recurrent architectures.","This survey also highlights diverse applications of SSMs across domains such as vision, video, audio, speech, language (especially long sequence modeling), medical (including genomics), chemical (like drug design), recommendation systems, and time series analysis, including tabular data.","Moreover, we consolidate the performance of SSMs on benchmark datasets like Long Range Arena (LRA), WikiText, Glue, Pile, ImageNet, Kinetics-400, sstv2, as well as video datasets such as Breakfast, COIN, LVU, and various time series datasets.","The project page for Mamba-360 work is available on this webpage.\\url{https://github.com/badripatro/mamba360}."],"url":"http://arxiv.org/abs/2404.16112v1","category":"cs.LG"}
{"created":"2024-04-24 18:04:50","title":"zkLLM: Zero Knowledge Proofs for Large Language Models","abstract":"The recent surge in artificial intelligence (AI), characterized by the prominence of large language models (LLMs), has ushered in fundamental transformations across the globe. However, alongside these advancements, concerns surrounding the legitimacy of LLMs have grown, posing legal challenges to their extensive applications. Compounding these concerns, the parameters of LLMs are often treated as intellectual property, restricting direct investigations.   In this study, we address a fundamental challenge within the realm of AI legislation: the need to establish the authenticity of outputs generated by LLMs. To tackle this issue, we present zkLLM, which stands as the inaugural specialized zero-knowledge proof tailored for LLMs to the best of our knowledge. Addressing the persistent challenge of non-arithmetic operations in deep learning, we introduce tlookup, a parallelized lookup argument designed for non-arithmetic tensor operations in deep learning, offering a solution with no asymptotic overhead. Furthermore, leveraging the foundation of tlookup, we introduce zkAttn, a specialized zero-knowledge proof crafted for the attention mechanism, carefully balancing considerations of running time, memory usage, and accuracy.   Empowered by our fully parallelized CUDA implementation, zkLLM emerges as a significant stride towards achieving efficient zero-knowledge verifiable computations over LLMs. Remarkably, for LLMs boasting 13 billion parameters, our approach enables the generation of a correctness proof for the entire inference process in under 15 minutes. The resulting proof, compactly sized at less than 200 kB, is designed to uphold the privacy of the model parameters, ensuring no inadvertent information leakage.","sentences":["The recent surge in artificial intelligence (AI), characterized by the prominence of large language models (LLMs), has ushered in fundamental transformations across the globe.","However, alongside these advancements, concerns surrounding the legitimacy of LLMs have grown, posing legal challenges to their extensive applications.","Compounding these concerns, the parameters of LLMs are often treated as intellectual property, restricting direct investigations.   ","In this study, we address a fundamental challenge within the realm of AI legislation: the need to establish the authenticity of outputs generated by LLMs.","To tackle this issue, we present zkLLM, which stands as the inaugural specialized zero-knowledge proof tailored for LLMs to the best of our knowledge.","Addressing the persistent challenge of non-arithmetic operations in deep learning, we introduce tlookup, a parallelized lookup argument designed for non-arithmetic tensor operations in deep learning, offering a solution with no asymptotic overhead.","Furthermore, leveraging the foundation of tlookup, we introduce zkAttn, a specialized zero-knowledge proof crafted for the attention mechanism, carefully balancing considerations of running time, memory usage, and accuracy.   ","Empowered by our fully parallelized CUDA implementation, zkLLM emerges as a significant stride towards achieving efficient zero-knowledge verifiable computations over LLMs.","Remarkably, for LLMs boasting 13 billion parameters, our approach enables the generation of a correctness proof for the entire inference process in under 15 minutes.","The resulting proof, compactly sized at less than 200 kB, is designed to uphold the privacy of the model parameters, ensuring no inadvertent information leakage."],"url":"http://arxiv.org/abs/2404.16109v1","category":"cs.LG"}
{"created":"2024-04-24 17:59:53","title":"MaGGIe: Masked Guided Gradual Human Instance Matting","abstract":"Human matting is a foundation task in image and video processing, where human foreground pixels are extracted from the input. Prior works either improve the accuracy by additional guidance or improve the temporal consistency of a single instance across frames. We propose a new framework MaGGIe, Masked Guided Gradual Human Instance Matting, which predicts alpha mattes progressively for each human instances while maintaining the computational cost, precision, and consistency. Our method leverages modern architectures, including transformer attention and sparse convolution, to output all instance mattes simultaneously without exploding memory and latency. Although keeping constant inference costs in the multiple-instance scenario, our framework achieves robust and versatile performance on our proposed synthesized benchmarks. With the higher quality image and video matting benchmarks, the novel multi-instance synthesis approach from publicly available sources is introduced to increase the generalization of models in real-world scenarios.","sentences":["Human matting is a foundation task in image and video processing, where human foreground pixels are extracted from the input.","Prior works either improve the accuracy by additional guidance or improve the temporal consistency of a single instance across frames.","We propose a new framework MaGGIe, Masked Guided Gradual Human Instance Matting, which predicts alpha mattes progressively for each human instances while maintaining the computational cost, precision, and consistency.","Our method leverages modern architectures, including transformer attention and sparse convolution, to output all instance mattes simultaneously without exploding memory and latency.","Although keeping constant inference costs in the multiple-instance scenario, our framework achieves robust and versatile performance on our proposed synthesized benchmarks.","With the higher quality image and video matting benchmarks, the novel multi-instance synthesis approach from publicly available sources is introduced to increase the generalization of models in real-world scenarios."],"url":"http://arxiv.org/abs/2404.16035v1","category":"cs.CV"}
{"created":"2024-04-24 17:59:24","title":"MoDE: CLIP Data Experts via Clustering","abstract":"The success of contrastive language-image pretraining (CLIP) relies on the supervision from the pairing between images and captions, which tends to be noisy in web-crawled data. We present Mixture of Data Experts (MoDE) and learn a system of CLIP data experts via clustering. Each data expert is trained on one data cluster, being less sensitive to false negative noises in other clusters. At inference time, we ensemble their outputs by applying weights determined through the correlation between task metadata and cluster conditions. To estimate the correlation precisely, the samples in one cluster should be semantically similar, but the number of data experts should still be reasonable for training and inference. As such, we consider the ontology in human language and propose to use fine-grained cluster centers to represent each data expert at a coarse-grained level. Experimental studies show that four CLIP data experts on ViT-B/16 outperform the ViT-L/14 by OpenAI CLIP and OpenCLIP on zero-shot image classification but with less ($<$35\\%) training cost. Meanwhile, MoDE can train all data expert asynchronously and can flexibly include new data experts. The code is available at https://github.com/facebookresearch/MetaCLIP/tree/main/mode.","sentences":["The success of contrastive language-image pretraining (CLIP) relies on the supervision from the pairing between images and captions, which tends to be noisy in web-crawled data.","We present Mixture of Data Experts (MoDE) and learn a system of CLIP data experts via clustering.","Each data expert is trained on one data cluster, being less sensitive to false negative noises in other clusters.","At inference time, we ensemble their outputs by applying weights determined through the correlation between task metadata and cluster conditions.","To estimate the correlation precisely, the samples in one cluster should be semantically similar, but the number of data experts should still be reasonable for training and inference.","As such, we consider the ontology in human language and propose to use fine-grained cluster centers to represent each data expert at a coarse-grained level.","Experimental studies show that four CLIP data experts on ViT-B/16 outperform the ViT-L/14 by OpenAI CLIP and OpenCLIP on zero-shot image classification but with less ($<$35\\%) training cost.","Meanwhile, MoDE can train all data expert asynchronously and can flexibly include new data experts.","The code is available at https://github.com/facebookresearch/MetaCLIP/tree/main/mode."],"url":"http://arxiv.org/abs/2404.16030v1","category":"cs.CV"}
{"created":"2024-04-24 17:55:47","title":"Learning Car-Following Behaviors Using Bayesian Matrix Normal Mixture Regression","abstract":"Learning and understanding car-following (CF) behaviors are crucial for microscopic traffic simulation. Traditional CF models, though simple, often lack generalization capabilities, while many data-driven methods, despite their robustness, operate as \"black boxes\" with limited interpretability. To bridge this gap, this work introduces a Bayesian Matrix Normal Mixture Regression (MNMR) model that simultaneously captures feature correlations and temporal dynamics inherent in CF behaviors. This approach is distinguished by its separate learning of row and column covariance matrices within the model framework, offering an insightful perspective into the human driver decision-making processes. Through extensive experiments, we assess the model's performance across various historical steps of inputs, predictive steps of outputs, and model complexities. The results consistently demonstrate our model's adeptness in effectively capturing the intricate correlations and temporal dynamics present during CF. A focused case study further illustrates the model's outperforming interpretability of identifying distinct operational conditions through the learned mean and covariance matrices. This not only underlines our model's effectiveness in understanding complex human driving behaviors in CF scenarios but also highlights its potential as a tool for enhancing the interpretability of CF behaviors in traffic simulations and autonomous driving systems.","sentences":["Learning and understanding car-following (CF) behaviors are crucial for microscopic traffic simulation.","Traditional CF models, though simple, often lack generalization capabilities, while many data-driven methods, despite their robustness, operate as \"black boxes\" with limited interpretability.","To bridge this gap, this work introduces a Bayesian Matrix Normal Mixture Regression (MNMR) model that simultaneously captures feature correlations and temporal dynamics inherent in CF behaviors.","This approach is distinguished by its separate learning of row and column covariance matrices within the model framework, offering an insightful perspective into the human driver decision-making processes.","Through extensive experiments, we assess the model's performance across various historical steps of inputs, predictive steps of outputs, and model complexities.","The results consistently demonstrate our model's adeptness in effectively capturing the intricate correlations and temporal dynamics present during CF.","A focused case study further illustrates the model's outperforming interpretability of identifying distinct operational conditions through the learned mean and covariance matrices.","This not only underlines our model's effectiveness in understanding complex human driving behaviors in CF scenarios but also highlights its potential as a tool for enhancing the interpretability of CF behaviors in traffic simulations and autonomous driving systems."],"url":"http://arxiv.org/abs/2404.16023v1","category":"stat.AP"}
{"created":"2024-04-24 17:52:44","title":"Measurement of multijet azimuthal correlations and determination of the strong coupling in proton-proton collisions at $\\sqrt{s}$ = 13 TeV","abstract":"A measurement is presented of a ratio observable that provides a measure of the azimuthal correlations among jets with large transverse momentum $p_\\mathrm{T}$. This observable is measured in multijet events over the range of $p_\\mathrm{T}$ = 360-3170 GeV based on data collected by the CMS experiment in proton-proton collisions at a centre-of-mass energy of 13 TeV, corresponding to an integrated luminosity of 134 fb$^{-1}$. The results are compared with predictions from Monte Carlo parton-shower event generator simulations, as well as with fixed-order perturbative quantum chromodynamics (pQCD) predictions at next-to-leading-order (NLO) accuracy obtained with different parton distribution functions (PDFs) and corrected for nonperturbative and electroweak effects. Data and theory agree within uncertainties. From the comparison of the measured observable with the pQCD prediction obtained with the NNPDF3.1 NLO PDFs, the strong coupling at the Z boson mass scale is $\\alpha_\\mathrm{S}(m_\\mathrm{Z})$ = 0.1177 $\\pm$ 0.0013 (exp) $_{-0.0073}^{+0.0116}$ (theo) = 0.1177$_{-0.0074}^{+0.0117}$, where the total uncertainty is dominated by the scale dependence of the fixed-order predictions. A test of the running of $\\alpha_\\mathrm{S}(m_\\mathrm{Z})$ in the TeV region shows no deviation from the expected NLO pQCD behaviour.","sentences":["A measurement is presented of a ratio observable that provides a measure of the azimuthal correlations among jets with large transverse momentum $p_\\mathrm{T}$. This observable is measured in multijet events over the range of $p_\\mathrm{T}$ = 360-3170 GeV based on data collected by the CMS experiment in proton-proton collisions at a centre-of-mass energy of 13 TeV, corresponding to an integrated luminosity of 134 fb$^{-1}$. The results are compared with predictions from Monte Carlo parton-shower event generator simulations, as well as with fixed-order perturbative quantum chromodynamics (pQCD) predictions at next-to-leading-order (NLO) accuracy obtained with different parton distribution functions (PDFs) and corrected for nonperturbative and electroweak effects.","Data and theory agree within uncertainties.","From the comparison of the measured observable with the pQCD prediction obtained with the NNPDF3.1 NLO PDFs, the strong coupling at the Z boson mass scale is $\\alpha_\\mathrm{S}(m_\\mathrm{Z})$ = 0.1177 $\\pm$ 0.0013 (exp) $_{-0.0073}^{+0.0116}$ (theo) = 0.1177$_{-0.0074}^{+0.0117}$, where the total uncertainty is dominated by the scale dependence of the fixed-order predictions.","A test of the running of $\\alpha_\\mathrm{S}(m_\\mathrm{Z})$ in the TeV region shows no deviation from the expected NLO pQCD behaviour."],"url":"http://arxiv.org/abs/2404.16082v1","category":"hep-ex"}
{"created":"2024-04-24 17:51:36","title":"The PRISM Alignment Project: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models","abstract":"Human feedback plays a central role in the alignment of Large Language Models (LLMs). However, open questions remain about the methods (how), domains (where), people (who) and objectives (to what end) of human feedback collection. To navigate these questions, we introduce PRISM, a new dataset which maps the sociodemographics and stated preferences of 1,500 diverse participants from 75 countries, to their contextual preferences and fine-grained feedback in 8,011 live conversations with 21 LLMs. PRISM contributes (i) wide geographic and demographic participation in human feedback data; (ii) two census-representative samples for understanding collective welfare (UK and US); and (iii) individualised feedback where every rating is linked to a detailed participant profile, thus permitting exploration of personalisation and attribution of sample artefacts. We focus on collecting conversations that centre subjective and multicultural perspectives on value-laden and controversial topics, where we expect the most interpersonal and cross-cultural disagreement. We demonstrate the usefulness of PRISM via three case studies of dialogue diversity, preference diversity, and welfare outcomes, showing that it matters which humans set alignment norms. As well as offering a rich community resource, we advocate for broader participation in AI development and a more inclusive approach to technology design.","sentences":["Human feedback plays a central role in the alignment of Large Language Models (LLMs).","However, open questions remain about the methods (how), domains (where), people (who) and objectives (to what end) of human feedback collection.","To navigate these questions, we introduce PRISM, a new dataset which maps the sociodemographics and stated preferences of 1,500 diverse participants from 75 countries, to their contextual preferences and fine-grained feedback in 8,011 live conversations with 21 LLMs.","PRISM contributes (i) wide geographic and demographic participation in human feedback data; (ii) two census-representative samples for understanding collective welfare (UK and US); and (iii) individualised feedback where every rating is linked to a detailed participant profile, thus permitting exploration of personalisation and attribution of sample artefacts.","We focus on collecting conversations that centre subjective and multicultural perspectives on value-laden and controversial topics, where we expect the most interpersonal and cross-cultural disagreement.","We demonstrate the usefulness of PRISM via three case studies of dialogue diversity, preference diversity, and welfare outcomes, showing that it matters which humans set alignment norms.","As well as offering a rich community resource, we advocate for broader participation in AI development and a more inclusive approach to technology design."],"url":"http://arxiv.org/abs/2404.16019v1","category":"cs.CL"}
{"created":"2024-04-24 17:50:37","title":"RetinaRegNet: A Versatile Approach for Retinal Image Registration","abstract":"We introduce the RetinaRegNet model, which can achieve state-of-the-art performance across various retinal image registration tasks. RetinaRegNet does not require training on any retinal images. It begins by establishing point correspondences between two retinal images using image features derived from diffusion models. This process involves the selection of feature points from the moving image using the SIFT algorithm alongside random point sampling. For each selected feature point, a 2D correlation map is computed by assessing the similarity between the feature vector at that point and the feature vectors of all pixels in the fixed image. The pixel with the highest similarity score in the correlation map corresponds to the feature point in the moving image. To remove outliers in the estimated point correspondences, we first applied an inverse consistency constraint, followed by a transformation-based outlier detector. This method proved to outperform the widely used random sample consensus (RANSAC) outlier detector by a significant margin. To handle large deformations, we utilized a two-stage image registration framework. A homography transformation was used in the first stage and a more accurate third-order polynomial transformation was used in the second stage. The model's effectiveness was demonstrated across three retinal image datasets: color fundus images, fluorescein angiography images, and laser speckle flowgraphy images. RetinaRegNet outperformed current state-of-the-art methods in all three datasets. It was especially effective for registering image pairs with large displacement and scaling deformations. This innovation holds promise for various applications in retinal image analysis. Our code is publicly available at https://github.com/mirthAI/RetinaRegNet.","sentences":["We introduce the RetinaRegNet model, which can achieve state-of-the-art performance across various retinal image registration tasks.","RetinaRegNet does not require training on any retinal images.","It begins by establishing point correspondences between two retinal images using image features derived from diffusion models.","This process involves the selection of feature points from the moving image using the SIFT algorithm alongside random point sampling.","For each selected feature point, a 2D correlation map is computed by assessing the similarity between the feature vector at that point and the feature vectors of all pixels in the fixed image.","The pixel with the highest similarity score in the correlation map corresponds to the feature point in the moving image.","To remove outliers in the estimated point correspondences, we first applied an inverse consistency constraint, followed by a transformation-based outlier detector.","This method proved to outperform the widely used random sample consensus (RANSAC) outlier detector by a significant margin.","To handle large deformations, we utilized a two-stage image registration framework.","A homography transformation was used in the first stage and a more accurate third-order polynomial transformation was used in the second stage.","The model's effectiveness was demonstrated across three retinal image datasets: color fundus images, fluorescein angiography images, and laser speckle flowgraphy images.","RetinaRegNet outperformed current state-of-the-art methods in all three datasets.","It was especially effective for registering image pairs with large displacement and scaling deformations.","This innovation holds promise for various applications in retinal image analysis.","Our code is publicly available at https://github.com/mirthAI/RetinaRegNet."],"url":"http://arxiv.org/abs/2404.16017v1","category":"cs.CV"}
{"created":"2024-04-24 17:48:38","title":"Neural Operators Learn the Local Physics of Magnetohydrodynamics","abstract":"Magnetohydrodynamics (MHD) plays a pivotal role in describing the dynamics of plasma and conductive fluids, essential for understanding phenomena such as the structure and evolution of stars and galaxies, and in nuclear fusion for plasma motion through ideal MHD equations. Solving these hyperbolic PDEs requires sophisticated numerical methods, presenting computational challenges due to complex structures and high costs. Recent advances introduce neural operators like the Fourier Neural Operator (FNO) as surrogate models for traditional numerical analyses. This study explores a modified Flux Fourier neural operator model to approximate the numerical flux of ideal MHD, offering a novel approach that outperforms existing neural operator models by enabling continuous inference, generalization outside sampled distributions, and faster computation compared to classical numerical schemes.","sentences":["Magnetohydrodynamics (MHD) plays a pivotal role in describing the dynamics of plasma and conductive fluids, essential for understanding phenomena such as the structure and evolution of stars and galaxies, and in nuclear fusion for plasma motion through ideal MHD equations.","Solving these hyperbolic PDEs requires sophisticated numerical methods, presenting computational challenges due to complex structures and high costs.","Recent advances introduce neural operators like the Fourier Neural Operator (FNO) as surrogate models for traditional numerical analyses.","This study explores a modified Flux Fourier neural operator model to approximate the numerical flux of ideal MHD, offering a novel approach that outperforms existing neural operator models by enabling continuous inference, generalization outside sampled distributions, and faster computation compared to classical numerical schemes."],"url":"http://arxiv.org/abs/2404.16015v1","category":"physics.comp-ph"}
{"created":"2024-04-24 17:47:22","title":"Improving Dictionary Learning with Gated Sparse Autoencoders","abstract":"Recent work has found that sparse autoencoders (SAEs) are an effective technique for unsupervised discovery of interpretable features in language models' (LMs) activations, by finding sparse, linear reconstructions of LM activations. We introduce the Gated Sparse Autoencoder (Gated SAE), which achieves a Pareto improvement over training with prevailing methods. In SAEs, the L1 penalty used to encourage sparsity introduces many undesirable biases, such as shrinkage -- systematic underestimation of feature activations. The key insight of Gated SAEs is to separate the functionality of (a) determining which directions to use and (b) estimating the magnitudes of those directions: this enables us to apply the L1 penalty only to the former, limiting the scope of undesirable side effects. Through training SAEs on LMs of up to 7B parameters we find that, in typical hyper-parameter ranges, Gated SAEs solve shrinkage, are similarly interpretable, and require half as many firing features to achieve comparable reconstruction fidelity.","sentences":["Recent work has found that sparse autoencoders (SAEs) are an effective technique for unsupervised discovery of interpretable features in language models' (LMs) activations, by finding sparse, linear reconstructions of LM activations.","We introduce the Gated Sparse Autoencoder (Gated SAE), which achieves a Pareto improvement over training with prevailing methods.","In SAEs, the L1 penalty used to encourage sparsity introduces many undesirable biases, such as shrinkage -- systematic underestimation of feature activations.","The key insight of Gated SAEs is to separate the functionality of (a) determining which directions to use and (b) estimating the magnitudes of those directions: this enables us to apply the L1 penalty only to the former, limiting the scope of undesirable side effects.","Through training SAEs on LMs of up to 7B parameters we find that, in typical hyper-parameter ranges, Gated SAEs solve shrinkage, are similarly interpretable, and require half as many firing features to achieve comparable reconstruction fidelity."],"url":"http://arxiv.org/abs/2404.16014v1","category":"cs.LG"}
{"created":"2024-04-24 17:40:36","title":"Lessons Learned in Quadruped Deployment in Livestock Farming","abstract":"The livestock industry faces several challenges, including labor-intensive management, the threat of predators and environmental sustainability concerns. Therefore, this paper explores the integration of quadruped robots in extensive livestock farming as a novel application of field robotics. The SELF-AIR project, an acronym for Supporting Extensive Livestock Farming with the use of Autonomous Intelligent Robots, exemplifies this innovative approach. Through advanced sensors, artificial intelligence, and autonomous navigation systems, these robots exhibit remarkable capabilities in navigating diverse terrains, monitoring large herds, and aiding in various farming tasks. This work provides insight into the SELF-AIR project, presenting the lessons learned.","sentences":["The livestock industry faces several challenges, including labor-intensive management, the threat of predators and environmental sustainability concerns.","Therefore, this paper explores the integration of quadruped robots in extensive livestock farming as a novel application of field robotics.","The SELF-AIR project, an acronym for Supporting Extensive Livestock Farming with the use of Autonomous Intelligent Robots, exemplifies this innovative approach.","Through advanced sensors, artificial intelligence, and autonomous navigation systems, these robots exhibit remarkable capabilities in navigating diverse terrains, monitoring large herds, and aiding in various farming tasks.","This work provides insight into the SELF-AIR project, presenting the lessons learned."],"url":"http://arxiv.org/abs/2404.16008v1","category":"cs.RO"}
{"created":"2024-04-24 17:37:05","title":"MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI","abstract":"Large Vision-Language Models (LVLMs) show significant strides in general-purpose multimodal applications such as visual dialogue and embodied navigation. However, existing multimodal evaluation benchmarks cover a limited number of multimodal tasks testing rudimentary capabilities, falling short in tracking LVLM development. In this study, we present MMT-Bench, a comprehensive benchmark designed to assess LVLMs across massive multimodal tasks requiring expert knowledge and deliberate visual recognition, localization, reasoning, and planning. MMT-Bench comprises $31,325$ meticulously curated multi-choice visual questions from various multimodal scenarios such as vehicle driving and embodied navigation, covering $32$ core meta-tasks and $162$ subtasks in multimodal understanding. Due to its extensive task coverage, MMT-Bench enables the evaluation of LVLMs using a task map, facilitating the discovery of in- and out-of-domain tasks. Evaluation results involving $30$ LVLMs such as the proprietary GPT-4V, GeminiProVision, and open-sourced InternVL-Chat, underscore the significant challenges posed by MMT-Bench. We anticipate that MMT-Bench will inspire the community to develop next-generation multimodal foundation models aimed at achieving general-purpose multimodal intelligence.","sentences":["Large Vision-Language Models (LVLMs) show significant strides in general-purpose multimodal applications such as visual dialogue and embodied navigation.","However, existing multimodal evaluation benchmarks cover a limited number of multimodal tasks testing rudimentary capabilities, falling short in tracking LVLM development.","In this study, we present MMT-Bench, a comprehensive benchmark designed to assess LVLMs across massive multimodal tasks requiring expert knowledge and deliberate visual recognition, localization, reasoning, and planning.","MMT-Bench comprises $31,325$ meticulously curated multi-choice visual questions from various multimodal scenarios such as vehicle driving and embodied navigation, covering $32$ core meta-tasks and $162$ subtasks in multimodal understanding.","Due to its extensive task coverage, MMT-Bench enables the evaluation of LVLMs using a task map, facilitating the discovery of in- and out-of-domain tasks.","Evaluation results involving $30$ LVLMs such as the proprietary GPT-4V, GeminiProVision, and open-sourced InternVL-Chat, underscore the significant challenges posed by MMT-Bench.","We anticipate that MMT-Bench will inspire the community to develop next-generation multimodal foundation models aimed at achieving general-purpose multimodal intelligence."],"url":"http://arxiv.org/abs/2404.16006v1","category":"cs.CV"}
{"created":"2024-04-24 17:00:24","title":"Robust Independent Validation of Experiment and Theory: Rivet version 4 release note","abstract":"The Rivet toolkit is the primary mechanism for phenomenological preservation of collider-physics measurements, containing both a computational core and API for analysis implementation, and a large collection of more than a thousand preserved analyses. In this note we summarise the main changes in the new Rivet 4 major release series. These include a major generalisation and more semantically coherent model for histograms and related data objects, a thorough clean-up of inelegant and legacy observable-computation tools, and new systems for extended analysis-data, incorporation of preserved machine-learning models, and serialization for high-performance computing applications. Where these changes introduce backward-incompatible interface changes, existing analyses have been updated and indications are given on how to update new analysis routines and workflows.","sentences":["The Rivet toolkit is the primary mechanism for phenomenological preservation of collider-physics measurements, containing both a computational core and API for analysis implementation, and a large collection of more than a thousand preserved analyses.","In this note we summarise the main changes in the new Rivet 4 major release series.","These include a major generalisation and more semantically coherent model for histograms and related data objects, a thorough clean-up of inelegant and legacy observable-computation tools, and new systems for extended analysis-data, incorporation of preserved machine-learning models, and serialization for high-performance computing applications.","Where these changes introduce backward-incompatible interface changes, existing analyses have been updated and indications are given on how to update new analysis routines and workflows."],"url":"http://arxiv.org/abs/2404.15984v1","category":"hep-ph"}
{"created":"2024-04-24 16:47:42","title":"Detecting entanglement from macroscopic measurements of the electric field and its fluctuations","abstract":"To address the outstanding task of detecting entanglement in large quantum systems, entanglement witnesses have emerged, addressing the separable nature of a state. Yet optimizing witnesses, or accessing them experimentally, often remains a challenge. We here introduce a family of entanglement witnesses for open quantum systems, based on the electric field -- its quadratures and the total fluorescence. More general than spin-squeezing inequalities, it can detect new classes of entangled states, as changing the direction for far-field observation opens up a continuous family of witnesses, without the need for a state tomography. Their efficiency is demonstrated by detecting, from almost any direction, the entanglement of collective single-photon states, such as long-lived states generated by cooperative spontaneous emission. Able to detect entanglement in large quantum systems, these electric-field-based witnesses can be used on any set of emitters described by the Pauli group, such as atomic systems (cold atoms and trapped ions), giant atoms, color centers, and superconducting qubits.","sentences":["To address the outstanding task of detecting entanglement in large quantum systems, entanglement witnesses have emerged, addressing the separable nature of a state.","Yet optimizing witnesses, or accessing them experimentally, often remains a challenge.","We here introduce a family of entanglement witnesses for open quantum systems, based on the electric field -- its quadratures and the total fluorescence.","More general than spin-squeezing inequalities, it can detect new classes of entangled states, as changing the direction for far-field observation opens up a continuous family of witnesses, without the need for a state tomography.","Their efficiency is demonstrated by detecting, from almost any direction, the entanglement of collective single-photon states, such as long-lived states generated by cooperative spontaneous emission.","Able to detect entanglement in large quantum systems, these electric-field-based witnesses can be used on any set of emitters described by the Pauli group, such as atomic systems (cold atoms and trapped ions), giant atoms, color centers, and superconducting qubits."],"url":"http://arxiv.org/abs/2404.15973v1","category":"quant-ph"}
{"created":"2024-04-24 16:44:25","title":"Boosting Architectural Generation via Prompts: Report","abstract":"In the realm of AI architectural design, the importance of prompts is becoming increasingly prominent. With advancements in artificial intelligence and large-scale model technology, more design tasks are being delegated to machine learning algorithms. This necessitates a method for designers to guide algorithms in producing their desired designs. Prompts serve as a guiding and motivational mechanism, playing a crucial role in AI-generated architectural design. This paper categorizes and summarizes common vocabulary used in architectural design, discussing how to craft effective prompts and their impact on the quality and creativity of generated results. Through careful prompt design, designers can better control the generated architectural design images, thereby achieving designs that are more aligned with requirements and innovative.","sentences":["In the realm of AI architectural design, the importance of prompts is becoming increasingly prominent.","With advancements in artificial intelligence and large-scale model technology, more design tasks are being delegated to machine learning algorithms.","This necessitates a method for designers to guide algorithms in producing their desired designs.","Prompts serve as a guiding and motivational mechanism, playing a crucial role in AI-generated architectural design.","This paper categorizes and summarizes common vocabulary used in architectural design, discussing how to craft effective prompts and their impact on the quality and creativity of generated results.","Through careful prompt design, designers can better control the generated architectural design images, thereby achieving designs that are more aligned with requirements and innovative."],"url":"http://arxiv.org/abs/2404.15971v1","category":"cs.HC"}
{"created":"2024-04-24 16:30:12","title":"Soil analysis with machine-learning-based processing of stepped-frequency GPR field measurements: Preliminary study","abstract":"Ground Penetrating Radar (GPR) has been widely studied as a tool for extracting soil parameters relevant to agriculture and horticulture. When combined with Machine-Learning-based (ML) methods, high-resolution Stepped Frequency Countinuous Wave Radar (SFCW) measurements hold the promise to give cost effective access to depth resolved soil parameters, including at root-level depth. In a first step in this direction, we perform an extensive field survey with a tractor mounted SFCW GPR instrument. Using ML data processing we test the GPR instrument's capabilities to predict the apparent electrical conductivity (ECaR) as measured by a simultaneously recording Electromagnetic Induction (EMI) instrument. The large-scale field measurement campaign with 3472 co-registered and geo-located GPR and EMI data samples distributed over ~6600 square meters was performed on a golf course. The selected terrain benefits from a high surface homogeneity, but also features the challenge of only small, and hence hard to discern, variations in the measured soil parameter. Based on the quantitative results we suggest the use of nugget-to-sill ratio as a performance metric for the evaluation of end-to-end ML performance in the agricultural setting and discuss the limiting factors in the multi-sensor regression setting. The code is released as open source and available at https://opensource.silicon-austria.com/xuc/soil-analysis-machine-learning-stepped-frequency-gpr.","sentences":["Ground Penetrating Radar (GPR) has been widely studied as a tool for extracting soil parameters relevant to agriculture and horticulture.","When combined with Machine-Learning-based (ML) methods, high-resolution Stepped Frequency Countinuous Wave Radar (SFCW) measurements hold the promise to give cost effective access to depth resolved soil parameters, including at root-level depth.","In a first step in this direction, we perform an extensive field survey with a tractor mounted SFCW GPR instrument.","Using ML data processing we test the GPR instrument's capabilities to predict the apparent electrical conductivity (ECaR) as measured by a simultaneously recording Electromagnetic Induction (EMI) instrument.","The large-scale field measurement campaign with 3472 co-registered and geo-located GPR and EMI data samples distributed over ~6600 square meters was performed on a golf course.","The selected terrain benefits from a high surface homogeneity, but also features the challenge of only small, and hence hard to discern, variations in the measured soil parameter.","Based on the quantitative results we suggest the use of nugget-to-sill ratio as a performance metric for the evaluation of end-to-end ML performance in the agricultural setting and discuss the limiting factors in the multi-sensor regression setting.","The code is released as open source and available at https://opensource.silicon-austria.com/xuc/soil-analysis-machine-learning-stepped-frequency-gpr."],"url":"http://arxiv.org/abs/2404.15961v1","category":"eess.SP"}
{"created":"2024-04-24 16:11:54","title":"Sequence can Secretly Tell You What to Discard","abstract":"Large Language Models (LLMs), despite their impressive performance on a wide range of tasks, require significant GPU memory and consume substantial computational resources. In addition to model weights, the memory occupied by KV cache increases linearly with sequence length, becoming a main bottleneck for inference. In this paper, we introduce a novel approach for optimizing the KV cache which significantly reduces its memory footprint. Through a comprehensive investigation, we find that on LLaMA2 series models, (i) the similarity between adjacent tokens' query vectors is remarkably high, and (ii) current query's attention calculation can rely solely on the attention information of a small portion of the preceding queries. Based on these observations, we propose CORM, a KV cache eviction policy that dynamically retains important key-value pairs for inference without finetuning the model. We validate that CORM reduces the inference memory usage of KV cache by up to 70% without noticeable performance degradation across six tasks in LongBench.","sentences":["Large Language Models (LLMs), despite their impressive performance on a wide range of tasks, require significant GPU memory and consume substantial computational resources.","In addition to model weights, the memory occupied by KV cache increases linearly with sequence length, becoming a main bottleneck for inference.","In this paper, we introduce a novel approach for optimizing the KV cache which significantly reduces its memory footprint.","Through a comprehensive investigation, we find that on LLaMA2 series models, (i) the similarity between adjacent tokens' query vectors is remarkably high, and (ii) current query's attention calculation can rely solely on the attention information of a small portion of the preceding queries.","Based on these observations, we propose CORM, a KV cache eviction policy that dynamically retains important key-value pairs for inference without finetuning the model.","We validate that CORM reduces the inference memory usage of KV cache by up to 70% without noticeable performance degradation across six tasks in LongBench."],"url":"http://arxiv.org/abs/2404.15949v1","category":"cs.CL"}
{"created":"2024-04-24 16:07:31","title":"Mammo-CLIP: Leveraging Contrastive Language-Image Pre-training (CLIP) for Enhanced Breast Cancer Diagnosis with Multi-view Mammography","abstract":"Although fusion of information from multiple views of mammograms plays an important role to increase accuracy of breast cancer detection, developing multi-view mammograms-based computer-aided diagnosis (CAD) schemes still faces challenges and no such CAD schemes have been used in clinical practice. To overcome the challenges, we investigate a new approach based on Contrastive Language-Image Pre-training (CLIP), which has sparked interest across various medical imaging tasks. By solving the challenges in (1) effectively adapting the single-view CLIP for multi-view feature fusion and (2) efficiently fine-tuning this parameter-dense model with limited samples and computational resources, we introduce Mammo-CLIP, the first multi-modal framework to process multi-view mammograms and corresponding simple texts. Mammo-CLIP uses an early feature fusion strategy to learn multi-view relationships in four mammograms acquired from the CC and MLO views of the left and right breasts. To enhance learning efficiency, plug-and-play adapters are added into CLIP image and text encoders for fine-tuning parameters and limiting updates to about 1% of the parameters. For framework evaluation, we assembled two datasets retrospectively. The first dataset, comprising 470 malignant and 479 benign cases, was used for few-shot fine-tuning and internal evaluation of the proposed Mammo-CLIP via 5-fold cross-validation. The second dataset, including 60 malignant and 294 benign cases, was used to test generalizability of Mammo-CLIP. Study results show that Mammo-CLIP outperforms the state-of-art cross-view transformer in AUC (0.841 vs. 0.817, 0.837 vs. 0.807) on both datasets. It also surpasses previous two CLIP-based methods by 20.3% and 14.3%. This study highlights the potential of applying the finetuned vision-language models for developing next-generation, image-text-based CAD schemes of breast cancer.","sentences":["Although fusion of information from multiple views of mammograms plays an important role to increase accuracy of breast cancer detection, developing multi-view mammograms-based computer-aided diagnosis (CAD) schemes still faces challenges and no such CAD schemes have been used in clinical practice.","To overcome the challenges, we investigate a new approach based on Contrastive Language-Image Pre-training (CLIP), which has sparked interest across various medical imaging tasks.","By solving the challenges in (1) effectively adapting the single-view CLIP for multi-view feature fusion and (2) efficiently fine-tuning this parameter-dense model with limited samples and computational resources, we introduce Mammo-CLIP, the first multi-modal framework to process multi-view mammograms and corresponding simple texts.","Mammo-CLIP uses an early feature fusion strategy to learn multi-view relationships in four mammograms acquired from the CC and MLO views of the left and right breasts.","To enhance learning efficiency, plug-and-play adapters are added into CLIP image and text encoders for fine-tuning parameters and limiting updates to about 1% of the parameters.","For framework evaluation, we assembled two datasets retrospectively.","The first dataset, comprising 470 malignant and 479 benign cases, was used for few-shot fine-tuning and internal evaluation of the proposed Mammo-CLIP via 5-fold cross-validation.","The second dataset, including 60 malignant and 294 benign cases, was used to test generalizability of Mammo-CLIP.","Study results show that Mammo-CLIP outperforms the state-of-art cross-view transformer in AUC (0.841 vs. 0.817, 0.837 vs. 0.807) on both datasets.","It also surpasses previous two CLIP-based methods by 20.3% and 14.3%.","This study highlights the potential of applying the finetuned vision-language models for developing next-generation, image-text-based CAD schemes of breast cancer."],"url":"http://arxiv.org/abs/2404.15946v1","category":"cs.CV"}
{"created":"2024-04-24 16:05:29","title":"The mathematical work of Anania Shirakatsi","abstract":"This paper aims to give a brief account of the mathematical work of the 7th-century Armenian polymath and natural philosopher Anania Shirakatsi. The three sections of Anania's ``Book of Arithmetic'' -- tables of arithmetic operations, a list of problems and their answers, and a collection of entertaining puzzles -- are presented and discussed, the focus being on the problems and solutions. A close examination of the structure of these problems reveals that Anania's proficiency in arithmetic was considerably more sophisticated than their mathematical contents might suggest. The geography of Anania's problems is illustrated through two maps highlighting the locations referenced within these problems.","sentences":["This paper aims to give a brief account of the mathematical work of the 7th-century Armenian polymath and natural philosopher Anania Shirakatsi.","The three sections of Anania's ``Book of Arithmetic'' -- tables of arithmetic operations, a list of problems and their answers, and a collection of entertaining puzzles -- are presented and discussed, the focus being on the problems and solutions.","A close examination of the structure of these problems reveals that Anania's proficiency in arithmetic was considerably more sophisticated than their mathematical contents might suggest.","The geography of Anania's problems is illustrated through two maps highlighting the locations referenced within these problems."],"url":"http://arxiv.org/abs/2404.15945v1","category":"math.HO"}
{"created":"2024-04-24 16:03:34","title":"Decentralized Personalized Federated Learning based on a Conditional Sparse-to-Sparser Scheme","abstract":"Decentralized Federated Learning (DFL) has become popular due to its robustness and avoidance of centralized coordination. In this paradigm, clients actively engage in training by exchanging models with their networked neighbors. However, DFL introduces increased costs in terms of training and communication. Existing methods focus on minimizing communication often overlooking training efficiency and data heterogeneity. To address this gap, we propose a novel \\textit{sparse-to-sparser} training scheme: DA-DPFL. DA-DPFL initializes with a subset of model parameters, which progressively reduces during training via \\textit{dynamic aggregation} and leads to substantial energy savings while retaining adequate information during critical learning periods.   Our experiments showcase that DA-DPFL substantially outperforms DFL baselines in test accuracy, while achieving up to $5$ times reduction in energy costs. We provide a theoretical analysis of DA-DPFL's convergence by solidifying its applicability in decentralized and personalized learning. The code is available at:https://github.com/EricLoong/da-dpfl","sentences":["Decentralized Federated Learning (DFL) has become popular due to its robustness and avoidance of centralized coordination.","In this paradigm, clients actively engage in training by exchanging models with their networked neighbors.","However, DFL introduces increased costs in terms of training and communication.","Existing methods focus on minimizing communication often overlooking training efficiency and data heterogeneity.","To address this gap, we propose a novel \\textit{sparse-to-sparser} training scheme: DA-DPFL.","DA-DPFL initializes with a subset of model parameters, which progressively reduces during training via \\textit{dynamic aggregation} and leads to substantial energy savings while retaining adequate information during critical learning periods.   ","Our experiments showcase that DA-DPFL substantially outperforms DFL baselines in test accuracy, while achieving up to $5$ times reduction in energy costs.","We provide a theoretical analysis of DA-DPFL's convergence by solidifying its applicability in decentralized and personalized learning.","The code is available at:https://github.com/EricLoong/da-dpfl"],"url":"http://arxiv.org/abs/2404.15943v2","category":"cs.LG"}
{"created":"2024-04-24 15:27:25","title":"KGValidator: A Framework for Automatic Validation of Knowledge Graph Construction","abstract":"This study explores the use of Large Language Models (LLMs) for automatic evaluation of knowledge graph (KG) completion models. Historically, validating information in KGs has been a challenging task, requiring large-scale human annotation at prohibitive cost. With the emergence of general-purpose generative AI and LLMs, it is now plausible that human-in-the-loop validation could be replaced by a generative agent. We introduce a framework for consistency and validation when using generative models to validate knowledge graphs. Our framework is based upon recent open-source developments for structural and semantic validation of LLM outputs, and upon flexible approaches to fact checking and verification, supported by the capacity to reference external knowledge sources of any kind. The design is easy to adapt and extend, and can be used to verify any kind of graph-structured data through a combination of model-intrinsic knowledge, user-supplied context, and agents capable of external knowledge retrieval.","sentences":["This study explores the use of Large Language Models (LLMs) for automatic evaluation of knowledge graph (KG) completion models.","Historically, validating information in KGs has been a challenging task, requiring large-scale human annotation at prohibitive cost.","With the emergence of general-purpose generative AI and LLMs, it is now plausible that human-in-the-loop validation could be replaced by a generative agent.","We introduce a framework for consistency and validation when using generative models to validate knowledge graphs.","Our framework is based upon recent open-source developments for structural and semantic validation of LLM outputs, and upon flexible approaches to fact checking and verification, supported by the capacity to reference external knowledge sources of any kind.","The design is easy to adapt and extend, and can be used to verify any kind of graph-structured data through a combination of model-intrinsic knowledge, user-supplied context, and agents capable of external knowledge retrieval."],"url":"http://arxiv.org/abs/2404.15923v1","category":"cs.AI"}
{"created":"2024-04-24 15:04:39","title":"Collective lattice excitations in the dynamic route for melting hydrodynamic 2D-crystals","abstract":"Surface stiffnesses engender steady patterns of Faraday waves (FWs), so called hydrodynamic crystals as correspond to ordered wave lattices made of discrete subharmonics under monochromatic driving. Mastering rules are both inertia-imposed parametric resonance for frequency-halving together with rigidity-driven nonlinearity for wavefield self-focusing. They harness the discretization needed for coherent FW-packets to localize in space and time. Collective lattice excitations are observed as dispersionless propagating dislocations that lead periodic modulations arising from explicit symmetry breaking. In a field theory perspective, a halving genesis for the collective distorting modes is revealed as the natural pathway for hydrodynamic crystal melting.","sentences":["Surface stiffnesses engender steady patterns of Faraday waves (FWs), so called hydrodynamic crystals as correspond to ordered wave lattices made of discrete subharmonics under monochromatic driving.","Mastering rules are both inertia-imposed parametric resonance for frequency-halving together with rigidity-driven nonlinearity for wavefield self-focusing.","They harness the discretization needed for coherent FW-packets to localize in space and time.","Collective lattice excitations are observed as dispersionless propagating dislocations that lead periodic modulations arising from explicit symmetry breaking.","In a field theory perspective, a halving genesis for the collective distorting modes is revealed as the natural pathway for hydrodynamic crystal melting."],"url":"http://arxiv.org/abs/2404.15912v1","category":"nlin.PS"}
{"created":"2024-04-24 14:57:37","title":"Drawing the Line: Deep Segmentation for Extracting Art from Ancient Etruscan Mirrors","abstract":"Etruscan mirrors constitute a significant category within Etruscan art and, therefore, undergo systematic examinations to obtain insights into ancient times. A crucial aspect of their analysis involves the labor-intensive task of manually tracing engravings from the backside. Additionally, this task is inherently challenging due to the damage these mirrors have sustained, introducing subjectivity into the process. We address these challenges by automating the process through photometric-stereo scanning in conjunction with deep segmentation networks which, however, requires effective usage of the limited data at hand. We accomplish this by incorporating predictions on a per-patch level, and various data augmentations, as well as exploring self-supervised learning. Compared to our baseline, we improve predictive performance w.r.t. the pseudo-F-Measure by around 16%. When assessing performance on complete mirrors against a human baseline, our approach yields quantitative similar performance to a human annotator and significantly outperforms existing binarization methods. With our proposed methodology, we streamline the annotation process, enhance its objectivity, and reduce overall workload, offering a valuable contribution to the examination of these historical artifacts and other non-traditional documents.","sentences":["Etruscan mirrors constitute a significant category within Etruscan art and, therefore, undergo systematic examinations to obtain insights into ancient times.","A crucial aspect of their analysis involves the labor-intensive task of manually tracing engravings from the backside.","Additionally, this task is inherently challenging due to the damage these mirrors have sustained, introducing subjectivity into the process.","We address these challenges by automating the process through photometric-stereo scanning in conjunction with deep segmentation networks which, however, requires effective usage of the limited data at hand.","We accomplish this by incorporating predictions on a per-patch level, and various data augmentations, as well as exploring self-supervised learning.","Compared to our baseline, we improve predictive performance w.r.t.","the pseudo-F-Measure by around 16%.","When assessing performance on complete mirrors against a human baseline, our approach yields quantitative similar performance to a human annotator and significantly outperforms existing binarization methods.","With our proposed methodology, we streamline the annotation process, enhance its objectivity, and reduce overall workload, offering a valuable contribution to the examination of these historical artifacts and other non-traditional documents."],"url":"http://arxiv.org/abs/2404.15903v1","category":"cs.CV"}
{"created":"2024-04-24 14:41:41","title":"ST-MambaSync: The Confluence of Mamba Structure and Spatio-Temporal Transformers for Precipitous Traffic Prediction","abstract":"Balancing accuracy with computational efficiency is paramount in machine learning, particularly when dealing with high-dimensional data, such as spatial-temporal datasets. This study introduces ST-MambaSync, an innovative framework that integrates a streamlined attention layer with a simplified state-space layer. The model achieves competitive accuracy in spatial-temporal prediction tasks. We delve into the relationship between attention mechanisms and the Mamba component, revealing that Mamba functions akin to attention within a residual network structure. This comparative analysis underpins the efficiency of state-space models, elucidating their capability to deliver superior performance at reduced computational costs.","sentences":["Balancing accuracy with computational efficiency is paramount in machine learning, particularly when dealing with high-dimensional data, such as spatial-temporal datasets.","This study introduces ST-MambaSync, an innovative framework that integrates a streamlined attention layer with a simplified state-space layer.","The model achieves competitive accuracy in spatial-temporal prediction tasks.","We delve into the relationship between attention mechanisms and the Mamba component, revealing that Mamba functions akin to attention within a residual network structure.","This comparative analysis underpins the efficiency of state-space models, elucidating their capability to deliver superior performance at reduced computational costs."],"url":"http://arxiv.org/abs/2404.15899v1","category":"cs.LG"}
{"created":"2024-04-24 14:32:34","title":"Assessing The Potential Of Mid-Sized Language Models For Clinical QA","abstract":"Large language models, such as GPT-4 and Med-PaLM, have shown impressive performance on clinical tasks; however, they require access to compute, are closed-source, and cannot be deployed on device. Mid-size models such as BioGPT-large, BioMedLM, LLaMA 2, and Mistral 7B avoid these drawbacks, but their capacity for clinical tasks has been understudied. To help assess their potential for clinical use and help researchers decide which model they should use, we compare their performance on two clinical question-answering (QA) tasks: MedQA and consumer query answering. We find that Mistral 7B is the best performing model, winning on all benchmarks and outperforming models trained specifically for the biomedical domain. While Mistral 7B's MedQA score of 63.0% approaches the original Med-PaLM, and it often can produce plausible responses to consumer health queries, room for improvement still exists. This study provides the first head-to-head assessment of open source mid-sized models on clinical tasks.","sentences":["Large language models, such as GPT-4 and Med-PaLM, have shown impressive performance on clinical tasks; however, they require access to compute, are closed-source, and cannot be deployed on device.","Mid-size models such as BioGPT-large, BioMedLM, LLaMA 2, and Mistral 7B avoid these drawbacks, but their capacity for clinical tasks has been understudied.","To help assess their potential for clinical use and help researchers decide which model they should use, we compare their performance on two clinical question-answering (QA) tasks: MedQA and consumer query answering.","We find that Mistral 7B is the best performing model, winning on all benchmarks and outperforming models trained specifically for the biomedical domain.","While Mistral 7B's MedQA score of 63.0% approaches the original Med-PaLM, and it often can produce plausible responses to consumer health queries, room for improvement still exists.","This study provides the first head-to-head assessment of open source mid-sized models on clinical tasks."],"url":"http://arxiv.org/abs/2404.15894v1","category":"cs.CL"}
{"created":"2024-04-24 14:23:01","title":"Near-Optimal Wafer-Scale Reduce","abstract":"Efficient Reduce and AllReduce communication collectives are a critical cornerstone of high-performance computing (HPC) applications. We present the first systematic investigation of Reduce and AllReduce on the Cerebras Wafer-Scale Engine (WSE). This architecture has been shown to achieve unprecedented performance both for machine learning workloads and other computational problems like FFT. We introduce a performance model to estimate the execution time of algorithms on the WSE and validate our predictions experimentally for a wide range of input sizes. In addition to existing implementations, we design and implement several new algorithms specifically tailored to the architecture. Moreover, we establish a lower bound for the runtime of a Reduce operation on the WSE. Based on our model, we automatically generate code that achieves near-optimal performance across the whole range of input sizes. Experiments demonstrate that our new Reduce and AllReduce algorithms outperform the current vendor solution by up to 3.27x. Additionally, our model predicts performance with less than 4% error. The proposed communication collectives increase the range of HPC applications that can benefit from the high throughput of the WSE. Our model-driven methodology demonstrates a disciplined approach that can lead the way to further algorithmic advancements on wafer-scale architectures.","sentences":["Efficient Reduce and AllReduce communication collectives are a critical cornerstone of high-performance computing (HPC) applications.","We present the first systematic investigation of Reduce and AllReduce on the Cerebras Wafer-Scale Engine (WSE).","This architecture has been shown to achieve unprecedented performance both for machine learning workloads and other computational problems like FFT.","We introduce a performance model to estimate the execution time of algorithms on the WSE and validate our predictions experimentally for a wide range of input sizes.","In addition to existing implementations, we design and implement several new algorithms specifically tailored to the architecture.","Moreover, we establish a lower bound for the runtime of a Reduce operation on the WSE.","Based on our model, we automatically generate code that achieves near-optimal performance across the whole range of input sizes.","Experiments demonstrate that our new Reduce and AllReduce algorithms outperform the current vendor solution by up to 3.27x.","Additionally, our model predicts performance with less than 4% error.","The proposed communication collectives increase the range of HPC applications that can benefit from the high throughput of the WSE.","Our model-driven methodology demonstrates a disciplined approach that can lead the way to further algorithmic advancements on wafer-scale architectures."],"url":"http://arxiv.org/abs/2404.15888v1","category":"cs.DC"}
{"created":"2024-04-24 13:59:19","title":"Unexplored Faces of Robustness and Out-of-Distribution: Covariate Shifts in Environment and Sensor Domains","abstract":"Computer vision applications predict on digital images acquired by a camera from physical scenes through light. However, conventional robustness benchmarks rely on perturbations in digitized images, diverging from distribution shifts occurring in the image acquisition process. To bridge this gap, we introduce a new distribution shift dataset, ImageNet-ES, comprising variations in environmental and camera sensor factors by directly capturing 202k images with a real camera in a controllable testbed. With the new dataset, we evaluate out-of-distribution (OOD) detection and model robustness. We find that existing OOD detection methods do not cope with the covariate shifts in ImageNet-ES, implying that the definition and detection of OOD should be revisited to embrace real-world distribution shifts. We also observe that the model becomes more robust in both ImageNet-C and -ES by learning environment and sensor variations in addition to existing digital augmentations. Lastly, our results suggest that effective shift mitigation via camera sensor control can significantly improve performance without increasing model size. With these findings, our benchmark may aid future research on robustness, OOD, and camera sensor control for computer vision. Our code and dataset are available at https://github.com/Edw2n/ImageNet-ES.","sentences":["Computer vision applications predict on digital images acquired by a camera from physical scenes through light.","However, conventional robustness benchmarks rely on perturbations in digitized images, diverging from distribution shifts occurring in the image acquisition process.","To bridge this gap, we introduce a new distribution shift dataset, ImageNet-ES, comprising variations in environmental and camera sensor factors by directly capturing 202k images with a real camera in a controllable testbed.","With the new dataset, we evaluate out-of-distribution (OOD) detection and model robustness.","We find that existing OOD detection methods do not cope with the covariate shifts in ImageNet-ES, implying that the definition and detection of OOD should be revisited to embrace real-world distribution shifts.","We also observe that the model becomes more robust in both ImageNet-C and -ES by learning environment and sensor variations in addition to existing digital augmentations.","Lastly, our results suggest that effective shift mitigation via camera sensor control can significantly improve performance without increasing model size.","With these findings, our benchmark may aid future research on robustness, OOD, and camera sensor control for computer vision.","Our code and dataset are available at https://github.com/Edw2n/ImageNet-ES."],"url":"http://arxiv.org/abs/2404.15882v2","category":"cs.CV"}
{"created":"2024-04-24 13:51:56","title":"Steal Now and Attack Later: Evaluating Robustness of Object Detection against Black-box Adversarial Attacks","abstract":"Latency attacks against object detection represent a variant of adversarial attacks that aim to inflate the inference time by generating additional ghost objects in a target image. However, generating ghost objects in the black-box scenario remains a challenge since information about these unqualified objects remains opaque. In this study, we demonstrate the feasibility of generating ghost objects in adversarial examples by extending the concept of \"steal now, decrypt later\" attacks. These adversarial examples, once produced, can be employed to exploit potential vulnerabilities in the AI service, giving rise to significant security concerns. The experimental results demonstrate that the proposed attack achieves successful attacks across various commonly used models and Google Vision API without any prior knowledge about the target model. Additionally, the average cost of each attack is less than \\$ 1 dollars, posing a significant threat to AI security.","sentences":["Latency attacks against object detection represent a variant of adversarial attacks that aim to inflate the inference time by generating additional ghost objects in a target image.","However, generating ghost objects in the black-box scenario remains a challenge since information about these unqualified objects remains opaque.","In this study, we demonstrate the feasibility of generating ghost objects in adversarial examples by extending the concept of \"steal now, decrypt later\" attacks.","These adversarial examples, once produced, can be employed to exploit potential vulnerabilities in the AI service, giving rise to significant security concerns.","The experimental results demonstrate that the proposed attack achieves successful attacks across various commonly used models and Google Vision API without any prior knowledge about the target model.","Additionally, the average cost of each attack is less than \\$ 1 dollars, posing a significant threat to AI security."],"url":"http://arxiv.org/abs/2404.15881v1","category":"cs.CV"}
{"created":"2024-04-24 13:48:38","title":"Revisiting Out-of-Distribution Detection in LiDAR-based 3D Object Detection","abstract":"LiDAR-based 3D object detection has become an essential part of automated driving due to its ability to localize and classify objects precisely in 3D. However, object detectors face a critical challenge when dealing with unknown foreground objects, particularly those that were not present in their original training data. These out-of-distribution (OOD) objects can lead to misclassifications, posing a significant risk to the safety and reliability of automated vehicles. Currently, LiDAR-based OOD object detection has not been well studied. We address this problem by generating synthetic training data for OOD objects by perturbing known object categories. Our idea is that these synthetic OOD objects produce different responses in the feature map of an object detector compared to in-distribution (ID) objects. We then extract features using a pre-trained and fixed object detector and train a simple multilayer perceptron (MLP) to classify each detection as either ID or OOD. In addition, we propose a new evaluation protocol that allows the use of existing datasets without modifying the point cloud, ensuring a more authentic evaluation of real-world scenarios. The effectiveness of our method is validated through experiments on the newly proposed nuScenes OOD benchmark. The source code is available at https://github.com/uulm-mrm/mmood3d.","sentences":["LiDAR-based 3D object detection has become an essential part of automated driving due to its ability to localize and classify objects precisely in 3D.","However, object detectors face a critical challenge when dealing with unknown foreground objects, particularly those that were not present in their original training data.","These out-of-distribution (OOD) objects can lead to misclassifications, posing a significant risk to the safety and reliability of automated vehicles.","Currently, LiDAR-based OOD object detection has not been well studied.","We address this problem by generating synthetic training data for OOD objects by perturbing known object categories.","Our idea is that these synthetic OOD objects produce different responses in the feature map of an object detector compared to in-distribution (ID) objects.","We then extract features using a pre-trained and fixed object detector and train a simple multilayer perceptron (MLP) to classify each detection as either ID or OOD.","In addition, we propose a new evaluation protocol that allows the use of existing datasets without modifying the point cloud, ensuring a more authentic evaluation of real-world scenarios.","The effectiveness of our method is validated through experiments on the newly proposed nuScenes OOD benchmark.","The source code is available at https://github.com/uulm-mrm/mmood3d."],"url":"http://arxiv.org/abs/2404.15879v1","category":"cs.CV"}
{"created":"2024-04-24 13:34:20","title":"Semantic Routing for Enhanced Performance of LLM-Assisted Intent-Based 5G Core Network Management and Orchestration","abstract":"Large language models (LLMs) are rapidly emerging in Artificial Intelligence (AI) applications, especially in the fields of natural language processing and generative AI. Not limited to text generation applications, these models inherently possess the opportunity to leverage prompt engineering, where the inputs of such models can be appropriately structured to articulate a model's purpose explicitly. A prominent example of this is intent-based networking, an emerging approach for automating and maintaining network operations and management. This paper presents semantic routing to achieve enhanced performance in LLM-assisted intent-based management and orchestration of 5G core networks. This work establishes an end-to-end intent extraction framework and presents a diverse dataset of sample user intents accompanied by a thorough analysis of the effects of encoders and quantization on overall system performance. The results show that using a semantic router improves the accuracy and efficiency of the LLM deployment compared to stand-alone LLMs with prompting architectures.","sentences":["Large language models (LLMs) are rapidly emerging in Artificial Intelligence (AI) applications, especially in the fields of natural language processing and generative AI.","Not limited to text generation applications, these models inherently possess the opportunity to leverage prompt engineering, where the inputs of such models can be appropriately structured to articulate a model's purpose explicitly.","A prominent example of this is intent-based networking, an emerging approach for automating and maintaining network operations and management.","This paper presents semantic routing to achieve enhanced performance in LLM-assisted intent-based management and orchestration of 5G core networks.","This work establishes an end-to-end intent extraction framework and presents a diverse dataset of sample user intents accompanied by a thorough analysis of the effects of encoders and quantization on overall system performance.","The results show that using a semantic router improves the accuracy and efficiency of the LLM deployment compared to stand-alone LLMs with prompting architectures."],"url":"http://arxiv.org/abs/2404.15869v1","category":"cs.NI"}
{"created":"2024-04-24 13:23:03","title":"Enhancing Diagnosis through AI-driven Analysis of Reflectance Confocal Microscopy","abstract":"Reflectance Confocal Microscopy (RCM) is a non-invasive imaging technique used in biomedical research and clinical dermatology. It provides virtual high-resolution images of the skin and superficial tissues, reducing the need for physical biopsies. RCM employs a laser light source to illuminate the tissue, capturing the reflected light to generate detailed images of microscopic structures at various depths. Recent studies explored AI and machine learning, particularly CNNs, for analyzing RCM images. Our study proposes a segmentation strategy based on textural features to identify clinically significant regions, empowering dermatologists in effective image interpretation and boosting diagnostic confidence. This approach promises to advance dermatological diagnosis and treatment.","sentences":["Reflectance Confocal Microscopy (RCM) is a non-invasive imaging technique used in biomedical research and clinical dermatology.","It provides virtual high-resolution images of the skin and superficial tissues, reducing the need for physical biopsies.","RCM employs a laser light source to illuminate the tissue, capturing the reflected light to generate detailed images of microscopic structures at various depths.","Recent studies explored AI and machine learning, particularly CNNs, for analyzing RCM images.","Our study proposes a segmentation strategy based on textural features to identify clinically significant regions, empowering dermatologists in effective image interpretation and boosting diagnostic confidence.","This approach promises to advance dermatological diagnosis and treatment."],"url":"http://arxiv.org/abs/2404.16080v1","category":"eess.IV"}
{"created":"2024-04-24 13:02:33","title":"QOPTLib: a Quantum Computing Oriented Benchmark for Combinatorial Optimization Problems","abstract":"In this paper, we propose a quantum computing oriented benchmark for combinatorial optimization. This benchmark, coined as QOPTLib, is composed of 40 instances equally distributed over four well-known problems: Traveling Salesman Problem, Vehicle Routing Problem, one-dimensional Bin Packing Problem and the Maximum Cut Problem. The sizes of the instances in QOPTLib not only correspond to computationally addressable sizes, but also to the maximum length approachable with non-zero likelihood of getting a good result. In this regard, it is important to highlight that hybrid approaches are also taken into consideration. Thus, this benchmark constitutes the first effort to provide users a general-purpose dataset. Also in this paper, we introduce a first full solving of QOPTLib using two solvers based on quantum annealing. Our main intention with this is to establish a preliminary baseline, hoping to inspire other researchers to beat these outcomes with newly proposed quantum-based algorithms.","sentences":["In this paper, we propose a quantum computing oriented benchmark for combinatorial optimization.","This benchmark, coined as QOPTLib, is composed of 40 instances equally distributed over four well-known problems: Traveling Salesman Problem, Vehicle Routing Problem, one-dimensional Bin Packing Problem and the Maximum Cut Problem.","The sizes of the instances in QOPTLib not only correspond to computationally addressable sizes, but also to the maximum length approachable with non-zero likelihood of getting a good result.","In this regard, it is important to highlight that hybrid approaches are also taken into consideration.","Thus, this benchmark constitutes the first effort to provide users a general-purpose dataset.","Also in this paper, we introduce a first full solving of QOPTLib using two solvers based on quantum annealing.","Our main intention with this is to establish a preliminary baseline, hoping to inspire other researchers to beat these outcomes with newly proposed quantum-based algorithms."],"url":"http://arxiv.org/abs/2404.15852v1","category":"quant-ph"}
{"created":"2024-04-24 12:58:33","title":"Jamming memory into acoustically trained dense suspensions under shear","abstract":"Systems driven far from equilibrium often retain structural memories of their processing history. This memory has, in some cases, been shown to dramatically alter the material response. For example, work hardening in crystalline metals can alter the hardness, yield strength, and tensile strength to prevent catastrophic failure. Whether memory of processing history can be similarly exploited in flowing systems, where significantly larger changes in structure should be possible, remains poorly understood. Here, we demonstrate a promising route to embedding such useful memories. We build on work showing that exposing a sheared dense suspension to acoustic perturbations of different power allows for dramatically tuning the sheared suspension viscosity and underlying structure. We find that, for sufficiently dense suspensions, upon removing the acoustic perturbations, the suspension shear jams with shear stress contributions from the maximum compressive and maximum extensive axes that reflect the acoustic training. Because the contributions from these two orthogonal axes to the total shear stress are antagonistic, it is possible to tune the resulting suspension response in surprising ways. For example, we show that differently trained sheared suspensions exhibit: 1) different susceptibility to the same acoustic perturbation; 2) orders of magnitude changes in their instantaneous viscosities upon shear reversal; and 3) even a shear stress that increases in magnitude upon shear cessation. To further illustrate the power of this approach for controlling suspension properties, we demonstrate that flowing states well below the shear jamming threshold can be shear jammed via acoustic training. Collectively, our work paves the way for using acoustically induced memory in dense suspensions to generate rapidly and widely tunable materials.","sentences":["Systems driven far from equilibrium often retain structural memories of their processing history.","This memory has, in some cases, been shown to dramatically alter the material response.","For example, work hardening in crystalline metals can alter the hardness, yield strength, and tensile strength to prevent catastrophic failure.","Whether memory of processing history can be similarly exploited in flowing systems, where significantly larger changes in structure should be possible, remains poorly understood.","Here, we demonstrate a promising route to embedding such useful memories.","We build on work showing that exposing a sheared dense suspension to acoustic perturbations of different power allows for dramatically tuning the sheared suspension viscosity and underlying structure.","We find that, for sufficiently dense suspensions, upon removing the acoustic perturbations, the suspension shear jams with shear stress contributions from the maximum compressive and maximum extensive axes that reflect the acoustic training.","Because the contributions from these two orthogonal axes to the total shear stress are antagonistic, it is possible to tune the resulting suspension response in surprising ways.","For example, we show that differently trained sheared suspensions exhibit: 1) different susceptibility to the same acoustic perturbation; 2) orders of magnitude changes in their instantaneous viscosities upon shear reversal; and 3) even a shear stress that increases in magnitude upon shear cessation.","To further illustrate the power of this approach for controlling suspension properties, we demonstrate that flowing states well below the shear jamming threshold can be shear jammed via acoustic training.","Collectively, our work paves the way for using acoustically induced memory in dense suspensions to generate rapidly and widely tunable materials."],"url":"http://arxiv.org/abs/2404.15850v1","category":"cond-mat.soft"}
{"created":"2024-04-24 12:41:04","title":"Learning World Models With Hierarchical Temporal Abstractions: A Probabilistic Perspective","abstract":"Machines that can replicate human intelligence with type 2 reasoning capabilities should be able to reason at multiple levels of spatio-temporal abstractions and scales using internal world models. Devising formalisms to develop such internal world models, which accurately reflect the causal hierarchies inherent in the dynamics of the real world, is a critical research challenge in the domains of artificial intelligence and machine learning. This thesis identifies several limitations with the prevalent use of state space models (SSMs) as internal world models and propose two new probabilistic formalisms namely Hidden-Parameter SSMs and Multi-Time Scale SSMs to address these drawbacks. The structure of graphical models in both formalisms facilitates scalable exact probabilistic inference using belief propagation, as well as end-to-end learning via backpropagation through time. This approach permits the development of scalable, adaptive hierarchical world models capable of representing nonstationary dynamics across multiple temporal abstractions and scales. Moreover, these probabilistic formalisms integrate the concept of uncertainty in world states, thus improving the system's capacity to emulate the stochastic nature of the real world and quantify the confidence in its predictions. The thesis also discuss how these formalisms are in line with related neuroscience literature on Bayesian brain hypothesis and predicitive processing. Our experiments on various real and simulated robots demonstrate that our formalisms can match and in many cases exceed the performance of contemporary transformer variants in making long-range future predictions. We conclude the thesis by reflecting on the limitations of our current models and suggesting directions for future research.","sentences":["Machines that can replicate human intelligence with type 2 reasoning capabilities should be able to reason at multiple levels of spatio-temporal abstractions and scales using internal world models.","Devising formalisms to develop such internal world models, which accurately reflect the causal hierarchies inherent in the dynamics of the real world, is a critical research challenge in the domains of artificial intelligence and machine learning.","This thesis identifies several limitations with the prevalent use of state space models (SSMs) as internal world models and propose two new probabilistic formalisms namely Hidden-Parameter SSMs and Multi-Time Scale SSMs to address these drawbacks.","The structure of graphical models in both formalisms facilitates scalable exact probabilistic inference using belief propagation, as well as end-to-end learning via backpropagation through time.","This approach permits the development of scalable, adaptive hierarchical world models capable of representing nonstationary dynamics across multiple temporal abstractions and scales.","Moreover, these probabilistic formalisms integrate the concept of uncertainty in world states, thus improving the system's capacity to emulate the stochastic nature of the real world and quantify the confidence in its predictions.","The thesis also discuss how these formalisms are in line with related neuroscience literature on Bayesian brain hypothesis and predicitive processing.","Our experiments on various real and simulated robots demonstrate that our formalisms can match and in many cases exceed the performance of contemporary transformer variants in making long-range future predictions.","We conclude the thesis by reflecting on the limitations of our current models and suggesting directions for future research."],"url":"http://arxiv.org/abs/2404.16078v1","category":"cs.AI"}
{"created":"2024-04-24 12:36:06","title":"Extending Cislunar Communication Network Reach Using Reconfigurable Intelligent Surfaces","abstract":"This study introduces a novel approach to enhance communication networks in the cislunar space by leveraging Reconfigurable Intelligent Surfaces (RIS). Using the ability of RIS to dynamically control electromagnetic waves, this paper tackles the challenges of signal attenuation, directivity, and divergence in cislunar missions, primarily caused by immense distances and that Earth-based station transmitters do not always face the Moon. A new optimization problem is formulated, whose objective is to maximize the received signal-to-noise ratio (SNR) for Earth-to-Moon communications. We derive a closed-form solution to the problem of determining the optimal RIS phase shift configuration based on the effective area of the RIS. Through extensive simulations, this paper demonstrates how optimal adjustments in RIS phase shifts can significantly enhance signal integrity, hinting at the substantial potential of RIS technology to revolutionize long-distance cislunar communication.","sentences":["This study introduces a novel approach to enhance communication networks in the cislunar space by leveraging Reconfigurable Intelligent Surfaces (RIS).","Using the ability of RIS to dynamically control electromagnetic waves, this paper tackles the challenges of signal attenuation, directivity, and divergence in cislunar missions, primarily caused by immense distances and that Earth-based station transmitters do not always face the Moon.","A new optimization problem is formulated, whose objective is to maximize the received signal-to-noise ratio (SNR) for Earth-to-Moon communications.","We derive a closed-form solution to the problem of determining the optimal RIS phase shift configuration based on the effective area of the RIS.","Through extensive simulations, this paper demonstrates how optimal adjustments in RIS phase shifts can significantly enhance signal integrity, hinting at the substantial potential of RIS technology to revolutionize long-distance cislunar communication."],"url":"http://arxiv.org/abs/2404.15842v1","category":"cs.ET"}
{"created":"2024-04-24 12:28:27","title":"Constructive Interpolation and Concept-Based Beth Definability for Description Logics via Sequents","abstract":"We introduce a constructive method applicable to a large number of description logics (DLs) for establishing the concept-based Beth definability property (CBP) based on sequent systems. Using the highly expressive DL RIQ as a case study, we introduce novel sequent calculi for RIQ-ontologies and show how certain interpolants can be computed from sequent calculus proofs, which permit the extraction of explicit definitions of implicitly definable concepts. To the best of our knowledge, this is the first sequent-based approach to computing interpolants and definitions within the context of DLs, as well as the first proof that RIQ enjoys the CBP. Moreover, due to the modularity of our sequent systems, our results hold for any restriction of RIQ, and are applicable to other DLs by suitable modifications.","sentences":["We introduce a constructive method applicable to a large number of description logics (DLs) for establishing the concept-based Beth definability property (CBP) based on sequent systems.","Using the highly expressive DL RIQ as a case study, we introduce novel sequent calculi for RIQ-ontologies and show how certain interpolants can be computed from sequent calculus proofs, which permit the extraction of explicit definitions of implicitly definable concepts.","To the best of our knowledge, this is the first sequent-based approach to computing interpolants and definitions within the context of DLs, as well as the first proof that RIQ enjoys the CBP.","Moreover, due to the modularity of our sequent systems, our results hold for any restriction of RIQ, and are applicable to other DLs by suitable modifications."],"url":"http://arxiv.org/abs/2404.15840v1","category":"cs.LO"}
{"created":"2024-04-24 12:05:20","title":"Accelerating Cavity Fault Prediction Using Deep Learning at Jefferson Laboratory","abstract":"Accelerating cavities are an integral part of the Continuous Electron Beam Accelerator Facility (CEBAF) at Jefferson Laboratory. When any of the over 400 cavities in CEBAF experiences a fault, it disrupts beam delivery to experimental user halls. In this study, we propose the use of a deep learning model to predict slowly developing cavity faults. By utilizing pre-fault signals, we train a LSTM-CNN binary classifier to distinguish between radio-frequency (RF) signals during normal operation and RF signals indicative of impending faults. We optimize the model by adjusting the fault confidence threshold and implementing a multiple consecutive window criterion to identify fault events, ensuring a low false positive rate. Results obtained from analysis of a real dataset collected from the accelerating cavities simulating a deployed scenario demonstrate the model's ability to identify normal signals with 99.99% accuracy and correctly predict 80% of slowly developing faults. Notably, these achievements were achieved in the context of a highly imbalanced dataset, and fault predictions were made several hundred milliseconds before the onset of the fault. Anticipating faults enables preemptive measures to improve operational efficiency by preventing or mitigating their occurrence.","sentences":["Accelerating cavities are an integral part of the Continuous Electron Beam Accelerator Facility (CEBAF) at Jefferson Laboratory.","When any of the over 400 cavities in CEBAF experiences a fault, it disrupts beam delivery to experimental user halls.","In this study, we propose the use of a deep learning model to predict slowly developing cavity faults.","By utilizing pre-fault signals, we train a LSTM-CNN binary classifier to distinguish between radio-frequency (RF) signals during normal operation and RF signals indicative of impending faults.","We optimize the model by adjusting the fault confidence threshold and implementing a multiple consecutive window criterion to identify fault events, ensuring a low false positive rate.","Results obtained from analysis of a real dataset collected from the accelerating cavities simulating a deployed scenario demonstrate the model's ability to identify normal signals with 99.99% accuracy and correctly predict 80% of slowly developing faults.","Notably, these achievements were achieved in the context of a highly imbalanced dataset, and fault predictions were made several hundred milliseconds before the onset of the fault.","Anticipating faults enables preemptive measures to improve operational efficiency by preventing or mitigating their occurrence."],"url":"http://arxiv.org/abs/2404.15829v1","category":"physics.acc-ph"}
{"created":"2024-04-24 11:57:37","title":"A Configurable and Efficient Memory Hierarchy for Neural Network Hardware Accelerator","abstract":"As machine learning applications continue to evolve, the demand for efficient hardware accelerators, specifically tailored for deep neural networks (DNNs), becomes increasingly vital. In this paper, we propose a configurable memory hierarchy framework tailored for per layer adaptive memory access patterns of DNNs. The hierarchy requests data on-demand from the off-chip memory to provide it to the accelerator's compute units. The objective is to strike an optimized balance between minimizing the required memory capacity and maintaining high accelerator performance. The framework is characterized by its configurability, allowing the creation of a tailored memory hierarchy with up to five levels. Furthermore, the framework incorporates an optional shift register as final level to increase the flexibility of the memory management process. A comprehensive loop-nest analysis of DNN layers shows that the framework can efficiently execute the access patterns of most loop unrolls. Synthesis results and a case study of the DNN accelerator UltraTrail indicate a possible reduction in chip area of up to 62.2% as smaller memory modules can be used. At the same time, the performance loss can be minimized to 2.4%.","sentences":["As machine learning applications continue to evolve, the demand for efficient hardware accelerators, specifically tailored for deep neural networks (DNNs), becomes increasingly vital.","In this paper, we propose a configurable memory hierarchy framework tailored for per layer adaptive memory access patterns of DNNs.","The hierarchy requests data on-demand from the off-chip memory to provide it to the accelerator's compute units.","The objective is to strike an optimized balance between minimizing the required memory capacity and maintaining high accelerator performance.","The framework is characterized by its configurability, allowing the creation of a tailored memory hierarchy with up to five levels.","Furthermore, the framework incorporates an optional shift register as final level to increase the flexibility of the memory management process.","A comprehensive loop-nest analysis of DNN layers shows that the framework can efficiently execute the access patterns of most loop unrolls.","Synthesis results and a case study of the DNN accelerator UltraTrail indicate a possible reduction in chip area of up to 62.2% as smaller memory modules can be used.","At the same time, the performance loss can be minimized to 2.4%."],"url":"http://arxiv.org/abs/2404.15823v1","category":"cs.AR"}
{"created":"2024-04-24 11:54:53","title":"Recursive Backwards Q-Learning in Deterministic Environments","abstract":"Reinforcement learning is a popular method of finding optimal solutions to complex problems. Algorithms like Q-learning excel at learning to solve stochastic problems without a model of their environment. However, they take longer to solve deterministic problems than is necessary. Q-learning can be improved to better solve deterministic problems by introducing such a model-based approach. This paper introduces the recursive backwards Q-learning (RBQL) agent, which explores and builds a model of the environment. After reaching a terminal state, it recursively propagates its value backwards through this model. This lets each state be evaluated to its optimal value without a lengthy learning process. In the example of finding the shortest path through a maze, this agent greatly outperforms a regular Q-learning agent.","sentences":["Reinforcement learning is a popular method of finding optimal solutions to complex problems.","Algorithms like Q-learning excel at learning to solve stochastic problems without a model of their environment.","However, they take longer to solve deterministic problems than is necessary.","Q-learning can be improved to better solve deterministic problems by introducing such a model-based approach.","This paper introduces the recursive backwards Q-learning (RBQL) agent, which explores and builds a model of the environment.","After reaching a terminal state, it recursively propagates its value backwards through this model.","This lets each state be evaluated to its optimal value without a lengthy learning process.","In the example of finding the shortest path through a maze, this agent greatly outperforms a regular Q-learning agent."],"url":"http://arxiv.org/abs/2404.15822v1","category":"cs.AI"}
{"created":"2024-04-24 11:35:02","title":"Fast Ensembling with Diffusion Schr\u00f6dinger Bridge","abstract":"Deep Ensemble (DE) approach is a straightforward technique used to enhance the performance of deep neural networks by training them from different initial points, converging towards various local optima. However, a limitation of this methodology lies in its high computational overhead for inference, arising from the necessity to store numerous learned parameters and execute individual forward passes for each parameter during the inference stage. We propose a novel approach called Diffusion Bridge Network (DBN) to address this challenge. Based on the theory of the Schr\\\"odinger bridge, this method directly learns to simulate an Stochastic Differential Equation (SDE) that connects the output distribution of a single ensemble member to the output distribution of the ensembled model, allowing us to obtain ensemble prediction without having to invoke forward pass through all the ensemble models. By substituting the heavy ensembles with this lightweight neural network constructing DBN, we achieved inference with reduced computational cost while maintaining accuracy and uncertainty scores on benchmark datasets such as CIFAR-10, CIFAR-100, and TinyImageNet. Our implementation is available at https://github.com/kim-hyunsu/dbn.","sentences":["Deep Ensemble (DE) approach is a straightforward technique used to enhance the performance of deep neural networks by training them from different initial points, converging towards various local optima.","However, a limitation of this methodology lies in its high computational overhead for inference, arising from the necessity to store numerous learned parameters and execute individual forward passes for each parameter during the inference stage.","We propose a novel approach called Diffusion Bridge Network (DBN) to address this challenge.","Based on the theory of the Schr\\\"odinger bridge, this method directly learns to simulate an Stochastic Differential Equation (SDE) that connects the output distribution of a single ensemble member to the output distribution of the ensembled model, allowing us to obtain ensemble prediction without having to invoke forward pass through all the ensemble models.","By substituting the heavy ensembles with this lightweight neural network constructing DBN, we achieved inference with reduced computational cost while maintaining accuracy and uncertainty scores on benchmark datasets such as CIFAR-10, CIFAR-100, and TinyImageNet.","Our implementation is available at https://github.com/kim-hyunsu/dbn."],"url":"http://arxiv.org/abs/2404.15814v1","category":"cs.LG"}
{"created":"2024-04-24 11:03:15","title":"GeckOpt: LLM System Efficiency via Intent-Based Tool Selection","abstract":"In this preliminary study, we investigate a GPT-driven intent-based reasoning approach to streamline tool selection for large language models (LLMs) aimed at system efficiency. By identifying the intent behind user prompts at runtime, we narrow down the API toolset required for task execution, reducing token consumption by up to 24.6\\%. Early results on a real-world, massively parallel Copilot platform with over 100 GPT-4-Turbo nodes show cost reductions and potential towards improving LLM-based system efficiency.","sentences":["In this preliminary study, we investigate a GPT-driven intent-based reasoning approach to streamline tool selection for large language models (LLMs) aimed at system efficiency.","By identifying the intent behind user prompts at runtime, we narrow down the API toolset required for task execution, reducing token consumption by up to 24.6\\%.","Early results on a real-world, massively parallel Copilot platform with over 100 GPT-4-Turbo nodes show cost reductions and potential towards improving LLM-based system efficiency."],"url":"http://arxiv.org/abs/2404.15804v1","category":"cs.LG"}
{"created":"2024-04-24 11:02:13","title":"Raformer: Redundancy-Aware Transformer for Video Wire Inpainting","abstract":"Video Wire Inpainting (VWI) is a prominent application in video inpainting, aimed at flawlessly removing wires in films or TV series, offering significant time and labor savings compared to manual frame-by-frame removal. However, wire removal poses greater challenges due to the wires being longer and slimmer than objects typically targeted in general video inpainting tasks, and often intersecting with people and background objects irregularly, which adds complexity to the inpainting process. Recognizing the limitations posed by existing video wire datasets, which are characterized by their small size, poor quality, and limited variety of scenes, we introduce a new VWI dataset with a novel mask generation strategy, namely Wire Removal Video Dataset 2 (WRV2) and Pseudo Wire-Shaped (PWS) Masks. WRV2 dataset comprises over 4,000 videos with an average length of 80 frames, designed to facilitate the development and efficacy of inpainting models. Building upon this, our research proposes the Redundancy-Aware Transformer (Raformer) method that addresses the unique challenges of wire removal in video inpainting. Unlike conventional approaches that indiscriminately process all frame patches, Raformer employs a novel strategy to selectively bypass redundant parts, such as static background segments devoid of valuable information for inpainting. At the core of Raformer is the Redundancy-Aware Attention (RAA) module, which isolates and accentuates essential content through a coarse-grained, window-based attention mechanism. This is complemented by a Soft Feature Alignment (SFA) module, which refines these features and achieves end-to-end feature alignment. Extensive experiments on both the traditional video inpainting datasets and our proposed WRV2 dataset demonstrate that Raformer outperforms other state-of-the-art methods.","sentences":["Video Wire Inpainting (VWI) is a prominent application in video inpainting, aimed at flawlessly removing wires in films or TV series, offering significant time and labor savings compared to manual frame-by-frame removal.","However, wire removal poses greater challenges due to the wires being longer and slimmer than objects typically targeted in general video inpainting tasks, and often intersecting with people and background objects irregularly, which adds complexity to the inpainting process.","Recognizing the limitations posed by existing video wire datasets, which are characterized by their small size, poor quality, and limited variety of scenes, we introduce a new VWI dataset with a novel mask generation strategy, namely Wire Removal Video Dataset 2 (WRV2) and Pseudo Wire-Shaped (PWS) Masks.","WRV2 dataset comprises over 4,000 videos with an average length of 80 frames, designed to facilitate the development and efficacy of inpainting models.","Building upon this, our research proposes the Redundancy-Aware Transformer (Raformer) method that addresses the unique challenges of wire removal in video inpainting.","Unlike conventional approaches that indiscriminately process all frame patches, Raformer employs a novel strategy to selectively bypass redundant parts, such as static background segments devoid of valuable information for inpainting.","At the core of Raformer is the Redundancy-Aware Attention (RAA) module, which isolates and accentuates essential content through a coarse-grained, window-based attention mechanism.","This is complemented by a Soft Feature Alignment (SFA) module, which refines these features and achieves end-to-end feature alignment.","Extensive experiments on both the traditional video inpainting datasets and our proposed WRV2 dataset demonstrate that Raformer outperforms other state-of-the-art methods."],"url":"http://arxiv.org/abs/2404.15802v1","category":"cs.CV"}
{"created":"2024-04-24 10:59:20","title":"MYCloth: Towards Intelligent and Interactive Online T-Shirt Customization based on User's Preference","abstract":"In conventional online T-shirt customization, consumers, \\ie, users, can achieve the intended design only after repeated adjustments of the design prototypes presented by sellers in online dialogues. However, this process is prone to limited visual feedback and cumbersome communication, thus detracting from users' customization experience and time. This paper presents an intelligent and interactive online customization system, named \\textbf{MYCloth}, aiming to enhance the T-shirt customization experience. Given the user's text input, our MYCloth employs ChatGPT to refine the text prompt and generate the intended paint of the cloth via the Stable Diffusion model. Our MYCloth also enables the user to preview the final outcome via a novel learning-based virtual try-on model. The whole system allows to iteratively adjust the cloth till optimal design is achieved. We verify the system's efficacy through a series of performance evaluations and user studies, highlighting its ability to streamline the online customization process and improve overall satisfaction.","sentences":["In conventional online T-shirt customization, consumers, \\ie, users, can achieve the intended design only after repeated adjustments of the design prototypes presented by sellers in online dialogues.","However, this process is prone to limited visual feedback and cumbersome communication, thus detracting from users' customization experience and time.","This paper presents an intelligent and interactive online customization system, named \\textbf{MYCloth}, aiming to enhance the T-shirt customization experience.","Given the user's text input, our MYCloth employs ChatGPT to refine the text prompt and generate the intended paint of the cloth via the Stable Diffusion model.","Our MYCloth also enables the user to preview the final outcome via a novel learning-based virtual try-on model.","The whole system allows to iteratively adjust the cloth till optimal design is achieved.","We verify the system's efficacy through a series of performance evaluations and user studies, highlighting its ability to streamline the online customization process and improve overall satisfaction."],"url":"http://arxiv.org/abs/2404.15801v1","category":"cs.HC"}
{"created":"2024-04-24 10:43:31","title":"Optimal Experimental Design for Large-Scale Inverse Problems via Multi-PDE-constrained Optimization","abstract":"Accurate parameter dependent electro-chemical numerical models for lithium-ion batteries are essential in industrial application. The exact parameters of each battery cell are unknown and a process of estimation is necessary to infer them. The parameter estimation generates an accurate model able to reproduce real cell data. The field of optimal input/experimental design deals with creating the experimental settings facilitating the estimation problem. Here we apply two different input design algorithms that aim at maximizing the observability of the true, unknown parameters: in the first algorithm, we design the applied current and the starting voltage. This lets the algorithm collect information on different states of charge, but requires long experimental times (60 000 s). In the second algorithm, we generate a continuous current, composed of concatenated optimal intervals. In this case, the experimental time is shorter (7000 s) and numerical experiments with virtual data give an even better accuracy results, but experiments with real battery data reveal that the accuracy could decrease hundredfold. As the design algorithms are built independent of the model, the same results and motivation are applicable to more complex battery cell models and, moreover, to other applications.","sentences":["Accurate parameter dependent electro-chemical numerical models for lithium-ion batteries are essential in industrial application.","The exact parameters of each battery cell are unknown and a process of estimation is necessary to infer them.","The parameter estimation generates an accurate model able to reproduce real cell data.","The field of optimal input/experimental design deals with creating the experimental settings facilitating the estimation problem.","Here we apply two different input design algorithms that aim at maximizing the observability of the true, unknown parameters: in the first algorithm, we design the applied current and the starting voltage.","This lets the algorithm collect information on different states of charge, but requires long experimental times (60 000 s).","In the second algorithm, we generate a continuous current, composed of concatenated optimal intervals.","In this case, the experimental time is shorter (7000 s) and numerical experiments with virtual data give an even better accuracy results, but experiments with real battery data reveal that the accuracy could decrease hundredfold.","As the design algorithms are built independent of the model, the same results and motivation are applicable to more complex battery cell models and, moreover, to other applications."],"url":"http://arxiv.org/abs/2404.15797v1","category":"math.ST"}
{"created":"2024-04-24 10:35:36","title":"Large Language Models as In-context AI Generators for Quality-Diversity","abstract":"Quality-Diversity (QD) approaches are a promising direction to develop open-ended processes as they can discover archives of high-quality solutions across diverse niches. While already successful in many applications, QD approaches usually rely on combining only one or two solutions to generate new candidate solutions. As observed in open-ended processes such as technological evolution, wisely combining large diversity of these solutions could lead to more innovative solutions and potentially boost the productivity of QD search. In this work, we propose to exploit the pattern-matching capabilities of generative models to enable such efficient solution combinations. We introduce In-context QD, a framework of techniques that aim to elicit the in-context capabilities of pre-trained Large Language Models (LLMs) to generate interesting solutions using the QD archive as context. Applied to a series of common QD domains, In-context QD displays promising results compared to both QD baselines and similar strategies developed for single-objective optimization. Additionally, this result holds across multiple values of parameter sizes and archive population sizes, as well as across domains with distinct characteristics from BBO functions to policy search. Finally, we perform an extensive ablation that highlights the key prompt design considerations that encourage the generation of promising solutions for QD.","sentences":["Quality-Diversity (QD) approaches are a promising direction to develop open-ended processes as they can discover archives of high-quality solutions across diverse niches.","While already successful in many applications, QD approaches usually rely on combining only one or two solutions to generate new candidate solutions.","As observed in open-ended processes such as technological evolution, wisely combining large diversity of these solutions could lead to more innovative solutions and potentially boost the productivity of QD search.","In this work, we propose to exploit the pattern-matching capabilities of generative models to enable such efficient solution combinations.","We introduce In-context QD, a framework of techniques that aim to elicit the in-context capabilities of pre-trained Large Language Models (LLMs) to generate interesting solutions using the QD archive as context.","Applied to a series of common QD domains, In-context QD displays promising results compared to both QD baselines and similar strategies developed for single-objective optimization.","Additionally, this result holds across multiple values of parameter sizes and archive population sizes, as well as across domains with distinct characteristics from BBO functions to policy search.","Finally, we perform an extensive ablation that highlights the key prompt design considerations that encourage the generation of promising solutions for QD."],"url":"http://arxiv.org/abs/2404.15794v1","category":"cs.NE"}
{"created":"2024-04-24 10:19:25","title":"Rethinking Model Prototyping through the MedMNIST+ Dataset Collection","abstract":"The integration of deep learning based systems in clinical practice is often impeded by challenges rooted in limited and heterogeneous medical datasets. In addition, prioritization of marginal performance improvements on a few, narrowly scoped benchmarks over clinical applicability has slowed down meaningful algorithmic progress. This trend often results in excessive fine-tuning of existing methods to achieve state-of-the-art performance on selected datasets rather than fostering clinically relevant innovations. In response, this work presents a comprehensive benchmark for the MedMNIST+ database to diversify the evaluation landscape and conduct a thorough analysis of common convolutional neural networks (CNNs) and Transformer-based architectures, for medical image classification. Our evaluation encompasses various medical datasets, training methodologies, and input resolutions, aiming to reassess the strengths and limitations of widely used model variants. Our findings suggest that computationally efficient training schemes and modern foundation models hold promise in bridging the gap between expensive end-to-end training and more resource-refined approaches. Additionally, contrary to prevailing assumptions, we observe that higher resolutions may not consistently improve performance beyond a certain threshold, advocating for the use of lower resolutions, particularly in prototyping stages, to expedite processing. Notably, our analysis reaffirms the competitiveness of convolutional models compared to ViT-based architectures emphasizing the importance of comprehending the intrinsic capabilities of different model architectures. Moreover, we hope that our standardized evaluation framework will help enhance transparency, reproducibility, and comparability on the MedMNIST+ dataset collection as well as future research within the field. Code will be released soon.","sentences":["The integration of deep learning based systems in clinical practice is often impeded by challenges rooted in limited and heterogeneous medical datasets.","In addition, prioritization of marginal performance improvements on a few, narrowly scoped benchmarks over clinical applicability has slowed down meaningful algorithmic progress.","This trend often results in excessive fine-tuning of existing methods to achieve state-of-the-art performance on selected datasets rather than fostering clinically relevant innovations.","In response, this work presents a comprehensive benchmark for the MedMNIST+ database to diversify the evaluation landscape and conduct a thorough analysis of common convolutional neural networks (CNNs) and Transformer-based architectures, for medical image classification.","Our evaluation encompasses various medical datasets, training methodologies, and input resolutions, aiming to reassess the strengths and limitations of widely used model variants.","Our findings suggest that computationally efficient training schemes and modern foundation models hold promise in bridging the gap between expensive end-to-end training and more resource-refined approaches.","Additionally, contrary to prevailing assumptions, we observe that higher resolutions may not consistently improve performance beyond a certain threshold, advocating for the use of lower resolutions, particularly in prototyping stages, to expedite processing.","Notably, our analysis reaffirms the competitiveness of convolutional models compared to ViT-based architectures emphasizing the importance of comprehending the intrinsic capabilities of different model architectures.","Moreover, we hope that our standardized evaluation framework will help enhance transparency, reproducibility, and comparability on the MedMNIST+ dataset collection as well as future research within the field.","Code will be released soon."],"url":"http://arxiv.org/abs/2404.15786v1","category":"eess.IV"}
{"created":"2024-04-24 10:03:37","title":"Real-Time Compressed Sensing for Joint Hyperspectral Image Transmission and Restoration for CubeSat","abstract":"This paper addresses the challenges associated with hyperspectral image (HSI) reconstruction from miniaturized satellites, which often suffer from stripe effects and are computationally resource-limited. We propose a Real-Time Compressed Sensing (RTCS) network designed to be lightweight and require only relatively few training samples for efficient and robust HSI reconstruction in the presence of the stripe effect and under noisy transmission conditions. The RTCS network features a simplified architecture that reduces the required training samples and allows for easy implementation on integer-8-based encoders, facilitating rapid compressed sensing for stripe-like HSI, which exactly matches the moderate design of miniaturized satellites on push broom scanning mechanism. This contrasts optimization-based models that demand high-precision floating-point operations, making them difficult to deploy on edge devices. Our encoder employs an integer-8-compatible linear projection for stripe-like HSI data transmission, ensuring real-time compressed sensing. Furthermore, based on the novel two-streamed architecture, an efficient HSI restoration decoder is proposed for the receiver side, allowing for edge-device reconstruction without needing a sophisticated central server. This is particularly crucial as an increasing number of miniaturized satellites necessitates significant computing resources on the ground station. Extensive experiments validate the superior performance of our approach, offering new and vital capabilities for existing miniaturized satellite systems.","sentences":["This paper addresses the challenges associated with hyperspectral image (HSI) reconstruction from miniaturized satellites, which often suffer from stripe effects and are computationally resource-limited.","We propose a Real-Time Compressed Sensing (RTCS) network designed to be lightweight and require only relatively few training samples for efficient and robust HSI reconstruction in the presence of the stripe effect and under noisy transmission conditions.","The RTCS network features a simplified architecture that reduces the required training samples and allows for easy implementation on integer-8-based encoders, facilitating rapid compressed sensing for stripe-like HSI, which exactly matches the moderate design of miniaturized satellites on push broom scanning mechanism.","This contrasts optimization-based models that demand high-precision floating-point operations, making them difficult to deploy on edge devices.","Our encoder employs an integer-8-compatible linear projection for stripe-like HSI data transmission, ensuring real-time compressed sensing.","Furthermore, based on the novel two-streamed architecture, an efficient HSI restoration decoder is proposed for the receiver side, allowing for edge-device reconstruction without needing a sophisticated central server.","This is particularly crucial as an increasing number of miniaturized satellites necessitates significant computing resources on the ground station.","Extensive experiments validate the superior performance of our approach, offering new and vital capabilities for existing miniaturized satellite systems."],"url":"http://arxiv.org/abs/2404.15781v1","category":"cs.CV"}
{"created":"2024-04-24 09:55:17","title":"The Past, Present, and Future of Plant Stress Research","abstract":"Life finds a way. For sessile organisms like plants, the need to adapt to changes in the environment is even more poignant. For humanity, the need to develop crops that can grow in diverse environments and feed our growing population is an existential one. The development of fast-growing, high-yielding crop varieties sparked the Green Revolution, and the advent of the genomics era enabled the development of customized transgenic crops enhanced for specific traits or resistances. Today, the proliferation of artificial intelligence (AI) allows scientists to rapidly screen through massive and complex datasets to uncover elusive patterns in the data, enabling us to create more robust and faster models for prediction and hypothesis generation in a bid to develop more stress-resilient plants. This review aims to provide an overview of the evolution of environmental stress research across the plant kingdom over the past fifty years. It will cover historical landmark concepts and discoveries that were seminal in advancing the field, provide a global snapshot of our current scientific progress, and conclude with a discussion on the advent of AI tools that would help accelerate scientific discovery.","sentences":["Life finds a way.","For sessile organisms like plants, the need to adapt to changes in the environment is even more poignant.","For humanity, the need to develop crops that can grow in diverse environments and feed our growing population is an existential one.","The development of fast-growing, high-yielding crop varieties sparked the Green Revolution, and the advent of the genomics era enabled the development of customized transgenic crops enhanced for specific traits or resistances.","Today, the proliferation of artificial intelligence (AI) allows scientists to rapidly screen through massive and complex datasets to uncover elusive patterns in the data, enabling us to create more robust and faster models for prediction and hypothesis generation in a bid to develop more stress-resilient plants.","This review aims to provide an overview of the evolution of environmental stress research across the plant kingdom over the past fifty years.","It will cover historical landmark concepts and discoveries that were seminal in advancing the field, provide a global snapshot of our current scientific progress, and conclude with a discussion on the advent of AI tools that would help accelerate scientific discovery."],"url":"http://arxiv.org/abs/2404.15776v1","category":"q-bio.QM"}
{"created":"2024-04-24 09:52:36","title":"Toward Physics-Aware Deep Learning Architectures for LiDAR Intensity Simulation","abstract":"Autonomous vehicles (AVs) heavily rely on LiDAR perception for environment understanding and navigation. LiDAR intensity provides valuable information about the reflected laser signals and plays a crucial role in enhancing the perception capabilities of AVs. However, accurately simulating LiDAR intensity remains a challenge due to the unavailability of material properties of the objects in the environment, and complex interactions between the laser beam and the environment. The proposed method aims to improve the accuracy of intensity simulation by incorporating physics-based modalities within the deep learning framework. One of the key entities that captures the interaction between the laser beam and the objects is the angle of incidence. In this work we demonstrate that the addition of the LiDAR incidence angle as a separate input to the deep neural networks significantly enhances the results. We present a comparative study between two prominent deep learning architectures: U-NET a Convolutional Neural Network (CNN), and Pix2Pix a Generative Adversarial Network (GAN). We implemented these two architectures for the intensity prediction task and used SemanticKITTI and VoxelScape datasets for experiments. The comparative analysis reveals that both architectures benefit from the incidence angle as an additional input. Moreover, the Pix2Pix architecture outperforms U-NET, especially when the incidence angle is incorporated.","sentences":["Autonomous vehicles (AVs) heavily rely on LiDAR perception for environment understanding and navigation.","LiDAR intensity provides valuable information about the reflected laser signals and plays a crucial role in enhancing the perception capabilities of AVs.","However, accurately simulating LiDAR intensity remains a challenge due to the unavailability of material properties of the objects in the environment, and complex interactions between the laser beam and the environment.","The proposed method aims to improve the accuracy of intensity simulation by incorporating physics-based modalities within the deep learning framework.","One of the key entities that captures the interaction between the laser beam and the objects is the angle of incidence.","In this work we demonstrate that the addition of the LiDAR incidence angle as a separate input to the deep neural networks significantly enhances the results.","We present a comparative study between two prominent deep learning architectures: U-NET a Convolutional Neural Network (CNN), and Pix2Pix a Generative Adversarial Network (GAN).","We implemented these two architectures for the intensity prediction task and used SemanticKITTI and VoxelScape datasets for experiments.","The comparative analysis reveals that both architectures benefit from the incidence angle as an additional input.","Moreover, the Pix2Pix architecture outperforms U-NET, especially when the incidence angle is incorporated."],"url":"http://arxiv.org/abs/2404.15774v1","category":"cs.CV"}
{"created":"2024-04-24 09:39:06","title":"Unifying Bayesian Flow Networks and Diffusion Models through Stochastic Differential Equations","abstract":"Bayesian flow networks (BFNs) iteratively refine the parameters, instead of the samples in diffusion models (DMs), of distributions at various noise levels through Bayesian inference. Owing to its differentiable nature, BFNs are promising in modeling both continuous and discrete data, while simultaneously maintaining fast sampling capabilities. This paper aims to understand and enhance BFNs by connecting them with DMs through stochastic differential equations (SDEs). We identify the linear SDEs corresponding to the noise-addition processes in BFNs, demonstrate that BFN's regression losses are aligned with denoise score matching, and validate the sampler in BFN as a first-order solver for the respective reverse-time SDE. Based on these findings and existing recipes of fast sampling in DMs, we propose specialized solvers for BFNs that markedly surpass the original BFN sampler in terms of sample quality with a limited number of function evaluations (e.g., 10) on both image and text datasets. Notably, our best sampler achieves an increase in speed of 5~20 times for free. Our code is available at https://github.com/ML-GSAI/BFN-Solver.","sentences":["Bayesian flow networks (BFNs) iteratively refine the parameters, instead of the samples in diffusion models (DMs), of distributions at various noise levels through Bayesian inference.","Owing to its differentiable nature, BFNs are promising in modeling both continuous and discrete data, while simultaneously maintaining fast sampling capabilities.","This paper aims to understand and enhance BFNs by connecting them with DMs through stochastic differential equations (SDEs).","We identify the linear SDEs corresponding to the noise-addition processes in BFNs, demonstrate that BFN's regression losses are aligned with denoise score matching, and validate the sampler in BFN as a first-order solver for the respective reverse-time SDE.","Based on these findings and existing recipes of fast sampling in DMs, we propose specialized solvers for BFNs that markedly surpass the original BFN sampler in terms of sample quality with a limited number of function evaluations (e.g., 10) on both image and text datasets.","Notably, our best sampler achieves an increase in speed of 5~20 times for free.","Our code is available at https://github.com/ML-GSAI/BFN-Solver."],"url":"http://arxiv.org/abs/2404.15766v1","category":"cs.LG"}
{"created":"2024-04-24 09:34:11","title":"Rechargeable UAV Trajectory Optimization for Real-Time Persistent Data Collection of Large-Scale Sensor Networks","abstract":"Continuous real-time data collection in wireless sensor networks is crucial for facilitating timely decision-making and environmental monitoring. Unmanned aerial vehicles (UAVs) have received plenty of attention for collecting data efficiently due to their high flexibility and enhanced communication ability, nonetheless, the limited onboard energy restricts UAVs' application on persistent missions, such as disaster search and rescue. In this paper, we propose a rechargeable UAV-assisted periodic data collection scheme, where the UAV replenishes energy through the wireless charging platform during the mission to provide persistent information services for the sensor nodes (SNs). Specifically, the total completion time is minimized by optimizing the trajectory of the UAV to reach the balance among the collecting time, flight time, and recharging time. However, optimally solving this problem is highly non-trivial due to the non-convex constraints and the involved integer variables. To address this issue, the formulated problem is decomposed into two subproblems, namely, UAV data collection trajectory optimization and SN clustering and UAV visiting order optimization. By exploiting the convex optimization techniques and proving the total time is non-decreasing with the cluster number, a periodic trajectory optimization algorithm based on successive convex approximation (SCA) and bisection search is proposed to solve the main problem. The simulation results show the efficiency of the proposed scheme in practical scenarios and the completion time of the proposed algorithm is on average 39% and 33% lower than the two benchmarks, respectively.","sentences":["Continuous real-time data collection in wireless sensor networks is crucial for facilitating timely decision-making and environmental monitoring.","Unmanned aerial vehicles (UAVs) have received plenty of attention for collecting data efficiently due to their high flexibility and enhanced communication ability, nonetheless, the limited onboard energy restricts UAVs' application on persistent missions, such as disaster search and rescue.","In this paper, we propose a rechargeable UAV-assisted periodic data collection scheme, where the UAV replenishes energy through the wireless charging platform during the mission to provide persistent information services for the sensor nodes (SNs).","Specifically, the total completion time is minimized by optimizing the trajectory of the UAV to reach the balance among the collecting time, flight time, and recharging time.","However, optimally solving this problem is highly non-trivial due to the non-convex constraints and the involved integer variables.","To address this issue, the formulated problem is decomposed into two subproblems, namely, UAV data collection trajectory optimization and SN clustering and UAV visiting order optimization.","By exploiting the convex optimization techniques and proving the total time is non-decreasing with the cluster number, a periodic trajectory optimization algorithm based on successive convex approximation (SCA) and bisection search is proposed to solve the main problem.","The simulation results show the efficiency of the proposed scheme in practical scenarios and the completion time of the proposed algorithm is on average 39% and 33% lower than the two benchmarks, respectively."],"url":"http://arxiv.org/abs/2404.15761v1","category":"eess.SP"}
{"created":"2024-04-24 09:33:10","title":"Debiasing Machine Unlearning with Counterfactual Examples","abstract":"The right to be forgotten (RTBF) seeks to safeguard individuals from the enduring effects of their historical actions by implementing machine-learning techniques. These techniques facilitate the deletion of previously acquired knowledge without requiring extensive model retraining. However, they often overlook a critical issue: unlearning processes bias. This bias emerges from two main sources: (1) data-level bias, characterized by uneven data removal, and (2) algorithm-level bias, which leads to the contamination of the remaining dataset, thereby degrading model accuracy. In this work, we analyze the causal factors behind the unlearning process and mitigate biases at both data and algorithmic levels. Typically, we introduce an intervention-based approach, where knowledge to forget is erased with a debiased dataset. Besides, we guide the forgetting procedure by leveraging counterfactual examples, as they maintain semantic data consistency without hurting performance on the remaining dataset. Experimental results demonstrate that our method outperforms existing machine unlearning baselines on evaluation metrics.","sentences":["The right to be forgotten (RTBF) seeks to safeguard individuals from the enduring effects of their historical actions by implementing machine-learning techniques.","These techniques facilitate the deletion of previously acquired knowledge without requiring extensive model retraining.","However, they often overlook a critical issue: unlearning processes bias.","This bias emerges from two main sources: (1) data-level bias, characterized by uneven data removal, and (2) algorithm-level bias, which leads to the contamination of the remaining dataset, thereby degrading model accuracy.","In this work, we analyze the causal factors behind the unlearning process and mitigate biases at both data and algorithmic levels.","Typically, we introduce an intervention-based approach, where knowledge to forget is erased with a debiased dataset.","Besides, we guide the forgetting procedure by leveraging counterfactual examples, as they maintain semantic data consistency without hurting performance on the remaining dataset.","Experimental results demonstrate that our method outperforms existing machine unlearning baselines on evaluation metrics."],"url":"http://arxiv.org/abs/2404.15760v1","category":"cs.LG"}
{"created":"2024-04-24 09:30:00","title":"Let's Think Dot by Dot: Hidden Computation in Transformer Language Models","abstract":"Chain-of-thought responses from language models improve performance across most benchmarks. However, it remains unclear to what extent these performance gains can be attributed to human-like task decomposition or simply the greater computation that additional tokens allow. We show that transformers can use meaningless filler tokens (e.g., '......') in place of a chain of thought to solve two hard algorithmic tasks they could not solve when responding without intermediate tokens. However, we find empirically that learning to use filler tokens is difficult and requires specific, dense supervision to converge. We also provide a theoretical characterization of the class of problems where filler tokens are useful in terms of the quantifier depth of a first-order formula. For problems satisfying this characterization, chain-of-thought tokens need not provide information about the intermediate computational steps involved in multi-token computations. In summary, our results show that additional tokens can provide computational benefits independent of token choice. The fact that intermediate tokens can act as filler tokens raises concerns about large language models engaging in unauditable, hidden computations that are increasingly detached from the observed chain-of-thought tokens.","sentences":["Chain-of-thought responses from language models improve performance across most benchmarks.","However, it remains unclear to what extent these performance gains can be attributed to human-like task decomposition or simply the greater computation that additional tokens allow.","We show that transformers can use meaningless filler tokens (e.g., '......') in place of a chain of thought to solve two hard algorithmic tasks they could not solve when responding without intermediate tokens.","However, we find empirically that learning to use filler tokens is difficult and requires specific, dense supervision to converge.","We also provide a theoretical characterization of the class of problems where filler tokens are useful in terms of the quantifier depth of a first-order formula.","For problems satisfying this characterization, chain-of-thought tokens need not provide information about the intermediate computational steps involved in multi-token computations.","In summary, our results show that additional tokens can provide computational benefits independent of token choice.","The fact that intermediate tokens can act as filler tokens raises concerns about large language models engaging in unauditable, hidden computations that are increasingly detached from the observed chain-of-thought tokens."],"url":"http://arxiv.org/abs/2404.15758v1","category":"cs.CL"}
{"created":"2024-04-24 09:20:33","title":"Supercompiler Code Optimization with Zero-Shot Reinforcement Learning","abstract":"Effective code optimization in compilers plays a central role in computer and software engineering. While compilers can be made to automatically search the optimization space without the need for user interventions, this is not a standard practice since the search is slow and cumbersome. Here we present CodeZero, an artificial intelligence agent trained extensively on large data to produce effective optimization strategies instantly for each program in a single trial of the agent. To overcome the huge range of possible test programs, we prepare a large dataset of training programs that emphasize quality, naturalness, and diversity. To tackle the vast space of possible optimizations, we adapt deep reinforcement learning to train the agent in a sample-efficient manner through interacting with a world model of the compiler environment. Evaluation on both benchmark suites and production-level code optimization problems demonstrates our agent's supercompiler performances and zero-shot generalization abilities, outperforming built-in optimization options designed by compiler experts. Our methodology kindles the great potential of artificial intelligence for engineering and paves the way for scaling machine learning techniques in the realm of code optimization.","sentences":["Effective code optimization in compilers plays a central role in computer and software engineering.","While compilers can be made to automatically search the optimization space without the need for user interventions, this is not a standard practice since the search is slow and cumbersome.","Here we present CodeZero, an artificial intelligence agent trained extensively on large data to produce effective optimization strategies instantly for each program in a single trial of the agent.","To overcome the huge range of possible test programs, we prepare a large dataset of training programs that emphasize quality, naturalness, and diversity.","To tackle the vast space of possible optimizations, we adapt deep reinforcement learning to train the agent in a sample-efficient manner through interacting with a world model of the compiler environment.","Evaluation on both benchmark suites and production-level code optimization problems demonstrates our agent's supercompiler performances and zero-shot generalization abilities, outperforming built-in optimization options designed by compiler experts.","Our methodology kindles the great potential of artificial intelligence for engineering and paves the way for scaling machine learning techniques in the realm of code optimization."],"url":"http://arxiv.org/abs/2404.16077v1","category":"cs.PL"}
{"created":"2024-04-24 09:15:22","title":"Introducing EEG Analyses to Help Personal Music Preference Prediction","abstract":"Nowadays, personalized recommender systems play an increasingly important role in music scenarios in our daily life with the preference prediction ability. However, existing methods mainly rely on users' implicit feedback (e.g., click, dwell time) which ignores the detailed user experience. This paper introduces Electroencephalography (EEG) signals to personal music preferences as a basis for the personalized recommender system. To realize collection in daily life, we use a dry-electrodes portable device to collect data. We perform a user study where participants listen to music and record preferences and moods. Meanwhile, EEG signals are collected with a portable device. Analysis of the collected data indicates a significant relationship between music preference, mood, and EEG signals. Furthermore, we conduct experiments to predict personalized music preference with the features of EEG signals. Experiments show significant improvement in rating prediction and preference classification with the help of EEG. Our work demonstrates the possibility of introducing EEG signals in personal music preference with portable devices. Moreover, our approach is not restricted to the music scenario, and the EEG signals as explicit feedback can be used in personalized recommendation tasks.","sentences":["Nowadays, personalized recommender systems play an increasingly important role in music scenarios in our daily life with the preference prediction ability.","However, existing methods mainly rely on users' implicit feedback (e.g., click, dwell time) which ignores the detailed user experience.","This paper introduces Electroencephalography (EEG) signals to personal music preferences as a basis for the personalized recommender system.","To realize collection in daily life, we use a dry-electrodes portable device to collect data.","We perform a user study where participants listen to music and record preferences and moods.","Meanwhile, EEG signals are collected with a portable device.","Analysis of the collected data indicates a significant relationship between music preference, mood, and EEG signals.","Furthermore, we conduct experiments to predict personalized music preference with the features of EEG signals.","Experiments show significant improvement in rating prediction and preference classification with the help of EEG.","Our work demonstrates the possibility of introducing EEG signals in personal music preference with portable devices.","Moreover, our approach is not restricted to the music scenario, and the EEG signals as explicit feedback can be used in personalized recommendation tasks."],"url":"http://arxiv.org/abs/2404.15753v1","category":"cs.HC"}
{"created":"2024-04-24 09:13:39","title":"Guided-SPSA: Simultaneous Perturbation Stochastic Approximation assisted by the Parameter Shift Rule","abstract":"The study of variational quantum algorithms (VQCs) has received significant attention from the quantum computing community in recent years. These hybrid algorithms, utilizing both classical and quantum components, are well-suited for noisy intermediate-scale quantum devices. Though estimating exact gradients using the parameter-shift rule to optimize the VQCs is realizable in NISQ devices, they do not scale well for larger problem sizes. The computational complexity, in terms of the number of circuit evaluations required for gradient estimation by the parameter-shift rule, scales linearly with the number of parameters in VQCs. On the other hand, techniques that approximate the gradients of the VQCs, such as the simultaneous perturbation stochastic approximation (SPSA), do not scale with the number of parameters but struggle with instability and often attain suboptimal solutions. In this work, we introduce a novel gradient estimation approach called Guided-SPSA, which meaningfully combines the parameter-shift rule and SPSA-based gradient approximation. The Guided-SPSA results in a 15% to 25% reduction in the number of circuit evaluations required during training for a similar or better optimality of the solution found compared to the parameter-shift rule. The Guided-SPSA outperforms standard SPSA in all scenarios and outperforms the parameter-shift rule in scenarios such as suboptimal initialization of the parameters. We demonstrate numerically the performance of Guided-SPSA on different paradigms of quantum machine learning, such as regression, classification, and reinforcement learning.","sentences":["The study of variational quantum algorithms (VQCs) has received significant attention from the quantum computing community in recent years.","These hybrid algorithms, utilizing both classical and quantum components, are well-suited for noisy intermediate-scale quantum devices.","Though estimating exact gradients using the parameter-shift rule to optimize the VQCs is realizable in NISQ devices, they do not scale well for larger problem sizes.","The computational complexity, in terms of the number of circuit evaluations required for gradient estimation by the parameter-shift rule, scales linearly with the number of parameters in VQCs.","On the other hand, techniques that approximate the gradients of the VQCs, such as the simultaneous perturbation stochastic approximation (SPSA), do not scale with the number of parameters but struggle with instability and often attain suboptimal solutions.","In this work, we introduce a novel gradient estimation approach called Guided-SPSA, which meaningfully combines the parameter-shift rule and SPSA-based gradient approximation.","The Guided-SPSA results in a 15% to 25% reduction in the number of circuit evaluations required during training for a similar or better optimality of the solution found compared to the parameter-shift rule.","The Guided-SPSA outperforms standard SPSA in all scenarios and outperforms the parameter-shift rule in scenarios such as suboptimal initialization of the parameters.","We demonstrate numerically the performance of Guided-SPSA on different paradigms of quantum machine learning, such as regression, classification, and reinforcement learning."],"url":"http://arxiv.org/abs/2404.15751v1","category":"quant-ph"}
{"created":"2024-04-24 09:04:13","title":"Reconstructing the Magnetic Field in an Arbitrary Domain via Data-driven Bayesian Methods and Numerical Simulations","abstract":"Inverse problems are prevalent in numerous scientific and engineering disciplines, where the objective is to determine unknown parameters within a physical system using indirect measurements or observations. The inherent challenge lies in deducing the most probable parameter values that align with the collected data. This study introduces an algorithm for reconstructing parameters by addressing an inverse problem formulated through differential equations underpinned by uncertain boundary conditions or variant parameters. We adopt a Bayesian approach for parameter inference, delineating the establishment of prior, likelihood, and posterior distributions, and the subsequent resolution of the maximum a posteriori problem via numerical optimization techniques. The proposed algorithm is applied to the task of magnetic field reconstruction within a conical domain, demonstrating precise recovery of the true parameter values.","sentences":["Inverse problems are prevalent in numerous scientific and engineering disciplines, where the objective is to determine unknown parameters within a physical system using indirect measurements or observations.","The inherent challenge lies in deducing the most probable parameter values that align with the collected data.","This study introduces an algorithm for reconstructing parameters by addressing an inverse problem formulated through differential equations underpinned by uncertain boundary conditions or variant parameters.","We adopt a Bayesian approach for parameter inference, delineating the establishment of prior, likelihood, and posterior distributions, and the subsequent resolution of the maximum a posteriori problem via numerical optimization techniques.","The proposed algorithm is applied to the task of magnetic field reconstruction within a conical domain, demonstrating precise recovery of the true parameter values."],"url":"http://arxiv.org/abs/2404.15745v1","category":"physics.comp-ph"}
{"created":"2024-04-24 09:04:05","title":"A General Black-box Adversarial Attack on Graph-based Fake News Detectors","abstract":"Graph Neural Network (GNN)-based fake news detectors apply various methods to construct graphs, aiming to learn distinctive news embeddings for classification. Since the construction details are unknown for attackers in a black-box scenario, it is unrealistic to conduct the classical adversarial attacks that require a specific adjacency matrix. In this paper, we propose the first general black-box adversarial attack framework, i.e., General Attack via Fake Social Interaction (GAFSI), against detectors based on different graph structures. Specifically, as sharing is an important social interaction for GNN-based fake news detectors to construct the graph, we simulate sharing behaviors to fool the detectors. Firstly, we propose a fraudster selection module to select engaged users leveraging local and global information. In addition, a post injection module guides the selected users to create shared relations by sending posts. The sharing records will be added to the social context, leading to a general attack against different detectors. Experimental results on empirical datasets demonstrate the effectiveness of GAFSI.","sentences":["Graph Neural Network (GNN)-based fake news detectors apply various methods to construct graphs, aiming to learn distinctive news embeddings for classification.","Since the construction details are unknown for attackers in a black-box scenario, it is unrealistic to conduct the classical adversarial attacks that require a specific adjacency matrix.","In this paper, we propose the first general black-box adversarial attack framework, i.e., General Attack via Fake Social Interaction (GAFSI), against detectors based on different graph structures.","Specifically, as sharing is an important social interaction for GNN-based fake news detectors to construct the graph, we simulate sharing behaviors to fool the detectors.","Firstly, we propose a fraudster selection module to select engaged users leveraging local and global information.","In addition, a post injection module guides the selected users to create shared relations by sending posts.","The sharing records will be added to the social context, leading to a general attack against different detectors.","Experimental results on empirical datasets demonstrate the effectiveness of GAFSI."],"url":"http://arxiv.org/abs/2404.15744v1","category":"cs.LG"}
{"created":"2024-04-24 08:50:45","title":"What Makes Multimodal In-Context Learning Work?","abstract":"Large Language Models have demonstrated remarkable performance across various tasks, exhibiting the capacity to swiftly acquire new skills, such as through In-Context Learning (ICL) with minimal demonstration examples. In this work, we present a comprehensive framework for investigating Multimodal ICL (M-ICL) in the context of Large Multimodal Models. We consider the best open-source multimodal models (e.g., IDEFICS, OpenFlamingo) and a wide range of multimodal tasks. Our study unveils several noteworthy findings: (1) M-ICL primarily relies on text-driven mechanisms, showing little to no influence from the image modality. (2) When used with advanced-ICL strategy (like RICES), M-ICL is not better than a simple strategy based on majority voting over context examples. Moreover, we identify several biases and limitations of M-ICL that warrant consideration prior to deployment. Code available at https://gitlab.com/folbaeni/multimodal-icl","sentences":["Large Language Models have demonstrated remarkable performance across various tasks, exhibiting the capacity to swiftly acquire new skills, such as through In-Context Learning (ICL) with minimal demonstration examples.","In this work, we present a comprehensive framework for investigating Multimodal ICL (M-ICL) in the context of Large Multimodal Models.","We consider the best open-source multimodal models (e.g., IDEFICS, OpenFlamingo) and a wide range of multimodal tasks.","Our study unveils several noteworthy findings: (1) M-ICL primarily relies on text-driven mechanisms, showing little to no influence from the image modality.","(2) When used with advanced-ICL strategy (like RICES), M-ICL is not better than a simple strategy based on majority voting over context examples.","Moreover, we identify several biases and limitations of M-ICL that warrant consideration prior to deployment.","Code available at https://gitlab.com/folbaeni/multimodal-icl"],"url":"http://arxiv.org/abs/2404.15736v2","category":"cs.CV"}
{"created":"2024-04-24 08:30:49","title":"Deep Predictive Model Learning with Parametric Bias: Handling Modeling Difficulties and Temporal Model Changes","abstract":"When a robot executes a task, it is necessary to model the relationship among its body, target objects, tools, and environment, and to control its body to realize the target state. However, it is difficult to model them using classical methods if the relationship is complex. In addition, when the relationship changes with time, it is necessary to deal with the temporal changes of the model. In this study, we have developed Deep Predictive Model with Parametric Bias (DPMPB) as a more human-like adaptive intelligence to deal with these modeling difficulties and temporal model changes. We categorize and summarize the theory of DPMPB and various task experiments on the actual robots, and discuss the effectiveness of DPMPB.","sentences":["When a robot executes a task, it is necessary to model the relationship among its body, target objects, tools, and environment, and to control its body to realize the target state.","However, it is difficult to model them using classical methods if the relationship is complex.","In addition, when the relationship changes with time, it is necessary to deal with the temporal changes of the model.","In this study, we have developed Deep Predictive Model with Parametric Bias (DPMPB) as a more human-like adaptive intelligence to deal with these modeling difficulties and temporal model changes.","We categorize and summarize the theory of DPMPB and various task experiments on the actual robots, and discuss the effectiveness of DPMPB."],"url":"http://arxiv.org/abs/2404.15726v1","category":"cs.RO"}
{"created":"2024-04-24 08:15:36","title":"SPARO: Selective Attention for Robust and Compositional Transformer Encodings for Vision","abstract":"Selective attention helps us focus on task-relevant aspects in the constant flood of our sensory input. This constraint in our perception allows us to robustly generalize under distractions and to new compositions of perceivable concepts. Transformers employ a similar notion of attention in their architecture, but representation learning models with transformer backbones like CLIP and DINO often fail to demonstrate robustness and compositionality. We highlight a missing architectural prior: unlike human perception, transformer encodings do not separately attend over individual concepts. In response, we propose SPARO, a read-out mechanism that partitions encodings into separately-attended slots, each produced by a single attention head. Using SPARO with CLIP imparts an inductive bias that the vision and text modalities are different views of a shared compositional world with the same corresponding concepts. Using SPARO, we demonstrate improvements on downstream recognition, robustness, retrieval, and compositionality benchmarks with CLIP (up to +14% for ImageNet, +4% for SugarCrepe), and on nearest neighbors and linear probe for ImageNet with DINO (+3% each). We also showcase a powerful ability to intervene and select individual SPARO concepts to further improve downstream task performance (up from +4% to +9% for SugarCrepe) and use this ability to study the robustness of SPARO's representation structure. Finally, we provide insights through ablation experiments and visualization of learned concepts.","sentences":["Selective attention helps us focus on task-relevant aspects in the constant flood of our sensory input.","This constraint in our perception allows us to robustly generalize under distractions and to new compositions of perceivable concepts.","Transformers employ a similar notion of attention in their architecture, but representation learning models with transformer backbones like CLIP and DINO often fail to demonstrate robustness and compositionality.","We highlight a missing architectural prior: unlike human perception, transformer encodings do not separately attend over individual concepts.","In response, we propose SPARO, a read-out mechanism that partitions encodings into separately-attended slots, each produced by a single attention head.","Using SPARO with CLIP imparts an inductive bias that the vision and text modalities are different views of a shared compositional world with the same corresponding concepts.","Using SPARO, we demonstrate improvements on downstream recognition, robustness, retrieval, and compositionality benchmarks with CLIP (up to +14% for ImageNet, +4% for SugarCrepe), and on nearest neighbors and linear probe for ImageNet with DINO (+3% each).","We also showcase a powerful ability to intervene and select individual SPARO concepts to further improve downstream task performance (up from +4% to +9% for SugarCrepe) and use this ability to study the robustness of SPARO's representation structure.","Finally, we provide insights through ablation experiments and visualization of learned concepts."],"url":"http://arxiv.org/abs/2404.15721v1","category":"cs.CV"}
{"created":"2024-04-24 08:13:02","title":"Annotator-Centric Active Learning for Subjective NLP Tasks","abstract":"To accurately capture the variability in human judgments for subjective NLP tasks, incorporating a wide range of perspectives in the annotation process is crucial. Active Learning (AL) addresses the high costs of collecting human annotations by strategically annotating the most informative samples. We introduce Annotator-Centric Active Learning (ACAL), which incorporates an annotator selection strategy following data sampling. Our objective is two-fold: (1) to efficiently approximate the full diversity of human judgments, and to assess model performance using annotator-centric metrics, which emphasize minority perspectives over a majority. We experiment with multiple annotator selection strategies across seven subjective NLP tasks, employing both traditional and novel, human-centered evaluation metrics. Our findings indicate that ACAL improves data efficiency and excels in annotator-centric performance evaluations. However, its success depends on the availability of a sufficiently large and diverse pool of annotators to sample from.","sentences":["To accurately capture the variability in human judgments for subjective NLP tasks, incorporating a wide range of perspectives in the annotation process is crucial.","Active Learning (AL) addresses the high costs of collecting human annotations by strategically annotating the most informative samples.","We introduce Annotator-Centric Active Learning (ACAL), which incorporates an annotator selection strategy following data sampling.","Our objective is two-fold: (1) to efficiently approximate the full diversity of human judgments, and to assess model performance using annotator-centric metrics, which emphasize minority perspectives over a majority.","We experiment with multiple annotator selection strategies across seven subjective NLP tasks, employing both traditional and novel, human-centered evaluation metrics.","Our findings indicate that ACAL improves data efficiency and excels in annotator-centric performance evaluations.","However, its success depends on the availability of a sufficiently large and diverse pool of annotators to sample from."],"url":"http://arxiv.org/abs/2404.15720v1","category":"cs.CL"}
{"created":"2024-04-24 08:11:50","title":"HDBN: A Novel Hybrid Dual-branch Network for Robust Skeleton-based Action Recognition","abstract":"Skeleton-based action recognition has gained considerable traction thanks to its utilization of succinct and robust skeletal representations. Nonetheless, current methodologies often lean towards utilizing a solitary backbone to model skeleton modality, which can be limited by inherent flaws in the network backbone. To address this and fully leverage the complementary characteristics of various network architectures, we propose a novel Hybrid Dual-Branch Network (HDBN) for robust skeleton-based action recognition, which benefits from the graph convolutional network's proficiency in handling graph-structured data and the powerful modeling capabilities of Transformers for global information. In detail, our proposed HDBN is divided into two trunk branches: MixGCN and MixFormer. The two branches utilize GCNs and Transformers to model both 2D and 3D skeletal modalities respectively. Our proposed HDBN emerged as one of the top solutions in the Multi-Modal Video Reasoning and Analyzing Competition (MMVRAC) of 2024 ICME Grand Challenge, achieving accuracies of 47.95% and 75.36% on two benchmarks of the UAV-Human dataset by outperforming most existing methods. Our code will be publicly available at: https://github.com/liujf69/ICMEW2024-Track10.","sentences":["Skeleton-based action recognition has gained considerable traction thanks to its utilization of succinct and robust skeletal representations.","Nonetheless, current methodologies often lean towards utilizing a solitary backbone to model skeleton modality, which can be limited by inherent flaws in the network backbone.","To address this and fully leverage the complementary characteristics of various network architectures, we propose a novel Hybrid Dual-Branch Network (HDBN) for robust skeleton-based action recognition, which benefits from the graph convolutional network's proficiency in handling graph-structured data and the powerful modeling capabilities of Transformers for global information.","In detail, our proposed HDBN is divided into two trunk branches: MixGCN and MixFormer.","The two branches utilize GCNs and Transformers to model both 2D and 3D skeletal modalities respectively.","Our proposed HDBN emerged as one of the top solutions in the Multi-Modal Video Reasoning and Analyzing Competition (MMVRAC) of 2024 ICME Grand Challenge, achieving accuracies of 47.95% and 75.36% on two benchmarks of the UAV-Human dataset by outperforming most existing methods.","Our code will be publicly available at: https://github.com/liujf69/ICMEW2024-Track10."],"url":"http://arxiv.org/abs/2404.15719v2","category":"cs.CV"}
{"created":"2024-04-24 08:07:16","title":"Ada-DF: An Adaptive Label Distribution Fusion Network For Facial Expression Recognition","abstract":"Facial expression recognition (FER) plays a significant role in our daily life. However, annotation ambiguity in the datasets could greatly hinder the performance. In this paper, we address FER task via label distribution learning paradigm, and develop a dual-branch Adaptive Distribution Fusion (Ada-DF) framework. One auxiliary branch is constructed to obtain the label distributions of samples. The class distributions of emotions are then computed through the label distributions of each emotion. Finally, those two distributions are adaptively fused according to the attention weights to train the target branch. Extensive experiments are conducted on three real-world datasets, RAF-DB, AffectNet and SFEW, where our Ada-DF shows advantages over the state-of-the-art works.","sentences":["Facial expression recognition (FER) plays a significant role in our daily life.","However, annotation ambiguity in the datasets could greatly hinder the performance.","In this paper, we address FER task via label distribution learning paradigm, and develop a dual-branch Adaptive Distribution Fusion (Ada-DF) framework.","One auxiliary branch is constructed to obtain the label distributions of samples.","The class distributions of emotions are then computed through the label distributions of each emotion.","Finally, those two distributions are adaptively fused according to the attention weights to train the target branch.","Extensive experiments are conducted on three real-world datasets, RAF-DB, AffectNet and SFEW, where our Ada-DF shows advantages over the state-of-the-art works."],"url":"http://arxiv.org/abs/2404.15714v1","category":"cs.CV"}
{"created":"2024-04-24 07:47:55","title":"Efficient Multi-Model Fusion with Adversarial Complementary Representation Learning","abstract":"Single-model systems often suffer from deficiencies in tasks such as speaker verification (SV) and image classification, relying heavily on partial prior knowledge during decision-making, resulting in suboptimal performance. Although multi-model fusion (MMF) can mitigate some of these issues, redundancy in learned representations may limits improvements. To this end, we propose an adversarial complementary representation learning (ACoRL) framework that enables newly trained models to avoid previously acquired knowledge, allowing each individual component model to learn maximally distinct, complementary representations. We make three detailed explanations of why this works and experimental results demonstrate that our method more efficiently improves performance compared to traditional MMF. Furthermore, attribution analysis validates the model trained under ACoRL acquires more complementary knowledge, highlighting the efficacy of our approach in enhancing efficiency and robustness across tasks.","sentences":["Single-model systems often suffer from deficiencies in tasks such as speaker verification (SV) and image classification, relying heavily on partial prior knowledge during decision-making, resulting in suboptimal performance.","Although multi-model fusion (MMF) can mitigate some of these issues, redundancy in learned representations may limits improvements.","To this end, we propose an adversarial complementary representation learning (ACoRL) framework that enables newly trained models to avoid previously acquired knowledge, allowing each individual component model to learn maximally distinct, complementary representations.","We make three detailed explanations of why this works and experimental results demonstrate that our method more efficiently improves performance compared to traditional MMF.","Furthermore, attribution analysis validates the model trained under ACoRL acquires more complementary knowledge, highlighting the efficacy of our approach in enhancing efficiency and robustness across tasks."],"url":"http://arxiv.org/abs/2404.15704v1","category":"cs.LG"}
{"created":"2024-04-24 07:26:40","title":"Spatial segregation of massive clusters in a simulation of colliding dwarf galaxies","abstract":"The collective properties of star clusters are investigated using a simulation of the collision between two dwarf galaxies. The characteristic power law of the cluster mass function, N(M), with a slope dlog N/dlog M ~ -1, is present from cluster birth and remains throughout the simulation. The maximum mass of a young cluster scales with the star formation rate (SFR). The relative average minimum separation, R(M)= N(M)^{1/p}D_min(M)/D(M_low), for average minimum distance D_min(M) between clusters of mass M, and for lowest mass, M_low, measured in projection (p=2) or three dimensions (p=3), has a negative slope, dlog R/dlog M ~ -0.2, for all masses and ages. This agrees with observations of R(M) in low-mass galaxies studied previously. Like the slope of N(M), R}(M) is apparently a property of cluster birth for dwarf galaxies that does not depend on SFR or time. The negative slope for R(M) implies that more massive clusters are centrally concentrated relative to lower mass clusters throughout the entire mass range. Cluster growth through coalescence is also investigated. The ratio of the kinetic to potential energy of all near-neighbor clusters is generally large, but a tail of low values in the distribution of this ratio suggests that a fraction of the clusters merge, ~8% by number throughout the ~300 Myr of the simulation and up to 60% by mass for young clusters in their first 10 Myr, scaling with the SFR above a certain threshold.","sentences":["The collective properties of star clusters are investigated using a simulation of the collision between two dwarf galaxies.","The characteristic power law of the cluster mass function, N(M), with a slope dlog N/dlog M ~ -1, is present from cluster birth and remains throughout the simulation.","The maximum mass of a young cluster scales with the star formation rate (SFR).","The relative average minimum separation, R(M)= N(M)^{1/p}D_min(M)/D(M_low), for average minimum distance D_min(M) between clusters of mass M, and for lowest mass, M_low, measured in projection (p=2) or three dimensions (p=3), has a negative slope, dlog R/dlog M ~ -0.2, for all masses and ages.","This agrees with observations of R(M) in low-mass galaxies studied previously.","Like the slope of N(M), R}(M) is apparently a property of cluster birth for dwarf galaxies that does not depend on SFR or time.","The negative slope for R(M) implies that more massive clusters are centrally concentrated relative to lower mass clusters throughout the entire mass range.","Cluster growth through coalescence is also investigated.","The ratio of the kinetic to potential energy of all near-neighbor clusters is generally large, but a tail of low values in the distribution of this ratio suggests that a fraction of the clusters merge, ~8% by number throughout the ~300 Myr of the simulation and up to 60% by mass for young clusters in their first 10 Myr, scaling with the SFR above a certain threshold."],"url":"http://arxiv.org/abs/2404.15698v1","category":"astro-ph.GA"}
{"created":"2024-04-24 07:25:36","title":"DeepFeatureX Net: Deep Features eXtractors based Network for discriminating synthetic from real images","abstract":"Deepfakes, synthetic images generated by deep learning algorithms, represent one of the biggest challenges in the field of Digital Forensics. The scientific community is working to develop approaches that can discriminate the origin of digital images (real or AI-generated). However, these methodologies face the challenge of generalization, that is, the ability to discern the nature of an image even if it is generated by an architecture not seen during training. This usually leads to a drop in performance. In this context, we propose a novel approach based on three blocks called Base Models, each of which is responsible for extracting the discriminative features of a specific image class (Diffusion Model-generated, GAN-generated, or real) as it is trained by exploiting deliberately unbalanced datasets. The features extracted from each block are then concatenated and processed to discriminate the origin of the input image. Experimental results showed that this approach not only demonstrates good robust capabilities to JPEG compression but also outperforms state-of-the-art methods in several generalization tests. Code, models and dataset are available at https://github.com/opontorno/block-based_deepfake-detection.","sentences":["Deepfakes, synthetic images generated by deep learning algorithms, represent one of the biggest challenges in the field of Digital Forensics.","The scientific community is working to develop approaches that can discriminate the origin of digital images (real or AI-generated).","However, these methodologies face the challenge of generalization, that is, the ability to discern the nature of an image even if it is generated by an architecture not seen during training.","This usually leads to a drop in performance.","In this context, we propose a novel approach based on three blocks called Base Models, each of which is responsible for extracting the discriminative features of a specific image class (Diffusion Model-generated, GAN-generated, or real) as it is trained by exploiting deliberately unbalanced datasets.","The features extracted from each block are then concatenated and processed to discriminate the origin of the input image.","Experimental results showed that this approach not only demonstrates good robust capabilities to JPEG compression but also outperforms state-of-the-art methods in several generalization tests.","Code, models and dataset are available at https://github.com/opontorno/block-based_deepfake-detection."],"url":"http://arxiv.org/abs/2404.15697v1","category":"cs.CV"}
{"created":"2024-04-24 06:59:59","title":"Long-term Off-Policy Evaluation and Learning","abstract":"Short- and long-term outcomes of an algorithm often differ, with damaging downstream effects. A known example is a click-bait algorithm, which may increase short-term clicks but damage long-term user engagement. A possible solution to estimate the long-term outcome is to run an online experiment or A/B test for the potential algorithms, but it takes months or even longer to observe the long-term outcomes of interest, making the algorithm selection process unacceptably slow. This work thus studies the problem of feasibly yet accurately estimating the long-term outcome of an algorithm using only historical and short-term experiment data. Existing approaches to this problem either need a restrictive assumption about the short-term outcomes called surrogacy or cannot effectively use short-term outcomes, which is inefficient. Therefore, we propose a new framework called Long-term Off-Policy Evaluation (LOPE), which is based on reward function decomposition. LOPE works under a more relaxed assumption than surrogacy and effectively leverages short-term rewards to substantially reduce the variance. Synthetic experiments show that LOPE outperforms existing approaches particularly when surrogacy is severely violated and the long-term reward is noisy. In addition, real-world experiments on large-scale A/B test data collected on a music streaming platform show that LOPE can estimate the long-term outcome of actual algorithms more accurately than existing feasible methods.","sentences":["Short- and long-term outcomes of an algorithm often differ, with damaging downstream effects.","A known example is a click-bait algorithm, which may increase short-term clicks but damage long-term user engagement.","A possible solution to estimate the long-term outcome is to run an online experiment or A/B test for the potential algorithms, but it takes months or even longer to observe the long-term outcomes of interest, making the algorithm selection process unacceptably slow.","This work thus studies the problem of feasibly yet accurately estimating the long-term outcome of an algorithm using only historical and short-term experiment data.","Existing approaches to this problem either need a restrictive assumption about the short-term outcomes called surrogacy or cannot effectively use short-term outcomes, which is inefficient.","Therefore, we propose a new framework called Long-term Off-Policy Evaluation (LOPE), which is based on reward function decomposition.","LOPE works under a more relaxed assumption than surrogacy and effectively leverages short-term rewards to substantially reduce the variance.","Synthetic experiments show that LOPE outperforms existing approaches particularly when surrogacy is severely violated and the long-term reward is noisy.","In addition, real-world experiments on large-scale A/B test data collected on a music streaming platform show that LOPE can estimate the long-term outcome of actual algorithms more accurately than existing feasible methods."],"url":"http://arxiv.org/abs/2404.15691v1","category":"cs.LG"}
{"created":"2024-04-24 06:52:53","title":"Graph Neural Networks for Vulnerability Detection: A Counterfactual Explanation","abstract":"Vulnerability detection is crucial for ensuring the security and reliability of software systems. Recently, Graph Neural Networks (GNNs) have emerged as a prominent code embedding approach for vulnerability detection, owing to their ability to capture the underlying semantic structure of source code. However, GNNs face significant challenges in explainability due to their inherently black-box nature. To this end, several factual reasoning-based explainers have been proposed. These explainers provide explanations for the predictions made by GNNs by analyzing the key features that contribute to the outcomes. We argue that these factual reasoning-based explanations cannot answer critical what-if questions: What would happen to the GNN's decision if we were to alter the code graph into alternative structures? Inspired by advancements of counterfactual reasoning in artificial intelligence, we propose CFExplainer, a novel counterfactual explainer for GNN-based vulnerability detection. Unlike factual reasoning-based explainers, CFExplainer seeks the minimal perturbation to the input code graph that leads to a change in the prediction, thereby addressing the what-if questions for vulnerability detection. We term this perturbation a counterfactual explanation, which can pinpoint the root causes of the detected vulnerability and furnish valuable insights for developers to undertake appropriate actions for fixing the vulnerability. Extensive experiments on four GNN-based vulnerability detection models demonstrate the effectiveness of CFExplainer over existing state-of-the-art factual reasoning-based explainers.","sentences":["Vulnerability detection is crucial for ensuring the security and reliability of software systems.","Recently, Graph Neural Networks (GNNs) have emerged as a prominent code embedding approach for vulnerability detection, owing to their ability to capture the underlying semantic structure of source code.","However, GNNs face significant challenges in explainability due to their inherently black-box nature.","To this end, several factual reasoning-based explainers have been proposed.","These explainers provide explanations for the predictions made by GNNs by analyzing the key features that contribute to the outcomes.","We argue that these factual reasoning-based explanations cannot answer critical what-if questions: What would happen to the GNN's decision if we were to alter the code graph into alternative structures?","Inspired by advancements of counterfactual reasoning in artificial intelligence, we propose CFExplainer, a novel counterfactual explainer for GNN-based vulnerability detection.","Unlike factual reasoning-based explainers, CFExplainer seeks the minimal perturbation to the input code graph that leads to a change in the prediction, thereby addressing the what-if questions for vulnerability detection.","We term this perturbation a counterfactual explanation, which can pinpoint the root causes of the detected vulnerability and furnish valuable insights for developers to undertake appropriate actions for fixing the vulnerability.","Extensive experiments on four GNN-based vulnerability detection models demonstrate the effectiveness of CFExplainer over existing state-of-the-art factual reasoning-based explainers."],"url":"http://arxiv.org/abs/2404.15687v1","category":"cs.SE"}
{"created":"2024-04-24 06:29:55","title":"Automated Creation of Source Code Variants of a Cryptographic Hash Function Implementation Using Generative Pre-Trained Transformer Models","abstract":"Generative pre-trained transformers (GPT's) are a type of large language machine learning model that are unusually adept at producing novel, and coherent, natural language. In this study the ability of GPT models to generate novel and correct versions, and notably very insecure versions, of implementations of the cryptographic hash function SHA-1 is examined. The GPT models Llama-2-70b-chat-h, Mistral-7B-Instruct-v0.1, and zephyr-7b-alpha are used. The GPT models are prompted to re-write each function using a modified version of the localGPT framework and langchain to provide word embedding context of the full source code and header files to the model, resulting in over 130,000 function re-write GPT output text blocks, approximately 40,000 of which were able to be parsed as C code and subsequently compiled. The generated code is analyzed for being compilable, correctness of the algorithm, memory leaks, compiler optimization stability, and character distance to the reference implementation. Remarkably, several generated function variants have a high implementation security risk of being correct for some test vectors, but incorrect for other test vectors. Additionally, many function implementations were not correct to the reference algorithm of SHA-1, but produced hashes that have some of the basic characteristics of hash functions. Many of the function re-writes contained serious flaws such as memory leaks, integer overflows, out of bounds accesses, use of uninitialised values, and compiler optimization instability. Compiler optimization settings and SHA-256 hash checksums of the compiled binaries are used to cluster implementations that are equivalent but may not have identical syntax - using this clustering over 100,000 novel and correct versions of the SHA-1 codebase were generated where each component C function of the reference implementation is different from the original code.","sentences":["Generative pre-trained transformers (GPT's) are a type of large language machine learning model that are unusually adept at producing novel, and coherent, natural language.","In this study the ability of GPT models to generate novel and correct versions, and notably very insecure versions, of implementations of the cryptographic hash function SHA-1 is examined.","The GPT models Llama-2-70b-chat-h, Mistral-7B-Instruct-v0.1, and zephyr-7b-alpha are used.","The GPT models are prompted to re-write each function using a modified version of the localGPT framework and langchain to provide word embedding context of the full source code and header files to the model, resulting in over 130,000 function re-write GPT output text blocks, approximately 40,000 of which were able to be parsed as C code and subsequently compiled.","The generated code is analyzed for being compilable, correctness of the algorithm, memory leaks, compiler optimization stability, and character distance to the reference implementation.","Remarkably, several generated function variants have a high implementation security risk of being correct for some test vectors, but incorrect for other test vectors.","Additionally, many function implementations were not correct to the reference algorithm of SHA-1, but produced hashes that have some of the basic characteristics of hash functions.","Many of the function re-writes contained serious flaws such as memory leaks, integer overflows, out of bounds accesses, use of uninitialised values, and compiler optimization instability.","Compiler optimization settings and SHA-256 hash checksums of the compiled binaries are used to cluster implementations that are equivalent but may not have identical syntax - using this clustering over 100,000 novel and correct versions of the SHA-1 codebase were generated where each component C function of the reference implementation is different from the original code."],"url":"http://arxiv.org/abs/2404.15681v1","category":"cs.CR"}
{"created":"2024-04-24 06:29:54","title":"Legitimate Power, Illegitimate Automation: The problem of ignoring legitimacy in automated decision systems","abstract":"Progress in machine learning and artificial intelligence has spurred the widespread adoption of automated decision systems (ADS). An extensive literature explores what conditions must be met for these systems' decisions to be fair. However, questions of legitimacy -- why those in control of ADS are entitled to make such decisions -- have received comparatively little attention. This paper shows that when such questions are raised theorists often incorrectly conflate legitimacy with either public acceptance or other substantive values such as fairness, accuracy, expertise or efficiency. In search of better theories, we conduct a critical analysis of the philosophical literature on the legitimacy of the state, focusing on consent, public reason, and democratic authorisation. This analysis reveals that the prevailing understanding of legitimacy in analytical political philosophy is also ill-suited to the task of establishing whether and when ADS are legitimate. The paper thus clarifies expectations for theories of ADS legitimacy and charts a path for a future research programme on the topic.","sentences":["Progress in machine learning and artificial intelligence has spurred the widespread adoption of automated decision systems (ADS).","An extensive literature explores what conditions must be met for these systems' decisions to be fair.","However, questions of legitimacy -- why those in control of ADS are entitled to make such decisions -- have received comparatively little attention.","This paper shows that when such questions are raised theorists often incorrectly conflate legitimacy with either public acceptance or other substantive values such as fairness, accuracy, expertise or efficiency.","In search of better theories, we conduct a critical analysis of the philosophical literature on the legitimacy of the state, focusing on consent, public reason, and democratic authorisation.","This analysis reveals that the prevailing understanding of legitimacy in analytical political philosophy is also ill-suited to the task of establishing whether and when ADS are legitimate.","The paper thus clarifies expectations for theories of ADS legitimacy and charts a path for a future research programme on the topic."],"url":"http://arxiv.org/abs/2404.15680v1","category":"cs.CY"}
{"created":"2024-04-24 06:16:09","title":"Retrieval and Distill: A Temporal Data Shift-Free Paradigm for Online Recommendation System","abstract":"Current recommendation systems are significantly affected by a serious issue of temporal data shift, which is the inconsistency between the distribution of historical data and that of online data. Most existing models focus on utilizing updated data, overlooking the transferable, temporal data shift-free information that can be learned from shifting data. We propose the Temporal Invariance of Association theorem, which suggests that given a fixed search space, the relationship between the data and the data in the search space keeps invariant over time. Leveraging this principle, we designed a retrieval-based recommendation system framework that can train a data shift-free relevance network using shifting data, significantly enhancing the predictive performance of the original model in the recommendation system. However, retrieval-based recommendation models face substantial inference time costs when deployed online. To address this, we further designed a distill framework that can distill information from the relevance network into a parameterized module using shifting data. The distilled model can be deployed online alongside the original model, with only a minimal increase in inference time. Extensive experiments on multiple real datasets demonstrate that our framework significantly improves the performance of the original model by utilizing shifting data.","sentences":["Current recommendation systems are significantly affected by a serious issue of temporal data shift, which is the inconsistency between the distribution of historical data and that of online data.","Most existing models focus on utilizing updated data, overlooking the transferable, temporal data shift-free information that can be learned from shifting data.","We propose the Temporal Invariance of Association theorem, which suggests that given a fixed search space, the relationship between the data and the data in the search space keeps invariant over time.","Leveraging this principle, we designed a retrieval-based recommendation system framework that can train a data shift-free relevance network using shifting data, significantly enhancing the predictive performance of the original model in the recommendation system.","However, retrieval-based recommendation models face substantial inference time costs when deployed online.","To address this, we further designed a distill framework that can distill information from the relevance network into a parameterized module using shifting data.","The distilled model can be deployed online alongside the original model, with only a minimal increase in inference time.","Extensive experiments on multiple real datasets demonstrate that our framework significantly improves the performance of the original model by utilizing shifting data."],"url":"http://arxiv.org/abs/2404.15678v2","category":"cs.IR"}
{"created":"2024-04-24 06:12:00","title":"Beyond Chain-of-Thought: A Survey of Chain-of-X Paradigms for LLMs","abstract":"Chain-of-Thought (CoT) has been a widely adopted prompting method, eliciting impressive reasoning abilities of Large Language Models (LLMs). Inspired by the sequential thought structure of CoT, a number of Chain-of-X (CoX) methods have been developed to address various challenges across diverse domains and tasks involving LLMs. In this paper, we provide a comprehensive survey of Chain-of-X methods for LLMs in different contexts. Specifically, we categorize them by taxonomies of nodes, i.e., the X in CoX, and application tasks. We also discuss the findings and implications of existing CoX methods, as well as potential future directions. Our survey aims to serve as a detailed and up-to-date resource for researchers seeking to apply the idea of CoT to broader scenarios.","sentences":["Chain-of-Thought (CoT) has been a widely adopted prompting method, eliciting impressive reasoning abilities of Large Language Models (LLMs).","Inspired by the sequential thought structure of CoT, a number of Chain-of-X (CoX) methods have been developed to address various challenges across diverse domains and tasks involving LLMs.","In this paper, we provide a comprehensive survey of Chain-of-X methods for LLMs in different contexts.","Specifically, we categorize them by taxonomies of nodes, i.e., the X in CoX, and application tasks.","We also discuss the findings and implications of existing CoX methods, as well as potential future directions.","Our survey aims to serve as a detailed and up-to-date resource for researchers seeking to apply the idea of CoT to broader scenarios."],"url":"http://arxiv.org/abs/2404.15676v1","category":"cs.CL"}
{"created":"2024-04-24 05:53:52","title":"MalleTrain: Deep Neural Network Training on Unfillable Supercomputer Nodes","abstract":"First-come first-serve scheduling can result in substantial (up to 10%) of transiently idle nodes on supercomputers. Recognizing that such unfilled nodes are well-suited for deep neural network (DNN) training, due to the flexible nature of DNN training tasks, Liu et al. proposed that the re-scaling DNN training tasks to fit gaps in schedules be formulated as a mixed-integer linear programming (MILP) problem, and demonstrated via simulation the potential benefits of the approach. Here, we introduce MalleTrain, a system that provides the first practical implementation of this approach and that furthermore generalizes it by allowing it use even for DNN training applications for which model information is unknown before runtime. Key to this latter innovation is the use of a lightweight online job profiling advisor (JPA) to collect critical scalability information for DNN jobs -- information that it then employs to optimize resource allocations dynamically, in real time. We describe the MalleTrain architecture and present the results of a detailed experimental evaluation on a supercomputer GPU cluster and several representative DNN training workloads, including neural architecture search and hyperparameter optimization. Our results not only confirm the practical feasibility of leveraging idle supercomputer nodes for DNN training but improve significantly on prior results, improving training throughput by up to 22.3\\% without requiring users to provide job scalability information.","sentences":["First-come first-serve scheduling can result in substantial (up to 10%) of transiently idle nodes on supercomputers.","Recognizing that such unfilled nodes are well-suited for deep neural network (DNN) training, due to the flexible nature of DNN training tasks, Liu et al. proposed that the re-scaling DNN training tasks to fit gaps in schedules be formulated as a mixed-integer linear programming (MILP) problem, and demonstrated via simulation the potential benefits of the approach.","Here, we introduce MalleTrain, a system that provides the first practical implementation of this approach and that furthermore generalizes it by allowing it use even for DNN training applications for which model information is unknown before runtime.","Key to this latter innovation is the use of a lightweight online job profiling advisor (JPA) to collect critical scalability information for DNN jobs -- information that it then employs to optimize resource allocations dynamically, in real time.","We describe the MalleTrain architecture and present the results of a detailed experimental evaluation on a supercomputer GPU cluster and several representative DNN training workloads, including neural architecture search and hyperparameter optimization.","Our results not only confirm the practical feasibility of leveraging idle supercomputer nodes for DNN training but improve significantly on prior results, improving training throughput by up to 22.3\\% without requiring users to provide job scalability information."],"url":"http://arxiv.org/abs/2404.15668v1","category":"cs.DC"}
{"created":"2024-04-24 05:53:20","title":"The Promise and Challenges of Using LLMs to Accelerate the Screening Process of Systematic Reviews","abstract":"Systematic review (SR) is a popular research method in software engineering (SE). However, conducting an SR takes an average of 67 weeks. Thus, automating any step of the SR process could reduce the effort associated with SRs. Our objective is to investigate if Large Language Models (LLMs) can accelerate title-abstract screening by simplifying abstracts for human screeners, and automating title-abstract screening. We performed an experiment where humans screened titles and abstracts for 20 papers with both original and simplified abstracts from a prior SR. The experiment with human screeners was reproduced with GPT-3.5 and GPT-4 LLMs to perform the same screening tasks. We also studied if different prompting techniques (Zero-shot (ZS), One-shot (OS), Few-shot (FS), and Few-shot with Chain-of-Thought (FS-CoT)) improve the screening performance of LLMs. Lastly, we studied if redesigning the prompt used in the LLM reproduction of screening leads to improved performance. Text simplification did not increase the screeners' screening performance, but reduced the time used in screening. Screeners' scientific literacy skills and researcher status predict screening performance. Some LLM and prompt combinations perform as well as human screeners in the screening tasks. Our results indicate that the GPT-4 LLM is better than its predecessor, GPT-3.5. Additionally, Few-shot and One-shot prompting outperforms Zero-shot prompting. Using LLMs for text simplification in the screening process does not significantly improve human performance. Using LLMs to automate title-abstract screening seems promising, but current LLMs are not significantly more accurate than human screeners. To recommend the use of LLMs in the screening process of SRs, more research is needed. We recommend future SR studies publish replication packages with screening data to enable more conclusive experimenting with LLM screening.","sentences":["Systematic review (SR) is a popular research method in software engineering (SE).","However, conducting an SR takes an average of 67 weeks.","Thus, automating any step of the SR process could reduce the effort associated with SRs.","Our objective is to investigate if Large Language Models (LLMs) can accelerate title-abstract screening by simplifying abstracts for human screeners, and automating title-abstract screening.","We performed an experiment where humans screened titles and abstracts for 20 papers with both original and simplified abstracts from a prior SR.","The experiment with human screeners was reproduced with GPT-3.5 and GPT-4 LLMs to perform the same screening tasks.","We also studied if different prompting techniques (Zero-shot (ZS), One-shot (OS), Few-shot (FS), and Few-shot with Chain-of-Thought (FS-CoT)) improve the screening performance of LLMs.","Lastly, we studied if redesigning the prompt used in the LLM reproduction of screening leads to improved performance.","Text simplification did not increase the screeners' screening performance, but reduced the time used in screening.","Screeners' scientific literacy skills and researcher status predict screening performance.","Some LLM and prompt combinations perform as well as human screeners in the screening tasks.","Our results indicate that the GPT-4 LLM is better than its predecessor, GPT-3.5.","Additionally, Few-shot and One-shot prompting outperforms Zero-shot prompting.","Using LLMs for text simplification in the screening process does not significantly improve human performance.","Using LLMs to automate title-abstract screening seems promising, but current LLMs are not significantly more accurate than human screeners.","To recommend the use of LLMs in the screening process of SRs, more research is needed.","We recommend future SR studies publish replication packages with screening data to enable more conclusive experimenting with LLM screening."],"url":"http://arxiv.org/abs/2404.15667v2","category":"cs.CL"}
{"created":"2024-04-24 05:24:08","title":"FedSI: Federated Subnetwork Inference for Efficient Uncertainty Quantification","abstract":"While deep neural networks (DNNs) based personalized federated learning (PFL) is demanding for addressing data heterogeneity and shows promising performance, existing methods for federated learning (FL) suffer from efficient systematic uncertainty quantification. The Bayesian DNNs-based PFL is usually questioned of either over-simplified model structures or high computational and memory costs. In this paper, we introduce FedSI, a novel Bayesian DNNs-based subnetwork inference PFL framework. FedSI is simple and scalable by leveraging Bayesian methods to incorporate systematic uncertainties effectively. It implements a client-specific subnetwork inference mechanism, selects network parameters with large variance to be inferred through posterior distributions, and fixes the rest as deterministic ones. FedSI achieves fast and scalable inference while preserving the systematic uncertainties to the fullest extent. Extensive experiments on three different benchmark datasets demonstrate that FedSI outperforms existing Bayesian and non-Bayesian FL baselines in heterogeneous FL scenarios.","sentences":["While deep neural networks (DNNs) based personalized federated learning (PFL) is demanding for addressing data heterogeneity and shows promising performance, existing methods for federated learning (FL) suffer from efficient systematic uncertainty quantification.","The Bayesian DNNs-based PFL is usually questioned of either over-simplified model structures or high computational and memory costs.","In this paper, we introduce FedSI, a novel Bayesian DNNs-based subnetwork inference PFL framework.","FedSI is simple and scalable by leveraging Bayesian methods to incorporate systematic uncertainties effectively.","It implements a client-specific subnetwork inference mechanism, selects network parameters with large variance to be inferred through posterior distributions, and fixes the rest as deterministic ones.","FedSI achieves fast and scalable inference while preserving the systematic uncertainties to the fullest extent.","Extensive experiments on three different benchmark datasets demonstrate that FedSI outperforms existing Bayesian and non-Bayesian FL baselines in heterogeneous FL scenarios."],"url":"http://arxiv.org/abs/2404.15657v1","category":"cs.LG"}
