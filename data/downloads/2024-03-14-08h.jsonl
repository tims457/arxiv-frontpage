{"created":"2024-03-13 17:59:56","title":"FastMAC: Stochastic Spectral Sampling of Correspondence Graph","abstract":"3D correspondence, i.e., a pair of 3D points, is a fundamental concept in computer vision. A set of 3D correspondences, when equipped with compatibility edges, forms a correspondence graph. This graph is a critical component in several state-of-the-art 3D point cloud registration approaches, e.g., the one based on maximal cliques (MAC). However, its properties have not been well understood. So we present the first study that introduces graph signal processing into the domain of correspondence graph. We exploit the generalized degree signal on correspondence graph and pursue sampling strategies that preserve high-frequency components of this signal. To address time-consuming singular value decomposition in deterministic sampling, we resort to a stochastic approximate sampling strategy. As such, the core of our method is the stochastic spectral sampling of correspondence graph. As an application, we build a complete 3D registration algorithm termed as FastMAC, that reaches real-time speed while leading to little to none performance drop. Through extensive experiments, we validate that FastMAC works for both indoor and outdoor benchmarks. For example, FastMAC can accelerate MAC by 80 times while maintaining high registration success rate on KITTI. Codes are publicly available at https://github.com/Forrest-110/FastMAC.","sentences":["3D correspondence, i.e., a pair of 3D points, is a fundamental concept in computer vision.","A set of 3D correspondences, when equipped with compatibility edges, forms a correspondence graph.","This graph is a critical component in several state-of-the-art 3D point cloud registration approaches, e.g., the one based on maximal cliques (MAC).","However, its properties have not been well understood.","So we present the first study that introduces graph signal processing into the domain of correspondence graph.","We exploit the generalized degree signal on correspondence graph and pursue sampling strategies that preserve high-frequency components of this signal.","To address time-consuming singular value decomposition in deterministic sampling, we resort to a stochastic approximate sampling strategy.","As such, the core of our method is the stochastic spectral sampling of correspondence graph.","As an application, we build a complete 3D registration algorithm termed as FastMAC, that reaches real-time speed while leading to little to none performance drop.","Through extensive experiments, we validate that FastMAC works for both indoor and outdoor benchmarks.","For example, FastMAC can accelerate MAC by 80 times while maintaining high registration success rate on KITTI.","Codes are publicly available at https://github.com/Forrest-110/FastMAC."],"url":"http://arxiv.org/abs/2403.08770v1","category":"cs.CV"}
{"created":"2024-03-13 17:59:54","title":"The thermalization of $\u03b3$-rays in radioactive expanding ejecta: A simple model and its application for Kilonovae and Ia SNe","abstract":"A semi-analytic approximation is derived for the time-dependent fraction $f_\\gamma(t)$ of the energy deposited by radioactive decay $\\gamma$-rays in a homologously expanding plasma of general structure. An analytic approximation is given for spherically symmetric plasma distributions. Applied to Kilonovae (KNe) associated with neutron stars mergers and Type Ia supernovae, our semi-analytic and analytic approximations reproduce, with a few percent and 10% accuracy, respectively, the energy deposition rates, $\\dot{Q}_\\text{dep}$, obtained in numeric Monte Carlo calculations. The time $t_\\gamma$ beyond which $\\gamma$-ray deposition is inefficient is determined by an effective frequency-independent $\\gamma$-ray opacity $\\kappa_{\\gamma,\\text{eff}}$, $t_\\gamma = \\sqrt{\\kappa_{\\gamma,\\text{eff}}\\langle\\Sigma\\rangle t^2}$, where $\\langle\\Sigma\\rangle\\propto t^{-2}$ is the average plasma column density. For $\\beta$-decay dominated energy release, $\\kappa_{\\gamma,\\text{eff}}$ is typically close to the effective Compton scattering opacity, $\\kappa_{\\gamma,\\text{eff}} \\approx 0.025~{\\rm {cm}^{2}\\,g^{-1}}$ with a weak dependence on composition. For KNe, $\\kappa_{\\gamma,\\text{eff}}$ depends mainly on the initial electron fraction $Y_e$, $\\kappa_{\\gamma,\\text{eff}} \\approx 0.03(0.05)~{\\rm {cm}^{2}\\,g^{-1}}$ for $Y_e \\gtrsim (\\lesssim) 0.25$ (in contrast with earlier work that found $\\kappa_{\\gamma,\\text{eff}}$ larger by 1-2 orders of magnitude for low $Y_e$), and is insensitive to the (large) nuclear physics uncertainties. Determining $t_\\gamma$ from observations will therefore measure the ejecta $\\langle\\Sigma\\rangle t^2$, providing a stringent test of models. For $\\langle\\Sigma\\rangle t^2=2\\times10^{11}~{\\rm g\\,{cm}^{-2}\\,s^2}$, a typical value expected for KNe, $t_\\gamma\\approx1$ d.","sentences":["A semi-analytic approximation is derived for the time-dependent fraction $f_\\gamma(t)$ of the energy deposited by radioactive decay $\\gamma$-rays in a homologously expanding plasma of general structure.","An analytic approximation is given for spherically symmetric plasma distributions.","Applied to Kilonovae (KNe) associated with neutron stars mergers and Type Ia supernovae, our semi-analytic and analytic approximations reproduce, with a few percent and 10% accuracy, respectively, the energy deposition rates, $\\dot{Q}_\\text{dep}$, obtained in numeric Monte Carlo calculations.","The time $t_\\gamma$ beyond which $\\gamma$-ray deposition is inefficient is determined by an effective frequency-independent $\\gamma$-ray opacity $\\kappa_{\\gamma,\\text{eff}}$, $t_\\gamma = \\sqrt{\\kappa_{\\gamma,\\text{eff}}\\langle\\Sigma\\rangle t^2}$, where $\\langle\\Sigma\\rangle\\propto t^{-2}$ is the average plasma column density.","For $\\beta$-decay dominated energy release, $\\kappa_{\\gamma,\\text{eff}}$ is typically close to the effective Compton scattering opacity, $\\kappa_{\\gamma,\\text{eff}} \\approx 0.025~{\\rm {cm}^{2}\\,g^{-1}}$ with a weak dependence on composition.","For KNe, $\\kappa_{\\gamma,\\text{eff}}$ depends mainly on the initial electron fraction $Y_e$, $\\kappa_{\\gamma,\\text{eff}} \\approx 0.03(0.05)~{\\rm {cm}^{2}\\,g^{-1}}$ for $Y_e \\gtrsim (\\lesssim) 0.25$ (in contrast with earlier work that found $\\kappa_{\\gamma,\\text{eff}}$ larger by 1-2 orders of magnitude for low $Y_e$), and is insensitive to the (large) nuclear physics uncertainties.","Determining $t_\\gamma$ from observations will therefore measure the ejecta $\\langle\\Sigma\\rangle t^2$, providing a stringent test of models.","For $\\langle\\Sigma\\rangle t^2=2\\times10^{11}~{\\rm g\\,{cm}^{-2}\\,s^2}$, a typical value expected for KNe, $t_\\gamma\\approx1$ d."],"url":"http://arxiv.org/abs/2403.08769v1","category":"astro-ph.HE"}
{"created":"2024-03-13 17:59:03","title":"An Analytic Description of Electron Thermalization in Kilonovae Ejecta","abstract":"A simple analytic description is provided of the rate of energy deposition by $\\beta$-decay electrons in the homologously expanding radioactive plasma ejected in neutron star mergers, valid for a wide range of ejecta parameters -- initial entropy, electron fraction $\\{s_0,Y_e\\}$ and density $\\rho t^3$. The formulae are derived using detailed numerical calculations following the time-dependent composition and $\\beta$-decay emission spectra (including the effect of delayed deposition). The deposition efficiency depends mainly on $\\rho t^3$ and only weakly on $\\{s_0,Y_e\\}$. The time $t_e$ at which the ratio between the rates of electron energy deposition and energy production drops to $1-e^{-1}$, is given by $t_e=t_{0e}\\Big(\\frac{\\rho t^3}{0.5(\\rho t^3)_0}\\Big)^a$, where $(\\rho t^3)_0=\\frac{0.05M_{\\odot}}{4\\pi(0.2c)^3}$, $t_{0e}(s_0,Y_e)\\approx17$ days and $0.4\\le a(s_0,Y_e)\\le0.5$. The fractional uncertainty in $t_e$ due to nuclear physics uncertainties is $\\approx10\\%$. The result $a\\le0.5$ reflects the fact that the characteristic $\\beta$-decay electron energies do not decrease with time (largely due to \"inverted decay chains\" in which a slowly-decaying isotope decays to a rapidly-decaying isotope with higher end-point energy). We provide an analytic approximation for the time-dependent electron energy deposition rate, reproducing the numerical results to better than $50\\%$ (typically $<30\\%$, well within the energy production rate uncertainty due to nuclear physics uncertainties) over a 3-4 orders-of-magnitude deposition rate decrease with time. Our results may be easily incorporated in calculations of kilonovae light curves (with general density and composition structures), eliminating the need to numerically follow the time-dependent electron spectra. Identifying $t_e$, e.g. in the bolometric light curve, will constrain the (properly averaged) ejecta $\\rho t^3$.","sentences":["A simple analytic description is provided of the rate of energy deposition by $\\beta$-decay electrons in the homologously expanding radioactive plasma ejected in neutron star mergers, valid for a wide range of ejecta parameters -- initial entropy, electron fraction $\\{s_0,Y_e\\}$ and density $\\rho t^3$. The formulae are derived using detailed numerical calculations following the time-dependent composition and $\\beta$-decay emission spectra (including the effect of delayed deposition).","The deposition efficiency depends mainly on $\\rho t^3$ and only weakly on $\\{s_0,Y_e\\}$. The time $t_e$ at which the ratio between the rates of electron energy deposition and energy production drops to $1-e^{-1}$, is given by $t_e=t_{0e}\\Big(\\frac{\\rho t^3}{0.5(\\rho t^3)_0}\\Big)^a$, where $(\\rho t^3)_0=\\frac{0.05M_{\\odot}}{4\\pi(0.2c)^3}$, $t_{0e}(s_0,Y_e)\\approx17$ days and $0.4\\le a(s_0,Y_e)\\le0.5$.","The fractional uncertainty in $t_e$ due to nuclear physics uncertainties is $\\approx10\\%$. The result $a\\le0.5$ reflects the fact that the characteristic $\\beta$-decay electron energies do not decrease with time (largely due to \"inverted decay chains\" in which a slowly-decaying isotope decays to a rapidly-decaying isotope with higher end-point energy).","We provide an analytic approximation for the time-dependent electron energy deposition rate, reproducing the numerical results to better than $50\\%$ (typically $<30\\%$, well within the energy production rate uncertainty due to nuclear physics uncertainties) over a 3-4 orders-of-magnitude deposition rate decrease with time.","Our results may be easily incorporated in calculations of kilonovae light curves (with general density and composition structures), eliminating the need to numerically follow the time-dependent electron spectra.","Identifying $t_e$, e.g. in the bolometric light curve, will constrain the (properly averaged) ejecta","$\\rho t^3$."],"url":"http://arxiv.org/abs/2403.08765v1","category":"astro-ph.HE"}
{"created":"2024-03-13 17:59:02","title":"VLOGGER: Multimodal Diffusion for Embodied Avatar Synthesis","abstract":"We propose VLOGGER, a method for audio-driven human video generation from a single input image of a person, which builds on the success of recent generative diffusion models. Our method consists of 1) a stochastic human-to-3d-motion diffusion model, and 2) a novel diffusion-based architecture that augments text-to-image models with both spatial and temporal controls. This supports the generation of high quality video of variable length, easily controllable through high-level representations of human faces and bodies. In contrast to previous work, our method does not require training for each person, does not rely on face detection and cropping, generates the complete image (not just the face or the lips), and considers a broad spectrum of scenarios (e.g. visible torso or diverse subject identities) that are critical to correctly synthesize humans who communicate. We also curate MENTOR, a new and diverse dataset with 3d pose and expression annotations, one order of magnitude larger than previous ones (800,000 identities) and with dynamic gestures, on which we train and ablate our main technical contributions.   VLOGGER outperforms state-of-the-art methods in three public benchmarks, considering image quality, identity preservation and temporal consistency while also generating upper-body gestures. We analyze the performance of VLOGGER with respect to multiple diversity metrics, showing that our architectural choices and the use of MENTOR benefit training a fair and unbiased model at scale. Finally we show applications in video editing and personalization.","sentences":["We propose VLOGGER, a method for audio-driven human video generation from a single input image of a person, which builds on the success of recent generative diffusion models.","Our method consists of 1) a stochastic human-to-3d-motion diffusion model, and 2) a novel diffusion-based architecture that augments text-to-image models with both spatial and temporal controls.","This supports the generation of high quality video of variable length, easily controllable through high-level representations of human faces and bodies.","In contrast to previous work, our method does not require training for each person, does not rely on face detection and cropping, generates the complete image (not just the face or the lips), and considers a broad spectrum of scenarios (e.g. visible torso or diverse subject identities) that are critical to correctly synthesize humans who communicate.","We also curate MENTOR, a new and diverse dataset with 3d pose and expression annotations, one order of magnitude larger than previous ones (800,000 identities) and with dynamic gestures, on which we train and ablate our main technical contributions.   ","VLOGGER outperforms state-of-the-art methods in three public benchmarks, considering image quality, identity preservation and temporal consistency while also generating upper-body gestures.","We analyze the performance of VLOGGER with respect to multiple diversity metrics, showing that our architectural choices and the use of MENTOR benefit training a fair and unbiased model at scale.","Finally we show applications in video editing and personalization."],"url":"http://arxiv.org/abs/2403.08764v1","category":"cs.CV"}
{"created":"2024-03-13 17:58:57","title":"Simple and Scalable Strategies to Continually Pre-train Large Language Models","abstract":"Large language models (LLMs) are routinely pre-trained on billions of tokens, only to start the process over again once new data becomes available. A much more efficient solution is to continually pre-train these models, saving significant compute compared to re-training. However, the distribution shift induced by new data typically results in degraded performance on previous data or poor adaptation to the new data. In this work, we show that a simple and scalable combination of learning rate (LR) re-warming, LR re-decaying, and replay of previous data is sufficient to match the performance of fully re-training from scratch on all available data, as measured by final loss and language model (LM) evaluation benchmarks. Specifically, we show this for a weak but realistic distribution shift between two commonly used LLM pre-training datasets (English$\\rightarrow$English) and a stronger distribution shift (English$\\rightarrow$German) at the $405$M parameter model scale with large dataset sizes (hundreds of billions of tokens). Selecting the weak but realistic shift for larger-scale experiments, we also find that our continual learning strategies match the re-training baseline for a 10B parameter LLM. Our results demonstrate that LLMs can be successfully updated via simple and scalable continual learning strategies, matching the re-training baseline using only a fraction of the compute. Finally, inspired by previous work, we propose alternatives to the cosine learning rate schedule that help circumvent forgetting induced by LR re-warming and that are not bound to a fixed token budget.","sentences":["Large language models (LLMs) are routinely pre-trained on billions of tokens, only to start the process over again once new data becomes available.","A much more efficient solution is to continually pre-train these models, saving significant compute compared to re-training.","However, the distribution shift induced by new data typically results in degraded performance on previous data or poor adaptation to the new data.","In this work, we show that a simple and scalable combination of learning rate (LR) re-warming, LR re-decaying, and replay of previous data is sufficient to match the performance of fully re-training from scratch on all available data, as measured by final loss and language model (LM) evaluation benchmarks.","Specifically, we show this for a weak but realistic distribution shift between two commonly used LLM pre-training datasets (English$\\rightarrow$English) and a stronger distribution shift (English$\\rightarrow$German) at the $405$M parameter model scale with large dataset sizes (hundreds of billions of tokens).","Selecting the weak but realistic shift for larger-scale experiments, we also find that our continual learning strategies match the re-training baseline for a 10B parameter LLM.","Our results demonstrate that LLMs can be successfully updated via simple and scalable continual learning strategies, matching the re-training baseline using only a fraction of the compute.","Finally, inspired by previous work, we propose alternatives to the cosine learning rate schedule that help circumvent forgetting induced by LR re-warming and that are not bound to a fixed token budget."],"url":"http://arxiv.org/abs/2403.08763v1","category":"cs.LG"}
{"created":"2024-03-13 17:57:26","title":"Observational tests in scale invariance I: galaxy clusters and rotation of galaxies","abstract":"Galaxy velocities in clusters, rotation curves of galaxies, and \"vertical\" oscillations in the Milky Way currently show too high velocities with respect to the masses thought to be involved. While these velocity excesses are currently interpreted as the consequence of dark matter, it can also be naturally explained as a consequence of scale invariant theory, which rests on a very simple first principle: the addition of a new fundamental symmetry. In the present work, the case of scale invariance, present in General Relativity and Maxwell equations for the empty space without charge and current, is considered. Cosmological models predict a rapid decrease of these effects with increasing mean density up to the critical density, where they totally disappear. Starting from the scale invariant geodesic equation by Dirac (1973), for which a demonstration by an action principle is presented, a modified Newton equation is derived. The solutions of this equation are applied to clusters of galaxies, galactic rotation at different redshifts and \"vertical\" motions in the Milky Way. In this new framework, the convergence of theoretical predictions and observations, in different gravitational systems, epochs, mass range and spatial scales, opens interesting perspectives that deserve to be explored further.","sentences":["Galaxy velocities in clusters, rotation curves of galaxies, and \"vertical\" oscillations in the Milky Way currently show too high velocities with respect to the masses thought to be involved.","While these velocity excesses are currently interpreted as the consequence of dark matter, it can also be naturally explained as a consequence of scale invariant theory, which rests on a very simple first principle: the addition of a new fundamental symmetry.","In the present work, the case of scale invariance, present in General Relativity and Maxwell equations for the empty space without charge and current, is considered.","Cosmological models predict a rapid decrease of these effects with increasing mean density up to the critical density, where they totally disappear.","Starting from the scale invariant geodesic equation by Dirac (1973), for which a demonstration by an action principle is presented, a modified Newton equation is derived.","The solutions of this equation are applied to clusters of galaxies, galactic rotation at different redshifts and \"vertical\" motions in the Milky Way.","In this new framework, the convergence of theoretical predictions and observations, in different gravitational systems, epochs, mass range and spatial scales, opens interesting perspectives that deserve to be explored further."],"url":"http://arxiv.org/abs/2403.08759v1","category":"astro-ph.GA"}
{"created":"2024-03-13 17:56:12","title":"Spatiotemporal Diffusion Model with Paired Sampling for Accelerated Cardiac Cine MRI","abstract":"Current deep learning reconstruction for accelerated cardiac cine MRI suffers from spatial and temporal blurring. We aim to improve image sharpness and motion delineation for cine MRI under high undersampling rates. A spatiotemporal diffusion enhancement model conditional on an existing deep learning reconstruction along with a novel paired sampling strategy was developed. The diffusion model provided sharper tissue boundaries and clearer motion than the original reconstruction in experts evaluation on clinical data. The innovative paired sampling strategy substantially reduced artificial noises in the generative results.","sentences":["Current deep learning reconstruction for accelerated cardiac cine MRI suffers from spatial and temporal blurring.","We aim to improve image sharpness and motion delineation for cine MRI under high undersampling rates.","A spatiotemporal diffusion enhancement model conditional on an existing deep learning reconstruction along with a novel paired sampling strategy was developed.","The diffusion model provided sharper tissue boundaries and clearer motion than the original reconstruction in experts evaluation on clinical data.","The innovative paired sampling strategy substantially reduced artificial noises in the generative results."],"url":"http://arxiv.org/abs/2403.08758v1","category":"eess.IV"}
{"created":"2024-03-13 17:55:34","title":"Efficient Combinatorial Optimization via Heat Diffusion","abstract":"Combinatorial optimization problems are widespread but inherently challenging due to their discrete nature.The primary limitation of existing methods is that they can only access a small fraction of the solution space at each iteration, resulting in limited efficiency for searching the global optimal. To overcome this challenge, diverging from conventional efforts of expanding the solver's search scope, we focus on enabling information to actively propagate to the solver through heat diffusion. By transforming the target function while preserving its optima, heat diffusion facilitates information flow from distant regions to the solver, providing more efficient navigation. Utilizing heat diffusion, we propose a framework for solving general combinatorial optimization problems. The proposed methodology demonstrates superior performance across a range of the most challenging and widely encountered combinatorial optimizations. Echoing recent advancements in harnessing thermodynamics for generative artificial intelligence, our study further reveals its significant potential in advancing combinatorial optimization.","sentences":["Combinatorial optimization problems are widespread but inherently challenging due to their discrete nature.","The primary limitation of existing methods is that they can only access a small fraction of the solution space at each iteration, resulting in limited efficiency for searching the global optimal.","To overcome this challenge, diverging from conventional efforts of expanding the solver's search scope, we focus on enabling information to actively propagate to the solver through heat diffusion.","By transforming the target function while preserving its optima, heat diffusion facilitates information flow from distant regions to the solver, providing more efficient navigation.","Utilizing heat diffusion, we propose a framework for solving general combinatorial optimization problems.","The proposed methodology demonstrates superior performance across a range of the most challenging and widely encountered combinatorial optimizations.","Echoing recent advancements in harnessing thermodynamics for generative artificial intelligence, our study further reveals its significant potential in advancing combinatorial optimization."],"url":"http://arxiv.org/abs/2403.08757v1","category":"stat.ML"}
{"created":"2024-03-13 17:53:47","title":"DAM: Dynamic Adapter Merging for Continual Video QA Learning","abstract":"We present a parameter-efficient method for continual video question-answering (VidQA) learning. Our method, named DAM, uses the proposed Dynamic Adapter Merging to (i) mitigate catastrophic forgetting, (ii) enable efficient adaptation to continually arriving datasets, (iii) handle inputs from unknown datasets during inference, and (iv) enable knowledge sharing across similar dataset domains. Given a set of continually streaming VidQA datasets, we sequentially train dataset-specific adapters for each dataset while freezing the parameters of a large pretrained video-language backbone. During inference, given a video-question sample from an unknown domain, our method first uses the proposed non-parametric router function to compute a probability for each adapter, reflecting how relevant that adapter is to the current video-question input instance. Subsequently, the proposed dynamic adapter merging scheme aggregates all the adapter weights into a new adapter instance tailored for that particular test sample to compute the final VidQA prediction, mitigating the impact of inaccurate router predictions and facilitating knowledge sharing across domains. Our DAM model outperforms prior state-of-the-art continual learning approaches by 9.1% while exhibiting 1.9% less forgetting on 6 VidQA datasets spanning various domains. We further extend DAM to continual image classification and image QA and outperform prior methods by a large margin. The code is publicly available at: https://github.com/klauscc/DAM","sentences":["We present a parameter-efficient method for continual video question-answering (VidQA) learning.","Our method, named DAM, uses the proposed Dynamic Adapter Merging to (i) mitigate catastrophic forgetting, (ii) enable efficient adaptation to continually arriving datasets, (iii) handle inputs from unknown datasets during inference, and (iv) enable knowledge sharing across similar dataset domains.","Given a set of continually streaming VidQA datasets, we sequentially train dataset-specific adapters for each dataset while freezing the parameters of a large pretrained video-language backbone.","During inference, given a video-question sample from an unknown domain, our method first uses the proposed non-parametric router function to compute a probability for each adapter, reflecting how relevant that adapter is to the current video-question input instance.","Subsequently, the proposed dynamic adapter merging scheme aggregates all the adapter weights into a new adapter instance tailored for that particular test sample to compute the final VidQA prediction, mitigating the impact of inaccurate router predictions and facilitating knowledge sharing across domains.","Our DAM model outperforms prior state-of-the-art continual learning approaches by 9.1% while exhibiting 1.9% less forgetting on 6 VidQA datasets spanning various domains.","We further extend DAM to continual image classification and image QA and outperform prior methods by a large margin.","The code is publicly available at: https://github.com/klauscc/DAM"],"url":"http://arxiv.org/abs/2403.08755v1","category":"cs.CV"}
{"created":"2024-03-13 17:51:47","title":"Invalid proxies and volatility changes","abstract":"When in proxy-SVARs the covariance matrix of VAR disturbances is subject to exogenous, permanent, nonrecurring breaks that generate target impulse response functions (IRFs) that change across volatility regimes, even strong, exogenous external instruments can result in inconsistent estimates of the dynamic causal effects of interest if the breaks are not properly accounted for. In such cases, it is essential to explicitly incorporate the shifts in unconditional volatility in order to point-identify the target structural shocks and possibly restore consistency. We demonstrate that, under a necessary and sufficient rank condition that leverages moments implied by changes in volatility, the target IRFs can be point-identified and consistently estimated. Importantly, standard asymptotic inference remains valid in this context despite (i) the covariance between the proxies and the instrumented structural shocks being local-to-zero, as in Staiger and Stock (1997), and (ii) the potential failure of instrument exogeneity. We introduce a novel identification strategy that appropriately combines external instruments with \"informative\" changes in volatility, thus obviating the need to assume proxy relevance and exogeneity in estimation. We illustrate the effectiveness of the suggested method by revisiting a fiscal proxy-SVAR previously estimated in the literature, complementing the fiscal instruments with information derived from the massive reduction in volatility observed in the transition from the Great Inflation to the Great Moderation regimes.","sentences":["When in proxy-SVARs the covariance matrix of VAR disturbances is subject to exogenous, permanent, nonrecurring breaks that generate target impulse response functions (IRFs) that change across volatility regimes, even strong, exogenous external instruments can result in inconsistent estimates of the dynamic causal effects of interest if the breaks are not properly accounted for.","In such cases, it is essential to explicitly incorporate the shifts in unconditional volatility in order to point-identify the target structural shocks and possibly restore consistency.","We demonstrate that, under a necessary and sufficient rank condition that leverages moments implied by changes in volatility, the target IRFs can be point-identified and consistently estimated.","Importantly, standard asymptotic inference remains valid in this context despite (i) the covariance between the proxies and the instrumented structural shocks being local-to-zero, as in Staiger and Stock (1997), and (ii) the potential failure of instrument exogeneity.","We introduce a novel identification strategy that appropriately combines external instruments with \"informative\" changes in volatility, thus obviating the need to assume proxy relevance and exogeneity in estimation.","We illustrate the effectiveness of the suggested method by revisiting a fiscal proxy-SVAR previously estimated in the literature, complementing the fiscal instruments with information derived from the massive reduction in volatility observed in the transition from the Great Inflation to the Great Moderation regimes."],"url":"http://arxiv.org/abs/2403.08753v1","category":"econ.EM"}
{"created":"2024-03-13 17:51:33","title":"A local model for the optical energy and momentum transfer in dielectric media and the microscopic origin of Abraham's force density","abstract":"We report on the continuity equations for linear momentum and energy associated to a recently introduced electromagnetic formulation based on classical dipolar sources [Eur. Phys. J. Plus 138, 1034 (2023)]. When connected to the mass-polariton quasi-particle dynamics, these equations provide a consistent microscopic description of the local optical energy and momentum transfer inside dielectric media, called microscopic mass-polariton formulation. This procedure also unveils the true microscopic origin of the long-known Abraham optical force density as an interplay between induced dipoles and mechanical stresses generated within the material.","sentences":["We report on the continuity equations for linear momentum and energy associated to a recently introduced electromagnetic formulation based on classical dipolar sources [Eur. Phys.","J. Plus 138, 1034 (2023)].","When connected to the mass-polariton quasi-particle dynamics, these equations provide a consistent microscopic description of the local optical energy and momentum transfer inside dielectric media, called microscopic mass-polariton formulation.","This procedure also unveils the true microscopic origin of the long-known Abraham optical force density as an interplay between induced dipoles and mechanical stresses generated within the material."],"url":"http://arxiv.org/abs/2403.08752v1","category":"physics.optics"}
{"created":"2024-03-13 17:51:08","title":"Cyclotomic Factors and LRS-Degeneracy","abstract":"We present three new, practical algorithms for polynomials in $\\mathbb{Z}[x]$: one to test if a polynomial is cyclotomic, one to determine which cyclotomic polynomials are factors, and one to determine whether the given polynomial is LRS-degenerate. A polynomial is ``LRS-degenerate'' iff it has two distinct roots $\\alpha, \\beta$ such that $\\beta = \\zeta \\alpha$ for some root of unity $\\zeta$. All three algorithms are based on ``intelligent brute force''. The first two produce the indexes of the cyclotomic polynomials; the third produces a list of degeneracy orders. The algorithms are implemented in CoCoALib.","sentences":["We present three new, practical algorithms for polynomials in $\\mathbb{Z}[x]$: one to test if a polynomial is cyclotomic, one to determine which cyclotomic polynomials are factors, and one to determine whether the given polynomial is LRS-degenerate.","A polynomial is ``LRS-degenerate'' iff it has two distinct roots $\\alpha, \\beta$ such that $\\beta = \\zeta \\alpha$ for some root of unity $\\zeta$. All three algorithms are based on ``intelligent brute force''.","The first two produce the indexes of the cyclotomic polynomials; the third produces a list of degeneracy orders.","The algorithms are implemented in CoCoALib."],"url":"http://arxiv.org/abs/2403.08751v1","category":"math.AC"}
{"created":"2024-03-13 17:48:39","title":"iCONTRA: Toward Thematic Collection Design Via Interactive Concept Transfer","abstract":"Creating thematic collections in industries demands innovative designs and cohesive concepts. Designers may face challenges in maintaining thematic consistency when drawing inspiration from existing objects, landscapes, or artifacts. While AI-powered graphic design tools offer help, they often fail to generate cohesive sets based on specific thematic concepts. In response, we introduce iCONTRA, an interactive CONcept TRAnsfer system. With a user-friendly interface, iCONTRA enables both experienced designers and novices to effortlessly explore creative design concepts and efficiently generate thematic collections. We also propose a zero-shot image editing algorithm, eliminating the need for fine-tuning models, which gradually integrates information from initial objects, ensuring consistency in the generation process without influencing the background. A pilot study suggests iCONTRA's potential to reduce designers' efforts. Experimental results demonstrate its effectiveness in producing consistent and high-quality object concept transfers. iCONTRA stands as a promising tool for innovation and creative exploration in thematic collection design. The source code will be available at: https://github.com/vdkhoi20/iCONTRA.","sentences":["Creating thematic collections in industries demands innovative designs and cohesive concepts.","Designers may face challenges in maintaining thematic consistency when drawing inspiration from existing objects, landscapes, or artifacts.","While AI-powered graphic design tools offer help, they often fail to generate cohesive sets based on specific thematic concepts.","In response, we introduce iCONTRA, an interactive CONcept TRAnsfer system.","With a user-friendly interface, iCONTRA enables both experienced designers and novices to effortlessly explore creative design concepts and efficiently generate thematic collections.","We also propose a zero-shot image editing algorithm, eliminating the need for fine-tuning models, which gradually integrates information from initial objects, ensuring consistency in the generation process without influencing the background.","A pilot study suggests iCONTRA's potential to reduce designers' efforts.","Experimental results demonstrate its effectiveness in producing consistent and high-quality object concept transfers.","iCONTRA stands as a promising tool for innovation and creative exploration in thematic collection design.","The source code will be available at: https://github.com/vdkhoi20/iCONTRA."],"url":"http://arxiv.org/abs/2403.08746v1","category":"cs.CV"}
{"created":"2024-03-13 17:46:28","title":"Steering LLMs Towards Unbiased Responses: A Causality-Guided Debiasing Framework","abstract":"Large language models (LLMs) can easily generate biased and discriminative responses. As LLMs tap into consequential decision-making (e.g., hiring and healthcare), it is of crucial importance to develop strategies to mitigate these biases. This paper focuses on social bias, tackling the association between demographic information and LLM outputs. We propose a causality-guided debiasing framework that utilizes causal understandings of (1) the data-generating process of the training corpus fed to LLMs, and (2) the internal reasoning process of LLM inference, to guide the design of prompts for debiasing LLM outputs through selection mechanisms. Our framework unifies existing de-biasing prompting approaches such as inhibitive instructions and in-context contrastive examples, and sheds light on new ways of debiasing by encouraging bias-free reasoning. Our strong empirical performance on real-world datasets demonstrates that our framework provides principled guidelines on debiasing LLM outputs even with only the black-box access.","sentences":["Large language models (LLMs) can easily generate biased and discriminative responses.","As LLMs tap into consequential decision-making (e.g., hiring and healthcare), it is of crucial importance to develop strategies to mitigate these biases.","This paper focuses on social bias, tackling the association between demographic information and LLM outputs.","We propose a causality-guided debiasing framework that utilizes causal understandings of (1) the data-generating process of the training corpus fed to LLMs, and (2) the internal reasoning process of LLM inference, to guide the design of prompts for debiasing LLM outputs through selection mechanisms.","Our framework unifies existing de-biasing prompting approaches such as inhibitive instructions and in-context contrastive examples, and sheds light on new ways of debiasing by encouraging bias-free reasoning.","Our strong empirical performance on real-world datasets demonstrates that our framework provides principled guidelines on debiasing LLM outputs even with only the black-box access."],"url":"http://arxiv.org/abs/2403.08743v1","category":"cs.CL"}
{"created":"2024-03-13 17:44:16","title":"Learning How to Strategically Disclose Information","abstract":"Strategic information disclosure, in its simplest form, considers a game between an information provider (sender) who has access to some private information that an information receiver is interested in. While the receiver takes an action that affects the utilities of both players, the sender can design information (or modify beliefs) of the receiver through signal commitment, hence posing a Stackelberg game. However, obtaining a Stackelberg equilibrium for this game traditionally requires the sender to have access to the receiver's objective. In this work, we consider an online version of information design where a sender interacts with a receiver of an unknown type who is adversarially chosen at each round. Restricting attention to Gaussian prior and quadratic costs for the sender and the receiver, we show that $\\mathcal{O}(\\sqrt{T})$ regret is achievable with full information feedback, where $T$ is the total number of interactions between the sender and the receiver. Further, we propose a novel parametrization that allows the sender to achieve $\\mathcal{O}(\\sqrt{T})$ regret for a general convex utility function. We then consider the Bayesian Persuasion problem with an additional cost term in the objective function, which penalizes signaling policies that are more informative and obtain $\\mathcal{O}(\\log(T))$ regret. Finally, we establish a sublinear regret bound for the partial information feedback setting and provide simulations to support our theoretical results.","sentences":["Strategic information disclosure, in its simplest form, considers a game between an information provider (sender) who has access to some private information that an information receiver is interested in.","While the receiver takes an action that affects the utilities of both players, the sender can design information (or modify beliefs) of the receiver through signal commitment, hence posing a Stackelberg game.","However, obtaining a Stackelberg equilibrium for this game traditionally requires the sender to have access to the receiver's objective.","In this work, we consider an online version of information design where a sender interacts with a receiver of an unknown type who is adversarially chosen at each round.","Restricting attention to Gaussian prior and quadratic costs for the sender and the receiver, we show that $\\mathcal{O}(\\sqrt{T})$ regret is achievable with full information feedback, where $T$ is the total number of interactions between the sender and the receiver.","Further, we propose a novel parametrization that allows the sender to achieve $\\mathcal{O}(\\sqrt{T})$ regret for a general convex utility function.","We then consider the Bayesian Persuasion problem with an additional cost term in the objective function, which penalizes signaling policies that are more informative and obtain $\\mathcal{O}(\\log(T))$ regret.","Finally, we establish a sublinear regret bound for the partial information feedback setting and provide simulations to support our theoretical results."],"url":"http://arxiv.org/abs/2403.08741v1","category":"cs.GT"}
{"created":"2024-03-13 17:42:32","title":"The Garden of Forking Paths: Observing Dynamic Parameters Distribution in Large Language Models","abstract":"A substantial gap persists in understanding the reasons behind the exceptional performance of the Transformer architecture in NLP. A particularly unexplored area involves the mechanistic description of how the distribution of parameters evolves over time during training. In this work we suggest that looking at the time evolution of the statistic distribution of model parameters, and specifically at bifurcation effects, can help understanding the model quality, potentially reducing training costs and evaluation efforts and empirically showing the reasons behind the effectiveness of weights sparsification.","sentences":["A substantial gap persists in understanding the reasons behind the exceptional performance of the Transformer architecture in NLP.","A particularly unexplored area involves the mechanistic description of how the distribution of parameters evolves over time during training.","In this work we suggest that looking at the time evolution of the statistic distribution of model parameters, and specifically at bifurcation effects, can help understanding the model quality, potentially reducing training costs and evaluation efforts and empirically showing the reasons behind the effectiveness of weights sparsification."],"url":"http://arxiv.org/abs/2403.08739v1","category":"cs.CL"}
{"created":"2024-03-13 17:37:54","title":"Interface Design Beyond Epitaxy: Oxide Heterostructures Comprising Symmetry-forbidden Interfaces","abstract":"Epitaxial growth of thin-film heterostructures is generally considered the most successful procedure to obtain interfaces of excellent structural and electronic quality between three-dimensional materials. However, these interfaces can only join material systems with crystal lattices of matching symmetries and lattice constants. We present a novel category of interfaces, the fabrication of which is membrane-based and does not require epitaxial growth. These interfaces therefore overcome limitations imposed by epitaxy. Leveraging the additional degrees of freedom gained, we demonstrate atomically clean interfaces between three-fold symmetric sapphire and four-fold symmetric SrTiO3. Atomic-resolution imaging reveals structurally well-defined interfaces with a novel moir\\'e-type reconstruction.","sentences":["Epitaxial growth of thin-film heterostructures is generally considered the most successful procedure to obtain interfaces of excellent structural and electronic quality between three-dimensional materials.","However, these interfaces can only join material systems with crystal lattices of matching symmetries and lattice constants.","We present a novel category of interfaces, the fabrication of which is membrane-based and does not require epitaxial growth.","These interfaces therefore overcome limitations imposed by epitaxy.","Leveraging the additional degrees of freedom gained, we demonstrate atomically clean interfaces between three-fold symmetric sapphire and four-fold symmetric SrTiO3.","Atomic-resolution imaging reveals structurally well-defined interfaces with a novel moir\\'e-type reconstruction."],"url":"http://arxiv.org/abs/2403.08736v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-13 17:35:50","title":"Light-driven interlayer propagation of collective-mode excitations in layered superconductors","abstract":"Superconductors exhibit a nonlinear interaction with an applied light, which can resonantly excite the collective amplitude (Higgs) mode. Here we study light-induced dynamics of layered superconductors, where each layer is coupled to adjacent layers via the Josephson coupling and the first few layers near the surface are driven by an in-plane-polarized light. We study the system under the assumption that the interlayer Coulomb interactions are sufficiently screened out and that the phase-difference mode becomes available in the low-energy regime. We find that interlayer transport is induced via excitations of the collective amplitude and phase-difference modes, even when the applied electric field is parallel to the planes. We provide analytic calculations as well as numerical simulations of the real-time dynamics, and investigate the influence on the light-induced interlayer Josephson current and intralayer third-harmonic generation.","sentences":["Superconductors exhibit a nonlinear interaction with an applied light, which can resonantly excite the collective amplitude (Higgs) mode.","Here we study light-induced dynamics of layered superconductors, where each layer is coupled to adjacent layers via the Josephson coupling and the first few layers near the surface are driven by an in-plane-polarized light.","We study the system under the assumption that the interlayer Coulomb interactions are sufficiently screened out and that the phase-difference mode becomes available in the low-energy regime.","We find that interlayer transport is induced via excitations of the collective amplitude and phase-difference modes, even when the applied electric field is parallel to the planes.","We provide analytic calculations as well as numerical simulations of the real-time dynamics, and investigate the influence on the light-induced interlayer Josephson current and intralayer third-harmonic generation."],"url":"http://arxiv.org/abs/2403.08734v1","category":"cond-mat.supr-con"}
{"created":"2024-03-13 17:31:23","title":"Molecular pentaquarks with hidden charm and double strangeness","abstract":"We analyze theoretically the coupled-channel meson-baryon interaction with global flavor $\\bar c c s s n$ and $\\bar c c s s s$, where mesons are pseudoscalars or vectors and baryons have $J ^P=1/2^+$ or $3/2^+$. The aim is to explore whether the nonlinear dynamics inherent in the unitarization process within coupled channels can dynamically generate double- and triple-strange pentaquark-type states ($P_{css}$ and $P_{csss}$ respectively), for which there is no experimental evidence to date. We evaluate the s-wave scattering matrix by implementing unitarity in coupled channels, using potential kernels obtained from t-channel vector meson exchange. The required $PPV$ and $VVV$ vertices are obtained from Lagrangians derived through appropriate extensions of the local hidden gauge symmetry approach to the charm sector, while capitalizing on the symmetry of the spin and flavor wave function to evaluate the $BBV$ vertex. We find four different poles in the double strange sector, some of them degenerate in spin. For the triple-strange channel we find the meson-baryon interaction insufficient to generate a bound or resonance state through the unitary coupled-channel dynamics.","sentences":["We analyze theoretically the coupled-channel meson-baryon interaction with global flavor $\\bar c c s s n$ and $\\bar c c s s s$, where mesons are pseudoscalars or vectors and baryons have $J ^P=1/2^+$ or $3/2^+$. The aim is to explore whether the nonlinear dynamics inherent in the unitarization process within coupled channels can dynamically generate double- and triple-strange pentaquark-type states ($P_{css}$ and $P_{csss}$ respectively), for which there is no experimental evidence to date.","We evaluate the s-wave scattering matrix by implementing unitarity in coupled channels, using potential kernels obtained from t-channel vector meson exchange.","The required $PPV$ and $VVV$ vertices are obtained from Lagrangians derived through appropriate extensions of the local hidden gauge symmetry approach to the charm sector, while capitalizing on the symmetry of the spin and flavor wave function to evaluate the $BBV$ vertex.","We find four different poles in the double strange sector, some of them degenerate in spin.","For the triple-strange channel we find the meson-baryon interaction insufficient to generate a bound or resonance state through the unitary coupled-channel dynamics."],"url":"http://arxiv.org/abs/2403.08732v1","category":"hep-ph"}
{"created":"2024-03-13 17:29:45","title":"Strengthening Multimodal Large Language Model with Bootstrapped Preference Optimization","abstract":"Multimodal Large Language Models (MLLMs) excel in generating responses based on visual inputs. However, they often suffer from a bias towards generating responses similar to their pretraining corpus, overshadowing the importance of visual information. We treat this bias as a \"preference\" for pretraining statistics, which hinders the model's grounding in visual input. To mitigate this issue, we propose Bootstrapped Preference Optimization (BPO), which conducts preference learning with datasets containing negative responses bootstrapped from the model itself. Specifically, we propose the following two strategies: 1) using distorted image inputs to the MLLM for eliciting responses that contain signified pretraining bias; 2) leveraging text-based LLM to explicitly inject erroneous but common elements into the original response. Those undesirable responses are paired with original annotated responses from the datasets to construct the preference dataset, which is subsequently utilized to perform preference learning. Our approach effectively suppresses pretrained LLM bias, enabling enhanced grounding in visual inputs. Extensive experimentation demonstrates significant performance improvements across multiple benchmarks, advancing the state-of-the-art in multimodal conversational systems.","sentences":["Multimodal Large Language Models (MLLMs) excel in generating responses based on visual inputs.","However, they often suffer from a bias towards generating responses similar to their pretraining corpus, overshadowing the importance of visual information.","We treat this bias as a \"preference\" for pretraining statistics, which hinders the model's grounding in visual input.","To mitigate this issue, we propose Bootstrapped Preference Optimization (BPO), which conducts preference learning with datasets containing negative responses bootstrapped from the model itself.","Specifically, we propose the following two strategies: 1) using distorted image inputs to the MLLM for eliciting responses that contain signified pretraining bias; 2) leveraging text-based LLM to explicitly inject erroneous but common elements into the original response.","Those undesirable responses are paired with original annotated responses from the datasets to construct the preference dataset, which is subsequently utilized to perform preference learning.","Our approach effectively suppresses pretrained LLM bias, enabling enhanced grounding in visual inputs.","Extensive experimentation demonstrates significant performance improvements across multiple benchmarks, advancing the state-of-the-art in multimodal conversational systems."],"url":"http://arxiv.org/abs/2403.08730v1","category":"cs.CL"}
{"created":"2024-03-13 17:29:05","title":"Efficient and practical Hamiltonian simulation from time-dependent product formulas","abstract":"In this work we propose an approach for implementing time-evolution of a quantum system using product formulas. The quantum algorithms we develop have provably better scaling (in terms of gate complexity and circuit depth) than a naive application of well-known Trotter formulas, for systems where the evolution is determined by a Hamiltonian with different energy scales (i.e., one part is \"large\" and another part is \"small\"). Our algorithms generate a decomposition of the evolution operator into a product of simple unitaries that are directly implementable on a quantum computer. Although the theoretical scaling is suboptimal compared with state-of-the-art algorithms (e.g., quantum signal processing), the performance of the algorithms we propose is highly competitive in practice. We illustrate this via extensive numerical simulations for several models. For instance, in the strong-field regime of the 1D transverse-field Ising model, our algorithms achieve an improvement of one order of magnitude in both the system size and evolution time that can be simulated with a fixed budget of 1000 arbitrary 2-qubit gates, compared with standard Trotter formulas.","sentences":["In this work we propose an approach for implementing time-evolution of a quantum system using product formulas.","The quantum algorithms we develop have provably better scaling (in terms of gate complexity and circuit depth) than a naive application of well-known Trotter formulas, for systems where the evolution is determined by a Hamiltonian with different energy scales (i.e., one part is \"large\" and another part is \"small\").","Our algorithms generate a decomposition of the evolution operator into a product of simple unitaries that are directly implementable on a quantum computer.","Although the theoretical scaling is suboptimal compared with state-of-the-art algorithms (e.g., quantum signal processing), the performance of the algorithms we propose is highly competitive in practice.","We illustrate this via extensive numerical simulations for several models.","For instance, in the strong-field regime of the 1D transverse-field Ising model, our algorithms achieve an improvement of one order of magnitude in both the system size and evolution time that can be simulated with a fixed budget of 1000 arbitrary 2-qubit gates, compared with standard Trotter formulas."],"url":"http://arxiv.org/abs/2403.08729v1","category":"quant-ph"}
{"created":"2024-03-13 17:28:20","title":"Ambient Diffusion Posterior Sampling: Solving Inverse Problems with Diffusion Models trained on Corrupted Data","abstract":"We provide a framework for solving inverse problems with diffusion models learned from linearly corrupted data. Our method, Ambient Diffusion Posterior Sampling (A-DPS), leverages a generative model pre-trained on one type of corruption (e.g. image inpainting) to perform posterior sampling conditioned on measurements from a potentially different forward process (e.g. image blurring). We test the efficacy of our approach on standard natural image datasets (CelebA, FFHQ, and AFHQ) and we show that A-DPS can sometimes outperform models trained on clean data for several image restoration tasks in both speed and performance. We further extend the Ambient Diffusion framework to train MRI models with access only to Fourier subsampled multi-coil MRI measurements at various acceleration factors (R=2, 4, 6, 8). We again observe that models trained on highly subsampled data are better priors for solving inverse problems in the high acceleration regime than models trained on fully sampled data. We open-source our code and the trained Ambient Diffusion MRI models: https://github.com/utcsilab/ambient-diffusion-mri .","sentences":["We provide a framework for solving inverse problems with diffusion models learned from linearly corrupted data.","Our method, Ambient Diffusion Posterior Sampling (A-DPS), leverages a generative model pre-trained on one type of corruption (e.g. image inpainting) to perform posterior sampling conditioned on measurements from a potentially different forward process (e.g. image blurring).","We test the efficacy of our approach on standard natural image datasets (CelebA, FFHQ, and AFHQ) and we show that A-DPS can sometimes outperform models trained on clean data for several image restoration tasks in both speed and performance.","We further extend the Ambient Diffusion framework to train MRI models with access only to Fourier subsampled multi-coil MRI measurements at various acceleration factors (R=2, 4, 6, 8).","We again observe that models trained on highly subsampled data are better priors for solving inverse problems in the high acceleration regime than models trained on fully sampled data.","We open-source our code and the trained Ambient Diffusion MRI models: https://github.com/utcsilab/ambient-diffusion-mri ."],"url":"http://arxiv.org/abs/2403.08728v1","category":"cs.CV"}
{"created":"2024-03-13 17:28:06","title":"The q-ary Gilbert-Varshamov bound can be improved for all but finitely many positive integers q","abstract":"For any positive integer $q\\geq 2$ and any real number $\\delta\\in(0,1)$, let $\\alpha_q(n,\\delta n)$ denote the maximum size of a subset of $\\mathbb{Z}_q^n$ with minimum Hamming distance at least $\\delta n$, where $\\mathbb{Z}_q=\\{0,1,\\dotsc,q-1\\}$ and $n\\in\\mathbb{N}$. The asymptotic rate function is defined by $ R_q(\\delta) = \\limsup_{n\\rightarrow\\infty}\\frac{1}{n}\\log_q\\alpha_q(n,\\delta n). $ The famous $q$-ary asymptotic Gilbert-Varshamov bound, obtained in the 1950s, states that \\[ R_q(\\delta) \\geq 1 - \\delta\\log_q(q-1)-\\delta\\log_q\\frac{1}{\\delta}-(1-\\delta)\\log_q\\frac{1}{1-\\delta} \\stackrel{\\mathrm{def}}{=}R_\\mathrm{GV}(\\delta,q) \\] for all positive integers $q\\geq 2$ and $0<\\delta<1-q^{-1}$. In the case that $q$ is an even power of a prime with $q\\geq 49$, the $q$-ary Gilbert-Varshamov bound was firstly improved by using algebraic geometry codes in the works of Tsfasman, Vladut, and Zink and of Ihara in the 1980s. The further investigation in algebraic geometry codes has shown that the $q$-ary Gilbert-Varshamov bound can also be improved in the case that $q$ is an odd power of a prime but not a prime with $q > 125$. However, it remains a long-standing open problem whether the $q$-ary Gilbert-Varshamov bound would be tight for those infinitely many integers $q$ which is a prime, except for Fermat primes not less than 257, and which is a generic positive integer not being a prime power.   In this paper, we prove that the $q$-ary Gilbert-Varshamov bound can be improved for all but finitely many positive integers $q\\geq 2$. It is shown that $ R_q(1/2) > R_\\mathrm{GV}(1/2,q) $ for all integers $q > \\exp(29)$. Furthermore, we show that the growth of the rate function $R_q(\\delta)$ for $\\delta\\in(0,1)$ fixed and $q$ growing large has a nontrivial lower bound. These new lower bounds are achieved by using codes from geometry of numbers introduced by Lenstra in the 1980s.","sentences":["For any positive integer $q\\geq 2$ and any real number $\\delta\\in(0,1)$, let $\\alpha_q(n,\\delta n)$ denote the maximum size of a subset of $\\mathbb{Z}_q^n$ with minimum Hamming distance at least $\\delta n$, where $\\mathbb{Z}_q=\\{0,1,\\dotsc,q-1\\}$ and $n\\in\\mathbb{N}$. The asymptotic rate function is defined by $ R_q(\\delta) = \\limsup_{n\\rightarrow\\infty}\\frac{1}{n}\\log_q\\alpha_q(n,\\delta n).","$ The famous $q$-ary asymptotic Gilbert-Varshamov bound, obtained in the 1950s, states that \\[ R_q(\\delta) \\geq 1 - \\delta\\log_q(q-1)-\\delta\\log_q\\frac{1}{\\delta}-(1-\\delta)\\log_q\\frac{1}{1-\\delta} \\stackrel{\\mathrm{def}}{=}R_\\mathrm{GV}(\\delta,q) \\] for all positive integers $q\\geq 2$ and $0<\\delta<1-q^{-1}$. In the case that $q$ is an even power of a prime with $q\\geq 49$, the $q$-ary Gilbert-Varshamov bound was firstly improved by using algebraic geometry codes in the works of Tsfasman, Vladut, and Zink and of Ihara in the 1980s.","The further investigation in algebraic geometry codes has shown that the $q$-ary Gilbert-Varshamov bound can also be improved in the case that $q$ is an odd power of a prime but not a prime with $q > 125$.","However, it remains a long-standing open problem whether the $q$-ary Gilbert-Varshamov bound would be tight for those infinitely many integers $q$ which is a prime, except for Fermat primes not less than 257, and which is a generic positive integer not being a prime power.   ","In this paper, we prove that the $q$-ary Gilbert-Varshamov bound can be improved for all but finitely many positive integers $q\\geq 2$.","It is shown that $ R_q(1/2) > R_\\mathrm{GV}(1/2,q) $ for all integers $q > \\exp(29)$.","Furthermore, we show that the growth of the rate function $R_q(\\delta)$ for $\\delta\\in(0,1)$ fixed and $q$ growing large has a nontrivial lower bound.","These new lower bounds are achieved by using codes from geometry of numbers introduced by Lenstra in the 1980s."],"url":"http://arxiv.org/abs/2403.08727v1","category":"math.CO"}
{"created":"2024-03-13 17:23:09","title":"Stabilizer Tensor Networks: universal quantum simulator on a basis of stabilizer states","abstract":"Efficient simulation of quantum computers relies on understanding and exploiting the properties of quantum states. This is the case for methods such as tensor networks, based on entanglement, and the tableau formalism, which represents stabilizer states. In this work, we integrate these two approaches to present a generalization of the tableau formalism used for Clifford circuit simulation. We explicitly prove how to update our formalism with Clifford gates, non-Clifford gates, and measurements, enabling universal circuit simulation. We also discuss how the framework allows for efficient simulation of more states, raising some interesting questions on the representation power of tensor networks and the quantum properties of resources such as entanglement and magic, and support our claims with simulations.","sentences":["Efficient simulation of quantum computers relies on understanding and exploiting the properties of quantum states.","This is the case for methods such as tensor networks, based on entanglement, and the tableau formalism, which represents stabilizer states.","In this work, we integrate these two approaches to present a generalization of the tableau formalism used for Clifford circuit simulation.","We explicitly prove how to update our formalism with Clifford gates, non-Clifford gates, and measurements, enabling universal circuit simulation.","We also discuss how the framework allows for efficient simulation of more states, raising some interesting questions on the representation power of tensor networks and the quantum properties of resources such as entanglement and magic, and support our claims with simulations."],"url":"http://arxiv.org/abs/2403.08724v1","category":"quant-ph"}
{"created":"2024-03-13 17:19:05","title":"Improved Trade-offs Between Amortization and Download Bandwidth for Linear HSS","abstract":"A Homomorphic Secret Sharing (HSS) scheme is a secret-sharing scheme that shares a secret $x$ among $s$ servers, and additionally allows an output client to reconstruct some function $f(x)$ using information that can be locally computed by each server. A key parameter in HSS schemes is download rate, which quantifies how much information the output client needs to download from the servers. Often, download rate is improved by amortizing over $\\ell$ instances of the problem, making $\\ell$ also a key parameter of interest.   Recent work (Fosli, Ishai, Kolobov, and Wootters 2022) established a limit on the download rate of linear HSS schemes for computing low-degree polynomials and constructed schemes that achieve this optimal download rate; their schemes required amortization over $\\ell = \\Omega(s \\log(s))$ instances of the problem. Subsequent work (Blackwell and Wootters, 2023) completely characterized linear HSS schemes that achieve optimal download rate in terms of a coding-theoretic notion termed optimal labelweight codes. A consequence of this characterization was that $\\ell = \\Omega(s \\log(s))$ is in fact necessary to achieve optimal download rate.   In this paper, we characterize all linear HSS schemes, showing that schemes of any download rate are equivalent to a generalization of optimal labelweight codes. This equivalence is constructive and provides a way to obtain an explicit linear HSS scheme from any linear code. Using this characterization, we present explicit linear HSS schemes with slightly sub-optimal rate but with much improved amortization $\\ell = O(s)$. Our constructions are based on algebraic geometry codes (specifically Hermitian codes and Goppa codes).","sentences":["A Homomorphic Secret Sharing (HSS) scheme is a secret-sharing scheme that shares a secret $x$ among $s$ servers, and additionally allows an output client to reconstruct some function $f(x)$ using information that can be locally computed by each server.","A key parameter in HSS schemes is download rate, which quantifies how much information the output client needs to download from the servers.","Often, download rate is improved by amortizing over $\\ell$ instances of the problem, making $\\ell$ also a key parameter of interest.   ","Recent work (Fosli, Ishai, Kolobov, and Wootters 2022) established a limit on the download rate of linear HSS schemes for computing low-degree polynomials and constructed schemes that achieve this optimal download rate; their schemes required amortization over $\\ell = \\Omega(s \\log(s))$ instances of the problem.","Subsequent work (Blackwell and Wootters, 2023) completely characterized linear HSS schemes that achieve optimal download rate in terms of a coding-theoretic notion termed optimal labelweight codes.","A consequence of this characterization was that $\\ell = \\Omega(s \\log(s))$ is in fact necessary to achieve optimal download rate.   ","In this paper, we characterize all linear HSS schemes, showing that schemes of any download rate are equivalent to a generalization of optimal labelweight codes.","This equivalence is constructive and provides a way to obtain an explicit linear HSS scheme from any linear code.","Using this characterization, we present explicit linear HSS schemes with slightly sub-optimal rate but with much improved amortization $\\ell = O(s)$.","Our constructions are based on algebraic geometry codes (specifically Hermitian codes and Goppa codes)."],"url":"http://arxiv.org/abs/2403.08719v1","category":"cs.IT"}
{"created":"2024-03-13 17:17:48","title":"SOTOPIA-$\u03c0$: Interactive Learning of Socially Intelligent Language Agents","abstract":"Humans learn social skills through both imitation and social interaction. This social learning process is largely understudied by existing research on building language agents. Motivated by this gap, we propose an interactive learning method, SOTOPIA-$\\pi$, improving the social intelligence of language agents. This method leverages behavior cloning and self-reinforcement training on filtered social interaction data according to large language model (LLM) ratings. We show that our training method allows a 7B LLM to reach the social goal completion ability of an expert model (GPT-4-based agent), while improving the safety of language agents and maintaining general QA ability on the MMLU benchmark. We also find that this training paradigm uncovers some difficulties in LLM-based evaluation of social intelligence: LLM-based evaluators overestimate the abilities of the language agents trained specifically for social interaction.","sentences":["Humans learn social skills through both imitation and social interaction.","This social learning process is largely understudied by existing research on building language agents.","Motivated by this gap, we propose an interactive learning method, SOTOPIA-$\\pi$, improving the social intelligence of language agents.","This method leverages behavior cloning and self-reinforcement training on filtered social interaction data according to large language model (LLM) ratings.","We show that our training method allows a 7B LLM to reach the social goal completion ability of an expert model (GPT-4-based agent), while improving the safety of language agents and maintaining general QA ability on the MMLU benchmark.","We also find that this training paradigm uncovers some difficulties in LLM-based evaluation of social intelligence: LLM-based evaluators overestimate the abilities of the language agents trained specifically for social interaction."],"url":"http://arxiv.org/abs/2403.08715v1","category":"cs.CL"}
{"created":"2024-03-13 17:16:21","title":"Controlled-Joint Remote Implementation of Operators and its Possible Generalization","abstract":"The existing notion of the shared entangled state-assisted remote preparation of unitary operator (equivalently the existing notion of quantum remote control) using local operation and classical communication is generalized to a scenario where under the control of a supervisor two users can jointly implement arbitrary unitaries (one unknown unitary operation by each or equivalently a single unitary decomposed into two unitaries of the same dimension and given to two users) on an unknown quantum state available with a geographically separated user. It is explicitly shown that the task can be performed using a four-qubit hyperentangled state, which is entangled simultaneously in both spatial and polarization degrees of freedom of photons. The proposed protocol which can be viewed as primitive for distributed photonic quantum computing is further generalized to the case that drops the restrictions on the number of controllers and the number of parties performing unitaries and allows both the numbers to be arbitrary. It is also shown that all the existing variants of quantum remote control schemes can be obtained as special cases of the present scheme.","sentences":["The existing notion of the shared entangled state-assisted remote preparation of unitary operator (equivalently the existing notion of quantum remote control) using local operation and classical communication is generalized to a scenario where under the control of a supervisor two users can jointly implement arbitrary unitaries (one unknown unitary operation by each or equivalently a single unitary decomposed into two unitaries of the same dimension and given to two users) on an unknown quantum state available with a geographically separated user.","It is explicitly shown that the task can be performed using a four-qubit hyperentangled state, which is entangled simultaneously in both spatial and polarization degrees of freedom of photons.","The proposed protocol which can be viewed as primitive for distributed photonic quantum computing is further generalized to the case that drops the restrictions on the number of controllers and the number of parties performing unitaries and allows both the numbers to be arbitrary.","It is also shown that all the existing variants of quantum remote control schemes can be obtained as special cases of the present scheme."],"url":"http://arxiv.org/abs/2403.08712v1","category":"quant-ph"}
{"created":"2024-03-13 17:12:36","title":"Improved Randomized Approximation of Hard Universality and Emptiness Problems","abstract":"We build on recent research on polynomial randomized approximation (PRAX) algorithms for the hard problems of NFA universality and NFA equivalence. Loosely speaking, PRAX algorithms use sampling of infinite domains within any desired accuracy $\\delta$. In the spirit of experimental mathematics, we extend the concept of PRAX algorithms to be applicable to the emptiness and universality problems in any domain whose instances admit a tractable distribution as defined in this paper. A technical result here is that a linear (w.r.t. $1/\\delta$) number of samples is sufficient, as opposed to the quadratic number of samples in previous papers. We show how the improved and generalized PRAX algorithms apply to universality and emptiness problems in various domains: ordinary automata, tautology testing of propositions, 2D automata, and to solution sets of certain Diophantine equations.","sentences":["We build on recent research on polynomial randomized approximation (PRAX) algorithms for the hard problems of NFA universality and NFA equivalence.","Loosely speaking, PRAX algorithms use sampling of infinite domains within any desired accuracy $\\delta$.","In the spirit of experimental mathematics, we extend the concept of PRAX algorithms to be applicable to the emptiness and universality problems in any domain whose instances admit a tractable distribution as defined in this paper.","A technical result here is that a linear (w.r.t. $1/\\delta$) number of samples is sufficient, as opposed to the quadratic number of samples in previous papers.","We show how the improved and generalized PRAX algorithms apply to universality and emptiness problems in various domains: ordinary automata, tautology testing of propositions, 2D automata, and to solution sets of certain Diophantine equations."],"url":"http://arxiv.org/abs/2403.08707v1","category":"cs.DS"}
{"created":"2024-03-13 17:12:33","title":"Optimal adaptation of surface-code decoders to local noise","abstract":"Information obtained from noise characterization of a quantum device can be used in classical decoding algorithms to improve the performance of quantum error-correcting codes. Focusing on the surface code under local (i.e. single-qubit) noise, we present a simple method to determine the maximum extent to which adapting a surface-code decoder to a noise feature can lead to a performance improvement. Our method is based on a tensor-network decoding algorithm, which uses the syndrome information as well as a process matrix description of the noise to compute a near-optimal correction. By selectively mischaracterizing the noise model input to the decoder and measuring the resulting loss in fidelity of the logical qubit, we can determine the relative importance of individual noise parameters for decoding. We apply this method to several physically relevant uncorrelated noise models with features such as coherence, spatial inhomogeneity and bias. While noise generally requires many parameters to describe completely, we find that to achieve near optimal decoding it appears only necessary adapt the decoder to a small number of critical parameters.","sentences":["Information obtained from noise characterization of a quantum device can be used in classical decoding algorithms to improve the performance of quantum error-correcting codes.","Focusing on the surface code under local (i.e. single-qubit) noise, we present a simple method to determine the maximum extent to which adapting a surface-code decoder to a noise feature can lead to a performance improvement.","Our method is based on a tensor-network decoding algorithm, which uses the syndrome information as well as a process matrix description of the noise to compute a near-optimal correction.","By selectively mischaracterizing the noise model input to the decoder and measuring the resulting loss in fidelity of the logical qubit, we can determine the relative importance of individual noise parameters for decoding.","We apply this method to several physically relevant uncorrelated noise models with features such as coherence, spatial inhomogeneity and bias.","While noise generally requires many parameters to describe completely, we find that to achieve near optimal decoding it appears only necessary adapt the decoder to a small number of critical parameters."],"url":"http://arxiv.org/abs/2403.08706v1","category":"quant-ph"}
{"created":"2024-03-13 17:09:32","title":"Scalarization of isolated black holes in scalar Gauss-Bonnet theory in the fixing-the-equations approach","abstract":"One of the most promising avenues to perform numerical evolutions in theories beyond General Relativity is the fixing-the-equations approach, a proposal in which new ``driver'' equations are added to the evolution equations in a way that allows for stable numerical evolutions. In this direction, we extend the numerical relativity code SpECTRE to evolve a ``fixed'' version of scalar Gauss-Bonnet theory in the decoupling limit, a phenomenologically interesting theory that allows for hairy black hole solutions in vacuum. We focus on isolated black hole systems both with and without linear and angular momentum, and propose a new driver equation to improve the recovery of such stationary solutions. We demonstrate the effectiveness of the latter by numerically evolving black holes that undergo spontaneous scalarization using different driver equations. Finally, we evaluate the accuracy of the obtained solutions by comparing with the original unaltered theory.","sentences":["One of the most promising avenues to perform numerical evolutions in theories beyond General Relativity is the fixing-the-equations approach, a proposal in which new ``driver'' equations are added to the evolution equations in a way that allows for stable numerical evolutions.","In this direction, we extend the numerical relativity code SpECTRE to evolve a ``fixed'' version of scalar Gauss-Bonnet theory in the decoupling limit, a phenomenologically interesting theory that allows for hairy black hole solutions in vacuum.","We focus on isolated black hole systems both with and without linear and angular momentum, and propose a new driver equation to improve the recovery of such stationary solutions.","We demonstrate the effectiveness of the latter by numerically evolving black holes that undergo spontaneous scalarization using different driver equations.","Finally, we evaluate the accuracy of the obtained solutions by comparing with the original unaltered theory."],"url":"http://arxiv.org/abs/2403.08705v1","category":"gr-qc"}
{"created":"2024-03-13 17:08:36","title":"Improved Dynamics for the Maximum Common Subgraph Problem","abstract":"The Maximum Common Subgraph (MCS) problem plays a crucial role across various domains, bridging theoretical exploration and practical applications in fields like bioinformatics and social network analysis. Despite its wide applicability, MCS is notoriously challenging and is classified as an NP-Complete (NPC) problem. This study introduces new heuristics aimed at mitigating these challenges through the reformulation of the MCS problem as the Maximum Clique and its complement, the Maximum Independent Set. Our first heuristic leverages the Motzkin-Straus theorem to reformulate the Maximum Clique Problem as a constrained optimization problem, continuing the work of Pelillo in Replicator Equations, Maximal Cliques, and Graph Isomorphism (1999) with replicator dynamics and introducing annealed imitation heuristics as in Dominant Sets and Hierarchical Clustering (Pavan and Pelillo, 2003) to improve chances of convergence to better local optima. The second technique applies heuristics drawn upon strategies for the Maximum Independent Set problem to efficiently reduce graph sizes as used by Akiwa and Iwata in 2014. This enables faster computation and, in many instances, yields near-optimal solutions. Furthermore we look at the implementation of both techniques in a single algorithm and find that it is a promising approach. Our techniques were tested on randomly generated Erd\\H{o}s-R\\'enyi graph pairs. Results indicate the potential for application and substantial impact on future research directions.","sentences":["The Maximum Common Subgraph (MCS) problem plays a crucial role across various domains, bridging theoretical exploration and practical applications in fields like bioinformatics and social network analysis.","Despite its wide applicability, MCS is notoriously challenging and is classified as an NP-Complete (NPC) problem.","This study introduces new heuristics aimed at mitigating these challenges through the reformulation of the MCS problem as the Maximum Clique and its complement, the Maximum Independent Set.","Our first heuristic leverages the Motzkin-Straus theorem to reformulate the Maximum Clique Problem as a constrained optimization problem, continuing the work of Pelillo in Replicator Equations, Maximal Cliques, and Graph Isomorphism (1999) with replicator dynamics and introducing annealed imitation heuristics as in Dominant Sets and Hierarchical Clustering (Pavan and Pelillo, 2003) to improve chances of convergence to better local optima.","The second technique applies heuristics drawn upon strategies for the Maximum Independent Set problem to efficiently reduce graph sizes as used by Akiwa and Iwata in 2014.","This enables faster computation and, in many instances, yields near-optimal solutions.","Furthermore we look at the implementation of both techniques in a single algorithm and find that it is a promising approach.","Our techniques were tested on randomly generated Erd\\H{o}s-R\\'enyi graph pairs.","Results indicate the potential for application and substantial impact on future research directions."],"url":"http://arxiv.org/abs/2403.08703v1","category":"cs.DM"}
{"created":"2024-03-13 17:05:05","title":"Review of Generative AI Methods in Cybersecurity","abstract":"Large language models (LLMs) and generative artificial intelligence (GenAI) constitute paradigm shifts in cybersecurity that present hitherto unseen challenges as well as opportunities. In examining the state-of-the-art application of GenAI in cybersecurity, this work highlights how models like Google's Gemini and ChatGPT-4 potentially enhance security protocols, vulnerability assessment, and threat identification. Our research highlights the significance of a novel approach that employs LLMs to identify and eliminate sophisticated cyber threats. This paper presents a thorough assessment of LLMs' ability to produce important security insights, hence broadening the potential applications of AI-driven cybersecurity solutions. Our findings demonstrate the significance of GenAI in improving digital security. It offers recommendations for further investigations into the intricate relationship between cybersecurity requirements and artificial intelligence's potential.","sentences":["Large language models (LLMs) and generative artificial intelligence (GenAI) constitute paradigm shifts in cybersecurity that present hitherto unseen challenges as well as opportunities.","In examining the state-of-the-art application of GenAI in cybersecurity, this work highlights how models like Google's Gemini and ChatGPT-4 potentially enhance security protocols, vulnerability assessment, and threat identification.","Our research highlights the significance of a novel approach that employs LLMs to identify and eliminate sophisticated cyber threats.","This paper presents a thorough assessment of LLMs' ability to produce important security insights, hence broadening the potential applications of AI-driven cybersecurity solutions.","Our findings demonstrate the significance of GenAI in improving digital security.","It offers recommendations for further investigations into the intricate relationship between cybersecurity requirements and artificial intelligence's potential."],"url":"http://arxiv.org/abs/2403.08701v1","category":"cs.CR"}
{"created":"2024-03-13 17:04:56","title":"Diffusion-based Iterative Counterfactual Explanations for Fetal Ultrasound Image Quality Assessment","abstract":"Obstetric ultrasound image quality is crucial for accurate diagnosis and monitoring of fetal health. However, producing high-quality standard planes is difficult, influenced by the sonographer's expertise and factors like the maternal BMI or the fetus dynamics. In this work, we propose using diffusion-based counterfactual explainable AI to generate realistic high-quality standard planes from low-quality non-standard ones. Through quantitative and qualitative evaluation, we demonstrate the effectiveness of our method in producing plausible counterfactuals of increased quality. This shows future promise both for enhancing training of clinicians by providing visual feedback, as well as for improving image quality and, consequently, downstream diagnosis and monitoring.","sentences":["Obstetric ultrasound image quality is crucial for accurate diagnosis and monitoring of fetal health.","However, producing high-quality standard planes is difficult, influenced by the sonographer's expertise and factors like the maternal BMI or the fetus dynamics.","In this work, we propose using diffusion-based counterfactual explainable AI to generate realistic high-quality standard planes from low-quality non-standard ones.","Through quantitative and qualitative evaluation, we demonstrate the effectiveness of our method in producing plausible counterfactuals of increased quality.","This shows future promise both for enhancing training of clinicians by providing visual feedback, as well as for improving image quality and, consequently, downstream diagnosis and monitoring."],"url":"http://arxiv.org/abs/2403.08700v1","category":"eess.IV"}
{"created":"2024-03-13 17:02:27","title":"Implicit Regularization of Gradient Flow on One-Layer Softmax Attention","abstract":"We study gradient flow on the exponential loss for a classification problem with a one-layer softmax attention model, where the key and query weight matrices are trained separately. Under a separability assumption on the data, we show that when gradient flow achieves the minimal loss value, it further implicitly minimizes the nuclear norm of the product of the key and query weight matrices. Such implicit regularization can be described by a Support Vector Machine (SVM) problem with respect to the attention weights. This finding contrasts with prior results showing that the gradient descent induces an implicit regularization on the Frobenius norm on the product weight matrix when the key and query matrices are combined into a single weight matrix for training. For diagonal key and query matrices, our analysis builds upon the reparameterization technique and exploits approximate KKT conditions of the SVM associated with the classification data. Moreover, the results are extended to general weights configurations given proper alignment of the weight matrices' singular spaces with the data features at initialization.","sentences":["We study gradient flow on the exponential loss for a classification problem with a one-layer softmax attention model, where the key and query weight matrices are trained separately.","Under a separability assumption on the data, we show that when gradient flow achieves the minimal loss value, it further implicitly minimizes the nuclear norm of the product of the key and query weight matrices.","Such implicit regularization can be described by a Support Vector Machine (SVM) problem with respect to the attention weights.","This finding contrasts with prior results showing that the gradient descent induces an implicit regularization on the Frobenius norm on the product weight matrix when the key and query matrices are combined into a single weight matrix for training.","For diagonal key and query matrices, our analysis builds upon the reparameterization technique and exploits approximate KKT conditions of the SVM associated with the classification data.","Moreover, the results are extended to general weights configurations given proper alignment of the weight matrices' singular spaces with the data features at initialization."],"url":"http://arxiv.org/abs/2403.08699v1","category":"cs.LG"}
{"created":"2024-03-13 16:59:01","title":"On the non-perturbative bulk Hilbert space of JT gravity","abstract":"What is the bulk Hilbert space of quantum gravity? In this paper, we resolve this problem in 2d JT gravity, both with and without matter, providing the first example of an explicit definition of a non-perturbative Hilbert space specified in terms of metric variables. The states are wavefunctions of the length and matter state, but with a non-trivial and highly degenerate inner product. We explicitly identify the null states, and discuss their importance for defining operators non-perturbatively. To highlight the power of the formalism we developed, we study the non-perturbative effects for two bulk linear operators that may serve as proxies for the experience of an observer falling into a two-sided black hole: one captures the length of an Einstein-Rosen bridge and the other captures the center-of-mass collision energy between two particles falling from opposite sides. We track the behavior of these operators up to times of order $e^{S_\\text{BH}}$, at which point the wavefunction spreads to the complete set of eigenstates of these operators. If these observables are indeed good proxies for the experience of an infalling observer, our results indicate an O(1) probability of detecting a firewall at late times that is self-averaging and universal.","sentences":["What is the bulk Hilbert space of quantum gravity?","In this paper, we resolve this problem in 2d JT gravity, both with and without matter, providing the first example of an explicit definition of a non-perturbative Hilbert space specified in terms of metric variables.","The states are wavefunctions of the length and matter state, but with a non-trivial and highly degenerate inner product.","We explicitly identify the null states, and discuss their importance for defining operators non-perturbatively.","To highlight the power of the formalism we developed, we study the non-perturbative effects for two bulk linear operators that may serve as proxies for the experience of an observer falling into a two-sided black hole: one captures the length of an Einstein-Rosen bridge and the other captures the center-of-mass collision energy between two particles falling from opposite sides.","We track the behavior of these operators up to times of order $e^{S_\\text{BH}}$, at which point the wavefunction spreads to the complete set of eigenstates of these operators.","If these observables are indeed good proxies for the experience of an infalling observer, our results indicate an O(1) probability of detecting a firewall at late times that is self-averaging and universal."],"url":"http://arxiv.org/abs/2403.08696v1","category":"hep-th"}
{"created":"2024-03-13 16:57:57","title":"TeaMs-RL: Teaching LLMs to Teach Themselves Better Instructions via Reinforcement Learning","abstract":"The development of Large Language Models (LLMs) often confronts challenges stemming from the heavy reliance on human annotators in the reinforcement learning with human feedback (RLHF) framework, or the frequent and costly external queries tied to the self-instruct paradigm. In this work, we pivot to Reinforcement Learning (RL) -- but with a twist. Diverging from the typical RLHF, which refines LLMs following instruction data training, we use RL to directly generate the foundational instruction dataset that alone suffices for fine-tuning. Our method, TeaMs-RL, uses a suite of textual operations and rules, prioritizing the diversification of training datasets. It facilitates the generation of high-quality data without excessive reliance on external advanced models, paving the way for a single fine-tuning step and negating the need for subsequent RLHF stages. Our findings highlight key advantages of our approach: reduced need for human involvement and fewer model queries (only $5.73\\%$ of WizardLM's total), along with enhanced capabilities of LLMs in crafting and comprehending complex instructions compared to strong baselines, and substantially improved model privacy protection.","sentences":["The development of Large Language Models (LLMs) often confronts challenges stemming from the heavy reliance on human annotators in the reinforcement learning with human feedback (RLHF) framework, or the frequent and costly external queries tied to the self-instruct paradigm.","In this work, we pivot to Reinforcement Learning (RL) -- but with a twist.","Diverging from the typical RLHF, which refines LLMs following instruction data training, we use RL to directly generate the foundational instruction dataset that alone suffices for fine-tuning.","Our method, TeaMs-RL, uses a suite of textual operations and rules, prioritizing the diversification of training datasets.","It facilitates the generation of high-quality data without excessive reliance on external advanced models, paving the way for a single fine-tuning step and negating the need for subsequent RLHF stages.","Our findings highlight key advantages of our approach: reduced need for human involvement and fewer model queries (only $5.73\\%$ of WizardLM's total), along with enhanced capabilities of LLMs in crafting and comprehending complex instructions compared to strong baselines, and substantially improved model privacy protection."],"url":"http://arxiv.org/abs/2403.08694v1","category":"cs.CL"}
{"created":"2024-03-13 16:56:05","title":"An Algorithm to Parallelise Parton Showers on a GPU","abstract":"The Single Instruction, Multiple Thread (SIMT) paradigm of GPU programming does not support the branching nature of a parton shower algorithm by definition. However, modern GPUs are designed to schedule threads with diverging processes independently, allowing them to handle such branches. With regular thread synchronization and careful treatment of the individual steps, one can simulate a parton shower on a GPU. We present a parallelized Sudakov veto algorithm designed to simulate parton branching on multiple events simultaneously. We also release a CUDA C++ program that generates matrix elements, showers partons, and computes jet rates and event shapes for LEP at 91.2 GeV on a GPU. To benchmark its performance, we also provide a near-identical C++ program designed to simulate events serially on a CPU. While the consequences of branching are not absent, we demonstrate that a GPU can provide the throughput of a many-core CPU. As an example, we show that the time taken to simulate 10^6 events on one NVIDIA TESLA V100 GPU is equivalent to that of 266 Intel Xeon E5-2620 CPUs.","sentences":["The Single Instruction, Multiple Thread (SIMT) paradigm of GPU programming does not support the branching nature of a parton shower algorithm by definition.","However, modern GPUs are designed to schedule threads with diverging processes independently, allowing them to handle such branches.","With regular thread synchronization and careful treatment of the individual steps, one can simulate a parton shower on a GPU.","We present a parallelized Sudakov veto algorithm designed to simulate parton branching on multiple events simultaneously.","We also release a CUDA C++ program that generates matrix elements, showers partons, and computes jet rates and event shapes for LEP at 91.2 GeV on a GPU.","To benchmark its performance, we also provide a near-identical C++ program designed to simulate events serially on a CPU.","While the consequences of branching are not absent, we demonstrate that a GPU can provide the throughput of a many-core CPU.","As an example, we show that the time taken to simulate 10^6 events on one NVIDIA TESLA V100 GPU is equivalent to that of 266","Intel Xeon E5-2620 CPUs."],"url":"http://arxiv.org/abs/2403.08692v1","category":"hep-ph"}
{"created":"2024-03-13 16:44:49","title":"Exploiting Structural Consistency of Chest Anatomy for Unsupervised Anomaly Detection in Radiography Images","abstract":"Radiography imaging protocols focus on particular body regions, therefore producing images of great similarity and yielding recurrent anatomical structures across patients. Exploiting this structured information could potentially ease the detection of anomalies from radiography images. To this end, we propose a Simple Space-Aware Memory Matrix for In-painting and Detecting anomalies from radiography images (abbreviated as SimSID). We formulate anomaly detection as an image reconstruction task, consisting of a space-aware memory matrix and an in-painting block in the feature space. During the training, SimSID can taxonomize the ingrained anatomical structures into recurrent visual patterns, and in the inference, it can identify anomalies (unseen/modified visual patterns) from the test image. Our SimSID surpasses the state of the arts in unsupervised anomaly detection by +8.0%, +5.0%, and +9.9% AUC scores on ZhangLab, COVIDx, and CheXpert benchmark datasets, respectively. Code: https://github.com/MrGiovanni/SimSID","sentences":["Radiography imaging protocols focus on particular body regions, therefore producing images of great similarity and yielding recurrent anatomical structures across patients.","Exploiting this structured information could potentially ease the detection of anomalies from radiography images.","To this end, we propose a Simple Space-Aware Memory Matrix for In-painting and Detecting anomalies from radiography images (abbreviated as SimSID).","We formulate anomaly detection as an image reconstruction task, consisting of a space-aware memory matrix and an in-painting block in the feature space.","During the training, SimSID can taxonomize the ingrained anatomical structures into recurrent visual patterns, and in the inference, it can identify anomalies (unseen/modified visual patterns) from the test image.","Our SimSID surpasses the state of the arts in unsupervised anomaly detection by +8.0%, +5.0%, and +9.9% AUC scores on ZhangLab, COVIDx, and CheXpert benchmark datasets, respectively.","Code: https://github.com/MrGiovanni/SimSID"],"url":"http://arxiv.org/abs/2403.08689v1","category":"eess.IV"}
{"created":"2024-03-13 16:44:39","title":"Token Alignment via Character Matching for Subword Completion","abstract":"Generative models, widely utilized in various applications, can often struggle with prompts corresponding to partial tokens. This struggle stems from tokenization, where partial tokens fall out of distribution during inference, leading to incorrect or nonsensical outputs. This paper examines a technique to alleviate the tokenization artifact on text completion in generative models, maintaining performance even in regular non-subword cases. The method, termed token alignment, involves backtracking to the last complete tokens and ensuring the model's generation aligns with the prompt. This approach showcases marked improvement across many partial token scenarios, including nuanced cases like space-prefix and partial indentation, with only a minor time increase. The technique and analysis detailed in this paper contribute to the continuous advancement of generative models in handling partial inputs, bearing relevance for applications like code completion and text autocompletion.","sentences":["Generative models, widely utilized in various applications, can often struggle with prompts corresponding to partial tokens.","This struggle stems from tokenization, where partial tokens fall out of distribution during inference, leading to incorrect or nonsensical outputs.","This paper examines a technique to alleviate the tokenization artifact on text completion in generative models, maintaining performance even in regular non-subword cases.","The method, termed token alignment, involves backtracking to the last complete tokens and ensuring the model's generation aligns with the prompt.","This approach showcases marked improvement across many partial token scenarios, including nuanced cases like space-prefix and partial indentation, with only a minor time increase.","The technique and analysis detailed in this paper contribute to the continuous advancement of generative models in handling partial inputs, bearing relevance for applications like code completion and text autocompletion."],"url":"http://arxiv.org/abs/2403.08688v1","category":"cs.CL"}
{"created":"2024-03-13 16:44:36","title":"Digital Twin-assisted Reinforcement Learning for Resource-aware Microservice Offloading in Edge Computing","abstract":"Collaborative edge computing (CEC) has emerged as a promising paradigm, enabling edge nodes to collaborate and execute microservices from end devices. Microservice offloading, a fundamentally important problem, decides when and where microservices are executed upon the arrival of services. However, the dynamic nature of the real-world CEC environment often leads to inefficient microservice offloading strategies, resulting in underutilized resources and network congestion. To address this challenge, we formulate an online joint microservice offloading and bandwidth allocation problem, JMOBA, to minimize the average completion time of services. In this paper, we introduce a novel microservice offloading algorithm, DTDRLMO, which leverages deep reinforcement learning (DRL) and digital twin technology. Specifically, we employ digital twin techniques to predict and adapt to changing edge node loads and network conditions of CEC in real-time. Furthermore, this approach enables the generation of an efficient offloading plan, selecting the most suitable edge node for each microservice. Simulation results on real-world and synthetic datasets demonstrate that DTDRLMO outperforms heuristic and learning-based methods in average service completion time.","sentences":["Collaborative edge computing (CEC) has emerged as a promising paradigm, enabling edge nodes to collaborate and execute microservices from end devices.","Microservice offloading, a fundamentally important problem, decides when and where microservices are executed upon the arrival of services.","However, the dynamic nature of the real-world CEC environment often leads to inefficient microservice offloading strategies, resulting in underutilized resources and network congestion.","To address this challenge, we formulate an online joint microservice offloading and bandwidth allocation problem, JMOBA, to minimize the average completion time of services.","In this paper, we introduce a novel microservice offloading algorithm, DTDRLMO, which leverages deep reinforcement learning (DRL) and digital twin technology.","Specifically, we employ digital twin techniques to predict and adapt to changing edge node loads and network conditions of CEC in real-time.","Furthermore, this approach enables the generation of an efficient offloading plan, selecting the most suitable edge node for each microservice.","Simulation results on real-world and synthetic datasets demonstrate that DTDRLMO outperforms heuristic and learning-based methods in average service completion time."],"url":"http://arxiv.org/abs/2403.08687v1","category":"cs.NI"}
{"created":"2024-03-13 16:36:15","title":"Towards a Consistent Calculation of the Lunar Response to Gravitational Waves","abstract":"The recent increasing interest in detecting gravitational waves (GWs) by lunar seismic measurement urges us to have a clear understanding of the response of the moon to passing GWs. In this paper, we clarify the relationship between two seemly different response functions which have been derived previously using two different methods, one taking the field-theory approach and the other using the tidal force induced by GWs. We revisit their derivation and prove, by both analytical arguments and numerical calculations, that the two response functions are equivalent. Their apparent difference can be attributed to the choice of different coordinates. Using the correct response function, we calculate the sensitivities (to GWs) of several designed lunar seismometers, and find that the sensitivity curves between $10^{-3}$ and $0.1$ Hz are much flatter than the previous calculations based on normal-mode model. Our results will help clarifying the scientific objectives of lunar GW observation, as well as provide important constraints on the design of lunar GW detectors.","sentences":["The recent increasing interest in detecting gravitational waves (GWs) by lunar seismic measurement urges us to have a clear understanding of the response of the moon to passing GWs.","In this paper, we clarify the relationship between two seemly different response functions which have been derived previously using two different methods, one taking the field-theory approach and the other using the tidal force induced by GWs.","We revisit their derivation and prove, by both analytical arguments and numerical calculations, that the two response functions are equivalent.","Their apparent difference can be attributed to the choice of different coordinates.","Using the correct response function, we calculate the sensitivities (to GWs) of several designed lunar seismometers, and find that the sensitivity curves between $10^{-3}$ and $0.1$ Hz are much flatter than the previous calculations based on normal-mode model.","Our results will help clarifying the scientific objectives of lunar GW observation, as well as provide important constraints on the design of lunar GW detectors."],"url":"http://arxiv.org/abs/2403.08681v1","category":"gr-qc"}
{"created":"2024-03-13 16:32:54","title":"Path-dependency of capital return in periodic growth processes","abstract":"Periodic growth processes are investigated. The expected value of the profit rate, on accrual basis, does not directly depend on divestments, neither on the capitalization path. The expected value of capitalization is path dependent. Because of the path-dependent capitalization, the return rate on capital is path-dependent, and the time-average return rate on capital differs from the expected-value return rate on capital for the growth cycle. In the absence of intermediate divestments, the internal rate of return is path-independent, thereby differing from the expected value of the rate of return on capital. It is shown that the rotation cycle length maximizing the return rate on equity is independent of market interest rate. Leveraging effect enters the microeconomics of the growth processes through a separate leveraging equation, where the leverage coefficient may reach positive or negative values. Correspondingly, from the viewpoint of wealth accumulation, the often-suggested dependency of suitable rotation length on discount rate appears to be a modeling artifact. In other words, the net present value computation is based on maximization of consumption utility, instead of a capital growth objective; borrowing is obligatory, but the leverage effect is absent.","sentences":["Periodic growth processes are investigated.","The expected value of the profit rate, on accrual basis, does not directly depend on divestments, neither on the capitalization path.","The expected value of capitalization is path dependent.","Because of the path-dependent capitalization, the return rate on capital is path-dependent, and the time-average return rate on capital differs from the expected-value return rate on capital for the growth cycle.","In the absence of intermediate divestments, the internal rate of return is path-independent, thereby differing from the expected value of the rate of return on capital.","It is shown that the rotation cycle length maximizing the return rate on equity is independent of market interest rate.","Leveraging effect enters the microeconomics of the growth processes through a separate leveraging equation, where the leverage coefficient may reach positive or negative values.","Correspondingly, from the viewpoint of wealth accumulation, the often-suggested dependency of suitable rotation length on discount rate appears to be a modeling artifact.","In other words, the net present value computation is based on maximization of consumption utility, instead of a capital growth objective; borrowing is obligatory, but the leverage effect is absent."],"url":"http://arxiv.org/abs/2403.08678v1","category":"econ.GN"}
{"created":"2024-03-13 16:30:46","title":"Axi-majoron for almost everything","abstract":"The details of the minimal cosmological standard model (MCSM) proposed in Ref. [1] are discussed. The model is based on the scale-symmetry and the global Peccei-Quinn(PQ) symmetry with a key assumption that the latter is broken only in the gravity sector in a scale-invariant manner. We show that the model provides a quite simple unified framework for the unknown history of the universe from inflation to the epoch of big-bang nucleosynthesis, simultaneously addressing key puzzles of high energy theory and cosmology: (i) the origin of scales, (ii) primordial inflation, (iii) matter-antimatter asymmetry, (iv) tiny neutrino masses, (v) dark matter, and (vi) the strong CP-problem. Scale symmetry can be exact, and the Planck scale is dynamically generated. The presence of Gauss-Bonnet term may safely retain dangerous non-perturbative symmetry-breaking effects negligible, allowing large-field trans-Planckian inflation along the PQ-field. Iso-curvature perturbations of axi-majorons are suppressed. A sizable amount of PQ-number asymmetry is generated at the end of inflation, and conserved afterwards. Domain wall problem is absent due to the PQ-number asymmetry. Baryogenesis can be realized by either inflationary Affleck-Dine mechanism or spontaneous leptogenesis thanks to the PQ-number asymmetry, or by resonant leptogenesis. Dark matter can be purely cold axi-majorons from the mis-alignment contribution only with the symmetry-breaking scale of $\\mathcal{O}(10^{12}) {\\rm GeV}$. Hot axi-majorons from the decay of the inflaton become a natural source for a sizable amount of dark radiation. Inflationary gravitational waves are expected to have information about some masse parameters of the left-handed and the right-handed neutrinos, thanks to the presence of an early matter-domination era driven by at least a long-lived right-handed neutrino species.","sentences":["The details of the minimal cosmological standard model (MCSM) proposed in Ref.","[1] are discussed.","The model is based on the scale-symmetry and the global Peccei-Quinn(PQ)","symmetry with a key assumption that the latter is broken only in the gravity sector in a scale-invariant manner.","We show that the model provides a quite simple unified framework for the unknown history of the universe from inflation to the epoch of big-bang nucleosynthesis, simultaneously addressing key puzzles of high energy theory and cosmology: (i) the origin of scales, (ii) primordial inflation, (iii) matter-antimatter asymmetry, (iv) tiny neutrino masses, (v) dark matter, and (vi) the strong CP-problem.","Scale symmetry can be exact, and the Planck scale is dynamically generated.","The presence of Gauss-Bonnet term may safely retain dangerous non-perturbative symmetry-breaking effects negligible, allowing large-field trans-Planckian inflation along the PQ-field.","Iso-curvature perturbations of axi-majorons are suppressed.","A sizable amount of PQ-number asymmetry is generated at the end of inflation, and conserved afterwards.","Domain wall problem is absent due to the PQ-number asymmetry.","Baryogenesis can be realized by either inflationary Affleck-Dine mechanism or spontaneous leptogenesis thanks to the PQ-number asymmetry, or by resonant leptogenesis.","Dark matter can be purely cold axi-majorons from the mis-alignment contribution only with the symmetry-breaking scale of $\\mathcal{O}(10^{12}) {\\rm GeV}$. Hot axi-majorons from the decay of the inflaton become a natural source for a sizable amount of dark radiation.","Inflationary gravitational waves are expected to have information about some masse parameters of the left-handed and the right-handed neutrinos, thanks to the presence of an early matter-domination era driven by at least a long-lived right-handed neutrino species."],"url":"http://arxiv.org/abs/2403.08675v1","category":"hep-ph"}
{"created":"2024-03-13 16:20:37","title":"Ancilla-free measurement of out-of-time-ordered correlation functions: General measurement protocol and Rydberg atom implementation","abstract":"We introduce a protocol that gives access to out-of-time-ordered correlation functions in many-body quantum systems. Unlike other such protocols, our proposal, which can be applied to arbitrary initial states, neither requires ancilla degrees of freedom to the quantum system of interest, nor has the need for randomized measurements. Nontrivial experimental capabilities required to implement the protocol are single-site measurements, single-site rotations, and backwards time evolution. To exemplify the implementation of the protocol, we put forward a strategy for Hamiltonian sign inversion $H\\to-H$ in arrays of Rydberg-dressed atoms. In this way, a complete and practical toolbox is obtained for the measurement of out-of-time-ordered correlations in equilibrium and nonequilibrium situations.","sentences":["We introduce a protocol that gives access to out-of-time-ordered correlation functions in many-body quantum systems.","Unlike other such protocols, our proposal, which can be applied to arbitrary initial states, neither requires ancilla degrees of freedom to the quantum system of interest, nor has the need for randomized measurements.","Nontrivial experimental capabilities required to implement the protocol are single-site measurements, single-site rotations, and backwards time evolution.","To exemplify the implementation of the protocol, we put forward a strategy for Hamiltonian sign inversion $H\\to-H$ in arrays of Rydberg-dressed atoms.","In this way, a complete and practical toolbox is obtained for the measurement of out-of-time-ordered correlations in equilibrium and nonequilibrium situations."],"url":"http://arxiv.org/abs/2403.08670v1","category":"quant-ph"}
{"created":"2024-03-13 16:20:30","title":"BHAC-QGP: three-dimensional MHD simulations of relativistic heavy-ion collisions, II. Application to Au-Au collisions","abstract":"We present BHAC-QGP, a new numerical code to simulate the evolution of matter created in heavy-ion collisions. BHAC-QGP is based on the Black Hole Accretion Code (BHAC), which has been designed to model astrophysical processes through the solution of the equations of general-relativistic magnetohydrodynamics. Like the mother code, BHAC-QGP uses Adaptive Mesh Refinement (AMR), which allows for a dynamic adjustment of the resolution in regions of the computational domain where a particularly high accuracy is needed. We here discuss a number of applications of BHAC-QGP to Au-Au collisions at Relativistic Heavy-Ion Collider (RHIC) energies and show that the code is able to reproduce results of other simulations of these scenarios, but with much higher accuracy.","sentences":["We present BHAC-QGP, a new numerical code to simulate the evolution of matter created in heavy-ion collisions.","BHAC-QGP is based on the Black Hole Accretion Code (BHAC), which has been designed to model astrophysical processes through the solution of the equations of general-relativistic magnetohydrodynamics.","Like the mother code, BHAC-QGP uses Adaptive Mesh Refinement (AMR), which allows for a dynamic adjustment of the resolution in regions of the computational domain where a particularly high accuracy is needed.","We here discuss a number of applications of BHAC-QGP to Au-Au collisions at Relativistic Heavy-Ion Collider (RHIC) energies and show that the code is able to reproduce results of other simulations of these scenarios, but with much higher accuracy."],"url":"http://arxiv.org/abs/2403.08669v1","category":"hep-ph"}
{"created":"2024-03-13 16:20:27","title":"BHAC-QGP: three-dimensional MHD simulations of relativistic heavy-ion collisions, I. Methods and tests","abstract":"We present BHAC-QGP, a new numerical code to simulate the evolution of matter created in heavy-ion collisions in the presence of electromagnetic fields. It is derived from the Black Hole Accretion Code (BHAC), which has been designed to model astrophysical processes in a general-relativistic magnetohydrodynamical description. As the original Black Hole Accretion Code, BHAC-QGP benefits from the use of Adaptive Mesh Refinement (AMR), which allows us to dynamically adjust the resolution where necessary, and makes use of time-dependent Milne coordinates and the ultrarelativistic equation of state, $P = e/3$. We demonstrate that BHAC-QGP accurately passes a number of systematic and rigorous tests.","sentences":["We present BHAC-QGP, a new numerical code to simulate the evolution of matter created in heavy-ion collisions in the presence of electromagnetic fields.","It is derived from the Black Hole Accretion Code (BHAC), which has been designed to model astrophysical processes in a general-relativistic magnetohydrodynamical description.","As the original Black Hole Accretion Code, BHAC-QGP benefits from the use of Adaptive Mesh Refinement (AMR), which allows us to dynamically adjust the resolution where necessary, and makes use of time-dependent Milne coordinates and the ultrarelativistic equation of state, $P = e/3$. We demonstrate that BHAC-QGP accurately passes a number of systematic and rigorous tests."],"url":"http://arxiv.org/abs/2403.08668v1","category":"hep-ph"}
{"created":"2024-03-13 16:19:05","title":"Surfaces and other Peano Continua with no Generic Chains","abstract":"The space of chains on a compact connected space encodes all the different ways of continuously growing out of a point until exhausting the space. A chain is generic if its orbit under the action of the underlying homeomorphism group is comeager: a space X has a generic chain if there is essentially just one type of chain on X. Gutman, Tsankov and Zucker proved that compact manifolds of dimension at least 3 do not have a generic chain. We extend and generalize their result, covering a large class of spaces which includes all compact surfaces except for the sphere and the real projective plane - for which the question remains open - as well as all other homogeneous Peano continua, circle excluded. If the spaces are moreover strongly locally homogeneous, which is the case for any closed manifold as well as the Menger curve, we prove that chains cannot be classified up to homeomorphism by countable structures, and that the underlying homomorphism groups have non-metrizable universal minimal flows. This is in contrast with the case of 1-dimensional manifolds: their homomorphism groups have metrizable universal minimal flow and we show that their chains are classifiable by countable structures and have a generic element. The proof of the main result relies on crafting a dictionary between open sets of chains on one side, and walks on finite connected graphs on the other. We then establish a novel combinatorial necessary condition for the existence of a generic chain, an off-by-one weak amalgamation principle, and prove that it is not satisfied under the hypotheses of our theorem.","sentences":["The space of chains on a compact connected space encodes all the different ways of continuously growing out of a point until exhausting the space.","A chain is generic if its orbit under the action of the underlying homeomorphism group is comeager: a space X has a generic chain if there is essentially just one type of chain on X. Gutman, Tsankov and Zucker proved that compact manifolds of dimension at least 3 do not have a generic chain.","We extend and generalize their result, covering a large class of spaces which includes all compact surfaces except for the sphere and the real projective plane - for which the question remains open - as well as all other homogeneous Peano continua, circle excluded.","If the spaces are moreover strongly locally homogeneous, which is the case for any closed manifold as well as the Menger curve, we prove that chains cannot be classified up to homeomorphism by countable structures, and that the underlying homomorphism groups have non-metrizable universal minimal flows.","This is in contrast with the case of 1-dimensional manifolds: their homomorphism groups have metrizable universal minimal flow and we show that their chains are classifiable by countable structures and have a generic element.","The proof of the main result relies on crafting a dictionary between open sets of chains on one side, and walks on finite connected graphs on the other.","We then establish a novel combinatorial necessary condition for the existence of a generic chain, an off-by-one weak amalgamation principle, and prove that it is not satisfied under the hypotheses of our theorem."],"url":"http://arxiv.org/abs/2403.08667v1","category":"math.DS"}
{"created":"2024-03-13 16:18:48","title":"The neutron decay anomaly, neutron stars and dark matter","abstract":"The discrepancies in different measurements of the lifetime of isolated neutrons could be resolved by considering an extra neutron decay channel into dark matter, with a branching ratio of the order of $O(1$\\%). Although the decay channel into a dark fermion $\\chi$ plus visible matter has been already experimentally excluded, a dark decay with either a scalar or dark photon remains still a possibility. In particular, a model with a fermion mass $m_\\chi\\approx 1$ GeV and a scalar $m_\\phi \\approx O(\\rm{MeV})$ could provide not only the required branching ratio to explain the anomaly but also a good dark matter (DM) candidate with the right thermal abundance today. Although the interaction DM-neutron will affect the formation of neutron stars, the combined effect of the dark matter self-interactions mediated by the light scalar and an effective repulsive interaction with the neutrons induced by the scalar-Higgs coupling would allow heavy enough neutron stars. The combined constraints from neutron lifetime, dark matter abundance, neutron star and Higgs physics, and Big Bang Nucleosynthesis, restrict the light scalar mass to the range $2 m_e < m_\\phi < 2 m_e + 0.0375$ MeV.","sentences":["The discrepancies in different measurements of the lifetime of isolated neutrons could be resolved by considering an extra neutron decay channel into dark matter, with a branching ratio of the order of $O(1$\\%).","Although the decay channel into a dark fermion $\\chi$ plus visible matter has been already experimentally excluded, a dark decay with either a scalar or dark photon remains still a possibility.","In particular, a model with a fermion mass $m_\\chi\\approx 1$ GeV and a scalar $m_\\phi \\approx O(\\rm{MeV})$ could provide not only the required branching ratio to explain the anomaly but also a good dark matter (DM) candidate with the right thermal abundance today.","Although the interaction DM-neutron will affect the formation of neutron stars, the combined effect of the dark matter self-interactions mediated by the light scalar and an effective repulsive interaction with the neutrons induced by the scalar-Higgs coupling would allow heavy enough neutron stars.","The combined constraints from neutron lifetime, dark matter abundance, neutron star and Higgs physics, and Big Bang Nucleosynthesis, restrict the light scalar mass to the range $2 m_e < m_\\phi <","2","m_e","+ 0.0375$","MeV."],"url":"http://arxiv.org/abs/2403.08666v1","category":"astro-ph.CO"}
{"created":"2024-03-13 16:17:09","title":"Zero-shot and Few-shot Generation Strategies for Artificial Clinical Records","abstract":"The challenge of accessing historical patient data for clinical research, while adhering to privacy regulations, is a significant obstacle in medical science. An innovative approach to circumvent this issue involves utilising synthetic medical records that mirror real patient data without compromising individual privacy. The creation of these synthetic datasets, particularly without using actual patient data to train Large Language Models (LLMs), presents a novel solution as gaining access to sensitive patient information to train models is also a challenge. This study assesses the capability of the Llama 2 LLM to create synthetic medical records that accurately reflect real patient information, employing zero-shot and few-shot prompting strategies for comparison against fine-tuned methodologies that do require sensitive patient data during training. We focus on generating synthetic narratives for the History of Present Illness section, utilising data from the MIMIC-IV dataset for comparison. In this work introduce a novel prompting technique that leverages a chain-of-thought approach, enhancing the model's ability to generate more accurate and contextually relevant medical narratives without prior fine-tuning. Our findings suggest that this chain-of-thought prompted approach allows the zero-shot model to achieve results on par with those of fine-tuned models, based on Rouge metrics evaluation.","sentences":["The challenge of accessing historical patient data for clinical research, while adhering to privacy regulations, is a significant obstacle in medical science.","An innovative approach to circumvent this issue involves utilising synthetic medical records that mirror real patient data without compromising individual privacy.","The creation of these synthetic datasets, particularly without using actual patient data to train Large Language Models (LLMs), presents a novel solution as gaining access to sensitive patient information to train models is also a challenge.","This study assesses the capability of the Llama 2 LLM to create synthetic medical records that accurately reflect real patient information, employing zero-shot and few-shot prompting strategies for comparison against fine-tuned methodologies that do require sensitive patient data during training.","We focus on generating synthetic narratives for the History of Present Illness section, utilising data from the MIMIC-IV dataset for comparison.","In this work introduce a novel prompting technique that leverages a chain-of-thought approach, enhancing the model's ability to generate more accurate and contextually relevant medical narratives without prior fine-tuning.","Our findings suggest that this chain-of-thought prompted approach allows the zero-shot model to achieve results on par with those of fine-tuned models, based on Rouge metrics evaluation."],"url":"http://arxiv.org/abs/2403.08664v1","category":"cs.CL"}
{"created":"2024-03-13 16:16:43","title":"QCSHQD: Quantum computing as a service for Hybrid classical-quantum software development: A Vision","abstract":"Quantum Computing (QC) is transitioning from theoretical frameworks to an indispensable powerhouse of computational capability, resulting in extensive adoption across both industrial and academic domains. QC presents exceptional advantages, including unparalleled processing speed and the potential to solve complex problems beyond the capabilities of classical computers. Nevertheless, academic researchers and industry practitioners encounter various challenges in harnessing the benefits of this technology. The limited accessibility of QC resources for classical developers, and a general lack of domain knowledge and expertise, represent insurmountable barrier, hence to address these challenges, we introduce a framework- Quantum Computing as a Service for Hybrid Classical-Quantum Software Development (QCSHQD), which leverages service-oriented strategies. Our framework comprises three principal components: an Integrated Development Environment (IDE) for user interaction, an abstraction layer dedicated to orchestrating quantum services, and a service provider responsible for executing services on quantum computer. This study presents a blueprint for QCSHQD, designed to democratize access to QC resources for classical developers who want to seamless harness QC power. The vision of QCSHQD paves the way for groundbreaking innovations by addressing key challenges of hybridization between classical and quantum computers.","sentences":["Quantum Computing (QC) is transitioning from theoretical frameworks to an indispensable powerhouse of computational capability, resulting in extensive adoption across both industrial and academic domains.","QC presents exceptional advantages, including unparalleled processing speed and the potential to solve complex problems beyond the capabilities of classical computers.","Nevertheless, academic researchers and industry practitioners encounter various challenges in harnessing the benefits of this technology.","The limited accessibility of QC resources for classical developers, and a general lack of domain knowledge and expertise, represent insurmountable barrier, hence to address these challenges, we introduce a framework- Quantum Computing as a Service for Hybrid Classical-Quantum Software Development (QCSHQD), which leverages service-oriented strategies.","Our framework comprises three principal components: an Integrated Development Environment (IDE) for user interaction, an abstraction layer dedicated to orchestrating quantum services, and a service provider responsible for executing services on quantum computer.","This study presents a blueprint for QCSHQD, designed to democratize access to QC resources for classical developers who want to seamless harness QC power.","The vision of QCSHQD paves the way for groundbreaking innovations by addressing key challenges of hybridization between classical and quantum computers."],"url":"http://arxiv.org/abs/2403.08663v1","category":"cs.SE"}
{"created":"2024-03-13 16:15:20","title":"Negative pressure as a quantum effect in free-streaming in the cosmological background","abstract":"We present a study of energy density and pressure of a free real scalar quantum field after its decoupling from a thermal bath in the spatially flat Friedman-Lemaitre-Robertson-Walker space-time by solving the Klein-Gordon equation both analytically and numerically for different predetermined scale factor functions $a(t)$. The energy density and pressure, defined by subtracting the vacuum expectation values at the decoupling time, feature corrections with respect to the classical free-streaming solution of the relativistic Boltzmann equation. We show that if the expansion rate is comparable or larger than $mc^2/\\hbar$ or $KT_0/\\hbar$ where $m$ is the mass and $T_0$ the decoupling temperature, both energy density and pressure gets strong quantum corrections which substantially modify their classical dependence on the scale factor $a(t)$ and drive pressure to large negative values. For a minimally coupled field with a very low mass in an expanding de Sitter universe quantum corrections are dominant driving pressure and energy density to become asymptotically constant with an equation of state $p/\\varepsilon \\simeq -1$, thereby mimicking a cosmological constant. For a minimally coupled massless field, quantum corrections are asymptotically dominant for any accelerated expansion.","sentences":["We present a study of energy density and pressure of a free real scalar quantum field after its decoupling from a thermal bath in the spatially flat Friedman-Lemaitre-Robertson-Walker space-time by solving the Klein-Gordon equation both analytically and numerically for different predetermined scale factor functions $a(t)$. The energy density and pressure, defined by subtracting the vacuum expectation values at the decoupling time, feature corrections with respect to the classical free-streaming solution of the relativistic Boltzmann equation.","We show that if the expansion rate is comparable or larger than $mc^2/\\hbar$ or $KT_0/\\hbar$ where $m$ is the mass and $T_0$ the decoupling temperature, both energy density and pressure gets strong quantum corrections which substantially modify their classical dependence on the scale factor $a(t)$ and drive pressure to large negative values.","For a minimally coupled field with a very low mass in an expanding de Sitter universe quantum corrections are dominant driving pressure and energy density to become asymptotically constant with an equation of state $p/\\varepsilon \\simeq -1$, thereby mimicking a cosmological constant.","For a minimally coupled massless field, quantum corrections are asymptotically dominant for any accelerated expansion."],"url":"http://arxiv.org/abs/2403.08661v1","category":"gr-qc"}
{"created":"2024-03-13 16:14:23","title":"Fourier Quasicrystals on $\\mathbb R^n$ Preliminary Report","abstract":"This paper has three aims. First, for $n \\geq 1$ we construct a family of real-rooted trigonometric polynomial maps $P : \\mathbb C^n \\mapsto \\mathbb C^n$ whose divisors are Fourier Quasicrystals (FQ). For $n = 1$ these divisors include the first nontrivial FQ with positive integer coefficients constructed by Kurasov and Sarnak \\cite{kurasovsarnak}, and for $n > 1$ they overlap with Meyer's curved model sets \\cite{meyer6} and two-dimensional \\cite{meyer7} and multidimensional \\cite{meyer8} crystalline measures. We prove that the divisors are FQ by directly computing their Fourier transforms using a formula derived in \\cite{lawton}. Second, we extend the relationship between real-rootedness and amoebas, derived for $n = 1$ by Alon, Cohen and Vinzant \\cite{alon}, to the case $n > 1.$ The extension uses results in \\cite{bushuevatsikh} about homology of complements of amoebas of algebraic sets of codimension $> 1.$ Third, we prove that the divisors of all uniformly generic real-rooted $P$ are FQ. The proof uses the formula relating Grothendieck residues and Newton polytopes derived by Gelfond and Khovanskii \\cite{gelfondkhovanskii1} . Finally, we note that Olevskii and Ulanovskii [60] have proved that all FQ are divisors of real-rooted trigonometric polynomials for $n = 1$ but that the situation for $n > 1$ remains unsolved.","sentences":["This paper has three aims.","First, for $n \\geq 1$ we construct a family of real-rooted trigonometric polynomial maps $P :","\\mathbb C^n \\mapsto \\mathbb C^n$ whose divisors are Fourier Quasicrystals (FQ).","For $n = 1$ these divisors include the first nontrivial FQ with positive integer coefficients constructed by Kurasov and Sarnak \\cite{kurasovsarnak}, and for $n > 1$ they overlap with Meyer's curved model sets \\cite{meyer6} and two-dimensional \\cite{meyer7} and multidimensional \\cite{meyer8} crystalline measures.","We prove that the divisors are FQ by directly computing their Fourier transforms using a formula derived in \\cite{lawton}.","Second, we extend the relationship between real-rootedness and amoebas, derived for $n = 1$ by Alon, Cohen and Vinzant \\cite{alon}, to the case $n > 1.$","The extension uses results in \\cite{bushuevatsikh} about homology of complements of amoebas of algebraic sets of codimension $> 1.$","Third, we prove that the divisors of all uniformly generic real-rooted $P$ are FQ.","The proof uses the formula relating Grothendieck residues and Newton polytopes derived by Gelfond and Khovanskii \\cite{gelfondkhovanskii1} .","Finally, we note that Olevskii and Ulanovskii","[60] have proved that all FQ are divisors of real-rooted trigonometric polynomials for $n = 1$ but that the situation for $n > 1$ remains unsolved."],"url":"http://arxiv.org/abs/2403.08659v1","category":"math.AG"}
{"created":"2024-03-13 16:10:11","title":"Predicting long timescale kinetics under variable experimental conditions with Kinetica.jl","abstract":"Predicting the degradation processes of molecules over long timescales is a key aspect of industrial materials design. However, it is made computationally challenging by the need to construct large networks of chemical reactions that are relevant to the experimental conditions that kinetic models must mirror, with every reaction requiring accurate kinetic data. Here we showcase Kinetica.jl, a new software package for constructing large-scale chemical reaction networks in a fully-automated fashion by exploring chemical reaction space with a kinetics-driven algorithm; coupled to efficient machine-learning models of activation energies for sampled elementary reactions, we show how this approach readily enables generation and kinetic characterization of networks containing $\\sim10^{3}$ chemical species and $10^{4}$ - $10^{5}$ reactions. Symbolic-numeric modelling of the generated reaction networks is used to allow for flexible, efficient computation of kinetic profiles under experimentally-realizable conditions such as continuously-variable temperature regimes, enabling direct connection between bottom-up reaction networks and experimental observations. Highly efficient propagation of long-timescale kinetic profiles is required for automated reaction network refinement and is enabled here by a new discrete kinetic approximation. The resulting Kinetica.jl simulation package therefore enables automated generation, characterization, and long-timescale modelling of complex chemical reaction systems. We demonstrate this for hydrocarbon pyrolysis simulated over timescales of seconds, using transient temperature profiles representing those of tubular flow reactor experiments.","sentences":["Predicting the degradation processes of molecules over long timescales is a key aspect of industrial materials design.","However, it is made computationally challenging by the need to construct large networks of chemical reactions that are relevant to the experimental conditions that kinetic models must mirror, with every reaction requiring accurate kinetic data.","Here we showcase Kinetica.jl, a new software package for constructing large-scale chemical reaction networks in a fully-automated fashion by exploring chemical reaction space with a kinetics-driven algorithm; coupled to efficient machine-learning models of activation energies for sampled elementary reactions, we show how this approach readily enables generation and kinetic characterization of networks containing $\\sim10^{3}$ chemical species and $10^{4}$ - $10^{5}$ reactions.","Symbolic-numeric modelling of the generated reaction networks is used to allow for flexible, efficient computation of kinetic profiles under experimentally-realizable conditions such as continuously-variable temperature regimes, enabling direct connection between bottom-up reaction networks and experimental observations.","Highly efficient propagation of long-timescale kinetic profiles is required for automated reaction network refinement and is enabled here by a new discrete kinetic approximation.","The resulting Kinetica.jl simulation package therefore enables automated generation, characterization, and long-timescale modelling of complex chemical reaction systems.","We demonstrate this for hydrocarbon pyrolysis simulated over timescales of seconds, using transient temperature profiles representing those of tubular flow reactor experiments."],"url":"http://arxiv.org/abs/2403.08657v1","category":"physics.chem-ph"}
{"created":"2024-03-13 16:10:04","title":"Physical Memory Attacks and a Memory Safe Management System for Memory Defense","abstract":"Programming errors, defective hardware components (such as hard disk spindle defects), and environmental hazards can lead to invalid memory operations. In addition, less predictable forms of environmental stress, such as radiation, thermal influence, and energy fluctuations, can induce hardware faults. Sometimes, a soft error can occur instead of a complete failure, such as a bit-flip. The 'natural' factors that can cause bit-flips are replicable through targeted attacks that result in significant compromises, including full privileged system access. Existing physical defense solutions have consistently been circumvented shortly after deployment. We will explore the concept of a novel software-based low-level layer that can protect vulnerable memory targeted by physical attack vectors related to bit-flip vulnerabilities.","sentences":["Programming errors, defective hardware components (such as hard disk spindle defects), and environmental hazards can lead to invalid memory operations.","In addition, less predictable forms of environmental stress, such as radiation, thermal influence, and energy fluctuations, can induce hardware faults.","Sometimes, a soft error can occur instead of a complete failure, such as a bit-flip.","The 'natural' factors that can cause bit-flips are replicable through targeted attacks that result in significant compromises, including full privileged system access.","Existing physical defense solutions have consistently been circumvented shortly after deployment.","We will explore the concept of a novel software-based low-level layer that can protect vulnerable memory targeted by physical attack vectors related to bit-flip vulnerabilities."],"url":"http://arxiv.org/abs/2403.08656v1","category":"cs.CR"}
{"created":"2024-03-13 16:06:07","title":"HAIFIT: Human-Centered AI for Fashion Image Translation","abstract":"In the realm of fashion design, sketches serve as the canvas for expressing an artist's distinctive drawing style and creative vision, capturing intricate details like stroke variations and texture nuances. The advent of sketch-to-image cross-modal translation technology has notably aided designers. However, existing methods often compromise these sketch details during image generation, resulting in images that deviate from the designer's intended concept. This limitation hampers the ability to offer designers a precise preview of the final output. To overcome this challenge, we introduce HAIFIT, a novel approach that transforms sketches into high-fidelity, lifelike clothing images by integrating multi-scale features and capturing extensive feature map dependencies from diverse perspectives. Through extensive qualitative and quantitative evaluations conducted on our self-collected dataset, our method demonstrates superior performance compared to existing methods in generating photorealistic clothing images. Our method excels in preserving the distinctive style and intricate details essential for fashion design applications.","sentences":["In the realm of fashion design, sketches serve as the canvas for expressing an artist's distinctive drawing style and creative vision, capturing intricate details like stroke variations and texture nuances.","The advent of sketch-to-image cross-modal translation technology has notably aided designers.","However, existing methods often compromise these sketch details during image generation, resulting in images that deviate from the designer's intended concept.","This limitation hampers the ability to offer designers a precise preview of the final output.","To overcome this challenge, we introduce HAIFIT, a novel approach that transforms sketches into high-fidelity, lifelike clothing images by integrating multi-scale features and capturing extensive feature map dependencies from diverse perspectives.","Through extensive qualitative and quantitative evaluations conducted on our self-collected dataset, our method demonstrates superior performance compared to existing methods in generating photorealistic clothing images.","Our method excels in preserving the distinctive style and intricate details essential for fashion design applications."],"url":"http://arxiv.org/abs/2403.08651v1","category":"cs.CV"}
{"created":"2024-03-13 16:05:18","title":"Data Augmentation in Human-Centric Vision","abstract":"This survey presents a comprehensive analysis of data augmentation techniques in human-centric vision tasks, a first of its kind in the field. It delves into a wide range of research areas including person ReID, human parsing, human pose estimation, and pedestrian detection, addressing the significant challenges posed by overfitting and limited training data in these domains. Our work categorizes data augmentation methods into two main types: data generation and data perturbation. Data generation covers techniques like graphic engine-based generation, generative model-based generation, and data recombination, while data perturbation is divided into image-level and human-level perturbations. Each method is tailored to the unique requirements of human-centric tasks, with some applicable across multiple areas. Our contributions include an extensive literature review, providing deep insights into the influence of these augmentation techniques in human-centric vision and highlighting the nuances of each method. We also discuss open issues and future directions, such as the integration of advanced generative models like Latent Diffusion Models, for creating more realistic and diverse training data. This survey not only encapsulates the current state of data augmentation in human-centric vision but also charts a course for future research, aiming to develop more robust, accurate, and efficient human-centric vision systems.","sentences":["This survey presents a comprehensive analysis of data augmentation techniques in human-centric vision tasks, a first of its kind in the field.","It delves into a wide range of research areas including person ReID, human parsing, human pose estimation, and pedestrian detection, addressing the significant challenges posed by overfitting and limited training data in these domains.","Our work categorizes data augmentation methods into two main types: data generation and data perturbation.","Data generation covers techniques like graphic engine-based generation, generative model-based generation, and data recombination, while data perturbation is divided into image-level and human-level perturbations.","Each method is tailored to the unique requirements of human-centric tasks, with some applicable across multiple areas.","Our contributions include an extensive literature review, providing deep insights into the influence of these augmentation techniques in human-centric vision and highlighting the nuances of each method.","We also discuss open issues and future directions, such as the integration of advanced generative models like Latent Diffusion Models, for creating more realistic and diverse training data.","This survey not only encapsulates the current state of data augmentation in human-centric vision but also charts a course for future research, aiming to develop more robust, accurate, and efficient human-centric vision systems."],"url":"http://arxiv.org/abs/2403.08650v1","category":"cs.CV"}
{"created":"2024-03-13 16:04:29","title":"A Causal Inspired Early-Branching Structure for Domain Generalization","abstract":"Learning domain-invariant semantic representations is crucial for achieving domain generalization (DG), where a model is required to perform well on unseen target domains. One critical challenge is that standard training often results in entangled semantic and domain-specific features. Previous works suggest formulating the problem from a causal perspective and solving the entanglement problem by enforcing marginal independence between the causal (\\ie semantic) and non-causal (\\ie domain-specific) features. Despite its simplicity, the basic marginal independent-based idea alone may be insufficient to identify the causal feature. By d-separation, we observe that the causal feature can be further characterized by being independent of the domain conditioned on the object, and we propose the following two strategies as complements for the basic framework.   First, the observation implicitly implies that for the same object, the causal feature should not be associated with the non-causal feature, revealing that the common practice of obtaining the two features with a shared base feature extractor and two lightweight prediction heads might be inappropriate. To meet the constraint, we propose a simple early-branching structure, where the causal and non-causal feature obtaining branches share the first few blocks while diverging thereafter, for better structure design; Second, the observation implies that the causal feature remains invariant across different domains for the same object. To this end, we suggest that augmentation should be incorporated into the framework to better characterize the causal feature, and we further suggest an effective random domain sampling scheme to fulfill the task. Theoretical and experimental results show that the two strategies are beneficial for the basic marginal independent-based framework. Code is available at \\url{https://github.com/liangchen527/CausEB}.","sentences":["Learning domain-invariant semantic representations is crucial for achieving domain generalization (DG), where a model is required to perform well on unseen target domains.","One critical challenge is that standard training often results in entangled semantic and domain-specific features.","Previous works suggest formulating the problem from a causal perspective and solving the entanglement problem by enforcing marginal independence between the causal (\\ie semantic) and non-causal (\\ie domain-specific) features.","Despite its simplicity, the basic marginal independent-based idea alone may be insufficient to identify the causal feature.","By d-separation, we observe that the causal feature can be further characterized by being independent of the domain conditioned on the object, and we propose the following two strategies as complements for the basic framework.   ","First, the observation implicitly implies that for the same object, the causal feature should not be associated with the non-causal feature, revealing that the common practice of obtaining the two features with a shared base feature extractor and two lightweight prediction heads might be inappropriate.","To meet the constraint, we propose a simple early-branching structure, where the causal and non-causal feature obtaining branches share the first few blocks while diverging thereafter, for better structure design; Second, the observation implies that the causal feature remains invariant across different domains for the same object.","To this end, we suggest that augmentation should be incorporated into the framework to better characterize the causal feature, and we further suggest an effective random domain sampling scheme to fulfill the task.","Theoretical and experimental results show that the two strategies are beneficial for the basic marginal independent-based framework.","Code is available at \\url{https://github.com/liangchen527/CausEB}."],"url":"http://arxiv.org/abs/2403.08649v1","category":"cs.CV"}
{"created":"2024-03-13 16:02:18","title":"Meta Reinforcement Learning for Resource Allocation in Aerial Active-RIS-assisted Networks with Rate-Splitting Multiple Access","abstract":"Mounting a reconfigurable intelligent surface (RIS) on an unmanned aerial vehicle (UAV) holds promise for improving traditional terrestrial network performance. Unlike conventional methods deploying passive RIS on UAVs, this study delves into the efficacy of an aerial active RIS (AARIS). Specifically, the downlink transmission of an AARIS network is investigated, where the base station (BS) leverages rate-splitting multiple access (RSMA) for effective interference management and benefits from the support of an AARIS for jointly amplifying and reflecting the BS's transmit signals. Considering both the non-trivial energy consumption of the active RIS and the limited energy storage of the UAV, we propose an innovative element selection strategy for optimizing the on/off status of RIS elements, which adaptively and remarkably manages the system's power consumption. To this end, a resource management problem is formulated, aiming to maximize the system energy efficiency (EE) by jointly optimizing the transmit beamforming at the BS, the element activation, the phase shift and the amplification factor at the RIS, the RSMA common data rate at users, as well as the UAV's trajectory. Due to the dynamicity nature of UAV and user mobility, a deep reinforcement learning (DRL) algorithm is designed for resource allocation, utilizing meta-learning to adaptively handle fast time-varying system dynamics. Simulations indicate that incorporating an active RIS at the UAV leads to substantial EE gain, compared to passive RIS-aided UAV. We observe the superiority of the RSMA-based AARIS system in terms of EE, compared to existing approaches adopting non-orthogonal multiple access (NOMA).","sentences":["Mounting a reconfigurable intelligent surface (RIS) on an unmanned aerial vehicle (UAV) holds promise for improving traditional terrestrial network performance.","Unlike conventional methods deploying passive RIS on UAVs, this study delves into the efficacy of an aerial active RIS (AARIS).","Specifically, the downlink transmission of an AARIS network is investigated, where the base station (BS) leverages rate-splitting multiple access (RSMA) for effective interference management and benefits from the support of an AARIS for jointly amplifying and reflecting the BS's transmit signals.","Considering both the non-trivial energy consumption of the active RIS and the limited energy storage of the UAV, we propose an innovative element selection strategy for optimizing the on/off status of RIS elements, which adaptively and remarkably manages the system's power consumption.","To this end, a resource management problem is formulated, aiming to maximize the system energy efficiency (EE) by jointly optimizing the transmit beamforming at the BS, the element activation, the phase shift and the amplification factor at the RIS, the RSMA common data rate at users, as well as the UAV's trajectory.","Due to the dynamicity nature of UAV and user mobility, a deep reinforcement learning (DRL) algorithm is designed for resource allocation, utilizing meta-learning to adaptively handle fast time-varying system dynamics.","Simulations indicate that incorporating an active RIS at the UAV leads to substantial EE gain, compared to passive RIS-aided UAV.","We observe the superiority of the RSMA-based AARIS system in terms of EE, compared to existing approaches adopting non-orthogonal multiple access (NOMA)."],"url":"http://arxiv.org/abs/2403.08648v1","category":"cs.IT"}
{"created":"2024-03-13 15:57:13","title":"Thermodynamic Integration for Dynamically Unstable Systems Using Interatomic Force Constants without Molecular Dynamics","abstract":"We demonstrate an efficient and accurate, general-purpose first-principles blueprint for calculating anharmonic vibrational free energy and predicting structural phase transition temperatures of solids. Thermodynamic integration is performed without molecular dynamics using only interatomic force constants to model analogues of the true potential and generate their thermal ensembles. By replacing \\textit{ab initio} molecular dynamics (AIMD) with statistical sampling of ensemble configurations and trading density-functional theory (DFT) energy calculations on each configuration for a set of matrix operations, our approach enables a faster thermodynamic integration by 4 orders of magnitude over the traditional route via AIMD. Experimental phase transition temperatures of a variety of strongly anharmonic materials with dynamical instabilities including shape-memory alloys are recovered to largely within 25% error. Such a combination of speed and accuracy enables the method to be deployed at a large-scale for predictive mapping of phase transition temperatures.","sentences":["We demonstrate an efficient and accurate, general-purpose first-principles blueprint for calculating anharmonic vibrational free energy and predicting structural phase transition temperatures of solids.","Thermodynamic integration is performed without molecular dynamics using only interatomic force constants to model analogues of the true potential and generate their thermal ensembles.","By replacing \\textit{ab initio} molecular dynamics (AIMD) with statistical sampling of ensemble configurations and trading density-functional theory (DFT) energy calculations on each configuration for a set of matrix operations, our approach enables a faster thermodynamic integration by 4 orders of magnitude over the traditional route via AIMD.","Experimental phase transition temperatures of a variety of strongly anharmonic materials with dynamical instabilities including shape-memory alloys are recovered to largely within 25% error.","Such a combination of speed and accuracy enables the method to be deployed at a large-scale for predictive mapping of phase transition temperatures."],"url":"http://arxiv.org/abs/2403.08644v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-13 15:55:49","title":"On the Aw-Rascle-Zhang traffic models with nonlocal look-ahead interactions","abstract":"We present a new family of second-order traffic flow models, extending the Aw-Rascle-Zhang (ARZ) model to incorporate nonlocal interactions. Our model includes a specific nonlocal Arrhenius-type look-ahead slowdown factor. We establish both local and global well-posedness theories for these nonlocal ARZ models.   In contrast to the local ARZ model, where generic smooth initial data typically lead to finite-time shock formation, we show that our nonlocal ARZ model exhibits global regularity for a class of smooth subcritical initial data. Our result highlights the potential of nonlocal interactions to mitigate shock formations in second-order traffic flow models.   Our analytical approach relies on investigating phase plane dynamics. We introduce a novel comparison principle based on a mediant inequality to effectively handle the nonlocal information inherent in our model.","sentences":["We present a new family of second-order traffic flow models, extending the Aw-Rascle-Zhang (ARZ) model to incorporate nonlocal interactions.","Our model includes a specific nonlocal Arrhenius-type look-ahead slowdown factor.","We establish both local and global well-posedness theories for these nonlocal ARZ models.   ","In contrast to the local ARZ model, where generic smooth initial data typically lead to finite-time shock formation, we show that our nonlocal ARZ model exhibits global regularity for a class of smooth subcritical initial data.","Our result highlights the potential of nonlocal interactions to mitigate shock formations in second-order traffic flow models.   ","Our analytical approach relies on investigating phase plane dynamics.","We introduce a novel comparison principle based on a mediant inequality to effectively handle the nonlocal information inherent in our model."],"url":"http://arxiv.org/abs/2403.08643v1","category":"math.AP"}
{"created":"2024-03-13 15:54:33","title":"Air-coupled ultrasound using broadband shock waves from piezoelectric spark igniters","abstract":"We used an optomechanical sensor to study the ultrasound generated by manually operated piezoelectric spark igniters. These low-energy sparks produce short-duration acoustic shock-wave pulses, with sub-microsecond rise times and frequency content extending well beyond 2 MHz in air. The same source-receiver combination was then used to demonstrate broadband characterization of solid (polymer and glass) plates in a simple setup, where single spark events yielded high-SNR data without the need for critical alignment. This setup also enabled us to estimate pressure excursions approaching 105 Pa at millimeter-scale distances from the spark. The results are in large part made possible by the small size, wide bandwidth, and high sensitivity of the optomechanical sensor, and might be of interest for air-coupled ultrasound applications in non-destructive testing.","sentences":["We used an optomechanical sensor to study the ultrasound generated by manually operated piezoelectric spark igniters.","These low-energy sparks produce short-duration acoustic shock-wave pulses, with sub-microsecond rise times and frequency content extending well beyond 2 MHz in air.","The same source-receiver combination was then used to demonstrate broadband characterization of solid (polymer and glass) plates in a simple setup, where single spark events yielded high-SNR data without the need for critical alignment.","This setup also enabled us to estimate pressure excursions approaching 105 Pa at millimeter-scale distances from the spark.","The results are in large part made possible by the small size, wide bandwidth, and high sensitivity of the optomechanical sensor, and might be of interest for air-coupled ultrasound applications in non-destructive testing."],"url":"http://arxiv.org/abs/2403.08641v1","category":"physics.app-ph"}
{"created":"2024-03-13 15:52:20","title":"Refractive COLMAP: Refractive Structure-from-Motion Revisited","abstract":"In this paper, we present a complete refractive Structure-from-Motion (RSfM) framework for underwater 3D reconstruction using refractive camera setups (for both, flat- and dome-port underwater housings). Despite notable achievements in refractive multi-view geometry over the past decade, a robust, complete and publicly available solution for such tasks is not available at present, and often practical applications have to resort to approximating refraction effects by the intrinsic (distortion) parameters of a pinhole camera model. To fill this gap, we have integrated refraction considerations throughout the entire SfM process within the state-of-the-art, open-source SfM framework COLMAP. Numerical simulations and reconstruction results on synthetically generated but photo-realistic images with ground truth validate that enabling refraction does not compromise accuracy or robustness as compared to in-air reconstructions. Finally, we demonstrate the capability of our approach for large-scale refractive scenarios using a dataset consisting of nearly 6000 images. The implementation is released as open-source at: https://cau-git.rz.uni-kiel.de/inf-ag-koeser/colmap_underwater.","sentences":["In this paper, we present a complete refractive Structure-from-Motion (RSfM) framework for underwater 3D reconstruction using refractive camera setups (for both, flat- and dome-port underwater housings).","Despite notable achievements in refractive multi-view geometry over the past decade, a robust, complete and publicly available solution for such tasks is not available at present, and often practical applications have to resort to approximating refraction effects by the intrinsic (distortion) parameters of a pinhole camera model.","To fill this gap, we have integrated refraction considerations throughout the entire SfM process within the state-of-the-art, open-source SfM framework COLMAP.","Numerical simulations and reconstruction results on synthetically generated but photo-realistic images with ground truth validate that enabling refraction does not compromise accuracy or robustness as compared to in-air reconstructions.","Finally, we demonstrate the capability of our approach for large-scale refractive scenarios using a dataset consisting of nearly 6000 images.","The implementation is released as open-source at: https://cau-git.rz.uni-kiel.de/inf-ag-koeser/colmap_underwater."],"url":"http://arxiv.org/abs/2403.08640v1","category":"cs.CV"}
{"created":"2024-03-13 15:50:26","title":"GR as a classical spin-2 theory?","abstract":"The self-interaction spin-2 approach to general relativity (GR) has been extremely influential in the particle physics community. Leaving no doubt regarding its heuristic value, we argue that a view of the metric field of GR as nothing but a stand-in for a self-coupling field in flat spacetime runs into a dilemma: either the view is physically incomplete in so far as it requires recourse to GR after all, or it leads to an absurd multiplication of alternative viewpoints on GR rendering any understanding of the metric field as nothing but a spin-2 field in flat spacetime unjustified.","sentences":["The self-interaction spin-2 approach to general relativity (GR) has been extremely influential in the particle physics community.","Leaving no doubt regarding its heuristic value, we argue that a view of the metric field of GR as nothing but a stand-in for a self-coupling field in flat spacetime runs into a dilemma: either the view is physically incomplete in so far as it requires recourse to GR after all, or it leads to an absurd multiplication of alternative viewpoints on GR rendering any understanding of the metric field as nothing but a spin-2 field in flat spacetime unjustified."],"url":"http://arxiv.org/abs/2403.08637v1","category":"physics.hist-ph"}
{"created":"2024-03-13 15:47:26","title":"Human Alignment of Large Language Models through Online Preference Optimisation","abstract":"Ensuring alignment of language models' outputs with human preferences is critical to guarantee a useful, safe, and pleasant user experience. Thus, human alignment has been extensively studied recently and several methods such as Reinforcement Learning from Human Feedback (RLHF), Direct Policy Optimisation (DPO) and Sequence Likelihood Calibration (SLiC) have emerged. In this paper, our contribution is two-fold. First, we show the equivalence between two recent alignment methods, namely Identity Policy Optimisation (IPO) and Nash Mirror Descent (Nash-MD). Second, we introduce a generalisation of IPO, named IPO-MD, that leverages the regularised sampling approach proposed by Nash-MD.   This equivalence may seem surprising at first sight, since IPO is an offline method whereas Nash-MD is an online method using a preference model. However, this equivalence can be proven when we consider the online version of IPO, that is when both generations are sampled by the online policy and annotated by a trained preference model. Optimising the IPO loss with such a stream of data becomes then equivalent to finding the Nash equilibrium of the preference model through self-play. Building on this equivalence, we introduce the IPO-MD algorithm that generates data with a mixture policy (between the online and reference policy) similarly as the general Nash-MD algorithm. We compare online-IPO and IPO-MD to different online versions of existing losses on preference data such as DPO and SLiC on a summarisation task.","sentences":["Ensuring alignment of language models' outputs with human preferences is critical to guarantee a useful, safe, and pleasant user experience.","Thus, human alignment has been extensively studied recently and several methods such as Reinforcement Learning from Human Feedback (RLHF), Direct Policy Optimisation (DPO) and Sequence Likelihood Calibration (SLiC) have emerged.","In this paper, our contribution is two-fold.","First, we show the equivalence between two recent alignment methods, namely Identity Policy Optimisation (IPO) and Nash Mirror Descent (Nash-MD).","Second, we introduce a generalisation of IPO, named IPO-MD, that leverages the regularised sampling approach proposed by Nash-MD.   ","This equivalence may seem surprising at first sight, since IPO is an offline method whereas Nash-MD is an online method using a preference model.","However, this equivalence can be proven when we consider the online version of IPO, that is when both generations are sampled by the online policy and annotated by a trained preference model.","Optimising the IPO loss with such a stream of data becomes then equivalent to finding the Nash equilibrium of the preference model through self-play.","Building on this equivalence, we introduce the IPO-MD algorithm that generates data with a mixture policy (between the online and reference policy) similarly as the general Nash-MD algorithm.","We compare online-IPO and IPO-MD to different online versions of existing losses on preference data such as DPO and SLiC on a summarisation task."],"url":"http://arxiv.org/abs/2403.08635v1","category":"cs.LG"}
{"created":"2024-03-13 15:46:38","title":"Entangled Photon-pair Generation in Nonlinear Thin-films","abstract":"We develop a fully vectorial and non-paraxial formalism to describe spontaneous parametric down-conversion in nonlinear thin films. The formalism is capable of treating slabs with a sub-wavelength thickness, describe the associated Fabry-P\\'erot effects, and even treat absorptive nonlinear materials. With this formalism, we perform an in-depth study of the dynamics of entangled photon-pair generation in nonlinear thin films, to provide a needed theoretical understanding for such systems that have recently attracted much experimental attention as sources of photon pairs. As an important example, we study the far-field radiation properties of photon pairs generated from a high-refractive-index nonlinear thin-film with Zinc-Blende structure, that is deposited on a linear low-refractive-index substrate. In particular, we study the thickness-dependent effect of Fabry-P\\'erot interferences on the far-field radiation pattern of the photon pairs. We also pay special attention to study of entanglement generation, and find the conditions under which maximally polarization-entangled photon pairs can be generated and detected in such nonlinear thin-films.","sentences":["We develop a fully vectorial and non-paraxial formalism to describe spontaneous parametric down-conversion in nonlinear thin films.","The formalism is capable of treating slabs with a sub-wavelength thickness, describe the associated Fabry-P\\'erot effects, and even treat absorptive nonlinear materials.","With this formalism, we perform an in-depth study of the dynamics of entangled photon-pair generation in nonlinear thin films, to provide a needed theoretical understanding for such systems that have recently attracted much experimental attention as sources of photon pairs.","As an important example, we study the far-field radiation properties of photon pairs generated from a high-refractive-index nonlinear thin-film with Zinc-Blende structure, that is deposited on a linear low-refractive-index substrate.","In particular, we study the thickness-dependent effect of Fabry-P\\'erot interferences on the far-field radiation pattern of the photon pairs.","We also pay special attention to study of entanglement generation, and find the conditions under which maximally polarization-entangled photon pairs can be generated and detected in such nonlinear thin-films."],"url":"http://arxiv.org/abs/2403.08633v1","category":"quant-ph"}
{"created":"2024-03-13 15:45:04","title":"Scaling Up Dynamic Human-Scene Interaction Modeling","abstract":"Confronting the challenges of data scarcity and advanced motion synthesis in human-scene interaction modeling, we introduce the TRUMANS dataset alongside a novel HSI motion synthesis method. TRUMANS stands as the most comprehensive motion-captured HSI dataset currently available, encompassing over 15 hours of human interactions across 100 indoor scenes. It intricately captures whole-body human motions and part-level object dynamics, focusing on the realism of contact. This dataset is further scaled up by transforming physical environments into exact virtual models and applying extensive augmentations to appearance and motion for both humans and objects while maintaining interaction fidelity. Utilizing TRUMANS, we devise a diffusion-based autoregressive model that efficiently generates HSI sequences of any length, taking into account both scene context and intended actions. In experiments, our approach shows remarkable zero-shot generalizability on a range of 3D scene datasets (e.g., PROX, Replica, ScanNet, ScanNet++), producing motions that closely mimic original motion-captured sequences, as confirmed by quantitative experiments and human studies.","sentences":["Confronting the challenges of data scarcity and advanced motion synthesis in human-scene interaction modeling, we introduce the TRUMANS dataset alongside a novel HSI motion synthesis method.","TRUMANS stands as the most comprehensive motion-captured HSI dataset currently available, encompassing over 15 hours of human interactions across 100 indoor scenes.","It intricately captures whole-body human motions and part-level object dynamics, focusing on the realism of contact.","This dataset is further scaled up by transforming physical environments into exact virtual models and applying extensive augmentations to appearance and motion for both humans and objects while maintaining interaction fidelity.","Utilizing TRUMANS, we devise a diffusion-based autoregressive model that efficiently generates HSI sequences of any length, taking into account both scene context and intended actions.","In experiments, our approach shows remarkable zero-shot generalizability on a range of 3D scene datasets (e.g., PROX, Replica, ScanNet, ScanNet++), producing motions that closely mimic original motion-captured sequences, as confirmed by quantitative experiments and human studies."],"url":"http://arxiv.org/abs/2403.08629v1","category":"cs.CV"}
{"created":"2024-03-13 15:40:17","title":"Multifidelity linear regression for scientific machine learning from scarce data","abstract":"Machine learning (ML) methods, which fit to data the parameters of a given parameterized model class, have garnered significant interest as potential methods for learning surrogate models for complex engineering systems for which traditional simulation is expensive. However, in many scientific and engineering settings, generating high-fidelity data on which to train ML models is expensive, and the available budget for generating training data is limited. ML models trained on the resulting scarce high-fidelity data have high variance and are sensitive to vagaries of the training data set. We propose a new multifidelity training approach for scientific machine learning that exploits the scientific context where data of varying fidelities and costs are available; for example high-fidelity data may be generated by an expensive fully resolved physics simulation whereas lower-fidelity data may arise from a cheaper model based on simplifying assumptions. We use the multifidelity data to define new multifidelity Monte Carlo estimators for the unknown parameters of linear regression models, and provide theoretical analyses that guarantee the approach's accuracy and improved robustness to small training budgets. Numerical results verify the theoretical analysis and demonstrate that multifidelity learned models trained on scarce high-fidelity data and additional low-fidelity data achieve order-of-magnitude lower model variance than standard models trained on only high-fidelity data of comparable cost. This illustrates that in the scarce data regime, our multifidelity training strategy yields models with lower expected error than standard training approaches.","sentences":["Machine learning (ML) methods, which fit to data the parameters of a given parameterized model class, have garnered significant interest as potential methods for learning surrogate models for complex engineering systems for which traditional simulation is expensive.","However, in many scientific and engineering settings, generating high-fidelity data on which to train ML models is expensive, and the available budget for generating training data is limited.","ML models trained on the resulting scarce high-fidelity data have high variance and are sensitive to vagaries of the training data set.","We propose a new multifidelity training approach for scientific machine learning that exploits the scientific context where data of varying fidelities and costs are available; for example high-fidelity data may be generated by an expensive fully resolved physics simulation whereas lower-fidelity data may arise from a cheaper model based on simplifying assumptions.","We use the multifidelity data to define new multifidelity Monte Carlo estimators for the unknown parameters of linear regression models, and provide theoretical analyses that guarantee the approach's accuracy and improved robustness to small training budgets.","Numerical results verify the theoretical analysis and demonstrate that multifidelity learned models trained on scarce high-fidelity data and additional low-fidelity data achieve order-of-magnitude lower model variance than standard models trained on only high-fidelity data of comparable cost.","This illustrates that in the scarce data regime, our multifidelity training strategy yields models with lower expected error than standard training approaches."],"url":"http://arxiv.org/abs/2403.08627v1","category":"stat.ML"}
{"created":"2024-03-13 15:39:57","title":"Towards a Privacy and Security-Aware Framework for Ethical AI: Guiding the Development and Assessment of AI Systems","abstract":"As artificial intelligence continues its unprecedented global expansion, accompanied by a proliferation of benefits, an increasing apprehension about the privacy and security implications of AI-enabled systems emerges. The pivotal question of effectively controlling AI development at both jurisdictional and organizational levels has become a prominent theme in contemporary discourse. While the European Parliament and Council have taken a decisive step by reaching a political agreement on the EU AI Act, the first comprehensive AI law, organizations still find it challenging to adapt to the fast-evolving AI landscape, lacking a universal tool for evaluating the privacy and security dimensions of their AI models and systems. In response to this critical challenge, this study conducts a systematic literature review spanning the years 2020 to 2023, with a primary focus on establishing a unified definition of key concepts in AI Ethics, particularly emphasizing the domains of privacy and security. Through the synthesis of knowledge extracted from the SLR, this study presents a conceptual framework tailored for privacy- and security-aware AI systems. This framework is designed to assist diverse stakeholders, including organizations, academic institutions, and governmental bodies, in both the development and critical assessment of AI systems. Essentially, the proposed framework serves as a guide for ethical decision-making, fostering an environment wherein AI is developed and utilized with a strong commitment to ethical principles. In addition, the study unravels the key issues and challenges surrounding the privacy and security dimensions, delineating promising avenues for future research, thereby contributing to the ongoing dialogue on the globalization and democratization of AI ethics.","sentences":["As artificial intelligence continues its unprecedented global expansion, accompanied by a proliferation of benefits, an increasing apprehension about the privacy and security implications of AI-enabled systems emerges.","The pivotal question of effectively controlling AI development at both jurisdictional and organizational levels has become a prominent theme in contemporary discourse.","While the European Parliament and Council have taken a decisive step by reaching a political agreement on the EU AI Act, the first comprehensive AI law, organizations still find it challenging to adapt to the fast-evolving AI landscape, lacking a universal tool for evaluating the privacy and security dimensions of their AI models and systems.","In response to this critical challenge, this study conducts a systematic literature review spanning the years 2020 to 2023, with a primary focus on establishing a unified definition of key concepts in AI Ethics, particularly emphasizing the domains of privacy and security.","Through the synthesis of knowledge extracted from the SLR, this study presents a conceptual framework tailored for privacy- and security-aware AI systems.","This framework is designed to assist diverse stakeholders, including organizations, academic institutions, and governmental bodies, in both the development and critical assessment of AI systems.","Essentially, the proposed framework serves as a guide for ethical decision-making, fostering an environment wherein AI is developed and utilized with a strong commitment to ethical principles.","In addition, the study unravels the key issues and challenges surrounding the privacy and security dimensions, delineating promising avenues for future research, thereby contributing to the ongoing dialogue on the globalization and democratization of AI ethics."],"url":"http://arxiv.org/abs/2403.08624v1","category":"cs.CY"}
{"created":"2024-03-13 15:36:53","title":"Environment-Induced Information Scrambling Transition with Charge Conservations","abstract":"In generic closed quantum systems, the complexity of operators increases under time evolution governed by the Heisenberg equation, reflecting the scrambling of local quantum information. However, when systems interact with an external environment, the system-environment coupling allows operators to escape from the system, inducing a dynamical transition between the scrambling phase and the dissipative phase. This transition is known as the environment-induced information scrambling transition, originally proposed in Majorana fermion systems. In this work, we advance this dicovery by investigating the transition in charge-conserved systems with space-time randomness. We construct solvable Brownian Sachdev-Ye-Kitaev models of complex fermions coupled to an environment, enabling the analytical computation of operator growth. We determine the critical dissipation strength, which is proportional to $n(1-n)$ with $n$ being the density of the complex fermions, arising from the suppression in the quantum Lyapunov exponent due to the Pauli blockade in the scattering process. We further analyze the density dependence of maximally scrambled operators at late time. Our results shed light on the intriguing interplay between information scrambling, dissipation, and conservation laws.","sentences":["In generic closed quantum systems, the complexity of operators increases under time evolution governed by the Heisenberg equation, reflecting the scrambling of local quantum information.","However, when systems interact with an external environment, the system-environment coupling allows operators to escape from the system, inducing a dynamical transition between the scrambling phase and the dissipative phase.","This transition is known as the environment-induced information scrambling transition, originally proposed in Majorana fermion systems.","In this work, we advance this dicovery by investigating the transition in charge-conserved systems with space-time randomness.","We construct solvable Brownian Sachdev-Ye-Kitaev models of complex fermions coupled to an environment, enabling the analytical computation of operator growth.","We determine the critical dissipation strength, which is proportional to $n(1-n)$ with $n$ being the density of the complex fermions, arising from the suppression in the quantum Lyapunov exponent due to the Pauli blockade in the scattering process.","We further analyze the density dependence of maximally scrambled operators at late time.","Our results shed light on the intriguing interplay between information scrambling, dissipation, and conservation laws."],"url":"http://arxiv.org/abs/2403.08622v1","category":"quant-ph"}
{"created":"2024-03-13 15:32:08","title":"Verifix: Post-Training Correction to Improve Label Noise Robustness with Verified Samples","abstract":"Label corruption, where training samples have incorrect labels, can significantly degrade the performance of machine learning models. This corruption often arises from non-expert labeling or adversarial attacks. Acquiring large, perfectly labeled datasets is costly, and retraining large models from scratch when a clean dataset becomes available is computationally expensive. To address this challenge, we propose Post-Training Correction, a new paradigm that adjusts model parameters after initial training to mitigate label noise, eliminating the need for retraining. We introduce Verifix, a novel Singular Value Decomposition (SVD) based algorithm that leverages a small, verified dataset to correct the model weights using a single update. Verifix uses SVD to estimate a Clean Activation Space and then projects the model's weights onto this space to suppress activations corresponding to corrupted data. We demonstrate Verifix's effectiveness on both synthetic and real-world label noise. Experiments on the CIFAR dataset with 25% synthetic corruption show 7.36% generalization improvements on average. Additionally, we observe generalization improvements of up to 2.63% on naturally corrupted datasets like WebVision1.0 and Clothing1M.","sentences":["Label corruption, where training samples have incorrect labels, can significantly degrade the performance of machine learning models.","This corruption often arises from non-expert labeling or adversarial attacks.","Acquiring large, perfectly labeled datasets is costly, and retraining large models from scratch when a clean dataset becomes available is computationally expensive.","To address this challenge, we propose Post-Training Correction, a new paradigm that adjusts model parameters after initial training to mitigate label noise, eliminating the need for retraining.","We introduce Verifix, a novel Singular Value Decomposition (SVD) based algorithm that leverages a small, verified dataset to correct the model weights using a single update.","Verifix uses SVD to estimate a Clean Activation Space and then projects the model's weights onto this space to suppress activations corresponding to corrupted data.","We demonstrate Verifix's effectiveness on both synthetic and real-world label noise.","Experiments on the CIFAR dataset with 25% synthetic corruption show 7.36% generalization improvements on average.","Additionally, we observe generalization improvements of up to 2.63% on naturally corrupted datasets like WebVision1.0 and Clothing1M."],"url":"http://arxiv.org/abs/2403.08618v1","category":"cs.LG"}
{"created":"2024-03-13 15:24:34","title":"Real-Time Sensor-Based Feedback Control for Obstacle Avoidance in Unknown Environments","abstract":"We revisit the Safety Velocity Cones (SVCs) obstacle avoidance approach for real-time autonomous navigation in an unknown $n$-dimensional environment. We propose a locally Lipschitz continuous implementation of the SVC controller using the distance-to-the-obstacle function and its gradient. We then show that the proposed implementation guarantees safe navigation in generic environments and almost globally asymptotic stability (AGAS) of the desired destination when the workspace contains strongly convex obstacles. The proposed computationally efficient control algorithm can be implemented onboard vehicles equipped with limited range sensors (e.g., LiDAR, depth camera), allowing the controller to be locally evaluated without requiring prior knowledge of the environment.","sentences":["We revisit the Safety Velocity Cones (SVCs) obstacle avoidance approach for real-time autonomous navigation in an unknown $n$-dimensional environment.","We propose a locally Lipschitz continuous implementation of the SVC controller using the distance-to-the-obstacle function and its gradient.","We then show that the proposed implementation guarantees safe navigation in generic environments and almost globally asymptotic stability (AGAS) of the desired destination when the workspace contains strongly convex obstacles.","The proposed computationally efficient control algorithm can be implemented onboard vehicles equipped with limited range sensors (e.g., LiDAR, depth camera), allowing the controller to be locally evaluated without requiring prior knowledge of the environment."],"url":"http://arxiv.org/abs/2403.08614v1","category":"eess.SY"}
{"created":"2024-03-13 15:23:55","title":"Link Prediction for Social Networks using Representation Learning and Heuristic-based Features","abstract":"The exponential growth in scale and relevance of social networks enable them to provide expansive insights. Predicting missing links in social networks efficiently can help in various modern-day business applications ranging from generating recommendations to influence analysis. Several categories of solutions exist for the same. Here, we explore various feature extraction techniques to generate representations of nodes and edges in a social network that allow us to predict missing links. We compare the results of using ten feature extraction techniques categorized across Structural embeddings, Neighborhood-based embeddings, Graph Neural Networks, and Graph Heuristics, followed by modeling with ensemble classifiers and custom Neural Networks. Further, we propose combining heuristic-based features and learned representations that demonstrate improved performance for the link prediction task on social network datasets. Using this method to generate accurate recommendations for many applications is a matter of further study that appears very promising. The code for all the experiments has been made public.","sentences":["The exponential growth in scale and relevance of social networks enable them to provide expansive insights.","Predicting missing links in social networks efficiently can help in various modern-day business applications ranging from generating recommendations to influence analysis.","Several categories of solutions exist for the same.","Here, we explore various feature extraction techniques to generate representations of nodes and edges in a social network that allow us to predict missing links.","We compare the results of using ten feature extraction techniques categorized across Structural embeddings, Neighborhood-based embeddings, Graph Neural Networks, and Graph Heuristics, followed by modeling with ensemble classifiers and custom Neural Networks.","Further, we propose combining heuristic-based features and learned representations that demonstrate improved performance for the link prediction task on social network datasets.","Using this method to generate accurate recommendations for many applications is a matter of further study that appears very promising.","The code for all the experiments has been made public."],"url":"http://arxiv.org/abs/2403.08613v1","category":"cs.SI"}
{"created":"2024-03-13 15:21:14","title":"On the Convergence of Locally Adaptive and Scalable Diffusion-Based Sampling Methods for Deep Bayesian Neural Network Posteriors","abstract":"Achieving robust uncertainty quantification for deep neural networks represents an important requirement in many real-world applications of deep learning such as medical imaging where it is necessary to assess the reliability of a neural network's prediction. Bayesian neural networks are a promising approach for modeling uncertainties in deep neural networks. Unfortunately, generating samples from the posterior distribution of neural networks is a major challenge. One significant advance in that direction would be the incorporation of adaptive step sizes, similar to modern neural network optimizers, into Monte Carlo Markov chain sampling algorithms without significantly increasing computational demand. Over the past years, several papers have introduced sampling algorithms with claims that they achieve this property. However, do they indeed converge to the correct distribution? In this paper, we demonstrate that these methods can have a substantial bias in the distribution they sample, even in the limit of vanishing step sizes and at full batch size.","sentences":["Achieving robust uncertainty quantification for deep neural networks represents an important requirement in many real-world applications of deep learning such as medical imaging where it is necessary to assess the reliability of a neural network's prediction.","Bayesian neural networks are a promising approach for modeling uncertainties in deep neural networks.","Unfortunately, generating samples from the posterior distribution of neural networks is a major challenge.","One significant advance in that direction would be the incorporation of adaptive step sizes, similar to modern neural network optimizers, into Monte Carlo Markov chain sampling algorithms without significantly increasing computational demand.","Over the past years, several papers have introduced sampling algorithms with claims that they achieve this property.","However, do they indeed converge to the correct distribution?","In this paper, we demonstrate that these methods can have a substantial bias in the distribution they sample, even in the limit of vanishing step sizes and at full batch size."],"url":"http://arxiv.org/abs/2403.08609v1","category":"cs.LG"}
{"created":"2024-03-13 15:20:30","title":"MedInsight: A Multi-Source Context Augmentation Framework for Generating Patient-Centric Medical Responses using Large Language Models","abstract":"Large Language Models (LLMs) have shown impressive capabilities in generating human-like responses. However, their lack of domain-specific knowledge limits their applicability in healthcare settings, where contextual and comprehensive responses are vital. To address this challenge and enable the generation of patient-centric responses that are contextually relevant and comprehensive, we propose MedInsight:a novel retrieval augmented framework that augments LLM inputs (prompts) with relevant background information from multiple sources. MedInsight extracts pertinent details from the patient's medical record or consultation transcript. It then integrates information from authoritative medical textbooks and curated web resources based on the patient's health history and condition. By constructing an augmented context combining the patient's record with relevant medical knowledge, MedInsight generates enriched, patient-specific responses tailored for healthcare applications such as diagnosis, treatment recommendations, or patient education. Experiments on the MTSamples dataset validate MedInsight's effectiveness in generating contextually appropriate medical responses. Quantitative evaluation using the Ragas metric and TruLens for answer similarity and answer correctness demonstrates the model's efficacy. Furthermore, human evaluation studies involving Subject Matter Expert (SMEs) confirm MedInsight's utility, with moderate inter-rater agreement on the relevance and correctness of the generated responses.","sentences":["Large Language Models (LLMs) have shown impressive capabilities in generating human-like responses.","However, their lack of domain-specific knowledge limits their applicability in healthcare settings, where contextual and comprehensive responses are vital.","To address this challenge and enable the generation of patient-centric responses that are contextually relevant and comprehensive, we propose MedInsight:a novel retrieval augmented framework that augments LLM inputs (prompts) with relevant background information from multiple sources.","MedInsight extracts pertinent details from the patient's medical record or consultation transcript.","It then integrates information from authoritative medical textbooks and curated web resources based on the patient's health history and condition.","By constructing an augmented context combining the patient's record with relevant medical knowledge, MedInsight generates enriched, patient-specific responses tailored for healthcare applications such as diagnosis, treatment recommendations, or patient education.","Experiments on the MTSamples dataset validate MedInsight's effectiveness in generating contextually appropriate medical responses.","Quantitative evaluation using the Ragas metric and TruLens for answer similarity and answer correctness demonstrates the model's efficacy.","Furthermore, human evaluation studies involving Subject Matter Expert (SMEs) confirm MedInsight's utility, with moderate inter-rater agreement on the relevance and correctness of the generated responses."],"url":"http://arxiv.org/abs/2403.08607v1","category":"cs.CL"}
{"created":"2024-03-13 15:15:51","title":"Effect of Earth's Oblateness on Black Hole Imaging Through Earth-Space and Space-Space VLBI","abstract":"Earth-based Very Long Baseline Interferometry (VLBI) has made rapid advances in imaging black holes. However, due to the limitations imposed on terrestrial VLBI by the Earth's finite size and turbulent atmosphere, it is imperative to have a space-based component in future VLBI missions. Herein, this paper investigates the effect of Earth's oblateness, also known as the $J_{2}$ effect, on orbiters in Earth-Space and Space-Space VLBI. The paper provides an extensive discussion on how the $J_{2}$ effect can directly impact orbit selection for black hole observations and how through informed choices of orbital parameters, the effect can be used to the mission's advantage, a fact that has not been addressed in existing space-VLBI investigations. We provide a comprehensive study of how the orbital parameters of several current space VLBI proposals will vary specifically due to the $J_{2}$ effect. For black hole accretion flow targets of interest, we have demonstrated how the $J_{2}$ effect leads to modest increase in shorter baseline coverage, filling gaps in the $(u,v)$ plane. Subsequently, we construct a simple analytical formalism that allows isolation of the impact of the $J_{2}$ effect on the $(u,v)$ plane without requiring computationally intensive orbit propagation simulations. By directly constructing $(u,v)$ coverage using the $J_{2}$ affected and invariant equations of motion, we obtain distinct coverage patterns for M87* and SgrA* that show extremely dense coverage on short baselines as well as long term orbital stability on longer baselines.","sentences":["Earth-based Very Long Baseline Interferometry (VLBI) has made rapid advances in imaging black holes.","However, due to the limitations imposed on terrestrial VLBI by the Earth's finite size and turbulent atmosphere, it is imperative to have a space-based component in future VLBI missions.","Herein, this paper investigates the effect of Earth's oblateness, also known as the $J_{2}$ effect, on orbiters in Earth-Space and Space-Space VLBI.","The paper provides an extensive discussion on how the $J_{2}$ effect can directly impact orbit selection for black hole observations and how through informed choices of orbital parameters, the effect can be used to the mission's advantage, a fact that has not been addressed in existing space-VLBI investigations.","We provide a comprehensive study of how the orbital parameters of several current space VLBI proposals will vary specifically due to the $J_{2}$ effect.","For black hole accretion flow targets of interest, we have demonstrated how the $J_{2}$ effect leads to modest increase in shorter baseline coverage, filling gaps in the $(u,v)$ plane.","Subsequently, we construct a simple analytical formalism that allows isolation of the impact of the $J_{2}$ effect on the $(u,v)$ plane without requiring computationally intensive orbit propagation simulations.","By directly constructing $(u,v)$ coverage using the $J_{2}$ affected and invariant equations of motion, we obtain distinct coverage patterns for M87* and SgrA* that show extremely dense coverage on short baselines as well as long term orbital stability on longer baselines."],"url":"http://arxiv.org/abs/2403.08606v1","category":"astro-ph.HE"}
{"created":"2024-03-13 15:13:44","title":"DevBench: A Comprehensive Benchmark for Software Development","abstract":"Recent advancements in large language models (LLMs) have significantly enhanced their coding capabilities. However, existing benchmarks predominantly focused on simplified or isolated aspects of programming, such as single-file code generation or repository issue debugging, falling short of measuring the full spectrum of challenges raised by real-world programming activities. To this end, we propose DevBench, a comprehensive benchmark that evaluates LLMs across various stages of the software development lifecycle, including software design, environment setup, implementation, acceptance testing, and unit testing. DevBench features a wide range of programming languages and domains, high-quality data collection, and carefully designed and verified metrics for each task. Empirical studies show that current LLMs, including GPT-4-Turbo, fail to solve the challenges presented within DevBench. Analyses reveal that models struggle with understanding the complex structures in the repository, managing the compilation process, and grasping advanced programming concepts. Our findings offer actionable insights for the future development of LLMs toward real-world programming applications. Our benchmark is available at https://github.com/open-compass/DevBench","sentences":["Recent advancements in large language models (LLMs) have significantly enhanced their coding capabilities.","However, existing benchmarks predominantly focused on simplified or isolated aspects of programming, such as single-file code generation or repository issue debugging, falling short of measuring the full spectrum of challenges raised by real-world programming activities.","To this end, we propose DevBench, a comprehensive benchmark that evaluates LLMs across various stages of the software development lifecycle, including software design, environment setup, implementation, acceptance testing, and unit testing.","DevBench features a wide range of programming languages and domains, high-quality data collection, and carefully designed and verified metrics for each task.","Empirical studies show that current LLMs, including GPT-4-Turbo, fail to solve the challenges presented within DevBench.","Analyses reveal that models struggle with understanding the complex structures in the repository, managing the compilation process, and grasping advanced programming concepts.","Our findings offer actionable insights for the future development of LLMs toward real-world programming applications.","Our benchmark is available at https://github.com/open-compass/DevBench"],"url":"http://arxiv.org/abs/2403.08604v1","category":"cs.CL"}
{"created":"2024-03-13 15:10:52","title":"Hyperbolic Anderson equations with general time-independent Gaussian noise: Stratonovich regime","abstract":"In this paper, we investigate the   hyperbolic Anderson equation   generated by a time-independent Gaussian noise   with two objectives: The solvability and intermittency.   First, we prove that Dalang's condition is necessary and sufficient   for existence of the solution. Second, we establish the   precise long time and high moment asymptotics for the solution under   the usual homogeneity assumption of the covariance of the Gaussian noise.   Our approach is fundamentally different from the ones existing in   literature.   The main contributions in our approach include the representation of Stratonovich   moment under Laplace transform via the moments of the Brownian motions in   Gaussian potentials   and some large deviation skills developed   in dealing effectively with the Stratonovich chaos expansion.","sentences":["In this paper, we investigate the   hyperbolic Anderson equation   generated by a time-independent Gaussian noise   with two objectives: The solvability and intermittency.   ","First, we prove that Dalang's condition is necessary and sufficient   for existence of the solution.","Second, we establish the   precise long time and high moment asymptotics for the solution under   the usual homogeneity assumption of the covariance of the Gaussian noise.   ","Our approach is fundamentally different from the ones existing in   literature.   ","The main contributions in our approach include the representation of Stratonovich   moment under Laplace transform via the moments of the Brownian motions in   Gaussian potentials   and some large deviation skills developed   in dealing effectively with the Stratonovich chaos expansion."],"url":"http://arxiv.org/abs/2403.08603v1","category":"math.PR"}
{"created":"2024-03-13 15:06:54","title":"Evaluation of Control/User-Plane Denial-of-Service (DoS) Attack on O-RAN Fronthaul Interface","abstract":"The open fronthaul interface defined by O-RAN ALLIANCE aims to support the interoperability between multi-vendor open radio access network (O-RAN) radio units (O-RU) and O-RAN distributed units (O-DU). This paper introduces a new tool that could be used to evaluate Denial-of-Service (DoS) attacks against the open fronthaul interface. We launched an array of control/user planes (C/U-Planes) attacks with the tool under different traffic types and data rates, and we evaluated their impacts on the throughput and block error rate (BLER) of real-world O-RAN systems with commercial hardware.","sentences":["The open fronthaul interface defined by O-RAN ALLIANCE aims to support the interoperability between multi-vendor open radio access network (O-RAN) radio units (O-RU) and O-RAN distributed units (O-DU).","This paper introduces a new tool that could be used to evaluate Denial-of-Service (DoS) attacks against the open fronthaul interface.","We launched an array of control/user planes (C/U-Planes) attacks with the tool under different traffic types and data rates, and we evaluated their impacts on the throughput and block error rate (BLER) of real-world O-RAN systems with commercial hardware."],"url":"http://arxiv.org/abs/2403.08600v1","category":"cs.NI"}
{"created":"2024-03-13 15:03:50","title":"Adaptive morphing of wing and tail for stable, resilient, and energy-efficient flight of avian-informed drones","abstract":"Avian-informed drones feature morphing wing and tail surfaces, enhancing agility and adaptability in flight. Despite their large potential, realising their full capabilities remains challenging due to the lack of generalized control strategies accommodating their large degrees of freedom and cross-coupling effects between their control surfaces. Here we propose a new body-rate controller for avian-informed drones that uses all available actuators to control the motion of the drone. The method exhibits robustness against physical perturbations, turbulent airflow, and even loss of certain actuators mid-flight. Furthermore, wing and tail morphing is leveraged to enhance energy efficiency at 8m/s, 10m/s and 12m/s using in-flight Bayesian optimization. The resulting morphing configurations yield significant gains across all three speeds of up to 11.5% compared to non-morphing configurations and display a strong resemblance to avian flight at different speeds. This research lays the groundwork for the development of autonomous avian-informed drones that operate under diverse wind conditions, emphasizing the role of morphing in improving energy efficiency.","sentences":["Avian-informed drones feature morphing wing and tail surfaces, enhancing agility and adaptability in flight.","Despite their large potential, realising their full capabilities remains challenging due to the lack of generalized control strategies accommodating their large degrees of freedom and cross-coupling effects between their control surfaces.","Here we propose a new body-rate controller for avian-informed drones that uses all available actuators to control the motion of the drone.","The method exhibits robustness against physical perturbations, turbulent airflow, and even loss of certain actuators mid-flight.","Furthermore, wing and tail morphing is leveraged to enhance energy efficiency at 8m/s, 10m/s and 12m/s using in-flight Bayesian optimization.","The resulting morphing configurations yield significant gains across all three speeds of up to 11.5% compared to non-morphing configurations and display a strong resemblance to avian flight at different speeds.","This research lays the groundwork for the development of autonomous avian-informed drones that operate under diverse wind conditions, emphasizing the role of morphing in improving energy efficiency."],"url":"http://arxiv.org/abs/2403.08598v1","category":"cs.RO"}
{"created":"2024-03-13 15:03:30","title":"Hamiltonian Boundary Value Methods (HBVMs) for functional differential equations with piecewise continuous arguments","abstract":"In this paper, a class of high-order methods to numerically solve Functional Differential Equations with Piecewise Continuous Arguments (FDEPCAs) is discussed. The framework stems from the expansion of the vector field associated with the reference differential equation along the shifted and scaled Legendre polynomial orthonormal basis, working on a suitable extension of Hamiltonian Boundary Value Methods. Within the design of the methods, a proper generalization of the perturbation results coming from the field of ordinary differential equations is considered, with the aim of handling the case of FDEPCAs. The error analysis of the devised family of methods is performed, while a few numerical tests on Hamiltonian FDEPCAs are provided, to give evidence to the theoretical findings and show the effectiveness of the obtained resolution strategy.","sentences":["In this paper, a class of high-order methods to numerically solve Functional Differential Equations with Piecewise Continuous Arguments (FDEPCAs) is discussed.","The framework stems from the expansion of the vector field associated with the reference differential equation along the shifted and scaled Legendre polynomial orthonormal basis, working on a suitable extension of Hamiltonian Boundary Value Methods.","Within the design of the methods, a proper generalization of the perturbation results coming from the field of ordinary differential equations is considered, with the aim of handling the case of FDEPCAs.","The error analysis of the devised family of methods is performed, while a few numerical tests on Hamiltonian FDEPCAs are provided, to give evidence to the theoretical findings and show the effectiveness of the obtained resolution strategy."],"url":"http://arxiv.org/abs/2403.08597v1","category":"math.NA"}
{"created":"2024-03-13 15:03:13","title":"Patching-based Deep Learning model for the Inpainting of Bragg Coherent Diffraction patterns affected by detectors' gaps","abstract":"We propose a deep learning algorithm for the inpainting of Bragg Coherent Diffraction Imaging (BCDI) patterns affected by detector gaps. These regions of missing intensity can compromise the accuracy of reconstruction algorithms, inducing artifacts in the final result. It is thus desirable to restore the intensity in these regions in order to ensure more reliable reconstructions. The key aspect of our method lies in the choice of training the neural network with cropped sections of both experimental diffraction data and simulated data and subsequently patching the predictions generated by the model along the gap, thus completing the full diffraction peak. This provides us with more experimental training data and allows for a faster model training due to the limited size, while the neural network can be applied to arbitrarily larger BCDI datasets. Moreover, our method not only broadens the scope of application but also ensures the preservation of data integrity and reliability in the face of challenging experimental conditions.","sentences":["We propose a deep learning algorithm for the inpainting of Bragg Coherent Diffraction Imaging (BCDI) patterns affected by detector gaps.","These regions of missing intensity can compromise the accuracy of reconstruction algorithms, inducing artifacts in the final result.","It is thus desirable to restore the intensity in these regions in order to ensure more reliable reconstructions.","The key aspect of our method lies in the choice of training the neural network with cropped sections of both experimental diffraction data and simulated data and subsequently patching the predictions generated by the model along the gap, thus completing the full diffraction peak.","This provides us with more experimental training data and allows for a faster model training due to the limited size, while the neural network can be applied to arbitrarily larger BCDI datasets.","Moreover, our method not only broadens the scope of application but also ensures the preservation of data integrity and reliability in the face of challenging experimental conditions."],"url":"http://arxiv.org/abs/2403.08596v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-13 14:59:07","title":"Call Me When Necessary: LLMs can Efficiently and Faithfully Reason over Structured Environments","abstract":"Large Language Models (LLMs) have shown potential in reasoning over structured environments, e.g., knowledge graph and table. Such tasks typically require multi-hop reasoning, i.e., match natural language utterance with instances in the environment. Previous methods leverage LLMs to incrementally build a reasoning path, where the LLMs either invoke tools or pick up schemas by step-by-step interacting with the environment. We propose Reasoning-Path-Editing (Readi), a novel framework where LLMs can efficiently and faithfully reason over structured environments. In Readi, LLMs initially generate a reasoning path given a query, and edit the path only when necessary. We instantiate the path on structured environments and provide feedback to edit the path if anything goes wrong. Experimental results on three KGQA datasets and two TableQA datasets show the effectiveness of Readi, significantly surpassing all LLM-based methods (by 9.1% on WebQSP, 12.4% on MQA-3H and 10.9% on WTQ), comparable with state-of-the-art fine-tuned methods (67% on CWQ and 74.7% on WebQSP) and substantially boosting the vanilla LLMs (by 14.9% on CWQ). Our code will be available upon publication.","sentences":["Large Language Models (LLMs) have shown potential in reasoning over structured environments, e.g., knowledge graph and table.","Such tasks typically require multi-hop reasoning, i.e., match natural language utterance with instances in the environment.","Previous methods leverage LLMs to incrementally build a reasoning path, where the LLMs either invoke tools or pick up schemas by step-by-step interacting with the environment.","We propose Reasoning-Path-Editing (Readi), a novel framework where LLMs can efficiently and faithfully reason over structured environments.","In Readi, LLMs initially generate a reasoning path given a query, and edit the path only when necessary.","We instantiate the path on structured environments and provide feedback to edit the path if anything goes wrong.","Experimental results on three KGQA datasets and two TableQA datasets show the effectiveness of Readi, significantly surpassing all LLM-based methods (by 9.1% on WebQSP, 12.4% on MQA-3H and 10.9% on WTQ), comparable with state-of-the-art fine-tuned methods (67% on CWQ and 74.7% on WebQSP) and substantially boosting the vanilla LLMs (by 14.9% on CWQ).","Our code will be available upon publication."],"url":"http://arxiv.org/abs/2403.08593v1","category":"cs.CL"}
{"created":"2024-03-13 14:57:10","title":"Data-Efficient Sleep Staging with Synthetic Time Series Pretraining","abstract":"Analyzing electroencephalographic (EEG) time series can be challenging, especially with deep neural networks, due to the large variability among human subjects and often small datasets. To address these challenges, various strategies, such as self-supervised learning, have been suggested, but they typically rely on extensive empirical datasets. Inspired by recent advances in computer vision, we propose a pretraining task termed \"frequency pretraining\" to pretrain a neural network for sleep staging by predicting the frequency content of randomly generated synthetic time series. Our experiments demonstrate that our method surpasses fully supervised learning in scenarios with limited data and few subjects, and matches its performance in regimes with many subjects. Furthermore, our results underline the relevance of frequency information for sleep stage scoring, while also demonstrating that deep neural networks utilize information beyond frequencies to enhance sleep staging performance, which is consistent with previous research. We anticipate that our approach will be advantageous across a broad spectrum of applications where EEG data is limited or derived from a small number of subjects, including the domain of brain-computer interfaces.","sentences":["Analyzing electroencephalographic (EEG) time series can be challenging, especially with deep neural networks, due to the large variability among human subjects and often small datasets.","To address these challenges, various strategies, such as self-supervised learning, have been suggested, but they typically rely on extensive empirical datasets.","Inspired by recent advances in computer vision, we propose a pretraining task termed \"frequency pretraining\" to pretrain a neural network for sleep staging by predicting the frequency content of randomly generated synthetic time series.","Our experiments demonstrate that our method surpasses fully supervised learning in scenarios with limited data and few subjects, and matches its performance in regimes with many subjects.","Furthermore, our results underline the relevance of frequency information for sleep stage scoring, while also demonstrating that deep neural networks utilize information beyond frequencies to enhance sleep staging performance, which is consistent with previous research.","We anticipate that our approach will be advantageous across a broad spectrum of applications where EEG data is limited or derived from a small number of subjects, including the domain of brain-computer interfaces."],"url":"http://arxiv.org/abs/2403.08592v1","category":"cs.LG"}
{"created":"2024-03-13 14:51:16","title":"Can physical information aid the generalization ability of Neural Networks for hydraulic modeling?","abstract":"Application of Neural Networks to river hydraulics is fledgling, despite the field suffering from data scarcity, a challenge for machine learning techniques. Consequently, many purely data-driven Neural Networks proved to lack predictive capabilities. In this work, we propose to mitigate such problem by introducing physical information into the training phase. The idea is borrowed from Physics-Informed Neural Networks which have been recently proposed in other contexts. Physics-Informed Neural Networks embed physical information in the form of the residual of the Partial Differential Equations (PDEs) governing the phenomenon and, as such, are conceived as neural solvers, i.e. an alternative to traditional numerical solvers. Such approach is seldom suitable for environmental hydraulics, where epistemic uncertainties are large, and computing residuals of PDEs exhibits difficulties similar to those faced by classical numerical methods. Instead, we envisaged the employment of Neural Networks as neural operators, featuring physical constraints formulated without resorting to PDEs. The proposed novel methodology shares similarities with data augmentation and regularization. We show that incorporating such soft physical information can improve predictive capabilities.","sentences":["Application of Neural Networks to river hydraulics is fledgling, despite the field suffering from data scarcity, a challenge for machine learning techniques.","Consequently, many purely data-driven Neural Networks proved to lack predictive capabilities.","In this work, we propose to mitigate such problem by introducing physical information into the training phase.","The idea is borrowed from Physics-Informed Neural Networks which have been recently proposed in other contexts.","Physics-Informed Neural Networks embed physical information in the form of the residual of the Partial Differential Equations (PDEs) governing the phenomenon and, as such, are conceived as neural solvers, i.e. an alternative to traditional numerical solvers.","Such approach is seldom suitable for environmental hydraulics, where epistemic uncertainties are large, and computing residuals of PDEs exhibits difficulties similar to those faced by classical numerical methods.","Instead, we envisaged the employment of Neural Networks as neural operators, featuring physical constraints formulated without resorting to PDEs.","The proposed novel methodology shares similarities with data augmentation and regularization.","We show that incorporating such soft physical information can improve predictive capabilities."],"url":"http://arxiv.org/abs/2403.08589v1","category":"cs.LG"}
{"created":"2024-03-13 14:42:06","title":"Improving Implicit Regularization of SGD with Preconditioning for Least Square Problems","abstract":"Stochastic gradient descent (SGD) exhibits strong algorithmic regularization effects in practice and plays an important role in the generalization of modern machine learning. However, prior research has revealed instances where the generalization performance of SGD is worse than ridge regression due to uneven optimization along different dimensions. Preconditioning offers a natural solution to this issue by rebalancing optimization across different directions. Yet, the extent to which preconditioning can enhance the generalization performance of SGD and whether it can bridge the existing gap with ridge regression remains uncertain. In this paper, we study the generalization performance of SGD with preconditioning for the least squared problem. We make a comprehensive comparison between preconditioned SGD and (standard \\& preconditioned) ridge regression. Our study makes several key contributions toward understanding and improving SGD with preconditioning. First, we establish excess risk bounds (generalization performance) for preconditioned SGD and ridge regression under an arbitrary preconditions matrix. Second, leveraging the excessive risk characterization of preconditioned SGD and ridge regression, we show that (through construction) there exists a simple preconditioned matrix that can outperform (standard \\& preconditioned) ridge regression. Finally, we show that our proposed preconditioning matrix is straightforward enough to allow robust estimation from finite samples while maintaining a theoretical advantage over ridge regression. Our empirical results align with our theoretical findings, collectively showcasing the enhanced regularization effect of preconditioned SGD.","sentences":["Stochastic gradient descent (SGD) exhibits strong algorithmic regularization effects in practice and plays an important role in the generalization of modern machine learning.","However, prior research has revealed instances where the generalization performance of SGD is worse than ridge regression due to uneven optimization along different dimensions.","Preconditioning offers a natural solution to this issue by rebalancing optimization across different directions.","Yet, the extent to which preconditioning can enhance the generalization performance of SGD and whether it can bridge the existing gap with ridge regression remains uncertain.","In this paper, we study the generalization performance of SGD with preconditioning for the least squared problem.","We make a comprehensive comparison between preconditioned SGD and (standard \\& preconditioned) ridge regression.","Our study makes several key contributions toward understanding and improving SGD with preconditioning.","First, we establish excess risk bounds (generalization performance) for preconditioned SGD and ridge regression under an arbitrary preconditions matrix.","Second, leveraging the excessive risk characterization of preconditioned SGD and ridge regression, we show that (through construction) there exists a simple preconditioned matrix that can outperform (standard \\& preconditioned) ridge regression.","Finally, we show that our proposed preconditioning matrix is straightforward enough to allow robust estimation from finite samples while maintaining a theoretical advantage over ridge regression.","Our empirical results align with our theoretical findings, collectively showcasing the enhanced regularization effect of preconditioned SGD."],"url":"http://arxiv.org/abs/2403.08585v1","category":"cs.LG"}
{"created":"2024-03-13 14:35:13","title":"Leveraging Compressed Frame Sizes For Ultra-Fast Video Classification","abstract":"Classifying videos into distinct categories, such as Sport and Music Video, is crucial for multimedia understanding and retrieval, especially when an immense volume of video content is being constantly generated. Traditional methods require video decompression to extract pixel-level features like color, texture, and motion, thereby increasing computational and storage demands. Moreover, these methods often suffer from performance degradation in low-quality videos. We present a novel approach that examines only the post-compression bitstream of a video to perform classification, eliminating the need for bitstream decoding. To validate our approach, we built a comprehensive data set comprising over 29,000 YouTube video clips, totaling 6,000 hours and spanning 11 distinct categories. Our evaluations indicate precision, accuracy, and recall rates consistently above 80%, many exceeding 90%, and some reaching 99%. The algorithm operates approximately 15,000 times faster than real-time for 30fps videos, outperforming traditional Dynamic Time Warping (DTW) algorithm by seven orders of magnitude.","sentences":["Classifying videos into distinct categories, such as Sport and Music Video, is crucial for multimedia understanding and retrieval, especially when an immense volume of video content is being constantly generated.","Traditional methods require video decompression to extract pixel-level features like color, texture, and motion, thereby increasing computational and storage demands.","Moreover, these methods often suffer from performance degradation in low-quality videos.","We present a novel approach that examines only the post-compression bitstream of a video to perform classification, eliminating the need for bitstream decoding.","To validate our approach, we built a comprehensive data set comprising over 29,000 YouTube video clips, totaling 6,000 hours and spanning 11 distinct categories.","Our evaluations indicate precision, accuracy, and recall rates consistently above 80%, many exceeding 90%, and some reaching 99%.","The algorithm operates approximately 15,000 times faster than real-time for 30fps videos, outperforming traditional Dynamic Time Warping (DTW) algorithm by seven orders of magnitude."],"url":"http://arxiv.org/abs/2403.08580v1","category":"cs.CV"}
{"created":"2024-03-13 14:34:34","title":"Machine Learning Optimized Orthogonal Basis Piecewise Polynomial Approximation","abstract":"Piecewise Polynomials (PPs) are utilized in several engineering disciplines, like trajectory planning, to approximate position profiles given in the form of a set of points. While the approximation target along with domain-specific requirements, like Ck -continuity, can be formulated as a system of equations and a result can be computed directly, such closed-form solutions posses limited flexibility with respect to polynomial degrees, polynomial bases or adding further domain-specific requirements. Sufficiently complex optimization goals soon call for the use of numerical methods, like gradient descent. Since gradient descent lies at the heart of training Artificial Neural Networks (ANNs), modern Machine Learning (ML) frameworks like TensorFlow come with a set of gradient-based optimizers potentially suitable for a wide range of optimization problems beyond the training task for ANNs. Our approach is to utilize the versatility of PP models and combine it with the potential of modern ML optimizers for the use in function approximation in 1D trajectory planning in the context of electronic cam design. We utilize available optimizers of the ML framework TensorFlow directly, outside of the scope of ANNs, to optimize model parameters of our PP model. In this paper, we show how an orthogonal polynomial basis contributes to improving approximation and continuity optimization performance. Utilizing Chebyshev polynomials of the first kind, we develop a novel regularization approach enabling clearly improved convergence behavior. We show that, using this regularization approach, Chebyshev basis performs better than power basis for all relevant optimizers in the combined approximation and continuity optimization setting and demonstrate usability of the presented approach within the electronic cam domain.","sentences":["Piecewise Polynomials (PPs) are utilized in several engineering disciplines, like trajectory planning, to approximate position profiles given in the form of a set of points.","While the approximation target along with domain-specific requirements, like Ck -continuity, can be formulated as a system of equations and a result can be computed directly, such closed-form solutions posses limited flexibility with respect to polynomial degrees, polynomial bases or adding further domain-specific requirements.","Sufficiently complex optimization goals soon call for the use of numerical methods, like gradient descent.","Since gradient descent lies at the heart of training Artificial Neural Networks (ANNs), modern Machine Learning (ML) frameworks like TensorFlow come with a set of gradient-based optimizers potentially suitable for a wide range of optimization problems beyond the training task for ANNs.","Our approach is to utilize the versatility of PP models and combine it with the potential of modern ML optimizers for the use in function approximation in 1D trajectory planning in the context of electronic cam design.","We utilize available optimizers of the ML framework TensorFlow directly, outside of the scope of ANNs, to optimize model parameters of our PP model.","In this paper, we show how an orthogonal polynomial basis contributes to improving approximation and continuity optimization performance.","Utilizing Chebyshev polynomials of the first kind, we develop a novel regularization approach enabling clearly improved convergence behavior.","We show that, using this regularization approach, Chebyshev basis performs better than power basis for all relevant optimizers in the combined approximation and continuity optimization setting and demonstrate usability of the presented approach within the electronic cam domain."],"url":"http://arxiv.org/abs/2403.08579v1","category":"cs.LG"}
{"created":"2024-03-13 14:29:53","title":"Global solutions of the one-dimensional compressible Euler equations with nonlocal interactions via the inviscid limit","abstract":"We are concerned with the global existence of finite-energy entropy solutions of the one-dimensional compressible Euler equations with (possibly) damping, alignment forces, and nonlocal interactions: Newtonian repulsion and quadratic confinement. Both the polytropic gas law and the general gas law are analyzed. This is achieved by constructing a sequence of solutions of the one-dimensional compressible Navier-Stokes-type equations with density-dependent viscosity under the stress-free boundary condition and then taking the vanishing viscosity limit. The main difficulties in this paper arise from the appearance of the nonlocal terms. In particular, some uniform higher moment estimates for the compressible Navier-Stokes equations on expanding intervals with stress-free boundary conditions are obtained by careful design of the approximate initial data.","sentences":["We are concerned with the global existence of finite-energy entropy solutions of the one-dimensional compressible Euler equations with (possibly) damping, alignment forces, and nonlocal interactions: Newtonian repulsion and quadratic confinement.","Both the polytropic gas law and the general gas law are analyzed.","This is achieved by constructing a sequence of solutions of the one-dimensional compressible Navier-Stokes-type equations with density-dependent viscosity under the stress-free boundary condition and then taking the vanishing viscosity limit.","The main difficulties in this paper arise from the appearance of the nonlocal terms.","In particular, some uniform higher moment estimates for the compressible Navier-Stokes equations on expanding intervals with stress-free boundary conditions are obtained by careful design of the approximate initial data."],"url":"http://arxiv.org/abs/2403.08576v1","category":"math.AP"}
{"created":"2024-03-13 14:22:25","title":"Quenched CLT for ancestral lineages of logistic branching random walks","abstract":"We consider random walks in dynamic random environments which arise naturally as spatial embeddings of ancestral lineages in spatial locally regulated population models. In particular, as the main result, we prove the quenched central limit theorem for a random walk in dynamic random environment generated by time reversal of logistic branching random walks in a regime where the population density is sufficiently high. As an important tool we consider as auxiliary models random walks in dynamic random environments defined in terms of the time-reversal of oriented percolation. We show that the quenched central limit theorem holds if the influence of the random medium on the walks is suitably weak. The proofs of the quenched central limit theorems in these models rely on coarse-graining arguments and a construction of regeneration times for a pair of conditionally independent random walks in the same medium, combined with a coupling that relates them to a pair of independent random walks in two independent copies of the medium.","sentences":["We consider random walks in dynamic random environments which arise naturally as spatial embeddings of ancestral lineages in spatial locally regulated population models.","In particular, as the main result, we prove the quenched central limit theorem for a random walk in dynamic random environment generated by time reversal of logistic branching random walks in a regime where the population density is sufficiently high.","As an important tool we consider as auxiliary models random walks in dynamic random environments defined in terms of the time-reversal of oriented percolation.","We show that the quenched central limit theorem holds if the influence of the random medium on the walks is suitably weak.","The proofs of the quenched central limit theorems in these models rely on coarse-graining arguments and a construction of regeneration times for a pair of conditionally independent random walks in the same medium, combined with a coupling that relates them to a pair of independent random walks in two independent copies of the medium."],"url":"http://arxiv.org/abs/2403.08567v1","category":"math.PR"}
{"created":"2024-03-13 14:19:38","title":"Deep Learning based Positioning with Multi-task Learning and Uncertainty-based Fusion","abstract":"Deep learning (DL) methods have been shown to improve the performance of several use cases for the fifth-generation (5G) New radio (NR) air interface. In this paper we investigate user equipment (UE) positioning using the channel state information (CSI) fingerprints between a UE and multiple base stations (BSs). In such a setup, a single DL model can be trained for UE positioning using the CSI fingerprints of the multiple BSs as input. Alternatively, based on the CSI at each BS, a separate DL model can be trained at each BS and then the output of the different models are combined to determine the UE's position. In this work we compare these different fusion techniques and show that fusing the output of separate models achieves higher positioning accuracy, especially in a dynamic scenario. We also show that the fusion of multiple outputs further benefits from considering the uncertainty of the output of the DL model at each BS. For a more efficient training of the DL model across BSs, we additionally propose a multi-task learning (MTL) scheme by sharing some parameters across the models while jointly training all models. This method, not only improves the accuracy of the individual models, but also of the final combined estimate. Lastly, we evaluate the reliability of the uncertainty estimation to ascertain which of the fusion methods provides the highest quality of uncertainty estimates.","sentences":["Deep learning (DL) methods have been shown to improve the performance of several use cases for the fifth-generation (5G) New radio (NR) air interface.","In this paper we investigate user equipment (UE) positioning using the channel state information (CSI) fingerprints between a UE and multiple base stations (BSs).","In such a setup, a single DL model can be trained for UE positioning using the CSI fingerprints of the multiple BSs as input.","Alternatively, based on the CSI at each BS, a separate DL model can be trained at each BS and then the output of the different models are combined to determine the UE's position.","In this work we compare these different fusion techniques and show that fusing the output of separate models achieves higher positioning accuracy, especially in a dynamic scenario.","We also show that the fusion of multiple outputs further benefits from considering the uncertainty of the output of the DL model at each BS.","For a more efficient training of the DL model across BSs, we additionally propose a multi-task learning (MTL) scheme by sharing some parameters across the models while jointly training all models.","This method, not only improves the accuracy of the individual models, but also of the final combined estimate.","Lastly, we evaluate the reliability of the uncertainty estimation to ascertain which of the fusion methods provides the highest quality of uncertainty estimates."],"url":"http://arxiv.org/abs/2403.08565v1","category":"eess.SP"}
{"created":"2024-03-13 14:19:08","title":"Non-discrimination Criteria for Generative Language Models","abstract":"Within recent years, generative AI, such as large language models, has undergone rapid development. As these models become increasingly available to the public, concerns arise about perpetuating and amplifying harmful biases in applications. Gender stereotypes can be harmful and limiting for the individuals they target, whether they consist of misrepresentation or discrimination. Recognizing gender bias as a pervasive societal construct, this paper studies how to uncover and quantify the presence of gender biases in generative language models. In particular, we derive generative AI analogues of three well-known non-discrimination criteria from classification, namely independence, separation and sufficiency. To demonstrate these criteria in action, we design prompts for each of the criteria with a focus on occupational gender stereotype, specifically utilizing the medical test to introduce the ground truth in the generative AI context. Our results address the presence of occupational gender bias within such conversational language models.","sentences":["Within recent years, generative AI, such as large language models, has undergone rapid development.","As these models become increasingly available to the public, concerns arise about perpetuating and amplifying harmful biases in applications.","Gender stereotypes can be harmful and limiting for the individuals they target, whether they consist of misrepresentation or discrimination.","Recognizing gender bias as a pervasive societal construct, this paper studies how to uncover and quantify the presence of gender biases in generative language models.","In particular, we derive generative AI analogues of three well-known non-discrimination criteria from classification, namely independence, separation and sufficiency.","To demonstrate these criteria in action, we design prompts for each of the criteria with a focus on occupational gender stereotype, specifically utilizing the medical test to introduce the ground truth in the generative AI context.","Our results address the presence of occupational gender bias within such conversational language models."],"url":"http://arxiv.org/abs/2403.08564v1","category":"cs.CL"}
{"created":"2024-03-13 14:17:34","title":"Distributed Deep Learning for Modulation Classification in 6G Cell-Free Wireless Networks","abstract":"In the evolution of 6th Generation (6G) technology, the emergence of cell-free networking presents a paradigm shift, revolutionizing user experiences within densely deployed networks where distributed access points collaborate. However, the integration of intelligent mechanisms is crucial for optimizing the efficiency, scalability, and adaptability of these 6G cell-free networks. One application aiming to optimize spectrum usage is Automatic Modulation Classification (AMC), a vital component for classifying and dynamically adjusting modulation schemes. This paper explores different distributed solutions for AMC in cell-free networks, addressing the training, computational complexity, and accuracy of two practical approaches. The first approach addresses scenarios where signal sharing is not feasible due to privacy concerns or fronthaul limitations. Our findings reveal that maintaining comparable accuracy is remarkably achievable, yet it comes with an increase in computational demand. The second approach considers a central model and multiple distributed models collaboratively classifying the modulation. This hybrid model leverages diversity gain through signal combining and requires synchronization and signal sharing. The hybrid model demonstrates superior performance, achieving a 2.5% improvement in accuracy with equivalent total computational load. Notably, the hybrid model distributes the computational load across multiple devices, resulting in a lower individual computational load.","sentences":["In the evolution of 6th Generation (6G) technology, the emergence of cell-free networking presents a paradigm shift, revolutionizing user experiences within densely deployed networks where distributed access points collaborate.","However, the integration of intelligent mechanisms is crucial for optimizing the efficiency, scalability, and adaptability of these 6G cell-free networks.","One application aiming to optimize spectrum usage is Automatic Modulation Classification (AMC), a vital component for classifying and dynamically adjusting modulation schemes.","This paper explores different distributed solutions for AMC in cell-free networks, addressing the training, computational complexity, and accuracy of two practical approaches.","The first approach addresses scenarios where signal sharing is not feasible due to privacy concerns or fronthaul limitations.","Our findings reveal that maintaining comparable accuracy is remarkably achievable, yet it comes with an increase in computational demand.","The second approach considers a central model and multiple distributed models collaboratively classifying the modulation.","This hybrid model leverages diversity gain through signal combining and requires synchronization and signal sharing.","The hybrid model demonstrates superior performance, achieving a 2.5% improvement in accuracy with equivalent total computational load.","Notably, the hybrid model distributes the computational load across multiple devices, resulting in a lower individual computational load."],"url":"http://arxiv.org/abs/2403.08563v1","category":"eess.SP"}
{"created":"2024-03-13 14:14:47","title":"Structural perspective on constraint-based learning of Markov networks","abstract":"Markov networks are probabilistic graphical models that employ undirected graphs to depict conditional independence relationships among variables. Our focus lies in constraint-based structure learning, which entails learning the undirected graph from data through the execution of conditional independence tests. We establish theoretical limits concerning two critical aspects of constraint-based learning of Markov networks: the number of tests and the sizes of the conditioning sets. These bounds uncover an exciting interplay between the structural properties of the graph and the amount of tests required to learn a Markov network. The starting point of our work is that the graph parameter maximum pairwise connectivity, $\\kappa$, that is, the maximum number of vertex-disjoint paths connecting a pair of vertices in the graph, is responsible for the sizes of independence tests required to learn the graph. On one hand, we show that at least one test with the size of the conditioning set at least $\\kappa$ is always necessary. On the other hand, we prove that any graph can be learned by performing tests of size at most $\\kappa$. This completely resolves the question of the minimum size of conditioning sets required to learn the graph. When it comes to the number of tests, our upper bound on the sizes of conditioning sets implies that every $n$-vertex graph can be learned by at most $n^{\\kappa}$ tests with conditioning sets of sizes at most $\\kappa$. We show that for any upper bound $q$ on the sizes of the conditioning sets, there exist graphs with $O(n q)$ vertices that require at least $n^{\\Omega(\\kappa)}$ tests to learn. This lower bound holds even when the treewidth and the maximum degree of the graph are at most $\\kappa+2$. On the positive side, we prove that every graph of bounded treewidth can be learned by a polynomial number of tests with conditioning sets of sizes at most $2\\kappa$.","sentences":["Markov networks are probabilistic graphical models that employ undirected graphs to depict conditional independence relationships among variables.","Our focus lies in constraint-based structure learning, which entails learning the undirected graph from data through the execution of conditional independence tests.","We establish theoretical limits concerning two critical aspects of constraint-based learning of Markov networks: the number of tests and the sizes of the conditioning sets.","These bounds uncover an exciting interplay between the structural properties of the graph and the amount of tests required to learn a Markov network.","The starting point of our work is that the graph parameter maximum pairwise connectivity, $\\kappa$, that is, the maximum number of vertex-disjoint paths connecting a pair of vertices in the graph, is responsible for the sizes of independence tests required to learn the graph.","On one hand, we show that at least one test with the size of the conditioning set at least $\\kappa$ is always necessary.","On the other hand, we prove that any graph can be learned by performing tests of size at most $\\kappa$. This completely resolves the question of the minimum size of conditioning sets required to learn the graph.","When it comes to the number of tests, our upper bound on the sizes of conditioning sets implies that every $n$-vertex graph can be learned by at most $n^{\\kappa}$ tests with conditioning sets of sizes at most $\\kappa$. We show that for any upper bound $q$ on the sizes of the conditioning sets, there exist graphs with $O(n q)$ vertices that require at least $n^{\\Omega(\\kappa)}$ tests to learn.","This lower bound holds even when the treewidth and the maximum degree of the graph are at most $\\kappa+2$. On the positive side, we prove that every graph of bounded treewidth can be learned by a polynomial number of tests with conditioning sets of sizes at most $2\\kappa$."],"url":"http://arxiv.org/abs/2403.08562v1","category":"cs.LG"}
{"created":"2024-03-13 14:08:25","title":"SM4Depth: Seamless Monocular Metric Depth Estimation across Multiple Cameras and Scenes by One Model","abstract":"The generalization of monocular metric depth estimation (MMDE) has been a longstanding challenge. Recent methods made progress by combining relative and metric depth or aligning input image focal length. However, they are still beset by challenges in camera, scene, and data levels: (1) Sensitivity to different cameras; (2) Inconsistent accuracy across scenes; (3) Reliance on massive training data. This paper proposes SM4Depth, a seamless MMDE method, to address all the issues above within a single network. First, we reveal that a consistent field of view (FOV) is the key to resolve ``metric ambiguity'' across cameras, which guides us to propose a more straightforward preprocessing unit. Second, to achieve consistently high accuracy across scenes, we explicitly model the metric scale determination as discretizing the depth interval into bins and propose variation-based unnormalized depth bins. This method bridges the depth gap of diverse scenes by reducing the ambiguity of the conventional metric bin. Third, to reduce the reliance on massive training data, we propose a ``divide and conquer\" solution. Instead of estimating directly from the vast solution space, the correct metric bins are estimated from multiple solution sub-spaces for complexity reduction. Finally, with just 150K RGB-D pairs and a consumer-grade GPU for training, SM4Depth achieves state-of-the-art performance on most previously unseen datasets, especially surpassing ZoeDepth and Metric3D on mRI$_\\theta$. The code can be found at https://github.com/1hao-Liu/SM4Depth.","sentences":["The generalization of monocular metric depth estimation (MMDE) has been a longstanding challenge.","Recent methods made progress by combining relative and metric depth or aligning input image focal length.","However, they are still beset by challenges in camera, scene, and data levels: (1) Sensitivity to different cameras; (2) Inconsistent accuracy across scenes; (3) Reliance on massive training data.","This paper proposes SM4Depth, a seamless MMDE method, to address all the issues above within a single network.","First, we reveal that a consistent field of view (FOV) is the key to resolve ``metric ambiguity'' across cameras, which guides us to propose a more straightforward preprocessing unit.","Second, to achieve consistently high accuracy across scenes, we explicitly model the metric scale determination as discretizing the depth interval into bins and propose variation-based unnormalized depth bins.","This method bridges the depth gap of diverse scenes by reducing the ambiguity of the conventional metric bin.","Third, to reduce the reliance on massive training data, we propose a ``divide and conquer\" solution.","Instead of estimating directly from the vast solution space, the correct metric bins are estimated from multiple solution sub-spaces for complexity reduction.","Finally, with just 150K RGB-D pairs and a consumer-grade GPU for training, SM4Depth achieves state-of-the-art performance on most previously unseen datasets, especially surpassing ZoeDepth and Metric3D on mRI$_\\theta$. The code can be found at https://github.com/1hao-Liu/SM4Depth."],"url":"http://arxiv.org/abs/2403.08556v1","category":"cs.CV"}
{"created":"2024-03-13 14:06:51","title":"Federated Knowledge Graph Unlearning via Diffusion Model","abstract":"Federated learning (FL) promotes the development and application of artificial intelligence technologies by enabling model sharing and collaboration while safeguarding data privacy. Knowledge graph (KG) embedding representation provides a foundation for knowledge reasoning and applications by mapping entities and relations into vector space. Federated KG embedding enables the utilization of knowledge from diverse client sources while safeguarding the privacy of local data. However, due to demands such as privacy protection and the need to adapt to dynamic data changes, investigations into machine unlearning (MU) have been sparked. However, it is challenging to maintain the performance of KG embedding models while forgetting the influence of specific forgotten data on the model. In this paper, we propose FedDM, a novel framework tailored for machine unlearning in federated knowledge graphs. Leveraging diffusion models, we generate noisy data to sensibly mitigate the influence of specific knowledge on FL models while preserving the overall performance concerning the remaining data. We conduct experimental evaluations on benchmark datasets to assess the efficacy of the proposed model. Extensive experiments demonstrate that FedDM yields promising results in knowledge forgetting.","sentences":["Federated learning (FL) promotes the development and application of artificial intelligence technologies by enabling model sharing and collaboration while safeguarding data privacy.","Knowledge graph (KG) embedding representation provides a foundation for knowledge reasoning and applications by mapping entities and relations into vector space.","Federated KG embedding enables the utilization of knowledge from diverse client sources while safeguarding the privacy of local data.","However, due to demands such as privacy protection and the need to adapt to dynamic data changes, investigations into machine unlearning (MU) have been sparked.","However, it is challenging to maintain the performance of KG embedding models while forgetting the influence of specific forgotten data on the model.","In this paper, we propose FedDM, a novel framework tailored for machine unlearning in federated knowledge graphs.","Leveraging diffusion models, we generate noisy data to sensibly mitigate the influence of specific knowledge on FL models while preserving the overall performance concerning the remaining data.","We conduct experimental evaluations on benchmark datasets to assess the efficacy of the proposed model.","Extensive experiments demonstrate that FedDM yields promising results in knowledge forgetting."],"url":"http://arxiv.org/abs/2403.08554v1","category":"cs.LG"}
{"created":"2024-03-13 14:02:54","title":"GaussianImage: 1000 FPS Image Representation and Compression by 2D Gaussian Splatting","abstract":"Implicit neural representations (INRs) recently achieved great success in image representation and compression, offering high visual quality and fast rendering speeds with 10-1000 FPS, assuming sufficient GPU resources are available. However, this requirement often hinders their use on low-end devices with limited memory. In response, we propose a groundbreaking paradigm of image representation and compression by 2D Gaussian Splatting, named GaussianImage. We first introduce 2D Gaussian to represent the image, where each Gaussian has 8 parameters including position, covariance and color. Subsequently, we unveil a novel rendering algorithm based on accumulated summation. Remarkably, our method with a minimum of 3$\\times$ lower GPU memory usage and 5$\\times$ faster fitting time not only rivals INRs (e.g., WIRE, I-NGP) in representation performance, but also delivers a faster rendering speed of 1500-2000 FPS regardless of parameter size. Furthermore, we integrate existing vector quantization technique to build an image codec. Experimental results demonstrate that our codec attains rate-distortion performance comparable to compression-based INRs such as COIN and COIN++, while facilitating decoding speeds of approximately 1000 FPS. Additionally, preliminary proof of concept shows that our codec surpasses COIN and COIN++ in performance when using partial bits-back coding.","sentences":["Implicit neural representations (INRs) recently achieved great success in image representation and compression, offering high visual quality and fast rendering speeds with 10-1000 FPS, assuming sufficient GPU resources are available.","However, this requirement often hinders their use on low-end devices with limited memory.","In response, we propose a groundbreaking paradigm of image representation and compression by 2D Gaussian Splatting, named GaussianImage.","We first introduce 2D Gaussian to represent the image, where each Gaussian has 8 parameters including position, covariance and color.","Subsequently, we unveil a novel rendering algorithm based on accumulated summation.","Remarkably, our method with a minimum of 3$\\times$ lower GPU memory usage and 5$\\times$ faster fitting time not only rivals INRs (e.g., WIRE, I-NGP) in representation performance, but also delivers a faster rendering speed of 1500-2000 FPS regardless of parameter size.","Furthermore, we integrate existing vector quantization technique to build an image codec.","Experimental results demonstrate that our codec attains rate-distortion performance comparable to compression-based INRs such as COIN and COIN++, while facilitating decoding speeds of approximately 1000 FPS.","Additionally, preliminary proof of concept shows that our codec surpasses COIN and COIN++ in performance when using partial bits-back coding."],"url":"http://arxiv.org/abs/2403.08551v1","category":"eess.IV"}
{"created":"2024-03-13 14:02:42","title":"CINA: Conditional Implicit Neural Atlas for Spatio-Temporal Representation of Fetal Brains","abstract":"We introduce a conditional implicit neural atlas (CINA) for spatio-temporal atlas generation from Magnetic Resonance Images (MRI) of the neurotypical and pathological fetal brain, that is fully independent of affine or non-rigid registration. During training, CINA learns a general representation of the fetal brain and encodes subject specific information into latent code. After training, CINA can construct a faithful atlas with tissue probability maps of the fetal brain for any gestational age (GA) and anatomical variation covered within the training domain. Thus, CINA is competent to represent both, neurotypical and pathological brains. Furthermore, a trained CINA model can be fit to brain MRI of unseen subjects via test-time optimization of the latent code. CINA can then produce probabilistic tissue maps tailored to a particular subject. We evaluate our method on a total of 198 T2 weighted MRI of normal and abnormal fetal brains from the dHCP and FeTA datasets. We demonstrate CINA's capability to represent a fetal brain atlas that can be flexibly conditioned on GA and on anatomical variations like ventricular volume or degree of cortical folding, making it a suitable tool for modeling both neurotypical and pathological brains. We quantify the fidelity of our atlas by means of tissue segmentation and age prediction and compare it to an established baseline. CINA demonstrates superior accuracy for neurotypical brains and pathological brains with ventriculomegaly. Moreover, CINA scores a mean absolute error of 0.23 weeks in fetal brain age prediction, further confirming an accurate representation of fetal brain development.","sentences":["We introduce a conditional implicit neural atlas (CINA) for spatio-temporal atlas generation from Magnetic Resonance Images (MRI) of the neurotypical and pathological fetal brain, that is fully independent of affine or non-rigid registration.","During training, CINA learns a general representation of the fetal brain and encodes subject specific information into latent code.","After training, CINA can construct a faithful atlas with tissue probability maps of the fetal brain for any gestational age (GA) and anatomical variation covered within the training domain.","Thus, CINA is competent to represent both, neurotypical and pathological brains.","Furthermore, a trained CINA model can be fit to brain MRI of unseen subjects via test-time optimization of the latent code.","CINA can then produce probabilistic tissue maps tailored to a particular subject.","We evaluate our method on a total of 198 T2 weighted MRI of normal and abnormal fetal brains from the dHCP and FeTA datasets.","We demonstrate CINA's capability to represent a fetal brain atlas that can be flexibly conditioned on GA and on anatomical variations like ventricular volume or degree of cortical folding, making it a suitable tool for modeling both neurotypical and pathological brains.","We quantify the fidelity of our atlas by means of tissue segmentation and age prediction and compare it to an established baseline.","CINA demonstrates superior accuracy for neurotypical brains and pathological brains with ventriculomegaly.","Moreover, CINA scores a mean absolute error of 0.23 weeks in fetal brain age prediction, further confirming an accurate representation of fetal brain development."],"url":"http://arxiv.org/abs/2403.08550v1","category":"cs.LG"}
{"created":"2024-03-13 14:00:18","title":"Wet TinyML: Chemical Neural Network Using Gene Regulation and Cell Plasticity","abstract":"In our earlier work, we introduced the concept of Gene Regulatory Neural Network (GRNN), which utilizes natural neural network-like structures inherent in biological cells to perform computing tasks using chemical inputs. We define this form of chemical-based neural network as Wet TinyML. The GRNN structures are based on the gene regulatory network and have weights associated with each link based on the estimated interactions between the genes. The GRNNs can be used for conventional computing by employing an application-based search process similar to the Network Architecture Search. This study advances this concept by incorporating cell plasticity, to further exploit natural cell's adaptability, in order to diversify the GRNN search that can match larger spectrum as well as dynamic computing tasks. As an example application, we show that through the directed cell plasticity, we can extract the mathematical regression evolution enabling it to match to dynamic system applications. We also conduct energy analysis by comparing the chemical energy of the GRNN to its silicon counterpart, where this analysis includes both artificial neural network algorithms executed on von Neumann architecture as well as neuromorphic processors. The concept of Wet TinyML can pave the way for the new emergence of chemical-based, energy-efficient and miniature Biological AI.","sentences":["In our earlier work, we introduced the concept of Gene Regulatory Neural Network (GRNN), which utilizes natural neural network-like structures inherent in biological cells to perform computing tasks using chemical inputs.","We define this form of chemical-based neural network as Wet TinyML.","The GRNN structures are based on the gene regulatory network and have weights associated with each link based on the estimated interactions between the genes.","The GRNNs can be used for conventional computing by employing an application-based search process similar to the Network Architecture Search.","This study advances this concept by incorporating cell plasticity, to further exploit natural cell's adaptability, in order to diversify the GRNN search that can match larger spectrum as well as dynamic computing tasks.","As an example application, we show that through the directed cell plasticity, we can extract the mathematical regression evolution enabling it to match to dynamic system applications.","We also conduct energy analysis by comparing the chemical energy of the GRNN to its silicon counterpart, where this analysis includes both artificial neural network algorithms executed on von Neumann architecture as well as neuromorphic processors.","The concept of Wet TinyML can pave the way for the new emergence of chemical-based, energy-efficient and miniature Biological AI."],"url":"http://arxiv.org/abs/2403.08549v1","category":"cs.NE"}
{"created":"2024-03-13 14:00:00","title":"Search for low-mass resonances decaying into two jets and produced in association with a photon or a jet at $\\sqrt{s}=13$ TeV with the ATLAS detector","abstract":"A search is performed for localized excesses in the low-mass dijet invariant mass distribution, targeting a hypothetical new particle decaying into two jets and produced in association with either a high transverse momentum photon or a jet. The search uses the full Run 2 data sample from LHC proton-proton collisions collected by the ATLAS experiment at a center-of-mass energy of 13 TeV during 2015-2018. Two variants of the search are presented for each type of initial-state radiation: one that makes no jet flavor requirements and one that requires both of the jets to have been identified as containing $b$-hadrons. No excess is observed relative to the Standard Model prediction, and the data are used to set upper limits on the production cross-section for a benchmark $Z'$ model and, separately, for generic, beyond the Standard Model scenarios which might produce a Gaussian-shaped contribution to dijet invariant mass distributions. The results extend the current constraints on dijet resonances to the mass range between 200 and 650 GeV.","sentences":["A search is performed for localized excesses in the low-mass dijet invariant mass distribution, targeting a hypothetical new particle decaying into two jets and produced in association with either a high transverse momentum photon or a jet.","The search uses the full Run 2 data sample from LHC proton-proton collisions collected by the ATLAS experiment at a center-of-mass energy of 13 TeV during 2015-2018.","Two variants of the search are presented for each type of initial-state radiation: one that makes no jet flavor requirements and one that requires both of the jets to have been identified as containing $b$-hadrons.","No excess is observed relative to the Standard Model prediction, and the data are used to set upper limits on the production cross-section for a benchmark $Z'$ model and, separately, for generic, beyond the Standard Model scenarios which might produce a Gaussian-shaped contribution to dijet invariant mass distributions.","The results extend the current constraints on dijet resonances to the mass range between 200 and 650 GeV."],"url":"http://arxiv.org/abs/2403.08547v1","category":"hep-ex"}
{"created":"2024-03-13 13:59:40","title":"Semantic Segmentation of Solar Radio Spikes at Low Frequencies","abstract":"Solar radio spikes are short lived, narrow bandwidth features in low frequency solar radio observations. The timing of their occurrence and the number of spikes in a given observation is often unpredictable. The high temporal and frequency of resolution of modern radio telescopes such as NenuFAR mean that manually identifying radio spikes is an arduous task. Machine learning approaches to data exploration in solar radio data is on the rise. Here we describe a convolutional neural network to identify the per pixel location of radio spikes as well as determine some simple characteristics of duration, spectral width and drift rate. The model, which we call SpikeNet, was trained using an Nvidia Tesla T4 16GB GPU with ~100000 sample spikes in a total time of 2.2 hours. The segmentation performs well with an intersection over union in the test set of ~0.85. The root mean squared error for predicted spike properties is of the order of 23%. Applying the algorithm to unlabelled data successfully generates segmentation masks although the accuracy of the predicted properties is less reliable, particularly when more than one spike is present in the same 64 X 64 pixel time-frequency range. We have successfully demonstrated that our convolutional neural network can locate and characterise solar radio spikes in a number of seconds compared to the weeks it would take for manual identification.","sentences":["Solar radio spikes are short lived, narrow bandwidth features in low frequency solar radio observations.","The timing of their occurrence and the number of spikes in a given observation is often unpredictable.","The high temporal and frequency of resolution of modern radio telescopes such as NenuFAR mean that manually identifying radio spikes is an arduous task.","Machine learning approaches to data exploration in solar radio data is on the rise.","Here we describe a convolutional neural network to identify the per pixel location of radio spikes as well as determine some simple characteristics of duration, spectral width and drift rate.","The model, which we call SpikeNet, was trained using an Nvidia Tesla T4 16GB GPU with ~100000 sample spikes in a total time of 2.2 hours.","The segmentation performs well with an intersection over union in the test set of ~0.85.","The root mean squared error for predicted spike properties is of the order of 23%.","Applying the algorithm to unlabelled data successfully generates segmentation masks although the accuracy of the predicted properties is less reliable, particularly when more than one spike is present in the same 64 X 64 pixel time-frequency range.","We have successfully demonstrated that our convolutional neural network can locate and characterise solar radio spikes in a number of seconds compared to the weeks it would take for manual identification."],"url":"http://arxiv.org/abs/2403.08546v1","category":"astro-ph.SR"}
{"created":"2024-03-13 13:58:08","title":"Direct numerical simulation of transition under free-stream turbulence and the influence of large integral length scales","abstract":"Under action of free-stream turbulence (FST), elongated streamwise streaky structures are generated inside the boundary layer, and their amplitude and wavelength are crucial for the transition onset. While turbulence intensity is strongly correlated with the transitional Reynolds number, characteristic length scales of the FST are often considered to have a slight impact on the transition location. However, a recent experiment by Fransson & Shahinfar (2020} shows significant effects of FST scales. They found that, for higher free-stream turbulence levels and larger integral length scales, an increase in the length scale postpones transition, contrary to established literature. Here, we aim at understanding these results by performing a series of high-fidelity simulations. These results provide understanding why the FST integral length scale affects the transition location differently. These integral length scales are so large that the wide streaks introduced in the boundary layer have substantially lower growth in the laminar region upstream of the transition to turbulence, than streaks induced by smaller integral length scales. The energy in the boundary layer subsequently propagate to smaller spanwise scales as a result of the non-linear interaction. When the energy has reached smaller spanwise scales larger amplitude streaks results in regions where the streak growth are larger. It takes longer for the energy from the wider streaks to propagate to the spanwise scales associated with the breakdown to turbulence, than for the those with smaller spanwise scales. Thus there is a faster transition for FST with lower integral length scales in this case.","sentences":["Under action of free-stream turbulence (FST), elongated streamwise streaky structures are generated inside the boundary layer, and their amplitude and wavelength are crucial for the transition onset.","While turbulence intensity is strongly correlated with the transitional Reynolds number, characteristic length scales of the FST are often considered to have a slight impact on the transition location.","However, a recent experiment by Fransson & Shahinfar (2020} shows significant effects of FST scales.","They found that, for higher free-stream turbulence levels and larger integral length scales, an increase in the length scale postpones transition, contrary to established literature.","Here, we aim at understanding these results by performing a series of high-fidelity simulations.","These results provide understanding why the FST integral length scale affects the transition location differently.","These integral length scales are so large that the wide streaks introduced in the boundary layer have substantially lower growth in the laminar region upstream of the transition to turbulence, than streaks induced by smaller integral length scales.","The energy in the boundary layer subsequently propagate to smaller spanwise scales as a result of the non-linear interaction.","When the energy has reached smaller spanwise scales larger amplitude streaks results in regions where the streak growth are larger.","It takes longer for the energy from the wider streaks to propagate to the spanwise scales associated with the breakdown to turbulence, than for the those with smaller spanwise scales.","Thus there is a faster transition for FST with lower integral length scales in this case."],"url":"http://arxiv.org/abs/2403.08543v1","category":"physics.flu-dyn"}
{"created":"2024-03-13 13:56:34","title":"AIGCs Confuse AI Too: Investigating and Explaining Synthetic Image-induced Hallucinations in Large Vision-Language Models","abstract":"The evolution of Artificial Intelligence Generated Contents (AIGCs) is advancing towards higher quality. The growing interactions with AIGCs present a new challenge to the data-driven AI community: While AI-generated contents have played a crucial role in a wide range of AI models, the potential hidden risks they introduce have not been thoroughly examined. Beyond human-oriented forgery detection, AI-generated content poses potential issues for AI models originally designed to process natural data. In this study, we underscore the exacerbated hallucination phenomena in Large Vision-Language Models (LVLMs) caused by AI-synthetic images. Remarkably, our findings shed light on a consistent AIGC \\textbf{hallucination bias}: the object hallucinations induced by synthetic images are characterized by a greater quantity and a more uniform position distribution, even these synthetic images do not manifest unrealistic or additional relevant visual features compared to natural images. Moreover, our investigations on Q-former and Linear projector reveal that synthetic images may present token deviations after visual projection, thereby amplifying the hallucination bias.","sentences":["The evolution of Artificial Intelligence Generated Contents (AIGCs) is advancing towards higher quality.","The growing interactions with AIGCs present a new challenge to the data-driven AI community: While AI-generated contents have played a crucial role in a wide range of AI models, the potential hidden risks they introduce have not been thoroughly examined.","Beyond human-oriented forgery detection, AI-generated content poses potential issues for AI models originally designed to process natural data.","In this study, we underscore the exacerbated hallucination phenomena in Large Vision-Language Models (LVLMs) caused by AI-synthetic images.","Remarkably, our findings shed light on a consistent AIGC \\textbf{hallucination bias}: the object hallucinations induced by synthetic images are characterized by a greater quantity and a more uniform position distribution, even these synthetic images do not manifest unrealistic or additional relevant visual features compared to natural images.","Moreover, our investigations on Q-former and Linear projector reveal that synthetic images may present token deviations after visual projection, thereby amplifying the hallucination bias."],"url":"http://arxiv.org/abs/2403.08542v1","category":"cs.CV"}
{"created":"2024-03-13 13:55:27","title":"On harvesting physical predictions from asymptotically safe quantum field theories","abstract":"Asymptotic safety is a powerful mechanism for obtaining a consistent and predictive quantum field theory beyond the realm of perturbation theory. It hinges on an interacting fixed point of the Wilsonian renormalization group flow which controls the microscopic dynamics. Connecting the fixed point to observations requires constructing the set of effective actions compatible with this microscopic dynamics. Technically, this information is stored in the UV-critical surface of the fixed point. In this work, we describe a novel approach for extracting this information based on analytical and pseudo-spectral methods. Our construction is illustrated at the level of the two-dimensional Ising model and easily generalizes to any asymptotically safe quantum field theory. It also constitutes an important step towards setting up a well-founded swampland program within the gravitational asymptotic safety program.","sentences":["Asymptotic safety is a powerful mechanism for obtaining a consistent and predictive quantum field theory beyond the realm of perturbation theory.","It hinges on an interacting fixed point of the Wilsonian renormalization group flow which controls the microscopic dynamics.","Connecting the fixed point to observations requires constructing the set of effective actions compatible with this microscopic dynamics.","Technically, this information is stored in the UV-critical surface of the fixed point.","In this work, we describe a novel approach for extracting this information based on analytical and pseudo-spectral methods.","Our construction is illustrated at the level of the two-dimensional Ising model and easily generalizes to any asymptotically safe quantum field theory.","It also constitutes an important step towards setting up a well-founded swampland program within the gravitational asymptotic safety program."],"url":"http://arxiv.org/abs/2403.08541v1","category":"hep-th"}
{"created":"2024-03-13 13:52:26","title":"Generation and high-resolution imaging of higher-order polarization via metasurface","abstract":"The generation and focusing properties of higher-order polarized beams have attracted lots of interests due to its significant applications. In this paper,we derived the formula of transforming linear polarization into higher-order polarization, which is applicable to generating arbitrary order polarization. Based on the derived formula, the focusing properties of higher-order polarization by dielectric metasurface lens are studied , which exhibit an Abbe-limit-breaking feature for small numerical aperture, i.e., NA<0.6. When a binary phase (0 & {\\pi}) is further imposed on the aperture of metasurface lens, the focusing spot of fourth-order polarization breaks Abbe limit even by 14.3% at NA= 0.6. In addition, the effect of fabrication tolerance, say, substrate thickness and central deviation, on the focusing feature of higher-order polarization is also investigated. Our study may find significant applications in achieving higher-resolution lithography and imaging, say, by just replacing conventional linear or circular polarization with higher-order polarization.","sentences":["The generation and focusing properties of higher-order polarized beams have attracted lots of interests due to its significant applications.","In this paper,we derived the formula of transforming linear polarization into higher-order polarization, which is applicable to generating arbitrary order polarization.","Based on the derived formula, the focusing properties of higher-order polarization by dielectric metasurface lens are studied , which exhibit an Abbe-limit-breaking feature for small numerical aperture, i.e., NA<0.6.","When a binary phase (0 & {\\pi}) is further imposed on the aperture of metasurface lens, the focusing spot of fourth-order polarization breaks Abbe limit even by 14.3% at NA= 0.6.","In addition, the effect of fabrication tolerance, say, substrate thickness and central deviation, on the focusing feature of higher-order polarization is also investigated.","Our study may find significant applications in achieving higher-resolution lithography and imaging, say, by just replacing conventional linear or circular polarization with higher-order polarization."],"url":"http://arxiv.org/abs/2403.08539v1","category":"physics.optics"}
{"created":"2024-03-13 13:51:02","title":"HOLMES: HOLonym-MEronym based Semantic inspection for Convolutional Image Classifiers","abstract":"Convolutional Neural Networks (CNNs) are nowadays the model of choice in Computer Vision, thanks to their ability to automatize the feature extraction process in visual tasks. However, the knowledge acquired during training is fully subsymbolic, and hence difficult to understand and explain to end users. In this paper, we propose a new technique called HOLMES (HOLonym-MEronym based Semantic inspection) that decomposes a label into a set of related concepts, and provides component-level explanations for an image classification model. Specifically, HOLMES leverages ontologies, web scraping and transfer learning to automatically construct meronym (parts)-based detectors for a given holonym (class). Then, it produces heatmaps at the meronym level and finally, by probing the holonym CNN with occluded images, it highlights the importance of each part on the classification output. Compared to state-of-the-art saliency methods, HOLMES takes a step further and provides information about both where and what the holonym CNN is looking at, without relying on densely annotated datasets and without forcing concepts to be associated to single computational units. Extensive experimental evaluation on different categories of objects (animals, tools and vehicles) shows the feasibility of our approach. On average, HOLMES explanations include at least two meronyms, and the ablation of a single meronym roughly halves the holonym model confidence. The resulting heatmaps were quantitatively evaluated using the deletion/insertion/preservation curves. All metrics were comparable to those achieved by GradCAM, while offering the advantage of further decomposing the heatmap in human-understandable concepts, thus highlighting both the relevance of meronyms to object classification, as well as HOLMES ability to capture it. The code is available at https://github.com/FrancesC0de/HOLMES.","sentences":["Convolutional Neural Networks (CNNs) are nowadays the model of choice in Computer Vision, thanks to their ability to automatize the feature extraction process in visual tasks.","However, the knowledge acquired during training is fully subsymbolic, and hence difficult to understand and explain to end users.","In this paper, we propose a new technique called HOLMES (HOLonym-MEronym based Semantic inspection) that decomposes a label into a set of related concepts, and provides component-level explanations for an image classification model.","Specifically, HOLMES leverages ontologies, web scraping and transfer learning to automatically construct meronym (parts)-based detectors for a given holonym (class).","Then, it produces heatmaps at the meronym level and finally, by probing the holonym CNN with occluded images, it highlights the importance of each part on the classification output.","Compared to state-of-the-art saliency methods, HOLMES takes a step further and provides information about both where and what the holonym CNN is looking at, without relying on densely annotated datasets and without forcing concepts to be associated to single computational units.","Extensive experimental evaluation on different categories of objects (animals, tools and vehicles) shows the feasibility of our approach.","On average, HOLMES explanations include at least two meronyms, and the ablation of a single meronym roughly halves the holonym model confidence.","The resulting heatmaps were quantitatively evaluated using the deletion/insertion/preservation curves.","All metrics were comparable to those achieved by GradCAM, while offering the advantage of further decomposing the heatmap in human-understandable concepts, thus highlighting both the relevance of meronyms to object classification, as well as HOLMES ability to capture it.","The code is available at https://github.com/FrancesC0de/HOLMES."],"url":"http://arxiv.org/abs/2403.08536v1","category":"cs.CV"}
{"created":"2024-03-13 13:50:16","title":"Robustness of Random Networks with Selective Reinforcement against Attacks","abstract":"We investigate the robustness of random networks reinforced by adding hidden edges against targeted attacks. This study focuses on two types of reinforcement: uniform reinforcement, where edges are randomly added to all nodes, and selective reinforcement, where edges are randomly added only to the minimum degree nodes of the given network. We use generating functions to derive the giant component size and the critical threshold for the targeted attacks on reinforced networks. Applying our analysis and Monte Carlo simulations to the targeted attacks on scale-free networks, it becomes clear that selective reinforcement significantly improves the robustness of networks against the targeted attacks.","sentences":["We investigate the robustness of random networks reinforced by adding hidden edges against targeted attacks.","This study focuses on two types of reinforcement: uniform reinforcement, where edges are randomly added to all nodes, and selective reinforcement, where edges are randomly added only to the minimum degree nodes of the given network.","We use generating functions to derive the giant component size and the critical threshold for the targeted attacks on reinforced networks.","Applying our analysis and Monte Carlo simulations to the targeted attacks on scale-free networks, it becomes clear that selective reinforcement significantly improves the robustness of networks against the targeted attacks."],"url":"http://arxiv.org/abs/2403.08535v1","category":"physics.soc-ph"}
{"created":"2024-03-13 13:40:31","title":"Inflation and reheating in quadratic metric-affine gravity with derivative couplings","abstract":"Within the framework of metric-affine theories of gravity, where both the metric and connection are treated as independent variables, we consider actions quadratic in the Ricci scalar curvature coupled non-minimally to a scalar field through derivative couplings. Our analysis delves into the inflationary predictions, revealing their consistency with the latest observational constraints across a wide range of parameters. This compatibility permits adjustments such as an increase in the spectral index and a reduction in the tensor-to-scalar ratio. While we do not propose a specific reheating mechanism, our analysis demonstrates that within the quadratic model of inflation, the maximum reheating temperature can reach $\\sim 3\\times10^{15}\\, {\\rm GeV}$.","sentences":["Within the framework of metric-affine theories of gravity, where both the metric and connection are treated as independent variables, we consider actions quadratic in the Ricci scalar curvature coupled non-minimally to a scalar field through derivative couplings.","Our analysis delves into the inflationary predictions, revealing their consistency with the latest observational constraints across a wide range of parameters.","This compatibility permits adjustments such as an increase in the spectral index and a reduction in the tensor-to-scalar ratio.","While we do not propose a specific reheating mechanism, our analysis demonstrates that within the quadratic model of inflation, the maximum reheating temperature can reach $\\sim 3\\times10^{15}\\, {\\rm GeV}$."],"url":"http://arxiv.org/abs/2403.08530v1","category":"gr-qc"}
{"created":"2024-03-13 13:39:48","title":"Near horizon symmetry of extremal spacelike-stretched black holes","abstract":"We analyze the near horizon structure of the extremal spacelike stretched black holes, exact solutions of topologically massive gravity. We show that the algebra of improved canonical generator is realized as a single centrally extended Virasoro algebra. We obtain the entropy of the solution by using the Cardy formula and compare the results with the corresponding non-extremal case.","sentences":["We analyze the near horizon structure of the extremal spacelike stretched black holes, exact solutions of topologically massive gravity.","We show that the algebra of improved canonical generator is realized as a single centrally extended Virasoro algebra.","We obtain the entropy of the solution by using the Cardy formula and compare the results with the corresponding non-extremal case."],"url":"http://arxiv.org/abs/2403.08529v1","category":"gr-qc"}
{"created":"2024-03-13 13:38:58","title":"Pig aggression classification using CNN, Transformers and Recurrent Networks","abstract":"The development of techniques that can be used to analyze and detect animal behavior is a crucial activity for the livestock sector, as it is possible to monitor the stress and animal welfare and contributes to decision making in the farm. Thus, the development of applications can assist breeders in making decisions to improve production performance and reduce costs, once the animal behavior is analyzed by humans and this can lead to susceptible errors and time consumption. Aggressiveness in pigs is an example of behavior that is studied to reduce its impact through animal classification and identification. However, this process is laborious and susceptible to errors, which can be reduced through automation by visually classifying videos captured in controlled environment. The captured videos can be used for training and, as a result, for classification through computer vision and artificial intelligence, employing neural network techniques. The main techniques utilized in this study are variants of transformers: STAM, TimeSformer, and ViViT, as well as techniques using convolutions, such as ResNet3D2, Resnet(2+1)D, and CnnLstm. These techniques were employed for pig video classification with the objective of identifying aggressive and non-aggressive behaviors. In this work, various techniques were compared to analyze the contribution of using transformers, in addition to the effectiveness of the convolution technique in video classification. The performance was evaluated using accuracy, precision, and recall. The TimerSformer technique showed the best results in video classification, with median accuracy of 0.729.","sentences":["The development of techniques that can be used to analyze and detect animal behavior is a crucial activity for the livestock sector, as it is possible to monitor the stress and animal welfare and contributes to decision making in the farm.","Thus, the development of applications can assist breeders in making decisions to improve production performance and reduce costs, once the animal behavior is analyzed by humans and this can lead to susceptible errors and time consumption.","Aggressiveness in pigs is an example of behavior that is studied to reduce its impact through animal classification and identification.","However, this process is laborious and susceptible to errors, which can be reduced through automation by visually classifying videos captured in controlled environment.","The captured videos can be used for training and, as a result, for classification through computer vision and artificial intelligence, employing neural network techniques.","The main techniques utilized in this study are variants of transformers: STAM, TimeSformer, and ViViT, as well as techniques using convolutions, such as ResNet3D2, Resnet(2+1)D, and CnnLstm.","These techniques were employed for pig video classification with the objective of identifying aggressive and non-aggressive behaviors.","In this work, various techniques were compared to analyze the contribution of using transformers, in addition to the effectiveness of the convolution technique in video classification.","The performance was evaluated using accuracy, precision, and recall.","The TimerSformer technique showed the best results in video classification, with median accuracy of 0.729."],"url":"http://arxiv.org/abs/2403.08528v1","category":"cs.CV"}
{"created":"2024-03-13 13:38:45","title":"Time-bin entanglement in the deterministic generation of linear photonic cluster states","abstract":"We investigate strategies for the efficient deterministic creation of trains of time-bin entangled photons using an individual quantum emitter described by a $\\Lambda$-type electronic system. We explicitly demonstrate generation of high-quality linear cluster states of substantial length in our full microscopic numerical simulations. The underlying scheme is based on the manipulation of ground state coherences through precise optical driving. One important finding is that the most easily accessible quality metrics, the achievable rotation fidelities, fall short in assessing the actual quantum correlations of the emitted photons in the face of losses. To address this, we explicitly calculate stabilizer generator expectation values as a superior gauge for the quantum properties of the many-photon state. Our results illustrate that with controlled minimization of losses and realistic system parameters for quantum-dot type systems, useful linear cluster states of significant lengths can be generated, showcasing promise of scalability for quantum information processing endeavors.","sentences":["We investigate strategies for the efficient deterministic creation of trains of time-bin entangled photons using an individual quantum emitter described by a $\\Lambda$-type electronic system.","We explicitly demonstrate generation of high-quality linear cluster states of substantial length in our full microscopic numerical simulations.","The underlying scheme is based on the manipulation of ground state coherences through precise optical driving.","One important finding is that the most easily accessible quality metrics, the achievable rotation fidelities, fall short in assessing the actual quantum correlations of the emitted photons in the face of losses.","To address this, we explicitly calculate stabilizer generator expectation values as a superior gauge for the quantum properties of the many-photon state.","Our results illustrate that with controlled minimization of losses and realistic system parameters for quantum-dot type systems, useful linear cluster states of significant lengths can be generated, showcasing promise of scalability for quantum information processing endeavors."],"url":"http://arxiv.org/abs/2403.08527v1","category":"quant-ph"}
{"created":"2024-03-13 13:34:03","title":"Field demonstration of a fully managed, L1 encrypted 3-node network with hybrid relayed-QKD and centralized symmetric classical key management","abstract":"We successfully demonstrated a fully-managed, field-deployed, three-node QKD ring network with L1-OTNsec encryption, that employs a hybrid scheme of QKD and classical yet quantum-safe centrally-generated symmetric keys to support point-to-point and relay consumers.","sentences":["We successfully demonstrated a fully-managed, field-deployed, three-node QKD ring network with L1-OTNsec encryption, that employs a hybrid scheme of QKD and classical yet quantum-safe centrally-generated symmetric keys to support point-to-point and relay consumers."],"url":"http://arxiv.org/abs/2403.08526v1","category":"quant-ph"}
{"created":"2024-03-13 13:30:45","title":"Elliptic billiard with harmonic potential: Classical description","abstract":"The classical dynamics of the isotropic two-dimensional harmonic oscillator confined by an elliptic hard wall is discussed. The interplay between the harmonic potential with circular symmetry and the boundary with elliptical symmetry does not spoil the separability in elliptic coordinates; however, it generates non-trivial energy and momentum dependencies in the billiard. We analyze the equi-momentum surfaces in the parameters space and classify the kinds of motion the particle can have in the billiard. The winding numbers and periods of the rotational and librational trajectories are analytically calculated and numerically verified. A remarkable finding is the possibility of having degenerate rotational trajectories with the same energy but different second constant of motion and different caustics and periods. The conditions to get these degenerate trajectories are analyzed. Similarly, we show that obtaining two different rotational trajectories with the same period and second constant of motion but different energy is possible.","sentences":["The classical dynamics of the isotropic two-dimensional harmonic oscillator confined by an elliptic hard wall is discussed.","The interplay between the harmonic potential with circular symmetry and the boundary with elliptical symmetry does not spoil the separability in elliptic coordinates; however, it generates non-trivial energy and momentum dependencies in the billiard.","We analyze the equi-momentum surfaces in the parameters space and classify the kinds of motion the particle can have in the billiard.","The winding numbers and periods of the rotational and librational trajectories are analytically calculated and numerically verified.","A remarkable finding is the possibility of having degenerate rotational trajectories with the same energy but different second constant of motion and different caustics and periods.","The conditions to get these degenerate trajectories are analyzed.","Similarly, we show that obtaining two different rotational trajectories with the same period and second constant of motion but different energy is possible."],"url":"http://arxiv.org/abs/2403.08523v1","category":"nlin.CD"}
{"created":"2024-03-13 13:30:12","title":"Random Groups are not $n$-Cubulated","abstract":"A group $G$ has $F\\mathcal C_n$ if every action on a $n$-dimensional $\\mathrm{CAT}(0)$ cube complex has a global fixed point. This provides a natural stratification between Serre's $FA$ and Kazhdan's $(T)$. For every $n$, we show that random groups in the plain words density model have $F\\mathcal C_n$ with overwhelming probability. The same result holds for random groups in the reduced words density model assuming there are sufficiently many generators. These are the first examples of cubulated hyperbolic groups with $F\\mathcal C_n$ for $n$ arbitrarily large.","sentences":["A group $G$ has $F\\mathcal C_n$ if every action on a $n$-dimensional $\\mathrm{CAT}(0)$ cube complex has a global fixed point.","This provides a natural stratification between Serre's $FA$ and Kazhdan's $(T)$. For every $n$, we show that random groups in the plain words density model have $F\\mathcal C_n$ with overwhelming probability.","The same result holds for random groups in the reduced words density model assuming there are sufficiently many generators.","These are the first examples of cubulated hyperbolic groups with $F\\mathcal C_n$ for $n$ arbitrarily large."],"url":"http://arxiv.org/abs/2403.08522v1","category":"math.GR"}
{"created":"2024-03-13 13:27:11","title":"Stability of Weyl node merging processes under symmetry constraints","abstract":"Changes in the number of Weyl nodes in Weyl semimetals occur through merging processes, usually involving a pair of oppositely charged nodes. More complicated processes involving multiple Weyl nodes are also possible, but they typically require fine tuning and are thus less stable. In this work, we study how symmetries affect the allowed merging processes and their stability, focusing on the combination of a two-fold rotation and time-reversal ($C_2\\mathcal{T}$) symmetry. We find that, counter-intuitively, processes involving a merging of three nodes are more generic than processes involving only two nodes. Our work suggests that multi-Weyl-merging may be observed in a large variety of quantum materials, and we discuss SrSi$_2$ and bilayer graphene as potential candidates","sentences":["Changes in the number of Weyl nodes in Weyl semimetals occur through merging processes, usually involving a pair of oppositely charged nodes.","More complicated processes involving multiple Weyl nodes are also possible, but they typically require fine tuning and are thus less stable.","In this work, we study how symmetries affect the allowed merging processes and their stability, focusing on the combination of a two-fold rotation and time-reversal ($C_2\\mathcal{T}$) symmetry.","We find that, counter-intuitively, processes involving a merging of three nodes are more generic than processes involving only two nodes.","Our work suggests that multi-Weyl-merging may be observed in a large variety of quantum materials, and we discuss SrSi$_2$ and bilayer graphene as potential candidates"],"url":"http://arxiv.org/abs/2403.08518v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-13 13:27:02","title":"An Extended View on Measuring Tor AS-level Adversaries","abstract":"Tor provides anonymity to millions of users around the globe which has made it a valuable target for malicious actors. As a low-latency anonymity system, it is vulnerable to traffic correlation attacks from strong passive adversaries such as large autonomous systems (ASes). In preliminary work, we have developed a measurement approach utilizing the RIPE Atlas framework -- a network of more than 11,000 probes worldwide -- to infer the risk of deanonymization for IPv4 clients in Germany and the US.   In this paper, we apply our methodology to additional scenarios providing a broader picture of the potential for deanonymization in the Tor network. In particular, we (a) repeat our earlier (2020) measurements in 2022 to observe changes over time, (b) adopt our approach for IPv6 to analyze the risk of deanonymization when using this next-generation Internet protocol, and (c) investigate the current situation in Russia, where censorship has been intensified after the beginning of Russia's full-scale invasion of Ukraine. According to our results, Tor provides user anonymity at consistent quality: While individual numbers vary in dependence of client and destination, we were able to identify ASes with the potential to conduct deanonymization attacks. For clients in Germany and the US, the overall picture, however, has not changed since 2020. In addition, the protocols (IPv4 vs. IPv6) do not significantly impact the risk of deanonymization. Russian users are able to securely evade censorship using Tor. Their general risk of deanonymization is, in fact, lower than in the other investigated countries. Beyond, the few ASes with the potential to successfully perform deanonymization are operated by Western companies, further reducing the risk for Russian users.","sentences":["Tor provides anonymity to millions of users around the globe which has made it a valuable target for malicious actors.","As a low-latency anonymity system, it is vulnerable to traffic correlation attacks from strong passive adversaries such as large autonomous systems (ASes).","In preliminary work, we have developed a measurement approach utilizing the RIPE Atlas framework -- a network of more than 11,000 probes worldwide -- to infer the risk of deanonymization for IPv4 clients in Germany and the US.   ","In this paper, we apply our methodology to additional scenarios providing a broader picture of the potential for deanonymization in the Tor network.","In particular, we (a) repeat our earlier (2020) measurements in 2022 to observe changes over time, (b) adopt our approach for IPv6 to analyze the risk of deanonymization when using this next-generation Internet protocol, and (c) investigate the current situation in Russia, where censorship has been intensified after the beginning of Russia's full-scale invasion of Ukraine.","According to our results, Tor provides user anonymity at consistent quality: While individual numbers vary in dependence of client and destination, we were able to identify ASes with the potential to conduct deanonymization attacks.","For clients in Germany and the US, the overall picture, however, has not changed since 2020.","In addition, the protocols (IPv4 vs. IPv6) do not significantly impact the risk of deanonymization.","Russian users are able to securely evade censorship using Tor.","Their general risk of deanonymization is, in fact, lower than in the other investigated countries.","Beyond, the few ASes with the potential to successfully perform deanonymization are operated by Western companies, further reducing the risk for Russian users."],"url":"http://arxiv.org/abs/2403.08517v1","category":"cs.NI"}
{"created":"2024-03-13 13:23:37","title":"3D Spectrum Mapping and Reconstruction under Multi-Radiation Source Scenarios","abstract":"Spectrum map construction, which is crucial in cognitive radio (CR) system, visualizes the invisible space of the electromagnetic spectrum for spectrum-resource management and allocation. Traditional reconstruction methods are generally for two-dimensional (2D) spectrum map and driven by abundant sampling data. In this paper, we propose a data-model-knowledge-driven reconstruction scheme to construct the three-dimensional (3D) spectrum map under multi-radiation source scenarios. We firstly design a maximum and minimum path loss difference (MMPLD) clustering algorithm to detect the number of radiation sources in a 3D space. Then, we develop a joint location-power estimation method based on the heuristic population evolutionary optimization algorithm. Considering the variation of electromagnetic environment, we self-learn the path loss (PL) model based on the sampling data. Finally, the 3D spectrum is reconstructed according to the self-learned PL model and the extracted knowledge of radiation sources. Simulations show that the proposed 3D spectrum map reconstruction scheme not only has splendid adaptability to the environment, but also achieves high spectrum construction accuracy even when the sampling rate is very low.","sentences":["Spectrum map construction, which is crucial in cognitive radio (CR) system, visualizes the invisible space of the electromagnetic spectrum for spectrum-resource management and allocation.","Traditional reconstruction methods are generally for two-dimensional (2D) spectrum map and driven by abundant sampling data.","In this paper, we propose a data-model-knowledge-driven reconstruction scheme to construct the three-dimensional (3D) spectrum map under multi-radiation source scenarios.","We firstly design a maximum and minimum path loss difference (MMPLD) clustering algorithm to detect the number of radiation sources in a 3D space.","Then, we develop a joint location-power estimation method based on the heuristic population evolutionary optimization algorithm.","Considering the variation of electromagnetic environment, we self-learn the path loss (PL) model based on the sampling data.","Finally, the 3D spectrum is reconstructed according to the self-learned PL model and the extracted knowledge of radiation sources.","Simulations show that the proposed 3D spectrum map reconstruction scheme not only has splendid adaptability to the environment, but also achieves high spectrum construction accuracy even when the sampling rate is very low."],"url":"http://arxiv.org/abs/2403.08513v1","category":"eess.SP"}
{"created":"2024-03-13 13:23:05","title":"UniLiDAR: Bridge the domain gap among different LiDARs for continual learning","abstract":"LiDAR-based 3D perception algorithms have evolved rapidly alongside the emergence of large datasets. Nonetheless, considerable performance degradation often ensues when models trained on a specific dataset are applied to other datasets or real-world scenarios with different LiDAR. This paper aims to develop a unified model capable of handling different LiDARs, enabling continual learning across diverse LiDAR datasets and seamless deployment across heterogeneous platforms. We observe that the gaps among datasets primarily manifest in geometric disparities (such as variations in beams and point counts) and semantic inconsistencies (taxonomy conflicts). To this end, this paper proposes UniLiDAR, an occupancy prediction pipeline that leverages geometric realignment and semantic label mapping to facilitate multiple datasets training and mitigate performance degradation during deployment on heterogeneous platforms. Moreover, our method can be easily combined with existing 3D perception models. The efficacy of the proposed approach in bridging LiDAR domain gaps is verified by comprehensive experiments on two prominent datasets: OpenOccupancy-nuScenes and SemanticKITTI. UniLiDAR elevates the mIoU of occupancy prediction by 15.7% and 12.5%, respectively, compared to the model trained on the directly merged dataset. Moreover, it outperforms several SOTA methods trained on individual datasets. We expect our research to facilitate further study of 3D generalization, the code will be available soon.","sentences":["LiDAR-based 3D perception algorithms have evolved rapidly alongside the emergence of large datasets.","Nonetheless, considerable performance degradation often ensues when models trained on a specific dataset are applied to other datasets or real-world scenarios with different LiDAR.","This paper aims to develop a unified model capable of handling different LiDARs, enabling continual learning across diverse LiDAR datasets and seamless deployment across heterogeneous platforms.","We observe that the gaps among datasets primarily manifest in geometric disparities (such as variations in beams and point counts) and semantic inconsistencies (taxonomy conflicts).","To this end, this paper proposes UniLiDAR, an occupancy prediction pipeline that leverages geometric realignment and semantic label mapping to facilitate multiple datasets training and mitigate performance degradation during deployment on heterogeneous platforms.","Moreover, our method can be easily combined with existing 3D perception models.","The efficacy of the proposed approach in bridging LiDAR domain gaps is verified by comprehensive experiments on two prominent datasets: OpenOccupancy-nuScenes and SemanticKITTI.","UniLiDAR elevates the mIoU of occupancy prediction by 15.7% and 12.5%, respectively, compared to the model trained on the directly merged dataset.","Moreover, it outperforms several SOTA methods trained on individual datasets.","We expect our research to facilitate further study of 3D generalization, the code will be available soon."],"url":"http://arxiv.org/abs/2403.08512v1","category":"cs.CV"}
{"created":"2024-03-13 13:12:57","title":"Content-aware Masked Image Modeling Transformer for Stereo Image Compression","abstract":"Existing learning-based stereo image codec adopt sophisticated transformation with simple entropy models derived from single image codecs to encode latent representations. However, those entropy models struggle to effectively capture the spatial-disparity characteristics inherent in stereo images, which leads to suboptimal rate-distortion results. In this paper, we propose a stereo image compression framework, named CAMSIC. CAMSIC independently transforms each image to latent representation and employs a powerful decoder-free Transformer entropy model to capture both spatial and disparity dependencies, by introducing a novel content-aware masked image modeling (MIM) technique. Our content-aware MIM facilitates efficient bidirectional interaction between prior information and estimated tokens, which naturally obviates the need for an extra Transformer decoder. Experiments show that our stereo image codec achieves state-of-the-art rate-distortion performance on two stereo image datasets Cityscapes and InStereo2K with fast encoding and decoding speed.","sentences":["Existing learning-based stereo image codec adopt sophisticated transformation with simple entropy models derived from single image codecs to encode latent representations.","However, those entropy models struggle to effectively capture the spatial-disparity characteristics inherent in stereo images, which leads to suboptimal rate-distortion results.","In this paper, we propose a stereo image compression framework, named CAMSIC.","CAMSIC independently transforms each image to latent representation and employs a powerful decoder-free Transformer entropy model to capture both spatial and disparity dependencies, by introducing a novel content-aware masked image modeling (MIM) technique.","Our content-aware MIM facilitates efficient bidirectional interaction between prior information and estimated tokens, which naturally obviates the need for an extra Transformer decoder.","Experiments show that our stereo image codec achieves state-of-the-art rate-distortion performance on two stereo image datasets Cityscapes and InStereo2K with fast encoding and decoding speed."],"url":"http://arxiv.org/abs/2403.08505v1","category":"eess.IV"}
{"created":"2024-03-13 13:10:46","title":"Small field chaos in spin glasses: universal predictions from the ultrametric tree and comparison with numerical simulations","abstract":"We study the chaotic behavior of the Gibbs state of spin-glasses under the application of an external magnetic field, in the crossover region where the field intensity scales proportional to $1/\\sqrt{N}$, being $N$ the system size. We show that Replica Symmetry Breaking (RSB) theory provides universal predictions for chaotic behavior: they depend only on the zero-field overlap probability function $P(q)$ and are independent of other features of the system. Using solely $P(q)$ as input we can analytically predict quantitatively the statistics of the states in a small field. In the infinite volume limit, each spin-glass sample is characterized by an infinite number of states that have a tree-like structure. We generate the corresponding probability distribution through efficient sampling using a representation based on the Bolthausen-Snitmann coalescent. In this way, we can compute quantitatively properties in the presence of a magnetic field in the crossover region, the overlap probability distribution in the presence of a small field and the degree of decorrelation as the field is increased. To test our computations, we have simulated the Bethe lattice spin glass and the 4D Edwards-Anderson model, finding in both cases excellent agreement with the universal predictions.","sentences":["We study the chaotic behavior of the Gibbs state of spin-glasses under the application of an external magnetic field, in the crossover region where the field intensity scales proportional to $1/\\sqrt{N}$, being $N$ the system size.","We show that Replica Symmetry Breaking (RSB) theory provides universal predictions for chaotic behavior: they depend only on the zero-field overlap probability function $P(q)$ and are independent of other features of the system.","Using solely $P(q)$ as input we can analytically predict quantitatively the statistics of the states in a small field.","In the infinite volume limit, each spin-glass sample is characterized by an infinite number of states that have a tree-like structure.","We generate the corresponding probability distribution through efficient sampling using a representation based on the Bolthausen-Snitmann coalescent.","In this way, we can compute quantitatively properties in the presence of a magnetic field in the crossover region, the overlap probability distribution in the presence of a small field and the degree of decorrelation as the field is increased.","To test our computations, we have simulated the Bethe lattice spin glass and the 4D Edwards-Anderson model, finding in both cases excellent agreement with the universal predictions."],"url":"http://arxiv.org/abs/2403.08503v1","category":"cond-mat.dis-nn"}
{"created":"2024-03-13 13:10:20","title":"Masked Generative Story Transformer with Character Guidance and Caption Augmentation","abstract":"Story Visualization (SV) is a challenging generative vision task, that requires both visual quality and consistency between different frames in generated image sequences. Previous approaches either employ some kind of memory mechanism to maintain context throughout an auto-regressive generation of the image sequence, or model the generation of the characters and their background separately, to improve the rendering of characters. On the contrary, we embrace a completely parallel transformer-based approach, exclusively relying on Cross-Attention with past and future captions to achieve consistency. Additionally, we propose a Character Guidance technique to focus on the generation of characters in an implicit manner, by forming a combination of text-conditional and character-conditional logits in the logit space. We also employ a caption-augmentation technique, carried out by a Large Language Model (LLM), to enhance the robustness of our approach. The combination of these methods culminates into state-of-the-art (SOTA) results over various metrics in the most prominent SV benchmark (Pororo-SV), attained with constraint resources while achieving superior computational complexity compared to previous arts. The validity of our quantitative results is supported by a human survey.","sentences":["Story Visualization (SV) is a challenging generative vision task, that requires both visual quality and consistency between different frames in generated image sequences.","Previous approaches either employ some kind of memory mechanism to maintain context throughout an auto-regressive generation of the image sequence, or model the generation of the characters and their background separately, to improve the rendering of characters.","On the contrary, we embrace a completely parallel transformer-based approach, exclusively relying on Cross-Attention with past and future captions to achieve consistency.","Additionally, we propose a Character Guidance technique to focus on the generation of characters in an implicit manner, by forming a combination of text-conditional and character-conditional logits in the logit space.","We also employ a caption-augmentation technique, carried out by a Large Language Model (LLM), to enhance the robustness of our approach.","The combination of these methods culminates into state-of-the-art (SOTA) results over various metrics in the most prominent SV benchmark (Pororo-SV), attained with constraint resources while achieving superior computational complexity compared to previous arts.","The validity of our quantitative results is supported by a human survey."],"url":"http://arxiv.org/abs/2403.08502v1","category":"cs.CV"}
{"created":"2024-03-13 13:05:41","title":"Universal and robust quantum coherent control based on a chirped-pulse driving protocol","abstract":"We propose a chirped-pulse driving protocol and reveal its exceptional property for quantum coherent control. The nonadiabatic passage generated by the driving protocol, which includes the population inversion and the nonadiabaticity-induced transition as its ingredients, is shown to be robust against pulse truncation. We further demonstrate that the protocol allows for universal manipulation on the qubit system through designing pulse sequences with either properly adjusted sweeping frequency or pulsing intensity.","sentences":["We propose a chirped-pulse driving protocol and reveal its exceptional property for quantum coherent control.","The nonadiabatic passage generated by the driving protocol, which includes the population inversion and the nonadiabaticity-induced transition as its ingredients, is shown to be robust against pulse truncation.","We further demonstrate that the protocol allows for universal manipulation on the qubit system through designing pulse sequences with either properly adjusted sweeping frequency or pulsing intensity."],"url":"http://arxiv.org/abs/2403.08496v1","category":"quant-ph"}
{"created":"2024-03-13 12:54:09","title":"Compliant Hierarchical Control for Arbitrary Equality and Inequality Tasks with Strict and Soft Priorities","abstract":"When a robotic system is redundant with respect to a given task, the remaining degrees of freedom can be used to satisfy additional objectives. With current robotic systems having more and more degrees of freedom, this can lead to an entire hierarchy of tasks that need to be solved according to given priorities. In this paper, the first compliant control strategy is presented that allows to consider an arbitrary number of equality and inequality tasks, while still preserving the natural inertia of the robot. The approach is therefore a generalization of a passivity-based controller to the case of an arbitrary number of equality and inequality tasks. The key idea of the method is to use a Weighted Hierarchical Quadratic Problem to extract the set of active tasks and use the latter to perform a coordinate transformation that inertially decouples the tasks. Thereby unifying the line of research focusing on optimization-based and passivity-based multi-task controllers. The method is validated in simulation.","sentences":["When a robotic system is redundant with respect to a given task, the remaining degrees of freedom can be used to satisfy additional objectives.","With current robotic systems having more and more degrees of freedom, this can lead to an entire hierarchy of tasks that need to be solved according to given priorities.","In this paper, the first compliant control strategy is presented that allows to consider an arbitrary number of equality and inequality tasks, while still preserving the natural inertia of the robot.","The approach is therefore a generalization of a passivity-based controller to the case of an arbitrary number of equality and inequality tasks.","The key idea of the method is to use a Weighted Hierarchical Quadratic Problem to extract the set of active tasks and use the latter to perform a coordinate transformation that inertially decouples the tasks.","Thereby unifying the line of research focusing on optimization-based and passivity-based multi-task controllers.","The method is validated in simulation."],"url":"http://arxiv.org/abs/2403.08491v1","category":"cs.RO"}
{"created":"2024-03-13 12:52:57","title":"CAM: A Collection of Snapshots of GitHub Java Repositories Together with Metrics","abstract":"Even though numerous researchers require stable datasets along with source code and basic metrics calculated on them, neither GitHub nor any other code hosting platform provides such a resource. Consequently, each researcher must download their own data, compute the necessary metrics, and then publish the dataset somewhere to ensure it remains accessible indefinitely. Our CAM (stands for ``Classes and Metrics'') project addresses this need. It is an open-source software capable of cloning Java repositories from GitHub, filtering out unnecessary files, parsing Java classes, and computing metrics such as Cyclomatic Complexity, Halstead Effort and Volume, C\\&K metrics, Maintainability Metrics, LCOM5 and HND, as well as some Git-based Metrics. At least once a year, we execute the entire script, a process which requires a minimum of ten days on a very powerful server, to generate a new dataset. Subsequently, we publish it on Amazon S3, thereby ensuring its availability as a reference for researchers. The latest archive of 2.2Gb that we published on the 2nd of March, 2024 includes 532K Java classes with 48 metrics for each class.","sentences":["Even though numerous researchers require stable datasets along with source code and basic metrics calculated on them, neither GitHub nor any other code hosting platform provides such a resource.","Consequently, each researcher must download their own data, compute the necessary metrics, and then publish the dataset somewhere to ensure it remains accessible indefinitely.","Our CAM (stands for ``Classes and Metrics'') project addresses this need.","It is an open-source software capable of cloning Java repositories from GitHub, filtering out unnecessary files, parsing Java classes, and computing metrics such as Cyclomatic Complexity, Halstead Effort and Volume, C\\&K metrics, Maintainability Metrics, LCOM5 and HND, as well as some Git-based Metrics.","At least once a year, we execute the entire script, a process which requires a minimum of ten days on a very powerful server, to generate a new dataset.","Subsequently, we publish it on Amazon S3, thereby ensuring its availability as a reference for researchers.","The latest archive of 2.2Gb that we published on the 2nd of March, 2024 includes 532K Java classes with 48 metrics for each class."],"url":"http://arxiv.org/abs/2403.08488v1","category":"cs.SE"}
{"created":"2024-03-13 12:52:37","title":"Model Will Tell: Training Membership Inference for Diffusion Models","abstract":"Diffusion models pose risks of privacy breaches and copyright disputes, primarily stemming from the potential utilization of unauthorized data during the training phase. The Training Membership Inference (TMI) task aims to determine whether a specific sample has been used in the training process of a target model, representing a critical tool for privacy violation verification. However, the increased stochasticity inherent in diffusion renders traditional shadow-model-based or metric-based methods ineffective when applied to diffusion models. Moreover, existing methods only yield binary classification labels which lack necessary comprehensibility in practical applications. In this paper, we explore a novel perspective for the TMI task by leveraging the intrinsic generative priors within the diffusion model. Compared with unseen samples, training samples exhibit stronger generative priors within the diffusion model, enabling the successful reconstruction of substantially degraded training images. Consequently, we propose the Degrade Restore Compare (DRC) framework. In this framework, an image undergoes sequential degradation and restoration, and its membership is determined by comparing it with the restored counterpart. Experimental results verify that our approach not only significantly outperforms existing methods in terms of accuracy but also provides comprehensible decision criteria, offering evidence for potential privacy violations.","sentences":["Diffusion models pose risks of privacy breaches and copyright disputes, primarily stemming from the potential utilization of unauthorized data during the training phase.","The Training Membership Inference (TMI) task aims to determine whether a specific sample has been used in the training process of a target model, representing a critical tool for privacy violation verification.","However, the increased stochasticity inherent in diffusion renders traditional shadow-model-based or metric-based methods ineffective when applied to diffusion models.","Moreover, existing methods only yield binary classification labels which lack necessary comprehensibility in practical applications.","In this paper, we explore a novel perspective for the TMI task by leveraging the intrinsic generative priors within the diffusion model.","Compared with unseen samples, training samples exhibit stronger generative priors within the diffusion model, enabling the successful reconstruction of substantially degraded training images.","Consequently, we propose the Degrade Restore Compare (DRC) framework.","In this framework, an image undergoes sequential degradation and restoration, and its membership is determined by comparing it with the restored counterpart.","Experimental results verify that our approach not only significantly outperforms existing methods in terms of accuracy but also provides comprehensible decision criteria, offering evidence for potential privacy violations."],"url":"http://arxiv.org/abs/2403.08487v1","category":"cs.CV"}
{"created":"2024-03-13 12:52:33","title":"Protocol Optimization for Functional Cardiac CT Imaging Using Noise Emulation in the Raw Data Domain","abstract":"Four-dimensional (4D) wide coverage computed tomography (CT) is an effective imaging modality for measuring the mechanical function of the myocardium. However, repeated CT measurement across several heartbeats is still a concern. A projection-domain noise emulation method is presented to generate accurate low-dose (mA modulated) 4D cardiac CT scans from high-dose scans, enabling protocol optimization to deliver sufficient image quality for functional cardiac analysis while using a dose level that is as low as reasonably achievable. Given a targeted low-dose mA modulation curve, the proposed noise emulation method injects both quantum and electronic noise of proper magnitude and correlation to the high-dose data in projection domain. A spatially varying detector gain term as well as its calibration method were proposed to further improve the noise emulation accuracy. To determine the low dose threshold, a projection domain image quality (IQ) metric was proposed that is based on the number of projection rays that do not fall under the non-linear region of the detector. Experiments were performed to validate the noise emulation method with both phantom and clinical data. For both phantom and clinical data, the low-dose emulated images exhibited similar noise magnitude, artifacts, and texture to that of the real low-dose images. The proposed channel-dependent detector gain term resulted in additional increase in emulation accuracy. Using the proposed IQ metric, recommended kVp and mA settings were calculated for low dose 4D Cardiac CT acquisitions for patients of different sizes. In conclusion, a detailed method to estimate system-dependent parameters for a raw-data based low dose emulation framework was described. The proposed low-dose emulation method can be used to prospectively select patient-specific minimal-dose protocols for functional cardiac CT.","sentences":["Four-dimensional (4D) wide coverage computed tomography (CT) is an effective imaging modality for measuring the mechanical function of the myocardium.","However, repeated CT measurement across several heartbeats is still a concern.","A projection-domain noise emulation method is presented to generate accurate low-dose (mA modulated) 4D cardiac CT scans from high-dose scans, enabling protocol optimization to deliver sufficient image quality for functional cardiac analysis while using a dose level that is as low as reasonably achievable.","Given a targeted low-dose mA modulation curve, the proposed noise emulation method injects both quantum and electronic noise of proper magnitude and correlation to the high-dose data in projection domain.","A spatially varying detector gain term as well as its calibration method were proposed to further improve the noise emulation accuracy.","To determine the low dose threshold, a projection domain image quality (IQ) metric was proposed that is based on the number of projection rays that do not fall under the non-linear region of the detector.","Experiments were performed to validate the noise emulation method with both phantom and clinical data.","For both phantom and clinical data, the low-dose emulated images exhibited similar noise magnitude, artifacts, and texture to that of the real low-dose images.","The proposed channel-dependent detector gain term resulted in additional increase in emulation accuracy.","Using the proposed IQ metric, recommended kVp and mA settings were calculated for low dose 4D Cardiac CT acquisitions for patients of different sizes.","In conclusion, a detailed method to estimate system-dependent parameters for a raw-data based low dose emulation framework was described.","The proposed low-dose emulation method can be used to prospectively select patient-specific minimal-dose protocols for functional cardiac CT."],"url":"http://arxiv.org/abs/2403.08486v1","category":"physics.med-ph"}
{"created":"2024-03-13 12:46:51","title":"SoK: Reducing the Vulnerability of Fine-tuned Language Models to Membership Inference Attacks","abstract":"Natural language processing models have experienced a significant upsurge in recent years, with numerous applications being built upon them. Many of these applications require fine-tuning generic base models on customized, proprietary datasets. This fine-tuning data is especially likely to contain personal or sensitive information about individuals, resulting in increased privacy risk. Membership inference attacks are the most commonly employed attack to assess the privacy leakage of a machine learning model. However, limited research is available on the factors that affect the vulnerability of language models to this kind of attack, or on the applicability of different defense strategies in the language domain. We provide the first systematic review of the vulnerability of fine-tuned large language models to membership inference attacks, the various factors that come into play, and the effectiveness of different defense strategies. We find that some training methods provide significantly reduced privacy risk, with the combination of differential privacy and low-rank adaptors achieving the best privacy protection against these attacks.","sentences":["Natural language processing models have experienced a significant upsurge in recent years, with numerous applications being built upon them.","Many of these applications require fine-tuning generic base models on customized, proprietary datasets.","This fine-tuning data is especially likely to contain personal or sensitive information about individuals, resulting in increased privacy risk.","Membership inference attacks are the most commonly employed attack to assess the privacy leakage of a machine learning model.","However, limited research is available on the factors that affect the vulnerability of language models to this kind of attack, or on the applicability of different defense strategies in the language domain.","We provide the first systematic review of the vulnerability of fine-tuned large language models to membership inference attacks, the various factors that come into play, and the effectiveness of different defense strategies.","We find that some training methods provide significantly reduced privacy risk, with the combination of differential privacy and low-rank adaptors achieving the best privacy protection against these attacks."],"url":"http://arxiv.org/abs/2403.08481v1","category":"cs.LG"}
{"created":"2024-03-13 12:46:36","title":"MD-Dose: A Diffusion Model based on the Mamba for Radiotherapy Dose Prediction","abstract":"Radiation therapy is crucial in cancer treatment. Experienced experts typically iteratively generate high-quality dose distribution maps, forming the basis for excellent radiation therapy plans. Therefore, automated prediction of dose distribution maps is significant in expediting the treatment process and providing a better starting point for developing radiation therapy plans. With the remarkable results of diffusion models in predicting high-frequency regions of dose distribution maps, dose prediction methods based on diffusion models have been extensively studied. However, existing methods mainly utilize CNNs or Transformers as denoising networks. CNNs lack the capture of global receptive fields, resulting in suboptimal prediction performance. Transformers excel in global modeling but face quadratic complexity with image size, resulting in significant computational overhead. To tackle these challenges, we introduce a novel diffusion model, MD-Dose, based on the Mamba architecture for predicting radiation therapy dose distribution in thoracic cancer patients. In the forward process, MD-Dose adds Gaussian noise to dose distribution maps to obtain pure noise images. In the backward process, MD-Dose utilizes a noise predictor based on the Mamba to predict the noise, ultimately outputting the dose distribution maps. Furthermore, We develop a Mamba encoder to extract structural information and integrate it into the noise predictor for localizing dose regions in the planning target volume (PTV) and organs at risk (OARs). Through extensive experiments on a dataset of 300 thoracic tumor patients, we showcase the superiority of MD-Dose in various metrics and time consumption.","sentences":["Radiation therapy is crucial in cancer treatment.","Experienced experts typically iteratively generate high-quality dose distribution maps, forming the basis for excellent radiation therapy plans.","Therefore, automated prediction of dose distribution maps is significant in expediting the treatment process and providing a better starting point for developing radiation therapy plans.","With the remarkable results of diffusion models in predicting high-frequency regions of dose distribution maps, dose prediction methods based on diffusion models have been extensively studied.","However, existing methods mainly utilize CNNs or Transformers as denoising networks.","CNNs lack the capture of global receptive fields, resulting in suboptimal prediction performance.","Transformers excel in global modeling but face quadratic complexity with image size, resulting in significant computational overhead.","To tackle these challenges, we introduce a novel diffusion model, MD-Dose, based on the Mamba architecture for predicting radiation therapy dose distribution in thoracic cancer patients.","In the forward process, MD-Dose adds Gaussian noise to dose distribution maps to obtain pure noise images.","In the backward process, MD-Dose utilizes a noise predictor based on the Mamba to predict the noise, ultimately outputting the dose distribution maps.","Furthermore, We develop a Mamba encoder to extract structural information and integrate it into the noise predictor for localizing dose regions in the planning target volume (PTV) and organs at risk (OARs).","Through extensive experiments on a dataset of 300 thoracic tumor patients, we showcase the superiority of MD-Dose in various metrics and time consumption."],"url":"http://arxiv.org/abs/2403.08479v1","category":"eess.IV"}
{"created":"2024-03-13 12:46:06","title":"Thermal Hall effect incorporating magnon damping in localized spin systems","abstract":"We propose a theory for thermal Hall transport mediated by magnons to address the impact of their damping resulting from magnon-magnon interactions in insulating magnets. This phenomenon is anticipated to be particularly significant in systems characterized by strong quantum fluctuations, exemplified by spin-1/2 systems. Employing a nonlinear flavor-wave theory, we analyze a general model for localized electron systems and develop a formulation for thermal conductivity based on a perturbation theory, utilizing bosonic Green's functions with a nonzero self-energy. We derive the expression of the thermal Hall conductivity incorporating magnon damping. To demonstrate the applicability of the obtained representation, we adopt it to two $S=1/2$ quantum spin models on a honeycomb lattice. In calculations for these systems, we make use of the self-consistent imaginary Dyson equation approach at finite temperatures for evaluating the magnon damping rate. In both systems, the thermal Hall conductivity is diminished due to the introduction of magnon damping over a wide temperature range. This effect arises due to the smearing of magnon spectra with nonzero Berry curvatures. We also discuss the relation to the damping of chiral edge modes of magnons. Our formulation can be applied to various localized electron systems as we begin with a general Hamiltonian for these systems. Our findings shed light on a new aspect of topological magnonics emergent from many-body effects and will stimulate further investigations on the impact of magnon damping on topological phenomena.","sentences":["We propose a theory for thermal Hall transport mediated by magnons to address the impact of their damping resulting from magnon-magnon interactions in insulating magnets.","This phenomenon is anticipated to be particularly significant in systems characterized by strong quantum fluctuations, exemplified by spin-1/2 systems.","Employing a nonlinear flavor-wave theory, we analyze a general model for localized electron systems and develop a formulation for thermal conductivity based on a perturbation theory, utilizing bosonic Green's functions with a nonzero self-energy.","We derive the expression of the thermal Hall conductivity incorporating magnon damping.","To demonstrate the applicability of the obtained representation, we adopt it to two $S=1/2$ quantum spin models on a honeycomb lattice.","In calculations for these systems, we make use of the self-consistent imaginary Dyson equation approach at finite temperatures for evaluating the magnon damping rate.","In both systems, the thermal Hall conductivity is diminished due to the introduction of magnon damping over a wide temperature range.","This effect arises due to the smearing of magnon spectra with nonzero Berry curvatures.","We also discuss the relation to the damping of chiral edge modes of magnons.","Our formulation can be applied to various localized electron systems as we begin with a general Hamiltonian for these systems.","Our findings shed light on a new aspect of topological magnonics emergent from many-body effects and will stimulate further investigations on the impact of magnon damping on topological phenomena."],"url":"http://arxiv.org/abs/2403.08478v1","category":"cond-mat.str-el"}
{"created":"2024-03-13 12:46:03","title":"Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts","abstract":"Conventional wisdom suggests parameter-efficient fine-tuning of foundation models as the state-of-the-art method for transfer learning in vision, replacing the rich literature of alternatives such as meta-learning. In trying to harness the best of both worlds, meta-tuning introduces a subsequent optimization stage of foundation models but has so far only shown limited success and crucially tends to underperform on out-of-domain (OOD) tasks. In this paper, we introduce Sparse MetA-Tuning (SMAT), a method inspired by sparse mixture-of-experts approaches and trained to isolate subsets of pre-trained parameters automatically for meta-tuning on each task. SMAT successfully overcomes OOD sensitivity and delivers on the promise of enhancing the transfer abilities of vision foundation models beyond parameter-efficient finetuning. We establish new state-of-the-art results on a challenging combination of Meta-Dataset augmented with additional OOD tasks in both zero-shot and gradient-based adaptation settings. In addition, we provide a thorough analysis of the superiority of learned over hand-designed sparsity patterns for sparse expert methods and the pivotal importance of the sparsity level in balancing between in-domain and out-of-domain generalization. Our code is publicly available.","sentences":["Conventional wisdom suggests parameter-efficient fine-tuning of foundation models as the state-of-the-art method for transfer learning in vision, replacing the rich literature of alternatives such as meta-learning.","In trying to harness the best of both worlds, meta-tuning introduces a subsequent optimization stage of foundation models but has so far only shown limited success and crucially tends to underperform on out-of-domain (OOD) tasks.","In this paper, we introduce Sparse MetA-Tuning (SMAT), a method inspired by sparse mixture-of-experts approaches and trained to isolate subsets of pre-trained parameters automatically for meta-tuning on each task.","SMAT successfully overcomes OOD sensitivity and delivers on the promise of enhancing the transfer abilities of vision foundation models beyond parameter-efficient finetuning.","We establish new state-of-the-art results on a challenging combination of Meta-Dataset augmented with additional OOD tasks in both zero-shot and gradient-based adaptation settings.","In addition, we provide a thorough analysis of the superiority of learned over hand-designed sparsity patterns for sparse expert methods and the pivotal importance of the sparsity level in balancing between in-domain and out-of-domain generalization.","Our code is publicly available."],"url":"http://arxiv.org/abs/2403.08477v1","category":"cs.CV"}
{"created":"2024-03-13 12:35:52","title":"Selective probing of longitudinal and transverse plasmon modes with electron phase-matching","abstract":"The optical properties of metallic nanoparticles are dominated by localized surface plasmons (LSPs). Their properties only depend on the constituting material, the size and shape of the nano-object as well as its surrounding medium. In anisotropic structures, such as metallic nanorods, two families of modes generally exist, transverse and longitudinal. Their spectral and spatial overlaps usually impede their separate measurements in electron energy loss spectroscopy (EELS). In this work, we propose three different strategies enabling to overcome this difficulty and selectively probe longitudinal and transverse modes. The first strategy is numeric and relies on morphing of nano-structures, rooted in the geometrical nature of LSPs. The two other strategies exploit the relativistic and wave nature of the electrons in an EELS experiment. The first one is the phase-matching between the electron and the plasmon excitation to enhance their coupling by either tilting the sample and modifying the electron kinetic energy. The second one - polarized EELS (pEELS) - exploits the wave nature of electrons to mimic selection rules analogous to the one existing in light spectroscopies. The above-mentioned strategies are exemplified - either experimentally or numerically - on a canonical plasmonic toy model: the nano-rod. The goal of the paper is to bring together the state-of-the-art concepts of EELS for plasmonics to tackle a pedestrian problem in this field.","sentences":["The optical properties of metallic nanoparticles are dominated by localized surface plasmons (LSPs).","Their properties only depend on the constituting material, the size and shape of the nano-object as well as its surrounding medium.","In anisotropic structures, such as metallic nanorods, two families of modes generally exist, transverse and longitudinal.","Their spectral and spatial overlaps usually impede their separate measurements in electron energy loss spectroscopy (EELS).","In this work, we propose three different strategies enabling to overcome this difficulty and selectively probe longitudinal and transverse modes.","The first strategy is numeric and relies on morphing of nano-structures, rooted in the geometrical nature of LSPs.","The two other strategies exploit the relativistic and wave nature of the electrons in an EELS experiment.","The first one is the phase-matching between the electron and the plasmon excitation to enhance their coupling by either tilting the sample and modifying the electron kinetic energy.","The second one - polarized EELS (pEELS) - exploits the wave nature of electrons to mimic selection rules analogous to the one existing in light spectroscopies.","The above-mentioned strategies are exemplified - either experimentally or numerically - on a canonical plasmonic toy model: the nano-rod.","The goal of the paper is to bring together the state-of-the-art concepts of EELS for plasmonics to tackle a pedestrian problem in this field."],"url":"http://arxiv.org/abs/2403.08471v1","category":"physics.optics"}
{"created":"2024-03-13 12:31:08","title":"An Analysis of Human Alignment of Latent Diffusion Models","abstract":"Diffusion models, trained on large amounts of data, showed remarkable performance for image synthesis. They have high error consistency with humans and low texture bias when used for classification. Furthermore, prior work demonstrated the decomposability of their bottleneck layer representations into semantic directions. In this work, we analyze how well such representations are aligned to human responses on a triplet odd-one-out task. We find that despite the aforementioned observations: I) The representational alignment with humans is comparable to that of models trained only on ImageNet-1k. II) The most aligned layers of the denoiser U-Net are intermediate layers and not the bottleneck. III) Text conditioning greatly improves alignment at high noise levels, hinting at the importance of abstract textual information, especially in the early stage of generation.","sentences":["Diffusion models, trained on large amounts of data, showed remarkable performance for image synthesis.","They have high error consistency with humans and low texture bias when used for classification.","Furthermore, prior work demonstrated the decomposability of their bottleneck layer representations into semantic directions.","In this work, we analyze how well such representations are aligned to human responses on a triplet odd-one-out task.","We find that despite the aforementioned observations: I)","The representational alignment with humans is comparable to that of models trained only on ImageNet-1k. II)","The most aligned layers of the denoiser U-Net are intermediate layers and not the bottleneck.","III)","Text conditioning greatly improves alignment at high noise levels, hinting at the importance of abstract textual information, especially in the early stage of generation."],"url":"http://arxiv.org/abs/2403.08469v1","category":"cs.LG"}
{"created":"2024-03-13 12:29:00","title":"Uniqueness of extremal charged black holes in de Sitter","abstract":"We prove a uniqueness theorem for the charged Nariai black holes and ultracold black holes in four dimensions. In particular, we show that an analytic solution to four-dimensional Einstein-Maxwell theory with a positive cosmological constant containing a static extremal Killing horizon with spherical cross-sections of large radius (compared to the cosmological scale), must be locally isometric to the extremal Reissner-Nordstr\\\"om-de Sitter black hole or its near-horizon geometry. The theorem generalises to extremal static horizons with small radius, establishing uniqueness of cold black holes for generic values of the radius.","sentences":["We prove a uniqueness theorem for the charged Nariai black holes and ultracold black holes in four dimensions.","In particular, we show that an analytic solution to four-dimensional Einstein-Maxwell theory with a positive cosmological constant containing a static extremal Killing horizon with spherical cross-sections of large radius (compared to the cosmological scale), must be locally isometric to the extremal Reissner-Nordstr\\\"om-de Sitter black hole or its near-horizon geometry.","The theorem generalises to extremal static horizons with small radius, establishing uniqueness of cold black holes for generic values of the radius."],"url":"http://arxiv.org/abs/2403.08467v1","category":"gr-qc"}
{"created":"2024-03-13 12:28:28","title":"Evidence of robust, universal conformal invariance in living biological matter","abstract":"Collective cellular movement plays a crucial role in many processes fundamental to health, including development, reproduction, infection, wound healing, and cancer. The emergent dynamics that arise in these systems are typically thought to depend on how cells interact with one another and the mechanisms used to drive motility, both of which exhibit remarkable diversity across different biological systems. Here, we report experimental evidence of a universal feature in the patterns of flow that spontaneously emerges in groups of collectively moving cells. Specifically, we demonstrate that the flows generated by collectively moving dog kidney cells, human breast cancer cells, and by two different strains of pathogenic bacteria, all exhibit conformal invariance. Remarkably, not only do our results show that all of these very different systems display robust conformal invariance, but we also discovered that the precise form of the invariance in all four systems is described by the Schramm-Loewner Evolution (SLE), and belongs to the percolation universality class. A continuum model of active matter can recapitulate both the observed conformal invariance and SLE form found in experiments. The presence of universal conformal invariance reveals that the macroscopic features of living biological matter exhibit universal translational, rotational, and scale symmetries that are independent of the microscopic properties of its constituents. Our results show that the patterns of flows generated by diverse cellular systems are highly conserved and that biological systems can unexpectedly be used to experimentally test predictions from the theories for conformally invariant structures","sentences":["Collective cellular movement plays a crucial role in many processes fundamental to health, including development, reproduction, infection, wound healing, and cancer.","The emergent dynamics that arise in these systems are typically thought to depend on how cells interact with one another and the mechanisms used to drive motility, both of which exhibit remarkable diversity across different biological systems.","Here, we report experimental evidence of a universal feature in the patterns of flow that spontaneously emerges in groups of collectively moving cells.","Specifically, we demonstrate that the flows generated by collectively moving dog kidney cells, human breast cancer cells, and by two different strains of pathogenic bacteria, all exhibit conformal invariance.","Remarkably, not only do our results show that all of these very different systems display robust conformal invariance, but we also discovered that the precise form of the invariance in all four systems is described by the Schramm-Loewner Evolution (SLE), and belongs to the percolation universality class.","A continuum model of active matter can recapitulate both the observed conformal invariance and SLE form found in experiments.","The presence of universal conformal invariance reveals that the macroscopic features of living biological matter exhibit universal translational, rotational, and scale symmetries that are independent of the microscopic properties of its constituents.","Our results show that the patterns of flows generated by diverse cellular systems are highly conserved and that biological systems can unexpectedly be used to experimentally test predictions from the theories for conformally invariant structures"],"url":"http://arxiv.org/abs/2403.08466v1","category":"cond-mat.soft"}
{"created":"2024-03-13 12:26:50","title":"A Comparison of SynDiffix Multi-table versus Single-table Synthetic Data","abstract":"SynDiffix is a new open-source tool for structured data synthesis. It has anonymization features that allow it to generate multiple synthetic tables while maintaining strong anonymity. Compared to the more common single-table approach, multi-table leads to more accurate data, since only the features of interest for a given analysis need be synthesized. This paper compares SynDiffix with 15 other commercial and academic synthetic data techniques using the SDNIST analysis framework, modified by us to accommodate multi-table synthetic data. The results show that SynDiffix is many times more accurate than other approaches for low-dimension tables, but somewhat worse than the best single-table techniques for high-dimension tables.","sentences":["SynDiffix is a new open-source tool for structured data synthesis.","It has anonymization features that allow it to generate multiple synthetic tables while maintaining strong anonymity.","Compared to the more common single-table approach, multi-table leads to more accurate data, since only the features of interest for a given analysis need be synthesized.","This paper compares SynDiffix with 15 other commercial and academic synthetic data techniques using the SDNIST analysis framework, modified by us to accommodate multi-table synthetic data.","The results show that SynDiffix is many times more accurate than other approaches for low-dimension tables, but somewhat worse than the best single-table techniques for high-dimension tables."],"url":"http://arxiv.org/abs/2403.08463v1","category":"cs.CR"}
{"created":"2024-03-13 12:20:20","title":"Towards Dense and Accurate Radar Perception Via Efficient Cross-Modal Diffusion Model","abstract":"Millimeter wave (mmWave) radars have attracted significant attention from both academia and industry due to their capability to operate in extreme weather conditions. However, they face challenges in terms of sparsity and noise interference, which hinder their application in the field of micro aerial vehicle (MAV) autonomous navigation. To this end, this paper proposes a novel approach to dense and accurate mmWave radar point cloud construction via cross-modal learning. Specifically, we introduce diffusion models, which possess state-of-the-art performance in generative modeling, to predict LiDAR-like point clouds from paired raw radar data. We also incorporate the most recent diffusion model inference accelerating techniques to ensure that the proposed method can be implemented on MAVs with limited computing resources.We validate the proposed method through extensive benchmark comparisons and real-world experiments, demonstrating its superior performance and generalization ability. Code and pretrained models will be available at https://github.com/ZJU-FAST-Lab/Radar-Diffusion.","sentences":["Millimeter wave (mmWave) radars have attracted significant attention from both academia and industry due to their capability to operate in extreme weather conditions.","However, they face challenges in terms of sparsity and noise interference, which hinder their application in the field of micro aerial vehicle (MAV) autonomous navigation.","To this end, this paper proposes a novel approach to dense and accurate mmWave radar point cloud construction via cross-modal learning.","Specifically, we introduce diffusion models, which possess state-of-the-art performance in generative modeling, to predict LiDAR-like point clouds from paired raw radar data.","We also incorporate the most recent diffusion model inference accelerating techniques to ensure that the proposed method can be implemented on MAVs with limited computing resources.","We validate the proposed method through extensive benchmark comparisons and real-world experiments, demonstrating its superior performance and generalization ability.","Code and pretrained models will be available at https://github.com/ZJU-FAST-Lab/Radar-Diffusion."],"url":"http://arxiv.org/abs/2403.08460v1","category":"cs.CV"}
{"created":"2024-03-13 12:09:44","title":"IAMCV Multi-Scenario Vehicle Interaction Dataset","abstract":"The acquisition and analysis of high-quality sensor data constitute an essential requirement in shaping the development of fully autonomous driving systems. This process is indispensable for enhancing road safety and ensuring the effectiveness of the technological advancements in the automotive industry. This study introduces the Interaction of Autonomous and Manually-Controlled Vehicles (IAMCV) dataset, a novel and extensive dataset focused on inter-vehicle interactions. The dataset, enriched with a sophisticated array of sensors such as Light Detection and Ranging, cameras, Inertial Measurement Unit/Global Positioning System, and vehicle bus data acquisition, provides a comprehensive representation of real-world driving scenarios that include roundabouts, intersections, country roads, and highways, recorded across diverse locations in Germany. Furthermore, the study shows the versatility of the IAMCV dataset through several proof-of-concept use cases. Firstly, an unsupervised trajectory clustering algorithm illustrates the dataset's capability in categorizing vehicle movements without the need for labeled training data. Secondly, we compare an online camera calibration method with the Robot Operating System-based standard, using images captured in the dataset. Finally, a preliminary test employing the YOLOv8 object-detection model is conducted, augmented by reflections on the transferability of object detection across various LIDAR resolutions. These use cases underscore the practical utility of the collected dataset, emphasizing its potential to advance research and innovation in the area of intelligent vehicles.","sentences":["The acquisition and analysis of high-quality sensor data constitute an essential requirement in shaping the development of fully autonomous driving systems.","This process is indispensable for enhancing road safety and ensuring the effectiveness of the technological advancements in the automotive industry.","This study introduces the Interaction of Autonomous and Manually-Controlled Vehicles (IAMCV) dataset, a novel and extensive dataset focused on inter-vehicle interactions.","The dataset, enriched with a sophisticated array of sensors such as Light Detection and Ranging, cameras, Inertial Measurement Unit/Global Positioning System, and vehicle bus data acquisition, provides a comprehensive representation of real-world driving scenarios that include roundabouts, intersections, country roads, and highways, recorded across diverse locations in Germany.","Furthermore, the study shows the versatility of the IAMCV dataset through several proof-of-concept use cases.","Firstly, an unsupervised trajectory clustering algorithm illustrates the dataset's capability in categorizing vehicle movements without the need for labeled training data.","Secondly, we compare an online camera calibration method with the Robot Operating System-based standard, using images captured in the dataset.","Finally, a preliminary test employing the YOLOv8 object-detection model is conducted, augmented by reflections on the transferability of object detection across various LIDAR resolutions.","These use cases underscore the practical utility of the collected dataset, emphasizing its potential to advance research and innovation in the area of intelligent vehicles."],"url":"http://arxiv.org/abs/2403.08455v1","category":"cs.RO"}
{"created":"2024-03-13 12:00:48","title":"Generating Synthetic Computed Tomography for Radiotherapy: SynthRAD2023 Challenge Report","abstract":"Radiation therapy plays a crucial role in cancer treatment, necessitating precise delivery of radiation to tumors while sparing healthy tissues over multiple days. Computed tomography (CT) is integral for treatment planning, offering electron density data crucial for accurate dose calculations. However, accurately representing patient anatomy is challenging, especially in adaptive radiotherapy, where CT is not acquired daily. Magnetic resonance imaging (MRI) provides superior soft-tissue contrast. Still, it lacks electron density information while cone beam CT (CBCT) lacks direct electron density calibration and is mainly used for patient positioning. Adopting MRI-only or CBCT-based adaptive radiotherapy eliminates the need for CT planning but presents challenges. Synthetic CT (sCT) generation techniques aim to address these challenges by using image synthesis to bridge the gap between MRI, CBCT, and CT. The SynthRAD2023 challenge was organized to compare synthetic CT generation methods using multi-center ground truth data from 1080 patients, divided into two tasks: 1) MRI-to-CT and 2) CBCT-to-CT. The evaluation included image similarity and dose-based metrics from proton and photon plans. The challenge attracted significant participation, with 617 registrations and 22/17 valid submissions for tasks 1/2. Top-performing teams achieved high structural similarity indices (>0.87/0.90) and gamma pass rates for photon (>98.1%/99.0%) and proton (>99.0%/97.3%) plans. However, no significant correlation was found between image similarity metrics and dose accuracy, emphasizing the need for dose evaluation when assessing the clinical applicability of sCT. SynthRAD2023 facilitated the investigation and benchmarking of sCT generation techniques, providing insights for developing MRI-only and CBCT-based adaptive radiotherapy.","sentences":["Radiation therapy plays a crucial role in cancer treatment, necessitating precise delivery of radiation to tumors while sparing healthy tissues over multiple days.","Computed tomography (CT) is integral for treatment planning, offering electron density data crucial for accurate dose calculations.","However, accurately representing patient anatomy is challenging, especially in adaptive radiotherapy, where CT is not acquired daily.","Magnetic resonance imaging (MRI) provides superior soft-tissue contrast.","Still, it lacks electron density information while cone beam CT (CBCT) lacks direct electron density calibration and is mainly used for patient positioning.","Adopting MRI-only or CBCT-based adaptive radiotherapy eliminates the need for CT planning but presents challenges.","Synthetic CT (sCT) generation techniques aim to address these challenges by using image synthesis to bridge the gap between MRI, CBCT, and CT.","The SynthRAD2023 challenge was organized to compare synthetic CT generation methods using multi-center ground truth data from 1080 patients, divided into two tasks: 1) MRI-to-CT and 2) CBCT-to-CT.","The evaluation included image similarity and dose-based metrics from proton and photon plans.","The challenge attracted significant participation, with 617 registrations and 22/17 valid submissions for tasks 1/2.","Top-performing teams achieved high structural similarity indices (>0.87/0.90) and gamma pass rates for photon (>98.1%/99.0%) and proton (>99.0%/97.3%) plans.","However, no significant correlation was found between image similarity metrics and dose accuracy, emphasizing the need for dose evaluation when assessing the clinical applicability of sCT.","SynthRAD2023 facilitated the investigation and benchmarking of sCT generation techniques, providing insights for developing MRI-only and CBCT-based adaptive radiotherapy."],"url":"http://arxiv.org/abs/2403.08447v1","category":"physics.med-ph"}
{"created":"2024-03-13 11:56:10","title":"COSTREAM: Learned Cost Models for Operator Placement in Edge-Cloud Environments","abstract":"In this work, we present COSTREAM, a novel learned cost model for Distributed Stream Processing Systems that provides accurate predictions of the execution costs of a streaming query in an edge-cloud environment. The cost model can be used to find an initial placement of operators across heterogeneous hardware, which is particularly important in these environments. In our evaluation, we demonstrate that COSTREAM can produce highly accurate cost estimates for the initial operator placement and even generalize to unseen placements, queries, and hardware. When using COSTREAM to optimize the placements of streaming operators, a median speed-up of around 21x can be achieved compared to baselines.","sentences":["In this work, we present COSTREAM, a novel learned cost model for Distributed Stream Processing Systems that provides accurate predictions of the execution costs of a streaming query in an edge-cloud environment.","The cost model can be used to find an initial placement of operators across heterogeneous hardware, which is particularly important in these environments.","In our evaluation, we demonstrate that COSTREAM can produce highly accurate cost estimates for the initial operator placement and even generalize to unseen placements, queries, and hardware.","When using COSTREAM to optimize the placements of streaming operators, a median speed-up of around 21x can be achieved compared to baselines."],"url":"http://arxiv.org/abs/2403.08444v1","category":"cs.DC"}
{"created":"2024-03-13 11:55:44","title":"Sensor Network Localization via Riemannian Conjugate Gradient and Rank Reduction: An Extended Version","abstract":"This paper addresses the Sensor Network Localization (SNL) problem using received signal strength. The SNL is formulated as an Euclidean Distance Matrix Completion (EDMC) problem under the unit ball sample model. Using the Burer-Monteiro factorization type cost function, the EDMC is solved by Riemannian conjugate gradient with Hager-Zhang line search method on a quotient manifold. A \"rank reduction\" preprocess is proposed for proper initialization and to achieve global convergence with high probability. Simulations on a synthetic scene show that our approach attains better localization accuracy and is computationally efficient compared to several baseline methods. Characterization of a small local basin of attraction around the global optima of the s-stress function under Bernoulli sampling rule and incoherence matrix completion framework is conducted for the first time. Theoretical result conjectures that the Euclidean distance problem with a structure-less sample mask can be effectively handled using spectral initialization followed by vanilla first-order methods. This preliminary analysis, along with the aforementioned numerical accomplishments, provides insights into revealing the landscape of the s-stress function and may stimulate the design of simpler algorithms to tackle the non-convex formulation of general EDMC problems.","sentences":["This paper addresses the Sensor Network Localization (SNL) problem using received signal strength.","The SNL is formulated as an Euclidean Distance Matrix Completion (EDMC) problem under the unit ball sample model.","Using the Burer-Monteiro factorization type cost function, the EDMC is solved by Riemannian conjugate gradient with Hager-Zhang line search method on a quotient manifold.","A \"rank reduction\" preprocess is proposed for proper initialization and to achieve global convergence with high probability.","Simulations on a synthetic scene show that our approach attains better localization accuracy and is computationally efficient compared to several baseline methods.","Characterization of a small local basin of attraction around the global optima of the s-stress function under Bernoulli sampling rule and incoherence matrix completion framework is conducted for the first time.","Theoretical result conjectures that the Euclidean distance problem with a structure-less sample mask can be effectively handled using spectral initialization followed by vanilla first-order methods.","This preliminary analysis, along with the aforementioned numerical accomplishments, provides insights into revealing the landscape of the s-stress function and may stimulate the design of simpler algorithms to tackle the non-convex formulation of general EDMC problems."],"url":"http://arxiv.org/abs/2403.08442v1","category":"eess.SP"}
{"created":"2024-03-13 11:54:25","title":"Stabilizer ground states: theory, algorithms and applications","abstract":"Stabilizer states have been commonly utilized in quantum information, quantum error correction, and quantum circuit simulation due to their simple mathematical structure. In this work, we apply stabilizer states to tackle quantum many-body problems and introduce the concept of stabilizer ground states. We present a simplified equivalent formalism for identifying stabilizer ground states of general Pauli Hamiltonians. Moreover, we also develop an exact and linear-scaled algorithm to obtain stabilizer ground states of 1D local Hamiltonians and thus free from discrete optimization. This proposed theoretical formalism and linear-scaled algorithm are not only applicable to finite-size systems, but also adaptable to infinite periodic systems. The scalability and efficiency of the algorithms are numerically benchmarked on different Hamiltonians. Finally, we demonstrate that stabilizer ground states can find various promising applications including better design on variational quantum algorithms, qualitative understanding of phase transitions, and cornerstones for more advanced ground state ansatzes.","sentences":["Stabilizer states have been commonly utilized in quantum information, quantum error correction, and quantum circuit simulation due to their simple mathematical structure.","In this work, we apply stabilizer states to tackle quantum many-body problems and introduce the concept of stabilizer ground states.","We present a simplified equivalent formalism for identifying stabilizer ground states of general Pauli Hamiltonians.","Moreover, we also develop an exact and linear-scaled algorithm to obtain stabilizer ground states of 1D local Hamiltonians and thus free from discrete optimization.","This proposed theoretical formalism and linear-scaled algorithm are not only applicable to finite-size systems, but also adaptable to infinite periodic systems.","The scalability and efficiency of the algorithms are numerically benchmarked on different Hamiltonians.","Finally, we demonstrate that stabilizer ground states can find various promising applications including better design on variational quantum algorithms, qualitative understanding of phase transitions, and cornerstones for more advanced ground state ansatzes."],"url":"http://arxiv.org/abs/2403.08441v1","category":"quant-ph"}
{"created":"2024-03-13 11:44:30","title":"Reproducibility and Geometric Intrinsic Dimensionality: An Investigation on Graph Neural Network Research","abstract":"Difficulties in replication and reproducibility of empirical evidences in machine learning research have become a prominent topic in recent years. Ensuring that machine learning research results are sound and reliable requires reproducibility, which verifies the reliability of research findings using the same code and data. This promotes open and accessible research, robust experimental workflows, and the rapid integration of new findings. Evaluating the degree to which research publications support these different aspects of reproducibility is one goal of the present work. For this we introduce an ontology of reproducibility in machine learning and apply it to methods for graph neural networks. Building on these efforts we turn towards another critical challenge in machine learning, namely the curse of dimensionality, which poses challenges in data collection, representation, and analysis, making it harder to find representative data and impeding the training and inference processes. Using the closely linked concept of geometric intrinsic dimension we investigate to which extend the used machine learning models are influenced by the intrinsic dimension of the data sets they are trained on.","sentences":["Difficulties in replication and reproducibility of empirical evidences in machine learning research have become a prominent topic in recent years.","Ensuring that machine learning research results are sound and reliable requires reproducibility, which verifies the reliability of research findings using the same code and data.","This promotes open and accessible research, robust experimental workflows, and the rapid integration of new findings.","Evaluating the degree to which research publications support these different aspects of reproducibility is one goal of the present work.","For this we introduce an ontology of reproducibility in machine learning and apply it to methods for graph neural networks.","Building on these efforts we turn towards another critical challenge in machine learning, namely the curse of dimensionality, which poses challenges in data collection, representation, and analysis, making it harder to find representative data and impeding the training and inference processes.","Using the closely linked concept of geometric intrinsic dimension we investigate to which extend the used machine learning models are influenced by the intrinsic dimension of the data sets they are trained on."],"url":"http://arxiv.org/abs/2403.08438v1","category":"cs.LG"}
{"created":"2024-03-13 11:39:30","title":"PFStorer: Personalized Face Restoration and Super-Resolution","abstract":"Recent developments in face restoration have achieved remarkable results in producing high-quality and lifelike outputs. The stunning results however often fail to be faithful with respect to the identity of the person as the models lack necessary context. In this paper, we explore the potential of personalized face restoration with diffusion models. In our approach a restoration model is personalized using a few images of the identity, leading to tailored restoration with respect to the identity while retaining fine-grained details. By using independent trainable blocks for personalization, the rich prior of a base restoration model can be exploited to its fullest. To avoid the model relying on parts of identity left in the conditioning low-quality images, a generative regularizer is employed. With a learnable parameter, the model learns to balance between the details generated based on the input image and the degree of personalization. Moreover, we improve the training pipeline of face restoration models to enable an alignment-free approach. We showcase the robust capabilities of our approach in several real-world scenarios with multiple identities, demonstrating our method's ability to generate fine-grained details with faithful restoration. In the user study we evaluate the perceptual quality and faithfulness of the genereated details, with our method being voted best 61% of the time compared to the second best with 25% of the votes.","sentences":["Recent developments in face restoration have achieved remarkable results in producing high-quality and lifelike outputs.","The stunning results however often fail to be faithful with respect to the identity of the person as the models lack necessary context.","In this paper, we explore the potential of personalized face restoration with diffusion models.","In our approach a restoration model is personalized using a few images of the identity, leading to tailored restoration with respect to the identity while retaining fine-grained details.","By using independent trainable blocks for personalization, the rich prior of a base restoration model can be exploited to its fullest.","To avoid the model relying on parts of identity left in the conditioning low-quality images, a generative regularizer is employed.","With a learnable parameter, the model learns to balance between the details generated based on the input image and the degree of personalization.","Moreover, we improve the training pipeline of face restoration models to enable an alignment-free approach.","We showcase the robust capabilities of our approach in several real-world scenarios with multiple identities, demonstrating our method's ability to generate fine-grained details with faithful restoration.","In the user study we evaluate the perceptual quality and faithfulness of the genereated details, with our method being voted best 61% of the time compared to the second best with 25% of the votes."],"url":"http://arxiv.org/abs/2403.08436v1","category":"cs.CV"}
{"created":"2024-03-13 11:31:17","title":"Modelling of initially stressed solids: structure of the energy density in the incompressible limit","abstract":"This study addresses the modelling of elastic bodies, particularly when the relaxed configuration is unknown or non-existent. We adopt the theory of initially stressed materials, incorporating the deformation gradient and stress state of the reference configuration (initial stress tensor) into the response function. We show that for the theory to be applicable, the response function of the relaxed material is invertible up to an element of the material symmetry group. Additionally, we establish that commonly imposed constitutive restrictions, namely the initial stress compatibility condition and initial stress reference independence, naturally arise when assuming an initial stress generated solely from elastic distortion. The paper delves into modelling aspects concerning incompressible materials, showcasing the expressibility of strain energy density as a function of the deviatoric part of the initial stress tensor and the isochoric part of the deformation gradient. This not only reduces the number of independent invariants in the energy functional, but also enhances numerical robustness in finite element simulations. The findings of this research hold significant implications for modelling materials with initial stress, extending potential applications to areas such as mechanobiology, soft robotics, and 4D printing.","sentences":["This study addresses the modelling of elastic bodies, particularly when the relaxed configuration is unknown or non-existent.","We adopt the theory of initially stressed materials, incorporating the deformation gradient and stress state of the reference configuration (initial stress tensor) into the response function.","We show that for the theory to be applicable, the response function of the relaxed material is invertible up to an element of the material symmetry group.","Additionally, we establish that commonly imposed constitutive restrictions, namely the initial stress compatibility condition and initial stress reference independence, naturally arise when assuming an initial stress generated solely from elastic distortion.","The paper delves into modelling aspects concerning incompressible materials, showcasing the expressibility of strain energy density as a function of the deviatoric part of the initial stress tensor and the isochoric part of the deformation gradient.","This not only reduces the number of independent invariants in the energy functional, but also enhances numerical robustness in finite element simulations.","The findings of this research hold significant implications for modelling materials with initial stress, extending potential applications to areas such as mechanobiology, soft robotics, and 4D printing."],"url":"http://arxiv.org/abs/2403.08432v1","category":"cond-mat.soft"}
{"created":"2024-03-13 11:29:37","title":"Search-based Optimisation of LLM Learning Shots for Story Point Estimation","abstract":"One of the ways Large Language Models (LLMs) are used to perform machine learning tasks is to provide them with a few examples before asking them to produce a prediction. This is a meta-learning process known as few-shot learning. In this paper, we use available Search-Based methods to optimise the number and combination of examples that can improve an LLM's estimation performance, when it is used to estimate story points for new agile tasks. Our preliminary results show that our SBSE technique improves the estimation performance of the LLM by 59.34% on average (in terms of mean absolute error of the estimation) over three datasets against a zero-shot setting.","sentences":["One of the ways Large Language Models (LLMs) are used to perform machine learning tasks is to provide them with a few examples before asking them to produce a prediction.","This is a meta-learning process known as few-shot learning.","In this paper, we use available Search-Based methods to optimise the number and combination of examples that can improve an LLM's estimation performance, when it is used to estimate story points for new agile tasks.","Our preliminary results show that our SBSE technique improves the estimation performance of the LLM by 59.34% on average (in terms of mean absolute error of the estimation) over three datasets against a zero-shot setting."],"url":"http://arxiv.org/abs/2403.08430v1","category":"cs.SE"}
{"created":"2024-03-13 11:29:13","title":"Software Vulnerability and Functionality Assessment using LLMs","abstract":"While code review is central to the software development process, it can be tedious and expensive to carry out. In this paper, we investigate whether and how Large Language Models (LLMs) can aid with code reviews. Our investigation focuses on two tasks that we argue are fundamental to good reviews: (i) flagging code with security vulnerabilities and (ii) performing software functionality validation, i.e., ensuring that code meets its intended functionality. To test performance on both tasks, we use zero-shot and chain-of-thought prompting to obtain final ``approve or reject'' recommendations. As data, we employ seminal code generation datasets (HumanEval and MBPP) along with expert-written code snippets with security vulnerabilities from the Common Weakness Enumeration (CWE). Our experiments consider a mixture of three proprietary models from OpenAI and smaller open-source LLMs. We find that the former outperforms the latter by a large margin. Motivated by promising results, we finally ask our models to provide detailed descriptions of security vulnerabilities. Results show that 36.7% of LLM-generated descriptions can be associated with true CWE vulnerabilities.","sentences":["While code review is central to the software development process, it can be tedious and expensive to carry out.","In this paper, we investigate whether and how Large Language Models (LLMs) can aid with code reviews.","Our investigation focuses on two tasks that we argue are fundamental to good reviews: (i) flagging code with security vulnerabilities and (ii) performing software functionality validation, i.e., ensuring that code meets its intended functionality.","To test performance on both tasks, we use zero-shot and chain-of-thought prompting to obtain final ``approve or reject'' recommendations.","As data, we employ seminal code generation datasets (HumanEval and MBPP) along with expert-written code snippets with security vulnerabilities from the Common Weakness Enumeration (CWE).","Our experiments consider a mixture of three proprietary models from OpenAI and smaller open-source LLMs.","We find that the former outperforms the latter by a large margin.","Motivated by promising results, we finally ask our models to provide detailed descriptions of security vulnerabilities.","Results show that 36.7% of LLM-generated descriptions can be associated with true CWE vulnerabilities."],"url":"http://arxiv.org/abs/2403.08429v1","category":"cs.SE"}
{"created":"2024-03-13 11:23:55","title":"Language-Driven Visual Consensus for Zero-Shot Semantic Segmentation","abstract":"The pre-trained vision-language model, exemplified by CLIP, advances zero-shot semantic segmentation by aligning visual features with class embeddings through a transformer decoder to generate semantic masks. Despite its effectiveness, prevailing methods within this paradigm encounter challenges, including overfitting on seen classes and small fragmentation in masks. To mitigate these issues, we propose a Language-Driven Visual Consensus (LDVC) approach, fostering improved alignment of semantic and visual information.Specifically, we leverage class embeddings as anchors due to their discrete and abstract nature, steering vision features toward class embeddings. Moreover, to circumvent noisy alignments from the vision part due to its redundant nature, we introduce route attention into self-attention for finding visual consensus, thereby enhancing semantic consistency within the same object. Equipped with a vision-language prompting strategy, our approach significantly boosts the generalization capacity of segmentation models for unseen classes. Experimental results underscore the effectiveness of our approach, showcasing mIoU gains of 4.5 on the PASCAL VOC 2012 and 3.6 on the COCO-Stuff 164k for unseen classes compared with the state-of-the-art methods.","sentences":["The pre-trained vision-language model, exemplified by CLIP, advances zero-shot semantic segmentation by aligning visual features with class embeddings through a transformer decoder to generate semantic masks.","Despite its effectiveness, prevailing methods within this paradigm encounter challenges, including overfitting on seen classes and small fragmentation in masks.","To mitigate these issues, we propose a Language-Driven Visual Consensus (LDVC) approach, fostering improved alignment of semantic and visual information.","Specifically, we leverage class embeddings as anchors due to their discrete and abstract nature, steering vision features toward class embeddings.","Moreover, to circumvent noisy alignments from the vision part due to its redundant nature, we introduce route attention into self-attention for finding visual consensus, thereby enhancing semantic consistency within the same object.","Equipped with a vision-language prompting strategy, our approach significantly boosts the generalization capacity of segmentation models for unseen classes.","Experimental results underscore the effectiveness of our approach, showcasing mIoU gains of 4.5 on the PASCAL VOC 2012 and 3.6 on the COCO-Stuff 164k for unseen classes compared with the state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.08426v1","category":"cs.CV"}
{"created":"2024-03-13 11:20:34","title":"Specification Overfitting in Artificial Intelligence","abstract":"Machine learning (ML) and artificial intelligence (AI) approaches are often criticized for their inherent bias and for their lack of control, accountability, and transparency. Consequently, regulatory bodies struggle with containing this technology's potential negative side effects. High-level requirements such as fairness and robustness need to be formalized into concrete specification metrics, imperfect proxies that capture isolated aspects of the underlying requirements. Given possible trade-offs between different metrics and their vulnerability to over-optimization, integrating specification metrics in system development processes is not trivial. This paper defines specification overfitting, a scenario where systems focus excessively on specified metrics to the detriment of high-level requirements and task performance. We present an extensive literature survey to categorize how researchers propose, measure, and optimize specification metrics in several AI fields (e.g., natural language processing, computer vision, reinforcement learning). Using a keyword-based search on papers from major AI conferences and journals between 2018 and mid-2023, we identify and analyze 74 papers that propose or optimize specification metrics. We find that although most papers implicitly address specification overfitting (e.g., by reporting more than one specification metric), they rarely discuss which role specification metrics should play in system development or explicitly define the scope and assumptions behind metric formulations.","sentences":["Machine learning (ML) and artificial intelligence (AI) approaches are often criticized for their inherent bias and for their lack of control, accountability, and transparency.","Consequently, regulatory bodies struggle with containing this technology's potential negative side effects.","High-level requirements such as fairness and robustness need to be formalized into concrete specification metrics, imperfect proxies that capture isolated aspects of the underlying requirements.","Given possible trade-offs between different metrics and their vulnerability to over-optimization, integrating specification metrics in system development processes is not trivial.","This paper defines specification overfitting, a scenario where systems focus excessively on specified metrics to the detriment of high-level requirements and task performance.","We present an extensive literature survey to categorize how researchers propose, measure, and optimize specification metrics in several AI fields (e.g., natural language processing, computer vision, reinforcement learning).","Using a keyword-based search on papers from major AI conferences and journals between 2018 and mid-2023, we identify and analyze 74 papers that propose or optimize specification metrics.","We find that although most papers implicitly address specification overfitting (e.g., by reporting more than one specification metric), they rarely discuss which role specification metrics should play in system development or explicitly define the scope and assumptions behind metric formulations."],"url":"http://arxiv.org/abs/2403.08425v1","category":"cs.AI"}
{"created":"2024-03-13 11:16:43","title":"Tastle: Distract Large Language Models for Automatic Jailbreak Attack","abstract":"Large language models (LLMs) have achieved significant advances in recent days. Extensive efforts have been made before the public release of LLMs to align their behaviors with human values. The primary goal of alignment is to ensure their helpfulness, honesty and harmlessness. However, even meticulously aligned LLMs remain vulnerable to malicious manipulations such as jailbreaking, leading to unintended behaviors. The jailbreak is to intentionally develop a malicious prompt that escapes from the LLM security restrictions to produce uncensored detrimental contents. Previous works explore different jailbreak methods for red teaming LLMs, yet they encounter challenges regarding to effectiveness and scalability. In this work, we propose Tastle, a novel black-box jailbreak framework for automated red teaming of LLMs. We designed malicious content concealing and memory reframing with an iterative optimization algorithm to jailbreak LLMs, motivated by the research about the distractibility and over-confidence phenomenon of LLMs. Extensive experiments of jailbreaking both open-source and proprietary LLMs demonstrate the superiority of our framework in terms of effectiveness, scalability and transferability. We also evaluate the effectiveness of existing jailbreak defense methods against our attack and highlight the crucial need to develop more effective and practical defense strategies.","sentences":["Large language models (LLMs) have achieved significant advances in recent days.","Extensive efforts have been made before the public release of LLMs to align their behaviors with human values.","The primary goal of alignment is to ensure their helpfulness, honesty and harmlessness.","However, even meticulously aligned LLMs remain vulnerable to malicious manipulations such as jailbreaking, leading to unintended behaviors.","The jailbreak is to intentionally develop a malicious prompt that escapes from the LLM security restrictions to produce uncensored detrimental contents.","Previous works explore different jailbreak methods for red teaming LLMs, yet they encounter challenges regarding to effectiveness and scalability.","In this work, we propose Tastle, a novel black-box jailbreak framework for automated red teaming of LLMs.","We designed malicious content concealing and memory reframing with an iterative optimization algorithm to jailbreak LLMs, motivated by the research about the distractibility and over-confidence phenomenon of LLMs.","Extensive experiments of jailbreaking both open-source and proprietary LLMs demonstrate the superiority of our framework in terms of effectiveness, scalability and transferability.","We also evaluate the effectiveness of existing jailbreak defense methods against our attack and highlight the crucial need to develop more effective and practical defense strategies."],"url":"http://arxiv.org/abs/2403.08424v1","category":"cs.CR"}
{"created":"2024-03-13 11:11:59","title":"Low-Cost and Real-Time Industrial Human Action Recognitions Based on Large-Scale Foundation Models","abstract":"Industrial managements, including quality control, cost and safety optimization, etc., heavily rely on high quality industrial human action recognitions (IHARs) which were hard to be implemented in large-scale industrial scenes due to their high costs and poor real-time performance. In this paper, we proposed a large-scale foundation model(LSFM)-based IHAR method, wherein various LSFMs and lightweight methods were jointly used, for the first time, to fulfill low-cost dataset establishment and real-time IHARs. Comprehensive tests on in-situ large-scale industrial manufacturing lines elucidated that the proposed method realized great reduction on employment costs, superior real-time performance, and satisfactory accuracy and generalization capabilities, indicating its great potential as a backbone IHAR method, especially for large-scale industrial applications.","sentences":["Industrial managements, including quality control, cost and safety optimization, etc., heavily rely on high quality industrial human action recognitions (IHARs) which were hard to be implemented in large-scale industrial scenes due to their high costs and poor real-time performance.","In this paper, we proposed a large-scale foundation model(LSFM)-based IHAR method, wherein various LSFMs and lightweight methods were jointly used, for the first time, to fulfill low-cost dataset establishment and real-time IHARs.","Comprehensive tests on in-situ large-scale industrial manufacturing lines elucidated that the proposed method realized great reduction on employment costs, superior real-time performance, and satisfactory accuracy and generalization capabilities, indicating its great potential as a backbone IHAR method, especially for large-scale industrial applications."],"url":"http://arxiv.org/abs/2403.08420v1","category":"cs.CV"}
{"created":"2024-03-13 10:58:55","title":"Causal Graph Neural Networks for Wildfire Danger Prediction","abstract":"Wildfire forecasting is notoriously hard due to the complex interplay of different factors such as weather conditions, vegetation types and human activities. Deep learning models show promise in dealing with this complexity by learning directly from data. However, to inform critical decision making, we argue that we need models that are right for the right reasons; that is, the implicit rules learned should be grounded by the underlying processes driving wildfires. In that direction, we propose integrating causality with Graph Neural Networks (GNNs) that explicitly model the causal mechanism among complex variables via graph learning. The causal adjacency matrix considers the synergistic effect among variables and removes the spurious links from highly correlated impacts. Our methodology's effectiveness is demonstrated through superior performance forecasting wildfire patterns in the European boreal and mediterranean biome. The gain is especially prominent in a highly imbalanced dataset, showcasing an enhanced robustness of the model to adapt to regime shifts in functional relationships. Furthermore, SHAP values from our trained model further enhance our understanding of the model's inner workings.","sentences":["Wildfire forecasting is notoriously hard due to the complex interplay of different factors such as weather conditions, vegetation types and human activities.","Deep learning models show promise in dealing with this complexity by learning directly from data.","However, to inform critical decision making, we argue that we need models that are right for the right reasons; that is, the implicit rules learned should be grounded by the underlying processes driving wildfires.","In that direction, we propose integrating causality with Graph Neural Networks (GNNs) that explicitly model the causal mechanism among complex variables via graph learning.","The causal adjacency matrix considers the synergistic effect among variables and removes the spurious links from highly correlated impacts.","Our methodology's effectiveness is demonstrated through superior performance forecasting wildfire patterns in the European boreal and mediterranean biome.","The gain is especially prominent in a highly imbalanced dataset, showcasing an enhanced robustness of the model to adapt to regime shifts in functional relationships.","Furthermore, SHAP values from our trained model further enhance our understanding of the model's inner workings."],"url":"http://arxiv.org/abs/2403.08414v1","category":"cs.LG"}
{"created":"2024-03-13 10:51:38","title":"Reduced Jeffries-Matusita distance: A Novel Loss Function to Improve Generalization Performance of Deep Classification Models","abstract":"The generalization performance of deep neural networks in classification tasks is a major concern in machine learning research. Despite widespread techniques used to diminish the over-fitting issue such as data augmentation, pseudo-labeling, regularization, and ensemble learning, this performance still needs to be enhanced with other approaches. In recent years, it has been theoretically demonstrated that the loss function characteristics i.e. its Lipschitzness and maximum value affect the generalization performance of deep neural networks which can be utilized as a guidance to propose novel distance measures. In this paper, by analyzing the aforementioned characteristics, we introduce a distance called Reduced Jeffries-Matusita as a loss function for training deep classification models to reduce the over-fitting issue. In our experiments, we evaluate the new loss function in two different problems: image classification in computer vision and node classification in the context of graph learning. The results show that the new distance measure stabilizes the training process significantly, enhances the generalization ability, and improves the performance of the models in the Accuracy and F1-score metrics, even if the training set size is small.","sentences":["The generalization performance of deep neural networks in classification tasks is a major concern in machine learning research.","Despite widespread techniques used to diminish the over-fitting issue such as data augmentation, pseudo-labeling, regularization, and ensemble learning, this performance still needs to be enhanced with other approaches.","In recent years, it has been theoretically demonstrated that the loss function characteristics i.e. its Lipschitzness and maximum value affect the generalization performance of deep neural networks which can be utilized as a guidance to propose novel distance measures.","In this paper, by analyzing the aforementioned characteristics, we introduce a distance called Reduced Jeffries-Matusita as a loss function for training deep classification models to reduce the over-fitting issue.","In our experiments, we evaluate the new loss function in two different problems: image classification in computer vision and node classification in the context of graph learning.","The results show that the new distance measure stabilizes the training process significantly, enhances the generalization ability, and improves the performance of the models in the Accuracy and F1-score metrics, even if the training set size is small."],"url":"http://arxiv.org/abs/2403.08408v1","category":"cs.LG"}
{"created":"2024-03-13 10:51:18","title":"Iterative Online Image Synthesis via Diffusion Model for Imbalanced Classification","abstract":"Accurate and robust classification of diseases is important for proper diagnosis and treatment. However, medical datasets often face challenges related to limited sample sizes and inherent imbalanced distributions, due to difficulties in data collection and variations in disease prevalence across different types. In this paper, we introduce an Iterative Online Image Synthesis (IOIS) framework to address the class imbalance problem in medical image classification. Our framework incorporates two key modules, namely Online Image Synthesis (OIS) and Accuracy Adaptive Sampling (AAS), which collectively target the imbalance classification issue at both the instance level and the class level. The OIS module alleviates the data insufficiency problem by generating representative samples tailored for online training of the classifier. On the other hand, the AAS module dynamically balances the synthesized samples among various classes, targeting those with low training accuracy. To evaluate the effectiveness of our proposed method in addressing imbalanced classification, we conduct experiments on the HAM10000 and APTOS datasets. The results obtained demonstrate the superiority of our approach over state-of-the-art methods as well as the effectiveness of each component. The source code will be released upon acceptance.","sentences":["Accurate and robust classification of diseases is important for proper diagnosis and treatment.","However, medical datasets often face challenges related to limited sample sizes and inherent imbalanced distributions, due to difficulties in data collection and variations in disease prevalence across different types.","In this paper, we introduce an Iterative Online Image Synthesis (IOIS) framework to address the class imbalance problem in medical image classification.","Our framework incorporates two key modules, namely Online Image Synthesis (OIS) and Accuracy Adaptive Sampling (AAS), which collectively target the imbalance classification issue at both the instance level and the class level.","The OIS module alleviates the data insufficiency problem by generating representative samples tailored for online training of the classifier.","On the other hand, the AAS module dynamically balances the synthesized samples among various classes, targeting those with low training accuracy.","To evaluate the effectiveness of our proposed method in addressing imbalanced classification, we conduct experiments on the HAM10000 and APTOS datasets.","The results obtained demonstrate the superiority of our approach over state-of-the-art methods as well as the effectiveness of each component.","The source code will be released upon acceptance."],"url":"http://arxiv.org/abs/2403.08407v1","category":"cs.CV"}
{"created":"2024-03-13 10:46:19","title":"Topology of Discrete Quantum Feedback Control","abstract":"A general framework for analyzing topology of quantum channels of single-particle systems is developed to find a class of genuinely dynamical topological phases that can be realized by means of discrete quantum feedback control. We provide a symmetry classification of quantum channels by identifying ten symmetry classes of discrete quantum feedback control with projective measurements. We construct various types of topological feedback control by using topological Maxwell's demons that achieve robust feedback-controlled chiral or helical transport against noise and decoherence. Topological feedback control thus offers a versatile tool for creating and controlling nonequilibrium topological phases in open quantum systems that are distinct from non-Hermitian and Lindbladian systems and should provide a guiding principle for topology-based design of quantum feedback control.","sentences":["A general framework for analyzing topology of quantum channels of single-particle systems is developed to find a class of genuinely dynamical topological phases that can be realized by means of discrete quantum feedback control.","We provide a symmetry classification of quantum channels by identifying ten symmetry classes of discrete quantum feedback control with projective measurements.","We construct various types of topological feedback control by using topological Maxwell's demons that achieve robust feedback-controlled chiral or helical transport against noise and decoherence.","Topological feedback control thus offers a versatile tool for creating and controlling nonequilibrium topological phases in open quantum systems that are distinct from non-Hermitian and Lindbladian systems and should provide a guiding principle for topology-based design of quantum feedback control."],"url":"http://arxiv.org/abs/2403.08406v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-13 10:44:15","title":"Coriolis darkening in late-type stars II. Effect of self-sustained magnetic fields in stratified convective envelope","abstract":"Modeling the surface brightness distribution of stars is of prime importance to interpret observations. Nevertheless, this remains quite challenging for cool stars as it requires one to model the MHD turbulence that develops in their convective envelope. In Paper I, the effect of the Coriolis acceleration on the surface heat flux has been studied by means of hydrodynamic simulations. In this paper, we aim to investigate the additional effect of dynamo magnetic fields. We focus on an envelope thickness that is representative of either a $\\sim0.35~M_\\odot$ M dwarf, a young red giant star or a pre-main sequence star. We performed a parametric study using numerical MHD simulations of anelastic convection in thick rotating spherical shells. For each model, we computed the mean surface distribution of the heat flux, and examined the leading-order effect of the magnetic field on the obtained latitudinal luminosity profile. We identify three different regimes. Close to the onset of convection, while the first unstable modes tend to convey heat more efficiently near the equator, magnetic fields are shown to generally enhance the mean heat flux close to the polar regions (and the tangent cylinder). By progressively increasing the Rayleigh number, the development of a prograde equatorial jet was previously shown to make the equator darker when no magnetic field is taken into account. For moderate Rayleigh numbers, magnetic fields can instead inverse the mean pole-equator brightness contrast (which means going from a darker to a brighter equator when a dynamo sets in) and finally induce a similar regime to that found close to the onset of convection. For more turbulent models with larger Rayleigh numbers, magnetic fields alternatively tend to smooth out the brightness contrast. This general behavior is shown to be related to the quenching of the surface differential rotation by magnetic fields.","sentences":["Modeling the surface brightness distribution of stars is of prime importance to interpret observations.","Nevertheless, this remains quite challenging for cool stars as it requires one to model the MHD turbulence that develops in their convective envelope.","In Paper I, the effect of the Coriolis acceleration on the surface heat flux has been studied by means of hydrodynamic simulations.","In this paper, we aim to investigate the additional effect of dynamo magnetic fields.","We focus on an envelope thickness that is representative of either a $\\sim0.35~M_\\odot$ M dwarf, a young red giant star or a pre-main sequence star.","We performed a parametric study using numerical MHD simulations of anelastic convection in thick rotating spherical shells.","For each model, we computed the mean surface distribution of the heat flux, and examined the leading-order effect of the magnetic field on the obtained latitudinal luminosity profile.","We identify three different regimes.","Close to the onset of convection, while the first unstable modes tend to convey heat more efficiently near the equator, magnetic fields are shown to generally enhance the mean heat flux close to the polar regions (and the tangent cylinder).","By progressively increasing the Rayleigh number, the development of a prograde equatorial jet was previously shown to make the equator darker when no magnetic field is taken into account.","For moderate Rayleigh numbers, magnetic fields can instead inverse the mean pole-equator brightness contrast (which means going from a darker to a brighter equator when a dynamo sets in) and finally induce a similar regime to that found close to the onset of convection.","For more turbulent models with larger Rayleigh numbers, magnetic fields alternatively tend to smooth out the brightness contrast.","This general behavior is shown to be related to the quenching of the surface differential rotation by magnetic fields."],"url":"http://arxiv.org/abs/2403.08405v1","category":"astro-ph.SR"}
{"created":"2024-03-13 10:39:37","title":"Stealthy and hyperuniform isotropic photonic bandgap structure in 3D","abstract":"In photonic crystals the propagation of light is governed by their photonic band structure, an ensemble of propagating states grouped into bands, separated by photonic band gaps. Due to discrete symmetries in spatially strictly periodic dielectric structures their photonic band structure is intrinsically anisotropic. However, for many applications, such as manufacturing artificial structural color materials or developing photonic computing devices, but also for the fundamental understanding of light-matter interactions, it is of major interest to seek materials with long range non-periodic dielectric structures which allow the formation of {\\it isotropic} photonic band gaps. Here, we report the first ever 3D isotropic photonic band gap for an optimized disordered stealthy hyperuniform structure for microwaves. The transmission spectra are directly compared to a diamond pattern and an amorphous structure with similar node density. The band structure is measured experimentally for all three microwave structures, manufactured by 3D-Laser-printing for meta-materials with refractive index up to $n=2.1$. Results agree well with finite-difference-time-domain numerical investigations and a priori calculations of the band-gap for the hyperuniform structure: the diamond structure shows gaps but being anisotropic as expected, the stealthy hyperuniform pattern shows an isotropic gap of very similar magnitude, while the amorphous structure does not show a gap at all. The centimeter scaled microwave structures may serve as prototypes for micrometer scaled structures with bandgaps in the technologically very interesting region of infrared (IR).","sentences":["In photonic crystals the propagation of light is governed by their photonic band structure, an ensemble of propagating states grouped into bands, separated by photonic band gaps.","Due to discrete symmetries in spatially strictly periodic dielectric structures their photonic band structure is intrinsically anisotropic.","However, for many applications, such as manufacturing artificial structural color materials or developing photonic computing devices, but also for the fundamental understanding of light-matter interactions, it is of major interest to seek materials with long range non-periodic dielectric structures which allow the formation of {\\it isotropic} photonic band gaps.","Here, we report the first ever 3D isotropic photonic band gap for an optimized disordered stealthy hyperuniform structure for microwaves.","The transmission spectra are directly compared to a diamond pattern and an amorphous structure with similar node density.","The band structure is measured experimentally for all three microwave structures, manufactured by 3D-Laser-printing for meta-materials with refractive index up to $n=2.1$. Results agree well with finite-difference-time-domain numerical investigations and a priori calculations of the band-gap for the hyperuniform structure: the diamond structure shows gaps but being anisotropic as expected, the stealthy hyperuniform pattern shows an isotropic gap of very similar magnitude, while the amorphous structure does not show a gap at all.","The centimeter scaled microwave structures may serve as prototypes for micrometer scaled structures with bandgaps in the technologically very interesting region of infrared (IR)."],"url":"http://arxiv.org/abs/2403.08404v1","category":"physics.app-ph"}
{"created":"2024-03-13 10:28:48","title":"Regular Friedmann Universes and Matter Transformations","abstract":"We apply a very simple procedure to construct non-singular cosmological models for flat Friedmann universes filled with minimally coupled scalar fields or by tachyon Born-Infeld-type fields. Remarkably, for the minimally coupled scalar field and the tachyon field, the regularity of the cosmological evolution, or in other words, the existence of bounce, implies the necessity of the transition between scalar fields with standard kinetic terms to those with phantom ones. In both cases, the potentials in the vicinity of the point of the transition have a non-analyticity of the cusp form that is characterized by the same exponent and is equal to 2/3. If, in the tachyon models evolution, the pressure changes its sign, then another transformation of the Born-Infeld-type field occurs: the tachyon transforms into a pseudotachyon, and vice versa. We also undertake an analysis of the stability of the cosmological evolution in our models; we rely on the study of the speed of sound squared.","sentences":["We apply a very simple procedure to construct non-singular cosmological models for flat Friedmann universes filled with minimally coupled scalar fields or by tachyon Born-Infeld-type fields.","Remarkably, for the minimally coupled scalar field and the tachyon field, the regularity of the cosmological evolution, or in other words, the existence of bounce, implies the necessity of the transition between scalar fields with standard kinetic terms to those with phantom ones.","In both cases, the potentials in the vicinity of the point of the transition have a non-analyticity of the cusp form that is characterized by the same exponent and is equal to 2/3.","If, in the tachyon models evolution, the pressure changes its sign, then another transformation of the Born-Infeld-type field occurs: the tachyon transforms into a pseudotachyon, and vice versa.","We also undertake an analysis of the stability of the cosmological evolution in our models; we rely on the study of the speed of sound squared."],"url":"http://arxiv.org/abs/2403.08400v1","category":"gr-qc"}
{"created":"2024-03-13 10:27:52","title":"System for systematic literature review using multiple AI agents: Concept and an empirical evaluation","abstract":"Systematic Literature Reviews (SLRs) have become the foundation of evidence-based studies, enabling researchers to identify, classify, and combine existing studies based on specific research questions. Conducting an SLR is largely a manual process. Over the previous years, researchers have made significant progress in automating certain phases of the SLR process, aiming to reduce the effort and time needed to carry out high-quality SLRs. However, there is still a lack of AI agent-based models that automate the entire SLR process. To this end, we introduce a novel multi-AI agent model designed to fully automate the process of conducting an SLR. By utilizing the capabilities of Large Language Models (LLMs), our proposed model streamlines the review process, enhancing efficiency and accuracy. The model operates through a user-friendly interface where researchers input their topic, and in response, the model generates a search string used to retrieve relevant academic papers. Subsequently, an inclusive and exclusive filtering process is applied, focusing on titles relevant to the specific research area. The model then autonomously summarizes the abstracts of these papers, retaining only those directly related to the field of study. In the final phase, the model conducts a thorough analysis of the selected papers concerning predefined research questions. We also evaluated the proposed model by sharing it with ten competent software engineering researchers for testing and analysis. The researchers expressed strong satisfaction with the proposed model and provided feedback for further improvement. The code for this project can be found on the GitHub repository at https://github.com/GPT-Laboratory/SLR-automation.","sentences":["Systematic Literature Reviews (SLRs) have become the foundation of evidence-based studies, enabling researchers to identify, classify, and combine existing studies based on specific research questions.","Conducting an SLR is largely a manual process.","Over the previous years, researchers have made significant progress in automating certain phases of the SLR process, aiming to reduce the effort and time needed to carry out high-quality SLRs.","However, there is still a lack of AI agent-based models that automate the entire SLR process.","To this end, we introduce a novel multi-AI agent model designed to fully automate the process of conducting an SLR.","By utilizing the capabilities of Large Language Models (LLMs), our proposed model streamlines the review process, enhancing efficiency and accuracy.","The model operates through a user-friendly interface where researchers input their topic, and in response, the model generates a search string used to retrieve relevant academic papers.","Subsequently, an inclusive and exclusive filtering process is applied, focusing on titles relevant to the specific research area.","The model then autonomously summarizes the abstracts of these papers, retaining only those directly related to the field of study.","In the final phase, the model conducts a thorough analysis of the selected papers concerning predefined research questions.","We also evaluated the proposed model by sharing it with ten competent software engineering researchers for testing and analysis.","The researchers expressed strong satisfaction with the proposed model and provided feedback for further improvement.","The code for this project can be found on the GitHub repository at https://github.com/GPT-Laboratory/SLR-automation."],"url":"http://arxiv.org/abs/2403.08399v1","category":"cs.SE"}
{"created":"2024-03-13 10:21:29","title":"A Picture Is Worth a Thousand Words: Exploring Diagram and Video-Based OOP Exercises to Counter LLM Over-Reliance","abstract":"Much research has highlighted the impressive capabilities of large language models (LLMs), like GPT and Bard, for solving introductory programming exercises. Recent work has shown that LLMs can effectively solve a range of more complex object-oriented programming (OOP) exercises with text-based specifications. This raises concerns about academic integrity, as students might use these models to complete assignments unethically, neglecting the development of important skills such as program design, problem-solving, and computational thinking. To address this, we propose an innovative approach to formulating OOP tasks using diagrams and videos, as a way to foster problem-solving and deter students from a copy-and-prompt approach in OOP courses. We introduce a novel notation system for specifying OOP assignments, encompassing structural and behavioral requirements, and assess its use in a classroom setting over a semester. Student perceptions of this approach are explored through a survey (n=56). Generally, students responded positively to diagrams and videos, with video-based projects being better received than diagram-based exercises. This notation appears to have several benefits, with students investing more effort in understanding the diagrams and feeling more motivated to engage with the video-based projects. Furthermore, students reported being less inclined to rely on LLM-based code generation tools for these diagram and video-based exercises. Experiments with GPT-4 and Bard's vision abilities revealed that they currently fall short in interpreting these diagrams to generate accurate code solutions.","sentences":["Much research has highlighted the impressive capabilities of large language models (LLMs), like GPT and Bard, for solving introductory programming exercises.","Recent work has shown that LLMs can effectively solve a range of more complex object-oriented programming (OOP) exercises with text-based specifications.","This raises concerns about academic integrity, as students might use these models to complete assignments unethically, neglecting the development of important skills such as program design, problem-solving, and computational thinking.","To address this, we propose an innovative approach to formulating OOP tasks using diagrams and videos, as a way to foster problem-solving and deter students from a copy-and-prompt approach in OOP courses.","We introduce a novel notation system for specifying OOP assignments, encompassing structural and behavioral requirements, and assess its use in a classroom setting over a semester.","Student perceptions of this approach are explored through a survey (n=56).","Generally, students responded positively to diagrams and videos, with video-based projects being better received than diagram-based exercises.","This notation appears to have several benefits, with students investing more effort in understanding the diagrams and feeling more motivated to engage with the video-based projects.","Furthermore, students reported being less inclined to rely on LLM-based code generation tools for these diagram and video-based exercises.","Experiments with GPT-4 and Bard's vision abilities revealed that they currently fall short in interpreting these diagrams to generate accurate code solutions."],"url":"http://arxiv.org/abs/2403.08396v1","category":"cs.SE"}
{"created":"2024-03-13 10:21:22","title":"Complementarity of which-path information in induced and stimulated coherences via four-wave mixing process from warm Rb atomic ensemble","abstract":"We report a systematic approach for establishing a complementary relationship between the interference visibility, concurrence, and predictability in the crossing of induced and stimulated coherences of two-mode squeezed coherent states. This is achieved using a double-path interferometer involving two independent four-wave mixing (FWM) atomic samples generated via spontaneous and stimulated FWM processes from a warm atomic ensemble of 87Rb. We demonstrate that the transition from quantum to classical behavior can be characterized by the induced coherence effect, distinguishing between the two-mode squeezed vacuum and coherent states. Moreover, our experimental scheme, employing two FWM atomic ensembles with long-coherent photons, provides valuable insights into the complementarity of which-path information in induced and stimulated coherences.","sentences":["We report a systematic approach for establishing a complementary relationship between the interference visibility, concurrence, and predictability in the crossing of induced and stimulated coherences of two-mode squeezed coherent states.","This is achieved using a double-path interferometer involving two independent four-wave mixing (FWM) atomic samples generated via spontaneous and stimulated FWM processes from a warm atomic ensemble of 87Rb.","We demonstrate that the transition from quantum to classical behavior can be characterized by the induced coherence effect, distinguishing between the two-mode squeezed vacuum and coherent states.","Moreover, our experimental scheme, employing two FWM atomic ensembles with long-coherent photons, provides valuable insights into the complementarity of which-path information in induced and stimulated coherences."],"url":"http://arxiv.org/abs/2403.08395v1","category":"quant-ph"}
{"created":"2024-03-13 10:19:10","title":"Worst-Case to Expander-Case Reductions: Derandomized and Generalized","abstract":"A recent paper by Abboud and Wallheimer [ITCS 2023] presents self-reductions for various fundamental graph problems, that transform worst-case instances to expanders, thus proving that the complexity remains unchanged if the input is assumed to be an expander. An interesting corollary of their self-reductions is that, if some problem admit such reduction, then the popular algorithmic paradigm based on expander-decompositions is useless against it. In this paper, we improve their core gadget, which augments a graph to make it an expander while retaining its important structure. Our new core construction has the benefit of being simple to analyze and generalize, while obtaining the following results:   1. A derandomization of the self-reductions, showing that the equivalence between worst-case and expander-case holds even for deterministic algorithms, and ruling out the use of expander-decompositions as a derandomization tool.   2. An extension of the results to other models of computation, such as the Fully Dynamic model and the Congested Clique model. In the former, we either improve or provide an alternative approach to some recent hardness results for dynamic expander graphs, by Henzinger, Paz, and Sricharan [ESA 2022].   In addition, we continue this line of research by designing new self-reductions for more problems, such as Max-Cut and dynamic Densest Subgraph, and demonstrating that the core gadget can be utilized to lift lower bounds based on the OMv Conjecture to expanders.","sentences":["A recent paper by Abboud and Wallheimer [ITCS 2023] presents self-reductions for various fundamental graph problems, that transform worst-case instances to expanders, thus proving that the complexity remains unchanged if the input is assumed to be an expander.","An interesting corollary of their self-reductions is that, if some problem admit such reduction, then the popular algorithmic paradigm based on expander-decompositions is useless against it.","In this paper, we improve their core gadget, which augments a graph to make it an expander while retaining its important structure.","Our new core construction has the benefit of being simple to analyze and generalize, while obtaining the following results:   1.","A derandomization of the self-reductions, showing that the equivalence between worst-case and expander-case holds even for deterministic algorithms, and ruling out the use of expander-decompositions as a derandomization tool.   ","2.","An extension of the results to other models of computation, such as the Fully Dynamic model and the Congested Clique model.","In the former, we either improve or provide an alternative approach to some recent hardness results for dynamic expander graphs, by Henzinger, Paz, and Sricharan [ESA 2022].   ","In addition, we continue this line of research by designing new self-reductions for more problems, such as Max-Cut and dynamic Densest Subgraph, and demonstrating that the core gadget can be utilized to lift lower bounds based on the OMv Conjecture to expanders."],"url":"http://arxiv.org/abs/2403.08394v1","category":"cs.DS"}
{"created":"2024-03-13 10:16:48","title":"A classification of $\\mathbb{F}_{p^k}$-braces using bilinear forms","abstract":"Let $\\mathbb{F}_{p^k}$ be a finite field of odd characteristic $p$. In this paper we give a classification, up to isomorphism, of the associative commutative $\\mathbb{F}_{p^k}$-algebras such that the resulting ring is radical, starting from the connection with their bi-brace structure. Such classification is the generalization in odd characteristic of the result proved by Civino at al. in characteristic $2$.","sentences":["Let $\\mathbb{F}_{p^k}$ be a finite field of odd characteristic $p$. In this paper we give a classification, up to isomorphism, of the associative commutative $\\mathbb{F}_{p^k}$-algebras such that the resulting ring is radical, starting from the connection with their bi-brace structure.","Such classification is the generalization in odd characteristic of the result proved by Civino at al. in characteristic $2$."],"url":"http://arxiv.org/abs/2403.08393v1","category":"math.GR"}
{"created":"2024-03-13 09:56:03","title":"Feasibility of detecting shadows in disks induced by infall","abstract":"Observations performed with high-resolution imaging techniques revealed the existence of shadows in circumstellar disks that can be explained by the misalignment of an inner with respect to an outer disk. The cause of misalignment, however, is still debated. In this study, we investigate the feasibility of observing shadows induced by one prominent scenario that may lead to misalignment, which involves the late infall of material onto a protostellar system. In particular, we use previously performed hydrodynamical simulations of such events, and generate flux maps in the visible, near-infrared, submillimeter, and millimeter wavelength range using Monte Carlo radiative transfer. Based on that, we derive synthetic observations of these systems performed with the instruments SPHERE/VLT and ALMA, which we use as a basis for our subsequent analysis. We find that near-infrared observations with SPHERE are particularly well suited for detecting shadows via direct imaging alongside other features such as gaps, arcs, and streamers. On the contrary, performing a shadow detection based on reconstructed ALMA observations is very challenging due to the high sensitivity that is required for this task. Thus, in cases that allow for a detection, sophisticated analyses may be needed, for instance by the utilization of carefully constructed azimuthal profiles, aiding the search for potentially shallow shadows. Lastly, we conclude that late infall-induced disk misalignment offers a plausible explanation for the emergence of shadows that are observed in various systems.","sentences":["Observations performed with high-resolution imaging techniques revealed the existence of shadows in circumstellar disks that can be explained by the misalignment of an inner with respect to an outer disk.","The cause of misalignment, however, is still debated.","In this study, we investigate the feasibility of observing shadows induced by one prominent scenario that may lead to misalignment, which involves the late infall of material onto a protostellar system.","In particular, we use previously performed hydrodynamical simulations of such events, and generate flux maps in the visible, near-infrared, submillimeter, and millimeter wavelength range using Monte Carlo radiative transfer.","Based on that, we derive synthetic observations of these systems performed with the instruments SPHERE/VLT and ALMA, which we use as a basis for our subsequent analysis.","We find that near-infrared observations with SPHERE are particularly well suited for detecting shadows via direct imaging alongside other features such as gaps, arcs, and streamers.","On the contrary, performing a shadow detection based on reconstructed ALMA observations is very challenging due to the high sensitivity that is required for this task.","Thus, in cases that allow for a detection, sophisticated analyses may be needed, for instance by the utilization of carefully constructed azimuthal profiles, aiding the search for potentially shallow shadows.","Lastly, we conclude that late infall-induced disk misalignment offers a plausible explanation for the emergence of shadows that are observed in various systems."],"url":"http://arxiv.org/abs/2403.08388v1","category":"astro-ph.SR"}
{"created":"2024-03-13 09:53:21","title":"Radial perturbations of Ellis-Bronnikov wormholes in slow rotation up to second order","abstract":"We consider slowly rotating Ellis-Bronnikov wormholes and investigate their radial perturbations ($\\mathrm{l}=0$), expanding up to second order in rotation. We present the detailed derivations in the general case, including symmetric and non-symmetric wormholes. The calculations show that the unstable mode present in the static case becomes less unstable with increasing rotation, until it reaches zero and then disappears. This indicates that wormhole solutions may become linearly mode stable at sufficiently fast rotation.","sentences":["We consider slowly rotating Ellis-Bronnikov wormholes and investigate their radial perturbations ($\\mathrm{l}=0$), expanding up to second order in rotation.","We present the detailed derivations in the general case, including symmetric and non-symmetric wormholes.","The calculations show that the unstable mode present in the static case becomes less unstable with increasing rotation, until it reaches zero and then disappears.","This indicates that wormhole solutions may become linearly mode stable at sufficiently fast rotation."],"url":"http://arxiv.org/abs/2403.08387v1","category":"gr-qc"}
{"created":"2024-03-13 09:49:26","title":"Optimizing Risk-averse Human-AI Hybrid Teams","abstract":"We anticipate increased instances of humans and AI systems working together in what we refer to as a hybrid team. The increase in collaboration is expected as AI systems gain proficiency and their adoption becomes more widespread. However, their behavior is not error-free, making hybrid teams a very suitable solution. As such, we consider methods for improving performance for these teams of humans and AI systems. For hybrid teams, we will refer to both the humans and AI systems as agents. To improve team performance over that seen for agents operating individually, we propose a manager which learns, through a standard Reinforcement Learning scheme, how to best delegate, over time, the responsibility of taking a decision to any of the agents. We further guide the manager's learning so they also minimize how many changes in delegation are made resulting from undesirable team behavior. We demonstrate the optimality of our manager's performance in several grid environments which include failure states which terminate an episode and should be avoided. We perform our experiments with teams of agents with varying degrees of acceptable risk, in the form of proximity to a failure state, and measure the manager's ability to make effective delegation decisions with respect to its own risk-based constraints, then compare these to the optimal decisions. Our results show our manager can successfully learn desirable delegations which result in team paths near/exactly optimal with respect to path length and number of delegations.","sentences":["We anticipate increased instances of humans and AI systems working together in what we refer to as a hybrid team.","The increase in collaboration is expected as AI systems gain proficiency and their adoption becomes more widespread.","However, their behavior is not error-free, making hybrid teams a very suitable solution.","As such, we consider methods for improving performance for these teams of humans and AI systems.","For hybrid teams, we will refer to both the humans and AI systems as agents.","To improve team performance over that seen for agents operating individually, we propose a manager which learns, through a standard Reinforcement Learning scheme, how to best delegate, over time, the responsibility of taking a decision to any of the agents.","We further guide the manager's learning so they also minimize how many changes in delegation are made resulting from undesirable team behavior.","We demonstrate the optimality of our manager's performance in several grid environments which include failure states which terminate an episode and should be avoided.","We perform our experiments with teams of agents with varying degrees of acceptable risk, in the form of proximity to a failure state, and measure the manager's ability to make effective delegation decisions with respect to its own risk-based constraints, then compare these to the optimal decisions.","Our results show our manager can successfully learn desirable delegations which result in team paths near/exactly optimal with respect to path length and number of delegations."],"url":"http://arxiv.org/abs/2403.08386v1","category":"cs.AI"}
{"created":"2024-03-13 09:48:11","title":"AADNet: Attention aware Demoir\u00e9ing Network","abstract":"Moire pattern frequently appears in photographs captured with mobile devices and digital cameras, potentially degrading image quality. Despite recent advancements in computer vision, image demoire'ing remains a challenging task due to the dynamic textures and variations in colour, shape, and frequency of moire patterns. Most existing methods struggle to generalize to unseen datasets, limiting their effectiveness in removing moire patterns from real-world scenarios. In this paper, we propose a novel lightweight architecture, AADNet (Attention Aware Demoireing Network), for high-resolution image demoire'ing that effectively works across different frequency bands and generalizes well to unseen datasets. Extensive experiments conducted on the UHDM dataset validate the effectiveness of our approach, resulting in high-fidelity images.","sentences":["Moire pattern frequently appears in photographs captured with mobile devices and digital cameras, potentially degrading image quality.","Despite recent advancements in computer vision, image demoire'ing remains a challenging task due to the dynamic textures and variations in colour, shape, and frequency of moire patterns.","Most existing methods struggle to generalize to unseen datasets, limiting their effectiveness in removing moire patterns from real-world scenarios.","In this paper, we propose a novel lightweight architecture, AADNet (Attention Aware Demoireing Network), for high-resolution image demoire'ing that effectively works across different frequency bands and generalizes well to unseen datasets.","Extensive experiments conducted on the UHDM dataset validate the effectiveness of our approach, resulting in high-fidelity images."],"url":"http://arxiv.org/abs/2403.08384v1","category":"cs.CV"}
{"created":"2024-03-13 09:47:04","title":"Tackling the Singularities at the Endpoints of Time Intervals in Diffusion Models","abstract":"Most diffusion models assume that the reverse process adheres to a Gaussian distribution. However, this approximation has not been rigorously validated, especially at singularities, where t=0 and t=1. Improperly dealing with such singularities leads to an average brightness issue in applications, and limits the generation of images with extreme brightness or darkness. We primarily focus on tackling singularities from both theoretical and practical perspectives. Initially, we establish the error bounds for the reverse process approximation, and showcase its Gaussian characteristics at singularity time steps. Based on this theoretical insight, we confirm the singularity at t=1 is conditionally removable while it at t=0 is an inherent property. Upon these significant conclusions, we propose a novel plug-and-play method SingDiffusion to address the initial singular time step sampling, which not only effectively resolves the average brightness issue for a wide range of diffusion models without extra training efforts, but also enhances their generation capability in achieving notable lower FID scores. Code and models are released at https://github.com/PangzeCheung/SingDiffusion.","sentences":["Most diffusion models assume that the reverse process adheres to a Gaussian distribution.","However, this approximation has not been rigorously validated, especially at singularities, where t=0 and t=1.","Improperly dealing with such singularities leads to an average brightness issue in applications, and limits the generation of images with extreme brightness or darkness.","We primarily focus on tackling singularities from both theoretical and practical perspectives.","Initially, we establish the error bounds for the reverse process approximation, and showcase its Gaussian characteristics at singularity time steps.","Based on this theoretical insight, we confirm the singularity at t=1 is conditionally removable while it at t=0 is an inherent property.","Upon these significant conclusions, we propose a novel plug-and-play method SingDiffusion to address the initial singular time step sampling, which not only effectively resolves the average brightness issue for a wide range of diffusion models without extra training efforts, but also enhances their generation capability in achieving notable lower FID scores.","Code and models are released at https://github.com/PangzeCheung/SingDiffusion."],"url":"http://arxiv.org/abs/2403.08381v1","category":"cs.CV"}
{"created":"2024-03-13 09:45:30","title":"Mitigate Target-level Insensitivity of Infrared Small Target Detection via Posterior Distribution Modeling","abstract":"Infrared Small Target Detection (IRSTD) aims to segment small targets from infrared clutter background. Existing methods mainly focus on discriminative approaches, i.e., a pixel-level front-background binary segmentation. Since infrared small targets are small and low signal-to-clutter ratio, empirical risk has few disturbances when a certain false alarm and missed detection exist, which seriously affect the further improvement of such methods. Motivated by the dense prediction generative methods, in this paper, we propose a diffusion model framework for Infrared Small Target Detection which compensates pixel-level discriminant with mask posterior distribution modeling. Furthermore, we design a Low-frequency Isolation in the wavelet domain to suppress the interference of intrinsic infrared noise on the diffusion noise estimation. This transition from the discriminative paradigm to generative one enables us to bypass the target-level insensitivity. Experiments show that the proposed method achieves competitive performance gains over state-of-the-art methods on NUAA-SIRST, IRSTD-1k, and NUDT-SIRST datasets. Code are available at https://github.com/Li-Haoqing/IRSTD-Diff.","sentences":["Infrared Small Target Detection (IRSTD) aims to segment small targets from infrared clutter background.","Existing methods mainly focus on discriminative approaches, i.e., a pixel-level front-background binary segmentation.","Since infrared small targets are small and low signal-to-clutter ratio, empirical risk has few disturbances when a certain false alarm and missed detection exist, which seriously affect the further improvement of such methods.","Motivated by the dense prediction generative methods, in this paper, we propose a diffusion model framework for Infrared Small Target Detection which compensates pixel-level discriminant with mask posterior distribution modeling.","Furthermore, we design a Low-frequency Isolation in the wavelet domain to suppress the interference of intrinsic infrared noise on the diffusion noise estimation.","This transition from the discriminative paradigm to generative one enables us to bypass the target-level insensitivity.","Experiments show that the proposed method achieves competitive performance gains over state-of-the-art methods on NUAA-SIRST, IRSTD-1k, and NUDT-SIRST datasets.","Code are available at https://github.com/Li-Haoqing/IRSTD-Diff."],"url":"http://arxiv.org/abs/2403.08380v1","category":"cs.CV"}
{"created":"2024-03-13 09:43:21","title":"Observational tests in scale invariance II: gravitational lensing","abstract":"We study the path of light rays passing near a massive object, in the context of the scale invariant equation of the geodesics first obtained by Dirac (1973). Using the exterior Schwarzschild solution for the metric, we derive the complete equations of the geodesics in the scale invariant context. We find that scale invariance introduces two additional terms to the Einstein term producing the deflection angle and that can potentially act over cosmological distances. Numerical integration of the scale-invariant geodesics, for the specific case of the z_L=1.94 lens galaxy in the extreme system JWST-ER1 (van Dokkum et al. 2023) shows that the two additional terms introduce only negligible effects, typically 1E-06 of the Einstein term. We conclude that the lensing deflection angle derived in Einstein's General Relativity is essentially independent of the scale invariant effects and that the photon's geodesics remain unchanged. We also explore the possible origin of the differences in the mass estimates from lensing and photometry in JWST-ER1 and in the SLACS galaxies, differences which appear larger at higher redshifts.","sentences":["We study the path of light rays passing near a massive object, in the context of the scale invariant equation of the geodesics first obtained by Dirac (1973).","Using the exterior Schwarzschild solution for the metric, we derive the complete equations of the geodesics in the scale invariant context.","We find that scale invariance introduces two additional terms to the Einstein term producing the deflection angle and that can potentially act over cosmological distances.","Numerical integration of the scale-invariant geodesics, for the specific case of the z_L=1.94 lens galaxy in the extreme system JWST-ER1 (van Dokkum et al. 2023)","shows that the two additional terms introduce only negligible effects, typically 1E-06 of the Einstein term.","We conclude that the lensing deflection angle derived in Einstein's General Relativity is essentially independent of the scale invariant effects and that the photon's geodesics remain unchanged.","We also explore the possible origin of the differences in the mass estimates from lensing and photometry in JWST-ER1 and in the SLACS galaxies, differences which appear larger at higher redshifts."],"url":"http://arxiv.org/abs/2403.08379v1","category":"astro-ph.GA"}
{"created":"2024-03-13 09:43:14","title":"A Generalized Framework with Adaptive Weighted Soft-Margin for Imbalanced SVM Classification","abstract":"Category imbalance is one of the most popular and important issues in the domain of classification. In this paper, we present a new generalized framework with Adaptive Weight function for soft-margin Weighted SVM (AW-WSVM), which aims to enhance the issue of imbalance and outlier sensitivity in standard support vector machine (SVM) for classifying two-class data. The weight coefficient is introduced into the unconstrained soft-margin support vector machines, and the sample weights are updated before each training. The Adaptive Weight function (AW function) is constructed from the distance between the samples and the decision hyperplane, assigning different weights to each sample. A weight update method is proposed, taking into account the proximity of the support vectors to the decision hyperplane. Before training, the weights of the corresponding samples are initialized according to different categories. Subsequently, the samples close to the decision hyperplane are identified and assigned more weights. At the same time, lower weights are assigned to samples that are far from the decision hyperplane. Furthermore, we also put forward an effective way to eliminate noise. To evaluate the strength of the proposed generalized framework, we conducted experiments on standard datasets and emotion classification datasets with different imbalanced ratios (IR). The experimental results prove that the proposed generalized framework outperforms in terms of accuracy, recall metrics and G-mean, validating the effectiveness of the weighted strategy provided in this paper in enhancing support vector machines.","sentences":["Category imbalance is one of the most popular and important issues in the domain of classification.","In this paper, we present a new generalized framework with Adaptive Weight function for soft-margin Weighted SVM (AW-WSVM), which aims to enhance the issue of imbalance and outlier sensitivity in standard support vector machine (SVM) for classifying two-class data.","The weight coefficient is introduced into the unconstrained soft-margin support vector machines, and the sample weights are updated before each training.","The Adaptive Weight function (AW function) is constructed from the distance between the samples and the decision hyperplane, assigning different weights to each sample.","A weight update method is proposed, taking into account the proximity of the support vectors to the decision hyperplane.","Before training, the weights of the corresponding samples are initialized according to different categories.","Subsequently, the samples close to the decision hyperplane are identified and assigned more weights.","At the same time, lower weights are assigned to samples that are far from the decision hyperplane.","Furthermore, we also put forward an effective way to eliminate noise.","To evaluate the strength of the proposed generalized framework, we conducted experiments on standard datasets and emotion classification datasets with different imbalanced ratios (IR).","The experimental results prove that the proposed generalized framework outperforms in terms of accuracy, recall metrics and G-mean, validating the effectiveness of the weighted strategy provided in this paper in enhancing support vector machines."],"url":"http://arxiv.org/abs/2403.08378v1","category":"cs.CV"}
{"created":"2024-03-13 09:38:39","title":"Translating between SQL Dialects for Cloud Migration","abstract":"Migrations of systems from on-site premises to the cloud has been a fundamental endeavor by many industrial institutions. A crucial component of such cloud migrations is the transition of databases to be hosted online. In this work, we consider the difficulties of this migration for SQL databases. While SQL is one of the prominent methods for storing database procedures, there are a plethora of different SQL dialects (e.g., MySQL, Postgres, etc.) which can complicate migrations when the on-premise SQL dialect differs to the dialect hosted on the cloud. Tools exist by common cloud provides such as AWS and Azure to aid in translating between dialects in order to mitigate the majority of the difficulties. However, these tools do not successfully translate $100\\%$ of the code. Consequently, software engineers must manually convert the remainder of the untranslated database. For large organizations, this task quickly becomes intractable and so more innovative solutions are required. We consider this challenge a novel yet vital industrial research problem for any large corporation that is considering cloud migrations. Furthermore, we introduce potential avenues of research to tackle this challenge that have yielded promising preliminary results.","sentences":["Migrations of systems from on-site premises to the cloud has been a fundamental endeavor by many industrial institutions.","A crucial component of such cloud migrations is the transition of databases to be hosted online.","In this work, we consider the difficulties of this migration for SQL databases.","While SQL is one of the prominent methods for storing database procedures, there are a plethora of different SQL dialects (e.g., MySQL, Postgres, etc.) which can complicate migrations when the on-premise SQL dialect differs to the dialect hosted on the cloud.","Tools exist by common cloud provides such as AWS and Azure to aid in translating between dialects in order to mitigate the majority of the difficulties.","However, these tools do not successfully translate $100\\%$ of the code.","Consequently, software engineers must manually convert the remainder of the untranslated database.","For large organizations, this task quickly becomes intractable and so more innovative solutions are required.","We consider this challenge a novel yet vital industrial research problem for any large corporation that is considering cloud migrations.","Furthermore, we introduce potential avenues of research to tackle this challenge that have yielded promising preliminary results."],"url":"http://arxiv.org/abs/2403.08375v1","category":"cs.DB"}
{"created":"2024-03-13 09:31:50","title":"SMART: Submodular Data Mixture Strategy for Instruction Tuning","abstract":"Instruction Tuning involves finetuning a language model on a collection of instruction-formatted datasets in order to enhance the generalizability of the model to unseen tasks. Studies have shown the importance of balancing different task proportions during finetuning, but finding the right balance remains challenging. Unfortunately, there's currently no systematic method beyond manual tuning or relying on practitioners' intuition. In this paper, we introduce SMART (Submodular data Mixture strAtegy for instRuction Tuning) - a novel data mixture strategy which makes use of a submodular function to assign importance scores to tasks which are then used to determine the mixture weights. Given a fine-tuning budget, SMART redistributes the budget among tasks and selects non-redundant samples from each task. Experimental results demonstrate that SMART significantly outperforms traditional methods such as examples proportional mixing and equal mixing. Furthermore, SMART facilitates the creation of data mixtures based on a few representative subsets of tasks alone and through task pruning analysis, we reveal that in a limited budget setting, allocating budget among a subset of representative tasks yields superior performance compared to distributing the budget among all tasks. The code for reproducing our results is open-sourced at https://github.com/kowndinya-renduchintala/SMART.","sentences":["Instruction Tuning involves finetuning a language model on a collection of instruction-formatted datasets in order to enhance the generalizability of the model to unseen tasks.","Studies have shown the importance of balancing different task proportions during finetuning, but finding the right balance remains challenging.","Unfortunately, there's currently no systematic method beyond manual tuning or relying on practitioners' intuition.","In this paper, we introduce SMART (Submodular data Mixture strAtegy for instRuction Tuning) - a novel data mixture strategy which makes use of a submodular function to assign importance scores to tasks which are then used to determine the mixture weights.","Given a fine-tuning budget, SMART redistributes the budget among tasks and selects non-redundant samples from each task.","Experimental results demonstrate that SMART significantly outperforms traditional methods such as examples proportional mixing and equal mixing.","Furthermore, SMART facilitates the creation of data mixtures based on a few representative subsets of tasks alone and through task pruning analysis, we reveal that in a limited budget setting, allocating budget among a subset of representative tasks yields superior performance compared to distributing the budget among all tasks.","The code for reproducing our results is open-sourced at https://github.com/kowndinya-renduchintala/SMART."],"url":"http://arxiv.org/abs/2403.08370v1","category":"cs.CL"}
{"created":"2024-03-13 09:28:34","title":"Lubin-Tate generalizations of the p-adic Fourier transform","abstract":"Fresnel and de Mathan proved that the p-adic Fourier transform is surjective. We reinterpret their result in terms of analytic boundaries, and extend it beyond the cyclotomic case. We also give some applications of their result to Schneider and Teitelbaum's p-adic Fourier theory, in particular to generalized Mahler expansions and to the geometry of the character variety.","sentences":["Fresnel and de Mathan proved that the p-adic Fourier transform is surjective.","We reinterpret their result in terms of analytic boundaries, and extend it beyond the cyclotomic case.","We also give some applications of their result to Schneider and Teitelbaum's p-adic Fourier theory, in particular to generalized Mahler expansions and to the geometry of the character variety."],"url":"http://arxiv.org/abs/2403.08367v1","category":"math.NT"}
{"created":"2024-03-13 09:28:23","title":"Amplified linear and nonlinear chiral sensing assisted by anapole modes in hybrid metasurfaces","abstract":"The interaction between chiral molecules and circularly polarized light is largely influenced by the local optical chirality density. This interaction prompts a substantial demand of the design of nanophotonic platforms capable of enhancing such effects across large and accessible volumes. Such a magnification requires nanostructures to provide high electric and magnetic field enhancements while keeping the phase relation of circular light. Dielectric nanostructures, particularly those able to support resonances, are uniquely suited for this task due to their capacity for high electric and magnetic field enhancements. On the other hand, efficient third harmonic generation calls for strong electric field resonances within dielectric materials, a feature often boosted by incorporating plasmonic materials into hybrid systems. In this numerical study, we propose a coupled silicon disk-gold ring system that can exploit the anapole-induced field confinement to provide a broadband magnified circular dichroism under realistic conditions, reaching values up to a 5-fold enhancement. We also demonstrate that this structure can be employed as an efficient third harmonic generator which, when integrated with chiral media, enables a 10-fold enhancement in circular dichroism. Furthermore, we numerically show that pulsed illumination at intensities up to 10 GW/cm2 does not induce temperature increments that could potentially damage the samples. These findings suggest that this system can be a promising and versatile approach towards ultrasensitive background-free chiral sensing.","sentences":["The interaction between chiral molecules and circularly polarized light is largely influenced by the local optical chirality density.","This interaction prompts a substantial demand of the design of nanophotonic platforms capable of enhancing such effects across large and accessible volumes.","Such a magnification requires nanostructures to provide high electric and magnetic field enhancements while keeping the phase relation of circular light.","Dielectric nanostructures, particularly those able to support resonances, are uniquely suited for this task due to their capacity for high electric and magnetic field enhancements.","On the other hand, efficient third harmonic generation calls for strong electric field resonances within dielectric materials, a feature often boosted by incorporating plasmonic materials into hybrid systems.","In this numerical study, we propose a coupled silicon disk-gold ring system that can exploit the anapole-induced field confinement to provide a broadband magnified circular dichroism under realistic conditions, reaching values up to a 5-fold enhancement.","We also demonstrate that this structure can be employed as an efficient third harmonic generator which, when integrated with chiral media, enables a 10-fold enhancement in circular dichroism.","Furthermore, we numerically show that pulsed illumination at intensities up to 10 GW/cm2 does not induce temperature increments that could potentially damage the samples.","These findings suggest that this system can be a promising and versatile approach towards ultrasensitive background-free chiral sensing."],"url":"http://arxiv.org/abs/2403.08366v1","category":"physics.optics"}
{"created":"2024-03-13 09:25:44","title":"APACE: Agile and Perception-Aware Trajectory Generation for Quadrotor Flights","abstract":"Various perception-aware planning approaches have attempted to enhance the state estimation accuracy during maneuvers, while the feature matchability among frames, a crucial factor influencing estimation accuracy, has often been overlooked. In this paper, we present APACE, an Agile and Perception-Aware trajeCtory gEneration framework for quadrotors aggressive flight, that takes into account feature matchability during trajectory planning. We seek to generate a perception-aware trajectory that reduces the error of visual-based estimator while satisfying the constraints on smoothness, safety, agility and the quadrotor dynamics. The perception objective is achieved by maximizing the number of covisible features while ensuring small enough parallax angles. Additionally, we propose a differentiable and accurate visibility model that allows decomposition of the trajectory planning problem for efficient optimization resolution. Through validations conducted in both a photorealistic simulator and real-world experiments, we demonstrate that the trajectories generated by our method significantly improve state estimation accuracy, with root mean square error (RMSE) reduced by up to an order of magnitude. The source code will be released to benefit the community.","sentences":["Various perception-aware planning approaches have attempted to enhance the state estimation accuracy during maneuvers, while the feature matchability among frames, a crucial factor influencing estimation accuracy, has often been overlooked.","In this paper, we present APACE, an Agile and Perception-Aware trajeCtory gEneration framework for quadrotors aggressive flight, that takes into account feature matchability during trajectory planning.","We seek to generate a perception-aware trajectory that reduces the error of visual-based estimator while satisfying the constraints on smoothness, safety, agility and the quadrotor dynamics.","The perception objective is achieved by maximizing the number of covisible features while ensuring small enough parallax angles.","Additionally, we propose a differentiable and accurate visibility model that allows decomposition of the trajectory planning problem for efficient optimization resolution.","Through validations conducted in both a photorealistic simulator and real-world experiments, we demonstrate that the trajectories generated by our method significantly improve state estimation accuracy, with root mean square error (RMSE) reduced by up to an order of magnitude.","The source code will be released to benefit the community."],"url":"http://arxiv.org/abs/2403.08365v1","category":"cs.RO"}
{"created":"2024-03-13 09:24:59","title":"Decoupled Federated Learning on Long-Tailed and Non-IID data with Feature Statistics","abstract":"Federated learning is designed to enhance data security and privacy, but faces challenges when dealing with heterogeneous data in long-tailed and non-IID distributions. This paper explores an overlooked scenario where tail classes are sparsely distributed over a few clients, causing the models trained with these classes to have a lower probability of being selected during client aggregation, leading to slower convergence rates and poorer model performance. To address this issue, we propose a two-stage Decoupled Federated learning framework using Feature Statistics (DFL-FS). In the first stage, the server estimates the client's class coverage distributions through masked local feature statistics clustering to select models for aggregation to accelerate convergence and enhance feature learning without privacy leakage. In the second stage, DFL-FS employs federated feature regeneration based on global feature statistics and utilizes resampling and weighted covariance to calibrate the global classifier to enhance the model's adaptability to long-tailed data distributions. We conducted experiments on CIFAR10-LT and CIFAR100-LT datasets with various long-tailed rates. The results demonstrate that our method outperforms state-of-the-art methods in both accuracy and convergence rate.","sentences":["Federated learning is designed to enhance data security and privacy, but faces challenges when dealing with heterogeneous data in long-tailed and non-IID distributions.","This paper explores an overlooked scenario where tail classes are sparsely distributed over a few clients, causing the models trained with these classes to have a lower probability of being selected during client aggregation, leading to slower convergence rates and poorer model performance.","To address this issue, we propose a two-stage Decoupled Federated learning framework using Feature Statistics (DFL-FS).","In the first stage, the server estimates the client's class coverage distributions through masked local feature statistics clustering to select models for aggregation to accelerate convergence and enhance feature learning without privacy leakage.","In the second stage, DFL-FS employs federated feature regeneration based on global feature statistics and utilizes resampling and weighted covariance to calibrate the global classifier to enhance the model's adaptability to long-tailed data distributions.","We conducted experiments on CIFAR10-LT and CIFAR100-LT datasets with various long-tailed rates.","The results demonstrate that our method outperforms state-of-the-art methods in both accuracy and convergence rate."],"url":"http://arxiv.org/abs/2403.08364v1","category":"cs.LG"}
{"created":"2024-03-13 09:19:05","title":"Assessment of background noise properties in time and time-frequency domains in the context of vibration-based local damage detection in real environment","abstract":"Any measurement in condition monitoring applications is associated with disturbing noise. Till now, most of the diagnostic procedures have assumed the Gaussian distribution for the noise. This paper shares a novel perspective to the problem of local damage detection. The acquired vector of observations is considered as an additive mixture of signal of interest (SOI) and noise with strongly non-Gaussian, heavy-tailed properties, that masks the SOI. The distribution properties of the background noise influence the selection of tools used for the signal analysis, particularly for local damage detection. Thus, it is extremely important to recognize and identify possible non-Gaussian behavior of the noise. The problem considered here is more general than the classical goodness-of-fit testing. The paper highlights the important role of variance, as most of the methods for signal analysis are based on the assumption of the finite-variance distribution of the underlying signal. The finite variance assumption is crucial but implicit to most indicators used in condition monitoring, (such as the root-mean-square value, the power spectral density, the kurtosis, the spectral correlation, etc.), in view that infinite variance implies moments higher than 2 are also infinite. The problem is demonstrated based on three popular types of non-Gaussian distributions observed for real vibration signals. We demonstrate how the properties of noise distribution in the time domain may change by its transformations to the time-frequency domain (spectrogram). Additionally, we propose a procedure to check the presence of the infinite-variance of the background noise. Our investigations are illustrated using simulation studies and real vibration signals from various machines.","sentences":["Any measurement in condition monitoring applications is associated with disturbing noise.","Till now, most of the diagnostic procedures have assumed the Gaussian distribution for the noise.","This paper shares a novel perspective to the problem of local damage detection.","The acquired vector of observations is considered as an additive mixture of signal of interest (SOI) and noise with strongly non-Gaussian, heavy-tailed properties, that masks the SOI.","The distribution properties of the background noise influence the selection of tools used for the signal analysis, particularly for local damage detection.","Thus, it is extremely important to recognize and identify possible non-Gaussian behavior of the noise.","The problem considered here is more general than the classical goodness-of-fit testing.","The paper highlights the important role of variance, as most of the methods for signal analysis are based on the assumption of the finite-variance distribution of the underlying signal.","The finite variance assumption is crucial but implicit to most indicators used in condition monitoring, (such as the root-mean-square value, the power spectral density, the kurtosis, the spectral correlation, etc.), in view that infinite variance implies moments higher than 2 are also infinite.","The problem is demonstrated based on three popular types of non-Gaussian distributions observed for real vibration signals.","We demonstrate how the properties of noise distribution in the time domain may change by its transformations to the time-frequency domain (spectrogram).","Additionally, we propose a procedure to check the presence of the infinite-variance of the background noise.","Our investigations are illustrated using simulation studies and real vibration signals from various machines."],"url":"http://arxiv.org/abs/2403.08359v1","category":"stat.ME"}
{"created":"2024-03-13 09:14:29","title":"Parameter Constraints on Traversable Wormholes within Beyond Horndeski Theories through Quasi-Periodic Oscillations","abstract":"{\\it Hunting} compact astrophysical objects such as black holes and wormholes, as well as testing gravity theories, are important issues in relativistic astrophysics. In this sense, theoretical and observational studies of quasiperiodic oscillations (QPOs) observed in (micro)quasars become helpful in exploring their central object, which can be a black hole or a wormhole. In the present work, we study the throat properties of traversable wormholes beyond Horndeski theory. Also, we investigate the circular motion of test particles orbiting the wormhole. We analyze the test particle's effective potential and angular momentum for circular orbits. Frequencies of radial and vertical oscillations of the particles around stable circular orbits have also been studied and applied in explaining the quasiperiodic oscillations mechanism in the relativistic precession (RP) model. Finally, we obtain constraint values for the parameters of Horndeski gravity and the mass of the wormhole candidates using QPOs observed in the microquasars GRO J1655-40, GRS 1915+105 \\& XTE J1550-564 and at the center of Milky Way galaxy through Monte-Carlo-Markovian-Chain (MCMC) analyses.","sentences":["{\\it Hunting} compact astrophysical objects such as black holes and wormholes, as well as testing gravity theories, are important issues in relativistic astrophysics.","In this sense, theoretical and observational studies of quasiperiodic oscillations (QPOs) observed in (micro)quasars become helpful in exploring their central object, which can be a black hole or a wormhole.","In the present work, we study the throat properties of traversable wormholes beyond Horndeski theory.","Also, we investigate the circular motion of test particles orbiting the wormhole.","We analyze the test particle's effective potential and angular momentum for circular orbits.","Frequencies of radial and vertical oscillations of the particles around stable circular orbits have also been studied and applied in explaining the quasiperiodic oscillations mechanism in the relativistic precession (RP) model.","Finally, we obtain constraint values for the parameters of Horndeski gravity and the mass of the wormhole candidates using QPOs observed in the microquasars GRO J1655-40, GRS 1915+105 \\& XTE J1550-564 and at the center of Milky Way galaxy through Monte-Carlo-Markovian-Chain (MCMC) analyses."],"url":"http://arxiv.org/abs/2403.08356v1","category":"gr-qc"}
{"created":"2024-03-13 09:00:38","title":"Data augmentation with automated machine learning: approaches and performance comparison with classical data augmentation methods","abstract":"Data augmentation is arguably the most important regularization technique commonly used to improve generalization performance of machine learning models. It primarily involves the application of appropriate data transformation operations to create new data samples with desired properties. Despite its effectiveness, the process is often challenging because of the time-consuming trial and error procedures for creating and testing different candidate augmentations and their hyperparameters manually. Automated data augmentation methods aim to automate the process. State-of-the-art approaches typically rely on automated machine learning (AutoML) principles. This work presents a comprehensive survey of AutoML-based data augmentation techniques. We discuss various approaches for accomplishing data augmentation with AutoML, including data manipulation, data integration and data synthesis techniques. We present extensive discussion of techniques for realizing each of the major subtasks of the data augmentation process: search space design, hyperparameter optimization and model evaluation. Finally, we carried out an extensive comparison and analysis of the performance of automated data augmentation techniques and state-of-the-art methods based on classical augmentation approaches. The results show that AutoML methods for data augmentation currently outperform state-of-the-art techniques based on conventional approaches.","sentences":["Data augmentation is arguably the most important regularization technique commonly used to improve generalization performance of machine learning models.","It primarily involves the application of appropriate data transformation operations to create new data samples with desired properties.","Despite its effectiveness, the process is often challenging because of the time-consuming trial and error procedures for creating and testing different candidate augmentations and their hyperparameters manually.","Automated data augmentation methods aim to automate the process.","State-of-the-art approaches typically rely on automated machine learning (AutoML) principles.","This work presents a comprehensive survey of AutoML-based data augmentation techniques.","We discuss various approaches for accomplishing data augmentation with AutoML, including data manipulation, data integration and data synthesis techniques.","We present extensive discussion of techniques for realizing each of the major subtasks of the data augmentation process: search space design, hyperparameter optimization and model evaluation.","Finally, we carried out an extensive comparison and analysis of the performance of automated data augmentation techniques and state-of-the-art methods based on classical augmentation approaches.","The results show that AutoML methods for data augmentation currently outperform state-of-the-art techniques based on conventional approaches."],"url":"http://arxiv.org/abs/2403.08352v1","category":"cs.LG"}
{"created":"2024-03-13 08:54:31","title":"CoIN: A Benchmark of Continual Instruction tuNing for Multimodel Large Language Model","abstract":"Instruction tuning represents a prevalent strategy employed by Multimodal Large Language Models (MLLMs) to align with human instructions and adapt to new tasks. Nevertheless, MLLMs encounter the challenge of adapting to users' evolving knowledge and demands. Therefore, how to retain existing skills while acquiring new knowledge needs to be investigated. In this paper, we present a comprehensive benchmark, namely Continual Instruction tuNing (CoIN), to assess existing MLLMs in the sequential instruction tuning paradigm. CoIN comprises 10 commonly used datasets spanning 8 task categories, ensuring a diverse range of instructions and tasks. Besides, the trained model is evaluated from two aspects: Instruction Following and General Knowledge, which assess the alignment with human intention and knowledge preserved for reasoning, respectively. Experiments on CoIN demonstrate that current powerful MLLMs still suffer catastrophic forgetting, and the failure in intention alignment assumes the main responsibility, instead of the knowledge forgetting. To this end, we introduce MoELoRA to MLLMs which is effective to retain the previous instruction alignment. Experimental results consistently illustrate the forgetting decreased from this method on CoIN.","sentences":["Instruction tuning represents a prevalent strategy employed by Multimodal Large Language Models (MLLMs) to align with human instructions and adapt to new tasks.","Nevertheless, MLLMs encounter the challenge of adapting to users' evolving knowledge and demands.","Therefore, how to retain existing skills while acquiring new knowledge needs to be investigated.","In this paper, we present a comprehensive benchmark, namely Continual Instruction tuNing (CoIN), to assess existing MLLMs in the sequential instruction tuning paradigm.","CoIN comprises 10 commonly used datasets spanning 8 task categories, ensuring a diverse range of instructions and tasks.","Besides, the trained model is evaluated from two aspects: Instruction Following and General Knowledge, which assess the alignment with human intention and knowledge preserved for reasoning, respectively.","Experiments on CoIN demonstrate that current powerful MLLMs still suffer catastrophic forgetting, and the failure in intention alignment assumes the main responsibility, instead of the knowledge forgetting.","To this end, we introduce MoELoRA to MLLMs which is effective to retain the previous instruction alignment.","Experimental results consistently illustrate the forgetting decreased from this method on CoIN."],"url":"http://arxiv.org/abs/2403.08350v1","category":"cs.CV"}
{"created":"2024-03-13 08:52:51","title":"A programmable topological photonic chip","abstract":"Controlling topological phases of light has allowed experimental observations of abundant topological phenomena and development of robust photonic devices. The prospect of more sophisticated controls with topological photonic devices for practical implementations requires high-level programmability. Here, we demonstrate a fully programmable topological photonic chip with large-scale integration of silicon photonic nanocircuits and microresonators. Photonic artificial atoms and their interactions in our compound system can be individually addressed and controlled, therefore allowing arbitrary altering of structural parameters and geometrical configurations for the observations of dynamic topological phase transitions and diverse photonic topological insulators. By individually programming artificial atoms on the generic chip, it has allowed comprehensive statistic characterisations of topological robustness against relatively weak disorders, as well as counterintuitive topological Anderson phase transitions induced by strong disorders. Our generic topological photonic chip that can be rapidly reprogrammed to implement multifunctionalities, prototypes a flexible and versatile platform for possible applications across fundamental science and topological technologies.","sentences":["Controlling topological phases of light has allowed experimental observations of abundant topological phenomena and development of robust photonic devices.","The prospect of more sophisticated controls with topological photonic devices for practical implementations requires high-level programmability.","Here, we demonstrate a fully programmable topological photonic chip with large-scale integration of silicon photonic nanocircuits and microresonators.","Photonic artificial atoms and their interactions in our compound system can be individually addressed and controlled, therefore allowing arbitrary altering of structural parameters and geometrical configurations for the observations of dynamic topological phase transitions and diverse photonic topological insulators.","By individually programming artificial atoms on the generic chip, it has allowed comprehensive statistic characterisations of topological robustness against relatively weak disorders, as well as counterintuitive topological Anderson phase transitions induced by strong disorders.","Our generic topological photonic chip that can be rapidly reprogrammed to implement multifunctionalities, prototypes a flexible and versatile platform for possible applications across fundamental science and topological technologies."],"url":"http://arxiv.org/abs/2403.08348v1","category":"physics.optics"}
{"created":"2024-03-13 08:51:10","title":"Discretization of Total Variation in Optimization with Integrality Constraint","abstract":"We introduce discretizations of infinite-dimensional optimization problems with total variation regularization and integrality constraints on the optimization variables. We advance the discretization of the dual formulation of the total variation term with Raviart--Thomas functions which is known from literature for certain convex problems. Since we have an integrality constraint, the previous analysis from Caillaud and Chambolle [10] does not hold anymore. Even weaker $\\Gamma$-convergence results do not hold anymore because the recovery sequences generally need to attain non-integer values to recover the total variation of the limit function. We solve this issue by introducing a discretization of the input functions on an embedded, finer mesh. A superlinear coupling of the mesh sizes implies an averaging on the coarser mesh of the Raviart--Thomas ansatz, which enables to recover the total variation of integer-valued limit functions with integer-valued discretized input functions. Moreover, we are able to estimate the discretized total variation of the recovery sequence by the total variation of its limit and an error depending on the mesh size ratio. For the discretized optimization problems, we additionally add a constraint that vanishes in the limit and enforces compactness of the sequence of minimizers, which yields their convergence to a minimizer of the original problem. This constraint contains a degree of freedom whose admissible range is determined. Its choice may have a strong impact on the solutions in practice as we demonstrate with an example from imaging.","sentences":["We introduce discretizations of infinite-dimensional optimization problems with total variation regularization and integrality constraints on the optimization variables.","We advance the discretization of the dual formulation of the total variation term with Raviart--Thomas functions which is known from literature for certain convex problems.","Since we have an integrality constraint, the previous analysis from Caillaud and Chambolle","[10] does not hold anymore.","Even weaker $\\Gamma$-convergence results do not hold anymore because the recovery sequences generally need to attain non-integer values to recover the total variation of the limit function.","We solve this issue by introducing a discretization of the input functions on an embedded, finer mesh.","A superlinear coupling of the mesh sizes implies an averaging on the coarser mesh of the Raviart--Thomas ansatz, which enables to recover the total variation of integer-valued limit functions with integer-valued discretized input functions.","Moreover, we are able to estimate the discretized total variation of the recovery sequence by the total variation of its limit and an error depending on the mesh size ratio.","For the discretized optimization problems, we additionally add a constraint that vanishes in the limit and enforces compactness of the sequence of minimizers, which yields their convergence to a minimizer of the original problem.","This constraint contains a degree of freedom whose admissible range is determined.","Its choice may have a strong impact on the solutions in practice as we demonstrate with an example from imaging."],"url":"http://arxiv.org/abs/2403.08346v1","category":"math.NA"}
{"created":"2024-03-13 08:50:15","title":"From human experts to machines: An LLM supported approach to ontology and knowledge graph construction","abstract":"The conventional process of building Ontologies and Knowledge Graphs (KGs) heavily relies on human domain experts to define entities and relationship types, establish hierarchies, maintain relevance to the domain, fill the ABox (or populate with instances), and ensure data quality (including amongst others accuracy and completeness). On the other hand, Large Language Models (LLMs) have recently gained popularity for their ability to understand and generate human-like natural language, offering promising ways to automate aspects of this process. This work explores the (semi-)automatic construction of KGs facilitated by open-source LLMs. Our pipeline involves formulating competency questions (CQs), developing an ontology (TBox) based on these CQs, constructing KGs using the developed ontology, and evaluating the resultant KG with minimal to no involvement of human experts. We showcase the feasibility of our semi-automated pipeline by creating a KG on deep learning methodologies by exploiting scholarly publications. To evaluate the answers generated via Retrieval-Augmented-Generation (RAG) as well as the KG concepts automatically extracted using LLMs, we design a judge LLM, which rates the generated content based on ground truth. Our findings suggest that employing LLMs could potentially reduce the human effort involved in the construction of KGs, although a human-in-the-loop approach is recommended to evaluate automatically generated KGs.","sentences":["The conventional process of building Ontologies and Knowledge Graphs (KGs) heavily relies on human domain experts to define entities and relationship types, establish hierarchies, maintain relevance to the domain, fill the ABox (or populate with instances), and ensure data quality (including amongst others accuracy and completeness).","On the other hand, Large Language Models (LLMs) have recently gained popularity for their ability to understand and generate human-like natural language, offering promising ways to automate aspects of this process.","This work explores the (semi-)automatic construction of KGs facilitated by open-source LLMs.","Our pipeline involves formulating competency questions (CQs), developing an ontology (TBox) based on these CQs, constructing KGs using the developed ontology, and evaluating the resultant KG with minimal to no involvement of human experts.","We showcase the feasibility of our semi-automated pipeline by creating a KG on deep learning methodologies by exploiting scholarly publications.","To evaluate the answers generated via Retrieval-Augmented-Generation (RAG) as well as the KG concepts automatically extracted using LLMs, we design a judge LLM, which rates the generated content based on ground truth.","Our findings suggest that employing LLMs could potentially reduce the human effort involved in the construction of KGs, although a human-in-the-loop approach is recommended to evaluate automatically generated KGs."],"url":"http://arxiv.org/abs/2403.08345v1","category":"cs.CL"}
{"created":"2024-03-13 08:49:40","title":"STMPL: Human Soft-Tissue Simulation","abstract":"In various applications, such as virtual reality and gaming, simulating the deformation of soft tissues in the human body during interactions with external objects is essential. Traditionally, Finite Element Methods (FEM) have been employed for this purpose, but they tend to be slow and resource-intensive. In this paper, we propose a unified representation of human body shape and soft tissue with a data-driven simulator of non-rigid deformations. This approach enables rapid simulation of realistic interactions.   Our method builds upon the SMPL model, which generates human body shapes considering rigid transformations. We extend SMPL by incorporating a soft tissue layer and an intuitive representation of external forces applied to the body during object interactions. Specifically, we mapped the 3D body shape and soft tissue and applied external forces to 2D UV maps. Leveraging a UNET architecture designed for 2D data, our approach achieves high-accuracy inference in real time. Our experiment shows that our method achieves plausible deformation of the soft tissue layer, even for unseen scenarios.","sentences":["In various applications, such as virtual reality and gaming, simulating the deformation of soft tissues in the human body during interactions with external objects is essential.","Traditionally, Finite Element Methods (FEM) have been employed for this purpose, but they tend to be slow and resource-intensive.","In this paper, we propose a unified representation of human body shape and soft tissue with a data-driven simulator of non-rigid deformations.","This approach enables rapid simulation of realistic interactions.   ","Our method builds upon the SMPL model, which generates human body shapes considering rigid transformations.","We extend SMPL by incorporating a soft tissue layer and an intuitive representation of external forces applied to the body during object interactions.","Specifically, we mapped the 3D body shape and soft tissue and applied external forces to 2D UV maps.","Leveraging a UNET architecture designed for 2D data, our approach achieves high-accuracy inference in real time.","Our experiment shows that our method achieves plausible deformation of the soft tissue layer, even for unseen scenarios."],"url":"http://arxiv.org/abs/2403.08344v1","category":"cs.CV"}
{"created":"2024-03-13 08:48:15","title":"Coverage and Rate Analysis for Integrated Sensing and Communication Networks","abstract":"Integrated sensing and communication (ISAC) is increasingly recognized as a pivotal technology for next-generation cellular networks, offering mutual benefits in both sensing and communication capabilities. This advancement necessitates a re-examination of the fundamental limits within networks where these two functions coexist via shared spectrum and infrastructures. However, traditional stochastic geometry-based performance analyses are confined to either communication or sensing networks separately. This paper bridges this gap by introducing a generalized stochastic geometry framework in ISAC networks. Based on this framework, we define and calculate the coverage and ergodic rate of sensing and communication performance under resource constraints. Then, we shed light on the fundamental limits of ISAC networks by presenting theoretical results for the coverage rate of the unified performance, taking into account the coupling effects of dual functions in coexistence networks. Further, we obtain the analytical formulations for evaluating the ergodic sensing rate constrained by the maximum communication rate, and the ergodic communication rate constrained by the maximum sensing rate. Extensive numerical results validate the accuracy of all theoretical derivations, and also indicate that denser networks significantly enhance ISAC coverage. Specifically, increasing the base station density from $1$ $\\text{km}^{-2}$ to $10$ $\\text{km}^{-2}$ can boost the ISAC coverage rate from $1.4\\%$ to $39.8\\%$. Further, results also reveal that with the increase of the constrained sensing rate, the ergodic communication rate improves significantly, but the reverse is not obvious.","sentences":["Integrated sensing and communication (ISAC) is increasingly recognized as a pivotal technology for next-generation cellular networks, offering mutual benefits in both sensing and communication capabilities.","This advancement necessitates a re-examination of the fundamental limits within networks where these two functions coexist via shared spectrum and infrastructures.","However, traditional stochastic geometry-based performance analyses are confined to either communication or sensing networks separately.","This paper bridges this gap by introducing a generalized stochastic geometry framework in ISAC networks.","Based on this framework, we define and calculate the coverage and ergodic rate of sensing and communication performance under resource constraints.","Then, we shed light on the fundamental limits of ISAC networks by presenting theoretical results for the coverage rate of the unified performance, taking into account the coupling effects of dual functions in coexistence networks.","Further, we obtain the analytical formulations for evaluating the ergodic sensing rate constrained by the maximum communication rate, and the ergodic communication rate constrained by the maximum sensing rate.","Extensive numerical results validate the accuracy of all theoretical derivations, and also indicate that denser networks significantly enhance ISAC coverage.","Specifically, increasing the base station density from $1$ $\\text{km}^{-2}$ to $10$ $\\text{km}^{-2}$ can boost the ISAC coverage rate from $1.4\\%$ to $39.8\\%$. Further, results also reveal that with the increase of the constrained sensing rate, the ergodic communication rate improves significantly, but the reverse is not obvious."],"url":"http://arxiv.org/abs/2403.08343v1","category":"cs.IT"}
{"created":"2024-03-13 08:44:43","title":"Schr{\u00f6}dinger eigenfunctions sharing the same modulus and applications to the control of quantum systems","abstract":"In this paper we investigate when linearly independent eigenfunctions of the Schr\\''odinger operator may have the same modulus. General properties are established and the one-dimensional case is treated in full generality. The study is motivated by its application to the bilinear control of the Schr{\\\"o}dinger equation. By assuming that the potentials of interaction satisfy a saturation property and by adapting a strategy recently proposed by Duca and Nersesyan, we discuss when the system can be steered arbitrarily fast between energy levels. Extensions of the previous results to quantum graphs are finally presented.","sentences":["In this paper we investigate when linearly independent eigenfunctions of the Schr\\''odinger operator may have the same modulus.","General properties are established and the one-dimensional case is treated in full generality.","The study is motivated by its application to the bilinear control of the Schr{\\\"o}dinger equation.","By assuming that the potentials of interaction satisfy a saturation property and by adapting a strategy recently proposed by Duca and Nersesyan, we discuss when the system can be steered arbitrarily fast between energy levels.","Extensions of the previous results to quantum graphs are finally presented."],"url":"http://arxiv.org/abs/2403.08341v1","category":"math.OC"}
{"created":"2024-03-13 08:43:06","title":"MorphoGear: An UAV with Multi-Limb Morphogenetic Gear for Rough-Terrain Locomotion","abstract":"Robots able to run, fly, and grasp have a high potential to solve a wide scope of tasks and navigate in complex environments. Several mechatronic designs of such robots with adaptive morphologies are emerging. However, the task of landing on an uneven surface, traversing rough terrain, and manipulating objects still presents high challenges.   This paper introduces the design of a novel rotor UAV MorphoGear with morphogenetic gear and includes a description of the robot's mechanics, electronics, and control architecture, as well as walking behavior and an analysis of experimental results. MorphoGear is able to fly, walk on surfaces with several gaits, and grasp objects with four compatible robotic limbs. Robotic limbs with three degrees of freedom (DoFs) are used by this UAV as pedipulators when walking or flying and as manipulators when performing actions in the environment. We performed a locomotion analysis of the landing gear of the robot. Three types of robot gaits have been developed.   The experimental results revealed low crosstrack error of the most accurate gait (mean of 1.9 cm and max of 5.5 cm) and the ability of the drone to move with a 210 mm step length. Another type of robot gait also showed low crosstrack error (mean of 2.3 cm and max of 6.9 cm). The proposed MorphoGear system can potentially achieve a high scope of tasks in environmental surveying, delivery, and high-altitude operations.","sentences":["Robots able to run, fly, and grasp have a high potential to solve a wide scope of tasks and navigate in complex environments.","Several mechatronic designs of such robots with adaptive morphologies are emerging.","However, the task of landing on an uneven surface, traversing rough terrain, and manipulating objects still presents high challenges.   ","This paper introduces the design of a novel rotor UAV MorphoGear with morphogenetic gear and includes a description of the robot's mechanics, electronics, and control architecture, as well as walking behavior and an analysis of experimental results.","MorphoGear is able to fly, walk on surfaces with several gaits, and grasp objects with four compatible robotic limbs.","Robotic limbs with three degrees of freedom (DoFs) are used by this UAV as pedipulators when walking or flying and as manipulators when performing actions in the environment.","We performed a locomotion analysis of the landing gear of the robot.","Three types of robot gaits have been developed.   ","The experimental results revealed low crosstrack error of the most accurate gait (mean of 1.9 cm and max of 5.5 cm) and the ability of the drone to move with a 210 mm step length.","Another type of robot gait also showed low crosstrack error (mean of 2.3 cm and max of 6.9 cm).","The proposed MorphoGear system can potentially achieve a high scope of tasks in environmental surveying, delivery, and high-altitude operations."],"url":"http://arxiv.org/abs/2403.08340v1","category":"cs.RO"}
{"created":"2024-03-13 08:43:01","title":"Low-Complexity Beam Training for Multi-RIS-Assisted Multi-User Communications","abstract":"In this paper, we investigate the beam training problem in the multi-user millimeter wave (mmWave) communication system, where multiple reconfigurable intelligent surfaces (RISs) are deployed to improve the coverage and the achievable rate. However, existing beam training techniques in mmWave systems suffer from the high complexity (i.e., exponential order) and low identification accuracy. To address these problems, we propose a novel hashing multi-arm beam (HMB) training scheme that reduces the training complexity to the logarithmic order with the high accuracy. Specifically, we first design a generation mechanism for HMB codebooks. Then, we propose a demultiplexing algorithm based on the soft decision to distinguish signals from different RIS reflective links. Finally, we utilize a multi-round voting mechanism to align the beams. Simulation results show that the proposed HMB training scheme enables simultaneous training for multiple RISs and multiple users, and reduces the beam training overhead to the logarithmic level. Moreover, it also shows that our proposed scheme can significantly improve the identification accuracy by at least 20% compared to existing beam training techniques.","sentences":["In this paper, we investigate the beam training problem in the multi-user millimeter wave (mmWave) communication system, where multiple reconfigurable intelligent surfaces (RISs) are deployed to improve the coverage and the achievable rate.","However, existing beam training techniques in mmWave systems suffer from the high complexity (i.e., exponential order) and low identification accuracy.","To address these problems, we propose a novel hashing multi-arm beam (HMB) training scheme that reduces the training complexity to the logarithmic order with the high accuracy.","Specifically, we first design a generation mechanism for HMB codebooks.","Then, we propose a demultiplexing algorithm based on the soft decision to distinguish signals from different RIS reflective links.","Finally, we utilize a multi-round voting mechanism to align the beams.","Simulation results show that the proposed HMB training scheme enables simultaneous training for multiple RISs and multiple users, and reduces the beam training overhead to the logarithmic level.","Moreover, it also shows that our proposed scheme can significantly improve the identification accuracy by at least 20% compared to existing beam training techniques."],"url":"http://arxiv.org/abs/2403.08339v1","category":"cs.IT"}
{"created":"2024-03-13 08:41:55","title":"LLM-Assisted Light: Leveraging Large Language Model Capabilities for Human-Mimetic Traffic Signal Control in Complex Urban Environments","abstract":"Traffic congestion in metropolitan areas presents a formidable challenge with far-reaching economic, environmental, and societal ramifications. Therefore, effective congestion management is imperative, with traffic signal control (TSC) systems being pivotal in this endeavor. Conventional TSC systems, designed upon rule-based algorithms or reinforcement learning (RL), frequently exhibit deficiencies in managing the complexities and variabilities of urban traffic flows, constrained by their limited capacity for adaptation to unfamiliar scenarios. In response to these limitations, this work introduces an innovative approach that integrates Large Language Models (LLMs) into TSC, harnessing their advanced reasoning and decision-making faculties. Specifically, a hybrid framework that augments LLMs with a suite of perception and decision-making tools is proposed, facilitating the interrogation of both the static and dynamic traffic information. This design places the LLM at the center of the decision-making process, combining external traffic data with established TSC methods. Moreover, a simulation platform is developed to corroborate the efficacy of the proposed framework. The findings from our simulations attest to the system's adeptness in adjusting to a multiplicity of traffic environments without the need for additional training. Notably, in cases of Sensor Outage (SO), our approach surpasses conventional RL-based systems by reducing the average waiting time by $20.4\\%$. This research signifies a notable advance in TSC strategies and paves the way for the integration of LLMs into real-world, dynamic scenarios, highlighting their potential to revolutionize traffic management. The related code is available at \\href{https://github.com/Traffic-Alpha/LLM-Assisted-Light}{https://github.com/Traffic-Alpha/LLM-Assisted-Light}.","sentences":["Traffic congestion in metropolitan areas presents a formidable challenge with far-reaching economic, environmental, and societal ramifications.","Therefore, effective congestion management is imperative, with traffic signal control (TSC) systems being pivotal in this endeavor.","Conventional TSC systems, designed upon rule-based algorithms or reinforcement learning (RL), frequently exhibit deficiencies in managing the complexities and variabilities of urban traffic flows, constrained by their limited capacity for adaptation to unfamiliar scenarios.","In response to these limitations, this work introduces an innovative approach that integrates Large Language Models (LLMs) into TSC, harnessing their advanced reasoning and decision-making faculties.","Specifically, a hybrid framework that augments LLMs with a suite of perception and decision-making tools is proposed, facilitating the interrogation of both the static and dynamic traffic information.","This design places the LLM at the center of the decision-making process, combining external traffic data with established TSC methods.","Moreover, a simulation platform is developed to corroborate the efficacy of the proposed framework.","The findings from our simulations attest to the system's adeptness in adjusting to a multiplicity of traffic environments without the need for additional training.","Notably, in cases of Sensor Outage (SO), our approach surpasses conventional RL-based systems by reducing the average waiting time by $20.4\\%$. This research signifies a notable advance in TSC strategies and paves the way for the integration of LLMs into real-world, dynamic scenarios, highlighting their potential to revolutionize traffic management.","The related code is available at \\href{https://github.com/Traffic-Alpha/LLM-Assisted-Light}{https://github.com/Traffic-Alpha/LLM-Assisted-Light}."],"url":"http://arxiv.org/abs/2403.08337v1","category":"eess.SY"}
{"created":"2024-03-13 08:40:49","title":"A Sparsity Principle for Partially Observable Causal Representation Learning","abstract":"Causal representation learning aims at identifying high-level causal variables from perceptual data. Most methods assume that all latent causal variables are captured in the high-dimensional observations. We instead consider a partially observed setting, in which each measurement only provides information about a subset of the underlying causal state. Prior work has studied this setting with multiple domains or views, each depending on a fixed subset of latents. Here, we focus on learning from unpaired observations from a dataset with an instance-dependent partial observability pattern. Our main contribution is to establish two identifiability results for this setting: one for linear mixing functions without parametric assumptions on the underlying causal model, and one for piecewise linear mixing functions with Gaussian latent causal variables. Based on these insights, we propose two methods for estimating the underlying causal variables by enforcing sparsity in the inferred representation. Experiments on different simulated datasets and established benchmarks highlight the effectiveness of our approach in recovering the ground-truth latents.","sentences":["Causal representation learning aims at identifying high-level causal variables from perceptual data.","Most methods assume that all latent causal variables are captured in the high-dimensional observations.","We instead consider a partially observed setting, in which each measurement only provides information about a subset of the underlying causal state.","Prior work has studied this setting with multiple domains or views, each depending on a fixed subset of latents.","Here, we focus on learning from unpaired observations from a dataset with an instance-dependent partial observability pattern.","Our main contribution is to establish two identifiability results for this setting: one for linear mixing functions without parametric assumptions on the underlying causal model, and one for piecewise linear mixing functions with Gaussian latent causal variables.","Based on these insights, we propose two methods for estimating the underlying causal variables by enforcing sparsity in the inferred representation.","Experiments on different simulated datasets and established benchmarks highlight the effectiveness of our approach in recovering the ground-truth latents."],"url":"http://arxiv.org/abs/2403.08335v1","category":"cs.LG"}
{"created":"2024-03-13 08:37:31","title":"Fast Inference of Removal-Based Node Influence","abstract":"Graph neural networks (GNNs) are widely utilized to capture the information spreading patterns in graphs. While remarkable performance has been achieved, there is a new trending topic of evaluating node influence. We propose a new method of evaluating node influence, which measures the prediction change of a trained GNN model caused by removing a node. A real-world application is, \"In the task of predicting Twitter accounts' polarity, had a particular account been removed, how would others' polarity change?\". We use the GNN as a surrogate model whose prediction could simulate the change of nodes or edges caused by node removal. To obtain the influence for every node, a straightforward way is to alternately remove every node and apply the trained GNN on the modified graph. It is reliable but time-consuming, so we need an efficient method. The related lines of work, such as graph adversarial attack and counterfactual explanation, cannot directly satisfy our needs, since they do not focus on the global influence score for every node. We propose an efficient and intuitive method, NOde-Removal-based fAst GNN inference (NORA), which uses the gradient to approximate the node-removal influence. It only costs one forward propagation and one backpropagation to approximate the influence score for all nodes. Extensive experiments on six datasets and six GNN models verify the effectiveness of NORA. Our code is available at https://github.com/weikai-li/NORA.git.","sentences":["Graph neural networks (GNNs) are widely utilized to capture the information spreading patterns in graphs.","While remarkable performance has been achieved, there is a new trending topic of evaluating node influence.","We propose a new method of evaluating node influence, which measures the prediction change of a trained GNN model caused by removing a node.","A real-world application is, \"In the task of predicting Twitter accounts' polarity, had a particular account been removed, how would others' polarity change?\".","We use the GNN as a surrogate model whose prediction could simulate the change of nodes or edges caused by node removal.","To obtain the influence for every node, a straightforward way is to alternately remove every node and apply the trained GNN on the modified graph.","It is reliable but time-consuming, so we need an efficient method.","The related lines of work, such as graph adversarial attack and counterfactual explanation, cannot directly satisfy our needs, since they do not focus on the global influence score for every node.","We propose an efficient and intuitive method, NOde-Removal-based fAst GNN inference (NORA), which uses the gradient to approximate the node-removal influence.","It only costs one forward propagation and one backpropagation to approximate the influence score for all nodes.","Extensive experiments on six datasets and six GNN models verify the effectiveness of NORA.","Our code is available at https://github.com/weikai-li/NORA.git."],"url":"http://arxiv.org/abs/2403.08333v1","category":"cs.LG"}
{"created":"2024-03-13 08:34:53","title":"Autoregressive Score Generation for Multi-trait Essay Scoring","abstract":"Recently, encoder-only pre-trained models such as BERT have been successfully applied in automated essay scoring (AES) to predict a single overall score. However, studies have yet to explore these models in multi-trait AES, possibly due to the inefficiency of replicating BERT-based models for each trait. Breaking away from the existing sole use of encoder, we propose an autoregressive prediction of multi-trait scores (ArTS), incorporating a decoding process by leveraging the pre-trained T5. Unlike prior regression or classification methods, we redefine AES as a score-generation task, allowing a single model to predict multiple scores. During decoding, the subsequent trait prediction can benefit by conditioning on the preceding trait scores. Experimental results proved the efficacy of ArTS, showing over 5% average improvements in both prompts and traits.","sentences":["Recently, encoder-only pre-trained models such as BERT have been successfully applied in automated essay scoring (AES) to predict a single overall score.","However, studies have yet to explore these models in multi-trait AES, possibly due to the inefficiency of replicating BERT-based models for each trait.","Breaking away from the existing sole use of encoder, we propose an autoregressive prediction of multi-trait scores (ArTS), incorporating a decoding process by leveraging the pre-trained T5.","Unlike prior regression or classification methods, we redefine AES as a score-generation task, allowing a single model to predict multiple scores.","During decoding, the subsequent trait prediction can benefit by conditioning on the preceding trait scores.","Experimental results proved the efficacy of ArTS, showing over 5% average improvements in both prompts and traits."],"url":"http://arxiv.org/abs/2403.08332v1","category":"cs.CL"}
{"created":"2024-03-13 08:24:53","title":"Noninteger high-harmonic generation from extended correlated systems","abstract":"The spectra produced by high-harmonic generation (HHG) typically exhibit well-defined peaks at odd integers times the laser frequency. However, in recent investigations of HHG from correlated materials, spectra exhibit signals at noninteger harmonics which do not conform to the well-known symmetry-based selection rules for HHG-spectra. Here, we use the Fermi-Hubbard model to study HHG from a linear chain of atoms. This model allows us to study both the correlated and uncorrelated phases through a specification of the amount of onsite electron-electron repulsion. The presence of signal at noninteger harmonics can be interpreted as originating from the population of multiple Floquet states. We show how this coupling to different Floquet states depends on the characteristics of the driving pulse and the strength of the electron-electron interaction in the system.","sentences":["The spectra produced by high-harmonic generation (HHG) typically exhibit well-defined peaks at odd integers times the laser frequency.","However, in recent investigations of HHG from correlated materials, spectra exhibit signals at noninteger harmonics which do not conform to the well-known symmetry-based selection rules for HHG-spectra.","Here, we use the Fermi-Hubbard model to study HHG from a linear chain of atoms.","This model allows us to study both the correlated and uncorrelated phases through a specification of the amount of onsite electron-electron repulsion.","The presence of signal at noninteger harmonics can be interpreted as originating from the population of multiple Floquet states.","We show how this coupling to different Floquet states depends on the characteristics of the driving pulse and the strength of the electron-electron interaction in the system."],"url":"http://arxiv.org/abs/2403.08327v1","category":"physics.atom-ph"}
{"created":"2024-03-13 08:21:21","title":"Dynamic flexoelectric instabilities in nematic liquid crystals","abstract":"Electro-hydrodynamic phenomena in liquid crystals constitute an old but still very active research area. The reason is that these phenomena play the key role in various applications of liquid crystals and due to the general interest of physical community to out-of-equilibrium systems. Nematic liquid crystals (NLCs) are ideally representative for such investigations. Our article aims to study theoretically the linear NLCs dynamics. We include into consideration orientation elastic energy, hydrodynamic motion, external alternating electric field, electric conductivity and flexoelectric polarization. We analyze the linear stability of the NLC film, determining dynamics of perturbations with respect to the homogeneous initial state of the NLC. For the purpose we compute eigen-values of the evolution matrix for a period of the external alternating electric field. These eigen-values determine the amplification factors for the modes during the period. The instability occurs when the principal eigen-value of the evolution matrix becomes unity by its absolute value. The condition determines the threshold (critical field) for the instability of the uniform state. It turns out that one might expect various types of the instability, only partially known and investigated in the literature. Particularly, we find that the flexoelectric instability may lead to two-dimensionally space modulated patterns exhibiting time oscillations. This type of the structures was somehow overlooked in the previous works.","sentences":["Electro-hydrodynamic phenomena in liquid crystals constitute an old but still very active research area.","The reason is that these phenomena play the key role in various applications of liquid crystals and due to the general interest of physical community to out-of-equilibrium systems.","Nematic liquid crystals (NLCs) are ideally representative for such investigations.","Our article aims to study theoretically the linear NLCs dynamics.","We include into consideration orientation elastic energy, hydrodynamic motion, external alternating electric field, electric conductivity and flexoelectric polarization.","We analyze the linear stability of the NLC film, determining dynamics of perturbations with respect to the homogeneous initial state of the NLC.","For the purpose we compute eigen-values of the evolution matrix for a period of the external alternating electric field.","These eigen-values determine the amplification factors for the modes during the period.","The instability occurs when the principal eigen-value of the evolution matrix becomes unity by its absolute value.","The condition determines the threshold (critical field) for the instability of the uniform state.","It turns out that one might expect various types of the instability, only partially known and investigated in the literature.","Particularly, we find that the flexoelectric instability may lead to two-dimensionally space modulated patterns exhibiting time oscillations.","This type of the structures was somehow overlooked in the previous works."],"url":"http://arxiv.org/abs/2403.08325v1","category":"cond-mat.soft"}
{"created":"2024-03-13 08:06:48","title":"Generalized free energy and thermodynamic phases of black holes in the gauged Kaluza-Klein theory","abstract":"In the context of the generalized (off-shell) free energy, we explore the phase emergence and corresponding phase transitions of charged dilaton $\\text{AdS}$ black holes in the gauged Kaluza-Klein (KK) theory where the KK vector field is gauged such that the fermionic fields are charged under the U(1)$_{\\text{KK}}$ gauge group. The black hole solutions are asymptotic to the AdS$_D$ geometry and can be realized as the dimensional reduction of the gauged supergravities on the compact internal manifolds, leading to the restriction as $4\\leq D\\leq 7$. By studying the behavior of the generalized free energy under the change of the ensemble temperature, we determine the thermodynamic phases and the corresponding phase transitions of black holes. This is confirmed by investigating the heat capacity at the constant pressure and the on-shell free energy. In the canonical ensemble, the thermodynamics of black holes can be classified into three different classes as follows: (i) $D=4$, (ii) $D=5$, and (iii) $D=6,7$. Whereas, in the grand canonical ensemble, the thermodynamics of black holes is independent of the number of spacetime dimensions and the pressure, but depends on the chemical potential $\\Phi$. The thermodynamic behavior of black holes can be classified into three different classes as follows: (i) $\\Phi<1$, (ii) $\\Phi>1$, and (iii) $\\Phi=1$.","sentences":["In the context of the generalized (off-shell) free energy, we explore the phase emergence and corresponding phase transitions of charged dilaton $\\text{AdS}$ black holes in the gauged Kaluza-Klein (KK) theory where the KK vector field is gauged such that the fermionic fields are charged under the U(1)$_{\\text{KK}}$ gauge group.","The black hole solutions are asymptotic to the AdS$_D$ geometry and can be realized as the dimensional reduction of the gauged supergravities on the compact internal manifolds, leading to the restriction as $4\\leq D\\leq 7$.","By studying the behavior of the generalized free energy under the change of the ensemble temperature, we determine the thermodynamic phases and the corresponding phase transitions of black holes.","This is confirmed by investigating the heat capacity at the constant pressure and the on-shell free energy.","In the canonical ensemble, the thermodynamics of black holes can be classified into three different classes as follows: (i) $D=4$, (ii) $D=5$, and (iii) $D=6,7$. Whereas, in the grand canonical ensemble, the thermodynamics of black holes is independent of the number of spacetime dimensions and the pressure, but depends on the chemical potential $\\Phi$. The thermodynamic behavior of black holes can be classified into three different classes as follows: (i) $\\Phi<1$, (ii) $\\Phi>1$, and (iii) $\\Phi=1$."],"url":"http://arxiv.org/abs/2403.08322v1","category":"gr-qc"}
{"created":"2024-03-13 08:06:41","title":"ManiGaussian: Dynamic Gaussian Splatting for Multi-task Robotic Manipulation","abstract":"Performing language-conditioned robotic manipulation tasks in unstructured environments is highly demanded for general intelligent robots. Conventional robotic manipulation methods usually learn semantic representation of the observation for action prediction, which ignores the scene-level spatiotemporal dynamics for human goal completion. In this paper, we propose a dynamic Gaussian Splatting method named ManiGaussian for multi-task robotic manipulation, which mines scene dynamics via future scene reconstruction. Specifically, we first formulate the dynamic Gaussian Splatting framework that infers the semantics propagation in the Gaussian embedding space, where the semantic representation is leveraged to predict the optimal robot action. Then, we build a Gaussian world model to parameterize the distribution in our dynamic Gaussian Splatting framework, which provides informative supervision in the interactive environment via future scene reconstruction. We evaluate our ManiGaussian on 10 RLBench tasks with 166 variations, and the results demonstrate our framework can outperform the state-of-the-art methods by 13.1\\% in average success rate.","sentences":["Performing language-conditioned robotic manipulation tasks in unstructured environments is highly demanded for general intelligent robots.","Conventional robotic manipulation methods usually learn semantic representation of the observation for action prediction, which ignores the scene-level spatiotemporal dynamics for human goal completion.","In this paper, we propose a dynamic Gaussian Splatting method named ManiGaussian for multi-task robotic manipulation, which mines scene dynamics via future scene reconstruction.","Specifically, we first formulate the dynamic Gaussian Splatting framework that infers the semantics propagation in the Gaussian embedding space, where the semantic representation is leveraged to predict the optimal robot action.","Then, we build a Gaussian world model to parameterize the distribution in our dynamic Gaussian Splatting framework, which provides informative supervision in the interactive environment via future scene reconstruction.","We evaluate our ManiGaussian on 10 RLBench tasks with 166 variations, and the results demonstrate our framework can outperform the state-of-the-art methods by 13.1\\% in average success rate."],"url":"http://arxiv.org/abs/2403.08321v1","category":"cs.RO"}
{"created":"2024-03-13 08:02:31","title":"Benchmarking quantum master equations beyond ultraweak coupling","abstract":"Recently, Nathan and Rudner derived a Gorini-Kossakowski-Sudarshan-Lindblad master equation from the Redfield equation. The claim is that the level of approximation is equal to that of the Redfield equation. This is ground breaking work since a quantum master equation that guarantees physical states and that is as accurate as the Redfield equation is still missing. Here we benchmark the Nathan-Rudner equation (NRE) against the exact solution of a damped harmonic oscillator and compare its performance to that of the time-dependent Redfield equation (RE). We find that which of the equations performs better depends on the regime considered. It turns out that the short-time dynamics is generally much better captured by the RE, whereas the NRE delivers similar results to the rotating-wave approximation. For the steady state, in the high-temperature limit the RE also performs better and its solution approaches the exact result for ultrahigh temperatures. Nevertheless, also here the NR equation constitutes a good approximation. In the low-temperature limit, in turn, the NRE still provides a good approximation to the steady state, while the solution of the RE becomes unphysical for too strong coupling. Moreover, we show that, like the RE, also the NRE approaches the correct steady state in the limit of vanishing system-bath coupling. However, in second-order system-bath coupling, where the RE is known to provide the steady-state coherences correctly but not the populations, the NRE generally neither reproduces the correct populations nor the coherences.","sentences":["Recently, Nathan and Rudner derived a Gorini-Kossakowski-Sudarshan-Lindblad master equation from the Redfield equation.","The claim is that the level of approximation is equal to that of the Redfield equation.","This is ground breaking work since a quantum master equation that guarantees physical states and that is as accurate as the Redfield equation is still missing.","Here we benchmark the Nathan-Rudner equation (NRE) against the exact solution of a damped harmonic oscillator and compare its performance to that of the time-dependent Redfield equation (RE).","We find that which of the equations performs better depends on the regime considered.","It turns out that the short-time dynamics is generally much better captured by the RE, whereas the NRE delivers similar results to the rotating-wave approximation.","For the steady state, in the high-temperature limit the RE also performs better and its solution approaches the exact result for ultrahigh temperatures.","Nevertheless, also here the NR equation constitutes a good approximation.","In the low-temperature limit, in turn, the NRE still provides a good approximation to the steady state, while the solution of the RE becomes unphysical for too strong coupling.","Moreover, we show that, like the RE, also the NRE approaches the correct steady state in the limit of vanishing system-bath coupling.","However, in second-order system-bath coupling, where the RE is known to provide the steady-state coherences correctly but not the populations, the NRE generally neither reproduces the correct populations nor the coherences."],"url":"http://arxiv.org/abs/2403.08320v1","category":"quant-ph"}
{"created":"2024-03-13 08:02:23","title":"Knowledge Conflicts for LLMs: A Survey","abstract":"This survey provides an in-depth analysis of knowledge conflicts for large language models (LLMs), highlighting the complex challenges they encounter when blending contextual and parametric knowledge. Our focus is on three categories of knowledge conflicts: context-memory, inter-context, and intra-memory conflict. These conflicts can significantly impact the trustworthiness and performance of LLMs, especially in real-world applications where noise and misinformation are common. By categorizing these conflicts, exploring the causes, examining the behaviors of LLMs under such conflicts, and reviewing available solutions, this survey aims to shed light on strategies for improving the robustness of LLMs, thereby serving as a valuable resource for advancing research in this evolving area.","sentences":["This survey provides an in-depth analysis of knowledge conflicts for large language models (LLMs), highlighting the complex challenges they encounter when blending contextual and parametric knowledge.","Our focus is on three categories of knowledge conflicts: context-memory, inter-context, and intra-memory conflict.","These conflicts can significantly impact the trustworthiness and performance of LLMs, especially in real-world applications where noise and misinformation are common.","By categorizing these conflicts, exploring the causes, examining the behaviors of LLMs under such conflicts, and reviewing available solutions, this survey aims to shed light on strategies for improving the robustness of LLMs, thereby serving as a valuable resource for advancing research in this evolving area."],"url":"http://arxiv.org/abs/2403.08319v1","category":"cs.CL"}
{"created":"2024-03-13 07:54:01","title":"From Channel Measurement to Training Data for PHY Layer AI Applications","abstract":"Learning-based techniques such as artificial intelligence (AI) and machine learning (ML) play an increasingly important role in the development of future communication networks. The success of a learning algorithm depends on the quality and quantity of the available training data. In the physical layer (PHY), channel information data can be obtained either through measurement campaigns or through simulations based on predefined channel models. Performing measurements can be time consuming while only gaining information about one specific position or scenario. Simulated data, on the other hand, are more generalized and reflect in most cases not a real environment but instead, a statistical approximation based on a mathematical model. This paper presents a procedure for acquiring channel data by means of fast and flexible software defined radio (SDR) based channel measurements along with a method for a parameter extraction that provides configuration input to the simulator. The procedure from the measurement to the simulated channel data is demonstrated in two exemplary propagation scenarios. It is shown, that in both cases the simulated data is in good accordance to the measurements","sentences":["Learning-based techniques such as artificial intelligence (AI) and machine learning (ML) play an increasingly important role in the development of future communication networks.","The success of a learning algorithm depends on the quality and quantity of the available training data.","In the physical layer (PHY), channel information data can be obtained either through measurement campaigns or through simulations based on predefined channel models.","Performing measurements can be time consuming while only gaining information about one specific position or scenario.","Simulated data, on the other hand, are more generalized and reflect in most cases not a real environment but instead, a statistical approximation based on a mathematical model.","This paper presents a procedure for acquiring channel data by means of fast and flexible software defined radio (SDR) based channel measurements along with a method for a parameter extraction that provides configuration input to the simulator.","The procedure from the measurement to the simulated channel data is demonstrated in two exemplary propagation scenarios.","It is shown, that in both cases the simulated data is in good accordance to the measurements"],"url":"http://arxiv.org/abs/2403.08317v1","category":"cs.NI"}
{"created":"2024-03-13 07:45:46","title":"An improvement on the Louvain algorithm using random walks","abstract":"We will present improvements to famous algorithms for community detection, namely Newman's spectral method algorithm and the Louvain algorithm. The Newman algorithm begins by treating the original graph as a single cluster, then repeats the process to split each cluster into two, based on the signs of the eigenvector corresponding to the secondlargest eigenvalue. Our improvement involves replacing the time-consuming computation of eigenvalues with a random walk during the splitting process. The Louvain algorithm iteratively performs the following steps until no increase in modularity can be achieved anymore: each step consists of two phases, phase 1 for partitioning the graph into clusters, and phase 2 for constructing a new graph where each vertex represents one cluster obtained from phase 1. We propose an improvement to this algorithm by adding our random walk algorithm as an additional phase for refining clusters obtained from phase 1. It maintains a complexity comparable to the Louvain algorithm while exhibiting superior efficiency. To validate the robustness and effectiveness of our proposed algorithms, we conducted experiments using randomly generated graphs and real-world data.","sentences":["We will present improvements to famous algorithms for community detection, namely Newman's spectral method algorithm and the Louvain algorithm.","The Newman algorithm begins by treating the original graph as a single cluster, then repeats the process to split each cluster into two, based on the signs of the eigenvector corresponding to the secondlargest eigenvalue.","Our improvement involves replacing the time-consuming computation of eigenvalues with a random walk during the splitting process.","The Louvain algorithm iteratively performs the following steps until no increase in modularity can be achieved anymore: each step consists of two phases, phase 1 for partitioning the graph into clusters, and phase 2 for constructing a new graph where each vertex represents one cluster obtained from phase 1.","We propose an improvement to this algorithm by adding our random walk algorithm as an additional phase for refining clusters obtained from phase 1.","It maintains a complexity comparable to the Louvain algorithm while exhibiting superior efficiency.","To validate the robustness and effectiveness of our proposed algorithms, we conducted experiments using randomly generated graphs and real-world data."],"url":"http://arxiv.org/abs/2403.08313v1","category":"cs.SI"}
{"created":"2024-03-13 07:44:14","title":"StreamingDialogue: Prolonged Dialogue Learning via Long Context Compression with Minimal Losses","abstract":"Standard Large Language Models (LLMs) struggle with handling dialogues with long contexts due to efficiency and consistency issues. According to our observation, dialogue contexts are highly structured, and the special token of \\textit{End-of-Utterance} (EoU) in dialogues has the potential to aggregate information. We refer to the EoU tokens as ``conversational attention sinks'' (conv-attn sinks). Accordingly, we introduce StreamingDialogue, which compresses long dialogue history into conv-attn sinks with minimal losses, and thus reduces computational complexity quadratically with the number of sinks (i.e., the number of utterances). Current LLMs already demonstrate the ability to handle long context window, e.g., a window size of 200k or more. To this end, by compressing utterances into EoUs, our method has the potential to handle more than 200k of utterances, resulting in a prolonged dialogue learning. In order to minimize information losses from reconstruction after compression, we design two learning strategies of short-memory reconstruction (SMR) and long-memory reactivation (LMR). Our method outperforms strong baselines in dialogue tasks and achieves a 4 $\\times$ speedup while reducing memory usage by 18 $\\times$ compared to dense attention recomputation.","sentences":["Standard Large Language Models (LLMs) struggle with handling dialogues with long contexts due to efficiency and consistency issues.","According to our observation, dialogue contexts are highly structured, and the special token of \\textit{End-of-Utterance} (EoU) in dialogues has the potential to aggregate information.","We refer to the EoU tokens as ``conversational attention sinks'' (conv-attn sinks).","Accordingly, we introduce StreamingDialogue, which compresses long dialogue history into conv-attn sinks with minimal losses, and thus reduces computational complexity quadratically with the number of sinks (i.e., the number of utterances).","Current LLMs already demonstrate the ability to handle long context window, e.g., a window size of 200k or more.","To this end, by compressing utterances into EoUs, our method has the potential to handle more than 200k of utterances, resulting in a prolonged dialogue learning.","In order to minimize information losses from reconstruction after compression, we design two learning strategies of short-memory reconstruction (SMR) and long-memory reactivation (LMR).","Our method outperforms strong baselines in dialogue tasks and achieves a 4 $\\times$ speedup while reducing memory usage by 18 $\\times$ compared to dense attention recomputation."],"url":"http://arxiv.org/abs/2403.08312v1","category":"cs.CL"}
{"created":"2024-03-13 17:35:28","title":"GaussCtrl: Multi-View Consistent Text-Driven 3D Gaussian Splatting Editing","abstract":"We propose GaussCtrl, a text-driven method to edit a 3D scene reconstructed by the 3D Gaussian Splatting (3DGS).   Our method first renders a collection of images by using the 3DGS and edits them by using a pre-trained 2D diffusion model (ControlNet) based on the input prompt, which is then used to optimise the 3D model.   Our key contribution is multi-view consistent editing, which enables editing all images together instead of iteratively editing one image while updating the 3D model as in previous works.   It leads to faster editing as well as higher visual quality.   This is achieved by the two terms:   (a) depth-conditioned editing that enforces geometric consistency across multi-view images by leveraging naturally consistent depth maps.   (b) attention-based latent code alignment that unifies the appearance of edited images by conditioning their editing to several reference views through self and cross-view attention between images' latent representations.   Experiments demonstrate that our method achieves faster editing and better visual results than previous state-of-the-art methods.","sentences":["We propose GaussCtrl, a text-driven method to edit a 3D scene reconstructed by the 3D Gaussian Splatting (3DGS).   ","Our method first renders a collection of images by using the 3DGS and edits them by using a pre-trained 2D diffusion model (ControlNet) based on the input prompt, which is then used to optimise the 3D model.   ","Our key contribution is multi-view consistent editing, which enables editing all images together instead of iteratively editing one image while updating the 3D model as in previous works.   ","It leads to faster editing as well as higher visual quality.   ","This is achieved by the two terms:   (a) depth-conditioned editing that enforces geometric consistency across multi-view images by leveraging naturally consistent depth maps.   ","(b) attention-based latent code alignment that unifies the appearance of edited images by conditioning their editing to several reference views through self and cross-view attention between images' latent representations.   ","Experiments demonstrate that our method achieves faster editing and better visual results than previous state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.08733v1","category":"cs.CV"}
{"created":"2024-03-13 17:25:17","title":"Driving non-trivial quantum phases in conventional semiconductors with intense excitonic fields","abstract":"Inducing novel quantum phases and topologies in materials using intense light fields is a key objective of modern condensed matter physics, but nonetheless faces significant experimental challenges. Alternately, theory predicts that in the dense limit, excitons - collective excitations composed of Coulomb-bound electron-hole pairs - could also drive exotic quantum phenomena. However, the direct observation of these phenomena requires the resolution of electronic structure in momentum space in the presence of excitons, which became possible only recently. Here, using time- and angle-resolved photoemission spectroscopy of an atomically thin semiconductor in the presence of a high-density of resonantly and coherently photoexcited excitons, we observe the Bardeen-Cooper-Schrieffer (BCS) excitonic state - analogous to the Cooper pairs of superconductivity. We see the valence band transform from a conventional paraboloid into a Mexican-hat like Bogoliubov dispersion - a hallmark of the excitonic insulator phase; and we observe the recently predicted giant exciton-driven Floquet effects. Our work realizes the promise that intense bosonic fields, other than photons, can also drive novel quantum phenomena and phases in materials.","sentences":["Inducing novel quantum phases and topologies in materials using intense light fields is a key objective of modern condensed matter physics, but nonetheless faces significant experimental challenges.","Alternately, theory predicts that in the dense limit, excitons - collective excitations composed of Coulomb-bound electron-hole pairs - could also drive exotic quantum phenomena.","However, the direct observation of these phenomena requires the resolution of electronic structure in momentum space in the presence of excitons, which became possible only recently.","Here, using time- and angle-resolved photoemission spectroscopy of an atomically thin semiconductor in the presence of a high-density of resonantly and coherently photoexcited excitons, we observe the Bardeen-Cooper-Schrieffer (BCS) excitonic state - analogous to the Cooper pairs of superconductivity.","We see the valence band transform from a conventional paraboloid into a Mexican-hat like Bogoliubov dispersion - a hallmark of the excitonic insulator phase; and we observe the recently predicted giant exciton-driven Floquet effects.","Our work realizes the promise that intense bosonic fields, other than photons, can also drive novel quantum phenomena and phases in materials."],"url":"http://arxiv.org/abs/2403.08725v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-13 17:15:06","title":"Boundary geometry controls topological defect transitions that determine lumen nucleation in embryonic development","abstract":"Topological defects determine the collective properties of anisotropic materials. How their configurations are controlled is not well understood however, especially in 3D, where bulk-surface coupling can render the geometry of confining boundaries relevant. This is particularly important in living matter, where 2D topological defects have been linked to essential biological functions, whereas the role of 3D defects is unclear. Motivated by multicellular systems interacting with extracellular boundaries, we consider a polar fluid confined within curved boundaries imposing weak surface anchoring. We report a novel charge-preserving transition between different defect configurations, controlled by the boundary shape, and invariant to changes in the material parameters. We test if this geometry-driven transition occurs in confined multicellular systems and investigate the biological role of 3D polar defects in the mouse epiblast -- an embryonic tissue consisting of apico-basally polarised cells. We find that fluid-filled lumina -- structures essential for subsequent embryonic development -- tend to form near defect positions of polar fluids in embryo-like confinement geometries. Moreover, by experimentally perturbing embryo shape beyond the transition point, we trigger the formation of additional lumen nucleation sites at the predicted position. Thus, our work reveals how boundary geometry controls polar defects, and how embryos use this mechanism for shape-dependent lumen formation. Because this defect control principle is independent of specific material properties, we expect it to apply universally to systems with orientational order.","sentences":["Topological defects determine the collective properties of anisotropic materials.","How their configurations are controlled is not well understood however, especially in 3D, where bulk-surface coupling can render the geometry of confining boundaries relevant.","This is particularly important in living matter, where 2D topological defects have been linked to essential biological functions, whereas the role of 3D defects is unclear.","Motivated by multicellular systems interacting with extracellular boundaries, we consider a polar fluid confined within curved boundaries imposing weak surface anchoring.","We report a novel charge-preserving transition between different defect configurations, controlled by the boundary shape, and invariant to changes in the material parameters.","We test if this geometry-driven transition occurs in confined multicellular systems and investigate the biological role of 3D polar defects in the mouse epiblast -- an embryonic tissue consisting of apico-basally polarised cells.","We find that fluid-filled lumina -- structures essential for subsequent embryonic development -- tend to form near defect positions of polar fluids in embryo-like confinement geometries.","Moreover, by experimentally perturbing embryo shape beyond the transition point, we trigger the formation of additional lumen nucleation sites at the predicted position.","Thus, our work reveals how boundary geometry controls polar defects, and how embryos use this mechanism for shape-dependent lumen formation.","Because this defect control principle is independent of specific material properties, we expect it to apply universally to systems with orientational order."],"url":"http://arxiv.org/abs/2403.08710v1","category":"cond-mat.soft"}
{"created":"2024-03-13 16:11:25","title":"A Distributed Adaptive Algorithm for Non-Smooth Spatial Filtering Problems in Wireless Sensor Networks","abstract":"A wireless sensor network often relies on a fusion center to process the data collected by each of its sensing nodes. Such an approach relies on the continuous transmission of raw data to the fusion center, which typically has a major impact on the sensors' battery life. To address this issue in the particular context of spatial filtering and signal fusion problems, we recently proposed the Distributed Adaptive Signal Fusion (DASF) algorithm, which distributively computes a spatial filter expressed as the solution of a smooth optimization problem involving the network-wide sensor signal statistics. In this work, we show that the DASF algorithm can be extended to compute the filters associated with a certain class of non-smooth optimization problems. This extension makes the addition of sparsity-inducing norms to the problem's cost function possible, allowing sensor selection to be performed in a distributed fashion, alongside the filtering task of interest, thereby further reducing the network's energy consumption. We provide a description of the algorithm, prove its convergence, and validate its performance and solution tracking capabilities with numerical experiments.","sentences":["A wireless sensor network often relies on a fusion center to process the data collected by each of its sensing nodes.","Such an approach relies on the continuous transmission of raw data to the fusion center, which typically has a major impact on the sensors' battery life.","To address this issue in the particular context of spatial filtering and signal fusion problems, we recently proposed the Distributed Adaptive Signal Fusion (DASF) algorithm, which distributively computes a spatial filter expressed as the solution of a smooth optimization problem involving the network-wide sensor signal statistics.","In this work, we show that the DASF algorithm can be extended to compute the filters associated with a certain class of non-smooth optimization problems.","This extension makes the addition of sparsity-inducing norms to the problem's cost function possible, allowing sensor selection to be performed in a distributed fashion, alongside the filtering task of interest, thereby further reducing the network's energy consumption.","We provide a description of the algorithm, prove its convergence, and validate its performance and solution tracking capabilities with numerical experiments."],"url":"http://arxiv.org/abs/2403.08658v1","category":"eess.SP"}
{"created":"2024-03-13 15:32:26","title":"Measurements of the charge ratio and polarization of cosmic-ray muons with the Super-Kamiokande detector","abstract":"We present the results of the charge ratio ($R$) and polarization ($P^{\\mu}_{0}$) measurements using the decay electron events collected from 2008 September to 2022 June by the Super-Kamiokande detector. Because of its underground location and long operation, we performed high precision measurements by accumulating cosmic-ray muons. We measured the muon charge ratio to be $R=1.32 \\pm 0.02$ $(\\mathrm{stat.}{+}\\mathrm{syst.})$ at $E_{\\mu}\\cos \\theta_{\\mathrm{Zenith}}=0.7^{+0.3}_{-0.2}$ $\\mathrm{TeV}$, where $E_{\\mu}$ is the muon energy and $\\theta_{\\mathrm{Zenith}}$ is the zenith angle of incoming cosmic-ray muons. This result is consistent with the Honda flux model while this suggests a tension with the $\\pi K$ model of $1.9\\sigma$. We also measured the muon polarization at the production location to be $P^{\\mu}_{0}=0.52 \\pm 0.02$ $(\\mathrm{stat.}{+}\\mathrm{syst.})$ at the muon momentum of $0.9^{+0.6}_{-0.1}$ $\\mathrm{TeV}/c$ at the surface of the mountain; this also suggests a tension with the Honda flux model of $1.5\\sigma$. This is the most precise measurement ever to experimentally determine the cosmic-ray muon polarization near $1~\\mathrm{TeV}/c$. These measurement results are useful to improve the atmospheric neutrino simulations.","sentences":["We present the results of the charge ratio ($R$) and polarization ($P^{\\mu}_{0}$) measurements using the decay electron events collected from 2008 September to 2022 June by the Super-Kamiokande detector.","Because of its underground location and long operation, we performed high precision measurements by accumulating cosmic-ray muons.","We measured the muon charge ratio to be $R=1.32 \\pm 0.02$ $(\\mathrm{stat.}{+}\\mathrm{syst.})$ at $E_{\\mu}\\cos \\theta_{\\mathrm{Zenith}}=0.7^{+0.3}_{-0.2}$ $\\mathrm{TeV}$, where $E_{\\mu}$ is the muon energy and $\\theta_{\\mathrm{Zenith}}$ is the zenith angle of incoming cosmic-ray muons.","This result is consistent with the Honda flux model while this suggests a tension with the $\\pi K$ model of $1.9\\sigma$. We also measured the muon polarization at the production location to be $P^{\\mu}_{0}=0.52 \\pm 0.02$ $(\\mathrm{stat.}{+}\\mathrm{syst.})$ at the muon momentum of $0.9^{+0.6}_{-0.1}$ $\\mathrm{TeV}/c$ at the surface of the mountain; this also suggests a tension with the Honda flux model of $1.5\\sigma$. This is the most precise measurement ever to experimentally determine the cosmic-ray muon polarization near $1~\\mathrm{TeV}/c$. These measurement results are useful to improve the atmospheric neutrino simulations."],"url":"http://arxiv.org/abs/2403.08619v1","category":"hep-ex"}
{"created":"2024-03-13 15:10:06","title":"Exploring global symmetry-breaking superradiant phase via phase competition","abstract":"Superradiant phase transitions play a fundamental role in understanding the mechanism of collective light-matter interaction at the quantum level. Here we investigate multiple superradiant phases and phase transitions with different symmetry-breaking patterns in a two-mode V-type Dicke model. Interestingly, we show that there exists a quadruple point where one normal phase, one global symmetry-breaking superradiant phase and two local symmetry-breaking superradiant phases meet. Such a global phase results from the phase competition between two local superradiant phases and can not occur in the standard $\\Lambda$- and $\\Xi$-type three-level configurations in quantum optics. Moreover, we exhibit a sequential first-order quantum phase transition from one local to the global again to the other local superradiant phase. Our study opens up a perspective of exploring multi-level quantum critical phenomena with global symmetry breaking.","sentences":["Superradiant phase transitions play a fundamental role in understanding the mechanism of collective light-matter interaction at the quantum level.","Here we investigate multiple superradiant phases and phase transitions with different symmetry-breaking patterns in a two-mode V-type Dicke model.","Interestingly, we show that there exists a quadruple point where one normal phase, one global symmetry-breaking superradiant phase and two local symmetry-breaking superradiant phases meet.","Such a global phase results from the phase competition between two local superradiant phases and can not occur in the standard $\\Lambda$- and $\\Xi$-type three-level configurations in quantum optics.","Moreover, we exhibit a sequential first-order quantum phase transition from one local to the global again to the other local superradiant phase.","Our study opens up a perspective of exploring multi-level quantum critical phenomena with global symmetry breaking."],"url":"http://arxiv.org/abs/2403.08602v1","category":"quant-ph"}
{"created":"2024-03-13 14:52:09","title":"Long-term monitoring of large-scale magnetic fields across optical and near-infrared domains with ESPaDOnS, Narval and SPIRou. The cases of EV Lac, DS Leo, and CN Leo","abstract":"Dynamo models of stellar magnetic fields for partly and fully convective stars are guided by observational constraints. Zeeman-Doppler imaging has revealed a variety of magnetic field geometries and, for fully convective stars in particular, a dichotomy: either strong, mostly axisymmetric, and dipole-dominated or weak, non-axisymmetric, and multipole-dominated. This dichotomy is explained by dynamo bistability or by long-term magnetic cycles, but there is no definite conclusion on the matter. We analysed optical spectropolarimetric data sets collected with ESPaDOnS and Narval between 2005 and 2016, and near-infrared SPIRou data obtained between 2019 and 2022 for three active M dwarfs with masses between 0.1 and 0.6 MSun: EV Lac, DS Leo, and CN Leo. We looked for changes in time series of longitudinal magnetic field, width of unpolarised mean-line profiles, and large-scale field topology as retrieved with principal component analysis and Zeeman-Doppler imaging. We retrieved pulsating (EV Lac), stable (DS Leo), and sine-like (CN Leo) long-term trends in longitudinal field. The width of near-infrared mean-line profiles exhibits rotational modulation only for DS Leo, whereas in the optical it is evident for both EV Lac and DS Leo. The line width variations are not necessarily correlated to those of the longitudinal field, suggesting complex relations between small- and large-scale field. We also recorded topological changes: a reduced axisymmetry for EV Lac and a transition from toroidal- to poloidal-dominated regime for DS Leo. For CN Leo, the topology remained dipolar and axisymmetric, with only an oscillation in field strength. Our results show a peculiar evolution of the magnetic field for each M dwarf, confirming that M dwarfs with distinct masses and rotation periods can undergo magnetic long-term variations, and suggesting a variety of cyclic behaviours of their magnetic fields.","sentences":["Dynamo models of stellar magnetic fields for partly and fully convective stars are guided by observational constraints.","Zeeman-Doppler imaging has revealed a variety of magnetic field geometries and, for fully convective stars in particular, a dichotomy: either strong, mostly axisymmetric, and dipole-dominated or weak, non-axisymmetric, and multipole-dominated.","This dichotomy is explained by dynamo bistability or by long-term magnetic cycles, but there is no definite conclusion on the matter.","We analysed optical spectropolarimetric data sets collected with ESPaDOnS and Narval between 2005 and 2016, and near-infrared SPIRou data obtained between 2019 and 2022 for three active M dwarfs with masses between 0.1 and 0.6 MSun:","EV Lac, DS Leo, and CN Leo.","We looked for changes in time series of longitudinal magnetic field, width of unpolarised mean-line profiles, and large-scale field topology as retrieved with principal component analysis and Zeeman-Doppler imaging.","We retrieved pulsating (EV Lac), stable (DS Leo), and sine-like (CN Leo) long-term trends in longitudinal field.","The width of near-infrared mean-line profiles exhibits rotational modulation only for DS Leo, whereas in the optical it is evident for both EV Lac and DS Leo.","The line width variations are not necessarily correlated to those of the longitudinal field, suggesting complex relations between small- and large-scale field.","We also recorded topological changes: a reduced axisymmetry for EV Lac and a transition from toroidal- to poloidal-dominated regime for DS Leo.","For CN Leo, the topology remained dipolar and axisymmetric, with only an oscillation in field strength.","Our results show a peculiar evolution of the magnetic field for each M dwarf, confirming that M dwarfs with distinct masses and rotation periods can undergo magnetic long-term variations, and suggesting a variety of cyclic behaviours of their magnetic fields."],"url":"http://arxiv.org/abs/2403.08590v1","category":"astro-ph.SR"}
{"created":"2024-03-13 14:10:10","title":"End-to-End Amp Modeling: From Data to Controllable Guitar Amplifier Models","abstract":"This paper describes a data-driven approach to creating real-time neural network models of guitar amplifiers, recreating the amplifiers' sonic response to arbitrary inputs at the full range of controls present on the physical device. While the focus on the paper is on the data collection pipeline, we demonstrate the effectiveness of this conditioned black-box approach by training an LSTM model to the task, and comparing its performance to an offline white-box SPICE circuit simulation. Our listening test results demonstrate that the neural amplifier modeling approach can match the subjective performance of a high-quality SPICE model, all while using an automated, non-intrusive data collection process, and an end-to-end trainable, real-time feasible neural network model.","sentences":["This paper describes a data-driven approach to creating real-time neural network models of guitar amplifiers, recreating the amplifiers' sonic response to arbitrary inputs at the full range of controls present on the physical device.","While the focus on the paper is on the data collection pipeline, we demonstrate the effectiveness of this conditioned black-box approach by training an LSTM model to the task, and comparing its performance to an offline white-box SPICE circuit simulation.","Our listening test results demonstrate that the neural amplifier modeling approach can match the subjective performance of a high-quality SPICE model, all while using an automated, non-intrusive data collection process, and an end-to-end trainable, real-time feasible neural network model."],"url":"http://arxiv.org/abs/2403.08559v1","category":"cs.SD"}
{"created":"2024-03-13 13:06:31","title":"Gaussian Splatting in Style","abstract":"Scene stylization extends the work of neural style transfer to three spatial dimensions. A vital challenge in this problem is to maintain the uniformity of the stylized appearance across a multi-view setting. A vast majority of the previous works achieve this by optimizing the scene with a specific style image. In contrast, we propose a novel architecture trained on a collection of style images, that at test time produces high quality stylized novel views. Our work builds up on the framework of 3D Gaussian splatting. For a given scene, we take the pretrained Gaussians and process them using a multi resolution hash grid and a tiny MLP to obtain the conditional stylised views. The explicit nature of 3D Gaussians give us inherent advantages over NeRF-based methods including geometric consistency, along with having a fast training and rendering regime. This enables our method to be useful for vast practical use cases such as in augmented or virtual reality applications. Through our experiments, we show our methods achieve state-of-the-art performance with superior visual quality on various indoor and outdoor real-world data.","sentences":["Scene stylization extends the work of neural style transfer to three spatial dimensions.","A vital challenge in this problem is to maintain the uniformity of the stylized appearance across a multi-view setting.","A vast majority of the previous works achieve this by optimizing the scene with a specific style image.","In contrast, we propose a novel architecture trained on a collection of style images, that at test time produces high quality stylized novel views.","Our work builds up on the framework of 3D Gaussian splatting.","For a given scene, we take the pretrained Gaussians and process them using a multi resolution hash grid and a tiny MLP to obtain the conditional stylised views.","The explicit nature of 3D Gaussians give us inherent advantages over NeRF-based methods including geometric consistency, along with having a fast training and rendering regime.","This enables our method to be useful for vast practical use cases such as in augmented or virtual reality applications.","Through our experiments, we show our methods achieve state-of-the-art performance with superior visual quality on various indoor and outdoor real-world data."],"url":"http://arxiv.org/abs/2403.08498v1","category":"cs.CV"}
{"created":"2024-03-13 08:38:21","title":"DONAPI: Malicious NPM Packages Detector using Behavior Sequence Knowledge Mapping","abstract":"With the growing popularity of modularity in software development comes the rise of package managers and language ecosystems. Among them, npm stands out as the most extensive package manager, hosting more than 2 million third-party open-source packages that greatly simplify the process of building code. However, this openness also brings security risks, as evidenced by numerous package poisoning incidents.   In this paper, we synchronize a local package cache containing more than 3.4 million packages in near real-time to give us access to more package code details. Further, we perform manual inspection and API call sequence analysis on packages collected from public datasets and security reports to build a hierarchical classification framework and behavioral knowledge base covering different sensitive behaviors. In addition, we propose the DONAPI, an automatic malicious npm packages detector that combines static and dynamic analysis. It makes preliminary judgments on the degree of maliciousness of packages by code reconstruction techniques and static analysis, extracts dynamic API call sequences to confirm and identify obfuscated content that static analysis can not handle alone, and finally tags malicious software packages based on the constructed behavior knowledge base. To date, we have identified and manually confirmed 325 malicious samples and discovered 2 unusual API calls and 246 API call sequences that have not appeared in known samples.","sentences":["With the growing popularity of modularity in software development comes the rise of package managers and language ecosystems.","Among them, npm stands out as the most extensive package manager, hosting more than 2 million third-party open-source packages that greatly simplify the process of building code.","However, this openness also brings security risks, as evidenced by numerous package poisoning incidents.   ","In this paper, we synchronize a local package cache containing more than 3.4 million packages in near real-time to give us access to more package code details.","Further, we perform manual inspection and API call sequence analysis on packages collected from public datasets and security reports to build a hierarchical classification framework and behavioral knowledge base covering different sensitive behaviors.","In addition, we propose the DONAPI, an automatic malicious npm packages detector that combines static and dynamic analysis.","It makes preliminary judgments on the degree of maliciousness of packages by code reconstruction techniques and static analysis, extracts dynamic API call sequences to confirm and identify obfuscated content that static analysis can not handle alone, and finally tags malicious software packages based on the constructed behavior knowledge base.","To date, we have identified and manually confirmed 325 malicious samples and discovered 2 unusual API calls and 246 API call sequences that have not appeared in known samples."],"url":"http://arxiv.org/abs/2403.08334v1","category":"cs.CR"}
{"created":"2024-03-13 07:38:20","title":"HRLAIF: Improvements in Helpfulness and Harmlessness in Open-domain Reinforcement Learning From AI Feedback","abstract":"Reinforcement Learning from AI Feedback (RLAIF) has the advantages of shorter annotation cycles and lower costs over Reinforcement Learning from Human Feedback (RLHF), making it highly efficient during the rapid strategy iteration periods of large language model (LLM) training. Using ChatGPT as a labeler to provide feedback on open-domain prompts in RLAIF training, we observe an increase in human evaluators' preference win ratio for model responses, but a decrease in evaluators' satisfaction rate. Analysis suggests that the decrease in satisfaction rate is mainly due to some responses becoming less helpful, particularly in terms of correctness and truthfulness, highlighting practical limitations of basic RLAIF. In this paper, we propose Hybrid Reinforcement Learning from AI Feedback (HRLAIF). This method enhances the accuracy of AI annotations for responses, making the model's helpfulness more robust in training process. Additionally, it employs AI for Red Teaming, further improving the model's harmlessness. Human evaluation results show that HRLAIF inherits the ability of RLAIF to enhance human preference for outcomes at a low cost while also improving the satisfaction rate of responses. Compared to the policy model before Reinforcement Learning (RL), it achieves an increase of 2.08\\% in satisfaction rate, effectively addressing the issue of a decrease of 4.58\\% in satisfaction rate after basic RLAIF.","sentences":["Reinforcement Learning from AI Feedback (RLAIF) has the advantages of shorter annotation cycles and lower costs over Reinforcement Learning from Human Feedback (RLHF), making it highly efficient during the rapid strategy iteration periods of large language model (LLM) training.","Using ChatGPT as a labeler to provide feedback on open-domain prompts in RLAIF training, we observe an increase in human evaluators' preference win ratio for model responses, but a decrease in evaluators' satisfaction rate.","Analysis suggests that the decrease in satisfaction rate is mainly due to some responses becoming less helpful, particularly in terms of correctness and truthfulness, highlighting practical limitations of basic RLAIF.","In this paper, we propose Hybrid Reinforcement Learning from AI Feedback (HRLAIF).","This method enhances the accuracy of AI annotations for responses, making the model's helpfulness more robust in training process.","Additionally, it employs AI for Red Teaming, further improving the model's harmlessness.","Human evaluation results show that HRLAIF inherits the ability of RLAIF to enhance human preference for outcomes at a low cost while also improving the satisfaction rate of responses.","Compared to the policy model before Reinforcement Learning (RL), it achieves an increase of 2.08\\% in satisfaction rate, effectively addressing the issue of a decrease of 4.58\\% in satisfaction rate after basic RLAIF."],"url":"http://arxiv.org/abs/2403.08309v1","category":"cs.LG"}
{"created":"2024-03-13 07:12:03","title":"AutoDev: Automated AI-Driven Development","abstract":"The landscape of software development has witnessed a paradigm shift with the advent of AI-powered assistants, exemplified by GitHub Copilot. However, existing solutions are not leveraging all the potential capabilities available in an IDE such as building, testing, executing code, git operations, etc. Therefore, they are constrained by their limited capabilities, primarily focusing on suggesting code snippets and file manipulation within a chat-based interface. To fill this gap, we present AutoDev, a fully automated AI-driven software development framework, designed for autonomous planning and execution of intricate software engineering tasks. AutoDev enables users to define complex software engineering objectives, which are assigned to AutoDev's autonomous AI Agents to achieve. These AI agents can perform diverse operations on a codebase, including file editing, retrieval, build processes, execution, testing, and git operations. They also have access to files, compiler output, build and testing logs, static analysis tools, and more. This enables the AI Agents to execute tasks in a fully automated manner with a comprehensive understanding of the contextual information required. Furthermore, AutoDev establishes a secure development environment by confining all operations within Docker containers. This framework incorporates guardrails to ensure user privacy and file security, allowing users to define specific permitted or restricted commands and operations within AutoDev. In our evaluation, we tested AutoDev on the HumanEval dataset, obtaining promising results with 91.5% and 87.8% of Pass@1 for code generation and test generation respectively, demonstrating its effectiveness in automating software engineering tasks while maintaining a secure and user-controlled development environment.","sentences":["The landscape of software development has witnessed a paradigm shift with the advent of AI-powered assistants, exemplified by GitHub Copilot.","However, existing solutions are not leveraging all the potential capabilities available in an IDE such as building, testing, executing code, git operations, etc.","Therefore, they are constrained by their limited capabilities, primarily focusing on suggesting code snippets and file manipulation within a chat-based interface.","To fill this gap, we present AutoDev, a fully automated AI-driven software development framework, designed for autonomous planning and execution of intricate software engineering tasks.","AutoDev enables users to define complex software engineering objectives, which are assigned to AutoDev's autonomous AI Agents to achieve.","These AI agents can perform diverse operations on a codebase, including file editing, retrieval, build processes, execution, testing, and git operations.","They also have access to files, compiler output, build and testing logs, static analysis tools, and more.","This enables the AI Agents to execute tasks in a fully automated manner with a comprehensive understanding of the contextual information required.","Furthermore, AutoDev establishes a secure development environment by confining all operations within Docker containers.","This framework incorporates guardrails to ensure user privacy and file security, allowing users to define specific permitted or restricted commands and operations within AutoDev.","In our evaluation, we tested AutoDev on the HumanEval dataset, obtaining promising results with 91.5% and 87.8% of Pass@1 for code generation and test generation respectively, demonstrating its effectiveness in automating software engineering tasks while maintaining a secure and user-controlled development environment."],"url":"http://arxiv.org/abs/2403.08299v1","category":"cs.SE"}
{"created":"2024-03-13 06:59:16","title":"Gemma: Open Models Based on Gemini Research and Technology","abstract":"This work introduces Gemma, a family of lightweight, state-of-the art open models built from the research and technology used to create Gemini models. Gemma models demonstrate strong performance across academic benchmarks for language understanding, reasoning, and safety. We release two sizes of models (2 billion and 7 billion parameters), and provide both pretrained and fine-tuned checkpoints. Gemma outperforms similarly sized open models on 11 out of 18 text-based tasks, and we present comprehensive evaluations of safety and responsibility aspects of the models, alongside a detailed description of model development. We believe the responsible release of LLMs is critical for improving the safety of frontier models, and for enabling the next wave of LLM innovations.","sentences":["This work introduces Gemma, a family of lightweight, state-of-the art open models built from the research and technology used to create Gemini models.","Gemma models demonstrate strong performance across academic benchmarks for language understanding, reasoning, and safety.","We release two sizes of models (2 billion and 7 billion parameters), and provide both pretrained and fine-tuned checkpoints.","Gemma outperforms similarly sized open models on 11 out of 18 text-based tasks, and we present comprehensive evaluations of safety and responsibility aspects of the models, alongside a detailed description of model development.","We believe the responsible release of LLMs is critical for improving the safety of frontier models, and for enabling the next wave of LLM innovations."],"url":"http://arxiv.org/abs/2403.08295v1","category":"cs.CL"}
{"created":"2024-03-13 06:54:47","title":"Generative Pretrained Structured Transformers: Unsupervised Syntactic Language Models at Scale","abstract":"A syntactic language model (SLM) incrementally generates a sentence with its syntactic tree in a left-to-right manner. We present Generative Pretrained Structured Transformers (GPST), an unsupervised SLM at scale capable of being pre-trained from scratch on raw texts with high parallelism. GPST circumvents the limitations of previous SLMs such as relying on gold trees and sequential training. It consists of two components, a usual SLM supervised by a uni-directional language modeling loss, and an additional composition model, which induces syntactic parse trees and computes constituent representations, supervised by a bi-directional language modeling loss. We propose a representation surrogate to enable joint parallel training of the two models in a hard-EM fashion. We pre-train GPST on OpenWebText, a corpus with $9$ billion tokens, and demonstrate the superiority of GPST over GPT-2 with a comparable size in numerous tasks covering both language understanding and language generation. Meanwhile, GPST also significantly outperforms existing unsupervised SLMs on left-to-right grammar induction, while holding a substantial acceleration on training.","sentences":["A syntactic language model (SLM) incrementally generates a sentence with its syntactic tree in a left-to-right manner.","We present Generative Pretrained Structured Transformers (GPST), an unsupervised SLM at scale capable of being pre-trained from scratch on raw texts with high parallelism.","GPST circumvents the limitations of previous SLMs such as relying on gold trees and sequential training.","It consists of two components, a usual SLM supervised by a uni-directional language modeling loss, and an additional composition model, which induces syntactic parse trees and computes constituent representations, supervised by a bi-directional language modeling loss.","We propose a representation surrogate to enable joint parallel training of the two models in a hard-EM fashion.","We pre-train GPST on OpenWebText, a corpus with $9$ billion tokens, and demonstrate the superiority of GPST over GPT-2 with a comparable size in numerous tasks covering both language understanding and language generation.","Meanwhile, GPST also significantly outperforms existing unsupervised SLMs on left-to-right grammar induction, while holding a substantial acceleration on training."],"url":"http://arxiv.org/abs/2403.08293v1","category":"cs.CL"}
{"created":"2024-03-13 06:54:38","title":"Weak Collocation Regression for Inferring Stochastic Dynamics with L\u00e9vy Noise","abstract":"With the rapid increase of observational, experimental and simulated data for stochastic systems, tremendous efforts have been devoted to identifying governing laws underlying the evolution of these systems. Despite the broad applications of non-Gaussian fluctuations in numerous physical phenomena, the data-driven approaches to extracting stochastic dynamics with L\\'{e}vy noise are relatively few. In this work, we propose a Weak Collocation Regression (WCR) to explicitly reveal unknown stochastic dynamical systems, i.e., the Stochastic Differential Equation (SDE) with both $\\alpha$-stable L\\'{e}vy noise and Gaussian noise, from discrete aggregate data. This method utilizes the evolution equation of the probability distribution function, i.e., the Fokker-Planck (FP) equation. With the weak form of the FP equation, the WCR constructs a linear system of unknown parameters where all integrals are evaluated by Monte Carlo method with the observations. Then, the unknown parameters are obtained by a sparse linear regression. For a SDE with L\\'{e}vy noise, the corresponding FP equation is a partial integro-differential equation (PIDE), which contains nonlocal terms, and is difficult to deal with. The weak form can avoid complicated multiple integrals. Our approach can simultaneously distinguish mixed noise types, even in multi-dimensional problems. Numerical experiments demonstrate that our method is accurate and computationally efficient.","sentences":["With the rapid increase of observational, experimental and simulated data for stochastic systems, tremendous efforts have been devoted to identifying governing laws underlying the evolution of these systems.","Despite the broad applications of non-Gaussian fluctuations in numerous physical phenomena, the data-driven approaches to extracting stochastic dynamics with L\\'{e}vy noise are relatively few.","In this work, we propose a Weak Collocation Regression (WCR) to explicitly reveal unknown stochastic dynamical systems, i.e., the Stochastic Differential Equation (SDE) with both $\\alpha$-stable L\\'{e}vy noise and Gaussian noise, from discrete aggregate data.","This method utilizes the evolution equation of the probability distribution function, i.e., the Fokker-Planck (FP) equation.","With the weak form of the FP equation, the WCR constructs a linear system of unknown parameters where all integrals are evaluated by Monte Carlo method with the observations.","Then, the unknown parameters are obtained by a sparse linear regression.","For a SDE with L\\'{e}vy noise, the corresponding FP equation is a partial integro-differential equation (PIDE), which contains nonlocal terms, and is difficult to deal with.","The weak form can avoid complicated multiple integrals.","Our approach can simultaneously distinguish mixed noise types, even in multi-dimensional problems.","Numerical experiments demonstrate that our method is accurate and computationally efficient."],"url":"http://arxiv.org/abs/2403.08292v1","category":"math.NA"}
{"created":"2024-03-13 06:54:15","title":"CleanAgent: Automating Data Standardization with LLM-based Agents","abstract":"Data standardization is a crucial part in data science life cycle. While tools like Pandas offer robust functionalities, their complexity and the manual effort required for customizing code to diverse column types pose significant challenges. Although large language models (LLMs) like ChatGPT have shown promise in automating this process through natural language understanding and code generation, it still demands expert-level programming knowledge and continuous interaction for prompt refinement. To solve these challenges, our key idea is to propose a Python library with declarative, unified APIs for standardizing column types, simplifying the code generation of LLM with concise API calls. We first propose Dataprep.Clean which is written as a component of the Dataprep Library, offers a significant reduction in complexity by enabling the standardization of specific column types with a single line of code. Then we introduce the CleanAgent framework integrating Dataprep.Clean and LLM-based agents to automate the data standardization process. With CleanAgent, data scientists need only provide their requirements once, allowing for a hands-free, automatic standardization process.","sentences":["Data standardization is a crucial part in data science life cycle.","While tools like Pandas offer robust functionalities, their complexity and the manual effort required for customizing code to diverse column types pose significant challenges.","Although large language models (LLMs) like ChatGPT have shown promise in automating this process through natural language understanding and code generation, it still demands expert-level programming knowledge and continuous interaction for prompt refinement.","To solve these challenges, our key idea is to propose a Python library with declarative, unified APIs for standardizing column types, simplifying the code generation of LLM with concise API calls.","We first propose Dataprep.","Clean which is written as a component of the Dataprep Library, offers a significant reduction in complexity by enabling the standardization of specific column types with a single line of code.","Then we introduce the CleanAgent framework integrating Dataprep.","Clean and LLM-based agents to automate the data standardization process.","With CleanAgent, data scientists need only provide their requirements once, allowing for a hands-free, automatic standardization process."],"url":"http://arxiv.org/abs/2403.08291v1","category":"cs.LG"}
{"created":"2024-03-13 06:44:30","title":"Optical-Cavity Manipulation Strategies of Conical Intersections Mediated Singlet Fission Systems","abstract":"We offer a theoretical perspective on simulation and engineering of polaritonic conical-intersection-driven singlet-fission (SF) materials. Using rubrene as an example and applying the numerically accurate Davydov-Ansatz methodology, we derive dynamic and spectroscopic responses of the system and demonstrate key mechanisms capable of SF manipulation, viz. cavity-induced enhancement/weakening/suppression of SF, population localization on the singlet state via engineering of the cavity-mode excitation, polaron/polariton decoupling, collective enhancement of SF. We outline unsolved problems and challenges in the field, and share our views on the development of the future lines of research. We emphasize the significance of careful modeling of cascades of polaritonic conical intersections in high excitation manifolds and envisage that collective geometric phase effects may remarkably affect the SF dynamics and yield. We argue that microscopic interpretation of the main regulatory mechanisms of the polaritonic conical-intersection-driven SF can substantially deepen our understanding of this process, thereby providing novel ideas and solutions for improving conversion efficiency in photovoltaics.","sentences":["We offer a theoretical perspective on simulation and engineering of polaritonic conical-intersection-driven singlet-fission (SF) materials.","Using rubrene as an example and applying the numerically accurate Davydov-Ansatz methodology, we derive dynamic and spectroscopic responses of the system and demonstrate key mechanisms capable of SF manipulation, viz.","cavity-induced enhancement/weakening/suppression of SF, population localization on the singlet state via engineering of the cavity-mode excitation, polaron/polariton decoupling, collective enhancement of SF.","We outline unsolved problems and challenges in the field, and share our views on the development of the future lines of research.","We emphasize the significance of careful modeling of cascades of polaritonic conical intersections in high excitation manifolds and envisage that collective geometric phase effects may remarkably affect the SF dynamics and yield.","We argue that microscopic interpretation of the main regulatory mechanisms of the polaritonic conical-intersection-driven SF can substantially deepen our understanding of this process, thereby providing novel ideas and solutions for improving conversion efficiency in photovoltaics."],"url":"http://arxiv.org/abs/2403.08286v1","category":"quant-ph"}
{"created":"2024-03-13 06:40:29","title":"Experimental observation of gapped shear waves and liquid-like to gas-like dynamical crossover in active granular matter","abstract":"Unlike crystalline solids, liquids do not exhibit long-range order. Their atoms undergo frequent structural rearrangements, resulting in the long-wavelength dynamics of shear fluctuations in liquids being diffusive, rather than propagating waves, as observed in crystals. When considering shorter time and length scales, molecular dynamics simulations and theoretical propositions suggest that collective shear excitations in liquids display a gap in wave-vector space, referred to as the $k$-gap. Above this gap, solid-like transverse waves re-emerge. However, direct experimental verification of this phenomenon in classical liquids remains elusive, with the only documented evidence from studies in two-dimensional dusty plasmas. Active granular systems provide a novel platform for exploring the emergence of collective dynamics and showcasing a rich interplay of complex phases and phenomena. Our study focuses on bi-disperse active Brownian vibrators. Through measurements of the pair correlation functions, mean square displacements, velocity auto-correlation functions, vibrational density of states, and a detailed analysis of microscopic atomic motion, we demonstrate that this active system exhibits both gas-like and liquid-like phases, depending on the packing fraction, despite pure hard-disk-like repulsive interactions. Notably, within the granular liquid-like phase, we experimentally validate the existence of a $k$-gap in the dispersion of transverse excitations. This gap becomes more significant with a decrease in packing fraction and disappears into the gas phase, aligning with theoretical expectations. Our results offer a direct experimental confirmation of the $k$-gap phenomenon, extending its relevance beyond classical thermal liquids to active granular systems, and reveal the existence of intriguing similarities between the physics of active granular matter and supercritical fluids.","sentences":["Unlike crystalline solids, liquids do not exhibit long-range order.","Their atoms undergo frequent structural rearrangements, resulting in the long-wavelength dynamics of shear fluctuations in liquids being diffusive, rather than propagating waves, as observed in crystals.","When considering shorter time and length scales, molecular dynamics simulations and theoretical propositions suggest that collective shear excitations in liquids display a gap in wave-vector space, referred to as the $k$-gap.","Above this gap, solid-like transverse waves re-emerge.","However, direct experimental verification of this phenomenon in classical liquids remains elusive, with the only documented evidence from studies in two-dimensional dusty plasmas.","Active granular systems provide a novel platform for exploring the emergence of collective dynamics and showcasing a rich interplay of complex phases and phenomena.","Our study focuses on bi-disperse active Brownian vibrators.","Through measurements of the pair correlation functions, mean square displacements, velocity auto-correlation functions, vibrational density of states, and a detailed analysis of microscopic atomic motion, we demonstrate that this active system exhibits both gas-like and liquid-like phases, depending on the packing fraction, despite pure hard-disk-like repulsive interactions.","Notably, within the granular liquid-like phase, we experimentally validate the existence of a $k$-gap in the dispersion of transverse excitations.","This gap becomes more significant with a decrease in packing fraction and disappears into the gas phase, aligning with theoretical expectations.","Our results offer a direct experimental confirmation of the $k$-gap phenomenon, extending its relevance beyond classical thermal liquids to active granular systems, and reveal the existence of intriguing similarities between the physics of active granular matter and supercritical fluids."],"url":"http://arxiv.org/abs/2403.08285v1","category":"cond-mat.soft"}
{"created":"2024-03-13 06:18:48","title":"Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models","abstract":"Underlying data distributions of natural language, programming code, and mathematical symbols vary vastly, presenting a complex challenge for large language models (LLMs) that strive to achieve high performance across all three domains simultaneously. Achieving a very high level of proficiency for an LLM within a specific domain often requires extensive training with relevant corpora, which is typically accompanied by a sacrifice in performance in other domains. In this paper, we propose to fuse models that are already highly-specialized directly. The proposed fusing framework, UltraFuser, consists of three distinct specialists that are already sufficiently trained on language, coding, and mathematics. A token-level gating mechanism is introduced to blend the specialists' outputs. A two-stage training strategy accompanied by balanced sampling is designed to ensure stability. To effectively train the fused model, we further construct a high-quality supervised instruction tuning dataset, UltraChat 2, which includes text, code, and mathematical content. This dataset comprises approximately 300,000 instructions and covers a wide range of topics in each domain. Experiments show that our model could simultaneously achieve mastery of the three crucial domains.","sentences":["Underlying data distributions of natural language, programming code, and mathematical symbols vary vastly, presenting a complex challenge for large language models (LLMs) that strive to achieve high performance across all three domains simultaneously.","Achieving a very high level of proficiency for an LLM within a specific domain often requires extensive training with relevant corpora, which is typically accompanied by a sacrifice in performance in other domains.","In this paper, we propose to fuse models that are already highly-specialized directly.","The proposed fusing framework, UltraFuser, consists of three distinct specialists that are already sufficiently trained on language, coding, and mathematics.","A token-level gating mechanism is introduced to blend the specialists' outputs.","A two-stage training strategy accompanied by balanced sampling is designed to ensure stability.","To effectively train the fused model, we further construct a high-quality supervised instruction tuning dataset, UltraChat 2, which includes text, code, and mathematical content.","This dataset comprises approximately 300,000 instructions and covers a wide range of topics in each domain.","Experiments show that our model could simultaneously achieve mastery of the three crucial domains."],"url":"http://arxiv.org/abs/2403.08281v1","category":"cs.CL"}
{"created":"2024-03-13 05:53:25","title":"LiqD: A Dynamic Liquid Level Detection Model under Tricky Small Containers","abstract":"In daily life and industrial production, it is crucial to accurately detect changes in liquid level in containers. Traditional contact measurement methods have some limitations, while emerging non-contact image processing technology shows good application prospects. This paper proposes a container dynamic liquid level detection model based on U^2-Net. This model uses the SAM model to generate an initial data set, and then evaluates and filters out high-quality pseudo-label images through the SemiReward framework to build an exclusive data set. The model uses U^2-Net to extract mask images of containers from the data set, and uses morphological processing to compensate for mask defects. Subsequently, the model calculates the grayscale difference between adjacent video frame images at the same position, segments the liquid level change area by setting a difference threshold, and finally uses a lightweight neural network to classify the liquid level state. This approach not only mitigates the impact of intricate surroundings, but also reduces the demand for training data, showing strong robustness and versatility. A large number of experimental results show that the proposed model can effectively detect the dynamic liquid level changes of the liquid in the container, providing a novel and efficient solution for related fields.","sentences":["In daily life and industrial production, it is crucial to accurately detect changes in liquid level in containers.","Traditional contact measurement methods have some limitations, while emerging non-contact image processing technology shows good application prospects.","This paper proposes a container dynamic liquid level detection model based on U^2-Net.","This model uses the SAM model to generate an initial data set, and then evaluates and filters out high-quality pseudo-label images through the SemiReward framework to build an exclusive data set.","The model uses U^2-Net to extract mask images of containers from the data set, and uses morphological processing to compensate for mask defects.","Subsequently, the model calculates the grayscale difference between adjacent video frame images at the same position, segments the liquid level change area by setting a difference threshold, and finally uses a lightweight neural network to classify the liquid level state.","This approach not only mitigates the impact of intricate surroundings, but also reduces the demand for training data, showing strong robustness and versatility.","A large number of experimental results show that the proposed model can effectively detect the dynamic liquid level changes of the liquid in the container, providing a novel and efficient solution for related fields."],"url":"http://arxiv.org/abs/2403.08273v1","category":"cs.CV"}
{"created":"2024-03-13 05:48:58","title":"Efficient Prompt Tuning of Large Vision-Language Model for Fine-Grained Ship Classification","abstract":"Fine-grained ship classification in remote sensing (RS-FGSC) poses a significant challenge due to the high similarity between classes and the limited availability of labeled data, limiting the effectiveness of traditional supervised classification methods. Recent advancements in large pre-trained Vision-Language Models (VLMs) have demonstrated impressive capabilities in few-shot or zero-shot learning, particularly in understanding image content. This study delves into harnessing the potential of VLMs to enhance classification accuracy for unseen ship categories, which holds considerable significance in scenarios with restricted data due to cost or privacy constraints. Directly fine-tuning VLMs for RS-FGSC often encounters the challenge of overfitting the seen classes, resulting in suboptimal generalization to unseen classes, which highlights the difficulty in differentiating complex backgrounds and capturing distinct ship features. To address these issues, we introduce a novel prompt tuning technique that employs a hierarchical, multi-granularity prompt design. Our approach integrates remote sensing ship priors through bias terms, learned from a small trainable network. This strategy enhances the model's generalization capabilities while improving its ability to discern intricate backgrounds and learn discriminative ship features. Furthermore, we contribute to the field by introducing a comprehensive dataset, FGSCM-52, significantly expanding existing datasets with more extensive data and detailed annotations for less common ship classes. Extensive experimental evaluations demonstrate the superiority of our proposed method over current state-of-the-art techniques. The source code will be made publicly available.","sentences":["Fine-grained ship classification in remote sensing (RS-FGSC) poses a significant challenge due to the high similarity between classes and the limited availability of labeled data, limiting the effectiveness of traditional supervised classification methods.","Recent advancements in large pre-trained Vision-Language Models (VLMs) have demonstrated impressive capabilities in few-shot or zero-shot learning, particularly in understanding image content.","This study delves into harnessing the potential of VLMs to enhance classification accuracy for unseen ship categories, which holds considerable significance in scenarios with restricted data due to cost or privacy constraints.","Directly fine-tuning VLMs for RS-FGSC often encounters the challenge of overfitting the seen classes, resulting in suboptimal generalization to unseen classes, which highlights the difficulty in differentiating complex backgrounds and capturing distinct ship features.","To address these issues, we introduce a novel prompt tuning technique that employs a hierarchical, multi-granularity prompt design.","Our approach integrates remote sensing ship priors through bias terms, learned from a small trainable network.","This strategy enhances the model's generalization capabilities while improving its ability to discern intricate backgrounds and learn discriminative ship features.","Furthermore, we contribute to the field by introducing a comprehensive dataset, FGSCM-52, significantly expanding existing datasets with more extensive data and detailed annotations for less common ship classes.","Extensive experimental evaluations demonstrate the superiority of our proposed method over current state-of-the-art techniques.","The source code will be made publicly available."],"url":"http://arxiv.org/abs/2403.08271v1","category":"cs.CV"}
{"created":"2024-03-13 05:35:55","title":"SNOW-SCA: ML-assisted Side-Channel Attack on SNOW-V","abstract":"This paper presents SNOW-SCA, the first power side-channel analysis (SCA) attack of a 5G mobile communication security standard candidate, SNOW-V, running on a 32-bit ARM Cortex-M4 microcontroller. First, we perform a generic known-key correlation (KKC) analysis to identify the leakage points. Next, a correlation power analysis (CPA) attack is performed, which reduces the attack complexity to two key guesses for each key byte. The correct secret key is then uniquely identified utilizing linear discriminant analysis (LDA). The profiled SCA attack with LDA achieves 100% accuracy after training with $<200$ traces, which means the attack succeeds with just a single trace. Overall, using the \\textit{combined CPA and LDA attack} model, the correct secret key byte is recovered with <50 traces collected using the ChipWhisperer platform. The entire 256-bit secret key of SNOW-V can be recovered incrementally using the proposed SCA attack. Finally, we suggest low-overhead countermeasures that can be used to prevent these SCA attacks.","sentences":["This paper presents SNOW-SCA, the first power side-channel analysis (SCA) attack of a 5G mobile communication security standard candidate, SNOW-V, running on a 32-bit ARM Cortex-M4 microcontroller.","First, we perform a generic known-key correlation (KKC) analysis to identify the leakage points.","Next, a correlation power analysis (CPA) attack is performed, which reduces the attack complexity to two key guesses for each key byte.","The correct secret key is then uniquely identified utilizing linear discriminant analysis (LDA).","The profiled SCA attack with LDA achieves 100% accuracy after training with $<200$ traces, which means the attack succeeds with just a single trace.","Overall, using the \\textit{combined CPA and LDA attack} model, the correct secret key byte is recovered with <50 traces collected using the ChipWhisperer platform.","The entire 256-bit secret key of SNOW-V can be recovered incrementally using the proposed SCA attack.","Finally, we suggest low-overhead countermeasures that can be used to prevent these SCA attacks."],"url":"http://arxiv.org/abs/2403.08267v1","category":"cs.CR"}
{"created":"2024-03-13 05:32:13","title":"Random Search as a Baseline for Sparse Neural Network Architecture Search","abstract":"Sparse neural networks have shown similar or better generalization performance than their dense counterparts while having higher parameter efficiency. This has motivated a number of works to learn, induce, or search for high performing sparse networks. While reports of quality or efficiency gains are impressive, standard baselines are lacking, therefore hindering having reliable comparability and reproducibility across methods. In this work, we provide an evaluation approach and a naive Random Search baseline method for finding good sparse configurations. We apply Random Search on the node space of an overparameterized network with the goal of finding better initialized sparse sub-networks that are positioned more advantageously in the loss landscape. We record sparse network post-training performances at various levels of sparsity and compare against both their fully connected parent networks and random sparse configurations at the same sparsity levels. We observe that for this architecture search task, initialized sparse networks found by Random Search neither perform better nor converge more efficiently than their random counterparts. Thus we conclude that Random Search may be viewed as a suitable neutral baseline for sparsity search methods.","sentences":["Sparse neural networks have shown similar or better generalization performance than their dense counterparts while having higher parameter efficiency.","This has motivated a number of works to learn, induce, or search for high performing sparse networks.","While reports of quality or efficiency gains are impressive, standard baselines are lacking, therefore hindering having reliable comparability and reproducibility across methods.","In this work, we provide an evaluation approach and a naive Random Search baseline method for finding good sparse configurations.","We apply Random Search on the node space of an overparameterized network with the goal of finding better initialized sparse sub-networks that are positioned more advantageously in the loss landscape.","We record sparse network post-training performances at various levels of sparsity and compare against both their fully connected parent networks and random sparse configurations at the same sparsity levels.","We observe that for this architecture search task, initialized sparse networks found by Random Search neither perform better nor converge more efficiently than their random counterparts.","Thus we conclude that Random Search may be viewed as a suitable neutral baseline for sparsity search methods."],"url":"http://arxiv.org/abs/2403.08265v1","category":"cs.LG"}
{"created":"2024-03-13 05:30:30","title":"GPT, Ontology, and CAABAC: A Tripartite Personalized Access Control Model Anchored by Compliance, Context and Attribute","abstract":"As digital healthcare evolves, the security of electronic health records (EHR) becomes increasingly crucial. This study presents the GPT-Onto-CAABAC framework, integrating Generative Pretrained Transformer (GPT), medical-legal ontologies and Context-Aware Attribute-Based Access Control (CAABAC) to enhance EHR access security. Unlike traditional models, GPT-Onto-CAABAC dynamically interprets policies and adapts to changing healthcare and legal environments, offering customized access control solutions. Through empirical evaluation, this framework is shown to be effective in improving EHR security by accurately aligning access decisions with complex regulatory and situational requirements. The findings suggest its broader applicability in sectors where access control must meet stringent compliance and adaptability standards.","sentences":["As digital healthcare evolves, the security of electronic health records (EHR) becomes increasingly crucial.","This study presents the GPT-Onto-CAABAC framework, integrating Generative Pretrained Transformer (GPT), medical-legal ontologies and Context-Aware Attribute-Based Access Control (CAABAC) to enhance EHR access security.","Unlike traditional models, GPT-Onto-CAABAC dynamically interprets policies and adapts to changing healthcare and legal environments, offering customized access control solutions.","Through empirical evaluation, this framework is shown to be effective in improving EHR security by accurately aligning access decisions with complex regulatory and situational requirements.","The findings suggest its broader applicability in sectors where access control must meet stringent compliance and adaptability standards."],"url":"http://arxiv.org/abs/2403.08264v1","category":"cs.CY"}
{"created":"2024-03-13 05:24:28","title":"CoroNetGAN: Controlled Pruning of GANs via Hypernetworks","abstract":"Generative Adversarial Networks (GANs) have proven to exhibit remarkable performance and are widely used across many generative computer vision applications. However, the unprecedented demand for the deployment of GANs on resource-constrained edge devices still poses a challenge due to huge number of parameters involved in the generation process. This has led to focused attention on the area of compressing GANs. Most of the existing works use knowledge distillation with the overhead of teacher dependency. Moreover, there is no ability to control the degree of compression in these methods. Hence, we propose CoroNet-GAN for compressing GAN using the combined strength of differentiable pruning method via hypernetworks. The proposed method provides the advantage of performing controllable compression while training along with reducing training time by a substantial factor. Experiments have been done on various conditional GAN architectures (Pix2Pix and CycleGAN) to signify the effectiveness of our approach on multiple benchmark datasets such as Edges-to-Shoes, Horse-to-Zebra and Summer-to-Winter. The results obtained illustrate that our approach succeeds to outperform the baselines on Zebra-to-Horse and Summer-to-Winter achieving the best FID score of 32.3 and 72.3 respectively, yielding high-fidelity images across all the datasets. Additionally, our approach also outperforms the state-of-the-art methods in achieving better inference time on various smart-phone chipsets and data-types making it a feasible solution for deployment on edge devices.","sentences":["Generative Adversarial Networks (GANs) have proven to exhibit remarkable performance and are widely used across many generative computer vision applications.","However, the unprecedented demand for the deployment of GANs on resource-constrained edge devices still poses a challenge due to huge number of parameters involved in the generation process.","This has led to focused attention on the area of compressing GANs.","Most of the existing works use knowledge distillation with the overhead of teacher dependency.","Moreover, there is no ability to control the degree of compression in these methods.","Hence, we propose CoroNet-GAN for compressing GAN using the combined strength of differentiable pruning method via hypernetworks.","The proposed method provides the advantage of performing controllable compression while training along with reducing training time by a substantial factor.","Experiments have been done on various conditional GAN architectures (Pix2Pix and CycleGAN) to signify the effectiveness of our approach on multiple benchmark datasets such as Edges-to-Shoes, Horse-to-Zebra and Summer-to-Winter.","The results obtained illustrate that our approach succeeds to outperform the baselines on Zebra-to-Horse and Summer-to-Winter achieving the best FID score of 32.3 and 72.3 respectively, yielding high-fidelity images across all the datasets.","Additionally, our approach also outperforms the state-of-the-art methods in achieving better inference time on various smart-phone chipsets and data-types making it a feasible solution for deployment on edge devices."],"url":"http://arxiv.org/abs/2403.08261v1","category":"cs.CV"}
{"created":"2024-03-13 05:08:10","title":"Emergence of Social Norms in Large Language Model-based Agent Societies","abstract":"The emergence of social norms has attracted much interest in a wide array of disciplines, ranging from social science and cognitive science to artificial intelligence. In this paper, we propose the first generative agent architecture that empowers the emergence of social norms within a population of large language model-based agents. Our architecture, named CRSEC, consists of four modules: Creation & Representation, Spreading, Evaluation, and Compliance. Our architecture addresses several important aspects of the emergent processes all in one: (i) where social norms come from, (ii) how they are formally represented, (iii) how they spread through agents' communications and observations, (iv) how they are examined with a sanity check and synthesized in the long term, and (v) how they are incorporated into agents' planning and actions. Our experiments deployed in the Smallville sandbox game environment demonstrate the capability of our architecture to establish social norms and reduce social conflicts within large language model-based multi-agent systems. The positive outcomes of our human evaluation, conducted with 30 evaluators, further affirm the effectiveness of our approach.","sentences":["The emergence of social norms has attracted much interest in a wide array of disciplines, ranging from social science and cognitive science to artificial intelligence.","In this paper, we propose the first generative agent architecture that empowers the emergence of social norms within a population of large language model-based agents.","Our architecture, named CRSEC, consists of four modules: Creation & Representation, Spreading, Evaluation, and Compliance.","Our architecture addresses several important aspects of the emergent processes all in one: (i) where social norms come from, (ii) how they are formally represented, (iii) how they spread through agents' communications and observations, (iv) how they are examined with a sanity check and synthesized in the long term, and (v) how they are incorporated into agents' planning and actions.","Our experiments deployed in the Smallville sandbox game environment demonstrate the capability of our architecture to establish social norms and reduce social conflicts within large language model-based multi-agent systems.","The positive outcomes of our human evaluation, conducted with 30 evaluators, further affirm the effectiveness of our approach."],"url":"http://arxiv.org/abs/2403.08251v1","category":"cs.MA"}
{"created":"2024-03-13 05:03:58","title":"CoPa: General Robotic Manipulation through Spatial Constraints of Parts with Foundation Models","abstract":"Foundation models pre-trained on web-scale data are shown to encapsulate extensive world knowledge beneficial for robotic manipulation in the form of task planning. However, the actual physical implementation of these plans often relies on task-specific learning methods, which require significant data collection and struggle with generalizability. In this work, we introduce Robotic Manipulation through Spatial Constraints of Parts (CoPa), a novel framework that leverages the common sense knowledge embedded within foundation models to generate a sequence of 6-DoF end-effector poses for open-world robotic manipulation. Specifically, we decompose the manipulation process into two phases: task-oriented grasping and task-aware motion planning. In the task-oriented grasping phase, we employ foundation vision-language models (VLMs) to select the object's grasping part through a novel coarse-to-fine grounding mechanism. During the task-aware motion planning phase, VLMs are utilized again to identify the spatial geometry constraints of task-relevant object parts, which are then used to derive post-grasp poses. We also demonstrate how CoPa can be seamlessly integrated with existing robotic planning algorithms to accomplish complex, long-horizon tasks. Our comprehensive real-world experiments show that CoPa possesses a fine-grained physical understanding of scenes, capable of handling open-set instructions and objects with minimal prompt engineering and without additional training. Project page: https://copa-2024.github.io/","sentences":["Foundation models pre-trained on web-scale data are shown to encapsulate extensive world knowledge beneficial for robotic manipulation in the form of task planning.","However, the actual physical implementation of these plans often relies on task-specific learning methods, which require significant data collection and struggle with generalizability.","In this work, we introduce Robotic Manipulation through Spatial Constraints of Parts (CoPa), a novel framework that leverages the common sense knowledge embedded within foundation models to generate a sequence of 6-DoF end-effector poses for open-world robotic manipulation.","Specifically, we decompose the manipulation process into two phases: task-oriented grasping and task-aware motion planning.","In the task-oriented grasping phase, we employ foundation vision-language models (VLMs) to select the object's grasping part through a novel coarse-to-fine grounding mechanism.","During the task-aware motion planning phase, VLMs are utilized again to identify the spatial geometry constraints of task-relevant object parts, which are then used to derive post-grasp poses.","We also demonstrate how CoPa can be seamlessly integrated with existing robotic planning algorithms to accomplish complex, long-horizon tasks.","Our comprehensive real-world experiments show that CoPa possesses a fine-grained physical understanding of scenes, capable of handling open-set instructions and objects with minimal prompt engineering and without additional training.","Project page: https://copa-2024.github.io/"],"url":"http://arxiv.org/abs/2403.08248v1","category":"cs.RO"}
{"created":"2024-03-13 04:43:10","title":"A Novel Feature Learning-based Bio-inspired Neural Network for Real-time Collision-free Rescue of Multi-Robot Systems","abstract":"Natural disasters and urban accidents drive the demand for rescue robots to provide safer, faster, and more efficient rescue trajectories. In this paper, a feature learning-based bio-inspired neural network (FLBBINN) is proposed to quickly generate a heuristic rescue path in complex and dynamic environments, as traditional approaches usually cannot provide a satisfactory solution to real-time responses to sudden environmental changes. The neurodynamic model is incorporated into the feature learning method that can use environmental information to improve path planning strategies. Task assignment and collision-free rescue trajectory are generated through robot poses and the dynamic landscape of neural activity. A dual-channel scale filter, a neural activity channel, and a secondary distance fusion are employed to extract and filter feature neurons. After completion of the feature learning process, a neurodynamics-based feature matrix is established to quickly generate the new heuristic rescue paths with parameter-driven topological adaptability. The proposed FLBBINN aims to reduce the computational complexity of the neural network-based approach and enable the feature learning method to achieve real-time responses to environmental changes. Several simulations and experiments have been conducted to evaluate the performance of the proposed FLBBINN. The results show that the proposed FLBBINN would significantly improve the speed, efficiency, and optimality for rescue operations.","sentences":["Natural disasters and urban accidents drive the demand for rescue robots to provide safer, faster, and more efficient rescue trajectories.","In this paper, a feature learning-based bio-inspired neural network (FLBBINN) is proposed to quickly generate a heuristic rescue path in complex and dynamic environments, as traditional approaches usually cannot provide a satisfactory solution to real-time responses to sudden environmental changes.","The neurodynamic model is incorporated into the feature learning method that can use environmental information to improve path planning strategies.","Task assignment and collision-free rescue trajectory are generated through robot poses and the dynamic landscape of neural activity.","A dual-channel scale filter, a neural activity channel, and a secondary distance fusion are employed to extract and filter feature neurons.","After completion of the feature learning process, a neurodynamics-based feature matrix is established to quickly generate the new heuristic rescue paths with parameter-driven topological adaptability.","The proposed FLBBINN aims to reduce the computational complexity of the neural network-based approach and enable the feature learning method to achieve real-time responses to environmental changes.","Several simulations and experiments have been conducted to evaluate the performance of the proposed FLBBINN.","The results show that the proposed FLBBINN would significantly improve the speed, efficiency, and optimality for rescue operations."],"url":"http://arxiv.org/abs/2403.08238v1","category":"cs.RO"}
{"created":"2024-03-13 03:47:08","title":"Robust Decision Aggregation with Adversarial Experts","abstract":"We consider a binary decision aggregation problem in the presence of both truthful and adversarial experts. The truthful experts will report their private signals truthfully with proper incentive, while the adversarial experts can report arbitrarily. The decision maker needs to design a robust aggregator to forecast the true state of the world based on the reports of experts. The decision maker does not know the specific information structure, which is a joint distribution of signals, states, and strategies of adversarial experts. We want to find the optimal aggregator minimizing regret under the worst information structure. The regret is defined by the difference in expected loss between the aggregator and a benchmark who makes the optimal decision given the joint distribution and reports of truthful experts.   We prove that when the truthful experts are symmetric and adversarial experts are not too numerous, the truncated mean is optimal, which means that we remove some lowest reports and highest reports and take averaging among the left reports. Moreover, for many settings, the optimal aggregators are in the family of piecewise linear functions. The regret is independent of the total number of experts but only depends on the ratio of adversaries. We evaluate our aggregators by numerical experiment in an ensemble learning task. We also obtain some negative results for the aggregation problem with adversarial experts under some more general information structures and experts' report space.","sentences":["We consider a binary decision aggregation problem in the presence of both truthful and adversarial experts.","The truthful experts will report their private signals truthfully with proper incentive, while the adversarial experts can report arbitrarily.","The decision maker needs to design a robust aggregator to forecast the true state of the world based on the reports of experts.","The decision maker does not know the specific information structure, which is a joint distribution of signals, states, and strategies of adversarial experts.","We want to find the optimal aggregator minimizing regret under the worst information structure.","The regret is defined by the difference in expected loss between the aggregator and a benchmark who makes the optimal decision given the joint distribution and reports of truthful experts.   ","We prove that when the truthful experts are symmetric and adversarial experts are not too numerous, the truncated mean is optimal, which means that we remove some lowest reports and highest reports and take averaging among the left reports.","Moreover, for many settings, the optimal aggregators are in the family of piecewise linear functions.","The regret is independent of the total number of experts but only depends on the ratio of adversaries.","We evaluate our aggregators by numerical experiment in an ensemble learning task.","We also obtain some negative results for the aggregation problem with adversarial experts under some more general information structures and experts' report space."],"url":"http://arxiv.org/abs/2403.08222v1","category":"cs.LG"}
{"created":"2024-03-13 03:45:14","title":"Efficient geometric Markov chain Monte Carlo for nonlinear Bayesian inversion enabled by derivative-informed neural operators","abstract":"We propose an operator learning approach to accelerate geometric Markov chain Monte Carlo (MCMC) for solving infinite-dimensional nonlinear Bayesian inverse problems. While geometric MCMC employs high-quality proposals that adapt to posterior local geometry, it requires computing local gradient and Hessian information of the log-likelihood, incurring a high cost when the parameter-to-observable (PtO) map is defined through expensive model simulations. We consider a delayed-acceptance geometric MCMC method driven by a neural operator surrogate of the PtO map, where the proposal is designed to exploit fast surrogate approximations of the log-likelihood and, simultaneously, its gradient and Hessian. To achieve a substantial speedup, the surrogate needs to be accurate in predicting both the observable and its parametric derivative (the derivative of the observable with respect to the parameter). Training such a surrogate via conventional operator learning using input--output samples often demands a prohibitively large number of model simulations. In this work, we present an extension of derivative-informed operator learning [O'Leary-Roseberry et al., J. Comput. Phys., 496 (2024)] using input--output--derivative training samples. Such a learning method leads to derivative-informed neural operator (DINO) surrogates that accurately predict the observable and its parametric derivative at a significantly lower training cost than the conventional method. Cost and error analysis for reduced basis DINO surrogates are provided. Numerical studies on PDE-constrained Bayesian inversion demonstrate that DINO-driven MCMC generates effective posterior samples 3--9 times faster than geometric MCMC and 60--97 times faster than prior geometry-based MCMC. Furthermore, the training cost of DINO surrogates breaks even after collecting merely 10--25 effective posterior samples compared to geometric MCMC.","sentences":["We propose an operator learning approach to accelerate geometric Markov chain Monte Carlo (MCMC) for solving infinite-dimensional nonlinear Bayesian inverse problems.","While geometric MCMC employs high-quality proposals that adapt to posterior local geometry, it requires computing local gradient and Hessian information of the log-likelihood, incurring a high cost when the parameter-to-observable (PtO) map is defined through expensive model simulations.","We consider a delayed-acceptance geometric MCMC method driven by a neural operator surrogate of the PtO map, where the proposal is designed to exploit fast surrogate approximations of the log-likelihood and, simultaneously, its gradient and Hessian.","To achieve a substantial speedup, the surrogate needs to be accurate in predicting both the observable and its parametric derivative (the derivative of the observable with respect to the parameter).","Training such a surrogate via conventional operator learning using input--output samples often demands a prohibitively large number of model simulations.","In this work, we present an extension of derivative-informed operator learning [O'Leary-Roseberry et al., J. Comput.","Phys., 496 (2024)] using input--output--derivative training samples.","Such a learning method leads to derivative-informed neural operator (DINO) surrogates that accurately predict the observable and its parametric derivative at a significantly lower training cost than the conventional method.","Cost and error analysis for reduced basis DINO surrogates are provided.","Numerical studies on PDE-constrained Bayesian inversion demonstrate that DINO-driven MCMC generates effective posterior samples 3--9 times faster than geometric MCMC and 60--97 times faster than prior geometry-based MCMC.","Furthermore, the training cost of DINO surrogates breaks even after collecting merely 10--25 effective posterior samples compared to geometric MCMC."],"url":"http://arxiv.org/abs/2403.08220v1","category":"math.NA"}
{"created":"2024-03-13 03:24:36","title":"LIX: Implicitly Infusing Spatial Geometric Prior Knowledge into Visual Semantic Segmentation for Autonomous Driving","abstract":"Despite the impressive performance achieved by data-fusion networks with duplex encoders for visual semantic segmentation, they become ineffective when spatial geometric data are not available. Implicitly infusing the spatial geometric prior knowledge acquired by a duplex-encoder teacher model into a single-encoder student model is a practical, albeit less explored research avenue. This paper delves into this topic and resorts to knowledge distillation approaches to address this problem. We introduce the Learning to Infuse \"X\" (LIX) framework, with novel contributions in both logit distillation and feature distillation aspects. We present a mathematical proof that underscores the limitation of using a single fixed weight in decoupled knowledge distillation and introduce a logit-wise dynamic weight controller as a solution to this issue. Furthermore, we develop an adaptively-recalibrated feature distillation algorithm, including two technical novelties: feature recalibration via kernel regression and in-depth feature consistency quantification via centered kernel alignment. Extensive experiments conducted with intermediate-fusion and late-fusion networks across various public datasets provide both quantitative and qualitative evaluations, demonstrating the superior performance of our LIX framework when compared to other state-of-the-art approaches.","sentences":["Despite the impressive performance achieved by data-fusion networks with duplex encoders for visual semantic segmentation, they become ineffective when spatial geometric data are not available.","Implicitly infusing the spatial geometric prior knowledge acquired by a duplex-encoder teacher model into a single-encoder student model is a practical, albeit less explored research avenue.","This paper delves into this topic and resorts to knowledge distillation approaches to address this problem.","We introduce the Learning to Infuse \"X\" (LIX) framework, with novel contributions in both logit distillation and feature distillation aspects.","We present a mathematical proof that underscores the limitation of using a single fixed weight in decoupled knowledge distillation and introduce a logit-wise dynamic weight controller as a solution to this issue.","Furthermore, we develop an adaptively-recalibrated feature distillation algorithm, including two technical novelties: feature recalibration via kernel regression and in-depth feature consistency quantification via centered kernel alignment.","Extensive experiments conducted with intermediate-fusion and late-fusion networks across various public datasets provide both quantitative and qualitative evaluations, demonstrating the superior performance of our LIX framework when compared to other state-of-the-art approaches."],"url":"http://arxiv.org/abs/2403.08215v1","category":"cs.CV"}
{"created":"2024-03-13 03:23:50","title":"P2LHAP:Wearable sensor-based human activity recognition, segmentation and forecast through Patch-to-Label Seq2Seq Transformer","abstract":"Traditional deep learning methods struggle to simultaneously segment, recognize, and forecast human activities from sensor data. This limits their usefulness in many fields such as healthcare and assisted living, where real-time understanding of ongoing and upcoming activities is crucial. This paper introduces P2LHAP, a novel Patch-to-Label Seq2Seq framework that tackles all three tasks in a efficient single-task model. P2LHAP divides sensor data streams into a sequence of \"patches\", served as input tokens, and outputs a sequence of patch-level activity labels including the predicted future activities. A unique smoothing technique based on surrounding patch labels, is proposed to identify activity boundaries accurately. Additionally, P2LHAP learns patch-level representation by sensor signal channel-independent Transformer encoders and decoders. All channels share embedding and Transformer weights across all sequences. Evaluated on three public datasets, P2LHAP significantly outperforms the state-of-the-art in all three tasks, demonstrating its effectiveness and potential for real-world applications.","sentences":["Traditional deep learning methods struggle to simultaneously segment, recognize, and forecast human activities from sensor data.","This limits their usefulness in many fields such as healthcare and assisted living, where real-time understanding of ongoing and upcoming activities is crucial.","This paper introduces P2LHAP, a novel Patch-to-Label Seq2Seq framework that tackles all three tasks in a efficient single-task model.","P2LHAP divides sensor data streams into a sequence of \"patches\", served as input tokens, and outputs a sequence of patch-level activity labels including the predicted future activities.","A unique smoothing technique based on surrounding patch labels, is proposed to identify activity boundaries accurately.","Additionally, P2LHAP learns patch-level representation by sensor signal channel-independent Transformer encoders and decoders.","All channels share embedding and Transformer weights across all sequences.","Evaluated on three public datasets, P2LHAP significantly outperforms the state-of-the-art in all three tasks, demonstrating its effectiveness and potential for real-world applications."],"url":"http://arxiv.org/abs/2403.08214v1","category":"cs.CV"}
{"created":"2024-03-13 03:15:05","title":"Large Language Models are Contrastive Reasoners","abstract":"Prompting methods play a crucial role in enhancing the capabilities of pre-trained large language models (LLMs). We explore how contrastive prompting (CP) significantly improves the ability of large language models to perform complex reasoning. We demonstrate that LLMs are decent contrastive reasoners by simply adding \"Let's give a correct and a wrong answer.\" before LLMs provide answers. Experiments on two large language models show that zero-shot contrastive prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks without any hand-crafted few-shot examples, such as increasing the accuracy on GSM8K from 35.9% to 88.8% and AQUA-RAT from 41.3% to 62.2% with the state-of-the-art GPT-4 model. Our method not only surpasses zero-shot CoT and few-shot CoT in most arithmetic and commonsense reasoning tasks but also can seamlessly integrate with existing prompting methods, resulting in improved or comparable results when compared to state-of-the-art methods. Our code is available at https://github.com/yao8839836/cp","sentences":["Prompting methods play a crucial role in enhancing the capabilities of pre-trained large language models (LLMs).","We explore how contrastive prompting (CP) significantly improves the ability of large language models to perform complex reasoning.","We demonstrate that LLMs are decent contrastive reasoners by simply adding \"Let's give a correct and a wrong answer.\"","before LLMs provide answers.","Experiments on two large language models show that zero-shot contrastive prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks without any hand-crafted few-shot examples, such as increasing the accuracy on GSM8K from 35.9% to 88.8% and AQUA-RAT from 41.3% to 62.2% with the state-of-the-art GPT-4 model.","Our method not only surpasses zero-shot CoT and few-shot CoT in most arithmetic and commonsense reasoning tasks but also can seamlessly integrate with existing prompting methods, resulting in improved or comparable results when compared to state-of-the-art methods.","Our code is available at https://github.com/yao8839836/cp"],"url":"http://arxiv.org/abs/2403.08211v1","category":"cs.CL"}
{"created":"2024-03-13 02:53:52","title":"Deep Submodular Peripteral Network","abstract":"Submodular functions, crucial for various applications, often lack practical learning methods for their acquisition. Seemingly unrelated, learning a scaling from oracles offering graded pairwise preferences (GPC) is underexplored, despite a rich history in psychometrics. In this paper, we introduce deep submodular peripteral networks (DSPNs), a novel parametric family of submodular functions, and methods for their training using a contrastive-learning inspired GPC-ready strategy to connect and then tackle both of the above challenges. We introduce newly devised GPC-style \"peripteral\" loss which leverages numerically graded relationships between pairs of objects (sets in our case). Unlike traditional contrastive learning, our method utilizes graded comparisons, extracting more nuanced information than just binary-outcome comparisons, and contrasts sets of any size (not just two). We also define a novel suite of automatic sampling strategies for training, including active-learning inspired submodular feedback. We demonstrate DSPNs' efficacy in learning submodularity from a costly target submodular function showing superiority in downstream tasks such as experimental design and streaming applications.","sentences":["Submodular functions, crucial for various applications, often lack practical learning methods for their acquisition.","Seemingly unrelated, learning a scaling from oracles offering graded pairwise preferences (GPC) is underexplored, despite a rich history in psychometrics.","In this paper, we introduce deep submodular peripteral networks (DSPNs), a novel parametric family of submodular functions, and methods for their training using a contrastive-learning inspired GPC-ready strategy to connect and then tackle both of the above challenges.","We introduce newly devised GPC-style \"peripteral\" loss which leverages numerically graded relationships between pairs of objects (sets in our case).","Unlike traditional contrastive learning, our method utilizes graded comparisons, extracting more nuanced information than just binary-outcome comparisons, and contrasts sets of any size (not just two).","We also define a novel suite of automatic sampling strategies for training, including active-learning inspired submodular feedback.","We demonstrate DSPNs' efficacy in learning submodularity from a costly target submodular function showing superiority in downstream tasks such as experimental design and streaming applications."],"url":"http://arxiv.org/abs/2403.08199v1","category":"cs.LG"}
{"created":"2024-03-13 02:44:33","title":"PAGE: Domain-Incremental Adaptation with Past-Agnostic Generative Replay for Smart Healthcare","abstract":"We propose PAGE, a domain-incremental adaptation strategy with past-agnostic generative replay for smart healthcare. PAGE enables generative replay without the aid of any preserved data or information from prior domains. When adapting to a new domain, it exploits real data from the new distribution and the current model to generate synthetic data that retain the learned knowledge of previous domains. By replaying the synthetic data with the new real data during training, PAGE achieves a good balance between domain adaptation and knowledge retention. In addition, we incorporate an extended inductive conformal prediction (EICP) method into PAGE to produce a confidence score and a credibility value for each detection result. This makes the predictions interpretable and provides statistical guarantees for disease detection in smart healthcare applications. We demonstrate PAGE's effectiveness in domain-incremental disease detection with three distinct disease datasets collected from commercially available WMSs. PAGE achieves highly competitive performance against state-of-the-art with superior scalability, data privacy, and feasibility. Furthermore, PAGE can enable up to 75% reduction in clinical workload with the help of EICP.","sentences":["We propose PAGE, a domain-incremental adaptation strategy with past-agnostic generative replay for smart healthcare.","PAGE enables generative replay without the aid of any preserved data or information from prior domains.","When adapting to a new domain, it exploits real data from the new distribution and the current model to generate synthetic data that retain the learned knowledge of previous domains.","By replaying the synthetic data with the new real data during training, PAGE achieves a good balance between domain adaptation and knowledge retention.","In addition, we incorporate an extended inductive conformal prediction (EICP) method into PAGE to produce a confidence score and a credibility value for each detection result.","This makes the predictions interpretable and provides statistical guarantees for disease detection in smart healthcare applications.","We demonstrate PAGE's effectiveness in domain-incremental disease detection with three distinct disease datasets collected from commercially available WMSs.","PAGE achieves highly competitive performance against state-of-the-art with superior scalability, data privacy, and feasibility.","Furthermore, PAGE can enable up to 75% reduction in clinical workload with the help of EICP."],"url":"http://arxiv.org/abs/2403.08197v1","category":"cs.LG"}
{"created":"2024-03-13 01:56:32","title":"Rethinking Loss Functions for Fact Verification","abstract":"We explore loss functions for fact verification in the FEVER shared task. While the cross-entropy loss is a standard objective for training verdict predictors, it fails to capture the heterogeneity among the FEVER verdict classes. In this paper, we develop two task-specific objectives tailored to FEVER. Experimental results confirm that the proposed objective functions outperform the standard cross-entropy. Performance is further improved when these objectives are combined with simple class weighting, which effectively overcomes the imbalance in the training data. The souce code is available at https://github.com/yuta-mukobara/RLF-KGAT","sentences":["We explore loss functions for fact verification in the FEVER shared task.","While the cross-entropy loss is a standard objective for training verdict predictors, it fails to capture the heterogeneity among the FEVER verdict classes.","In this paper, we develop two task-specific objectives tailored to FEVER.","Experimental results confirm that the proposed objective functions outperform the standard cross-entropy.","Performance is further improved when these objectives are combined with simple class weighting, which effectively overcomes the imbalance in the training data.","The souce code is available at https://github.com/yuta-mukobara/RLF-KGAT"],"url":"http://arxiv.org/abs/2403.08174v1","category":"cs.CL"}
{"created":"2024-03-13 01:38:42","title":"MolBind: Multimodal Alignment of Language, Molecules, and Proteins","abstract":"Recent advancements in biology and chemistry have leveraged multi-modal learning, integrating molecules and their natural language descriptions to enhance drug discovery. However, current pre-training frameworks are limited to two modalities, and designing a unified network to process different modalities (e.g., natural language, 2D molecular graphs, 3D molecular conformations, and 3D proteins) remains challenging due to inherent gaps among them. In this work, we propose MolBind, a framework that trains encoders for multiple modalities through contrastive learning, mapping all modalities to a shared feature space for multi-modal semantic alignment. To facilitate effective pre-training of MolBind on multiple modalities, we also build and collect a high-quality dataset with four modalities, MolBind-M4, including graph-language, conformation-language, graph-conformation, and conformation-protein paired data. MolBind shows superior zero-shot learning performance across a wide range of tasks, demonstrating its strong capability of capturing the underlying semantics of multiple modalities.","sentences":["Recent advancements in biology and chemistry have leveraged multi-modal learning, integrating molecules and their natural language descriptions to enhance drug discovery.","However, current pre-training frameworks are limited to two modalities, and designing a unified network to process different modalities (e.g., natural language, 2D molecular graphs, 3D molecular conformations, and 3D proteins) remains challenging due to inherent gaps among them.","In this work, we propose MolBind, a framework that trains encoders for multiple modalities through contrastive learning, mapping all modalities to a shared feature space for multi-modal semantic alignment.","To facilitate effective pre-training of MolBind on multiple modalities, we also build and collect a high-quality dataset with four modalities, MolBind-M4, including graph-language, conformation-language, graph-conformation, and conformation-protein paired data.","MolBind shows superior zero-shot learning performance across a wide range of tasks, demonstrating its strong capability of capturing the underlying semantics of multiple modalities."],"url":"http://arxiv.org/abs/2403.08167v1","category":"cs.LG"}
{"created":"2024-03-13 01:07:55","title":"LAFS: Landmark-based Facial Self-supervised Learning for Face Recognition","abstract":"In this work we focus on learning facial representations that can be adapted to train effective face recognition models, particularly in the absence of labels. Firstly, compared with existing labelled face datasets, a vastly larger magnitude of unlabeled faces exists in the real world. We explore the learning strategy of these unlabeled facial images through self-supervised pretraining to transfer generalized face recognition performance. Moreover, motivated by one recent finding, that is, the face saliency area is critical for face recognition, in contrast to utilizing random cropped blocks of images for constructing augmentations in pretraining, we utilize patches localized by extracted facial landmarks. This enables our method - namely LAndmark-based Facial Self-supervised learning LAFS), to learn key representation that is more critical for face recognition. We also incorporate two landmark-specific augmentations which introduce more diversity of landmark information to further regularize the learning. With learned landmark-based facial representations, we further adapt the representation for face recognition with regularization mitigating variations in landmark positions. Our method achieves significant improvement over the state-of-the-art on multiple face recognition benchmarks, especially on more challenging few-shot scenarios.","sentences":["In this work we focus on learning facial representations that can be adapted to train effective face recognition models, particularly in the absence of labels.","Firstly, compared with existing labelled face datasets, a vastly larger magnitude of unlabeled faces exists in the real world.","We explore the learning strategy of these unlabeled facial images through self-supervised pretraining to transfer generalized face recognition performance.","Moreover, motivated by one recent finding, that is, the face saliency area is critical for face recognition, in contrast to utilizing random cropped blocks of images for constructing augmentations in pretraining, we utilize patches localized by extracted facial landmarks.","This enables our method - namely LAndmark-based Facial Self-supervised learning LAFS), to learn key representation that is more critical for face recognition.","We also incorporate two landmark-specific augmentations which introduce more diversity of landmark information to further regularize the learning.","With learned landmark-based facial representations, we further adapt the representation for face recognition with regularization mitigating variations in landmark positions.","Our method achieves significant improvement over the state-of-the-art on multiple face recognition benchmarks, especially on more challenging few-shot scenarios."],"url":"http://arxiv.org/abs/2403.08161v1","category":"cs.CV"}
{"created":"2024-03-13 00:30:47","title":"The Runtime of Random Local Search on the Generalized Needle Problem","abstract":"In their recent work, C. Doerr and Krejca (Transactions on Evolutionary Computation, 2023) proved upper bounds on the expected runtime of the randomized local search heuristic on generalized Needle functions. Based on these upper bounds, they deduce in a not fully rigorous manner a drastic influence of the needle radius $k$ on the runtime.   In this short article, we add the missing lower bound necessary to determine the influence of parameter $k$ on the runtime. To this aim, we derive an exact description of the expected runtime, which also significantly improves the upper bound given by C. Doerr and Krejca. We also describe asymptotic estimates of the expected runtime.","sentences":["In their recent work, C. Doerr and Krejca (Transactions on Evolutionary Computation, 2023) proved upper bounds on the expected runtime of the randomized local search heuristic on generalized Needle functions.","Based on these upper bounds, they deduce in a not fully rigorous manner a drastic influence of the needle radius $k$ on the runtime.   ","In this short article, we add the missing lower bound necessary to determine the influence of parameter $k$ on the runtime.","To this aim, we derive an exact description of the expected runtime, which also significantly improves the upper bound given by C. Doerr and Krejca.","We also describe asymptotic estimates of the expected runtime."],"url":"http://arxiv.org/abs/2403.08153v1","category":"cs.NE"}
{"created":"2024-03-13 00:27:19","title":"Measuring the Energy Consumption and Efficiency of Deep Neural Networks: An Empirical Analysis and Design Recommendations","abstract":"Addressing the so-called ``Red-AI'' trend of rising energy consumption by large-scale neural networks, this study investigates the actual energy consumption, as measured by node-level watt-meters, of training various fully connected neural network architectures. We introduce the BUTTER-E dataset, an augmentation to the BUTTER Empirical Deep Learning dataset, containing energy consumption and performance data from 63,527 individual experimental runs spanning 30,582 distinct configurations: 13 datasets, 20 sizes (number of trainable parameters), 8 network ``shapes'', and 14 depths on both CPU and GPU hardware collected using node-level watt-meters. This dataset reveals the complex relationship between dataset size, network structure, and energy use, and highlights the impact of cache effects. We propose a straightforward and effective energy model that accounts for network size, computing, and memory hierarchy. Our analysis also uncovers a surprising, hardware-mediated non-linear relationship between energy efficiency and network design, challenging the assumption that reducing the number of parameters or FLOPs is the best way to achieve greater energy efficiency. Highlighting the need for cache-considerate algorithm development, we suggest a combined approach to energy efficient network, algorithm, and hardware design. This work contributes to the fields of sustainable computing and Green AI, offering practical guidance for creating more energy-efficient neural networks and promoting sustainable AI.","sentences":["Addressing the so-called ``Red-AI'' trend of rising energy consumption by large-scale neural networks, this study investigates the actual energy consumption, as measured by node-level watt-meters, of training various fully connected neural network architectures.","We introduce the BUTTER-E dataset, an augmentation to the BUTTER Empirical Deep Learning dataset, containing energy consumption and performance data from 63,527 individual experimental runs spanning 30,582 distinct configurations: 13 datasets, 20 sizes (number of trainable parameters), 8 network ``shapes'', and 14 depths on both CPU and GPU hardware collected using node-level watt-meters.","This dataset reveals the complex relationship between dataset size, network structure, and energy use, and highlights the impact of cache effects.","We propose a straightforward and effective energy model that accounts for network size, computing, and memory hierarchy.","Our analysis also uncovers a surprising, hardware-mediated non-linear relationship between energy efficiency and network design, challenging the assumption that reducing the number of parameters or FLOPs is the best way to achieve greater energy efficiency.","Highlighting the need for cache-considerate algorithm development, we suggest a combined approach to energy efficient network, algorithm, and hardware design.","This work contributes to the fields of sustainable computing and Green AI, offering practical guidance for creating more energy-efficient neural networks and promoting sustainable AI."],"url":"http://arxiv.org/abs/2403.08151v1","category":"cs.LG"}
{"created":"2024-03-13 00:19:44","title":"On the Feasibility of EEG-based Motor Intention Detection for Real-Time Robot Assistive Control","abstract":"This paper explores the feasibility of employing EEG-based intention detection for real-time robot assistive control. We focus on predicting and distinguishing motor intentions of left/right arm movements by presenting: i) an offline data collection and training pipeline, used to train a classifier for left/right motion intention prediction, and ii) an online real-time prediction pipeline leveraging the trained classifier and integrated with an assistive robot. Central to our approach is a rich feature representation composed of the tangent space projection of time-windowed sample covariance matrices from EEG filtered signals and derivatives; allowing for a simple SVM classifier to achieve unprecedented accuracy and real-time performance. In pre-recorded real-time settings (160 Hz), a peak accuracy of 86.88% is achieved, surpassing prior works. In robot-in-the-loop settings, our system successfully detects intended motion solely from EEG data with 70% accuracy, triggering a robot to execute an assistive task. We provide a comprehensive evaluation of the proposed classifier.","sentences":["This paper explores the feasibility of employing EEG-based intention detection for real-time robot assistive control.","We focus on predicting and distinguishing motor intentions of left/right arm movements by presenting: i) an offline data collection and training pipeline, used to train a classifier for left/right motion intention prediction, and ii) an online real-time prediction pipeline leveraging the trained classifier and integrated with an assistive robot.","Central to our approach is a rich feature representation composed of the tangent space projection of time-windowed sample covariance matrices from EEG filtered signals and derivatives; allowing for a simple SVM classifier to achieve unprecedented accuracy and real-time performance.","In pre-recorded real-time settings (160 Hz), a peak accuracy of 86.88% is achieved, surpassing prior works.","In robot-in-the-loop settings, our system successfully detects intended motion solely from EEG data with 70% accuracy, triggering a robot to execute an assistive task.","We provide a comprehensive evaluation of the proposed classifier."],"url":"http://arxiv.org/abs/2403.08149v1","category":"cs.RO"}
{"created":"2024-03-12 23:47:28","title":"From Paper to Card: Transforming Design Implications with Generative AI","abstract":"Communicating design implications is common within the HCI community when publishing academic papers, yet these papers are rarely read and used by designers. One solution is to use design cards as a form of translational resource that communicates valuable insights from papers in a more digestible and accessible format to assist in design processes. However, creating design cards can be time-consuming, and authors may lack the resources/know-how to produce cards. Through an iterative design process, we built a system that helps create design cards from academic papers using an LLM and text-to-image model. Our evaluation with designers (N=21) and authors of selected papers (N=12) revealed that designers perceived the design implications from our design cards as more inspiring and generative, compared to reading original paper texts, and the authors viewed our system as an effective way of communicating their design implications. We also propose future enhancements for AI-generated design cards.","sentences":["Communicating design implications is common within the HCI community when publishing academic papers, yet these papers are rarely read and used by designers.","One solution is to use design cards as a form of translational resource that communicates valuable insights from papers in a more digestible and accessible format to assist in design processes.","However, creating design cards can be time-consuming, and authors may lack the resources/know-how to produce cards.","Through an iterative design process, we built a system that helps create design cards from academic papers using an LLM and text-to-image model.","Our evaluation with designers (N=21) and authors of selected papers (N=12) revealed that designers perceived the design implications from our design cards as more inspiring and generative, compared to reading original paper texts, and the authors viewed our system as an effective way of communicating their design implications.","We also propose future enhancements for AI-generated design cards."],"url":"http://arxiv.org/abs/2403.08137v1","category":"cs.HC"}
{"created":"2024-03-12 23:47:00","title":"RoboCertProb: Property Specification for Probabilistic RoboChart Models","abstract":"RoboChart is a core notation in the RoboStar framework which brings modern modelling and formal verification technologies into software engineering for robotics. It is a timed and probabilistic domain-specific language for robotics and provides a UML-like architectural and state machine modelling. This work presents RoboCertProb for specifying quantitative properties of probabilistic robotic systems modelled in RoboChart. RoboCertProb's semantics is based on PCTL*. To interpret RoboCertProb over RoboChart models, we give a Markov semantics (DTMCs and MDPs) to RoboChart, derived from its existing transformation semantics to the PRISM language. In addition to property specification, RoboCertProb also entitles us to configure loose constants and unspecified functions and operations in RoboChart models. It allows us to set up environmental inputs to verify reactive probabilistic systems not directly supported in probabilistic model checkers like PRISM because they employ a closed-world assumption. We implement RoboCertProb in an accompanying tool of RoboChart, RoboTool, for specifying properties and automatically generating PRISM properties from them to formally verify RoboChart models using PRISM. We have used it to analyse the behaviour of software controllers for two real robots: an industrial painting robot and an agricultural robot for treating plants with UV lights.","sentences":["RoboChart is a core notation in the RoboStar framework which brings modern modelling and formal verification technologies into software engineering for robotics.","It is a timed and probabilistic domain-specific language for robotics and provides a UML-like architectural and state machine modelling.","This work presents RoboCertProb for specifying quantitative properties of probabilistic robotic systems modelled in RoboChart.","RoboCertProb's semantics is based on PCTL*.","To interpret RoboCertProb over RoboChart models, we give a Markov semantics (DTMCs and MDPs) to RoboChart, derived from its existing transformation semantics to the PRISM language.","In addition to property specification, RoboCertProb also entitles us to configure loose constants and unspecified functions and operations in RoboChart models.","It allows us to set up environmental inputs to verify reactive probabilistic systems not directly supported in probabilistic model checkers like PRISM because they employ a closed-world assumption.","We implement RoboCertProb in an accompanying tool of RoboChart, RoboTool, for specifying properties and automatically generating PRISM properties from them to formally verify RoboChart models using PRISM.","We have used it to analyse the behaviour of software controllers for two real robots: an industrial painting robot and an agricultural robot for treating plants with UV lights."],"url":"http://arxiv.org/abs/2403.08136v1","category":"cs.LO"}
{"created":"2024-03-12 23:40:51","title":"Physics-Inspired Deep Learning Anti-Aliasing Framework in Efficient Channel State Feedback","abstract":"Acquiring downlink channel state information (CSI) at the base station is vital for optimizing performance in massive Multiple input multiple output (MIMO) Frequency-Division Duplexing (FDD) systems. While deep learning architectures have been successful in facilitating UE-side CSI feedback and gNB-side recovery, the undersampling issue prior to CSI feedback is often overlooked. This issue, which arises from low density pilot placement in current standards, results in significant aliasing effects in outdoor channels and consequently limits CSI recovery performance. To this end, this work introduces a new CSI upsampling framework at the gNB as a post-processing solution to address the gaps caused by undersampling. Leveraging the physical principles of discrete Fourier transform shifting theorem and multipath reciprocity, our framework effectively uses uplink CSI to mitigate aliasing effects. We further develop a learning-based method that integrates the proposed algorithm with the Iterative Shrinkage-Thresholding Algorithm Net (ISTA-Net) architecture, enhancing our approach for non-uniform sampling recovery. Our numerical results show that both our rule-based and deep learning methods significantly outperform traditional interpolation techniques and current state-of-the-art approaches in terms of performance.","sentences":["Acquiring downlink channel state information (CSI) at the base station is vital for optimizing performance in massive Multiple input multiple output (MIMO) Frequency-Division Duplexing (FDD) systems.","While deep learning architectures have been successful in facilitating UE-side CSI feedback and gNB-side recovery, the undersampling issue prior to CSI feedback is often overlooked.","This issue, which arises from low density pilot placement in current standards, results in significant aliasing effects in outdoor channels and consequently limits CSI recovery performance.","To this end, this work introduces a new CSI upsampling framework at the gNB as a post-processing solution to address the gaps caused by undersampling.","Leveraging the physical principles of discrete Fourier transform shifting theorem and multipath reciprocity, our framework effectively uses uplink CSI to mitigate aliasing effects.","We further develop a learning-based method that integrates the proposed algorithm with the Iterative Shrinkage-Thresholding Algorithm Net (ISTA-Net) architecture, enhancing our approach for non-uniform sampling recovery.","Our numerical results show that both our rule-based and deep learning methods significantly outperform traditional interpolation techniques and current state-of-the-art approaches in terms of performance."],"url":"http://arxiv.org/abs/2403.08133v1","category":"eess.SP"}
{"created":"2024-03-12 23:33:49","title":"Guidelines for the Creation of Analysis Ready Data","abstract":"Globally, there is an increased need for guidelines to produce high-quality data outputs for analysis. There is no framework currently exists providing guidelines for a comprehensive approach in producing analysis ready data (ARD). Through critically reviewing and summarising current literature, this paper proposes such guidelines for the creation of ARD. The guidelines proposed in this paper inform ten steps in the generation of ARD: ethics, project documentation, data governance, data management, data storage, data discovery and collection, data cleaning, quality assurance, metadata, and data dictionary. These steps are illustrated through a substantive case study which aimed to create ARD for a digital spatial platform: the Australian Child and Youth Wellbeing Atlas (ACYWA).","sentences":["Globally, there is an increased need for guidelines to produce high-quality data outputs for analysis.","There is no framework currently exists providing guidelines for a comprehensive approach in producing analysis ready data (ARD).","Through critically reviewing and summarising current literature, this paper proposes such guidelines for the creation of ARD.","The guidelines proposed in this paper inform ten steps in the generation of ARD: ethics, project documentation, data governance, data management, data storage, data discovery and collection, data cleaning, quality assurance, metadata, and data dictionary.","These steps are illustrated through a substantive case study which aimed to create ARD for a digital spatial platform: the Australian Child and Youth Wellbeing Atlas (ACYWA)."],"url":"http://arxiv.org/abs/2403.08127v1","category":"cs.DB"}
{"created":"2024-03-12 23:21:09","title":"Towards Independence Criterion in Machine Unlearning of Features and Labels","abstract":"This work delves into the complexities of machine unlearning in the face of distributional shifts, particularly focusing on the challenges posed by non-uniform feature and label removal. With the advent of regulations like the GDPR emphasizing data privacy and the right to be forgotten, machine learning models face the daunting task of unlearning sensitive information without compromising their integrity or performance. Our research introduces a novel approach that leverages influence functions and principles of distributional independence to address these challenges. By proposing a comprehensive framework for machine unlearning, we aim to ensure privacy protection while maintaining model performance and adaptability across varying distributions. Our method not only facilitates efficient data removal but also dynamically adjusts the model to preserve its generalization capabilities. Through extensive experimentation, we demonstrate the efficacy of our approach in scenarios characterized by significant distributional shifts, making substantial contributions to the field of machine unlearning. This research paves the way for developing more resilient and adaptable unlearning techniques, ensuring models remain robust and accurate in the dynamic landscape of data privacy and machine learning.","sentences":["This work delves into the complexities of machine unlearning in the face of distributional shifts, particularly focusing on the challenges posed by non-uniform feature and label removal.","With the advent of regulations like the GDPR emphasizing data privacy and the right to be forgotten, machine learning models face the daunting task of unlearning sensitive information without compromising their integrity or performance.","Our research introduces a novel approach that leverages influence functions and principles of distributional independence to address these challenges.","By proposing a comprehensive framework for machine unlearning, we aim to ensure privacy protection while maintaining model performance and adaptability across varying distributions.","Our method not only facilitates efficient data removal but also dynamically adjusts the model to preserve its generalization capabilities.","Through extensive experimentation, we demonstrate the efficacy of our approach in scenarios characterized by significant distributional shifts, making substantial contributions to the field of machine unlearning.","This research paves the way for developing more resilient and adaptable unlearning techniques, ensuring models remain robust and accurate in the dynamic landscape of data privacy and machine learning."],"url":"http://arxiv.org/abs/2403.08124v1","category":"cs.LG"}
{"created":"2024-03-12 22:57:53","title":"Characterising harmful data sources when constructing multi-fidelity surrogate models","abstract":"Surrogate modelling techniques have seen growing attention in recent years when applied to both modelling and optimisation of industrial design problems. These techniques are highly relevant when assessing the performance of a particular design carries a high cost, as the overall cost can be mitigated via the construction of a model to be queried in lieu of the available high-cost source. The construction of these models can sometimes employ other sources of information which are both cheaper and less accurate. The existence of these sources however poses the question of which sources should be used when constructing a model. Recent studies have attempted to characterise harmful data sources to guide practitioners in choosing when to ignore a certain source. These studies have done so in a synthetic setting, characterising sources using a large amount of data that is not available in practice. Some of these studies have also been shown to potentially suffer from bias in the benchmarks used in the analysis. In this study, we present a characterisation of harmful low-fidelity sources using only the limited data available to train a surrogate model. We employ recently developed benchmark filtering techniques to conduct a bias-free assessment, providing objectively varied benchmark suites of different sizes for future research. Analysing one of these benchmark suites with the technique known as Instance Space Analysis, we provide an intuitive visualisation of when a low-fidelity source should be used and use this analysis to provide guidelines that can be used in an applied industrial setting.","sentences":["Surrogate modelling techniques have seen growing attention in recent years when applied to both modelling and optimisation of industrial design problems.","These techniques are highly relevant when assessing the performance of a particular design carries a high cost, as the overall cost can be mitigated via the construction of a model to be queried in lieu of the available high-cost source.","The construction of these models can sometimes employ other sources of information which are both cheaper and less accurate.","The existence of these sources however poses the question of which sources should be used when constructing a model.","Recent studies have attempted to characterise harmful data sources to guide practitioners in choosing when to ignore a certain source.","These studies have done so in a synthetic setting, characterising sources using a large amount of data that is not available in practice.","Some of these studies have also been shown to potentially suffer from bias in the benchmarks used in the analysis.","In this study, we present a characterisation of harmful low-fidelity sources using only the limited data available to train a surrogate model.","We employ recently developed benchmark filtering techniques to conduct a bias-free assessment, providing objectively varied benchmark suites of different sizes for future research.","Analysing one of these benchmark suites with the technique known as Instance Space Analysis, we provide an intuitive visualisation of when a low-fidelity source should be used and use this analysis to provide guidelines that can be used in an applied industrial setting."],"url":"http://arxiv.org/abs/2403.08118v1","category":"stat.ME"}
{"created":"2024-03-12 22:53:32","title":"Legally Binding but Unfair? Towards Assessing Fairness of Privacy Policies","abstract":"Privacy policies are expected to inform data subjects about their data protection rights. They should explain the data controller's data management practices, and make facts such as retention periods or data transfers to third parties transparent. Privacy policies only fulfill their purpose, if they are correctly perceived, interpreted, understood, and trusted by the data subject. Amongst others, this requires that a privacy policy is written in a fair way, e.g., it does not use polarizing terms, does not require a certain education, or does not assume a particular social background. In this work-in-progress paper, we outline our approach to assessing fairness in privacy policies. To this end, we identify from fundamental legal sources and fairness research, how the dimensions informational fairness, representational fairness and ethics/morality are related to privacy policies. We propose options to automatically assess policies in these fairness dimensions, based on text statistics, linguistic methods and artificial intelligence. Finally, we conduct initial experiments with German privacy policies to provide evidence that our approach is applicable. Our experiments indicate that there are indeed issues in all three dimensions of fairness. For example, our approach finds out if a policy discriminates against individuals with impaired reading skills or certain demographics, and identifies questionable ethics. This is important, as future privacy policies may be used in a corpus for legal artificial intelligence models.","sentences":["Privacy policies are expected to inform data subjects about their data protection rights.","They should explain the data controller's data management practices, and make facts such as retention periods or data transfers to third parties transparent.","Privacy policies only fulfill their purpose, if they are correctly perceived, interpreted, understood, and trusted by the data subject.","Amongst others, this requires that a privacy policy is written in a fair way, e.g., it does not use polarizing terms, does not require a certain education, or does not assume a particular social background.","In this work-in-progress paper, we outline our approach to assessing fairness in privacy policies.","To this end, we identify from fundamental legal sources and fairness research, how the dimensions informational fairness, representational fairness and ethics/morality are related to privacy policies.","We propose options to automatically assess policies in these fairness dimensions, based on text statistics, linguistic methods and artificial intelligence.","Finally, we conduct initial experiments with German privacy policies to provide evidence that our approach is applicable.","Our experiments indicate that there are indeed issues in all three dimensions of fairness.","For example, our approach finds out if a policy discriminates against individuals with impaired reading skills or certain demographics, and identifies questionable ethics.","This is important, as future privacy policies may be used in a corpus for legal artificial intelligence models."],"url":"http://arxiv.org/abs/2403.08115v1","category":"cs.CY"}
{"created":"2024-03-12 22:36:27","title":"AI-Assisted Causal Pathway Diagram for Human-Centered Design","abstract":"This paper explores the integration of causal pathway diagrams (CPD) into human-centered design (HCD), investigating how these diagrams can enhance the early stages of the design process. A dedicated CPD plugin for the online collaborative whiteboard platform Miro was developed to streamline diagram creation and offer real-time AI-driven guidance. Through a user study with designers (N=20), we found that CPD's branching and its emphasis on causal connections supported both divergent and convergent processes during design. CPD can also facilitate communication among stakeholders. Additionally, we found our plugin significantly reduces designers' cognitive workload and increases their creativity during brainstorming, highlighting the implications of AI-assisted tools in supporting creative work and evidence-based designs.","sentences":["This paper explores the integration of causal pathway diagrams (CPD) into human-centered design (HCD), investigating how these diagrams can enhance the early stages of the design process.","A dedicated CPD plugin for the online collaborative whiteboard platform Miro was developed to streamline diagram creation and offer real-time AI-driven guidance.","Through a user study with designers (N=20), we found that CPD's branching and its emphasis on causal connections supported both divergent and convergent processes during design.","CPD can also facilitate communication among stakeholders.","Additionally, we found our plugin significantly reduces designers' cognitive workload and increases their creativity during brainstorming, highlighting the implications of AI-assisted tools in supporting creative work and evidence-based designs."],"url":"http://arxiv.org/abs/2403.08111v1","category":"cs.HC"}
{"created":"2024-03-12 22:26:45","title":"V-PRISM: Probabilistic Mapping of Unknown Tabletop Scenes","abstract":"The ability to construct concise scene representations from sensor input is central to the field of robotics. This paper addresses the problem of robustly creating a 3D representation of a tabletop scene from a segmented RGB-D image. These representations are then critical for a range of downstream manipulation tasks. Many previous attempts to tackle this problem do not capture accurate uncertainty, which is required to subsequently produce safe motion plans. In this paper, we cast the representation of 3D tabletop scenes as a multi-class classification problem. To tackle this, we introduce \\ourmethod{}, a framework and method for robustly creating probabilistic 3D segmentation maps of tabletop scenes. Our maps contain both occupancy estimates, segmentation information, and principled uncertainty measures. We evaluate the robustness of our method in (1) procedurally generated scenes using open-source object datasets, and (2) real-world tabletop data collected from a depth camera. Our experiments show that our approach outperforms alternative continuous reconstruction approaches that do not explicitly reason about objects in a multi-class formulation.","sentences":["The ability to construct concise scene representations from sensor input is central to the field of robotics.","This paper addresses the problem of robustly creating a 3D representation of a tabletop scene from a segmented RGB-D image.","These representations are then critical for a range of downstream manipulation tasks.","Many previous attempts to tackle this problem do not capture accurate uncertainty, which is required to subsequently produce safe motion plans.","In this paper, we cast the representation of 3D tabletop scenes as a multi-class classification problem.","To tackle this, we introduce \\ourmethod{}, a framework and method for robustly creating probabilistic 3D segmentation maps of tabletop scenes.","Our maps contain both occupancy estimates, segmentation information, and principled uncertainty measures.","We evaluate the robustness of our method in (1) procedurally generated scenes using open-source object datasets, and (2) real-world tabletop data collected from a depth camera.","Our experiments show that our approach outperforms alternative continuous reconstruction approaches that do not explicitly reason about objects in a multi-class formulation."],"url":"http://arxiv.org/abs/2403.08106v1","category":"cs.RO"}
{"created":"2024-03-12 22:23:11","title":"Minimal reconstructions of a coloring","abstract":"A coloring on a finite or countable set $X$ is a function $\\varphi: [X]^{2} \\to \\{0,1\\}$, where $[X]^{2}$ is the collection of unordered pairs of $X$. The collection of homogeneous sets for $\\varphi$, denoted by $Hom(\\varphi)$, consist of all $H \\subseteq X$ such that $\\varphi$ is constant on $[H]^2$; clearly, $Hom(\\varphi) = Hom(1-\\varphi)$. A coloring $\\varphi$ is \\textit{reconstructible} up to complementation from its homogeneous sets if, for any coloring $\\psi$ on $X$ such that $Hom(\\varphi) = Hom(\\psi)$, either $\\psi = \\varphi$ or $\\psi = 1-\\varphi$. By $\\mathcal{R}$ we denote the collection of all colorings reconstructible from their homogeneous sets. Let $\\varphi$ and $\\psi$ be colorings on $X$, and set \\[ D(\\varphi, \\psi) = \\{ \\{x,y\\} \\in [X]^2: \\; \\psi\\{x,y\\} \\neq \\varphi\\{x,y\\}\\}. \\] If $\\varphi\\not\\in \\mathcal{R}$, let \\[ r(\\varphi) = \\min\\{|D(\\varphi, \\psi)|: \\; Hom(\\varphi) = Hom(\\psi), \\, \\psi \\neq \\varphi, \\, \\psi \\neq 1-\\varphi\\}. \\] A coloring $\\psi$ such that $Hom(\\varphi)=Hom(\\psi)$, $\\varphi\\neq \\psi$ and $1-\\varphi\\neq \\psi$ is called a {\\em non trivial reconstruction} of $\\varphi$. If, in addition, $r(\\varphi) =|D(\\varphi, \\psi)|$, we call $\\psi$ a {\\em minimal reconstruction} of $\\varphi$. The purpose of this article is to study the minimal reconstructions of a coloring. We show that, for large enough $X$, $r(\\varphi)$ can only takes the values $1$ or $4$.","sentences":["A coloring on a finite or countable set $X$ is a function $\\varphi: [X]^{2} \\to \\{0,1\\}$, where $[X]^{2}$ is the collection of unordered pairs of $X$. The collection of homogeneous sets for $\\varphi$, denoted by $Hom(\\varphi)$, consist of all $H \\subseteq X$ such that $\\varphi$ is constant on $[H]^2$; clearly, $Hom(\\varphi) =","Hom(1-\\varphi)$. A coloring $\\varphi$ is \\textit{reconstructible} up to complementation from its homogeneous sets if, for any coloring $\\psi$ on $X$ such that $Hom(\\varphi) = Hom(\\psi)$, either $\\psi = \\varphi$ or $\\psi = 1-\\varphi$. By $\\mathcal{R}$ we denote the collection of all colorings reconstructible from their homogeneous sets.","Let $\\varphi$ and $\\psi$ be colorings on $X$, and set \\[ D(\\varphi, \\psi)","= \\{ \\{x,y\\} \\in","[X]^2: \\; \\psi\\{x,y\\} \\neq \\varphi\\{x,y\\}\\}.","\\]","If $\\varphi\\not\\in \\mathcal{R}$, let \\[ r(\\varphi) = \\min\\{|D(\\varphi, \\psi)|: \\; Hom(\\varphi) = Hom(\\psi), \\, \\psi \\neq \\varphi, \\, \\psi \\neq 1-\\varphi\\}.","\\]","A coloring $\\psi$ such that $Hom(\\varphi)=Hom(\\psi)$, $\\varphi\\neq \\psi$ and $1-\\varphi\\neq \\psi$ is called a {\\em non trivial reconstruction} of $\\varphi$. If, in addition, $r(\\varphi) =|D(\\varphi, \\psi)|$, we call $\\psi$ a {\\em minimal reconstruction} of $\\varphi$. The purpose of this article is to study the minimal reconstructions of a coloring.","We show that, for large enough $X$, $r(\\varphi)$ can only takes the values $1$ or $4$."],"url":"http://arxiv.org/abs/2403.08104v1","category":"math.CO"}
{"created":"2024-03-12 22:23:08","title":"Contextual Clarity: Generating Sentences with Transformer Models using Context-Reverso Data","abstract":"In the age of information abundance, the ability to provide users with contextually relevant and concise information is crucial. Keyword in Context (KIC) generation is a task that plays a vital role in and generation applications, such as search engines, personal assistants, and content summarization. In this paper, we present a novel approach to generating unambiguous and brief sentence-contexts for given keywords using the T5 transformer model, leveraging data obtained from the Context-Reverso API. The code is available at https://github.com/Rusamus/word2context/tree/main .","sentences":["In the age of information abundance, the ability to provide users with contextually relevant and concise information is crucial.","Keyword in Context (KIC) generation is a task that plays a vital role in and generation applications, such as search engines, personal assistants, and content summarization.","In this paper, we present a novel approach to generating unambiguous and brief sentence-contexts for given keywords using the T5 transformer model, leveraging data obtained from the Context-Reverso API.","The code is available at https://github.com/Rusamus/word2context/tree/main ."],"url":"http://arxiv.org/abs/2403.08103v1","category":"cs.CL"}
{"created":"2024-03-12 22:21:50","title":"A systematic study of projection biases in the Weak Lensing analysis of cosmic shear and the combination of galaxy clustering and galaxy-galaxy lensing","abstract":"This paper presents the results of a systematic study of projection biases in the Weak Lensing analysis of cosmic shear and the combination of galaxy clustering and galaxy-galaxy lensing using data collected during the first-year of running the Dark Energy Survey experiment. The study uses $\\Lambda$CDM as the cosmological model and two-point correlation functions for the WL analysis. The results in this paper show that, independent of the WL analysis, projection biases of more than $1\\sigma$ exist, and are a function of the position of the true values of the parameters $h$, $n_{s}$, $\\Omega_{b}$, and $\\Omega_{\\nu}h^{2}$ with respect to their prior probabilities. For cosmic shear, and the combination of galaxy clustering and galaxy-galaxy lensing, this study shows that the coverage probability of the $68.27\\%$ credible intervals ranges from as high as $93\\%$ to as low as $16\\%$, and that these credible intervals are inflated, on average, by $29\\%$ for cosmic shear and $20\\%$ for the combination of galaxy clustering and galaxy-galaxy lensing. The results of the study also show that, in six out of nine tested cases, the reduction in error bars obtained by transforming credible intervals into confidence intervals is equivalent to an increase in the amount of data by a factor of three.","sentences":["This paper presents the results of a systematic study of projection biases in the Weak Lensing analysis of cosmic shear and the combination of galaxy clustering and galaxy-galaxy lensing using data collected during the first-year of running the Dark Energy Survey experiment.","The study uses $\\Lambda$CDM as the cosmological model and two-point correlation functions for the WL analysis.","The results in this paper show that, independent of the WL analysis, projection biases of more than $1\\sigma$ exist, and are a function of the position of the true values of the parameters $h$, $n_{s}$, $\\Omega_{b}$, and $\\Omega_{\\nu}h^{2}$ with respect to their prior probabilities.","For cosmic shear, and the combination of galaxy clustering and galaxy-galaxy lensing, this study shows that the coverage probability of the $68.27\\%$ credible intervals ranges from as high as $93\\%$ to as low as $16\\%$, and that these credible intervals are inflated, on average, by $29\\%$ for cosmic shear and $20\\%$ for the combination of galaxy clustering and galaxy-galaxy lensing.","The results of the study also show that, in six out of nine tested cases, the reduction in error bars obtained by transforming credible intervals into confidence intervals is equivalent to an increase in the amount of data by a factor of three."],"url":"http://arxiv.org/abs/2403.08101v1","category":"astro-ph.CO"}
{"created":"2024-03-12 21:58:53","title":"Controllability of shapes through Landmark Manifolds","abstract":"Landmark manifolds consist of distinct points that are often used to describe shapes. We show that in the Euclidean space, we can preselect two vector fields such that their flows will be able to take any collection of landmarks to another, regardless of the number of landmarks we choose.","sentences":["Landmark manifolds consist of distinct points that are often used to describe shapes.","We show that in the Euclidean space, we can preselect two vector fields such that their flows will be able to take any collection of landmarks to another, regardless of the number of landmarks we choose."],"url":"http://arxiv.org/abs/2403.08090v1","category":"math.DG"}
{"created":"2024-03-12 21:15:38","title":"Mechanics of Next Token Prediction with Self-Attention","abstract":"Transformer-based language models are trained on large datasets to predict the next token given an input sequence. Despite this simple training objective, they have led to revolutionary advances in natural language processing. Underlying this success is the self-attention mechanism. In this work, we ask: $\\textit{What}$ $\\textit{does}$ $\\textit{a}$ $\\textit{single}$ $\\textit{self-attention}$ $\\textit{layer}$ $\\textit{learn}$ $\\textit{from}$ $\\textit{next-token}$ $\\textit{prediction?}$ We show that training self-attention with gradient descent learns an automaton which generates the next token in two distinct steps: $\\textbf{(1)}$ $\\textbf{Hard}$ $\\textbf{retrieval:}$ Given input sequence, self-attention precisely selects the $\\textit{high-priority}$ $\\textit{input}$ $\\textit{tokens}$ associated with the last input token. $\\textbf{(2)}$ $\\textbf{Soft}$ $\\textbf{composition:}$ It then creates a convex combination of the high-priority tokens from which the next token can be sampled. Under suitable conditions, we rigorously characterize these mechanics through a directed graph over tokens extracted from the training data. We prove that gradient descent implicitly discovers the strongly-connected components (SCC) of this graph and self-attention learns to retrieve the tokens that belong to the highest-priority SCC available in the context window. Our theory relies on decomposing the model weights into a directional component and a finite component that correspond to hard retrieval and soft composition steps respectively. This also formalizes a related implicit bias formula conjectured in [Tarzanagh et al. 2023]. We hope that these findings shed light on how self-attention processes sequential data and pave the path toward demystifying more complex architectures.","sentences":["Transformer-based language models are trained on large datasets to predict the next token given an input sequence.","Despite this simple training objective, they have led to revolutionary advances in natural language processing.","Underlying this success is the self-attention mechanism.","In this work, we ask: $\\textit{What}$ $\\textit{does}$ $\\textit{a}$ $\\textit{single}$ $\\textit{self-attention}$ $\\textit{layer}$ $\\textit{learn}$ $\\textit{from}$ $\\textit{next-token}$ $\\textit{prediction?}$ We show that training self-attention with gradient descent learns an automaton which generates the next token in two distinct steps: $\\textbf{(1)}$ $\\textbf{Hard}$ $\\textbf{retrieval:}$ Given input sequence, self-attention precisely selects the $\\textit{high-priority}$ $\\textit{input}$ $\\textit{tokens}$ associated with the last input token.","$\\textbf{(2)}$ $\\textbf{Soft}$ $\\textbf{composition:}$ It then creates a convex combination of the high-priority tokens from which the next token can be sampled.","Under suitable conditions, we rigorously characterize these mechanics through a directed graph over tokens extracted from the training data.","We prove that gradient descent implicitly discovers the strongly-connected components (SCC) of this graph and self-attention learns to retrieve the tokens that belong to the highest-priority SCC available in the context window.","Our theory relies on decomposing the model weights into a directional component and a finite component that correspond to hard retrieval and soft composition steps respectively.","This also formalizes a related implicit bias formula conjectured in [Tarzanagh et al. 2023].","We hope that these findings shed light on how self-attention processes sequential data and pave the path toward demystifying more complex architectures."],"url":"http://arxiv.org/abs/2403.08081v1","category":"cs.LG"}
{"created":"2024-03-12 21:06:19","title":"A Multimodal Intermediate Fusion Network with Manifold Learning for Stress Detection","abstract":"Multimodal deep learning methods capture synergistic features from multiple modalities and have the potential to improve accuracy for stress detection compared to unimodal methods. However, this accuracy gain typically comes from high computational cost due to the high-dimensional feature spaces, especially for intermediate fusion. Dimensionality reduction is one way to optimize multimodal learning by simplifying data and making the features more amenable to processing and analysis, thereby reducing computational complexity. This paper introduces an intermediate multimodal fusion network with manifold learning-based dimensionality reduction. The multimodal network generates independent representations from biometric signals and facial landmarks through 1D-CNN and 2D-CNN. Finally, these features are fused and fed to another 1D-CNN layer, followed by a fully connected dense layer. We compared various dimensionality reduction techniques for different variations of unimodal and multimodal networks. We observe that the intermediate-level fusion with the Multi-Dimensional Scaling (MDS) manifold method showed promising results with an accuracy of 96.00\\% in a Leave-One-Subject-Out Cross-Validation (LOSO-CV) paradigm over other dimensional reduction methods. MDS had the highest computational cost among manifold learning methods. However, while outperforming other networks, it managed to reduce the computational cost of the proposed networks by 25\\% when compared to six well-known conventional feature selection methods used in the preprocessing step.","sentences":["Multimodal deep learning methods capture synergistic features from multiple modalities and have the potential to improve accuracy for stress detection compared to unimodal methods.","However, this accuracy gain typically comes from high computational cost due to the high-dimensional feature spaces, especially for intermediate fusion.","Dimensionality reduction is one way to optimize multimodal learning by simplifying data and making the features more amenable to processing and analysis, thereby reducing computational complexity.","This paper introduces an intermediate multimodal fusion network with manifold learning-based dimensionality reduction.","The multimodal network generates independent representations from biometric signals and facial landmarks through 1D-CNN and 2D-CNN.","Finally, these features are fused and fed to another 1D-CNN layer, followed by a fully connected dense layer.","We compared various dimensionality reduction techniques for different variations of unimodal and multimodal networks.","We observe that the intermediate-level fusion with the Multi-Dimensional Scaling (MDS) manifold method showed promising results with an accuracy of 96.00\\% in a Leave-One-Subject-Out Cross-Validation (LOSO-CV) paradigm over other dimensional reduction methods.","MDS had the highest computational cost among manifold learning methods.","However, while outperforming other networks, it managed to reduce the computational cost of the proposed networks by 25\\% when compared to six well-known conventional feature selection methods used in the preprocessing step."],"url":"http://arxiv.org/abs/2403.08077v1","category":"cs.CV"}
{"created":"2024-03-12 20:11:38","title":"FluoroSAM: A Language-aligned Foundation Model for X-ray Image Segmentation","abstract":"Automated X-ray image segmentation would accelerate research and development in diagnostic and interventional precision medicine. Prior efforts have contributed task-specific models capable of solving specific image analysis problems, but the utility of these models is restricted to their particular task domain, and expanding to broader use requires additional data, labels, and retraining efforts. Recently, foundation models (FMs) -- machine learning models trained on large amounts of highly variable data thus enabling broad applicability -- have emerged as promising tools for automated image analysis. Existing FMs for medical image analysis focus on scenarios and modalities where objects are clearly defined by visually apparent boundaries, such as surgical tool segmentation in endoscopy. X-ray imaging, by contrast, does not generally offer such clearly delineated boundaries or structure priors. During X-ray image formation, complex 3D structures are projected in transmission onto the imaging plane, resulting in overlapping features of varying opacity and shape. To pave the way toward an FM for comprehensive and automated analysis of arbitrary medical X-ray images, we develop FluoroSAM, a language-aligned variant of the Segment-Anything Model, trained from scratch on 1.6M synthetic X-ray images. FluoroSAM is trained on data including masks for 128 organ types and 464 non-anatomical objects, such as tools and implants. In real X-ray images of cadaveric specimens, FluoroSAM is able to segment bony anatomical structures based on text-only prompting with 0.51 and 0.79 DICE with point-based refinement, outperforming competing SAM variants for all structures. FluoroSAM is also capable of zero-shot generalization to segmenting classes beyond the training set thanks to its language alignment, which we demonstrate for full lung segmentation on real chest X-rays.","sentences":["Automated X-ray image segmentation would accelerate research and development in diagnostic and interventional precision medicine.","Prior efforts have contributed task-specific models capable of solving specific image analysis problems, but the utility of these models is restricted to their particular task domain, and expanding to broader use requires additional data, labels, and retraining efforts.","Recently, foundation models (FMs) -- machine learning models trained on large amounts of highly variable data thus enabling broad applicability -- have emerged as promising tools for automated image analysis.","Existing FMs for medical image analysis focus on scenarios and modalities where objects are clearly defined by visually apparent boundaries, such as surgical tool segmentation in endoscopy.","X-ray imaging, by contrast, does not generally offer such clearly delineated boundaries or structure priors.","During X-ray image formation, complex 3D structures are projected in transmission onto the imaging plane, resulting in overlapping features of varying opacity and shape.","To pave the way toward an FM for comprehensive and automated analysis of arbitrary medical X-ray images, we develop FluoroSAM, a language-aligned variant of the Segment-Anything Model, trained from scratch on 1.6M synthetic X-ray images.","FluoroSAM is trained on data including masks for 128 organ types and 464 non-anatomical objects, such as tools and implants.","In real X-ray images of cadaveric specimens, FluoroSAM is able to segment bony anatomical structures based on text-only prompting with 0.51 and 0.79 DICE with point-based refinement, outperforming competing SAM variants for all structures.","FluoroSAM is also capable of zero-shot generalization to segmenting classes beyond the training set thanks to its language alignment, which we demonstrate for full lung segmentation on real chest X-rays."],"url":"http://arxiv.org/abs/2403.08059v1","category":"cs.CV"}
{"created":"2024-03-12 20:05:14","title":"MineXR: Mining Personalized Extended Reality Interfaces","abstract":"Extended Reality (XR) interfaces offer engaging user experiences, but their effective design requires a nuanced understanding of user behavior and preferences. This knowledge is challenging to obtain without the widespread adoption of XR devices. We introduce MineXR, a design mining workflow and data analysis platform for collecting and analyzing personalized XR user interaction and experience data. MineXR enables elicitation of personalized interfaces from participants of a data collection: for any particular context, participants create interface elements using application screenshots from their own smartphone, place them in the environment, and simultaneously preview the resulting XR layout on a headset. Using MineXR, we contribute a dataset of personalized XR interfaces collected from 31 participants, consisting of 695 XR widgets created from 178 unique applications. We provide insights for XR widget functionalities, categories, clusters, UI element types, and placement. Our open-source tools and data support researchers and designers in developing future XR interfaces.","sentences":["Extended Reality (XR) interfaces offer engaging user experiences, but their effective design requires a nuanced understanding of user behavior and preferences.","This knowledge is challenging to obtain without the widespread adoption of XR devices.","We introduce MineXR, a design mining workflow and data analysis platform for collecting and analyzing personalized XR user interaction and experience data.","MineXR enables elicitation of personalized interfaces from participants of a data collection: for any particular context, participants create interface elements using application screenshots from their own smartphone, place them in the environment, and simultaneously preview the resulting XR layout on a headset.","Using MineXR, we contribute a dataset of personalized XR interfaces collected from 31 participants, consisting of 695 XR widgets created from 178 unique applications.","We provide insights for XR widget functionalities, categories, clusters, UI element types, and placement.","Our open-source tools and data support researchers and designers in developing future XR interfaces."],"url":"http://arxiv.org/abs/2403.08057v1","category":"cs.HC"}
{"created":"2024-03-12 19:46:59","title":"TutoAI: A Cross-domain Framework for AI-assisted Mixed-media Tutorial Creation on Physical Tasks","abstract":"Mixed-media tutorials, which integrate videos, images, text, and diagrams to teach procedural skills, offer more browsable alternatives than timeline-based videos. However, manually creating such tutorials is tedious, and existing automated solutions are often restricted to a particular domain. While AI models hold promise, it is unclear how to effectively harness their powers, given the multi-modal data involved and the vast landscape of models. We present TutoAI, a cross-domain framework for AI-assisted mixed-media tutorial creation on physical tasks. First, we distill common tutorial components by surveying existing work; then, we present an approach to identify, assemble, and evaluate AI models for component extraction; finally, we propose guidelines for designing user interfaces (UI) that support tutorial creation based on AI-generated components. We show that TutoAI has achieved higher or similar quality compared to a baseline model in preliminary user studies.","sentences":["Mixed-media tutorials, which integrate videos, images, text, and diagrams to teach procedural skills, offer more browsable alternatives than timeline-based videos.","However, manually creating such tutorials is tedious, and existing automated solutions are often restricted to a particular domain.","While AI models hold promise, it is unclear how to effectively harness their powers, given the multi-modal data involved and the vast landscape of models.","We present TutoAI, a cross-domain framework for AI-assisted mixed-media tutorial creation on physical tasks.","First, we distill common tutorial components by surveying existing work; then, we present an approach to identify, assemble, and evaluate AI models for component extraction; finally, we propose guidelines for designing user interfaces (UI) that support tutorial creation based on AI-generated components.","We show that TutoAI has achieved higher or similar quality compared to a baseline model in preliminary user studies."],"url":"http://arxiv.org/abs/2403.08049v1","category":"cs.HC"}
{"created":"2024-03-12 19:15:20","title":"A Review of Cybersecurity Incidents in the Food and Agriculture Sector","abstract":"The increasing utilization of emerging technologies in the Food & Agriculture (FA) sector has heightened the need for security to minimize cyber risks. Considering this aspect, this manuscript reviews disclosed and documented cybersecurity incidents in the FA sector. For this purpose, thirty cybersecurity incidents were identified, which took place between July 2011 and April 2023. The details of these incidents are reported from multiple sources such as: the private industry and flash notifications generated by the Federal Bureau of Investigation (FBI), internal reports from the affected organizations, and available media sources. Considering the available information, a brief description of the security threat, ransom amount, and impact on the organization are discussed for each incident. This review reports an increased frequency of cybersecurity threats to the FA sector. To minimize these cyber risks, popular cybersecurity frameworks and recent agriculture-specific cybersecurity solutions are also discussed. Further, the need for AI assurance in the FA sector is explained, and the Farmer-Centered AI (FCAI) framework is proposed. The main aim of the FCAI framework is to support farmers in decision-making for agricultural production, by incorporating AI assurance. Lastly, the effects of the reported cyber incidents on other critical infrastructures, food security, and the economy are noted, along with specifying the open issues for future development.","sentences":["The increasing utilization of emerging technologies in the Food & Agriculture (FA) sector has heightened the need for security to minimize cyber risks.","Considering this aspect, this manuscript reviews disclosed and documented cybersecurity incidents in the FA sector.","For this purpose, thirty cybersecurity incidents were identified, which took place between July 2011 and April 2023.","The details of these incidents are reported from multiple sources such as: the private industry and flash notifications generated by the Federal Bureau of Investigation (FBI), internal reports from the affected organizations, and available media sources.","Considering the available information, a brief description of the security threat, ransom amount, and impact on the organization are discussed for each incident.","This review reports an increased frequency of cybersecurity threats to the FA sector.","To minimize these cyber risks, popular cybersecurity frameworks and recent agriculture-specific cybersecurity solutions are also discussed.","Further, the need for AI assurance in the FA sector is explained, and the Farmer-Centered AI (FCAI) framework is proposed.","The main aim of the FCAI framework is to support farmers in decision-making for agricultural production, by incorporating AI assurance.","Lastly, the effects of the reported cyber incidents on other critical infrastructures, food security, and the economy are noted, along with specifying the open issues for future development."],"url":"http://arxiv.org/abs/2403.08036v1","category":"cs.CR"}
{"created":"2024-03-12 19:12:28","title":"Harnessing Artificial Intelligence to Combat Online Hate: Exploring the Challenges and Opportunities of Large Language Models in Hate Speech Detection","abstract":"Large language models (LLMs) excel in many diverse applications beyond language generation, e.g., translation, summarization, and sentiment analysis. One intriguing application is in text classification. This becomes pertinent in the realm of identifying hateful or toxic speech -- a domain fraught with challenges and ethical dilemmas. In our study, we have two objectives: firstly, to offer a literature review revolving around LLMs as classifiers, emphasizing their role in detecting and classifying hateful or toxic content. Subsequently, we explore the efficacy of several LLMs in classifying hate speech: identifying which LLMs excel in this task as well as their underlying attributes and training. Providing insight into the factors that contribute to an LLM proficiency (or lack thereof) in discerning hateful content. By combining a comprehensive literature review with an empirical analysis, our paper strives to shed light on the capabilities and constraints of LLMs in the crucial domain of hate speech detection.","sentences":["Large language models (LLMs) excel in many diverse applications beyond language generation, e.g., translation, summarization, and sentiment analysis.","One intriguing application is in text classification.","This becomes pertinent in the realm of identifying hateful or toxic speech -- a domain fraught with challenges and ethical dilemmas.","In our study, we have two objectives: firstly, to offer a literature review revolving around LLMs as classifiers, emphasizing their role in detecting and classifying hateful or toxic content.","Subsequently, we explore the efficacy of several LLMs in classifying hate speech: identifying which LLMs excel in this task as well as their underlying attributes and training.","Providing insight into the factors that contribute to an LLM proficiency (or lack thereof) in discerning hateful content.","By combining a comprehensive literature review with an empirical analysis, our paper strives to shed light on the capabilities and constraints of LLMs in the crucial domain of hate speech detection."],"url":"http://arxiv.org/abs/2403.08035v1","category":"cs.CL"}
{"created":"2024-03-12 19:06:23","title":"LG-Traj: LLM Guided Pedestrian Trajectory Prediction","abstract":"Accurate pedestrian trajectory prediction is crucial for various applications, and it requires a deep understanding of pedestrian motion patterns in dynamic environments. However, existing pedestrian trajectory prediction methods still need more exploration to fully leverage these motion patterns. This paper investigates the possibilities of using Large Language Models (LLMs) to improve pedestrian trajectory prediction tasks by inducing motion cues. We introduce LG-Traj, a novel approach incorporating LLMs to generate motion cues present in pedestrian past/observed trajectories. Our approach also incorporates motion cues present in pedestrian future trajectories by clustering future trajectories of training data using a mixture of Gaussians. These motion cues, along with pedestrian coordinates, facilitate a better understanding of the underlying representation. Furthermore, we utilize singular value decomposition to augment the observed trajectories, incorporating them into the model learning process to further enhance representation learning. Our method employs a transformer-based architecture comprising a motion encoder to model motion patterns and a social decoder to capture social interactions among pedestrians. We demonstrate the effectiveness of our approach on popular pedestrian trajectory prediction benchmarks, namely ETH-UCY and SDD, and present various ablation experiments to validate our approach.","sentences":["Accurate pedestrian trajectory prediction is crucial for various applications, and it requires a deep understanding of pedestrian motion patterns in dynamic environments.","However, existing pedestrian trajectory prediction methods still need more exploration to fully leverage these motion patterns.","This paper investigates the possibilities of using Large Language Models (LLMs) to improve pedestrian trajectory prediction tasks by inducing motion cues.","We introduce LG-Traj, a novel approach incorporating LLMs to generate motion cues present in pedestrian past/observed trajectories.","Our approach also incorporates motion cues present in pedestrian future trajectories by clustering future trajectories of training data using a mixture of Gaussians.","These motion cues, along with pedestrian coordinates, facilitate a better understanding of the underlying representation.","Furthermore, we utilize singular value decomposition to augment the observed trajectories, incorporating them into the model learning process to further enhance representation learning.","Our method employs a transformer-based architecture comprising a motion encoder to model motion patterns and a social decoder to capture social interactions among pedestrians.","We demonstrate the effectiveness of our approach on popular pedestrian trajectory prediction benchmarks, namely ETH-UCY and SDD, and present various ablation experiments to validate our approach."],"url":"http://arxiv.org/abs/2403.08032v1","category":"cs.CV"}
{"created":"2024-03-12 18:55:23","title":"McCatch: Scalable Microcluster Detection in Dimensional and Nondimensional Datasets","abstract":"How could we have an outlier detector that works even with nondimensional data, and ranks together both singleton microclusters ('one-off' outliers) and nonsingleton microclusters by their anomaly scores? How to obtain scores that are principled in one scalable and 'hands-off' manner? Microclusters of outliers indicate coalition or repetition in fraud activities, etc.; their identification is thus highly desirable. This paper presents McCatch: a new algorithm that detects microclusters by leveraging our proposed 'Oracle' plot (1NN Distance versus Group 1NN Distance). We study 31 real and synthetic datasets with up to 1M data elements to show that McCatch is the only method that answers both of the questions above; and, it outperforms 11 other methods, especially when the data has nonsingleton microclusters or is nondimensional. We also showcase McCatch's ability to detect meaningful microclusters in graphs, fingerprints, logs of network connections, text data, and satellite imagery. For example, it found a 30-elements microcluster of confirmed 'Denial of Service' attacks in the network logs, taking only ~3 minutes for 222K data elements on a stock desktop.","sentences":["How could we have an outlier detector that works even with nondimensional data, and ranks together both singleton microclusters ('one-off' outliers) and nonsingleton microclusters by their anomaly scores?","How to obtain scores that are principled in one scalable and 'hands-off' manner?","Microclusters of outliers indicate coalition or repetition in fraud activities, etc.; their identification is thus highly desirable.","This paper presents McCatch: a new algorithm that detects microclusters by leveraging our proposed 'Oracle' plot (1NN Distance versus Group 1NN Distance).","We study 31 real and synthetic datasets with up to 1M data elements to show that McCatch is the only method that answers both of the questions above; and, it outperforms 11 other methods, especially when the data has nonsingleton microclusters or is nondimensional.","We also showcase McCatch's ability to detect meaningful microclusters in graphs, fingerprints, logs of network connections, text data, and satellite imagery.","For example, it found a 30-elements microcluster of confirmed 'Denial of Service' attacks in the network logs, taking only ~3 minutes for 222K data elements on a stock desktop."],"url":"http://arxiv.org/abs/2403.08027v1","category":"cs.LG"}
{"created":"2024-03-12 18:28:32","title":"Red Teaming Models for Hyperspectral Image Analysis Using Explainable AI","abstract":"Remote sensing (RS) applications in the space domain demand machine learning (ML) models that are reliable, robust, and quality-assured, making red teaming a vital approach for identifying and exposing potential flaws and biases. Since both fields advance independently, there is a notable gap in integrating red teaming strategies into RS. This paper introduces a methodology for examining ML models operating on hyperspectral images within the HYPERVIEW challenge, focusing on soil parameters' estimation. We use post-hoc explanation methods from the Explainable AI (XAI) domain to critically assess the best performing model that won the HYPERVIEW challenge and served as an inspiration for the model deployed on board the INTUITION-1 hyperspectral mission. Our approach effectively red teams the model by pinpointing and validating key shortcomings, constructing a model that achieves comparable performance using just 1% of the input features and a mere up to 5% performance loss. Additionally, we propose a novel way of visualizing explanations that integrate domain-specific information about hyperspectral bands (wavelengths) and data transformations to better suit interpreting models for hyperspectral image analysis.","sentences":["Remote sensing (RS) applications in the space domain demand machine learning (ML) models that are reliable, robust, and quality-assured, making red teaming a vital approach for identifying and exposing potential flaws and biases.","Since both fields advance independently, there is a notable gap in integrating red teaming strategies into RS.","This paper introduces a methodology for examining ML models operating on hyperspectral images within the HYPERVIEW challenge, focusing on soil parameters' estimation.","We use post-hoc explanation methods from the Explainable AI (XAI) domain to critically assess the best performing model that won the HYPERVIEW challenge and served as an inspiration for the model deployed on board the INTUITION-1 hyperspectral mission.","Our approach effectively red teams the model by pinpointing and validating key shortcomings, constructing a model that achieves comparable performance using just 1% of the input features and a mere up to 5% performance loss.","Additionally, we propose a novel way of visualizing explanations that integrate domain-specific information about hyperspectral bands (wavelengths) and data transformations to better suit interpreting models for hyperspectral image analysis."],"url":"http://arxiv.org/abs/2403.08017v1","category":"cs.CV"}
{"created":"2024-03-12 18:21:20","title":"Gujarati-English Code-Switching Speech Recognition using ensemble prediction of spoken language","abstract":"An important and difficult task in code-switched speech recognition is to recognize the language, as lots of words in two languages can sound similar, especially in some accents. We focus on improving performance of end-to-end Automatic Speech Recognition models by conditioning transformer layers on language ID of words and character in the output in an per layer supervised manner. To this end, we propose two methods of introducing language specific parameters and explainability in the multi-head attention mechanism, and implement a Temporal Loss that helps maintain continuity in input alignment. Despite being unable to reduce WER significantly, our method shows promise in predicting the correct language from just spoken data. We introduce regularization in the language prediction by dropping LID in the sequence, which helps align long repeated output sequences.","sentences":["An important and difficult task in code-switched speech recognition is to recognize the language, as lots of words in two languages can sound similar, especially in some accents.","We focus on improving performance of end-to-end Automatic Speech Recognition models by conditioning transformer layers on language ID of words and character in the output in an per layer supervised manner.","To this end, we propose two methods of introducing language specific parameters and explainability in the multi-head attention mechanism, and implement a Temporal Loss that helps maintain continuity in input alignment.","Despite being unable to reduce WER significantly, our method shows promise in predicting the correct language from just spoken data.","We introduce regularization in the language prediction by dropping LID in the sequence, which helps align long repeated output sequences."],"url":"http://arxiv.org/abs/2403.08011v1","category":"cs.CL"}
{"created":"2024-03-12 18:12:50","title":"Pix2Pix-OnTheFly: Leveraging LLMs for Instruction-Guided Image Editing","abstract":"The combination of language processing and image processing keeps attracting increased interest given recent impressive advances that leverage the combined strengths of both domains of research. Among these advances, the task of editing an image on the basis solely of a natural language instruction stands out as a most challenging endeavour. While recent approaches for this task resort, in one way or other, to some form of preliminary preparation, training or fine-tuning, this paper explores a novel approach: We propose a preparation-free method that permits instruction-guided image editing on the fly. This approach is organized along three steps properly orchestrated that resort to image captioning and DDIM inversion, followed by obtaining the edit direction embedding, followed by image editing proper. While dispensing with preliminary preparation, our approach demonstrates to be effective and competitive, outperforming recent, state of the art models for this task when evaluated on the MAGICBRUSH dataset.","sentences":["The combination of language processing and image processing keeps attracting increased interest given recent impressive advances that leverage the combined strengths of both domains of research.","Among these advances, the task of editing an image on the basis solely of a natural language instruction stands out as a most challenging endeavour.","While recent approaches for this task resort, in one way or other, to some form of preliminary preparation, training or fine-tuning, this paper explores a novel approach: We propose a preparation-free method that permits instruction-guided image editing on the fly.","This approach is organized along three steps properly orchestrated that resort to image captioning and DDIM inversion, followed by obtaining the edit direction embedding, followed by image editing proper.","While dispensing with preliminary preparation, our approach demonstrates to be effective and competitive, outperforming recent, state of the art models for this task when evaluated on the MAGICBRUSH dataset."],"url":"http://arxiv.org/abs/2403.08004v1","category":"cs.CL"}
{"created":"2024-03-12 18:03:08","title":"Motifs, Phrases, and Beyond: The Modelling of Structure in Symbolic Music Generation","abstract":"Modelling musical structure is vital yet challenging for artificial intelligence systems that generate symbolic music compositions. This literature review dissects the evolution of techniques for incorporating coherent structure, from symbolic approaches to foundational and transformative deep learning methods that harness the power of computation and data across a wide variety of training paradigms. In the later stages, we review an emerging technique which we refer to as \"sub-task decomposition\" that involves decomposing music generation into separate high-level structural planning and content creation stages. Such systems incorporate some form of musical knowledge or neuro-symbolic methods by extracting melodic skeletons or structural templates to guide the generation. Progress is evident in capturing motifs and repetitions across all three eras reviewed, yet modelling the nuanced development of themes across extended compositions in the style of human composers remains difficult. We outline several key future directions to realize the synergistic benefits of combining approaches from all eras examined.","sentences":["Modelling musical structure is vital yet challenging for artificial intelligence systems that generate symbolic music compositions.","This literature review dissects the evolution of techniques for incorporating coherent structure, from symbolic approaches to foundational and transformative deep learning methods that harness the power of computation and data across a wide variety of training paradigms.","In the later stages, we review an emerging technique which we refer to as \"sub-task decomposition\" that involves decomposing music generation into separate high-level structural planning and content creation stages.","Such systems incorporate some form of musical knowledge or neuro-symbolic methods by extracting melodic skeletons or structural templates to guide the generation.","Progress is evident in capturing motifs and repetitions across all three eras reviewed, yet modelling the nuanced development of themes across extended compositions in the style of human composers remains difficult.","We outline several key future directions to realize the synergistic benefits of combining approaches from all eras examined."],"url":"http://arxiv.org/abs/2403.07995v1","category":"cs.SD"}
{"created":"2024-03-12 18:00:02","title":"Do Agents Dream of Electric Sheep?: Improving Generalization in Reinforcement Learning through Generative Learning","abstract":"The Overfitted Brain hypothesis suggests dreams happen to allow generalization in the human brain. Here, we ask if the same is true for reinforcement learning agents as well. Given limited experience in a real environment, we use imagination-based reinforcement learning to train a policy on dream-like episodes, where non-imaginative, predicted trajectories are modified through generative augmentations. Experiments on four ProcGen environments show that, compared to classic imagination and offline training on collected experience, our method can reach a higher level of generalization when dealing with sparsely rewarded environments.","sentences":["The Overfitted Brain hypothesis suggests dreams happen to allow generalization in the human brain.","Here, we ask if the same is true for reinforcement learning agents as well.","Given limited experience in a real environment, we use imagination-based reinforcement learning to train a policy on dream-like episodes, where non-imaginative, predicted trajectories are modified through generative augmentations.","Experiments on four ProcGen environments show that, compared to classic imagination and offline training on collected experience, our method can reach a higher level of generalization when dealing with sparsely rewarded environments."],"url":"http://arxiv.org/abs/2403.07979v1","category":"cs.LG"}
{"created":"2024-03-12 18:00:00","title":"Superphot+: Realtime Fitting and Classification of Supernova Light Curves","abstract":"Photometric classifications of supernova (SN) light curves have become necessary to utilize the full potential of large samples of observations obtained from wide-field photometric surveys, such as the Zwicky Transient Facility (ZTF) and the Vera C. Rubin Observatory. Here, we present a photometric classifier for SN light curves that does not rely on redshift information and still maintains comparable accuracy to redshift-dependent classifiers. Our new package, Superphot+, uses a parametric model to extract meaningful features from multiband SN light curves. We train a gradient-boosted machine with fit parameters from 6,061 ZTF SNe that pass data quality cuts and are spectroscopically classified as one of five classes: SN Ia, SN II, SN Ib/c, SN IIn, and SLSN-I. Without redshift information, our classifier yields a class-averaged F1-score of 0.61 +/- 0.02 and a total accuracy of 0.83 +/- 0.01. Including redshift information improves these metrics to 0.71 +/- 0.02 and 0.88 +/- 0.01, respectively. We assign new class probabilities to 3,558 ZTF transients that show SN-like characteristics (based on the ALeRCE Broker light curve and stamp classifiers), but lack spectroscopic classifications. Finally, we compare our predicted SN labels with those generated by the ALeRCE light curve classifier, finding that the two classifiers agree on photometric labels for 82 +/- 2% of light curves with spectroscopic labels and 72% of light curves without spectroscopic labels. Superphot+ is currently classifying ZTF SNe in real time via the ANTARES Broker, and is designed for simple adaptation to six-band Rubin light curves in the future.","sentences":["Photometric classifications of supernova (SN) light curves have become necessary to utilize the full potential of large samples of observations obtained from wide-field photometric surveys, such as the Zwicky Transient Facility (ZTF) and the Vera C. Rubin Observatory.","Here, we present a photometric classifier for SN light curves that does not rely on redshift information and still maintains comparable accuracy to redshift-dependent classifiers.","Our new package, Superphot+, uses a parametric model to extract meaningful features from multiband SN light curves.","We train a gradient-boosted machine with fit parameters from 6,061 ZTF SNe that pass data quality cuts and are spectroscopically classified as one of five classes: SN Ia, SN II, SN Ib/c, SN IIn, and SLSN-I. Without redshift information, our classifier yields a class-averaged F1-score of 0.61 +/- 0.02 and a total accuracy of 0.83 +/- 0.01.","Including redshift information improves these metrics to 0.71 +/- 0.02 and 0.88 +/- 0.01, respectively.","We assign new class probabilities to 3,558 ZTF transients that show SN-like characteristics (based on the ALeRCE Broker light curve and stamp classifiers), but lack spectroscopic classifications.","Finally, we compare our predicted SN labels with those generated by the ALeRCE light curve classifier, finding that the two classifiers agree on photometric labels for 82 +/- 2% of light curves with spectroscopic labels and 72% of light curves without spectroscopic labels.","Superphot+ is currently classifying ZTF SNe in real time via the ANTARES Broker, and is designed for simple adaptation to six-band Rubin light curves in the future."],"url":"http://arxiv.org/abs/2403.07975v1","category":"astro-ph.HE"}
{"created":"2024-03-12 17:58:04","title":"LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code","abstract":"Large Language Models (LLMs) applied to code-related applications have emerged as a prominent field, attracting significant interest from both academia and industry. However, as new and improved LLMs are developed, existing evaluation benchmarks (e.g., HumanEval, MBPP) are no longer sufficient for assessing their capabilities. In this work, we propose LiveCodeBench, a comprehensive and contamination-free evaluation of LLMs for code, which continuously collects new problems over time from contests across three competition platforms, namely LeetCode, AtCoder, and CodeForces. Notably, our benchmark also focuses on a broader range of code related capabilities, such as self-repair, code execution, and test output prediction, beyond just code generation. Currently, LiveCodeBench hosts four hundred high-quality coding problems that were published between May 2023 and February 2024. We have evaluated 9 base LLMs and 20 instruction-tuned LLMs on LiveCodeBench. We present empirical findings on contamination, holistic performance comparisons, potential overfitting in existing benchmarks as well as individual model comparisons. We will release all prompts and model completions for further community analysis, along with a general toolkit for adding new scenarios and model","sentences":["Large Language Models (LLMs) applied to code-related applications have emerged as a prominent field, attracting significant interest from both academia and industry.","However, as new and improved LLMs are developed, existing evaluation benchmarks (e.g., HumanEval, MBPP) are no longer sufficient for assessing their capabilities.","In this work, we propose LiveCodeBench, a comprehensive and contamination-free evaluation of LLMs for code, which continuously collects new problems over time from contests across three competition platforms, namely LeetCode, AtCoder, and CodeForces.","Notably, our benchmark also focuses on a broader range of code related capabilities, such as self-repair, code execution, and test output prediction, beyond just code generation.","Currently, LiveCodeBench hosts four hundred high-quality coding problems that were published between May 2023 and February 2024.","We have evaluated 9 base LLMs and 20 instruction-tuned LLMs on LiveCodeBench.","We present empirical findings on contamination, holistic performance comparisons, potential overfitting in existing benchmarks as well as individual model comparisons.","We will release all prompts and model completions for further community analysis, along with a general toolkit for adding new scenarios and model"],"url":"http://arxiv.org/abs/2403.07974v1","category":"cs.SE"}
{"created":"2024-03-12 15:15:48","title":"Evaluating the Impact of Vaccine Hesitancy on the Allocation of Vital Resources During COVID-19 Pandemic","abstract":"The COVID-19 pandemic highlighted significant challenges in the allocation of vital healthcare resources. Existing epidemiological models, specifically compartmental models, aimed to predict the spread of the COVID-19 virus and its impact on the population, but they overlooked the influence of \\ac{VH} on disease dynamics, including the expected number of hospitalizations and fatalities. We propose improvements to the \\ac{SEIR} model for COVID-19 by incorporating the influence of vaccination, \\ac{VH}, and resource availability on the disease dynamics. We collect publicly available data and perform data analysis to capture \\ac{VH} dynamic changes over time and develop scenario paths for \\ac{VH}. We simulate the proposed compartmental model for each \\ac{VH} path to explain the impacts of public attitudes toward vaccination, the impacts of healthcare resources on patient outcomes, and the timing of vaccination rollout on the progression and severity of the epidemic. Our analysis demonstrates that reducing \\ac{VH} improves health outcomes, reinforcing the importance of addressing \\ac{VH} to curb the spread of infectious diseases. Our results show that adequate levels of critical healthcare resources are crucial for minimizing fatalities and also highlight the life-saving impact of timely and effective vaccination programs.","sentences":["The COVID-19 pandemic highlighted significant challenges in the allocation of vital healthcare resources.","Existing epidemiological models, specifically compartmental models, aimed to predict the spread of the COVID-19 virus and its impact on the population, but they overlooked the influence of \\ac{VH} on disease dynamics, including the expected number of hospitalizations and fatalities.","We propose improvements to the \\ac{SEIR} model for COVID-19 by incorporating the influence of vaccination, \\ac{VH}, and resource availability on the disease dynamics.","We collect publicly available data and perform data analysis to capture \\ac{VH} dynamic changes over time and develop scenario paths for \\ac{VH}.","We simulate the proposed compartmental model for each \\ac{VH} path to explain the impacts of public attitudes toward vaccination, the impacts of healthcare resources on patient outcomes, and the timing of vaccination rollout on the progression and severity of the epidemic.","Our analysis demonstrates that reducing \\ac{VH} improves health outcomes, reinforcing the importance of addressing \\ac{VH} to curb the spread of infectious diseases.","Our results show that adequate levels of critical healthcare resources are crucial for minimizing fatalities and also highlight the life-saving impact of timely and effective vaccination programs."],"url":"http://arxiv.org/abs/2403.07971v1","category":"physics.soc-ph"}
{"created":"2024-03-12 15:13:12","title":"DSEG-LIME - Improving Image Explanation by Hierarchical Data-Driven Segmentation","abstract":"Explainable Artificial Intelligence is critical in unraveling decision-making processes in complex machine learning models. LIME (Local Interpretable Model-agnostic Explanations) is a well-known XAI framework for image analysis. It utilizes image segmentation to create features to identify relevant areas for classification. Consequently, poor segmentation can compromise the consistency of the explanation and undermine the importance of the segments, affecting the overall interpretability. Addressing these challenges, we introduce DSEG-LIME (Data-Driven Segmentation LIME), featuring: i) a data-driven segmentation for human-recognized feature generation, and ii) a hierarchical segmentation procedure through composition. We benchmark DSEG-LIME on pre-trained models with images from the ImageNet dataset - scenarios without domain-specific knowledge. The analysis includes a quantitative evaluation using established XAI metrics, complemented by a qualitative assessment through a user study. Our findings demonstrate that DSEG outperforms in most of the XAI metrics and enhances the alignment of explanations with human-recognized concepts, significantly improving interpretability. The code is available under: https://github. com/patrick-knab/DSEG-LIME","sentences":["Explainable Artificial Intelligence is critical in unraveling decision-making processes in complex machine learning models.","LIME (Local Interpretable Model-agnostic Explanations) is a well-known XAI framework for image analysis.","It utilizes image segmentation to create features to identify relevant areas for classification.","Consequently, poor segmentation can compromise the consistency of the explanation and undermine the importance of the segments, affecting the overall interpretability.","Addressing these challenges, we introduce DSEG-LIME (Data-Driven Segmentation LIME), featuring: i) a data-driven segmentation for human-recognized feature generation, and ii) a hierarchical segmentation procedure through composition.","We benchmark DSEG-LIME on pre-trained models with images from the ImageNet dataset - scenarios without domain-specific knowledge.","The analysis includes a quantitative evaluation using established XAI metrics, complemented by a qualitative assessment through a user study.","Our findings demonstrate that DSEG outperforms in most of the XAI metrics and enhances the alignment of explanations with human-recognized concepts, significantly improving interpretability.","The code is available under: https://github. com/patrick-knab/DSEG-LIME"],"url":"http://arxiv.org/abs/2403.07733v1","category":"cs.CV"}
{"created":"2024-03-12 14:56:34","title":"KnowCoder: Coding Structured Knowledge into LLMs for Universal Information Extraction","abstract":"In this paper, we propose KnowCoder, a Large Language Model (LLM) to conduct Universal Information Extraction (UIE) via code generation. KnowCoder aims to develop a kind of unified schema representation that LLMs can easily understand and an effective learning framework that encourages LLMs to follow schemas and extract structured knowledge accurately. To achieve these, KnowCoder introduces a code-style schema representation method to uniformly transform different schemas into Python classes, with which complex schema information, such as constraints among tasks in UIE, can be captured in an LLM-friendly manner. We further construct a code-style schema library covering over $\\textbf{30,000}$ types of knowledge, which is the largest one for UIE, to the best of our knowledge. To ease the learning process of LLMs, KnowCoder contains a two-phase learning framework that enhances its schema understanding ability via code pretraining and its schema following ability via instruction tuning. After code pretraining on around $1.5$B automatically constructed data, KnowCoder already attains remarkable generalization ability and achieves relative improvements by $\\textbf{49.8\\%}$ F1, compared to LLaMA2, under the few-shot setting. After instruction tuning, KnowCoder further exhibits strong generalization ability on unseen schemas and achieves up to $\\textbf{12.5\\%}$ and $\\textbf{21.9\\%}$, compared to sota baselines, under the zero-shot setting and the low resource setting, respectively. Additionally, based on our unified schema representations, various human-annotated datasets can simultaneously be utilized to refine KnowCoder, which achieves significant improvements up to $\\textbf{7.5\\%}$ under the supervised setting.","sentences":["In this paper, we propose KnowCoder, a Large Language Model (LLM) to conduct Universal Information Extraction (UIE) via code generation.","KnowCoder aims to develop a kind of unified schema representation that LLMs can easily understand and an effective learning framework that encourages LLMs to follow schemas and extract structured knowledge accurately.","To achieve these, KnowCoder introduces a code-style schema representation method to uniformly transform different schemas into Python classes, with which complex schema information, such as constraints among tasks in UIE, can be captured in an LLM-friendly manner.","We further construct a code-style schema library covering over $\\textbf{30,000}$ types of knowledge, which is the largest one for UIE, to the best of our knowledge.","To ease the learning process of LLMs, KnowCoder contains a two-phase learning framework that enhances its schema understanding ability via code pretraining and its schema following ability via instruction tuning.","After code pretraining on around $1.5$B automatically constructed data, KnowCoder already attains remarkable generalization ability and achieves relative improvements by $\\textbf{49.8\\%}$ F1, compared to LLaMA2, under the few-shot setting.","After instruction tuning, KnowCoder further exhibits strong generalization ability on unseen schemas and achieves up to $\\textbf{12.5\\%}$ and $\\textbf{21.9\\%}$, compared to sota baselines, under the zero-shot setting and the low resource setting, respectively.","Additionally, based on our unified schema representations, various human-annotated datasets can simultaneously be utilized to refine KnowCoder, which achieves significant improvements up to $\\textbf{7.5\\%}$ under the supervised setting."],"url":"http://arxiv.org/abs/2403.07969v1","category":"cs.LG"}
{"created":"2024-03-12 13:59:23","title":"Do Deep Neural Network Solutions Form a Star Domain?","abstract":"Entezari et al. (2022) conjectured that neural network solution sets reachable via stochastic gradient descent (SGD) are convex, considering permutation invariances. This means that two independent solutions can be connected by a linear path with low loss, given one of them is appropriately permuted. However, current methods to test this theory often fail to eliminate loss barriers between two independent solutions (Ainsworth et al., 2022; Benzing et al., 2022). In this work, we conjecture that a more relaxed claim holds: the SGD solution set is a star domain that contains a star model that is linearly connected to all the other solutions via paths with low loss values, modulo permutations. We propose the Starlight algorithm that finds a star model of a given learning task. We validate our claim by showing that this star model is linearly connected with other independently found solutions. As an additional benefit of our study, we demonstrate better uncertainty estimates on Bayesian Model Averaging over the obtained star domain. Code is available at https://github.com/aktsonthalia/starlight.","sentences":["Entezari et al.","(2022) conjectured that neural network solution sets reachable via stochastic gradient descent (SGD) are convex, considering permutation invariances.","This means that two independent solutions can be connected by a linear path with low loss, given one of them is appropriately permuted.","However, current methods to test this theory often fail to eliminate loss barriers between two independent solutions (Ainsworth et al., 2022; Benzing et al., 2022).","In this work, we conjecture that a more relaxed claim holds: the SGD solution set is a star domain that contains a star model that is linearly connected to all the other solutions via paths with low loss values, modulo permutations.","We propose the Starlight algorithm that finds a star model of a given learning task.","We validate our claim by showing that this star model is linearly connected with other independently found solutions.","As an additional benefit of our study, we demonstrate better uncertainty estimates on Bayesian Model Averaging over the obtained star domain.","Code is available at https://github.com/aktsonthalia/starlight."],"url":"http://arxiv.org/abs/2403.07968v1","category":"cs.LG"}
{"created":"2024-03-12 11:56:38","title":"Conditional computation in neural networks: principles and research trends","abstract":"This article summarizes principles and ideas from the emerging area of applying \\textit{conditional computation} methods to the design of neural networks. In particular, we focus on neural networks that can dynamically activate or de-activate parts of their computational graph conditionally on their input. Examples include the dynamic selection of, e.g., input tokens, layers (or sets of layers), and sub-modules inside each layer (e.g., channels in a convolutional filter). We first provide a general formalism to describe these techniques in an uniform way. Then, we introduce three notable implementations of these principles: mixture-of-experts (MoEs) networks, token selection mechanisms, and early-exit neural networks. The paper aims to provide a tutorial-like introduction to this growing field. To this end, we analyze the benefits of these modular designs in terms of efficiency, explainability, and transfer learning, with a focus on emerging applicative areas ranging from automated scientific discovery to semantic communication.","sentences":["This article summarizes principles and ideas from the emerging area of applying \\textit{conditional computation} methods to the design of neural networks.","In particular, we focus on neural networks that can dynamically activate or de-activate parts of their computational graph conditionally on their input.","Examples include the dynamic selection of, e.g., input tokens, layers (or sets of layers), and sub-modules inside each layer (e.g., channels in a convolutional filter).","We first provide a general formalism to describe these techniques in an uniform way.","Then, we introduce three notable implementations of these principles: mixture-of-experts (MoEs) networks, token selection mechanisms, and early-exit neural networks.","The paper aims to provide a tutorial-like introduction to this growing field.","To this end, we analyze the benefits of these modular designs in terms of efficiency, explainability, and transfer learning, with a focus on emerging applicative areas ranging from automated scientific discovery to semantic communication."],"url":"http://arxiv.org/abs/2403.07965v1","category":"cs.LG"}
{"created":"2024-03-12 11:51:30","title":"Optimal Design and Implementation of an Open-source Emulation Platform for User-Centric Shared E-mobility Services","abstract":"In response to the escalating global challenge of increasing emissions and pollution in transportation, shared electric mobility services, encompassing e-cars, e-bikes, and e-scooters, have emerged as a popular strategy. However, existingshared electric mobility services exhibit critical design deficiencies, including insufficient service integration, imprecise energy consumption forecasting, limited scalability and geographical coverage, and a notable absence of a user-centric perspective, particularly in the context of multi-modal transportation. More importantly, there is no consolidated open-source framework which could benefit the e-mobility research community. This paper aims to bridge this gap by providing a pioneering open-source framework for shared e-mobility. The proposed framework, with an agent-in-the-loop approach and modular architecture, is tailored to diverse user preferences and offers enhanced customization. We demonstrate the viability of this framework by solving an integrated multi-modal route-optimization problem using the modified Ant Colony Optimization (ACO) algorithm. The primary contribution of this work is to provide a collaborative and transparent framework to tackle the dynamic challenges in the field of e-mobility research using a consolidated approach.","sentences":["In response to the escalating global challenge of increasing emissions and pollution in transportation, shared electric mobility services, encompassing e-cars, e-bikes, and e-scooters, have emerged as a popular strategy.","However, existingshared electric mobility services exhibit critical design deficiencies, including insufficient service integration, imprecise energy consumption forecasting, limited scalability and geographical coverage, and a notable absence of a user-centric perspective, particularly in the context of multi-modal transportation.","More importantly, there is no consolidated open-source framework which could benefit the e-mobility research community.","This paper aims to bridge this gap by providing a pioneering open-source framework for shared e-mobility.","The proposed framework, with an agent-in-the-loop approach and modular architecture, is tailored to diverse user preferences and offers enhanced customization.","We demonstrate the viability of this framework by solving an integrated multi-modal route-optimization problem using the modified Ant Colony Optimization (ACO) algorithm.","The primary contribution of this work is to provide a collaborative and transparent framework to tackle the dynamic challenges in the field of e-mobility research using a consolidated approach."],"url":"http://arxiv.org/abs/2403.07964v1","category":"cs.AI"}
{"created":"2024-03-13 17:59:50","title":"3DFIRES: Few Image 3D REconstruction for Scenes with Hidden Surface","abstract":"This paper introduces 3DFIRES, a novel system for scene-level 3D reconstruction from posed images. Designed to work with as few as one view, 3DFIRES reconstructs the complete geometry of unseen scenes, including hidden surfaces. With multiple view inputs, our method produces full reconstruction within all camera frustums. A key feature of our approach is the fusion of multi-view information at the feature level, enabling the production of coherent and comprehensive 3D reconstruction. We train our system on non-watertight scans from large-scale real scene dataset. We show it matches the efficacy of single-view reconstruction methods with only one input and surpasses existing techniques in both quantitative and qualitative measures for sparse-view 3D reconstruction.","sentences":["This paper introduces 3DFIRES, a novel system for scene-level 3D reconstruction from posed images.","Designed to work with as few as one view, 3DFIRES reconstructs the complete geometry of unseen scenes, including hidden surfaces.","With multiple view inputs, our method produces full reconstruction within all camera frustums.","A key feature of our approach is the fusion of multi-view information at the feature level, enabling the production of coherent and comprehensive 3D reconstruction.","We train our system on non-watertight scans from large-scale real scene dataset.","We show it matches the efficacy of single-view reconstruction methods with only one input and surpasses existing techniques in both quantitative and qualitative measures for sparse-view 3D reconstruction."],"url":"http://arxiv.org/abs/2403.08768v1","category":"cs.CV"}
{"created":"2024-03-13 17:59:19","title":"On the eigenvalues of the harmonic oscillator with a Gaussian perturbation","abstract":"We test the analytical expressions for the first two eigenvalues of the harmonic oscillator with a Gaussian perturbation proposed recently. Our numerical eigenvalues show that those expressions are valid in an interval of the coupling parameter that is greater than the one estimated by the authors. We also calculate critical values of the coupling parameter and several exceptional points in the complex plane.","sentences":["We test the analytical expressions for the first two eigenvalues of the harmonic oscillator with a Gaussian perturbation proposed recently.","Our numerical eigenvalues show that those expressions are valid in an interval of the coupling parameter that is greater than the one estimated by the authors.","We also calculate critical values of the coupling parameter and several exceptional points in the complex plane."],"url":"http://arxiv.org/abs/2403.08767v1","category":"quant-ph"}
{"created":"2024-03-13 17:59:04","title":"MonoOcc: Digging into Monocular Semantic Occupancy Prediction","abstract":"Monocular Semantic Occupancy Prediction aims to infer the complete 3D geometry and semantic information of scenes from only 2D images. It has garnered significant attention, particularly due to its potential to enhance the 3D perception of autonomous vehicles. However, existing methods rely on a complex cascaded framework with relatively limited information to restore 3D scenes, including a dependency on supervision solely on the whole network's output, single-frame input, and the utilization of a small backbone. These challenges, in turn, hinder the optimization of the framework and yield inferior prediction results, particularly concerning smaller and long-tailed objects. To address these issues, we propose MonoOcc. In particular, we (i) improve the monocular occupancy prediction framework by proposing an auxiliary semantic loss as supervision to the shallow layers of the framework and an image-conditioned cross-attention module to refine voxel features with visual clues, and (ii) employ a distillation module that transfers temporal information and richer knowledge from a larger image backbone to the monocular semantic occupancy prediction framework with low cost of hardware. With these advantages, our method yields state-of-the-art performance on the camera-based SemanticKITTI Scene Completion benchmark. Codes and models can be accessed at https://github.com/ucaszyp/MonoOcc","sentences":["Monocular Semantic Occupancy Prediction aims to infer the complete 3D geometry and semantic information of scenes from only 2D images.","It has garnered significant attention, particularly due to its potential to enhance the 3D perception of autonomous vehicles.","However, existing methods rely on a complex cascaded framework with relatively limited information to restore 3D scenes, including a dependency on supervision solely on the whole network's output, single-frame input, and the utilization of a small backbone.","These challenges, in turn, hinder the optimization of the framework and yield inferior prediction results, particularly concerning smaller and long-tailed objects.","To address these issues, we propose MonoOcc.","In particular, we (i) improve the monocular occupancy prediction framework by proposing an auxiliary semantic loss as supervision to the shallow layers of the framework and an image-conditioned cross-attention module to refine voxel features with visual clues, and (ii) employ a distillation module that transfers temporal information and richer knowledge from a larger image backbone to the monocular semantic occupancy prediction framework with low cost of hardware.","With these advantages, our method yields state-of-the-art performance on the camera-based SemanticKITTI Scene Completion benchmark.","Codes and models can be accessed at https://github.com/ucaszyp/MonoOcc"],"url":"http://arxiv.org/abs/2403.08766v1","category":"cs.CV"}
{"created":"2024-03-13 17:50:45","title":"Poisson suspensions without roots","abstract":"We construct rigid Poisson suspensions without roots. The discrete rational component in spectrum of an ergodic automorphism S prevents some roots from existing. If S is tensorly multiplied by an ergodic automorphism of the space with a sigma-finite measure, discrete spectrum disappears in this product, but like the smile of Cheshire Cat, the memory of it can remain in the form of the absence of roots. In additional conditions, this effect is inherited by the Poisson suspension over the above product. Starting from this idea, but without using the tensor product, we describe simple rank-one constructions for which the Poisson suspensions are rigid and have no roots.","sentences":["We construct rigid Poisson suspensions without roots.","The discrete rational component in spectrum of an ergodic automorphism S prevents some roots from existing.","If S is tensorly multiplied by an ergodic automorphism of the space with a sigma-finite measure, discrete spectrum disappears in this product, but like the smile of Cheshire Cat, the memory of it can remain in the form of the absence of roots.","In additional conditions, this effect is inherited by the Poisson suspension over the above product.","Starting from this idea, but without using the tensor product, we describe simple rank-one constructions for which the Poisson suspensions are rigid and have no roots."],"url":"http://arxiv.org/abs/2403.08747v1","category":"math.DS"}
{"created":"2024-03-13 17:47:02","title":"Boundary controllability for a fourth order degenerate parabolic equation with a singular potential","abstract":"In this paper, we prove the null controllability of a one-dimensional fourth-order degenerate parabolic equation with a singular potential. Here, we analyze cases where boundary control conditions are applied at the left endpoint. We utilize a spectral decomposition involving Bessel functions and their zeros in a convenient weighted Sobolev space for a degenerate parabolic operator with specific boundary conditions. We establish the well-posedness of the system using semigroup operator theory. Subsequently, we employ the moment method by Fattorini and Russell to obtain an upper estimate of the cost of controllability. Additionally, we derive a lower estimate of the cost of controllability using a representation theorem for analytic functions of exponential type.","sentences":["In this paper, we prove the null controllability of a one-dimensional fourth-order degenerate parabolic equation with a singular potential.","Here, we analyze cases where boundary control conditions are applied at the left endpoint.","We utilize a spectral decomposition involving Bessel functions and their zeros in a convenient weighted Sobolev space for a degenerate parabolic operator with specific boundary conditions.","We establish the well-posedness of the system using semigroup operator theory.","Subsequently, we employ the moment method by Fattorini and Russell to obtain an upper estimate of the cost of controllability.","Additionally, we derive a lower estimate of the cost of controllability using a representation theorem for analytic functions of exponential type."],"url":"http://arxiv.org/abs/2403.08745v1","category":"math.AP"}
{"created":"2024-03-13 17:44:15","title":"Acoustic Side Channel Attack on Keyboards Based on Typing Patterns","abstract":"Acoustic side-channel attacks on keyboards can bypass security measures in many systems that use keyboards as one of the input devices. These attacks aim to reveal users' sensitive information by targeting the sounds made by their keyboards as they type. Most existing approaches in this field ignore the negative impacts of typing patterns and environmental noise in their results. This paper seeks to address these shortcomings by proposing an applicable method that takes into account the user's typing pattern in a realistic environment. Our method achieved an average success rate of 43% across all our case studies when considering real-world scenarios.","sentences":["Acoustic side-channel attacks on keyboards can bypass security measures in many systems that use keyboards as one of the input devices.","These attacks aim to reveal users' sensitive information by targeting the sounds made by their keyboards as they type.","Most existing approaches in this field ignore the negative impacts of typing patterns and environmental noise in their results.","This paper seeks to address these shortcomings by proposing an applicable method that takes into account the user's typing pattern in a realistic environment.","Our method achieved an average success rate of 43% across all our case studies when considering real-world scenarios."],"url":"http://arxiv.org/abs/2403.08740v1","category":"cs.CR"}
{"created":"2024-03-13 17:38:05","title":"ILCiteR: Evidence-grounded Interpretable Local Citation Recommendation","abstract":"Existing Machine Learning approaches for local citation recommendation directly map or translate a query, which is typically a claim or an entity mention, to citation-worthy research papers. Within such a formulation, it is challenging to pinpoint why one should cite a specific research paper for a particular query, leading to limited recommendation interpretability. To alleviate this, we introduce the evidence-grounded local citation recommendation task, where the target latent space comprises evidence spans for recommending specific papers. Using a distantly-supervised evidence retrieval and multi-step re-ranking framework, our proposed system, ILCiteR, recommends papers to cite for a query grounded on similar evidence spans extracted from the existing research literature. Unlike past formulations that simply output recommendations, ILCiteR retrieves ranked lists of evidence span and recommended paper pairs. Secondly, previously proposed neural models for citation recommendation require expensive training on massive labeled data, ideally after every significant update to the pool of candidate papers. In contrast, ILCiteR relies solely on distant supervision from a dynamic evidence database and pre-trained Transformer-based Language Models without any model training. We contribute a novel dataset for the evidence-grounded local citation recommendation task and demonstrate the efficacy of our proposed conditional neural rank-ensembling approach for re-ranking evidence spans.","sentences":["Existing Machine Learning approaches for local citation recommendation directly map or translate a query, which is typically a claim or an entity mention, to citation-worthy research papers.","Within such a formulation, it is challenging to pinpoint why one should cite a specific research paper for a particular query, leading to limited recommendation interpretability.","To alleviate this, we introduce the evidence-grounded local citation recommendation task, where the target latent space comprises evidence spans for recommending specific papers.","Using a distantly-supervised evidence retrieval and multi-step re-ranking framework, our proposed system, ILCiteR, recommends papers to cite for a query grounded on similar evidence spans extracted from the existing research literature.","Unlike past formulations that simply output recommendations, ILCiteR retrieves ranked lists of evidence span and recommended paper pairs.","Secondly, previously proposed neural models for citation recommendation require expensive training on massive labeled data, ideally after every significant update to the pool of candidate papers.","In contrast, ILCiteR relies solely on distant supervision from a dynamic evidence database and pre-trained Transformer-based Language Models without any model training.","We contribute a novel dataset for the evidence-grounded local citation recommendation task and demonstrate the efficacy of our proposed conditional neural rank-ensembling approach for re-ranking evidence spans."],"url":"http://arxiv.org/abs/2403.08737v1","category":"cs.IR"}
{"created":"2024-03-13 17:30:06","title":"Fault Localization in a Microfabricated Surface Ion Trap using Diamond Nitrogen-Vacancy Center Magnetometry","abstract":"As quantum computing hardware becomes more complex with ongoing design innovations and growing capabilities, the quantum computing community needs increasingly powerful techniques for fabrication failure root-cause analysis. This is especially true for trapped-ion quantum computing. As trapped-ion quantum computing aims to scale to thousands of ions, the electrode numbers are growing to several hundred with likely integrated-photonic components also adding to the electrical and fabrication complexity, making faults even harder to locate. In this work, we used a high-resolution quantum magnetic imaging technique, based on nitrogen-vacancy (NV) centers in diamond, to investigate short-circuit faults in an ion trap chip. We imaged currents from these short-circuit faults to ground and compared to intentionally-created faults, finding that the root-cause of the faults was failures in the on-chip trench capacitors. This work, where we exploited the performance advantages of a quantum magnetic sensing technique to troubleshoot a piece of quantum computing hardware, is a unique example of the evolving synergy between emerging quantum technologies to achieve capabilities that were previously inaccessible.","sentences":["As quantum computing hardware becomes more complex with ongoing design innovations and growing capabilities, the quantum computing community needs increasingly powerful techniques for fabrication failure root-cause analysis.","This is especially true for trapped-ion quantum computing.","As trapped-ion quantum computing aims to scale to thousands of ions, the electrode numbers are growing to several hundred with likely integrated-photonic components also adding to the electrical and fabrication complexity, making faults even harder to locate.","In this work, we used a high-resolution quantum magnetic imaging technique, based on nitrogen-vacancy (NV) centers in diamond, to investigate short-circuit faults in an ion trap chip.","We imaged currents from these short-circuit faults to ground and compared to intentionally-created faults, finding that the root-cause of the faults was failures in the on-chip trench capacitors.","This work, where we exploited the performance advantages of a quantum magnetic sensing technique to troubleshoot a piece of quantum computing hardware, is a unique example of the evolving synergy between emerging quantum technologies to achieve capabilities that were previously inaccessible."],"url":"http://arxiv.org/abs/2403.08731v1","category":"physics.ins-det"}
{"created":"2024-03-13 17:27:31","title":"Euclid: Testing photometric selection of emission-line galaxy targets","abstract":"Multi-object spectroscopic galaxy surveys typically make use of photometric and colour criteria to select targets. Conversely, the Euclid NISP slitless spectrograph will record spectra for every source over its field of view. Slitless spectroscopy has the advantage of avoiding defining a priori a galaxy sample, but at the price of making the selection function harder to quantify. The Euclid Wide Survey aims at building robust statistical samples of emission-line galaxies with fluxes in the Halpha-NII complex brighter than 2e-16 erg/s/cm^2 and within 0.9<z<1.8. At faint fluxes, we expect significant contamination by wrongly measured redshifts, either due to emission-line misidentification or noise fluctuations, with the consequence of reducing the purity of the final samples. This can be significantly improved by exploiting Euclid photometric information to identify emission-line galaxies over the redshifts of interest. To this goal, we compare and quantify the performance of six machine-learning classification algorithms. We consider the case when only Euclid photometric and morphological measurements are used and when these are supplemented by ground-based photometric data. We train and test the classifiers on two mock galaxy samples, the EL-COSMOS and Euclid Flagship2 catalogues. Dense neural networks and support vector classifiers obtain the best performance, with comparable results in terms of the adopted metrics. When training on Euclid photometry alone, these can remove 87% of the sources that are fainter than the nominal flux limit or lie outside the range 0.9<z<1.8, a figure that increases to 97% when ground-based photometry is included. These results show how by using the photometric information available to Euclid it will be possible to efficiently identify and discard spurious interlopers, allowing us to build robust spectroscopic samples for cosmological investigations.","sentences":["Multi-object spectroscopic galaxy surveys typically make use of photometric and colour criteria to select targets.","Conversely, the Euclid NISP slitless spectrograph will record spectra for every source over its field of view.","Slitless spectroscopy has the advantage of avoiding defining a priori a galaxy sample, but at the price of making the selection function harder to quantify.","The Euclid Wide Survey aims at building robust statistical samples of emission-line galaxies with fluxes in the Halpha-NII complex brighter than 2e-16 erg/s/cm^2 and within 0.9<z<1.8.","At faint fluxes, we expect significant contamination by wrongly measured redshifts, either due to emission-line misidentification or noise fluctuations, with the consequence of reducing the purity of the final samples.","This can be significantly improved by exploiting Euclid photometric information to identify emission-line galaxies over the redshifts of interest.","To this goal, we compare and quantify the performance of six machine-learning classification algorithms.","We consider the case when only Euclid photometric and morphological measurements are used and when these are supplemented by ground-based photometric data.","We train and test the classifiers on two mock galaxy samples, the EL-COSMOS and Euclid Flagship2 catalogues.","Dense neural networks and support vector classifiers obtain the best performance, with comparable results in terms of the adopted metrics.","When training on Euclid photometry alone, these can remove 87% of the sources that are fainter than the nominal flux limit or lie outside the range 0.9<z<1.8, a figure that increases to 97% when ground-based photometry is included.","These results show how by using the photometric information available to Euclid it will be possible to efficiently identify and discard spurious interlopers, allowing us to build robust spectroscopic samples for cosmological investigations."],"url":"http://arxiv.org/abs/2403.08726v1","category":"astro-ph.CO"}
{"created":"2024-03-13 17:22:37","title":"Asymptotic polynomial approximation in the Bloch space","abstract":"We investigate asymptotic polynomial approximation for a class of weighted Bloch functions in the unit disc. Our main result is a structural theorem on asymptotic polynomial approximation in the unit disc, in the flavor of the classical Plessner Theorem on asymptotic values of meromorphic functions. This provides the appropriate set up for studying metric and geometric properties of sets E on the unit circle for which the following simultaneous approximation phenomenon occurs: there exists analytic polynomials which converge uniformly to zero on E and to a non-zero function in the weighted Bloch norm. We offer a characterization completely within the realm of real-analysis, establish a connection to removable sets for analytic Sobolev functions in the complex plane, and provide several necessary conditions in terms of entropy, Hausdorff content and condenser capacity. Furthermore, we demonstrate two principal applications of our developments, which go in different directions. First, we shall deduce a rather subtle consequence in the theme of smooth approximation in de Branges-Rovnyak spaces. Secondly, we answer some questions that were raised almost a decade ago in the theory of Universal Taylor series.","sentences":["We investigate asymptotic polynomial approximation for a class of weighted Bloch functions in the unit disc.","Our main result is a structural theorem on asymptotic polynomial approximation in the unit disc, in the flavor of the classical Plessner Theorem on asymptotic values of meromorphic functions.","This provides the appropriate set up for studying metric and geometric properties of sets E on the unit circle for which the following simultaneous approximation phenomenon occurs: there exists analytic polynomials which converge uniformly to zero on E and to a non-zero function in the weighted Bloch norm.","We offer a characterization completely within the realm of real-analysis, establish a connection to removable sets for analytic Sobolev functions in the complex plane, and provide several necessary conditions in terms of entropy, Hausdorff content and condenser capacity.","Furthermore, we demonstrate two principal applications of our developments, which go in different directions.","First, we shall deduce a rather subtle consequence in the theme of smooth approximation in de Branges-Rovnyak spaces.","Secondly, we answer some questions that were raised almost a decade ago in the theory of Universal Taylor series."],"url":"http://arxiv.org/abs/2403.08723v1","category":"math.CV"}
{"created":"2024-03-13 17:20:25","title":"Historical Astronomical Diagrams Decomposition in Geometric Primitives","abstract":"Automatically extracting the geometric content from the hundreds of thousands of diagrams drawn in historical manuscripts would enable historians to study the diffusion of astronomical knowledge on a global scale. However, state-of-the-art vectorization methods, often designed to tackle modern data, are not adapted to the complexity and diversity of historical astronomical diagrams. Our contribution is thus twofold. First, we introduce a unique dataset of 303 astronomical diagrams from diverse traditions, ranging from the XIIth to the XVIIIth century, annotated with more than 3000 line segments, circles and arcs. Second, we develop a model that builds on DINO-DETR to enable the prediction of multiple geometric primitives. We show that it can be trained solely on synthetic data and accurately predict primitives on our challenging dataset. Our approach widely improves over the LETR baseline, which is restricted to lines, by introducing a meaningful parametrization for multiple primitives, jointly training for detection and parameter refinement, using deformable attention and training on rich synthetic data. Our dataset and code are available on our webpage.","sentences":["Automatically extracting the geometric content from the hundreds of thousands of diagrams drawn in historical manuscripts would enable historians to study the diffusion of astronomical knowledge on a global scale.","However, state-of-the-art vectorization methods, often designed to tackle modern data, are not adapted to the complexity and diversity of historical astronomical diagrams.","Our contribution is thus twofold.","First, we introduce a unique dataset of 303 astronomical diagrams from diverse traditions, ranging from the XIIth to the XVIIIth century, annotated with more than 3000 line segments, circles and arcs.","Second, we develop a model that builds on DINO-DETR to enable the prediction of multiple geometric primitives.","We show that it can be trained solely on synthetic data and accurately predict primitives on our challenging dataset.","Our approach widely improves over the LETR baseline, which is restricted to lines, by introducing a meaningful parametrization for multiple primitives, jointly training for detection and parameter refinement, using deformable attention and training on rich synthetic data.","Our dataset and code are available on our webpage."],"url":"http://arxiv.org/abs/2403.08721v1","category":"cs.CV"}
{"created":"2024-03-13 17:18:39","title":"Probabilistic Metaplasticity for Continual Learning with Memristors","abstract":"Crossbar architectures utilizing memristor devices hold promise to address continual learning challenges in resource-constrained edge devices. However, these nanoscale devices often exhibit low precision and high variability in conductance modulation, rendering them unsuitable for continual learning solutions that consolidate weights through precise modulation. This issue can be circumvented by accumulating weight gradients in auxiliary high-precision memory and updating memristor weights when gradients are equivalent to memristor weight resolution. However, it leads to frequent memory access, high memory overhead, and energy dissipation. In this research, we propose probabilistic metaplasticity, which consolidates weights by modulating their update probability rather than magnitude. The proposed mechanism eliminates high-precision modification to weight magnitude and consequently, high-precision memory for gradient accumulation. We demonstrate the efficacy of the proposed mechanism by integrating probabilistic metaplasticity into a spiking network trained on an error threshold with low-precision memristor weights. Evaluations of two continual learning benchmarks show that probabilistic metaplasticity consumes ~67% lower memory for additional parameters and up to two orders of magnitude lower energy during parameter updates compared to an auxiliary memory-based solution while achieving state-of-the-art performance. The proposed model shows potential for energy-efficient continual learning with low-precision emerging devices.","sentences":["Crossbar architectures utilizing memristor devices hold promise to address continual learning challenges in resource-constrained edge devices.","However, these nanoscale devices often exhibit low precision and high variability in conductance modulation, rendering them unsuitable for continual learning solutions that consolidate weights through precise modulation.","This issue can be circumvented by accumulating weight gradients in auxiliary high-precision memory and updating memristor weights when gradients are equivalent to memristor weight resolution.","However, it leads to frequent memory access, high memory overhead, and energy dissipation.","In this research, we propose probabilistic metaplasticity, which consolidates weights by modulating their update probability rather than magnitude.","The proposed mechanism eliminates high-precision modification to weight magnitude and consequently, high-precision memory for gradient accumulation.","We demonstrate the efficacy of the proposed mechanism by integrating probabilistic metaplasticity into a spiking network trained on an error threshold with low-precision memristor weights.","Evaluations of two continual learning benchmarks show that probabilistic metaplasticity consumes ~67% lower memory for additional parameters and up to two orders of magnitude lower energy during parameter updates compared to an auxiliary memory-based solution while achieving state-of-the-art performance.","The proposed model shows potential for energy-efficient continual learning with low-precision emerging devices."],"url":"http://arxiv.org/abs/2403.08718v1","category":"eess.SY"}
{"created":"2024-03-13 17:18:19","title":"DIFFTACTILE: A Physics-based Differentiable Tactile Simulator for Contact-rich Robotic Manipulation","abstract":"We introduce DIFFTACTILE, a physics-based differentiable tactile simulation system designed to enhance robotic manipulation with dense and physically accurate tactile feedback. In contrast to prior tactile simulators which primarily focus on manipulating rigid bodies and often rely on simplified approximations to model stress and deformations of materials in contact, DIFFTACTILE emphasizes physics-based contact modeling with high fidelity, supporting simulations of diverse contact modes and interactions with objects possessing a wide range of material properties. Our system incorporates several key components, including a Finite Element Method (FEM)-based soft body model for simulating the sensing elastomer, a multi-material simulator for modeling diverse object types (such as elastic, elastoplastic, cables) under manipulation, a penalty-based contact model for handling contact dynamics. The differentiable nature of our system facilitates gradient-based optimization for both 1) refining physical properties in simulation using real-world data, hence narrowing the sim-to-real gap and 2) efficient learning of tactile-assisted grasping and contact-rich manipulation skills. Additionally, we introduce a method to infer the optical response of our tactile sensor to contact using an efficient pixel-based neural module. We anticipate that DIFFTACTILE will serve as a useful platform for studying contact-rich manipulations, leveraging the benefits of dense tactile feedback and differentiable physics. Code and supplementary materials are available at the project website https://difftactile.github.io/.","sentences":["We introduce DIFFTACTILE, a physics-based differentiable tactile simulation system designed to enhance robotic manipulation with dense and physically accurate tactile feedback.","In contrast to prior tactile simulators which primarily focus on manipulating rigid bodies and often rely on simplified approximations to model stress and deformations of materials in contact, DIFFTACTILE emphasizes physics-based contact modeling with high fidelity, supporting simulations of diverse contact modes and interactions with objects possessing a wide range of material properties.","Our system incorporates several key components, including a Finite Element Method (FEM)-based soft body model for simulating the sensing elastomer, a multi-material simulator for modeling diverse object types (such as elastic, elastoplastic, cables) under manipulation, a penalty-based contact model for handling contact dynamics.","The differentiable nature of our system facilitates gradient-based optimization for both 1) refining physical properties in simulation using real-world data, hence narrowing the sim-to-real gap and 2) efficient learning of tactile-assisted grasping and contact-rich manipulation skills.","Additionally, we introduce a method to infer the optical response of our tactile sensor to contact using an efficient pixel-based neural module.","We anticipate that DIFFTACTILE will serve as a useful platform for studying contact-rich manipulations, leveraging the benefits of dense tactile feedback and differentiable physics.","Code and supplementary materials are available at the project website https://difftactile.github.io/."],"url":"http://arxiv.org/abs/2403.08716v1","category":"cs.RO"}
{"created":"2024-03-13 17:15:38","title":"On the geometric phase and its role in the design of elastic topological materials","abstract":"The geometric phase provides important mathematical insights to understand the occurrence and evolution of the dynamic response in a diverse spectrum of systems ranging from quantum to classical mechanics. While the concept of geometric phase, which is an additional phase factor occurring in dynamical systems, holds the same meaning across different fields of application, its use and interpretation can acquire important nuances specific to the system of interest. In recent years, the development of the concept of quantum topological materials and its extension to classical mechanical systems have renewed the interest in the study of the geometric phase. This study reviews the concept of geometric phase and discusses, by means of either established or original results, its role in the design of elastic materials. Concepts of differential geometry and topology are put forward to provide a theoretical understanding of the geometric phase and its connection to the physical properties of the system. Then, the concept of geometric phase is applied to different types of elastic waveguides to explain how either topologically trivial or non-trivial behavior can emerge based on a proper geometric design of the waveguide.","sentences":["The geometric phase provides important mathematical insights to understand the occurrence and evolution of the dynamic response in a diverse spectrum of systems ranging from quantum to classical mechanics.","While the concept of geometric phase, which is an additional phase factor occurring in dynamical systems, holds the same meaning across different fields of application, its use and interpretation can acquire important nuances specific to the system of interest.","In recent years, the development of the concept of quantum topological materials and its extension to classical mechanical systems have renewed the interest in the study of the geometric phase.","This study reviews the concept of geometric phase and discusses, by means of either established or original results, its role in the design of elastic materials.","Concepts of differential geometry and topology are put forward to provide a theoretical understanding of the geometric phase and its connection to the physical properties of the system.","Then, the concept of geometric phase is applied to different types of elastic waveguides to explain how either topologically trivial or non-trivial behavior can emerge based on a proper geometric design of the waveguide."],"url":"http://arxiv.org/abs/2403.08711v1","category":"physics.app-ph"}
{"created":"2024-03-13 17:15:05","title":"On the Microlocal Regularity of the Gevrey Vectors for second order partial differential operators with non negative characteristic form of first kind","abstract":"We study the microlocal regularity of the analytic/Gevrey vectors for the following class of second order partial differential equations \\begin{align*}   P(x,D) = \\sum_{\\ell,j=1}^{n} a_{\\ell,j}(x) D_{\\ell} D_{j} + \\sum_{\\ell=1}^{n} i b_{\\ell}(x) D_{\\ell} +c(x), \\end{align*} where $a_{\\ell,j}(x) = a_{j,\\ell}(x)$, $b_{\\ell}(x)$, $\\ell,j \\in \\lbrace 1,\\dots,\\, n\\rbrace$, are real valued real Gevrey functions of order $s$ and $c(x)$ is a Gevrey function of order $s$, $s \\geq 1$, on $\\Omega$ open neighborhood of the origin in $\\mathbb{R}^{n}$. Thus providing a microlocal version of a result due to M. Derridj in \"Gevrey regularity of Gevrey vectors of second order partial differential operators with non negative characteristic form\", Complex Anal. Synerg. $\\mathbf{6}$, 10 (2020), https://doi.org/10.1007/s40627-020-00047-8.","sentences":["We study the microlocal regularity of the analytic/Gevrey vectors for the following class of second order partial differential equations \\begin{align*}   P(x,D) = \\sum_{\\ell,j=1}^{n} a_{\\ell,j}(x) D_{\\ell} D_{j} + \\sum_{\\ell=1}^{n} i b_{\\ell}(x) D_{\\ell} +c(x), \\end{align*} where $a_{\\ell,j}(x)","= a_{j,\\ell}(x)$, $b_{\\ell}(x)$, $\\ell,j \\in \\lbrace 1,\\dots,\\, n\\rbrace$, are real valued real Gevrey functions of order $s$ and $c(x)$ is a Gevrey function of order $s$, $s \\geq 1$, on $\\Omega$ open neighborhood of the origin in $\\mathbb{R}^{n}$. Thus providing a microlocal version of a result due to M. Derridj in \"Gevrey regularity of Gevrey vectors of second order partial differential operators with non negative characteristic form\", Complex Anal.","Synerg.","$\\mathbf{6}$, 10 (2020), https://doi.org/10.1007/s40627-020-00047-8."],"url":"http://arxiv.org/abs/2403.08709v1","category":"math.AP"}
{"created":"2024-03-13 16:49:36","title":"Controllability of continuous networks and a kernel-based learning approximation","abstract":"Residual deep neural networks are formulated as interacting particle systems leading to a description through neural differential equations, and, in the case of large input data, through mean-field neural networks. The mean-field description allows also the recast of the training processes as a controllability problem for the solution to the mean-field dynamics. We show theoretical results on the controllability of the linear microscopic and mean-field dynamics through the Hilbert Uniqueness Method and propose a computational approach based on kernel learning methods to solve numerically, and efficiently, the training problem. Further aspects of the structural properties of the mean-field equation will be reviewed.","sentences":["Residual deep neural networks are formulated as interacting particle systems leading to a description through neural differential equations, and, in the case of large input data, through mean-field neural networks.","The mean-field description allows also the recast of the training processes as a controllability problem for the solution to the mean-field dynamics.","We show theoretical results on the controllability of the linear microscopic and mean-field dynamics through the Hilbert Uniqueness Method and propose a computational approach based on kernel learning methods to solve numerically, and efficiently, the training problem.","Further aspects of the structural properties of the mean-field equation will be reviewed."],"url":"http://arxiv.org/abs/2403.08690v1","category":"math.OC"}
{"created":"2024-03-13 16:42:05","title":"Elastic shape analysis computations for clustering left atrial appendage geometries of atrial fibrillation patients","abstract":"Morphological variations in the left atrial appendage (LAA) are associated with different levels of ischemic stroke risk for patients with atrial fibrillation (AF). Studying LAA morphology can elucidate mechanisms behind this association and lead to the development of advanced stroke risk stratification tools. However, current categorical descriptions of LAA morphologies are qualitative and inconsistent across studies, which impedes advancements in our understanding of stroke pathogenesis in AF. To mitigate these issues, we introduce a quantitative pipeline that combines elastic shape analysis with unsupervised learning for the categorization of LAA morphology in AF patients. As part of our pipeline, we compute pairwise elastic distances between LAA meshes from a cohort of 20 AF patients, and leverage these distances to cluster our shape data. We demonstrate that our method clusters LAA morphologies based on distinctive shape features, overcoming the innate inconsistencies of current LAA categorization systems, and paving the way for improved stroke risk metrics using objective LAA shape groups.","sentences":["Morphological variations in the left atrial appendage (LAA) are associated with different levels of ischemic stroke risk for patients with atrial fibrillation (AF).","Studying LAA morphology can elucidate mechanisms behind this association and lead to the development of advanced stroke risk stratification tools.","However, current categorical descriptions of LAA morphologies are qualitative and inconsistent across studies, which impedes advancements in our understanding of stroke pathogenesis in AF.","To mitigate these issues, we introduce a quantitative pipeline that combines elastic shape analysis with unsupervised learning for the categorization of LAA morphology in AF patients.","As part of our pipeline, we compute pairwise elastic distances between LAA meshes from a cohort of 20 AF patients, and leverage these distances to cluster our shape data.","We demonstrate that our method clusters LAA morphologies based on distinctive shape features, overcoming the innate inconsistencies of current LAA categorization systems, and paving the way for improved stroke risk metrics using objective LAA shape groups."],"url":"http://arxiv.org/abs/2403.08685v1","category":"q-bio.QM"}
{"created":"2024-03-13 16:39:32","title":"Single file motion of robot swarms","abstract":"We present experimental results on the single file motion of a group of robots interacting with each other through position sensors. We successfully replicate the fundamental diagram typical of these systems, with a transition from free flow to congested traffic as the density of the system increases. In the latter scenario we also observe the characteristic stop-and-go waves. The unique advantages of this novel system, such as experimental stability and repeatability, allow for extended experimental runs, facilitating a comprehensive statistical analysis of the global dynamics. Above a certain density, we observe a divergence of the average jam duration and the average number of robots involved in it. This discovery enables us to precisely identify another transition: from congested intermittent flow (for intermediate densities) to a totally congested scenario for high densities. Beyond this finding, the present work demonstrates the suitability of robot swarms to model complex behaviors in many particle systems.","sentences":["We present experimental results on the single file motion of a group of robots interacting with each other through position sensors.","We successfully replicate the fundamental diagram typical of these systems, with a transition from free flow to congested traffic as the density of the system increases.","In the latter scenario we also observe the characteristic stop-and-go waves.","The unique advantages of this novel system, such as experimental stability and repeatability, allow for extended experimental runs, facilitating a comprehensive statistical analysis of the global dynamics.","Above a certain density, we observe a divergence of the average jam duration and the average number of robots involved in it.","This discovery enables us to precisely identify another transition: from congested intermittent flow (for intermediate densities) to a totally congested scenario for high densities.","Beyond this finding, the present work demonstrates the suitability of robot swarms to model complex behaviors in many particle systems."],"url":"http://arxiv.org/abs/2403.08683v1","category":"nlin.AO"}
{"created":"2024-03-13 16:38:26","title":"OneVOS: Unifying Video Object Segmentation with All-in-One Transformer Framework","abstract":"Contemporary Video Object Segmentation (VOS) approaches typically consist stages of feature extraction, matching, memory management, and multiple objects aggregation. Recent advanced models either employ a discrete modeling for these components in a sequential manner, or optimize a combined pipeline through substructure aggregation. However, these existing explicit staged approaches prevent the VOS framework from being optimized as a unified whole, leading to the limited capacity and suboptimal performance in tackling complex videos. In this paper, we propose OneVOS, a novel framework that unifies the core components of VOS with All-in-One Transformer. Specifically, to unify all aforementioned modules into a vision transformer, we model all the features of frames, masks and memory for multiple objects as transformer tokens, and integrally accomplish feature extraction, matching and memory management of multiple objects through the flexible attention mechanism. Furthermore, a Unidirectional Hybrid Attention is proposed through a double decoupling of the original attention operation, to rectify semantic errors and ambiguities of stored tokens in OneVOS framework. Finally, to alleviate the storage burden and expedite inference, we propose the Dynamic Token Selector, which unveils the working mechanism of OneVOS and naturally leads to a more efficient version of OneVOS. Extensive experiments demonstrate the superiority of OneVOS, achieving state-of-the-art performance across 7 datasets, particularly excelling in complex LVOS and MOSE datasets with 70.1% and 66.4% $J \\& F$ scores, surpassing previous state-of-the-art methods by 4.2% and 7.0%, respectively. And our code will be available for reproducibility and further research.","sentences":["Contemporary Video Object Segmentation (VOS) approaches typically consist stages of feature extraction, matching, memory management, and multiple objects aggregation.","Recent advanced models either employ a discrete modeling for these components in a sequential manner, or optimize a combined pipeline through substructure aggregation.","However, these existing explicit staged approaches prevent the VOS framework from being optimized as a unified whole, leading to the limited capacity and suboptimal performance in tackling complex videos.","In this paper, we propose OneVOS, a novel framework that unifies the core components of VOS with All-in-One Transformer.","Specifically, to unify all aforementioned modules into a vision transformer, we model all the features of frames, masks and memory for multiple objects as transformer tokens, and integrally accomplish feature extraction, matching and memory management of multiple objects through the flexible attention mechanism.","Furthermore, a Unidirectional Hybrid Attention is proposed through a double decoupling of the original attention operation, to rectify semantic errors and ambiguities of stored tokens in OneVOS framework.","Finally, to alleviate the storage burden and expedite inference, we propose the Dynamic Token Selector, which unveils the working mechanism of OneVOS and naturally leads to a more efficient version of OneVOS.","Extensive experiments demonstrate the superiority of OneVOS, achieving state-of-the-art performance across 7 datasets, particularly excelling in complex LVOS and MOSE datasets with 70.1% and 66.4% $J \\& F$ scores, surpassing previous state-of-the-art methods by 4.2% and 7.0%, respectively.","And our code will be available for reproducibility and further research."],"url":"http://arxiv.org/abs/2403.08682v1","category":"cs.CV"}
{"created":"2024-03-13 16:35:59","title":"Towards the THz Networks in the 6G Era","abstract":"This commentary dedicates to envision what role THz is going to play in the coming human-centric 6G era. Three distinct THz network types including outdoor, indoor, and body area networks are discussed, with an emphasis on their capabilities in human body detection. Synthesizing these networks will unlock a bunch of fascinating applications across industrial, biomedical and entertainment fields, significantly enhancing the quality of human life.","sentences":["This commentary dedicates to envision what role THz is going to play in the coming human-centric 6G era.","Three distinct THz network types including outdoor, indoor, and body area networks are discussed, with an emphasis on their capabilities in human body detection.","Synthesizing these networks will unlock a bunch of fascinating applications across industrial, biomedical and entertainment fields, significantly enhancing the quality of human life."],"url":"http://arxiv.org/abs/2403.08680v1","category":"eess.SY"}
{"created":"2024-03-13 16:34:12","title":"Antiferromagnetic ordering and glassy nature in NASICON type NaFe$_2$PO$_4$(SO$_4$)$_2$","abstract":"We investigate crystal structure and magnetic properties including spin relaxation and magnetocaloric effect in NASICON type NaFe$_2$PO$_4$(SO$_4$)$_2$ sample. The Rietveld refinement of x-ray and neutron diffraction patterns show a rhombohedral crystal structure with the R$\\bar{3}$c space group. The core-level spectra confirm the desired oxidation state of constituent elements. The {\\it dc}--magnetic susceptibility ($\\chi$) behavior in zero field-cooled (ZFC) and field-cooled (FC) modes show the ordering temperature $\\approx$50~K. Interestingly, the analysis of temperature dependent neutron diffraction patterns reveal an A-type antiferromagnetic (AFM) structure with the ordered moment of 3.8 $\\mu_{B}$/Fe$^{3+}$ at 5~K, and a magnetostriction below $T_{\\rm N}=$ 50~K. Further, the peak position in the {\\it ac}--$\\chi$ is found to be invariant with the excitation frequency supporting the notion of dominating AFM transition. Also, the unsaturated isothermal magnetization curve supports the AFM ordering of the moments; however, the observed coercivity suggests the presence of weak ferromagnetic (FM) correlations at 5~K. On the other hand, a clear bifurcation between ZFC and FC curves of {\\it dc}--$\\chi$ and the observed decrease in peak height of {\\it ac}--$\\chi$ with frequency suggest for the complex magnetic interactions. The spin relaxation behavior in thermo-remanent magnetization and aging measurements indicate the glassy states at 5~K. Moreover, the Arrott plots and magnetocaloric analysis reveal the AFM--FM interactions in the sample at lower temperatures.","sentences":["We investigate crystal structure and magnetic properties including spin relaxation and magnetocaloric effect in NASICON type NaFe$_2$PO$_4$(SO$_4$)$_2$ sample.","The Rietveld refinement of x-ray and neutron diffraction patterns show a rhombohedral crystal structure with the R$\\bar{3}$c space group.","The core-level spectra confirm the desired oxidation state of constituent elements.","The {\\it dc}--magnetic susceptibility ($\\chi$) behavior in zero field-cooled (ZFC) and field-cooled (FC) modes show the ordering temperature $\\approx$50~K. Interestingly, the analysis of temperature dependent neutron diffraction patterns reveal an A-type antiferromagnetic (AFM) structure with the ordered moment of 3.8 $\\mu_{B}$/Fe$^{3+}$ at 5~K, and a magnetostriction below $T_{\\rm N}=$ 50~K. Further, the peak position in the {\\it ac}--$\\chi$ is found to be invariant with the excitation frequency supporting the notion of dominating AFM transition.","Also, the unsaturated isothermal magnetization curve supports the AFM ordering of the moments; however, the observed coercivity suggests the presence of weak ferromagnetic (FM) correlations at 5~K. On the other hand, a clear bifurcation between ZFC and FC curves of {\\it dc}--$\\chi$ and the observed decrease in peak height of {\\it ac}--$\\chi$ with frequency suggest for the complex magnetic interactions.","The spin relaxation behavior in thermo-remanent magnetization and aging measurements indicate the glassy states at 5~K. Moreover, the Arrott plots and magnetocaloric analysis reveal the AFM--FM interactions in the sample at lower temperatures."],"url":"http://arxiv.org/abs/2403.08679v1","category":"cond-mat.str-el"}
{"created":"2024-03-13 16:14:29","title":"Room temperature charge density wave in a tetragonal polymorph of Gd2Os3Si5 and study of its origin in the R2T3X5 (R = Rare earth, T = transition metal, X = Si, Ge) series","abstract":"Charge density wave (CDW) systems are proposed to exhibit application potential for electronic and optoelectronic devices. Therefore, identifying new materials that exhibit a CDW state at room temperature is crucial for the development of CDW-based devices. Here, we present a non-layered tetragonal polymorph of Gd2Os3Si5, which exhibits a CDW state at room temperature. Gd2Os3Si5 crystallizes in the U2Mn3Si5-type tetragonal crystal structure with the space group P4/mnc. Single-crystal x-ray diffraction (SXRD) analysis shows that Gd2Os3Si5 possesses an incommensurately modulated structure with modulation wave vector q = (0.53, 0, 0), while the modulation reduces the symmetry to orthorhombic Cccm({\\sigma}00)0s0. This differs in contrast to isostructural Sm2Ru3Ge5, where the modulated phase has been reported to possess the superspace symmetry Pm({\\alpha} 0 {\\gamma})0. However, reinvestigation of Sm2Ru3Ge5 suggests that its modulated crystal structure can alternatively be described by Cccm({\\sigma}00)0s0, with modulations similar to Gd2Os3Si5. The magnetic susceptibility, \\c{hi}(T), exhibits a maximum at low temperatures that indicates an antiferromagnetic transition at TN = 5.5 K. The \\c{hi}(T) furthermore shows an anomaly at around 345 K, suggesting a CDW transition at TCDW = 345 K, that corroborates the result from high-temperature SXRD measurements. Interestingly, R2T3X5 compounds are known to crystallize either in the tetragonal Sc2Fe3Si5 type structure or in the orthorhombic U2Co3Si5 structure type. Not all of the compounds in the R2T3X5 series undergo CDW phase transitions. We find that R2T3X5 compounds will exhibit a CDW transition, if the condition : 0.526 < c/sqrt(ab) < 0.543 is satisfied. We suggest the wave vector-dependent electron-phonon coupling to be the dominant mechanism of CDW formation in the tetragonal polymorph of Gd2Os3Si5.","sentences":["Charge density wave (CDW) systems are proposed to exhibit application potential for electronic and optoelectronic devices.","Therefore, identifying new materials that exhibit a CDW state at room temperature is crucial for the development of CDW-based devices.","Here, we present a non-layered tetragonal polymorph of Gd2Os3Si5, which exhibits a CDW state at room temperature.","Gd2Os3Si5 crystallizes in the U2Mn3Si5-type tetragonal crystal structure with the space group P4/mnc.","Single-crystal x-ray diffraction (SXRD) analysis shows that Gd2Os3Si5 possesses an incommensurately modulated structure with modulation wave vector q = (0.53, 0, 0), while the modulation reduces the symmetry to orthorhombic Cccm({\\sigma}00)0s0.","This differs in contrast to isostructural Sm2Ru3Ge5, where the modulated phase has been reported to possess the superspace symmetry Pm({\\alpha} 0 {\\gamma})0.","However, reinvestigation of Sm2Ru3Ge5 suggests that its modulated crystal structure can alternatively be described by Cccm({\\sigma}00)0s0, with modulations similar to Gd2Os3Si5.","The magnetic susceptibility, \\c{hi}(T), exhibits a maximum at low temperatures that indicates an antiferromagnetic transition at TN = 5.5 K. The \\c{hi}(T) furthermore shows an anomaly at around 345 K, suggesting a CDW transition at TCDW = 345 K, that corroborates the result from high-temperature SXRD measurements.","Interestingly, R2T3X5 compounds are known to crystallize either in the tetragonal Sc2Fe3Si5 type structure or in the orthorhombic U2Co3Si5 structure type.","Not all of the compounds in the R2T3X5 series undergo CDW phase transitions.","We find that R2T3X5 compounds will exhibit a CDW transition, if the condition : 0.526 <","c/sqrt(ab) < 0.543 is satisfied.","We suggest the wave vector-dependent electron-phonon coupling to be the dominant mechanism of CDW formation in the tetragonal polymorph of Gd2Os3Si5."],"url":"http://arxiv.org/abs/2403.08660v1","category":"cond-mat.str-el"}
{"created":"2024-03-13 16:09:54","title":"Efficient electronic cooling above 2 K by niobium-based superconducting tunnel junctions","abstract":"Numerous applications, from industrial non-destructive imaging through ultra-sensitive photon counting to various implementations of solid-state quantum computers require low temperatures for their sensor and processor chips. Replacing the bulky cryo-liquid based cooling stages of cryo-enabled instruments by chip scale refrigeration is envisioned to disruptively reduce the system size similarly as microprocessors did for computers. Chip scale cooling has been demonstrated with electronic refrigerators based on tunnel junctions in the sub-1 K temperature range. Here, we extend the operation temperature to above 2 K, thereby, bridging the gap between electronic and pulse tube refrigerators. We report on scalable Al-AlOx-Nb superconducting tunnel junction cooler technology that can deliver electronic cooling power of ~ mW/mm^2, which is enough to demonstrate significant electron temperature reduction of 0.82 K against the phonon bath at 2.4 K (34% relative cooling). Our work shows that the key material of integrated superconducting circuits (niobium) enables powerful cryogenic refrigerator technology. This result is a prerequisite for practical cryogenic chip scale refrigerators and, at the same time, it introduces a new electro-thermal tool for quantum heat transport experiments.","sentences":["Numerous applications, from industrial non-destructive imaging through ultra-sensitive photon counting to various implementations of solid-state quantum computers require low temperatures for their sensor and processor chips.","Replacing the bulky cryo-liquid based cooling stages of cryo-enabled instruments by chip scale refrigeration is envisioned to disruptively reduce the system size similarly as microprocessors did for computers.","Chip scale cooling has been demonstrated with electronic refrigerators based on tunnel junctions in the sub-1 K temperature range.","Here, we extend the operation temperature to above 2 K, thereby, bridging the gap between electronic and pulse tube refrigerators.","We report on scalable Al-AlOx-Nb superconducting tunnel junction cooler technology that can deliver electronic cooling power of ~ mW/mm^2, which is enough to demonstrate significant electron temperature reduction of 0.82 K against the phonon bath at 2.4 K (34% relative cooling).","Our work shows that the key material of integrated superconducting circuits (niobium) enables powerful cryogenic refrigerator technology.","This result is a prerequisite for practical cryogenic chip scale refrigerators and, at the same time, it introduces a new electro-thermal tool for quantum heat transport experiments."],"url":"http://arxiv.org/abs/2403.08655v1","category":"cond-mat.supr-con"}
{"created":"2024-03-13 16:08:59","title":"An Efficient End-to-End Approach to Noise Invariant Speech Features via Multi-Task Learning","abstract":"Self-supervised speech representation learning enables the extraction of meaningful features from raw waveforms. These features can then be efficiently used across multiple downstream tasks. However, two significant issues arise when considering the deployment of such methods ``in-the-wild\": (i) Their large size, which can be prohibitive for edge applications; and (ii) their robustness to detrimental factors, such as noise and/or reverberation, that can heavily degrade the performance of such systems. In this work, we propose RobustDistiller, a novel knowledge distillation mechanism that tackles both problems jointly. Simultaneously to the distillation recipe, we apply a multi-task learning objective to encourage the network to learn noise-invariant representations by denoising the input. The proposed mechanism is evaluated on twelve different downstream tasks. It outperforms several benchmarks regardless of noise type, or noise and reverberation levels. Experimental results show that the new Student model with 23M parameters can achieve results comparable to the Teacher model with 95M parameters. Lastly, we show that the proposed recipe can be applied to other distillation methodologies, such as the recent DPWavLM. For reproducibility, code and model checkpoints will be made available at \\mbox{\\url{https://github.com/Hguimaraes/robustdistiller}}.","sentences":["Self-supervised speech representation learning enables the extraction of meaningful features from raw waveforms.","These features can then be efficiently used across multiple downstream tasks.","However, two significant issues arise when considering the deployment of such methods ``in-the-wild\": (i) Their large size, which can be prohibitive for edge applications; and (ii) their robustness to detrimental factors, such as noise and/or reverberation, that can heavily degrade the performance of such systems.","In this work, we propose RobustDistiller, a novel knowledge distillation mechanism that tackles both problems jointly.","Simultaneously to the distillation recipe, we apply a multi-task learning objective to encourage the network to learn noise-invariant representations by denoising the input.","The proposed mechanism is evaluated on twelve different downstream tasks.","It outperforms several benchmarks regardless of noise type, or noise and reverberation levels.","Experimental results show that the new Student model with 23M parameters can achieve results comparable to the Teacher model with 95M parameters.","Lastly, we show that the proposed recipe can be applied to other distillation methodologies, such as the recent DPWavLM.","For reproducibility, code and model checkpoints will be made available at \\mbox{\\url{https://github.com/Hguimaraes/robustdistiller}}."],"url":"http://arxiv.org/abs/2403.08654v1","category":"eess.AS"}
{"created":"2024-03-13 16:00:41","title":"Covariance Fitting Interferometric Phase Linking: Modular Framework and Optimization Algorithms","abstract":"Interferometric phase linking (IPL) has become a prominent technique for processing images of areas containing distributed scaterrers in SAR interferometry. Traditionally, IPL consists in estimating consistent phase differences between all pairs of SAR images in a time series from the sample covariance matrix of pixel patches on a sliding window. This paper reformulates this task as a covariance fitting problem: in this setup, IPL appears as a form of projection of an input covariance matrix so that it satisfies the phase closure property. Given this modular formulation, we propose an overview of covariance matrix estimates, regularization options, and matrix distances, that can be of interest when processing multi-temporal SAR data. In particular, we will observe that most of the existing IPL algorithms appear as special instances of this framework. We then present tools to efficiently solve related optimization problems on the torus of phase-only complex vectors: majorization-minimization and Riemannian optimization. We conclude by illustrating the merits of different options on a real-world case study.","sentences":["Interferometric phase linking (IPL) has become a prominent technique for processing images of areas containing distributed scaterrers in SAR interferometry.","Traditionally, IPL consists in estimating consistent phase differences between all pairs of SAR images in a time series from the sample covariance matrix of pixel patches on a sliding window.","This paper reformulates this task as a covariance fitting problem: in this setup, IPL appears as a form of projection of an input covariance matrix so that it satisfies the phase closure property.","Given this modular formulation, we propose an overview of covariance matrix estimates, regularization options, and matrix distances, that can be of interest when processing multi-temporal SAR data.","In particular, we will observe that most of the existing IPL algorithms appear as special instances of this framework.","We then present tools to efficiently solve related optimization problems on the torus of phase-only complex vectors: majorization-minimization and Riemannian optimization.","We conclude by illustrating the merits of different options on a real-world case study."],"url":"http://arxiv.org/abs/2403.08646v1","category":"stat.AP"}
{"created":"2024-03-13 15:54:57","title":"Reweight-annealing method for calculating the value of partition function via quantum Monte Carlo","abstract":"Efficient and accurate algorithm for partition function, free energy and thermal entropy calculations is of great significance in statistical physics and quantum many-body physics. Here we present an unbiased but low-technical-barrier algorithm within the quantum Monte Carlo framework, which has exceptionally high accuracy and no systemic error. Compared with the conventional specific heat integral method and Wang-Landau sampling algorithm, our method can obtain a much more accurate result of the sub-leading coefficient of the entropy. This method can be widely used in both classical and quantum Monte Carlo simulations and is easy to be parallelized on computer.","sentences":["Efficient and accurate algorithm for partition function, free energy and thermal entropy calculations is of great significance in statistical physics and quantum many-body physics.","Here we present an unbiased but low-technical-barrier algorithm within the quantum Monte Carlo framework, which has exceptionally high accuracy and no systemic error.","Compared with the conventional specific heat integral method and Wang-Landau sampling algorithm, our method can obtain a much more accurate result of the sub-leading coefficient of the entropy.","This method can be widely used in both classical and quantum Monte Carlo simulations and is easy to be parallelized on computer."],"url":"http://arxiv.org/abs/2403.08642v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-13 15:40:08","title":"Pulse-echo ultrasound attenuation tomography","abstract":"We present the first fully two-dimensional attenuation imaging technique developed for pulse-echo ultrasound systems. Unlike state-of-the-art techniques, which use line-by-line acquisitions, our method uses steered emissions to constrain attenuation values at each location with multiple crossing wave paths, essential to resolve the spatial variations of this tissue property. At every location, we compute normalized cross-correlations between the beamformed images that are obtained from emissions at different steering angles. We demonstrate that their log-amplitudes provide the changes between attenuation-induced amplitude losses undergone by the different incident waves. This allows us to formulate a linear tomographic problem, which we efficiently solve via a Tikhonov-regularized least-squares approach. The performance of our tomography technique is first validated in numerical examples and then experimentally demonstrated in custom-made tissue-mimicking phantoms with inclusions of varying size, echogenicity, and attenuation. We show that this technique is particularly good at resolving lateral variations in tissue attenuation and remains accurate in media with varying echogenicity. Based on a similar principle, this method can be easily combined with computed ultrasound tomography in echo mode (CUTE) for speed-of-sound imaging, paving the way towards a multi-modal ultrasound tomography framework characterizing multiple acoustic tissue properties simultaneously.","sentences":["We present the first fully two-dimensional attenuation imaging technique developed for pulse-echo ultrasound systems.","Unlike state-of-the-art techniques, which use line-by-line acquisitions, our method uses steered emissions to constrain attenuation values at each location with multiple crossing wave paths, essential to resolve the spatial variations of this tissue property.","At every location, we compute normalized cross-correlations between the beamformed images that are obtained from emissions at different steering angles.","We demonstrate that their log-amplitudes provide the changes between attenuation-induced amplitude losses undergone by the different incident waves.","This allows us to formulate a linear tomographic problem, which we efficiently solve via a Tikhonov-regularized least-squares approach.","The performance of our tomography technique is first validated in numerical examples and then experimentally demonstrated in custom-made tissue-mimicking phantoms with inclusions of varying size, echogenicity, and attenuation.","We show that this technique is particularly good at resolving lateral variations in tissue attenuation and remains accurate in media with varying echogenicity.","Based on a similar principle, this method can be easily combined with computed ultrasound tomography in echo mode (CUTE) for speed-of-sound imaging, paving the way towards a multi-modal ultrasound tomography framework characterizing multiple acoustic tissue properties simultaneously."],"url":"http://arxiv.org/abs/2403.08626v1","category":"physics.med-ph"}
{"created":"2024-03-13 15:40:06","title":"Variance Minimisation of the Lipkin-Meshkov-Glick Model on a Quantum Computer","abstract":"Quantum computing can potentially provide advantages for specific computational tasks. The simulation of fermionic systems is one such task that lends itself well to quantum computation, with applications in nuclear physics and electronic systems. Here we present work in which we use a variance minimisation method to find the full spectrum of energy eigenvalues of the Lipkin-Meshkov-Glick model; an exactly-solvable nuclear shell model-type system. We perform these calculations using both quantum simulators and real quantum hardware accessed via IBM cloud-based quantum computers. Using these IBM quantum computers we are able to obtain all eigenvalues for the cases of three and seven fermions (nucleons) in the Lipkin-Meshkov-Glick model.","sentences":["Quantum computing can potentially provide advantages for specific computational tasks.","The simulation of fermionic systems is one such task that lends itself well to quantum computation, with applications in nuclear physics and electronic systems.","Here we present work in which we use a variance minimisation method to find the full spectrum of energy eigenvalues of the Lipkin-Meshkov-Glick model; an exactly-solvable nuclear shell model-type system.","We perform these calculations using both quantum simulators and real quantum hardware accessed via IBM cloud-based quantum computers.","Using these IBM quantum computers we are able to obtain all eigenvalues for the cases of three and seven fermions (nucleons) in the Lipkin-Meshkov-Glick model."],"url":"http://arxiv.org/abs/2403.08625v1","category":"quant-ph"}
{"created":"2024-03-13 15:34:15","title":"Demailly-Lelong numbers on complex spaces","abstract":"We establish a pointwise comparison of two notions of Lelong numbers of plurisubharmonic functions defined on singular complex spaces. This shows a conjecture proposed by Berman-Boucksom-Eyssidieux-Guedj-Zeriahi, affirming that the Demailly-Lelong number can be determined through a combination of intersection numbers given by the divisorial part of the potential and the SNC divisors over a log resolution of the maximal ideal of a given point. We also provide an estimate for quotient singularities and sharp estimates for two-dimensional ADE singularities.","sentences":["We establish a pointwise comparison of two notions of Lelong numbers of plurisubharmonic functions defined on singular complex spaces.","This shows a conjecture proposed by Berman-Boucksom-Eyssidieux-Guedj-Zeriahi, affirming that the Demailly-Lelong number can be determined through a combination of intersection numbers given by the divisorial part of the potential and the SNC divisors over a log resolution of the maximal ideal of a given point.","We also provide an estimate for quotient singularities and sharp estimates for two-dimensional ADE singularities."],"url":"http://arxiv.org/abs/2403.08620v1","category":"math.CV"}
{"created":"2024-03-13 15:22:18","title":"An Algorithmic Theory of Simplicity in Mechanism Design","abstract":"A growing body of work in economics and computation focuses on the trade-off between implementability and simplicity in mechanism design. The goal is to develop a theory that not only allows to design an incentive structure easy to grasp for imperfectly rational agents, but also understand the ensuing limitations on the class of mechanisms that enforce it. In this context, the concept of OSP mechanisms has assumed a prominent role since they provably account for the absence of contingent reasoning skills, a specific cognitive limitation. For single-dimensional agents, it is known that OSP mechanisms need to use certain greedy algorithms.   In this work, we introduce a notion that interpolates between OSP and SOSP, a more stringent notion where agents only plan a subset of their own future moves. We provide an algorithmic characterization of this novel class of mechanisms for single-dimensional domains and binary allocation problems, that precisely measures the interplay between simplicity and implementability. We build on this to show how mechanisms based on reverse greedy algorithms (a.k.a., deferred acceptance auctions) are algorithmically more robust to imperfectly rationality than those adopting greedy algorithms.","sentences":["A growing body of work in economics and computation focuses on the trade-off between implementability and simplicity in mechanism design.","The goal is to develop a theory that not only allows to design an incentive structure easy to grasp for imperfectly rational agents, but also understand the ensuing limitations on the class of mechanisms that enforce it.","In this context, the concept of OSP mechanisms has assumed a prominent role since they provably account for the absence of contingent reasoning skills, a specific cognitive limitation.","For single-dimensional agents, it is known that OSP mechanisms need to use certain greedy algorithms.   ","In this work, we introduce a notion that interpolates between OSP and SOSP, a more stringent notion where agents only plan a subset of their own future moves.","We provide an algorithmic characterization of this novel class of mechanisms for single-dimensional domains and binary allocation problems, that precisely measures the interplay between simplicity and implementability.","We build on this to show how mechanisms based on reverse greedy algorithms (a.k.a., deferred acceptance auctions) are algorithmically more robust to imperfectly rationality than those adopting greedy algorithms."],"url":"http://arxiv.org/abs/2403.08610v1","category":"cs.GT"}
{"created":"2024-03-13 15:21:13","title":"Nuclear coherent $\u03c0^0$ photoproduction with charge-exchange and spin-flip rescattering","abstract":"In this work, we present an updated model for nuclear $\\pi^0$ photoproduction, which incorporates pion second-order rescattering on intermediate excited nuclear states. Our approach is based on the distorted wave impulse approximation in momentum space. The many-body medium effects are incorporated in the complex effective $\\Delta$ self-energy, employing the results of the recently developed second-order pion-nucleus scattering potential. The experimental data for ${}^{12}$C and ${}^{40}$Ca are successfully described without the need to fit the model parameters of the photoproduction amplitude as a result of incorporating the second-order part of nuclear photoproduction potential, which involves intermediate nucleon spin-flip and charge exchange.","sentences":["In this work, we present an updated model for nuclear $\\pi^0$ photoproduction, which incorporates pion second-order rescattering on intermediate excited nuclear states.","Our approach is based on the distorted wave impulse approximation in momentum space.","The many-body medium effects are incorporated in the complex effective $\\Delta$ self-energy, employing the results of the recently developed second-order pion-nucleus scattering potential.","The experimental data for ${}^{12}$C and ${}^{40}$Ca are successfully described without the need to fit the model parameters of the photoproduction amplitude as a result of incorporating the second-order part of nuclear photoproduction potential, which involves intermediate nucleon spin-flip and charge exchange."],"url":"http://arxiv.org/abs/2403.08608v1","category":"nucl-th"}
{"created":"2024-03-13 15:05:36","title":"The role of susceptible individuals in spreading dynamics","abstract":"Exploring the internal mechanism of information spreading is critical for understanding and controlling the process. Traditional spreading models often assume individuals play the same role in the spreading process. In reality, however, individuals' diverse characteristics contribute differently to the spreading performance, leading to a heterogeneous infection rate across the system. To investigate network spreading dynamics under heterogeneous infection rates, we integrate two individual-level features -- influence (i.e., the ability to influence neighbors) and susceptibility (i.e., the extent to be influenced by neighbors) -- into the independent cascade model. Our findings reveal significant differences in spreading performance under heterogeneous and constant infection rates, with traditional structural centrality metrics proving more effective in the latter scenario. Additionally, we take the constant and heterogeneous infection rates into a state-of-the-art maximization algorithm, the well-known TIM algorithm, and find the seeds selected by heterogeneous infection rates are more dispersed compared to those under constant rates. Lastly, we find that both individuals' influence and susceptibility are vital to the spreading performance. Strikingly, susceptible individuals are particularly important to spreading when information is disseminated by social celebrities. By integrating influence and susceptibility into the spreading model, we gain a more profound understanding of the underlying mechanisms driving information spreading.","sentences":["Exploring the internal mechanism of information spreading is critical for understanding and controlling the process.","Traditional spreading models often assume individuals play the same role in the spreading process.","In reality, however, individuals' diverse characteristics contribute differently to the spreading performance, leading to a heterogeneous infection rate across the system.","To investigate network spreading dynamics under heterogeneous infection rates, we integrate two individual-level features -- influence (i.e., the ability to influence neighbors) and susceptibility (i.e., the extent to be influenced by neighbors) -- into the independent cascade model.","Our findings reveal significant differences in spreading performance under heterogeneous and constant infection rates, with traditional structural centrality metrics proving more effective in the latter scenario.","Additionally, we take the constant and heterogeneous infection rates into a state-of-the-art maximization algorithm, the well-known TIM algorithm, and find the seeds selected by heterogeneous infection rates are more dispersed compared to those under constant rates.","Lastly, we find that both individuals' influence and susceptibility are vital to the spreading performance.","Strikingly, susceptible individuals are particularly important to spreading when information is disseminated by social celebrities.","By integrating influence and susceptibility into the spreading model, we gain a more profound understanding of the underlying mechanisms driving information spreading."],"url":"http://arxiv.org/abs/2403.08599v1","category":"cs.SI"}
{"created":"2024-03-13 14:46:38","title":"Quantum plasmonics model of refractive index sensing using photon correlations","abstract":"The interaction between the electric dipole moments of a quantum emitter and a metal nanoparticle gives rise to unique optical properties, such as interference-induced photon correlations, that could be useful for enhanced intensity-based sensing. Using the quantum theory of photodetection, we propose a nanosensor system comprising a quantum emitter and a metal nanoparticle that explores the possibility of utilizing higher-order photon correlations for refractive index sensing. Both the refractive index sensitivity and resolution of the nanosensor, whose scattering spectrum lies within the visible region, are predicted. The sensor is supported by a substrate and driven weakly by a coherent field. By calculating the mean photocount and its second factorial moment resulting from the scattered field of the system, the sensing performance of the intensity and intensity-intensity correlation, are compared at optimal driving wavelengths. The mean photocount was found to be inherently low, inhibiting the role of interference-induced photon antibunching in minimizing the sensor's intensity shot noise. However, a regime in which the noise could be reduced below the shot noise limit is identified, leading to a quantum enhancement in the sensing performance.","sentences":["The interaction between the electric dipole moments of a quantum emitter and a metal nanoparticle gives rise to unique optical properties, such as interference-induced photon correlations, that could be useful for enhanced intensity-based sensing.","Using the quantum theory of photodetection, we propose a nanosensor system comprising a quantum emitter and a metal nanoparticle that explores the possibility of utilizing higher-order photon correlations for refractive index sensing.","Both the refractive index sensitivity and resolution of the nanosensor, whose scattering spectrum lies within the visible region, are predicted.","The sensor is supported by a substrate and driven weakly by a coherent field.","By calculating the mean photocount and its second factorial moment resulting from the scattered field of the system, the sensing performance of the intensity and intensity-intensity correlation, are compared at optimal driving wavelengths.","The mean photocount was found to be inherently low, inhibiting the role of interference-induced photon antibunching in minimizing the sensor's intensity shot noise.","However, a regime in which the noise could be reduced below the shot noise limit is identified, leading to a quantum enhancement in the sensing performance."],"url":"http://arxiv.org/abs/2403.08588v1","category":"quant-ph"}
{"created":"2024-03-13 14:35:47","title":"Simultaneous mapping of magnetic and atomic structure for direct visualization of nanoscale magnetoelastic coupling","abstract":"Achieving a correlative measurement of both magnetic and atomic structures at the nanoscale is imperative to understand the fundamental magnetism of matters and for fostering the development of new magnetic nanomaterials. Conventional microscopy methods fall short in providing the two information simultaneously. Here, we develop a new approach to simultaneously map the magnetic field and atomic structure at the nanoscale using Lorentz 4-dimensional scanning transmission electron microscopy (Ltz-4D-STEM). This method enables precise measurement of the characteristic atomic and magnetic structures across an extensive field of view, a critical aspect for investigating real-world ferromagnetic materials. It offers a comprehensive visualization and statistical evaluation of the different structural information at a pixel-by-pixel correlation. The new method allows to directly visualize the magnetoelastic coupling and the resulting complex magnetization arrangement as well as the competition between magnetoelastic and magnetostatic energy. This approach opens new avenues for in-depth studying the structure-property correlation of nanoscale magnetic materials.","sentences":["Achieving a correlative measurement of both magnetic and atomic structures at the nanoscale is imperative to understand the fundamental magnetism of matters and for fostering the development of new magnetic nanomaterials.","Conventional microscopy methods fall short in providing the two information simultaneously.","Here, we develop a new approach to simultaneously map the magnetic field and atomic structure at the nanoscale using Lorentz 4-dimensional scanning transmission electron microscopy (Ltz-4D-STEM).","This method enables precise measurement of the characteristic atomic and magnetic structures across an extensive field of view, a critical aspect for investigating real-world ferromagnetic materials.","It offers a comprehensive visualization and statistical evaluation of the different structural information at a pixel-by-pixel correlation.","The new method allows to directly visualize the magnetoelastic coupling and the resulting complex magnetization arrangement as well as the competition between magnetoelastic and magnetostatic energy.","This approach opens new avenues for in-depth studying the structure-property correlation of nanoscale magnetic materials."],"url":"http://arxiv.org/abs/2403.08582v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-13 14:33:50","title":"Coherent competition and control between three-wave mixing and four-wave mixing in superconducting circuits","abstract":"Exploring intermixing and interplay between different frequency-mixing processes has always been one of the interesting subjects at the interface of nonlinear optics with quantum optics. Here we investigate coherent competition and control between three-wave mixing (TWM) and four-wave mixing (FWM) in a cyclic three-level superconducting quantum system. In the weak control-field regime, strong competition leads to an alternating oscillation between TWM and FWM signals and this oscillation is a signature of strong energy exchange between these two nonlinear processes. In particular, such oscillation is absent from conventional multi-wave mixing in atomic systems. Surprisingly, synchronous TWM and FWM processes are demonstrated in the strong control-field regime and, at the same time, their efficiencies can be as high as 40% and 45%, respectively. Our study shows that these competitive behaviors between TWM and FWM can be manipulated by tuning the control-field intensity.","sentences":["Exploring intermixing and interplay between different frequency-mixing processes has always been one of the interesting subjects at the interface of nonlinear optics with quantum optics.","Here we investigate coherent competition and control between three-wave mixing (TWM) and four-wave mixing (FWM) in a cyclic three-level superconducting quantum system.","In the weak control-field regime, strong competition leads to an alternating oscillation between TWM and FWM signals and this oscillation is a signature of strong energy exchange between these two nonlinear processes.","In particular, such oscillation is absent from conventional multi-wave mixing in atomic systems.","Surprisingly, synchronous TWM and FWM processes are demonstrated in the strong control-field regime and, at the same time, their efficiencies can be as high as 40% and 45%, respectively.","Our study shows that these competitive behaviors between TWM and FWM can be manipulated by tuning the control-field intensity."],"url":"http://arxiv.org/abs/2403.08578v1","category":"quant-ph"}
{"created":"2024-03-13 14:29:10","title":"Optimizing Conical Intersections Without Explicit Use of Non-Adiabatic Couplings","abstract":"We present two alternative methods for optimizing minimum energy conical intersection (MECI) molecular geometries without knowledge of the derivative coupling (DC). These methods are based on the utilization of Lagrange multipliers: i) one method uses an approximate calculation of the DC, while the other ii) do not require the DC. Both methods use the fact that information of the DC is contained in the Hessian of the squared energy difference. Tests done on a set of small molecular systems, in comparison with other methods, show the ability of the proposed methods to optimize MECIs. Finally, we apply the methods to the furimamide molecule, to optimize and characterize its S$_1$ /S$_2$ MECI, and to optimizing the S$_0$ /S$_1$ MECI of the silver trimer.","sentences":["We present two alternative methods for optimizing minimum energy conical intersection (MECI) molecular geometries without knowledge of the derivative coupling (DC).","These methods are based on the utilization of Lagrange multipliers: i) one method uses an approximate calculation of the DC, while the other ii) do not require the DC.","Both methods use the fact that information of the DC is contained in the Hessian of the squared energy difference.","Tests done on a set of small molecular systems, in comparison with other methods, show the ability of the proposed methods to optimize MECIs.","Finally, we apply the methods to the furimamide molecule, to optimize and characterize its S$_1$ /S$_2$ MECI, and to optimizing the S$_0$ /S$_1$ MECI of the silver trimer."],"url":"http://arxiv.org/abs/2403.08574v1","category":"physics.chem-ph"}
{"created":"2024-03-13 14:29:09","title":"System-bath correlations and finite-time operation enhance the efficiency of a dissipative quantum battery","abstract":"The reduced state of a small system strongly coupled to a thermal bath may be athermal and used as a small battery once disconnected. If the disconnecting process is too slow, the coupling between the battery and the bath weakens, and at some point, the battery will be in a thermal state that can not be used as a battery. Thus, the unitarily extractable energy (a.k.a ergotropy) decreases with the disconnection time. The work required to disconnect the battery also depends on the disconnection time. We study the efficiency of this battery, defined as the ratio between the ergotropy to the work cost of disconnecting and connecting the battery back to the bath to close the cycle, as a function of the disconnecting time in the Caldeira-Leggett model of a quantum battery. We consider two scenarios. In the first scenario, we assume that the discharged battery is uncorrelated to the bath at the connecting time and find that the efficiency peaks at an optimal disconnecting time. In the second scenario, the discharged battery is correlated to the bath, and find that the optimal efficiency corresponds to an instantaneous disconnection. On top of these results, we analyze various thermodynamic quantities for these Caldeira-Leggett quantum batteries that allow us to express the first and second laws of thermodynamics in the mentioned cycles in simple form despite the system-bath initial correlations and strong coupling regime of the working device.","sentences":["The reduced state of a small system strongly coupled to a thermal bath may be athermal and used as a small battery once disconnected.","If the disconnecting process is too slow, the coupling between the battery and the bath weakens, and at some point, the battery will be in a thermal state that can not be used as a battery.","Thus, the unitarily extractable energy (a.k.a ergotropy) decreases with the disconnection time.","The work required to disconnect the battery also depends on the disconnection time.","We study the efficiency of this battery, defined as the ratio between the ergotropy to the work cost of disconnecting and connecting the battery back to the bath to close the cycle, as a function of the disconnecting time in the Caldeira-Leggett model of a quantum battery.","We consider two scenarios.","In the first scenario, we assume that the discharged battery is uncorrelated to the bath at the connecting time and find that the efficiency peaks at an optimal disconnecting time.","In the second scenario, the discharged battery is correlated to the bath, and find that the optimal efficiency corresponds to an instantaneous disconnection.","On top of these results, we analyze various thermodynamic quantities for these Caldeira-Leggett quantum batteries that allow us to express the first and second laws of thermodynamics in the mentioned cycles in simple form despite the system-bath initial correlations and strong coupling regime of the working device."],"url":"http://arxiv.org/abs/2403.08573v1","category":"quant-ph"}
{"created":"2024-03-13 14:25:16","title":"Ab initio Density Response and Local Field Factor of Warm Dense Hydrogen","abstract":"We present quasi-exact ab initio path integral Monte Carlo (PIMC) results for the partial static density responses and local field factors of hydrogen in the warm dense matter regime, from solid density conditions to the strongly compressed case. The full dynamic treatment of electrons and protons on the same footing allows us to rigorously quantify both electronic and ionic exchange--correlation effects in the system, and to compare with earlier incomplete models such as the archetypal uniform electron gas [Phys. Rev. Lett. 125, 235001 (2020)] or electrons in a fixed ion snapshot potential [Phys. Rev. Lett. 129, 066402 (2022)] that do not take into account the interplay between the two constituents. The full electronic density response is highly sensitive to electronic localization around the ions, and our results constitute unambiguous predictions for upcoming X-ray Thomson scattering (XRTS) experiments with hydrogen jets and fusion plasmas. All PIMC results are made freely available and can directly be used for a gamut of applications, including inertial confinement fusion calculations and the modelling of dense astrophysical objects. Moreover, they constitute invaluable benchmark data for approximate but computationally less demanding approaches such as density functional theory or PIMC within the fixed-node approximation.","sentences":["We present quasi-exact ab initio path integral Monte Carlo (PIMC) results for the partial static density responses and local field factors of hydrogen in the warm dense matter regime, from solid density conditions to the strongly compressed case.","The full dynamic treatment of electrons and protons on the same footing allows us to rigorously quantify both electronic and ionic exchange--correlation effects in the system, and to compare with earlier incomplete models such as the archetypal uniform electron gas","[Phys. Rev. Lett.","125, 235001 (2020)] or electrons in a fixed ion snapshot potential [Phys. Rev. Lett.","129, 066402 (2022)] that do not take into account the interplay between the two constituents.","The full electronic density response is highly sensitive to electronic localization around the ions, and our results constitute unambiguous predictions for upcoming X-ray Thomson scattering (XRTS) experiments with hydrogen jets and fusion plasmas.","All PIMC results are made freely available and can directly be used for a gamut of applications, including inertial confinement fusion calculations and the modelling of dense astrophysical objects.","Moreover, they constitute invaluable benchmark data for approximate but computationally less demanding approaches such as density functional theory or PIMC within the fixed-node approximation."],"url":"http://arxiv.org/abs/2403.08570v1","category":"physics.plasm-ph"}
{"created":"2024-03-13 14:06:53","title":"Heterogeneous Nucleation and Growth of Sessile Chemically Active Droplets","abstract":"Droplets are essential for spatially controlling biomolecules in cells. To work properly, cells need to control the emergence and morphology of droplets. On the one hand, driven chemical reactions can affect droplets profoundly. For instance, reactions can control how droplets nucleate and how large they grow. On the other hand, droplets coexist with various organelles and other structures inside cells, which could affect their nucleation and morphology. To understand the interplay of these two aspects, we study a continuous field theory of active phase separation. Our numerical simulations reveal that reactions suppress nucleation while attractive walls enhance it. Intriguingly, these two effects are coupled, leading to shapes that deviate substantially from the spherical caps predicted for passive systems. These distortions result from anisotropic fluxes responding to the boundary conditions dictated by the Young-Dupr\\'e equation. Interestingly, an electrostatic analogy of chemical reactions confirms these effects. We thus demonstrate how driven chemical reactions affect the emergence and morphology of droplets, which could be crucial for understanding biological cells and improving technical applications, e.g., in chemical engineering.","sentences":["Droplets are essential for spatially controlling biomolecules in cells.","To work properly, cells need to control the emergence and morphology of droplets.","On the one hand, driven chemical reactions can affect droplets profoundly.","For instance, reactions can control how droplets nucleate and how large they grow.","On the other hand, droplets coexist with various organelles and other structures inside cells, which could affect their nucleation and morphology.","To understand the interplay of these two aspects, we study a continuous field theory of active phase separation.","Our numerical simulations reveal that reactions suppress nucleation while attractive walls enhance it.","Intriguingly, these two effects are coupled, leading to shapes that deviate substantially from the spherical caps predicted for passive systems.","These distortions result from anisotropic fluxes responding to the boundary conditions dictated by the Young-Dupr\\'e equation.","Interestingly, an electrostatic analogy of chemical reactions confirms these effects.","We thus demonstrate how driven chemical reactions affect the emergence and morphology of droplets, which could be crucial for understanding biological cells and improving technical applications, e.g., in chemical engineering."],"url":"http://arxiv.org/abs/2403.08555v1","category":"cond-mat.soft"}
{"created":"2024-03-13 14:06:18","title":"Regret Analysis of Policy Optimization over Submanifolds for Linearly Constrained Online LQG","abstract":"Recent advancement in online optimization and control has provided novel tools to study online linear quadratic regulator (LQR) problems, where cost matrices are varying adversarially over time. However, the controller parameterization of existing works may not satisfy practical conditions like sparsity due to physical connections. In this work, we study online linear quadratic Gaussian problems with a given linear constraint imposed on the controller. Inspired by the recent work of [1] which proposed, for a linearly constrained policy optimization of an offline LQR, a second order method equipped with a Riemannian metric that emerges naturally in the context of optimal control problems, we propose online optimistic Newton on manifold (OONM) which provides an online controller based on the prediction on the first and second order information of the function sequence. To quantify the proposed algorithm, we leverage the notion of regret defined as the sub-optimality of its cumulative cost to that of a (locally) minimizing controller sequence and provide the regret bound in terms of the path-length of the minimizer sequence. Simulation results are also provided to verify the property of OONM.","sentences":["Recent advancement in online optimization and control has provided novel tools to study online linear quadratic regulator (LQR) problems, where cost matrices are varying adversarially over time.","However, the controller parameterization of existing works may not satisfy practical conditions like sparsity due to physical connections.","In this work, we study online linear quadratic Gaussian problems with a given linear constraint imposed on the controller.","Inspired by the recent work of [1] which proposed, for a linearly constrained policy optimization of an offline LQR, a second order method equipped with a Riemannian metric that emerges naturally in the context of optimal control problems, we propose online optimistic Newton on manifold (OONM) which provides an online controller based on the prediction on the first and second order information of the function sequence.","To quantify the proposed algorithm, we leverage the notion of regret defined as the sub-optimality of its cumulative cost to that of a (locally) minimizing controller sequence and provide the regret bound in terms of the path-length of the minimizer sequence.","Simulation results are also provided to verify the property of OONM."],"url":"http://arxiv.org/abs/2403.08553v1","category":"math.OC"}
{"created":"2024-03-13 13:59:36","title":"Crash Chronicles: relative contribution from comets and carbonaceous asteroids to Earth's volatile budget in the context of an Early Instability","abstract":"Recent models of solar system formation suggest that a dynamical instability among the giant planets happened within the first 100 Myr after disk dispersal, perhaps before the Moon-forming impact. As a direct consequence, a bombardment of volatile-rich impactors may have taken place on Earth before internal and atmospheric reservoirs were decoupled. However, such a timing has been interpreted to potentially be at odds with the disparate inventories of Xe isotopes in Earth's mantle compared to its atmosphere. This study aims to assess the dynamical effects of an Early Instability on the delivery of carbonaceous asteroids and comets to Earth, and address the implications for the Earth's volatile budget. We perform 20 high-resolution dynamical simulations of solar system formation from the time of gas disk dispersal, each starting with 1600 carbonaceous asteroids and 10000 comets, taking into account the dynamical perturbations from an early giant planet instability. Before the Moon-forming impact, the cumulative collision rate of comets with Earth is about 4 orders of magnitude lower than that of carbonaceous asteroids. After the Moon-forming impact, this ratio either decreases or increases, often by orders of magnitude, depending on the dynamics of individual simulations. An increase in the relative contribution of comets happens in 30\\% of our simulations. In these cases, the delivery of noble gases from each source is comparable, given that the abundance of 132Xe is 3 orders of magnitude greater in comets than in carbonaceous chondrites. The increase in cometary flux relative to carbonaceous asteroids at late times may thus offer an explanation for the Xe signature dichotomy between the Earth's mantle and atmosphere.","sentences":["Recent models of solar system formation suggest that a dynamical instability among the giant planets happened within the first 100 Myr after disk dispersal, perhaps before the Moon-forming impact.","As a direct consequence, a bombardment of volatile-rich impactors may have taken place on Earth before internal and atmospheric reservoirs were decoupled.","However, such a timing has been interpreted to potentially be at odds with the disparate inventories of Xe isotopes in Earth's mantle compared to its atmosphere.","This study aims to assess the dynamical effects of an Early Instability on the delivery of carbonaceous asteroids and comets to Earth, and address the implications for the Earth's volatile budget.","We perform 20 high-resolution dynamical simulations of solar system formation from the time of gas disk dispersal, each starting with 1600 carbonaceous asteroids and 10000 comets, taking into account the dynamical perturbations from an early giant planet instability.","Before the Moon-forming impact, the cumulative collision rate of comets with Earth is about 4 orders of magnitude lower than that of carbonaceous asteroids.","After the Moon-forming impact, this ratio either decreases or increases, often by orders of magnitude, depending on the dynamics of individual simulations.","An increase in the relative contribution of comets happens in 30\\% of our simulations.","In these cases, the delivery of noble gases from each source is comparable, given that the abundance of 132Xe is 3 orders of magnitude greater in comets than in carbonaceous chondrites.","The increase in cometary flux relative to carbonaceous asteroids at late times may thus offer an explanation for the Xe signature dichotomy between the Earth's mantle and atmosphere."],"url":"http://arxiv.org/abs/2403.08545v1","category":"astro-ph.EP"}
{"created":"2024-03-13 13:52:23","title":"Calibrating coordinate system alignment in a scanning transmission electron microscope using a digital twin","abstract":"In four-dimensional scanning transmission electron microscopy (4D STEM) a focused beam is scanned over a specimen and a diffraction pattern is recorded at each position using a pixelated detector. During the experiment, it must be ensured that the scan coordinate system of the beam is correctly calibrated relative to the detector coordinate system. Various simplified and approximate models are used implicitly and explicitly for understanding and analyzing the recorded data, requiring translation between the physical reality of the instrument and the abstractions used in data interpretation. Here, we introduce a calibration method where interactive live data processing in combination with a digital twin is used to match a set of models and their parameters with the action of a real-world instrument.","sentences":["In four-dimensional scanning transmission electron microscopy (4D STEM) a focused beam is scanned over a specimen and a diffraction pattern is recorded at each position using a pixelated detector.","During the experiment, it must be ensured that the scan coordinate system of the beam is correctly calibrated relative to the detector coordinate system.","Various simplified and approximate models are used implicitly and explicitly for understanding and analyzing the recorded data, requiring translation between the physical reality of the instrument and the abstractions used in data interpretation.","Here, we introduce a calibration method where interactive live data processing in combination with a digital twin is used to match a set of models and their parameters with the action of a real-world instrument."],"url":"http://arxiv.org/abs/2403.08538v1","category":"physics.ins-det"}
{"created":"2024-03-13 13:42:59","title":"Free fermions, neutrality and modular transformations","abstract":"With a view towards higher-spin applications, we study the partition function of a free complex fermion in 2d CFT, restricted to the neutral (zero fermion number) sector. This restriction leads to a partial theta function with a combinatoric interpretation in terms of Dyson's crank of a partition. More crucially, this partition function can be expressed in terms of a q-hypergeometric function with quantum modular properties. This allows us to find its high-temperature asymptotics, including subleading terms which agree with, but also go beyond, what one obtains by imposing neutrality thermodynamically through a chemical potential. We evaluate the asymptotic density of states for this neutral partition function, including the first few subleading terms. Our results should be extendable to more fermions, as well as to higher-spin chemical potentials, which would be of relevance to the higher-spin/minimal model correspondence.","sentences":["With a view towards higher-spin applications, we study the partition function of a free complex fermion in 2d CFT, restricted to the neutral (zero fermion number) sector.","This restriction leads to a partial theta function with a combinatoric interpretation in terms of Dyson's crank of a partition.","More crucially, this partition function can be expressed in terms of a q-hypergeometric function with quantum modular properties.","This allows us to find its high-temperature asymptotics, including subleading terms which agree with, but also go beyond, what one obtains by imposing neutrality thermodynamically through a chemical potential.","We evaluate the asymptotic density of states for this neutral partition function, including the first few subleading terms.","Our results should be extendable to more fermions, as well as to higher-spin chemical potentials, which would be of relevance to the higher-spin/minimal model correspondence."],"url":"http://arxiv.org/abs/2403.08531v1","category":"hep-th"}
{"created":"2024-03-13 13:32:32","title":"Analytical Forward Dynamics Modeling of Linearly Actuated Heavy-Duty Parallel-Serial Manipulators","abstract":"This paper presents a new geometric and recursive algorithm for analytically computing the forward dynamics of heavy-duty parallel-serial mechanisms. Our solution relies on expressing the dynamics of a class of linearly-actuated parallel mechanism to a lower dimensional dual Lie algebra to find an analytical solution for the inverse dynamics problem. Thus, by applying the articulated-body inertias method, we successfully provide analytic expressions for the total wrench in the linear-actuator reference frame, the linear acceleration of the actuator, and the total wrench exerted in the base reference frame of the closed loop. This new formulation allows to backwardly project and assemble inertia matrices and wrench bias of multiple closed-loops mechanisms. The final algorithm holds an O(n) algorithmic complexity, where $n$ is the number of degrees of freedom (DoF). We provide accuracy results to demonstrate its efficiency with 1-DoF closed-loop mechanism and 4-DoF manipulator composed by serial and parallel mechanisms. Additionally, we release a URDF multi-DoF code for this recursive algorithm.","sentences":["This paper presents a new geometric and recursive algorithm for analytically computing the forward dynamics of heavy-duty parallel-serial mechanisms.","Our solution relies on expressing the dynamics of a class of linearly-actuated parallel mechanism to a lower dimensional dual Lie algebra to find an analytical solution for the inverse dynamics problem.","Thus, by applying the articulated-body inertias method, we successfully provide analytic expressions for the total wrench in the linear-actuator reference frame, the linear acceleration of the actuator, and the total wrench exerted in the base reference frame of the closed loop.","This new formulation allows to backwardly project and assemble inertia matrices and wrench bias of multiple closed-loops mechanisms.","The final algorithm holds an O(n) algorithmic complexity, where $n$ is the number of degrees of freedom (DoF).","We provide accuracy results to demonstrate its efficiency with 1-DoF closed-loop mechanism and 4-DoF manipulator composed by serial and parallel mechanisms.","Additionally, we release a URDF multi-DoF code for this recursive algorithm."],"url":"http://arxiv.org/abs/2403.08524v1","category":"cs.RO"}
{"created":"2024-03-13 13:28:11","title":"Colloidal Homogenisation for the Hydrodynamics of Nematic Liquid Crystals","abstract":"This paper analytically explores a simplified model for the hydrodynamics of nematic liquid crystal colloids. We integrate a Stokes equation for the velocity field with a Ginzburg-Landau transported heat flow for the director field. The study focuses on a bounded spatial domain containing periodically distributed colloidal particles, which impose no-anchoring conditions on the nematic liquid crystal. By progressively reducing the particle size to zero and simultaneously increasing the number of particles, we delve into the associated homogenisation problem. Our analysis uncovers a form of decoupling where the velocity field asymptotically satisfies a Darcy equation, independent of the director, while the director follows a gradient flow, unaffected by the velocity field. One of the most intricate aspects of the homogenisation process is the absence of an extension operator for the director field that preserves the uniform estimates related to the system's energy. We address this challenge with a novel variation of the Aubin-Lions lemma, specifically adapted for homogenisation problems.","sentences":["This paper analytically explores a simplified model for the hydrodynamics of nematic liquid crystal colloids.","We integrate a Stokes equation for the velocity field with a Ginzburg-Landau transported heat flow for the director field.","The study focuses on a bounded spatial domain containing periodically distributed colloidal particles, which impose no-anchoring conditions on the nematic liquid crystal.","By progressively reducing the particle size to zero and simultaneously increasing the number of particles, we delve into the associated homogenisation problem.","Our analysis uncovers a form of decoupling where the velocity field asymptotically satisfies a Darcy equation, independent of the director, while the director follows a gradient flow, unaffected by the velocity field.","One of the most intricate aspects of the homogenisation process is the absence of an extension operator for the director field that preserves the uniform estimates related to the system's energy.","We address this challenge with a novel variation of the Aubin-Lions lemma, specifically adapted for homogenisation problems."],"url":"http://arxiv.org/abs/2403.08520v1","category":"math.AP"}
{"created":"2024-03-13 13:27:40","title":"Projective Quantum Eigensolver via Adiabatically Decoupled Subsystem Evolution: a Resource Efficient Approach to Molecular Energetics in Noisy Quantum Computers","abstract":"Quantum computers hold immense potential in the field of chemistry, ushering new frontiers to solve complex many body problems that are beyond the reach of classical computers. However, noise in the current quantum hardware limits their applicability to large chemical systems. This work encompasses the development of a projective formalism that aims to compute ground-state energies of molecular systems accurately using Noisy Intermediate Scale Quantum (NISQ) hardware in a resource efficient manner. Our approach is reliant upon the formulation of a bipartitely decoupled parameterized ansatz within the disentangled unitary coupled cluster (dUCC) framework based on the principles of synergetics. Such decoupling emulates the total parameter optimization in a lower dimensional manifold, while a mutual synergistic relationship among the parameters is exploited to ensure characteristic accuracy. Without any pre-circuit measurements, our method leads to a highly compact fixed-depth ansatz with shallower circuits and fewer expectation value evaluations. Through analytical and numerical demonstrations, we demonstrate the method's superior performance under noise while concurrently ensuring requisite accuracy in future fault-tolerant systems. This approach enables rapid exploration of emerging chemical spaces by efficient utilization of near-term quantum hardware resources.","sentences":["Quantum computers hold immense potential in the field of chemistry, ushering new frontiers to solve complex many body problems that are beyond the reach of classical computers.","However, noise in the current quantum hardware limits their applicability to large chemical systems.","This work encompasses the development of a projective formalism that aims to compute ground-state energies of molecular systems accurately using Noisy Intermediate Scale Quantum (NISQ) hardware in a resource efficient manner.","Our approach is reliant upon the formulation of a bipartitely decoupled parameterized ansatz within the disentangled unitary coupled cluster (dUCC) framework based on the principles of synergetics.","Such decoupling emulates the total parameter optimization in a lower dimensional manifold, while a mutual synergistic relationship among the parameters is exploited to ensure characteristic accuracy.","Without any pre-circuit measurements, our method leads to a highly compact fixed-depth ansatz with shallower circuits and fewer expectation value evaluations.","Through analytical and numerical demonstrations, we demonstrate the method's superior performance under noise while concurrently ensuring requisite accuracy in future fault-tolerant systems.","This approach enables rapid exploration of emerging chemical spaces by efficient utilization of near-term quantum hardware resources."],"url":"http://arxiv.org/abs/2403.08519v1","category":"quant-ph"}
{"created":"2024-03-13 13:24:24","title":"Plotinus: A Satellite Internet Digital Twin System","abstract":"The development of integrated space-air-ground network (SAGIN) requires sophisticated satellite Internet emulation tools that can handle complex, dynamic topologies and offer in-depth analysis. Existing emulation platforms struggle with challenges like the need for detailed implementation across all network layers, real-time response times, and the ability to scale. Plotinus, a new digital twin system based on microservices for satellite Internet emulation, aims to solve these problems. It features a modular design, allowing for easy replacement of the physical layer to emulate different aerial vehicles and analyze channel interference. It also enables the replacement of path computation methods to simplify testing and deploying algorithms. In particular, Plotinus allows for real-time emulation with live network traffic, enhancing the realism of network models. Evaluation result shows that Plotinus's effective emulation of dynamic satellite networks with real-world devices. Its adaptability for various communication models and algorithm testing highlights Plotinus's role as a vital tool for developing and analyzing SAGIN systems, offering a scalable, real-time response, and flexible digital twin system.","sentences":["The development of integrated space-air-ground network (SAGIN) requires sophisticated satellite Internet emulation tools that can handle complex, dynamic topologies and offer in-depth analysis.","Existing emulation platforms struggle with challenges like the need for detailed implementation across all network layers, real-time response times, and the ability to scale.","Plotinus, a new digital twin system based on microservices for satellite Internet emulation, aims to solve these problems.","It features a modular design, allowing for easy replacement of the physical layer to emulate different aerial vehicles and analyze channel interference.","It also enables the replacement of path computation methods to simplify testing and deploying algorithms.","In particular, Plotinus allows for real-time emulation with live network traffic, enhancing the realism of network models.","Evaluation result shows that Plotinus's effective emulation of dynamic satellite networks with real-world devices.","Its adaptability for various communication models and algorithm testing highlights Plotinus's role as a vital tool for developing and analyzing SAGIN systems, offering a scalable, real-time response, and flexible digital twin system."],"url":"http://arxiv.org/abs/2403.08515v1","category":"cs.NI"}
{"created":"2024-03-13 13:16:26","title":"A Multimodal Fusion Network For Student Emotion Recognition Based on Transformer and Tensor Product","abstract":"In recent years, there have been frequent incidents of foreign objects intruding into railway and Airport runways. These objects can include pedestrians, vehicles, animals, and debris. This paper introduces an improved YOLOv5 architecture incorporating FasterNet and attention mechanisms to enhance the detection of foreign objects on railways and Airport runways. This study proposes a new dataset, AARFOD (Aero and Rail Foreign Object Detection), which combines two public datasets for detecting foreign objects in aviation and railway systems. The dataset aims to improve the recognition capabilities of foreign object targets. Experimental results on this large dataset have demonstrated significant performance improvements of the proposed model over the baseline YOLOv5 model, reducing computational requirements. improved YOLO model shows a significant improvement in precision by 1.2%, recall rate by 1.0%, and mAP@.5 by 0.6%, while mAP@.5-.95 remained unchanged. The parameters were reduced by approximately 25.12%, and GFLOPs were reduced by about 10.63%. In the ablation experiment, it is found that the FasterNet module can significantly reduce the number of parameters of the model, and the reference of the attention mechanism can slow down the performance loss caused by lightweight.","sentences":["In recent years, there have been frequent incidents of foreign objects intruding into railway and Airport runways.","These objects can include pedestrians, vehicles, animals, and debris.","This paper introduces an improved YOLOv5 architecture incorporating FasterNet and attention mechanisms to enhance the detection of foreign objects on railways and Airport runways.","This study proposes a new dataset, AARFOD (Aero and Rail Foreign Object Detection), which combines two public datasets for detecting foreign objects in aviation and railway systems.","The dataset aims to improve the recognition capabilities of foreign object targets.","Experimental results on this large dataset have demonstrated significant performance improvements of the proposed model over the baseline YOLOv5 model, reducing computational requirements.","improved YOLO model shows a significant improvement in precision by 1.2%, recall rate by 1.0%, and mAP@.5 by 0.6%, while mAP@.5-.95 remained unchanged.","The parameters were reduced by approximately 25.12%, and GFLOPs were reduced by about 10.63%.","In the ablation experiment, it is found that the FasterNet module can significantly reduce the number of parameters of the model, and the reference of the attention mechanism can slow down the performance loss caused by lightweight."],"url":"http://arxiv.org/abs/2403.08511v1","category":"cs.CV"}
{"created":"2024-03-13 13:15:18","title":"Torsion-free connections of second-order maximally superintegrable systems","abstract":"Second-order (maximally) conformally superintegrable systems play an important role as models of mechanical systems, including systems such as the Kepler-Coulomb system and the isotropic harmonic oscillator. The present paper is dedicated to understanding non- and semi-degenerate systems. We obtain \"projective flatness\" results for two torsion-free connections naturally associated to such systems. This viewpoint sheds some light onto the interrelationship of properly and conformally (second-order maximally) superintegrable systems from a geometrical perspective. It is shown that the semi-degenerate secondary structure tensor can be viewed as the Ricci curvature of a natural torsion-free connection defined by the primary structure tensor (and similarly in the non-degenerate case). It is also shown that properly semi-degenerate systems are characterised, similar to the non-degenerate case, by the vanishing of the secondary structure tensor.","sentences":["Second-order (maximally) conformally superintegrable systems play an important role as models of mechanical systems, including systems such as the Kepler-Coulomb system and the isotropic harmonic oscillator.","The present paper is dedicated to understanding non- and semi-degenerate systems.","We obtain \"projective flatness\" results for two torsion-free connections naturally associated to such systems.","This viewpoint sheds some light onto the interrelationship of properly and conformally (second-order maximally) superintegrable systems from a geometrical perspective.","It is shown that the semi-degenerate secondary structure tensor can be viewed as the Ricci curvature of a natural torsion-free connection defined by the primary structure tensor (and similarly in the non-degenerate case).","It is also shown that properly semi-degenerate systems are characterised, similar to the non-degenerate case, by the vanishing of the secondary structure tensor."],"url":"http://arxiv.org/abs/2403.08509v1","category":"math.DG"}
{"created":"2024-03-13 13:15:14","title":"Quantum simulation in hybrid transmission lines","abstract":"Platforms based on transmission lines are nowadays employed for the simulation of standard phenomena in quantum electrodynamics and quantum field theory. In this work, we propose a hybrid platform, in which a right-handed transmission line is connected to a left-handed transmission line by means of a superconducting quantum interference device (SQUID). We examine the interaction between the two transmission lines, as well as the excitation flow along the composed platform. We show that, by activating specific resonance conditions, this platform can be used as a quantum simulator of different phenomena in quantum optics, multimode quantum systems and quantum thermodynamics.","sentences":["Platforms based on transmission lines are nowadays employed for the simulation of standard phenomena in quantum electrodynamics and quantum field theory.","In this work, we propose a hybrid platform, in which a right-handed transmission line is connected to a left-handed transmission line by means of a superconducting quantum interference device (SQUID).","We examine the interaction between the two transmission lines, as well as the excitation flow along the composed platform.","We show that, by activating specific resonance conditions, this platform can be used as a quantum simulator of different phenomena in quantum optics, multimode quantum systems and quantum thermodynamics."],"url":"http://arxiv.org/abs/2403.08508v1","category":"quant-ph"}
{"created":"2024-03-13 13:15:13","title":"MobileAtlas: Geographically Decoupled Measurements in Cellular Networks for Security and Privacy Research","abstract":"Cellular networks are not merely data access networks to the Internet. Their distinct services and ability to form large complex compounds for roaming purposes make them an attractive research target in their own right. Their promise of providing a consistent service with comparable privacy and security across roaming partners falls apart at close inspection.   Thus, there is a need for controlled testbeds and measurement tools for cellular access networks doing justice to the technology's unique structure and global scope. Particularly, such measurements suffer from a combinatorial explosion of operators, mobile plans, and services. To cope with these challenges, we built a framework that geographically decouples the SIM from the cellular modem by selectively connecting both remotely. This allows testing any subscriber with any operator at any modem location within minutes without moving parts. The resulting GSM/UMTS/LTE measurement and testbed platform offers a controlled experimentation environment, which is scalable and cost-effective. The platform is extensible and fully open-sourced, allowing other researchers to contribute locations, SIM cards, and measurement scripts.   Using the above framework, our international experiments in commercial networks revealed exploitable inconsistencies in traffic metering, leading to multiple phreaking opportunities, i.e., fare-dodging. We also expose problematic IPv6 firewall configurations, hidden SIM card communication to the home network, and fingerprint dial progress tones to track victims across different roaming networks and countries with voice calls.","sentences":["Cellular networks are not merely data access networks to the Internet.","Their distinct services and ability to form large complex compounds for roaming purposes make them an attractive research target in their own right.","Their promise of providing a consistent service with comparable privacy and security across roaming partners falls apart at close inspection.   ","Thus, there is a need for controlled testbeds and measurement tools for cellular access networks doing justice to the technology's unique structure and global scope.","Particularly, such measurements suffer from a combinatorial explosion of operators, mobile plans, and services.","To cope with these challenges, we built a framework that geographically decouples the SIM from the cellular modem by selectively connecting both remotely.","This allows testing any subscriber with any operator at any modem location within minutes without moving parts.","The resulting GSM/UMTS/LTE measurement and testbed platform offers a controlled experimentation environment, which is scalable and cost-effective.","The platform is extensible and fully open-sourced, allowing other researchers to contribute locations, SIM cards, and measurement scripts.   ","Using the above framework, our international experiments in commercial networks revealed exploitable inconsistencies in traffic metering, leading to multiple phreaking opportunities, i.e., fare-dodging.","We also expose problematic IPv6 firewall configurations, hidden SIM card communication to the home network, and fingerprint dial progress tones to track victims across different roaming networks and countries with voice calls."],"url":"http://arxiv.org/abs/2403.08507v1","category":"cs.NI"}
{"created":"2024-03-13 13:08:16","title":"Governing Through the Cloud: The Intermediary Role of Compute Providers in AI Regulation","abstract":"As jurisdictions around the world take their first steps toward regulating the most powerful AI systems, such as the EU AI Act and the US Executive Order 14110, there is a growing need for effective enforcement mechanisms that can verify compliance and respond to violations. We argue that compute providers should have legal obligations and ethical responsibilities associated with AI development and deployment, both to provide secure infrastructure and to serve as intermediaries for AI regulation. Compute providers can play an essential role in a regulatory ecosystem via four key capacities: as securers, safeguarding AI systems and critical infrastructure; as record keepers, enhancing visibility for policymakers; as verifiers of customer activities, ensuring oversight; and as enforcers, taking actions against rule violations. We analyze the technical feasibility of performing these functions in a targeted and privacy-conscious manner and present a range of technical instruments. In particular, we describe how non-confidential information, to which compute providers largely already have access, can provide two key governance-relevant properties of a computational workload: its type-e.g., large-scale training or inference-and the amount of compute it has consumed. Using AI Executive Order 14110 as a case study, we outline how the US is beginning to implement record keeping requirements for compute providers. We also explore how verification and enforcement roles could be added to establish a comprehensive AI compute oversight scheme. We argue that internationalization will be key to effective implementation, and highlight the critical challenge of balancing confidentiality and privacy with risk mitigation as the role of compute providers in AI regulation expands.","sentences":["As jurisdictions around the world take their first steps toward regulating the most powerful AI systems, such as the EU AI Act and the US Executive Order 14110, there is a growing need for effective enforcement mechanisms that can verify compliance and respond to violations.","We argue that compute providers should have legal obligations and ethical responsibilities associated with AI development and deployment, both to provide secure infrastructure and to serve as intermediaries for AI regulation.","Compute providers can play an essential role in a regulatory ecosystem via four key capacities: as securers, safeguarding AI systems and critical infrastructure; as record keepers, enhancing visibility for policymakers; as verifiers of customer activities, ensuring oversight; and as enforcers, taking actions against rule violations.","We analyze the technical feasibility of performing these functions in a targeted and privacy-conscious manner and present a range of technical instruments.","In particular, we describe how non-confidential information, to which compute providers largely already have access, can provide two key governance-relevant properties of a computational workload: its type-e.g., large-scale training or inference-and the amount of compute it has consumed.","Using AI Executive Order 14110 as a case study, we outline how the US is beginning to implement record keeping requirements for compute providers.","We also explore how verification and enforcement roles could be added to establish a comprehensive AI compute oversight scheme.","We argue that internationalization will be key to effective implementation, and highlight the critical challenge of balancing confidentiality and privacy with risk mitigation as the role of compute providers in AI regulation expands."],"url":"http://arxiv.org/abs/2403.08501v1","category":"cs.CY"}
{"created":"2024-03-13 13:05:42","title":"Viro's patchworking and the signed reduced A-discriminant","abstract":"Computing the isotopy type of a hypersurface, defined as the positive real zero set of a multivariate polynomial, is a challenging problem in real algebraic geometry. We focus on the case where the defining polynomial has combinatorially restricted exponent vectors and fixed coefficient signs, enabling faster computation of the isotopy type. In particular, Viro's patchworking provides a polyhedral complex that has the same isotopy type as the hypersurface, for certain choices of the coefficients. So we present properties of the signed support, focussing mainly on the case of n-variate (n+3)-nomials, that ensure all possible isotopy types can be obtained via patchworking. To prove this, we study the signed reduced A-discriminant and show that it has a simple structure if the signed support satisfies some combinatorial conditions.","sentences":["Computing the isotopy type of a hypersurface, defined as the positive real zero set of a multivariate polynomial, is a challenging problem in real algebraic geometry.","We focus on the case where the defining polynomial has combinatorially restricted exponent vectors and fixed coefficient signs, enabling faster computation of the isotopy type.","In particular, Viro's patchworking provides a polyhedral complex that has the same isotopy type as the hypersurface, for certain choices of the coefficients.","So we present properties of the signed support, focussing mainly on the case of n-variate (n+3)-nomials, that ensure all possible isotopy types can be obtained via patchworking.","To prove this, we study the signed reduced A-discriminant and show that it has a simple structure if the signed support satisfies some combinatorial conditions."],"url":"http://arxiv.org/abs/2403.08497v1","category":"math.AG"}
{"created":"2024-03-13 13:04:58","title":"Automatic Interactive Evaluation for Large Language Models with State Aware Patient Simulator","abstract":"Large Language Models (LLMs) have demonstrated remarkable proficiency in human interactions, yet their application within the medical field remains insufficiently explored. Previous works mainly focus on the performance of medical knowledge with examinations, which is far from the realistic scenarios, falling short in assessing the abilities of LLMs on clinical tasks. In the quest to enhance the application of Large Language Models (LLMs) in healthcare, this paper introduces the Automated Interactive Evaluation (AIE) framework and the State-Aware Patient Simulator (SAPS), targeting the gap between traditional LLM evaluations and the nuanced demands of clinical practice. Unlike prior methods that rely on static medical knowledge assessments, AIE and SAPS provide a dynamic, realistic platform for assessing LLMs through multi-turn doctor-patient simulations. This approach offers a closer approximation to real clinical scenarios and allows for a detailed analysis of LLM behaviors in response to complex patient interactions. Our extensive experimental validation demonstrates the effectiveness of the AIE framework, with outcomes that align well with human evaluations, underscoring its potential to revolutionize medical LLM testing for improved healthcare delivery.","sentences":["Large Language Models (LLMs) have demonstrated remarkable proficiency in human interactions, yet their application within the medical field remains insufficiently explored.","Previous works mainly focus on the performance of medical knowledge with examinations, which is far from the realistic scenarios, falling short in assessing the abilities of LLMs on clinical tasks.","In the quest to enhance the application of Large Language Models (LLMs) in healthcare, this paper introduces the Automated Interactive Evaluation (AIE) framework and the State-Aware Patient Simulator (SAPS), targeting the gap between traditional LLM evaluations and the nuanced demands of clinical practice.","Unlike prior methods that rely on static medical knowledge assessments, AIE and SAPS provide a dynamic, realistic platform for assessing LLMs through multi-turn doctor-patient simulations.","This approach offers a closer approximation to real clinical scenarios and allows for a detailed analysis of LLM behaviors in response to complex patient interactions.","Our extensive experimental validation demonstrates the effectiveness of the AIE framework, with outcomes that align well with human evaluations, underscoring its potential to revolutionize medical LLM testing for improved healthcare delivery."],"url":"http://arxiv.org/abs/2403.08495v1","category":"cs.CL"}
{"created":"2024-03-13 12:54:07","title":"Sharp detection of the onset of Floquet heating using eigenstate sensitivity","abstract":"Chaotic Floquet systems at sufficiently low driving frequencies are known to heat up to an infinite temperature ensemble in the thermodynamic limit. However at high driving frequencies, Floquet systems remain energetically stable in a robust prethermal phase with exponentially long heating times. We propose sensitivity (susceptibility) of Floquet eigenstates against infinitesimal deformations of the drive, as a sharp and sensitive measure to detect this heating transition. It also captures various regimes (timescales) of Floquet thermalization accurately. Particularly, we find that at low frequencies near the onset of unbounded heating, Floquet eigenstates are maximally sensitive to perturbations and consequently the scaled susceptibility develops a sharp maximum. We further connect our results to the relaxation dynamics of local observables to show that near the onset of Floquet heating, the system is nonergodic with slow glassy dynamics despite being nonintegrable at all driving frequencies.","sentences":["Chaotic Floquet systems at sufficiently low driving frequencies are known to heat up to an infinite temperature ensemble in the thermodynamic limit.","However at high driving frequencies, Floquet systems remain energetically stable in a robust prethermal phase with exponentially long heating times.","We propose sensitivity (susceptibility) of Floquet eigenstates against infinitesimal deformations of the drive, as a sharp and sensitive measure to detect this heating transition.","It also captures various regimes (timescales) of Floquet thermalization accurately.","Particularly, we find that at low frequencies near the onset of unbounded heating, Floquet eigenstates are maximally sensitive to perturbations and consequently the scaled susceptibility develops a sharp maximum.","We further connect our results to the relaxation dynamics of local observables to show that near the onset of Floquet heating, the system is nonergodic with slow glassy dynamics despite being nonintegrable at all driving frequencies."],"url":"http://arxiv.org/abs/2403.08490v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-13 12:53:38","title":"Effect of the charge asymmetry and orbital angular momentum in the entrance channel on the hindrance to complete fusion","abstract":"The hindrance to complete fusion is studied as a function of the charge asymmetry of colliding nuclei and orbital angular momentum of the collision. The formation and evolution of a dinuclear system (DNS) in the heavy ion collisions at energies near the Coulomb barrier is calculated in the framework of the DNS model. The DNS evolution is considered as nucleon transfer between its fragments. The results prove that a hindrance at formation of a compound nucleus (CN) is related with the quasifission process which is breakup of the DNS into products instead to reach the equilibrated state of the CN. The role of the angular momentum in the charge (mass) distribution of the reaction products for the given mass asymmetry of the colliding nuclei has been demonstrated. The results of this work have been compared with the measured data for the quasifission yields in the $^{12}$C+$^{204}$Pb and $^{48}$Ca+$^{168}$Er reactions to show the role of the mass asymmetry of the entrance channel.","sentences":["The hindrance to complete fusion is studied as a function of the charge asymmetry of colliding nuclei and orbital angular momentum of the collision.","The formation and evolution of a dinuclear system (DNS) in the heavy ion collisions at energies near the Coulomb barrier is calculated in the framework of the DNS model.","The DNS evolution is considered as nucleon transfer between its fragments.","The results prove that a hindrance at formation of a compound nucleus (CN) is related with the quasifission process which is breakup of the DNS into products instead to reach the equilibrated state of the CN.","The role of the angular momentum in the charge (mass) distribution of the reaction products for the given mass asymmetry of the colliding nuclei has been demonstrated.","The results of this work have been compared with the measured data for the quasifission yields in the $^{12}$C+$^{204}$Pb and $^{48}$Ca+$^{168}$Er reactions to show the role of the mass asymmetry of the entrance channel."],"url":"http://arxiv.org/abs/2403.08489v1","category":"nucl-th"}
{"created":"2024-03-13 12:49:04","title":"Bose-Einstein condensation in canonical ensemble with fixed total momentum","abstract":"We consider Bose-Einstein condensation of noninteracting homogeneous three-dimensional gas in canonical ensemble when both particle number $N$ and total momentum $\\mathbf{P}$ of all particles are fixed. Using the saddle point method, we derive the large-$N$ analytical approximations for partition function, free energy, and statistical distributions of occupation numbers of different single-particle energy levels. At temperatures below the critical point of phase transition, we predict, in some ranges of $\\mathbf{P}$, fragmentation of the condensate, when more than one single-particle level is macroscopically occupied. The occupation number distributions have approximately Gaussian shapes for the levels hosting the condensate, and exponential shapes for other, noncondensate levels. Our analysis demonstrates breaking of Galilean invariance of moving finite-temperature many-particle system in the presence of Bose-Einstein condensation and extends the theory of moving and rotating quantum systems to the finite-temperature large-$N$ limit.","sentences":["We consider Bose-Einstein condensation of noninteracting homogeneous three-dimensional gas in canonical ensemble when both particle number $N$ and total momentum $\\mathbf{P}$ of all particles are fixed.","Using the saddle point method, we derive the large-$N$ analytical approximations for partition function, free energy, and statistical distributions of occupation numbers of different single-particle energy levels.","At temperatures below the critical point of phase transition, we predict, in some ranges of $\\mathbf{P}$, fragmentation of the condensate, when more than one single-particle level is macroscopically occupied.","The occupation number distributions have approximately Gaussian shapes for the levels hosting the condensate, and exponential shapes for other, noncondensate levels.","Our analysis demonstrates breaking of Galilean invariance of moving finite-temperature many-particle system in the presence of Bose-Einstein condensation and extends the theory of moving and rotating quantum systems to the finite-temperature large-$N$ limit."],"url":"http://arxiv.org/abs/2403.08482v1","category":"cond-mat.quant-gas"}
{"created":"2024-03-13 12:40:32","title":"Emergent Continuous Time Crystal in Dissipative Quantum Spin System without Driving","abstract":"Time crystal, a nonequilibrium phenomenon extending spontaneous symmetry breaking into the temporal dimension, holds fundamental significance in understanding quantum many-body physics. In this work, we explore the nonequilibrium phase diagram of a two-dimensional dissipative Heisenberg spin system in the absence of explicit driving. We numerically identify the emergence of novel nonstationary oscillatory states by analyzing the spin dynamics. These states are categorized as limit cycle and chaos based on the Lyapunov exponent. Remarkably, the observed limit cycle behavior represents a continuous time crystal (CTC), spontaneously breaking the continuous time translation symmetry of the system. We further confirm those oscillatory behaviors by studying the stability against local perturbations applied to the system. Finally, we investigate the robustness of the emergent CTC by introducing isotropic Gaussian-type white noise into the interactions. This study provides many insights into the intricate interplay between dissipation-induced decay processes and interaction-induced spin precession, deepening our understanding of dissipative quantum many-body systems.","sentences":["Time crystal, a nonequilibrium phenomenon extending spontaneous symmetry breaking into the temporal dimension, holds fundamental significance in understanding quantum many-body physics.","In this work, we explore the nonequilibrium phase diagram of a two-dimensional dissipative Heisenberg spin system in the absence of explicit driving.","We numerically identify the emergence of novel nonstationary oscillatory states by analyzing the spin dynamics.","These states are categorized as limit cycle and chaos based on the Lyapunov exponent.","Remarkably, the observed limit cycle behavior represents a continuous time crystal (CTC), spontaneously breaking the continuous time translation symmetry of the system.","We further confirm those oscillatory behaviors by studying the stability against local perturbations applied to the system.","Finally, we investigate the robustness of the emergent CTC by introducing isotropic Gaussian-type white noise into the interactions.","This study provides many insights into the intricate interplay between dissipation-induced decay processes and interaction-induced spin precession, deepening our understanding of dissipative quantum many-body systems."],"url":"http://arxiv.org/abs/2403.08476v1","category":"quant-ph"}
{"created":"2024-03-13 12:39:11","title":"NLQxform-UI: A Natural Language Interface for Querying DBLP Interactively","abstract":"In recent years, the DBLP computer science bibliography has been prominently used for searching scholarly information, such as publications, scholars, and venues. However, its current search service lacks the capability to handle complex queries, which limits the usability of DBLP. In this paper, we present NLQxform-UI, a web-based natural language interface that enables users to query DBLP directly with complex natural language questions. NLQxform-UI automatically translates given questions into SPARQL queries and executes the queries over the DBLP knowledge graph to retrieve answers. The querying process is presented to users in an interactive manner, which improves the transparency of the system and helps examine the returned answers. Also, intermediate results in the querying process can be previewed and manually altered to improve the accuracy of the system. NLQxform-UI has been completely open-sourced: https://github.com/ruijie-wang-uzh/NLQxform-UI.","sentences":["In recent years, the DBLP computer science bibliography has been prominently used for searching scholarly information, such as publications, scholars, and venues.","However, its current search service lacks the capability to handle complex queries, which limits the usability of DBLP.","In this paper, we present NLQxform-UI, a web-based natural language interface that enables users to query DBLP directly with complex natural language questions.","NLQxform-UI automatically translates given questions into SPARQL queries and executes the queries over the DBLP knowledge graph to retrieve answers.","The querying process is presented to users in an interactive manner, which improves the transparency of the system and helps examine the returned answers.","Also, intermediate results in the querying process can be previewed and manually altered to improve the accuracy of the system.","NLQxform-UI has been completely open-sourced: https://github.com/ruijie-wang-uzh/NLQxform-UI."],"url":"http://arxiv.org/abs/2403.08475v1","category":"cs.IR"}
{"created":"2024-03-13 12:39:02","title":"Negative Wigner function by decaying interaction from equilibrium","abstract":"Bosonic systems with negative Wigner function superposition states are fundamentally witnessing nonlinear quantum dynamics beyond linearized systems and, recently, have become essential resources of quantum technology with many applications. Typically, they appear due to sophisticated combination of external drives, nonlinear control, measurements or strong nonlinear dissipation of subsystems to an environment. Here, we propose a conceptually different and more autonomous way to obtain such states, avoiding these ingredients, using purely sudden interaction decay in the paradigmatic interacting qubit-oscillator system weakly coupled to bath at thermal equilibrium in a low-temperature limit. We demonstrate simultaneously detectable unconditional negative Wigner function and quantum coherence and their qualitative enhancement employing more qubits.","sentences":["Bosonic systems with negative Wigner function superposition states are fundamentally witnessing nonlinear quantum dynamics beyond linearized systems and, recently, have become essential resources of quantum technology with many applications.","Typically, they appear due to sophisticated combination of external drives, nonlinear control, measurements or strong nonlinear dissipation of subsystems to an environment.","Here, we propose a conceptually different and more autonomous way to obtain such states, avoiding these ingredients, using purely sudden interaction decay in the paradigmatic interacting qubit-oscillator system weakly coupled to bath at thermal equilibrium in a low-temperature limit.","We demonstrate simultaneously detectable unconditional negative Wigner function and quantum coherence and their qualitative enhancement employing more qubits."],"url":"http://arxiv.org/abs/2403.08474v1","category":"quant-ph"}
{"created":"2024-03-13 12:38:43","title":"Mechanism Design Optimization through CAD-Based Bayesian Optimization and Quantified Constraints","abstract":"This research delves into optimizing mechanism design, with an emphasis on the energy efficiency and the expansive design possibilities of reciprocating mechanisms. It investigates how to efficiently integrate Computer-Aided Design (CAD) simulations with Bayesian Optimization (BO) and a constrained design space, aiming to enhance the design optimization process beyond the confines of traditional kinematic and dynamic analysis. The study sets out to create a novel optimization framework that merges CAD simulations with a BO strategy. Initially, the feasibility of a mechanism design is assessed through CAD-motion simulations, which gauge its practicality. Upon deeming a design feasible, an evaluation via CAD-motion simulations is conducted to ascertain the objective value. This research proposes utilizing non-parametric Gaussian processes for crafting a surrogate model of the objective function, considering the design space's static and dynamic constraints. The findings reveal that the introduced CAD-based Bayesian Optimization framework adeptly identifies optimal design parameters that minimize root mean square (RMS) torque while complying with predetermined constraints. This method markedly diminishes the complexity seen in analytical approaches, rendering it adaptable to intricate mechanisms and practicable for machine builders. The framework evidences the utility of integrating constraints in the optimization process, showing promise for attaining globally optimal designs efficiently. A case study on an emergency ventilator, with three design parameters, demonstrates a 71% RMS torque reduction after 255 CAD-based evaluations, underscoring the approach's effectiveness and its potential for refining mechanism design optimization.","sentences":["This research delves into optimizing mechanism design, with an emphasis on the energy efficiency and the expansive design possibilities of reciprocating mechanisms.","It investigates how to efficiently integrate Computer-Aided Design (CAD) simulations with Bayesian Optimization (BO) and a constrained design space, aiming to enhance the design optimization process beyond the confines of traditional kinematic and dynamic analysis.","The study sets out to create a novel optimization framework that merges CAD simulations with a BO strategy.","Initially, the feasibility of a mechanism design is assessed through CAD-motion simulations, which gauge its practicality.","Upon deeming a design feasible, an evaluation via CAD-motion simulations is conducted to ascertain the objective value.","This research proposes utilizing non-parametric Gaussian processes for crafting a surrogate model of the objective function, considering the design space's static and dynamic constraints.","The findings reveal that the introduced CAD-based Bayesian Optimization framework adeptly identifies optimal design parameters that minimize root mean square (RMS) torque while complying with predetermined constraints.","This method markedly diminishes the complexity seen in analytical approaches, rendering it adaptable to intricate mechanisms and practicable for machine builders.","The framework evidences the utility of integrating constraints in the optimization process, showing promise for attaining globally optimal designs efficiently.","A case study on an emergency ventilator, with three design parameters, demonstrates a 71% RMS torque reduction after 255 CAD-based evaluations, underscoring the approach's effectiveness and its potential for refining mechanism design optimization."],"url":"http://arxiv.org/abs/2403.08473v1","category":"eess.SY"}
{"created":"2024-03-13 12:34:39","title":"Convergence of ADAM for Lipschitz Objective Functions","abstract":"The aim of this paper is to prove the exponential convergence, local   and global, of Adam algorithm under precise conditions on the   parameters, when the objective function lacks   differentiability. More precisely, we require Lipschitz continuity,   and control on the gradient whenever it exists. We provide also   examples of interesting functions that satisfies the required   restrictions.","sentences":["The aim of this paper is to prove the exponential convergence, local   and global, of Adam algorithm under precise conditions on the   parameters, when the objective function lacks   differentiability.","More precisely, we require Lipschitz continuity,   and control on the gradient whenever it exists.","We provide also   examples of interesting functions that satisfies the required   restrictions."],"url":"http://arxiv.org/abs/2403.08470v1","category":"math.OC"}
{"created":"2024-03-13 12:25:47","title":"Authorship Verification based on the Likelihood Ratio of Grammar Models","abstract":"Authorship Verification (AV) is the process of analyzing a set of documents to determine whether they were written by a specific author. This problem often arises in forensic scenarios, e.g., in cases where the documents in question constitute evidence for a crime. Existing state-of-the-art AV methods use computational solutions that are not supported by a plausible scientific explanation for their functioning and that are often difficult for analysts to interpret. To address this, we propose a method relying on calculating a quantity we call $\\lambda_G$ (LambdaG): the ratio between the likelihood of a document given a model of the Grammar for the candidate author and the likelihood of the same document given a model of the Grammar for a reference population. These Grammar Models are estimated using $n$-gram language models that are trained solely on grammatical features. Despite not needing large amounts of data for training, LambdaG still outperforms other established AV methods with higher computational complexity, including a fine-tuned Siamese Transformer network. Our empirical evaluation based on four baseline methods applied to twelve datasets shows that LambdaG leads to better results in terms of both accuracy and AUC in eleven cases and in all twelve cases if considering only topic-agnostic methods. The algorithm is also highly robust to important variations in the genre of the reference population in many cross-genre comparisons. In addition to these properties, we demonstrate how LambdaG is easier to interpret than the current state-of-the-art. We argue that the advantage of LambdaG over other methods is due to fact that it is compatible with Cognitive Linguistic theories of language processing.","sentences":["Authorship Verification (AV) is the process of analyzing a set of documents to determine whether they were written by a specific author.","This problem often arises in forensic scenarios, e.g., in cases where the documents in question constitute evidence for a crime.","Existing state-of-the-art AV methods use computational solutions that are not supported by a plausible scientific explanation for their functioning and that are often difficult for analysts to interpret.","To address this, we propose a method relying on calculating a quantity we call $\\lambda_G$ (LambdaG): the ratio between the likelihood of a document given a model of the Grammar for the candidate author and the likelihood of the same document given a model of the Grammar for a reference population.","These Grammar Models are estimated using $n$-gram language models that are trained solely on grammatical features.","Despite not needing large amounts of data for training, LambdaG still outperforms other established AV methods with higher computational complexity, including a fine-tuned Siamese Transformer network.","Our empirical evaluation based on four baseline methods applied to twelve datasets shows that LambdaG leads to better results in terms of both accuracy and AUC in eleven cases and in all twelve cases if considering only topic-agnostic methods.","The algorithm is also highly robust to important variations in the genre of the reference population in many cross-genre comparisons.","In addition to these properties, we demonstrate how LambdaG is easier to interpret than the current state-of-the-art.","We argue that the advantage of LambdaG over other methods is due to fact that it is compatible with Cognitive Linguistic theories of language processing."],"url":"http://arxiv.org/abs/2403.08462v1","category":"cs.CL"}
{"created":"2024-03-13 12:20:03","title":"Symmetry restoration and quantum Mpemba effect in symmetric random circuits","abstract":"Entanglement asymmetry, which serves as a diagnostic tool for symmetry breaking and a proxy for thermalization, has recently been proposed and studied in the context of symmetry restoration for quantum many-body systems undergoing a quench. In this Letter, we investigate symmetry restoration in various symmetric random quantum circuits, particularly focusing on the U(1) symmetry case. In contrast to non-symmetric random circuits where the U(1) symmetry of a small subsystem can always be restored at late times, we reveal that symmetry restoration can fail in U(1) symmetric circuits for certain small symmetry-broken initial states in finite-size systems. In the early-time dynamics, we observe an intriguing quantum Mpemba effect implying that symmetry is restored faster when the initial state is more asymmetric. Furthermore, we also investigate the entanglement asymmetry dynamics for SU(2) and $Z_{2}$ symmetric circuits and identify the presence and absence of the quantum Mpemba effect for the corresponding symmetries, respectively. A unified understanding of these results is provided through the lens of quantum thermalization with conserved charges.","sentences":["Entanglement asymmetry, which serves as a diagnostic tool for symmetry breaking and a proxy for thermalization, has recently been proposed and studied in the context of symmetry restoration for quantum many-body systems undergoing a quench.","In this Letter, we investigate symmetry restoration in various symmetric random quantum circuits, particularly focusing on the U(1) symmetry case.","In contrast to non-symmetric random circuits where the U(1) symmetry of a small subsystem can always be restored at late times, we reveal that symmetry restoration can fail in U(1) symmetric circuits for certain small symmetry-broken initial states in finite-size systems.","In the early-time dynamics, we observe an intriguing quantum Mpemba effect implying that symmetry is restored faster when the initial state is more asymmetric.","Furthermore, we also investigate the entanglement asymmetry dynamics for SU(2) and $Z_{2}$ symmetric circuits and identify the presence and absence of the quantum Mpemba effect for the corresponding symmetries, respectively.","A unified understanding of these results is provided through the lens of quantum thermalization with conserved charges."],"url":"http://arxiv.org/abs/2403.08459v1","category":"quant-ph"}
{"created":"2024-03-13 12:17:30","title":"Non-linear collision-induced breakage equation: finite volume and semi-analytical methods","abstract":"The non-linear collision-induced breakage equation has significant applications in particulate processes. Two semi-analytical techniques, namely homotopy analysis method (HAM) and accelerated homotopy perturbation method (AHPM) are investigated along with the well-known finite volume method (FVM) to comprehend the dynamical behavior of the non-linear system, i.e., the concentration function, the total number and the total mass of the particles in the system. The theoretical convergence analyses of the series solutions of HAM and AHPM are discussed. In addition, the error estimations of the truncated solutions of both methods equip the maximum absolute error bound. To justify the applicability and accuracy of these methods, numerical simulations are compared with the findings of FVM and analytical solutions considering three physical problems.","sentences":["The non-linear collision-induced breakage equation has significant applications in particulate processes.","Two semi-analytical techniques, namely homotopy analysis method (HAM) and accelerated homotopy perturbation method (AHPM) are investigated along with the well-known finite volume method (FVM) to comprehend the dynamical behavior of the non-linear system, i.e., the concentration function, the total number and the total mass of the particles in the system.","The theoretical convergence analyses of the series solutions of HAM and AHPM are discussed.","In addition, the error estimations of the truncated solutions of both methods equip the maximum absolute error bound.","To justify the applicability and accuracy of these methods, numerical simulations are compared with the findings of FVM and analytical solutions considering three physical problems."],"url":"http://arxiv.org/abs/2403.08457v1","category":"math.NA"}
{"created":"2024-03-13 12:05:45","title":"Criticality in an imidazolium ionic liquid fully wetting a sapphire support","abstract":"Hypothesis: Ionic liquids have various applications in catalytic reaction environments. In those systems, their interaction with interfaces is key to their performance as a liquid phase. We hypothesize that the way a monolayer ionic liquid phase interacts with interfaces like a sapphire substrate is significantly dependent on temperature and that critical behavior can be observed in the structural properties of the liquid film.   Methods and simulations: We perform molecular dynamics simulations of imidazolium-based ionic liquid monolayers deposited on a sapphire substrate at temperatures from 200K to 400K. We develop computational tools to analyze structural properties of molecular arrangement in the monolayer, the structure of the film and the defects spontaneously forming and healing.   Findings: We observe a clear structural phase transition at around 300K from a solid-like to a liquid-like behavior of a film. Below the critical point an alternating crystalline structure of cations and anions with alignment of periodic vectors with the underlying substrate grid is observed, with frozen defects. Above the critical temperature, the pattern becomes isotropic within the contact layer that displays dynamic defects of a characteristic size. Our results highlight the importance of confinement to the phase behavior of the system.","sentences":["Hypothesis: Ionic liquids have various applications in catalytic reaction environments.","In those systems, their interaction with interfaces is key to their performance as a liquid phase.","We hypothesize that the way a monolayer ionic liquid phase interacts with interfaces like a sapphire substrate is significantly dependent on temperature and that critical behavior can be observed in the structural properties of the liquid film.   Methods and simulations: We perform molecular dynamics simulations of imidazolium-based ionic liquid monolayers deposited on a sapphire substrate at temperatures from 200K to 400K. We develop computational tools to analyze structural properties of molecular arrangement in the monolayer, the structure of the film and the defects spontaneously forming and healing.   ","Findings: We observe a clear structural phase transition at around 300K from a solid-like to a liquid-like behavior of a film.","Below the critical point an alternating crystalline structure of cations and anions with alignment of periodic vectors with the underlying substrate grid is observed, with frozen defects.","Above the critical temperature, the pattern becomes isotropic within the contact layer that displays dynamic defects of a characteristic size.","Our results highlight the importance of confinement to the phase behavior of the system."],"url":"http://arxiv.org/abs/2403.08449v1","category":"cond-mat.soft"}
{"created":"2024-03-13 12:03:27","title":"Actor-Critic Physics-informed Neural Lyapunov Control","abstract":"Designing control policies for stabilization tasks with provable guarantees is a long-standing problem in nonlinear control. A crucial performance metric is the size of the resulting region of attraction, which essentially serves as a robustness \"margin\" of the closed-loop system against uncertainties. In this paper, we propose a new method to train a stabilizing neural network controller along with its corresponding Lyapunov certificate, aiming to maximize the resulting region of attraction while respecting the actuation constraints. Crucial to our approach is the use of Zubov's Partial Differential Equation (PDE), which precisely characterizes the true region of attraction of a given control policy. Our framework follows an actor-critic pattern where we alternate between improving the control policy (actor) and learning a Zubov function (critic). Finally, we compute the largest certifiable region of attraction by invoking an SMT solver after the training procedure. Our numerical experiments on several design problems show consistent and significant improvements in the size of the resulting region of attraction.","sentences":["Designing control policies for stabilization tasks with provable guarantees is a long-standing problem in nonlinear control.","A crucial performance metric is the size of the resulting region of attraction, which essentially serves as a robustness \"margin\" of the closed-loop system against uncertainties.","In this paper, we propose a new method to train a stabilizing neural network controller along with its corresponding Lyapunov certificate, aiming to maximize the resulting region of attraction while respecting the actuation constraints.","Crucial to our approach is the use of Zubov's Partial Differential Equation (PDE), which precisely characterizes the true region of attraction of a given control policy.","Our framework follows an actor-critic pattern where we alternate between improving the control policy (actor) and learning a Zubov function (critic).","Finally, we compute the largest certifiable region of attraction by invoking an SMT solver after the training procedure.","Our numerical experiments on several design problems show consistent and significant improvements in the size of the resulting region of attraction."],"url":"http://arxiv.org/abs/2403.08448v1","category":"cs.LG"}
{"created":"2024-03-13 11:41:17","title":"How motility drives the glassy dynamics in confluent epithelial monolayers?","abstract":"As wounds heal, embryos develop, cancer spreads, or asthma progresses, the cellular monolayer undergoes a glass transition from a solid-like jammed to a fluid-like flowing state. Two primary characteristics of these systems, confluency, and self-propulsion, make them distinct from particulate systems. Are the glassy dynamics in these biological systems and equilibrium particulate systems different? Despite the biological significance of glassiness in these systems, no analytical framework, which is indispensable for deeper insights, exists. Here, we extend one of the most popular theories of equilibrium glasses, the random first-order transition (RFOT) theory, for confluent systems with self-propulsion. One crucial result of this work is that, unlike in particulate systems, the confluency affects the effective persistence time-scale of the active force, described by its rotational diffusion $D_r^{\\text{eff}}$. Unlike in particulate systems, this value differs from the bare rotational diffusion of the active propulsion force due to cell shape dynamics which acts to rectify the force dynamics: $D_r^{\\text{eff}}$ is equal to $D_r$ when $D_r$ is small, and saturates when $D_r$ is large. We present simulation results for the glassy dynamics in active confluent models and find that the results are consistent with existing experimental data, and conform remarkably well with our theory. In addition, we show that the theoretical predictions agree nicely with and explain previously published simulation results. Our analytical theory provides a foundation for rationalizing and a quantitative understanding of various glassy characteristics of these biological systems.","sentences":["As wounds heal, embryos develop, cancer spreads, or asthma progresses, the cellular monolayer undergoes a glass transition from a solid-like jammed to a fluid-like flowing state.","Two primary characteristics of these systems, confluency, and self-propulsion, make them distinct from particulate systems.","Are the glassy dynamics in these biological systems and equilibrium particulate systems different?","Despite the biological significance of glassiness in these systems, no analytical framework, which is indispensable for deeper insights, exists.","Here, we extend one of the most popular theories of equilibrium glasses, the random first-order transition (RFOT) theory, for confluent systems with self-propulsion.","One crucial result of this work is that, unlike in particulate systems, the confluency affects the effective persistence time-scale of the active force, described by its rotational diffusion $D_r^{\\text{eff}}$. Unlike in particulate systems, this value differs from the bare rotational diffusion of the active propulsion force due to cell shape dynamics which acts to rectify the force dynamics: $D_r^{\\text{eff}}$ is equal to $D_r$ when $D_r$ is small, and saturates when $D_r$ is large.","We present simulation results for the glassy dynamics in active confluent models and find that the results are consistent with existing experimental data, and conform remarkably well with our theory.","In addition, we show that the theoretical predictions agree nicely with and explain previously published simulation results.","Our analytical theory provides a foundation for rationalizing and a quantitative understanding of various glassy characteristics of these biological systems."],"url":"http://arxiv.org/abs/2403.08437v1","category":"cond-mat.soft"}
{"created":"2024-03-13 11:35:02","title":"GRF-based Predictive Flocking Control with Dynamic Pattern Formation","abstract":"It is promising but challenging to design flocking control for a robot swarm to autonomously follow changing patterns or shapes in a optimal distributed manner. The optimal flocking control with dynamic pattern formation is, therefore, investigated in this paper. A predictive flocking control algorithm is proposed based on a Gibbs random field (GRF), where bio-inspired potential energies are used to charaterize ``robot-robot'' and ``robot-environment'' interactions. Specialized performance-related energies, e.g., motion smoothness, are introduced in the proposed design to improve the flocking behaviors. The optimal control is obtained by maximizing a posterior distribution of a GRF. A region-based shape control is accomplished for pattern formation in light of a mean shift technique. The proposed algorithm is evaluated via the comparison with two state-of-the-art flocking control methods in an environment with obstacles. Both numerical simulations and real-world experiments are conducted to demonstrate the efficiency of the proposed design.","sentences":["It is promising but challenging to design flocking control for a robot swarm to autonomously follow changing patterns or shapes in a optimal distributed manner.","The optimal flocking control with dynamic pattern formation is, therefore, investigated in this paper.","A predictive flocking control algorithm is proposed based on a Gibbs random field (GRF), where bio-inspired potential energies are used to charaterize ``robot-robot'' and ``robot-environment'' interactions.","Specialized performance-related energies, e.g., motion smoothness, are introduced in the proposed design to improve the flocking behaviors.","The optimal control is obtained by maximizing a posterior distribution of a GRF.","A region-based shape control is accomplished for pattern formation in light of a mean shift technique.","The proposed algorithm is evaluated via the comparison with two state-of-the-art flocking control methods in an environment with obstacles.","Both numerical simulations and real-world experiments are conducted to demonstrate the efficiency of the proposed design."],"url":"http://arxiv.org/abs/2403.08434v1","category":"cs.RO"}
{"created":"2024-03-13 11:26:43","title":"DeepCSHAP: Utilizing Shapley Values to Explain Deep Complex-Valued Neural Networks","abstract":"Deep Neural Networks are widely used in academy as well as corporate and public applications, including safety critical applications such as health care and autonomous driving. The ability to explain their output is critical for safety reasons as well as acceptance among applicants. A multitude of methods have been proposed to explain real-valued neural networks. Recently, complex-valued neural networks have emerged as a new class of neural networks dealing with complex-valued input data without the necessity of projecting them onto $\\mathbb{R}^2$. This brings up the need to develop explanation algorithms for this kind of neural networks. In this paper we provide these developments. While we focus on adapting the widely used DeepSHAP algorithm to the complex domain, we also present versions of four gradient based explanation methods suitable for use in complex-valued neural networks. We evaluate the explanation quality of all presented algorithms and provide all of them as an open source library adaptable to most recent complex-valued neural network architectures.","sentences":["Deep Neural Networks are widely used in academy as well as corporate and public applications, including safety critical applications such as health care and autonomous driving.","The ability to explain their output is critical for safety reasons as well as acceptance among applicants.","A multitude of methods have been proposed to explain real-valued neural networks.","Recently, complex-valued neural networks have emerged as a new class of neural networks dealing with complex-valued input data without the necessity of projecting them onto $\\mathbb{R}^2$. This brings up the need to develop explanation algorithms for this kind of neural networks.","In this paper we provide these developments.","While we focus on adapting the widely used DeepSHAP algorithm to the complex domain, we also present versions of four gradient based explanation methods suitable for use in complex-valued neural networks.","We evaluate the explanation quality of all presented algorithms and provide all of them as an open source library adaptable to most recent complex-valued neural network architectures."],"url":"http://arxiv.org/abs/2403.08428v1","category":"cs.LG"}
{"created":"2024-03-13 11:15:26","title":"Gluonic gravitational form factors of the proton","abstract":"The gravitational form factors (GFFs) are a fundamental and elegant way to describe the structure of nucleons and nuclei. Their Fourier transform allows a description of the spatial distribution of the mass, angular momentum, pressure, and shear force densities for both quarks and gluons in the nucleon. While previous investigations predominantly focused on the proton electromagnetic form factors (EMFFs) leading to the charge and magnetization distributions determination, the current emphasis has shifted towards expanding our understanding of the gravitational form factors of quarks and gluons where little is known. In particular, more recently, the proton {\\it gluonic} GFFs have been the target of an intensive investigation at Jefferson Lab. This endeavor, is not without its challenges, particularly in navigating the complexities associated with the near-threshold region. Nevertheless, it provides a bedrock for future nucleon and nuclei gluonic structure studies at the future EIC. In this talk, I will focus on the recent results of $J/\\psi$ photoproduction near-threshold on the proton at Jefferson Lab to determine, in particular, the elusive {\\it gluonic} gravitational form factors. We discuss the caveats of their extraction in the threshold region and mention the complementary measurements of $\\Upsilon$ at the EIC critical to access the trace anomaly and gain insight into the origin of the nucleon mass.","sentences":["The gravitational form factors (GFFs) are a fundamental and elegant way to describe the structure of nucleons and nuclei.","Their Fourier transform allows a description of the spatial distribution of the mass, angular momentum, pressure, and shear force densities for both quarks and gluons in the nucleon.","While previous investigations predominantly focused on the proton electromagnetic form factors (EMFFs) leading to the charge and magnetization distributions determination, the current emphasis has shifted towards expanding our understanding of the gravitational form factors of quarks and gluons where little is known.","In particular, more recently, the proton {\\it gluonic} GFFs have been the target of an intensive investigation at Jefferson Lab.","This endeavor, is not without its challenges, particularly in navigating the complexities associated with the near-threshold region.","Nevertheless, it provides a bedrock for future nucleon and nuclei gluonic structure studies at the future EIC.","In this talk, I will focus on the recent results of $J/\\psi$ photoproduction near-threshold on the proton at Jefferson Lab to determine, in particular, the elusive {\\it gluonic} gravitational form factors.","We discuss the caveats of their extraction in the threshold region and mention the complementary measurements of $\\Upsilon$ at the EIC critical to access the trace anomaly and gain insight into the origin of the nucleon mass."],"url":"http://arxiv.org/abs/2403.08423v1","category":"nucl-ex"}
{"created":"2024-03-13 11:14:10","title":"Stochastic action for the entanglement of a noisy monitored two-qubit system","abstract":"We study the effect of local unitary noise on the entanglement evolution of a two-qubit system subject to local monitoring and inter-qubit coupling. We construct a stochastic Hamiltonian by incorporating the noise into the Chantasri-Dressel-Jordan path integral and use it to identify the optimal entanglement dynamics and to develop a diagrammatic method for a closed-form approximation of the average entanglement dynamics with an analytical dependence on the noise and measurement intensity. We find that both the optimal trajectory and diagrammatic expansion capture the oscillations of entanglement at short times. Numerical investigation of long-time steady-state entanglement reveals a non-monotonic relationship between concurrence and noise strength.","sentences":["We study the effect of local unitary noise on the entanglement evolution of a two-qubit system subject to local monitoring and inter-qubit coupling.","We construct a stochastic Hamiltonian by incorporating the noise into the Chantasri-Dressel-Jordan path integral and use it to identify the optimal entanglement dynamics and to develop a diagrammatic method for a closed-form approximation of the average entanglement dynamics with an analytical dependence on the noise and measurement intensity.","We find that both the optimal trajectory and diagrammatic expansion capture the oscillations of entanglement at short times.","Numerical investigation of long-time steady-state entanglement reveals a non-monotonic relationship between concurrence and noise strength."],"url":"http://arxiv.org/abs/2403.08422v1","category":"quant-ph"}
{"created":"2024-03-13 11:10:09","title":"Boundary and distributed optimal control for a population dynamics PDE model with discontinuous in time Galerkin FEM schemes","abstract":"We consider fully discrete finite element approximations for a semilinear optimal control system of partial differential equations in two cases: for distributed and Robin boundary control. The ecological predator-prey optimal control model is approximated by conforming finite element methods mimicking the spatial part, while a discontinuous Galerkin method is used for the time discretization. We investigate the sensitivity of the solution distance from the target function, in cases with smooth and rough initial data. We employ low, and higher-order polynomials in time and space whenever proper regularity is present. The approximation schemes considered are with and without control constraints, driving efficiently the system to desired states realized using non-linear gradient methods.","sentences":["We consider fully discrete finite element approximations for a semilinear optimal control system of partial differential equations in two cases: for distributed and Robin boundary control.","The ecological predator-prey optimal control model is approximated by conforming finite element methods mimicking the spatial part, while a discontinuous Galerkin method is used for the time discretization.","We investigate the sensitivity of the solution distance from the target function, in cases with smooth and rough initial data.","We employ low, and higher-order polynomials in time and space whenever proper regularity is present.","The approximation schemes considered are with and without control constraints, driving efficiently the system to desired states realized using non-linear gradient methods."],"url":"http://arxiv.org/abs/2403.08419v1","category":"math.NA"}
{"created":"2024-03-13 11:02:47","title":"Superfluid fraction of interacting bosonic gases","abstract":"The superfluid fraction $f$ of a quantum fluid is defined in terms of the response of the system to a weak and constant drag. Notably, Leggett long ago derived two simple expressions providing a rigorous upper bound and a heuristic lower bound for $f$. Here we study the superfluid fraction of bosonic gases in various two-dimensional potentials, such as regular optical lattices and disordered speckles, by solving the Gross-Pitaevskii equation and performing Diffusion Monte Carlo simulations. We show that under conditions relevant for most ultracold experiments the bounds proposed by Leggett provide a surprisingly narrow bracketing of the exact value of the superfluid fraction.","sentences":["The superfluid fraction $f$ of a quantum fluid is defined in terms of the response of the system to a weak and constant drag.","Notably, Leggett long ago derived two simple expressions providing a rigorous upper bound and a heuristic lower bound for $f$. Here we study the superfluid fraction of bosonic gases in various two-dimensional potentials, such as regular optical lattices and disordered speckles, by solving the Gross-Pitaevskii equation and performing Diffusion Monte Carlo simulations.","We show that under conditions relevant for most ultracold experiments the bounds proposed by Leggett provide a surprisingly narrow bracketing of the exact value of the superfluid fraction."],"url":"http://arxiv.org/abs/2403.08416v1","category":"cond-mat.quant-gas"}
{"created":"2024-03-13 11:02:42","title":"Intersection of a Moran type Sierpinski carpet and a line with rational slope","abstract":"In 2005, Liu et al. calculated the dimensionality of the intersection of Sierpinski carpet and a straight line with rational slope in the sense of Lebesgue measure.Sierpinski carpet is a self-similar set in two-dimensional planes obtained by an iterative function system, so each layer has the same structure. While the Sierpinski carpet set with Moran structure is the limit set obtained by the action of two iterative function systems in the two-dimensional plane, which we denote as ~$F_{\\sigma}$~. And the structure of each layer may be different, controlled by 0,1 sequence ~$\\sigma$~ and controlled by the set. In this paper, the upper and lower box dimensions of the set ~$F_{\\sigma}$~ and the straight line ~$L_{a}$~ with rational slope are calculated, where ~$a$~ is the intercept of the straight line. In addition, we consider some related problem. The main difficulty in the research is that the structure of each layer of the set ~$F_{\\sigma}$~ may be different, so the structure of each layer needs to be considered with the help of the sequence ~$\\sigma$~ in the calculation process.","sentences":["In 2005, Liu et al. calculated the dimensionality of the intersection of Sierpinski carpet and a straight line with rational slope in the sense of Lebesgue measure.","Sierpinski carpet is a self-similar set in two-dimensional planes obtained by an iterative function system, so each layer has the same structure.","While the Sierpinski carpet set with Moran structure is the limit set obtained by the action of two iterative function systems in the two-dimensional plane, which we denote as ~$F_{\\sigma}$~.","And the structure of each layer may be different, controlled by 0,1 sequence ~$\\sigma$~ and controlled by the set.","In this paper, the upper and lower box dimensions of the set ~$F_{\\sigma}$~ and the straight line ~$L_{a}$~ with rational slope are calculated, where ~$a$~ is the intercept of the straight line.","In addition, we consider some related problem.","The main difficulty in the research is that the structure of each layer of the set ~$F_{\\sigma}$~ may be different, so the structure of each layer needs to be considered with the help of the sequence ~$\\sigma$~ in the calculation process."],"url":"http://arxiv.org/abs/2403.08415v1","category":"math.DS"}
{"created":"2024-03-13 10:53:55","title":"Painlev\u00e9 Analysis, Prelle-Singer Approach, Symmetries and Integrability of Damped H\u00e9non-Heiles System","abstract":"We consider a modified damped version of H\\'enon-Heiles system and investigate its integrability. By extending the Painlev\\'e analysis of ordinary differential equations we find that the modified H\\'enon-Heiles system possesses the Painlev\\'e property for three distinct parametric restrictions. For each of the identified cases, we construct two independent integrals of motion using the well known Prelle-Singer method. We then derive a set of nontrivial non-point symmetries for each of the identified integrable cases of the modified H\\'enon-Heiles system. We infer that the modified H\\'enon-Heiles system is integrable for three distinct parametric restrictions. Exact solutions are given explicitly for two integrable cases.","sentences":["We consider a modified damped version of H\\'enon-Heiles system and investigate its integrability.","By extending the Painlev\\'e analysis of ordinary differential equations we find that the modified H\\'enon-Heiles system possesses the Painlev\\'e property for three distinct parametric restrictions.","For each of the identified cases, we construct two independent integrals of motion using the well known Prelle-Singer method.","We then derive a set of nontrivial non-point symmetries for each of the identified integrable cases of the modified H\\'enon-Heiles system.","We infer that the modified H\\'enon-Heiles system is integrable for three distinct parametric restrictions.","Exact solutions are given explicitly for two integrable cases."],"url":"http://arxiv.org/abs/2403.08410v1","category":"nlin.SI"}
{"created":"2024-03-13 10:37:52","title":"FSDR: A Novel Deep Learning-based Feature Selection Algorithm for Pseudo Time-Series Data using Discrete Relaxation","abstract":"Conventional feature selection algorithms applied to Pseudo Time-Series (PTS) data, which consists of observations arranged in sequential order without adhering to a conventional temporal dimension, often exhibit impractical computational complexities with high dimensional data. To address this challenge, we introduce a Deep Learning (DL)-based feature selection algorithm: Feature Selection through Discrete Relaxation (FSDR), tailored for PTS data. Unlike the existing feature selection algorithms, FSDR learns the important features as model parameters using discrete relaxation, which refers to the process of approximating a discrete optimisation problem with a continuous one. FSDR is capable of accommodating a high number of feature dimensions, a capability beyond the reach of existing DL-based or traditional methods. Through testing on a hyperspectral dataset (i.e., a type of PTS data), our experimental results demonstrate that FSDR outperforms three commonly used feature selection algorithms, taking into account a balance among execution time, $R^2$, and $RMSE$.","sentences":["Conventional feature selection algorithms applied to Pseudo Time-Series (PTS) data, which consists of observations arranged in sequential order without adhering to a conventional temporal dimension, often exhibit impractical computational complexities with high dimensional data.","To address this challenge, we introduce a Deep Learning (DL)-based feature selection algorithm: Feature Selection through Discrete Relaxation (FSDR), tailored for PTS data.","Unlike the existing feature selection algorithms, FSDR learns the important features as model parameters using discrete relaxation, which refers to the process of approximating a discrete optimisation problem with a continuous one.","FSDR is capable of accommodating a high number of feature dimensions, a capability beyond the reach of existing DL-based or traditional methods.","Through testing on a hyperspectral dataset (i.e., a type of PTS data), our experimental results demonstrate that FSDR outperforms three commonly used feature selection algorithms, taking into account a balance among execution time, $R^2$, and $RMSE$."],"url":"http://arxiv.org/abs/2403.08403v1","category":"cs.LG"}
{"created":"2024-03-13 10:23:21","title":"Remote UGV Control via Practical Wireless Channels: A Model Predictive Control Approach","abstract":"In addressing wireless networked control systems (WNCS) subject to unexpected packet loss and uncertainties, this paper presents a practical Model Predictive Control (MPC) based control scheme with considerations of of packet dropouts, latency, process noise and measurement noise. A discussion of the quasi-static Rayleigh fading channel is presented herein to enhance the realism of the underlying assumption in a real-world context. To achieve a desirable performance, the proposed control scheme leverages the predictive capabilities of a direct multiple shooting MPC, employs a compensation strategy to mitigate the impact of wireless channel imperfections. Instead of feeding noisy measurements into the MPC, we employ an Extended Kalman Filter (EKF) to mitigate the influence of measurement noise and process disturbances. Finally, we implement the proposed MPC algorithm on a simulated Unmanned Ground Vehicle (UGV) and conduct a series of experiments to evaluate the performance of our control scheme across various scenarios. Through our simulation results and comparative analyses, we have substantiated the effectiveness and improvements brought about by our approach through the utilization of multiple metrics.","sentences":["In addressing wireless networked control systems (WNCS) subject to unexpected packet loss and uncertainties, this paper presents a practical Model Predictive Control (MPC) based control scheme with considerations of of packet dropouts, latency, process noise and measurement noise.","A discussion of the quasi-static Rayleigh fading channel is presented herein to enhance the realism of the underlying assumption in a real-world context.","To achieve a desirable performance, the proposed control scheme leverages the predictive capabilities of a direct multiple shooting MPC, employs a compensation strategy to mitigate the impact of wireless channel imperfections.","Instead of feeding noisy measurements into the MPC, we employ an Extended Kalman Filter (EKF) to mitigate the influence of measurement noise and process disturbances.","Finally, we implement the proposed MPC algorithm on a simulated Unmanned Ground Vehicle (UGV) and conduct a series of experiments to evaluate the performance of our control scheme across various scenarios.","Through our simulation results and comparative analyses, we have substantiated the effectiveness and improvements brought about by our approach through the utilization of multiple metrics."],"url":"http://arxiv.org/abs/2403.08398v1","category":"eess.SY"}
{"created":"2024-03-13 10:11:31","title":"Nonwoven Reinforced Photocurable Poly(glycerol seba-cate)-Based Hydrogels","abstract":"Implantable hydrogels should ideally possess mechanical properties matched to the surrounding tissues to enable adequate mechanical function while regeneration occurs. This can be challenging, especially when degradable systems with high water content and hydrolysable chemical bonds are required in anatomical sites under constant mechanical stimulation, e.g. a foot ulcer cavity. In these circumstances, the design of hydrogel composites is a promising strategy to provide controlled structural features and macroscopic properties over time. To explore this strategy, the synthesis of a new photocurable elastomeric polymer, poly(glycerol-co-sebacic acid-co-lactic acid-co-polyethylene glycol) acrylate (PGSLPA), is investigated, along with its processing into UV-cured hydrogels, electrospun nonwovens and fibre-reinforced variants, without the need for a high temperature curing step or use of hazardous solvents. The mechanical properties of bioresorbable PGSLPA hydrogels were studied with and without electrospun nonwoven reinforcement and with varied layered configurations, aiming to determine the effects of microstructure on bulk compressive strength and elasticity. The nonwoven reinforced PGSLPA hydrogels exhibited a 60 % increase in compressive strength and an 80 % increase in elastic moduli compared to fibre-free PGSLPA samples. Mechanical properties of the fibre-reinforced hydrogels could also be modulated by altering the layering arrangement of the nonwoven and hydrogel phase. The nanofibre reinforced PGSLPA hydrogels also exhibited good elastic recovery, as evidenced by hysteresis in compression fatigue stress-strain evaluations showing a return to original dimensions.","sentences":["Implantable hydrogels should ideally possess mechanical properties matched to the surrounding tissues to enable adequate mechanical function while regeneration occurs.","This can be challenging, especially when degradable systems with high water content and hydrolysable chemical bonds are required in anatomical sites under constant mechanical stimulation, e.g. a foot ulcer cavity.","In these circumstances, the design of hydrogel composites is a promising strategy to provide controlled structural features and macroscopic properties over time.","To explore this strategy, the synthesis of a new photocurable elastomeric polymer, poly(glycerol-co-sebacic acid-co-lactic acid-co-polyethylene glycol) acrylate (PGSLPA), is investigated, along with its processing into UV-cured hydrogels, electrospun nonwovens and fibre-reinforced variants, without the need for a high temperature curing step or use of hazardous solvents.","The mechanical properties of bioresorbable PGSLPA hydrogels were studied with and without electrospun nonwoven reinforcement and with varied layered configurations, aiming to determine the effects of microstructure on bulk compressive strength and elasticity.","The nonwoven reinforced PGSLPA hydrogels exhibited a 60 % increase in compressive strength and an 80 % increase in elastic moduli compared to fibre-free PGSLPA samples.","Mechanical properties of the fibre-reinforced hydrogels could also be modulated by altering the layering arrangement of the nonwoven and hydrogel phase.","The nanofibre reinforced PGSLPA hydrogels also exhibited good elastic recovery, as evidenced by hysteresis in compression fatigue stress-strain evaluations showing a return to original dimensions."],"url":"http://arxiv.org/abs/2403.08392v1","category":"q-bio.TO"}
{"created":"2024-03-13 10:03:03","title":"Order parameters in quasi-1D spin systems","abstract":"In this work we extend the idea of the meanfield. Meanfields approximately map - through some self consistency relation - a complex, usually manybody, problem to a simpler more readily solvable problem. Prototypical examples of simpler meanfield problem (meanfield systems) are the single site and free particle problems - which are solvable. Here we propose a new class of simple meanfield systems where the simple problem to be solved is a 1D spin chain. These meanfields are particularly useful for studying quasi-1D models. We illustrate this idea by considering meanfields for the Ising and ferromagnetic Heisenberg models with one direction coupled much more strongly then the other directions (quasi-1D systems) which map at meanfield level onto the 1D Ising and 1D ferromagnetic Heisenberg models. Magnetic phase transition temperatures and are obtained for both models.","sentences":["In this work we extend the idea of the meanfield.","Meanfields approximately map - through some self consistency relation - a complex, usually manybody, problem to a simpler more readily solvable problem.","Prototypical examples of simpler meanfield problem (meanfield systems) are the single site and free particle problems - which are solvable.","Here we propose a new class of simple meanfield systems where the simple problem to be solved is a 1D spin chain.","These meanfields are particularly useful for studying quasi-1D models.","We illustrate this idea by considering meanfields for the Ising and ferromagnetic Heisenberg models with one direction coupled much more strongly then the other directions (quasi-1D systems) which map at meanfield level onto the 1D Ising and 1D ferromagnetic Heisenberg models.","Magnetic phase transition temperatures and are obtained for both models."],"url":"http://arxiv.org/abs/2403.08389v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-13 09:38:22","title":"Error-Free Near-Optimal Validated Agreement","abstract":"Byzantine agreement enables n processes to agree on a common L-bit value, despite t > 0 arbitrary failures. A long line of work has been dedicated to improving the worst-case bit complexity of Byzantine agreement in synchrony. This has culminated in COOL, an error-free (deterministically secure against a computationally unbounded adversary) algorithm that achieves a near-optimal bit complexity of O(nL + n^2 log n). COOL satisfies strong validity: if all correct processes propose the same value, only that value can be decided. Thus, whenever correct processes do not a priori agree, COOL might decide on \"bottom\", thus limiting its application in today's state machine replication (SMR) and blockchain protocols. In this work, we focus on the aforementioned limitation. Can we design an error-free near-optimal Byzantine agreement algorithm applicable in today's SMR and blockchain protocols? Can we design an error-free near-optimal agreement algorithm with external validity (a.k.a. validated agreement) stipulating that only values valid according to a predetermined predicate can be decided?   This paper answers the question affirmatively. Namely, we present EXT, an error-free synchronous Byzantine agreement algorithm that satisfies external (along with strong) validity while exchanging O(n log n L + n^2 log n) bits in the worst case. Importantly, EXT is optimally resilient (tolerates t < n / 3 failures) and terminates in optimal O(n) rounds. Perhaps surprisingly, we construct EXT by exploiting existing concepts: (1) the recursive framework proposed by Berman, Garay and Perry and Coan and Welch and recently restated by Momose and Ren, (2) the aforementioned COOL algorithm introduced by Chen, and (3) the data dissemination primitive introduced by Das, Xiang and Ren.","sentences":["Byzantine agreement enables n processes to agree on a common L-bit value, despite t > 0 arbitrary failures.","A long line of work has been dedicated to improving the worst-case bit complexity of Byzantine agreement in synchrony.","This has culminated in COOL, an error-free (deterministically secure against a computationally unbounded adversary) algorithm that achieves a near-optimal bit complexity of O(nL + n^2 log n).","COOL satisfies strong validity: if all correct processes propose the same value, only that value can be decided.","Thus, whenever correct processes do not a priori agree, COOL might decide on \"bottom\", thus limiting its application in today's state machine replication (SMR) and blockchain protocols.","In this work, we focus on the aforementioned limitation.","Can we design an error-free near-optimal Byzantine agreement algorithm applicable in today's SMR and blockchain protocols?","Can we design an error-free near-optimal agreement algorithm with external validity (a.k.a. validated agreement) stipulating that only values valid according to a predetermined predicate can be decided?   ","This paper answers the question affirmatively.","Namely, we present EXT, an error-free synchronous Byzantine agreement algorithm that satisfies external (along with strong) validity while exchanging O(n log n L + n^2 log n) bits in the worst case.","Importantly, EXT is optimally resilient (tolerates t < n / 3 failures) and terminates in optimal O(n) rounds.","Perhaps surprisingly, we construct EXT by exploiting existing concepts: (1) the recursive framework proposed by Berman, Garay and Perry and Coan and Welch and recently restated by Momose and Ren, (2) the aforementioned COOL algorithm introduced by Chen, and (3) the data dissemination primitive introduced by Das, Xiang and Ren."],"url":"http://arxiv.org/abs/2403.08374v1","category":"cs.DC"}
{"created":"2024-03-13 09:34:38","title":"Towards the superlubricity of polymer-steel interfaces with ionic liquids and carbon nanotubes","abstract":"Frictional losses are responsible for significant energy waste in many practical applications, and superlubricity with a coefficient of friction lower than 0.01 is the goal of tribologists. In this paper, metal-on-polymer contact was analysed and close to superlubricity conditions for this material configuration were explored. A new lubricant has been proposed hinge on the phosphorus-based ionic liquid and carbon nanotubes as thickeners. Additionally, carbon nanotube mesh was doped with copper nanoparticles that allowed for the close to superlubricity state in a mild steel/polymer contact configuration under low normal load conditions. The adsorption of phosphorus onto metallic and polymer surfaces has been reported in EDS analysis. The formulation of the new lubricant allowed for stable dispersion with a carbon nanotube content as low as 0.1% wt. The carbon nanotubes and Cu nanoparticles have been analysed using TEM and SEM imaging. A tribological test in a block-on-ring system has been carried out. The wear of material, topography, and surface free energy have been analysed along with SEM/EDS images to explore the underlying mechanisms of friction and wear.","sentences":["Frictional losses are responsible for significant energy waste in many practical applications, and superlubricity with a coefficient of friction lower than 0.01 is the goal of tribologists.","In this paper, metal-on-polymer contact was analysed and close to superlubricity conditions for this material configuration were explored.","A new lubricant has been proposed hinge on the phosphorus-based ionic liquid and carbon nanotubes as thickeners.","Additionally, carbon nanotube mesh was doped with copper nanoparticles that allowed for the close to superlubricity state in a mild steel/polymer contact configuration under low normal load conditions.","The adsorption of phosphorus onto metallic and polymer surfaces has been reported in EDS analysis.","The formulation of the new lubricant allowed for stable dispersion with a carbon nanotube content as low as 0.1% wt.","The carbon nanotubes and Cu nanoparticles have been analysed using TEM and SEM imaging.","A tribological test in a block-on-ring system has been carried out.","The wear of material, topography, and surface free energy have been analysed along with SEM/EDS images to explore the underlying mechanisms of friction and wear."],"url":"http://arxiv.org/abs/2403.08373v1","category":"physics.chem-ph"}
{"created":"2024-03-13 09:32:03","title":"User-Centric Beam Selection and Precoding Design for Coordinated Multiple-Satellite Systems","abstract":"This paper introduces a joint optimization framework for user-centric beam selection and linear precoding (LP) design in a coordinated multiple-satellite (CoMSat) system, employing a Digital-Fourier-Transform-based (DFT) beamforming (BF) technique. Regarding serving users at their target SINRs and minimizing the total transmit power, the scheme aims to efficiently determine satellites for users to associate with and activate the best cluster of beams together with optimizing LP for every satellite-to-user transmission. These technical objectives are first framed as a complex mixed-integer programming (MIP) challenge. To tackle this, we reformulate it into a joint cluster association and LP design problem. Then, by theoretically analyzing the duality relationship between downlink and uplink transmissions, we develop an efficient iterative method to identify the optimal solution. Additionally, a simpler duality approach for rapid beam selection and LP design is presented for comparison purposes. Simulation results underscore the effectiveness of our proposed schemes across various settings.","sentences":["This paper introduces a joint optimization framework for user-centric beam selection and linear precoding (LP) design in a coordinated multiple-satellite (CoMSat) system, employing a Digital-Fourier-Transform-based (DFT) beamforming (BF) technique.","Regarding serving users at their target SINRs and minimizing the total transmit power, the scheme aims to efficiently determine satellites for users to associate with and activate the best cluster of beams together with optimizing LP for every satellite-to-user transmission.","These technical objectives are first framed as a complex mixed-integer programming (MIP) challenge.","To tackle this, we reformulate it into a joint cluster association and LP design problem.","Then, by theoretically analyzing the duality relationship between downlink and uplink transmissions, we develop an efficient iterative method to identify the optimal solution.","Additionally, a simpler duality approach for rapid beam selection and LP design is presented for comparison purposes.","Simulation results underscore the effectiveness of our proposed schemes across various settings."],"url":"http://arxiv.org/abs/2403.08371v1","category":"eess.SP"}
{"created":"2024-03-13 09:30:31","title":"Inhomogeneous Floquet thermalization","abstract":"How a closed system thermalizes, especially in the absence of global conservation laws but in the presence of disorder and interactions, is one of the central questions in non-equilibrium statistical mechanics. We explore this for a disordered, periodically driven Ising chain. Our numerical results reveal inhomogeneous thermalization leading to a distribution of thermalization timescales within a single disordered sample, which we encode via a distribution of effective local temperatures. Using this, we find an excellent collapse $\\textit{without}$ $\\textit{any}$ $\\textit{fitting}$ $\\textit{parameters}$ of the local relaxation dynamics for the entire range of disorder values in the ergodic regime when adapting the disorder-averaged diagonal entanglement entropy as internal `time' of the system. This approach evidences a remarkably uniform parametrization of the dynamical many-body evolution of local temperature within the otherwise highly heterogeneous ergodic regime, independent of the strength of the disorder.","sentences":["How a closed system thermalizes, especially in the absence of global conservation laws but in the presence of disorder and interactions, is one of the central questions in non-equilibrium statistical mechanics.","We explore this for a disordered, periodically driven Ising chain.","Our numerical results reveal inhomogeneous thermalization leading to a distribution of thermalization timescales within a single disordered sample, which we encode via a distribution of effective local temperatures.","Using this, we find an excellent collapse $\\textit{without}$ $\\textit{any}$ $\\textit{fitting}$ $\\textit{parameters}$ of the local relaxation dynamics for the entire range of disorder values in the ergodic regime when adapting the disorder-averaged diagonal entanglement entropy as internal `time' of the system.","This approach evidences a remarkably uniform parametrization of the dynamical many-body evolution of local temperature within the otherwise highly heterogeneous ergodic regime, independent of the strength of the disorder."],"url":"http://arxiv.org/abs/2403.08369v1","category":"cond-mat.dis-nn"}
{"created":"2024-03-13 09:30:08","title":"METER: a mobile vision transformer architecture for monocular depth estimation","abstract":"Depth estimation is a fundamental knowledge for autonomous systems that need to assess their own state and perceive the surrounding environment. Deep learning algorithms for depth estimation have gained significant interest in recent years, owing to the potential benefits of this methodology in overcoming the limitations of active depth sensing systems. Moreover, due to the low cost and size of monocular cameras, researchers have focused their attention on monocular depth estimation (MDE), which consists in estimating a dense depth map from a single RGB video frame. State of the art MDE models typically rely on vision transformers (ViT) architectures that are highly deep and complex, making them unsuitable for fast inference on devices with hardware constraints. Purposely, in this paper, we address the problem of exploiting ViT in MDE on embedded devices. Those systems are usually characterized by limited memory capabilities and low-power CPU/GPU. We propose METER, a novel lightweight vision transformer architecture capable of achieving state of the art estimations and low latency inference performances on the considered embedded hardwares: NVIDIA Jetson TX1 and NVIDIA Jetson Nano. We provide a solution consisting of three alternative configurations of METER, a novel loss function to balance pixel estimation and reconstruction of image details, and a new data augmentation strategy to improve the overall final predictions. The proposed method outperforms previous lightweight works over the two benchmark datasets: the indoor NYU Depth v2 and the outdoor KITTI.","sentences":["Depth estimation is a fundamental knowledge for autonomous systems that need to assess their own state and perceive the surrounding environment.","Deep learning algorithms for depth estimation have gained significant interest in recent years, owing to the potential benefits of this methodology in overcoming the limitations of active depth sensing systems.","Moreover, due to the low cost and size of monocular cameras, researchers have focused their attention on monocular depth estimation (MDE), which consists in estimating a dense depth map from a single RGB video frame.","State of the art MDE models typically rely on vision transformers (ViT) architectures that are highly deep and complex, making them unsuitable for fast inference on devices with hardware constraints.","Purposely, in this paper, we address the problem of exploiting ViT in MDE on embedded devices.","Those systems are usually characterized by limited memory capabilities and low-power CPU/GPU.","We propose METER, a novel lightweight vision transformer architecture capable of achieving state of the art estimations and low latency inference performances on the considered embedded hardwares: NVIDIA Jetson TX1 and NVIDIA Jetson Nano.","We provide a solution consisting of three alternative configurations of METER, a novel loss function to balance pixel estimation and reconstruction of image details, and a new data augmentation strategy to improve the overall final predictions.","The proposed method outperforms previous lightweight works over the two benchmark datasets: the indoor NYU Depth v2 and the outdoor KITTI."],"url":"http://arxiv.org/abs/2403.08368v1","category":"cs.CV"}
{"created":"2024-03-13 09:18:46","title":"Log Summarisation for Defect Evolution Analysis","abstract":"Log analysis and monitoring are essential aspects in software maintenance and identifying defects. In particular, the temporal nature and vast size of log data leads to an interesting and important research question: How can logs be summarised and monitored over time? While this has been a fundamental topic of research in the software engineering community, work has typically focused on heuristic-, syntax-, or static-based methods. In this work, we suggest an online semantic-based clustering approach to error logs that dynamically updates the log clusters to enable monitoring code error life-cycles. We also introduce a novel metric to evaluate the performance of temporal log clusters. We test our system and evaluation metric with an industrial dataset and find that our solution outperforms similar systems. We hope that our work encourages further temporal exploration in defect datasets.","sentences":["Log analysis and monitoring are essential aspects in software maintenance and identifying defects.","In particular, the temporal nature and vast size of log data leads to an interesting and important research question: How can logs be summarised and monitored over time?","While this has been a fundamental topic of research in the software engineering community, work has typically focused on heuristic-, syntax-, or static-based methods.","In this work, we suggest an online semantic-based clustering approach to error logs that dynamically updates the log clusters to enable monitoring code error life-cycles.","We also introduce a novel metric to evaluate the performance of temporal log clusters.","We test our system and evaluation metric with an industrial dataset and find that our solution outperforms similar systems.","We hope that our work encourages further temporal exploration in defect datasets."],"url":"http://arxiv.org/abs/2403.08358v1","category":"cs.SE"}
{"created":"2024-03-13 09:18:27","title":"Geometric and electronic properties of two kinds of CrO2 magnetic monolayers: D3d and D2h phases","abstract":"Due to the high magnetic coupling strength between the Cr elements, the bulk phase CrO2 is one of several ferromagnetic oxides known to have the highest Curie temperature. When the dimensionality of the material is reduced from 3D to 2D, the 2D CrO2 system material is expected to maintain a high Curie temperature. In this work, we predict two new phases of CrO2 monolayer (D3d and D2h) by using first-principles calculations. We have found that the Curie temperature of 2D CrO2 is much lower than that of its bulk phase, but still remains as high as 191K, which is comparable to that of Fe2Cr2Ge6. In addition, 1L D3d-CrO2 is in the ferromagnetic state, while 1L D2h-CrO2 is in the antiferromagnetic state. Also, the different geometric structure affects its electrical properties: the 1L D3d-CrO2 is a half-metal while 1L D2h-CrO2 is a semiconductor. Our studies have shown that there is a wealth of electrical and magnetic properties in CrO2.","sentences":["Due to the high magnetic coupling strength between the Cr elements, the bulk phase CrO2 is one of several ferromagnetic oxides known to have the highest Curie temperature.","When the dimensionality of the material is reduced from 3D to 2D, the 2D CrO2 system material is expected to maintain a high Curie temperature.","In this work, we predict two new phases of CrO2 monolayer (D3d and D2h) by using first-principles calculations.","We have found that the Curie temperature of 2D CrO2 is much lower than that of its bulk phase, but still remains as high as 191K, which is comparable to that of Fe2Cr2Ge6.","In addition, 1L D3d-CrO2 is in the ferromagnetic state, while 1L D2h-CrO2 is in the antiferromagnetic state.","Also, the different geometric structure affects its electrical properties: the 1L D3d-CrO2 is a half-metal while 1L D2h-CrO2 is a semiconductor.","Our studies have shown that there is a wealth of electrical and magnetic properties in CrO2."],"url":"http://arxiv.org/abs/2403.08357v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-13 09:02:16","title":"A continuous beam monochromator for matter waves","abstract":"Atom and, of late, molecule interferometers find application in both the crucible of fundamental research and industrial pursuits. A prevalent methodology in the construction of atom interferometers involves the utilisation of gratings fashioned from laser beams. While this approach imparts commendable precision, it is hampered by its incapacity to attain exceedingly short wavelengths and its dependence on intricate laser systems for operational efficacy. All applications require the control of matter waves, particularly the particle's velocity. In this manuscript, we propose a continuous beam monochromator scheme reaching enormously high velocity purification with speed ratios in the order of $10^3$ based on atom-surface diffraction. Beyond these high purifications, the proposed scheme simplifies the application by reducing the degree of freedom to a single angle, selecting the wanted particle's velocity.","sentences":["Atom and, of late, molecule interferometers find application in both the crucible of fundamental research and industrial pursuits.","A prevalent methodology in the construction of atom interferometers involves the utilisation of gratings fashioned from laser beams.","While this approach imparts commendable precision, it is hampered by its incapacity to attain exceedingly short wavelengths and its dependence on intricate laser systems for operational efficacy.","All applications require the control of matter waves, particularly the particle's velocity.","In this manuscript, we propose a continuous beam monochromator scheme reaching enormously high velocity purification with speed ratios in the order of $10^3$ based on atom-surface diffraction.","Beyond these high purifications, the proposed scheme simplifies the application by reducing the degree of freedom to a single angle, selecting the wanted particle's velocity."],"url":"http://arxiv.org/abs/2403.08353v1","category":"quant-ph"}
{"created":"2024-03-13 08:41:08","title":"Mean field error estimate of the random batch method for large interacting particle system","abstract":"The random batch method (RBM) proposed in [Jin et al., J. Comput. Phys., 400(2020), 108877] for large interacting particle systems is an efficient with linear complexity in particle numbers and highly scalable algorithm for $N$-particle interacting systems and their mean-field limits when $N$ is large. We consider in this work the quantitative error estimate of RBM toward its mean-field limit, the Fokker-Planck equation. Under mild assumptions, we obtain a uniform-in-time $O(\\tau^2 + 1/N)$ bound on the scaled relative entropy between the joint law of the random batch particles and the tensorized law at the mean-field limit, where $\\tau$ is the time step size and $N$ is the number of particles. Therefore, we improve the existing rate in discretization step size from $O(\\sqrt{\\tau})$ to $O(\\tau)$ in terms of the Wasserstein distance.","sentences":["The random batch method (RBM) proposed in [Jin et al., J. Comput.","Phys., 400(2020), 108877] for large interacting particle systems is an efficient with linear complexity in particle numbers and highly scalable algorithm for $N$-particle interacting systems and their mean-field limits when $N$ is large.","We consider in this work the quantitative error estimate of RBM toward its mean-field limit, the Fokker-Planck equation.","Under mild assumptions, we obtain a uniform-in-time $O(\\tau^2 + 1/N)$ bound on the scaled relative entropy between the joint law of the random batch particles and the tensorized law at the mean-field limit, where $\\tau$ is the time step size and $N$ is the number of particles.","Therefore, we improve the existing rate in discretization step size from $O(\\sqrt{\\tau})$ to $O(\\tau)$ in terms of the Wasserstein distance."],"url":"http://arxiv.org/abs/2403.08336v1","category":"math.NA"}
{"created":"2024-03-13 08:34:40","title":"Bayesian Optimization that Limits Search Region to Lower Dimensions Utilizing Local GPR","abstract":"Optimization of product and system characteristics is required in many fields, including design and control. Bayesian optimization (BO) is often used when there are high observing costs, because BO theoretically guarantees an upper bound on regret. However, computational costs increase exponentially with the number of parameters to be optimized, decreasing search efficiency. We propose a BO that limits the search region to lower dimensions and utilizes local Gaussian process regression (LGPR) to scale the BO to higher dimensions. LGPR treats the low-dimensional search region as \"local,\" improving prediction accuracies there. The LGPR model is trained on a local subset of data specific to that region. This improves prediction accuracy and search efficiency and reduces the time complexity of matrix inversion in the Gaussian process regression. In evaluations with 20D Ackley and Rosenbrock functions, search efficiencies are equal to or higher than those of the compared methods, improved by about 69% and 40% from the case without LGPR. We apply our method to an automatic design task for a power semiconductor device. We successfully reduce the specific on-resistance to 25% better than a conventional method and 3.4% better than without LGPR.","sentences":["Optimization of product and system characteristics is required in many fields, including design and control.","Bayesian optimization (BO) is often used when there are high observing costs, because BO theoretically guarantees an upper bound on regret.","However, computational costs increase exponentially with the number of parameters to be optimized, decreasing search efficiency.","We propose a BO that limits the search region to lower dimensions and utilizes local Gaussian process regression (LGPR) to scale the BO to higher dimensions.","LGPR treats the low-dimensional search region as \"local,\" improving prediction accuracies there.","The LGPR model is trained on a local subset of data specific to that region.","This improves prediction accuracy and search efficiency and reduces the time complexity of matrix inversion in the Gaussian process regression.","In evaluations with 20D Ackley and Rosenbrock functions, search efficiencies are equal to or higher than those of the compared methods, improved by about 69% and 40% from the case without LGPR.","We apply our method to an automatic design task for a power semiconductor device.","We successfully reduce the specific on-resistance to 25% better than a conventional method and 3.4% better than without LGPR."],"url":"http://arxiv.org/abs/2403.08331v1","category":"cs.LG"}
{"created":"2024-03-13 07:51:26","title":"NNLO QCD corrections to $\u0394\u0393_s$ in the $B_s-\\overline{B}_s$ system","abstract":"This report summarises recent advances made in the calculation of the NNLO QCD corrections to the width difference $\\Delta\\Gamma_s$ in the $B_s-\\overline{B}_s$ system. The inclusion of the effects due to current-current operators leads to an updated prediction of $\\Delta\\Gamma_s = (0.076\\pm 0.017)\\,\\text{ps}^{-1}$, which narrows the gap between theory and experiment.","sentences":["This report summarises recent advances made in the calculation of the NNLO QCD corrections to the width difference $\\Delta\\Gamma_s$ in the $B_s-\\overline{B}_s$ system.","The inclusion of the effects due to current-current operators leads to an updated prediction of $\\Delta\\Gamma_s = (0.076\\pm 0.017)\\,\\text{ps}^{-1}$, which narrows the gap between theory and experiment."],"url":"http://arxiv.org/abs/2403.08316v1","category":"hep-ph"}
{"created":"2024-03-13 07:43:45","title":"When Code Smells Meet ML: On the Lifecycle of ML-specific Code Smells in ML-enabled Systems","abstract":"Context. The adoption of Machine Learning (ML)--enabled systems is steadily increasing. Nevertheless, there is a shortage of ML-specific quality assurance approaches, possibly because of the limited knowledge of how quality-related concerns emerge and evolve in ML-enabled systems. Objective. We aim to investigate the emergence and evolution of specific types of quality-related concerns known as ML-specific code smells, i.e., sub-optimal implementation solutions applied on ML pipelines that may significantly decrease both the quality and maintainability of ML-enabled systems. More specifically, we present a plan to study ML-specific code smells by empirically analyzing (i) their prevalence in real ML-enabled systems, (ii) how they are introduced and removed, and (iii) their survivability. Method. We will conduct an exploratory study, mining a large dataset of ML-enabled systems and analyzing over 400k commits about 337 projects. We will track and inspect the introduction and evolution of ML smells through CodeSmile, a novel ML smell detector that we will build to enable our investigation and to detect ML-specific code smells.","sentences":["Context.","The adoption of Machine Learning (ML)--enabled systems is steadily increasing.","Nevertheless, there is a shortage of ML-specific quality assurance approaches, possibly because of the limited knowledge of how quality-related concerns emerge and evolve in ML-enabled systems.","Objective.","We aim to investigate the emergence and evolution of specific types of quality-related concerns known as ML-specific code smells, i.e., sub-optimal implementation solutions applied on ML pipelines that may significantly decrease both the quality and maintainability of ML-enabled systems.","More specifically, we present a plan to study ML-specific code smells by empirically analyzing (i) their prevalence in real ML-enabled systems, (ii) how they are introduced and removed, and (iii) their survivability.","Method.","We will conduct an exploratory study, mining a large dataset of ML-enabled systems and analyzing over 400k commits about 337 projects.","We will track and inspect the introduction and evolution of ML smells through CodeSmile, a novel ML smell detector that we will build to enable our investigation and to detect ML-specific code smells."],"url":"http://arxiv.org/abs/2403.08311v1","category":"cs.SE"}
{"created":"2024-03-13 07:37:43","title":"Interval Replacements of Persistence Modules","abstract":"We define (1) a notion of a compression system $\\xi$ for a finite poset $\\mathbf{P}$, which assigns each interval subposet $I$ to a poset morphism $\\xi_I \\colon Q_I \\to \\mathbf{P}$ and (2) an $I$-rank of a persistence module $M$ with respect to $\\xi$, the family of which is called the interval rank invariant. A compression system $\\xi$ makes it possible to define the interval replacement (also called the interval-decomposable approximation) not only for 2D persistence modules but also for any persistence modules over any finite poset. We will show that the forming of the interval replacement preserves the interval rank invariant that is a stronger property than the preservation of the usual rank invariant. Moreover, we will give an explicit formula of the $I$-rank of $M$ with respect to $\\xi$ in terms of the structure linear maps of $M$ under a mild existence condition of joins and meets in $I$ in the case where $\\xi_I$ is the inclusion of $I$ into $\\mathbf{P}$, or more generally, $\\xi_I$ ''essentially covers'' $I$.","sentences":["We define (1) a notion of a compression system $\\xi$ for a finite poset $\\mathbf{P}$, which assigns each interval subposet $I$ to a poset morphism $\\xi_I \\colon Q_I \\to \\mathbf{P}$ and (2) an $I$-rank of a persistence module $M$ with respect to $\\xi$, the family of which is called the interval rank invariant.","A compression system $\\xi$ makes it possible to define the interval replacement (also called the interval-decomposable approximation) not only for 2D persistence modules but also for any persistence modules over any finite poset.","We will show that the forming of the interval replacement preserves the interval rank invariant that is a stronger property than the preservation of the usual rank invariant.","Moreover, we will give an explicit formula of the $I$-rank of $M$ with respect to $\\xi$ in terms of the structure linear maps of $M$ under a mild existence condition of joins and meets in $I$ in the case where $\\xi_I$ is the inclusion of $I$ into $\\mathbf{P}$, or more generally, $\\xi_I$ ''essentially covers'' $I$."],"url":"http://arxiv.org/abs/2403.08308v1","category":"math.RT"}
{"created":"2024-03-13 06:57:23","title":"Attack Deterministic Conditional Image Generative Models for Diverse and Controllable Generation","abstract":"Existing generative adversarial network (GAN) based conditional image generative models typically produce fixed output for the same conditional input, which is unreasonable for highly subjective tasks, such as large-mask image inpainting or style transfer. On the other hand, GAN-based diverse image generative methods require retraining/fine-tuning the network or designing complex noise injection functions, which is computationally expensive, task-specific, or struggle to generate high-quality results. Given that many deterministic conditional image generative models have been able to produce high-quality yet fixed results, we raise an intriguing question: is it possible for pre-trained deterministic conditional image generative models to generate diverse results without changing network structures or parameters? To answer this question, we re-examine the conditional image generation tasks from the perspective of adversarial attack and propose a simple and efficient plug-in projected gradient descent (PGD) like method for diverse and controllable image generation. The key idea is attacking the pre-trained deterministic generative models by adding a micro perturbation to the input condition. In this way, diverse results can be generated without any adjustment of network structures or fine-tuning of the pre-trained models. In addition, we can also control the diverse results to be generated by specifying the attack direction according to a reference text or image. Our work opens the door to applying adversarial attack to low-level vision tasks, and experiments on various conditional image generation tasks demonstrate the effectiveness and superiority of the proposed method.","sentences":["Existing generative adversarial network (GAN) based conditional image generative models typically produce fixed output for the same conditional input, which is unreasonable for highly subjective tasks, such as large-mask image inpainting or style transfer.","On the other hand, GAN-based diverse image generative methods require retraining/fine-tuning the network or designing complex noise injection functions, which is computationally expensive, task-specific, or struggle to generate high-quality results.","Given that many deterministic conditional image generative models have been able to produce high-quality yet fixed results, we raise an intriguing question: is it possible for pre-trained deterministic conditional image generative models to generate diverse results without changing network structures or parameters?","To answer this question, we re-examine the conditional image generation tasks from the perspective of adversarial attack and propose a simple and efficient plug-in projected gradient descent (PGD) like method for diverse and controllable image generation.","The key idea is attacking the pre-trained deterministic generative models by adding a micro perturbation to the input condition.","In this way, diverse results can be generated without any adjustment of network structures or fine-tuning of the pre-trained models.","In addition, we can also control the diverse results to be generated by specifying the attack direction according to a reference text or image.","Our work opens the door to applying adversarial attack to low-level vision tasks, and experiments on various conditional image generation tasks demonstrate the effectiveness and superiority of the proposed method."],"url":"http://arxiv.org/abs/2403.08294v1","category":"cs.CV"}
{"created":"2024-03-13 06:49:04","title":"Influence of cholesterol on hydrogen-bond dynamics of water molecules in lipid-bilayer systems at varying temperatures","abstract":"Cholesterol (Chol) plays a crucial role in shaping the intricate physicochemical attributes of biomembranes, exerting considerable influence on water molecules proximal to the membrane interface. In this study, we conducted molecular dynamics simulations on the bilayers of two lipid species, dipalmitoyl phosphatidylcholine (DPPC) and palmitoyl sphingomyelin (PSM); they are distinct with respect to the structures of the hydrogen-bond (H-bond) acceptors. Our investigation focuses on the dynamic properties and H-bonds of water molecules in the lipid-membrane systems, with particular emphasis on the influence of Chol at varying temperatures. Notably, in the gel phase at 303 K, the presence of Chol extends the lifetimes of H-bonds of the oxygen atoms acting as H-bond acceptors within DPPC with water molecules by a factor of 1.5 to 2.5. In the liquid-crystalline phase at 323 K, on the other hand, H-bonding dynamics with lipid membranes remain largely unaffected by Chol. This observed shift in H-bonding states serves as a crucial key to unraveling the subtle control mechanisms governing water dynamics in lipid-membrane systems.","sentences":["Cholesterol (Chol) plays a crucial role in shaping the intricate physicochemical attributes of biomembranes, exerting considerable influence on water molecules proximal to the membrane interface.","In this study, we conducted molecular dynamics simulations on the bilayers of two lipid species, dipalmitoyl phosphatidylcholine (DPPC) and palmitoyl sphingomyelin (PSM); they are distinct with respect to the structures of the hydrogen-bond (H-bond) acceptors.","Our investigation focuses on the dynamic properties and H-bonds of water molecules in the lipid-membrane systems, with particular emphasis on the influence of Chol at varying temperatures.","Notably, in the gel phase at 303 K, the presence of Chol extends the lifetimes of H-bonds of the oxygen atoms acting as H-bond acceptors within DPPC with water molecules by a factor of 1.5 to 2.5.","In the liquid-crystalline phase at 323 K, on the other hand, H-bonding dynamics with lipid membranes remain largely unaffected by Chol.","This observed shift in H-bonding states serves as a crucial key to unraveling the subtle control mechanisms governing water dynamics in lipid-membrane systems."],"url":"http://arxiv.org/abs/2403.08289v1","category":"cond-mat.soft"}
{"created":"2024-03-13 06:48:05","title":"Performance assessment of the effective core potentials under the Fermionic neural network: first and second row elements","abstract":"The rapid development of deep learning techniques has driven the emergence of a neural network-based variational Monte Carlo method (referred to as FermiNet), which has manifested high accuracy and strong predictive power in the electronic structure calculations of atoms, molecules as well as some periodic systems. Recently, the implementation of the effective core potential (ECP) scheme in it further facilitates more efficient calculations in practice. But there still lack a more comprehensive assessment on the ECP's performance under the FermiNet. In this work, we set sail to fill this gap by conducting extensive tests on the first two row elements regarding their atomic spectral and molecular properties. Our major finding is that in general the qualities of ECPs have been correctly reflected under the FermiNet, and a more recently built ECP, ccECP, seems to prevail on the overall performance. Meanwhile, a variation of the transferability between different ECPs has also been observed in the results of the hydrides. On the other hand, the high accuracy of the all-electron calculations is hindered by the absence of relativistic effects as one gets to the second row. Meanwhile the numerical instabilities are more often seen in the all-electron calculations, which could be another source of errors. Finally, with more in-depth discussions, we generate possible directions for developing and improving the FermiNet in the near future.","sentences":["The rapid development of deep learning techniques has driven the emergence of a neural network-based variational Monte Carlo method (referred to as FermiNet), which has manifested high accuracy and strong predictive power in the electronic structure calculations of atoms, molecules as well as some periodic systems.","Recently, the implementation of the effective core potential (ECP) scheme in it further facilitates more efficient calculations in practice.","But there still lack a more comprehensive assessment on the ECP's performance under the FermiNet.","In this work, we set sail to fill this gap by conducting extensive tests on the first two row elements regarding their atomic spectral and molecular properties.","Our major finding is that in general the qualities of ECPs have been correctly reflected under the FermiNet, and a more recently built ECP, ccECP, seems to prevail on the overall performance.","Meanwhile, a variation of the transferability between different ECPs has also been observed in the results of the hydrides.","On the other hand, the high accuracy of the all-electron calculations is hindered by the absence of relativistic effects as one gets to the second row.","Meanwhile the numerical instabilities are more often seen in the all-electron calculations, which could be another source of errors.","Finally, with more in-depth discussions, we generate possible directions for developing and improving the FermiNet in the near future."],"url":"http://arxiv.org/abs/2403.08287v1","category":"physics.comp-ph"}
{"created":"2024-03-13 06:28:37","title":"Optimized Detection and Classification on GTRSB: Advancing Traffic Sign Recognition with Convolutional Neural Networks","abstract":"In the rapidly evolving landscape of transportation, the proliferation of automobiles has made road traffic more complex, necessitating advanced vision-assisted technologies for enhanced safety and navigation. These technologies are imperative for providing critical traffic sign information, influencing driver behavior, and supporting vehicle control, especially for drivers with disabilities and in the burgeoning field of autonomous vehicles. Traffic sign detection and recognition have emerged as key areas of research due to their essential roles in ensuring road safety and compliance with traffic regulations. Traditional computer vision methods have faced challenges in achieving optimal accuracy and speed due to real-world variabilities. However, the advent of deep learning and Convolutional Neural Networks (CNNs) has revolutionized this domain, offering solutions that significantly surpass previous capabilities in terms of speed and reliability. This paper presents an innovative approach leveraging CNNs that achieves an accuracy of nearly 96\\%, highlighting the potential for even greater precision through advanced localization techniques. Our findings not only contribute to the ongoing advancement of traffic sign recognition technology but also underscore the critical impact of these developments on road safety and the future of autonomous driving.","sentences":["In the rapidly evolving landscape of transportation, the proliferation of automobiles has made road traffic more complex, necessitating advanced vision-assisted technologies for enhanced safety and navigation.","These technologies are imperative for providing critical traffic sign information, influencing driver behavior, and supporting vehicle control, especially for drivers with disabilities and in the burgeoning field of autonomous vehicles.","Traffic sign detection and recognition have emerged as key areas of research due to their essential roles in ensuring road safety and compliance with traffic regulations.","Traditional computer vision methods have faced challenges in achieving optimal accuracy and speed due to real-world variabilities.","However, the advent of deep learning and Convolutional Neural Networks (CNNs) has revolutionized this domain, offering solutions that significantly surpass previous capabilities in terms of speed and reliability.","This paper presents an innovative approach leveraging CNNs that achieves an accuracy of nearly 96\\%, highlighting the potential for even greater precision through advanced localization techniques.","Our findings not only contribute to the ongoing advancement of traffic sign recognition technology but also underscore the critical impact of these developments on road safety and the future of autonomous driving."],"url":"http://arxiv.org/abs/2403.08283v1","category":"cs.CV"}
{"created":"2024-03-13 06:22:17","title":"Hierarchical Auto-Organizing System for Open-Ended Multi-Agent Navigation","abstract":"Navigating complex environments in Minecraft poses significant challenges for multi-agent systems due to the game's dynamic and unpredictable open-world setting. Agents need to interact with the environment and coordinate their actions with other agents to achieve common objectives. However, traditional approaches often struggle to efficiently manage inter-agent communication and task distribution, which are crucial for effective multi-agent navigation. Furthermore, processing and integrating multi-modal information (such as visual, textual, and auditory data) is essential for agents to fully comprehend their goals and navigate the environment successfully. To address this issue, we design the HAS framework to auto-organize groups of LLM-based agents to complete Navigation tasks. In our approach, we devise a hierarchical auto-organizing navigation system, which is characterized by 1) a hierarchical system for multi-agent organization, ensuring centralized planning and decentralized execution; 2) an auto-organizing and intra-communication mechanism, enabling dynamic group adjustment under subtasks; 3) a multi-modal information platform, facilitating multi-modal perception to perform the three navigation tasks with one system. To assess organizational behavior, we design a series of navigation tasks in the Minecraft environment, which includes searching and exploring. We aim to develop embodied organizations that push the boundaries of embodied AI, moving it towards a more human-like organizational structure.","sentences":["Navigating complex environments in Minecraft poses significant challenges for multi-agent systems due to the game's dynamic and unpredictable open-world setting.","Agents need to interact with the environment and coordinate their actions with other agents to achieve common objectives.","However, traditional approaches often struggle to efficiently manage inter-agent communication and task distribution, which are crucial for effective multi-agent navigation.","Furthermore, processing and integrating multi-modal information (such as visual, textual, and auditory data) is essential for agents to fully comprehend their goals and navigate the environment successfully.","To address this issue, we design the HAS framework to auto-organize groups of LLM-based agents to complete Navigation tasks.","In our approach, we devise a hierarchical auto-organizing navigation system, which is characterized by 1) a hierarchical system for multi-agent organization, ensuring centralized planning and decentralized execution; 2)","an auto-organizing and intra-communication mechanism, enabling dynamic group adjustment under subtasks; 3) a multi-modal information platform, facilitating multi-modal perception to perform the three navigation tasks with one system.","To assess organizational behavior, we design a series of navigation tasks in the Minecraft environment, which includes searching and exploring.","We aim to develop embodied organizations that push the boundaries of embodied AI, moving it towards a more human-like organizational structure."],"url":"http://arxiv.org/abs/2403.08282v1","category":"cs.CV"}
{"created":"2024-03-13 06:14:17","title":"Point-to-set Principle and Constructive Dimension Faithfulness","abstract":"We introduce a constructive analogue of $\\Phi$-dimension, a notion of Hausdorff dimension developed using a restricted class of coverings of a set. A class of coverings $\\Phi$ is said to be \"faithful\" to Hausdorff dimension if the $\\Phi$-dimension and Hausdorff dimension coincide for every set.   We prove a Point-to-Set Principle for $\\Phi$-dimension, through which we get Point-to-Set Principles for Hausdorff Dimension, continued-fraction dimension and dimension of Cantor Coverings as special cases. Using the Point-to-Set Principle for Cantor coverings and a new technique for the construction of sequences satisfying a certain Kolmogorov complexity condition, we show that the notions of faithfulness of Cantor coverings at the Hausdorff and constructive levels are equivalent.   We adapt the result by Albeverio, Ivanenko, Lebid, and Torbin to derive the necessary and sufficient conditions for the constructive dimension faithfulness of the coverings generated by the Cantor series expansion. This condition yields two general classes of representations of reals, one whose constructive dimensions that are equivalent to the constructive Hausdorff dimensions, and another, whose effective dimensions are different from the effective Hausdorff dimensions, completely classifying Cantor series expansions of reals.","sentences":["We introduce a constructive analogue of $\\Phi$-dimension, a notion of Hausdorff dimension developed using a restricted class of coverings of a set.","A class of coverings $\\Phi$ is said to be \"faithful\" to Hausdorff dimension if the $\\Phi$-dimension and Hausdorff dimension coincide for every set.   ","We prove a Point-to-Set Principle for $\\Phi$-dimension, through which we get Point-to-Set Principles for Hausdorff Dimension, continued-fraction dimension and dimension of Cantor Coverings as special cases.","Using the Point-to-Set Principle for Cantor coverings and a new technique for the construction of sequences satisfying a certain Kolmogorov complexity condition, we show that the notions of faithfulness of Cantor coverings at the Hausdorff and constructive levels are equivalent.   ","We adapt the result by Albeverio, Ivanenko, Lebid, and Torbin to derive the necessary and sufficient conditions for the constructive dimension faithfulness of the coverings generated by the Cantor series expansion.","This condition yields two general classes of representations of reals, one whose constructive dimensions that are equivalent to the constructive Hausdorff dimensions, and another, whose effective dimensions are different from the effective Hausdorff dimensions, completely classifying Cantor series expansions of reals."],"url":"http://arxiv.org/abs/2403.08278v1","category":"cs.IT"}
{"created":"2024-03-13 05:56:27","title":"Unique electronic and optical properties of stacking-modulated bilayer graphene under external magnetic fields","abstract":"This study delves into the magneto-electronic and magneto-optical properties of stacking-modulated bilayer graphene. By manipulating domain walls (DWs) across AB-BA domains periodically, we unveil oscillatory Landau subbands and the associated optical excitations. The DWs act as periodic potentials, yielding fascinating 1D spectral features. Our exploration reveals 1D phenomena localized to Bernal stacking, DW regions, and stacking boundaries, highlighting the intriguing formation of Landau state quantization influenced by the commensuration between the magnetic length and the system. The stable quantized localization within different regions leads to the emergence of unconventional quantized subbands. This study provides valuable insights into the essential properties of stacking-modulated bilayer graphene.","sentences":["This study delves into the magneto-electronic and magneto-optical properties of stacking-modulated bilayer graphene.","By manipulating domain walls (DWs) across AB-BA domains periodically, we unveil oscillatory Landau subbands and the associated optical excitations.","The DWs act as periodic potentials, yielding fascinating 1D spectral features.","Our exploration reveals 1D phenomena localized to Bernal stacking, DW regions, and stacking boundaries, highlighting the intriguing formation of Landau state quantization influenced by the commensuration between the magnetic length and the system.","The stable quantized localization within different regions leads to the emergence of unconventional quantized subbands.","This study provides valuable insights into the essential properties of stacking-modulated bilayer graphene."],"url":"http://arxiv.org/abs/2403.08274v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-13 05:51:57","title":"RECIPE4U: Student-ChatGPT Interaction Dataset in EFL Writing Education","abstract":"The integration of generative AI in education is expanding, yet empirical analyses of large-scale and real-world interactions between students and AI systems still remain limited. Addressing this gap, we present RECIPE4U (RECIPE for University), a dataset sourced from a semester-long experiment with 212 college students in English as Foreign Language (EFL) writing courses. During the study, students engaged in dialogues with ChatGPT to revise their essays. RECIPE4U includes comprehensive records of these interactions, including conversation logs, students' intent, students' self-rated satisfaction, and students' essay edit histories. In particular, we annotate the students' utterances in RECIPE4U with 13 intention labels based on our coding schemes. We establish baseline results for two subtasks in task-oriented dialogue systems within educational contexts: intent detection and satisfaction estimation. As a foundational step, we explore student-ChatGPT interaction patterns through RECIPE4U and analyze them by focusing on students' dialogue, essay data statistics, and students' essay edits. We further illustrate potential applications of RECIPE4U dataset for enhancing the incorporation of LLMs in educational frameworks. RECIPE4U is publicly available at https://zeunie.github.io/RECIPE4U/.","sentences":["The integration of generative AI in education is expanding, yet empirical analyses of large-scale and real-world interactions between students and AI systems still remain limited.","Addressing this gap, we present RECIPE4U (RECIPE for University), a dataset sourced from a semester-long experiment with 212 college students in English as Foreign Language (EFL) writing courses.","During the study, students engaged in dialogues with ChatGPT to revise their essays.","RECIPE4U includes comprehensive records of these interactions, including conversation logs, students' intent, students' self-rated satisfaction, and students' essay edit histories.","In particular, we annotate the students' utterances in RECIPE4U with 13 intention labels based on our coding schemes.","We establish baseline results for two subtasks in task-oriented dialogue systems within educational contexts: intent detection and satisfaction estimation.","As a foundational step, we explore student-ChatGPT interaction patterns through RECIPE4U and analyze them by focusing on students' dialogue, essay data statistics, and students' essay edits.","We further illustrate potential applications of RECIPE4U dataset for enhancing the incorporation of LLMs in educational frameworks.","RECIPE4U is publicly available at https://zeunie.github.io/RECIPE4U/."],"url":"http://arxiv.org/abs/2403.08272v1","category":"cs.CL"}
{"created":"2024-03-13 05:28:46","title":"Generic autonomous system approach to interacting dark energy models","abstract":"We explore an autonomous system analysis of dark energy models with interactions between dark energy and cold dark matter in a general systematic approach to cosmological fluids. We investigate two types of models such as local and non-local ones. In particular, a local form of interaction is directly proportional to only the energy density, while a non-local interaction is directly proportional to the energy density as well as the Hubble parameter. As a consequence, it is explicitly demonstrated that in both cases there exist the stability points in terms of cosmological parameters. This work aims at obtaining acceleration and stability using interaction models without modifying the matter or geometric component of the Universe.","sentences":["We explore an autonomous system analysis of dark energy models with interactions between dark energy and cold dark matter in a general systematic approach to cosmological fluids.","We investigate two types of models such as local and non-local ones.","In particular, a local form of interaction is directly proportional to only the energy density, while a non-local interaction is directly proportional to the energy density as well as the Hubble parameter.","As a consequence, it is explicitly demonstrated that in both cases there exist the stability points in terms of cosmological parameters.","This work aims at obtaining acceleration and stability using interaction models without modifying the matter or geometric component of the Universe."],"url":"http://arxiv.org/abs/2403.08263v1","category":"gr-qc"}
{"created":"2024-03-13 05:23:20","title":"Understanding Reader Takeaways in Thematic Maps Under Varying Text, Detail, and Spatial Autocorrelation","abstract":"Maps are crucial in conveying geospatial data in diverse contexts such as news and scientific reports. This research, utilizing thematic maps, probes deeper into the underexplored intersection of text framing and map types in influencing map interpretation. In this work, we conducted experiments to evaluate how textual detail and semantic content variations affect the quality of insights derived from map examination. We also explored the influence of explanatory annotations across different map types (e.g., choropleth, hexbin, isarithmic), base map details, and changing levels of spatial autocorrelation in the data. From two online experiments with $N=103$ participants, we found that annotations, their specific attributes, and map type used to present the data significantly shape the quality of takeaways. Notably, we found that the effectiveness of annotations hinges on their contextual integration. These findings offer valuable guidance to the visualization community for crafting impactful thematic geospatial representations.","sentences":["Maps are crucial in conveying geospatial data in diverse contexts such as news and scientific reports.","This research, utilizing thematic maps, probes deeper into the underexplored intersection of text framing and map types in influencing map interpretation.","In this work, we conducted experiments to evaluate how textual detail and semantic content variations affect the quality of insights derived from map examination.","We also explored the influence of explanatory annotations across different map types (e.g., choropleth, hexbin, isarithmic), base map details, and changing levels of spatial autocorrelation in the data.","From two online experiments with $N=103$ participants, we found that annotations, their specific attributes, and map type used to present the data significantly shape the quality of takeaways.","Notably, we found that the effectiveness of annotations hinges on their contextual integration.","These findings offer valuable guidance to the visualization community for crafting impactful thematic geospatial representations."],"url":"http://arxiv.org/abs/2403.08260v1","category":"cs.HC"}
{"created":"2024-03-13 05:20:57","title":"The trace operator of quasi-plurisubharmonic functions on compact K\u00e4hler manifolds","abstract":"We introduce the trace operator for quasi-plurisubharmonic functions on compact K\\\"ahler manifolds, allowing to study the singularities of such functions along submanifolds where their generic Lelong numbers vanish. Using this construction we obtain novel Ohsawa--Takegoshi extension theorems and give applications to restricted volumes of big line bundles.","sentences":["We introduce the trace operator for quasi-plurisubharmonic functions on compact K\\\"ahler manifolds, allowing to study the singularities of such functions along submanifolds where their generic Lelong numbers vanish.","Using this construction we obtain novel Ohsawa--Takegoshi extension theorems and give applications to restricted volumes of big line bundles."],"url":"http://arxiv.org/abs/2403.08259v1","category":"math.DG"}
{"created":"2024-03-13 05:07:01","title":"Slowly rotating anisotropic relativistic stars","abstract":"The present paper is devoted to a study of the equilibrium configurations of slowly rotating anisotropic stars in the framework of general relativity. For that purpose, we provide the equations of structure where the rotation is treated to second order in the angular velocity. These equations extend those first derived by Hartle for slowly rotating isotropic stars. As an application of the new formalism, we study the rotational properties of Bowers-Liang fluid spheres. A result of particular interest is that the ellipticity and mass quadrupole moment are negative for certain highly anisotropic configurations, thus such systems are prolate rather than oblate. Furthermore, for configurations with high anisotropy, and compactness close to their critical value, quantities like the moment of inertia, change of mass, and mass quadrupole moment approach to the corresponding Kerr black hole values, similar to other ultracompact systems like sub-Buchdahl Schwarzschild stars and analytic rotating gravastars.","sentences":["The present paper is devoted to a study of the equilibrium configurations of slowly rotating anisotropic stars in the framework of general relativity.","For that purpose, we provide the equations of structure where the rotation is treated to second order in the angular velocity.","These equations extend those first derived by Hartle for slowly rotating isotropic stars.","As an application of the new formalism, we study the rotational properties of Bowers-Liang fluid spheres.","A result of particular interest is that the ellipticity and mass quadrupole moment are negative for certain highly anisotropic configurations, thus such systems are prolate rather than oblate.","Furthermore, for configurations with high anisotropy, and compactness close to their critical value, quantities like the moment of inertia, change of mass, and mass quadrupole moment approach to the corresponding Kerr black hole values, similar to other ultracompact systems like sub-Buchdahl Schwarzschild stars and analytic rotating gravastars."],"url":"http://arxiv.org/abs/2403.08250v1","category":"gr-qc"}
{"created":"2024-03-13 04:58:58","title":"Evaluating the Efficiency and Cost-effectiveness of RPB-based CO2 Capture: A Comprehensive Approach to Simultaneous Design and Operating Condition Optimization","abstract":"Despite ongoing global initiatives to reduce CO2 emissions, implementing large-scale CO2 capture using amine solvents is fraught with economic uncertainties and technical hurdles. The Rotating Packed Bed (RPB) presents a promising alternative to traditional packed towers, offering compact design and adaptability. Nonetheless, scaling RPB processes to an industrial level is challenging due to the nascent nature of its application. The complexity of designing RPB units, setting operating conditions, and evaluating process performance adds layers of difficulty to the adoption of RPB-based systems in industries. This study introduces an optimization-driven design and evaluation for CO2 capture processes utilizing RPB columns. By employing detailed process simulation, we aim to concurrently optimize unit design and operating parameters, underscoring its advantage over conventional sequential approaches. Our process design method integrates heuristic design recommendations as constraints, resulting in 9.4% to 12.7% cost savings compared to conventional sequential design methods. Furthermore, our comprehensive process-level analysis reveals that using concentrated MEA solvent can yield total cost savings of 13.4% to 25.0% compared to the standard 30wt% MEA solvent. Additionally, the RPB unit can deliver an 8.5 to 23.6 times reduction in packing volume. While the commercial-scale feasibility of RPB technology has been established, the advancement of this field hinges on acquiring a broader and more robust dataset from commercial-scale implementations. Employing strategic methods like modularization could significantly reduce the entry barriers for CO2 capture projects, facilitating their broader adoption and implementation.","sentences":["Despite ongoing global initiatives to reduce CO2 emissions, implementing large-scale CO2 capture using amine solvents is fraught with economic uncertainties and technical hurdles.","The Rotating Packed Bed (RPB) presents a promising alternative to traditional packed towers, offering compact design and adaptability.","Nonetheless, scaling RPB processes to an industrial level is challenging due to the nascent nature of its application.","The complexity of designing RPB units, setting operating conditions, and evaluating process performance adds layers of difficulty to the adoption of RPB-based systems in industries.","This study introduces an optimization-driven design and evaluation for CO2 capture processes utilizing RPB columns.","By employing detailed process simulation, we aim to concurrently optimize unit design and operating parameters, underscoring its advantage over conventional sequential approaches.","Our process design method integrates heuristic design recommendations as constraints, resulting in 9.4% to 12.7% cost savings compared to conventional sequential design methods.","Furthermore, our comprehensive process-level analysis reveals that using concentrated MEA solvent can yield total cost savings of 13.4% to 25.0% compared to the standard 30wt% MEA solvent.","Additionally, the RPB unit can deliver an 8.5 to 23.6 times reduction in packing volume.","While the commercial-scale feasibility of RPB technology has been established, the advancement of this field hinges on acquiring a broader and more robust dataset from commercial-scale implementations.","Employing strategic methods like modularization could significantly reduce the entry barriers for CO2 capture projects, facilitating their broader adoption and implementation."],"url":"http://arxiv.org/abs/2403.08244v1","category":"cs.CE"}
{"created":"2024-03-13 04:46:11","title":"Capturing electronic correlations in electron-phonon interactions in molecular systems with the GW approximation","abstract":"Electron-phonon interactions are of great importance to a variety of physical phenomena, and their accurate description is an important goal for first-principles calculations. Isolated examples of materials and molecular systems have emerged where electron-phonon coupling is enhanced over density functional theory (DFT) when using the Green's-function-based ab initio GW method, which provides a more accurate description of electronic correlations. It is however unclear how general this enhancement is, and how employing high-end quantum chemistry methods, which further improve the description of electronic correlations, might further alter electron-phonon interactions over GW or DFT. Here, we address these questions by computing the renormalization of the highest occupied molecular orbital energies of Thiel's set of organic molecules by harmonic vibrations using DFT, GW and equation-of-motion coupled-cluster calculations. We find that GW can increase the magnitude of the electron-phonon coupling across this set of molecules by an average factor of 1.1-1.8 compared to DFT, while equation-of-motion coupled-cluster leads to an increase of 1.4-2. The electron-phonon coupling predicted with the ab initio GW method is generally in much closer agreement to coupled cluster values compared to DFT, establishing GW as an accurate way of computing electron-phonon phenomena in molecules and beyond at a much lower computational cost than higher-end quantum chemistry techniques.","sentences":["Electron-phonon interactions are of great importance to a variety of physical phenomena, and their accurate description is an important goal for first-principles calculations.","Isolated examples of materials and molecular systems have emerged where electron-phonon coupling is enhanced over density functional theory (DFT) when using the Green's-function-based ab initio GW method, which provides a more accurate description of electronic correlations.","It is however unclear how general this enhancement is, and how employing high-end quantum chemistry methods, which further improve the description of electronic correlations, might further alter electron-phonon interactions over GW or DFT.","Here, we address these questions by computing the renormalization of the highest occupied molecular orbital energies of Thiel's set of organic molecules by harmonic vibrations using DFT, GW and equation-of-motion coupled-cluster calculations.","We find that GW can increase the magnitude of the electron-phonon coupling across this set of molecules by an average factor of 1.1-1.8 compared to DFT, while equation-of-motion coupled-cluster leads to an increase of 1.4-2.","The electron-phonon coupling predicted with the ab initio GW method is generally in much closer agreement to coupled cluster values compared to DFT, establishing GW as an accurate way of computing electron-phonon phenomena in molecules and beyond at a much lower computational cost than higher-end quantum chemistry techniques."],"url":"http://arxiv.org/abs/2403.08240v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-13 04:45:40","title":"Continuous Object State Recognition for Cooking Robots Using Pre-Trained Vision-Language Models and Black-box Optimization","abstract":"The state recognition of the environment and objects by robots is generally based on the judgement of the current state as a classification problem. On the other hand, state changes of food in cooking happen continuously and need to be captured not only at a certain time point but also continuously over time. In addition, the state changes of food are complex and cannot be easily described by manual programming. Therefore, we propose a method to recognize the continuous state changes of food for cooking robots through the spoken language using pre-trained large-scale vision-language models. By using models that can compute the similarity between images and texts continuously over time, we can capture the state changes of food while cooking. We also show that by adjusting the weighting of each text prompt based on fitting the similarity changes to a sigmoid function and then performing black-box optimization, more accurate and robust continuous state recognition can be achieved. We demonstrate the effectiveness and limitations of this method by performing the recognition of water boiling, butter melting, egg cooking, and onion stir-frying.","sentences":["The state recognition of the environment and objects by robots is generally based on the judgement of the current state as a classification problem.","On the other hand, state changes of food in cooking happen continuously and need to be captured not only at a certain time point but also continuously over time.","In addition, the state changes of food are complex and cannot be easily described by manual programming.","Therefore, we propose a method to recognize the continuous state changes of food for cooking robots through the spoken language using pre-trained large-scale vision-language models.","By using models that can compute the similarity between images and texts continuously over time, we can capture the state changes of food while cooking.","We also show that by adjusting the weighting of each text prompt based on fitting the similarity changes to a sigmoid function and then performing black-box optimization, more accurate and robust continuous state recognition can be achieved.","We demonstrate the effectiveness and limitations of this method by performing the recognition of water boiling, butter melting, egg cooking, and onion stir-frying."],"url":"http://arxiv.org/abs/2403.08239v1","category":"cs.RO"}
{"created":"2024-03-13 04:23:16","title":"Zero modes of velocity field and topological invariant in quantum torus","abstract":"We propose the velocity field approach to characterize topological invariants of quantum states. We introduce the indexes of the velocity field flow based on the zero modes of the velocity field and find that these zero modes play the role of effective topological charges or defects linking to Euler characteristic by the Poincar\\'{e}-Hopf theorem. The global property of the indexes is topological invariants against the parameter deformation. We demonstrate this approach by the quantum torus model and compare the topological invariant with that obtained from the Chern number. We find that the physical mechanism of the topological invariant based on the zero modes of the velocity field is different from that of the topological invariant by the Chern number. The topological invariant characterized by the velocity field describes a homeomorphic topological invariant associated with the zero modes on the submanifold of the base manifold of the SU(2)-fibre bundle for quantum torus, whereas the Chern number characterizes a homotopy invariant associated with the exceptional points in the Brillouin zone. We also propose the generalized winding number in terms of the velocity field for both Hermitian and non-Hermitian systems. This gives a connection between the zero mode and winding number in the velocity space. These results enrich the topological invariants of quantum states and promises us a novel insight to understanding topological invariants of quantum states as well as expected to be further applied in more generic models.","sentences":["We propose the velocity field approach to characterize topological invariants of quantum states.","We introduce the indexes of the velocity field flow based on the zero modes of the velocity field and find that these zero modes play the role of effective topological charges or defects linking to Euler characteristic by the Poincar\\'{e}-Hopf theorem.","The global property of the indexes is topological invariants against the parameter deformation.","We demonstrate this approach by the quantum torus model and compare the topological invariant with that obtained from the Chern number.","We find that the physical mechanism of the topological invariant based on the zero modes of the velocity field is different from that of the topological invariant by the Chern number.","The topological invariant characterized by the velocity field describes a homeomorphic topological invariant associated with the zero modes on the submanifold of the base manifold of the SU(2)-fibre bundle for quantum torus, whereas the Chern number characterizes a homotopy invariant associated with the exceptional points in the Brillouin zone.","We also propose the generalized winding number in terms of the velocity field for both Hermitian and non-Hermitian systems.","This gives a connection between the zero mode and winding number in the velocity space.","These results enrich the topological invariants of quantum states and promises us a novel insight to understanding topological invariants of quantum states as well as expected to be further applied in more generic models."],"url":"http://arxiv.org/abs/2403.08232v1","category":"quant-ph"}
{"created":"2024-03-13 03:46:46","title":"Help Supporters: Exploring the Design Space of Assistive Technologies to Support Face-to-Face Help Between Blind and Sighted Strangers","abstract":"Blind and low-vision (BLV) people face many challenges when venturing into public environments, often wishing it were easier to get help from people nearby. Ironically, while many sighted individuals are willing to help, such interactions are infrequent. Asking for help is socially awkward for BLV people, and sighted people lack experience in helping BLV people. Through a mixed-ability research-through-design process, we explore four diverse approaches toward how assistive technology can serve as help supporters that collaborate with both BLV and sighted parties throughout the help process. These approaches span two phases: the connection phase (finding someone to help) and the collaboration phase (facilitating help after finding someone). Our findings from a 20-participant mixed-ability study reveal how help supporters can best facilitate connection, which types of information they should present during both phases, and more. We discuss design implications for future approaches to support face-to-face help.","sentences":["Blind and low-vision (BLV) people face many challenges when venturing into public environments, often wishing it were easier to get help from people nearby.","Ironically, while many sighted individuals are willing to help, such interactions are infrequent.","Asking for help is socially awkward for BLV people, and sighted people lack experience in helping BLV people.","Through a mixed-ability research-through-design process, we explore four diverse approaches toward how assistive technology can serve as help supporters that collaborate with both BLV and sighted parties throughout the help process.","These approaches span two phases: the connection phase (finding someone to help) and the collaboration phase (facilitating help after finding someone).","Our findings from a 20-participant mixed-ability study reveal how help supporters can best facilitate connection, which types of information they should present during both phases, and more.","We discuss design implications for future approaches to support face-to-face help."],"url":"http://arxiv.org/abs/2403.08221v1","category":"cs.HC"}
{"created":"2024-03-13 03:34:00","title":"SpaceOctopus: An Octopus-inspired Motion Planning Framework for Multi-arm Space Robot","abstract":"Space robots have played a critical role in autonomous maintenance and space junk removal. Multi-arm space robots can efficiently complete the target capture and base reorientation tasks due to their flexibility and the collaborative capabilities between the arms. However, the complex coupling properties arising from both the multiple arms and the free-floating base present challenges to the motion planning problems of multi-arm space robots. We observe that the octopus elegantly achieves similar goals when grabbing prey and escaping from danger. Inspired by the distributed control of octopuses' limbs, we develop a multi-level decentralized motion planning framework to manage the movement of different arms of space robots. This motion planning framework integrates naturally with the multi-agent reinforcement learning (MARL) paradigm. The results indicate that our method outperforms the previous method (centralized training). Leveraging the flexibility of the decentralized framework, we reassemble policies trained for different tasks, enabling the space robot to complete trajectory planning tasks while adjusting the base attitude without further learning. Furthermore, our experiments confirm the superior robustness of our method in the face of external disturbances, changing base masses, and even the failure of one arm.","sentences":["Space robots have played a critical role in autonomous maintenance and space junk removal.","Multi-arm space robots can efficiently complete the target capture and base reorientation tasks due to their flexibility and the collaborative capabilities between the arms.","However, the complex coupling properties arising from both the multiple arms and the free-floating base present challenges to the motion planning problems of multi-arm space robots.","We observe that the octopus elegantly achieves similar goals when grabbing prey and escaping from danger.","Inspired by the distributed control of octopuses' limbs, we develop a multi-level decentralized motion planning framework to manage the movement of different arms of space robots.","This motion planning framework integrates naturally with the multi-agent reinforcement learning (MARL) paradigm.","The results indicate that our method outperforms the previous method (centralized training).","Leveraging the flexibility of the decentralized framework, we reassemble policies trained for different tasks, enabling the space robot to complete trajectory planning tasks while adjusting the base attitude without further learning.","Furthermore, our experiments confirm the superior robustness of our method in the face of external disturbances, changing base masses, and even the failure of one arm."],"url":"http://arxiv.org/abs/2403.08219v1","category":"cs.RO"}
{"created":"2024-03-13 03:32:23","title":"Non-Hermitian sensing in the absence of exceptional points","abstract":"Open systems possess unique potentials in high-precision sensing, yet the majority of previous studies rely on the spectral singularities known as exceptional points. Here we theoretically propose and experimentally demonstrate universal non-Hermitian sensing in the absence of exceptional points. The scheme makes use of the intrinsic sensitivity of a non-Hermitian probe to weak external fields, which can be understood as the direct consequence of non-Hermiticity. We confirm the basic mechanism by simulating the sensor-field dynamics using photon interferometry, and, as a concrete example, demonstrate the enhanced sensing of signals encoded in the setting angle of a wave plate. While the sensitivity of the probe is ultimately limited by the measurement noise, we find the non-Hermitian sensor showing superior performance under background noises that cannot be suppressed through repetitive measurements. Our experiment opens the avenue of enhanced sensing without exceptional points, complementing existing efforts aimed at harnessing the unique features of open systems.","sentences":["Open systems possess unique potentials in high-precision sensing, yet the majority of previous studies rely on the spectral singularities known as exceptional points.","Here we theoretically propose and experimentally demonstrate universal non-Hermitian sensing in the absence of exceptional points.","The scheme makes use of the intrinsic sensitivity of a non-Hermitian probe to weak external fields, which can be understood as the direct consequence of non-Hermiticity.","We confirm the basic mechanism by simulating the sensor-field dynamics using photon interferometry, and, as a concrete example, demonstrate the enhanced sensing of signals encoded in the setting angle of a wave plate.","While the sensitivity of the probe is ultimately limited by the measurement noise, we find the non-Hermitian sensor showing superior performance under background noises that cannot be suppressed through repetitive measurements.","Our experiment opens the avenue of enhanced sensing without exceptional points, complementing existing efforts aimed at harnessing the unique features of open systems."],"url":"http://arxiv.org/abs/2403.08218v1","category":"quant-ph"}
{"created":"2024-03-13 03:14:28","title":"An exact upper bound for the minimum size of a path system that weakly separates a clique","abstract":"We show that for each natural number $n$ the clique $K_n$ has a weakly separating path system of size $n+2$, improving the previous best known upper bound of $(1+o(1))n$. Since $n-1$ is a lower bound, this is almost tight.","sentences":["We show that for each natural number $n$ the clique $K_n$ has a weakly separating path system of size $n+2$, improving the previous best known upper bound of $(1+o(1))n$. Since $n-1$ is a lower bound, this is almost tight."],"url":"http://arxiv.org/abs/2403.08210v1","category":"math.CO"}
{"created":"2024-03-13 03:10:11","title":"Advancing Security in AI Systems: A Novel Approach to Detecting Backdoors in Deep Neural Networks","abstract":"In the rapidly evolving landscape of communication and network security, the increasing reliance on deep neural networks (DNNs) and cloud services for data processing presents a significant vulnerability: the potential for backdoors that can be exploited by malicious actors. Our approach leverages advanced tensor decomposition algorithms Independent Vector Analysis (IVA), Multiset Canonical Correlation Analysis (MCCA), and Parallel Factor Analysis (PARAFAC2) to meticulously analyze the weights of pre-trained DNNs and distinguish between backdoored and clean models effectively. The key strengths of our method lie in its domain independence, adaptability to various network architectures, and ability to operate without access to the training data of the scrutinized models. This not only ensures versatility across different application scenarios but also addresses the challenge of identifying backdoors without prior knowledge of the specific triggers employed to alter network behavior. We have applied our detection pipeline to three distinct computer vision datasets, encompassing both image classification and object detection tasks. The results demonstrate a marked improvement in both accuracy and efficiency over existing backdoor detection methods. This advancement enhances the security of deep learning and AI in networked systems, providing essential cybersecurity against evolving threats in emerging technologies.","sentences":["In the rapidly evolving landscape of communication and network security, the increasing reliance on deep neural networks (DNNs) and cloud services for data processing presents a significant vulnerability: the potential for backdoors that can be exploited by malicious actors.","Our approach leverages advanced tensor decomposition algorithms Independent Vector Analysis (IVA), Multiset Canonical Correlation Analysis (MCCA), and Parallel Factor Analysis (PARAFAC2) to meticulously analyze the weights of pre-trained DNNs and distinguish between backdoored and clean models effectively.","The key strengths of our method lie in its domain independence, adaptability to various network architectures, and ability to operate without access to the training data of the scrutinized models.","This not only ensures versatility across different application scenarios but also addresses the challenge of identifying backdoors without prior knowledge of the specific triggers employed to alter network behavior.","We have applied our detection pipeline to three distinct computer vision datasets, encompassing both image classification and object detection tasks.","The results demonstrate a marked improvement in both accuracy and efficiency over existing backdoor detection methods.","This advancement enhances the security of deep learning and AI in networked systems, providing essential cybersecurity against evolving threats in emerging technologies."],"url":"http://arxiv.org/abs/2403.08208v1","category":"cs.CR"}
{"created":"2024-03-13 03:03:40","title":"BG-HGNN: Toward Scalable and Efficient Heterogeneous Graph Neural Network","abstract":"Many computer vision and machine learning problems are modelled as learning tasks on heterogeneous graphs, featuring a wide array of relations from diverse types of nodes and edges. Heterogeneous graph neural networks (HGNNs) stand out as a promising neural model class designed for heterogeneous graphs. Built on traditional GNNs, existing HGNNs employ different parameter spaces to model the varied relationships. However, the practical effectiveness of existing HGNNs is often limited to simple heterogeneous graphs with few relation types. This paper first highlights and demonstrates that the standard approach employed by existing HGNNs inevitably leads to parameter explosion and relation collapse, making HGNNs less effective or impractical for complex heterogeneous graphs with numerous relation types. To overcome this issue, we introduce a novel framework, Blend&Grind-HGNN (BG-HGNN), which effectively tackles the challenges by carefully integrating different relations into a unified feature space manageable by a single set of parameters. This results in a refined HGNN method that is more efficient and effective in learning from heterogeneous graphs, especially when the number of relations grows. Our empirical studies illustrate that BG-HGNN significantly surpasses existing HGNNs in terms of parameter efficiency (up to 28.96 $\\times$), training throughput (up to 8.12 $\\times$), and accuracy (up to 1.07 $\\times$).","sentences":["Many computer vision and machine learning problems are modelled as learning tasks on heterogeneous graphs, featuring a wide array of relations from diverse types of nodes and edges.","Heterogeneous graph neural networks (HGNNs) stand out as a promising neural model class designed for heterogeneous graphs.","Built on traditional GNNs, existing HGNNs employ different parameter spaces to model the varied relationships.","However, the practical effectiveness of existing HGNNs is often limited to simple heterogeneous graphs with few relation types.","This paper first highlights and demonstrates that the standard approach employed by existing HGNNs inevitably leads to parameter explosion and relation collapse, making HGNNs less effective or impractical for complex heterogeneous graphs with numerous relation types.","To overcome this issue, we introduce a novel framework, Blend&Grind-HGNN (BG-HGNN), which effectively tackles the challenges by carefully integrating different relations into a unified feature space manageable by a single set of parameters.","This results in a refined HGNN method that is more efficient and effective in learning from heterogeneous graphs, especially when the number of relations grows.","Our empirical studies illustrate that BG-HGNN significantly surpasses existing HGNNs in terms of parameter efficiency (up to 28.96 $\\times$), training throughput (up to 8.12 $\\times$), and accuracy (up to 1.07 $\\times$)."],"url":"http://arxiv.org/abs/2403.08207v1","category":"cs.LG"}
{"created":"2024-03-13 17:17:36","title":"Dynamic computerized tomography using inexact models and motion estimation","abstract":"Reconstructing a dynamic object with affine motion in computerized tomography (CT) leads to motion artifacts if the motion is not taken into account. In most cases, the actual motion is neither known nor can be determined easily. As a consequence, the respective model that describes CT is incomplete. The iterative RESESOP-Kaczmarz method can - under certain conditions and by exploiting the modeling error - reconstruct dynamic objects at different time points even if the exact motion is unknown. However, the method is very time-consuming. To speed the reconstruction process up and obtain better results, we combine the following three steps: 1. RESESOP-Kacmarz with only a few iterations is implemented to reconstruct the object at different time points. 2. The motion is estimated via landmark detection, e.g. using deep learning. 3. The estimated motion is integrated into the reconstruction process, allowing the use of dynamic filtered backprojection. We give a short review of all methods involved and present numerical results as a proof of principle.","sentences":["Reconstructing a dynamic object with affine motion in computerized tomography (CT) leads to motion artifacts if the motion is not taken into account.","In most cases, the actual motion is neither known nor can be determined easily.","As a consequence, the respective model that describes CT is incomplete.","The iterative RESESOP-Kaczmarz method can - under certain conditions and by exploiting the modeling error - reconstruct dynamic objects at different time points even if the exact motion is unknown.","However, the method is very time-consuming.","To speed the reconstruction process up and obtain better results, we combine the following three steps: 1. RESESOP-Kacmarz with only a few iterations is implemented to reconstruct the object at different time points.","2.","The motion is estimated via landmark detection, e.g. using deep learning.","3. The estimated motion is integrated into the reconstruction process, allowing the use of dynamic filtered backprojection.","We give a short review of all methods involved and present numerical results as a proof of principle."],"url":"http://arxiv.org/abs/2403.08714v1","category":"math.NA"}
{"created":"2024-03-13 17:06:52","title":"On the Stochasticity of Aerosol-Cloud Interactions within a Data-driven Framework","abstract":"Aerosol-cloud interactions (ACI) pose the largest uncertainty for climate projections. Among many challenges of understanding ACI, the question of whether ACI is deterministic or stochastic has not been explicitly formulated and asked. Here we attempt to answer this question by predicting cloud droplet number concentration Nc from aerosol number concentration Na and ambient conditions. We use aerosol properties, vertical velocity fluctuation w', and meteorological states (temperature T and water vapor mixing ratio q_v) from the ACTIVATE field observations (2020 to 2022) as predictor variables to estimate Nc. We show that the climatological Nc can be successfully predicted using a machine learning model despite the strongly nonlinear and multi-scale nature of ACI. However, the observation-trained machine learning model fails to predict Nc in individual cases while it successfully predicts Nc of randomly selected data points that cover a broad spatiotemporal scale, suggesting the stochastic nature of ACI at fine spatiotemporal scales.","sentences":["Aerosol-cloud interactions (ACI) pose the largest uncertainty for climate projections.","Among many challenges of understanding ACI, the question of whether ACI is deterministic or stochastic has not been explicitly formulated and asked.","Here we attempt to answer this question by predicting cloud droplet number concentration Nc from aerosol number concentration Na and ambient conditions.","We use aerosol properties, vertical velocity fluctuation w', and meteorological states (temperature T and water vapor mixing ratio q_v) from the ACTIVATE field observations (2020 to 2022) as predictor variables to estimate Nc.","We show that the climatological Nc can be successfully predicted using a machine learning model despite the strongly nonlinear and multi-scale nature of ACI.","However, the observation-trained machine learning model fails to predict Nc in individual cases while it successfully predicts Nc of randomly selected data points that cover a broad spatiotemporal scale, suggesting the stochastic nature of ACI at fine spatiotemporal scales."],"url":"http://arxiv.org/abs/2403.08702v1","category":"physics.ao-ph"}
{"created":"2024-03-13 16:52:55","title":"On the large deviation principle for Metropolis-Hastings Markov Chains: the Lyapunov function condition and examples","abstract":"With an aim to analyse the performance of Markov chain Monte Carlo (MCMC) methods, in our recent work we derive a large deviation principle (LDP) for the empirical measures of Metropolis-Hastings (MH) chains on a continuous state space. One of the (sufficient) assumptions for the LDP involves the existence of a particular type of Lyapunov function, and it was left as an open question whether or not such a function exists for specific choices of MH samplers. In this paper we analyse the properties of such Lyapunov functions and investigate their existence for some of the most popular choices of MCMC samplers built on MH dynamics: Independent Metropolis Hastings, Random Walk Metropolis, and the Metropolis-adjusted Langevin algorithm. We establish under what conditions such a Lyapunov function exists, and from this obtain LDPs for some instances of the MCMC algorithms under consideration. To the best of our knowledge, these are the first large deviation results for empirical measures associated with Metropolis-Hastings chains for specific choices of proposal and target distributions.","sentences":["With an aim to analyse the performance of Markov chain Monte Carlo (MCMC) methods, in our recent work we derive a large deviation principle (LDP) for the empirical measures of Metropolis-Hastings (MH) chains on a continuous state space.","One of the (sufficient) assumptions for the LDP involves the existence of a particular type of Lyapunov function, and it was left as an open question whether or not such a function exists for specific choices of MH samplers.","In this paper we analyse the properties of such Lyapunov functions and investigate their existence for some of the most popular choices of MCMC samplers built on MH dynamics: Independent Metropolis Hastings, Random Walk Metropolis, and the Metropolis-adjusted Langevin algorithm.","We establish under what conditions such a Lyapunov function exists, and from this obtain LDPs for some instances of the MCMC algorithms under consideration.","To the best of our knowledge, these are the first large deviation results for empirical measures associated with Metropolis-Hastings chains for specific choices of proposal and target distributions."],"url":"http://arxiv.org/abs/2403.08691v1","category":"math.PR"}
{"created":"2024-03-13 16:06:26","title":"Extracting Explanations, Justification, and Uncertainty from Black-Box Deep Neural Networks","abstract":"Deep Neural Networks (DNNs) do not inherently compute or exhibit empirically-justified task confidence. In mission critical applications, it is important to both understand associated DNN reasoning and its supporting evidence. In this paper, we propose a novel Bayesian approach to extract explanations, justifications, and uncertainty estimates from DNNs. Our approach is efficient both in terms of memory and computation, and can be applied to any black box DNN without any retraining, including applications to anomaly detection and out-of-distribution detection tasks. We validate our approach on the CIFAR-10 dataset, and show that it can significantly improve the interpretability and reliability of DNNs.","sentences":["Deep Neural Networks (DNNs) do not inherently compute or exhibit empirically-justified task confidence.","In mission critical applications, it is important to both understand associated DNN reasoning and its supporting evidence.","In this paper, we propose a novel Bayesian approach to extract explanations, justifications, and uncertainty estimates from DNNs.","Our approach is efficient both in terms of memory and computation, and can be applied to any black box DNN without any retraining, including applications to anomaly detection and out-of-distribution detection tasks.","We validate our approach on the CIFAR-10 dataset, and show that it can significantly improve the interpretability and reliability of DNNs."],"url":"http://arxiv.org/abs/2403.08652v1","category":"cs.LG"}
{"created":"2024-03-13 15:51:03","title":"Disparate Effect Of Missing Mediators On Transportability of Causal Effects","abstract":"Transported mediation effects provide an avenue to understand how upstream interventions (such as improved neighborhood conditions like green spaces) would work differently when applied to different populations as a result of factors that mediate the effects. However, when mediators are missing in the population where the effect is to be transported, these estimates could be biased. We study this issue of missing mediators, motivated by challenges in public health, wherein mediators can be missing, not at random. We propose a sensitivity analysis framework that quantifies the impact of missing mediator data on transported mediation effects. This framework enables us to identify the settings under which the conditional transported mediation effect is rendered insignificant for the subgroup with missing mediator data. Specifically, we provide the bounds on the transported mediation effect as a function of missingness. We then apply the framework to longitudinal data from the Moving to Opportunity Study, a large-scale housing voucher experiment, to quantify the effect of missing mediators on transport effect estimates of voucher receipt, an upstream intervention on living location, in childhood on subsequent risk of mental health or substance use disorder mediated through parental health across sites. Our findings provide a tangible understanding of how much missing data can be withstood for unbiased effect estimates.","sentences":["Transported mediation effects provide an avenue to understand how upstream interventions (such as improved neighborhood conditions like green spaces) would work differently when applied to different populations as a result of factors that mediate the effects.","However, when mediators are missing in the population where the effect is to be transported, these estimates could be biased.","We study this issue of missing mediators, motivated by challenges in public health, wherein mediators can be missing, not at random.","We propose a sensitivity analysis framework that quantifies the impact of missing mediator data on transported mediation effects.","This framework enables us to identify the settings under which the conditional transported mediation effect is rendered insignificant for the subgroup with missing mediator data.","Specifically, we provide the bounds on the transported mediation effect as a function of missingness.","We then apply the framework to longitudinal data from the Moving to Opportunity Study, a large-scale housing voucher experiment, to quantify the effect of missing mediators on transport effect estimates of voucher receipt, an upstream intervention on living location, in childhood on subsequent risk of mental health or substance use disorder mediated through parental health across sites.","Our findings provide a tangible understanding of how much missing data can be withstood for unbiased effect estimates."],"url":"http://arxiv.org/abs/2403.08638v1","category":"cs.LG"}
{"created":"2024-03-13 15:41:20","title":"Optimal sub-Gaussian variance proxy for truncated Gaussian and exponential random variables","abstract":"This paper establishes the optimal sub-Gaussian variance proxy for truncated Gaussian and truncated exponential random variables. The proofs rely on first characterizing the optimal variance proxy as the unique solution to a set of two equations and then observing that for these two truncated distributions, one may find explicit solutions to this set of equations. Moreover, we establish the conditions under which the optimal variance proxy coincides with the variance, thereby characterizing the strict sub-Gaussianity of the truncated random variables. Specifically, we demonstrate that truncated Gaussian variables exhibit strict sub-Gaussian behavior if and only if they are symmetric, meaning their truncation is symmetric with respect to the mean. Conversely, truncated exponential variables are shown to never exhibit strict sub-Gaussian properties. These findings contribute to the understanding of these prevalent probability distributions in statistics and machine learning, providing a valuable foundation for improved and optimal modeling and decision-making processes.","sentences":["This paper establishes the optimal sub-Gaussian variance proxy for truncated Gaussian and truncated exponential random variables.","The proofs rely on first characterizing the optimal variance proxy as the unique solution to a set of two equations and then observing that for these two truncated distributions, one may find explicit solutions to this set of equations.","Moreover, we establish the conditions under which the optimal variance proxy coincides with the variance, thereby characterizing the strict sub-Gaussianity of the truncated random variables.","Specifically, we demonstrate that truncated Gaussian variables exhibit strict sub-Gaussian behavior if and only if they are symmetric, meaning their truncation is symmetric with respect to the mean.","Conversely, truncated exponential variables are shown to never exhibit strict sub-Gaussian properties.","These findings contribute to the understanding of these prevalent probability distributions in statistics and machine learning, providing a valuable foundation for improved and optimal modeling and decision-making processes."],"url":"http://arxiv.org/abs/2403.08628v1","category":"math.ST"}
{"created":"2024-03-13 15:27:05","title":"TopoTB: A software package for calculating the electronic structure and topological properties of the tight-binding model","abstract":"We present TopoTB, a software package written in the Mathematica language, designed to compute electronic structures, topological properties, and phase diagrams based on tight-binding models. TopoTB is user-friendly, with an interactive user interface that enables the tuning of model parameters for fitting the target energy bands in a WYSIWYG way. In addition, TopoTB also includes functionalities for processing results from Density Functional Theory calculations. The outputs of TopoTB are rich and readable, and they can be displayed in various styles. These features make TopoTB a useful tool for the theoretical study of materials.","sentences":["We present TopoTB, a software package written in the Mathematica language, designed to compute electronic structures, topological properties, and phase diagrams based on tight-binding models.","TopoTB is user-friendly, with an interactive user interface that enables the tuning of model parameters for fitting the target energy bands in a WYSIWYG way.","In addition, TopoTB also includes functionalities for processing results from Density Functional Theory calculations.","The outputs of TopoTB are rich and readable, and they can be displayed in various styles.","These features make TopoTB a useful tool for the theoretical study of materials."],"url":"http://arxiv.org/abs/2403.08615v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-13 15:22:40","title":"Galaxy dispersion measured by Fast Radio Bursts as a probe of baryonic feedback models","abstract":"Fast Radio Bursts (FRBs) are a sensitive probe of the electron distribution in both the large-scale structure and their host galaxies through the dispersion measure (DM) of the radio pulse. Baryonic feedback models are crucial for modelling small scales for ongoing cosmological surveys that are expected to change the electron distribution in galaxies in a way that can be probed by FRB observations. In this paper, we explore the impact of baryonic feedback on FRB hosts using numerical simulations and make a detailed study of the host galaxy dispersion as a function of redshift, galaxy type, feedback model and how these properties vary in independent simulation codes. We find that the host galaxy dispersion varies dramatically between different implementations of baryonic feedback, allowing FRBs with host identification to be a valuable probe of feedback physics and thus provide necessary priors for upcoming analysis of the statistical properties of the large-scale structure.   We further find that any dependency on the exact location of events within the halo is small. While there exists an evolution of the dispersion measure with redshift and halo mass, it is largely driven by varying star formation rates of the halo. Spectral information from FRB hosts can therefore be used to put priors on the host galaxy dispersion measure, and FRBs can be used to distinguish between competing models of baryonic feedback in future studies.","sentences":["Fast Radio Bursts (FRBs) are a sensitive probe of the electron distribution in both the large-scale structure and their host galaxies through the dispersion measure (DM) of the radio pulse.","Baryonic feedback models are crucial for modelling small scales for ongoing cosmological surveys that are expected to change the electron distribution in galaxies in a way that can be probed by FRB observations.","In this paper, we explore the impact of baryonic feedback on FRB hosts using numerical simulations and make a detailed study of the host galaxy dispersion as a function of redshift, galaxy type, feedback model and how these properties vary in independent simulation codes.","We find that the host galaxy dispersion varies dramatically between different implementations of baryonic feedback, allowing FRBs with host identification to be a valuable probe of feedback physics and thus provide necessary priors for upcoming analysis of the statistical properties of the large-scale structure.   ","We further find that any dependency on the exact location of events within the halo is small.","While there exists an evolution of the dispersion measure with redshift and halo mass, it is largely driven by varying star formation rates of the halo.","Spectral information from FRB hosts can therefore be used to put priors on the host galaxy dispersion measure, and FRBs can be used to distinguish between competing models of baryonic feedback in future studies."],"url":"http://arxiv.org/abs/2403.08611v1","category":"astro-ph.CO"}
{"created":"2024-03-13 15:15:21","title":"Language-Grounded Dynamic Scene Graphs for Interactive Object Search with Mobile Manipulation","abstract":"To fully leverage the capabilities of mobile manipulation robots, it is imperative that they are able to autonomously execute long-horizon tasks in large unexplored environments. While large language models (LLMs) have shown emergent reasoning skills on arbitrary tasks, existing work primarily concentrates on explored environments, typically focusing on either navigation or manipulation tasks in isolation. In this work, we propose MoMa-LLM, a novel approach that grounds language models within structured representations derived from open-vocabulary scene graphs, dynamically updated as the environment is explored. We tightly interleave these representations with an object-centric action space. The resulting approach is zero-shot, open-vocabulary, and readily extendable to a spectrum of mobile manipulation and household robotic tasks. We demonstrate the effectiveness of MoMa-LLM in a novel semantic interactive search task in large realistic indoor environments. In extensive experiments in both simulation and the real world, we show substantially improved search efficiency compared to conventional baselines and state-of-the-art approaches, as well as its applicability to more abstract tasks. We make the code publicly available at http://moma-llm.cs.uni-freiburg.de.","sentences":["To fully leverage the capabilities of mobile manipulation robots, it is imperative that they are able to autonomously execute long-horizon tasks in large unexplored environments.","While large language models (LLMs) have shown emergent reasoning skills on arbitrary tasks, existing work primarily concentrates on explored environments, typically focusing on either navigation or manipulation tasks in isolation.","In this work, we propose MoMa-LLM, a novel approach that grounds language models within structured representations derived from open-vocabulary scene graphs, dynamically updated as the environment is explored.","We tightly interleave these representations with an object-centric action space.","The resulting approach is zero-shot, open-vocabulary, and readily extendable to a spectrum of mobile manipulation and household robotic tasks.","We demonstrate the effectiveness of MoMa-LLM in a novel semantic interactive search task in large realistic indoor environments.","In extensive experiments in both simulation and the real world, we show substantially improved search efficiency compared to conventional baselines and state-of-the-art approaches, as well as its applicability to more abstract tasks.","We make the code publicly available at http://moma-llm.cs.uni-freiburg.de."],"url":"http://arxiv.org/abs/2403.08605v1","category":"cs.RO"}
{"created":"2024-03-13 15:07:23","title":"Toward mapping turbulence in the intracluster medium III. Constraints on the turbulent power spectrum with Athena/X-IFU","abstract":"Context. Future X-ray observatories with high spectral resolution and imaging capabilities will enable measurements and mappings of emission line shifts in the intracluster medium (ICM). Such direct measurements can serve as unique probes of turbulent motions in the ICM. Determining the level and scales of turbulence will improve our understanding of the galaxy cluster dynamical evolution and assembly, together with a more precise evaluation of the non thermal support pressure budget. This will allow for more accurate constraints to be placed on the masses of galaxy clusters, among other potential benfits. Aims. In this view, we implemented the methods presented in the previous instalments of our work to characterize the turbulence in the ICM in a feasibility study with the X-IFU on board the future European X-ray observatory, Athena. Methods. From idealized mock observations of a toy model cluster, we reconstructed the second-order structure function built with the observed velocity field to constrain the turbulence. We carefully accounted for the various sources of errors to derive the most realistic and comprehensive error budget within the limits of our approach. With prior assumptions on the dissipation scale and power spectrum slope, we constrained the parameters of the turbulent power spectrum model through the use of MCMC sampling. Results. With favourable assumptions, we were able to retrieve the injection scale, velocity dispersion, and power spectrum slope, with 1sigma uncertainties better than ~15% of the input values. We demonstrated the efficiency of our carefully set framework to constrain the turbulence in the ICM from high-resolution X-ray spectroscopic observations, paving the way for more in-depth investigation of the optimal required observing strategy within a more restrictive observational setup with the future X-IFU instrument.","sentences":["Context.","Future X-ray observatories with high spectral resolution and imaging capabilities will enable measurements and mappings of emission line shifts in the intracluster medium (ICM).","Such direct measurements can serve as unique probes of turbulent motions in the ICM.","Determining the level and scales of turbulence will improve our understanding of the galaxy cluster dynamical evolution and assembly, together with a more precise evaluation of the non thermal support pressure budget.","This will allow for more accurate constraints to be placed on the masses of galaxy clusters, among other potential benfits.","Aims.","In this view, we implemented the methods presented in the previous instalments of our work to characterize the turbulence in the ICM in a feasibility study with the X-IFU on board the future European X-ray observatory, Athena. Methods.","From idealized mock observations of a toy model cluster, we reconstructed the second-order structure function built with the observed velocity field to constrain the turbulence.","We carefully accounted for the various sources of errors to derive the most realistic and comprehensive error budget within the limits of our approach.","With prior assumptions on the dissipation scale and power spectrum slope, we constrained the parameters of the turbulent power spectrum model through the use of MCMC sampling.","Results.","With favourable assumptions, we were able to retrieve the injection scale, velocity dispersion, and power spectrum slope, with 1sigma uncertainties better than ~15% of the input values.","We demonstrated the efficiency of our carefully set framework to constrain the turbulence in the ICM from high-resolution X-ray spectroscopic observations, paving the way for more in-depth investigation of the optimal required observing strategy within a more restrictive observational setup with the future X-IFU instrument."],"url":"http://arxiv.org/abs/2403.08601v1","category":"astro-ph.CO"}
{"created":"2024-03-13 14:42:55","title":"PRAGO: Differentiable Multi-View Pose Optimization From Objectness Detections","abstract":"Robustly estimating camera poses from a set of images is a fundamental task which remains challenging for differentiable methods, especially in the case of small and sparse camera pose graphs. To overcome this challenge, we propose Pose-refined Rotation Averaging Graph Optimization (PRAGO). From a set of objectness detections on unordered images, our method reconstructs the rotational pose, and in turn, the absolute pose, in a differentiable manner benefiting from the optimization of a sequence of geometrical tasks. We show how our objectness pose-refinement module in PRAGO is able to refine the inherent ambiguities in pairwise relative pose estimation without removing edges and avoiding making early decisions on the viability of graph edges. PRAGO then refines the absolute rotations through iterative graph construction, reweighting the graph edges to compute the final rotational pose, which can be converted into absolute poses using translation averaging. We show that PRAGO is able to outperform non-differentiable solvers on small and sparse scenes extracted from 7-Scenes achieving a relative improvement of 21% for rotations while achieving similar translation estimates.","sentences":["Robustly estimating camera poses from a set of images is a fundamental task which remains challenging for differentiable methods, especially in the case of small and sparse camera pose graphs.","To overcome this challenge, we propose Pose-refined Rotation Averaging Graph Optimization (PRAGO).","From a set of objectness detections on unordered images, our method reconstructs the rotational pose, and in turn, the absolute pose, in a differentiable manner benefiting from the optimization of a sequence of geometrical tasks.","We show how our objectness pose-refinement module in PRAGO is able to refine the inherent ambiguities in pairwise relative pose estimation without removing edges and avoiding making early decisions on the viability of graph edges.","PRAGO then refines the absolute rotations through iterative graph construction, reweighting the graph edges to compute the final rotational pose, which can be converted into absolute poses using translation averaging.","We show that PRAGO is able to outperform non-differentiable solvers on small and sparse scenes extracted from 7-Scenes achieving a relative improvement of 21% for rotations while achieving similar translation estimates."],"url":"http://arxiv.org/abs/2403.08586v1","category":"cs.CV"}
{"created":"2024-03-13 14:10:01","title":"Narrowly-Banded Spectra with Peak Frequency Around 1 GHz of FRB 20201124A: Implications for Energy Function and Radiation Physics","abstract":"The radiation physics of fast radio bursts (FRBs) remains an open question. Current observations have discovered that narrowly-banded bursts of FRB 20201124A are active in 0.4-2 GHz and their spectral peak frequency ($\\nu^{\\rm obs}_{p}$) are mostly toward $\\sim 1$ GHz. Utilizing a sample of 1268 bursts of FRB 20201124A detected with the FAST telescope, we show that the $1\\sigma$ spectral regime of 71.4\\% events (in-band bursts) is within the FAST bandpass. Their intrinsic burst energies ($E^{\\rm obs}_{\\rm BWe}$) and spectral widths ($\\sigma_s^{\\rm obs}$) are well measured by fitting the spectral profile with a Gaussian function. The derived $E^{\\rm obs}_{\\rm BWe}$ and $\\sigma_s^{\\rm obs}$ distributions are log-normal and centering at $\\log E^{\\rm obs}_{\\rm BWe}/{\\rm erg}=37.2~ (\\sigma=0.76)$ and $\\log \\sigma_s^{\\rm obs}/{\\rm GHz}=-1.16~ (\\sigma=0.17)$. Our Monte Carlo simulation analysis infers its intrinsic $\\nu_p$ distribution as a normal function centered at $\\nu_{p,c}=1.16$ GHz ($\\sigma=0.22$) and its intrinsic energy function as $\\Phi(E)\\propto E^{-0.60}e^{-E/E_c}$ with $E_c=9.49 \\times 10^{37}$ erg. We compare these results with that of typical repeating FRBs 20121102A and 20190520B that are active over a broad frequency range at several specific frequencies and discuss possible observational biases on the estimation of the event rate and energy function. Based on these results, we argue that FRB 20201124A likely occurs in a fine-tuned plasma for maser radiations at a narrow frequency range, while FRB 20121102A and FRB 20190520B could involve clumpy plasma conditions that make maser emission around several specific frequencies in a broad range.","sentences":["The radiation physics of fast radio bursts (FRBs) remains an open question.","Current observations have discovered that narrowly-banded bursts of FRB 20201124A are active in 0.4-2 GHz and their spectral peak frequency ($\\nu^{\\rm obs}_{p}$) are mostly toward $\\sim 1$ GHz.","Utilizing a sample of 1268 bursts of FRB 20201124A detected with the FAST telescope, we show that the $1\\sigma$ spectral regime of 71.4\\% events (in-band bursts) is within the FAST bandpass.","Their intrinsic burst energies ($E^{\\rm obs}_{\\rm BWe}$) and spectral widths ($\\sigma_s^{\\rm obs}$) are well measured by fitting the spectral profile with a Gaussian function.","The derived $E^{\\rm obs}_{\\rm BWe}$ and $\\sigma_s^{\\rm obs}$ distributions are log-normal and centering at $\\log E^{\\rm obs}_{\\rm BWe}/{\\rm erg}=37.2~ (\\sigma=0.76)$ and $\\log \\sigma_s^{\\rm obs}/{\\rm GHz}=-1.16~ (\\sigma=0.17)$. Our Monte Carlo simulation analysis infers its intrinsic $\\nu_p$ distribution as a normal function centered at $\\nu_{p,c}=1.16$ GHz ($\\sigma=0.22$) and its intrinsic energy function as $\\Phi(E)\\propto E^{-0.60}e^{-E/E_c}$ with $E_c=9.49 \\times 10^{37}$ erg.","We compare these results with that of typical repeating FRBs 20121102A and 20190520B that are active over a broad frequency range at several specific frequencies and discuss possible observational biases on the estimation of the event rate and energy function.","Based on these results, we argue that FRB 20201124A likely occurs in a fine-tuned plasma for maser radiations at a narrow frequency range, while FRB 20121102A and FRB 20190520B could involve clumpy plasma conditions that make maser emission around several specific frequencies in a broad range."],"url":"http://arxiv.org/abs/2403.08558v1","category":"astro-ph.HE"}
{"created":"2024-03-13 14:00:10","title":"The Massalia asteroid family as the origin of ordinary L chondrites","abstract":"Studies of micrometeorites in mid-Ordovician limestones and Earth's impact craters indicate that our planet witnessed a massive infall of ordinary L chondrite material 466 million years (My) ago (Heck et al. 2017, Schmieder & Kring 2020, Kenkmann 2021) that may have been at the origin of the first major mass extinction event (Schmitz et al. 2019). The breakup of a large asteroid in the main belt is the likely cause of this massive infall. In modern times, material originating from this breakup still dominates meteorite falls (>20% of all falls) (Swindle et al. 2014). Here, we provide spectroscopic observations and dynamical evidence that the Massalia collisional family is the only plausible source of this catastrophic event and of the most abundant class of meteorites falling on Earth today. It is suitably located in the inner belt, at low-inclination orbits, which corresponds to the observed distribution of L-chondrite-like near-Earth objects (NEOs) and of interplanetary dust concentrated at 1.4 degrees (Sykes 1990, Reach et al. 1997).","sentences":["Studies of micrometeorites in mid-Ordovician limestones and Earth's impact craters indicate that our planet witnessed a massive infall of ordinary L chondrite material 466 million years (My) ago (Heck et al. 2017, Schmieder & Kring 2020, Kenkmann 2021) that may have been at the origin of the first major mass extinction event (Schmitz et al. 2019).","The breakup of a large asteroid in the main belt is the likely cause of this massive infall.","In modern times, material originating from this breakup still dominates meteorite falls (>20% of all falls)","(Swindle et al. 2014).","Here, we provide spectroscopic observations and dynamical evidence that the Massalia collisional family is the only plausible source of this catastrophic event and of the most abundant class of meteorites falling on Earth today.","It is suitably located in the inner belt, at low-inclination orbits, which corresponds to the observed distribution of L-chondrite-like near-Earth objects (NEOs) and of interplanetary dust concentrated at 1.4 degrees (Sykes 1990, Reach et al. 1997)."],"url":"http://arxiv.org/abs/2403.08548v1","category":"astro-ph.EP"}
{"created":"2024-03-13 13:49:12","title":"Ensuring connectedness for the Maximum Quasi-clique and Densest $k$-subgraph problems","abstract":"Given an undirected graph $G$, a quasi-clique is a subgraph of $G$ whose density is at least $\\gamma$ $(0 < \\gamma \\leq 1)$. Two optimization problems can be defined for quasi-cliques: the Maximum Quasi-Clique (MQC) Problem, which finds a quasi-clique with maximum vertex cardinality, and the Densest $k$-Subgraph (DKS) Problem, which finds the densest subgraph given a fixed cardinality constraint. Most existing approaches to solve both problems often disregard the requirement of connectedness, which may lead to solutions containing isolated components that are meaningless for many real-life applications. To address this issue, we propose two flow-based connectedness constraints to be integrated into known Mixed-Integer Linear Programming (MILP) formulations for either MQC or DKS problems. We compare the performance of MILP formulations enhanced with our connectedness constraints in terms of both running time and number of solved instances against existing approaches that ensure quasi-clique connectedness. Experimental results demonstrate that our constraints are quite competitive, making them valuable for practical applications requiring connectedness.","sentences":["Given an undirected graph $G$, a quasi-clique is a subgraph of $G$ whose density is at least $\\gamma$ $(0 <","\\gamma \\leq 1)$.","Two optimization problems can be defined for quasi-cliques: the Maximum Quasi-Clique (MQC) Problem, which finds a quasi-clique with maximum vertex cardinality, and the Densest $k$-Subgraph (DKS) Problem, which finds the densest subgraph given a fixed cardinality constraint.","Most existing approaches to solve both problems often disregard the requirement of connectedness, which may lead to solutions containing isolated components that are meaningless for many real-life applications.","To address this issue, we propose two flow-based connectedness constraints to be integrated into known Mixed-Integer Linear Programming (MILP) formulations for either MQC or DKS problems.","We compare the performance of MILP formulations enhanced with our connectedness constraints in terms of both running time and number of solved instances against existing approaches that ensure quasi-clique connectedness.","Experimental results demonstrate that our constraints are quite competitive, making them valuable for practical applications requiring connectedness."],"url":"http://arxiv.org/abs/2403.08534v1","category":"cs.DM"}
{"created":"2024-03-13 13:33:35","title":"From Weak to Strong Sound Event Labels using Adaptive Change-Point Detection and Active Learning","abstract":"In this work we propose an audio recording segmentation method based on an adaptive change point detection (A-CPD) for machine guided weak label annotation of audio recording segments. The goal is to maximize the amount of information gained about the temporal activation's of the target sounds. For each unlabeled audio recording, we use a prediction model to derive a probability curve used to guide annotation. The prediction model is initially pre-trained on available annotated sound event data with classes that are disjoint from the classes in the unlabeled dataset. The prediction model then gradually adapts to the annotations provided by the annotator in an active learning loop. The queries used to guide the weak label annotator towards strong labels are derived using change point detection on these probabilities. We show that it is possible to derive strong labels of high quality even with a limited annotation budget, and show favorable results for A-CPD when compared to two baseline query strategies.","sentences":["In this work we propose an audio recording segmentation method based on an adaptive change point detection (A-CPD) for machine guided weak label annotation of audio recording segments.","The goal is to maximize the amount of information gained about the temporal activation's of the target sounds.","For each unlabeled audio recording, we use a prediction model to derive a probability curve used to guide annotation.","The prediction model is initially pre-trained on available annotated sound event data with classes that are disjoint from the classes in the unlabeled dataset.","The prediction model then gradually adapts to the annotations provided by the annotator in an active learning loop.","The queries used to guide the weak label annotator towards strong labels are derived using change point detection on these probabilities.","We show that it is possible to derive strong labels of high quality even with a limited annotation budget, and show favorable results for A-CPD when compared to two baseline query strategies."],"url":"http://arxiv.org/abs/2403.08525v1","category":"cs.SD"}
{"created":"2024-03-13 13:24:57","title":"What Does the Large Magellanic Cloud Look Like? It Depends on [M/H] and Age","abstract":"We offer a new way to look at the Large Magellanic Cloud through stellar mono-abundance and mono-age-mono-abundance maps. These maps are based on $\\gtrsim 500\\,000$ member stars with photo-spectroscopic [M/H] and age estimates from Gaia DR3 data, and they are the first area-complete, metallicity- and age-differentiated stellar maps of any disk galaxy. Azimuthally averaged, these maps reveal a surprisingly simple picture of the Milky Way's largest satellite galaxy. For any [M/H] below -0.1 dex, the LMC's radial profile is well described by a simple exponential, but with a scale length that steadily shrinks towards higher metallicities, from nearly 2.3~kpc at [M/H]$=-1.8$ to only 0.75~kpc at [M/H]$=-0.25$. The prominence of the bar decreases dramatically with [M/H], making it barely discernible at [M/H]$\\lesssim -1.5$. Yet, even for metal-rich populations, the bar has little impact on the azimuthally averaged profile of the mono-abundance components. Including ages, we find that the scale length is a greater function of age than of metallicity, with younger populations far more centrally concentrated. At old ages, the scale length decreases with increasing metallicity; at young ages, the scale-length is independent of metallicity. These findings provide quantitative support for a scenario where the LMC built its stellar structure effectively outside in.","sentences":["We offer a new way to look at the Large Magellanic Cloud through stellar mono-abundance and mono-age-mono-abundance maps.","These maps are based on $\\gtrsim 500\\,000$ member stars with photo-spectroscopic [M/H] and age estimates from Gaia DR3 data, and they are the first area-complete, metallicity- and age-differentiated stellar maps of any disk galaxy.","Azimuthally averaged, these maps reveal a surprisingly simple picture of the Milky Way's largest satellite galaxy.","For any [M/H] below -0.1 dex, the LMC's radial profile is well described by a simple exponential, but with a scale length that steadily shrinks towards higher metallicities, from nearly 2.3~kpc at [M/H]$=-1.8$ to only 0.75~kpc at [M/H]$=-0.25$. The prominence of the bar decreases dramatically with [M/H], making it barely discernible at [M/H]$\\lesssim","-1.5$. Yet, even for metal-rich populations, the bar has little impact on the azimuthally averaged profile of the mono-abundance components.","Including ages, we find that the scale length is a greater function of age than of metallicity, with younger populations far more centrally concentrated.","At old ages, the scale length decreases with increasing metallicity; at young ages, the scale-length is independent of metallicity.","These findings provide quantitative support for a scenario where the LMC built its stellar structure effectively outside in."],"url":"http://arxiv.org/abs/2403.08516v1","category":"astro-ph.GA"}
{"created":"2024-03-13 13:24:09","title":"Spatial Latent Gaussian Modelling with Change of Support","abstract":"Spatial data are often derived from multiple sources (e.g. satellites, in-situ sensors, survey samples) with different supports, but associated with the same properties of a spatial phenomenon of interest. It is common for predictors to also be measured on different spatial supports than the response variables. Although there is no standard way to work with spatial data with different supports, a prevalent approach used by practitioners has been to use downscaling or interpolation to project all the variables of analysis towards a common support, and then using standard spatial models. The main disadvantage with this approach is that simple interpolation can introduce biases and, more importantly, the uncertainty associated with the change of support is not taken into account in parameter estimation. In this article, we propose a Bayesian spatial latent Gaussian model that can handle data with different rectilinear supports in both the response variable and predictors. Our approach allows to handle changes of support more naturally according to the properties of the spatial stochastic process being used, and to take into account the uncertainty from the change of support in parameter estimation and prediction. We use spatial stochastic processes as linear combinations of basis functions where Gaussian Markov random fields define the weights. Our hierarchical modelling approach can be described by the following steps: (i) define a latent model where response variables and predictors are considered as latent stochastic processes with continuous support, (ii) link the continuous-index set stochastic processes with its projection to the support of the observed data, (iii) link the projected process with the observed data. We show the applicability of our approach by simulation studies and modelling land suitability for improved grassland in Rhondda Cynon Taf, a county borough in Wales.","sentences":["Spatial data are often derived from multiple sources (e.g. satellites, in-situ sensors, survey samples) with different supports, but associated with the same properties of a spatial phenomenon of interest.","It is common for predictors to also be measured on different spatial supports than the response variables.","Although there is no standard way to work with spatial data with different supports, a prevalent approach used by practitioners has been to use downscaling or interpolation to project all the variables of analysis towards a common support, and then using standard spatial models.","The main disadvantage with this approach is that simple interpolation can introduce biases and, more importantly, the uncertainty associated with the change of support is not taken into account in parameter estimation.","In this article, we propose a Bayesian spatial latent Gaussian model that can handle data with different rectilinear supports in both the response variable and predictors.","Our approach allows to handle changes of support more naturally according to the properties of the spatial stochastic process being used, and to take into account the uncertainty from the change of support in parameter estimation and prediction.","We use spatial stochastic processes as linear combinations of basis functions where Gaussian Markov random fields define the weights.","Our hierarchical modelling approach can be described by the following steps: (i) define a latent model where response variables and predictors are considered as latent stochastic processes with continuous support, (ii) link the continuous-index set stochastic processes with its projection to the support of the observed data, (iii) link the projected process with the observed data.","We show the applicability of our approach by simulation studies and modelling land suitability for improved grassland in Rhondda Cynon Taf, a county borough in Wales."],"url":"http://arxiv.org/abs/2403.08514v1","category":"stat.ME"}
{"created":"2024-03-13 13:02:04","title":"On the structure of graded Lie superalgebras","abstract":"We study the structure of graded Lie superalgebras with arbitrary dimension and over an arbitrary field ${\\mathbb K}$. We show that any of such algebras ${\\mathfrak L}$ with a symmetric $G$-support is of the form ${\\mathfrak L} = U + \\sum\\limits_{j}I_{j}$ with $U$ a subspace of ${\\mathfrak L}_1$ and any $I_{j}$ a well described graded ideal of ${\\mathfrak L}$, satisfying $[I_j,I_k] = 0$ if $j\\neq k$. Under certain conditions, it is shown that ${\\mathfrak L} = (\\bigoplus\\limits_{k \\in K} I_k) \\oplus (\\bigoplus\\limits_{q \\in Q} I_q),$ where any $I_k$ is a gr-simple graded ideal of ${\\mathfrak L}$ and any $I_q$ a completely determined low dimensional non gr-simple graded ideal of ${\\mathfrak L}$, satisfying $[I_q,I_{q'}] = 0$ for any $q'\\in Q$ with $q \\neq q'$.","sentences":["We study the structure of graded Lie superalgebras with arbitrary dimension and over an arbitrary field ${\\mathbb K}$. We show that any of such algebras ${\\mathfrak L}$ with a symmetric $G$-support is of the form ${\\mathfrak L} = U + \\sum\\limits_{j}I_{j}$ with $U$ a subspace of ${\\mathfrak L}_1$ and any $I_{j}$ a well described graded ideal of ${\\mathfrak L}$, satisfying $[I_j,I_k] = 0$ if $j\\neq k$.","Under certain conditions, it is shown that ${\\mathfrak L} = (\\bigoplus\\limits_{k \\in K} I_k) \\oplus (\\bigoplus\\limits_{q \\in Q} I_q),$ where any $I_k$ is a gr-simple graded ideal of ${\\mathfrak L}$ and any $I_q$ a completely determined low dimensional non gr-simple graded ideal of ${\\mathfrak L}$, satisfying $[I_q,I_{q'}] = 0$ for any $q'\\in Q$ with $q \\neq q'$."],"url":"http://arxiv.org/abs/2403.08494v1","category":"math.RT"}
{"created":"2024-03-13 12:58:36","title":"A Prediction Model for Rumor Forwarding Behavior Based on Uncertain Time Series","abstract":"The rapid spread of rumors in social media is mainly caused by individual retweets. This paper applies uncertainty time series analysis (UTSA) to analyze a rumor retweeting behavior on Weibo. First, the rumor forwarding is modeled using uncertain time series, including order selection, parameter estimation, residual analysis, uncertainty hypothesis testing and forecast, and the validity of using uncertain time series analysis is further supported by analyzing the characteristics of the residual plot. The experimental results show that the uncertain time series can better predict the next stage of rumor forwarding. The results of the study have important practical significance for rumor management and the management of social media information dissemination.","sentences":["The rapid spread of rumors in social media is mainly caused by individual retweets.","This paper applies uncertainty time series analysis (UTSA) to analyze a rumor retweeting behavior on Weibo.","First, the rumor forwarding is modeled using uncertain time series, including order selection, parameter estimation, residual analysis, uncertainty hypothesis testing and forecast, and the validity of using uncertain time series analysis is further supported by analyzing the characteristics of the residual plot.","The experimental results show that the uncertain time series can better predict the next stage of rumor forwarding.","The results of the study have important practical significance for rumor management and the management of social media information dissemination."],"url":"http://arxiv.org/abs/2403.08493v1","category":"cs.SI"}
{"created":"2024-03-13 12:51:53","title":"Opportunities and open questions in modern $\u03b2$ decay","abstract":"For well over half a century, precision studies of neutron and nuclear $\\beta$ decays have been at the forefront of searches for exotic electroweak physics. Recent advances in nuclear ab initio theory and the widespread use of effective field theories means that its modern understanding is going through a transitional phase. This has been propelled by current tensions in the global data set leading to renewed scrutiny of its theoretical ingredients. In parallel, a host of novel techniques and methods are being investigated that are able to sidestep many traditional systematic uncertainties and require a diverse palette of skills and collaboration with material science and condensed matter physics. We highlight the current opportunities and open questions with the aim of facilitating the transition to a more modern understanding of $\\beta$ decay.","sentences":["For well over half a century, precision studies of neutron and nuclear $\\beta$ decays have been at the forefront of searches for exotic electroweak physics.","Recent advances in nuclear ab initio theory and the widespread use of effective field theories means that its modern understanding is going through a transitional phase.","This has been propelled by current tensions in the global data set leading to renewed scrutiny of its theoretical ingredients.","In parallel, a host of novel techniques and methods are being investigated that are able to sidestep many traditional systematic uncertainties and require a diverse palette of skills and collaboration with material science and condensed matter physics.","We highlight the current opportunities and open questions with the aim of facilitating the transition to a more modern understanding of $\\beta$ decay."],"url":"http://arxiv.org/abs/2403.08485v1","category":"nucl-th"}
{"created":"2024-03-13 12:08:43","title":"Metallicities for more than 10 million stars derived from Gaia BP/RP spectra","abstract":"Context. The third Gaia Data Release, which includes BP/RP spectra for 219 million sources, has opened a new window in the exploration of the chemical history and evolution of the Milky Way. The wealth of information encapsulated in these data is far greater than their low resolving power (R=50) at first glance would suggest, as shown in many studies. We zero in on the use of this data for the purpose of the detection of ''new'' metal-poor stars, which are hard to find yet essential for understanding - among other - several aspects of the origin of the Galaxy, star formation and the creation of the elements. Aims. We strive to refine a metal-poor candidate selection method which was developed with simulated Gaia BP/RP spectra, with an ultimate objective of providing the community with both a recipe to select stars for medium/high resolution observations and a catalogue of stellar metallicities. Methods. We used a datased comprised of GALAH DR3 and SAGA database stars in order to verify and adjust to real world data our selection method. For that purpose, we used dereddening as a mean to tackle the issue of extinction, and then we applied our fine-tuned method to select metal-poor candidates, which we thereafter observed and analysed. Results. We were able to infer metallicities for GALAH DR3 and SAGA stars - with color excesses up to E(B-V)<1.5 - with an uncertainty of 0.36 dex, which is good enough for the purpose of identifying new metal-poor stars. Further, we selected 26 metal-poor candidates - via our method - for observations. As spectral analysis showed, 100% of them had [Fe/H]<-2.0, 57% had [Fe/H]<-2.5 and 8% had [Fe/H]<-3.0. We inferred metallicities for these stars with an uncertainty of 0.31 dex, as was proven when comparing to the spectroscopic [Fe/H]. Finally, we assembled a catalogue of metallicities for 10 861 062 stars.","sentences":["Context.","The third Gaia Data Release, which includes BP/RP spectra for 219 million sources, has opened a new window in the exploration of the chemical history and evolution of the Milky Way.","The wealth of information encapsulated in these data is far greater than their low resolving power (R=50) at first glance would suggest, as shown in many studies.","We zero in on the use of this data for the purpose of the detection of ''new'' metal-poor stars, which are hard to find yet essential for understanding - among other - several aspects of the origin of the Galaxy, star formation and the creation of the elements.","Aims.","We strive to refine a metal-poor candidate selection method which was developed with simulated Gaia BP/RP spectra, with an ultimate objective of providing the community with both a recipe to select stars for medium/high resolution observations and a catalogue of stellar metallicities.","Methods.","We used a datased comprised of GALAH DR3 and SAGA database stars in order to verify and adjust to real world data our selection method.","For that purpose, we used dereddening as a mean to tackle the issue of extinction, and then we applied our fine-tuned method to select metal-poor candidates, which we thereafter observed and analysed.","Results.","We were able to infer metallicities for GALAH DR3 and SAGA stars - with color excesses up to E(B-V)<1.5 - with an uncertainty of 0.36 dex, which is good enough for the purpose of identifying new metal-poor stars.","Further, we selected 26 metal-poor candidates - via our method - for observations.","As spectral analysis showed, 100% of them had [Fe/H]<-2.0, 57% had [Fe/H]<-2.5 and 8% had [Fe/H]<-3.0.","We inferred metallicities for these stars with an uncertainty of 0.31 dex, as was proven when comparing to the spectroscopic [Fe/H].","Finally, we assembled a catalogue of metallicities for 10 861 062 stars."],"url":"http://arxiv.org/abs/2403.08454v1","category":"astro-ph.SR"}
{"created":"2024-03-13 12:07:14","title":"Better Fit: Accommodate Variations in Clothing Types for Virtual Try-on","abstract":"Image-based virtual try-on aims to transfer target in-shop clothing to a dressed model image, the objectives of which are totally taking off original clothing while preserving the contents outside of the try-on area, naturally wearing target clothing and correctly inpainting the gap between target clothing and original clothing. Tremendous efforts have been made to facilitate this popular research area, but cannot keep the type of target clothing with the try-on area affected by original clothing. In this paper, we focus on the unpaired virtual try-on situation where target clothing and original clothing on the model are different, i.e., the practical scenario. To break the correlation between the try-on area and the original clothing and make the model learn the correct information to inpaint, we propose an adaptive mask training paradigm that dynamically adjusts training masks. It not only improves the alignment and fit of clothing but also significantly enhances the fidelity of virtual try-on experience. Furthermore, we for the first time propose two metrics for unpaired try-on evaluation, the Semantic-Densepose-Ratio (SDR) and Skeleton-LPIPS (S-LPIPS), to evaluate the correctness of clothing type and the accuracy of clothing texture. For unpaired try-on validation, we construct a comprehensive cross-try-on benchmark (Cross-27) with distinctive clothing items and model physiques, covering a broad try-on scenarios. Experiments demonstrate the effectiveness of the proposed methods, contributing to the advancement of virtual try-on technology and offering new insights and tools for future research in the field. The code, model and benchmark will be publicly released.","sentences":["Image-based virtual try-on aims to transfer target in-shop clothing to a dressed model image, the objectives of which are totally taking off original clothing while preserving the contents outside of the try-on area, naturally wearing target clothing and correctly inpainting the gap between target clothing and original clothing.","Tremendous efforts have been made to facilitate this popular research area, but cannot keep the type of target clothing with the try-on area affected by original clothing.","In this paper, we focus on the unpaired virtual try-on situation where target clothing and original clothing on the model are different, i.e., the practical scenario.","To break the correlation between the try-on area and the original clothing and make the model learn the correct information to inpaint, we propose an adaptive mask training paradigm that dynamically adjusts training masks.","It not only improves the alignment and fit of clothing but also significantly enhances the fidelity of virtual try-on experience.","Furthermore, we for the first time propose two metrics for unpaired try-on evaluation, the Semantic-Densepose-Ratio (SDR) and Skeleton-LPIPS (S-LPIPS), to evaluate the correctness of clothing type and the accuracy of clothing texture.","For unpaired try-on validation, we construct a comprehensive cross-try-on benchmark (Cross-27) with distinctive clothing items and model physiques, covering a broad try-on scenarios.","Experiments demonstrate the effectiveness of the proposed methods, contributing to the advancement of virtual try-on technology and offering new insights and tools for future research in the field.","The code, model and benchmark will be publicly released."],"url":"http://arxiv.org/abs/2403.08453v1","category":"cs.CV"}
{"created":"2024-03-13 11:36:55","title":"Asymptotic behaviour of integer programming and the $\\text{v}$-function of a graded filtration","abstract":"The $\\text{v}$-function of a graded filtration $\\mathcal{I}=\\{I_{[k]}\\}_{k\\ge0}$ is introduced. Under the assumption that $\\mathcal{I}$ is Noetherian, we prove that the $\\text{v}$-function $\\text{v}(I_{[k]})$ is an eventually quasi-linear function. This result applies to several situations, including ordinary powers, and integral closures of ordinary powers, among others. As another application, we investigate the asymptotic behaviour of certain integer programming problems. Finally, we present the Macaulay2 package \\texttt{VNumber}.","sentences":["The $\\text{v}$-function of a graded filtration $\\mathcal{I}=\\{I_{[k]}\\}_{k\\ge0}$ is introduced.","Under the assumption that $\\mathcal{I}$ is Noetherian, we prove that the $\\text{v}$-function $\\text{v}(I_{[k]})$ is an eventually quasi-linear function.","This result applies to several situations, including ordinary powers, and integral closures of ordinary powers, among others.","As another application, we investigate the asymptotic behaviour of certain integer programming problems.","Finally, we present the Macaulay2 package \\texttt{VNumber}."],"url":"http://arxiv.org/abs/2403.08435v1","category":"math.AC"}
{"created":"2024-03-13 11:24:21","title":"Annihilation of positrons from AGN jets as a possible source of cosmic gamma-ray background at energies below 511 keV","abstract":"The origin of the diffuse gamma-ray background in the range from hundreds keV to several MeV is not known conclusively. From current models and observations it is believed that, at least partially, this background is formed by blazars and remnants of supernovae (SN) of type Ia in distant galaxies. However, these contributions are not sufficient to reproduce the observed level of the signal. In this work we propose another source which could contribute to this background, namely the jets of active galactic nuclei (AGN). The composition of jets is not known, but there are observational hints that the fraction of positrons there is substantial. Positrons are partially evacuated to the intergalactic medium and partially mix with the circumgalactic medium and annihilate there comparatively quickly. Using the AGN luminosity function, we estimated the positron production rate and the contribution of the positron annihilation to the cosmic background below 511 keV. We also estimated the analogous contribution from positron annihilation within SN Ia remnants in distant galaxies. The contribution of AGNs is estimated to be a factor of 5 - 10 smaller than the observed background intensity, and the contribution from SNe is yet smaller by one order of magnitude. Nevertheless, the contribution of AGNs appeared to be larger than the contribution of blazars estimated from Swift-BAT and Fermi-LAT observations. The main uncertainty in our model is the fraction of positrons remaining in the circumgalactic medium which makes our estimation an upper limit.","sentences":["The origin of the diffuse gamma-ray background in the range from hundreds keV to several MeV is not known conclusively.","From current models and observations it is believed that, at least partially, this background is formed by blazars and remnants of supernovae (SN) of type Ia in distant galaxies.","However, these contributions are not sufficient to reproduce the observed level of the signal.","In this work we propose another source which could contribute to this background, namely the jets of active galactic nuclei (AGN).","The composition of jets is not known, but there are observational hints that the fraction of positrons there is substantial.","Positrons are partially evacuated to the intergalactic medium and partially mix with the circumgalactic medium and annihilate there comparatively quickly.","Using the AGN luminosity function, we estimated the positron production rate and the contribution of the positron annihilation to the cosmic background below 511 keV.","We also estimated the analogous contribution from positron annihilation within SN Ia remnants in distant galaxies.","The contribution of AGNs is estimated to be a factor of 5 - 10 smaller than the observed background intensity, and the contribution from SNe is yet smaller by one order of magnitude.","Nevertheless, the contribution of AGNs appeared to be larger than the contribution of blazars estimated from Swift-BAT and Fermi-LAT observations.","The main uncertainty in our model is the fraction of positrons remaining in the circumgalactic medium which makes our estimation an upper limit."],"url":"http://arxiv.org/abs/2403.08427v1","category":"astro-ph.HE"}
{"created":"2024-03-13 11:07:39","title":"Powers and roots of partial isometric covariant representations","abstract":"Isometric covariant representations play an important role in the study of Cuntz-Pimsner algebras. In this article, we study partial isometric covariant representations and explore under what conditions powers and roots of partial isometric covariant representations are also partial isometric covariant representations.","sentences":["Isometric covariant representations play an important role in the study of Cuntz-Pimsner algebras.","In this article, we study partial isometric covariant representations and explore under what conditions powers and roots of partial isometric covariant representations are also partial isometric covariant representations."],"url":"http://arxiv.org/abs/2403.08418v1","category":"math.OA"}
{"created":"2024-03-13 10:52:07","title":"On the universal properties of stochastic processes under optimally tuned Poisson restart","abstract":"Poisson restart assumes that a stochastic process is interrupted and starts again at random time moments. A number of studies have demonstrated that this strategy may minimize the expected completion time in some classes of random search tasks. What is more, it turned out that under optimally tuned restart rate, any stochastic process, regardless of its nature and statistical details, satisfies a number of universal relations for the statistical moments of completion time. In this paper, we describe several new universal properties of optimally restarted processes. Also we obtain a universal inequality for the quadratic statistical moments of completion time in the optimization problem where stochastic process has several possible completion scenarios.","sentences":["Poisson restart assumes that a stochastic process is interrupted and starts again at random time moments.","A number of studies have demonstrated that this strategy may minimize the expected completion time in some classes of random search tasks.","What is more, it turned out that under optimally tuned restart rate, any stochastic process, regardless of its nature and statistical details, satisfies a number of universal relations for the statistical moments of completion time.","In this paper, we describe several new universal properties of optimally restarted processes.","Also we obtain a universal inequality for the quadratic statistical moments of completion time in the optimization problem where stochastic process has several possible completion scenarios."],"url":"http://arxiv.org/abs/2403.08409v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-13 10:29:01","title":"CLASSY IX: The Chemical Evolution of the Ne, S, Cl, and Ar Elements","abstract":"To study the chemical evolution across cosmic epochs, we investigate Ne, S, Cl, and Ar abundance patterns in the COS Legacy Archive Spectroscopic SurveY (CLASSY). CLASSY comprises local star-forming galaxies (0.02 < z < 0.18) with enhanced star-formation rates, making them strong analogues to high-z star-forming galaxies. With direct measurements of electron temperature, we derive accurate ionic abundances for all elements and assess ionization correction factors (ICFs) to account for unseen ions and derive total abundances. We find Ne/O, S/O, Cl/O, and Ar/O exhibit constant trends with gas-phase metallicity for 12+log(O/H) < 8.5 but significant correlation for Ne/O and Ar/O with metallicity for 12+log(O/H) > 8.5, likely due to ICFs. Thus, applicability of the ICFs to integrated spectra of galaxies could bias results, underestimating true abundance ratios. Using CLASSY as a local reference, we assess the evolution of Ne/O, S/O, and Ar/O in galaxies at z>3, finding no cosmic evolution of Ne/O, while the lack of direct abundance determinations for S/O and Ar/O can bias the interpretation of the evolution of these elements. We determine the fundamental metallicity relationship (FMR) for CLASSY and compare to the high-redshift FMR, finding no evolution. Finally, we perform the first mass-neon relationship analysis across cosmic epochs, finding a slight evolution to high Ne at later epochs. The robust abundance patterns of CLASSY galaxies and their broad range of physical properties provide essential benchmarks for interpreting the chemical enrichment of the early galaxies observed with the JWST.","sentences":["To study the chemical evolution across cosmic epochs, we investigate Ne, S, Cl, and Ar abundance patterns in the COS Legacy Archive Spectroscopic SurveY (CLASSY).","CLASSY comprises local star-forming galaxies (0.02 < z < 0.18) with enhanced star-formation rates, making them strong analogues to high-z star-forming galaxies.","With direct measurements of electron temperature, we derive accurate ionic abundances for all elements and assess ionization correction factors (ICFs) to account for unseen ions and derive total abundances.","We find Ne/O, S/O, Cl/O, and Ar/O exhibit constant trends with gas-phase metallicity for 12+log(O/H) < 8.5 but significant correlation for Ne/O and Ar/O with metallicity for 12+log(O/H) >","8.5,","likely due to ICFs.","Thus, applicability of the ICFs to integrated spectra of galaxies could bias results, underestimating true abundance ratios.","Using CLASSY as a local reference, we assess the evolution of Ne/O, S/O, and Ar/O in galaxies at z>3, finding no cosmic evolution of Ne/O, while the lack of direct abundance determinations for S/O and Ar/O can bias the interpretation of the evolution of these elements.","We determine the fundamental metallicity relationship (FMR) for CLASSY and compare to the high-redshift FMR, finding no evolution.","Finally, we perform the first mass-neon relationship analysis across cosmic epochs, finding a slight evolution to high Ne at later epochs.","The robust abundance patterns of CLASSY galaxies and their broad range of physical properties provide essential benchmarks for interpreting the chemical enrichment of the early galaxies observed with the JWST."],"url":"http://arxiv.org/abs/2403.08401v1","category":"astro-ph.GA"}
{"created":"2024-03-13 10:21:32","title":"Interactive environments for training children's curiosity through the practice of metacognitive skills: a pilot study","abstract":"Curiosity-driven learning has shown significant positive effects on students' learning experiences and outcomes. But despite this importance, reports show that children lack this skill, especially in formal educational settings. To address this challenge, we propose an 8-session workshop that aims to enhance children's curiosity through training a set of specific metacognitive skills we hypothesize are involved in its process. Our workshop contains animated videos presenting declarative knowledge about curiosity and the said metacognitive skills as well as practice sessions to apply these skills during a reading-comprehension task, using a web platform designed for this study (e.g. expressing uncertainty, formulating questions, etc). We conduct a pilot study with 15 primary school students, aged between 8 and 10. Our first results show a positive impact on children's metacognitive efficiency and their ability to express their curiosity through question-asking behaviors.","sentences":["Curiosity-driven learning has shown significant positive effects on students' learning experiences and outcomes.","But despite this importance, reports show that children lack this skill, especially in formal educational settings.","To address this challenge, we propose an 8-session workshop that aims to enhance children's curiosity through training a set of specific metacognitive skills we hypothesize are involved in its process.","Our workshop contains animated videos presenting declarative knowledge about curiosity and the said metacognitive skills as well as practice sessions to apply these skills during a reading-comprehension task, using a web platform designed for this study (e.g. expressing uncertainty, formulating questions, etc).","We conduct a pilot study with 15 primary school students, aged between 8 and 10.","Our first results show a positive impact on children's metacognitive efficiency and their ability to express their curiosity through question-asking behaviors."],"url":"http://arxiv.org/abs/2403.08397v1","category":"cs.CY"}
{"created":"2024-03-13 08:11:27","title":"Sparse Bayesian Learning-Based Hierarchical Construction for 3D Radio Environment Maps Incorporating Channel Shadowing","abstract":"The radio environment map (REM) visually displays the spectrum information over the geographical map and plays a significant role in monitoring, management, and security of spectrum resources.In this paper, we present an efficient 3D REM construction scheme based on the sparse Bayesian learning (SBL), which aims to recovery the accurate REM with limited and optimized sampling data.In order to reduce the number of sampling sensors, an efficient sparse sampling method for unknown scenarios is proposed. For the given construction accuracy and the priority of each location, the quantity and sampling locations can be jointly optimized.With the sparse sampled data, by mining the spectrum situation sparsity and channel propagation characteristics, an SBL-based spectrum data hierarchical recovery algorithm is developed to estimate the missing data of unsampled locations.Finally, the simulated 3D REM data in the campus scenario are used to verify the proposed methods as well as to compare with the state-of-the-art. We also analyze the recovery performance and the impact of different parameters on the constructed REMs. Numerical results demonstrate that the proposed scheme can ensure the construction accuracy and improve the computational efficiency under the low sampling rate.","sentences":["The radio environment map (REM) visually displays the spectrum information over the geographical map and plays a significant role in monitoring, management, and security of spectrum resources.","In this paper, we present an efficient 3D REM construction scheme based on the sparse Bayesian learning (SBL), which aims to recovery the accurate REM with limited and optimized sampling data.","In order to reduce the number of sampling sensors, an efficient sparse sampling method for unknown scenarios is proposed.","For the given construction accuracy and the priority of each location, the quantity and sampling locations can be jointly optimized.","With the sparse sampled data, by mining the spectrum situation sparsity and channel propagation characteristics, an SBL-based spectrum data hierarchical recovery algorithm is developed to estimate the missing data of unsampled locations.","Finally, the simulated 3D REM data in the campus scenario are used to verify the proposed methods as well as to compare with the state-of-the-art.","We also analyze the recovery performance and the impact of different parameters on the constructed REMs.","Numerical results demonstrate that the proposed scheme can ensure the construction accuracy and improve the computational efficiency under the low sampling rate."],"url":"http://arxiv.org/abs/2403.08323v1","category":"eess.SP"}
{"created":"2024-03-13 08:00:07","title":"DrFER: Learning Disentangled Representations for 3D Facial Expression Recognition","abstract":"Facial Expression Recognition (FER) has consistently been a focal point in the field of facial analysis. In the context of existing methodologies for 3D FER or 2D+3D FER, the extraction of expression features often gets entangled with identity information, compromising the distinctiveness of these features. To tackle this challenge, we introduce the innovative DrFER method, which brings the concept of disentangled representation learning to the field of 3D FER. DrFER employs a dual-branch framework to effectively disentangle expression information from identity information. Diverging from prior disentanglement endeavors in the 3D facial domain, we have carefully reconfigured both the loss functions and network structure to make the overall framework adaptable to point cloud data. This adaptation enhances the capability of the framework in recognizing facial expressions, even in cases involving varying head poses. Extensive evaluations conducted on the BU-3DFE and Bosphorus datasets substantiate that DrFER surpasses the performance of other 3D FER methods.","sentences":["Facial Expression Recognition (FER) has consistently been a focal point in the field of facial analysis.","In the context of existing methodologies for 3D FER or 2D+3D FER, the extraction of expression features often gets entangled with identity information, compromising the distinctiveness of these features.","To tackle this challenge, we introduce the innovative DrFER method, which brings the concept of disentangled representation learning to the field of 3D FER.","DrFER employs a dual-branch framework to effectively disentangle expression information from identity information.","Diverging from prior disentanglement endeavors in the 3D facial domain, we have carefully reconfigured both the loss functions and network structure to make the overall framework adaptable to point cloud data.","This adaptation enhances the capability of the framework in recognizing facial expressions, even in cases involving varying head poses.","Extensive evaluations conducted on the BU-3DFE and Bosphorus datasets substantiate that DrFER surpasses the performance of other 3D FER methods."],"url":"http://arxiv.org/abs/2403.08318v1","category":"cs.CV"}
{"created":"2024-03-13 07:42:21","title":"StyleDyRF: Zero-shot 4D Style Transfer for Dynamic Neural Radiance Fields","abstract":"4D style transfer aims at transferring arbitrary visual style to the synthesized novel views of a dynamic 4D scene with varying viewpoints and times. Existing efforts on 3D style transfer can effectively combine the visual features of style images and neural radiance fields (NeRF) but fail to handle the 4D dynamic scenes limited by the static scene assumption. Consequently, we aim to handle the novel challenging problem of 4D style transfer for the first time, which further requires the consistency of stylized results on dynamic objects. In this paper, we introduce StyleDyRF, a method that represents the 4D feature space by deforming a canonical feature volume and learns a linear style transformation matrix on the feature volume in a data-driven fashion. To obtain the canonical feature volume, the rays at each time step are deformed with the geometric prior of a pre-trained dynamic NeRF to render the feature map under the supervision of pre-trained visual encoders. With the content and style cues in the canonical feature volume and the style image, we can learn the style transformation matrix from their covariance matrices with lightweight neural networks. The learned style transformation matrix can reflect a direct matching of feature covariance from the content volume to the given style pattern, in analogy with the optimization of the Gram matrix in traditional 2D neural style transfer. The experimental results show that our method not only renders 4D photorealistic style transfer results in a zero-shot manner but also outperforms existing methods in terms of visual quality and consistency.","sentences":["4D style transfer aims at transferring arbitrary visual style to the synthesized novel views of a dynamic 4D scene with varying viewpoints and times.","Existing efforts on 3D style transfer can effectively combine the visual features of style images and neural radiance fields (NeRF) but fail to handle the 4D dynamic scenes limited by the static scene assumption.","Consequently, we aim to handle the novel challenging problem of 4D style transfer for the first time, which further requires the consistency of stylized results on dynamic objects.","In this paper, we introduce StyleDyRF, a method that represents the 4D feature space by deforming a canonical feature volume and learns a linear style transformation matrix on the feature volume in a data-driven fashion.","To obtain the canonical feature volume, the rays at each time step are deformed with the geometric prior of a pre-trained dynamic NeRF to render the feature map under the supervision of pre-trained visual encoders.","With the content and style cues in the canonical feature volume and the style image, we can learn the style transformation matrix from their covariance matrices with lightweight neural networks.","The learned style transformation matrix can reflect a direct matching of feature covariance from the content volume to the given style pattern, in analogy with the optimization of the Gram matrix in traditional 2D neural style transfer.","The experimental results show that our method not only renders 4D photorealistic style transfer results in a zero-shot manner but also outperforms existing methods in terms of visual quality and consistency."],"url":"http://arxiv.org/abs/2403.08310v1","category":"cs.CV"}
{"created":"2024-03-13 07:37:35","title":"An existence result for accretive growth in elastic solids","abstract":"We investigate a model for the accretive growth of an elastic solid. The reference configuration of the body is accreted in its normal direction, with space- and deformation-dependent accretion rate. The time-dependent reference configuration is identified via the level sets of the unique viscosity solution of a suitable generalized eikonal equation. After proving the global-in-time well-posedness of the quasistatic equilibrium under prescribed growth, we prove the existence of a local-in-time solution for the coupled equilibrium-growth problem, where both mechanical displacement and time-evolving set are unknown. A distinctive challenge is the limited regularity of the growing body, which calls for proving a new uniform Korn inequality.","sentences":["We investigate a model for the accretive growth of an elastic solid.","The reference configuration of the body is accreted in its normal direction, with space- and deformation-dependent accretion rate.","The time-dependent reference configuration is identified via the level sets of the unique viscosity solution of a suitable generalized eikonal equation.","After proving the global-in-time well-posedness of the quasistatic equilibrium under prescribed growth, we prove the existence of a local-in-time solution for the coupled equilibrium-growth problem, where both mechanical displacement and time-evolving set are unknown.","A distinctive challenge is the limited regularity of the growing body, which calls for proving a new uniform Korn inequality."],"url":"http://arxiv.org/abs/2403.08307v1","category":"math.AP"}
{"created":"2024-03-13 07:15:50","title":"Spin relaxation in inhomogeneous magnetic fields with depolarizing boundaries","abstract":"Field-inhomogeneity-induced relaxation of atomic spins confined in vapor cells with depolarizing walls is studied. In contrast to nuclear spins, such as noble-gas spins, which experience minimal polarization loss at cell walls, atomic spins in uncoated cells undergo randomization at the boundaries. This distinct boundary condition results in a varied dependence of the relaxation rate on the field gradient. By solving the Bloch-Torrey equation under fully depolarizing boundary conditions, we illustrate that the relaxation rate induced by field inhomogeneity is more pronounced for spins with a smaller original relaxation rate (in the absence of the inhomogeneous field). We establish an upper limit for the relaxation rate through calculations in the perturbation regime. Moreover, we connect it to the spin-exchange-relaxation-free magnetometers, demonstrating that its linewidth is most sensitive to inhomogeneous fields along the magnetometer's sensitive axis. Our theoretical result agrees with the experimental data for cells subjected to small pump power. However, deviations in larger input-power scenarios underscore the importance of considering pump field attenuation, which leads to uniformly distributed light shift that behaves as an inhomogeneous magnetic field.","sentences":["Field-inhomogeneity-induced relaxation of atomic spins confined in vapor cells with depolarizing walls is studied.","In contrast to nuclear spins, such as noble-gas spins, which experience minimal polarization loss at cell walls, atomic spins in uncoated cells undergo randomization at the boundaries.","This distinct boundary condition results in a varied dependence of the relaxation rate on the field gradient.","By solving the Bloch-Torrey equation under fully depolarizing boundary conditions, we illustrate that the relaxation rate induced by field inhomogeneity is more pronounced for spins with a smaller original relaxation rate (in the absence of the inhomogeneous field).","We establish an upper limit for the relaxation rate through calculations in the perturbation regime.","Moreover, we connect it to the spin-exchange-relaxation-free magnetometers, demonstrating that its linewidth is most sensitive to inhomogeneous fields along the magnetometer's sensitive axis.","Our theoretical result agrees with the experimental data for cells subjected to small pump power.","However, deviations in larger input-power scenarios underscore the importance of considering pump field attenuation, which leads to uniformly distributed light shift that behaves as an inhomogeneous magnetic field."],"url":"http://arxiv.org/abs/2403.08300v1","category":"quant-ph"}
{"created":"2024-03-13 07:10:24","title":"Physics-Informed Deep Learning for Motion-Corrected Reconstruction of Quantitative Brain MRI","abstract":"We propose PHIMO, a physics-informed learning-based motion correction method tailored to quantitative MRI. PHIMO leverages information from the signal evolution to exclude motion-corrupted k-space lines from a data-consistent reconstruction. We demonstrate the potential of PHIMO for the application of T2* quantification from gradient echo MRI, which is particularly sensitive to motion due to its sensitivity to magnetic field inhomogeneities. A state-of-the-art technique for motion correction requires redundant acquisition of the k-space center, prolonging the acquisition. We show that PHIMO can detect and exclude intra-scan motion events and, thus, correct for severe motion artifacts. PHIMO approaches the performance of the state-of-the-art motion correction method, while substantially reducing the acquisition time by over 40%, facilitating clinical applicability. Our code is available at https://github.com/HannahEichhorn/PHIMO.","sentences":["We propose PHIMO, a physics-informed learning-based motion correction method tailored to quantitative MRI.","PHIMO leverages information from the signal evolution to exclude motion-corrupted k-space lines from a data-consistent reconstruction.","We demonstrate the potential of PHIMO for the application of T2* quantification from gradient echo MRI, which is particularly sensitive to motion due to its sensitivity to magnetic field inhomogeneities.","A state-of-the-art technique for motion correction requires redundant acquisition of the k-space center, prolonging the acquisition.","We show that PHIMO can detect and exclude intra-scan motion events and, thus, correct for severe motion artifacts.","PHIMO approaches the performance of the state-of-the-art motion correction method, while substantially reducing the acquisition time by over 40%, facilitating clinical applicability.","Our code is available at https://github.com/HannahEichhorn/PHIMO."],"url":"http://arxiv.org/abs/2403.08298v1","category":"eess.IV"}
{"created":"2024-03-13 07:08:54","title":"Semi-Transparent Image Sensors for Eye-Tracking Applications","abstract":"Image sensors hold a pivotal role in society due to their ability to capture vast amounts of information. Traditionally, image sensors are opaque due to light absorption in both the pixels and the read-out electronics that are stacked on top of each other. Making image sensors visibly transparent would have a far-reaching impact in numerous areas such as human-computer interfaces, smart displays, and both augmented and virtual reality. In this paper, we present the development and analysis of the first semi-transparent image sensor and its applicability as an eye-tracking device. The device consists of an 8x8 array of semi-transparent photodetectors and electrodes disposed on a fully transparent substrate. Each pixel of the array has a size of 60 x 140 {\\mu}m and an optical transparency of 85-95%. Pixels have a high sensitivity, with more than 90% of them showing a noise equivalent irradiance < 10-4 W/m2 for wavelengths of 637 nm. As the semi-transparent photodetectors have a large amount of built-in gain, the opaque read-out electronics can be placed far away from the detector array to ensure maximum transparency and fill factor. Indeed, the operation and appearance of transparent image sensors present a fundamental shift in how we think about cameras and imaging, as these devices can be concealed in plain sight.","sentences":["Image sensors hold a pivotal role in society due to their ability to capture vast amounts of information.","Traditionally, image sensors are opaque due to light absorption in both the pixels and the read-out electronics that are stacked on top of each other.","Making image sensors visibly transparent would have a far-reaching impact in numerous areas such as human-computer interfaces, smart displays, and both augmented and virtual reality.","In this paper, we present the development and analysis of the first semi-transparent image sensor and its applicability as an eye-tracking device.","The device consists of an 8x8 array of semi-transparent photodetectors and electrodes disposed on a fully transparent substrate.","Each pixel of the array has a size of 60 x 140 {\\mu}m and an optical transparency of 85-95%.","Pixels have a high sensitivity, with more than 90% of them showing a noise equivalent irradiance <","10-4 W/m2 for wavelengths of 637 nm.","As the semi-transparent photodetectors have a large amount of built-in gain, the opaque read-out electronics can be placed far away from the detector array to ensure maximum transparency and fill factor.","Indeed, the operation and appearance of transparent image sensors present a fundamental shift in how we think about cameras and imaging, as these devices can be concealed in plain sight."],"url":"http://arxiv.org/abs/2403.08297v1","category":"physics.optics"}
{"created":"2024-03-13 06:34:49","title":"MGIC: A Multi-Label Gradient Inversion Attack based on Canny Edge Detection on Federated Learning","abstract":"As a new distributed computing framework that can protect data privacy, federated learning (FL) has attracted more and more attention in recent years. It receives gradients from users to train the global model and releases the trained global model to working users. Nonetheless, the gradient inversion (GI) attack reflects the risk of privacy leakage in federated learning. Attackers only need to use gradients through hundreds of thousands of simple iterations to obtain relatively accurate private data stored on users' local devices. For this, some works propose simple but effective strategies to obtain user data under a single-label dataset. However, these strategies induce a satisfactory visual effect of the inversion image at the expense of higher time costs. Due to the semantic limitation of a single label, the image obtained by gradient inversion may have semantic errors. We present a novel gradient inversion strategy based on canny edge detection (MGIC) in both the multi-label and single-label datasets. To reduce semantic errors caused by a single label, we add new convolution layers' blocks in the trained model to obtain the image's multi-label. Through multi-label representation, serious semantic errors in inversion images are reduced. Then, we analyze the impact of parameters on the difficulty of input image reconstruction and discuss how image multi-subjects affect the inversion performance. Our proposed strategy has better visual inversion image results than the most widely used ones, saving more than 78% of time costs in the ImageNet dataset.","sentences":["As a new distributed computing framework that can protect data privacy, federated learning (FL) has attracted more and more attention in recent years.","It receives gradients from users to train the global model and releases the trained global model to working users.","Nonetheless, the gradient inversion (GI) attack reflects the risk of privacy leakage in federated learning.","Attackers only need to use gradients through hundreds of thousands of simple iterations to obtain relatively accurate private data stored on users' local devices.","For this, some works propose simple but effective strategies to obtain user data under a single-label dataset.","However, these strategies induce a satisfactory visual effect of the inversion image at the expense of higher time costs.","Due to the semantic limitation of a single label, the image obtained by gradient inversion may have semantic errors.","We present a novel gradient inversion strategy based on canny edge detection (MGIC) in both the multi-label and single-label datasets.","To reduce semantic errors caused by a single label, we add new convolution layers' blocks in the trained model to obtain the image's multi-label.","Through multi-label representation, serious semantic errors in inversion images are reduced.","Then, we analyze the impact of parameters on the difficulty of input image reconstruction and discuss how image multi-subjects affect the inversion performance.","Our proposed strategy has better visual inversion image results than the most widely used ones, saving more than 78% of time costs in the ImageNet dataset."],"url":"http://arxiv.org/abs/2403.08284v1","category":"cs.CV"}
{"created":"2024-03-13 06:16:03","title":"On the conservation laws and the structure of the nonlinearity for SQG and its generalizations","abstract":"Using a new definition for the nonlinear term, we prove that all weak solutions to the SQG equation (and mSQG) conserve the angular momentum. This result is new for the weak solutions of [Resnick, '95] and rules out the possibility of anomalous dissipation of angular momentum. We also prove conservation of the Hamiltonian under conjecturally optimal assumptions, sharpening a well-known criterion of [Cheskidov-Constantin-Friedlander-Shvydkoy, '08]. Moreover, we show that our new estimate for the nonlinearity is optimal and that it characterizes the mSQG nonlinearity uniquely among active scalar nonlinearities with a scaling symmetry.","sentences":["Using a new definition for the nonlinear term, we prove that all weak solutions to the SQG equation (and mSQG) conserve the angular momentum.","This result is new for the weak solutions of [Resnick, '95] and rules out the possibility of anomalous dissipation of angular momentum.","We also prove conservation of the Hamiltonian under conjecturally optimal assumptions, sharpening a well-known criterion of [Cheskidov-Constantin-Friedlander-Shvydkoy, '08].","Moreover, we show that our new estimate for the nonlinearity is optimal and that it characterizes the mSQG nonlinearity uniquely among active scalar nonlinearities with a scaling symmetry."],"url":"http://arxiv.org/abs/2403.08279v1","category":"math.AP"}
{"created":"2024-03-13 05:13:17","title":"Make Me Happier: Evoking Emotions Through Image Diffusion Models","abstract":"Despite the rapid progress in image generation, emotional image editing remains under-explored. The semantics, context, and structure of an image can evoke emotional responses, making emotional image editing techniques valuable for various real-world applications, including treatment of psychological disorders, commercialization of products, and artistic design. For the first time, we present a novel challenge of emotion-evoked image generation, aiming to synthesize images that evoke target emotions while retaining the semantics and structures of the original scenes. To address this challenge, we propose a diffusion model capable of effectively understanding and editing source images to convey desired emotions and sentiments. Moreover, due to the lack of emotion editing datasets, we provide a unique dataset consisting of 340,000 pairs of images and their emotion annotations. Furthermore, we conduct human psychophysics experiments and introduce four new evaluation metrics to systematically benchmark all the methods. Experimental results demonstrate that our method surpasses all competitive baselines. Our diffusion model is capable of identifying emotional cues from original images, editing images that elicit desired emotions, and meanwhile, preserving the semantic structure of the original images. All code, model, and data will be made public.","sentences":["Despite the rapid progress in image generation, emotional image editing remains under-explored.","The semantics, context, and structure of an image can evoke emotional responses, making emotional image editing techniques valuable for various real-world applications, including treatment of psychological disorders, commercialization of products, and artistic design.","For the first time, we present a novel challenge of emotion-evoked image generation, aiming to synthesize images that evoke target emotions while retaining the semantics and structures of the original scenes.","To address this challenge, we propose a diffusion model capable of effectively understanding and editing source images to convey desired emotions and sentiments.","Moreover, due to the lack of emotion editing datasets, we provide a unique dataset consisting of 340,000 pairs of images and their emotion annotations.","Furthermore, we conduct human psychophysics experiments and introduce four new evaluation metrics to systematically benchmark all the methods.","Experimental results demonstrate that our method surpasses all competitive baselines.","Our diffusion model is capable of identifying emotional cues from original images, editing images that elicit desired emotions, and meanwhile, preserving the semantic structure of the original images.","All code, model, and data will be made public."],"url":"http://arxiv.org/abs/2403.08255v1","category":"cs.CV"}
{"created":"2024-03-13 05:11:24","title":"Machine Unlearning: Taxonomy, Metrics, Applications, Challenges, and Prospects","abstract":"Personal digital data is a critical asset, and governments worldwide have enforced laws and regulations to protect data privacy. Data users have been endowed with the right to be forgotten of their data. In the course of machine learning (ML), the forgotten right requires a model provider to delete user data and its subsequent impact on ML models upon user requests. Machine unlearning emerges to address this, which has garnered ever-increasing attention from both industry and academia. While the area has developed rapidly, there is a lack of comprehensive surveys to capture the latest advancements. Recognizing this shortage, we conduct an extensive exploration to map the landscape of machine unlearning including the (fine-grained) taxonomy of unlearning algorithms under centralized and distributed settings, debate on approximate unlearning, verification and evaluation metrics, challenges and solutions for unlearning under different applications, as well as attacks targeting machine unlearning. The survey concludes by outlining potential directions for future research, hoping to serve as a guide for interested scholars.","sentences":["Personal digital data is a critical asset, and governments worldwide have enforced laws and regulations to protect data privacy.","Data users have been endowed with the right to be forgotten of their data.","In the course of machine learning (ML), the forgotten right requires a model provider to delete user data and its subsequent impact on ML models upon user requests.","Machine unlearning emerges to address this, which has garnered ever-increasing attention from both industry and academia.","While the area has developed rapidly, there is a lack of comprehensive surveys to capture the latest advancements.","Recognizing this shortage, we conduct an extensive exploration to map the landscape of machine unlearning including the (fine-grained) taxonomy of unlearning algorithms under centralized and distributed settings, debate on approximate unlearning, verification and evaluation metrics, challenges and solutions for unlearning under different applications, as well as attacks targeting machine unlearning.","The survey concludes by outlining potential directions for future research, hoping to serve as a guide for interested scholars."],"url":"http://arxiv.org/abs/2403.08254v1","category":"cs.LG"}
{"created":"2024-03-13 05:08:47","title":"PNeSM: Arbitrary 3D Scene Stylization via Prompt-Based Neural Style Mapping","abstract":"3D scene stylization refers to transform the appearance of a 3D scene to match a given style image, ensuring that images rendered from different viewpoints exhibit the same style as the given style image, while maintaining the 3D consistency of the stylized scene. Several existing methods have obtained impressive results in stylizing 3D scenes. However, the models proposed by these methods need to be re-trained when applied to a new scene. In other words, their models are coupled with a specific scene and cannot adapt to arbitrary other scenes. To address this issue, we propose a novel 3D scene stylization framework to transfer an arbitrary style to an arbitrary scene, without any style-related or scene-related re-training. Concretely, we first map the appearance of the 3D scene into a 2D style pattern space, which realizes complete disentanglement of the geometry and appearance of the 3D scene and makes our model be generalized to arbitrary 3D scenes. Then we stylize the appearance of the 3D scene in the 2D style pattern space via a prompt-based 2D stylization algorithm. Experimental results demonstrate that our proposed framework is superior to SOTA methods in both visual quality and generalization.","sentences":["3D scene stylization refers to transform the appearance of a 3D scene to match a given style image, ensuring that images rendered from different viewpoints exhibit the same style as the given style image, while maintaining the 3D consistency of the stylized scene.","Several existing methods have obtained impressive results in stylizing 3D scenes.","However, the models proposed by these methods need to be re-trained when applied to a new scene.","In other words, their models are coupled with a specific scene and cannot adapt to arbitrary other scenes.","To address this issue, we propose a novel 3D scene stylization framework to transfer an arbitrary style to an arbitrary scene, without any style-related or scene-related re-training.","Concretely, we first map the appearance of the 3D scene into a 2D style pattern space, which realizes complete disentanglement of the geometry and appearance of the 3D scene and makes our model be generalized to arbitrary 3D scenes.","Then we stylize the appearance of the 3D scene in the 2D style pattern space via a prompt-based 2D stylization algorithm.","Experimental results demonstrate that our proposed framework is superior to SOTA methods in both visual quality and generalization."],"url":"http://arxiv.org/abs/2403.08252v1","category":"cs.CV"}
{"created":"2024-03-13 05:00:23","title":"Scattered Mixture-of-Experts Implementation","abstract":"We present ScatterMoE, an implementation of Sparse Mixture-of-Experts (SMoE) on GPUs. ScatterMoE builds upon existing implementations, and overcoming some of the limitations to improve inference and training speed, and memory footprint. This implementation achieves this by avoiding padding and making excessive copies of the input. We introduce ParallelLinear, the main component we use to build our implementation and the various kernels used to speed up the operation. We benchmark our implementation against Megablocks, and show that it enables a higher throughput and lower memory footprint. We also show how ParallelLinear enables extension of the Mixture-of-Experts concept by demonstrating with an implementation of Mixture of Attention.","sentences":["We present ScatterMoE, an implementation of Sparse Mixture-of-Experts (SMoE) on GPUs.","ScatterMoE builds upon existing implementations, and overcoming some of the limitations to improve inference and training speed, and memory footprint.","This implementation achieves this by avoiding padding and making excessive copies of the input.","We introduce ParallelLinear, the main component we use to build our implementation and the various kernels used to speed up the operation.","We benchmark our implementation against Megablocks, and show that it enables a higher throughput and lower memory footprint.","We also show how ParallelLinear enables extension of the Mixture-of-Experts concept by demonstrating with an implementation of Mixture of Attention."],"url":"http://arxiv.org/abs/2403.08245v1","category":"cs.LG"}
{"created":"2024-03-13 04:14:33","title":"Boosting Disfluency Detection with Large Language Model as Disfluency Generator","abstract":"Current disfluency detection methods heavily rely on costly and scarce human-annotated data. To tackle this issue, some approaches employ heuristic or statistical features to generate disfluent sentences, partially improving detection performance. However, these sentences often deviate from real-life scenarios, constraining overall model enhancement. In this study, we propose a lightweight data augmentation approach for disfluency detection, utilizing the superior generative and semantic understanding capabilities of large language model (LLM) to generate disfluent sentences as augmentation data. We leverage LLM to generate diverse and more realistic sentences guided by specific prompts, without the need for fine-tuning the LLM. Subsequently, we apply an uncertainty-aware data filtering approach to improve the quality of the generated sentences, utilized in training a small detection model for improved performance. Experiments using enhanced data yielded state-of-the-art results. The results showed that using a small amount of LLM-generated enhanced data can significantly improve performance, thereby further enhancing cost-effectiveness.","sentences":["Current disfluency detection methods heavily rely on costly and scarce human-annotated data.","To tackle this issue, some approaches employ heuristic or statistical features to generate disfluent sentences, partially improving detection performance.","However, these sentences often deviate from real-life scenarios, constraining overall model enhancement.","In this study, we propose a lightweight data augmentation approach for disfluency detection, utilizing the superior generative and semantic understanding capabilities of large language model (LLM) to generate disfluent sentences as augmentation data.","We leverage LLM to generate diverse and more realistic sentences guided by specific prompts, without the need for fine-tuning the LLM.","Subsequently, we apply an uncertainty-aware data filtering approach to improve the quality of the generated sentences, utilized in training a small detection model for improved performance.","Experiments using enhanced data yielded state-of-the-art results.","The results showed that using a small amount of LLM-generated enhanced data can significantly improve performance, thereby further enhancing cost-effectiveness."],"url":"http://arxiv.org/abs/2403.08229v1","category":"cs.CL"}
{"created":"2024-03-13 04:11:38","title":"Matching Non-Identical Objects","abstract":"Not identical but similar objects are everywhere in the world. Examples include four-legged animals such as dogs and cats, cars of different models, akin flowers in various colors, and countless others. In this study, we address a novel task of matching such non-identical objects. We propose a simple weighting scheme of descriptors that enhance various sparse image matching methods, which are originally designed for matching identical objects captured from different perspectives, and achieve semantically robust matching. The experiments show successful matching between non-identical objects in various cases including domain shift. Further, we present a first evaluation of the robustness of the image matching methods under common corruptions, which is a sort of domain shift, and the proposed method improves the matching in this case as well.","sentences":["Not identical but similar objects are everywhere in the world.","Examples include four-legged animals such as dogs and cats, cars of different models, akin flowers in various colors, and countless others.","In this study, we address a novel task of matching such non-identical objects.","We propose a simple weighting scheme of descriptors that enhance various sparse image matching methods, which are originally designed for matching identical objects captured from different perspectives, and achieve semantically robust matching.","The experiments show successful matching between non-identical objects in various cases including domain shift.","Further, we present a first evaluation of the robustness of the image matching methods under common corruptions, which is a sort of domain shift, and the proposed method improves the matching in this case as well."],"url":"http://arxiv.org/abs/2403.08227v1","category":"cs.CV"}
{"created":"2024-03-13 03:28:39","title":"PaddingFlow: Improving Normalizing Flows with Padding-Dimensional Noise","abstract":"Normalizing flow is a generative modeling approach with efficient sampling. However, Flow-based models suffer two issues, which are manifold and discrete data. If the target distribution is a manifold, which means the dimension of the latent target distribution and the dimension of the data distribution are unmatched, flow-based models might perform badly. Discrete data makes flow-based models collapse into a degenerate mixture of point masses. In this paper, to sidestep such two issues we propose PaddingFlow, a novel dequantization method, which improves normalizing flows with padding-dimensional noise. PaddingFlow is easy to implement, computationally cheap, widely suitable for various tasks, and generates samples that are unbiased estimations of the data. Especially, our method can overcome the limitation of existing dequantization methods that have to change the data distribution, which might degrade performance. We validate our method on the main benchmarks of unconditional density estimation, including five tabular datasets and four image datasets for VAE models, and the IK experiments which are conditional density estimation. The results show that PaddingFlow can provide improvement on all tasks in this paper.","sentences":["Normalizing flow is a generative modeling approach with efficient sampling.","However, Flow-based models suffer two issues, which are manifold and discrete data.","If the target distribution is a manifold, which means the dimension of the latent target distribution and the dimension of the data distribution are unmatched, flow-based models might perform badly.","Discrete data makes flow-based models collapse into a degenerate mixture of point masses.","In this paper, to sidestep such two issues we propose PaddingFlow, a novel dequantization method, which improves normalizing flows with padding-dimensional noise.","PaddingFlow is easy to implement, computationally cheap, widely suitable for various tasks, and generates samples that are unbiased estimations of the data.","Especially, our method can overcome the limitation of existing dequantization methods that have to change the data distribution, which might degrade performance.","We validate our method on the main benchmarks of unconditional density estimation, including five tabular datasets and four image datasets for VAE models, and the IK experiments which are conditional density estimation.","The results show that PaddingFlow can provide improvement on all tasks in this paper."],"url":"http://arxiv.org/abs/2403.08216v1","category":"cs.LG"}
{"created":"2024-03-13 03:22:02","title":"Can Large Language Models Identify Authorship?","abstract":"The ability to accurately identify authorship is crucial for verifying content authenticity and mitigating misinformation. Large Language Models (LLMs) have demonstrated exceptional capacity for reasoning and problem-solving. However, their potential in authorship analysis, encompassing authorship verification and attribution, remains underexplored. This paper conducts a comprehensive evaluation of LLMs in these critical tasks. Traditional studies have depended on hand-crafted stylistic features, whereas state-of-the-art approaches leverage text embeddings from pre-trained language models. These methods, which typically require fine-tuning on labeled data, often suffer from performance degradation in cross-domain applications and provide limited explainability. This work seeks to address three research questions: (1) Can LLMs perform zero-shot, end-to-end authorship verification effectively? (2) Are LLMs capable of accurately attributing authorship among multiple candidates authors (e.g., 10 and 20)? (3) How can LLMs provide explainability in authorship analysis, particularly through the role of linguistic features? Moreover, we investigate the integration of explicit linguistic features to guide LLMs in their reasoning processes. Our extensive assessment demonstrates LLMs' proficiency in both tasks without the need for domain-specific fine-tuning, providing insights into their decision-making via a detailed analysis of linguistic features. This establishes a new benchmark for future research on LLM-based authorship analysis. The code and data are available at https://github.com/baixianghuang/authorship-llm.","sentences":["The ability to accurately identify authorship is crucial for verifying content authenticity and mitigating misinformation.","Large Language Models (LLMs) have demonstrated exceptional capacity for reasoning and problem-solving.","However, their potential in authorship analysis, encompassing authorship verification and attribution, remains underexplored.","This paper conducts a comprehensive evaluation of LLMs in these critical tasks.","Traditional studies have depended on hand-crafted stylistic features, whereas state-of-the-art approaches leverage text embeddings from pre-trained language models.","These methods, which typically require fine-tuning on labeled data, often suffer from performance degradation in cross-domain applications and provide limited explainability.","This work seeks to address three research questions: (1) Can LLMs perform zero-shot, end-to-end authorship verification effectively?","(2) Are LLMs capable of accurately attributing authorship among multiple candidates authors (e.g., 10 and 20)?","(3) How can LLMs provide explainability in authorship analysis, particularly through the role of linguistic features?","Moreover, we investigate the integration of explicit linguistic features to guide LLMs in their reasoning processes.","Our extensive assessment demonstrates LLMs' proficiency in both tasks without the need for domain-specific fine-tuning, providing insights into their decision-making via a detailed analysis of linguistic features.","This establishes a new benchmark for future research on LLM-based authorship analysis.","The code and data are available at https://github.com/baixianghuang/authorship-llm."],"url":"http://arxiv.org/abs/2403.08213v1","category":"cs.CL"}
{"created":"2024-03-13 03:11:50","title":"Height-bounded Lempel-Ziv encodings","abstract":"We introduce height-bounded LZ encodings (LZHB), a new family of compressed representations that is a variant of Lempel-Ziv parsings with a focus on allowing fast access to arbitrary positions of the text directly via the compressed representation. Any LZHB encoding whose referencing height is bounded by $h$ allows access to an arbitrary position of the underlying text using $O(h)$ predecessor queries. We show that there exists a constant $c$ such that the size $\\hat{z}_{\\mathit{HB}(c\\log n)}$ of the optimal (smallest) LZHB encoding whose height is bounded by $c\\log n$ for any string of length $n$ is $O(\\hat{g}_{\\mathrm{rl}})$, where $\\hat{g}_{\\mathrm{rl}}$ is the size of the smallest run-length grammar. Furthermore, we show that there exists a family of strings such that $\\hat{z}_{\\mathit{HB}(c\\log n)} = o(\\hat{g}_{\\mathrm{rl}})$, thus making $\\hat{z}_{\\mathit{HB}(c\\log n)}$ one of the smallest known repetitiveness measures for which $O(\\mathit{polylog} n)$ time access is possible using $O(\\hat{z}_{\\mathit{HB}(c\\log n)})$ space. While computing the optimal LZHB representation for any given height seems difficult, we propose linear and near linear time greedy algorithms which we show experimentally can efficiently find small LZHB representations in practice.","sentences":["We introduce height-bounded LZ encodings (LZHB), a new family of compressed representations that is a variant of Lempel-Ziv parsings with a focus on allowing fast access to arbitrary positions of the text directly via the compressed representation.","Any LZHB encoding whose referencing height is bounded by $h$ allows access to an arbitrary position of the underlying text using $O(h)$ predecessor queries.","We show that there exists a constant $c$ such that the size $\\hat{z}_{\\mathit{HB}(c\\log n)}$ of the optimal (smallest) LZHB encoding whose height is bounded by $c\\log n$ for any string of length $n$ is $O(\\hat{g}_{\\mathrm{rl}})$, where $\\hat{g}_{\\mathrm{rl}}$ is the size of the smallest run-length grammar.","Furthermore, we show that there exists a family of strings such that $\\hat{z}_{\\mathit{HB}(c\\log n)} = o(\\hat{g}_{\\mathrm{rl}})$, thus making $\\hat{z}_{\\mathit{HB}(c\\log n)}$ one of the smallest known repetitiveness measures for which $O(\\mathit{polylog} n)$ time access is possible using $O(\\hat{z}_{\\mathit{HB}(c\\log n)})$ space.","While computing the optimal LZHB representation for any given height seems difficult, we propose linear and near linear time greedy algorithms which we show experimentally can efficiently find small LZHB representations in practice."],"url":"http://arxiv.org/abs/2403.08209v1","category":"cs.DS"}
{"created":"2024-03-13 02:55:10","title":"Trading Large Orders in the Presence of Multiple High-Frequency Anticipatory Traders","abstract":"We investigate a market with a normal-speed informed trader (IT) who may employ mixed strategy and multiple anticipatory high-frequency traders (HFTs) who are under different inventory pressures, in a three-period Kyle's model. The pure- and mixed-strategy equilibria are considered and the results provide recommendations for IT's randomization strategy with different numbers of HFTs. Some surprising results about investors' profits arise: the improvement of anticipatory traders' speed or a more precise prediction may harm themselves but help IT.","sentences":["We investigate a market with a normal-speed informed trader (IT) who may employ mixed strategy and multiple anticipatory high-frequency traders (HFTs) who are under different inventory pressures, in a three-period Kyle's model.","The pure- and mixed-strategy equilibria are considered and the results provide recommendations for IT's randomization strategy with different numbers of HFTs.","Some surprising results about investors' profits arise: the improvement of anticipatory traders' speed or a more precise prediction may harm themselves but help IT."],"url":"http://arxiv.org/abs/2403.08202v1","category":"q-fin.TR"}
{"created":"2024-03-13 02:46:17","title":"Validating and Exploring Large Geographic Corpora","abstract":"This paper investigates the impact of corpus creation decisions on large multi-lingual geographic web corpora. Beginning with a 427 billion word corpus derived from the Common Crawl, three methods are used to improve the quality of sub-corpora representing specific language-country pairs like New Zealand English: (i) the agreement of independent language identification systems, (ii) hash-based deduplication, and (iii) location-specific outlier detection. The impact of each of these steps is then evaluated at the language level and the country level by using corpus similarity measures to compare each resulting corpus with baseline data sets. The goal is to understand the impact of upstream data cleaning decisions on downstream corpora with a specific focus on under-represented languages and populations. The evaluation shows that the validity of sub-corpora is improved with each stage of cleaning but that this improvement is unevenly distributed across languages and populations. This result shows how standard corpus creation techniques can accidentally exclude under-represented populations.","sentences":["This paper investigates the impact of corpus creation decisions on large multi-lingual geographic web corpora.","Beginning with a 427 billion word corpus derived from the Common Crawl, three methods are used to improve the quality of sub-corpora representing specific language-country pairs like New Zealand English: (i) the agreement of independent language identification systems, (ii) hash-based deduplication, and (iii) location-specific outlier detection.","The impact of each of these steps is then evaluated at the language level and the country level by using corpus similarity measures to compare each resulting corpus with baseline data sets.","The goal is to understand the impact of upstream data cleaning decisions on downstream corpora with a specific focus on under-represented languages and populations.","The evaluation shows that the validity of sub-corpora is improved with each stage of cleaning but that this improvement is unevenly distributed across languages and populations.","This result shows how standard corpus creation techniques can accidentally exclude under-represented populations."],"url":"http://arxiv.org/abs/2403.08198v1","category":"cs.CL"}
{"created":"2024-03-13 02:33:57","title":"Unsupervised Learning of Hybrid Latent Dynamics: A Learn-to-Identify Framework","abstract":"Modern applications increasingly require unsupervised learning of latent dynamics from high-dimensional time-series. This presents a significant challenge of identifiability: many abstract latent representations may reconstruct observations, yet do they guarantee an adequate identification of the governing dynamics? This paper investigates this challenge from two angles: the use of physics inductive bias specific to the data being modeled, and a learn-to-identify strategy that separates forecasting objectives from the data used for the identification. We combine these two strategies in a novel framework for unsupervised meta-learning of hybrid latent dynamics (Meta-HyLaD) with: 1) a latent dynamic function that hybridize known mathematical expressions of prior physics with neural functions describing its unknown errors, and 2) a meta-learning formulation to learn to separately identify both components of the hybrid dynamics. Through extensive experiments on five physics and one biomedical systems, we provide strong evidence for the benefits of Meta-HyLaD to integrate rich prior knowledge while identifying their gap to observed data.","sentences":["Modern applications increasingly require unsupervised learning of latent dynamics from high-dimensional time-series.","This presents a significant challenge of identifiability: many abstract latent representations may reconstruct observations, yet do they guarantee an adequate identification of the governing dynamics?","This paper investigates this challenge from two angles: the use of physics inductive bias specific to the data being modeled, and a learn-to-identify strategy that separates forecasting objectives from the data used for the identification.","We combine these two strategies in a novel framework for unsupervised meta-learning of hybrid latent dynamics (Meta-HyLaD) with: 1) a latent dynamic function that hybridize known mathematical expressions of prior physics with neural functions describing its unknown errors, and 2) a meta-learning formulation to learn to separately identify both components of the hybrid dynamics.","Through extensive experiments on five physics and one biomedical systems, we provide strong evidence for the benefits of Meta-HyLaD to integrate rich prior knowledge while identifying their gap to observed data."],"url":"http://arxiv.org/abs/2403.08194v1","category":"cs.LG"}
{"created":"2024-03-13 02:18:33","title":"Perceive With Confidence: Statistical Safety Assurances for Navigation with Learning-Based Perception","abstract":"Rapid advances in perception have enabled large pre-trained models to be used out of the box for processing high-dimensional, noisy, and partial observations of the world into rich geometric representations (e.g., occupancy predictions). However, safe integration of these models onto robots remains challenging due to a lack of reliable performance in unfamiliar environments. In this work, we present a framework for rigorously quantifying the uncertainty of pre-trained perception models for occupancy prediction in order to provide end-to-end statistical safety assurances for navigation. We build on techniques from conformal prediction for producing a calibrated perception system that lightly processes the outputs of a pre-trained model while ensuring generalization to novel environments and robustness to distribution shifts in states when perceptual outputs are used in conjunction with a planner. The calibrated system can be used in combination with any safe planner to provide an end-to-end statistical assurance on safety in a new environment with a user-specified threshold $1-\\epsilon$. We evaluate the resulting approach - which we refer to as Perceive with Confidence (PwC) - with experiments in simulation and on hardware where a quadruped robot navigates through indoor environments containing objects unseen during training or calibration. These experiments validate the safety assurances provided by PwC and demonstrate significant improvements in empirical safety rates compared to baselines.","sentences":["Rapid advances in perception have enabled large pre-trained models to be used out of the box for processing high-dimensional, noisy, and partial observations of the world into rich geometric representations (e.g., occupancy predictions).","However, safe integration of these models onto robots remains challenging due to a lack of reliable performance in unfamiliar environments.","In this work, we present a framework for rigorously quantifying the uncertainty of pre-trained perception models for occupancy prediction in order to provide end-to-end statistical safety assurances for navigation.","We build on techniques from conformal prediction for producing a calibrated perception system that lightly processes the outputs of a pre-trained model while ensuring generalization to novel environments and robustness to distribution shifts in states when perceptual outputs are used in conjunction with a planner.","The calibrated system can be used in combination with any safe planner to provide an end-to-end statistical assurance on safety in a new environment with a user-specified threshold $1-\\epsilon$. We evaluate the resulting approach - which we refer to as Perceive with Confidence (PwC) - with experiments in simulation and on hardware where a quadruped robot navigates through indoor environments containing objects unseen during training or calibration.","These experiments validate the safety assurances provided by PwC and demonstrate significant improvements in empirical safety rates compared to baselines."],"url":"http://arxiv.org/abs/2403.08185v1","category":"cs.RO"}
{"created":"2024-03-13 02:12:49","title":"Quantum skyrmion dynamics studied by neural network quantum states","abstract":"We study the dynamics of quantum skyrmions under a magnetic field gradient using neural network quantum states. First, we obtain a quantum skyrmion lattice ground state using variational Monte Carlo with a restricted Boltzmann machine as the variational ansatz for a quantum Heisenberg model with Dzyaloshinskii-Moriya interaction. Then, using the time-dependent variational principle, we study the real-time evolution of quantum skyrmions after a Hamiltonian quench with an inhomogeneous external magnetic field. We show that field gradients are an effective way of manipulating and moving quantum skyrmions. Furthermore, we demonstrate that quantum skyrmions can decay when interacting with each other. This work shows that neural network quantum states offer a promising way of studying the real-time evolution of quantum magnetic systems that are outside the realm of exact diagonalization.","sentences":["We study the dynamics of quantum skyrmions under a magnetic field gradient using neural network quantum states.","First, we obtain a quantum skyrmion lattice ground state using variational Monte Carlo with a restricted Boltzmann machine as the variational ansatz for a quantum Heisenberg model with Dzyaloshinskii-Moriya interaction.","Then, using the time-dependent variational principle, we study the real-time evolution of quantum skyrmions after a Hamiltonian quench with an inhomogeneous external magnetic field.","We show that field gradients are an effective way of manipulating and moving quantum skyrmions.","Furthermore, we demonstrate that quantum skyrmions can decay when interacting with each other.","This work shows that neural network quantum states offer a promising way of studying the real-time evolution of quantum magnetic systems that are outside the realm of exact diagonalization."],"url":"http://arxiv.org/abs/2403.08184v1","category":"cond-mat.dis-nn"}
{"created":"2024-03-13 02:12:03","title":"Causal Interpretation of Estimands Defined by Exposure Mappings","abstract":"In settings with interference, it is common to utilize estimands defined by exposure mappings to summarize the impact of variation in treatment assignments local to the ego. This paper studies their causal interpretation under weak restrictions on interference. We demonstrate that the estimands can exhibit unpalatable sign reversals under conventional identification conditions. This motivates the formulation of sign preservation criteria for causal interpretability. To satisfy preferred criteria, it is necessary to impose restrictions on interference, either in potential outcomes or selection into treatment. We provide sufficient conditions and show that they are satisfied by a nonparametric model allowing for a complex form of interference in both the outcome and selection stages.","sentences":["In settings with interference, it is common to utilize estimands defined by exposure mappings to summarize the impact of variation in treatment assignments local to the ego.","This paper studies their causal interpretation under weak restrictions on interference.","We demonstrate that the estimands can exhibit unpalatable sign reversals under conventional identification conditions.","This motivates the formulation of sign preservation criteria for causal interpretability.","To satisfy preferred criteria, it is necessary to impose restrictions on interference, either in potential outcomes or selection into treatment.","We provide sufficient conditions and show that they are satisfied by a nonparametric model allowing for a complex form of interference in both the outcome and selection stages."],"url":"http://arxiv.org/abs/2403.08183v1","category":"econ.EM"}
{"created":"2024-03-13 02:10:46","title":"Differential Privacy in Nonlinear Dynamical Systems with Tracking Performance Guarantees","abstract":"We introduce a novel approach to make the tracking error of a class of nonlinear systems differentially private in addition to guaranteeing the tracking error performance. We use funnel control to make the tracking error evolve within a performance funnel that is pre-specified by the user. We make the performance funnel differentially private by adding a bounded continuous noise generated from an Ornstein-Uhlenbeck-type process. Since the funnel controller is a function of the performance funnel, the noise adds randomized perturbation to the control input. We show that, as a consequence of the differential privacy of the performance funnel, the tracking error is also differentially private. As a result, the tracking error is bounded by the noisy funnel boundary while maintaining privacy. We show a simulation result to demonstrate the framework.","sentences":["We introduce a novel approach to make the tracking error of a class of nonlinear systems differentially private in addition to guaranteeing the tracking error performance.","We use funnel control to make the tracking error evolve within a performance funnel that is pre-specified by the user.","We make the performance funnel differentially private by adding a bounded continuous noise generated from an Ornstein-Uhlenbeck-type process.","Since the funnel controller is a function of the performance funnel, the noise adds randomized perturbation to the control input.","We show that, as a consequence of the differential privacy of the performance funnel, the tracking error is also differentially private.","As a result, the tracking error is bounded by the noisy funnel boundary while maintaining privacy.","We show a simulation result to demonstrate the framework."],"url":"http://arxiv.org/abs/2403.08181v1","category":"cs.SY"}
{"created":"2024-03-13 01:44:57","title":"Globalized distributionally robust optimization with multi core sets","abstract":"It is essential to capture the true probability distribution of uncertain data in the distributionally robust optimization (DRO). The uncertain data presents multimodality in numerous application scenarios, in the sense that the probability density function of the uncertain data has two or more modes (local maximums). In this paper, we propose a globalized distributionally robust optimization framework with multiple core sets (MGDRO) to handle the multimodal data. This framework captures the multimodal structure via a penalty function composed of the minimum distances from the random vector to all core sets. Under some assumptions, the MGDRO model can be reformulated as tractable semi-definite programs for both moment-based and metric-based ambiguity sets. We applied the MGDRO models to a multi-product newswendor problem with multimodal demands. The numerical results turn out that the MGDRO models outperform traditional DRO models and other multimodal models greatly.","sentences":["It is essential to capture the true probability distribution of uncertain data in the distributionally robust optimization (DRO).","The uncertain data presents multimodality in numerous application scenarios, in the sense that the probability density function of the uncertain data has two or more modes (local maximums).","In this paper, we propose a globalized distributionally robust optimization framework with multiple core sets (MGDRO) to handle the multimodal data.","This framework captures the multimodal structure via a penalty function composed of the minimum distances from the random vector to all core sets.","Under some assumptions, the MGDRO model can be reformulated as tractable semi-definite programs for both moment-based and metric-based ambiguity sets.","We applied the MGDRO models to a multi-product newswendor problem with multimodal demands.","The numerical results turn out that the MGDRO models outperform traditional DRO models and other multimodal models greatly."],"url":"http://arxiv.org/abs/2403.08169v1","category":"math.OC"}
{"created":"2024-03-13 00:08:26","title":"Ultra-long relaxation of a Kramers qubit formed in a bilayer graphene quantum dot","abstract":"The intrinsic valley degree of freedom makes bilayer graphene a unique platform for emerging types of semiconducting qubits. The single-carrier quantum dot ground state exhibits a two-fold degeneracy where the two states have opposite spin and valley quantum numbers. By breaking the time-reversal symmetry of this ground state with an out-of-plane magnetic field, a novel type of qubit (Kramers qubit), encoded in the two-dimensional spin-valley subspace, becomes accessible. The Kramers qubit is robust against known spin- and valley-mixing mechanisms, as it requires a simultaneous change of both quantum numbers, potentially resulting in long relaxation and coherence times. We measure the relaxation time of a single carrier in the excited states of a bilayer graphene quantum dot at small ($\\sim \\mathrm{mT}$) and zero magnetic fields. We demonstrate ultra-long spin-valley relaxation times of the Kramers qubit exceeding $30~\\mathrm{s}$, which is about two orders of magnitude longer than the spin relaxation time of $400~\\mathrm{ms}$. The demonstrated high-fidelity single-shot readout and long relaxation times are the foundation for novel, long-lived semiconductor qubits.","sentences":["The intrinsic valley degree of freedom makes bilayer graphene a unique platform for emerging types of semiconducting qubits.","The single-carrier quantum dot ground state exhibits a two-fold degeneracy where the two states have opposite spin and valley quantum numbers.","By breaking the time-reversal symmetry of this ground state with an out-of-plane magnetic field, a novel type of qubit (Kramers qubit), encoded in the two-dimensional spin-valley subspace, becomes accessible.","The Kramers qubit is robust against known spin- and valley-mixing mechanisms, as it requires a simultaneous change of both quantum numbers, potentially resulting in long relaxation and coherence times.","We measure the relaxation time of a single carrier in the excited states of a bilayer graphene quantum dot at small ($\\sim \\mathrm{mT}$) and zero magnetic fields.","We demonstrate ultra-long spin-valley relaxation times of the Kramers qubit exceeding $30~\\mathrm{s}$, which is about two orders of magnitude longer than the spin relaxation time of $400~\\mathrm{ms}$. The demonstrated high-fidelity single-shot readout and long relaxation times are the foundation for novel, long-lived semiconductor qubits."],"url":"http://arxiv.org/abs/2403.08143v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-13 00:04:07","title":"ShadowRemovalNet: Efficient Real-Time Shadow Removal","abstract":"Shadows significantly impact computer vision tasks, particularly in outdoor environments. State-of-the-art shadow removal methods are typically too computationally intensive for real-time image processing on edge hardware. We propose ShadowRemovalNet, a novel method designed for real-time image processing on resource-constrained hardware. ShadowRemovalNet achieves significantly higher frame rates compared to existing methods, making it suitable for real-time computer vision pipelines like those used in field robotics. Beyond speed, ShadowRemovalNet offers advantages in efficiency and simplicity, as it does not require a separate shadow mask during inference. ShadowRemovalNet also addresses challenges associated with Generative Adversarial Networks (GANs) for shadow removal, including artefacts, inaccurate mask estimations, and inconsistent supervision between shadow and boundary pixels. To address these limitations, we introduce a novel loss function that substantially reduces shadow removal errors. ShadowRemovalNet's efficiency and straightforwardness make it a robust and effective solution for real-time shadow removal in outdoor robotics and edge computing applications.","sentences":["Shadows significantly impact computer vision tasks, particularly in outdoor environments.","State-of-the-art shadow removal methods are typically too computationally intensive for real-time image processing on edge hardware.","We propose ShadowRemovalNet, a novel method designed for real-time image processing on resource-constrained hardware.","ShadowRemovalNet achieves significantly higher frame rates compared to existing methods, making it suitable for real-time computer vision pipelines like those used in field robotics.","Beyond speed, ShadowRemovalNet offers advantages in efficiency and simplicity, as it does not require a separate shadow mask during inference.","ShadowRemovalNet also addresses challenges associated with Generative Adversarial Networks (GANs) for shadow removal, including artefacts, inaccurate mask estimations, and inconsistent supervision between shadow and boundary pixels.","To address these limitations, we introduce a novel loss function that substantially reduces shadow removal errors.","ShadowRemovalNet's efficiency and straightforwardness make it a robust and effective solution for real-time shadow removal in outdoor robotics and edge computing applications."],"url":"http://arxiv.org/abs/2403.08142v1","category":"cs.CV"}
{"created":"2024-03-12 23:39:54","title":"Information Leakage through Physical Layer Supply Voltage Coupling Vulnerability","abstract":"Side-channel attacks exploit variations in non-functional behaviors to expose sensitive information across security boundaries. Existing methods leverage side-channels based on power consumption, electromagnetic radiation, silicon substrate coupling, and channels created by malicious implants. Power-based side-channel attacks are widely known for extracting information from data processed within a device while assuming that an attacker has physical access or the ability to modify the device. In this paper, we introduce a novel side-channel vulnerability that leaks data-dependent power variations through physical layer supply voltage coupling (PSVC). Unlike traditional power side-channel attacks, the proposed vulnerability allows an adversary to mount an attack and extract information without modifying the device. We assess the effectiveness of PSVC vulnerability through three case studies, demonstrating several end-to-end attacks on general-purpose microcontrollers with varying adversary capabilities. These case studies provide evidence for the existence of PSVC vulnerability, its applicability for on-chip as well as on-board side-channel attacks, and how it can eliminate the need for physical access to the target device, making it applicable to any off-the-shelf hardware. Our experiments also reveal that designing devices to operate at the lowest operational voltage significantly reduces the risk of PSVC side-channel vulnerability.","sentences":["Side-channel attacks exploit variations in non-functional behaviors to expose sensitive information across security boundaries.","Existing methods leverage side-channels based on power consumption, electromagnetic radiation, silicon substrate coupling, and channels created by malicious implants.","Power-based side-channel attacks are widely known for extracting information from data processed within a device while assuming that an attacker has physical access or the ability to modify the device.","In this paper, we introduce a novel side-channel vulnerability that leaks data-dependent power variations through physical layer supply voltage coupling (PSVC).","Unlike traditional power side-channel attacks, the proposed vulnerability allows an adversary to mount an attack and extract information without modifying the device.","We assess the effectiveness of PSVC vulnerability through three case studies, demonstrating several end-to-end attacks on general-purpose microcontrollers with varying adversary capabilities.","These case studies provide evidence for the existence of PSVC vulnerability, its applicability for on-chip as well as on-board side-channel attacks, and how it can eliminate the need for physical access to the target device, making it applicable to any off-the-shelf hardware.","Our experiments also reveal that designing devices to operate at the lowest operational voltage significantly reduces the risk of PSVC side-channel vulnerability."],"url":"http://arxiv.org/abs/2403.08132v1","category":"cs.CR"}
{"created":"2024-03-12 23:39:43","title":"Cost-Effective Methodology for Complex Tuning Searches in HPC: Navigating Interdependencies and Dimensionality","abstract":"Tuning searches are pivotal in High-Performance Computing (HPC), addressing complex optimization challenges in computational applications. The complexity arises not only from finely tuning parameters within routines but also potential interdependencies among them, rendering traditional optimization methods inefficient. Instead of scrutinizing interdependencies among parameters and routines, practitioners often face the dilemma of conducting independent tuning searches for each routine, thereby overlooking interdependence, or pursuing a more resource-intensive joint search for all routines. This decision is driven by the consideration that some interdependence analysis and high-dimensional decomposition techniques in literature may be prohibitively expensive in HPC tuning searches. Our methodology adapts and refines these methods to ensure computational feasibility while maximizing performance gains in real-world scenarios. Our methodology leverages a cost-effective interdependence analysis to decide whether to merge several tuning searches into a joint search or conduct orthogonal searches. Tested on synthetic functions with varying levels of parameter interdependence, our methodology efficiently explores the search space. In comparison to Bayesian-optimization-based full independent or fully joint searches, our methodology suggested an optimized breakdown of independent and merged searches that led to final configurations up to 8% more accurate, reducing the search time by up to 95%. When applied to GPU-offloaded Real-Time Time-Dependent Density Functional Theory (RT-TDDFT), an application in computational materials science that challenges modern HPC autotuners, our methodology achieved an effective tuning search. Its adaptability and efficiency extend beyond RT-TDDFT, making it valuable for related applications in HPC.","sentences":["Tuning searches are pivotal in High-Performance Computing (HPC), addressing complex optimization challenges in computational applications.","The complexity arises not only from finely tuning parameters within routines but also potential interdependencies among them, rendering traditional optimization methods inefficient.","Instead of scrutinizing interdependencies among parameters and routines, practitioners often face the dilemma of conducting independent tuning searches for each routine, thereby overlooking interdependence, or pursuing a more resource-intensive joint search for all routines.","This decision is driven by the consideration that some interdependence analysis and high-dimensional decomposition techniques in literature may be prohibitively expensive in HPC tuning searches.","Our methodology adapts and refines these methods to ensure computational feasibility while maximizing performance gains in real-world scenarios.","Our methodology leverages a cost-effective interdependence analysis to decide whether to merge several tuning searches into a joint search or conduct orthogonal searches.","Tested on synthetic functions with varying levels of parameter interdependence, our methodology efficiently explores the search space.","In comparison to Bayesian-optimization-based full independent or fully joint searches, our methodology suggested an optimized breakdown of independent and merged searches that led to final configurations up to 8% more accurate, reducing the search time by up to 95%.","When applied to GPU-offloaded Real-Time Time-Dependent Density Functional Theory (RT-TDDFT), an application in computational materials science that challenges modern HPC autotuners, our methodology achieved an effective tuning search.","Its adaptability and efficiency extend beyond RT-TDDFT, making it valuable for related applications in HPC."],"url":"http://arxiv.org/abs/2403.08131v1","category":"cs.DC"}
{"created":"2024-03-12 23:38:29","title":"Imputation of Counterfactual Outcomes when the Errors are Predictable","abstract":"A crucial input into causal inference is the imputed counterfactual outcome.   Imputation error can arise because of sampling uncertainty from estimating the prediction model using the untreated observations, or from out-of-sample information not captured by the model. While the literature has focused on sampling uncertainty, it vanishes with the sample size. Often overlooked is the possibility that the out-of-sample error can be informative about the missing counterfactual outcome if it is mutually or serially correlated. Motivated by the best linear unbiased predictor (\\blup) of \\citet{goldberger:62} in a time series setting, we propose an improved predictor of potential outcome when the errors are correlated. The proposed \\pup\\; is practical as it is not restricted to linear models,   can be used with consistent estimators already developed, and improves mean-squared error for a large class of strong mixing error processes. Ignoring predictability in the errors can distort conditional inference. However, the precise impact will depend on the choice of estimator as well as the realized values of the residuals.","sentences":["A crucial input into causal inference is the imputed counterfactual outcome.   ","Imputation error can arise because of sampling uncertainty from estimating the prediction model using the untreated observations, or from out-of-sample information not captured by the model.","While the literature has focused on sampling uncertainty, it vanishes with the sample size.","Often overlooked is the possibility that the out-of-sample error can be informative about the missing counterfactual outcome if it is mutually or serially correlated.","Motivated by the best linear unbiased predictor (\\blup) of \\citet{goldberger:62} in a time series setting, we propose an improved predictor of potential outcome when the errors are correlated.","The proposed \\pup\\; is practical as it is not restricted to linear models,   can be used with consistent estimators already developed, and improves mean-squared error for a large class of strong mixing error processes.","Ignoring predictability in the errors can distort conditional inference.","However, the precise impact will depend on the choice of estimator as well as the realized values of the residuals."],"url":"http://arxiv.org/abs/2403.08130v1","category":"econ.EM"}
{"created":"2024-03-12 23:31:06","title":"Quantum Channel Conditioning and Measurement Models","abstract":"If $H_1$ and $H_2$ are finite-dimensional Hilbert spaces, a channel from $H_1$ to $H_2$ is a completely positive, linear map $\\mathcal{I}$ that takes the set of states $\\mathcal{S}(H_1)$ for $H_1$ to the set of states $\\mathcal{S}(H_2)$ for $H_2$. Corresponding to $\\mathcal{I}$ there is a unique dual map $\\mathcal{I}^*$ from the set of effects $\\mathcal{E}(H_2)$ for $H_2$ to the set of effects $\\mathcal{E}(H_1)$ for $H_1$. We call $\\mathcal{I}^*(b)$ the effect $b$ conditioned by $\\mathcal{I}$ and the set $\\mathcal{I}^c = \\mathcal{I}^*(\\mathcal{E}(H_2))$ the conditioned set of $\\mathcal{I}$. We point out that $\\mathcal{I}^c$ is a convex subeffect algebra of the effect algebra $\\mathcal{E}(H_1)$. We extend this definition to the conditioning $\\mathcal{I}^*(B)$ for an observable $B$ on $H_2$ and say that an observable $A$ is in $\\mathcal{I}^c$ if $A=\\mathcal{I}^*(B)$ for some observable $B$. We show that $\\mathcal{I}^c$ is closed under post-processing and taking parts. We also define the conditioning of instruments by channels. These concepts are illustrated using examples of Holevo instruments and channels. We next discuss measurement models and their corresponding observables and instruments. We show that calculations can be simplified by employing Kraus and Holevo separable channels. Such channels allow one to separate the components of a tensor product.","sentences":["If $H_1$ and $H_2$ are finite-dimensional Hilbert spaces, a channel from $H_1$ to $H_2$ is a completely positive, linear map $\\mathcal{I}$ that takes the set of states $\\mathcal{S}(H_1)$ for $H_1$ to the set of states $\\mathcal{S}(H_2)$ for $H_2$. Corresponding to $\\mathcal{I}$ there is a unique dual map $\\mathcal{I}^*$ from the set of effects $\\mathcal{E}(H_2)$ for $H_2$ to the set of effects $\\mathcal{E}(H_1)$ for $H_1$. We call $\\mathcal{I}^*(b)$ the effect $b$ conditioned by $\\mathcal{I}$ and the set $\\mathcal{I}^c = \\mathcal{I}^*(\\mathcal{E}(H_2))$ the conditioned set of $\\mathcal{I}$. We point out that $\\mathcal{I}^c$ is a convex subeffect algebra of the effect algebra $\\mathcal{E}(H_1)$.","We extend this definition to the conditioning $\\mathcal{I}^*(B)$ for an observable $B$ on $H_2$ and say that an observable $A$ is in $\\mathcal{I}^c$ if $A=\\mathcal{I}^*(B)$ for some observable $B$. We show that $\\mathcal{I}^c$ is closed under post-processing and taking parts.","We also define the conditioning of instruments by channels.","These concepts are illustrated using examples of Holevo instruments and channels.","We next discuss measurement models and their corresponding observables and instruments.","We show that calculations can be simplified by employing Kraus and Holevo separable channels.","Such channels allow one to separate the components of a tensor product."],"url":"http://arxiv.org/abs/2403.08126v1","category":"quant-ph"}
{"created":"2024-03-12 23:17:32","title":"Early Directional Convergence in Deep Homogeneous Neural Networks for Small Initializations","abstract":"This paper studies the gradient flow dynamics that arise when training deep homogeneous neural networks, starting with small initializations. The present work considers neural networks that are assumed to have locally Lipschitz gradients and an order of homogeneity strictly greater than two. This paper demonstrates that for sufficiently small initializations, during the early stages of training, the weights of the neural network remain small in norm and approximately converge in direction along the Karush-Kuhn-Tucker (KKT) points of the neural correlation function introduced in [1]. Additionally, for square loss and under a separability assumption on the weights of neural networks, a similar directional convergence of gradient flow dynamics is shown near certain saddle points of the loss function.","sentences":["This paper studies the gradient flow dynamics that arise when training deep homogeneous neural networks, starting with small initializations.","The present work considers neural networks that are assumed to have locally Lipschitz gradients and an order of homogeneity strictly greater than two.","This paper demonstrates that for sufficiently small initializations, during the early stages of training, the weights of the neural network remain small in norm and approximately converge in direction along the Karush-Kuhn-Tucker (KKT) points of the neural correlation function introduced in [1].","Additionally, for square loss and under a separability assumption on the weights of neural networks, a similar directional convergence of gradient flow dynamics is shown near certain saddle points of the loss function."],"url":"http://arxiv.org/abs/2403.08121v1","category":"cs.LG"}
{"created":"2024-03-12 23:05:10","title":"CMax-SLAM: Event-based Rotational-Motion Bundle Adjustment and SLAM System using Contrast Maximization","abstract":"Event cameras are bio-inspired visual sensors that capture pixel-wise intensity changes and output asynchronous event streams. They show great potential over conventional cameras to handle challenging scenarios in robotics and computer vision, such as high-speed and high dynamic range. This paper considers the problem of rotational motion estimation using event cameras. Several event-based rotation estimation methods have been developed in the past decade, but their performance has not been evaluated and compared under unified criteria yet. In addition, these prior works do not consider a global refinement step. To this end, we conduct a systematic study of this problem with two objectives in mind: summarizing previous works and presenting our own solution. First, we compare prior works both theoretically and experimentally. Second, we propose the first event-based rotation-only bundle adjustment (BA) approach. We formulate it leveraging the state-of-the-art Contrast Maximization (CMax) framework, which is principled and avoids the need to convert events into frames. Third, we use the proposed BA to build CMax-SLAM, the first event-based rotation-only SLAM system comprising a front-end and a back-end. Our BA is able to run both offline (trajectory smoothing) and online (CMax-SLAM back-end). To demonstrate the performance and versatility of our method, we present comprehensive experiments on synthetic and real-world datasets, including indoor, outdoor and space scenarios. We discuss the pitfalls of real-world evaluation and propose a proxy for the reprojection error as the figure of merit to evaluate event-based rotation BA methods. We release the source code and novel data sequences to benefit the community. We hope this work leads to a better understanding and fosters further research on event-based ego-motion estimation. Project page: https://github.com/tub-rip/cmax_slam","sentences":["Event cameras are bio-inspired visual sensors that capture pixel-wise intensity changes and output asynchronous event streams.","They show great potential over conventional cameras to handle challenging scenarios in robotics and computer vision, such as high-speed and high dynamic range.","This paper considers the problem of rotational motion estimation using event cameras.","Several event-based rotation estimation methods have been developed in the past decade, but their performance has not been evaluated and compared under unified criteria yet.","In addition, these prior works do not consider a global refinement step.","To this end, we conduct a systematic study of this problem with two objectives in mind: summarizing previous works and presenting our own solution.","First, we compare prior works both theoretically and experimentally.","Second, we propose the first event-based rotation-only bundle adjustment (BA) approach.","We formulate it leveraging the state-of-the-art Contrast Maximization (CMax) framework, which is principled and avoids the need to convert events into frames.","Third, we use the proposed BA to build CMax-SLAM, the first event-based rotation-only SLAM system comprising a front-end and a back-end.","Our BA is able to run both offline (trajectory smoothing) and online (CMax-SLAM back-end).","To demonstrate the performance and versatility of our method, we present comprehensive experiments on synthetic and real-world datasets, including indoor, outdoor and space scenarios.","We discuss the pitfalls of real-world evaluation and propose a proxy for the reprojection error as the figure of merit to evaluate event-based rotation BA methods.","We release the source code and novel data sequences to benefit the community.","We hope this work leads to a better understanding and fosters further research on event-based ego-motion estimation.","Project page: https://github.com/tub-rip/cmax_slam"],"url":"http://arxiv.org/abs/2403.08119v1","category":"cs.RO"}
{"created":"2024-03-12 22:53:21","title":"Guaranteeing Service in Connected Microgrids: Storage Planning and Optimal Power Sharing Policy","abstract":"The integration of renewable energy sources (RES) into power distribution grids poses challenges to system reliability due to the inherent uncertainty in their power production. To address this issue, battery energy sources (BESs) are being increasingly used as a promising solution to counter the uncertainty associated with RES power production. During the overall system planning stage, the optimal capacity of the BES has to be decided. In the operational phase, policies on when to charge the BESs and when to use them to support loads must be determined so that the BES remains within its operating range, avoiding depletion of charge on one hand and remaining within acceptable margins of maximum charge on the other. In this paper, a stochastic control framework is used to determine battery capacity, for microgrids, which ensures that during the operational phase, BESs' operating range is respected with pre-specified high probability. We provide an explicit analytical expression of the required BESs energy capacity for a single microgrid with RES as the main power source. Leveraging insights from the single microgrid case, the article focuses on the design and planning of BESs for the two-microgrid scenario. In this setting, microgrids are allowed to share power while respecting the capacity constraints imposed by the power lines. We characterize the optimal power transfer policy between the microgrids and the optimal BES capacity for multiple microgrids. This provides the BES savings arising from connecting the microgrids.","sentences":["The integration of renewable energy sources (RES) into power distribution grids poses challenges to system reliability due to the inherent uncertainty in their power production.","To address this issue, battery energy sources (BESs) are being increasingly used as a promising solution to counter the uncertainty associated with RES power production.","During the overall system planning stage, the optimal capacity of the BES has to be decided.","In the operational phase, policies on when to charge the BESs and when to use them to support loads must be determined so that the BES remains within its operating range, avoiding depletion of charge on one hand and remaining within acceptable margins of maximum charge on the other.","In this paper, a stochastic control framework is used to determine battery capacity, for microgrids, which ensures that during the operational phase, BESs' operating range is respected with pre-specified high probability.","We provide an explicit analytical expression of the required BESs energy capacity for a single microgrid with RES as the main power source.","Leveraging insights from the single microgrid case, the article focuses on the design and planning of BESs for the two-microgrid scenario.","In this setting, microgrids are allowed to share power while respecting the capacity constraints imposed by the power lines.","We characterize the optimal power transfer policy between the microgrids and the optimal BES capacity for multiple microgrids.","This provides the BES savings arising from connecting the microgrids."],"url":"http://arxiv.org/abs/2403.08114v1","category":"eess.SY"}
{"created":"2024-03-12 22:33:02","title":"TaskCLIP: Extend Large Vision-Language Model for Task Oriented Object Detection","abstract":"Task-oriented object detection aims to find objects suitable for accomplishing specific tasks. As a challenging task, it requires simultaneous visual data processing and reasoning under ambiguous semantics. Recent solutions are mainly all-in-one models. However, the object detection backbones are pre-trained without text supervision. Thus, to incorporate task requirements, their intricate models undergo extensive learning on a highly imbalanced and scarce dataset, resulting in capped performance, laborious training, and poor generalizability. In contrast, we propose TaskCLIP, a more natural two-stage design composed of general object detection and task-guided object selection. Particularly for the latter, we resort to the recently successful large Vision-Language Models (VLMs) as our backbone, which provides rich semantic knowledge and a uniform embedding space for images and texts. Nevertheless, the naive application of VLMs leads to sub-optimal quality, due to the misalignment between embeddings of object images and their visual attributes, which are mainly adjective phrases. To this end, we design a transformer-based aligner after the pre-trained VLMs to re-calibrate both embeddings. Finally, we employ a trainable score function to post-process the VLM matching results for object selection. Experimental results demonstrate that our TaskCLIP outperforms the state-of-the-art DETR-based model TOIST by 3.5% and only requires a single NVIDIA RTX 4090 for both training and inference.","sentences":["Task-oriented object detection aims to find objects suitable for accomplishing specific tasks.","As a challenging task, it requires simultaneous visual data processing and reasoning under ambiguous semantics.","Recent solutions are mainly all-in-one models.","However, the object detection backbones are pre-trained without text supervision.","Thus, to incorporate task requirements, their intricate models undergo extensive learning on a highly imbalanced and scarce dataset, resulting in capped performance, laborious training, and poor generalizability.","In contrast, we propose TaskCLIP, a more natural two-stage design composed of general object detection and task-guided object selection.","Particularly for the latter, we resort to the recently successful large Vision-Language Models (VLMs) as our backbone, which provides rich semantic knowledge and a uniform embedding space for images and texts.","Nevertheless, the naive application of VLMs leads to sub-optimal quality, due to the misalignment between embeddings of object images and their visual attributes, which are mainly adjective phrases.","To this end, we design a transformer-based aligner after the pre-trained VLMs to re-calibrate both embeddings.","Finally, we employ a trainable score function to post-process the VLM matching results for object selection.","Experimental results demonstrate that our TaskCLIP outperforms the state-of-the-art DETR-based model TOIST by 3.5% and only requires a single NVIDIA RTX 4090 for both training and inference."],"url":"http://arxiv.org/abs/2403.08108v1","category":"cs.CV"}
{"created":"2024-03-12 22:16:48","title":"EXCOGITO, an extensible coarse-graining toolbox for the investigation of biomolecules by means of low-resolution representation","abstract":"Bottom-up coarse-grained (CG) models proved to be essential to complement and sometimes even replace all-atom representations of soft matter systems and biological macromolecules. The development of low-resolution models takes the moves from the reduction of the degrees of freedom employed, that is, the definition of a mapping between a system's high-resolution description and its simplified counterpart. Even in absence of an explicit parametrisation and simulation of a CG model, the observation of the atomistic system in simpler terms can be informative: this idea is leveraged by the mapping entropy, a measure of the information loss inherent to the process of coarsening. Mapping entropy lies at the heart of the extensible coarse-graining toolbox, or EXCOGITO, developed to perform a number of operations and analyses on molecular systems pivoting around the properties of mappings. EXCOGITO can process an all-atom trajectory to compute the mapping entropy, identify the mapping that minimizes it, and establish quantitative relations between a low-resolution representation and geometrical, structural, and energetic features of the system. Here, the software, which is available free of charge under an open source licence, is presented and showcased to introduce potential users to its capabilities and usage.","sentences":["Bottom-up coarse-grained (CG) models proved to be essential to complement and sometimes even replace all-atom representations of soft matter systems and biological macromolecules.","The development of low-resolution models takes the moves from the reduction of the degrees of freedom employed, that is, the definition of a mapping between a system's high-resolution description and its simplified counterpart.","Even in absence of an explicit parametrisation and simulation of a CG model, the observation of the atomistic system in simpler terms can be informative: this idea is leveraged by the mapping entropy, a measure of the information loss inherent to the process of coarsening.","Mapping entropy lies at the heart of the extensible coarse-graining toolbox, or EXCOGITO, developed to perform a number of operations and analyses on molecular systems pivoting around the properties of mappings.","EXCOGITO can process an all-atom trajectory to compute the mapping entropy, identify the mapping that minimizes it, and establish quantitative relations between a low-resolution representation and geometrical, structural, and energetic features of the system.","Here, the software, which is available free of charge under an open source licence, is presented and showcased to introduce potential users to its capabilities and usage."],"url":"http://arxiv.org/abs/2403.08097v1","category":"cond-mat.soft"}
{"created":"2024-03-12 22:03:19","title":"Mitigating the Impact of Attribute Editing on Face Recognition","abstract":"Facial attribute editing using generative models can impair automated face recognition. This degradation persists even with recent identity-preserving models such as InstantID. To mitigate this issue, we propose two techniques that perform local and global attribute editing. Local editing operates on the finer details via a regularization-free method based on ControlNet conditioned on depth maps and auxiliary semantic segmentation masks. Global editing operates on coarser details via a regularization-based method guided by custom loss and regularization set. In this work, we empirically ablate twenty-six facial semantic, demographic and expression-based attributes altered using state-of-the-art generative models and evaluate them using ArcFace and AdaFace matchers on CelebA, CelebAMaskHQ and LFW datasets. Finally, we use LLaVA, a vision-language framework for attribute prediction to validate our editing techniques. Our methods outperform SoTA (BLIP, InstantID) at facial editing while retaining identity.","sentences":["Facial attribute editing using generative models can impair automated face recognition.","This degradation persists even with recent identity-preserving models such as InstantID.","To mitigate this issue, we propose two techniques that perform local and global attribute editing.","Local editing operates on the finer details via a regularization-free method based on ControlNet conditioned on depth maps and auxiliary semantic segmentation masks.","Global editing operates on coarser details via a regularization-based method guided by custom loss and regularization set.","In this work, we empirically ablate twenty-six facial semantic, demographic and expression-based attributes altered using state-of-the-art generative models and evaluate them using ArcFace and AdaFace matchers on CelebA, CelebAMaskHQ and LFW datasets.","Finally, we use LLaVA, a vision-language framework for attribute prediction to validate our editing techniques.","Our methods outperform SoTA (BLIP, InstantID) at facial editing while retaining identity."],"url":"http://arxiv.org/abs/2403.08092v1","category":"cs.CV"}
{"created":"2024-03-12 21:16:25","title":"Data Monetization Pathways and Complex Dynamic Game Equilibrium Analysis in the Energy Industry","abstract":"As the most critical production factor in the era of the digital economy, data will have a significant impact on social production and development. Energy enterprises possess data that is interconnected with multiple industries, characterized by diverse needs, sensitivity, and long-term nature. The path to monetizing energy enterprises' data is challenging yet crucial. This paper explores the game-theoretic aspects of the data monetization process in energy enterprises by considering the relationships between enterprises and trading platforms. We construct a class of game decision models and study their equilibrium strategies. Our analysis shows that enterprises and platforms can adjust respective benefits by regulating the wholesale price of data and the intensity of data value mining to form a benign equilibrium state. Furthermore, by integrating nonlinear dynamical theory, we discuss the dynamic characteristics present in multi-period repeated game processes. We find that decision-makers should keep the adjustment parameters and initial states within reasonable ranges in multi-period dynamic decision-making to avoid market failure. Finally, based on the theoretical and numerical analysis, we provide decision insights and recommendations for enterprise decision-making to facilitate data monetization through strategic interactions with trading platforms.","sentences":["As the most critical production factor in the era of the digital economy, data will have a significant impact on social production and development.","Energy enterprises possess data that is interconnected with multiple industries, characterized by diverse needs, sensitivity, and long-term nature.","The path to monetizing energy enterprises' data is challenging yet crucial.","This paper explores the game-theoretic aspects of the data monetization process in energy enterprises by considering the relationships between enterprises and trading platforms.","We construct a class of game decision models and study their equilibrium strategies.","Our analysis shows that enterprises and platforms can adjust respective benefits by regulating the wholesale price of data and the intensity of data value mining to form a benign equilibrium state.","Furthermore, by integrating nonlinear dynamical theory, we discuss the dynamic characteristics present in multi-period repeated game processes.","We find that decision-makers should keep the adjustment parameters and initial states within reasonable ranges in multi-period dynamic decision-making to avoid market failure.","Finally, based on the theoretical and numerical analysis, we provide decision insights and recommendations for enterprise decision-making to facilitate data monetization through strategic interactions with trading platforms."],"url":"http://arxiv.org/abs/2403.08082v1","category":"cs.GT"}
{"created":"2024-03-12 20:58:51","title":"Several isoperimetric inequalities of Dirichlet and Neumann eigenvalues of the Witten-Laplacian","abstract":"In this paper, by mainly using the rearrangement technique and suitably constructing trial functions, under the constraint of fixed weighted volume, we can successfully obtain several isoperimetric inequalities for the first and the second Dirichlet eigenvalues, the first nonzero Neumann eigenvalue of the Witten-Laplacian on bounded domains in space forms. These spectral isoperimetric inequalities extend those classical ones (i.e. the Faber-Krahn inequality, the Hong-Krahn-Szeg\\H{o} inequality and the Szeg\\H{o}-Weinberger inequality) of the Laplacian.","sentences":["In this paper, by mainly using the rearrangement technique and suitably constructing trial functions, under the constraint of fixed weighted volume, we can successfully obtain several isoperimetric inequalities for the first and the second Dirichlet eigenvalues, the first nonzero Neumann eigenvalue of the Witten-Laplacian on bounded domains in space forms.","These spectral isoperimetric inequalities extend those classical ones (i.e. the Faber-Krahn inequality, the Hong-Krahn-Szeg\\H{o} inequality and the Szeg\\H{o}-Weinberger inequality) of the Laplacian."],"url":"http://arxiv.org/abs/2403.08075v1","category":"math.AP"}
{"created":"2024-03-12 20:31:58","title":"Zero-Rating, One Big Mess: Analyzing Differential Pricing Practices of European MNOs","abstract":"Zero-rating, the practice of not billing data traffic that belongs to certain applications, has become popular within the mobile ecosystem around the globe. There is an ongoing debate whether mobile operators should be allowed to differentiate traffic or whether net neutrality regulations should prevent this. Despite the importance of this issue, we know little about the technical aspects of zero-rating offers since the implementation is kept secret by mobile operators and therefore is opaque to end-users and regulatory agencies.   This work aims to independently audit classification practices used for zero-rating of four popular applications at seven different mobile operators in the EU. We execute and evaluate more than 300 controlled experiments within domestic and internationally roamed environments and identify potentially problematic behavior at almost all investigated operators. With this study, we hope to increase transparency around the current practices and inform future decisions and policies.","sentences":["Zero-rating, the practice of not billing data traffic that belongs to certain applications, has become popular within the mobile ecosystem around the globe.","There is an ongoing debate whether mobile operators should be allowed to differentiate traffic or whether net neutrality regulations should prevent this.","Despite the importance of this issue, we know little about the technical aspects of zero-rating offers since the implementation is kept secret by mobile operators and therefore is opaque to end-users and regulatory agencies.   ","This work aims to independently audit classification practices used for zero-rating of four popular applications at seven different mobile operators in the EU.","We execute and evaluate more than 300 controlled experiments within domestic and internationally roamed environments and identify potentially problematic behavior at almost all investigated operators.","With this study, we hope to increase transparency around the current practices and inform future decisions and policies."],"url":"http://arxiv.org/abs/2403.08066v1","category":"cs.NI"}
{"created":"2024-03-12 20:26:51","title":"Gaze-based Human-Robot Interaction System for Infrastructure Inspections","abstract":"Routine inspections for critical infrastructures such as bridges are required in most jurisdictions worldwide. Such routine inspections are largely visual in nature, which are qualitative, subjective, and not repeatable. Although robotic infrastructure inspections address such limitations, they cannot replace the superior ability of experts to make decisions in complex situations, thus making human-robot interaction systems a promising technology. This study presents a novel gaze-based human-robot interaction system, designed to augment the visual inspection performance through mixed reality. Through holograms from a mixed reality device, gaze can be utilized effectively to estimate the properties of the defect in real-time. Additionally, inspectors can monitor the inspection progress online, which enhances the speed of the entire inspection process. Limited controlled experiments demonstrate its effectiveness across various users and defect types. To our knowledge, this is the first demonstration of the real-time application of eye gaze in civil infrastructure inspections.","sentences":["Routine inspections for critical infrastructures such as bridges are required in most jurisdictions worldwide.","Such routine inspections are largely visual in nature, which are qualitative, subjective, and not repeatable.","Although robotic infrastructure inspections address such limitations, they cannot replace the superior ability of experts to make decisions in complex situations, thus making human-robot interaction systems a promising technology.","This study presents a novel gaze-based human-robot interaction system, designed to augment the visual inspection performance through mixed reality.","Through holograms from a mixed reality device, gaze can be utilized effectively to estimate the properties of the defect in real-time.","Additionally, inspectors can monitor the inspection progress online, which enhances the speed of the entire inspection process.","Limited controlled experiments demonstrate its effectiveness across various users and defect types.","To our knowledge, this is the first demonstration of the real-time application of eye gaze in civil infrastructure inspections."],"url":"http://arxiv.org/abs/2403.08061v1","category":"cs.RO"}
{"created":"2024-03-12 20:04:09","title":"Improving Memory Dependence Prediction with Static Analysis","abstract":"This paper explores the potential of communicating information gained by static analysis from compilers to Out-of-Order (OoO) machines, focusing on the memory dependence predictor (MDP). The MDP enables loads to issue without all in-flight store addresses being known, with minimal memory order violations. We use LLVM to find loads with no dependencies and label them via their opcode. These labelled loads skip making lookups into the MDP, improving prediction accuracy by reducing false dependencies. We communicate this information in a minimally intrusive way, i.e.~without introducing additional hardware costs or instruction bandwidth, providing these improvements without any additional overhead in the CPU. We find that in select cases in Spec2017, a significant number of load instructions can skip interacting with the MDP and lead to a performance gain. These results point to greater possibilities for static analysis as a source of near zero cost performance gains in future CPU designs.","sentences":["This paper explores the potential of communicating information gained by static analysis from compilers to Out-of-Order (OoO) machines, focusing on the memory dependence predictor (MDP).","The MDP enables loads to issue without all in-flight store addresses being known, with minimal memory order violations.","We use LLVM to find loads with no dependencies and label them via their opcode.","These labelled loads skip making lookups into the MDP, improving prediction accuracy by reducing false dependencies.","We communicate this information in a minimally intrusive way, i.e.~without introducing additional hardware costs or instruction bandwidth, providing these improvements without any additional overhead in the CPU.","We find that in select cases in Spec2017, a significant number of load instructions can skip interacting with the MDP and lead to a performance gain.","These results point to greater possibilities for static analysis as a source of near zero cost performance gains in future CPU designs."],"url":"http://arxiv.org/abs/2403.08056v1","category":"cs.PL"}
{"created":"2024-03-12 20:01:36","title":"Learning-based Prescribed-Time Safety for Control of Unknown Systems with Control Barrier Functions","abstract":"In many control system applications, state constraint satisfaction needs to be guaranteed within a prescribed time. While this issue has been partially addressed for systems with known dynamics, it remains largely unaddressed for systems with unknown dynamics. In this paper, we propose a Gaussian process-based time-varying control method that leverages backstepping and control barrier functions to achieve safety requirements within prescribed time windows. It can be used to keep a system within a safe region or to make it return to a safe region within a limited time window. These properties are cemented by rigorous theoretical results. The effectiveness of the proposed controller is demonstrated in a simulation of a robotic manipulator.","sentences":["In many control system applications, state constraint satisfaction needs to be guaranteed within a prescribed time.","While this issue has been partially addressed for systems with known dynamics, it remains largely unaddressed for systems with unknown dynamics.","In this paper, we propose a Gaussian process-based time-varying control method that leverages backstepping and control barrier functions to achieve safety requirements within prescribed time windows.","It can be used to keep a system within a safe region or to make it return to a safe region within a limited time window.","These properties are cemented by rigorous theoretical results.","The effectiveness of the proposed controller is demonstrated in a simulation of a robotic manipulator."],"url":"http://arxiv.org/abs/2403.08054v1","category":"eess.SY"}
{"created":"2024-03-12 19:47:44","title":"Experimental demonstration of the shadow of a laser beam","abstract":"Light, being massless, casts no shadow; under ordinary circumstances, photons pass right through each other unimpeded. Here, we demonstrate a laser beam acting like an object - the beam casts a shadow upon a surface when the beam is illuminated by another light source. We observe a regular shadow in the sense it can be seen by the naked eye, it follows the contours of the surface it falls on, and it follows the position and shape of the object (the laser beam). Specifically, we use a nonlinear optical process involving four atomic levels of ruby. We find a maximum contrast of approximately 22 percent, similar to that of a shadow of a tree on a sunny day. Making light itself cast a shadow opens new possibilities for fabrication, imaging, and illumination.","sentences":["Light, being massless, casts no shadow; under ordinary circumstances, photons pass right through each other unimpeded.","Here, we demonstrate a laser beam acting like an object - the beam casts a shadow upon a surface when the beam is illuminated by another light source.","We observe a regular shadow in the sense it can be seen by the naked eye, it follows the contours of the surface it falls on, and it follows the position and shape of the object (the laser beam).","Specifically, we use a nonlinear optical process involving four atomic levels of ruby.","We find a maximum contrast of approximately 22 percent, similar to that of a shadow of a tree on a sunny day.","Making light itself cast a shadow opens new possibilities for fabrication, imaging, and illumination."],"url":"http://arxiv.org/abs/2403.08050v1","category":"physics.optics"}
{"created":"2024-03-12 19:25:13","title":"What would Plato say? Concepts and notions from Greek philosophy applied to gamification mechanics for a meaningful and ethical gamification","abstract":"Gamification, the integration of game mechanics in non-game settings, has become increasingly prevalent in various digital platforms; however, its ethical and societal impacts are often overlooked. This paper delves into how Platonic and Aristotelian philosophies can provide a critical framework for understanding and evaluating the ethical dimensions of gamification. Plato's allegory of the cave and theory of forms are used to analyse the perception of reality in gamified environments, questioning their authenticity and the value of virtual achievements, while Aristotle's virtue ethics, with its emphasis on moderation, virtue, and eudaimonia (true and full happiness), can help assess how gamification influences user behaviour and ethical decision-making. The paper critically examines various gamification elements, such as the hero's journey, altruistic actions, badge levels, and user autonomy, through these philosophical lenses, and addresses the ethical responsibilities of gamification designers, advocating for a balanced approach that prioritizes user well-being and ethical development over commercial interests. By bridging ancient philosophical insights with modern digital culture, this research contributes to a deeper understanding of the ethical implications of gamification, emphasizing the need for responsible and virtuous design in digital applications.","sentences":["Gamification, the integration of game mechanics in non-game settings, has become increasingly prevalent in various digital platforms; however, its ethical and societal impacts are often overlooked.","This paper delves into how Platonic and Aristotelian philosophies can provide a critical framework for understanding and evaluating the ethical dimensions of gamification.","Plato's allegory of the cave and theory of forms are used to analyse the perception of reality in gamified environments, questioning their authenticity and the value of virtual achievements, while Aristotle's virtue ethics, with its emphasis on moderation, virtue, and eudaimonia (true and full happiness), can help assess how gamification influences user behaviour and ethical decision-making.","The paper critically examines various gamification elements, such as the hero's journey, altruistic actions, badge levels, and user autonomy, through these philosophical lenses, and addresses the ethical responsibilities of gamification designers, advocating for a balanced approach that prioritizes user well-being and ethical development over commercial interests.","By bridging ancient philosophical insights with modern digital culture, this research contributes to a deeper understanding of the ethical implications of gamification, emphasizing the need for responsible and virtuous design in digital applications."],"url":"http://arxiv.org/abs/2403.08041v1","category":"cs.HC"}
{"created":"2024-03-12 19:23:13","title":"MicroT: Low-Energy and Adaptive Models for MCUs","abstract":"We propose MicroT, a low-energy, multi-task adaptive model framework for resource-constrained MCUs. We divide the original model into a feature extractor and a classifier. The feature extractor is obtained through self-supervised knowledge distillation and further optimized into part and full models through model splitting and joint training. These models are then deployed on MCUs, with classifiers added and trained on local tasks, ultimately performing stage-decision for joint inference. In this process, the part model initially processes the sample, and if the confidence score falls below the set threshold, the full model will resume and continue the inference. We evaluate MicroT on two models, three datasets, and two MCU boards. Our experimental evaluation shows that MicroT effectively improves model performance and reduces energy consumption when dealing with multiple local tasks. Compared to the unoptimized feature extractor, MicroT can improve accuracy by up to 9.87%. On MCUs, compared to the standard full model inference, MicroT can save up to about 29.13% in energy consumption. MicroT also allows users to adaptively adjust the stage-decision ratio as needed, better balancing model performance and energy consumption. Under the standard stage-decision ratio configuration, MicroT can increase accuracy by 5.91% and save about 14.47% of energy consumption.","sentences":["We propose MicroT, a low-energy, multi-task adaptive model framework for resource-constrained MCUs.","We divide the original model into a feature extractor and a classifier.","The feature extractor is obtained through self-supervised knowledge distillation and further optimized into part and full models through model splitting and joint training.","These models are then deployed on MCUs, with classifiers added and trained on local tasks, ultimately performing stage-decision for joint inference.","In this process, the part model initially processes the sample, and if the confidence score falls below the set threshold, the full model will resume and continue the inference.","We evaluate MicroT on two models, three datasets, and two MCU boards.","Our experimental evaluation shows that MicroT effectively improves model performance and reduces energy consumption when dealing with multiple local tasks.","Compared to the unoptimized feature extractor, MicroT can improve accuracy by up to 9.87%.","On MCUs, compared to the standard full model inference, MicroT can save up to about 29.13% in energy consumption.","MicroT also allows users to adaptively adjust the stage-decision ratio as needed, better balancing model performance and energy consumption.","Under the standard stage-decision ratio configuration, MicroT can increase accuracy by 5.91% and save about 14.47% of energy consumption."],"url":"http://arxiv.org/abs/2403.08040v1","category":"cs.LG"}
{"created":"2024-03-12 19:23:01","title":"On closed definable subsets in Hensel minimal structures","abstract":"This paper deals with Hensel minimal structures on non-trivially valued fields $K$. The main aim is to establish the following two properties of closed 0-definable subsets $A$ in the affine spaces $K^{n}$. Every such subset $A$ is the zero locus of a continuous 0-definable function $f:K^{n} \\to K$, and there exists a 0-definable retraction $r: K^{n} \\to A$. While the former property is a non-Archimedean counterpart of the one from o-minimal geometry, the former does not hold in real geometry in general. The proofs make use of a model-theoretic compactness argument and ubiquity of clopen sets in non-Archimedean geometry.","sentences":["This paper deals with Hensel minimal structures on non-trivially valued fields $K$. The main aim is to establish the following two properties of closed 0-definable subsets $A$ in the affine spaces $K^{n}$. Every such subset $A$ is the zero locus of a continuous 0-definable function $f:K^{n} \\to K$, and there exists a 0-definable retraction $r: K^{n} \\to A$.","While the former property is a non-Archimedean counterpart of the one from o-minimal geometry, the former does not hold in real geometry in general.","The proofs make use of a model-theoretic compactness argument and ubiquity of clopen sets in non-Archimedean geometry."],"url":"http://arxiv.org/abs/2403.08039v1","category":"math.LO"}
{"created":"2024-03-12 19:10:13","title":"Vibronic correlations in molecular strong field dynamics","abstract":"We investigate ultrafast vibronic dynamics triggered by intense femtosecond infrared pulses in small molecules. Our study is based on numerical simulations performed with 2D model molecules, and analyzed in the perspective of the renown Lochfrass and Bond-Softening models. We give a new interpretation of the observed nuclear wave packet dynamics, with a focus on the phase of the bond oscillations. Our simulations also reveal intricate features in the field-induced nuclear motion that are not accounted for by existing models. Our analyses assign these features to strong dynamic correlations between the active electron and the nuclei, which significantly depend on the carrier envelope phase of the pulse, even for relatively ``long'' pulses, which should make them experimentally observable.","sentences":["We investigate ultrafast vibronic dynamics triggered by intense femtosecond infrared pulses in small molecules.","Our study is based on numerical simulations performed with 2D model molecules, and analyzed in the perspective of the renown Lochfrass and Bond-Softening models.","We give a new interpretation of the observed nuclear wave packet dynamics, with a focus on the phase of the bond oscillations.","Our simulations also reveal intricate features in the field-induced nuclear motion that are not accounted for by existing models.","Our analyses assign these features to strong dynamic correlations between the active electron and the nuclei, which significantly depend on the carrier envelope phase of the pulse, even for relatively ``long'' pulses, which should make them experimentally observable."],"url":"http://arxiv.org/abs/2403.08034v1","category":"physics.chem-ph"}
{"created":"2024-03-12 19:04:52","title":"Score-based mechanisms","abstract":"We propose a mechanism design framework that incorporates both soft information, which can be freely manipulated, and semi-hard information, which entails a cost for falsification. The framework captures various contexts such as school choice, public housing, organ transplant and manipulations of classification algorithms. We first provide a canonical class of mechanisms for these settings. The key idea is to treat the submission of hard information as an observable and payoff-relevant action and the contractible part of the mechanism as a mapping from submitted scores to a distribution over decisions (a score-based decision rule). Each type report triggers a distribution over score submission requests and a distribution over decision rules. We provide conditions under which score-based mechanisms are without loss of generality. In other words, situations under which the agent does not make any type reports and decides without a mediator what score to submit in a score-based decision rule. We proceed to characterize optimal approval mechanisms in the presence of manipulable hard information. In several leading settings optimal mechanisms are score-based (and thus do not rely on soft information) and involve costly screening. The solution methodology we employ is suitable both for concave cost functions and quadratic costs and is applicable to a wide range of contexts in economics and in computer science.","sentences":["We propose a mechanism design framework that incorporates both soft information, which can be freely manipulated, and semi-hard information, which entails a cost for falsification.","The framework captures various contexts such as school choice, public housing, organ transplant and manipulations of classification algorithms.","We first provide a canonical class of mechanisms for these settings.","The key idea is to treat the submission of hard information as an observable and payoff-relevant action and the contractible part of the mechanism as a mapping from submitted scores to a distribution over decisions (a score-based decision rule).","Each type report triggers a distribution over score submission requests and a distribution over decision rules.","We provide conditions under which score-based mechanisms are without loss of generality.","In other words, situations under which the agent does not make any type reports and decides without a mediator what score to submit in a score-based decision rule.","We proceed to characterize optimal approval mechanisms in the presence of manipulable hard information.","In several leading settings optimal mechanisms are score-based (and thus do not rely on soft information) and involve costly screening.","The solution methodology we employ is suitable both for concave cost functions and quadratic costs and is applicable to a wide range of contexts in economics and in computer science."],"url":"http://arxiv.org/abs/2403.08031v1","category":"econ.TH"}
{"created":"2024-03-12 18:41:03","title":"An asymptotic Grad-Shafranov equation for quasisymmetric stellarators","abstract":"A first-order model is derived for quasisymmetric stellarators where the vacuum field due to coils is dominant, but plasma-current-induced terms are not negligible and can contribute to magnetic differential equations, with $\\beta$ of the order of the ratio of induced to vacuum fields. Under these assumptions, it is proven that the aspect ratio must be large and a simple expression can be obtained for the lowest-order vacuum field. The first-order correction, which involves both vacuum and current-driven fields, is governed by a Grad-Shafranov equation and the requirement that flux surfaces exist. These two equations are not always consistent, and so this model is generally overconstrained, but special solutions exist that satisfy both equations simultaneously. One family of such solutions are the first-order near-axis solutions. Thus, the first-order near-axis model is a subset of the model presented here. Several other solutions outside the scope of the near-axis model are also found. A case study comparing one such solution to a VMEC-generated solution shows good agreement.","sentences":["A first-order model is derived for quasisymmetric stellarators where the vacuum field due to coils is dominant, but plasma-current-induced terms are not negligible and can contribute to magnetic differential equations, with $\\beta$ of the order of the ratio of induced to vacuum fields.","Under these assumptions, it is proven that the aspect ratio must be large and a simple expression can be obtained for the lowest-order vacuum field.","The first-order correction, which involves both vacuum and current-driven fields, is governed by a Grad-Shafranov equation and the requirement that flux surfaces exist.","These two equations are not always consistent, and so this model is generally overconstrained, but special solutions exist that satisfy both equations simultaneously.","One family of such solutions are the first-order near-axis solutions.","Thus, the first-order near-axis model is a subset of the model presented here.","Several other solutions outside the scope of the near-axis model are also found.","A case study comparing one such solution to a VMEC-generated solution shows good agreement."],"url":"http://arxiv.org/abs/2403.08022v1","category":"physics.plasm-ph"}
{"created":"2024-03-12 18:38:29","title":"Top anomalous chromomagnetic dipole moment in the Bestest Little Higgs Model","abstract":"We investigate the anomalous Chromomagnetic Dipole Moment (CMDM), $\\hat{\\mu}^{BLHM}_t$ of the top quark in the Bestest Little Higgs Model (BLHM). We include new interactions with the involvement of the extended CKM matrix of the BLHM and we explore most of the allowed parameter space, obtaining multiple CMDM in the range of $10^{-4}-10^{-2}$. We consider experimental and model parameter uncertainties to integrate them into all our calculations using a Monte Carlo method. This enables us to determine the extent to which deviations arising from experimental errors can be accommodated within the statistical errors of the model and which relate to the physics framework of the BLHM, guiding future theory, phenomenological, and experimental research.","sentences":["We investigate the anomalous Chromomagnetic Dipole Moment (CMDM), $\\hat{\\mu}^{BLHM}_t$ of the top quark in the Bestest Little Higgs Model (BLHM).","We include new interactions with the involvement of the extended CKM matrix of the BLHM and we explore most of the allowed parameter space, obtaining multiple CMDM in the range of $10^{-4}-10^{-2}$. We consider experimental and model parameter uncertainties to integrate them into all our calculations using a Monte Carlo method.","This enables us to determine the extent to which deviations arising from experimental errors can be accommodated within the statistical errors of the model and which relate to the physics framework of the BLHM, guiding future theory, phenomenological, and experimental research."],"url":"http://arxiv.org/abs/2403.08021v1","category":"hep-ph"}
{"created":"2024-03-12 18:28:13","title":"Aedes aegypti Egg Counting with Neural Networks for Object Detection","abstract":"Aedes aegypti is still one of the main concerns when it comes to disease vectors. Among the many ways to deal with it, there are important protocols that make use of egg numbers in ovitraps to calculate indices, such as the LIRAa and the Breteau Index, which can provide information on predictable outbursts and epidemics. Also, there are many research lines that require egg numbers, specially when mass production of mosquitoes is needed. Egg counting is a laborious and error-prone task that can be automated via computer vision-based techniques, specially deep learning-based counting with object detection. In this work, we propose a new dataset comprising field and laboratory eggs, along with test results of three neural networks applied to the task: Faster R-CNN, Side-Aware Boundary Localization and FoveaBox.","sentences":["Aedes aegypti is still one of the main concerns when it comes to disease vectors.","Among the many ways to deal with it, there are important protocols that make use of egg numbers in ovitraps to calculate indices, such as the LIRAa and the Breteau Index, which can provide information on predictable outbursts and epidemics.","Also, there are many research lines that require egg numbers, specially when mass production of mosquitoes is needed.","Egg counting is a laborious and error-prone task that can be automated via computer vision-based techniques, specially deep learning-based counting with object detection.","In this work, we propose a new dataset comprising field and laboratory eggs, along with test results of three neural networks applied to the task: Faster R-CNN, Side-Aware Boundary Localization and FoveaBox."],"url":"http://arxiv.org/abs/2403.08016v1","category":"eess.IV"}
{"created":"2024-03-12 18:25:10","title":"Supervised Time Series Classification for Anomaly Detection in Subsea Engineering","abstract":"Time series classification is of significant importance in monitoring structural systems. In this work, we investigate the use of supervised machine learning classification algorithms on simulated data based on a physical system with two states: Intact and Broken. We provide a comprehensive discussion of the preprocessing of temporal data, using measures of statistical dispersion and dimension reduction techniques. We present an intuitive baseline method and discuss its efficiency. We conclude with a comparison of the various methods based on different performance metrics, showing the advantage of using machine learning techniques as a tool in decision making.","sentences":["Time series classification is of significant importance in monitoring structural systems.","In this work, we investigate the use of supervised machine learning classification algorithms on simulated data based on a physical system with two states: Intact and Broken.","We provide a comprehensive discussion of the preprocessing of temporal data, using measures of statistical dispersion and dimension reduction techniques.","We present an intuitive baseline method and discuss its efficiency.","We conclude with a comparison of the various methods based on different performance metrics, showing the advantage of using machine learning techniques as a tool in decision making."],"url":"http://arxiv.org/abs/2403.08013v1","category":"cs.LG"}
{"created":"2024-03-12 18:19:47","title":"Debatrix: Multi-dimensinal Debate Judge with Iterative Chronological Analysis Based on LLM","abstract":"How can we construct an automated debate judge to evaluate an extensive, vibrant, multi-turn debate? This task is challenging, as judging a debate involves grappling with lengthy texts, intricate argument relationships, and multi-dimensional assessments. At the same time, current research mainly focuses on short dialogues, rarely touching upon the evaluation of an entire debate. In this paper, by leveraging Large Language Models (LLMs), we propose Debatrix, which makes the analysis and assessment of multi-turn debates more aligned with majority preferences. Specifically, Debatrix features a vertical, iterative chronological analysis and a horizontal, multi-dimensional evaluation collaboration. To align with real-world debate scenarios, we introduced the PanelBench benchmark, comparing our system's performance to actual debate outcomes. The findings indicate a notable enhancement over directly using LLMs for debate evaluation. Source code and benchmark data are available online at https://github.com/ljcleo/Debatrix .","sentences":["How can we construct an automated debate judge to evaluate an extensive, vibrant, multi-turn debate?","This task is challenging, as judging a debate involves grappling with lengthy texts, intricate argument relationships, and multi-dimensional assessments.","At the same time, current research mainly focuses on short dialogues, rarely touching upon the evaluation of an entire debate.","In this paper, by leveraging Large Language Models (LLMs), we propose Debatrix, which makes the analysis and assessment of multi-turn debates more aligned with majority preferences.","Specifically, Debatrix features a vertical, iterative chronological analysis and a horizontal, multi-dimensional evaluation collaboration.","To align with real-world debate scenarios, we introduced the PanelBench benchmark, comparing our system's performance to actual debate outcomes.","The findings indicate a notable enhancement over directly using LLMs for debate evaluation.","Source code and benchmark data are available online at https://github.com/ljcleo/Debatrix ."],"url":"http://arxiv.org/abs/2403.08010v1","category":"cs.CL"}
{"created":"2024-03-12 18:12:02","title":"Training Small Multimodal Models to Bridge Biomedical Competency Gap: A Case Study in Radiology Imaging","abstract":"The scaling laws and extraordinary performance of large foundation models motivate the development and utilization of such large models in biomedicine. However, despite early promising results on some biomedical benchmarks, there are still major challenges that need to be addressed before these models can be used in real-world applications. Frontier models such as GPT-4V still have major competency gaps in multimodal capabilities for biomedical applications. Moreover, pragmatic issues such as access, cost, latency, and compliance make it hard for clinicians to use privately-hosted state-of-the-art large models directly on private patient data. In this paper, we explore training open-source small multimodal models (SMMs) to bridge biomedical competency gaps for unmet clinical needs. To maximize data efficiency, we adopt a modular approach by incorporating state-of-the-art pre-trained models for image and text modalities, and focusing on training a lightweight adapter to ground each modality to the text embedding space. We conduct a comprehensive study of this approach on radiology imaging. For training, we assemble a large dataset with over 1 million image-text pairs. For evaluation, we propose a clinically driven novel approach using GPT-4 and demonstrate its parity with expert evaluation. We also study grounding qualitatively using attention. For best practice, we conduct a systematic ablation study on various choices in data engineering and multimodal training. The resulting LLaVA-Rad (7B) model attains state-of-the-art results on radiology tasks such as report generation and cross-modal retrieval, even outperforming much larger models such as GPT-4V and Med-PaLM M (84B). LLaVA-Rad is fast and can be run on a single V100 GPU in private settings, offering a promising state-of-the-art tool for real-world clinical applications.","sentences":["The scaling laws and extraordinary performance of large foundation models motivate the development and utilization of such large models in biomedicine.","However, despite early promising results on some biomedical benchmarks, there are still major challenges that need to be addressed before these models can be used in real-world applications.","Frontier models such as GPT-4V still have major competency gaps in multimodal capabilities for biomedical applications.","Moreover, pragmatic issues such as access, cost, latency, and compliance make it hard for clinicians to use privately-hosted state-of-the-art large models directly on private patient data.","In this paper, we explore training open-source small multimodal models (SMMs) to bridge biomedical competency gaps for unmet clinical needs.","To maximize data efficiency, we adopt a modular approach by incorporating state-of-the-art pre-trained models for image and text modalities, and focusing on training a lightweight adapter to ground each modality to the text embedding space.","We conduct a comprehensive study of this approach on radiology imaging.","For training, we assemble a large dataset with over 1 million image-text pairs.","For evaluation, we propose a clinically driven novel approach using GPT-4 and demonstrate its parity with expert evaluation.","We also study grounding qualitatively using attention.","For best practice, we conduct a systematic ablation study on various choices in data engineering and multimodal training.","The resulting LLaVA-Rad (7B) model attains state-of-the-art results on radiology tasks such as report generation and cross-modal retrieval, even outperforming much larger models such as GPT-4V and Med-PaLM M (84B).","LLaVA-Rad is fast and can be run on a single V100 GPU in private settings, offering a promising state-of-the-art tool for real-world clinical applications."],"url":"http://arxiv.org/abs/2403.08002v1","category":"cs.CL"}
{"created":"2024-03-12 18:05:38","title":"Fast-Forward Reality: Authoring Error-Free Context-Aware Policies with Real-Time Unit Tests in Extended Reality","abstract":"Advances in ubiquitous computing have enabled end-user authoring of context-aware policies (CAPs) that control smart devices based on specific contexts of the user and environment. However, authoring CAPs accurately and avoiding run-time errors is challenging for end-users as it is difficult to foresee CAP behaviors under complex real-world conditions. We propose Fast-Forward Reality, an Extended Reality (XR) based authoring workflow that enables end-users to iteratively author and refine CAPs by validating their behaviors via simulated unit test cases. We develop a computational approach to automatically generate test cases based on the authored CAP and the user's context history. Our system delivers each test case with immersive visualizations in XR, facilitating users to verify the CAP behavior and identify necessary refinements. We evaluated Fast-Forward Reality in a user study (N=12). Our authoring and validation process improved the accuracy of CAPs and the users provided positive feedback on the system usability.","sentences":["Advances in ubiquitous computing have enabled end-user authoring of context-aware policies (CAPs) that control smart devices based on specific contexts of the user and environment.","However, authoring CAPs accurately and avoiding run-time errors is challenging for end-users as it is difficult to foresee CAP behaviors under complex real-world conditions.","We propose Fast-Forward Reality, an Extended Reality (XR) based authoring workflow that enables end-users to iteratively author and refine CAPs by validating their behaviors via simulated unit test cases.","We develop a computational approach to automatically generate test cases based on the authored CAP and the user's context history.","Our system delivers each test case with immersive visualizations in XR, facilitating users to verify the CAP behavior and identify necessary refinements.","We evaluated Fast-Forward Reality in a user study (N=12).","Our authoring and validation process improved the accuracy of CAPs and the users provided positive feedback on the system usability."],"url":"http://arxiv.org/abs/2403.07997v1","category":"cs.HC"}
{"created":"2024-03-12 18:03:52","title":"TriOS Schwarzschild Orbit Modeling: Robustness of Parameter Inference for Masses and Shapes of Triaxial Galaxies with Supermassive Black Holes","abstract":"Evidence for the majority of the supermassive black holes in the local universe has been obtained dynamically from stellar motions with the Schwarzschild orbit superposition method. However, there have been only a handful of studies using simulated data to examine the ability of this method to reliably recover known input black hole masses $M_{BH}$ and other galaxy parameters. Here we conduct a comprehensive assessment of the reliability of the triaxial Schwarzschild method at $\\textit{simultaneously}$ determining $M_{BH}$, stellar mass-to-light ratio $M^{*}/L$, dark matter mass, and three intrinsic triaxial shape parameters of simulated galaxies. For each of 25 rounds of mock observations using simulated stellar kinematics and the $\\texttt{TriOS}$ code, we derive best-fitting parameters and confidence intervals after a full search in the 6D parameter space with our likelihood-based model inference scheme. The two key mass parameters, $M_{BH}$ and $M^{*}/L$, are recovered within the 68% confidence interval, and other parameters are recovered between 68% and 95% confidence intervals. The spatially varying velocity anisotropy of the stellar orbits is also well recovered. We explore whether the goodness-of-fit measure used for galaxy model selection in our pipeline is biased by variable complexity across the 6D parameter space. In our tests, adding a penalty term to the likelihood measure either makes little difference, or worsens the recovery in some cases.","sentences":["Evidence for the majority of the supermassive black holes in the local universe has been obtained dynamically from stellar motions with the Schwarzschild orbit superposition method.","However, there have been only a handful of studies using simulated data to examine the ability of this method to reliably recover known input black hole masses $M_{BH}$ and other galaxy parameters.","Here we conduct a comprehensive assessment of the reliability of the triaxial Schwarzschild method at $\\textit{simultaneously}$ determining $M_{BH}$, stellar mass-to-light ratio $M^{*}/L$, dark matter mass, and three intrinsic triaxial shape parameters of simulated galaxies.","For each of 25 rounds of mock observations using simulated stellar kinematics and the $\\texttt{TriOS}$ code, we derive best-fitting parameters and confidence intervals after a full search in the 6D parameter space with our likelihood-based model inference scheme.","The two key mass parameters, $M_{BH}$ and $M^{*}/L$, are recovered within the 68% confidence interval, and other parameters are recovered between 68% and 95% confidence intervals.","The spatially varying velocity anisotropy of the stellar orbits is also well recovered.","We explore whether the goodness-of-fit measure used for galaxy model selection in our pipeline is biased by variable complexity across the 6D parameter space.","In our tests, adding a penalty term to the likelihood measure either makes little difference, or worsens the recovery in some cases."],"url":"http://arxiv.org/abs/2403.07996v1","category":"astro-ph.GA"}
{"created":"2024-03-12 18:00:29","title":"Configuration and EMT Simulation of the 240-bus MiniWECC System Integrating Offshore Wind Farms (OWFs)","abstract":"As offshore wind farms (OWFs) become increasingly prevalent in Northern California and Southern Oregon, they introduce faster dynamics into the Western Electricity Coordinating Council (WECC) system, reshaping its dynamic behavior. Accordingly, electromagnetic transient (EMT) simulation is essential to assess high frequency dynamics of the WECC system with integrated OWFs. Against this background, this paper presents the integration of detailed dynamic models of OWFs into a 240-bus miniWECC system in PSCAD software. The sequential initialization technique is employed to facilitate the smooth initiation of a large-scale system in an EMT simulation. The performance of the configured model is assessed under wind speed variations and grounded faults, demonstrating the effectiveness of the miniWECC system with OWFs. This system serves as a valuable basic use case for validating the fast dynamic performance of future WECC systems with high penetration of wind energy.","sentences":["As offshore wind farms (OWFs) become increasingly prevalent in Northern California and Southern Oregon, they introduce faster dynamics into the Western Electricity Coordinating Council (WECC) system, reshaping its dynamic behavior.","Accordingly, electromagnetic transient (EMT) simulation is essential to assess high frequency dynamics of the WECC system with integrated OWFs.","Against this background, this paper presents the integration of detailed dynamic models of OWFs into a 240-bus miniWECC system in PSCAD software.","The sequential initialization technique is employed to facilitate the smooth initiation of a large-scale system in an EMT simulation.","The performance of the configured model is assessed under wind speed variations and grounded faults, demonstrating the effectiveness of the miniWECC system with OWFs.","This system serves as a valuable basic use case for validating the fast dynamic performance of future WECC systems with high penetration of wind energy."],"url":"http://arxiv.org/abs/2403.07988v1","category":"eess.SY"}
{"created":"2024-03-12 18:00:12","title":"A sub-solar metallicity on the ultra-short period planet HIP 65Ab","abstract":"Studying and understanding the physical and chemical processes that govern hot Jupiters gives us insights on the formation of these giant planets. Having a constraint on the molecular composition of their atmosphere can help us pinpoint their evolution timeline. Namely, the metal enrichment and carbon-to-oxygen ratio can give us information about where in the protoplanetary disk a giant planet may have accreted its envelope, and subsequently, indicate if it went through migration. Here we present the first analysis of the atmosphere of the hot Jupiter HIP 65Ab. Using near-infrared high-resolution observations from the IGRINS spectrograph, we detect H$_2$O and CO absorption in the dayside atmosphere of HIP 65Ab. Using a high-resolution retrieval framework, we find a CO abundance of log(CO) = $-3.85^{+0.33}_{-0.36}$, which is slightly under abundant with expectation from solar composition models. We also recover a low water abundance of log(H$_2$O) = $-4.42\\pm{0.18}$, depleted by 1 order of magnitude relative to a solar-like composition. Upper limits on the abundance of all other relevant major carbon- and oxygen-bearing molecules are also obtained. Overall, our results are consistent with a sub-stellar metallicity but slightly elevated C/O. Such a composition may indicate that HIP 65Ab accreted its envelope from beyond the water snowline and underwent a disk-free migration to its current location. Alternatively, some of the oxygen on HIP 65Ab could be condensed out of the atmosphere, in which case the observed gas-phase abundances would not reflect the true bulk envelope composition.","sentences":["Studying and understanding the physical and chemical processes that govern hot Jupiters gives us insights on the formation of these giant planets.","Having a constraint on the molecular composition of their atmosphere can help us pinpoint their evolution timeline.","Namely, the metal enrichment and carbon-to-oxygen ratio can give us information about where in the protoplanetary disk a giant planet may have accreted its envelope, and subsequently, indicate if it went through migration.","Here we present the first analysis of the atmosphere of the hot Jupiter HIP 65Ab.","Using near-infrared high-resolution observations from the IGRINS spectrograph, we detect H$_2$O and CO absorption in the dayside atmosphere of HIP 65Ab.","Using a high-resolution retrieval framework, we find a CO abundance of log(CO)","= $-3.85^{+0.33}_{-0.36}$, which is slightly under abundant with expectation from solar composition models.","We also recover a low water abundance of log(H$_2$O)","= $-4.42\\pm{0.18}$, depleted by 1 order of magnitude relative to a solar-like composition.","Upper limits on the abundance of all other relevant major carbon- and oxygen-bearing molecules are also obtained.","Overall, our results are consistent with a sub-stellar metallicity but slightly elevated C/O.","Such a composition may indicate that HIP 65Ab accreted its envelope from beyond the water snowline and underwent a disk-free migration to its current location.","Alternatively, some of the oxygen on HIP 65Ab could be condensed out of the atmosphere, in which case the observed gas-phase abundances would not reflect the true bulk envelope composition."],"url":"http://arxiv.org/abs/2403.07983v1","category":"astro-ph.EP"}
{"created":"2024-03-12 17:52:56","title":"Flexible Non-intrusive Dynamic Instrumentation for WebAssembly","abstract":"A key strength of managed runtimes over hardware is the ability to gain detailed insight into the dynamic execution of programs with instrumentation. Analyses such as code coverage, execution frequency, tracing, and debugging, are all made easier in a virtual setting. As a portable, low-level bytecode, WebAssembly offers inexpensive in-process sandboxing with high performance. Yet to date, Wasm engines have not offered much insight into executing programs, supporting at best bytecode-level stepping and basic source maps, but no instrumentation capabilities. In this paper, we show the first non-intrusive dynamic instrumentation system for WebAssembly in the open-source Wizard Research Engine. Our innovative design offers a flexible, complete hierarchy of instrumentation primitives that support building high-level, complex analyses in terms of low-level, programmable probes. In contrast to emulation or machine code instrumentation, injecting probes at the bytecode level increases expressiveness and vastly simplifies the implementation by reusing the engine's JIT compiler, interpreter, and deoptimization mechanism rather than building new ones. Wizard supports both dynamic instrumentation insertion and removal while providing consistency guarantees, which is key to composing multiple analyses without interference. We detail a fully-featured implementation in a high-performance multi-tier Wasm engine, show novel optimizations specifically designed to minimize instrumentation overhead, and evaluate performance characteristics under load from various analyses. This design is well-suited for production engine adoption as probes can be implemented to have no impact on production performance when not in use.","sentences":["A key strength of managed runtimes over hardware is the ability to gain detailed insight into the dynamic execution of programs with instrumentation.","Analyses such as code coverage, execution frequency, tracing, and debugging, are all made easier in a virtual setting.","As a portable, low-level bytecode, WebAssembly offers inexpensive in-process sandboxing with high performance.","Yet to date, Wasm engines have not offered much insight into executing programs, supporting at best bytecode-level stepping and basic source maps, but no instrumentation capabilities.","In this paper, we show the first non-intrusive dynamic instrumentation system for WebAssembly in the open-source Wizard Research Engine.","Our innovative design offers a flexible, complete hierarchy of instrumentation primitives that support building high-level, complex analyses in terms of low-level, programmable probes.","In contrast to emulation or machine code instrumentation, injecting probes at the bytecode level increases expressiveness and vastly simplifies the implementation by reusing the engine's JIT compiler, interpreter, and deoptimization mechanism rather than building new ones.","Wizard supports both dynamic instrumentation insertion and removal while providing consistency guarantees, which is key to composing multiple analyses without interference.","We detail a fully-featured implementation in a high-performance multi-tier Wasm engine, show novel optimizations specifically designed to minimize instrumentation overhead, and evaluate performance characteristics under load from various analyses.","This design is well-suited for production engine adoption as probes can be implemented to have no impact on production performance when not in use."],"url":"http://arxiv.org/abs/2403.07973v1","category":"cs.PL"}
{"created":"2024-03-13 17:51:02","title":"Neural reproducing kernel Banach spaces and representer theorems for deep networks","abstract":"Studying the function spaces defined by neural networks helps to understand the corresponding learning models and their inductive bias. While in some limits neural networks correspond to function spaces that are reproducing kernel Hilbert spaces, these regimes do not capture the properties of the networks used in practice. In contrast, in this paper we show that deep neural networks define suitable reproducing kernel Banach spaces.   These spaces are equipped with norms that enforce a form of sparsity, enabling them to adapt to potential latent structures within the input data and their representations. In particular, leveraging the theory of reproducing kernel Banach spaces, combined with variational results, we derive representer theorems that justify the finite architectures commonly employed in applications. Our study extends analogous results for shallow networks and can be seen as a step towards considering more practically plausible neural architectures.","sentences":["Studying the function spaces defined by neural networks helps to understand the corresponding learning models and their inductive bias.","While in some limits neural networks correspond to function spaces that are reproducing kernel Hilbert spaces, these regimes do not capture the properties of the networks used in practice.","In contrast, in this paper we show that deep neural networks define suitable reproducing kernel Banach spaces.   ","These spaces are equipped with norms that enforce a form of sparsity, enabling them to adapt to potential latent structures within the input data and their representations.","In particular, leveraging the theory of reproducing kernel Banach spaces, combined with variational results, we derive representer theorems that justify the finite architectures commonly employed in applications.","Our study extends analogous results for shallow networks and can be seen as a step towards considering more practically plausible neural architectures."],"url":"http://arxiv.org/abs/2403.08750v1","category":"stat.ML"}
{"created":"2024-03-13 16:16:20","title":"Self-Supervised Learning for Covariance Estimation","abstract":"We consider the use of deep learning for covariance estimation. We propose to globally learn a neural network that will then be applied locally at inference time. Leveraging recent advancements in self-supervised foundational models, we train the network without any labeling by simply masking different samples and learning to predict their covariance given their surrounding neighbors. The architecture is based on the popular attention mechanism. Its main advantage over classical methods is the automatic exploitation of global characteristics without any distributional assumptions or regularization. It can be pre-trained as a foundation model and then be repurposed for various downstream tasks, e.g., adaptive target detection in radar or hyperspectral imagery.","sentences":["We consider the use of deep learning for covariance estimation.","We propose to globally learn a neural network that will then be applied locally at inference time.","Leveraging recent advancements in self-supervised foundational models, we train the network without any labeling by simply masking different samples and learning to predict their covariance given their surrounding neighbors.","The architecture is based on the popular attention mechanism.","Its main advantage over classical methods is the automatic exploitation of global characteristics without any distributional assumptions or regularization.","It can be pre-trained as a foundation model and then be repurposed for various downstream tasks, e.g., adaptive target detection in radar or hyperspectral imagery."],"url":"http://arxiv.org/abs/2403.08662v1","category":"eess.SP"}
{"created":"2024-03-13 14:24:09","title":"Consistent Prompting for Rehearsal-Free Continual Learning","abstract":"Continual learning empowers models to adapt autonomously to the ever-changing environment or data streams without forgetting old knowledge. Prompt-based approaches are built on frozen pre-trained models to learn the task-specific prompts and classifiers efficiently. Existing prompt-based methods are inconsistent between training and testing, limiting their effectiveness. Two types of inconsistency are revealed. Test predictions are made from all classifiers while training only focuses on the current task classifier without holistic alignment, leading to Classifier inconsistency. Prompt inconsistency indicates that the prompt selected during testing may not correspond to the one associated with this task during training. In this paper, we propose a novel prompt-based method, Consistent Prompting (CPrompt), for more aligned training and testing. Specifically, all existing classifiers are exposed to prompt training, resulting in classifier consistency learning. In addition, prompt consistency learning is proposed to enhance prediction robustness and boost prompt selection accuracy. Our Consistent Prompting surpasses its prompt-based counterparts and achieves state-of-the-art performance on multiple continual learning benchmarks. Detailed analysis shows that improvements come from more consistent training and testing.","sentences":["Continual learning empowers models to adapt autonomously to the ever-changing environment or data streams without forgetting old knowledge.","Prompt-based approaches are built on frozen pre-trained models to learn the task-specific prompts and classifiers efficiently.","Existing prompt-based methods are inconsistent between training and testing, limiting their effectiveness.","Two types of inconsistency are revealed.","Test predictions are made from all classifiers while training only focuses on the current task classifier without holistic alignment, leading to Classifier inconsistency.","Prompt inconsistency indicates that the prompt selected during testing may not correspond to the one associated with this task during training.","In this paper, we propose a novel prompt-based method, Consistent Prompting (CPrompt), for more aligned training and testing.","Specifically, all existing classifiers are exposed to prompt training, resulting in classifier consistency learning.","In addition, prompt consistency learning is proposed to enhance prediction robustness and boost prompt selection accuracy.","Our Consistent Prompting surpasses its prompt-based counterparts and achieves state-of-the-art performance on multiple continual learning benchmarks.","Detailed analysis shows that improvements come from more consistent training and testing."],"url":"http://arxiv.org/abs/2403.08568v1","category":"cs.CV"}
{"created":"2024-03-13 05:46:23","title":"A posteriori error estimates for the Generalized Burgers-Huxley equation with weakly singular kernels","abstract":"This paper explores the residual based a posteriori error estimations for the generalized Burgers-Huxley equation (GBHE) featuring weakly singular kernels. Initially, we present a reliable and efficient error estimator for both the stationary GBHE and the semi-discrete GBHE with memory, utilizing the discontinuous Galerkin finite element method (DGFEM) in spatial dimensions. Additionally, employing backward Euler and Crank Nicolson discretization in the temporal domain and DGFEM in spatial dimensions, we introduce an estimator for the fully discrete GBHE, taking into account the influence of past history. The paper also establishes optimal $L^2$ error estimates for both the stationary GBHE and GBHE. Ultimately, we validate the effectiveness of the proposed error estimator through numerical results, demonstrating its efficacy in an adaptive refinement strategy.","sentences":["This paper explores the residual based a posteriori error estimations for the generalized Burgers-Huxley equation (GBHE) featuring weakly singular kernels.","Initially, we present a reliable and efficient error estimator for both the stationary GBHE and the semi-discrete GBHE with memory, utilizing the discontinuous Galerkin finite element method (DGFEM) in spatial dimensions.","Additionally, employing backward Euler and Crank Nicolson discretization in the temporal domain and DGFEM in spatial dimensions, we introduce an estimator for the fully discrete GBHE, taking into account the influence of past history.","The paper also establishes optimal $L^2$ error estimates for both the stationary GBHE and GBHE.","Ultimately, we validate the effectiveness of the proposed error estimator through numerical results, demonstrating its efficacy in an adaptive refinement strategy."],"url":"http://arxiv.org/abs/2403.08269v1","category":"math.NA"}
{"created":"2024-03-13 04:11:41","title":"Empowering Robotics with Large Language Models: osmAG Map Comprehension with LLMs","abstract":"Recently, Large Language Models (LLMs) have demonstrated great potential in robotic applications by providing essential general knowledge for situations that can not be pre-programmed beforehand. Generally speaking, mobile robots need to understand maps to execute tasks such as localization or navigation. In this letter, we address the problem of enabling LLMs to comprehend Area Graph, a text-based map representation, in order to enhance their applicability in the field of mobile robotics. Area Graph is a hierarchical, topometric semantic map representation utilizing polygons to demark areas such as rooms, corridors or buildings. In contrast to commonly used map representations, such as occupancy grid maps or point clouds, osmAG (Area Graph in OpensStreetMap format) is stored in a XML textual format naturally readable by LLMs. Furthermore, conventional robotic algorithms such as localization and path planning are compatible with osmAG, facilitating this map representation comprehensible by LLMs, traditional robotic algorithms and humans. Our experiments show that with a proper map representation, LLMs possess the capability to understand maps and answer queries based on that understanding. Following simple fine-tuning of LLaMA2 models, it surpassed ChatGPT-3.5 in tasks involving topology and hierarchy understanding. Our dataset, dataset generation code, fine-tuned LoRA adapters can be accessed at https://github.com/xiefujing/LLM-osmAG-Comprehension.","sentences":["Recently, Large Language Models (LLMs) have demonstrated great potential in robotic applications by providing essential general knowledge for situations that can not be pre-programmed beforehand.","Generally speaking, mobile robots need to understand maps to execute tasks such as localization or navigation.","In this letter, we address the problem of enabling LLMs to comprehend Area Graph, a text-based map representation, in order to enhance their applicability in the field of mobile robotics.","Area Graph is a hierarchical, topometric semantic map representation utilizing polygons to demark areas such as rooms, corridors or buildings.","In contrast to commonly used map representations, such as occupancy grid maps or point clouds, osmAG (Area Graph in OpensStreetMap format) is stored in a XML textual format naturally readable by LLMs.","Furthermore, conventional robotic algorithms such as localization and path planning are compatible with osmAG, facilitating this map representation comprehensible by LLMs, traditional robotic algorithms and humans.","Our experiments show that with a proper map representation, LLMs possess the capability to understand maps and answer queries based on that understanding.","Following simple fine-tuning of LLaMA2 models, it surpassed ChatGPT-3.5 in tasks involving topology and hierarchy understanding.","Our dataset, dataset generation code, fine-tuned LoRA adapters can be accessed at https://github.com/xiefujing/LLM-osmAG-Comprehension."],"url":"http://arxiv.org/abs/2403.08228v1","category":"cs.RO"}
{"created":"2024-03-13 02:55:27","title":"Learnable Community-Aware Transformer for Brain Connectome Analysis with Token Clustering","abstract":"Neuroscientific research has revealed that the complex brain network can be organized into distinct functional communities, each characterized by a cohesive group of regions of interest (ROIs) with strong interconnections. These communities play a crucial role in comprehending the functional organization of the brain and its implications for neurological conditions, including Autism Spectrum Disorder (ASD) and biological differences, such as in gender. Traditional models have been constrained by the necessity of predefined community clusters, limiting their flexibility and adaptability in deciphering the brain's functional organization. Furthermore, these models were restricted by a fixed number of communities, hindering their ability to accurately represent the brain's dynamic nature. In this study, we present a token clustering brain transformer-based model ($\\texttt{TC-BrainTF}$) for joint community clustering and classification. Our approach proposes a novel token clustering (TC) module based on the transformer architecture, which utilizes learnable prompt tokens with orthogonal loss where each ROI embedding is projected onto the prompt embedding space, effectively clustering ROIs into communities and reducing the dimensions of the node representation via merging with communities. Our results demonstrate that our learnable community-aware model $\\texttt{TC-BrainTF}$ offers improved accuracy in identifying ASD and classifying genders through rigorous testing on ABIDE and HCP datasets. Additionally, the qualitative analysis on $\\texttt{TC-BrainTF}$ has demonstrated the effectiveness of the designed TC module and its relevance to neuroscience interpretations.","sentences":["Neuroscientific research has revealed that the complex brain network can be organized into distinct functional communities, each characterized by a cohesive group of regions of interest (ROIs) with strong interconnections.","These communities play a crucial role in comprehending the functional organization of the brain and its implications for neurological conditions, including Autism Spectrum Disorder (ASD) and biological differences, such as in gender.","Traditional models have been constrained by the necessity of predefined community clusters, limiting their flexibility and adaptability in deciphering the brain's functional organization.","Furthermore, these models were restricted by a fixed number of communities, hindering their ability to accurately represent the brain's dynamic nature.","In this study, we present a token clustering brain transformer-based model ($\\texttt{TC-BrainTF}$) for joint community clustering and classification.","Our approach proposes a novel token clustering (TC) module based on the transformer architecture, which utilizes learnable prompt tokens with orthogonal loss where each ROI embedding is projected onto the prompt embedding space, effectively clustering ROIs into communities and reducing the dimensions of the node representation via merging with communities.","Our results demonstrate that our learnable community-aware model $\\texttt{TC-BrainTF}$ offers improved accuracy in identifying ASD and classifying genders through rigorous testing on ABIDE and HCP datasets.","Additionally, the qualitative analysis on $\\texttt{TC-BrainTF}$ has demonstrated the effectiveness of the designed TC module and its relevance to neuroscience interpretations."],"url":"http://arxiv.org/abs/2403.08203v1","category":"q-bio.NC"}
{"created":"2024-03-13 02:41:53","title":"SpeechColab Leaderboard: An Open-Source Platform for Automatic Speech Recognition Evaluation","abstract":"In the wake of the surging tide of deep learning over the past decade, Automatic Speech Recognition (ASR) has garnered substantial attention, leading to the emergence of numerous publicly accessible ASR systems that are actively being integrated into our daily lives. Nonetheless, the impartial and replicable evaluation of these ASR systems encounters challenges due to various crucial subtleties. In this paper we introduce the SpeechColab Leaderboard, a general-purpose, open-source platform designed for ASR evaluation. With this platform: (i) We report a comprehensive benchmark, unveiling the current state-of-the-art panorama for ASR systems, covering both open-source models and industrial commercial services. (ii) We quantize how distinct nuances in the scoring pipeline influence the final benchmark outcomes. These include nuances related to capitalization, punctuation, interjection, contraction, synonym usage, compound words, etc. These issues have gained prominence in the context of the transition towards an End-to-End future. (iii) We propose a practical modification to the conventional Token-Error-Rate (TER) evaluation metric, with inspirations from Kolmogorov complexity and Normalized Information Distance (NID). This adaptation, called modified-TER (mTER), achieves proper normalization and symmetrical treatment of reference and hypothesis. By leveraging this platform as a large-scale testing ground, this study demonstrates the robustness and backward compatibility of mTER when compared to TER. The SpeechColab Leaderboard is accessible at https://github.com/SpeechColab/Leaderboard","sentences":["In the wake of the surging tide of deep learning over the past decade, Automatic Speech Recognition (ASR) has garnered substantial attention, leading to the emergence of numerous publicly accessible ASR systems that are actively being integrated into our daily lives.","Nonetheless, the impartial and replicable evaluation of these ASR systems encounters challenges due to various crucial subtleties.","In this paper we introduce the SpeechColab Leaderboard, a general-purpose, open-source platform designed for ASR evaluation.","With this platform: (i) We report a comprehensive benchmark, unveiling the current state-of-the-art panorama for ASR systems, covering both open-source models and industrial commercial services.","(ii) We quantize how distinct nuances in the scoring pipeline influence the final benchmark outcomes.","These include nuances related to capitalization, punctuation, interjection, contraction, synonym usage, compound words, etc.","These issues have gained prominence in the context of the transition towards an End-to-End future.","(iii) We propose a practical modification to the conventional Token-Error-Rate (TER) evaluation metric, with inspirations from Kolmogorov complexity and Normalized Information Distance (NID).","This adaptation, called modified-TER (mTER), achieves proper normalization and symmetrical treatment of reference and hypothesis.","By leveraging this platform as a large-scale testing ground, this study demonstrates the robustness and backward compatibility of mTER when compared to TER.","The SpeechColab Leaderboard is accessible at https://github.com/SpeechColab/Leaderboard"],"url":"http://arxiv.org/abs/2403.08196v1","category":"cs.CL"}
{"created":"2024-03-13 02:33:28","title":"Learning-driven Physically-aware Large-scale Circuit Gate Sizing","abstract":"Gate sizing plays an important role in timing optimization after physical design. Existing machine learning-based gate sizing works cannot optimize timing on multiple timing paths simultaneously and neglect the physical constraint on layouts. They cause sub-optimal sizing solutions and low-efficiency issues when compared with commercial gate sizing tools. In this work, we propose a learning-driven physically-aware gate sizing framework to optimize timing performance on large-scale circuits efficiently. In our gradient descent optimization-based work, for obtaining accurate gradients, a multi-modal gate sizing-aware timing model is achieved via learning timing information on multiple timing paths and physical information on multiple-scaled layouts jointly. Then, gradient generation based on the sizing-oriented estimator and adaptive back-propagation are developed to update gate sizes. Our results demonstrate that our work achieves higher timing performance improvements in a faster way compared with the commercial gate sizing tool.","sentences":["Gate sizing plays an important role in timing optimization after physical design.","Existing machine learning-based gate sizing works cannot optimize timing on multiple timing paths simultaneously and neglect the physical constraint on layouts.","They cause sub-optimal sizing solutions and low-efficiency issues when compared with commercial gate sizing tools.","In this work, we propose a learning-driven physically-aware gate sizing framework to optimize timing performance on large-scale circuits efficiently.","In our gradient descent optimization-based work, for obtaining accurate gradients, a multi-modal gate sizing-aware timing model is achieved via learning timing information on multiple timing paths and physical information on multiple-scaled layouts jointly.","Then, gradient generation based on the sizing-oriented estimator and adaptive back-propagation are developed to update gate sizes.","Our results demonstrate that our work achieves higher timing performance improvements in a faster way compared with the commercial gate sizing tool."],"url":"http://arxiv.org/abs/2403.08193v1","category":"cs.LG"}
{"created":"2024-03-13 02:05:21","title":"Effects of wave damping and finite perpendicular scale on three-dimensional Alfv\u00e9n wave parametric decay in low-beta plasmas","abstract":"Shear Alfven wave parametric decay instability (PDI) provides a potential path toward significant wave dissipation and plasma heating. However, fundamental questions regarding how PDI is excited in a realistic three-dimensional (3D) open system and how critically the finite perpendicular wave scale -- as found in both the laboratory and space plasmas -- affects the excitation remain poorly understood. Here, we present the first 3D, open-boundary, hybrid kinetic-fluid simulations of kinetic Alfven wave PDI in low-beta plasmas. Key findings are that the PDI excitation is strongly limited by the wave damping present, including electron-ion collisional damping (represented by a constant resistivity) and geometrical attenuation associated with the finite-scale Alfven wave, and ion Landau damping of the child acoustic wave. The perpendicular wave scale alone, however, plays no discernible role, with different wave scales exhibiting similar instability growth. These findings are corroborated by theoretical analysis and estimates. The new understanding of 3D kinetic Alfven wave PDI physics is essential for laboratory study of the basic plasma process and may also help evaluate the relevance/role of PDI in low-beta space plasmas.","sentences":["Shear Alfven wave parametric decay instability (PDI) provides a potential path toward significant wave dissipation and plasma heating.","However, fundamental questions regarding how PDI is excited in a realistic three-dimensional (3D) open system and how critically the finite perpendicular wave scale -- as found in both the laboratory and space plasmas -- affects the excitation remain poorly understood.","Here, we present the first 3D, open-boundary, hybrid kinetic-fluid simulations of kinetic Alfven wave PDI in low-beta plasmas.","Key findings are that the PDI excitation is strongly limited by the wave damping present, including electron-ion collisional damping (represented by a constant resistivity) and geometrical attenuation associated with the finite-scale Alfven wave, and ion Landau damping of the child acoustic wave.","The perpendicular wave scale alone, however, plays no discernible role, with different wave scales exhibiting similar instability growth.","These findings are corroborated by theoretical analysis and estimates.","The new understanding of 3D kinetic Alfven wave PDI physics is essential for laboratory study of the basic plasma process and may also help evaluate the relevance/role of PDI in low-beta space plasmas."],"url":"http://arxiv.org/abs/2403.08179v1","category":"physics.plasm-ph"}
{"created":"2024-03-13 01:48:01","title":"Versatile Defense Against Adversarial Attacks on Image Recognition","abstract":"Adversarial attacks present a significant security risk to image recognition tasks. Defending against these attacks in a real-life setting can be compared to the way antivirus software works, with a key consideration being how well the defense can adapt to new and evolving attacks. Another important factor is the resources involved in terms of time and cost for training defense models and updating the model database. Training many models that are specific to each type of attack can be time-consuming and expensive. Ideally, we should be able to train one single model that can handle a wide range of attacks. It appears that a defense method based on image-to-image translation may be capable of this. The proposed versatile defense approach in this paper only requires training one model to effectively resist various unknown adversarial attacks. The trained model has successfully improved the classification accuracy from nearly zero to an average of 86%, performing better than other defense methods proposed in prior studies. When facing the PGD attack and the MI-FGSM attack, versatile defense model even outperforms the attack-specific models trained based on these two attacks. The robustness check also shows that our versatile defense model performs stably regardless with the attack strength.","sentences":["Adversarial attacks present a significant security risk to image recognition tasks.","Defending against these attacks in a real-life setting can be compared to the way antivirus software works, with a key consideration being how well the defense can adapt to new and evolving attacks.","Another important factor is the resources involved in terms of time and cost for training defense models and updating the model database.","Training many models that are specific to each type of attack can be time-consuming and expensive.","Ideally, we should be able to train one single model that can handle a wide range of attacks.","It appears that a defense method based on image-to-image translation may be capable of this.","The proposed versatile defense approach in this paper only requires training one model to effectively resist various unknown adversarial attacks.","The trained model has successfully improved the classification accuracy from nearly zero to an average of 86%, performing better than other defense methods proposed in prior studies.","When facing the PGD attack and the MI-FGSM attack, versatile defense model even outperforms the attack-specific models trained based on these two attacks.","The robustness check also shows that our versatile defense model performs stably regardless with the attack strength."],"url":"http://arxiv.org/abs/2403.08170v1","category":"cs.CV"}
{"created":"2024-03-13 01:18:55","title":"Iterative Learning for Joint Image Denoising and Motion Artifact Correction of 3D Brain MRI","abstract":"Image noise and motion artifacts greatly affect the quality of brain MRI and negatively influence downstream medical image analysis. Previous studies often focus on 2D methods that process each volumetric MR image slice-by-slice, thus losing important 3D anatomical information. Additionally, these studies generally treat image denoising and artifact correction as two standalone tasks, without considering their potential relationship, especially on low-quality images where severe noise and motion artifacts occur simultaneously. To address these issues, we propose a Joint image Denoising and motion Artifact Correction (JDAC) framework via iterative learning to handle noisy MRIs with motion artifacts, consisting of an adaptive denoising model and an anti-artifact model. In the adaptive denoising model, we first design a novel noise level estimation strategy, and then adaptively reduce the noise through a U-Net backbone with feature normalization conditioning on the estimated noise variance. The anti-artifact model employs another U-Net for eliminating motion artifacts, incorporating a novel gradient-based loss function designed to maintain the integrity of brain anatomy during the motion correction process. These two models are iteratively employed for joint image denoising and artifact correction through an iterative learning framework. An early stopping strategy depending on noise level estimation is applied to accelerate the iteration process. The denoising model is trained with 9,544 T1-weighted MRIs with manually added Gaussian noise as supervision. The anti-artifact model is trained on 552 T1-weighted MRIs with motion artifacts and paired motion-free images. Experimental results on a public dataset and a clinical study suggest the effectiveness of JDAC in both tasks of denoising and motion artifact correction, compared with several state-of-the-art methods.","sentences":["Image noise and motion artifacts greatly affect the quality of brain MRI and negatively influence downstream medical image analysis.","Previous studies often focus on 2D methods that process each volumetric MR image slice-by-slice, thus losing important 3D anatomical information.","Additionally, these studies generally treat image denoising and artifact correction as two standalone tasks, without considering their potential relationship, especially on low-quality images where severe noise and motion artifacts occur simultaneously.","To address these issues, we propose a Joint image Denoising and motion Artifact Correction (JDAC) framework via iterative learning to handle noisy MRIs with motion artifacts, consisting of an adaptive denoising model and an anti-artifact model.","In the adaptive denoising model, we first design a novel noise level estimation strategy, and then adaptively reduce the noise through a U-Net backbone with feature normalization conditioning on the estimated noise variance.","The anti-artifact model employs another U-Net for eliminating motion artifacts, incorporating a novel gradient-based loss function designed to maintain the integrity of brain anatomy during the motion correction process.","These two models are iteratively employed for joint image denoising and artifact correction through an iterative learning framework.","An early stopping strategy depending on noise level estimation is applied to accelerate the iteration process.","The denoising model is trained with 9,544 T1-weighted MRIs with manually added Gaussian noise as supervision.","The anti-artifact model is trained on 552 T1-weighted MRIs with motion artifacts and paired motion-free images.","Experimental results on a public dataset and a clinical study suggest the effectiveness of JDAC in both tasks of denoising and motion artifact correction, compared with several state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.08162v1","category":"eess.IV"}
{"created":"2024-03-13 00:43:10","title":"NeRF-Supervised Feature Point Detection and Description","abstract":"Feature point detection and description is the backbone for various computer vision applications, such as Structure-from-Motion, visual SLAM, and visual place recognition. While learning-based methods have surpassed traditional handcrafted techniques, their training often relies on simplistic homography-based simulations of multi-view perspectives, limiting model generalisability. This paper introduces a novel approach leveraging neural radiance fields (NeRFs) for realistic multi-view training data generation. We create a diverse multi-view dataset using NeRFs, consisting of indoor and outdoor scenes. Our proposed methodology adapts state-of-the-art feature detectors and descriptors to train on NeRF-synthesised views supervised by perspective projective geometry. Our experiments demonstrate that the proposed methods achieve competitive or superior performance on standard benchmarks for relative pose estimation, point cloud registration, and homography estimation while requiring significantly less training data compared to existing approaches.","sentences":["Feature point detection and description is the backbone for various computer vision applications, such as Structure-from-Motion, visual SLAM, and visual place recognition.","While learning-based methods have surpassed traditional handcrafted techniques, their training often relies on simplistic homography-based simulations of multi-view perspectives, limiting model generalisability.","This paper introduces a novel approach leveraging neural radiance fields (NeRFs) for realistic multi-view training data generation.","We create a diverse multi-view dataset using NeRFs, consisting of indoor and outdoor scenes.","Our proposed methodology adapts state-of-the-art feature detectors and descriptors to train on NeRF-synthesised views supervised by perspective projective geometry.","Our experiments demonstrate that the proposed methods achieve competitive or superior performance on standard benchmarks for relative pose estimation, point cloud registration, and homography estimation while requiring significantly less training data compared to existing approaches."],"url":"http://arxiv.org/abs/2403.08156v1","category":"cs.CV"}
{"created":"2024-03-12 23:59:15","title":"BAGEL: Bootstrapping Agents by Guiding Exploration with Language","abstract":"Following natural language instructions by executing actions in digital environments (e.g. web-browsers and REST APIs) is a challenging task for language model (LM) agents. Unfortunately, LM agents often fail to generalize to new environments without human demonstrations. This work presents BAGEL, a method for bootstrapping LM agents without human supervision. BAGEL converts a seed set of randomly explored trajectories or synthetic instructions, into demonstrations, via round-trips between two noisy LM components: an LM labeler which converts a trajectory into a synthetic instruction, and a zero-shot LM agent which maps the synthetic instruction into a refined trajectory. By performing these round-trips iteratively, BAGEL quickly converts the initial distribution of trajectories towards those that are well-described by natural language. We use BAGEL demonstrations to adapt a zero shot LM agent at test time via in-context learning over retrieved demonstrations, and find improvements of over 2-13% absolute on ToolQA and MiniWob++, with up to 13x reduction in execution failures.","sentences":["Following natural language instructions by executing actions in digital environments (e.g. web-browsers and REST APIs) is a challenging task for language model (LM) agents.","Unfortunately, LM agents often fail to generalize to new environments without human demonstrations.","This work presents BAGEL, a method for bootstrapping LM agents without human supervision.","BAGEL converts a seed set of randomly explored trajectories or synthetic instructions, into demonstrations, via round-trips between two noisy LM components: an LM labeler which converts a trajectory into a synthetic instruction, and a zero-shot LM agent which maps the synthetic instruction into a refined trajectory.","By performing these round-trips iteratively, BAGEL quickly converts the initial distribution of trajectories towards those that are well-described by natural language.","We use BAGEL demonstrations to adapt a zero shot LM agent at test time via in-context learning over retrieved demonstrations, and find improvements of over 2-13% absolute on ToolQA and MiniWob++, with up to 13x reduction in execution failures."],"url":"http://arxiv.org/abs/2403.08140v1","category":"cs.CL"}
{"created":"2024-03-12 23:20:28","title":"6D Movable Antenna Based on User Distribution: Modeling and Optimization","abstract":"In this paper, we propose a new six-dimensional (6D) movable antenna (6DMA) system for future wireless networks to improve the communication performance. Unlike the traditional fixed-position antenna (FPA) and existing fluid antenna/two-dimensional (2D) movable antenna (FA/2DMA) systems that adjust the positions of antennas only, the proposed 6DMA system consists of distributed antenna surfaces with independently adjustable three-dimensional (3D) positions as well as 3D rotations within a given space. In particular, this paper applies the 6DMA to the base station (BS) in wireless networks to provide full degrees of freedom (DoFs) for the BS to adapt to the dynamic user spatial distribution in the network. However, a challenging new problem arises on how to optimally control the 6D positions and rotations of all 6DMA surfaces at the BS to maximize the network capacity based on the user spatial distribution, subject to the practical constraints on 6D antennas' movement. To tackle this problem, we first model the 6DMA-enabled BS and the user channels with the BS in terms of 6D positions and rotations of all 6DMA surfaces. Next, we propose an efficient alternating optimization algorithm to search for the best 6D positions and rotations of all 6DMA surfaces by leveraging the Monte Carlo simulation technique. Specifically, we sequentially optimize the 3D position/3D rotation of each 6DMA surface with those of the other surfaces fixed in an iterative manner. Numerical results show that our proposed 6DMA-BS can significantly improve the network capacity as compared to the benchmark BS architectures with FPAs or MAs with limited/partial movability, especially when the user distribution is more spatially non-uniform.","sentences":["In this paper, we propose a new six-dimensional (6D) movable antenna (6DMA) system for future wireless networks to improve the communication performance.","Unlike the traditional fixed-position antenna (FPA) and existing fluid antenna/two-dimensional (2D) movable antenna (FA/2DMA) systems that adjust the positions of antennas only, the proposed 6DMA system consists of distributed antenna surfaces with independently adjustable three-dimensional (3D) positions as well as 3D rotations within a given space.","In particular, this paper applies the 6DMA to the base station (BS) in wireless networks to provide full degrees of freedom (DoFs) for the BS to adapt to the dynamic user spatial distribution in the network.","However, a challenging new problem arises on how to optimally control the 6D positions and rotations of all 6DMA surfaces at the BS to maximize the network capacity based on the user spatial distribution, subject to the practical constraints on 6D antennas' movement.","To tackle this problem, we first model the 6DMA-enabled BS and the user channels with the BS in terms of 6D positions and rotations of all 6DMA surfaces.","Next, we propose an efficient alternating optimization algorithm to search for the best 6D positions and rotations of all 6DMA surfaces by leveraging the Monte Carlo simulation technique.","Specifically, we sequentially optimize the 3D position/3D rotation of each 6DMA surface with those of the other surfaces fixed in an iterative manner.","Numerical results show that our proposed 6DMA-BS can significantly improve the network capacity as compared to the benchmark BS architectures with FPAs or MAs with limited/partial movability, especially when the user distribution is more spatially non-uniform."],"url":"http://arxiv.org/abs/2403.08123v1","category":"cs.IT"}
{"created":"2024-03-12 23:19:28","title":"High energy dissipation rates from the impingement of free paper-thin sheets of liquids: Determination of the volume of the energy dissipation zone","abstract":"The micromixing time of impinging thin liquid sheets depends upon the energy dissipation rate. The kinetic energy released by the impingement has been previously studied and was found to be a function of the coefficient of restitution of the collision. In this work, the volume within which the released kinetic energy is dissipated was investigated. The volume of energy dissipation was determined by measuring the time required for the velocity of the single sheet (prior to the collision) to be reduced to the velocity in the mixed sheet (after the collision). Although different, the velocity in single sheets and the velocity in mixed sheets have been previously shown to be constant. High-speed video was used to measure the velocity of features, generated in the front single sheet, as they passed through the impingement zone and into the mixed sheet. The experimental results showed that the time required for the velocity change was approximately equal to the residence time of liquid in the impingement zone. A new equation for the energy dissipation rate was developed and compared with the energy dissipation rate derived from turbulence energy-cascade theory. This comparison showed that the large-eddy turnover time was approximately equal to the residence time in the impingement zone; a result that is in accordance with the notion from turbulence energy-cascade theory that large, energy-containing eddies lose their energy within approximately one large-eddy turnover time. Within the impingement zone, the large-eddy kinetic energy was found to decay exponentially with time.","sentences":["The micromixing time of impinging thin liquid sheets depends upon the energy dissipation rate.","The kinetic energy released by the impingement has been previously studied and was found to be a function of the coefficient of restitution of the collision.","In this work, the volume within which the released kinetic energy is dissipated was investigated.","The volume of energy dissipation was determined by measuring the time required for the velocity of the single sheet (prior to the collision) to be reduced to the velocity in the mixed sheet (after the collision).","Although different, the velocity in single sheets and the velocity in mixed sheets have been previously shown to be constant.","High-speed video was used to measure the velocity of features, generated in the front single sheet, as they passed through the impingement zone and into the mixed sheet.","The experimental results showed that the time required for the velocity change was approximately equal to the residence time of liquid in the impingement zone.","A new equation for the energy dissipation rate was developed and compared with the energy dissipation rate derived from turbulence energy-cascade theory.","This comparison showed that the large-eddy turnover time was approximately equal to the residence time in the impingement zone; a result that is in accordance with the notion from turbulence energy-cascade theory that large, energy-containing eddies lose their energy within approximately one large-eddy turnover time.","Within the impingement zone, the large-eddy kinetic energy was found to decay exponentially with time."],"url":"http://arxiv.org/abs/2403.08122v1","category":"physics.flu-dyn"}
{"created":"2024-03-12 22:21:48","title":"Efficient Language Model Architectures for Differentially Private Federated Learning","abstract":"Cross-device federated learning (FL) is a technique that trains a model on data distributed across typically millions of edge devices without data leaving the devices. SGD is the standard client optimizer for on device training in cross-device FL, favored for its memory and computational efficiency. However, in centralized training of neural language models, adaptive optimizers are preferred as they offer improved stability and performance. In light of this, we ask if language models can be modified such that they can be efficiently trained with SGD client optimizers and answer this affirmatively.   We propose a scale-invariant Coupled Input Forget Gate (SI CIFG) recurrent network by modifying the sigmoid and tanh activations in the recurrent cell and show that this new model converges faster and achieves better utility than the standard CIFG recurrent model in cross-device FL in large scale experiments. We further show that the proposed scale invariant modification also helps in federated learning of larger transformer models. Finally, we demonstrate the scale invariant modification is also compatible with other non-adaptive algorithms. Particularly, our results suggest an improved privacy utility trade-off in federated learning with differential privacy.","sentences":["Cross-device federated learning (FL) is a technique that trains a model on data distributed across typically millions of edge devices without data leaving the devices.","SGD is the standard client optimizer for on device training in cross-device FL, favored for its memory and computational efficiency.","However, in centralized training of neural language models, adaptive optimizers are preferred as they offer improved stability and performance.","In light of this, we ask if language models can be modified such that they can be efficiently trained with SGD client optimizers and answer this affirmatively.   ","We propose a scale-invariant Coupled Input Forget Gate (SI CIFG) recurrent network by modifying the sigmoid and tanh activations in the recurrent cell and show that this new model converges faster and achieves better utility than the standard CIFG recurrent model in cross-device FL in large scale experiments.","We further show that the proposed scale invariant modification also helps in federated learning of larger transformer models.","Finally, we demonstrate the scale invariant modification is also compatible with other non-adaptive algorithms.","Particularly, our results suggest an improved privacy utility trade-off in federated learning with differential privacy."],"url":"http://arxiv.org/abs/2403.08100v1","category":"cs.LG"}
{"created":"2024-03-12 22:19:58","title":"Application of Distributed Arithmetic to Adaptive Filtering Algorithms: Trends, Challenges and Future","abstract":"The utilization of distributed arithmetic (DA) in AF algorithms has gained significant attention in recent years due to its potential to enhance computational efficiency and reduce resource requirements. This paper presents an exploration of the application of DA to adaptive filtering (AF) algorithms, analyzing trends, discussing challenges, and outlining future prospects. It begins by providing an overview of both DA and AF algorithms, highlighting their individual merits and established applications. Subsequently, the integration of DA into AF algorithms is explored, showcasing its ability to optimize multiply-accumulate operations and mitigate the computational burden associated with AF algorithms. Throughout the paper, the critical trends observed in the field are discussed, including advancements in DA-based hardware architectures. Moreover, the challenges encountered in implementing DA-based AF is also discussed. The continued evolution of DA techniques to cater to the demands of modern AF applications, including real-time processing, resource-constrained environments, and high-dimensional data streams is anticipated. In conclusion, this paper consolidates the current state of applying DA to AF algorithms, offering insights into prevailing trends, discussing challenges, and presenting future research and development in the field. The fusion of these two domains holds promise for achieving improved computational efficiency, reduced hardware complexity, and enhanced performance in various signal processing applications.","sentences":["The utilization of distributed arithmetic (DA) in AF algorithms has gained significant attention in recent years due to its potential to enhance computational efficiency and reduce resource requirements.","This paper presents an exploration of the application of DA to adaptive filtering (AF) algorithms, analyzing trends, discussing challenges, and outlining future prospects.","It begins by providing an overview of both DA and AF algorithms, highlighting their individual merits and established applications.","Subsequently, the integration of DA into AF algorithms is explored, showcasing its ability to optimize multiply-accumulate operations and mitigate the computational burden associated with AF algorithms.","Throughout the paper, the critical trends observed in the field are discussed, including advancements in DA-based hardware architectures.","Moreover, the challenges encountered in implementing DA-based AF is also discussed.","The continued evolution of DA techniques to cater to the demands of modern AF applications, including real-time processing, resource-constrained environments, and high-dimensional data streams is anticipated.","In conclusion, this paper consolidates the current state of applying DA to AF algorithms, offering insights into prevailing trends, discussing challenges, and presenting future research and development in the field.","The fusion of these two domains holds promise for achieving improved computational efficiency, reduced hardware complexity, and enhanced performance in various signal processing applications."],"url":"http://arxiv.org/abs/2403.08099v1","category":"eess.SY"}
{"created":"2024-03-12 19:36:43","title":"Neural, Muscular, and Perceptual responses with shoulder exoskeleton use over Days","abstract":"Passive shoulder exoskeletons have been widely introduced in the industry to aid upper extremity movements during repetitive overhead work. As an ergonomic intervention, it is important to understand how users adapt to these devices over time and if these induce external stress while working. The study evaluated the use of an exoskeleton over a period of 3 days by assessing the neural, physiological, and perceptual responses of twenty-four participants by comparing a physical task against the same task with an additional cognitive workload. Over days adaptation to task irrespective of task and group were identified. Electromyography (EMG) analysis of shoulder and back muscles reveals lower muscle activity in the exoskeleton group irrespective of task. Functional connectivity analysis using functional near infrared spectroscopy (fNIRS) reveals that exoskeletons benefit users by reducing task demands in the motor planning and execution regions. Sex-based differences were also identified in these neuromuscular assessments.","sentences":["Passive shoulder exoskeletons have been widely introduced in the industry to aid upper extremity movements during repetitive overhead work.","As an ergonomic intervention, it is important to understand how users adapt to these devices over time and if these induce external stress while working.","The study evaluated the use of an exoskeleton over a period of 3 days by assessing the neural, physiological, and perceptual responses of twenty-four participants by comparing a physical task against the same task with an additional cognitive workload.","Over days adaptation to task irrespective of task and group were identified.","Electromyography (EMG) analysis of shoulder and back muscles reveals lower muscle activity in the exoskeleton group irrespective of task.","Functional connectivity analysis using functional near infrared spectroscopy (fNIRS) reveals that exoskeletons benefit users by reducing task demands in the motor planning and execution regions.","Sex-based differences were also identified in these neuromuscular assessments."],"url":"http://arxiv.org/abs/2403.08044v1","category":"q-bio.QM"}
{"created":"2024-03-12 19:34:50","title":"CT evaluation of 2D and 3D holistic deep learning methods for the volumetric segmentation of airway lesions","abstract":"This research embarked on a comparative exploration of the holistic segmentation capabilities of Convolutional Neural Networks (CNNs) in both 2D and 3D formats, focusing on cystic fibrosis (CF) lesions. The study utilized data from two CF reference centers, covering five major CF structural changes. Initially, it compared the 2D and 3D models, highlighting the 3D model's superior capability in capturing complex features like mucus plugs and consolidations. To improve the 2D model's performance, a loss adapted to fine structures segmentation was implemented and evaluated, significantly enhancing its accuracy, though not surpassing the 3D model's performance. The models underwent further validation through external evaluation against pulmonary function tests (PFTs), confirming the robustness of the findings. Moreover, this study went beyond comparing metrics; it also included comprehensive assessments of the models' interpretability and reliability, providing valuable insights for their clinical application.","sentences":["This research embarked on a comparative exploration of the holistic segmentation capabilities of Convolutional Neural Networks (CNNs) in both 2D and 3D formats, focusing on cystic fibrosis (CF) lesions.","The study utilized data from two CF reference centers, covering five major CF structural changes.","Initially, it compared the 2D and 3D models, highlighting the 3D model's superior capability in capturing complex features like mucus plugs and consolidations.","To improve the 2D model's performance, a loss adapted to fine structures segmentation was implemented and evaluated, significantly enhancing its accuracy, though not surpassing the 3D model's performance.","The models underwent further validation through external evaluation against pulmonary function tests (PFTs), confirming the robustness of the findings.","Moreover, this study went beyond comparing metrics; it also included comprehensive assessments of the models' interpretability and reliability, providing valuable insights for their clinical application."],"url":"http://arxiv.org/abs/2403.08042v1","category":"eess.IV"}
{"created":"2024-03-12 18:27:15","title":"The intermittently-resonant coevolution of migrating planets and their pulsating stars","abstract":"Hot Jupiters are expected to form far from their host star and move toward close-in, circular orbits via a smooth, monotonic decay due to mild and constant tidal dissipation. Yet, three systems have recently been found exhibiting planet-induced stellar pulsations suggesting unexpectedly strong tidal interactions. Here we combine stellar evolution and tide models to show that dynamical tides raised by eccentric gas giants can give rise to chains of resonance locks with multiple modes, enriching the dynamics seen in single-mode resonance locking of circularized systems. These series of resonance locks yield orders-of-magnitude larger changes in eccentricity and harmonic pulsations relative to those expected from a single episode of resonance locking or nonresonant tidal interactions. Resonances become more frequent as a star evolves off the main sequence providing an alternative explanation to the origin of some stellar pulsators and yielding the concept of \"dormant migrating giants\". Evolution trajectories are characterized by competing episodes of inward/outward migration and spin-up/-down of the star which are sensitive to the system parameters, revealing a new challenge in modeling migration paths and in contextualizing the observed populations of giant exoplanets and stellar binaries. This sensitivity however offers a new window to constrain the stellar properties of planetary hosts via tidal asteroseismology.","sentences":["Hot Jupiters are expected to form far from their host star and move toward close-in, circular orbits via a smooth, monotonic decay due to mild and constant tidal dissipation.","Yet, three systems have recently been found exhibiting planet-induced stellar pulsations suggesting unexpectedly strong tidal interactions.","Here we combine stellar evolution and tide models to show that dynamical tides raised by eccentric gas giants can give rise to chains of resonance locks with multiple modes, enriching the dynamics seen in single-mode resonance locking of circularized systems.","These series of resonance locks yield orders-of-magnitude larger changes in eccentricity and harmonic pulsations relative to those expected from a single episode of resonance locking or nonresonant tidal interactions.","Resonances become more frequent as a star evolves off the main sequence providing an alternative explanation to the origin of some stellar pulsators and yielding the concept of \"dormant migrating giants\".","Evolution trajectories are characterized by competing episodes of inward/outward migration and spin-up/-down of the star which are sensitive to the system parameters, revealing a new challenge in modeling migration paths and in contextualizing the observed populations of giant exoplanets and stellar binaries.","This sensitivity however offers a new window to constrain the stellar properties of planetary hosts via tidal asteroseismology."],"url":"http://arxiv.org/abs/2403.08014v1","category":"astro-ph.EP"}
{"created":"2024-03-12 18:01:10","title":"Dynamic Field of View Reduction Related to Subjective Sickness Measures in an HMD-based Data Analysis Task","abstract":"Various factors influence the degree of cybersickness a user can suffer in an immersive virtual environment, some of which can be controlled without adapting the virtual environment itself. When using HMDs, one example is the size of the field of view. However, the degree to which factors like this can be manipulated without affecting the user negatively in other ways is limited. Another prominent characteristic of cybersickness is that it affects individuals very differently. Therefore, to account for both the possible disruptive nature of alleviating factors and the high interpersonal variance, a promising approach may be to intervene only in cases where users experience discomfort symptoms, and only as much as necessary. Thus, we conducted a first experiment, where the field of view was decreased when people feel uncomfortable, to evaluate the possible positive impact on sickness and negative influence on presence. While we found no significant evidence for any of these possible effects, interesting further results and observations were made.","sentences":["Various factors influence the degree of cybersickness a user can suffer in an immersive virtual environment, some of which can be controlled without adapting the virtual environment itself.","When using HMDs, one example is the size of the field of view.","However, the degree to which factors like this can be manipulated without affecting the user negatively in other ways is limited.","Another prominent characteristic of cybersickness is that it affects individuals very differently.","Therefore, to account for both the possible disruptive nature of alleviating factors and the high interpersonal variance, a promising approach may be to intervene only in cases where users experience discomfort symptoms, and only as much as necessary.","Thus, we conducted a first experiment, where the field of view was decreased when people feel uncomfortable, to evaluate the possible positive impact on sickness and negative influence on presence.","While we found no significant evidence for any of these possible effects, interesting further results and observations were made."],"url":"http://arxiv.org/abs/2403.07992v1","category":"cs.HC"}
{"created":"2024-03-12 18:00:58","title":"Dissipative frequency converter: from Lindblad dynamics to non-Hermitian topology","abstract":"A topological frequency converter represents a dynamical counterpart of the integer quantum Hall effect, where a two-level system enacts a quantized time-averaged power transfer between two driving modes of incommensurate frequency. Here, we investigate as to what extent temporal coherence in the quantum dynamics of the two-level system is important for the topological quantization of the converter. To this end, we consider dissipative channels corresponding to spontaneous decay and dephasing in the instantaneous eigenbasis of the Hamiltonian as well as spontaneous decay in a fixed basis. The dissipation is modelled using both a full Lindblad and an effective non-Hermitian (NH) Hamiltonian description. For all three dissipation channels we find a transition from the unperturbed dynamics to a quantum watchdog effect, which destroys any power transfer in the strong coupling limit. This is striking because the watchdog effect leads to perfectly adiabatic dynamics in the instantaneous eigenbasis, at first glance similar to the unperturbed case. Furthermore, it is found that dephasing immediately leads to an exponential decay of the power transfer in time due to loss of polarisation in the mixed quantum state. Finally, we discuss the appearance in the effective NH trajectory description of non-adiabatic processes, which are suppressed in the full Lindblad dynamics.","sentences":["A topological frequency converter represents a dynamical counterpart of the integer quantum Hall effect, where a two-level system enacts a quantized time-averaged power transfer between two driving modes of incommensurate frequency.","Here, we investigate as to what extent temporal coherence in the quantum dynamics of the two-level system is important for the topological quantization of the converter.","To this end, we consider dissipative channels corresponding to spontaneous decay and dephasing in the instantaneous eigenbasis of the Hamiltonian as well as spontaneous decay in a fixed basis.","The dissipation is modelled using both a full Lindblad and an effective non-Hermitian (NH) Hamiltonian description.","For all three dissipation channels we find a transition from the unperturbed dynamics to a quantum watchdog effect, which destroys any power transfer in the strong coupling limit.","This is striking because the watchdog effect leads to perfectly adiabatic dynamics in the instantaneous eigenbasis, at first glance similar to the unperturbed case.","Furthermore, it is found that dephasing immediately leads to an exponential decay of the power transfer in time due to loss of polarisation in the mixed quantum state.","Finally, we discuss the appearance in the effective NH trajectory description of non-adiabatic processes, which are suppressed in the full Lindblad dynamics."],"url":"http://arxiv.org/abs/2403.07991v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-12 15:08:54","title":"Zitterbewegung CP violation in a Schwarzschild spacetime","abstract":"Neutral kaons oscillations in a Schwarzschild spacetime are analyzed. The interplay between two oscillations: (i) mixing associated with second order weak coupling and (ii) strange quark's zitterbewegung, introduces a coupling responsible for the observed CP violation. This curvature induced violation is a CPT violation with T conservation rather than a T violation with CPT conservation. The non-dissipative Hermitian evolution of the kaons system leads to the identification of this CPT violation. Then, the finite lifetime of the short-lived kaons induces a dissipative rotation of the imaginary violation parameter such that it becomes real and appears as a T violation. The consequences of this elucidation of the origin of neutral kaons CP violation are then discussed and some open perspectives are identified.","sentences":["Neutral kaons oscillations in a Schwarzschild spacetime are analyzed.","The interplay between two oscillations: (i) mixing associated with second order weak coupling and (ii) strange quark's zitterbewegung, introduces a coupling responsible for the observed CP violation.","This curvature induced violation is a CPT violation with T conservation rather than a T violation with CPT conservation.","The non-dissipative Hermitian evolution of the kaons system leads to the identification of this CPT violation.","Then, the finite lifetime of the short-lived kaons induces a dissipative rotation of the imaginary violation parameter such that it becomes real and appears as a T violation.","The consequences of this elucidation of the origin of neutral kaons CP violation are then discussed and some open perspectives are identified."],"url":"http://arxiv.org/abs/2403.07970v1","category":"hep-ph"}
{"created":"2024-03-12 08:27:53","title":"Efficient Post-Training Augmentation for Adaptive Inference in Heterogeneous and Distributed IoT Environments","abstract":"Early Exit Neural Networks (EENNs) present a solution to enhance the efficiency of neural network deployments. However, creating EENNs is challenging and requires specialized domain knowledge, due to the large amount of additional design choices. To address this issue, we propose an automated augmentation flow that focuses on converting an existing model into an EENN. It performs all required design decisions for the deployment to heterogeneous or distributed hardware targets: Our framework constructs the EENN architecture, maps its subgraphs to the hardware targets, and configures its decision mechanism. To the best of our knowledge, it is the first framework that is able to perform all of these steps.   We evaluated our approach on a collection of Internet-of-Things and standard image classification use cases. For a speech command detection task, our solution was able to reduce the mean operations per inference by 59.67%. For an ECG classification task, it was able to terminate all samples early, reducing the mean inference energy by 74.9% and computations by 78.3%. On CIFAR-10, our solution was able to achieve up to a 58.75% reduction in computations.   The search on a ResNet-152 base model for CIFAR-10 took less than nine hours on a laptop CPU. Our proposed approach enables the creation of EENN optimized for IoT environments and can reduce the inference cost of Deep Learning applications on embedded and fog platforms, while also significantly reducing the search cost - making it more accessible for scientists and engineers in industry and research. The low search cost improves the accessibility of EENNs, with the potential to improve the efficiency of neural networks in a wide range of practical applications.","sentences":["Early Exit Neural Networks (EENNs) present a solution to enhance the efficiency of neural network deployments.","However, creating EENNs is challenging and requires specialized domain knowledge, due to the large amount of additional design choices.","To address this issue, we propose an automated augmentation flow that focuses on converting an existing model into an EENN.","It performs all required design decisions for the deployment to heterogeneous or distributed hardware targets: Our framework constructs the EENN architecture, maps its subgraphs to the hardware targets, and configures its decision mechanism.","To the best of our knowledge, it is the first framework that is able to perform all of these steps.   ","We evaluated our approach on a collection of Internet-of-Things and standard image classification use cases.","For a speech command detection task, our solution was able to reduce the mean operations per inference by 59.67%.","For an ECG classification task, it was able to terminate all samples early, reducing the mean inference energy by 74.9% and computations by 78.3%.","On CIFAR-10, our solution was able to achieve up to a 58.75% reduction in computations.   ","The search on a ResNet-152 base model for CIFAR-10 took less than nine hours on a laptop CPU.","Our proposed approach enables the creation of EENN optimized for IoT environments and can reduce the inference cost of Deep Learning applications on embedded and fog platforms, while also significantly reducing the search cost - making it more accessible for scientists and engineers in industry and research.","The low search cost improves the accessibility of EENNs, with the potential to improve the efficiency of neural networks in a wide range of practical applications."],"url":"http://arxiv.org/abs/2403.07957v1","category":"cs.LG"}
{"created":"2024-03-12 06:26:17","title":"Optimizing Polynomial Graph Filters: A Novel Adaptive Krylov Subspace Approach","abstract":"Graph Neural Networks (GNNs), known as spectral graph filters, find a wide range of applications in web networks. To bypass eigendecomposition, polynomial graph filters are proposed to approximate graph filters by leveraging various polynomial bases for filter training. However, no existing studies have explored the diverse polynomial graph filters from a unified perspective for optimization.   In this paper, we first unify polynomial graph filters, as well as the optimal filters of identical degrees into the Krylov subspace of the same order, thus providing equivalent expressive power theoretically. Next, we investigate the asymptotic convergence property of polynomials from the unified Krylov subspace perspective, revealing their limited adaptability in graphs with varying heterophily degrees. Inspired by those facts, we design a novel adaptive Krylov subspace approach to optimize polynomial bases with provable controllability over the graph spectrum so as to adapt various heterophily graphs. Subsequently, we propose AdaptKry, an optimized polynomial graph filter utilizing bases from the adaptive Krylov subspaces. Meanwhile, in light of the diverse spectral properties of complex graphs, we extend AdaptKry by leveraging multiple adaptive Krylov bases without incurring extra training costs. As a consequence, extended AdaptKry is able to capture the intricate characteristics of graphs and provide insights into their inherent complexity. We conduct extensive experiments across a series of real-world datasets. The experimental results demonstrate the superior filtering capability of AdaptKry, as well as the optimized efficacy of the adaptive Krylov basis.","sentences":["Graph Neural Networks (GNNs), known as spectral graph filters, find a wide range of applications in web networks.","To bypass eigendecomposition, polynomial graph filters are proposed to approximate graph filters by leveraging various polynomial bases for filter training.","However, no existing studies have explored the diverse polynomial graph filters from a unified perspective for optimization.   ","In this paper, we first unify polynomial graph filters, as well as the optimal filters of identical degrees into the Krylov subspace of the same order, thus providing equivalent expressive power theoretically.","Next, we investigate the asymptotic convergence property of polynomials from the unified Krylov subspace perspective, revealing their limited adaptability in graphs with varying heterophily degrees.","Inspired by those facts, we design a novel adaptive Krylov subspace approach to optimize polynomial bases with provable controllability over the graph spectrum so as to adapt various heterophily graphs.","Subsequently, we propose AdaptKry, an optimized polynomial graph filter utilizing bases from the adaptive Krylov subspaces.","Meanwhile, in light of the diverse spectral properties of complex graphs, we extend AdaptKry by leveraging multiple adaptive Krylov bases without incurring extra training costs.","As a consequence, extended AdaptKry is able to capture the intricate characteristics of graphs and provide insights into their inherent complexity.","We conduct extensive experiments across a series of real-world datasets.","The experimental results demonstrate the superior filtering capability of AdaptKry, as well as the optimized efficacy of the adaptive Krylov basis."],"url":"http://arxiv.org/abs/2403.07954v1","category":"cs.LG"}
{"created":"2024-03-12 02:28:29","title":"SAMDA: Leveraging SAM on Few-Shot Domain Adaptation for Electronic Microscopy Segmentation","abstract":"It has been shown that traditional deep learning methods for electronic microscopy segmentation usually suffer from low transferability when samples and annotations are limited, while large-scale vision foundation models are more robust when transferring between different domains but facing sub-optimal improvement under fine-tuning. In this work, we present a new few-shot domain adaptation framework SAMDA, which combines the Segment Anything Model(SAM) with nnUNet in the embedding space to achieve high transferability and accuracy. Specifically, we choose the Unet-based network as the \"expert\" component to learn segmentation features efficiently and design a SAM-based adaptation module as the \"generic\" component for domain transfer. By amalgamating the \"generic\" and \"expert\" components, we mitigate the modality imbalance in the complex pre-training knowledge inherent to large-scale Vision Foundation models and the challenge of transferability inherent to traditional neural networks. The effectiveness of our model is evaluated on two electron microscopic image datasets with different modalities for mitochondria segmentation, which improves the dice coefficient on the target domain by 6.7%. Also, the SAM-based adaptor performs significantly better with only a single annotated image than the 10-shot domain adaptation on nnUNet. We further verify our model on four MRI datasets from different sources to prove its generalization ability.","sentences":["It has been shown that traditional deep learning methods for electronic microscopy segmentation usually suffer from low transferability when samples and annotations are limited, while large-scale vision foundation models are more robust when transferring between different domains but facing sub-optimal improvement under fine-tuning.","In this work, we present a new few-shot domain adaptation framework SAMDA, which combines the Segment Anything Model(SAM) with nnUNet in the embedding space to achieve high transferability and accuracy.","Specifically, we choose the Unet-based network as the \"expert\" component to learn segmentation features efficiently and design a SAM-based adaptation module as the \"generic\" component for domain transfer.","By amalgamating the \"generic\" and \"expert\" components, we mitigate the modality imbalance in the complex pre-training knowledge inherent to large-scale Vision Foundation models and the challenge of transferability inherent to traditional neural networks.","The effectiveness of our model is evaluated on two electron microscopic image datasets with different modalities for mitochondria segmentation, which improves the dice coefficient on the target domain by 6.7%.","Also, the SAM-based adaptor performs significantly better with only a single annotated image than the 10-shot domain adaptation on nnUNet.","We further verify our model on four MRI datasets from different sources to prove its generalization ability."],"url":"http://arxiv.org/abs/2403.07951v1","category":"eess.IV"}
{"created":"2024-03-11 15:58:15","title":"DiPrompT: Disentangled Prompt Tuning for Multiple Latent Domain Generalization in Federated Learning","abstract":"Federated learning (FL) has emerged as a powerful paradigm for learning from decentralized data, and federated domain generalization further considers the test dataset (target domain) is absent from the decentralized training data (source domains). However, most existing FL methods assume that domain labels are provided during training, and their evaluation imposes explicit constraints on the number of domains, which must strictly match the number of clients. Because of the underutilization of numerous edge devices and additional cross-client domain annotations in the real world, such restrictions may be impractical and involve potential privacy leaks. In this paper, we propose an efficient and novel approach, called Disentangled Prompt Tuning (DiPrompT), a method that tackles the above restrictions by learning adaptive prompts for domain generalization in a distributed manner. Specifically, we first design two types of prompts, i.e., global prompt to capture general knowledge across all clients and domain prompts to capture domain-specific knowledge. They eliminate the restriction on the one-to-one mapping between source domains and local clients. Furthermore, a dynamic query metric is introduced to automatically search the suitable domain label for each sample, which includes two-substep text-image alignments based on prompt tuning without labor-intensive annotation. Extensive experiments on multiple datasets demonstrate that our DiPrompT achieves superior domain generalization performance over state-of-the-art FL methods when domain labels are not provided, and even outperforms many centralized learning methods using domain labels.","sentences":["Federated learning (FL) has emerged as a powerful paradigm for learning from decentralized data, and federated domain generalization further considers the test dataset (target domain) is absent from the decentralized training data (source domains).","However, most existing FL methods assume that domain labels are provided during training, and their evaluation imposes explicit constraints on the number of domains, which must strictly match the number of clients.","Because of the underutilization of numerous edge devices and additional cross-client domain annotations in the real world, such restrictions may be impractical and involve potential privacy leaks.","In this paper, we propose an efficient and novel approach, called Disentangled Prompt Tuning (DiPrompT), a method that tackles the above restrictions by learning adaptive prompts for domain generalization in a distributed manner.","Specifically, we first design two types of prompts, i.e., global prompt to capture general knowledge across all clients and domain prompts to capture domain-specific knowledge.","They eliminate the restriction on the one-to-one mapping between source domains and local clients.","Furthermore, a dynamic query metric is introduced to automatically search the suitable domain label for each sample, which includes two-substep text-image alignments based on prompt tuning without labor-intensive annotation.","Extensive experiments on multiple datasets demonstrate that our DiPrompT achieves superior domain generalization performance over state-of-the-art FL methods when domain labels are not provided, and even outperforms many centralized learning methods using domain labels."],"url":"http://arxiv.org/abs/2403.08506v1","category":"cs.LG"}
{"created":"2024-03-13 17:58:00","title":"MIM4D: Masked Modeling with Multi-View Video for Autonomous Driving Representation Learning","abstract":"Learning robust and scalable visual representations from massive multi-view video data remains a challenge in computer vision and autonomous driving. Existing pre-training methods either rely on expensive supervised learning with 3D annotations, limiting the scalability, or focus on single-frame or monocular inputs, neglecting the temporal information. We propose MIM4D, a novel pre-training paradigm based on dual masked image modeling (MIM). MIM4D leverages both spatial and temporal relations by training on masked multi-view video inputs. It constructs pseudo-3D features using continuous scene flow and projects them onto 2D plane for supervision. To address the lack of dense 3D supervision, MIM4D reconstruct pixels by employing 3D volumetric differentiable rendering to learn geometric representations. We demonstrate that MIM4D achieves state-of-the-art performance on the nuScenes dataset for visual representation learning in autonomous driving. It significantly improves existing methods on multiple downstream tasks, including BEV segmentation (8.7% IoU), 3D object detection (3.5% mAP), and HD map construction (1.4% mAP). Our work offers a new choice for learning representation at scale in autonomous driving. Code and models are released at https://github.com/hustvl/MIM4D","sentences":["Learning robust and scalable visual representations from massive multi-view video data remains a challenge in computer vision and autonomous driving.","Existing pre-training methods either rely on expensive supervised learning with 3D annotations, limiting the scalability, or focus on single-frame or monocular inputs, neglecting the temporal information.","We propose MIM4D, a novel pre-training paradigm based on dual masked image modeling (MIM).","MIM4D leverages both spatial and temporal relations by training on masked multi-view video inputs.","It constructs pseudo-3D features using continuous scene flow and projects them onto 2D plane for supervision.","To address the lack of dense 3D supervision, MIM4D reconstruct pixels by employing 3D volumetric differentiable rendering to learn geometric representations.","We demonstrate that MIM4D achieves state-of-the-art performance on the nuScenes dataset for visual representation learning in autonomous driving.","It significantly improves existing methods on multiple downstream tasks, including BEV segmentation (8.7% IoU), 3D object detection (3.5% mAP), and HD map construction (1.4% mAP).","Our work offers a new choice for learning representation at scale in autonomous driving.","Code and models are released at https://github.com/hustvl/MIM4D"],"url":"http://arxiv.org/abs/2403.08760v1","category":"cs.CV"}
{"created":"2024-03-13 17:21:07","title":"Isotope effects in supercooled H$_2$O and D$_2$O and a corresponding-states-like rescaling of the temperature and pressure","abstract":"Water shows anomalous properties that are enhanced upon supercooling. The unusual behavior is observed in both H$_2$O and D$_2$O, however with different temperature dependences for the two isotopes. It is often noted that comparing the properties of the isotopes at two different temperatures (i.e., a temperature shift) approximately accounts for many of the observations with a temperature shift of 7.2 K in the temperature of maximum density being the most well-known example. However, the physical justification for such a shift is unclear. Motivated by recent work demonstrating a corresponding-states-like rescaling for water properties in three classical water models that all exhibit a liquid-liquid transition and critical point (B. Uralcan, et al., J. Chem. Phys. 150, 064503 (2019)), the applicability of this approach for reconciling the differences in temperature- and pressure-dependent thermodynamic properties of H$_2$O and D$_2$O is investigated here. Utilizing previously published data and equations-of-state for H$_2$O and D$_2$O, we show that the available data and models for these isotopes are consistent with such a low temperature correspondence. These observations provide support for the hypothesis that a liquid-liquid critical point, which is predicted to occur at low temperatures and high pressures, is the origin of many of water's anomalies.","sentences":["Water shows anomalous properties that are enhanced upon supercooling.","The unusual behavior is observed in both H$_2$O and D$_2$O, however with different temperature dependences for the two isotopes.","It is often noted that comparing the properties of the isotopes at two different temperatures (i.e., a temperature shift) approximately accounts for many of the observations with a temperature shift of 7.2 K in the temperature of maximum density being the most well-known example.","However, the physical justification for such a shift is unclear.","Motivated by recent work demonstrating a corresponding-states-like rescaling for water properties in three classical water models that all exhibit a liquid-liquid transition and critical point (B. Uralcan, et al., J. Chem.","Phys. 150, 064503 (2019)), the applicability of this approach for reconciling the differences in temperature- and pressure-dependent thermodynamic properties of H$_2$O and D$_2$O is investigated here.","Utilizing previously published data and equations-of-state for H$_2$O and D$_2$O, we show that the available data and models for these isotopes are consistent with such a low temperature correspondence.","These observations provide support for the hypothesis that a liquid-liquid critical point, which is predicted to occur at low temperatures and high pressures, is the origin of many of water's anomalies."],"url":"http://arxiv.org/abs/2403.08722v1","category":"physics.chem-ph"}
{"created":"2024-03-13 16:58:37","title":"Deep Learning for In-Orbit Cloud Segmentation and Classification in Hyperspectral Satellite Data","abstract":"This article explores the latest Convolutional Neural Networks (CNNs) for cloud detection aboard hyperspectral satellites. The performance of the latest 1D CNN (1D-Justo-LiuNet) and two recent 2D CNNs (nnU-net and 2D-Justo-UNet-Simple) for cloud segmentation and classification is assessed. Evaluation criteria include precision and computational efficiency for in-orbit deployment. Experiments utilize NASA's EO-1 Hyperion data, with varying spectral channel numbers after Principal Component Analysis. Results indicate that 1D-Justo-LiuNet achieves the highest accuracy, outperforming 2D CNNs, while maintaining compactness with larger spectral channel sets, albeit with increased inference times. However, the performance of 1D CNN degrades with significant channel reduction. In this context, the 2D-Justo-UNet-Simple offers the best balance for in-orbit deployment, considering precision, memory, and time costs. While nnU-net is suitable for on-ground processing, deployment of lightweight 1D-Justo-LiuNet is recommended for high-precision applications. Alternatively, lightweight 2D-Justo-UNet-Simple is recommended for balanced costs between timing and precision in orbit.","sentences":["This article explores the latest Convolutional Neural Networks (CNNs) for cloud detection aboard hyperspectral satellites.","The performance of the latest 1D CNN (1D-Justo-LiuNet) and two recent 2D CNNs (nnU-net and 2D-Justo-UNet-Simple) for cloud segmentation and classification is assessed.","Evaluation criteria include precision and computational efficiency for in-orbit deployment.","Experiments utilize NASA's EO-1 Hyperion data, with varying spectral channel numbers after Principal Component Analysis.","Results indicate that 1D-Justo-LiuNet achieves the highest accuracy, outperforming 2D CNNs, while maintaining compactness with larger spectral channel sets, albeit with increased inference times.","However, the performance of 1D CNN degrades with significant channel reduction.","In this context, the 2D-Justo-UNet-Simple offers the best balance for in-orbit deployment, considering precision, memory, and time costs.","While nnU-net is suitable for on-ground processing, deployment of lightweight 1D-Justo-LiuNet is recommended for high-precision applications.","Alternatively, lightweight 2D-Justo-UNet-Simple is recommended for balanced costs between timing and precision in orbit."],"url":"http://arxiv.org/abs/2403.08695v1","category":"cs.CV"}
{"created":"2024-03-13 16:25:55","title":"When can we Approximate Wide Contrastive Models with Neural Tangent Kernels and Principal Component Analysis?","abstract":"Contrastive learning is a paradigm for learning representations from unlabelled data that has been highly successful for image and text data. Several recent works have examined contrastive losses to claim that contrastive models effectively learn spectral embeddings, while few works show relations between (wide) contrastive models and kernel principal component analysis (PCA). However, it is not known if trained contrastive models indeed correspond to kernel methods or PCA. In this work, we analyze the training dynamics of two-layer contrastive models, with non-linear activation, and answer when these models are close to PCA or kernel methods. It is well known in the supervised setting that neural networks are equivalent to neural tangent kernel (NTK) machines, and that the NTK of infinitely wide networks remains constant during training. We provide the first convergence results of NTK for contrastive losses, and present a nuanced picture: NTK of wide networks remains almost constant for cosine similarity based contrastive losses, but not for losses based on dot product similarity. We further study the training dynamics of contrastive models with orthogonality constraints on output layer, which is implicitly assumed in works relating contrastive learning to spectral embedding. Our deviation bounds suggest that representations learned by contrastive models are close to the principal components of a certain matrix computed from random features. We empirically show that our theoretical results possibly hold beyond two-layer networks.","sentences":["Contrastive learning is a paradigm for learning representations from unlabelled data that has been highly successful for image and text data.","Several recent works have examined contrastive losses to claim that contrastive models effectively learn spectral embeddings, while few works show relations between (wide) contrastive models and kernel principal component analysis (PCA).","However, it is not known if trained contrastive models indeed correspond to kernel methods or PCA.","In this work, we analyze the training dynamics of two-layer contrastive models, with non-linear activation, and answer when these models are close to PCA or kernel methods.","It is well known in the supervised setting that neural networks are equivalent to neural tangent kernel (NTK) machines, and that the NTK of infinitely wide networks remains constant during training.","We provide the first convergence results of NTK for contrastive losses, and present a nuanced picture: NTK of wide networks remains almost constant for cosine similarity based contrastive losses, but not for losses based on dot product similarity.","We further study the training dynamics of contrastive models with orthogonality constraints on output layer, which is implicitly assumed in works relating contrastive learning to spectral embedding.","Our deviation bounds suggest that representations learned by contrastive models are close to the principal components of a certain matrix computed from random features.","We empirically show that our theoretical results possibly hold beyond two-layer networks."],"url":"http://arxiv.org/abs/2403.08673v1","category":"cs.LG"}
{"created":"2024-03-13 16:22:30","title":"Non-linear collision-induced breakage equation: approximate solution and error estimation","abstract":"This article aims to provide approximate solutions for the non-linear collision-induced breakage equation using two different semi-analytical schemes, i.e., variational iteration method (VIM) and optimized decomposition method (ODM). The study also includes the detailed convergence analysis and error estimation for ODM in the case of product collisional ($K(\\epsilon,\\rho)=\\epsilon\\rho$) and breakage ($b(\\epsilon,\\rho,\\sigma)=\\frac{2}{\\rho}$) kernels with an exponential decay initial condition. By contrasting estimated concentration function and moments with exact solutions, the novelty of the suggested approaches is presented considering three numerical examples. Interestingly, in one case, VIM provides a closed-form solution, however, finite term series solutions obtained via both schemes supply a great approximation for the concentration function and moments.","sentences":["This article aims to provide approximate solutions for the non-linear collision-induced breakage equation using two different semi-analytical schemes, i.e., variational iteration method (VIM) and optimized decomposition method (ODM).","The study also includes the detailed convergence analysis and error estimation for ODM in the case of product collisional ($K(\\epsilon,\\rho)=\\epsilon\\rho$) and breakage ($b(\\epsilon,\\rho,\\sigma)=\\frac{2}{\\rho}$) kernels with an exponential decay initial condition.","By contrasting estimated concentration function and moments with exact solutions, the novelty of the suggested approaches is presented considering three numerical examples.","Interestingly, in one case, VIM provides a closed-form solution, however, finite term series solutions obtained via both schemes supply a great approximation for the concentration function and moments."],"url":"http://arxiv.org/abs/2403.08672v1","category":"math.NA"}
{"created":"2024-03-13 16:06:27","title":"Physics-Guided Inverse Regression for Crop Quality Assessment","abstract":"We present an innovative approach leveraging Physics-Guided Neural Networks (PGNNs) for enhancing agricultural quality assessments. Central to our methodology is the application of physics-guided inverse regression, a technique that significantly improves the model's ability to precisely predict quality metrics of crops. This approach directly addresses the challenges of scalability, speed, and practicality that traditional assessment methods face. By integrating physical principles, notably Fick`s second law of diffusion, into neural network architectures, our developed PGNN model achieves a notable advancement in enhancing both the interpretability and accuracy of assessments. Empirical validation conducted on cucumbers and mushrooms demonstrates the superior capability of our model in outperforming conventional computer vision techniques in postharvest quality evaluation. This underscores our contribution as a scalable and efficient solution to the pressing demands of global food supply challenges.","sentences":["We present an innovative approach leveraging Physics-Guided Neural Networks (PGNNs) for enhancing agricultural quality assessments.","Central to our methodology is the application of physics-guided inverse regression, a technique that significantly improves the model's ability to precisely predict quality metrics of crops.","This approach directly addresses the challenges of scalability, speed, and practicality that traditional assessment methods face.","By integrating physical principles, notably Fick`s second law of diffusion, into neural network architectures, our developed PGNN model achieves a notable advancement in enhancing both the interpretability and accuracy of assessments.","Empirical validation conducted on cucumbers and mushrooms demonstrates the superior capability of our model in outperforming conventional computer vision techniques in postharvest quality evaluation.","This underscores our contribution as a scalable and efficient solution to the pressing demands of global food supply challenges."],"url":"http://arxiv.org/abs/2403.08653v1","category":"stat.ME"}
{"created":"2024-03-13 16:01:09","title":"Rigidity of Einstein manifolds with positive Yamabe invariant","abstract":"We provide optimal pinching results on closed Einstein manifolds with positive Yamabe invariant in any dimension, extending the optimal bound for the scalar curvature due to Gursky and LeBrun in dimension four. We also improve the known bounds of the Yamabe invariant \\emph{via} the $L^{\\frac{n}{2}}$-norm of the Weyl tensor for low-dimensional Einstein manifolds. Finally, we discuss some advances on an algebraic inequality involving the Weyl tensor for dimensions $5$ and $6$.","sentences":["We provide optimal pinching results on closed Einstein manifolds with positive Yamabe invariant in any dimension, extending the optimal bound for the scalar curvature due to Gursky and LeBrun in dimension four.","We also improve the known bounds of the Yamabe invariant \\emph{via} the $L^{\\frac{n}{2}}$-norm of the Weyl tensor for low-dimensional Einstein manifolds.","Finally, we discuss some advances on an algebraic inequality involving the Weyl tensor for dimensions $5$ and $6$."],"url":"http://arxiv.org/abs/2403.08647v1","category":"math.DG"}
{"created":"2024-03-13 15:46:37","title":"A Decade's Battle on Dataset Bias: Are We There Yet?","abstract":"We revisit the \"dataset classification\" experiment suggested by Torralba and Efros a decade ago, in the new era with large-scale, diverse, and hopefully less biased datasets as well as more capable neural network architectures. Surprisingly, we observe that modern neural networks can achieve excellent accuracy in classifying which dataset an image is from: e.g., we report 84.7% accuracy on held-out validation data for the three-way classification problem consisting of the YFCC, CC, and DataComp datasets. Our further experiments show that such a dataset classifier could learn semantic features that are generalizable and transferable, which cannot be simply explained by memorization. We hope our discovery will inspire the community to rethink the issue involving dataset bias and model capabilities.","sentences":["We revisit the \"dataset classification\" experiment suggested by Torralba and Efros a decade ago, in the new era with large-scale, diverse, and hopefully less biased datasets as well as more capable neural network architectures.","Surprisingly, we observe that modern neural networks can achieve excellent accuracy in classifying which dataset an image is from: e.g., we report 84.7% accuracy on held-out validation data for the three-way classification problem consisting of the YFCC, CC, and DataComp datasets.","Our further experiments show that such a dataset classifier could learn semantic features that are generalizable and transferable, which cannot be simply explained by memorization.","We hope our discovery will inspire the community to rethink the issue involving dataset bias and model capabilities."],"url":"http://arxiv.org/abs/2403.08632v1","category":"cs.CV"}
{"created":"2024-03-13 15:45:29","title":"Leveraging Non-Decimated Wavelet Packet Features and Transformer Models for Time Series Forecasting","abstract":"This article combines wavelet analysis techniques with machine learning methods for univariate time series forecasting, focusing on three main contributions. Firstly, we consider the use of Daubechies wavelets with different numbers of vanishing moments as input features to both non-temporal and temporal forecasting methods, by selecting these numbers during the cross-validation phase. Secondly, we compare the use of both the non-decimated wavelet transform and the non-decimated wavelet packet transform for computing these features, the latter providing a much larger set of potentially useful coefficient vectors. The wavelet coefficients are computed using a shifted version of the typical pyramidal algorithm to ensure no leakage of future information into these inputs. Thirdly, we evaluate the use of these wavelet features on a significantly wider set of forecasting methods than previous studies, including both temporal and non-temporal models, and both statistical and deep learning-based methods. The latter include state-of-the-art transformer-based neural network architectures. Our experiments suggest significant benefit in replacing higher-order lagged features with wavelet features across all examined non-temporal methods for one-step-forward forecasting, and modest benefit when used as inputs for temporal deep learning-based models for long-horizon forecasting.","sentences":["This article combines wavelet analysis techniques with machine learning methods for univariate time series forecasting, focusing on three main contributions.","Firstly, we consider the use of Daubechies wavelets with different numbers of vanishing moments as input features to both non-temporal and temporal forecasting methods, by selecting these numbers during the cross-validation phase.","Secondly, we compare the use of both the non-decimated wavelet transform and the non-decimated wavelet packet transform for computing these features, the latter providing a much larger set of potentially useful coefficient vectors.","The wavelet coefficients are computed using a shifted version of the typical pyramidal algorithm to ensure no leakage of future information into these inputs.","Thirdly, we evaluate the use of these wavelet features on a significantly wider set of forecasting methods than previous studies, including both temporal and non-temporal models, and both statistical and deep learning-based methods.","The latter include state-of-the-art transformer-based neural network architectures.","Our experiments suggest significant benefit in replacing higher-order lagged features with wavelet features across all examined non-temporal methods for one-step-forward forecasting, and modest benefit when used as inputs for temporal deep learning-based models for long-horizon forecasting."],"url":"http://arxiv.org/abs/2403.08630v1","category":"stat.ME"}
{"created":"2024-03-13 15:35:45","title":"Spin-resolved counting statistics as a sensitive probe of spin correlation in transport through a quantum dot spin valve","abstract":"We investigate the noise in spin transport through a single quantum dot (QD) tunnel coupled to ferromagnetic electrodes with noncollinear magnetizations. Based on a spin-resolved quantum master equation, auto- and cross-correlations of spin-resolved currents are analyzed to reveal the underlying spin transport dynamics and characteristics for various polarizations. We find the currents of majority and minority spins could be strongly autocorrelated despite uncorrelated charge transfer. The interplay between tunnel coupling and the Coulomb interaction gives rise to an exchange magnetic field, leading to the precession of the accumulated spin in the QD. It strongly suppresses the bunching of spin tunneling events and results in a unique double-peak structure in the noise of the net spin current. The spin autocorrelation is found to be susceptible to magnetization alignments, which may serve as a sensitive tool to measure the magnetization directions between the ferromagnetic electrodes.","sentences":["We investigate the noise in spin transport through a single quantum dot (QD) tunnel coupled to ferromagnetic electrodes with noncollinear magnetizations.","Based on a spin-resolved quantum master equation, auto- and cross-correlations of spin-resolved currents are analyzed to reveal the underlying spin transport dynamics and characteristics for various polarizations.","We find the currents of majority and minority spins could be strongly autocorrelated despite uncorrelated charge transfer.","The interplay between tunnel coupling and the Coulomb interaction gives rise to an exchange magnetic field, leading to the precession of the accumulated spin in the QD.","It strongly suppresses the bunching of spin tunneling events and results in a unique double-peak structure in the noise of the net spin current.","The spin autocorrelation is found to be susceptible to magnetization alignments, which may serve as a sensitive tool to measure the magnetization directions between the ferromagnetic electrodes."],"url":"http://arxiv.org/abs/2403.08621v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-13 14:36:09","title":"Electroweak Evolution Equations and Isospin Conservation","abstract":"In processes taking place at energies much higher than the weak scale, electroweak corrections can be taken into account by using electroweak evolution equations, that are analogous to the DGLAP equations in QCD. We show that weak isospin conservation in these equations imposes to modify the expressions of the splitting functions commonly used in the literature. These modifications have a profound impact on the parton distribution functions.","sentences":["In processes taking place at energies much higher than the weak scale, electroweak corrections can be taken into account by using electroweak evolution equations, that are analogous to the DGLAP equations in QCD.","We show that weak isospin conservation in these equations imposes to modify the expressions of the splitting functions commonly used in the literature.","These modifications have a profound impact on the parton distribution functions."],"url":"http://arxiv.org/abs/2403.08583v1","category":"hep-ph"}
{"created":"2024-03-13 14:29:11","title":"Higher order Schauder estimates for parabolic equations with degenerate or singular weights","abstract":"In this paper, we complete the analysis initiated in [AFV24] establishing some higher order $C^{k+2,\\alpha}$ Schauder estimates ($k \\in \\mathbb{N}$) for a a class of parabolic equations with weights that are degenerate/singular on a characteristic hyperplane. The $C^{2,\\alpha}$-estimates are obtained through a blow-up argument and a Liouville theorem, while the higher order estimates are obtained by a fine iteration procedure. As a byproduct, we present two applications. First, we prove similar Schauder estimates when the degeneracy/singularity of the weight occurs on a regular hypersurface of cylindrical type. Second, we provide an alternative proof of the higher order boundary Harnack principles established in [BG16,Kuk22].","sentences":["In this paper, we complete the analysis initiated in [AFV24] establishing some higher order $C^{k+2,\\alpha}$ Schauder estimates ($k \\in \\mathbb{N}$) for a a class of parabolic equations with weights that are degenerate/singular on a characteristic hyperplane.","The $C^{2,\\alpha}$-estimates are obtained through a blow-up argument and a Liouville theorem, while the higher order estimates are obtained by a fine iteration procedure.","As a byproduct, we present two applications.","First, we prove similar Schauder estimates when the degeneracy/singularity of the weight occurs on a regular hypersurface of cylindrical type.","Second, we provide an alternative proof of the higher order boundary Harnack principles established in [BG16,Kuk22]."],"url":"http://arxiv.org/abs/2403.08575v1","category":"math.AP"}
{"created":"2024-03-13 14:25:15","title":"A Physics-driven GraphSAGE Method for Physical Process Simulations Described by Partial Differential Equations","abstract":"Physics-informed neural networks (PINNs) have successfully addressed various computational physics problems based on partial differential equations (PDEs). However, while tackling issues related to irregularities like singularities and oscillations, trained solutions usually suffer low accuracy. In addition, most current works only offer the trained solution for predetermined input parameters. If any change occurs in input parameters, transfer learning or retraining is required, and traditional numerical techniques also need an independent simulation. In this work, a physics-driven GraphSAGE approach (PD-GraphSAGE) based on the Galerkin method and piecewise polynomial nodal basis functions is presented to solve computational problems governed by irregular PDEs and to develop parametric PDE surrogate models. This approach employs graph representations of physical domains, thereby reducing the demands for evaluated points due to local refinement. A distance-related edge feature and a feature mapping strategy are devised to help training and convergence for singularity and oscillation situations, respectively. The merits of the proposed method are demonstrated through a couple of cases. Moreover, the robust PDE surrogate model for heat conduction problems parameterized by the Gaussian random field source is successfully established, which not only provides the solution accurately but is several times faster than the finite element method in our experiments.","sentences":["Physics-informed neural networks (PINNs) have successfully addressed various computational physics problems based on partial differential equations (PDEs).","However, while tackling issues related to irregularities like singularities and oscillations, trained solutions usually suffer low accuracy.","In addition, most current works only offer the trained solution for predetermined input parameters.","If any change occurs in input parameters, transfer learning or retraining is required, and traditional numerical techniques also need an independent simulation.","In this work, a physics-driven GraphSAGE approach (PD-GraphSAGE) based on the Galerkin method and piecewise polynomial nodal basis functions is presented to solve computational problems governed by irregular PDEs and to develop parametric PDE surrogate models.","This approach employs graph representations of physical domains, thereby reducing the demands for evaluated points due to local refinement.","A distance-related edge feature and a feature mapping strategy are devised to help training and convergence for singularity and oscillation situations, respectively.","The merits of the proposed method are demonstrated through a couple of cases.","Moreover, the robust PDE surrogate model for heat conduction problems parameterized by the Gaussian random field source is successfully established, which not only provides the solution accurately but is several times faster than the finite element method in our experiments."],"url":"http://arxiv.org/abs/2403.08569v1","category":"cs.LG"}
{"created":"2024-03-13 14:22:13","title":"A Novel Implicit Neural Representation for Volume Data","abstract":"The storage of medical images is one of the challenges in the medical imaging field. There are variable works that use implicit neural representation (INR) to compress volumetric medical images. However, there is room to improve the compression rate for volumetric medical images. Most of the INR techniques need a huge amount of GPU memory and a long training time for high-quality medical volume rendering. In this paper, we present a novel implicit neural representation to compress volume data using our proposed architecture, that is, the Lanczos downsampling scheme, SIREN deep network, and SRDenseNet high-resolution scheme. Our architecture can effectively reduce training time, and gain a high compression rate while retaining the final rendering quality. Moreover, it can save GPU memory in comparison with the existing works. The experiments show that the quality of reconstructed images and training speed using our architecture is higher than current works which use the SIREN only. Besides, the GPU memory cost is evidently decreased","sentences":["The storage of medical images is one of the challenges in the medical imaging field.","There are variable works that use implicit neural representation (INR) to compress volumetric medical images.","However, there is room to improve the compression rate for volumetric medical images.","Most of the INR techniques need a huge amount of GPU memory and a long training time for high-quality medical volume rendering.","In this paper, we present a novel implicit neural representation to compress volume data using our proposed architecture, that is, the Lanczos downsampling scheme, SIREN deep network, and SRDenseNet high-resolution scheme.","Our architecture can effectively reduce training time, and gain a high compression rate while retaining the final rendering quality.","Moreover, it can save GPU memory in comparison with the existing works.","The experiments show that the quality of reconstructed images and training speed using our architecture is higher than current works which use the SIREN only.","Besides, the GPU memory cost is evidently decreased"],"url":"http://arxiv.org/abs/2403.08566v1","category":"eess.IV"}
{"created":"2024-03-13 14:05:08","title":"Young asteroid families as the primary source of meteorites","abstract":"Understanding the origin of bright shooting stars and their meteorite samples is among the most ancient astronomy-related questions that at larger scales has human consequences [1-3]. As of today, only ${\\sim}\\,6\\%$ of meteorite falls have been firmly linked to their sources (Moon, Mars, and asteroid (4) Vesta [4-6]). Here, we show that ${\\sim}\\,70\\%$ of meteorites originate from three recent breakups of $D > 30\\,{\\rm km}$ asteroids that occurred 5.8, 7.5 and less than ${\\sim}\\,40$ million years ago. These breakups, including the well-known Karin family [7], took place in the prominent yet old Koronis and Massalia families and are at the origin of the dominance of H and L ordinary chondrites among meteorite falls. These young families distinguish themselves amidst all main belt asteroids by having a uniquely high abundance of small fragments. Their size-frequency distribution remains steep for a few tens of millions of years, exceeding temporarily the production of metre-sized fragments by the largest old asteroid families (e.g., Flora, Vesta). Supporting evidence includes the existence of associated dust bands [8-10], the cosmic-ray exposure ages of H-chondrite meteorites [11,12], or the distribution of pre-atmospheric orbits of meteorites [13-15].","sentences":["Understanding the origin of bright shooting stars and their meteorite samples is among the most ancient astronomy-related questions that at larger scales has human consequences [1-3].","As of today, only ${\\sim}\\,6\\%$ of meteorite falls have been firmly linked to their sources (Moon, Mars, and asteroid (4) Vesta [4-6]).","Here, we show that ${\\sim}\\,70\\%$ of meteorites originate from three recent breakups of $D > 30\\,{\\rm km}$ asteroids that occurred 5.8, 7.5 and less than ${\\sim}\\,40$ million years ago.","These breakups, including the well-known Karin family [7], took place in the prominent yet old Koronis and Massalia families and are at the origin of the dominance of H and L ordinary chondrites among meteorite falls.","These young families distinguish themselves amidst all main belt asteroids by having a uniquely high abundance of small fragments.","Their size-frequency distribution remains steep for a few tens of millions of years, exceeding temporarily the production of metre-sized fragments by the largest old asteroid families (e.g., Flora, Vesta).","Supporting evidence includes the existence of associated dust bands","[8-10], the cosmic-ray exposure ages of H-chondrite meteorites [11,12], or the distribution of pre-atmospheric orbits of meteorites [13-15]."],"url":"http://arxiv.org/abs/2403.08552v1","category":"astro-ph.EP"}
{"created":"2024-03-13 12:06:39","title":"Increasing stability for inverse source problem with limited-aperture far field data at multi-frequencies","abstract":"We study the increasing stability of an inverse source problem for the Helmholtz equation from limited-aperture far field data at multiple wave numbers. The measurement data are givenby the far field patterns $u^\\infity(\\hat{x},k)$ for all observation directions in some neighborhood of a fixed direction $\\hat{x}$ and for all wave numbers k belonging to a finite interval $(0,K)$. In this paper, we discuss the increasing stability with respect to the width of the wavenumber interval $K>1$. In three dimensions we establish stability estimates of the $L^2$-norm and $H^{-1}$-norm of the source function from the far field data. The ill-posedness of the inverse source problem turns out to be of H\\\"older type while increasing the wavenumber band K. We also discuss an analytic continuation argument of the far-field data with respect to the wavenumbers at a fixed direction.","sentences":["We study the increasing stability of an inverse source problem for the Helmholtz equation from limited-aperture far field data at multiple wave numbers.","The measurement data are givenby the far field patterns $u^\\infity(\\hat{x},k)$ for all observation directions in some neighborhood of a fixed direction $\\hat{x}$ and for all wave numbers k belonging to a finite interval $(0,K)$. In this paper, we discuss the increasing stability with respect to the width of the wavenumber interval $K>1$. In three dimensions we establish stability estimates of the $L^2$-norm and $H^{-1}$-norm of the source function from the far field data.","The ill-posedness of the inverse source problem turns out to be of H\\\"older type while increasing the wavenumber band","K. We also discuss an analytic continuation argument of the far-field data with respect to the wavenumbers at a fixed direction."],"url":"http://arxiv.org/abs/2403.08450v1","category":"math.AP"}
{"created":"2024-03-13 11:57:54","title":"$L^2$ decay for large perturbations of viscous shocks for multi-D Burgers equation","abstract":"We consider a planar viscous shock of moderate strength for a scalar viscous conservation law in multi-D. We consider a strictly convex flux, as a small perturbation of the Burgers flux, along the normal direction to the shock front. However, for the transversal directions, we do not have any restrictions on flux function. We first show the contraction property for any large perturbations in $L^2$ of the planar viscous shock. If the initial $L^2$-perturbation is also in $L^1$, the large perturbation converges to zero in $L^2$ as time goes to infinity with $t^{-1/4}$ decay rate. The contraction and decay estimates hold up to dynamical shift. For the results, we do not impose any smallness conditions on the initial value. This result extends the 1D case \\cite{Kang-V-1} by the first author and Vasseur to the multi-dimensional case.","sentences":["We consider a planar viscous shock of moderate strength for a scalar viscous conservation law in multi-D. We consider a strictly convex flux, as a small perturbation of the Burgers flux, along the normal direction to the shock front.","However, for the transversal directions, we do not have any restrictions on flux function.","We first show the contraction property for any large perturbations in $L^2$ of the planar viscous shock.","If the initial $L^2$-perturbation is also in $L^1$, the large perturbation converges to zero in $L^2$ as time goes to infinity with $t^{-1/4}$ decay rate.","The contraction and decay estimates hold up to dynamical shift.","For the results, we do not impose any smallness conditions on the initial value.","This result extends the 1D case \\cite{Kang-V-1} by the first author and Vasseur to the multi-dimensional case."],"url":"http://arxiv.org/abs/2403.08445v1","category":"math.AP"}
{"created":"2024-03-13 11:50:40","title":"Increasing stability for inverse acoustic source problems in the time domain","abstract":"This paper is concerned with inverse source problems for the acoustic wave equation in the full space R^3, where the source term is compactly supported in both time and spatial variables. The main goal is to investigate increasing stability for the wave equation in terms of the interval length of given parameters (e.g., bandwith of the temporal component of the source function). We establish increasing stability estimates of the L^2 -norm of the source function by using only the Dirichlet boundary data. Our method relies on the Huygens principle, the Fourier transform and explicit bounds for the continuation of analytic functions.","sentences":["This paper is concerned with inverse source problems for the acoustic wave equation in the full space R^3, where the source term is compactly supported in both time and spatial variables.","The main goal is to investigate increasing stability for the wave equation in terms of the interval length of given parameters (e.g., bandwith of the temporal component of the source function).","We establish increasing stability estimates of the L^2 -norm of the source function by using only the Dirichlet boundary data.","Our method relies on the Huygens principle, the Fourier transform and explicit bounds for the continuation of analytic functions."],"url":"http://arxiv.org/abs/2403.08440v1","category":"math.AP"}
{"created":"2024-03-13 11:46:38","title":"Characterisation of Anti-Arrhythmic Drug Effects on Cardiac Electrophysiology using Physics-Informed Neural Networks","abstract":"The ability to accurately infer cardiac electrophysiological (EP) properties is key to improving arrhythmia diagnosis and treatment. In this work, we developed a physics-informed neural networks (PINNs) framework to predict how different myocardial EP parameters are modulated by anti-arrhythmic drugs. Using $\\textit{in vitro}$ optical mapping images and the 3-channel Fenton-Karma model, we estimated the changes in ionic channel conductance caused by these drugs.   Our framework successfully characterised the action of drugs HMR1556, nifedipine and lidocaine - respectively, blockade of $I_{K}$, $I_{Ca}$, and $I_{Na}$ currents - by estimating that they decreased the respective channel conductance by $31.8\\pm2.7\\%$ $(p=8.2 \\times 10^{-5})$, $80.9\\pm21.6\\%$ $(p=0.02)$, and $8.6\\pm0.5\\%$ $ (p=0.03)$, leaving the conductance of other channels unchanged. For carbenoxolone, whose main action is the blockade of intercellular gap junctions, PINNs also successfully predicted no significant changes $(p>0.09)$ in all ionic conductances.   Our results are an important step towards the deployment of PINNs for model parameter estimation from experimental data, bringing this framework closer to clinical or laboratory images analysis and for the personalisation of mathematical models.","sentences":["The ability to accurately infer cardiac electrophysiological (EP) properties is key to improving arrhythmia diagnosis and treatment.","In this work, we developed a physics-informed neural networks (PINNs) framework to predict how different myocardial EP parameters are modulated by anti-arrhythmic drugs.","Using $\\textit{in vitro}$ optical mapping images and the 3-channel Fenton-Karma model, we estimated the changes in ionic channel conductance caused by these drugs.   ","Our framework successfully characterised the action of drugs HMR1556, nifedipine and lidocaine - respectively, blockade of $I_{K}$, $I_{Ca}$, and $I_{Na}$ currents - by estimating that they decreased the respective channel conductance by $31.8\\pm2.7\\%$ $(p=8.2 \\times 10^{-5})$, $80.9\\pm21.6\\%$ $(p=0.02)$, and $8.6\\pm0.5\\%$ $ (p=0.03)$, leaving the conductance of other channels unchanged.","For carbenoxolone, whose main action is the blockade of intercellular gap junctions, PINNs also successfully predicted no significant changes $(p>0.09)$ in all ionic conductances.   ","Our results are an important step towards the deployment of PINNs for model parameter estimation from experimental data, bringing this framework closer to clinical or laboratory images analysis and for the personalisation of mathematical models."],"url":"http://arxiv.org/abs/2403.08439v1","category":"q-bio.QM"}
{"created":"2024-03-13 11:05:40","title":"The Development and Performance of a Machine Learning Based Mobile Platform for Visually Determining the Etiology of Penile Pathology","abstract":"Machine-learning algorithms can facilitate low-cost, user-guided visual diagnostic platforms for addressing disparities in access to sexual health services. We developed a clinical image dataset using original and augmented images for five penile diseases: herpes eruption, syphilitic chancres, penile candidiasis, penile cancer, and genital warts. We used a U-net architecture model for semantic pixel segmentation into background or subject image, the Inception-ResNet version 2 neural architecture to classify each pixel as diseased or non-diseased, and a salience map using GradCAM++. We trained the model on a random 91% sample of the image database using 150 epochs per image, and evaluated the model on the remaining 9% of images, assessing recall (or sensitivity), precision, specificity, and F1-score (accuracy). Of the 239 images in the validation dataset, 45 (18.8%) were of genital warts, 43 (18.0%) were of HSV infection, 29 (12.1%) were of penile cancer, 40 (16.7%) were of penile candidiasis, 37 (15.5%) were of syphilitic chancres, and 45 (18.8%) were of non-diseased penises. The overall accuracy of the model for correctly classifying the diseased image was 0.944. Between July 1st and October 1st 2023, there were 2,640 unique users of the mobile platform. Among a random sample of submissions (n=437), 271 (62.0%) were from the United States, 64 (14.6%) from Singapore, 41 (9.4%) from Candia, 40 (9.2%) from the United Kingdom, and 21 (4.8%) from Vietnam. The majority (n=277 [63.4%]) were between 18 and 30 years old. We report on the development of a machine-learning model for classifying five penile diseases, which demonstrated excellent performance on a validation dataset. That model is currently in use globally and has the potential to improve access to diagnostic services for penile diseases.","sentences":["Machine-learning algorithms can facilitate low-cost, user-guided visual diagnostic platforms for addressing disparities in access to sexual health services.","We developed a clinical image dataset using original and augmented images for five penile diseases: herpes eruption, syphilitic chancres, penile candidiasis, penile cancer, and genital warts.","We used a U-net architecture model for semantic pixel segmentation into background or subject image, the Inception-ResNet version 2 neural architecture to classify each pixel as diseased or non-diseased, and a salience map using GradCAM++.","We trained the model on a random 91% sample of the image database using 150 epochs per image, and evaluated the model on the remaining 9% of images, assessing recall (or sensitivity), precision, specificity, and F1-score (accuracy).","Of the 239 images in the validation dataset, 45 (18.8%) were of genital warts, 43 (18.0%) were of HSV infection, 29 (12.1%) were of penile cancer, 40 (16.7%) were of penile candidiasis, 37 (15.5%) were of syphilitic chancres, and 45 (18.8%) were of non-diseased penises.","The overall accuracy of the model for correctly classifying the diseased image was 0.944.","Between July 1st and October 1st 2023, there were 2,640 unique users of the mobile platform.","Among a random sample of submissions (n=437), 271 (62.0%) were from the United States, 64 (14.6%) from Singapore, 41 (9.4%) from Candia, 40 (9.2%) from the United Kingdom, and 21 (4.8%) from Vietnam.","The majority (n=277","[63.4%]) were between 18 and 30 years old.","We report on the development of a machine-learning model for classifying five penile diseases, which demonstrated excellent performance on a validation dataset.","That model is currently in use globally and has the potential to improve access to diagnostic services for penile diseases."],"url":"http://arxiv.org/abs/2403.08417v1","category":"eess.IV"}
{"created":"2024-03-13 10:57:22","title":"Robust Distributed Compression with Learned Heegard-Berger Scheme","abstract":"We consider lossy compression of an information source when decoder-only side information may be absent. This setup, also referred to as the Heegard-Berger or Kaspi problem, is a special case of robust distributed source coding. Building upon previous works on neural network-based distributed compressors developed for the decoder-only side information (Wyner-Ziv) case, we propose learning-based schemes that are amenable to the availability of side information. We find that our learned compressors mimic the achievability part of the Heegard-Berger theorem and yield interpretable results operating close to information-theoretic bounds. Depending on the availability of the side information, our neural compressors recover characteristics of the point-to-point (i.e., with no side information) and the Wyner-Ziv coding strategies that include binning in the source space, although no structure exploiting knowledge of the source and side information was imposed into the design.","sentences":["We consider lossy compression of an information source when decoder-only side information may be absent.","This setup, also referred to as the Heegard-Berger or Kaspi problem, is a special case of robust distributed source coding.","Building upon previous works on neural network-based distributed compressors developed for the decoder-only side information (Wyner-Ziv) case, we propose learning-based schemes that are amenable to the availability of side information.","We find that our learned compressors mimic the achievability part of the Heegard-Berger theorem and yield interpretable results operating close to information-theoretic bounds.","Depending on the availability of the side information, our neural compressors recover characteristics of the point-to-point (i.e., with no side information) and the Wyner-Ziv coding strategies that include binning in the source space, although no structure exploiting knowledge of the source and side information was imposed into the design."],"url":"http://arxiv.org/abs/2403.08411v1","category":"cs.IT"}
{"created":"2024-03-13 10:37:50","title":"The prescribed Ricci curvature problem on 5-dimensional nilpotent Lie groups","abstract":"In this paper, using the Milnor-type theorem technique, we provide on each nilpotent five dimensional Lie group, some global existence result of a pair (g, c) consisting of a left-invariant Riemannian metric g and a positive constant c such that Ric(g) =cT, where Ric(g) is the Ricci curvature of g and T a given left-invariant symmetric (0, 2)-tensor field.","sentences":["In this paper, using the Milnor-type theorem technique, we provide on each nilpotent five dimensional Lie group, some global existence result of a pair (g, c) consisting of a left-invariant Riemannian metric g and a positive constant c such that Ric(g) =cT, where Ric(g) is the Ricci curvature of g and T a given left-invariant symmetric (0, 2)-tensor field."],"url":"http://arxiv.org/abs/2403.08402v1","category":"math.DG"}
{"created":"2024-03-13 09:49:14","title":"Unraveling many-body effects in ZnO: Combined study using momentum-resolved electron energy-loss spectroscopy and first-principles calculations","abstract":"We present a detailed study of the dielectric response of ZnO using a combination of low-loss momentum-resolved electron energy-loss spectroscopy (EELS) and first-principles calculations at several levels of theory, from the independent particle and the random phase approximation with different variants of density functional theory (DFT), including hybrid and DFT$+U$ schemes; to the Bethe-Salpeter equation (BSE). We use a method based on the $f$-sum rule to obtain the momentum-resolved experimental loss function and absorption spectra from EELS measurements. We characterize the main features in the direct and inverse dielectric functions of ZnO and their dispersion, associating them to single-particle features in the electronic band structure, while highlighting the important role of many-body effects such as plasmons and excitons. We discuss different signatures of the high anisotropy in the response function of ZnO, including the symmetry of the excitonic wave-functions.","sentences":["We present a detailed study of the dielectric response of ZnO using a combination of low-loss momentum-resolved electron energy-loss spectroscopy (EELS) and first-principles calculations at several levels of theory, from the independent particle and the random phase approximation with different variants of density functional theory (DFT), including hybrid and DFT$+U$ schemes; to the Bethe-Salpeter equation (BSE).","We use a method based on the $f$-sum rule to obtain the momentum-resolved experimental loss function and absorption spectra from EELS measurements.","We characterize the main features in the direct and inverse dielectric functions of ZnO and their dispersion, associating them to single-particle features in the electronic band structure, while highlighting the important role of many-body effects such as plasmons and excitons.","We discuss different signatures of the high anisotropy in the response function of ZnO, including the symmetry of the excitonic wave-functions."],"url":"http://arxiv.org/abs/2403.08385v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-13 09:39:15","title":"Nonlinear Manifold Learning Determines Microgel Size from Raman Spectroscopy","abstract":"Polymer particle size constitutes a crucial characteristic of product quality in polymerization. Raman spectroscopy is an established and reliable process analytical technology for in-line concentration monitoring. Recent approaches and some theoretical considerations show a correlation between Raman signals and particle sizes but do not determine polymer size from Raman spectroscopic measurements accurately and reliably. With this in mind, we propose three alternative machine learning workflows to perform this task, all involving diffusion maps, a nonlinear manifold learning technique for dimensionality reduction: (i) directly from diffusion maps, (ii) alternating diffusion maps, and (iii) conformal autoencoder neural networks. We apply the workflows to a data set of Raman spectra with associated size measured via dynamic light scattering of 47 microgel (cross-linked polymer) samples in a diameter range of 208nm to 483 nm. The conformal autoencoders substantially outperform state-of-the-art methods and results for the first time in a promising prediction of polymer size from Raman spectra.","sentences":["Polymer particle size constitutes a crucial characteristic of product quality in polymerization.","Raman spectroscopy is an established and reliable process analytical technology for in-line concentration monitoring.","Recent approaches and some theoretical considerations show a correlation between Raman signals and particle sizes but do not determine polymer size from Raman spectroscopic measurements accurately and reliably.","With this in mind, we propose three alternative machine learning workflows to perform this task, all involving diffusion maps, a nonlinear manifold learning technique for dimensionality reduction: (i) directly from diffusion maps, (ii) alternating diffusion maps, and (iii) conformal autoencoder neural networks.","We apply the workflows to a data set of Raman spectra with associated size measured via dynamic light scattering of 47 microgel (cross-linked polymer) samples in a diameter range of 208nm to 483 nm.","The conformal autoencoders substantially outperform state-of-the-art methods and results for the first time in a promising prediction of polymer size from Raman spectra."],"url":"http://arxiv.org/abs/2403.08376v1","category":"cs.LG"}
{"created":"2024-03-13 08:29:58","title":"Activating Wider Areas in Image Super-Resolution","abstract":"The prevalence of convolution neural networks (CNNs) and vision transformers (ViTs) has markedly revolutionized the area of single-image super-resolution (SISR). To further boost the SR performances, several techniques, such as residual learning and attention mechanism, are introduced, which can be largely attributed to a wider range of activated area, that is, the input pixels that strongly influence the SR results. However, the possibility of further improving SR performance through another versatile vision backbone remains an unresolved challenge. To address this issue, in this paper, we unleash the representation potential of the modern state space model, i.e., Vision Mamba (Vim), in the context of SISR. Specifically, we present three recipes for better utilization of Vim-based models: 1) Integration into a MetaFormer-style block; 2) Pre-training on a larger and broader dataset; 3) Employing complementary attention mechanism, upon which we introduce the MMA. The resulting network MMA is capable of finding the most relevant and representative input pixels to reconstruct the corresponding high-resolution images. Comprehensive experimental analysis reveals that MMA not only achieves competitive or even superior performance compared to state-of-the-art SISR methods but also maintains relatively low memory and computational overheads (e.g., +0.5 dB PSNR elevation on Manga109 dataset with 19.8 M parameters at the scale of 2). Furthermore, MMA proves its versatility in lightweight SR applications. Through this work, we aim to illuminate the potential applications of state space models in the broader realm of image processing rather than SISR, encouraging further exploration in this innovative direction.","sentences":["The prevalence of convolution neural networks (CNNs) and vision transformers (ViTs) has markedly revolutionized the area of single-image super-resolution (SISR).","To further boost the SR performances, several techniques, such as residual learning and attention mechanism, are introduced, which can be largely attributed to a wider range of activated area, that is, the input pixels that strongly influence the SR results.","However, the possibility of further improving SR performance through another versatile vision backbone remains an unresolved challenge.","To address this issue, in this paper, we unleash the representation potential of the modern state space model, i.e., Vision Mamba (Vim), in the context of SISR.","Specifically, we present three recipes for better utilization of Vim-based models: 1) Integration into a MetaFormer-style block; 2) Pre-training on a larger and broader dataset; 3) Employing complementary attention mechanism, upon which we introduce the MMA.","The resulting network MMA is capable of finding the most relevant and representative input pixels to reconstruct the corresponding high-resolution images.","Comprehensive experimental analysis reveals that MMA not only achieves competitive or even superior performance compared to state-of-the-art SISR methods but also maintains relatively low memory and computational overheads (e.g., +0.5 dB PSNR elevation on Manga109 dataset with 19.8 M parameters at the scale of 2).","Furthermore, MMA proves its versatility in lightweight SR applications.","Through this work, we aim to illuminate the potential applications of state space models in the broader realm of image processing rather than SISR, encouraging further exploration in this innovative direction."],"url":"http://arxiv.org/abs/2403.08330v1","category":"cs.CV"}
{"created":"2024-03-13 08:29:12","title":"Structural investigation of the liquid crystalline phases of three homologues from the series of 4-pentylphenyl-4'-n-alkyloxythiobenzoates (n = 9, 10, 11)","abstract":"Polarizing optical microscopy and differential scanning calorimetry are used to determine the phase sequence of three liquid crystalline 4-pentylphenyl-4'-n-alkyloxythiobenzoates with n = 9, 10, 11. The X-ray diffraction method is applied for structural characterization of the liquid crystalline phases. The smectic layer spacing, tilt angle, average distance between the long axes of molecules and correlation length of the short-range order are determined as a function of temperature. For the crystal-like smectic phases with hexagonal or herring-bone packing, the unit cell parameters are obtained. The presence of the tilted hexagonal phase for n = 10, 11 and tilted herring-bone phase for n = 9, 10 is indicated, although the direction of the tilt cannot be determined.","sentences":["Polarizing optical microscopy and differential scanning calorimetry are used to determine the phase sequence of three liquid crystalline 4-pentylphenyl-4'-n-alkyloxythiobenzoates with n = 9, 10, 11.","The X-ray diffraction method is applied for structural characterization of the liquid crystalline phases.","The smectic layer spacing, tilt angle, average distance between the long axes of molecules and correlation length of the short-range order are determined as a function of temperature.","For the crystal-like smectic phases with hexagonal or herring-bone packing, the unit cell parameters are obtained.","The presence of the tilted hexagonal phase for n = 10, 11 and tilted herring-bone phase for n = 9, 10 is indicated, although the direction of the tilt cannot be determined."],"url":"http://arxiv.org/abs/2403.08328v1","category":"cond-mat.soft"}
{"created":"2024-03-13 07:49:50","title":"Is Context Helpful for Chat Translation Evaluation?","abstract":"Despite the recent success of automatic metrics for assessing translation quality, their application in evaluating the quality of machine-translated chats has been limited. Unlike more structured texts like news, chat conversations are often unstructured, short, and heavily reliant on contextual information. This poses questions about the reliability of existing sentence-level metrics in this domain as well as the role of context in assessing the translation quality. Motivated by this, we conduct a meta-evaluation of existing sentence-level automatic metrics, primarily designed for structured domains such as news, to assess the quality of machine-translated chats. We find that reference-free metrics lag behind reference-based ones, especially when evaluating translation quality in out-of-English settings. We then investigate how incorporating conversational contextual information in these metrics affects their performance. Our findings show that augmenting neural learned metrics with contextual information helps improve correlation with human judgments in the reference-free scenario and when evaluating translations in out-of-English settings. Finally, we propose a new evaluation metric, Context-MQM, that utilizes bilingual context with a large language model (LLM) and further validate that adding context helps even for LLM-based evaluation metrics.","sentences":["Despite the recent success of automatic metrics for assessing translation quality, their application in evaluating the quality of machine-translated chats has been limited.","Unlike more structured texts like news, chat conversations are often unstructured, short, and heavily reliant on contextual information.","This poses questions about the reliability of existing sentence-level metrics in this domain as well as the role of context in assessing the translation quality.","Motivated by this, we conduct a meta-evaluation of existing sentence-level automatic metrics, primarily designed for structured domains such as news, to assess the quality of machine-translated chats.","We find that reference-free metrics lag behind reference-based ones, especially when evaluating translation quality in out-of-English settings.","We then investigate how incorporating conversational contextual information in these metrics affects their performance.","Our findings show that augmenting neural learned metrics with contextual information helps improve correlation with human judgments in the reference-free scenario and when evaluating translations in out-of-English settings.","Finally, we propose a new evaluation metric, Context-MQM, that utilizes bilingual context with a large language model (LLM) and further validate that adding context helps even for LLM-based evaluation metrics."],"url":"http://arxiv.org/abs/2403.08314v1","category":"cs.CL"}
{"created":"2024-03-13 07:21:21","title":"Online Multi-Contact Feedback Model Predictive Control for Interactive Robotic Tasks","abstract":"In this paper, we propose a model predictive control (MPC) that accomplishes interactive robotic tasks, in which multiple contacts may occur at unknown locations. To address such scenarios, we made an explicit contact feedback loop in the MPC framework. An algorithm called Multi-Contact Particle Filter with Exploration Particle (MCP-EP) is employed to establish real-time feedback of multi-contact information. Then the interaction locations and forces are accommodated in the MPC framework via a spring contact model. Moreover, we achieved real-time control for a 7 degrees of freedom robot without any simplifying assumptions by employing a Differential-Dynamic-Programming algorithm. We achieved 6.8kHz, 1.9kHz, and 1.8kHz update rates of the MPC for 0, 1, and 2 contacts, respectively. This allows the robot to handle unexpected contacts in real time. Real-world experiments show the effectiveness of the proposed method in various scenarios.","sentences":["In this paper, we propose a model predictive control (MPC) that accomplishes interactive robotic tasks, in which multiple contacts may occur at unknown locations.","To address such scenarios, we made an explicit contact feedback loop in the MPC framework.","An algorithm called Multi-Contact Particle Filter with Exploration Particle (MCP-EP) is employed to establish real-time feedback of multi-contact information.","Then the interaction locations and forces are accommodated in the MPC framework via a spring contact model.","Moreover, we achieved real-time control for a 7 degrees of freedom robot without any simplifying assumptions by employing a Differential-Dynamic-Programming algorithm.","We achieved 6.8kHz, 1.9kHz, and 1.8kHz update rates of the MPC for 0, 1, and 2 contacts, respectively.","This allows the robot to handle unexpected contacts in real time.","Real-world experiments show the effectiveness of the proposed method in various scenarios."],"url":"http://arxiv.org/abs/2403.08302v1","category":"cs.RO"}
{"created":"2024-03-13 06:11:41","title":"VIGFace: Virtual Identity Generation Model for Face Image Synthesis","abstract":"Deep learning-based face recognition continues to face challenges due to its reliance on huge datasets obtained from web crawling, which can be costly to gather and raise significant real-world privacy concerns. To address this issue, we propose VIGFace, a novel framework capable of generating synthetic facial images. Initially, we train the face recognition model using a real face dataset and create a feature space for both real and virtual IDs where virtual prototypes are orthogonal to other prototypes. Subsequently, we generate synthetic images by using the diffusion model based on the feature space. Our proposed framework provides two significant benefits. Firstly, it allows for creating virtual facial images without concerns about portrait rights, guaranteeing that the generated virtual face images are clearly differentiated from existing individuals. Secondly, it serves as an effective augmentation method by incorporating real existing images. Further experiments demonstrate the efficacy of our framework, achieving state-of-the-art results from both perspectives without any external data.","sentences":["Deep learning-based face recognition continues to face challenges due to its reliance on huge datasets obtained from web crawling, which can be costly to gather and raise significant real-world privacy concerns.","To address this issue, we propose VIGFace, a novel framework capable of generating synthetic facial images.","Initially, we train the face recognition model using a real face dataset and create a feature space for both real and virtual IDs where virtual prototypes are orthogonal to other prototypes.","Subsequently, we generate synthetic images by using the diffusion model based on the feature space.","Our proposed framework provides two significant benefits.","Firstly, it allows for creating virtual facial images without concerns about portrait rights, guaranteeing that the generated virtual face images are clearly differentiated from existing individuals.","Secondly, it serves as an effective augmentation method by incorporating real existing images.","Further experiments demonstrate the efficacy of our framework, achieving state-of-the-art results from both perspectives without any external data."],"url":"http://arxiv.org/abs/2403.08277v1","category":"cs.CV"}
{"created":"2024-03-13 05:57:44","title":"Fully discrete finite difference schemes for the Fractional Korteweg-de Vries equation","abstract":"In this paper, we present and analyze fully discrete finite difference schemes designed for solving the initial value problem associated with the fractional Korteweg-de Vries (KdV) equation involving the fractional Laplacian. We design the scheme by introducing the discrete fractional Laplacian operator which is consistent with the continuous operator, and posses certain properties which are instrumental for the convergence analysis. Assuming the initial data (u_0 \\in H^{1+\\alpha}(\\mathbb{R})), where (\\alpha \\in [1,2)), our study establishes the convergence of the approximate solutions obtained by the fully discrete finite difference schemes to a classical solution of the fractional KdV equation. Theoretical results are validated through several numerical illustrations for various values of fractional exponent $\\alpha$. Furthermore, we demonstrate that the Crank-Nicolson finite difference scheme preserves the inherent conserved quantities along with the improved convergence rates.","sentences":["In this paper, we present and analyze fully discrete finite difference schemes designed for solving the initial value problem associated with the fractional Korteweg-de Vries (KdV) equation involving the fractional Laplacian.","We design the scheme by introducing the discrete fractional Laplacian operator which is consistent with the continuous operator, and posses certain properties which are instrumental for the convergence analysis.","Assuming the initial data (u_0 \\in H^{1+\\alpha}(\\mathbb{R})), where (\\alpha \\in [1,2)), our study establishes the convergence of the approximate solutions obtained by the fully discrete finite difference schemes to a classical solution of the fractional KdV equation.","Theoretical results are validated through several numerical illustrations for various values of fractional exponent $\\alpha$.","Furthermore, we demonstrate that the Crank-Nicolson finite difference scheme preserves the inherent conserved quantities along with the improved convergence rates."],"url":"http://arxiv.org/abs/2403.08275v1","category":"math.NA"}
{"created":"2024-03-13 04:57:44","title":"Spin characters of the symmetric group which are proportional to linear characters in characteristic 2","abstract":"For a finite group, it is interesting to determine when two ordinary irreducible representations have the same $p$-modular reduction; that is, when two rows of the decomposition matrix in characteristic $p$ are equal, or equivalently when the corresponding $p$-modular Brauer characters are the same. We complete this task for the double covers of the symmetric group when $p=2$, by determining when the $2$-modular reduction of an irreducible spin representation coincides with a $2$-modular Specht module. In fact, we obtain a more general result: we determine when an irreducible spin representation has $2$-modular Brauer character proportional to that of a Specht module. In the course of the proof, we use induction and restriction functors to construct a function on generalised characters which has the effect of swapping runners in abacus displays for the labelling partitions.","sentences":["For a finite group, it is interesting to determine when two ordinary irreducible representations have the same $p$-modular reduction; that is, when two rows of the decomposition matrix in characteristic $p$ are equal, or equivalently when the corresponding $p$-modular Brauer characters are the same.","We complete this task for the double covers of the symmetric group when $p=2$, by determining when the $2$-modular reduction of an irreducible spin representation coincides with a $2$-modular Specht module.","In fact, we obtain a more general result: we determine when an irreducible spin representation has $2$-modular Brauer character proportional to that of a Specht module.","In the course of the proof, we use induction and restriction functors to construct a function on generalised characters which has the effect of swapping runners in abacus displays for the labelling partitions."],"url":"http://arxiv.org/abs/2403.08243v1","category":"math.RT"}
{"created":"2024-03-13 03:50:07","title":"The long-term evolution of the GW170817 remnant","abstract":"GW170817 represents the first observed binary neutron star merger event by humanity. The observation of GW170817 has identified the correlation between Kilonova, gravitational wave and short GRB. The shocks from GW170817 have the capacity to inject significant thermal and kinetic energies into the interstellar medium and evolve for over a million years. In this letter, we adopt the special relativity fluid dynamics equations to simulate the evolution of the GW170817 remnant over a span of one million years. Our simulations yield the evolution profiles of the velocity, density, mass, radius, luminosity, and energies of the remnant. We estimate that the GW170817 remnant will reach the average maximum luminosity $ 2.56\\times 10^{39}$ erg s$^{-1}$at approximately $3.96\\times 10^4$ yr. At the end of the cooling stage, the contaminated radius and mass are $48.35$ pc and $2.25\\times 10^4 M_{\\odot}$, respectively.","sentences":["GW170817 represents the first observed binary neutron star merger event by humanity.","The observation of GW170817 has identified the correlation between Kilonova, gravitational wave and short GRB.","The shocks from GW170817 have the capacity to inject significant thermal and kinetic energies into the interstellar medium and evolve for over a million years.","In this letter, we adopt the special relativity fluid dynamics equations to simulate the evolution of the GW170817 remnant over a span of one million years.","Our simulations yield the evolution profiles of the velocity, density, mass, radius, luminosity, and energies of the remnant.","We estimate that the GW170817 remnant will reach the average maximum luminosity $ 2.56\\times 10^{39}$ erg s$^{-1}$at approximately $3.96\\times 10^4$ yr.","At the end of the cooling stage, the contaminated radius and mass are $48.35$ pc and $2.25\\times 10^4 M_{\\odot}$, respectively."],"url":"http://arxiv.org/abs/2403.08223v1","category":"astro-ph.HE"}
{"created":"2024-03-13 02:56:43","title":"PMCV hypersurfaces in non-flat pseudo-Riemannian space forms","abstract":"In this paper, we prove that PMCV (i.e. \\Delta\\vec{H} is proportional to \\vec{H}) hypersurface M^n_r of a non-flat pseudo-Riemannian space form N^{n+1}_s(c) with at most two distinct principal curvatures is minimal or locally isoparametric, and compute the mean curvature for the isoparametric ones. As an application, we give full classification results of such non-minimal Lorentzian hypersurfaces of non-flat Lorentz space forms.","sentences":["In this paper, we prove that PMCV (i.e. \\Delta\\vec{H} is proportional to \\vec{H}) hypersurface M^n_r of a non-flat pseudo-Riemannian space form N^{n+1}_s(c) with at most two distinct principal curvatures is minimal or locally isoparametric, and compute the mean curvature for the isoparametric ones.","As an application, we give full classification results of such non-minimal Lorentzian hypersurfaces of non-flat Lorentz space forms."],"url":"http://arxiv.org/abs/2403.08205v1","category":"math.DG"}
{"created":"2024-03-13 02:56:31","title":"AutoDFP: Automatic Data-Free Pruning via Channel Similarity Reconstruction","abstract":"Structured pruning methods are developed to bridge the gap between the massive scale of neural networks and the limited hardware resources. Most current structured pruning methods rely on training datasets to fine-tune the compressed model, resulting in high computational burdens and being inapplicable for scenarios with stringent requirements on privacy and security. As an alternative, some data-free methods have been proposed, however, these methods often require handcraft parameter tuning and can only achieve inflexible reconstruction. In this paper, we propose the Automatic Data-Free Pruning (AutoDFP) method that achieves automatic pruning and reconstruction without fine-tuning. Our approach is based on the assumption that the loss of information can be partially compensated by retaining focused information from similar channels. Specifically, We formulate data-free pruning as an optimization problem, which can be effectively addressed through reinforcement learning. AutoDFP assesses the similarity of channels for each layer and provides this information to the reinforcement learning agent, guiding the pruning and reconstruction process of the network. We evaluate AutoDFP with multiple networks on multiple datasets, achieving impressive compression results. For instance, on the CIFAR-10 dataset, AutoDFP demonstrates a 2.87\\% reduction in accuracy loss compared to the recently proposed data-free pruning method DFPC with fewer FLOPs on VGG-16. Furthermore, on the ImageNet dataset, AutoDFP achieves 43.17\\% higher accuracy than the SOTA method with the same 80\\% preserved ratio on MobileNet-V1.","sentences":["Structured pruning methods are developed to bridge the gap between the massive scale of neural networks and the limited hardware resources.","Most current structured pruning methods rely on training datasets to fine-tune the compressed model, resulting in high computational burdens and being inapplicable for scenarios with stringent requirements on privacy and security.","As an alternative, some data-free methods have been proposed, however, these methods often require handcraft parameter tuning and can only achieve inflexible reconstruction.","In this paper, we propose the Automatic Data-Free Pruning (AutoDFP) method that achieves automatic pruning and reconstruction without fine-tuning.","Our approach is based on the assumption that the loss of information can be partially compensated by retaining focused information from similar channels.","Specifically, We formulate data-free pruning as an optimization problem, which can be effectively addressed through reinforcement learning.","AutoDFP assesses the similarity of channels for each layer and provides this information to the reinforcement learning agent, guiding the pruning and reconstruction process of the network.","We evaluate AutoDFP with multiple networks on multiple datasets, achieving impressive compression results.","For instance, on the CIFAR-10 dataset, AutoDFP demonstrates a 2.87\\% reduction in accuracy loss compared to the recently proposed data-free pruning method DFPC with fewer FLOPs on VGG-16.","Furthermore, on the ImageNet dataset, AutoDFP achieves 43.17\\% higher accuracy than the SOTA method with the same 80\\% preserved ratio on MobileNet-V1."],"url":"http://arxiv.org/abs/2403.08204v1","category":"cs.LG"}
{"created":"2024-03-13 02:55:06","title":"Generalised Taylor dispersion of chiral microswimmers","abstract":"Transport phenomena of microswimmers in fluid flows play a crucial role in various biological processes, including bioconvection and cell sorting. In this paper, we investigate the dispersion behavior of chiral microswimmers in a simple shear flow utilizing the generalized Taylor dispersion (GTD) theory, motivated by biased locomotion of bacterial swimmers known as bacterial rheotaxis. We thus focus on the influence of shear-induced torque effects due to particle chirality, employing an extended Jeffery equation for individual deterministic dynamics. We then numerically calculate macroscopic parameters including averaged swimming velocity and effective diffusion tensor using spherical harmonic expansion, and argue the obtained results based on the fixed points and their stability of the orientational dynamical systems. Our results reveal that chiral effects induce biased locomotion and we observe qualitative transitions in the orientational distribution with increasing Pecl\\'et number, aligning with previous experimental findings. The diffusion tensor analysis highlights significant reduction in the diffusion coefficient perpendicular to the flow plane due to chirality. This suggests potential applications in flow-mediated cell separation and we numerically demonstrate such chirality-induced fluid transportation. The presented methods will be useful in predicting and controlling dispersion behaviors of such chiral microswimmers.","sentences":["Transport phenomena of microswimmers in fluid flows play a crucial role in various biological processes, including bioconvection and cell sorting.","In this paper, we investigate the dispersion behavior of chiral microswimmers in a simple shear flow utilizing the generalized Taylor dispersion (GTD) theory, motivated by biased locomotion of bacterial swimmers known as bacterial rheotaxis.","We thus focus on the influence of shear-induced torque effects due to particle chirality, employing an extended Jeffery equation for individual deterministic dynamics.","We then numerically calculate macroscopic parameters including averaged swimming velocity and effective diffusion tensor using spherical harmonic expansion, and argue the obtained results based on the fixed points and their stability of the orientational dynamical systems.","Our results reveal that chiral effects induce biased locomotion and we observe qualitative transitions in the orientational distribution with increasing Pecl\\'et number, aligning with previous experimental findings.","The diffusion tensor analysis highlights significant reduction in the diffusion coefficient perpendicular to the flow plane due to chirality.","This suggests potential applications in flow-mediated cell separation and we numerically demonstrate such chirality-induced fluid transportation.","The presented methods will be useful in predicting and controlling dispersion behaviors of such chiral microswimmers."],"url":"http://arxiv.org/abs/2403.08201v1","category":"physics.flu-dyn"}
{"created":"2024-03-13 02:23:13","title":"Embedded Translations for Low-resource Automated Glossing","abstract":"We investigate automatic interlinear glossing in low-resource settings. We augment a hard-attentional neural model with embedded translation information extracted from interlinear glossed text. After encoding these translations using large language models, specifically BERT and T5, we introduce a character-level decoder for generating glossed output. Aided by these enhancements, our model demonstrates an average improvement of 3.97\\%-points over the previous state of the art on datasets from the SIGMORPHON 2023 Shared Task on Interlinear Glossing. In a simulated ultra low-resource setting, trained on as few as 100 sentences, our system achieves an average 9.78\\%-point improvement over the plain hard-attentional baseline. These results highlight the critical role of translation information in boosting the system's performance, especially in processing and interpreting modest data sources. Our findings suggest a promising avenue for the documentation and preservation of languages, with our experiments on shared task datasets indicating significant advancements over the existing state of the art.","sentences":["We investigate automatic interlinear glossing in low-resource settings.","We augment a hard-attentional neural model with embedded translation information extracted from interlinear glossed text.","After encoding these translations using large language models, specifically BERT and T5, we introduce a character-level decoder for generating glossed output.","Aided by these enhancements, our model demonstrates an average improvement of 3.97\\%-points over the previous state of the art on datasets from the SIGMORPHON 2023 Shared Task on Interlinear Glossing.","In a simulated ultra low-resource setting, trained on as few as 100 sentences, our system achieves an average 9.78\\%-point improvement over the plain hard-attentional baseline.","These results highlight the critical role of translation information in boosting the system's performance, especially in processing and interpreting modest data sources.","Our findings suggest a promising avenue for the documentation and preservation of languages, with our experiments on shared task datasets indicating significant advancements over the existing state of the art."],"url":"http://arxiv.org/abs/2403.08189v1","category":"cs.CL"}
{"created":"2024-03-13 01:51:30","title":"Tractable Local Equilibria in Non-Concave Games","abstract":"While Online Gradient Descent and other no-regret learning procedures are known to efficiently converge to coarse correlated equilibrium in games where each agent's utility is concave in their own strategy, this is not the case when the utilities are non-concave, a situation that is common in machine learning applications where the agents' strategies are parameterized by deep neural networks, or the agents' utilities are computed by a neural network, or both. Indeed, non-concave games present a host of game-theoretic and optimization challenges: (i) Nash equilibria may fail to exist; (ii) local Nash equilibria exist but are intractable; and (iii) mixed Nash, correlated, and coarse correlated equilibria have infinite support in general, and are intractable. To sidestep these challenges we propose a new solution concept, termed $(\\varepsilon, \\Phi(\\delta))$-local equilibrium, which generalizes local Nash equilibrium in non-concave games, as well as (coarse) correlated equilibrium in concave games. Importantly, we show that two instantiations of this solution concept capture the convergence guarantees of Online Gradient Descent and no-regret learning, which we show efficiently converge to this type of equilibrium in non-concave games with smooth utilities.","sentences":["While Online Gradient Descent and other no-regret learning procedures are known to efficiently converge to coarse correlated equilibrium in games where each agent's utility is concave in their own strategy, this is not the case when the utilities are non-concave, a situation that is common in machine learning applications where the agents' strategies are parameterized by deep neural networks, or the agents' utilities are computed by a neural network, or both.","Indeed, non-concave games present a host of game-theoretic and optimization challenges: (i) Nash equilibria may fail to exist; (ii) local Nash equilibria exist but are intractable; and (iii) mixed Nash, correlated, and coarse correlated equilibria have infinite support in general, and are intractable.","To sidestep these challenges we propose a new solution concept, termed $(\\varepsilon, \\Phi(\\delta))$-local equilibrium, which generalizes local Nash equilibrium in non-concave games, as well as (coarse) correlated equilibrium in concave games.","Importantly, we show that two instantiations of this solution concept capture the convergence guarantees of Online Gradient Descent and no-regret learning, which we show efficiently converge to this type of equilibrium in non-concave games with smooth utilities."],"url":"http://arxiv.org/abs/2403.08171v1","category":"cs.GT"}
{"created":"2024-03-13 01:32:43","title":"A Darcy law with memory by homogenisation for evolving microstructure","abstract":"We consider the homogenisation of the instationary Stokes equations in a porous medium with an a-priori given evolving microstructure. In order to pass to the homogenisation limit, we transform the Stokes equations to a domain with a fixed periodic microstructure. The homogenisation result is a Darcy-type equation with memory term and has the form of an integro-differential equation. The evolving microstructure leads to a time and space dependent permeability coefficient and the local change of the porosity causes an additional source term for the pressure.","sentences":["We consider the homogenisation of the instationary Stokes equations in a porous medium with an a-priori given evolving microstructure.","In order to pass to the homogenisation limit, we transform the Stokes equations to a domain with a fixed periodic microstructure.","The homogenisation result is a Darcy-type equation with memory term and has the form of an integro-differential equation.","The evolving microstructure leads to a time and space dependent permeability coefficient and the local change of the porosity causes an additional source term for the pressure."],"url":"http://arxiv.org/abs/2403.08166v1","category":"math.AP"}
{"created":"2024-03-13 01:27:57","title":"EM-TTS: Efficiently Trained Low-Resource Mongolian Lightweight Text-to-Speech","abstract":"Recently, deep learning-based Text-to-Speech (TTS) systems have achieved high-quality speech synthesis results. Recurrent neural networks have become a standard modeling technique for sequential data in TTS systems and are widely used. However, training a TTS model which includes RNN components requires powerful GPU performance and takes a long time. In contrast, CNN-based sequence synthesis techniques can significantly reduce the parameters and training time of a TTS model while guaranteeing a certain performance due to their high parallelism, which alleviate these economic costs of training. In this paper, we propose a lightweight TTS system based on deep convolutional neural networks, which is a two-stage training end-to-end TTS model and does not employ any recurrent units. Our model consists of two stages: Text2Spectrum and SSRN. The former is used to encode phonemes into a coarse mel spectrogram and the latter is used to synthesize the complete spectrum from the coarse mel spectrogram. Meanwhile, we improve the robustness of our model by a series of data augmentations, such as noise suppression, time warping, frequency masking and time masking, for solving the low resource mongolian problem. Experiments show that our model can reduce the training time and parameters while ensuring the quality and naturalness of the synthesized speech compared to using mainstream TTS models. Our method uses NCMMSC2022-MTTSC Challenge dataset for validation, which significantly reduces training time while maintaining a certain accuracy.","sentences":["Recently, deep learning-based Text-to-Speech (TTS) systems have achieved high-quality speech synthesis results.","Recurrent neural networks have become a standard modeling technique for sequential data in TTS systems and are widely used.","However, training a TTS model which includes RNN components requires powerful GPU performance and takes a long time.","In contrast, CNN-based sequence synthesis techniques can significantly reduce the parameters and training time of a TTS model while guaranteeing a certain performance due to their high parallelism, which alleviate these economic costs of training.","In this paper, we propose a lightweight TTS system based on deep convolutional neural networks, which is a two-stage training end-to-end TTS model and does not employ any recurrent units.","Our model consists of two stages: Text2Spectrum and SSRN.","The former is used to encode phonemes into a coarse mel spectrogram and the latter is used to synthesize the complete spectrum from the coarse mel spectrogram.","Meanwhile, we improve the robustness of our model by a series of data augmentations, such as noise suppression, time warping, frequency masking and time masking, for solving the low resource mongolian problem.","Experiments show that our model can reduce the training time and parameters while ensuring the quality and naturalness of the synthesized speech compared to using mainstream TTS models.","Our method uses NCMMSC2022-MTTSC Challenge dataset for validation, which significantly reduces training time while maintaining a certain accuracy."],"url":"http://arxiv.org/abs/2403.08164v1","category":"cs.SD"}
{"created":"2024-03-13 00:59:25","title":"Asymptotics of Random Feature Regression Beyond the Linear Scaling Regime","abstract":"Recent advances in machine learning have been achieved by using overparametrized models trained until near interpolation of the training data. It was shown, e.g., through the double descent phenomenon, that the number of parameters is a poor proxy for the model complexity and generalization capabilities. This leaves open the question of understanding the impact of parametrization on the performance of these models. How does model complexity and generalization depend on the number of parameters $p$? How should we choose $p$ relative to the sample size $n$ to achieve optimal test error?   In this paper, we investigate the example of random feature ridge regression (RFRR). This model can be seen either as a finite-rank approximation to kernel ridge regression (KRR), or as a simplified model for neural networks trained in the so-called lazy regime. We consider covariates uniformly distributed on the $d$-dimensional sphere and compute sharp asymptotics for the RFRR test error in the high-dimensional polynomial scaling, where $p,n,d \\to \\infty$ while $p/ d^{\\kappa_1}$ and $n / d^{\\kappa_2}$ stay constant, for all $\\kappa_1 , \\kappa_2 \\in \\mathbb{R}_{>0}$. These asymptotics precisely characterize the impact of the number of random features and regularization parameter on the test performance. In particular, RFRR exhibits an intuitive trade-off between approximation and generalization power. For $n = o(p)$, the sample size $n$ is the bottleneck and RFRR achieves the same performance as KRR (which is equivalent to taking $p = \\infty$). On the other hand, if $p = o(n)$, the number of random features $p$ is the limiting factor and RFRR test error matches the approximation error of the random feature model class (akin to taking $n = \\infty$). Finally, a double descent appears at $n= p$, a phenomenon that was previously only characterized in the linear scaling $\\kappa_1 = \\kappa_2 = 1$.","sentences":["Recent advances in machine learning have been achieved by using overparametrized models trained until near interpolation of the training data.","It was shown, e.g., through the double descent phenomenon, that the number of parameters is a poor proxy for the model complexity and generalization capabilities.","This leaves open the question of understanding the impact of parametrization on the performance of these models.","How does model complexity and generalization depend on the number of parameters $p$?","How should we choose $p$ relative to the sample size $n$ to achieve optimal test error?   ","In this paper, we investigate the example of random feature ridge regression (RFRR).","This model can be seen either as a finite-rank approximation to kernel ridge regression (KRR), or as a simplified model for neural networks trained in the so-called lazy regime.","We consider covariates uniformly distributed on the $d$-dimensional sphere and compute sharp asymptotics for the RFRR test error in the high-dimensional polynomial scaling, where $p,n,d \\to \\infty$ while $p/ d^{\\kappa_1}$ and $n / d^{\\kappa_2}$ stay constant, for all $\\kappa_1 , \\kappa_2 \\in \\mathbb{R}_{>0}$. These asymptotics precisely characterize the impact of the number of random features and regularization parameter on the test performance.","In particular, RFRR exhibits an intuitive trade-off between approximation and generalization power.","For $n = o(p)$, the sample size $n$ is the bottleneck and RFRR achieves the same performance as KRR (which is equivalent to taking $p = \\infty$).","On the other hand, if $p = o(n)$, the number of random features $p$ is the limiting factor and RFRR test error matches the approximation error of the random feature model class (akin to taking $n = \\infty$).","Finally, a double descent appears at $n= p$, a phenomenon that was previously only characterized in the linear scaling $\\kappa_1 = \\kappa_2 = 1$."],"url":"http://arxiv.org/abs/2403.08160v1","category":"stat.ML"}
{"created":"2024-03-13 00:54:33","title":"Dipolar BF theory and dipolar braiding statistics","abstract":"We analyze the recently proposed dipolar BF theory with couplings to charge and dipole currents. The quasiparticles of the theory are either charge-like or dipole-like, and the mutual braiding statistics between charge-like and dipole-like quasiparticles are dipolar, meaning that it depends on the position of the quasiparticle being encircled. The braiding statistics between two dipole-like quasiparticles is that of ordinary anyons. We further prove that the dipolar BF theory is equivalent to the rank-2 tensor BF theory developed earlier as an effective theory for the rank-2 toric code. Although the two theories are equivalent, the dipolar BF formulation embodies the dipole symmetry explicitly and gives a clean insight into the way the dipole symmetry manifests itself in various conservation laws and the dipolar braiding statistics.","sentences":["We analyze the recently proposed dipolar BF theory with couplings to charge and dipole currents.","The quasiparticles of the theory are either charge-like or dipole-like, and the mutual braiding statistics between charge-like and dipole-like quasiparticles are dipolar, meaning that it depends on the position of the quasiparticle being encircled.","The braiding statistics between two dipole-like quasiparticles is that of ordinary anyons.","We further prove that the dipolar BF theory is equivalent to the rank-2 tensor BF theory developed earlier as an effective theory for the rank-2 toric code.","Although the two theories are equivalent, the dipolar BF formulation embodies the dipole symmetry explicitly and gives a clean insight into the way the dipole symmetry manifests itself in various conservation laws and the dipolar braiding statistics."],"url":"http://arxiv.org/abs/2403.08158v1","category":"cond-mat.str-el"}
{"created":"2024-03-13 00:48:41","title":"Multiscale Low-Frequency Memory Network for Improved Feature Extraction in Convolutional Neural Networks","abstract":"Deep learning and Convolutional Neural Networks (CNNs) have driven major transformations in diverse research areas. However, their limitations in handling low-frequency information present obstacles in certain tasks like interpreting global structures or managing smooth transition images. Despite the promising performance of transformer structures in numerous tasks, their intricate optimization complexities highlight the persistent need for refined CNN enhancements using limited resources. Responding to these complexities, we introduce a novel framework, the Multiscale Low-Frequency Memory (MLFM) Network, with the goal to harness the full potential of CNNs while keeping their complexity unchanged. The MLFM efficiently preserves low-frequency information, enhancing performance in targeted computer vision tasks. Central to our MLFM is the Low-Frequency Memory Unit (LFMU), which stores various low-frequency data and forms a parallel channel to the core network. A key advantage of MLFM is its seamless compatibility with various prevalent networks, requiring no alterations to their original core structure. Testing on ImageNet demonstrated substantial accuracy improvements in multiple 2D CNNs, including ResNet, MobileNet, EfficientNet, and ConvNeXt. Furthermore, we showcase MLFM's versatility beyond traditional image classification by successfully integrating it into image-to-image translation tasks, specifically in semantic segmentation networks like FCN and U-Net. In conclusion, our work signifies a pivotal stride in the journey of optimizing the efficacy and efficiency of CNNs with limited resources. This research builds upon the existing CNN foundations and paves the way for future advancements in computer vision. Our codes are available at https://github.com/AlphaWuSeu/ MLFM.","sentences":["Deep learning and Convolutional Neural Networks (CNNs) have driven major transformations in diverse research areas.","However, their limitations in handling low-frequency information present obstacles in certain tasks like interpreting global structures or managing smooth transition images.","Despite the promising performance of transformer structures in numerous tasks, their intricate optimization complexities highlight the persistent need for refined CNN enhancements using limited resources.","Responding to these complexities, we introduce a novel framework, the Multiscale Low-Frequency Memory (MLFM) Network, with the goal to harness the full potential of CNNs while keeping their complexity unchanged.","The MLFM efficiently preserves low-frequency information, enhancing performance in targeted computer vision tasks.","Central to our MLFM is the Low-Frequency Memory Unit (LFMU), which stores various low-frequency data and forms a parallel channel to the core network.","A key advantage of MLFM is its seamless compatibility with various prevalent networks, requiring no alterations to their original core structure.","Testing on ImageNet demonstrated substantial accuracy improvements in multiple 2D CNNs, including ResNet, MobileNet, EfficientNet, and ConvNeXt.","Furthermore, we showcase MLFM's versatility beyond traditional image classification by successfully integrating it into image-to-image translation tasks, specifically in semantic segmentation networks like FCN and U-Net.","In conclusion, our work signifies a pivotal stride in the journey of optimizing the efficacy and efficiency of CNNs with limited resources.","This research builds upon the existing CNN foundations and paves the way for future advancements in computer vision.","Our codes are available at https://github.com/AlphaWuSeu/ MLFM."],"url":"http://arxiv.org/abs/2403.08157v1","category":"cs.CV"}
{"created":"2024-03-13 00:17:48","title":"Nodal solutions to Paneitz-type equations","abstract":"On a closed Riemannian manifold $(M^n ,g)$ with a proper isoparametric function $f$ we consider the equation $\\Delta^2 u -\\alpha \\Delta u +\\beta u = u^q$, where $\\alpha$ and $\\beta$ are positive constants satisfying that $\\alpha^2 \\geq 4 \\beta$. We let ${\\bf m}$ be the minimum of the dimensions of the focal varieties of $f$ and $q_f = \\frac{n-{\\bf m}+4}{n-{\\bf m}-4}$, $q_f = \\infty$ if $n\\leq {\\bf m}+4$. We prove the existence of infinitely many nodal solutions of the equation assuming that $1<q<q_f$. The solutions are $f$-invariant. To obtain the result, first we prove a $C^0-$estimate for positive $f$-invariant solutions of the equation. Then we prove the existence of mountain pass solutions with arbitrarily large energy.","sentences":["On a closed Riemannian manifold $(M^n ,g)$ with a proper isoparametric function $f$ we consider the equation $\\Delta^2 u -\\alpha \\Delta u +\\beta u = u^q$, where $\\alpha$ and $\\beta$ are positive constants satisfying that $\\alpha^2 \\geq 4 \\beta$. We let ${\\bf m}$ be the minimum of the dimensions of the focal varieties of $f$ and $q_f = \\frac{n-{\\bf m}+4}{n-{\\bf m}-4}$, $q_f = \\infty$ if $n\\leq {\\bf m}+4$. We prove the existence of infinitely many nodal solutions of the equation assuming that $1<q<q_f$. The solutions are $f$-invariant.","To obtain the result, first we prove a $C^0-$estimate for positive $f$-invariant solutions of the equation.","Then we prove the existence of mountain pass solutions with arbitrarily large energy."],"url":"http://arxiv.org/abs/2403.08146v1","category":"math.AP"}
{"created":"2024-03-12 23:54:33","title":"Superspecial genus-$4$ double covers of elliptic curves","abstract":"In this paper we study genus-$4$ curves obtained as double covers of elliptic curves. Firstly we give explicit defining equations of such curves with explicit criterion for whether it is nonsingular, and show the irreducibility of the long polynomial determining whether the curve is nonsingular, in any characteristic $\\ne 2,3$. Secondly as an application we enumerate superspecial genus-$4$ double covers of elliptic curves in small characteristic.","sentences":["In this paper we study genus-$4$ curves obtained as double covers of elliptic curves.","Firstly we give explicit defining equations of such curves with explicit criterion for whether it is nonsingular, and show the irreducibility of the long polynomial determining whether the curve is nonsingular, in any characteristic $\\ne 2,3$.","Secondly as an application we enumerate superspecial genus-$4$ double covers of elliptic curves in small characteristic."],"url":"http://arxiv.org/abs/2403.08139v1","category":"math.AG"}
{"created":"2024-03-12 23:27:30","title":"Q-SLAM: Quadric Representations for Monocular SLAM","abstract":"Monocular SLAM has long grappled with the challenge of accurately modeling 3D geometries. Recent advances in Neural Radiance Fields (NeRF)-based monocular SLAM have shown promise, yet these methods typically focus on novel view synthesis rather than precise 3D geometry modeling. This focus results in a significant disconnect between NeRF applications, i.e., novel-view synthesis and the requirements of SLAM. We identify that the gap results from the volumetric representations used in NeRF, which are often dense and noisy. In this study, we propose a novel approach that reimagines volumetric representations through the lens of quadric forms. We posit that most scene components can be effectively represented as quadric planes. Leveraging this assumption, we reshape the volumetric representations with million of cubes by several quadric planes, which leads to more accurate and efficient modeling of 3D scenes in SLAM contexts. Our method involves two key steps: First, we use the quadric assumption to enhance coarse depth estimations obtained from tracking modules, e.g., Droid-SLAM. This step alone significantly improves depth estimation accuracy. Second, in the subsequent mapping phase, we diverge from previous NeRF-based SLAM methods that distribute sampling points across the entire volume space. Instead, we concentrate sampling points around quadric planes and aggregate them using a novel quadric-decomposed Transformer. Additionally, we introduce an end-to-end joint optimization strategy that synchronizes pose estimation with 3D reconstruction.","sentences":["Monocular SLAM has long grappled with the challenge of accurately modeling 3D geometries.","Recent advances in Neural Radiance Fields (NeRF)-based monocular SLAM have shown promise, yet these methods typically focus on novel view synthesis rather than precise 3D geometry modeling.","This focus results in a significant disconnect between NeRF applications, i.e., novel-view synthesis and the requirements of SLAM.","We identify that the gap results from the volumetric representations used in NeRF, which are often dense and noisy.","In this study, we propose a novel approach that reimagines volumetric representations through the lens of quadric forms.","We posit that most scene components can be effectively represented as quadric planes.","Leveraging this assumption, we reshape the volumetric representations with million of cubes by several quadric planes, which leads to more accurate and efficient modeling of 3D scenes in SLAM contexts.","Our method involves two key steps:","First, we use the quadric assumption to enhance coarse depth estimations obtained from tracking modules, e.g., Droid-SLAM.","This step alone significantly improves depth estimation accuracy.","Second, in the subsequent mapping phase, we diverge from previous NeRF-based SLAM methods that distribute sampling points across the entire volume space.","Instead, we concentrate sampling points around quadric planes and aggregate them using a novel quadric-decomposed Transformer.","Additionally, we introduce an end-to-end joint optimization strategy that synchronizes pose estimation with 3D reconstruction."],"url":"http://arxiv.org/abs/2403.08125v1","category":"cs.CV"}
{"created":"2024-03-12 22:56:41","title":"Cyclic homology of categorical coalgebras and the free loop space","abstract":"We prove that the cyclic chain complex of the categorical coalgebra of singular chains on an arbitrary topological space $X$ is naturally quasi-isomorphic to the $S^1$-equivariant chains of free loop space of $X$. This statement does not require any hypotheses on $X$ or on the commutative ring of coefficients. Along the way, we introduce a family of polytopes, coined as Goodwillie polytopes, that controls the combinatorics behind the relationship of the coHochschild complex of a categorical coalgebra and the Hochschild complex of its associated differential graded category.","sentences":["We prove that the cyclic chain complex of the categorical coalgebra of singular chains on an arbitrary topological space $X$ is naturally quasi-isomorphic to the $S^1$-equivariant chains of free loop space of $X$. This statement does not require any hypotheses on $X$ or on the commutative ring of coefficients.","Along the way, we introduce a family of polytopes, coined as Goodwillie polytopes, that controls the combinatorics behind the relationship of the coHochschild complex of a categorical coalgebra and the Hochschild complex of its associated differential graded category."],"url":"http://arxiv.org/abs/2403.08116v1","category":"math.AT"}
{"created":"2024-03-12 21:49:49","title":"Nucleon charge and magnetisation distributions: flavour separation and zeroes","abstract":"A symmetry-preserving truncation of the quantum field equations describing hadron properties is used to deliver parameter-free predictions for all nucleon elastic electromagnetic form factors and their flavour separation to large values of momentum transfer, $Q^2$. The proton electric form factor, $G_E^p$, possesses a zero, whereas that of the neutron, $G_E^n$, does not. The difference owes to the behaviour of the Pauli form factor of the proton's singly-represented valence $d$-quark. Consequently, $G_E^n>G_E^p$ on a material large-$Q^2$ domain. These predictions can be tested in modern experiments.","sentences":["A symmetry-preserving truncation of the quantum field equations describing hadron properties is used to deliver parameter-free predictions for all nucleon elastic electromagnetic form factors and their flavour separation to large values of momentum transfer, $Q^2$. The proton electric form factor, $G_E^p$, possesses a zero, whereas that of the neutron, $G_E^n$, does not.","The difference owes to the behaviour of the Pauli form factor of the proton's singly-represented valence $d$-quark.","Consequently, $G_E^n>G_E^p$ on a material large-$Q^2$ domain.","These predictions can be tested in modern experiments."],"url":"http://arxiv.org/abs/2403.08088v1","category":"hep-ph"}
{"created":"2024-03-12 21:26:29","title":"Extending Irksome: improvements in automated Runge--Kutta time stepping for finite element methods","abstract":"Irksome is a library based on the Unified Form Language (UFL) that enables automated generation of Runge--Kutta methods for time-stepping finite element spatial discretizations of partial differential equations (PDE). Allowing users to express semidiscrete forms of PDE, it generates UFL representations for the stage-coupled variational problems to be solved at each time step. The Firedrake package then generates efficient code for evaluating these variational problems and allows users a wide range of options to deploy efficient algebraic solvers in PETSc.   In this paper, we describe several recent advances in Irksome. These include alternate formulations of the Runge--Kutta time-stepping methods and optimized support for diagonally implicit (DIRK) methods. Additionally, we present new and improved tools for building preconditioners for the resulting linear and linearized systems, demonstrating that these can lead to efficient approaches for solving fully implicit Runge-Kutta discretizations.   The new features are demonstrated through a sequence of computational examples demonstrating the high-level interface and obtained solver performance.","sentences":["Irksome is a library based on the Unified Form Language (UFL) that enables automated generation of Runge--Kutta methods for time-stepping finite element spatial discretizations of partial differential equations (PDE).","Allowing users to express semidiscrete forms of PDE, it generates UFL representations for the stage-coupled variational problems to be solved at each time step.","The Firedrake package then generates efficient code for evaluating these variational problems and allows users a wide range of options to deploy efficient algebraic solvers in PETSc.   ","In this paper, we describe several recent advances in Irksome.","These include alternate formulations of the Runge--Kutta time-stepping methods and optimized support for diagonally implicit (DIRK) methods.","Additionally, we present new and improved tools for building preconditioners for the resulting linear and linearized systems, demonstrating that these can lead to efficient approaches for solving fully implicit Runge-Kutta discretizations.   ","The new features are demonstrated through a sequence of computational examples demonstrating the high-level interface and obtained solver performance."],"url":"http://arxiv.org/abs/2403.08084v1","category":"math.NA"}
{"created":"2024-03-12 21:11:55","title":"Automated discovery of reprogrammable nonlinear dynamic metamaterials","abstract":"Harnessing the rich nonlinear dynamics of highly-deformable materials has the potential to unlock the next generation of functional smart materials and devices. However, unlocking such potential requires effective strategies to spatially design optimal material architectures for desired nonlinear dynamic responses such as guiding of nonlinear elastic waves, energy focusing, and cloaking. Here, we introduce an inverse-design framework for the discovery of flexible mechanical metamaterials with a target nonlinear dynamic response. The desired dynamic task is encoded via optimal tuning of the full-scale metamaterial geometry through an inverse-design approach powered by a custom-developed fully-differentiable simulation environment. By deploying such strategy, we design mechanical metamaterials tailored for energy focusing, energy splitting, dynamic protection, and nonlinear motion conversion. Furthermore, we illustrate that our design framework can be expanded to automatically discover reprogrammable architectures capable of switching between different dynamic tasks. For instance, we encode two strongly competing tasks -- energy focusing and dynamic protection -- within a single architecture, utilizing static pre-compression to switch between these behaviors. The discovered designs are physically realized and experimentally tested, demonstrating the robustness of the engineered tasks. All together, our approach opens an untapped avenue towards designer materials with tailored robotic-like reprogrammable functionalities.","sentences":["Harnessing the rich nonlinear dynamics of highly-deformable materials has the potential to unlock the next generation of functional smart materials and devices.","However, unlocking such potential requires effective strategies to spatially design optimal material architectures for desired nonlinear dynamic responses such as guiding of nonlinear elastic waves, energy focusing, and cloaking.","Here, we introduce an inverse-design framework for the discovery of flexible mechanical metamaterials with a target nonlinear dynamic response.","The desired dynamic task is encoded via optimal tuning of the full-scale metamaterial geometry through an inverse-design approach powered by a custom-developed fully-differentiable simulation environment.","By deploying such strategy, we design mechanical metamaterials tailored for energy focusing, energy splitting, dynamic protection, and nonlinear motion conversion.","Furthermore, we illustrate that our design framework can be expanded to automatically discover reprogrammable architectures capable of switching between different dynamic tasks.","For instance, we encode two strongly competing tasks -- energy focusing and dynamic protection -- within a single architecture, utilizing static pre-compression to switch between these behaviors.","The discovered designs are physically realized and experimentally tested, demonstrating the robustness of the engineered tasks.","All together, our approach opens an untapped avenue towards designer materials with tailored robotic-like reprogrammable functionalities."],"url":"http://arxiv.org/abs/2403.08078v1","category":"physics.app-ph"}
{"created":"2024-03-12 20:55:35","title":"Minimal-Ambiguity Scattering Matrix Estimation with Load-Tunable Ports","abstract":"We address the following generic wave problem: is the estimation of an arbitrarily complex linear $N$-port system's scattering matrix possible if waves can be input and output only via $N_\\mathrm{A}<N$ ports while the remaining $N_\\mathrm{S}=N-N_\\mathrm{A}$ ports are terminated with tunable loads? Fundamentally, this problem is intriguing because it ultimately probes to what extent inherent structure in Maxwell's equations constrains the scattering coefficients. Various limited versions of the problem are of temporary scientific and technological interest, ranging from optimal non-invasive focusing on perturbation-inducing targets in complex media, via the characterization of miniaturized, embedded, receive-only and/or multi-element antenna systems to physics-compliant end-to-end channel models for complex metasurface-programmable \"smart radio environments\". More generally, solutions to the problem may yield promising measurement techniques to characterize an arbitrary linear $N$-port system with an $N_\\mathrm{A}$-port measurement device, where $N_\\mathrm{A} \\ll N$. We show theoretically that if $N_\\mathrm{A}\\geq 2$ and at least three distinct tunable loads are available, the problem can be solved except for sign ambiguities on the off-diagonal scattering coefficients involving the $N_\\mathrm{S}$ not-directly-accessible (NDA) ports. If the transmission from at least one accessible port to the NDA ports can be measured, the sign ambiguity can be lifted. We corroborate our results with microwave experiments on an 8-port chaotic cavity with $N_\\mathrm{A}=N_\\mathrm{S}=4$. Moreover, we reveal additional constraining structure in Maxwell's equations by showing that a limitation to phase-insensitive measurements only results in a mild additional blockwise phase ambiguity that can be lifted simultaneously with the sign ambiguity.","sentences":["We address the following generic wave problem: is the estimation of an arbitrarily complex linear $N$-port system's scattering matrix possible if waves can be input and output only via $N_\\mathrm{A}<N$ ports while the remaining $N_\\mathrm{S}=N-N_\\mathrm{A}$ ports are terminated with tunable loads?","Fundamentally, this problem is intriguing because it ultimately probes to what extent inherent structure in Maxwell's equations constrains the scattering coefficients.","Various limited versions of the problem are of temporary scientific and technological interest, ranging from optimal non-invasive focusing on perturbation-inducing targets in complex media, via the characterization of miniaturized, embedded, receive-only and/or multi-element antenna systems to physics-compliant end-to-end channel models for complex metasurface-programmable \"smart radio environments\".","More generally, solutions to the problem may yield promising measurement techniques to characterize an arbitrary linear $N$-port system with an $N_\\mathrm{A}$-port measurement device, where $N_\\mathrm{A} \\ll N$. We show theoretically that if $N_\\mathrm{A}\\geq 2$ and at least three distinct tunable loads are available, the problem can be solved except for sign ambiguities on the off-diagonal scattering coefficients involving the $N_\\mathrm{S}$ not-directly-accessible (NDA) ports.","If the transmission from at least one accessible port to the NDA ports can be measured, the sign ambiguity can be lifted.","We corroborate our results with microwave experiments on an 8-port chaotic cavity with $N_\\mathrm{A}=N_\\mathrm{S}=4$. Moreover, we reveal additional constraining structure in Maxwell's equations by showing that a limitation to phase-insensitive measurements only results in a mild additional blockwise phase ambiguity that can be lifted simultaneously with the sign ambiguity."],"url":"http://arxiv.org/abs/2403.08074v1","category":"physics.app-ph"}
{"created":"2024-03-12 20:45:10","title":"On the Ashbaugh-Benguria type conjecture about lower-order Neumann eigenvalues of the Witten-Laplacian","abstract":"An isoperimetric inequality for lower order nonzero Neumann eigenvalues of the Witten-Laplacian on bounded domains in a Euclidean space or a hyperbolic space has been proven in this paper. About this conclusion, we would like to point out two things:   It strengthens the well-known Szeg\\H{o}-Weinberger inequality for nonzero Neumann eigenvalues of the classical free membrane problem given in [J. Rational Mech. Anal. 3 (1954) 343--356] and [J. Rational Mech. Anal. 5 (1956) 633--636];   Recently, Xia-Wang [Math. Ann. 385 (2023) 863--879] gave a very important progress to the celebrated conjecture of M. S. Ashbaugh and R. D. Benguria proposed in [SIAM J. Math. Anal. 24 (1993) 557--570]. It is easy to see that our conclusion here covers Xia-Wang's this progress as a special case.   In this paper, we have also proposed two open problems which can be seen as a generalization of Ashbaugh-Benguria's conjecture mentioned above.","sentences":["An isoperimetric inequality for lower order nonzero Neumann eigenvalues of the Witten-Laplacian on bounded domains in a Euclidean space or a hyperbolic space has been proven in this paper.","About this conclusion, we would like to point out two things:   It strengthens the well-known Szeg\\H{o}-Weinberger inequality for nonzero Neumann eigenvalues of the classical free membrane problem given in [J. Rational Mech.","Anal. 3 (1954) 343--356] and [J. Rational Mech.","Anal. 5 (1956) 633--636];   ","Recently, Xia-Wang [Math.","Ann. 385 (2023) 863--879] gave a very important progress to the celebrated conjecture of M. S. Ashbaugh and R. D. Benguria proposed in [SIAM J. Math.","Anal. 24 (1993) 557--570].","It is easy to see that our conclusion here covers Xia-Wang's this progress as a special case.   ","In this paper, we have also proposed two open problems which can be seen as a generalization of Ashbaugh-Benguria's conjecture mentioned above."],"url":"http://arxiv.org/abs/2403.08070v1","category":"math.AP"}
{"created":"2024-03-12 20:31:16","title":"System Design Approach for Control of Differentially Private Dynamical Systems","abstract":"This paper introduces a novel approach to concurrently design dynamic controllers and correlated differential privacy noise in dynamic control systems. An increase in privacy noise increases the system's privacy but adversely affects the system's performance. Our approach optimizes the noise distribution while shaping closed-loop system dynamics such that the privacy noise has the least impact on system performance and the most effect on system privacy. We further add privacy noise to both control input and system output to privatize the system's state for an adversary with access to both communication channels and direct output measurements. The study also suggests tailored privacy bounds for different states, providing a comprehensive framework for jointly optimizing system performance and privacy in the context of differential privacy.","sentences":["This paper introduces a novel approach to concurrently design dynamic controllers and correlated differential privacy noise in dynamic control systems.","An increase in privacy noise increases the system's privacy but adversely affects the system's performance.","Our approach optimizes the noise distribution while shaping closed-loop system dynamics such that the privacy noise has the least impact on system performance and the most effect on system privacy.","We further add privacy noise to both control input and system output to privatize the system's state for an adversary with access to both communication channels and direct output measurements.","The study also suggests tailored privacy bounds for different states, providing a comprehensive framework for jointly optimizing system performance and privacy in the context of differential privacy."],"url":"http://arxiv.org/abs/2403.08065v1","category":"cs.SY"}
{"created":"2024-03-12 20:02:39","title":"DrivAerNet: A Parametric Car Dataset for Data-Driven Aerodynamic Design and Graph-Based Drag Prediction","abstract":"This study introduces DrivAerNet, a large-scale high-fidelity CFD dataset of 3D industry-standard car shapes, and RegDGCNN, a dynamic graph convolutional neural network model, both aimed at aerodynamic car design through machine learning. DrivAerNet, with its 4000 detailed 3D car meshes using 0.5 million surface mesh faces and comprehensive aerodynamic performance data comprising of full 3D pressure, velocity fields, and wall-shear stresses, addresses the critical need for extensive datasets to train deep learning models in engineering applications. It is 60\\% larger than the previously available largest public dataset of cars, and is the only open-source dataset that also models wheels and underbody. RegDGCNN leverages this large-scale dataset to provide high-precision drag estimates directly from 3D meshes, bypassing traditional limitations such as the need for 2D image rendering or Signed Distance Fields (SDF). By enabling fast drag estimation in seconds, RegDGCNN facilitates rapid aerodynamic assessments, offering a substantial leap towards integrating data-driven methods in automotive design. Together, DrivAerNet and RegDGCNN promise to accelerate the car design process and contribute to the development of more efficient vehicles. To lay the groundwork for future innovations in the field, the dataset and code used in our study are publicly accessible at \\url{https://github.com/Mohamedelrefaie/DrivAerNet}","sentences":["This study introduces DrivAerNet, a large-scale high-fidelity CFD dataset of 3D industry-standard car shapes, and RegDGCNN, a dynamic graph convolutional neural network model, both aimed at aerodynamic car design through machine learning.","DrivAerNet, with its 4000 detailed 3D car meshes using 0.5 million surface mesh faces and comprehensive aerodynamic performance data comprising of full 3D pressure, velocity fields, and wall-shear stresses, addresses the critical need for extensive datasets to train deep learning models in engineering applications.","It is 60\\% larger than the previously available largest public dataset of cars, and is the only open-source dataset that also models wheels and underbody.","RegDGCNN leverages this large-scale dataset to provide high-precision drag estimates directly from 3D meshes, bypassing traditional limitations such as the need for 2D image rendering or Signed Distance Fields (SDF).","By enabling fast drag estimation in seconds, RegDGCNN facilitates rapid aerodynamic assessments, offering a substantial leap towards integrating data-driven methods in automotive design.","Together, DrivAerNet and RegDGCNN promise to accelerate the car design process and contribute to the development of more efficient vehicles.","To lay the groundwork for future innovations in the field, the dataset and code used in our study are publicly accessible at \\url{https://github.com/Mohamedelrefaie/DrivAerNet}"],"url":"http://arxiv.org/abs/2403.08055v1","category":"cs.LG"}
{"created":"2024-03-12 19:57:31","title":"A Computational Method for $H_2$-optimal Estimator and State Feedback Controller Synthesis for PDEs","abstract":"In this paper, we present solvable, convex formulations of $H_2$-optimal state estimation and state-feedback control problems for a general class of linear Partial Differential Equations (PDEs) with one spatial dimension. These convex formulations are derived by using an analysis and control framework called the `Partial Integral Equation' (PIE) framework, which utilizes the PIE representation of infinite-dimensional systems. Since PIEs are parameterized by Partial Integral (PI) operators that form an algebra, $H_2$-optimal estimation and control problems for PIEs can be formulated as Linear PI Inequalities (LPIs). Furthermore, if a PDE admits a PIE representation, then the stability and $H_2$ performance of the PIE system implies that of the PDE system. Consequently, the optimal estimator and controller obtained for a PIE using LPIs provide the same stability and performance when applied to the corresponding PDE. These LPI optimization problems can be solved computationally using semi-definite programming solvers because such problems can be formulated using Linear Matrix Inequalities by using positive matrices to parameterize a cone of positive PI operators. We illustrate the application of these methods by constructing observers and controllers for some standard PDE examples.","sentences":["In this paper, we present solvable, convex formulations of $H_2$-optimal state estimation and state-feedback control problems for a general class of linear Partial Differential Equations (PDEs) with one spatial dimension.","These convex formulations are derived by using an analysis and control framework called the `Partial Integral Equation' (PIE) framework, which utilizes the PIE representation of infinite-dimensional systems.","Since PIEs are parameterized by Partial Integral (PI) operators that form an algebra, $H_2$-optimal estimation and control problems for PIEs can be formulated as Linear PI Inequalities (LPIs).","Furthermore, if a PDE admits a PIE representation, then the stability and $H_2$ performance of the PIE system implies that of the PDE system.","Consequently, the optimal estimator and controller obtained for a PIE using LPIs provide the same stability and performance when applied to the corresponding PDE.","These LPI optimization problems can be solved computationally using semi-definite programming solvers because such problems can be formulated using Linear Matrix Inequalities by using positive matrices to parameterize a cone of positive PI operators.","We illustrate the application of these methods by constructing observers and controllers for some standard PDE examples."],"url":"http://arxiv.org/abs/2403.08052v1","category":"math.OC"}
{"created":"2024-03-12 19:01:20","title":"HOLISMOKES -- XII. Time-delay Measurements of Strongly Lensed Type Ia Supernovae using a Long Short-Term Memory Network","abstract":"Strongly lensed Type Ia supernovae (LSNe Ia) are a promising probe to measure the Hubble constant ($H_0$) directly. To use LSNe Ia for cosmography, a time-delay measurement between the multiple images, a lens-mass model, and a mass reconstruction along the line of sight are required. In this work, we present the machine learning network LSTM-FCNN which is a combination of a Long Short-Term Memory Network (LSTM) and a fully-connected neural network (FCNN). The LSTM-FCNN is designed to measure time delays on a sample of LSNe Ia spanning a broad range of properties, which we expect to find with the upcoming Rubin Observatory Legacy Survey of Space and Time (LSST) and for which follow-up observations are planned. With follow-up observations in $i$ band (cadence of one to three days with a single-epoch $5\\sigma$ depth of 24.5 mag), we reach a bias-free delay measurement with a precision around 0.7 days over a large sample of LSNe Ia. The LSTM-FCNN is far more general than previous machine learning approaches such as the Random Forest (RF), where a RF has to be trained for each observational pattern separately, and yet the LSTM-FCNN outperforms the RF by a factor of roughly three. Therefore, the LSTM-FCNN is a very promising approach to achieve robust time delays in LSNe Ia, which is important for a precise and accurate constraint on $H_0$","sentences":["Strongly lensed Type Ia supernovae (LSNe Ia) are a promising probe to measure the Hubble constant ($H_0$) directly.","To use LSNe Ia for cosmography, a time-delay measurement between the multiple images, a lens-mass model, and a mass reconstruction along the line of sight are required.","In this work, we present the machine learning network LSTM-FCNN which is a combination of a Long Short-Term Memory Network (LSTM) and a fully-connected neural network (FCNN).","The LSTM-FCNN is designed to measure time delays on a sample of LSNe Ia spanning a broad range of properties, which we expect to find with the upcoming Rubin Observatory Legacy Survey of Space and Time (LSST) and for which follow-up observations are planned.","With follow-up observations in $i$ band (cadence of one to three days with a single-epoch $5\\sigma$ depth of 24.5 mag), we reach a bias-free delay measurement with a precision around 0.7 days over a large sample of LSNe Ia. The LSTM-FCNN is far more general than previous machine learning approaches such as the Random Forest (RF), where a RF has to be trained for each observational pattern separately, and yet the LSTM-FCNN outperforms the RF by a factor of roughly three.","Therefore, the LSTM-FCNN is a very promising approach to achieve robust time delays in LSNe Ia, which is important for a precise and accurate constraint on $H_0$"],"url":"http://arxiv.org/abs/2403.08029v1","category":"astro-ph.CO"}
{"created":"2024-03-12 18:46:56","title":"xMLP: Revolutionizing Private Inference with Exclusive Square Activation","abstract":"Private Inference (PI) enables deep neural networks (DNNs) to work on private data without leaking sensitive information by exploiting cryptographic primitives such as multi-party computation (MPC) and homomorphic encryption (HE). However, the use of non-linear activations such as ReLU in DNNs can lead to impractically high PI latency in existing PI systems, as ReLU requires the use of costly MPC computations, such as Garbled Circuits. Since square activations can be processed by Beaver's triples hundreds of times faster compared to ReLU, they are more friendly to PI tasks, but using them leads to a notable drop in model accuracy. This paper starts by exploring the reason for such an accuracy drop after using square activations, and concludes that this is due to an \"information compounding\" effect. Leveraging this insight, we propose xMLP, a novel DNN architecture that uses square activations exclusively while maintaining parity in both accuracy and efficiency with ReLU-based DNNs. Our experiments on CIFAR-100 and ImageNet show that xMLP models consistently achieve better performance than ResNet models with fewer activation layers and parameters while maintaining consistent performance with its ReLU-based variants. Remarkably, when compared to state-of-the-art PI Models, xMLP demonstrates superior performance, achieving a 0.58% increase in accuracy with 7x faster PI speed. Moreover, it delivers a significant accuracy improvement of 4.96% while maintaining the same PI latency. When offloading PI to the GPU, xMLP is up to 700x faster than the previous state-of-the-art PI model with comparable accuracy.","sentences":["Private Inference (PI) enables deep neural networks (DNNs) to work on private data without leaking sensitive information by exploiting cryptographic primitives such as multi-party computation (MPC) and homomorphic encryption (HE).","However, the use of non-linear activations such as ReLU in DNNs can lead to impractically high PI latency in existing PI systems, as ReLU requires the use of costly MPC computations, such as Garbled Circuits.","Since square activations can be processed by Beaver's triples hundreds of times faster compared to ReLU, they are more friendly to PI tasks, but using them leads to a notable drop in model accuracy.","This paper starts by exploring the reason for such an accuracy drop after using square activations, and concludes that this is due to an \"information compounding\" effect.","Leveraging this insight, we propose xMLP, a novel DNN architecture that uses square activations exclusively while maintaining parity in both accuracy and efficiency with ReLU-based DNNs.","Our experiments on CIFAR-100 and ImageNet show that xMLP models consistently achieve better performance than ResNet models with fewer activation layers and parameters while maintaining consistent performance with its ReLU-based variants.","Remarkably, when compared to state-of-the-art PI Models, xMLP demonstrates superior performance, achieving a 0.58% increase in accuracy with 7x faster PI speed.","Moreover, it delivers a significant accuracy improvement of 4.96% while maintaining the same PI latency.","When offloading PI to the GPU, xMLP is up to 700x faster than the previous state-of-the-art PI model with comparable accuracy."],"url":"http://arxiv.org/abs/2403.08024v1","category":"cs.LG"}
{"created":"2024-03-12 18:13:25","title":"Stable free boundary minimal hypersurfaces in locally wedge-shaped manifolds","abstract":"We prove that a stable $C^{1,1}$-to-edge properly embedded free boundary minimal hypersurface $\\Sigma^3$ of a $4$-dimensional wedge domain $\\Omega^4_{\\theta}$ with angle $\\theta\\in (0,\\pi]$ is flat.","sentences":["We prove that a stable $C^{1,1}$-to-edge properly embedded free boundary minimal hypersurface $\\Sigma^3$ of a $4$-dimensional wedge domain $\\Omega^4_{\\theta}$ with angle $\\theta\\in (0,\\pi]$ is flat."],"url":"http://arxiv.org/abs/2403.08005v1","category":"math.DG"}
{"created":"2024-03-12 18:10:55","title":"Existence and uniqueness of weak solutions for the generalized stochastic Navier-Stokes-Voigt equations","abstract":"In this work, we consider the incompressible generalized Navier-Stokes-Voigt equations in a bounded domain $\\mathcal{O}\\subset\\mathbb{R}^d$, $d\\geq 2$, driven by a multiplicative Gaussian noise. The considered momentum equation is given by:   \\begin{align*}   \\mathrm{d}\\left(\\boldsymbol{u} - \\kappa \\Delta \\boldsymbol{u}\\right) = \\left[\\boldsymbol{f} +\\operatorname{div} \\left(-\\pi\\mathbf{I}+\\nu|\\mathbf{D}(\\boldsymbol{u})|^{p-2}\\mathbf{D}(\\boldsymbol{u})-\\boldsymbol{u}\\otimes \\boldsymbol{u}\\right)\\right]\\mathrm{d} t + \\Phi(\\boldsymbol{u})\\mathrm{d} \\mathrm{W}(t).   \\end{align*} In the case of $d=2,3$, $\\boldsymbol{u}$ accounts for the velocity field, $\\pi$ is the pressure, $\\boldsymbol{f}$ is a body force and the final term stay for the stochastic forces. Here, $\\kappa$ and $\\nu$ are given positive constants that account for the kinematic viscosity and relaxation time, and the power-law index $p$ is another constant (assumed $p>1$) that characterizes the flow. We use the usual notation $\\mathbf{I}$ for the unit tensor and $\\mathbf{D}(\\boldsymbol{u}):=\\frac{1}{2}\\left(\\nabla \\boldsymbol{u} + (\\nabla \\boldsymbol{u})^{\\top}\\right)$ for the symmetric part of velocity gradient. For $p\\in\\big(\\frac{2d}{d+2},\\infty\\big)$, we first prove the existence of a martingale solution. Then we show the pathwise uniqueness of solutions. We employ the classical Yamada-Watanabe theorem to ensure the existence of a unique probabilistic strong solution.Then we show the pathwise uniqueness of solutions. We employ the classical Yamada-Watanabe theorem to ensure the existence of a unique probabilistic strong solution.","sentences":["In this work, we consider the incompressible generalized Navier-Stokes-Voigt equations in a bounded domain $\\mathcal{O}\\subset\\mathbb{R}^d$, $d\\geq 2$, driven by a multiplicative Gaussian noise.","The considered momentum equation is given by:   \\begin{align*}   \\mathrm{d}\\left(\\boldsymbol{u} - \\kappa \\Delta \\boldsymbol{u}\\right)","= \\left[\\boldsymbol{f} +\\operatorname{div} \\left(-\\pi\\mathbf{I}+\\nu|\\mathbf{D}(\\boldsymbol{u})|^{p-2}\\mathbf{D}(\\boldsymbol{u})-\\boldsymbol{u}\\otimes \\boldsymbol{u}\\right)\\right]\\mathrm{d} t + \\Phi(\\boldsymbol{u})\\mathrm{d} \\mathrm{W}(t).   ","\\end{align*} In the case of $d=2,3$, $\\boldsymbol{u}$ accounts for the velocity field, $\\pi$ is the pressure, $\\boldsymbol{f}$ is a body force and the final term stay for the stochastic forces.","Here, $\\kappa$ and $\\nu$ are given positive constants that account for the kinematic viscosity and relaxation time, and the power-law index $p$ is another constant (assumed $p>1$) that characterizes the flow.","We use the usual notation $\\mathbf{I}$ for the unit tensor and $\\mathbf{D}(\\boldsymbol{u}):=\\frac{1}{2}\\left(\\nabla \\boldsymbol{u} + (\\nabla \\boldsymbol{u})^{\\top}\\right)$ for the symmetric part of velocity gradient.","For $p\\in\\big(\\frac{2d}{d+2},\\infty\\big)$, we first prove the existence of a martingale solution.","Then we show the pathwise uniqueness of solutions.","We employ the classical Yamada-Watanabe theorem to ensure the existence of a unique probabilistic strong solution.","Then we show the pathwise uniqueness of solutions.","We employ the classical Yamada-Watanabe theorem to ensure the existence of a unique probabilistic strong solution."],"url":"http://arxiv.org/abs/2403.08001v1","category":"math.PR"}
{"created":"2024-03-12 18:00:25","title":"Nucleation Rate in a High-Temperature Quantum Field Theory with Hard Particles","abstract":"We study the effects of thermal, hard particles on nucleation rates in high-temperature quantum field theories (QFTs). The hard particles, which form the heat bath for nucleating fields, can be regarded as the most qualitatively distinct aspect of high-temperature QFTs in comparison with Langer's classical nucleation theory: They have their own dynamics and interactions with the nucleating fields, and consequently they do not obey a purely equilibrium distribution during nucleation. We focus on the situation where the hard particles are correctly described by a Vlasov equation, and show that the hard particles can be included into Langer's framework, with only the dynamical part of the nucleation rate changing. The statistical part retains its form and is in accordance with the effective field theory approach to thermal nucleation. It is also shown that the dominant damping from a light bosonic quantum field comes from the corresponding long-wavelength classical field instead of the corresponding bosonic hard particles.","sentences":["We study the effects of thermal, hard particles on nucleation rates in high-temperature quantum field theories (QFTs).","The hard particles, which form the heat bath for nucleating fields, can be regarded as the most qualitatively distinct aspect of high-temperature QFTs in comparison with Langer's classical nucleation theory: They have their own dynamics and interactions with the nucleating fields, and consequently they do not obey a purely equilibrium distribution during nucleation.","We focus on the situation where the hard particles are correctly described by a Vlasov equation, and show that the hard particles can be included into Langer's framework, with only the dynamical part of the nucleation rate changing.","The statistical part retains its form and is in accordance with the effective field theory approach to thermal nucleation.","It is also shown that the dominant damping from a light bosonic quantum field comes from the corresponding long-wavelength classical field instead of the corresponding bosonic hard particles."],"url":"http://arxiv.org/abs/2403.07987v1","category":"hep-ph"}
{"created":"2024-03-12 18:00:03","title":"Angular momentum sensitivities in scalar-tensor theories","abstract":"Scalar-tensor theories have a long history as possible phenomenological alternatives to General Relativity, but are known to potentially produce deviations from the (strong) equivalence principle in systems involving self-gravitating objects, as a result of the presence of an additional gravitational scalar field besides the tensor modes of General Relativity. We describe here a novel mechanism whereby the equivalence principle is violated for an isolated rotating neutron star, if the gravitational scalar field is changing in time far from the system. We show that the neutron star rotational period changes due to an effective coupling (\"angular momentum sensitivity\") to the gravitational scalar, and compute that coupling for viable equations of state for nuclear matter. We comment on the relevance of our findings for testing scalar-tensor theories and models of ultralight dark matter with pulsar timing observations, a topic that we tackle in a companion paper.","sentences":["Scalar-tensor theories have a long history as possible phenomenological alternatives to General Relativity, but are known to potentially produce deviations from the (strong) equivalence principle in systems involving self-gravitating objects, as a result of the presence of an additional gravitational scalar field besides the tensor modes of General Relativity.","We describe here a novel mechanism whereby the equivalence principle is violated for an isolated rotating neutron star, if the gravitational scalar field is changing in time far from the system.","We show that the neutron star rotational period changes due to an effective coupling (\"angular momentum sensitivity\") to the gravitational scalar, and compute that coupling for viable equations of state for nuclear matter.","We comment on the relevance of our findings for testing scalar-tensor theories and models of ultralight dark matter with pulsar timing observations, a topic that we tackle in a companion paper."],"url":"http://arxiv.org/abs/2403.07980v1","category":"gr-qc"}
{"created":"2024-03-13 17:58:34","title":"Segmentation of Knee Bones for Osteoarthritis Assessment: A Comparative Analysis of Supervised, Few-Shot, and Zero-Shot Learning Approaches","abstract":"Knee osteoarthritis is a degenerative joint disease that induces chronic pain and disability. Bone morphological analysis is a promising tool to understand the mechanical aspect of this disorder. This study proposes a 2D bone morphological analysis using manually segmented bones to explore morphological features related to distinct pain conditions. Furthermore, six semantic segmentation algorithms are assessed for extracting femur and tibia bones from X-ray images. Our analysis reveals that the morphology of the femur undergoes significant changes in instances where pain worsens. Conversely, improvements in pain may not manifest pronounced alterations in bone shape. The few-shot-learning-based algorithm, UniverSeg, demonstrated superior segmentation results with Dice scores of 99.69% for femur and 99.60% for tibia. Regarding pain condition classification, the zero-shot-learning-based algorithm, CP-SAM, achieved the highest accuracy at 66% among all models. UniverSeg is recommended for automatic knee bone segmentation, while SAM models show potential with prompt encoder modifications for optimized outcomes. These findings highlight the effectiveness of few-shot learning for semantic segmentation and the potential of zero-shot learning in enhancing classification models for knee osteoarthritis diagnosis.","sentences":["Knee osteoarthritis is a degenerative joint disease that induces chronic pain and disability.","Bone morphological analysis is a promising tool to understand the mechanical aspect of this disorder.","This study proposes a 2D bone morphological analysis using manually segmented bones to explore morphological features related to distinct pain conditions.","Furthermore, six semantic segmentation algorithms are assessed for extracting femur and tibia bones from X-ray images.","Our analysis reveals that the morphology of the femur undergoes significant changes in instances where pain worsens.","Conversely, improvements in pain may not manifest pronounced alterations in bone shape.","The few-shot-learning-based algorithm, UniverSeg, demonstrated superior segmentation results with Dice scores of 99.69% for femur and 99.60% for tibia.","Regarding pain condition classification, the zero-shot-learning-based algorithm, CP-SAM, achieved the highest accuracy at 66% among all models.","UniverSeg is recommended for automatic knee bone segmentation, while SAM models show potential with prompt encoder modifications for optimized outcomes.","These findings highlight the effectiveness of few-shot learning for semantic segmentation and the potential of zero-shot learning in enhancing classification models for knee osteoarthritis diagnosis."],"url":"http://arxiv.org/abs/2403.08761v1","category":"eess.IV"}
{"created":"2024-03-13 17:50:59","title":"Real-time 3D semantic occupancy prediction for autonomous vehicles using memory-efficient sparse convolution","abstract":"In autonomous vehicles, understanding the surrounding 3D environment of the ego vehicle in real-time is essential. A compact way to represent scenes while encoding geometric distances and semantic object information is via 3D semantic occupancy maps. State of the art 3D mapping methods leverage transformers with cross-attention mechanisms to elevate 2D vision-centric camera features into the 3D domain. However, these methods encounter significant challenges in real-time applications due to their high computational demands during inference. This limitation is particularly problematic in autonomous vehicles, where GPU resources must be shared with other tasks such as localization and planning. In this paper, we introduce an approach that extracts features from front-view 2D camera images and LiDAR scans, then employs a sparse convolution network (Minkowski Engine), for 3D semantic occupancy prediction. Given that outdoor scenes in autonomous driving scenarios are inherently sparse, the utilization of sparse convolution is particularly apt. By jointly solving the problems of 3D scene completion of sparse scenes and 3D semantic segmentation, we provide a more efficient learning framework suitable for real-time applications in autonomous vehicles. We also demonstrate competitive accuracy on the nuScenes dataset.","sentences":["In autonomous vehicles, understanding the surrounding 3D environment of the ego vehicle in real-time is essential.","A compact way to represent scenes while encoding geometric distances and semantic object information is via 3D semantic occupancy maps.","State of the art 3D mapping methods leverage transformers with cross-attention mechanisms to elevate 2D vision-centric camera features into the 3D domain.","However, these methods encounter significant challenges in real-time applications due to their high computational demands during inference.","This limitation is particularly problematic in autonomous vehicles, where GPU resources must be shared with other tasks such as localization and planning.","In this paper, we introduce an approach that extracts features from front-view 2D camera images and LiDAR scans, then employs a sparse convolution network (Minkowski Engine), for 3D semantic occupancy prediction.","Given that outdoor scenes in autonomous driving scenarios are inherently sparse, the utilization of sparse convolution is particularly apt.","By jointly solving the problems of 3D scene completion of sparse scenes and 3D semantic segmentation, we provide a more efficient learning framework suitable for real-time applications in autonomous vehicles.","We also demonstrate competitive accuracy on the nuScenes dataset."],"url":"http://arxiv.org/abs/2403.08748v1","category":"cs.RO"}
{"created":"2024-03-13 17:42:03","title":"Improving Acoustic Word Embeddings through Correspondence Training of Self-supervised Speech Representations","abstract":"Acoustic word embeddings (AWEs) are vector representations of spoken words. An effective method for obtaining AWEs is the Correspondence Auto-Encoder (CAE). In the past, the CAE method has been associated with traditional MFCC features. Representations obtained from self-supervised learning (SSL)-based speech models such as HuBERT, Wav2vec2, etc., are outperforming MFCC in many downstream tasks. However, they have not been well studied in the context of learning AWEs. This work explores the effectiveness of CAE with SSL-based speech representations to obtain improved AWEs. Additionally, the capabilities of SSL-based speech models are explored in cross-lingual scenarios for obtaining AWEs. Experiments are conducted on five languages: Polish, Portuguese, Spanish, French, and English. HuBERT-based CAE model achieves the best results for word discrimination in all languages, despite Hu-BERT being pre-trained on English only. Also, the HuBERT-based CAE model works well in cross-lingual settings. It outperforms MFCC-based CAE models trained on the target languages when trained on one source language and tested on target languages.","sentences":["Acoustic word embeddings (AWEs) are vector representations of spoken words.","An effective method for obtaining AWEs is the Correspondence Auto-Encoder (CAE).","In the past, the CAE method has been associated with traditional MFCC features.","Representations obtained from self-supervised learning (SSL)-based speech models such as HuBERT, Wav2vec2, etc., are outperforming MFCC in many downstream tasks.","However, they have not been well studied in the context of learning AWEs.","This work explores the effectiveness of CAE with SSL-based speech representations to obtain improved AWEs.","Additionally, the capabilities of SSL-based speech models are explored in cross-lingual scenarios for obtaining AWEs.","Experiments are conducted on five languages: Polish, Portuguese, Spanish, French, and English.","HuBERT-based CAE model achieves the best results for word discrimination in all languages, despite Hu-BERT being pre-trained on English only.","Also, the HuBERT-based CAE model works well in cross-lingual settings.","It outperforms MFCC-based CAE models trained on the target languages when trained on one source language and tested on target languages."],"url":"http://arxiv.org/abs/2403.08738v1","category":"cs.CL"}
{"created":"2024-03-13 15:51:23","title":"HIMap: HybrId Representation Learning for End-to-end Vectorized HD Map Construction","abstract":"Vectorized High-Definition (HD) map construction requires predictions of the category and point coordinates of map elements (e.g. road boundary, lane divider, pedestrian crossing, etc.). State-of-the-art methods are mainly based on point-level representation learning for regressing accurate point coordinates. However, this pipeline has limitations in obtaining element-level information and handling element-level failures, e.g. erroneous element shape or entanglement between elements. To tackle the above issues, we propose a simple yet effective HybrId framework named HIMap to sufficiently learn and interact both point-level and element-level information. Concretely, we introduce a hybrid representation called HIQuery to represent all map elements, and propose a point-element interactor to interactively extract and encode the hybrid information of elements, e.g. point position and element shape, into the HIQuery. Additionally, we present a point-element consistency constraint to enhance the consistency between the point-level and element-level information. Finally, the output point-element integrated HIQuery can be directly converted into map elements' class, point coordinates, and mask. We conduct extensive experiments and consistently outperform previous methods on both nuScenes and Argoverse2 datasets. Notably, our method achieves $77.8$ mAP on the nuScenes dataset, remarkably superior to previous SOTAs by $8.3$ mAP at least.","sentences":["Vectorized High-Definition (HD) map construction requires predictions of the category and point coordinates of map elements (e.g. road boundary, lane divider, pedestrian crossing, etc.).","State-of-the-art methods are mainly based on point-level representation learning for regressing accurate point coordinates.","However, this pipeline has limitations in obtaining element-level information and handling element-level failures, e.g. erroneous element shape or entanglement between elements.","To tackle the above issues, we propose a simple yet effective HybrId framework named HIMap to sufficiently learn and interact both point-level and element-level information.","Concretely, we introduce a hybrid representation called HIQuery to represent all map elements, and propose a point-element interactor to interactively extract and encode the hybrid information of elements, e.g. point position and element shape, into the HIQuery.","Additionally, we present a point-element consistency constraint to enhance the consistency between the point-level and element-level information.","Finally, the output point-element integrated HIQuery can be directly converted into map elements' class, point coordinates, and mask.","We conduct extensive experiments and consistently outperform previous methods on both nuScenes and Argoverse2 datasets.","Notably, our method achieves $77.8$ mAP on the nuScenes dataset, remarkably superior to previous SOTAs by $8.3$ mAP at least."],"url":"http://arxiv.org/abs/2403.08639v1","category":"cs.CV"}
{"created":"2024-03-13 14:54:04","title":"ActionDiffusion: An Action-aware Diffusion Model for Procedure Planning in Instructional Videos","abstract":"We present ActionDiffusion -- a novel diffusion model for procedure planning in instructional videos that is the first to take temporal inter-dependencies between actions into account in a diffusion model for procedure planning. This approach is in stark contrast to existing methods that fail to exploit the rich information content available in the particular order in which actions are performed. Our method unifies the learning of temporal dependencies between actions and denoising of the action plan in the diffusion process by projecting the action information into the noise space. This is achieved 1) by adding action embeddings in the noise masks in the noise-adding phase and 2) by introducing an attention mechanism in the noise prediction network to learn the correlations between different action steps. We report extensive experiments on three instructional video benchmark datasets (CrossTask, Coin, and NIV) and show that our method outperforms previous state-of-the-art methods on all metrics on CrossTask and NIV and all metrics except accuracy on Coin dataset. We show that by adding action embeddings into the noise mask the diffusion model can better learn action temporal dependencies and increase the performances on procedure planning.","sentences":["We present ActionDiffusion -- a novel diffusion model for procedure planning in instructional videos that is the first to take temporal inter-dependencies between actions into account in a diffusion model for procedure planning.","This approach is in stark contrast to existing methods that fail to exploit the rich information content available in the particular order in which actions are performed.","Our method unifies the learning of temporal dependencies between actions and denoising of the action plan in the diffusion process by projecting the action information into the noise space.","This is achieved 1) by adding action embeddings in the noise masks in the noise-adding phase and 2) by introducing an attention mechanism in the noise prediction network to learn the correlations between different action steps.","We report extensive experiments on three instructional video benchmark datasets (CrossTask, Coin, and NIV) and show that our method outperforms previous state-of-the-art methods on all metrics on CrossTask and NIV and all metrics except accuracy on Coin dataset.","We show that by adding action embeddings into the noise mask the diffusion model can better learn action temporal dependencies and increase the performances on procedure planning."],"url":"http://arxiv.org/abs/2403.08591v1","category":"cs.CV"}
{"created":"2024-03-13 14:37:00","title":"Local Binary and Multiclass SVMs Trained on a Quantum Annealer","abstract":"Support vector machines (SVMs) are widely used machine learning models (e.g., in remote sensing), with formulations for both classification and regression tasks. In the last years, with the advent of working quantum annealers, hybrid SVM models characterised by quantum training and classical execution have been introduced. These models have demonstrated comparable performance to their classical counterparts. However, they are limited in the training set size due to the restricted connectivity of the current quantum annealers. Hence, to take advantage of large datasets (like those related to Earth observation), a strategy is required. In the classical domain, local SVMs, namely, SVMs trained on the data samples selected by a k-nearest neighbors model, have already proven successful. Here, the local application of quantum-trained SVM models is proposed and empirically assessed. In particular, this approach allows overcoming the constraints on the training set size of the quantum-trained models while enhancing their performance. In practice, the FaLK-SVM method, designed for efficient local SVMs, has been combined with quantum-trained SVM models for binary and multiclass classification. In addition, for comparison, FaLK-SVM has been interfaced for the first time with a classical single-step multiclass SVM model (CS SVM). Concerning the empirical evaluation, D-Wave's quantum annealers and real-world datasets taken from the remote sensing domain have been employed. The results have shown the effectiveness and scalability of the proposed approach, but also its practical applicability in a real-world large-scale scenario.","sentences":["Support vector machines (SVMs) are widely used machine learning models (e.g., in remote sensing), with formulations for both classification and regression tasks.","In the last years, with the advent of working quantum annealers, hybrid SVM models characterised by quantum training and classical execution have been introduced.","These models have demonstrated comparable performance to their classical counterparts.","However, they are limited in the training set size due to the restricted connectivity of the current quantum annealers.","Hence, to take advantage of large datasets (like those related to Earth observation), a strategy is required.","In the classical domain, local SVMs, namely, SVMs trained on the data samples selected by a k-nearest neighbors model, have already proven successful.","Here, the local application of quantum-trained SVM models is proposed and empirically assessed.","In particular, this approach allows overcoming the constraints on the training set size of the quantum-trained models while enhancing their performance.","In practice, the FaLK-SVM method, designed for efficient local SVMs, has been combined with quantum-trained SVM models for binary and multiclass classification.","In addition, for comparison, FaLK-SVM has been interfaced for the first time with a classical single-step multiclass SVM model (CS SVM).","Concerning the empirical evaluation, D-Wave's quantum annealers and real-world datasets taken from the remote sensing domain have been employed.","The results have shown the effectiveness and scalability of the proposed approach, but also its practical applicability in a real-world large-scale scenario."],"url":"http://arxiv.org/abs/2403.08584v1","category":"cs.ET"}
{"created":"2024-03-13 14:28:02","title":"Caformer: Rethinking Time Series Analysis from Causal Perspective","abstract":"Time series analysis is a vital task with broad applications in various domains. However, effectively capturing cross-dimension and cross-time dependencies in non-stationary time series poses significant challenges, particularly in the context of environmental factors. The spurious correlation induced by the environment confounds the causal relationships between cross-dimension and cross-time dependencies. In this paper, we introduce a novel framework called Caformer (\\underline{\\textbf{Ca}}usal Trans\\underline{\\textbf{former}}) for time series analysis from a causal perspective. Specifically, our framework comprises three components: Dynamic Learner, Environment Learner, and Dependency Learner. The Dynamic Learner unveils dynamic interactions among dimensions, the Environment Learner mitigates spurious correlations caused by environment with a back-door adjustment, and the Dependency Learner aims to infer robust interactions across both time and dimensions. Our Caformer demonstrates consistent state-of-the-art performance across five mainstream time series analysis tasks, including long- and short-term forecasting, imputation, classification, and anomaly detection, with proper interpretability.","sentences":["Time series analysis is a vital task with broad applications in various domains.","However, effectively capturing cross-dimension and cross-time dependencies in non-stationary time series poses significant challenges, particularly in the context of environmental factors.","The spurious correlation induced by the environment confounds the causal relationships between cross-dimension and cross-time dependencies.","In this paper, we introduce a novel framework called Caformer (\\underline{\\textbf{Ca}}usal Trans\\underline{\\textbf{former}}) for time series analysis from a causal perspective.","Specifically, our framework comprises three components: Dynamic Learner, Environment Learner, and Dependency Learner.","The Dynamic Learner unveils dynamic interactions among dimensions, the Environment Learner mitigates spurious correlations caused by environment with a back-door adjustment, and the Dependency Learner aims to infer robust interactions across both time and dimensions.","Our Caformer demonstrates consistent state-of-the-art performance across five mainstream time series analysis tasks, including long- and short-term forecasting, imputation, classification, and anomaly detection, with proper interpretability."],"url":"http://arxiv.org/abs/2403.08572v1","category":"cs.LG"}
{"created":"2024-03-13 13:54:00","title":"Language models scale reliably with over-training and on downstream tasks","abstract":"Scaling laws are useful guides for developing language models, but there are still gaps between current scaling studies and how language models are ultimately trained and evaluated. For instance, scaling is usually studied in the compute-optimal training regime (i.e., \"Chinchilla optimal\" regime); however, in practice, models are often over-trained to reduce inference costs. Moreover, scaling laws mostly predict loss on next-token prediction, but ultimately models are compared based on downstream task performance. In this paper, we address both shortcomings. To do so, we create a testbed of 104 models with 0.011B to 6.9B parameters trained with various numbers of tokens on three data distributions. First, we investigate scaling in the over-trained regime. We fit scaling laws that extrapolate in both the number of model parameters and the ratio of training tokens to parameters. This enables us to predict the validation loss of a 1.4B parameter, 900B token run (i.e., 32$\\times$ over-trained) and a 6.9B parameter, 138B token run$\\unicode{x2014}$each from experiments that take 300$\\times$ less compute. Second, we relate the perplexity of a language model to its downstream task performance via a power law. We use this law to predict top-1 error averaged over downstream tasks for the two aforementioned models using experiments that take 20$\\times$ less compute. Our experiments are available at https://github.com/mlfoundations/scaling.","sentences":["Scaling laws are useful guides for developing language models, but there are still gaps between current scaling studies and how language models are ultimately trained and evaluated.","For instance, scaling is usually studied in the compute-optimal training regime (i.e., \"Chinchilla optimal\" regime); however, in practice, models are often over-trained to reduce inference costs.","Moreover, scaling laws mostly predict loss on next-token prediction, but ultimately models are compared based on downstream task performance.","In this paper, we address both shortcomings.","To do so, we create a testbed of 104 models with 0.011B to 6.9B parameters trained with various numbers of tokens on three data distributions.","First, we investigate scaling in the over-trained regime.","We fit scaling laws that extrapolate in both the number of model parameters and the ratio of training tokens to parameters.","This enables us to predict the validation loss of a 1.4B parameter, 900B token run (i.e., 32$\\times$ over-trained) and a 6.9B parameter, 138B token run$\\unicode{x2014}$each from experiments that take 300$\\times$ less compute.","Second, we relate the perplexity of a language model to its downstream task performance via a power law.","We use this law to predict top-1 error averaged over downstream tasks for the two aforementioned models using experiments that take 20$\\times$ less compute.","Our experiments are available at https://github.com/mlfoundations/scaling."],"url":"http://arxiv.org/abs/2403.08540v1","category":"cs.CL"}
{"created":"2024-03-13 12:55:43","title":"Rich Semantic Knowledge Enhanced Large Language Models for Few-shot Chinese Spell Checking","abstract":"Chinese Spell Checking (CSC) is a widely used technology, which plays a vital role in speech to text (STT) and optical character recognition (OCR). Most of the existing CSC approaches relying on BERT architecture achieve excellent performance. However, limited by the scale of the foundation model, BERT-based method does not work well in few-shot scenarios, showing certain limitations in practical applications. In this paper, we explore using an in-context learning method named RS-LLM (Rich Semantic based LLMs) to introduce large language models (LLMs) as the foundation model. Besides, we study the impact of introducing various Chinese rich semantic information in our framework. We found that by introducing a small number of specific Chinese rich semantic structures, LLMs achieve better performance than the BERT-based model on few-shot CSC task. Furthermore, we conduct experiments on multiple datasets, and the experimental results verified the superiority of our proposed framework.","sentences":["Chinese Spell Checking (CSC) is a widely used technology, which plays a vital role in speech to text (STT) and optical character recognition (OCR).","Most of the existing CSC approaches relying on BERT architecture achieve excellent performance.","However, limited by the scale of the foundation model, BERT-based method does not work well in few-shot scenarios, showing certain limitations in practical applications.","In this paper, we explore using an in-context learning method named RS-LLM (Rich Semantic based LLMs) to introduce large language models (LLMs) as the foundation model.","Besides, we study the impact of introducing various Chinese rich semantic information in our framework.","We found that by introducing a small number of specific Chinese rich semantic structures, LLMs achieve better performance than the BERT-based model on few-shot CSC task.","Furthermore, we conduct experiments on multiple datasets, and the experimental results verified the superiority of our proposed framework."],"url":"http://arxiv.org/abs/2403.08492v1","category":"cs.CL"}
{"created":"2024-03-13 12:26:55","title":"Diffusion Models with Implicit Guidance for Medical Anomaly Detection","abstract":"Diffusion models have advanced unsupervised anomaly detection by improving the transformation of pathological images into pseudo-healthy equivalents. Nonetheless, standard approaches may compromise critical information during pathology removal, leading to restorations that do not align with unaffected regions in the original scans. Such discrepancies can inadvertently increase false positive rates and reduce specificity, complicating radiological evaluations. This paper introduces Temporal Harmonization for Optimal Restoration (THOR), which refines the de-noising process by integrating implicit guidance through temporal anomaly maps. THOR aims to preserve the integrity of healthy tissue in areas unaffected by pathology. Comparative evaluations show that THOR surpasses existing diffusion-based methods in detecting and segmenting anomalies in brain MRIs and wrist X-rays. Code: https://github.com/ci-ber/THOR_DDPM.","sentences":["Diffusion models have advanced unsupervised anomaly detection by improving the transformation of pathological images into pseudo-healthy equivalents.","Nonetheless, standard approaches may compromise critical information during pathology removal, leading to restorations that do not align with unaffected regions in the original scans.","Such discrepancies can inadvertently increase false positive rates and reduce specificity, complicating radiological evaluations.","This paper introduces Temporal Harmonization for Optimal Restoration (THOR), which refines the de-noising process by integrating implicit guidance through temporal anomaly maps.","THOR aims to preserve the integrity of healthy tissue in areas unaffected by pathology.","Comparative evaluations show that THOR surpasses existing diffusion-based methods in detecting and segmenting anomalies in brain MRIs and wrist X-rays.","Code: https://github.com/ci-ber/THOR_DDPM."],"url":"http://arxiv.org/abs/2403.08464v1","category":"eess.IV"}
{"created":"2024-03-13 12:06:42","title":"An Integrated Usability Framework for Evaluating Open Government Data Portals: Comparative Analysis of EU and GCC Countries","abstract":"This study explores the critical role of open government data (OGD) portals in fostering transparency and collaboration between diverse stakeholders. Recognizing the challenges of usability, communication with diverse populations, and strategic value creation, this paper develops an integrated framework for evaluating OGD portal effectiveness that accommodates user diversity (regardless of their data literacy and language), evaluates collaboration and participation, and the ability of users to explore and understand the data provided through them. The framework is validated by applying it to 33 national portals across European Union and Gulf Cooperation Council (GCC) countries, as a result of which we rank OGD portals, identify some good practices that lower-performing portals can learn from, and common shortcomings. Notably, the study unveils the competitive and innovative nature of GCC OGD portals, pinpointing specific improvement areas such as multilingual support and data understandability. The findings underscore the growing trend of exposing data quality metrics and advocate for enhanced two-way communication channels between users and portal representatives. Overall, the study contributes to accelerating the development of user-friendly, collaborative, and sustainable OGD portals while addressing gaps identified in previous research.","sentences":["This study explores the critical role of open government data (OGD) portals in fostering transparency and collaboration between diverse stakeholders.","Recognizing the challenges of usability, communication with diverse populations, and strategic value creation, this paper develops an integrated framework for evaluating OGD portal effectiveness that accommodates user diversity (regardless of their data literacy and language), evaluates collaboration and participation, and the ability of users to explore and understand the data provided through them.","The framework is validated by applying it to 33 national portals across European Union and Gulf Cooperation Council (GCC) countries, as a result of which we rank OGD portals, identify some good practices that lower-performing portals can learn from, and common shortcomings.","Notably, the study unveils the competitive and innovative nature of GCC OGD portals, pinpointing specific improvement areas such as multilingual support and data understandability.","The findings underscore the growing trend of exposing data quality metrics and advocate for enhanced two-way communication channels between users and portal representatives.","Overall, the study contributes to accelerating the development of user-friendly, collaborative, and sustainable OGD portals while addressing gaps identified in previous research."],"url":"http://arxiv.org/abs/2403.08451v1","category":"cs.CY"}
{"created":"2024-03-13 09:48:04","title":"RAF-GI: Towards Robust, Accurate and Fast-Convergent Gradient Inversion Attack in Federated Learning","abstract":"Federated learning (FL) empowers privacy-preservation in model training by only exposing users' model gradients. Yet, FL users are susceptible to the gradient inversion (GI) attack which can reconstruct ground-truth training data such as images based on model gradients. However, reconstructing high-resolution images by existing GI attack works faces two challenges: inferior accuracy and slow-convergence, especially when the context is complicated, e.g., the training batch size is much greater than 1 on each FL user. To address these challenges, we present a Robust, Accurate and Fast-convergent GI attack algorithm, called RAF-GI, with two components: 1) Additional Convolution Block (ACB) which can restore labels with up to 20% improvement compared with existing works; 2) Total variance, three-channel mEan and cAnny edge detection regularization term (TEA), which is a white-box attack strategy to reconstruct images based on labels inferred by ACB. Moreover, RAF-GI is robust that can still accurately reconstruct ground-truth data when the users' training batch size is no more than 48. Our experimental results manifest that RAF-GI can diminish 94% time costs while achieving superb inversion quality in ImageNet dataset. Notably, with a batch size of 1, RAF-GI exhibits a 7.89 higher Peak Signal-to-Noise Ratio (PSNR) compared to the state-of-the-art baselines.","sentences":["Federated learning (FL) empowers privacy-preservation in model training by only exposing users' model gradients.","Yet, FL users are susceptible to the gradient inversion (GI) attack which can reconstruct ground-truth training data such as images based on model gradients.","However, reconstructing high-resolution images by existing GI attack works faces two challenges: inferior accuracy and slow-convergence, especially when the context is complicated, e.g., the training batch size is much greater than 1 on each FL user.","To address these challenges, we present a Robust, Accurate and Fast-convergent GI attack algorithm, called RAF-GI, with two components: 1) Additional Convolution Block (ACB) which can restore labels with up to 20% improvement compared with existing works; 2) Total variance, three-channel mEan and cAnny edge detection regularization term (TEA), which is a white-box attack strategy to reconstruct images based on labels inferred by ACB.","Moreover, RAF-GI is robust that can still accurately reconstruct ground-truth data when the users' training batch size is no more than 48.","Our experimental results manifest that RAF-GI can diminish 94% time costs while achieving superb inversion quality in ImageNet dataset.","Notably, with a batch size of 1, RAF-GI exhibits a 7.89 higher Peak Signal-to-Noise Ratio (PSNR) compared to the state-of-the-art baselines."],"url":"http://arxiv.org/abs/2403.08383v1","category":"cs.CV"}
{"created":"2024-03-13 09:42:46","title":"Learning to Describe for Predicting Zero-shot Drug-Drug Interactions","abstract":"Adverse drug-drug interactions~(DDIs) can compromise the effectiveness of concurrent drug administration, posing a significant challenge in healthcare. As the development of new drugs continues, the potential for unknown adverse effects resulting from DDIs becomes a growing concern. Traditional computational methods for DDI prediction may fail to capture interactions for new drugs due to the lack of knowledge. In this paper, we introduce a new problem setup as zero-shot DDI prediction that deals with the case of new drugs. Leveraging textual information from online databases like DrugBank and PubChem, we propose an innovative approach TextDDI with a language model-based DDI predictor and a reinforcement learning~(RL)-based information selector, enabling the selection of concise and pertinent text for accurate DDI prediction on new drugs. Empirical results show the benefits of the proposed approach on several settings including zero-shot and few-shot DDI prediction, and the selected texts are semantically relevant. Our code and data are available at \\url{https://github.com/zhufq00/DDIs-Prediction}.","sentences":["Adverse drug-drug interactions~(DDIs) can compromise the effectiveness of concurrent drug administration, posing a significant challenge in healthcare.","As the development of new drugs continues, the potential for unknown adverse effects resulting from DDIs becomes a growing concern.","Traditional computational methods for DDI prediction may fail to capture interactions for new drugs due to the lack of knowledge.","In this paper, we introduce a new problem setup as zero-shot DDI prediction that deals with the case of new drugs.","Leveraging textual information from online databases like DrugBank and PubChem, we propose an innovative approach TextDDI with a language model-based DDI predictor and a reinforcement learning~(RL)-based information selector, enabling the selection of concise and pertinent text for accurate DDI prediction on new drugs.","Empirical results show the benefits of the proposed approach on several settings including zero-shot and few-shot DDI prediction, and the selected texts are semantically relevant.","Our code and data are available at \\url{https://github.com/zhufq00/DDIs-Prediction}."],"url":"http://arxiv.org/abs/2403.08377v1","category":"cs.CL"}
{"created":"2024-03-13 09:22:30","title":"Mean-Field Microcanonical Gradient Descent","abstract":"Microcanonical gradient descent is a sampling procedure for energy-based models allowing for efficient sampling of distributions in high dimension. It works by transporting samples from a high-entropy distribution, such as Gaussian white noise, to a low-energy region using gradient descent. We put this model in the framework of normalizing flows, showing how it can often overfit by losing an unnecessary amount of entropy in the descent. As a remedy, we propose a mean-field microcanonical gradient descent that samples several weakly coupled data points simultaneously, allowing for better control of the entropy loss while paying little in terms of likelihood fit. We study these models in the context of financial time series, illustrating the improvements on both synthetic and real data.","sentences":["Microcanonical gradient descent is a sampling procedure for energy-based models allowing for efficient sampling of distributions in high dimension.","It works by transporting samples from a high-entropy distribution, such as Gaussian white noise, to a low-energy region using gradient descent.","We put this model in the framework of normalizing flows, showing how it can often overfit by losing an unnecessary amount of entropy in the descent.","As a remedy, we propose a mean-field microcanonical gradient descent that samples several weakly coupled data points simultaneously, allowing for better control of the entropy loss while paying little in terms of likelihood fit.","We study these models in the context of financial time series, illustrating the improvements on both synthetic and real data."],"url":"http://arxiv.org/abs/2403.08362v1","category":"stat.ML"}
{"created":"2024-03-13 09:12:16","title":"NaturalVLM: Leveraging Fine-grained Natural Language for Affordance-Guided Visual Manipulation","abstract":"Enabling home-assistant robots to perceive and manipulate a diverse range of 3D objects based on human language instructions is a pivotal challenge. Prior research has predominantly focused on simplistic and task-oriented instructions, i.e., \"Slide the top drawer open\". However, many real-world tasks demand intricate multi-step reasoning, and without human instructions, these will become extremely difficult for robot manipulation. To address these challenges, we introduce a comprehensive benchmark, NrVLM, comprising 15 distinct manipulation tasks, containing over 4500 episodes meticulously annotated with fine-grained language instructions. We split the long-term task process into several steps, with each step having a natural language instruction. Moreover, we propose a novel learning framework that completes the manipulation task step-by-step according to the fine-grained instructions. Specifically, we first identify the instruction to execute, taking into account visual observations and the end-effector's current state. Subsequently, our approach facilitates explicit learning through action-prompts and perception-prompts to promote manipulation-aware cross-modality alignment. Leveraging both visual observations and linguistic guidance, our model outputs a sequence of actionable predictions for manipulation, including contact points and end-effector poses. We evaluate our method and baselines using the proposed benchmark NrVLM. The experimental results demonstrate the effectiveness of our approach. For additional details, please refer to https://sites.google.com/view/naturalvlm.","sentences":["Enabling home-assistant robots to perceive and manipulate a diverse range of 3D objects based on human language instructions is a pivotal challenge.","Prior research has predominantly focused on simplistic and task-oriented instructions, i.e., \"Slide the top drawer open\".","However, many real-world tasks demand intricate multi-step reasoning, and without human instructions, these will become extremely difficult for robot manipulation.","To address these challenges, we introduce a comprehensive benchmark, NrVLM, comprising 15 distinct manipulation tasks, containing over 4500 episodes meticulously annotated with fine-grained language instructions.","We split the long-term task process into several steps, with each step having a natural language instruction.","Moreover, we propose a novel learning framework that completes the manipulation task step-by-step according to the fine-grained instructions.","Specifically, we first identify the instruction to execute, taking into account visual observations and the end-effector's current state.","Subsequently, our approach facilitates explicit learning through action-prompts and perception-prompts to promote manipulation-aware cross-modality alignment.","Leveraging both visual observations and linguistic guidance, our model outputs a sequence of actionable predictions for manipulation, including contact points and end-effector poses.","We evaluate our method and baselines using the proposed benchmark NrVLM.","The experimental results demonstrate the effectiveness of our approach.","For additional details, please refer to https://sites.google.com/view/naturalvlm."],"url":"http://arxiv.org/abs/2403.08355v1","category":"cs.RO"}
{"created":"2024-03-13 05:46:36","title":"Identity-aware Dual-constraint Network for Cloth-Changing Person Re-identification","abstract":"Cloth-Changing Person Re-Identification (CC-ReID) aims to accurately identify the target person in more realistic surveillance scenarios, where pedestrians usually change their clothing. Despite great progress, limited cloth-changing training samples in existing CC-ReID datasets still prevent the model from adequately learning cloth-irrelevant features. In addition, due to the absence of explicit supervision to keep the model constantly focused on cloth-irrelevant areas, existing methods are still hampered by the disruption of clothing variations. To solve the above issues, we propose an Identity-aware Dual-constraint Network (IDNet) for the CC-ReID task. Specifically, to help the model extract cloth-irrelevant clues, we propose a Clothes Diversity Augmentation (CDA), which generates more realistic cloth-changing samples by enriching the clothing color while preserving the texture. In addition, a Multi-scale Constraint Block (MCB) is designed, which extracts fine-grained identity-related features and effectively transfers cloth-irrelevant knowledge. Moreover, a Counterfactual-guided Attention Module (CAM) is presented, which learns cloth-irrelevant features from channel and space dimensions and utilizes the counterfactual intervention for supervising the attention map to highlight identity-related regions. Finally, a Semantic Alignment Constraint (SAC) is designed to facilitate high-level semantic feature interaction. Comprehensive experiments on four CC-ReID datasets indicate that our method outperforms prior state-of-the-art approaches.","sentences":["Cloth-Changing Person Re-Identification (CC-ReID) aims to accurately identify the target person in more realistic surveillance scenarios, where pedestrians usually change their clothing.","Despite great progress, limited cloth-changing training samples in existing CC-ReID datasets still prevent the model from adequately learning cloth-irrelevant features.","In addition, due to the absence of explicit supervision to keep the model constantly focused on cloth-irrelevant areas, existing methods are still hampered by the disruption of clothing variations.","To solve the above issues, we propose an Identity-aware Dual-constraint Network (IDNet) for the CC-ReID task.","Specifically, to help the model extract cloth-irrelevant clues, we propose a Clothes Diversity Augmentation (CDA), which generates more realistic cloth-changing samples by enriching the clothing color while preserving the texture.","In addition, a Multi-scale Constraint Block (MCB) is designed, which extracts fine-grained identity-related features and effectively transfers cloth-irrelevant knowledge.","Moreover, a Counterfactual-guided Attention Module (CAM) is presented, which learns cloth-irrelevant features from channel and space dimensions and utilizes the counterfactual intervention for supervising the attention map to highlight identity-related regions.","Finally, a Semantic Alignment Constraint (SAC) is designed to facilitate high-level semantic feature interaction.","Comprehensive experiments on four CC-ReID datasets indicate that our method outperforms prior state-of-the-art approaches."],"url":"http://arxiv.org/abs/2403.08270v1","category":"cs.CV"}
{"created":"2024-03-13 05:33:52","title":"Sketch2Manga: Shaded Manga Screening from Sketch with Diffusion Models","abstract":"While manga is a popular entertainment form, creating manga is tedious, especially adding screentones to the created sketch, namely manga screening. Unfortunately, there is no existing method that tailors for automatic manga screening, probably due to the difficulty of generating high-quality shaded high-frequency screentones. The classic manga screening approaches generally require user input to provide screentone exemplars or a reference manga image. The recent deep learning models enables the automatic generation by learning from a large-scale dataset. However, the state-of-the-art models still fail to generate high-quality shaded screentones due to the lack of a tailored model and high-quality manga training data. In this paper, we propose a novel sketch-to-manga framework that first generates a color illustration from the sketch and then generates a screentoned manga based on the intensity guidance. Our method significantly outperforms existing methods in generating high-quality manga with shaded high-frequency screentones.","sentences":["While manga is a popular entertainment form, creating manga is tedious, especially adding screentones to the created sketch, namely manga screening.","Unfortunately, there is no existing method that tailors for automatic manga screening, probably due to the difficulty of generating high-quality shaded high-frequency screentones.","The classic manga screening approaches generally require user input to provide screentone exemplars or a reference manga image.","The recent deep learning models enables the automatic generation by learning from a large-scale dataset.","However, the state-of-the-art models still fail to generate high-quality shaded screentones due to the lack of a tailored model and high-quality manga training data.","In this paper, we propose a novel sketch-to-manga framework that first generates a color illustration from the sketch and then generates a screentoned manga based on the intensity guidance.","Our method significantly outperforms existing methods in generating high-quality manga with shaded high-frequency screentones."],"url":"http://arxiv.org/abs/2403.08266v1","category":"cs.CV"}
{"created":"2024-03-13 05:20:45","title":"Skipformer: A Skip-and-Recover Strategy for Efficient Speech Recognition","abstract":"Conformer-based attention models have become the de facto backbone model for Automatic Speech Recognition tasks. A blank symbol is usually introduced to align the input and output sequences for CTC or RNN-T models. Unfortunately, the long input length overloads computational budget and memory consumption quadratically by attention mechanism. In this work, we propose a \"Skip-and-Recover\" Conformer architecture, named Skipformer, to squeeze sequence input length dynamically and inhomogeneously. Skipformer uses an intermediate CTC output as criteria to split frames into three groups: crucial, skipping and ignoring. The crucial group feeds into next conformer blocks and its output joint with skipping group by original temporal order as the final encoder output. Experiments show that our model reduces the input sequence length by 31 times on Aishell-1 and 22 times on Librispeech corpus. Meanwhile, the model can achieve better recognition accuracy and faster inference speed than recent baseline models. Our code is open-sourced and available online.","sentences":["Conformer-based attention models have become the de facto backbone model for Automatic Speech Recognition tasks.","A blank symbol is usually introduced to align the input and output sequences for CTC or RNN-T models.","Unfortunately, the long input length overloads computational budget and memory consumption quadratically by attention mechanism.","In this work, we propose a \"Skip-and-Recover\" Conformer architecture, named Skipformer, to squeeze sequence input length dynamically and inhomogeneously.","Skipformer uses an intermediate CTC output as criteria to split frames into three groups: crucial, skipping and ignoring.","The crucial group feeds into next conformer blocks and its output joint with skipping group by original temporal order as the final encoder output.","Experiments show that our model reduces the input sequence length by 31 times on Aishell-1 and 22 times on Librispeech corpus.","Meanwhile, the model can achieve better recognition accuracy and faster inference speed than recent baseline models.","Our code is open-sourced and available online."],"url":"http://arxiv.org/abs/2403.08258v1","category":"cs.CL"}
{"created":"2024-03-13 05:00:42","title":"Towards Unified Modeling for Positive and Negative Preferences in Sign-Aware Recommendation","abstract":"Recently, sign-aware graph recommendation has drawn much attention as it will learn users' negative preferences besides positive ones from both positive and negative interactions (i.e., links in a graph) with items. To accommodate the different semantics of negative and positive links, existing works utilize two independent encoders to model users' positive and negative preferences, respectively. However, these approaches cannot learn the negative preferences from high-order heterogeneous interactions between users and items formed by multiple links with different signs, resulting in inaccurate and incomplete negative user preferences. To cope with these intractable issues, we propose a novel \\textbf{L}ight \\textbf{S}igned \\textbf{G}raph Convolution Network specifically for \\textbf{Rec}ommendation (\\textbf{LSGRec}), which adopts a unified modeling approach to simultaneously model high-order users' positive and negative preferences on a signed user-item interaction graph. Specifically, for the negative preferences within high-order heterogeneous interactions, first-order negative preferences are captured by the negative links, while high-order negative preferences are propagated along positive edges. Then, recommendation results are generated based on positive preferences and optimized with negative ones. Finally, we train representations of users and items through different auxiliary tasks. Extensive experiments on three real-world datasets demonstrate that our method outperforms existing baselines regarding performance and computational efficiency. Our code is available at \\url{https://anonymous.4open.science/r/LSGRec-BB95}.","sentences":["Recently, sign-aware graph recommendation has drawn much attention as it will learn users' negative preferences besides positive ones from both positive and negative interactions (i.e., links in a graph) with items.","To accommodate the different semantics of negative and positive links, existing works utilize two independent encoders to model users' positive and negative preferences, respectively.","However, these approaches cannot learn the negative preferences from high-order heterogeneous interactions between users and items formed by multiple links with different signs, resulting in inaccurate and incomplete negative user preferences.","To cope with these intractable issues, we propose a novel \\textbf{L}ight \\textbf{S}igned \\textbf{G}raph Convolution Network specifically for \\textbf{Rec}ommendation (\\textbf{LSGRec}), which adopts a unified modeling approach to simultaneously model high-order users' positive and negative preferences on a signed user-item interaction graph.","Specifically, for the negative preferences within high-order heterogeneous interactions, first-order negative preferences are captured by the negative links, while high-order negative preferences are propagated along positive edges.","Then, recommendation results are generated based on positive preferences and optimized with negative ones.","Finally, we train representations of users and items through different auxiliary tasks.","Extensive experiments on three real-world datasets demonstrate that our method outperforms existing baselines regarding performance and computational efficiency.","Our code is available at \\url{https://anonymous.4open.science/r/LSGRec-BB95}."],"url":"http://arxiv.org/abs/2403.08246v1","category":"cs.IR"}
{"created":"2024-03-13 04:36:24","title":"Point Cloud Compression via Constrained Optimal Transport","abstract":"This paper presents a novel point cloud compression method COT-PCC by formulating the task as a constrained optimal transport (COT) problem. COT-PCC takes the bitrate of compressed features as an extra constraint of optimal transport (OT) which learns the distribution transformation between original and reconstructed points. Specifically, the formulated COT is implemented with a generative adversarial network (GAN) and a bitrate loss for training. The discriminator measures the Wasserstein distance between input and reconstructed points, and a generator calculates the optimal mapping between distributions of input and reconstructed point cloud. Moreover, we introduce a learnable sampling module for downsampling in the compression procedure. Extensive results on both sparse and dense point cloud datasets demonstrate that COT-PCC outperforms state-of-the-art methods in terms of both CD and PSNR metrics. Source codes are available at \\url{https://github.com/cognaclee/PCC-COT}.","sentences":["This paper presents a novel point cloud compression method COT-PCC by formulating the task as a constrained optimal transport (COT) problem.","COT-PCC takes the bitrate of compressed features as an extra constraint of optimal transport (OT) which learns the distribution transformation between original and reconstructed points.","Specifically, the formulated COT is implemented with a generative adversarial network (GAN) and a bitrate loss for training.","The discriminator measures the Wasserstein distance between input and reconstructed points, and a generator calculates the optimal mapping between distributions of input and reconstructed point cloud.","Moreover, we introduce a learnable sampling module for downsampling in the compression procedure.","Extensive results on both sparse and dense point cloud datasets demonstrate that COT-PCC outperforms state-of-the-art methods in terms of both CD and PSNR metrics.","Source codes are available at \\url{https://github.com/cognaclee/PCC-COT}."],"url":"http://arxiv.org/abs/2403.08236v1","category":"cs.CV"}
{"created":"2024-03-13 04:01:20","title":"REPAIR: Rank Correlation and Noisy Pair Half-replacing with Memory for Noisy Correspondence","abstract":"The presence of noise in acquired data invariably leads to performance degradation in cross-modal matching. Unfortunately, obtaining precise annotations in the multimodal field is expensive, which has prompted some methods to tackle the mismatched data pair issue in cross-modal matching contexts, termed as noisy correspondence. However, most of these existing noisy correspondence methods exhibit the following limitations: a) the problem of self-reinforcing error accumulation, and b) improper handling of noisy data pair. To tackle the two problems, we propose a generalized framework termed as Rank corrElation and noisy Pair hAlf-replacing wIth memoRy (REPAIR), which benefits from maintaining a memory bank for features of matched pairs. Specifically, we calculate the distances between the features in the memory bank and those of the target pair for each respective modality, and use the rank correlation of these two sets of distances to estimate the soft correspondence label of the target pair. Estimating soft correspondence based on memory bank features rather than using a similarity network can avoid the accumulation of errors due to incorrect network identifications. For pairs that are completely mismatched, REPAIR searches the memory bank for the most matching feature to replace one feature of one modality, instead of using the original pair directly or merely discarding the mismatched pair. We conduct experiments on three cross-modal datasets, i.e., Flickr30K, MSCOCO, and CC152K, proving the effectiveness and robustness of our REPAIR on synthetic and real-world noise.","sentences":["The presence of noise in acquired data invariably leads to performance degradation in cross-modal matching.","Unfortunately, obtaining precise annotations in the multimodal field is expensive, which has prompted some methods to tackle the mismatched data pair issue in cross-modal matching contexts, termed as noisy correspondence.","However, most of these existing noisy correspondence methods exhibit the following limitations: a) the problem of self-reinforcing error accumulation, and b) improper handling of noisy data pair.","To tackle the two problems, we propose a generalized framework termed as Rank corrElation and noisy Pair hAlf-replacing wIth memoRy (REPAIR), which benefits from maintaining a memory bank for features of matched pairs.","Specifically, we calculate the distances between the features in the memory bank and those of the target pair for each respective modality, and use the rank correlation of these two sets of distances to estimate the soft correspondence label of the target pair.","Estimating soft correspondence based on memory bank features rather than using a similarity network can avoid the accumulation of errors due to incorrect network identifications.","For pairs that are completely mismatched, REPAIR searches the memory bank for the most matching feature to replace one feature of one modality, instead of using the original pair directly or merely discarding the mismatched pair.","We conduct experiments on three cross-modal datasets, i.e., Flickr30K, MSCOCO, and CC152K, proving the effectiveness and robustness of our REPAIR on synthetic and real-world noise."],"url":"http://arxiv.org/abs/2403.08224v1","category":"cs.CV"}
{"created":"2024-03-13 03:31:26","title":"Research on the Application of Deep Learning-based BERT Model in Sentiment Analysis","abstract":"This paper explores the application of deep learning techniques, particularly focusing on BERT models, in sentiment analysis. It begins by introducing the fundamental concept of sentiment analysis and how deep learning methods are utilized in this domain. Subsequently, it delves into the architecture and characteristics of BERT models. Through detailed explanation, it elucidates the application effects and optimization strategies of BERT models in sentiment analysis, supported by experimental validation. The experimental findings indicate that BERT models exhibit robust performance in sentiment analysis tasks, with notable enhancements post fine-tuning. Lastly, the paper concludes by summarizing the potential applications of BERT models in sentiment analysis and suggests directions for future research and practical implementations.","sentences":["This paper explores the application of deep learning techniques, particularly focusing on BERT models, in sentiment analysis.","It begins by introducing the fundamental concept of sentiment analysis and how deep learning methods are utilized in this domain.","Subsequently, it delves into the architecture and characteristics of BERT models.","Through detailed explanation, it elucidates the application effects and optimization strategies of BERT models in sentiment analysis, supported by experimental validation.","The experimental findings indicate that BERT models exhibit robust performance in sentiment analysis tasks, with notable enhancements post fine-tuning.","Lastly, the paper concludes by summarizing the potential applications of BERT models in sentiment analysis and suggests directions for future research and practical implementations."],"url":"http://arxiv.org/abs/2403.08217v1","category":"cs.CL"}
{"created":"2024-03-13 02:54:59","title":"Prototyping and Experimental Results for Environment-Aware Millimeter Wave Beam Alignment via Channel Knowledge Map","abstract":"Channel knowledge map (CKM), which aims to directly reflect the intrinsic channel properties of the local wireless environment, is a novel technique for achieving environmentaware communication. In this paper, to alleviate the large training overhead in millimeter wave (mmWave) beam alignment, an environment-aware and training-free beam alignment prototype is established based on a typical CKM, termed beam index map (BIM). To this end, a general CKM construction method is first presented, and an indoor BIM is constructed offline to learn the candidate transmit and receive beam index pairs for each grid in the experimental area. Furthermore, based on the location information of the receiver (or the dynamic obstacles) from the ultra-wide band (UWB) positioning system, the established BIM is used to achieve training-free beam alignment by directly providing the beam indexes for the transmitter and receiver. Three typical scenarios are considered in the experiment, including quasi-static environment with line-of-sight (LoS) link, quasistatic environment without LoS link and dynamic environment. Besides, the receiver orientation measured from the gyroscope is also used to help CKM predict more accurate beam indexes. The experiment results show that compared with the benchmark location-based beam alignment strategy, the CKM-based beam alignment strategy can achieve much higher received power, which is close to that achieved by exhaustive beam search, but with significantly reduced training overhead.","sentences":["Channel knowledge map (CKM), which aims to directly reflect the intrinsic channel properties of the local wireless environment, is a novel technique for achieving environmentaware communication.","In this paper, to alleviate the large training overhead in millimeter wave (mmWave) beam alignment, an environment-aware and training-free beam alignment prototype is established based on a typical CKM, termed beam index map (BIM).","To this end, a general CKM construction method is first presented, and an indoor BIM is constructed offline to learn the candidate transmit and receive beam index pairs for each grid in the experimental area.","Furthermore, based on the location information of the receiver (or the dynamic obstacles) from the ultra-wide band (UWB) positioning system, the established BIM is used to achieve training-free beam alignment by directly providing the beam indexes for the transmitter and receiver.","Three typical scenarios are considered in the experiment, including quasi-static environment with line-of-sight (LoS) link, quasistatic environment without LoS link and dynamic environment.","Besides, the receiver orientation measured from the gyroscope is also used to help CKM predict more accurate beam indexes.","The experiment results show that compared with the benchmark location-based beam alignment strategy, the CKM-based beam alignment strategy can achieve much higher received power, which is close to that achieved by exhaustive beam search, but with significantly reduced training overhead."],"url":"http://arxiv.org/abs/2403.08200v1","category":"eess.SY"}
{"created":"2024-03-13 02:26:15","title":"Synchronized Dual-arm Rearrangement via Cooperative mTSP","abstract":"Synchronized dual-arm rearrangement is widely studied as a common scenario in industrial applications. It often faces scalability challenges due to the computational complexity of robotic arm rearrangement and the high-dimensional nature of dual-arm planning. To address these challenges, we formulated the problem as cooperative mTSP, a variant of mTSP where agents share cooperative costs, and utilized reinforcement learning for its solution. Our approach involved representing rearrangement tasks using a task state graph that captured spatial relationships and a cooperative cost matrix that provided details about action costs. Taking these representations as observations, we designed an attention-based network to effectively combine them and provide rational task scheduling. Furthermore, a cost predictor is also introduced to directly evaluate actions during both training and planning, significantly expediting the planning process. Our experimental results demonstrate that our approach outperforms existing methods in terms of both performance and planning efficiency.","sentences":["Synchronized dual-arm rearrangement is widely studied as a common scenario in industrial applications.","It often faces scalability challenges due to the computational complexity of robotic arm rearrangement and the high-dimensional nature of dual-arm planning.","To address these challenges, we formulated the problem as cooperative mTSP, a variant of mTSP where agents share cooperative costs, and utilized reinforcement learning for its solution.","Our approach involved representing rearrangement tasks using a task state graph that captured spatial relationships and a cooperative cost matrix that provided details about action costs.","Taking these representations as observations, we designed an attention-based network to effectively combine them and provide rational task scheduling.","Furthermore, a cost predictor is also introduced to directly evaluate actions during both training and planning, significantly expediting the planning process.","Our experimental results demonstrate that our approach outperforms existing methods in terms of both performance and planning efficiency."],"url":"http://arxiv.org/abs/2403.08191v1","category":"cs.RO"}
{"created":"2024-03-13 02:11:04","title":"SeCG: Semantic-Enhanced 3D Visual Grounding via Cross-modal Graph Attention","abstract":"3D visual grounding aims to automatically locate the 3D region of the specified object given the corresponding textual description. Existing works fail to distinguish similar objects especially when multiple referred objects are involved in the description. Experiments show that direct matching of language and visual modal has limited capacity to comprehend complex referential relationships in utterances. It is mainly due to the interference caused by redundant visual information in cross-modal alignment. To strengthen relation-orientated mapping between different modalities, we propose SeCG, a semantic-enhanced relational learning model based on a graph network with our designed memory graph attention layer. Our method replaces original language-independent encoding with cross-modal encoding in visual analysis. More text-related feature expressions are obtained through the guidance of global semantics and implicit relationships. Experimental results on ReferIt3D and ScanRefer benchmarks show that the proposed method outperforms the existing state-of-the-art methods, particularly improving the localization performance for the multi-relation challenges.","sentences":["3D visual grounding aims to automatically locate the 3D region of the specified object given the corresponding textual description.","Existing works fail to distinguish similar objects especially when multiple referred objects are involved in the description.","Experiments show that direct matching of language and visual modal has limited capacity to comprehend complex referential relationships in utterances.","It is mainly due to the interference caused by redundant visual information in cross-modal alignment.","To strengthen relation-orientated mapping between different modalities, we propose SeCG, a semantic-enhanced relational learning model based on a graph network with our designed memory graph attention layer.","Our method replaces original language-independent encoding with cross-modal encoding in visual analysis.","More text-related feature expressions are obtained through the guidance of global semantics and implicit relationships.","Experimental results on ReferIt3D and ScanRefer benchmarks show that the proposed method outperforms the existing state-of-the-art methods, particularly improving the localization performance for the multi-relation challenges."],"url":"http://arxiv.org/abs/2403.08182v1","category":"cs.CV"}
{"created":"2024-03-13 02:04:57","title":"Learning Barrier-Certified Polynomial Dynamical Systems for Obstacle Avoidance with Robots","abstract":"Established techniques that enable robots to learn from demonstrations are based on learning a stable dynamical system (DS). To increase the robots' resilience to perturbations during tasks that involve static obstacle avoidance, we propose incorporating barrier certificates into an optimization problem to learn a stable and barrier-certified DS. Such optimization problem can be very complex or extremely conservative when the traditional linear parameter-varying formulation is used. Thus, different from previous approaches in the literature, we propose to use polynomial representations for DSs, which yields an optimization problem that can be tackled by sum-of-squares techniques. Finally, our approach can handle obstacle shapes that fall outside the scope of assumptions typically found in the literature concerning obstacle avoidance within the DS learning framework. Supplementary material can be found at the project webpage: https://martinschonger.github.io/abc-ds","sentences":["Established techniques that enable robots to learn from demonstrations are based on learning a stable dynamical system (DS).","To increase the robots' resilience to perturbations during tasks that involve static obstacle avoidance, we propose incorporating barrier certificates into an optimization problem to learn a stable and barrier-certified DS.","Such optimization problem can be very complex or extremely conservative when the traditional linear parameter-varying formulation is used.","Thus, different from previous approaches in the literature, we propose to use polynomial representations for DSs, which yields an optimization problem that can be tackled by sum-of-squares techniques.","Finally, our approach can handle obstacle shapes that fall outside the scope of assumptions typically found in the literature concerning obstacle avoidance within the DS learning framework.","Supplementary material can be found at the project webpage: https://martinschonger.github.io/abc-ds"],"url":"http://arxiv.org/abs/2403.08178v1","category":"cs.RO"}
{"created":"2024-03-13 00:32:30","title":"The Effect of Different Optimization Strategies to Physics-Constrained Deep Learning for Soil Moisture Estimation","abstract":"Soil moisture is a key hydrological parameter that has significant importance to human society and the environment. Accurate modeling and monitoring of soil moisture in crop fields, especially in the root zone (top 100 cm of soil), is essential for improving agricultural production and crop yield with the help of precision irrigation and farming tools. Realizing the full sensor data potential depends greatly on advanced analytical and predictive domain-aware models. In this work, we propose a physics-constrained deep learning (P-DL) framework to integrate physics-based principles on water transport and water sensing signals for effective reconstruction of the soil moisture dynamics. We adopt three different optimizers, namely Adam, RMSprop, and GD, to minimize the loss function of P-DL during the training process. In the illustrative case study, we demonstrate the empirical convergence of Adam optimizers outperforms the other optimization methods in both mini-batch and full-batch training.","sentences":["Soil moisture is a key hydrological parameter that has significant importance to human society and the environment.","Accurate modeling and monitoring of soil moisture in crop fields, especially in the root zone (top 100 cm of soil), is essential for improving agricultural production and crop yield with the help of precision irrigation and farming tools.","Realizing the full sensor data potential depends greatly on advanced analytical and predictive domain-aware models.","In this work, we propose a physics-constrained deep learning (P-DL) framework to integrate physics-based principles on water transport and water sensing signals for effective reconstruction of the soil moisture dynamics.","We adopt three different optimizers, namely Adam, RMSprop, and GD, to minimize the loss function of P-DL during the training process.","In the illustrative case study, we demonstrate the empirical convergence of Adam optimizers outperforms the other optimization methods in both mini-batch and full-batch training."],"url":"http://arxiv.org/abs/2403.08154v1","category":"cs.LG"}
{"created":"2024-03-13 00:30:09","title":"Multi-Fidelity Reinforcement Learning for Time-Optimal Quadrotor Re-planning","abstract":"High-speed online trajectory planning for UAVs poses a significant challenge due to the need for precise modeling of complex dynamics while also being constrained by computational limitations. This paper presents a multi-fidelity reinforcement learning method (MFRL) that aims to effectively create a realistic dynamics model and simultaneously train a planning policy that can be readily deployed in real-time applications. The proposed method involves the co-training of a planning policy and a reward estimator; the latter predicts the performance of the policy's output and is trained efficiently through multi-fidelity Bayesian optimization. This optimization approach models the correlation between different fidelity levels, thereby constructing a high-fidelity model based on a low-fidelity foundation, which enables the accurate development of the reward model with limited high-fidelity experiments. The framework is further extended to include real-world flight experiments in reinforcement learning training, allowing the reward model to precisely reflect real-world constraints and broadening the policy's applicability to real-world scenarios. We present rigorous evaluations by training and testing the planning policy in both simulated and real-world environments. The resulting trained policy not only generates faster and more reliable trajectories compared to the baseline snap minimization method, but it also achieves trajectory updates in 2 ms on average, while the baseline method takes several minutes.","sentences":["High-speed online trajectory planning for UAVs poses a significant challenge due to the need for precise modeling of complex dynamics while also being constrained by computational limitations.","This paper presents a multi-fidelity reinforcement learning method (MFRL) that aims to effectively create a realistic dynamics model and simultaneously train a planning policy that can be readily deployed in real-time applications.","The proposed method involves the co-training of a planning policy and a reward estimator; the latter predicts the performance of the policy's output and is trained efficiently through multi-fidelity Bayesian optimization.","This optimization approach models the correlation between different fidelity levels, thereby constructing a high-fidelity model based on a low-fidelity foundation, which enables the accurate development of the reward model with limited high-fidelity experiments.","The framework is further extended to include real-world flight experiments in reinforcement learning training, allowing the reward model to precisely reflect real-world constraints and broadening the policy's applicability to real-world scenarios.","We present rigorous evaluations by training and testing the planning policy in both simulated and real-world environments.","The resulting trained policy not only generates faster and more reliable trajectories compared to the baseline snap minimization method, but it also achieves trajectory updates in 2 ms on average, while the baseline method takes several minutes."],"url":"http://arxiv.org/abs/2403.08152v1","category":"cs.RO"}
{"created":"2024-03-13 00:19:06","title":"Representing Molecules as Random Walks Over Interpretable Grammars","abstract":"Recent research in molecular discovery has primarily been devoted to small, drug-like molecules, leaving many similarly important applications in material design without adequate technology. These applications often rely on more complex molecular structures with fewer examples that are carefully designed using known substructures. We propose a data-efficient and interpretable model for representing and reasoning over such molecules in terms of graph grammars that explicitly describe the hierarchical design space featuring motifs to be the design basis. We present a novel representation in the form of random walks over the design space, which facilitates both molecule generation and property prediction. We demonstrate clear advantages over existing methods in terms of performance, efficiency, and synthesizability of predicted molecules, and we provide detailed insights into the method's chemical interpretability.","sentences":["Recent research in molecular discovery has primarily been devoted to small, drug-like molecules, leaving many similarly important applications in material design without adequate technology.","These applications often rely on more complex molecular structures with fewer examples that are carefully designed using known substructures.","We propose a data-efficient and interpretable model for representing and reasoning over such molecules in terms of graph grammars that explicitly describe the hierarchical design space featuring motifs to be the design basis.","We present a novel representation in the form of random walks over the design space, which facilitates both molecule generation and property prediction.","We demonstrate clear advantages over existing methods in terms of performance, efficiency, and synthesizability of predicted molecules, and we provide detailed insights into the method's chemical interpretability."],"url":"http://arxiv.org/abs/2403.08147v1","category":"cs.LG"}
{"created":"2024-03-13 00:11:27","title":"Algorithmic Information Disclosure in Optimal Auctions","abstract":"This paper studies a joint design problem where a seller can design both the signal structures for the agents to learn their values, and the allocation and payment rules for selling the item. In his seminal work, Myerson (1981) shows how to design the optimal auction with exogenous signals. We show that the problem becomes NP-hard when the seller also has the ability to design the signal structures. Our main result is a polynomial-time approximation scheme (PTAS) for computing the optimal joint design with at most an $\\epsilon$ multiplicative loss in expected revenue. Moreover, we show that in our joint design problem, the seller can significantly reduce the information rent of the agents by providing partial information, which ensures a revenue that is at least $1 - \\frac{1}{e}$ of the optimal welfare for all valuation distributions.","sentences":["This paper studies a joint design problem where a seller can design both the signal structures for the agents to learn their values, and the allocation and payment rules for selling the item.","In his seminal work, Myerson (1981) shows how to design the optimal auction with exogenous signals.","We show that the problem becomes NP-hard when the seller also has the ability to design the signal structures.","Our main result is a polynomial-time approximation scheme (PTAS) for computing the optimal joint design with at most an $\\epsilon$ multiplicative loss in expected revenue.","Moreover, we show that in our joint design problem, the seller can significantly reduce the information rent of the agents by providing partial information, which ensures a revenue that is at least $1 - \\frac{1}{e}$ of the optimal welfare for all valuation distributions."],"url":"http://arxiv.org/abs/2403.08145v1","category":"cs.GT"}
{"created":"2024-03-13 00:10:18","title":"Prosody for Intuitive Robotic Interface Design: It's Not What You Said, It's How You Said It","abstract":"In this paper, we investigate the use of 'prosody' (the musical elements of speech) as a communicative signal for intuitive human-robot interaction interfaces. Our approach, rooted in Research through Design (RtD), examines the application of prosody in directing a quadruped robot navigation. We involved ten team members in an experiment to command a robot through an obstacle course using natural interaction. A human operator, serving as the robot's sensory and processing proxy, translated human communication into a basic set of navigation commands, effectively simulating an intuitive interface. During our analysis of interaction videos, when lexical and visual cues proved insufficient for accurate command interpretation, we turned to non-verbal auditory cues. Qualitative evidence suggests that participants intuitively relied on prosody to control robot navigation. We highlight specific distinct prosodic constructs that emerged from this preliminary exploration and discuss their pragmatic functions. This work contributes a discussion on the broader potential of prosody as a multifunctional communicative signal for designing future intuitive robotic interfaces, enabling lifelong learning and personalization in human-robot interaction.","sentences":["In this paper, we investigate the use of 'prosody' (the musical elements of speech) as a communicative signal for intuitive human-robot interaction interfaces.","Our approach, rooted in Research through Design (RtD), examines the application of prosody in directing a quadruped robot navigation.","We involved ten team members in an experiment to command a robot through an obstacle course using natural interaction.","A human operator, serving as the robot's sensory and processing proxy, translated human communication into a basic set of navigation commands, effectively simulating an intuitive interface.","During our analysis of interaction videos, when lexical and visual cues proved insufficient for accurate command interpretation, we turned to non-verbal auditory cues.","Qualitative evidence suggests that participants intuitively relied on prosody to control robot navigation.","We highlight specific distinct prosodic constructs that emerged from this preliminary exploration and discuss their pragmatic functions.","This work contributes a discussion on the broader potential of prosody as a multifunctional communicative signal for designing future intuitive robotic interfaces, enabling lifelong learning and personalization in human-robot interaction."],"url":"http://arxiv.org/abs/2403.08144v1","category":"cs.RO"}
{"created":"2024-03-12 22:33:08","title":"VANP: Learning Where to See for Navigation with Self-Supervised Vision-Action Pre-Training","abstract":"Humans excel at efficiently navigating through crowds without collision by focusing on specific visual regions relevant to navigation. However, most robotic visual navigation methods rely on deep learning models pre-trained on vision tasks, which prioritize salient objects -- not necessarily relevant to navigation and potentially misleading. Alternative approaches train specialized navigation models from scratch, requiring significant computation. On the other hand, self-supervised learning has revolutionized computer vision and natural language processing, but its application to robotic navigation remains underexplored due to the difficulty of defining effective self-supervision signals. Motivated by these observations, in this work, we propose a Self-Supervised Vision-Action Model for Visual Navigation Pre-Training (VANP). Instead of detecting salient objects that are beneficial for tasks such as classification or detection, VANP learns to focus only on specific visual regions that are relevant to the navigation task. To achieve this, VANP uses a history of visual observations, future actions, and a goal image for self-supervision, and embeds them using two small Transformer Encoders. Then, VANP maximizes the information between the embeddings by using a mutual information maximization objective function. We demonstrate that most VANP-extracted features match with human navigation intuition. VANP achieves comparable performance as models learned end-to-end with half the training time and models trained on a large-scale, fully supervised dataset, i.e., ImageNet, with only 0.08% data.","sentences":["Humans excel at efficiently navigating through crowds without collision by focusing on specific visual regions relevant to navigation.","However, most robotic visual navigation methods rely on deep learning models pre-trained on vision tasks, which prioritize salient objects -- not necessarily relevant to navigation and potentially misleading.","Alternative approaches train specialized navigation models from scratch, requiring significant computation.","On the other hand, self-supervised learning has revolutionized computer vision and natural language processing, but its application to robotic navigation remains underexplored due to the difficulty of defining effective self-supervision signals.","Motivated by these observations, in this work, we propose a Self-Supervised Vision-Action Model for Visual Navigation Pre-Training (VANP).","Instead of detecting salient objects that are beneficial for tasks such as classification or detection, VANP learns to focus only on specific visual regions that are relevant to the navigation task.","To achieve this, VANP uses a history of visual observations, future actions, and a goal image for self-supervision, and embeds them using two small Transformer Encoders.","Then, VANP maximizes the information between the embeddings by using a mutual information maximization objective function.","We demonstrate that most VANP-extracted features match with human navigation intuition.","VANP achieves comparable performance as models learned end-to-end with half the training time and models trained on a large-scale, fully supervised dataset, i.e., ImageNet, with only 0.08% data."],"url":"http://arxiv.org/abs/2403.08109v1","category":"cs.RO"}
{"created":"2024-03-12 20:10:04","title":"CHAI: Clustered Head Attention for Efficient LLM Inference","abstract":"Large Language Models (LLMs) with hundreds of billions of parameters have transformed the field of machine learning. However, serving these models at inference time is both compute and memory intensive, where a single request can require multiple GPUs and tens of Gigabytes of memory. Multi-Head Attention is one of the key components of LLMs, which can account for over 50% of LLMs memory and compute requirement. We observe that there is a high amount of redundancy across heads on which tokens they pay attention to. Based on this insight, we propose Clustered Head Attention (CHAI). CHAI combines heads with a high amount of correlation for self-attention at runtime, thus reducing both memory and compute. In our experiments, we show that CHAI is able to reduce the memory requirements for storing K,V cache by up to 21.4% and inference time latency by up to 1.73x without any fine-tuning required. CHAI achieves this with a maximum 3.2% deviation in accuracy across 3 different models (i.e. OPT-66B, LLAMA-7B, LLAMA-33B) and 5 different evaluation datasets.","sentences":["Large Language Models (LLMs) with hundreds of billions of parameters have transformed the field of machine learning.","However, serving these models at inference time is both compute and memory intensive, where a single request can require multiple GPUs and tens of Gigabytes of memory.","Multi-Head Attention is one of the key components of LLMs, which can account for over 50% of LLMs memory and compute requirement.","We observe that there is a high amount of redundancy across heads on which tokens they pay attention to.","Based on this insight, we propose Clustered Head Attention (CHAI).","CHAI combines heads with a high amount of correlation for self-attention at runtime, thus reducing both memory and compute.","In our experiments, we show that CHAI is able to reduce the memory requirements for storing K,V cache by up to 21.4% and inference time latency by up to 1.73x without any fine-tuning required.","CHAI achieves this with a maximum 3.2% deviation in accuracy across 3 different models (i.e. OPT-66B, LLAMA-7B, LLAMA-33B) and 5 different evaluation datasets."],"url":"http://arxiv.org/abs/2403.08058v1","category":"cs.LG"}
{"created":"2024-03-12 19:34:54","title":"Authorship Style Transfer with Policy Optimization","abstract":"Authorship style transfer aims to rewrite a given text into a specified target while preserving the original meaning in the source. Existing approaches rely on the availability of a large number of target style exemplars for model training. However, these overlook cases where a limited number of target style examples are available. The development of parameter-efficient transfer learning techniques and policy optimization (PO) approaches suggest lightweight PO is a feasible approach to low-resource style transfer. In this work, we propose a simple two step tune-and-optimize technique for low-resource textual style transfer. We apply our technique to authorship transfer as well as a larger-data native language style task and in both cases find it outperforms state-of-the-art baseline models.","sentences":["Authorship style transfer aims to rewrite a given text into a specified target while preserving the original meaning in the source.","Existing approaches rely on the availability of a large number of target style exemplars for model training.","However, these overlook cases where a limited number of target style examples are available.","The development of parameter-efficient transfer learning techniques and policy optimization (PO) approaches suggest lightweight PO is a feasible approach to low-resource style transfer.","In this work, we propose a simple two step tune-and-optimize technique for low-resource textual style transfer.","We apply our technique to authorship transfer as well as a larger-data native language style task and in both cases find it outperforms state-of-the-art baseline models."],"url":"http://arxiv.org/abs/2403.08043v1","category":"cs.CL"}
{"created":"2024-03-12 18:36:59","title":"MRC-Net: 6-DoF Pose Estimation with MultiScale Residual Correlation","abstract":"We propose a single-shot approach to determining 6-DoF pose of an object with available 3D computer-aided design (CAD) model from a single RGB image. Our method, dubbed MRC-Net, comprises two stages. The first performs pose classification and renders the 3D object in the classified pose. The second stage performs regression to predict fine-grained residual pose within class. Connecting the two stages is a novel multi-scale residual correlation (MRC) layer that captures high-and-low level correspondences between the input image and rendering from first stage. MRC-Net employs a Siamese network with shared weights between both stages to learn embeddings for input and rendered images. To mitigate ambiguity when predicting discrete pose class labels on symmetric objects, we use soft probabilistic labels to define pose class in the first stage. We demonstrate state-of-the-art accuracy, outperforming all competing RGB-based methods on four challenging BOP benchmark datasets: T-LESS, LM-O, YCB-V, and ITODD. Our method is non-iterative and requires no complex post-processing.","sentences":["We propose a single-shot approach to determining 6-DoF pose of an object with available 3D computer-aided design (CAD) model from a single RGB image.","Our method, dubbed MRC-Net, comprises two stages.","The first performs pose classification and renders the 3D object in the classified pose.","The second stage performs regression to predict fine-grained residual pose within class.","Connecting the two stages is a novel multi-scale residual correlation (MRC) layer that captures high-and-low level correspondences between the input image and rendering from first stage.","MRC-Net employs a Siamese network with shared weights between both stages to learn embeddings for input and rendered images.","To mitigate ambiguity when predicting discrete pose class labels on symmetric objects, we use soft probabilistic labels to define pose class in the first stage.","We demonstrate state-of-the-art accuracy, outperforming all competing RGB-based methods on four challenging BOP benchmark datasets: T-LESS, LM-O, YCB-V, and ITODD.","Our method is non-iterative and requires no complex post-processing."],"url":"http://arxiv.org/abs/2403.08019v1","category":"cs.CV"}
{"created":"2024-03-12 18:36:18","title":"Learning Data Association for Multi-Object Tracking using Only Coordinates","abstract":"We propose a novel Transformer-based module to address the data association problem for multi-object tracking. From detections obtained by a pretrained detector, this module uses only coordinates from bounding boxes to estimate an affinity score between pairs of tracks extracted from two distinct temporal windows. This module, named TWiX, is trained on sets of tracks with the objective of discriminating pairs of tracks coming from the same object from those which are not. Our module does not use the intersection over union measure, nor does it requires any motion priors or any camera motion compensation technique. By inserting TWiX within an online cascade matching pipeline, our tracker C-TWiX achieves state-of-the-art performance on the DanceTrack and KITTIMOT datasets, and gets competitive results on the MOT17 dataset. The code will be made available upon publication.","sentences":["We propose a novel Transformer-based module to address the data association problem for multi-object tracking.","From detections obtained by a pretrained detector, this module uses only coordinates from bounding boxes to estimate an affinity score between pairs of tracks extracted from two distinct temporal windows.","This module, named TWiX, is trained on sets of tracks with the objective of discriminating pairs of tracks coming from the same object from those which are not.","Our module does not use the intersection over union measure, nor does it requires any motion priors or any camera motion compensation technique.","By inserting TWiX within an online cascade matching pipeline, our tracker C-TWiX achieves state-of-the-art performance on the DanceTrack and KITTIMOT datasets, and gets competitive results on the MOT17 dataset.","The code will be made available upon publication."],"url":"http://arxiv.org/abs/2403.08018v1","category":"cs.CV"}
{"created":"2024-03-13 15:22:56","title":"Tangential Fixpoint Iterations for Gromov-Wasserstein Barycenters","abstract":"The Gromov-Wasserstein (GW) transport problem is a relaxation of classic optimal transport, which seeks a transport between two measures while preserving their internal geometry. Due to meeting this theoretical underpinning, it is a valuable tool for the analysis of objects that do not possess a natural embedding or should be studied independently of it. Prime applications can thus be found in e.g. shape matching, classification and interpolation tasks. To tackle the latter, one theoretically justified approach is the employment of multi-marginal GW transport and GW barycenters, which are Fr\\'echet means with respect to the GW distance. However, because the computation of GW itself already poses a quadratic and non-convex optimization problem, the determination of GW barycenters is a hard task and algorithms for their computation are scarce. In this paper, we revisit a known procedure for the determination of Fr\\'echet means in Riemannian manifolds via tangential approximations in the context of GW. We provide a characterization of barycenters in the GW tangent space, which ultimately gives rise to a fixpoint iteration for approximating GW barycenters using multi-marginal plans. We propose a relaxation of this fixpoint iteration and show that it monotonously decreases the barycenter loss. In certain cases our proposed method naturally provides us with barycentric embeddings. The resulting algorithm is capable of producing qualitative shape interpolations between multiple 3d shapes with support sizes of over thousands of points in reasonable time. In addition, we verify our method on shape classification and multi-graph matching tasks.","sentences":["The Gromov-Wasserstein (GW) transport problem is a relaxation of classic optimal transport, which seeks a transport between two measures while preserving their internal geometry.","Due to meeting this theoretical underpinning, it is a valuable tool for the analysis of objects that do not possess a natural embedding or should be studied independently of it.","Prime applications can thus be found in e.g. shape matching, classification and interpolation tasks.","To tackle the latter, one theoretically justified approach is the employment of multi-marginal GW transport and GW barycenters, which are Fr\\'echet means with respect to the GW distance.","However, because the computation of GW itself already poses a quadratic and non-convex optimization problem, the determination of GW barycenters is a hard task and algorithms for their computation are scarce.","In this paper, we revisit a known procedure for the determination of Fr\\'echet means in Riemannian manifolds via tangential approximations in the context of GW.","We provide a characterization of barycenters in the GW tangent space, which ultimately gives rise to a fixpoint iteration for approximating GW barycenters using multi-marginal plans.","We propose a relaxation of this fixpoint iteration and show that it monotonously decreases the barycenter loss.","In certain cases our proposed method naturally provides us with barycentric embeddings.","The resulting algorithm is capable of producing qualitative shape interpolations between multiple 3d shapes with support sizes of over thousands of points in reasonable time.","In addition, we verify our method on shape classification and multi-graph matching tasks."],"url":"http://arxiv.org/abs/2403.08612v1","category":"math.NA"}
{"created":"2024-03-13 14:43:36","title":"An improved particle swarm optimization algorithm and its application to search for new magnetic ground states in the Hubbard model","abstract":"An improved particle swarm optimization algorithm is proposed and its superiority over standard particle swarm optimization algorithm is tested on two typical benchmark functions. By employing this algorithm to search for the magnetic ground states of the Hubbard model on the real-space square lattice with finite size based on the mean-field approximation, two new magnetic states, namely the double striped-type antiferromagnetic state and the triple antiferromagnetic state, are found. We further perform mean-field calculations in the thermodynamical limit to confirm that these two new magnetic states are not a result of a finite-size effect, where the properties of the double striped-type antiferromagnetic state are also presented.","sentences":["An improved particle swarm optimization algorithm is proposed and its superiority over standard particle swarm optimization algorithm is tested on two typical benchmark functions.","By employing this algorithm to search for the magnetic ground states of the Hubbard model on the real-space square lattice with finite size based on the mean-field approximation, two new magnetic states, namely the double striped-type antiferromagnetic state and the triple antiferromagnetic state, are found.","We further perform mean-field calculations in the thermodynamical limit to confirm that these two new magnetic states are not a result of a finite-size effect, where the properties of the double striped-type antiferromagnetic state are also presented."],"url":"http://arxiv.org/abs/2403.08587v1","category":"cond-mat.str-el"}
{"created":"2024-03-13 14:35:17","title":"Impact of spin-entropy on the thermoelectric properties of a 2D magnet","abstract":"Heat-to-charge conversion efficiency of thermoelectric materials is closely linked to the entropy per charge carrier. Thus, magnetic materials are promising building blocks for highly efficient energy harvesters, as their carrier entropy is boosted by a spin degree of freedom. In this work, we investigate how this spin entropy impacts heat-to-charge conversion in A-type antiferromagnet CrSBr. We perform simultaneous measurements of electrical conductance and thermocurrent while changing magnetic order using temperature and magnetic field as tuning parameters. We find a strong enhancement of the thermoelectric power factor around the N\\'eel temperature. We further reveal that the power factor at low temperature can be increased by up to 600% upon applying a magnetic field. Our results demonstrate that the thermoelectric properties of 2D magnets can be optimized by exploiting the sizeable impact of spin entropy and confirm thermoelectric measurements as a sensitive tool to investigate subtle magnetic phase transitions in low-dimensional magnets.","sentences":["Heat-to-charge conversion efficiency of thermoelectric materials is closely linked to the entropy per charge carrier.","Thus, magnetic materials are promising building blocks for highly efficient energy harvesters, as their carrier entropy is boosted by a spin degree of freedom.","In this work, we investigate how this spin entropy impacts heat-to-charge conversion in A-type antiferromagnet CrSBr.","We perform simultaneous measurements of electrical conductance and thermocurrent while changing magnetic order using temperature and magnetic field as tuning parameters.","We find a strong enhancement of the thermoelectric power factor around the N\\'eel temperature.","We further reveal that the power factor at low temperature can be increased by up to 600% upon applying a magnetic field.","Our results demonstrate that the thermoelectric properties of 2D magnets can be optimized by exploiting the sizeable impact of spin entropy and confirm thermoelectric measurements as a sensitive tool to investigate subtle magnetic phase transitions in low-dimensional magnets."],"url":"http://arxiv.org/abs/2403.08581v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-13 12:50:23","title":"Data-oriented Dynamic Fine-tuning Parameter Selection Strategy for FISH Mask based Efficient Fine-tuning","abstract":"In view of the huge number of parameters of Large language models (LLMs) , tuning all parameters is very costly, and accordingly fine-tuning specific parameters is more sensible. Most of parameter efficient fine-tuning (PEFT) concentrate on parameter selection strategies, such as additive method, selective method and reparametrization-based method. However, there are few methods that consider the impact of data samples on parameter selecting, such as Fish Mask based method. Fish Mask randomly choose a part of data samples and treat them equally during parameter selection, which is unable to dynamically select optimal parameters for inconstant data distributions. In this work, we adopt a data-oriented perspective, then proposing an IRD ($\\mathrm{\\underline I}$terative sample-parameter $\\mathrm{\\underline R}$ange $\\mathrm{\\underline D}$ecreasing) algorithm to search the best setting of sample-parameter pair for FISH Mask. In each iteration, by searching the set of samples and parameters with larger Fish information, IRD can find better sample-parameter pair in most scale. We demonstrate the effectiveness and rationality of proposed strategy by conducting experiments on GLUE benchmark. Experimental results show our strategy optimizes the parameter selection and achieves preferable performance.","sentences":["In view of the huge number of parameters of Large language models (LLMs) , tuning all parameters is very costly, and accordingly fine-tuning specific parameters is more sensible.","Most of parameter efficient fine-tuning (PEFT) concentrate on parameter selection strategies, such as additive method, selective method and reparametrization-based method.","However, there are few methods that consider the impact of data samples on parameter selecting, such as Fish Mask based method.","Fish Mask randomly choose a part of data samples and treat them equally during parameter selection, which is unable to dynamically select optimal parameters for inconstant data distributions.","In this work, we adopt a data-oriented perspective, then proposing an IRD ($\\mathrm{\\underline I}$terative sample-parameter $\\mathrm{\\underline R}$ange $\\mathrm{\\underline D}$ecreasing) algorithm to search the best setting of sample-parameter pair for FISH Mask.","In each iteration, by searching the set of samples and parameters with larger Fish information, IRD can find better sample-parameter pair in most scale.","We demonstrate the effectiveness and rationality of proposed strategy by conducting experiments on GLUE benchmark.","Experimental results show our strategy optimizes the parameter selection and achieves preferable performance."],"url":"http://arxiv.org/abs/2403.08484v1","category":"cs.CL"}
{"created":"2024-03-13 12:49:49","title":"Study of Physical Characteristics of the New Half-Heusler Alloy BaHgSn by DFT Analysis","abstract":"To investigate the physical characteristics of the half-Heusler BaHgSn molecule, we used theoretical calculations within the Density Functional Theory (DFT) framework utilizing the LSDA+mBJ technique in this study. Using the optimal lattice parameters, we discover that half-Heusler BaHgSn exhibits a Dirac semimetal behavior with a band gap of 0.1 eV. Thomas Charpin's numerical first-principles calculation approach was applied to determine the elastic constants of hexagonal BaHgSn alloys. The material's optical characteristics verified its prospective use in infrared-visible devices. According to a thermo-electric properties analysis, at 20x10^18 {\\Omega}-1.m-1.s-1, the electrical conductivity reaches its maximum after increasing gradually up to 500 K. Compared to other compounds, these results indicate that BaHgSn has potential for use in opto-electronic and thermo-electric devices.","sentences":["To investigate the physical characteristics of the half-Heusler BaHgSn molecule, we used theoretical calculations within the Density Functional Theory (DFT) framework utilizing the LSDA+mBJ technique in this study.","Using the optimal lattice parameters, we discover that half-Heusler BaHgSn exhibits a Dirac semimetal behavior with a band gap of 0.1 eV. Thomas Charpin's numerical first-principles calculation approach was applied to determine the elastic constants of hexagonal BaHgSn alloys.","The material's optical characteristics verified its prospective use in infrared-visible devices.","According to a thermo-electric properties analysis, at 20x10^18 {\\Omega}-1.m-1.s-1, the electrical conductivity reaches its maximum after increasing gradually up to 500 K. Compared to other compounds, these results indicate that BaHgSn has potential for use in opto-electronic and thermo-electric devices."],"url":"http://arxiv.org/abs/2403.08483v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-13 12:25:34","title":"The Heisenberg-RIXS instrument at the European XFEL","abstract":"Resonant Inelastic X-ray Scattering (RIXS) is an ideal X-ray spectroscopy method to push the combination of energy and time resolutions to the Fourier transform ultimate limit, because it is unaffected by the core-hole lifetime energy broadening. And in pump-probe experiments the interaction time is made very short by the same core-hole lifetime. RIXS is very photon hungry so it takes great advantage from high repetition rate pulsed X-ray sources like the European XFEL. The hRIXS instrument is designed for RIXS experiments in the soft X-ray range with energy resolution approaching the Fourier and the Heisenberg limits. It is based on a spherical grating with variable line spacing (VLS) and a position-sensitive 2D detector. Initially, two gratings are installed to adequately cover the whole photon energy range. With optimized spot size on the sample and small pixel detector the energy resolution can be better than 40 meV at any photon energy below 1000 eV. At the SCS instrument of the European XFEL the spectrometer can be easily positioned thanks to air-pads on a high-quality floor, allowing the scattering angle to be continuously adjusted over the 65-145 deg range. It can be coupled to two different sample interaction chamber, one for liquid jets and one for solids, each equipped at the state-of-the-art and compatible for optical laser pumping in collinear geometry. The measured performances, in terms of energy resolution and count rate on the detector, closely match design expectations. hRIXS is open to public users since the summer of 2022.","sentences":["Resonant Inelastic X-ray Scattering (RIXS) is an ideal X-ray spectroscopy method to push the combination of energy and time resolutions to the Fourier transform ultimate limit, because it is unaffected by the core-hole lifetime energy broadening.","And in pump-probe experiments the interaction time is made very short by the same core-hole lifetime.","RIXS is very photon hungry so it takes great advantage from high repetition rate pulsed X-ray sources like the European XFEL.","The hRIXS instrument is designed for RIXS experiments in the soft X-ray range with energy resolution approaching the Fourier and the Heisenberg limits.","It is based on a spherical grating with variable line spacing (VLS) and a position-sensitive 2D detector.","Initially, two gratings are installed to adequately cover the whole photon energy range.","With optimized spot size on the sample and small pixel detector the energy resolution can be better than 40 meV at any photon energy below 1000 eV. At the SCS instrument of the European XFEL the spectrometer can be easily positioned thanks to air-pads on a high-quality floor, allowing the scattering angle to be continuously adjusted over the 65-145 deg range.","It can be coupled to two different sample interaction chamber, one for liquid jets and one for solids, each equipped at the state-of-the-art and compatible for optical laser pumping in collinear geometry.","The measured performances, in terms of energy resolution and count rate on the detector, closely match design expectations.","hRIXS is open to public users since the summer of 2022."],"url":"http://arxiv.org/abs/2403.08461v1","category":"cond-mat.str-el"}
{"created":"2024-03-13 08:42:54","title":"Curved commutators in the plane","abstract":"We complete the $L^p$ boundedness theory of commutators of Hilbert transforms along monomial curves by providing the previously missing lower bounds. This optimal result now covers all monomial curves while previous results had significant geometric restrictions. We also, for the first time, develop the corresponding necessity theory for curves with non-vanishing torsion.","sentences":["We complete the $L^p$ boundedness theory of commutators of Hilbert transforms along monomial curves by providing the previously missing lower bounds.","This optimal result now covers all monomial curves while previous results had significant geometric restrictions.","We also, for the first time, develop the corresponding necessity theory for curves with non-vanishing torsion."],"url":"http://arxiv.org/abs/2403.08338v1","category":"math.CA"}
{"created":"2024-03-13 08:29:28","title":"Slow convergence of the moment-SOS hierarchy for an elementary polynomial optimization problem","abstract":"We describe a parametric univariate quadratic optimization problem for which the moment-SOS hierarchy has finite but increasingly slow convergence when the parameter tends to its limit value. We estimate the order of finite convergence as a function of the parameter.","sentences":["We describe a parametric univariate quadratic optimization problem for which the moment-SOS hierarchy has finite but increasingly slow convergence when the parameter tends to its limit value.","We estimate the order of finite convergence as a function of the parameter."],"url":"http://arxiv.org/abs/2403.08329v1","category":"math.OC"}
{"created":"2024-03-13 07:51:12","title":"Intense and Stable Blue Light Emission from CsPbBr$_3$/Cs$_4$PbBr$_6$ Heterostructures Embedded in Transparent Nanoporous Films","abstract":"Lead halide perovskite nanocrystals are attractive for light emitting devices both as electroluminescent and color converting materials, since they combine intense and narrow emissions with good charge injection and transport properties. However, most perovskite nanocrystals shine at green and red wavelengths, the observation of intense and stable blue emission still being a challenging target. In this work, we report a method to attain intense and enduring blue emission (470-480 nm), with a photoluminescence quantum yield (PLQY) of 40%, originated from very small CsPbBr$_3$ nanocrystals (diameter<3nm) formed by controllably exposing Cs$_4$PbBr$_6$ to humidity. This process is mediated by the void network of a mesoporous transparent scaffold in which the zero-dimensional (0D) Cs$_4$PbBr$_6$ lattice is embedded, which allows the fine control over water adsorption and condensation that determines the optimization of the synthetic procedure and, eventually, the nanocrystal size. By temperature dependent photoemission analysis of samples with different [CsPbBr$_3$]/[Cs$_4$PbBr$_6$] volume ratios, we show that the bright blue emission observed results from the efficient charge transfer to the CsPbBr$_3$ inclusions from the Cs$_4$PbBr$_6$ host. Our approach provides a means to attain highly efficient transparent blue light emitting films that complete the palette offered by perovskite nanocrystals for lighting and display applications.","sentences":["Lead halide perovskite nanocrystals are attractive for light emitting devices both as electroluminescent and color converting materials, since they combine intense and narrow emissions with good charge injection and transport properties.","However, most perovskite nanocrystals shine at green and red wavelengths, the observation of intense and stable blue emission still being a challenging target.","In this work, we report a method to attain intense and enduring blue emission (470-480 nm), with a photoluminescence quantum yield (PLQY) of 40%, originated from very small CsPbBr$_3$ nanocrystals (diameter<3nm) formed by controllably exposing Cs$_4$PbBr$_6$ to humidity.","This process is mediated by the void network of a mesoporous transparent scaffold in which the zero-dimensional (0D)","Cs$_4$PbBr$_6$ lattice is embedded, which allows the fine control over water adsorption and condensation that determines the optimization of the synthetic procedure and, eventually, the nanocrystal size.","By temperature dependent photoemission analysis of samples with different [CsPbBr$_3$]/[Cs$_4$PbBr$_6$] volume ratios, we show that the bright blue emission observed results from the efficient charge transfer to the CsPbBr$_3$ inclusions from the Cs$_4$PbBr$_6$ host.","Our approach provides a means to attain highly efficient transparent blue light emitting films that complete the palette offered by perovskite nanocrystals for lighting and display applications."],"url":"http://arxiv.org/abs/2403.08315v1","category":"physics.app-ph"}
{"created":"2024-03-13 01:54:33","title":"A bargain for mergesorts (functional pearl) -- How to prove your mergesort correct and stable, almost for free","abstract":"We present a novel characterization of stable mergesort functions using relational parametricity, and show that it implies the correctness of mergesort. As a result, one can prove the correctness of several variations of mergesort (e.g., top-down, bottom-up, tail-recursive, non-tail-recursive, smooth, and non-smooth mergesorts) by proving the characterization property for each variation. To further motivate this work, we show a performance trade-off between tail-recursive and non-tail-recursive mergesorts that (1) the former in call-by-value evaluation avoids using up stack space and is efficient and (2) the latter in call-by-need evaluation is an optimal incremental sort, meaning that it performs only $\\mathcal{O}(n + k \\log k)$ comparisons to compute the least (or greatest) $k$ items of a list of length $n$. Thanks to our characterization and the parametricity translation, we deduced the correctness results, including stability, of various implementations of mergesort for lists, including highly optimized ones, in the Coq proof assistant.","sentences":["We present a novel characterization of stable mergesort functions using relational parametricity, and show that it implies the correctness of mergesort.","As a result, one can prove the correctness of several variations of mergesort (e.g., top-down, bottom-up, tail-recursive, non-tail-recursive, smooth, and non-smooth mergesorts) by proving the characterization property for each variation.","To further motivate this work, we show a performance trade-off between tail-recursive and non-tail-recursive mergesorts that (1) the former in call-by-value evaluation avoids using up stack space and is efficient and (2) the latter in call-by-need evaluation is an optimal incremental sort, meaning that it performs only $\\mathcal{O}(n + k \\log k)$ comparisons to compute the least (or greatest)","$k$ items of a list of length $n$. Thanks to our characterization and the parametricity translation, we deduced the correctness results, including stability, of various implementations of mergesort for lists, including highly optimized ones, in the Coq proof assistant."],"url":"http://arxiv.org/abs/2403.08173v1","category":"cs.LO"}
{"created":"2024-03-13 00:56:19","title":"Exponential Stability of Parametric Optimization-Based Controllers via Lur'e Contractivity","abstract":"In this letter, we investigate sufficient conditions for the exponential stability of LTI systems driven by controllers derived from parametric optimization problems. Our primary focus is on parametric projection controllers, namely parametric programs whose objective function is the squared distance to a nominal controller. Leveraging the virtual system method of analysis and a novel contractivity result for Lur'e systems, we establish a sufficient LMI condition for the exponential stability of an LTI system with a parametric projection-based controller. Separately, we prove additional results for single-integrator systems. Finally, we apply our results to state-dependent saturated control systems and control barrier function-based control and provide numerical simulations.","sentences":["In this letter, we investigate sufficient conditions for the exponential stability of LTI systems driven by controllers derived from parametric optimization problems.","Our primary focus is on parametric projection controllers, namely parametric programs whose objective function is the squared distance to a nominal controller.","Leveraging the virtual system method of analysis and a novel contractivity result for Lur'e systems, we establish a sufficient LMI condition for the exponential stability of an LTI system with a parametric projection-based controller.","Separately, we prove additional results for single-integrator systems.","Finally, we apply our results to state-dependent saturated control systems and control barrier function-based control and provide numerical simulations."],"url":"http://arxiv.org/abs/2403.08159v1","category":"math.OC"}
{"created":"2024-03-12 22:24:05","title":"Highway Preferential Attachment Models for Geographic Routing","abstract":"In the 1960s, the world-renowned social psychologist Stanley Milgram conducted experiments that showed that not only do there exist ``short chains'' of acquaintances between any two arbitrary people, but that these arbitrary strangers are able to find these short chains. This phenomenon, known as the \\emph{small-world phenomenon}, is explained in part by any model that has a low diameter, such as the Barab\\'asi and Albert's \\emph{preferential attachment} model, but these models do not display the same efficient routing that Milgram's experiments showed. In the year 2000, Kleinberg proposed a model with an efficient $\\mathcal{O}(\\log^2{n})$ greedy routing algorithm. In 2004, Martel and Nguyen showed that Kleinberg's analysis was tight, while also showing that Kleinberg's model had an expected diameter of only $\\Theta(\\log{n})$ -- a much smaller value than the greedy routing algorithm's path lengths. In 2022, Goodrich and Ozel proposed the \\emph{neighborhood preferential attachment} model (NPA), combining elements from Barab\\'asi and Albert's model with Kleinberg's model, and experimentally showed that the resulting model outperformed Kleinberg's greedy routing performance on U.S. road networks. While they displayed impressive empirical results, they did not provide any theoretical analysis of their model. In this paper, we first provide a theoretical analysis of a generalization of Kleinberg's original model and show that it can achieve expected $\\mathcal{O}(\\log{n})$ routing, a much better result than Kleinberg's model. We then propose a new model, \\emph{windowed NPA}, that is similar to the neighborhood preferential attachment model but has provable theoretical guarantees w.h.p. We show that this model is able to achieve $\\mathcal{O}(\\log^{1 + \\epsilon}{n})$ greedy routing for any $\\epsilon > 0$.","sentences":["In the 1960s, the world-renowned social psychologist Stanley Milgram conducted experiments that showed that not only do there exist ``short chains'' of acquaintances between any two arbitrary people, but that these arbitrary strangers are able to find these short chains.","This phenomenon, known as the \\emph{small-world phenomenon}, is explained in part by any model that has a low diameter, such as the Barab\\'asi and Albert's \\emph{preferential attachment} model, but these models do not display the same efficient routing that Milgram's experiments showed.","In the year 2000, Kleinberg proposed a model with an efficient $\\mathcal{O}(\\log^2{n})$ greedy routing algorithm.","In 2004, Martel and Nguyen showed that Kleinberg's analysis was tight, while also showing that Kleinberg's model had an expected diameter of only $\\Theta(\\log{n})$ -- a much smaller value than the greedy routing algorithm's path lengths.","In 2022, Goodrich and Ozel proposed the \\emph{neighborhood preferential attachment} model (NPA), combining elements from Barab\\'asi and Albert's model with Kleinberg's model, and experimentally showed that the resulting model outperformed Kleinberg's greedy routing performance on U.S. road networks.","While they displayed impressive empirical results, they did not provide any theoretical analysis of their model.","In this paper, we first provide a theoretical analysis of a generalization of Kleinberg's original model and show that it can achieve expected $\\mathcal{O}(\\log{n})$ routing, a much better result than Kleinberg's model.","We then propose a new model, \\emph{windowed NPA}, that is similar to the neighborhood preferential attachment model but has provable theoretical guarantees w.h.p.","We show that this model is able to achieve $\\mathcal{O}(\\log^{1 + \\epsilon}{n})$ greedy routing for any $\\epsilon > 0$."],"url":"http://arxiv.org/abs/2403.08105v1","category":"cs.DS"}
{"created":"2024-03-12 21:13:39","title":"The Randomized Block Coordinate Descent Method in the H\u00f6lder Smooth Setting","abstract":"This work provides the first convergence analysis for the Randomized Block Coordinate Descent method for minimizing a function that is both H\\\"older smooth and block H\\\"older smooth. Our analysis applies to objective functions that are non-convex, convex, and strongly convex. For non-convex functions, we show that the expected gradient norm reduces at an $O\\left(k^{\\frac{\\gamma}{1+\\gamma}}\\right)$ rate, where $k$ is the iteration count and $\\gamma$ is the H\\\"older exponent. For convex functions, we show that the expected suboptimality gap reduces at the rate $O\\left(k^{-\\gamma}\\right)$. In the strongly convex setting, we show this rate for the expected suboptimality gap improves to $O\\left(k^{-\\frac{2\\gamma}{1-\\gamma}}\\right)$ when $\\gamma>1$ and to a linear rate when $\\gamma=1$. Notably, these new convergence rates coincide with those furnished in the existing literature for the Lipschitz smooth setting.","sentences":["This work provides the first convergence analysis for the Randomized Block Coordinate Descent method for minimizing a function that is both H\\\"older smooth and block H\\\"older smooth.","Our analysis applies to objective functions that are non-convex, convex, and strongly convex.","For non-convex functions, we show that the expected gradient norm reduces at an $O\\left(k^{\\frac{\\gamma}{1+\\gamma}}\\right)$ rate, where $k$ is the iteration count and $\\gamma$ is the H\\\"older exponent.","For convex functions, we show that the expected suboptimality gap reduces at the rate $O\\left(k^{-\\gamma}\\right)$. In the strongly convex setting, we show this rate for the expected suboptimality gap improves to $O\\left(k^{-\\frac{2\\gamma}{1-\\gamma}}\\right)$ when $\\gamma>1$ and to a linear rate when $\\gamma=1$. Notably, these new convergence rates coincide with those furnished in the existing literature for the Lipschitz smooth setting."],"url":"http://arxiv.org/abs/2403.08080v1","category":"math.OC"}
{"created":"2024-03-12 20:28:00","title":"Towards Code Generation for Octree-Based Multigrid Solvers","abstract":"This paper presents a novel method designed to generate multigrid solvers optimized for octree-based software frameworks. Our approach focuses on accurately capturing local features within a domain while leveraging the efficiency inherent in multigrid techniques. We outline the essential steps involved in generating specialized kernels for local refinement and communication routines, integrating on-the-fly interpolations to seamlessly transfer information between refinement levels. For this purpose, we established a software coupling via an automatic fusion of generated multigrid solvers and communication kernels with manual implementations of complex octree data structures and algorithms often found in established software frameworks. We demonstrate the effectiveness of our method through numerical experiments with different interpolation orders. Large-scale benchmarks conducted on the SuperMUC-NG CPU cluster underscore the advantages of our approach, offering a comparison against a reference implementation to highlight the benefits of our method and code generation in general.","sentences":["This paper presents a novel method designed to generate multigrid solvers optimized for octree-based software frameworks.","Our approach focuses on accurately capturing local features within a domain while leveraging the efficiency inherent in multigrid techniques.","We outline the essential steps involved in generating specialized kernels for local refinement and communication routines, integrating on-the-fly interpolations to seamlessly transfer information between refinement levels.","For this purpose, we established a software coupling via an automatic fusion of generated multigrid solvers and communication kernels with manual implementations of complex octree data structures and algorithms often found in established software frameworks.","We demonstrate the effectiveness of our method through numerical experiments with different interpolation orders.","Large-scale benchmarks conducted on the SuperMUC-NG CPU cluster underscore the advantages of our approach, offering a comparison against a reference implementation to highlight the benefits of our method and code generation in general."],"url":"http://arxiv.org/abs/2403.08063v1","category":"cs.CE"}
{"created":"2024-03-12 19:53:50","title":"Multi-Apartment Rent Division","abstract":"Rent division is the well-studied problem of fairly assigning rooms and dividing rent among a set of roommates within a single apartment. A shortcoming of existing solutions is that renters are assumed to be considering apartments in isolation, whereas in reality, renters can choose among multiple apartments. In this paper, we generalize the rent division problem to the multi-apartment setting, where the goal is to both fairly choose an apartment among a set of alternatives and fairly assign rooms and rents within the chosen apartment. Our main contribution is a generalization of envy-freeness called rearrangeable envy-freeness. We show that a solution satisfying rearrangeable envy-freeness is guaranteed to exist and that it is possible to optimize over all rearrangeable envy-free solutions in polynomial time. We also define an even stronger fairness notion called universal envy-freeness and study its existence when values are drawn randomly.","sentences":["Rent division is the well-studied problem of fairly assigning rooms and dividing rent among a set of roommates within a single apartment.","A shortcoming of existing solutions is that renters are assumed to be considering apartments in isolation, whereas in reality, renters can choose among multiple apartments.","In this paper, we generalize the rent division problem to the multi-apartment setting, where the goal is to both fairly choose an apartment among a set of alternatives and fairly assign rooms and rents within the chosen apartment.","Our main contribution is a generalization of envy-freeness called rearrangeable envy-freeness.","We show that a solution satisfying rearrangeable envy-freeness is guaranteed to exist and that it is possible to optimize over all rearrangeable envy-free solutions in polynomial time.","We also define an even stronger fairness notion called universal envy-freeness and study its existence when values are drawn randomly."],"url":"http://arxiv.org/abs/2403.08051v1","category":"cs.GT"}
{"created":"2024-03-12 18:38:15","title":"Epidemiology, Trajectories and Outcomes of Acute Kidney Injury Among Hospitalized Patients: A Retrospective Multicenter Large Cohort Study","abstract":"Background: Acute kidney injury (AKI) is a clinical syndrome affecting almost one fifth of hospitalized patients, as well as more than half of the patients who are admitted to the intensive care unit (ICU). Stratifying AKI patients into groups based on severity and duration would facilitate more targeted efforts for treating AKI. Methods: In a retrospective, multicenter and longitudinal cohort study of 935,679 patients who were admitted between 2012 and 2020 to health centers included in OneFlorida+ Network, we analyzed the impact of AKI trajectories which are rapidly reversed AKI, persistent AKI with renal recovery, and persistent AKI without renal recovery on patients' clinical outcomes, including hospital, 30-day, 1-year, and 3-year mortality, kidney replacement therapy, new chronic kidney disease (CKD) within 90 days or 1-year of discharge, CKD progression within 1-year of discharge, resource utilization, hospital disposition, and major complications during hospitalization. As analytical approaches, Kaplan-Meier estimators and survival curves, Cox proportional-hazards regression model, logistic regression model, Kruskal-Wallis test, analysis of variance, chi-square, Fisher's exact test were used. Results: Among 2,187,254 encounters, 14% had AKI, of which 63%, 21%, and 16% had Stage 1, 2, and 3, respectively, as the worst AKI stage. Fraction of patients with persistent AKI was 31%. Patients with AKI had worse clinical outcomes and increased resource utilization compared to patients without the condition. One-year mortality was 5 times greater for patients with persistent AKI compared to those without AKI. Conclusions: Persistent AKI was associated with prolonged hospitalization, increased ICU admission and mortality compared to the other groups. This may emphasize the critical need for devising strategies targeting effective management of AKI and prevention of persisting AKI.","sentences":["Background: Acute kidney injury (AKI) is a clinical syndrome affecting almost one fifth of hospitalized patients, as well as more than half of the patients who are admitted to the intensive care unit (ICU).","Stratifying AKI patients into groups based on severity and duration would facilitate more targeted efforts for treating AKI.","Methods: In a retrospective, multicenter and longitudinal cohort study of 935,679 patients who were admitted between 2012 and 2020 to health centers included in OneFlorida+ Network, we analyzed the impact of AKI trajectories which are rapidly reversed AKI, persistent AKI with renal recovery, and persistent AKI without renal recovery on patients' clinical outcomes, including hospital, 30-day, 1-year, and 3-year mortality, kidney replacement therapy, new chronic kidney disease (CKD) within 90 days or 1-year of discharge, CKD progression within 1-year of discharge, resource utilization, hospital disposition, and major complications during hospitalization.","As analytical approaches, Kaplan-Meier estimators and survival curves, Cox proportional-hazards regression model, logistic regression model, Kruskal-Wallis test, analysis of variance, chi-square, Fisher's exact test were used.","Results: Among 2,187,254 encounters, 14% had AKI, of which 63%, 21%, and 16% had Stage 1, 2, and 3, respectively, as the worst AKI stage.","Fraction of patients with persistent AKI was 31%.","Patients with AKI had worse clinical outcomes and increased resource utilization compared to patients without the condition.","One-year mortality was 5 times greater for patients with persistent AKI compared to those without AKI.","Conclusions:","Persistent AKI was associated with prolonged hospitalization, increased ICU admission and mortality compared to the other groups.","This may emphasize the critical need for devising strategies targeting effective management of AKI and prevention of persisting AKI."],"url":"http://arxiv.org/abs/2403.08020v1","category":"stat.AP"}
{"created":"2024-03-13 17:51:01","title":"Clinically Feasible Diffusion Reconstruction for Highly-Accelerated Cardiac Cine MRI","abstract":"The currently limited quality of accelerated cardiac cine reconstruction may potentially be improved by the emerging diffusion models, but the clinically unacceptable long processing time poses a challenge. We aim to develop a clinically feasible diffusion-model-based reconstruction pipeline to improve the image quality of cine MRI. A multi-in multi-out diffusion enhancement model together with fast inference strategies were developed to be used in conjunction with a reconstruction model. The diffusion reconstruction reduced spatial and temporal blurring in prospectively undersampled clinical data, as validated by experts inspection. The 1.5s per video processing time enabled the approach to be applied in clinical scenarios.","sentences":["The currently limited quality of accelerated cardiac cine reconstruction may potentially be improved by the emerging diffusion models, but the clinically unacceptable long processing time poses a challenge.","We aim to develop a clinically feasible diffusion-model-based reconstruction pipeline to improve the image quality of cine MRI.","A multi-in multi-out diffusion enhancement model together with fast inference strategies were developed to be used in conjunction with a reconstruction model.","The diffusion reconstruction reduced spatial and temporal blurring in prospectively undersampled clinical data, as validated by experts inspection.","The 1.5s per video processing time enabled the approach to be applied in clinical scenarios."],"url":"http://arxiv.org/abs/2403.08749v1","category":"eess.IV"}
{"created":"2024-03-13 17:46:50","title":"GTP before ATP: The energy currency at the origin of genes","abstract":"Life is an exergonic chemical reaction. Many individual reactions in metabolism entail slightly endergonic processes that are coupled to free energy release, typically as ATP hydrolysis, in order to go forward. ATP is almost always supplied by the rotor-stator ATP synthetase (the ATPase), which harnesses chemiosmotic ion gradients. Because the ATPase is a protein, it arose after the ribosome did. Here we address two questions using comparative physiology: What was the energy currency of metabolism before the origin of the ATPase? How (and why) did ATP come to be the universal energy currency? About 27 percent of a cell's energy budget is consumed as GTP during translation. The universality of GTP-dependence in ribosome function indicates that GTP was the ancestral energy currency of protein synthesis. The use of GTP in translation and ATP in small molecule synthesis are conserved across all lineages, representing energetic compartments that arose in the last universal common ancestor, LUCA.","sentences":["Life is an exergonic chemical reaction.","Many individual reactions in metabolism entail slightly endergonic processes that are coupled to free energy release, typically as ATP hydrolysis, in order to go forward.","ATP is almost always supplied by the rotor-stator ATP synthetase (the ATPase), which harnesses chemiosmotic ion gradients.","Because the ATPase is a protein, it arose after the ribosome did.","Here we address two questions using comparative physiology: What was the energy currency of metabolism before the origin of the ATPase?","How (and why) did ATP come to be the universal energy currency?","About 27 percent of a cell's energy budget is consumed as GTP during translation.","The universality of GTP-dependence in ribosome function indicates that GTP was the ancestral energy currency of protein synthesis.","The use of GTP in translation and ATP in small molecule synthesis are conserved across all lineages, representing energetic compartments that arose in the last universal common ancestor, LUCA."],"url":"http://arxiv.org/abs/2403.08744v1","category":"physics.bio-ph"}
{"created":"2024-03-13 17:19:54","title":"Interpreting 95 GeV di-photon/$b\\bar{b}$ excesses as a lightest Higgs boson of the MRSSM","abstract":"The Minimal R-symmetric Supersymmetric Standard Model (MRSSM) is a well motivated BSM model which can accommodate the observed 125 GeV Higgs boson in agreement with electroweak precision observables, in particular with the $W$ boson mass and $T$ parameter. In the 2016 paper we showed that the SM-like 125 GeV Higgs state can be also realised as the second-to-lightest scalar of the MRSSM, leaving room for another sub-100 GeV state. Motivated by the recent ATLAS and CMS observation of the di-photon excess at a mass of around 95 GeV we investigate the possibility whether this could be the lightest CP-even MRSSM scalar in a variation of our benchmarks presented in the 2016 work. We show that such a state can also simultaneously explain the excess in the $b\\bar{b}$ final state observed around the same mass value at LEP. Due to the R-symmetric nature of the model, a light singlet-like Higgs state leads necessarily to a light bino-singlino Dirac dark matter candidate, which can give a correct relic density while evading current experimental bounds. Dark matter and LHC searches place further bounds on this scenario and point to parameter regions which are viable and of interest for the LHC Run III and upcoming dark matter experiments.","sentences":["The Minimal R-symmetric Supersymmetric Standard Model (MRSSM) is a well motivated BSM model which can accommodate the observed 125 GeV Higgs boson in agreement with electroweak precision observables, in particular with the $W$ boson mass and $T$ parameter.","In the 2016 paper we showed that the SM-like 125 GeV Higgs state can be also realised as the second-to-lightest scalar of the MRSSM, leaving room for another sub-100 GeV state.","Motivated by the recent ATLAS and CMS observation of the di-photon excess at a mass of around 95 GeV we investigate the possibility whether this could be the lightest CP-even MRSSM scalar in a variation of our benchmarks presented in the 2016 work.","We show that such a state can also simultaneously explain the excess in the $b\\bar{b}$ final state observed around the same mass value at LEP.","Due to the R-symmetric nature of the model, a light singlet-like Higgs state leads necessarily to a light bino-singlino Dirac dark matter candidate, which can give a correct relic density while evading current experimental bounds.","Dark matter and LHC searches place further bounds on this scenario and point to parameter regions which are viable and of interest for the LHC Run III and upcoming dark matter experiments."],"url":"http://arxiv.org/abs/2403.08720v1","category":"hep-ph"}
{"created":"2024-03-13 17:13:55","title":"The importance of stretching rate in achieving true stress relaxation in the elasto-capillary thinning of dilute solutions","abstract":"This work focuses on inferring the molecular state of the polymer chain required to induce elasto-capillary stress relaxation and the accurate measure of the polymer relaxation time in uniaxial stretching of dilute polymer solutions. This work is facilitated by the discovery that constant velocity applied at early times leads to initial constant extension rate before reaching the Rayleigh-Plateau instability. Such constant rate experiments are used to correlate initial stretching kinematics with the thinning dynamics in the elasto-capillary Regime. We show that there is a minimum initial strain-rate required to induce rate independent elastic effects. Below the minimum extension rate, insufficient stretching of the chain is observed before capillary instability, such that the polymer stress is comparable to the capillary stress at long times and true stress relaxation is not achieved. Above the minimum strain-rate, the chain reaches a critical stretch before instability, such that during the unstable filament thinning the polymer stress is significantly larger than the capillary stress and true stress relaxation is observed. Using a single relaxation mode Oldroyd-B model, we show that the the minimum strain rate leads to a required initial stretch of the chain before reaching the Rayleigh Plateau limit. Along with the accurate measure of relaxation time, this work introduces a characteristic dimensionless group, called the stretchability factor, that can be used to quantitatively compare different materials based on the overall material deformation/kinematic behavior, not just the relaxation time. Overall, these results demonstrate a useful methodology to study the stretching of dilute solutions using a constant velocity stretching scheme.","sentences":["This work focuses on inferring the molecular state of the polymer chain required to induce elasto-capillary stress relaxation and the accurate measure of the polymer relaxation time in uniaxial stretching of dilute polymer solutions.","This work is facilitated by the discovery that constant velocity applied at early times leads to initial constant extension rate before reaching the Rayleigh-Plateau instability.","Such constant rate experiments are used to correlate initial stretching kinematics with the thinning dynamics in the elasto-capillary Regime.","We show that there is a minimum initial strain-rate required to induce rate independent elastic effects.","Below the minimum extension rate, insufficient stretching of the chain is observed before capillary instability, such that the polymer stress is comparable to the capillary stress at long times and true stress relaxation is not achieved.","Above the minimum strain-rate, the chain reaches a critical stretch before instability, such that during the unstable filament thinning the polymer stress is significantly larger than the capillary stress and true stress relaxation is observed.","Using a single relaxation mode Oldroyd-B model, we show that the the minimum strain rate leads to a required initial stretch of the chain before reaching the Rayleigh Plateau limit.","Along with the accurate measure of relaxation time, this work introduces a characteristic dimensionless group, called the stretchability factor, that can be used to quantitatively compare different materials based on the overall material deformation/kinematic behavior, not just the relaxation time.","Overall, these results demonstrate a useful methodology to study the stretching of dilute solutions using a constant velocity stretching scheme."],"url":"http://arxiv.org/abs/2403.08708v1","category":"cond-mat.soft"}
{"created":"2024-03-13 16:41:02","title":"Bubble breakup probability in turbulent flows","abstract":"Bubbles drive gas and chemical transfers in various industrial and geophysical context, in which flows are typically turbulent. A knowledge of the bubble size distributions is then necessary to quantify mass fluxes across interfaces. In a turbulent flow, every bubble might break, depending on both the ratio between inertial and capillary forces at its scale, namely the Weber number We. For inhomogeneous and unstationary flows, the residence time within a turbulent region will also determine the break-up probability. In this work, we use a stochastic linear model, whose parameters have been measured using direct numerical simulations, to infer the breakup probability of bubbles in turbulence as function of the Weber number and the residence time. Our model shows that bubble breakup is a memoryless process, whose breakup rate varies exponentially with $\\textrm{We}^{-1}$. This linear model successfully reproduces breakup rates previously measured experimentally.","sentences":["Bubbles drive gas and chemical transfers in various industrial and geophysical context, in which flows are typically turbulent.","A knowledge of the bubble size distributions is then necessary to quantify mass fluxes across interfaces.","In a turbulent flow, every bubble might break, depending on both the ratio between inertial and capillary forces at its scale, namely the Weber number We.","For inhomogeneous and unstationary flows, the residence time within a turbulent region will also determine the break-up probability.","In this work, we use a stochastic linear model, whose parameters have been measured using direct numerical simulations, to infer the breakup probability of bubbles in turbulence as function of the Weber number and the residence time.","Our model shows that bubble breakup is a memoryless process, whose breakup rate varies exponentially with $\\textrm{We}^{-1}$. This linear model successfully reproduces breakup rates previously measured experimentally."],"url":"http://arxiv.org/abs/2403.08684v1","category":"physics.flu-dyn"}
{"created":"2024-03-13 16:31:57","title":"One-Loop Quantum Stress-Energy Tensor for the Kink and sine-Gordon Solitons","abstract":"We compute the renormalized one-loop quantum corrections to the energy density $T_{00}(x)$ and pressure $T_{11}(x)$ for solitons in the $1+1$ dimensional scalar sine-Gordon and kink models. We show how precise implementation of counterterms in dimensional regularization resolves previously identified discrepancies between the integral of $T_{00}(x)$ and the known correction to the total energy.","sentences":["We compute the renormalized one-loop quantum corrections to the energy density $T_{00}(x)$ and pressure $T_{11}(x)$ for solitons in the $1+1$ dimensional scalar sine-Gordon and kink models.","We show how precise implementation of counterterms in dimensional regularization resolves previously identified discrepancies between the integral of $T_{00}(x)$ and the known correction to the total energy."],"url":"http://arxiv.org/abs/2403.08677v1","category":"hep-th"}
{"created":"2024-03-13 15:58:57","title":"Fractional distortion in hyperbolic groups","abstract":"For all integers $p>q>0$ and $k >0$, and all non-elementary torsion-free hyperbolic groups $H$, we construct a hyperbolic group $G$ in which $H$ is a subgroup, such that the distortion function of $H$ in $G$ grows like $\\exp^k(n^{p/q})$. Here, $\\exp^k$ denotes the $k$-fold-iterated exponential function.","sentences":["For all integers $p>q>0$ and $k >0$, and all non-elementary torsion-free hyperbolic groups $H$, we construct a hyperbolic group $G$ in which $H$ is a subgroup, such that the distortion function of $H$ in $G$ grows like $\\exp^k(n^{p/q})$. Here, $\\exp^k$ denotes the $k$-fold-iterated exponential function."],"url":"http://arxiv.org/abs/2403.08645v1","category":"math.GR"}
{"created":"2024-03-13 15:48:52","title":"Coupling of quantum-dot states via elastic-cotunneling and crossed Andreev reflection in a minimal Kitaev chain","abstract":"Recently, exciting progress has been made in using the superconducting nanowires coupled to gate-defined quantum dots (QDs) to mimic the Kiteav chain and realize the Majorana-bound states via a poor man's route. The essential ingredient is to balance the interdot elastic-cotunneling (ECT) and crossed Andreev reflection (CAR). As theoretically proposed, this can be mediated by the Andreev bound states (ABSs) formed in the superconducting nanowires. However, most of the gate-tuning asymmetric features observed in experiments can not be captured using the current theoretical models. To address this insufficiency, here, we consider a full model that explicitly includes all the details of both the QD states and the ABSs. Remarkable agreement is found with the recent experimental observations, where our model correctly reveals the gate-tuning asymmetry in ECTs and by which the average QD state energy can also be extracted. In contrast, CARs do not depend on the tuning of QD states. Moreover, armed with the tunability of ECTs and CARs with QD states, we also predict a controllable anisotropic superexchange interaction between electron spins in the two separated QDs.","sentences":["Recently, exciting progress has been made in using the superconducting nanowires coupled to gate-defined quantum dots (QDs) to mimic the Kiteav chain and realize the Majorana-bound states via a poor man's route.","The essential ingredient is to balance the interdot elastic-cotunneling (ECT) and crossed Andreev reflection (CAR).","As theoretically proposed, this can be mediated by the Andreev bound states (ABSs) formed in the superconducting nanowires.","However, most of the gate-tuning asymmetric features observed in experiments can not be captured using the current theoretical models.","To address this insufficiency, here, we consider a full model that explicitly includes all the details of both the QD states and the ABSs.","Remarkable agreement is found with the recent experimental observations, where our model correctly reveals the gate-tuning asymmetry in ECTs and by which the average QD state energy can also be extracted.","In contrast, CARs do not depend on the tuning of QD states.","Moreover, armed with the tunability of ECTs and CARs with QD states, we also predict a controllable anisotropic superexchange interaction between electron spins in the two separated QDs."],"url":"http://arxiv.org/abs/2403.08636v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-13 15:39:45","title":"The algebraic structure of hyperbolic graph braid groups","abstract":"Genevois has recently classified which graph braid groups on $\\ge 3$ strands are word hyperbolic. In the $3$-strand case, he asked whether all such word hyperbolic groups are actually free; this reduced to checking two infinite classes of graphs. We give positive and negative answers to this question: one class of graphs always has free $3$-strand braid group, while the braid groups in the other class usually contain surface subgroups.","sentences":["Genevois has recently classified which graph braid groups on $\\ge 3$ strands are word hyperbolic.","In the $3$-strand case, he asked whether all such word hyperbolic groups are actually free; this reduced to checking two infinite classes of graphs.","We give positive and negative answers to this question: one class of graphs always has free $3$-strand braid group, while the braid groups in the other class usually contain surface subgroups."],"url":"http://arxiv.org/abs/2403.08623v1","category":"math.GR"}
{"created":"2024-03-13 14:11:28","title":"Microdosimetry of a clinical carbon-ion pencil beam at MedAustron -- Part 1: experimental characterization","abstract":"This paper characterizes the microdosimetric spectra of a single-energy carbon-ion pencil beam at MedAustron using a miniature solid-state silicon microdosimeter to estimate the impact of the lateral distribution of the different fragments on the microdosimetric spectra. The microdosimeter was fixed at one depth and then laterally moved away from the central beam axis in steps of approximately 2 mm. The measurements were taken in both horizontal and vertical direction in a water phantom at different depths. In a position on the distal dose fall-off beyond the Bragg peak, the frequency-mean and the dose-mean lineal energies were derived using either the entire range of y-values, or a sub-range of y values, presumingly corresponding mainly to contributions from primary particles. The measured microdosimetric spectra do not exhibit a significant change up to 4 mm away from the beam central axis. For lateral positions more than 4 mm away from the central axis, the relative contribution of the lower lineal-energy part of the spectrum increases with lateral distance due to the increased partial dose from secondary fragments. The average values yF and yD are almost constant for each partial contribution. However, when all particles are considered together, the average value of yF and yD varies with distance from the axis due to the changing dose fractions of these two components varying by 30 % and 10 % respectively up to the most off axis vertical position. Characteristic features in the microdosimetric spectra providing strong indications of the presence of helium and boron fragments have been observed downstream of the distal part of the Bragg peak. We were able to investigate the radiation quality as function of off-axis position. These measurements emphasize variation of the radiation quality within the beam and this has implications in terms of relative biological effectiveness.","sentences":["This paper characterizes the microdosimetric spectra of a single-energy carbon-ion pencil beam at MedAustron using a miniature solid-state silicon microdosimeter to estimate the impact of the lateral distribution of the different fragments on the microdosimetric spectra.","The microdosimeter was fixed at one depth and then laterally moved away from the central beam axis in steps of approximately 2 mm.","The measurements were taken in both horizontal and vertical direction in a water phantom at different depths.","In a position on the distal dose fall-off beyond the Bragg peak, the frequency-mean and the dose-mean lineal energies were derived using either the entire range of y-values, or a sub-range of y values, presumingly corresponding mainly to contributions from primary particles.","The measured microdosimetric spectra do not exhibit a significant change up to 4 mm away from the beam central axis.","For lateral positions more than 4 mm away from the central axis, the relative contribution of the lower lineal-energy part of the spectrum increases with lateral distance due to the increased partial dose from secondary fragments.","The average values yF and yD are almost constant for each partial contribution.","However, when all particles are considered together, the average value of yF and yD varies with distance from the axis due to the changing dose fractions of these two components varying by 30 % and 10 % respectively up to the most off axis vertical position.","Characteristic features in the microdosimetric spectra providing strong indications of the presence of helium and boron fragments have been observed downstream of the distal part of the Bragg peak.","We were able to investigate the radiation quality as function of off-axis position.","These measurements emphasize variation of the radiation quality within the beam and this has implications in terms of relative biological effectiveness."],"url":"http://arxiv.org/abs/2403.08561v1","category":"physics.med-ph"}
{"created":"2024-03-13 14:10:55","title":"Decoupling of the structure functions in momentum space based on the Laplace transformation","abstract":"Using Laplace transform techniques, we describe the determination of the longitudinal structure function $F_{L}(x,Q^2)$, at the leading-order approximation in momentum space, from the structure function $F_{2}(x,Q^2)$ and its derivative with respect to ${\\ln}Q^2$ in a kinematical region of low values of the Bjorken variable $x$. Since the $x$ dependence of $F_2(x,Q^2)$ and its evolution with $Q^2$ are determined much better by the data than $F_L(x,Q^2)$, this method provides both a direct check on $F_L(x,Q^2)$ where measured, and a way of extending $F_L(x,Q^2)$ into regions of $x$ and $Q^2$ where there are currently no data. In our calculations, we ultilize the Block-Durand-Ha parametrization for the structure function $F_{2}(x,Q^2)$ [M. M. Block, L. Durand and P. Ha, Phys.Rev.D {\\bf89}, 094027 (2014)]. We find that the Laplace transform method in momentum space provides correct behaviors of the extracted longitudinal structure function $F_{L}(x,Q^2)$ and that our obtained results are in line with data from the H1 Collaboration and other results for $F_{L}(x,Q^2)$ obtained using Mellin transform method.","sentences":["Using Laplace transform techniques, we describe the determination of the longitudinal structure function $F_{L}(x,Q^2)$, at the leading-order approximation in momentum space, from the structure function $F_{2}(x,Q^2)$ and its derivative with respect to ${\\ln}Q^2$ in a kinematical region of low values of the Bjorken variable $x$.","Since the $x$ dependence of $F_2(x,Q^2)$ and its evolution with $Q^2$ are determined much better by the data than $F_L(x,Q^2)$, this method provides both a direct check on $F_L(x,Q^2)$ where measured, and a way of extending $F_L(x,Q^2)$ into regions of $x$ and $Q^2$ where there are currently no data.","In our calculations, we ultilize the Block-Durand-Ha parametrization for the structure function $F_{2}(x,Q^2)$ [M. M. Block, L. Durand and P. Ha, Phys.Rev.D {\\bf89}, 094027 (2014)].","We find that the Laplace transform method in momentum space provides correct behaviors of the extracted longitudinal structure function $F_{L}(x,Q^2)$ and that our obtained results are in line with data from the H1 Collaboration and other results for $F_{L}(x,Q^2)$ obtained using Mellin transform method."],"url":"http://arxiv.org/abs/2403.08560v1","category":"hep-ph"}
{"created":"2024-03-13 13:58:44","title":"Comparing jet-shaped point symmetry in cluster cooling flows and supernovae","abstract":"I point out similarities between point-symmetric X-ray morphologies in cooling flow groups and clusters of galaxies, which are observed to be shaped by jets, and point-symmetric morphologies of eight core-collapse supernova (CCSN) remnants. I use these similarities to strengthen the jittering jet explosion mechanism (JJEM) of CCSNe, which predicts that the last pairs of jets to be launched by the newly born neutron star might shape some CCSN remnants to point-symmetric morphology. The point-symmetric morphologies in both types of objects are composed of two or more pairs of opposite bubbles (cavities), nozzles, some clumps, small protrusions (termed ears), and rims. The typically large volume of a CCSN remnant shaped by jets implies that the shaping jets carry an energy comparable to that of the ejecta, which in turn implies that jets exploded the remnant's massive star progenitor. The morphological similarities studied here add to the similarity of CCSN remnants, not only point-symmetric ones, to planetary nebulae shaped by jets. Together, these similarities solidify the JJEM as the main explosion mechanism of CCSNe. I consider the identification of point-symmetry in CCSNe, as expected by jet-shaping in the JJEM, to be the most severe challenge to the competing neutrino-driven explosion mechanism. I reiterate my earlier claim, but in a more vocal voice, that the main explosion mechanism of CCSNe is the JJEM.","sentences":["I point out similarities between point-symmetric X-ray morphologies in cooling flow groups and clusters of galaxies, which are observed to be shaped by jets, and point-symmetric morphologies of eight core-collapse supernova (CCSN) remnants.","I use these similarities to strengthen the jittering jet explosion mechanism (JJEM) of CCSNe, which predicts that the last pairs of jets to be launched by the newly born neutron star might shape some CCSN remnants to point-symmetric morphology.","The point-symmetric morphologies in both types of objects are composed of two or more pairs of opposite bubbles (cavities), nozzles, some clumps, small protrusions (termed ears), and rims.","The typically large volume of a CCSN remnant shaped by jets implies that the shaping jets carry an energy comparable to that of the ejecta, which in turn implies that jets exploded the remnant's massive star progenitor.","The morphological similarities studied here add to the similarity of CCSN remnants, not only point-symmetric ones, to planetary nebulae shaped by jets.","Together, these similarities solidify the JJEM as the main explosion mechanism of CCSNe.","I consider the identification of point-symmetry in CCSNe, as expected by jet-shaping in the JJEM, to be the most severe challenge to the competing neutrino-driven explosion mechanism.","I reiterate my earlier claim, but in a more vocal voice, that the main explosion mechanism of CCSNe is the JJEM."],"url":"http://arxiv.org/abs/2403.08544v1","category":"astro-ph.HE"}
{"created":"2024-03-13 11:31:08","title":"Spatially resolved emission lines in galaxies at $4\\leq z < 10$ from the JADES survey: evidence for enhanced central star formation","abstract":"We present the first statistical investigation of spatially resolved emission-line properties in a sample of 63 low-mass galaxies at $4\\leq z<10$, using JWST/NIRSpec MSA data from the JWST Advanced Deep Extragalactic (JADES) survey focusing on deep, spatially resolved spectroscopy in the GOODS-S extragalactic field. By performing a stacking of the 2D spectra of the galaxies in our sample, we find an increasing or flat radial trend with increasing radius for [OIII]$\\lambda5007$/H$\\beta$ and a decreasing one for [NeIII]$\\lambda3869$/[OII]$\\lambda3727$ (3--4 $\\sigma$ significance). These results are still valid when stacking the sample in two redshift bins (i.e., $4\\leq z<5.5$ and $5.5\\leq z<10$). The comparison with star-formation photoionization models suggests that the ionization parameter increases by $\\sim 0.5$ dex with redshift. We find a tentative metallicity gradient that increases with radius (i.e., 'inverted') in both redshift bins. Moreover, our analysis reveals strong negative gradients for the equivalent width of \\Hbeta (7$\\sigma$ significance). This trend persists even after removing known AGN candidates, therefore, it is consistent with a radial gradient primarily in stellar age and secondarily in metallicity. Taken all together, our results suggest that the sample is dominated by active central star formation, with possibly inverted metallicity gradients sustained by recent episodes of accretion of pristine gas or strong radial flows. Deeper observations and larger samples are needed to confirm these preliminary results and to validate our interpretation.","sentences":["We present the first statistical investigation of spatially resolved emission-line properties in a sample of 63 low-mass galaxies at $4\\leq z<10$, using JWST/NIRSpec MSA data from the JWST Advanced Deep Extragalactic (JADES) survey focusing on deep, spatially resolved spectroscopy in the GOODS-S extragalactic field.","By performing a stacking of the 2D spectra of the galaxies in our sample, we find an increasing or flat radial trend with increasing radius for [OIII]$\\lambda5007$/H$\\beta$ and a decreasing one for [NeIII]$\\lambda3869$/[OII]$\\lambda3727$ (3--4 $\\sigma$ significance).","These results are still valid when stacking the sample in two redshift bins (i.e., $4\\leq z<5.5$ and $5.5\\leq z<10$).","The comparison with star-formation photoionization models suggests that the ionization parameter increases by $\\sim 0.5$ dex with redshift.","We find a tentative metallicity gradient that increases with radius (i.e., 'inverted') in both redshift bins.","Moreover, our analysis reveals strong negative gradients for the equivalent width of \\Hbeta (7$\\sigma$ significance).","This trend persists even after removing known AGN candidates, therefore, it is consistent with a radial gradient primarily in stellar age and secondarily in metallicity.","Taken all together, our results suggest that the sample is dominated by active central star formation, with possibly inverted metallicity gradients sustained by recent episodes of accretion of pristine gas or strong radial flows.","Deeper observations and larger samples are needed to confirm these preliminary results and to validate our interpretation."],"url":"http://arxiv.org/abs/2403.08431v1","category":"astro-ph.GA"}
{"created":"2024-03-13 11:13:56","title":"Measures of relevance to the success of streaming platforms","abstract":"Digital streaming platforms, including Twitch, Spotify, Netflix, Disney, and Kindle, have emerged as one of the main sources of entertainment with significant growth potential. Many of these platforms distribute royalties among streamers, artists, producers, or writers based on their impact. In this paper, we measure the relevance of each of these contributors to the overall success of the platform, which is information that can play a key role in revenue allocation. We perform an axiomatic analysis to provide normative foundations for three relevance metrics: the uniform, the proportional, and the subscriber-proportional indicators. The last two indicators implement the so-called pro-rata and user-centric models, which are extensively applied to distribute revenues in the music streaming market. The axioms we propose formalize different principles of fairness, stability, and non-manipulability, and are tailor-made for the streaming context. We complete our analysis with a case study that measures the influence of the 19 most-followed streamers worldwide on the Twitch platform.","sentences":["Digital streaming platforms, including Twitch, Spotify, Netflix, Disney, and Kindle, have emerged as one of the main sources of entertainment with significant growth potential.","Many of these platforms distribute royalties among streamers, artists, producers, or writers based on their impact.","In this paper, we measure the relevance of each of these contributors to the overall success of the platform, which is information that can play a key role in revenue allocation.","We perform an axiomatic analysis to provide normative foundations for three relevance metrics: the uniform, the proportional, and the subscriber-proportional indicators.","The last two indicators implement the so-called pro-rata and user-centric models, which are extensively applied to distribute revenues in the music streaming market.","The axioms we propose formalize different principles of fairness, stability, and non-manipulability, and are tailor-made for the streaming context.","We complete our analysis with a case study that measures the influence of the 19 most-followed streamers worldwide on the Twitch platform."],"url":"http://arxiv.org/abs/2403.08421v1","category":"econ.TH"}
{"created":"2024-03-13 09:47:14","title":"Chiral spin state and nematic ferromagnet in the spin-1 Kitaev-$\u0393$ model","abstract":"The higher-spin Kitaev magnets, in which the Kitaev interaction and off-diagonal exchange couplings are overwhelmingly large, have emerged as a fertile avenue to explore exotic phases and unusual excitations. In this work, we study the quantum phase diagram of the spin-1 Kitaev-$\\Gamma$ model on the honeycomb lattice using density-matrix renormalization group. It harbours six distinct phases and the intriguing findings are three magnetically ordered phases in which both time-reversal symmetry and lattice symmetry albeit of different sort are broken spontaneously. The chiral spin state originates from the order-by-disorder effect and exhibits an almost saturated scalar spin chirality at the quantum level. Depending on the relative strength of the two interactions, it also features columnar or plaquette valence-bond-solid-like pattern as a consequence of the translational symmetry breaking. In parallel, the nematic ferromagnets are situated at ferromagnetic Kitaev side and possess small but finite ferromagnetic ordering. The lattice-rotational symmetry breaking enforces nonequivalent bond energy along one of the three bonds. Although the intrinsic difference between the two nematic ferromagnets remains elusive, the discontinuities in the von Neumann entropy, hexagonal plaquette operator, and Wilson loop operator convincingly suggest that they are separated via a first-order phase transition.","sentences":["The higher-spin Kitaev magnets, in which the Kitaev interaction and off-diagonal exchange couplings are overwhelmingly large, have emerged as a fertile avenue to explore exotic phases and unusual excitations.","In this work, we study the quantum phase diagram of the spin-1 Kitaev-$\\Gamma$ model on the honeycomb lattice using density-matrix renormalization group.","It harbours six distinct phases and the intriguing findings are three magnetically ordered phases in which both time-reversal symmetry and lattice symmetry albeit of different sort are broken spontaneously.","The chiral spin state originates from the order-by-disorder effect and exhibits an almost saturated scalar spin chirality at the quantum level.","Depending on the relative strength of the two interactions, it also features columnar or plaquette valence-bond-solid-like pattern as a consequence of the translational symmetry breaking.","In parallel, the nematic ferromagnets are situated at ferromagnetic Kitaev side and possess small but finite ferromagnetic ordering.","The lattice-rotational symmetry breaking enforces nonequivalent bond energy along one of the three bonds.","Although the intrinsic difference between the two nematic ferromagnets remains elusive, the discontinuities in the von Neumann entropy, hexagonal plaquette operator, and Wilson loop operator convincingly suggest that they are separated via a first-order phase transition."],"url":"http://arxiv.org/abs/2403.08382v1","category":"cond-mat.str-el"}
{"created":"2024-03-13 09:23:53","title":"ShareYourReality: Investigating Haptic Feedback and Agency in Virtual Avatar Co-embodiment","abstract":"Virtual co-embodiment enables two users to share a single avatar in Virtual Reality (VR). During such experiences, the illusion of shared motion control can break during joint-action activities, highlighting the need for position-aware feedback mechanisms. Drawing on the perceptual crossing paradigm, we explore how haptics can enable non-verbal coordination between co-embodied participants. In a within-subjects study (20 participant pairs), we examined the effects of vibrotactile haptic feedback (None, Present) and avatar control distribution (25-75%, 50-50%, 75-25%) across two VR reaching tasks (Targeted, Free-choice) on participants Sense of Agency (SoA), co-presence, body ownership, and motion synchrony. We found (a) lower SoA in the free-choice with haptics than without, (b) higher SoA during the shared targeted task, (c) co-presence and body ownership were significantly higher in the free-choice task, (d) players hand motions synchronized more in the targeted task. We provide cautionary considerations when including haptic feedback mechanisms for avatar co-embodiment experiences.","sentences":["Virtual co-embodiment enables two users to share a single avatar in Virtual Reality (VR).","During such experiences, the illusion of shared motion control can break during joint-action activities, highlighting the need for position-aware feedback mechanisms.","Drawing on the perceptual crossing paradigm, we explore how haptics can enable non-verbal coordination between co-embodied participants.","In a within-subjects study (20 participant pairs), we examined the effects of vibrotactile haptic feedback (None, Present) and avatar control distribution (25-75%, 50-50%, 75-25%) across two VR reaching tasks (Targeted, Free-choice) on participants Sense of Agency (SoA), co-presence, body ownership, and motion synchrony.","We found (a) lower SoA in the free-choice with haptics than without, (b) higher SoA during the shared targeted task, (c) co-presence and body ownership were significantly higher in the free-choice task, (d) players hand motions synchronized more in the targeted task.","We provide cautionary considerations when including haptic feedback mechanisms for avatar co-embodiment experiences."],"url":"http://arxiv.org/abs/2403.08363v1","category":"cs.HC"}
{"created":"2024-03-13 09:21:52","title":"Search for cosmic-ray boosted sub-MeV dark matter-electron scatterings in PandaX-4T","abstract":"We report the first search for the elastic scatterings between cosmic-ray boosted sub-MeV dark matter and electrons in the PandaX-4T liquid xenon experiment. Sub-MeV dark matter particles can be accelerated by scattering with electrons in the cosmic rays and produce detectable electron recoil signals in the detector. Using the commissioning data from PandaX-4T of 0.63~tonne$\\cdot$year exposure, we set new constraints on DM-electron scattering cross sections for DM masses ranging from 10~eV/$c^2$ to 3~keV/$c^2$.","sentences":["We report the first search for the elastic scatterings between cosmic-ray boosted sub-MeV dark matter and electrons in the PandaX-4T liquid xenon experiment.","Sub-MeV dark matter particles can be accelerated by scattering with electrons in the cosmic rays and produce detectable electron recoil signals in the detector.","Using the commissioning data from PandaX-4T of 0.63~tonne$\\cdot$year exposure, we set new constraints on DM-electron scattering cross sections for DM masses ranging from 10~eV/$c^2$ to 3~keV/$c^2$."],"url":"http://arxiv.org/abs/2403.08361v1","category":"hep-ex"}
{"created":"2024-03-13 08:45:36","title":"Suppression of diffraction in deep-inelastic scattering on nuclei and dynamical mechanism of leading twist nuclear shadowing","abstract":"Using the leading twist approach (LTA) to nuclear shadowing, we calculate the ratios of diffractive and usual parton distributions for a heavy nucleus (Pb) and the proton, $R_{A/p}=(f_{i/A}^{D(3)}/f_{i/A})/(f_{i/p}^{D(3)}/f_{i/p})$, for coherent and summed (coherent plus quasi-elastic) nuclear deep-inelastic scattering. We find that $R_{A/p} \\approx 0.5-1$ for quarks and $R_{A/p} \\approx 0.5-1.3$ for gluons in a broad range of $x$, which reaffirms the difference from the nuclear enhancement of $R_{A/p}$ predicted in the gluon saturation framework. We demonstrate that the magnitude of $R_{A/p}$ is controlled by the cross section of the interaction of hadronic fluctuations of the virtual photon with target nucleons, which explains an enhancement of $R_{A/p}$ in the color dipole model and its suppression in LTA. We argue that the black disk limit of LTA corresponds to $R_{A/p}=1$ and $R^{\\rm coh}_{A/p}=0.86$ for the summed and coherent scattering, respectively. Relying on an intuitive definition of the saturation scale, we show that the ratio of the saturation scales of a heavy nucleus and proton $Q_{sA}^2(b)/Q_{sp}^2(b) \\approx 1$ at small impact parameters $b$ due to the strong leading twist nuclear shadowing and diluteness of the nuclear density.","sentences":["Using the leading twist approach (LTA) to nuclear shadowing, we calculate the ratios of diffractive and usual parton distributions for a heavy nucleus (Pb) and the proton, $R_{A/p}=(f_{i/A}^{D(3)}/f_{i/A})/(f_{i/p}^{D(3)}/f_{i/p})$, for coherent and summed (coherent plus quasi-elastic) nuclear deep-inelastic scattering.","We find that $R_{A/p} \\approx 0.5-1$ for quarks and $R_{A/p} \\approx 0.5-1.3$ for gluons in a broad range of $x$, which reaffirms the difference from the nuclear enhancement of $R_{A/p}$ predicted in the gluon saturation framework.","We demonstrate that the magnitude of $R_{A/p}$ is controlled by the cross section of the interaction of hadronic fluctuations of the virtual photon with target nucleons, which explains an enhancement of $R_{A/p}$ in the color dipole model and its suppression in LTA.","We argue that the black disk limit of LTA corresponds to $R_{A/p}=1$ and $R^{\\rm coh}_{A/p}=0.86$ for the summed and coherent scattering, respectively.","Relying on an intuitive definition of the saturation scale, we show that the ratio of the saturation scales of a heavy nucleus and proton $Q_{sA}^2(b)/Q_{sp}^2(b) \\approx 1$ at small impact parameters $b$ due to the strong leading twist nuclear shadowing and diluteness of the nuclear density."],"url":"http://arxiv.org/abs/2403.08342v1","category":"hep-ph"}
{"created":"2024-03-13 07:26:50","title":"Freeness of hyperplane arrangements associated with gain graphs","abstract":"Athanasiadis studied arrangements obtained by adding shifted hyperplanes to the braid arrangement. Bailey studied arrangements obtained by adding tilted hyperplanes to the braid arrangement. These two kinds of arrangements are associated with directed graphs and their freeness was characterized in terms of the graphs. The results show coincidence of freeness. Namely, if Athanasiadis' arrangement is free, then the corresponding Bailey's arrangement is free, and vice versa.   In this paper, we generalize this phenomenon by using gain graphs.","sentences":["Athanasiadis studied arrangements obtained by adding shifted hyperplanes to the braid arrangement.","Bailey studied arrangements obtained by adding tilted hyperplanes to the braid arrangement.","These two kinds of arrangements are associated with directed graphs and their freeness was characterized in terms of the graphs.","The results show coincidence of freeness.","Namely, if Athanasiadis' arrangement is free, then the corresponding Bailey's arrangement is free, and vice versa.   ","In this paper, we generalize this phenomenon by using gain graphs."],"url":"http://arxiv.org/abs/2403.08304v1","category":"math.CO"}
{"created":"2024-03-13 07:19:11","title":"Probing the stellar populations and star formation history of early-type galaxies at $0 < z < 1.1$ in the rest-frame ultraviolet","abstract":"We measure the evolution of the rest-frame $NUV-V$ colors for early-type galaxies in clusters at $0<z<1.1$ using data from the Hyper Suprime-Cam Subaru Strategic Program (HSC-SSP), CFHT Large Area U-band Deep Survey (CLAUDS) and local SDSS clusters observed with GALEX. Our results show that there is an excess in the ultraviolet spectrum in most quiescent galaxies (compared to the expectations from models fitting their optical/infrared colors and spectra) below $z\\sim0.6$, beyond which the excess UV emission fades rapidly. This evolution of the UV color is only consistent with the presence of a highly evolved, hot horizontal branch sub-population in these galaxies (amongst the majority cool and optically bright stars), comprising on average 10\\% of the total stellar mass and forming at $z>3$. The blue UV colors of early-type galaxies at low-intermediate redshifts are likely driven by this sub-population being enriched in helium up to $\\sim44\\%$. At $z>0.8$ (when the extra UV component has not yet appeared) the data allows us to constrain the star formation histories of galaxies by fitting models to the evolution of their UV colors: we find that the epoch at which the stellar populations formed ranges between $3<z_{form}<10$ (corresponding to $0.5-2.2$ Gyrs after the Big Bang) with a star-formation e-folding timescale of $\\tau=0.35-0.7$ Gyr, suggesting that these galaxies formed the majority of stars at very high redshift, with a brief yet intense burst of star-formation activity. The star formation history and chemical evolution of early-type galaxies resemble those of globular clusters, albeit on much larger scales.","sentences":["We measure the evolution of the rest-frame $NUV-V$ colors for early-type galaxies in clusters at $0<z<1.1$ using data from the Hyper Suprime-Cam Subaru Strategic Program (HSC-SSP), CFHT Large Area U-band Deep Survey (CLAUDS) and local SDSS clusters observed with GALEX.","Our results show that there is an excess in the ultraviolet spectrum in most quiescent galaxies (compared to the expectations from models fitting their optical/infrared colors and spectra) below $z\\sim0.6$, beyond which the excess UV emission fades rapidly.","This evolution of the UV color is only consistent with the presence of a highly evolved, hot horizontal branch sub-population in these galaxies (amongst the majority cool and optically bright stars), comprising on average 10\\% of the total stellar mass and forming at $z>3$. The blue UV colors of early-type galaxies at low-intermediate redshifts are likely driven by this sub-population being enriched in helium up to $\\sim44\\%$. At $z>0.8$ (when the extra UV component has not yet appeared)","the data allows us to constrain the star formation histories of galaxies by fitting models to the evolution of their UV colors: we find that the epoch at which the stellar populations formed ranges between $3<z_{form}<10$ (corresponding to $0.5-2.2$ Gyrs after the Big Bang) with a star-formation e-folding timescale of $\\tau=0.35-0.7$ Gyr, suggesting that these galaxies formed the majority of stars at very high redshift, with a brief yet intense burst of star-formation activity.","The star formation history and chemical evolution of early-type galaxies resemble those of globular clusters, albeit on much larger scales."],"url":"http://arxiv.org/abs/2403.08301v1","category":"astro-ph.GA"}
{"created":"2024-03-13 06:49:58","title":"A fast wavefield evaluation method using a modified proxy-surface accelerated interpolative decomposition for scattering problems in two dimensions","abstract":"This paper presents a fast wavefield evaluation method for wave scattering problems. The typical previous fast method is the fast multipole method (FMM). The FMM, however, requires the combined use of non-fast direct evaluations near the boundaries of scatterers. The proposed method is based on a modified proxy-surface method accelerated interpolative decomposition. It is, therefore, effective even if evaluation points are near the boundary and, moreover, is free from analytical expansion of kernel functions unlike FMM. The validness and effectiveness of the proposed method are shown by numerical examples.","sentences":["This paper presents a fast wavefield evaluation method for wave scattering problems.","The typical previous fast method is the fast multipole method (FMM).","The FMM, however, requires the combined use of non-fast direct evaluations near the boundaries of scatterers.","The proposed method is based on a modified proxy-surface method accelerated interpolative decomposition.","It is, therefore, effective even if evaluation points are near the boundary and, moreover, is free from analytical expansion of kernel functions unlike FMM.","The validness and effectiveness of the proposed method are shown by numerical examples."],"url":"http://arxiv.org/abs/2403.08290v1","category":"math.NA"}
{"created":"2024-03-13 06:08:53","title":"Synergy between Spin and Orbital Angular Momenta on a M\u00f6bius Strip","abstract":"Spin and orbital angular momenta are fundamental physical characteristics described by polarization and spatial degrees of freedom, respectively. Polarization is a feature of vector fields while spatial phase gradient determines the orbital angular momentum ubiquitous to any scalar field. Common wisdom treats these two degrees of freedom as distinct and independent principles to manipulate wave propagations. Here, we demonstrate their synergy. This is achieved by introducing two orthogonal $p$-orbitals as eigenbases, whose spatial modal features are exploited to generate orbital angular momenta and the associated orbital orientations provide means to simultaneously manipulate polarizations. Through periodic modulation and directional coupling, we realize a full cyclic evolution of the synchronized and synergized spin-orbital angular momenta. Remarkably, this evolution acquires a nontrivial geometric phase, leading to its representation on a M\\\"obius strip. Experimentally, an acoustic cavity array is designed, whose dipole resonances precisely mimic the $p$-orbitals. The acoustic waves, uniquely, see the pressure (scalar) field as a spatial feature and carry an intrinsic polarization defined by the velocity (vector) field, serving as an ideal platform to observe the synergy of spin and orbital angular momenta. Based on such a property, we further showcase a spin-orbital-Hall effect, highlighting the intricate locking of handedness, directionality, spin density and spatial mode profile. Our study unveils a fundamental connection between spin and orbital angular momenta, promising avenues for novel applications in information coding and high-capacity communications.","sentences":["Spin and orbital angular momenta are fundamental physical characteristics described by polarization and spatial degrees of freedom, respectively.","Polarization is a feature of vector fields while spatial phase gradient determines the orbital angular momentum ubiquitous to any scalar field.","Common wisdom treats these two degrees of freedom as distinct and independent principles to manipulate wave propagations.","Here, we demonstrate their synergy.","This is achieved by introducing two orthogonal $p$-orbitals as eigenbases, whose spatial modal features are exploited to generate orbital angular momenta and the associated orbital orientations provide means to simultaneously manipulate polarizations.","Through periodic modulation and directional coupling, we realize a full cyclic evolution of the synchronized and synergized spin-orbital angular momenta.","Remarkably, this evolution acquires a nontrivial geometric phase, leading to its representation on a M\\\"obius strip.","Experimentally, an acoustic cavity array is designed, whose dipole resonances precisely mimic the $p$-orbitals.","The acoustic waves, uniquely, see the pressure (scalar) field as a spatial feature and carry an intrinsic polarization defined by the velocity (vector) field, serving as an ideal platform to observe the synergy of spin and orbital angular momenta.","Based on such a property, we further showcase a spin-orbital-Hall effect, highlighting the intricate locking of handedness, directionality, spin density and spatial mode profile.","Our study unveils a fundamental connection between spin and orbital angular momenta, promising avenues for novel applications in information coding and high-capacity communications."],"url":"http://arxiv.org/abs/2403.08276v1","category":"physics.app-ph"}
{"created":"2024-03-13 05:25:49","title":"BiTT: Bi-directional Texture Reconstruction of Interacting Two Hands from a Single Image","abstract":"Creating personalized hand avatars is important to offer a realistic experience to users on AR / VR platforms. While most prior studies focused on reconstructing 3D hand shapes, some recent work has tackled the reconstruction of hand textures on top of shapes. However, these methods are often limited to capturing pixels on the visible side of a hand, requiring diverse views of the hand in a video or multiple images as input. In this paper, we propose a novel method, BiTT(Bi-directional Texture reconstruction of Two hands), which is the first end-to-end trainable method for relightable, pose-free texture reconstruction of two interacting hands taking only a single RGB image, by three novel components: 1)\\ bi-directional (left $\\leftrightarrow$ right) texture reconstruction using the texture symmetry of left / right hands, 2) utilizing a texture parametric model for hand texture recovery, and 3)\\ the overall coarse-to-fine stage pipeline for reconstructing personalized texture of two interacting hands. BiTT first estimates the scene light condition and albedo image from an input image, then reconstructs the texture of both hands through the texture parametric model and bi-directional texture reconstructor. In experiments using InterHand2.6M and RGB2Hands datasets, our method significantly outperforms state-of-the-art hand texture reconstruction methods quantitatively and qualitatively. The code is available at https://github.com/yunminjin2/BiTT","sentences":["Creating personalized hand avatars is important to offer a realistic experience to users on AR / VR platforms.","While most prior studies focused on reconstructing 3D hand shapes, some recent work has tackled the reconstruction of hand textures on top of shapes.","However, these methods are often limited to capturing pixels on the visible side of a hand, requiring diverse views of the hand in a video or multiple images as input.","In this paper, we propose a novel method, BiTT(Bi-directional Texture reconstruction of Two hands), which is the first end-to-end trainable method for relightable, pose-free texture reconstruction of two interacting hands taking only a single RGB image, by three novel components: 1)\\ bi-directional (left $\\leftrightarrow$ right) texture reconstruction using the texture symmetry of left / right hands, 2) utilizing a texture parametric model for hand texture recovery, and 3)\\ the overall coarse-to-fine stage pipeline for reconstructing personalized texture of two interacting hands.","BiTT first estimates the scene light condition and albedo image from an input image, then reconstructs the texture of both hands through the texture parametric model and bi-directional texture reconstructor.","In experiments using InterHand2.6M and RGB2Hands datasets, our method significantly outperforms state-of-the-art hand texture reconstruction methods quantitatively and qualitatively.","The code is available at https://github.com/yunminjin2/BiTT"],"url":"http://arxiv.org/abs/2403.08262v1","category":"cs.CV"}
{"created":"2024-03-13 04:55:09","title":"The 3D Lyman-$\u03b1$ Forest Power Spectrum from eBOSS DR16","abstract":"We measure the three-dimensional power spectrum (P3D) of the transmitted flux in the Lyman-$\\alpha$ (Ly-$\\alpha$) forest using the complete extended Baryon Oscillation Spectroscopic Survey data release 16 (eBOSS DR16). This sample consists of 205,012 quasar spectra in the redshift range 2 <= z <= 4 at an effective redshift z=2.334. We propose a pair-count spectral estimator in configuration space, weighting each pair by exp(ikr), for wave vector k and pixel pair separation r, effectively measuring the anisotropic power spectrum without the need for fast Fourier transforms. This accounts for the window matrix in a tractable way, avoiding artifacts found in Fourier-transform based power spectrum estimators due to the sparse sampling transverse to the line-of-sight of Ly-$\\alpha$ skewers. We extensively test our pipeline on two sets of mocks: (i) idealized Gaussian random fields with a sparse sampling of Ly-$\\alpha$ skewers, and (ii) log-normal LyaCoLoRe mocks including realistic noise levels, the eBOSS survey geometry and contaminants. On eBOSS DR16 data, the Kaiser formula with a non-linear correction term obtained from hydrodynamic simulations yields a good fit to the power spectrum data in the range 0.02 <= k <= 0.35 h/Mpc at the 1-2 sigma level with a covariance matrix derived from LyaCoLoRe mocks. We demonstrate a promising new approach for full-shape cosmological analyses of Ly-$\\alpha$ forest data from cosmological surveys such as eBOSS, the currently observing Dark Energy Spectroscopic Instrument and future surveys such as the Prime Focus Spectrograph, WEAVE-QSO and 4MOST.","sentences":["We measure the three-dimensional power spectrum (P3D) of the transmitted flux in the Lyman-$\\alpha$ (Ly-$\\alpha$) forest using the complete extended Baryon Oscillation Spectroscopic Survey data release 16 (eBOSS DR16).","This sample consists of 205,012 quasar spectra in the redshift range 2 <= z <= 4 at an effective redshift z=2.334.","We propose a pair-count spectral estimator in configuration space, weighting each pair by exp(ikr), for wave vector k and pixel pair separation r, effectively measuring the anisotropic power spectrum without the need for fast Fourier transforms.","This accounts for the window matrix in a tractable way, avoiding artifacts found in Fourier-transform based power spectrum estimators due to the sparse sampling transverse to the line-of-sight of Ly-$\\alpha$ skewers.","We extensively test our pipeline on two sets of mocks: (i) idealized Gaussian random fields with a sparse sampling of Ly-$\\alpha$ skewers, and (ii) log-normal LyaCoLoRe mocks including realistic noise levels, the eBOSS survey geometry and contaminants.","On eBOSS DR16 data, the Kaiser formula with a non-linear correction term obtained from hydrodynamic simulations yields a good fit to the power spectrum data in the range 0.02 <= k <= 0.35 h/Mpc at the 1-2 sigma level with a covariance matrix derived from LyaCoLoRe mocks.","We demonstrate a promising new approach for full-shape cosmological analyses of Ly-$\\alpha$ forest data from cosmological surveys such as eBOSS, the currently observing Dark Energy Spectroscopic Instrument and future surveys such as the Prime Focus Spectrograph, WEAVE-QSO and 4MOST."],"url":"http://arxiv.org/abs/2403.08241v1","category":"astro-ph.CO"}
{"created":"2024-03-13 04:36:52","title":"The effect of cation-disorder on lithium transport in halide superionic conductors","abstract":"Among the chloride-based Li-ion solid electrolytes, Li$_2$ZrCl$_6$ (LZC) have emerged as potential candidates due to their affordability, moisture stability, and high ionic conductivity. LZC synthesized by solid-state heating exhibits limited Li-ion conductivity while the mechanochemical ball-milled material is more conductive. In this computational study, we integrate thermodynamic modeling, using cluster-expansion Monte Carlo, and kinetic modeling, using molecular dynamics, to investigate whether cation disorder can be achieved in LZC, and how it affects Li-ion transport. Our results indicate that fast Li-ion conductivity is induced by the activation of Li/vacancy disorder, which itself depends on the degree of Zr disorder. We find that the very high-temperature scale at which equilibrium Zr-disorder can form precludes any equilibrium synthesis processes for achieving fast Li-ion conductivity, rationalizing why only non-equilibrium synthesis methods, such as ball milling leads to good conductivity. We identify as the critical mechanism the lack of Li/vacancy disorder near room temperature when Zr is well-ordered. Our simulations further show that the Li/vacancy order-disorder transition temperature is lowered by Zr disorder, which is necessary for creating high Li diffusivity at room temperature. The insights obtained from this study raise a challenge for the large-scale production of these materials and the potential for the long-term stability of their properties.","sentences":["Among the chloride-based Li-ion solid electrolytes, Li$_2$ZrCl$_6$ (LZC) have emerged as potential candidates due to their affordability, moisture stability, and high ionic conductivity.","LZC synthesized by solid-state heating exhibits limited Li-ion conductivity while the mechanochemical ball-milled material is more conductive.","In this computational study, we integrate thermodynamic modeling, using cluster-expansion Monte Carlo, and kinetic modeling, using molecular dynamics, to investigate whether cation disorder can be achieved in LZC, and how it affects Li-ion transport.","Our results indicate that fast Li-ion conductivity is induced by the activation of Li/vacancy disorder, which itself depends on the degree of Zr disorder.","We find that the very high-temperature scale at which equilibrium Zr-disorder can form precludes any equilibrium synthesis processes for achieving fast Li-ion conductivity, rationalizing why only non-equilibrium synthesis methods, such as ball milling leads to good conductivity.","We identify as the critical mechanism the lack of Li/vacancy disorder near room temperature when Zr is well-ordered.","Our simulations further show that the Li/vacancy order-disorder transition temperature is lowered by Zr disorder, which is necessary for creating high Li diffusivity at room temperature.","The insights obtained from this study raise a challenge for the large-scale production of these materials and the potential for the long-term stability of their properties."],"url":"http://arxiv.org/abs/2403.08237v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-13 04:24:26","title":"A Parallel Beam Splitting Based on Gradient Metasurface: Preparation and Fusion of Quantum Entanglement","abstract":"Gradient metasurface, formed by a set of subwavelength unit cells with different phase modulation, is widely used in polarized beam splitting (BS) in the classical and quantum optics. Specifically, its phase gradient allows the path and polarization of multiple output lights to be locked by corresponding inputs.Using this unique path-polarization locked property, we demonstrate that the single metasurface can function as sequentially linked beamsplitters, enabling the parallelization of a series of BS processes. Such a parallel BS metasurface provides a multi-beam interference capability for both classical and quantum light manipulation. Taking this advantage, we first prepare path and polarization hybrid entangled states of two, three, and multi photons from unentangled photon sources. Then, the ability of parallel BS-facilitated entanglement is applied to demonstrate entanglement fusion among entangled photon pairs, which can greatly enlarge the entanglement dimension. The principle of parallel BS through the metasurface opens up a versatile way to manipulate the quantum state at the micro/nano scale, which will have potential applications in on-chip quantum optics and quantum information processing.","sentences":["Gradient metasurface, formed by a set of subwavelength unit cells with different phase modulation, is widely used in polarized beam splitting (BS) in the classical and quantum optics.","Specifically, its phase gradient allows the path and polarization of multiple output lights to be locked by corresponding inputs.","Using this unique path-polarization locked property, we demonstrate that the single metasurface can function as sequentially linked beamsplitters, enabling the parallelization of a series of BS processes.","Such a parallel BS metasurface provides a multi-beam interference capability for both classical and quantum light manipulation.","Taking this advantage, we first prepare path and polarization hybrid entangled states of two, three, and multi photons from unentangled photon sources.","Then, the ability of parallel BS-facilitated entanglement is applied to demonstrate entanglement fusion among entangled photon pairs, which can greatly enlarge the entanglement dimension.","The principle of parallel BS through the metasurface opens up a versatile way to manipulate the quantum state at the micro/nano scale, which will have potential applications in on-chip quantum optics and quantum information processing."],"url":"http://arxiv.org/abs/2403.08233v1","category":"physics.optics"}
{"created":"2024-03-13 04:02:14","title":"Probing the heavy Higgs boson production and decay $H_0$ of the Bestest Little Higgs Model at the LHC and the FCC-hh","abstract":"In the Bestest Little Higgs Model (BLHM) scenario, we analyze the branching ratios and production cross-section of the heavy Higgs boson $H_0$. The analysis is performed at the tree level and the one-loop level. In addition, we present results of the possible production of the heavy Higgs boson $H_0$ via gluon fusion for the center-of-mass energies and the integrated luminosities of the LHC, HE-LHC, HL-LHC, and FCC-hh. Our results show a very optimistic scenario for studying the $H_0$ scalar predicted by the BLHM and for the energies and luminosities of current and future hadron colliders.","sentences":["In the Bestest Little Higgs Model (BLHM) scenario, we analyze the branching ratios and production cross-section of the heavy Higgs boson $H_0$. The analysis is performed at the tree level and the one-loop level.","In addition, we present results of the possible production of the heavy Higgs boson $H_0$ via gluon fusion for the center-of-mass energies and the integrated luminosities of the LHC, HE-LHC, HL-LHC, and FCC-hh.","Our results show a very optimistic scenario for studying the $H_0$ scalar predicted by the BLHM and for the energies and luminosities of current and future hadron colliders."],"url":"http://arxiv.org/abs/2403.08225v1","category":"hep-ph"}
{"created":"2024-03-13 03:03:15","title":"Discrete Semantic Tokenization for Deep CTR Prediction","abstract":"Incorporating item content information into click-through rate (CTR) prediction models remains a challenge, especially with the time and space constraints of industrial scenarios. The content-encoding paradigm, which integrates user and item encoders directly into CTR models, prioritizes space over time. In contrast, the embedding-based paradigm transforms item and user semantics into latent embeddings and then caches them, prioritizes space over time. In this paper, we introduce a new semantic-token paradigm and propose a discrete semantic tokenization approach, namely UIST, for user and item representation. UIST facilitates swift training and inference while maintaining a conservative memory footprint. Specifically, UIST quantizes dense embedding vectors into discrete tokens with shorter lengths and employs a hierarchical mixture inference module to weigh the contribution of each user--item token pair. Our experimental results on news recommendation showcase the effectiveness and efficiency (about 200-fold space compression) of UIST for CTR prediction.","sentences":["Incorporating item content information into click-through rate (CTR) prediction models remains a challenge, especially with the time and space constraints of industrial scenarios.","The content-encoding paradigm, which integrates user and item encoders directly into CTR models, prioritizes space over time.","In contrast, the embedding-based paradigm transforms item and user semantics into latent embeddings and then caches them, prioritizes space over time.","In this paper, we introduce a new semantic-token paradigm and propose a discrete semantic tokenization approach, namely UIST, for user and item representation.","UIST facilitates swift training and inference while maintaining a conservative memory footprint.","Specifically, UIST quantizes dense embedding vectors into discrete tokens with shorter lengths and employs a hierarchical mixture inference module to weigh the contribution of each user--item token pair.","Our experimental results on news recommendation showcase the effectiveness and efficiency (about 200-fold space compression) of UIST for CTR prediction."],"url":"http://arxiv.org/abs/2403.08206v1","category":"cs.IR"}
{"created":"2024-03-13 01:31:36","title":"SN 2023zaw: an ultra-stripped, nickel-poor supernova from a low-mass progenitor","abstract":"We present SN 2023zaw $-$ a sub-luminous ($\\mathrm{M_r} = -16.7$ mag) and rapidly-evolving supernova ($\\mathrm{t_{1/2,r}} = 4.9$ days), with the lowest nickel mass ($\\approx0.002$ $\\mathrm{M_\\odot}$) measured among all stripped-envelope supernovae discovered to date. The photospheric spectra are dominated by broad He I and Ca NIR emission lines with velocities of $\\sim10\\ 000 - 12\\ 000$ $\\mathrm{km\\ s^{-1}}$. The late-time spectra show prominent narrow He I emission lines at $\\sim$1000$\\ \\mathrm{km\\ s^{-1}}$, indicative of interaction with He-rich circumstellar material. SN 2023zaw is located in the spiral arm of a star-forming galaxy. We perform radiation-hydrodynamical and analytical modeling of the lightcurve by fitting with a combination of shock-cooling emission and nickel decay. The progenitor has a best-fit envelope mass of $\\approx0.2$ $\\mathrm{M_\\odot}$ and an envelope radius of $\\approx50$ $\\mathrm{R_\\odot}$. The extremely low nickel mass and low ejecta mass ($\\approx0.5$ $\\mathrm{M_\\odot}$) suggest an ultra-stripped SN, which originates from a mass-losing low mass He-star (ZAMS mass $<$ 10 $\\mathrm{M_\\odot}$) in a close binary system. This is a channel to form double neutron star systems, whose merger is detectable with LIGO. SN 2023zaw underscores the existence of a previously undiscovered population of extremely low nickel mass ($< 0.005$ $\\mathrm{M_\\odot}$) stripped-envelope supernovae, which can be explored with deep and high-cadence transient surveys.","sentences":["We present SN 2023zaw $-$ a sub-luminous ($\\mathrm{M_r} = -16.7$ mag) and rapidly-evolving supernova ($\\mathrm{t_{1/2,r}} = 4.9$ days), with the lowest nickel mass ($\\approx0.002$ $\\mathrm{M_\\odot}$) measured among all stripped-envelope supernovae discovered to date.","The photospheric spectra are dominated by broad","He I and Ca NIR emission lines with velocities of $\\sim10\\ 000 - 12\\ 000$ $\\mathrm{km\\ s^{-1}}$.","The late-time spectra show prominent narrow","He I emission lines at $\\sim$1000$\\ \\mathrm{km\\ s^{-1}}$, indicative of interaction with He-rich circumstellar material.","SN 2023zaw is located in the spiral arm of a star-forming galaxy.","We perform radiation-hydrodynamical and analytical modeling of the lightcurve by fitting with a combination of shock-cooling emission and nickel decay.","The progenitor has a best-fit envelope mass of $\\approx0.2$ $\\mathrm{M_\\odot}$ and an envelope radius of $\\approx50$ $\\mathrm{R_\\odot}$. The extremely low nickel mass and low ejecta mass ($\\approx0.5$ $\\mathrm{M_\\odot}$) suggest an ultra-stripped SN, which originates from a mass-losing low mass He-star (ZAMS mass $<$ 10 $\\mathrm{M_\\odot}$) in a close binary system.","This is a channel to form double neutron star systems, whose merger is detectable with LIGO.","SN 2023zaw underscores the existence of a previously undiscovered population of extremely low nickel mass ($< 0.005$ $\\mathrm{M_\\odot}$) stripped-envelope supernovae, which can be explored with deep and high-cadence transient surveys."],"url":"http://arxiv.org/abs/2403.08165v1","category":"astro-ph.HE"}
{"created":"2024-03-13 01:25:18","title":"Effective Underwater Glider Path Planning in Dynamic 3D Environments Using Multi-Point Potential Fields","abstract":"Underwater gliders (UGs) have emerged as highly effective unmanned vehicles for ocean exploration. However, their operation in dynamic and complex underwater environments necessitates robust path-planning strategies. Previous studies have primarily focused on global energy or time-efficient path planning in explored environments, overlooking challenges posed by unpredictable flow conditions and unknown obstacles in varying and dynamic areas like fjords and near-harbor waters. This paper introduces and improves a real-time path planning method, Multi-Point Potential Field (MPPF), tailored for UGs operating in 3D space as they are constrained by buoyancy propulsion and internal actuation. The proposed MPPF method addresses obstacles, flow fields, and local minima, enhancing the efficiency and robustness of UG path planning. A low-cost prototype, the Research Oriented Underwater Glider for Hands-on Investigative Engineering (ROUGHIE), is utilized for validation. Through case studies and simulations, the efficacy of the enhanced MPPF method is demonstrated, highlighting its potential for real-world applications in underwater exploration.","sentences":["Underwater gliders (UGs) have emerged as highly effective unmanned vehicles for ocean exploration.","However, their operation in dynamic and complex underwater environments necessitates robust path-planning strategies.","Previous studies have primarily focused on global energy or time-efficient path planning in explored environments, overlooking challenges posed by unpredictable flow conditions and unknown obstacles in varying and dynamic areas like fjords and near-harbor waters.","This paper introduces and improves a real-time path planning method, Multi-Point Potential Field (MPPF), tailored for UGs operating in 3D space as they are constrained by buoyancy propulsion and internal actuation.","The proposed MPPF method addresses obstacles, flow fields, and local minima, enhancing the efficiency and robustness of UG path planning.","A low-cost prototype, the Research Oriented Underwater Glider for Hands-on Investigative Engineering (ROUGHIE), is utilized for validation.","Through case studies and simulations, the efficacy of the enhanced MPPF method is demonstrated, highlighting its potential for real-world applications in underwater exploration."],"url":"http://arxiv.org/abs/2403.08163v1","category":"cs.RO"}
{"created":"2024-03-13 00:37:31","title":"Enhancing Space Situational Awareness to Mitigate Risk: A Case Study in the Misidentification of Starlink Satellites as UAP in Commercial Aviation","abstract":"Over the past several years, the misidentification of SpaceX Starlink satellites as Unidentified Aerial Phenomena (UAP) by pilots and laypersons has generated unnecessary aviation risk and confusion. The many deployment and orbital evolution strategies, coupled with changing sun specular reflection angles, contribute to this gap in space situational awareness. In this paper we present a case analysis of an incident that generated multiple, corroborating reports of a UAP from five pilots on two commercial airline flights over the Pacific Ocean on August 10th, 2022. This incident included two cell phone photos and a video of an unrecognizable and possibly anomalous phenomenon. We then use supplemental two-line elements (TLEs) for the Starlink train of satellites launched that same day and Automatic Dependent Surveillance Broadcast (ADS-B) data from the flight with the photographs to reconstruct a view of these satellites from the cockpit at the time and place of the sighting. The success of this work demonstrates an approach that could, in principle, warn aviators about satellites that could be visible in unusual or novel illumination configurations, thus increasing space situational awareness and supporting aviation safety. We conclude with recommendations for governments and satellite operators to provide better a-priori data that can be used to create advisories to aviators and the public. The automated simulation of known specular reflection off constellations of satellites could also support researchers investigating sightings of unfamiliar aerial or aerospace objects as likely being from normal versus novel space events.","sentences":["Over the past several years, the misidentification of SpaceX Starlink satellites as Unidentified Aerial Phenomena (UAP) by pilots and laypersons has generated unnecessary aviation risk and confusion.","The many deployment and orbital evolution strategies, coupled with changing sun specular reflection angles, contribute to this gap in space situational awareness.","In this paper we present a case analysis of an incident that generated multiple, corroborating reports of a UAP from five pilots on two commercial airline flights over the Pacific Ocean on August 10th, 2022.","This incident included two cell phone photos and a video of an unrecognizable and possibly anomalous phenomenon.","We then use supplemental two-line elements (TLEs) for the Starlink train of satellites launched that same day and Automatic Dependent Surveillance Broadcast (ADS-B) data from the flight with the photographs to reconstruct a view of these satellites from the cockpit at the time and place of the sighting.","The success of this work demonstrates an approach that could, in principle, warn aviators about satellites that could be visible in unusual or novel illumination configurations, thus increasing space situational awareness and supporting aviation safety.","We conclude with recommendations for governments and satellite operators to provide better a-priori data that can be used to create advisories to aviators and the public.","The automated simulation of known specular reflection off constellations of satellites could also support researchers investigating sightings of unfamiliar aerial or aerospace objects as likely being from normal versus novel space events."],"url":"http://arxiv.org/abs/2403.08155v1","category":"physics.soc-ph"}
{"created":"2024-03-13 00:00:35","title":"Parallel Diffusion Coefficient of Energetic Charged Particles in the Inner Heliosphere from the Turbulent Magnetic Fields Measured by Parker Solar Probe","abstract":"Diffusion coefficients of energetic charged particles in turbulent magnetic fields are a fundamental aspect of diffusive transport theory but remain incompletely understood. In this work, we use quasi-linear theory to evaluate the spatial variation of the parallel diffusion coefficient $\\kappa_\\parallel$ from the measured magnetic turbulence power spectra in the inner heliosphere. We consider the magnetic field and plasma velocity measurements from Parker Solar Probe made during Orbits 5-13. The parallel diffusion coefficient is calculated as a function of radial distance from 0.062 to 0.8 AU, and the particle energy from 100 keV to 1GeV. We find that $\\kappa_\\parallel$ increases exponentially with both heliocentric distance and energy of particles. The fluctuations in $\\kappa_\\parallel$ are related to the episodes of large-scale magnetic structures in the solar wind. By fitting the results, we also provide an empirical formula of $\\kappa_{\\parallel}=(5.16\\pm1.22) \\times 10^{18} \\: r^{1.17\\pm0.08} \\: E^{0.71\\pm 0.02} \\; (cm^2/s)$ in the inner heliosphere which can be used as a reference in studying the transport and acceleration of solar energetic particles as well as the modulation of cosmic rays.","sentences":["Diffusion coefficients of energetic charged particles in turbulent magnetic fields are a fundamental aspect of diffusive transport theory but remain incompletely understood.","In this work, we use quasi-linear theory to evaluate the spatial variation of the parallel diffusion coefficient $\\kappa_\\parallel$ from the measured magnetic turbulence power spectra in the inner heliosphere.","We consider the magnetic field and plasma velocity measurements from Parker Solar Probe made during Orbits 5-13.","The parallel diffusion coefficient is calculated as a function of radial distance from 0.062 to 0.8 AU, and the particle energy from 100 keV to 1GeV.","We find that $\\kappa_\\parallel$ increases exponentially with both heliocentric distance and energy of particles.","The fluctuations in $\\kappa_\\parallel$ are related to the episodes of large-scale magnetic structures in the solar wind.","By fitting the results, we also provide an empirical formula of $\\kappa_{\\parallel}=(5.16\\pm1.22) \\times 10^{18} \\: r^{1.17\\pm0.08} \\: E^{0.71\\pm 0.02} \\; (cm^2/s)$ in the inner heliosphere which can be used as a reference in studying the transport and acceleration of solar energetic particles as well as the modulation of cosmic rays."],"url":"http://arxiv.org/abs/2403.08141v1","category":"astro-ph.SR"}
{"created":"2024-03-12 23:43:25","title":"Coupling between magnetic reconnection, energy release, and particle acceleration in the X17.2 2003 October 28 solar flare","abstract":"The 2003 October 28 (X17.2) eruptive flare was a unique event. The coronal electric field and the {\\pi}-decay {\\gamma}-ray emission flux had the highest values ever inferred in solar flares. This study reveals physical links between the magnetic reconnection process, the energy release, and the acceleration of electrons and ions to high energies in the chain of the magnetic energy transformations in the impulsive phase of the solar flare. The global reconnection rate and the local reconnection rate are calculated from flare ribbon separation in H{\\alpha} filtergrams and photospheric magnetic field maps. Available results of INTEGRAL and CORONAS-F/SONG observations are combined with Konus-Wind data to quantify time behavior of electron and proton acceleration. Prompt {\\gamma}-ray lines and delayed 2.2 MeV line temporal profiles observed with Konus-Wind and INTEGRAL/SPI used to detect and quantify the nuclei with energies of 10-70 MeV. The global and local reconnection rates reach their peaks at the end of the main rise phase of the flare. The spectral analysis of the high-energy {\\gamma}-ray emission revealed a close association between the acceleration process efficiency and the reconnection rates. High-energy bremsstrahlung continuum and narrow {\\gamma}-ray lines were observed in the main rise phase. In the main energy release phase, the upper energy of the bremsstrahlung spectrum was significantly reduced and the pion-decay {\\gamma}-ray emission appeared abruptly. We discuss the reasons why the change of the acceleration regime occurred along with the large-scale magnetic field restructuration of this flare. We argue that the main energy release and proton acceleration up to subrelativistic energies began just when the reconnection rate was going through the maximum, i.e., after a major change of the flare topology.","sentences":["The 2003 October 28 (X17.2) eruptive flare was a unique event.","The coronal electric field and the {\\pi}-decay {\\gamma}-ray emission flux had the highest values ever inferred in solar flares.","This study reveals physical links between the magnetic reconnection process, the energy release, and the acceleration of electrons and ions to high energies in the chain of the magnetic energy transformations in the impulsive phase of the solar flare.","The global reconnection rate and the local reconnection rate are calculated from flare ribbon separation in H{\\alpha} filtergrams and photospheric magnetic field maps.","Available results of INTEGRAL and CORONAS-F/SONG observations are combined with Konus-Wind data to quantify time behavior of electron and proton acceleration.","Prompt {\\gamma}-ray lines and delayed 2.2 MeV line temporal profiles observed with Konus-Wind and INTEGRAL/SPI used to detect and quantify the nuclei with energies of 10-70 MeV. The global and local reconnection rates reach their peaks at the end of the main rise phase of the flare.","The spectral analysis of the high-energy {\\gamma}-ray emission revealed a close association between the acceleration process efficiency and the reconnection rates.","High-energy bremsstrahlung continuum and narrow {\\gamma}-ray lines were observed in the main rise phase.","In the main energy release phase, the upper energy of the bremsstrahlung spectrum was significantly reduced and the pion-decay {\\gamma}-ray emission appeared abruptly.","We discuss the reasons why the change of the acceleration regime occurred along with the large-scale magnetic field restructuration of this flare.","We argue that the main energy release and proton acceleration up to subrelativistic energies began just when the reconnection rate was going through the maximum, i.e., after a major change of the flare topology."],"url":"http://arxiv.org/abs/2403.08135v1","category":"astro-ph.SR"}
{"created":"2024-03-12 23:42:28","title":"Evaluation of the AMOEBA force field for simulating metal halide perovskites in the solid state and in solution","abstract":"In this work, we compare existing non-polarizable force fields developed to study the solid or solution phases of hybrid organic-inorganic halide perovskites with the AMOEBA polarizable force field. The aim is to test whether more computationally expensive polarizable force fields like AMOEBA offer better transferability between solution and solid phases, with the ultimate goal being the study of crystal nucleation, growth and other interfacial phenomena involving these ionic compounds. In the context of hybrid perovskites, AMOEBA force field parameters already exist for several elements in solution and we decided to leave them unchanged and to only parameterize the missing ones (Pb\\textsuperscript{2+} and CH\\textsubscript{3}NH\\textsubscript{3}\\textsuperscript{+} ions) in order to maximise transferability and avoid over-fitting to the specific examples studied here. Overall, we find that AMOEBA yields accurate hydration free energies (within 5\\%) for typical ionic species while showing the correct ordering of stability for the different crystal polymorphs of CsPbI\\textsubscript{3} and CH\\textsubscript{3}NH\\textsubscript{3}PbI\\textsubscript{3}. While the existing parameters do not accurately reproduce all transition temperatures and lattice parameters, AMOEBA offers better transferability between solution and solid states than existing non-polarizable force fields.","sentences":["In this work, we compare existing non-polarizable force fields developed to study the solid or solution phases of hybrid organic-inorganic halide perovskites with the AMOEBA polarizable force field.","The aim is to test whether more computationally expensive polarizable force fields like AMOEBA offer better transferability between solution and solid phases, with the ultimate goal being the study of crystal nucleation, growth and other interfacial phenomena involving these ionic compounds.","In the context of hybrid perovskites, AMOEBA force field parameters already exist for several elements in solution and we decided to leave them unchanged and to only parameterize the missing ones (Pb\\textsuperscript{2+} and CH\\textsubscript{3}NH\\textsubscript{3}\\textsuperscript{+} ions) in order to maximise transferability and avoid over-fitting to the specific examples studied here.","Overall, we find that AMOEBA yields accurate hydration free energies (within 5\\%) for typical ionic species while showing the correct ordering of stability for the different crystal polymorphs of CsPbI\\textsubscript{3} and CH\\textsubscript{3}NH\\textsubscript{3}PbI\\textsubscript{3}.","While the existing parameters do not accurately reproduce all transition temperatures and lattice parameters, AMOEBA offers better transferability between solution and solid states than existing non-polarizable force fields."],"url":"http://arxiv.org/abs/2403.08134v1","category":"cond-mat.mtrl-sci"}
